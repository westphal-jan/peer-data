{"id": "1703.01560", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2017", "title": "LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation", "abstract": "We present LR-GAN: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks (GANs), the proposed GAN learns to generate image background and foregrounds separately and recursively, and stitch the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape and pose. The whole model is unsupervised, and is trained in an end-to-end manner with gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than DCGAN.", "histories": [["v1", "Sun, 5 Mar 2017 05:17:56 GMT  (7175kb,D)", "http://arxiv.org/abs/1703.01560v1", "21 pages, 22 figures, published as conference paper at ICLR 2017"], ["v2", "Sat, 1 Jul 2017 10:30:54 GMT  (7378kb,D)", "http://arxiv.org/abs/1703.01560v2", "21 pages, 22 figures, published as a conference paper at ICLR 2017, code available"], ["v3", "Wed, 2 Aug 2017 03:51:57 GMT  (7378kb,D)", "http://arxiv.org/abs/1703.01560v3", "21 pages, 22 figures, published as a conference paper at ICLR 2017, code available on GitHub"]], "COMMENTS": "21 pages, 22 figures, published as conference paper at ICLR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["jianwei yang", "anitha kannan", "dhruv batra", "devi parikh"], "accepted": true, "id": "1703.01560"}, "pdf": {"name": "1703.01560.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jianwei Yang", "Anitha Kannan"], "emails": ["jw2yang@vt.edu", "akannan@fb.com", "parikh}@gatech.edu"], "sections": [{"heading": null, "text": "We present LR-GAN: a contradictory image generation model that takes scene structure and context into account. Unlike previous generative opposing networks (GANs), the proposed GAN learns to generate background and foregrounds separately and recursively, and to sew foregrounds to the background in a context-relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape and pose; the entire model is unattended and trained continuously with gradient descent methods; the experiments show that LR-GAN can produce more natural images with objects that are more recognizable to humans than DCGAN."}, {"heading": "1 INTRODUCTION", "text": "In fact, it is so that it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is"}, {"heading": "2 RELATED WORK", "text": "In recent years, the number of people who are able to increase significantly, both in terms of the number of people who are in the city and in terms of the number of people who are in the city, has doubled in the last ten years, the number of people who are in the city has doubled in the last ten years, the number of people who are in the city has doubled, the number of people who are in the city has doubled, the number of people who are in the city has doubled, the number of people who are in the city has doubled, the number of people who are in the city has doubled."}, {"heading": "3 PRELIMINARIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 GENERATIVE ADVERSARIAL NETWORKS", "text": "Generative Adversarial Networks (GANs) consist of a generator G and a discriminator D, which are trained simultaneously with competing targets: Generator G is trained to generate samples that can \"deceive\" a discriminator D. While the discriminator is trained to classify its inputs as either real (from the training data set) or fake (from the samples of G. This competition results in a Minmax formulation with a value function: (1), in which z is a random vector from a standard multivariate Gaussian or a uniform distribution pz (z)]] + Ez-pz (z) [log (G (z); \u03b8G)]] where z is a random vector from a standard Gaussian or uniform distribution pz (z), G (G) is the mapping z to the data space, D (x) is the probability that x is realized."}, {"heading": "3.2 LAYERED STRUCTURE OF IMAGE", "text": "An image from our 3D world typically contains a multi-layer structure. One way to represent an image layer is in its appearance and shape. An image x with two layers, foreground f and background b can be factored in as: x = f m + b (1 \u2212 m), (2) where m is the mask that represents the shapes of the image layers and the elemental multiplication operator. Some existing methods assume that accessing the shape of the object occurs either during training (Isola & Liu, 2013) or both during training and during testing (Reed et al., 2016a; Yan et al., 2015). Displaying images in multi-layer structures is even easy for videos with moving objects (Darrell & Pentland, 1991; Wang & Adelson et al., 2005). Vondrick et al. (2016) generates videos by generating a fixed background and moving foregrounds separately."}, {"heading": "4 LAYERED RECURSIVE GAN (LR-GAN)", "text": "The basic structure of LR-GAN is similar to GAN: It consists of a discriminator and a generator, which are trained simultaneously with the Minmax formulation of GAN, as described in \u00a7.3.1. The most important innovation of our work is the layering of recursive generators, which we describe in this paragraph.The generator in LR-GAN is recursive because the image is constructed recursively using a recursive network. In this recursive step, an object layer is composed, which is \"glued\" to the previously generated image. Object layer near-time is parameterized by the following three components: \"canonical\" appearance \"ft, shape (or mask) mt, and pose (or affine transformation) in which the object is warped before inserting into the image composition."}, {"heading": "4.1 DETAILS OF GENERATOR ARCHITECTURE", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "4.2 NEW EVALUATION METRICS", "text": "In fact, most of them are not \"yes,\" but \"no,\" \"no,\" \"no,\" \"no,\" \"no,\" \"no,\" \"no,\" \"no,\" \"no,\" \"no,\" \"no,\" \"no,\" \"no,\" \"no,\" \",\", \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\", \"\", \"\", \"\", \"\", \",\" \",\", \",\", \"\", \",\" \",\", \",\" \",\", \"\", \"\", \",\" \",\" \",\" \",\" \",\", \"\", \",\" \",\", \",\", \",\" \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\" \"\", \",\", \"\", \",\", \",\", \",\" \"\", \",\", \",\", \"\", \",\", \"\", \",\" \",\", \",\", \",\", \"\", \",\", \",\", \",\", \",\", \"\", \",\", \",\", \",\" \",\", \",\", \",\", \",\", \"\", \",\", \",\", \",\", \"\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\" \",\", \",\", \",\", \",\" \",\", \",\", \",\", \",\" \",\", \",\", \"\", \",\", \",\" \",\", \",\" \",\", \",\", \",\" \",\", \"\", \"\", \",\", \",\", \",\", \",\" \"\", \",\", \",\", \",\", \",\", \",\", \","}, {"heading": "5 EXPERIMENT", "text": "We perform qualitative and quantitative assessments on three sets of data: 1) MNIST (LeCun et al., 1998); 2) CIFAR-10 (Krizhevsky & Hinton, 2009); 3) CUB-200 (Welinder et al., 2010).To add the variability of MNIST images, we need to go back to 32 \u00d7 32 \u00b0 C. Each image has a different background grayscale value and a different transformed number than the digits 48 \u00d7 48 uniform backgrounds with a random grayscale between 32 \u00b0 C."}, {"heading": "5.1 QUALITATIVE RESULTS", "text": "Figures 3 and 4 show the generated samples for CIFAR-10 and CUB-200, respectively. MNIST results are shown in the next subsection. As we can see from the images, the composition of our model means that the images do not merge artifacts between background and foreground. In CIFAR-10, we see horses and cars with clear shapes. In CUB-200, bird shapes tend to be even sharper."}, {"heading": "5.2 MNIST-ONE AND MNIST-TWO", "text": "Fig. 5 shows the generation results of our model on MNIST-ONE. As we can see, our model generates the background and foreground in a separate timeframe and can almost perfectly decouple the foreground numbers from the background. Although the initial values of the mask are randomly distributed in the range of (0, 1) after the training, the masks are almost binary and precisely cut the numbers out of the generated foreground. Further results on MNIST-ONE (including human studies) can be found in the appendix (Section 6.3). Fig. 6 shows the generation results for MNIST-TWO. Similarly, the model is also able to generate the background and the two foreground objects separately. The foreground generator tends to generate a single digit number for each timeframe. In the meantime, it captures the context information from the previous time steps. If the first digit on the second left side is placed on the second on the right-hand side of the tenth, it will teleconference the second."}, {"heading": "5.3 CUB-200", "text": "We examine the effectiveness of our model, which was trained on the CUB-200 bird dataset. In Fig. 1, we showed a random number of generated images, together with the intermediate results of the model. While we are completely unattended, the model is able to successfully untangle foreground and background for a large part of the samples, as demonstrated by the bird-like masks generated. We perform a comparative study based on the Amazon Mechanical Turk (AMT) between DCGAN and LRGAN to quantify the relative visual quality of the generated images. We first generated 1000 samples from both models. We then performed a perfect match between the two image sets using the Hungarian algorithm on L2 normal pitch in the pixel space. This resulted in 1000 image pairs. Some examination pairs are shown in Fig. 7. For each image pair, 9 judges are asked to select the more realistic image pair based on our majority tuning of 38.4%, we determine that 3AN is varied to 0."}, {"heading": "5.4 CIFAR-10", "text": "In fact, most of them are able to survive on their own."}, {"heading": "5.5 IMPORTANCE OF TRANSFORMATIONS", "text": "Fig. 11 shows the results of a worn-out model without affinity transformations in the foreground planes and compares the results with the complete model in which these transformations are included. We note that a significant problem occurs that the decompositions are degenerated, in the sense that the model is not able to break the symmetry between foreground and background planes, often creating object phenomena in the background plane of the model and vice versa. In CUB-200, the final generated images have some mixing between foreground and background. This is especially the case for images without bird-shaped masks. In CIFAR-10, a number of generated masks are reversed. In this case, the background images are cut out as foreground objects. The foreground generator assumes almost all of the obligation to produce the final images, making it more difficult to generate images as clear as the model with transformations."}, {"heading": "5.6 IMPORTANCE OF SHAPES", "text": "We perform another ablation study by removing the mask generator to understand the meaning of modeling object shapes. In this case, the generated foreground is simply glued to the generated background after transformation. There is no alpha blending between foreground and background. Generating results for three sets of data, MNIST-ONE, CUB-200, CIFAR-10, are in Fig. 12. As we can see, the model works well for generating MNIST-ONE, but it does not create reasonable images in the other sets of data. Specifically, the training does not even run for CUB-200. Based on these results, we qualitatively show that the mask generator in our model is quite important for achieving plausible results, especially for realistic images."}, {"heading": "6 APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 ALGORITHM", "text": "Algo. 1 illustrates the generation process in our model. g (?) evaluates the function g at?. \u0445 is a composition operator that composes its operands in such a way that f-g (?) = f (g (?)). Algorithm 1 Stochastic Layered Recursive Image Generation 1: z0 \u0445 N (0, I) 2: x0 = Gb (z0). Background generator 3: h0l \u2190 0 4: c0l \u2190 0 5: for t [1 \u00b7 T] do 6: zt \u0445 N (0, I) 7: htl, c t l \u2190 LSTM ([[zt, h t \u2212 1 l, c t \u2212 1 l]). Pass through LSTM8: if t = 1 then 9: yt \u2190 htl10: otherwise 11: yt \u2190 Elf ([htl h h \u2212 1 f]. Pass through non-linear embedding layers E l f 12: end if 13: st Gcf (ft) object is divided if Gst (15)."}, {"heading": "6.2 MODEL CONFIGURATIONS", "text": "Table 2 lists the information and model configuration for different datasets, the dimensions of random vectors and hidden vectors are all set to 100. We also compare the number of parameters in DCGAN and LR-GAN. The numbers before \"/\" are our model, after \"/\" are DCGAN. Based on the same notation used in (Zhao et al., 2016), the architectures for the different datasets are:"}, {"heading": "6.3 RESULTS ON MNIST-ONE", "text": "We perform human studies on the results of the generation MNIST-ONE. Specifically, we produce 1,000 images using both LR-GAN and DCGAN. As references, we also include 1,000 real images. Then, we ask the users on AMT to designate each image as one of the digits (0-9). We also offer them an option \"not recognizable\" if the generated image does not contain a digit. Each image was evaluated by 5 unique workers. Similar to CIFAR-10, if an image is recognized by all 5 users as the same digit, it is assigned to quality level 5. If it is not recognizable by all users, it is assigned to quality level 0. Fig. 13 (left) shows the number of images assigned to all six quality levels. Compared to DCGAN, our model has generated more samples with a high quality level."}, {"heading": "6.4 MORE RESULTS ON CUB-200", "text": "In this experiment, we reduce the minimum allowed object size to 1.1, allowing the model to create larger foreground objects. Results are in Fig. 15. Similar to the results, when the constraint is 1.2, the sharp bird-like masks are automatically generated by our model."}, {"heading": "6.5 MORE RESULTS ON CIFAR-10", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.5.1 QUALITATIVE RESULTS", "text": "In Fig. 16, we show more results on CIFAR-10 if we set the minimum allowed object scale to 1.1. The right column block also shows the training images that come closest to the generated images (cosinal similarity in the pixel space). We see that our model does not store the training data."}, {"heading": "6.5.2 WALKING IN THE LATENT SPACE", "text": "Similar to DCGAN, we also show results by walking in latent space. Note that our model has two or more inputs, so we can walk along each of them or their combination. In Fig. 17, we create multiple foregrounds for the same predetermined generated background. We find that our model consistently creates context-compatible foregrounds. For example, the foreground generator for the grassy backgrounds creates horses and deer and airplane-like objects for the blue sky."}, {"heading": "6.5.3 WORD CLOUD BASED ON HUMAN STUDY", "text": "As mentioned above, we conducted human studies on CIFAR-10. In addition to asking people to select a name from a list for an image, we also conducted another human study in which we asked people to use a word (freeform) to describe the main object in the image. Each image was \"named\" by five unique persons. We create word clouds for real images, images created by DCGAN and LR-GAN, as shown in Fig. 18."}, {"heading": "6.6 RESULTS ON LFW FACE DATASET", "text": "Unlike previous work that used cropped and aligned faces, we directly create the original images that contain a large portion of the backgrounds.This configuration helps to verify the efficiency of LR-GAN to model the appearance, shape, and pose of the object. In Fig. 19, we show the (intermediate) generation results of LR-GAN. Surprisingly, the unsupervised model creates background and faces in separate steps, and the masks created accurately represent face shapes.In addition, the model learns where the generated faces must be placed so that the entire image becomes natural.For comparison, see (Kwak & Zhang, 2016), which does not model the transformation."}, {"heading": "6.7 STATISTICS ON TRANSFORMATION MATRICES", "text": "In this part, we analyze the statistics on the transformation matrices generated by our model for different datasets, including MNIST-ONE, CUB-200, CIFAR-10, and LFW. We used affine transformation in our model, so there are 6 parameters, scaling in the x-coordinate (sx), scaling in the y-coordinate (sy), translation in the x-coordinate (tx), translation in the y-coordinate (rx), and rotation in the y-coordinate (ry). In Fig. 20, we show the histograms on different parameters for different datasets. These histograms show that the model generates non-trivial varied scaling, transformation, and rotation on all datasets. For different datasets, the learned transformation has different patterns. We assume that this is mainly determined by the configurations of objects in the images."}, {"heading": "6.8 CONDITIONAL IMAGE GENERATION", "text": "In view of our model, object-like masks (shapes) can be created for images, we conducted an experiment to assess whether our model can potentially be used for image segmentation and object recognition, we made some changes to the model, for the background generator the input is a real image instead of a random vector, then the image is passed through an encoder to extract the hidden features that replace the random vector z0 and fed to the background generator, for the foreground generator we subtract the image generated by the background generator to get a residual image, then this residual image is fed to the same encoder to get the hidden features used as input for the foreground generator. In our conditional model we want to reconstruct the image, so we add a reconstruction loss along with the opposing loss."}], "references": [{"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1606.03657,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Robust estimation of a multi-layered motion representation", "author": ["Trevor Darrell", "Alex Pentland"], "venue": "IEEE Workshop on Visual Motion,", "citeRegEx": "Darrell and Pentland.,? \\Q1991\\E", "shortCiteRegEx": "Darrell and Pentland.", "year": 1991}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Emily L Denton", "Soumith Chintala", "Rob Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Attend, infer, repeat: Fast scene understanding with generative models", "author": ["S.M. Ali Eslami", "Nicolas Heess", "Theophane Weber", "Yuval Tassa", "Koray Kavukcuoglu", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "Eslami et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Eslami et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1502.046239,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["Gary B. Huang", "Manu Ramesh", "Tamara Berg", "Erik Learned-Miller"], "venue": "Technical Report 07-49,", "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "Efficient inference in occlusion-aware generative models of images", "author": ["Jonathan Huang", "Kevin Murphy"], "venue": "CoRR, abs/1511.06362,", "citeRegEx": "Huang and Murphy.,? \\Q2015\\E", "shortCiteRegEx": "Huang and Murphy.", "year": 2015}, {"title": "Generating images with recurrent adversarial networks", "author": ["Daniel Jiwoong Im", "Chris Dongjoo Kim", "Hui Jiang", "Roland Memisevic"], "venue": "arXiv preprint arXiv:1602.05110,", "citeRegEx": "Im et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Im et al\\.", "year": 2016}, {"title": "Scene collaging: Analysis and synthesis of natural images with semantic layers", "author": ["Phillip Isola", "Ce Liu"], "venue": "In IEEE International Conference on Computer Vision, pp", "citeRegEx": "Isola and Liu.,? \\Q2013\\E", "shortCiteRegEx": "Isola and Liu.", "year": 2013}, {"title": "Spatial transformer networks", "author": ["Max Jaderberg", "Karen Simonyan", "Andrew Zisserman", "koray kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Generative model for layers of appearance and deformation", "author": ["Anitha Kannan", "Nebojsa Jojic", "Brendan Frey"], "venue": null, "citeRegEx": "Kannan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2005}, {"title": "Improving variational inference with inverse autoregressive flow", "author": ["Diederik P Kingma", "Tim Salimans", "Max Welling"], "venue": "arXiv preprint arXiv:1606.04934,", "citeRegEx": "Kingma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2016}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Generating images part by part with composite generative adversarial networks", "author": ["Hanock Kwak", "Byoung-Tak Zhang"], "venue": "arXiv preprint arXiv:1607.05387,", "citeRegEx": "Kwak and Zhang.,? \\Q2016\\E", "shortCiteRegEx": "Kwak and Zhang.", "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Generating images from captions with attention", "author": ["Elman Mansimov", "Emilio Parisotto", "Jimmy Lei Ba", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1511.02793,", "citeRegEx": "Mansimov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mansimov et al\\.", "year": 2015}, {"title": "A parametric texture model based on joint statistics of complex wavelet coefficients", "author": ["Javier Portilla", "Eero P Simoncelli"], "venue": "International journal of computer vision,", "citeRegEx": "Portilla and Simoncelli.,? \\Q2000\\E", "shortCiteRegEx": "Portilla and Simoncelli.", "year": 2000}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Learning what and where to draw", "author": ["Scott Reed", "Zeynep Akata", "Santosh Mohan", "Samuel Tenka", "Bernt Schiele", "Honglak Lee"], "venue": "arXiv preprint arXiv:1610.02454,", "citeRegEx": "Reed et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2016}, {"title": "Generative adversarial text to image synthesis", "author": ["Scott Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee"], "venue": "arXiv preprint arXiv:1605.05396,", "citeRegEx": "Reed et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2016}, {"title": "Learning a generative model of images by factoring appearance and shape", "author": ["Nicolas Le Roux", "Nicolas Heess", "Jamie Shotton", "John Winn"], "venue": "Neural Computation,", "citeRegEx": "Roux et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2011}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "arXiv preprint arXiv:1606.03498,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Generating videos with scene dynamics", "author": ["Carl Vondrick", "Hamed Pirsiavash", "Antonio Torralba"], "venue": "arXiv preprint arXiv:1609.02612,", "citeRegEx": "Vondrick et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vondrick et al\\.", "year": 2016}, {"title": "Representing moving images with layers", "author": ["John Wang", "Edward Adelson"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Wang and Adelson.,? \\Q1994\\E", "shortCiteRegEx": "Wang and Adelson.", "year": 1994}, {"title": "Generative image modeling using style and structure adversarial networks", "author": ["Xiaolong Wang", "Abhinav Gupta"], "venue": "arXiv preprint arXiv:1603.05631,", "citeRegEx": "Wang and Gupta.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Gupta.", "year": 2016}, {"title": "Caltech-UCSD Birds 200", "author": ["P. Welinder", "S. Branson", "T. Mita", "C. Wah", "F. Schroff", "S. Belongie", "P. Perona"], "venue": "Technical Report CNS-TR-2010-001, California Institute of Technology,", "citeRegEx": "Welinder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Welinder et al\\.", "year": 2010}, {"title": "Attribute2image: Conditional image generation from visual attributes", "author": ["Xinchen Yan", "Jimei Yang", "Kihyuk Sohn", "Honglak Lee"], "venue": "CoRR, abs/1512.00570,", "citeRegEx": "Yan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2015}, {"title": "Energy-based generative adversarial network", "author": ["Junbo Zhao", "Michael Mathieu", "Yann LeCun"], "venue": "arXiv preprint arXiv:1609.03126,", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}, {"title": "Generative visual manipulation on the natural image manifold", "author": ["Jun-Yan Zhu", "Philipp Kr\u00e4henb\u00fchl", "Eli Shechtman", "Alexei A Efros"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "Generative adversarial networks (GANs) (Goodfellow et al., 2014) have shown significant promise as generative models for natural images.", "startOffset": 39, "endOffset": 64}, {"referenceID": 18, "context": "A flurry of recent work has proposed improvements over the original GAN work for image generation (Radford et al., 2015; Denton et al., 2015; Salimans et al., 2016; Chen et al., 2016; Zhu et al., 2016; Zhao et al., 2016), multi-stage image generation including part-based models (Im et al.", "startOffset": 98, "endOffset": 220}, {"referenceID": 2, "context": "A flurry of recent work has proposed improvements over the original GAN work for image generation (Radford et al., 2015; Denton et al., 2015; Salimans et al., 2016; Chen et al., 2016; Zhu et al., 2016; Zhao et al., 2016), multi-stage image generation including part-based models (Im et al.", "startOffset": 98, "endOffset": 220}, {"referenceID": 22, "context": "A flurry of recent work has proposed improvements over the original GAN work for image generation (Radford et al., 2015; Denton et al., 2015; Salimans et al., 2016; Chen et al., 2016; Zhu et al., 2016; Zhao et al., 2016), multi-stage image generation including part-based models (Im et al.", "startOffset": 98, "endOffset": 220}, {"referenceID": 0, "context": "A flurry of recent work has proposed improvements over the original GAN work for image generation (Radford et al., 2015; Denton et al., 2015; Salimans et al., 2016; Chen et al., 2016; Zhu et al., 2016; Zhao et al., 2016), multi-stage image generation including part-based models (Im et al.", "startOffset": 98, "endOffset": 220}, {"referenceID": 29, "context": "A flurry of recent work has proposed improvements over the original GAN work for image generation (Radford et al., 2015; Denton et al., 2015; Salimans et al., 2016; Chen et al., 2016; Zhu et al., 2016; Zhao et al., 2016), multi-stage image generation including part-based models (Im et al.", "startOffset": 98, "endOffset": 220}, {"referenceID": 28, "context": "A flurry of recent work has proposed improvements over the original GAN work for image generation (Radford et al., 2015; Denton et al., 2015; Salimans et al., 2016; Chen et al., 2016; Zhu et al., 2016; Zhao et al., 2016), multi-stage image generation including part-based models (Im et al.", "startOffset": 98, "endOffset": 220}, {"referenceID": 8, "context": ", 2016), multi-stage image generation including part-based models (Im et al., 2016; Kwak & Zhang, 2016), image generation conditioned on input text or attributes (Mansimov et al.", "startOffset": 66, "endOffset": 103}, {"referenceID": 23, "context": ", 2016b;a), image generation based on 3D structure (Wang & Gupta, 2016), and even video generation (Vondrick et al., 2016).", "startOffset": 99, "endOffset": 122}, {"referenceID": 26, "context": "Figure 1: Generation results of our model on CUB-200 (Welinder et al., 2010).", "startOffset": 53, "endOffset": 76}, {"referenceID": 15, "context": "We mainly evaluate our approach on four datasets: MNIST-ONE (one digit) and MNIST-TWO (two digits) synthesized from MNIST (LeCun et al., 1998), CIFAR-10 (Krizhevsky & Hinton, 2009) and CUB-200 (Welinder et al.", "startOffset": 122, "endOffset": 142}, {"referenceID": 26, "context": ", 1998), CIFAR-10 (Krizhevsky & Hinton, 2009) and CUB-200 (Welinder et al., 2010).", "startOffset": 58, "endOffset": 81}, {"referenceID": 12, "context": "Recent improvements in image generation using deep neural networks mainly fall into one of the two stochastic models: variational autoencoders (VAEs) (Kingma et al., 2016) and generative adversarial networks (GANs) (Goodfellow et al.", "startOffset": 150, "endOffset": 171}, {"referenceID": 4, "context": ", 2016) and generative adversarial networks (GANs) (Goodfellow et al., 2014).", "startOffset": 51, "endOffset": 76}, {"referenceID": 5, "context": "Sequential models have been pivotal for improved image generation using variational autoencoders: DRAW (Gregor et al., 2015) uses attention based recurrence conditioning on the canvas drawn so far.", "startOffset": 103, "endOffset": 124}, {"referenceID": 2, "context": "Early compelling results using GANs used sequential coarse-to-fine multiscale generation and classconditioning (Denton et al., 2015).", "startOffset": 111, "endOffset": 132}, {"referenceID": 22, "context": "Since then, improved training schemes (Salimans et al., 2016) and better convolutional structure (Radford et al.", "startOffset": 38, "endOffset": 61}, {"referenceID": 18, "context": ", 2016) and better convolutional structure (Radford et al., 2015) have improved the generation results using", "startOffset": 43, "endOffset": 65}, {"referenceID": 2, "context": "In Eslami et al. (2016), a recurrent generative model that draws one object at a time to the canvas was used as the decoder in VAE.", "startOffset": 3, "endOffset": 24}, {"referenceID": 7, "context": "One closely related work combining recursive structure with GAN is that of Im et al. (2016) but it does not explicitly model object composition and follows a similar paradigm as by Gregor et al.", "startOffset": 75, "endOffset": 92}, {"referenceID": 5, "context": "(2016) but it does not explicitly model object composition and follows a similar paradigm as by Gregor et al. (2015). Another closely related work is that of Kwak & Zhang (2016).", "startOffset": 96, "endOffset": 117}, {"referenceID": 5, "context": "(2016) but it does not explicitly model object composition and follows a similar paradigm as by Gregor et al. (2015). Another closely related work is that of Kwak & Zhang (2016). It combines recursive structure and alpha blending.", "startOffset": 96, "endOffset": 178}, {"referenceID": 5, "context": "(2016) but it does not explicitly model object composition and follows a similar paradigm as by Gregor et al. (2015). Another closely related work is that of Kwak & Zhang (2016). It combines recursive structure and alpha blending. However, our work differs in three main ways: (1) We explicitly use a generator for modeling the foreground poses. That provides significant advantage for natural images, as shown by our ablation studies; (2) Our shape generator is separate from the appearance generator. This factored representation allows more flexibility in the generated scenes; (3) Our recursive framework generates subsequent objects conditioned on the current and previous hidden vectors, and previously generated object. This allows for explicit contextual modeling among generated elements in the scene. See Fig. 17 for contextually relevant foregrounds generated for the same background, or Fig. 6 for meaningful placement of two MNIST digits relative to each. Models that provide supervision to image generation using conditioning variables have also been proposed: Style/Structure GANs (Wang & Gupta, 2016) learns separate generative models for style and structure that are then composed to obtain final images. In Reed et al. (2016a), GAN based image generation is conditioned on text and the region in the image where the text manifests, specified during training via keypoints or bounding boxes.", "startOffset": 96, "endOffset": 1245}, {"referenceID": 2, "context": "While the GANs framework is largely agnostic to the choice of G and D, it is clear that generative models with the \u2018right\u2019 inductive biases will be more effective in learning from the gradient information (Denton et al., 2015; Im et al., 2016; Gregor et al., 2015; Reed et al., 2016a; Yan et al., 2015).", "startOffset": 205, "endOffset": 302}, {"referenceID": 8, "context": "While the GANs framework is largely agnostic to the choice of G and D, it is clear that generative models with the \u2018right\u2019 inductive biases will be more effective in learning from the gradient information (Denton et al., 2015; Im et al., 2016; Gregor et al., 2015; Reed et al., 2016a; Yan et al., 2015).", "startOffset": 205, "endOffset": 302}, {"referenceID": 5, "context": "While the GANs framework is largely agnostic to the choice of G and D, it is clear that generative models with the \u2018right\u2019 inductive biases will be more effective in learning from the gradient information (Denton et al., 2015; Im et al., 2016; Gregor et al., 2015; Reed et al., 2016a; Yan et al., 2015).", "startOffset": 205, "endOffset": 302}, {"referenceID": 27, "context": "While the GANs framework is largely agnostic to the choice of G and D, it is clear that generative models with the \u2018right\u2019 inductive biases will be more effective in learning from the gradient information (Denton et al., 2015; Im et al., 2016; Gregor et al., 2015; Reed et al., 2016a; Yan et al., 2015).", "startOffset": 205, "endOffset": 302}, {"referenceID": 27, "context": "Some existing methods assume the access to the shape of the object either during training (Isola & Liu, 2013) or both at train and test time (Reed et al., 2016a; Yan et al., 2015).", "startOffset": 141, "endOffset": 179}, {"referenceID": 11, "context": "Representing images in layered structure is even straightforward for video with moving objects (Darrell & Pentland, 1991; Wang & Adelson, 1994; Kannan et al., 2005).", "startOffset": 95, "endOffset": 164}, {"referenceID": 11, "context": "Representing images in layered structure is even straightforward for video with moving objects (Darrell & Pentland, 1991; Wang & Adelson, 1994; Kannan et al., 2005). Vondrick et al. (2016) generates videos by separately generating a fixed background and moving foregrounds.", "startOffset": 144, "endOffset": 189}, {"referenceID": 11, "context": "Representing images in layered structure is even straightforward for video with moving objects (Darrell & Pentland, 1991; Wang & Adelson, 1994; Kannan et al., 2005). Vondrick et al. (2016) generates videos by separately generating a fixed background and moving foregrounds. A similar way of generating single image can be found in Kwak & Zhang (2016). Another way is modeling the layered structure with object appearance and pose as:", "startOffset": 144, "endOffset": 351}, {"referenceID": 21, "context": "Several works fall into this group (Roux et al., 2011; Huang & Murphy, 2015; Eslami et al., 2016).", "startOffset": 35, "endOffset": 97}, {"referenceID": 3, "context": "Several works fall into this group (Roux et al., 2011; Huang & Murphy, 2015; Eslami et al., 2016).", "startOffset": 35, "endOffset": 97}, {"referenceID": 3, "context": ", 2011; Huang & Murphy, 2015; Eslami et al., 2016). In Huang & Murphy (2015), images are decomposed into layers of objects with specific poses in a variational autoencoder framework, while the number of objects (i.", "startOffset": 30, "endOffset": 77}, {"referenceID": 3, "context": ", 2011; Huang & Murphy, 2015; Eslami et al., 2016). In Huang & Murphy (2015), images are decomposed into layers of objects with specific poses in a variational autoencoder framework, while the number of objects (i.e., layers) is adaptively estimated in Eslami et al. (2016). To contrast with these works, LR-GAN uses a layered composition, and the foreground layers simultaneously model all three dominant factors of variation: appearance f , shape m and pose a.", "startOffset": 30, "endOffset": 274}, {"referenceID": 10, "context": "As in Jaderberg et al. (2015), we predict the affine transformation matrix with a linear layer Tf that has six-dimensional outputs.", "startOffset": 6, "endOffset": 30}, {"referenceID": 10, "context": "As in Jaderberg et al. (2015), we predict the affine transformation matrix with a linear layer Tf that has six-dimensional outputs. Then based on the predicted transformation matrix, we use a grid generator Gg to generate the corresponding sampling coordinates in the input for each location at the output. The generated foreground appearance and mask share the same transformation matrix, and thus the same sampling grid. Given the grid, the sampler S will simultaneously sample the ft and mt to obtain f\u0302t and m\u0302t, respectively. Different from Jaderberg et al. (2015), our sampler here normally performs downsampling, since the the foreground typically has smaller size than the background.", "startOffset": 6, "endOffset": 570}, {"referenceID": 4, "context": "2 NEW EVALUATION METRICS Several metrics have been proposed to evaluate GANs, such as Gaussian parzen window (Goodfellow et al., 2014), Generative Adversarial Metric (GAM) (Im et al.", "startOffset": 109, "endOffset": 134}, {"referenceID": 8, "context": ", 2014), Generative Adversarial Metric (GAM) (Im et al., 2016) and Inception Score (Salimans et al.", "startOffset": 45, "endOffset": 62}, {"referenceID": 22, "context": ", 2016) and Inception Score (Salimans et al., 2016).", "startOffset": 28, "endOffset": 51}, {"referenceID": 22, "context": "Most recently, Inception Score has been used in several works (Salimans et al., 2016; Zhao et al., 2016).", "startOffset": 62, "endOffset": 104}, {"referenceID": 28, "context": "Most recently, Inception Score has been used in several works (Salimans et al., 2016; Zhao et al., 2016).", "startOffset": 62, "endOffset": 104}, {"referenceID": 15, "context": "We conduct qualitative and quantitative evaluations on three datasets: 1) MNIST (LeCun et al., 1998); 2) CIFAR-10 (Krizhevsky & Hinton, 2009); 3) CUB-200 (Welinder et al.", "startOffset": 80, "endOffset": 100}, {"referenceID": 26, "context": ", 1998); 2) CIFAR-10 (Krizhevsky & Hinton, 2009); 3) CUB-200 (Welinder et al., 2010).", "startOffset": 61, "endOffset": 84}, {"referenceID": 18, "context": "We compare our results to that of DCGAN (Radford et al., 2015).", "startOffset": 40, "endOffset": 62}, {"referenceID": 22, "context": "We use three metrics for quantitative evaluation, including Inception Score (Salimans et al., 2016) and the proposed Adversarial Accuracy, Adversarial Divergence.", "startOffset": 76, "endOffset": 99}, {"referenceID": 22, "context": "Following Salimans et al. (2016), we generated 50,000 images", "startOffset": 10, "endOffset": 33}, {"referenceID": 22, "context": "06 \u2020Evaluate using the pre-trained Inception net as Salimans et al. (2016) \u2020\u2020Evaluate using the supervisedly trained classifier based on the discriminator in LR-GAN.", "startOffset": 52, "endOffset": 75}, {"referenceID": 22, "context": "The standard Inception Score is based on the Inception net as in Salimans et al. (2016), and the contextual Inception Score is based on our trained classifier model.", "startOffset": 65, "endOffset": 88}], "year": 2017, "abstractText": "We present LR-GAN: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks (GANs), the proposed GAN learns to generate image background and foregrounds separately and recursively, and stitch the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape and pose. The whole model is unsupervised, and is trained in an end-to-end manner with gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than DCGAN.", "creator": "LaTeX with hyperref package"}}}