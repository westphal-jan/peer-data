{"id": "1703.01680", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2017", "title": "Multi-Objective Non-parametric Sequential Prediction", "abstract": "Online-learning research has mainly been focusing on minimizing one objective function. In many real-world applications, however, several objective functions have to be considered simultaneously. Recently, an algorithm for dealing with several objective functions in the i.i.d. case has been presented. In this paper, we extend the multi-objective framework to the case of stationary and ergodic processes, thus allowing dependencies among observations. We first identify an asymptomatic lower bound for any prediction strategy and then present an algorithm whose predictions achieve the optimal solution while fulfilling any continuous and convex constraining criterion.", "histories": [["v1", "Sun, 5 Mar 2017 22:41:00 GMT  (209kb)", "https://arxiv.org/abs/1703.01680v1", null], ["v2", "Thu, 9 Mar 2017 19:46:50 GMT  (208kb)", "http://arxiv.org/abs/1703.01680v2", null], ["v3", "Sun, 19 Mar 2017 15:50:42 GMT  (211kb)", "http://arxiv.org/abs/1703.01680v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["guy uziel", "ran el-yaniv"], "accepted": true, "id": "1703.01680"}, "pdf": {"name": "1703.01680.pdf", "metadata": {"source": "CRF", "title": "Multi-Objective Non-parametric Sequential Prediction", "authors": ["Guy Uziel"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 3.01 680v 3 [cs.L GOnline-learning research has mainly focused on minimizing objective function. However, in many real world applications, multiple objective functions need to be considered at the same time. Recently, an algorithm for handling multiple objective functions was presented in the i.i.d. case. In this paper, we expand the multi-objective framework to include stationary and ergodic processes, allowing interdependencies between observations. We first identify an asymptomatic lower limit for each prediction strategy and then present an algorithm whose predictions achieve the optimal solution while meeting any continuous and convex limiting criterion."}, {"heading": "1 Introduction", "text": "In the traditional learning environment, and especially in sequential prediction under uncertainty, the learner is evaluated by a single loss function, which is not fully known in every iteration. [9] When dealing with multiple objectives, since it is impossible to minimize all objectives at the same time, a target is chosen as the main function to keep the objectives below a certain threshold. This approach is very problematic in terms of an objective function that is required to deal with multiple objective functions by assigning a predefined weight to each target. However, the difficulty lies in assigning an appropriate weight to each target in order to keep the objectives below a certain threshold."}, {"heading": "2 Problem Formulation", "text": "Let's consider the following prediction game. Let's leave X, [\u2212 D, D] d \"Rd is a compact observation room in which D > 0. At each turn, n = 1, 2,. - The player is obliged to make a prediction in which Y-Rm is a compact and convex set, based on past observations, Xn \u2212 11, (x1,., xn \u2212 1) and, xi \u2212 X (X01 is the empty observation). After we have made the prediction yn, the observation xn is revealed and the player suffers two losses, u (yn, xn) and c (n, xn), where u and c continuous functions and convex w.r.t. their first reasoning. Let's consider the player's prediction strategy as a sequence S, {Sn}."}, {"heading": "3 Optimallity of V\u2217", "text": "In this section we prove that the average u-loss of any prediction strategy cannot be less than V-Q (= real function). This result is a generalization of the known result of [1] with respect to the best possible result under a single target. Before determining and testing this optimal result, we specify a known problem and prove two lemmas that can be used repeatedly in this paper. The first problem is known as a generalized ergodic theorem. The second and third lemmas concern the continuity of the sad point w.r.t. The probability distribution can be done, for example, by imposing some regularity conditions on the constraint function (see, for example: [20].Lemma 1 (ergodicity, [8]). Let X = {Xi} - be a stationary and ergodic process."}, {"heading": "4 Minimax Histogram Based Aggregation", "text": "We are now ready to present our mMinimax histogram based on aggregation (MHA) algorithm and prove that its predictions are as good as the best strategy. Theorem 1 allows us to redefine our goal: to find a prediction strategy that is designed to apply to each individual process a countable set of experts (Hk, h), in which an ExpertHk, l, a pair (yik, l) is created by experts (Hk, h). We do this by maintaining a countable set of experts (Hk, h), in which an ExpertHk, l, a pair (yik, l) is executed by experts."}, {"heading": "5 Concluding Remarks", "text": "In this paper, we have introduced the Minimax HistogramAggregation (MHA) algorithm for multiple objective sequential predictions. We have extended the general setting in which the unknown underlying process is stationary and ergodic to the case of multiple targets, and in view of the fact that the underlying process is \u03b3-practicable, we have extended the known result of [1] with respect to the asymptotic lower limit of the prediction by a single target, in the case of multiple targets. We have proven that MHA is a strictly positive strategy whose predictions are also retrospectively opposed to the optimal solution. In practice, however, it is not possible to maintain an infinite number of experts. Therefore, it is customary to apply such algorithms with a finite number of experts (see [14, 12, 13, 17]. Despite the fact that we can apply the infinite X case in this case, the proof is that we apply the universal X observation."}], "references": [{"title": "The strong law of large numbers for sequential decisions under uncertainty", "author": ["P.H. Algoet"], "venue": "IEEE Transactions on Information Theory, 40(3):609\u2013633,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Asymptotic optimality and asymptotic equipartition properties of log-optimum investment", "author": ["P.H. Algoet", "T.M. Cover"], "venue": "The Annals of Probability, pages 876\u2013898,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1988}, {"title": "Optimization iii", "author": ["A. Ben-Tal", "A. Nemirovsky"], "venue": "Lecture Notes,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Stochastic optimal control: The discrete time case, volume 23", "author": ["D. Bertsekas", "S. Shreve"], "venue": "Academic Press New York,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1978}, {"title": "Nonparametric sequential prediction of time series", "author": ["G. Biau", "K. Bleakley", "L. Gy\u00f6rfi", "G. Ottucs\u00e1k"], "venue": "Journal of Nonparametric Statistics, 22(3):297\u2013317,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Sequential quantile prediction of time series", "author": ["G. Biau", "B. Patra"], "venue": "IEEE Transactions on Information Theory, 57(3):1664\u20131674,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Online Computation and Competitive Analysis", "author": ["A. Borodin", "R. El-Yaniv"], "venue": "Cambridge University Press,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "The individual ergodic theorem of information theory", "author": ["L. Breiman"], "venue": "The Annals of Mathematical Statistics, 28(3):809\u2013811,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1957}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Cambridge University Press,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Minimax theorems", "author": ["K. Fan"], "venue": "Proceedings of the National Academy of Sciences, 39(1):42\u201347,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1953}, {"title": "Strategies for sequential prediction of stationary time series", "author": ["L. Gy\u00f6rfi", "G. Lugosi"], "venue": "InModeling uncertainty, pages 225\u2013248. Springer,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Nonparametric kernel-based sequential investment strategies", "author": ["L. Gy\u00f6rfi", "G. Lugosi", "F. Udina"], "venue": "Mathematical Finance, 16(2):337\u2013357,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Nonparametric prediction", "author": ["L. Gy\u00f6rfi", "D. Sch\u00e4fer"], "venue": "Advances in learning theory: methods, models and applications, 339:354,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Kernel-based semi-log-optimal empirical portfolio selection strategies", "author": ["L. Gy\u00f6rfi", "A. Urb\u00e1n", "I. Vajda"], "venue": "International Journal of Theoretical and Applied Finance, 10(03):505\u2013516,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "The weak aggregating algorithm and weak mixability", "author": ["Y. Kalnishkan", "M. Vyugin"], "venue": "International Conference on Computational Learning Theory, pages 188\u2013203. Springer,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Online portfolio selection: A survey", "author": ["B. Li", "S.C.H. Hoi"], "venue": "ACM Computing Surveys (CSUR), 46(3):35,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Corn: Correlation-driven nonparametric learning approach for portfolio selection", "author": ["B. Li", "S.C.H Hoi", "V. Gopalkrishnan"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):21,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Nash equilibrium computation in subnetwork zero-sum games with switching communications", "author": ["Y. Lou", "Y. Hong", "L. Xie", "G. Shi", "K. Johansson"], "venue": "IEEE Transactions on Automatic Control, 61(10):2920\u20132935,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Trading regret for efficiency: online convex optimization with long term constraints", "author": ["M. Mahdavi", "R. Jin", "T. Yang"], "venue": "Journal of Machine Learning Research, 13(Sep):2503\u20132528,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Stochastic convex optimization with multiple objectives", "author": ["M. Mahdavi", "T. Yang", "R. Jin"], "venue": "Advances in Neural Information Processing Systems, pages 1115\u2013 1123,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Measurable selection theorems for minimax stochastic optimization problems", "author": ["A. Nowak"], "venue": "SIAM Journal on Control and Optimization, 23(3):466\u2013476,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1985}, {"title": "Neyman-pearson classification, convexity and stochastic constraints", "author": ["P. Rigollet", "X. Tong"], "venue": "Journal of Machine Learning Research, 12(Oct):2831\u20132855,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Almost sure convergence, vol", "author": ["W. Stout"], "venue": "24 of probability and mathematical statistics,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1974}, {"title": "Competing with stationary prediction strategies", "author": ["V. Vovk"], "venue": "International Conference on Computational Learning Theory, pages 439\u2013453. Springer,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 8, "context": "1 Introduction In the traditional online learning setting, and in particular in sequential prediction under uncertainty, the learner is evaluated by a single loss function that is not completely known at each iteration [9].", "startOffset": 219, "endOffset": 222}, {"referenceID": 15, "context": "For example, in online portfolio selection [16, 7], the player may want to maximize wealth while keeping the risk (i.", "startOffset": 43, "endOffset": 50}, {"referenceID": 6, "context": "For example, in online portfolio selection [16, 7], the player may want to maximize wealth while keeping the risk (i.", "startOffset": 43, "endOffset": 50}, {"referenceID": 21, "context": ", [22]) (which extends the objective in classical binary classification) where the goal is to learn a classifier achieving low type II error whose type I error is kept below a given threshold.", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": "Recently, [19] presented an algorithm for dealing with the case of one main objective and fully-known constraints.", "startOffset": 10, "endOffset": 14}, {"referenceID": 19, "context": "In a subsequent work, [20] proposed a framework for", "startOffset": 22, "endOffset": 26}, {"referenceID": 13, "context": "For example, in online portfolio selection, [14, 12], [13], and [17] proposed non-parametric online strategies that guarantee, under mild conditions, the best possible outcome.", "startOffset": 44, "endOffset": 52}, {"referenceID": 11, "context": "For example, in online portfolio selection, [14, 12], [13], and [17] proposed non-parametric online strategies that guarantee, under mild conditions, the best possible outcome.", "startOffset": 44, "endOffset": 52}, {"referenceID": 12, "context": "For example, in online portfolio selection, [14, 12], [13], and [17] proposed non-parametric online strategies that guarantee, under mild conditions, the best possible outcome.", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "For example, in online portfolio selection, [14, 12], [13], and [17] proposed non-parametric online strategies that guarantee, under mild conditions, the best possible outcome.", "startOffset": 64, "endOffset": 68}, {"referenceID": 4, "context": "Another interesting example in this regard is the work on time-series prediction by [5], [11], and [6].", "startOffset": 84, "endOffset": 87}, {"referenceID": 10, "context": "Another interesting example in this regard is the work on time-series prediction by [5], [11], and [6].", "startOffset": 89, "endOffset": 93}, {"referenceID": 5, "context": "Another interesting example in this regard is the work on time-series prediction by [5], [11], and [6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 23, "context": "The algorithm presented in this paper utilizes as a sub-routine the Weak Aggregating Algorithm of [24], and [15] to handle multiple objectives.", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": "The algorithm presented in this paper utilizes as a sub-routine the Weak Aggregating Algorithm of [24], and [15] to handle multiple objectives.", "startOffset": 108, "endOffset": 112}, {"referenceID": 2, "context": "Therefore, the problem is equivalent to finding the saddle point of the Lagrangian function [3], namely, min y\u2208Y max \u03bb\u2208R+ L(y, \u03bb), where the Lagrangian is L(y, \u03bb) , (EP\u221e [u(y,X0)] + \u03bb (EP\u221e [c(y,X0)]\u2212 \u03b3)) .", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "This result is a generalization of the well-known result of [1] regarding the best possible outcome under a single objective.", "startOffset": 60, "endOffset": 63}, {"referenceID": 19, "context": ", [20]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 7, "context": "Lemma 1 (Ergodicity, [8]).", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "32 from [4], we have that sup\u03bb\u2208\u039b EQ [l(y, \u03bb,X)] is continuous inQ\u00d7Y .", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "The last part of the lemma follows directly from Fan\u2019s minimax theorem [10].", "startOffset": 71, "endOffset": 75}, {"referenceID": 20, "context": "The first part of the proof follows immediately from the minimax measurable theorem of [21] due to the compactness of Y,\u039b,X and the properties of the loss function L.", "startOffset": 87, "endOffset": 91}, {"referenceID": 1, "context": "The proof of the second part is similar to the one presented in Theorem 3 of [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 22, "context": ", [23]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 23, "context": "In order to ensure that the performance of MHA will be as good as any other expert for both the y and the \u03bb predictions, we apply the Weak Aggregating Algorithm of [24], and [15] twice simultaneously.", "startOffset": 164, "endOffset": 168}, {"referenceID": 14, "context": "In order to ensure that the performance of MHA will be as good as any other expert for both the y and the \u03bb predictions, we apply the Weak Aggregating Algorithm of [24], and [15] twice simultaneously.", "startOffset": 174, "endOffset": 178}, {"referenceID": 17, "context": ", [18]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": ", [11, 13]), we get that P \u2217(k,h) X \u22121 \u2212k \u2192 P{X0|X\u22121 \u2212k} weakly as h \u2192 \u221e.", "startOffset": 2, "endOffset": 10}, {"referenceID": 12, "context": ", [11, 13]), we get that P \u2217(k,h) X \u22121 \u2212k \u2192 P{X0|X\u22121 \u2212k} weakly as h \u2192 \u221e.", "startOffset": 2, "endOffset": 10}, {"referenceID": 14, "context": "Applying Lemma 5 in [15], we know that the x updates guarantee that for every expert Hk,h,", "startOffset": 20, "endOffset": 24}], "year": 2017, "abstractText": "Online-learning research has mainly been focusing on minimizing one objective function. In many real-world applications, however, several objective functions have to be considered simultaneously. Recently, an algorithm for dealing with several objective functions in the i.i.d. case has been presented. In this paper, we extend the multi-objective framework to the case of stationary and ergodic processes, thus allowing dependencies among observations. We first identify an asymptomatic lower bound for any prediction strategy and then present an algorithm whose predictions achieve the optimal solution while fulfilling any continuous and convex constraining criterion.", "creator": "LaTeX with hyperref package"}}}