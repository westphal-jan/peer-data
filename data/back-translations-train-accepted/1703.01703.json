{"id": "1703.01703", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Third-Person Imitation Learning", "abstract": "Reinforcement learning (RL) makes it possible to train agents capable of achiev- ing sophisticated goals in complex and uncertain environments. A key difficulty in reinforcement learning is specifying a reward function for the agent to optimize. Traditionally, imitation learning in RL has been used to overcome this problem. Unfortunately, hitherto imitation learning methods tend to require that demonstra- tions are supplied in the first-person: the agent is provided with a sequence of states and a specification of the actions that it should have taken. While powerful, this kind of imitation learning is limited by the relatively hard problem of collect- ing first-person demonstrations. Humans address this problem by learning from third-person demonstrations: they observe other humans perform tasks, infer the task, and accomplish the same task themselves.", "histories": [["v1", "Mon, 6 Mar 2017 02:02:34 GMT  (2204kb,D)", "http://arxiv.org/abs/1703.01703v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bradly c stadie", "pieter abbeel", "ilya sutskever"], "accepted": true, "id": "1703.01703"}, "pdf": {"name": "1703.01703.pdf", "metadata": {"source": "CRF", "title": "THIRD-PERSON IMITATION LEARNING", "authors": ["Bradly C. Stadie", "Pieter Abbeel", "Ilya Sutskever"], "emails": ["ilyasu@openai.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, the number of those who are able to increase has multiplied, many times more than the number of those who are able to retaliate."}, {"heading": "2 RELATED WORK", "text": "In fact, it is the case that most people who are able to put themselves in the world take themselves to task. (...) It is not that they are able to understand the world. (...) It is not that they are able to change the world. (...) It is not that they are able to change the world. (...) It is not that they are able to change the world. (...) It is not that they are able to change the world. (...) It is that they are not able to change the world. (...) It is not that they are able to change the world. (...) It is not that they are able to change the world. (...) It is not that they are able to change the world. (...) It is not that they are able to change the world. (...) It is not that they are able to change the world. (...) It is not that they are able to change the world."}, {"heading": "3 BACKGROUND AND PRELIMINARIES", "text": "??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "4 A FORMAL DEFINITION OF THE THIRD-PERSON IMITATION LEARNING PROBLEM", "text": "Formally, the learning problem of imitation of the third person can be explained as follows: Suppose we get two Markov decision processes M\u03c0E and M\u03c0\u03b8. Suppose there is still a series of traces \u03c1 = {(s1,.., sn)} ni = 0 that were generated within the framework of a policy \u03c0E that acts optimally under an unknown reward R\u03c0E. In imitation learning of the third person, one tries to recover by a substitute, namely by a policy \u03c0\u03b8 = f (\u03c1) that behaves optimally in relation to R\u03c0\u03b8."}, {"heading": "5 A THIRD-PERSON IMITATION LEARNING ALGORITHM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 GAME FORMULATION", "text": "This algorithm is able to distinguish successfully between expert and beginner strategies, even when the strategies are executed in different environments. Subsequently, this discriminatory signal can be used to train expert strategies in new domains using RL, forcing the beginner policy to question the expert policy, making these observations look like in our experiments. We start by remembering that in the algorithms proposed by Ho & Ermon, the loss in Equation 2 is used to train a discriminator who is able to distinguish experts from non-expert strategies. Unfortunately, (2) we are likely to fail in cases where the experts and non-experts act in other environments, as DR learns these differences quickly and uses them as a strong classification."}, {"heading": "5.2 ALGORITHM", "text": "In order to solve the game formulation in Equation (5), we perform alternately (partially) optimizations via the \u03c0\u03b8 guidelines and the reward function and domain confusion encoded by DR, DD, DF. Optimization via DR, DD, DF is performed by stochastic gradient descent with ADAM Kingma & Ba (2014). Our generator step (\u03c0\u03b8) is similar to the generator step in the by algorithm (Ho & Ermon, 2016). We simply use \u2212 logDR as a reward. Using policy gradient methods (TRPO), we train the generator to minimize these costs and thus drive policy toward replication of expert behavior. Once the generator step is done, we start again with the discriminator step. The entire process is summarized in Algorithm 1."}, {"heading": "6 EXPERIMENTS", "text": "We try to answer the following questions through experiments: 1. Is it possible to solve the problem of imitation of the third person = imitation of learning success in simple environments? That is, given a collection of image-based rollouts of experts in one area, is it possible to train a policy in another area that replicates the essence of the original behavior? 2. Does the algorithm we propose take into account both the confusion and the speed? 3. How sensitive is our proposed algorithm to the selection of hyperparameters used in use? 4. How sensitive is our proposed algorithm to changes in the camera angle? 5. How does our method compare with some reasonable foundations? Algorithm 1. A third person imitates the learning algorithm. 1. Let CE be the default entropy loss. 2. Let G be a function that reverses the gradient sign during repropogation and acts as a new identity card for N, 2."}, {"heading": "6.1 ENVIRONMENTS", "text": "To evaluate our algorithm, we consider three environments in the MuJoCo physics simulator. There are two different versions of each environment, an expert variant and a beginner variant. Our goal is to train a cost function that is domain agnostic and can therefore be trained with images from the expert area, but can still cause a reasonable price in the beginner area. See Figure 1 for a visualization of the differences between expert and beginner environments for the three tasks. Point: A point mass tries to reach a point in a plane. Note that the color of the target and the camera angle change significantly between domains. Reacher: A two-stage DOF arm tries to reach a specified point in the plane. The camera angle, the length of the arms, and the color of the target point are changed between domains. Note that changing the camera angle significantly changes the background color when the image angle changes from mostly gray to about 30 percent black, which is found to be a major challenge for us to control the camera angle."}, {"heading": "6.2 EVALUATIONS", "text": "In fact, most of them will be able to go to another world, where they can go to another world, where they can go to another world, where they can go to another world."}, {"heading": "7 DISCUSSION AND FUTURE WORK", "text": "We argue that this issue will be important in the future as reinforcement and generative hostile learning techniques improve, and the cost of collecting ego samples remains high. We presented an algorithm that builds on generative adversarial imitation learning and is capable of solving simple imitation tasks for third parties. A promising direction for future work in this area is to jointly train policy characteristics and cost characteristics at the pixel level, allowing for the reuse of image characteristics. A code for training a third party imitation learning agent in the areas of this paper is presented here: https: / / github.com / bstadie / third _ person _ im"}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work was carried out partly at OpenAI and partly at Berkeley. The work at Berkeley was partly supported by Darpa within the framework of the Simplex program and the FunLoL program."}, {"heading": "8 APPENDIX A: LEARNING CURVES FOR BASELINES", "text": "Here we plot the learning curves for each of the basic lines mentioned in the Experiments section as a standalone graph to better examine the variance of each learning curve."}, {"heading": "9 APPENDIX B: ARCHITECTURE PARAMETERS", "text": "Joint Feature Extractor: Input is an image size of 50 x 50 with 3 channels, RGB. Layers consist of two sinuous layers followed by a maximum pooling layer of size 2. Layers use 5 filters of size 3 each. Domain Discriminator and Class Discriminator: Input is the domain agnostic output of sinuous layers. Layers are two feed-forward layers of size 128 followed by a final feed-forward layer of size 2 and a soft-max layer to determine the log probabilities. ADAM is used for discriminator training with a learning rate of 0.001. The RL Generator uses the standard TRPO implementation available in RLLab."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A. Ng"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Abbeel and Ng.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel and Ng.", "year": 2004}, {"title": "Autonomous helicopter aerobatics through apprenticeship learning", "author": ["Pieter Abbeel", "Adam Coates", "Andrew Y Ng"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Abbeel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2010}, {"title": "A survey of robot learning from demonstration", "author": ["Brenna D Argall", "Sonia Chernova", "Manuela Veloso", "Brett Browning"], "venue": "Robotics and autonomous systems,", "citeRegEx": "Argall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Argall et al\\.", "year": 2009}, {"title": "Tabula rasa: Model transfer for object category detection", "author": ["Yusuf Aytar", "Andrew Zisserman"], "venue": "In 2011 International Conference on Computer Vision,", "citeRegEx": "Aytar and Zisserman.,? \\Q2011\\E", "shortCiteRegEx": "Aytar and Zisserman.", "year": 2011}, {"title": "Kernelized infomax clustering", "author": ["D. Barber", "F.V. Agakov"], "venue": null, "citeRegEx": "Barber and Agakov.,? \\Q2005\\E", "shortCiteRegEx": "Barber and Agakov.", "year": 2005}, {"title": "Relative entropy inverse reinforcement learning", "author": ["A. Boularias", "J. Kober", "J. Peters"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Boularias et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boularias et al\\.", "year": 2011}, {"title": "Unsupervised classifiers, mutual information and phantom targets", "author": ["J.S. Bridle", "A.J. Heading", "D.J. MacKay"], "venue": null, "citeRegEx": "Bridle et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Bridle et al\\.", "year": 1992}, {"title": "Signature verification using a siamese time delay neural network", "author": ["Jane Bromley", "James W Bentz", "L\u00e9on Bottou", "Isabelle Guyon", "Yann LeCun", "Cliff Moore", "Eduard S\u00e4ckinger", "Roopak Shah"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Bromley et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bromley et al\\.", "year": 1993}, {"title": "Robot programming by demonstration", "author": ["Sylvain Calinon"], "venue": "EPFL Press,", "citeRegEx": "Calinon.,? \\Q2009\\E", "shortCiteRegEx": "Calinon.", "year": 2009}, {"title": "On learning, representing, and generalizing a task in a humanoid robot", "author": ["Sylvain Calinon", "Florent Guenter", "Aude Billard"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics),", "citeRegEx": "Calinon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Calinon et al\\.", "year": 2007}, {"title": "Understanding prior intentions enables two\u2013year\u2013olds to imitatively learn a complex task", "author": ["Malinda Carpenter", "Josep Call", "Michael Tomasello"], "venue": "Child development,", "citeRegEx": "Carpenter et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Carpenter et al\\.", "year": 2002}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Sumit Chopra", "Raia Hadsell", "Yann LeCun"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905),", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Direct loss minimization inverse optimal control", "author": ["A. Doerr", "N. Ratliff", "J. Bohg", "M. Toussaint", "S. Schaal"], "venue": "In Proceedings of Robotics: Science and Systems (R:SS),", "citeRegEx": "Doerr et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Doerr et al\\.", "year": 2015}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "venue": "In ICML, pp", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Learning with augmented features for heterogeneous domain adaptation", "author": ["Lixin Duan", "Dong Xu", "Ivor Tsang"], "venue": "arXiv preprint arXiv:1206.4660,", "citeRegEx": "Duan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2012}, {"title": "Guided cost learning: Deep inverse optimal control via policy optimization", "author": ["C. Finn", "S. Levine", "P. Abbeel"], "venue": null, "citeRegEx": "Finn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2016}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Y. Ganin", "V. Lempitsky"], "venue": "Arxiv preprint 1409.7495,", "citeRegEx": "Ganin and Lempitsky.,? \\Q2014\\E", "shortCiteRegEx": "Ganin and Lempitsky.", "year": 2014}, {"title": "An object-based approach to map human hand synergies onto robotic hands with dissimilar kinematics", "author": ["G Gioioso", "G Salvietti", "M Malvezzi", "D Prattichizzo"], "venue": "Robotics: Science and Systems VIII, pp", "citeRegEx": "Gioioso et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gioioso et al\\.", "year": 2013}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Learning dexterous manipulation for a soft robotic hand from human demonstration", "author": ["Abhishek Gupta", "Clemens Eppner", "Sergey Levine", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1603.06348,", "citeRegEx": "Gupta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2016}, {"title": "Generative adversarial imitation learning", "author": ["J. Ho", "S. Ermon"], "venue": "arXiv pre-print: 1606.03476,", "citeRegEx": "Ho and Ermon.,? \\Q2016\\E", "shortCiteRegEx": "Ho and Ermon.", "year": 2016}, {"title": "Efficient learning of domain-invariant image representations", "author": ["Judy Hoffman", "Erik Rodner", "Jeff Donahue", "Trevor Darrell", "Kate Saenko"], "venue": "arXiv preprint arXiv:1301.3224,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Learning objective functions for manipulation", "author": ["M. Kalakrishnan", "P. Pastor", "L. Righetti", "S. Schaal"], "venue": "In International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Kalakrishnan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalakrishnan et al\\.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Discriminative clustering by regularized information maximization", "author": ["A. Krause", "P. Perona", "R.G. Gomes"], "venue": null, "citeRegEx": "Krause et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2010}, {"title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["Brian Kulis", "Kate Saenko", "Trevor Darrell"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Kulis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulis et al\\.", "year": 2011}, {"title": "Nonlinear inverse reinforcement learning with gaussian processes", "author": ["S. Levine", "Z. Popovic", "V. Koltun"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Levine et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2011}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Learning transferable features with deep adaptation networks", "author": ["Mingsheng Long", "Jianmin Wang"], "venue": "CoRR, abs/1502.02791,", "citeRegEx": "Long and Wang.,? \\Q2015\\E", "shortCiteRegEx": "Long and Wang.", "year": 2015}, {"title": "Domain adaptation: Learning bounds and algorithms", "author": ["Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "arXiv preprint arXiv:0902.3430,", "citeRegEx": "Mansour et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mansour et al\\.", "year": 2009}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Nine billion correspondence problems. Imitation and Social Learning in Robots, Humans and Animals: Behavioural, Social and Communicative Dimensions", "author": ["Chrystopher L Nehaniv"], "venue": null, "citeRegEx": "Nehaniv.,? \\Q2007\\E", "shortCiteRegEx": "Nehaniv.", "year": 2007}, {"title": "Like me?-measures of correspondence and imitation", "author": ["Chrystopher L Nehaniv", "Kerstin Dautenhahn"], "venue": "Cybernetics & Systems,", "citeRegEx": "Nehaniv and Dautenhahn.,? \\Q2001\\E", "shortCiteRegEx": "Nehaniv and Dautenhahn.", "year": 2001}, {"title": "Algorithms for inverse reinforcement learning", "author": ["A. Ng", "S. Russell"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Ng and Russell,? \\Q2000\\E", "shortCiteRegEx": "Ng and Russell", "year": 2000}, {"title": "Alvinn: An autonomous land vehicle in a neural network", "author": ["Dean A Pomerleau"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Pomerleau.,? \\Q1989\\E", "shortCiteRegEx": "Pomerleau.", "year": 1989}, {"title": "Bayesian inverse reinforcement learning", "author": ["D. Ramachandran", "E. Amir"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Ramachandran and Amir.,? \\Q2007\\E", "shortCiteRegEx": "Ramachandran and Amir.", "year": 2007}, {"title": "Maximum margin planning", "author": ["N. Ratliff", "J.A. Bagnell", "M.A. Zinkevich"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Ratliff et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2006}, {"title": "Boosting structured prediction for imitation learning", "author": ["N. Ratliff", "D. Bradley", "J.A. Bagnell", "J. Chestnutt"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Ratliff et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2007}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["St\u00e9phane Ross", "Geoffrey J Gordon", "Drew Bagnell"], "venue": "In AISTATS, pp", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Is imitation learning the route to humanoid robots", "author": ["Stefan Schaal"], "venue": "Trends in cognitive sciences,", "citeRegEx": "Schaal.,? \\Q1999\\E", "shortCiteRegEx": "Schaal.", "year": 1999}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I. Jordan", "Pieter Abbeel"], "venue": "Arxiv preprint 1502.05477,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1506.02438,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Learning shared latent structure for image synthesis and robotic imitation", "author": ["Aaron Shon", "Keith Grochow", "Aaron Hertzmann", "Rajesh P Rao"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Shon et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Shon et al\\.", "year": 2005}, {"title": "Deep domain confusion: Maximizing for domain invariance", "author": ["Eric Tzeng", "Judy Hoffman", "Ning Zhang", "Kate Saenko", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1412.3474,", "citeRegEx": "Tzeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tzeng et al\\.", "year": 2014}, {"title": "Towards adapting deep visuomotor representations from simulated to real environments", "author": ["Eric Tzeng", "Coline Devin", "Judy Hoffman", "Chelsea Finn", "Xingchao Peng", "Pieter Abbeel", "Sergey Levine", "Kate Saenko", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1511.07111,", "citeRegEx": "Tzeng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tzeng et al\\.", "year": 2015}, {"title": "Maximum entropy deep inverse reinforcement learning", "author": ["M. Wulfmeier", "P. Ondruska", "I. Posner"], "venue": null, "citeRegEx": "Wulfmeier et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Wulfmeier et al\\.", "year": 2017}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["B. Ziebart", "A. Maas", "J.A. Bagnell", "A.K. Dey"], "venue": null, "citeRegEx": "Ziebart et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 28, "context": "In recent years, combining techniques from deep learning with reinforcement learning has yielded a string of successful applications in game playing and robotics Mnih et al. (2015; 2016); Schulman et al. (2015a); Levine et al.", "startOffset": 162, "endOffset": 212}, {"referenceID": 26, "context": "(2015a); Levine et al. (2016). These successful applications, and the speed at which the abilities of RL algorithms have been increasing, makes it an exciting area of research with significant potential for future applications.", "startOffset": 9, "endOffset": 30}, {"referenceID": 26, "context": "(2015a); Levine et al. (2016). These successful applications, and the speed at which the abilities of RL algorithms have been increasing, makes it an exciting area of research with significant potential for future applications. One of the major weaknesses of RL is the need to manually specify a reward function. For each task we wish our agent to accomplish, we must provide it with a reward function whose maximizer will precisely recover the desired behavior. This weakness is addressed by the field of Inverse Reinforcement Learning (IRL). Given a set of expert trajectories, IRL algorithms produce a reward function under which these the expert trajectories enjoy the property of optimality. Recently, there has been a significant amount of work on IRL, and current algorithms can infer a reward function from a very modest number of demonstrations (e.g,. Abbeel & Ng (2004); Ratliff et al.", "startOffset": 9, "endOffset": 880}, {"referenceID": 26, "context": "(2015a); Levine et al. (2016). These successful applications, and the speed at which the abilities of RL algorithms have been increasing, makes it an exciting area of research with significant potential for future applications. One of the major weaknesses of RL is the need to manually specify a reward function. For each task we wish our agent to accomplish, we must provide it with a reward function whose maximizer will precisely recover the desired behavior. This weakness is addressed by the field of Inverse Reinforcement Learning (IRL). Given a set of expert trajectories, IRL algorithms produce a reward function under which these the expert trajectories enjoy the property of optimality. Recently, there has been a significant amount of work on IRL, and current algorithms can infer a reward function from a very modest number of demonstrations (e.g,. Abbeel & Ng (2004); Ratliff et al. (2006); Ziebart et al.", "startOffset": 9, "endOffset": 903}, {"referenceID": 26, "context": "(2015a); Levine et al. (2016). These successful applications, and the speed at which the abilities of RL algorithms have been increasing, makes it an exciting area of research with significant potential for future applications. One of the major weaknesses of RL is the need to manually specify a reward function. For each task we wish our agent to accomplish, we must provide it with a reward function whose maximizer will precisely recover the desired behavior. This weakness is addressed by the field of Inverse Reinforcement Learning (IRL). Given a set of expert trajectories, IRL algorithms produce a reward function under which these the expert trajectories enjoy the property of optimality. Recently, there has been a significant amount of work on IRL, and current algorithms can infer a reward function from a very modest number of demonstrations (e.g,. Abbeel & Ng (2004); Ratliff et al. (2006); Ziebart et al. (2008); Levine et al.", "startOffset": 9, "endOffset": 926}, {"referenceID": 26, "context": "(2015a); Levine et al. (2016). These successful applications, and the speed at which the abilities of RL algorithms have been increasing, makes it an exciting area of research with significant potential for future applications. One of the major weaknesses of RL is the need to manually specify a reward function. For each task we wish our agent to accomplish, we must provide it with a reward function whose maximizer will precisely recover the desired behavior. This weakness is addressed by the field of Inverse Reinforcement Learning (IRL). Given a set of expert trajectories, IRL algorithms produce a reward function under which these the expert trajectories enjoy the property of optimality. Recently, there has been a significant amount of work on IRL, and current algorithms can infer a reward function from a very modest number of demonstrations (e.g,. Abbeel & Ng (2004); Ratliff et al. (2006); Ziebart et al. (2008); Levine et al. (2011); Ho & Ermon (2016); Finn et al.", "startOffset": 9, "endOffset": 948}, {"referenceID": 26, "context": "(2015a); Levine et al. (2016). These successful applications, and the speed at which the abilities of RL algorithms have been increasing, makes it an exciting area of research with significant potential for future applications. One of the major weaknesses of RL is the need to manually specify a reward function. For each task we wish our agent to accomplish, we must provide it with a reward function whose maximizer will precisely recover the desired behavior. This weakness is addressed by the field of Inverse Reinforcement Learning (IRL). Given a set of expert trajectories, IRL algorithms produce a reward function under which these the expert trajectories enjoy the property of optimality. Recently, there has been a significant amount of work on IRL, and current algorithms can infer a reward function from a very modest number of demonstrations (e.g,. Abbeel & Ng (2004); Ratliff et al. (2006); Ziebart et al. (2008); Levine et al. (2011); Ho & Ermon (2016); Finn et al.", "startOffset": 9, "endOffset": 967}, {"referenceID": 16, "context": "(2011); Ho & Ermon (2016); Finn et al. (2016)).", "startOffset": 27, "endOffset": 46}, {"referenceID": 44, "context": "We offer an approach to this problem by borrowing ideas from domain confusion Tzeng et al. (2014) and generative adversarial networks (GANs) Goodfellow et al.", "startOffset": 78, "endOffset": 98}, {"referenceID": 19, "context": "(2014) and generative adversarial networks (GANs) Goodfellow et al. (2014). The high-level idea is to introduce an optimizer under which we can recover both a domain-agnostic representation of the agent\u2019s observations, and a cost function which utilizes this domain-agnostic representation to capture the essence of expert trajectories.", "startOffset": 50, "endOffset": 75}, {"referenceID": 19, "context": "(2014) and generative adversarial networks (GANs) Goodfellow et al. (2014). The high-level idea is to introduce an optimizer under which we can recover both a domain-agnostic representation of the agent\u2019s observations, and a cost function which utilizes this domain-agnostic representation to capture the essence of expert trajectories. We formulate this as a third-person RL-GAN problem, and our solution builds on the first-person RL-GAN formulation by Ho & Ermon (2016). Surprisingly, we find that this simple approach has been able to solve the problems that are presented in this paper (illustrated in Figure 1), even though the student\u2019s observations are related in a complicated way to the teacher\u2019s demonstrations (given that the observations and the demonstrations are pixel-level).", "startOffset": 50, "endOffset": 473}, {"referenceID": 41, "context": "Imitation learning has a long history, with several good survey articles, including (Schaal, 1999; Calinon, 2009; Argall et al., 2009).", "startOffset": 84, "endOffset": 134}, {"referenceID": 8, "context": "Imitation learning has a long history, with several good survey articles, including (Schaal, 1999; Calinon, 2009; Argall et al., 2009).", "startOffset": 84, "endOffset": 134}, {"referenceID": 2, "context": "Imitation learning has a long history, with several good survey articles, including (Schaal, 1999; Calinon, 2009; Argall et al., 2009).", "startOffset": 84, "endOffset": 134}, {"referenceID": 9, "context": "This reward function could be represented as nearness to a trajectory (Calinon et al., 2007; Abbeel et al., 2010), as a weighted combination of features (Abbeel & Ng, 2004; Ratliff et al.", "startOffset": 70, "endOffset": 113}, {"referenceID": 1, "context": "This reward function could be represented as nearness to a trajectory (Calinon et al., 2007; Abbeel et al., 2010), as a weighted combination of features (Abbeel & Ng, 2004; Ratliff et al.", "startOffset": 70, "endOffset": 113}, {"referenceID": 38, "context": ", 2010), as a weighted combination of features (Abbeel & Ng, 2004; Ratliff et al., 2006; Ramachandran & Amir, 2007; Ziebart et al., 2008; Boularias et al., 2011; Kalakrishnan et al., 2013; Doerr et al., 2015), or could also involve feature learning (Ratliff et al.", "startOffset": 47, "endOffset": 208}, {"referenceID": 5, "context": ", 2010), as a weighted combination of features (Abbeel & Ng, 2004; Ratliff et al., 2006; Ramachandran & Amir, 2007; Ziebart et al., 2008; Boularias et al., 2011; Kalakrishnan et al., 2013; Doerr et al., 2015), or could also involve feature learning (Ratliff et al.", "startOffset": 47, "endOffset": 208}, {"referenceID": 23, "context": ", 2010), as a weighted combination of features (Abbeel & Ng, 2004; Ratliff et al., 2006; Ramachandran & Amir, 2007; Ziebart et al., 2008; Boularias et al., 2011; Kalakrishnan et al., 2013; Doerr et al., 2015), or could also involve feature learning (Ratliff et al.", "startOffset": 47, "endOffset": 208}, {"referenceID": 13, "context": ", 2010), as a weighted combination of features (Abbeel & Ng, 2004; Ratliff et al., 2006; Ramachandran & Amir, 2007; Ziebart et al., 2008; Boularias et al., 2011; Kalakrishnan et al., 2013; Doerr et al., 2015), or could also involve feature learning (Ratliff et al.", "startOffset": 47, "endOffset": 208}, {"referenceID": 39, "context": ", 2015), or could also involve feature learning (Ratliff et al., 2007; Levine et al., 2011; Wulfmeier et al., 2015; Finn et al., 2016; Ho & Ermon, 2016).", "startOffset": 48, "endOffset": 152}, {"referenceID": 27, "context": ", 2015), or could also involve feature learning (Ratliff et al., 2007; Levine et al., 2011; Wulfmeier et al., 2015; Finn et al., 2016; Ho & Ermon, 2016).", "startOffset": 48, "endOffset": 152}, {"referenceID": 16, "context": ", 2015), or could also involve feature learning (Ratliff et al., 2007; Levine et al., 2011; Wulfmeier et al., 2015; Finn et al., 2016; Ho & Ermon, 2016).", "startOffset": 48, "endOffset": 152}, {"referenceID": 1, "context": "Imitation learning has a long history, with several good survey articles, including (Schaal, 1999; Calinon, 2009; Argall et al., 2009). Two main lines of work within imitation learning are: 1) behavioral cloning, where the demonstrations are used to directly learn a mapping from observations to actions using supervised learning, potentially with interleaving learning and data collection (e.g., Pomerleau (1989); Ross et al.", "startOffset": 114, "endOffset": 414}, {"referenceID": 1, "context": "Imitation learning has a long history, with several good survey articles, including (Schaal, 1999; Calinon, 2009; Argall et al., 2009). Two main lines of work within imitation learning are: 1) behavioral cloning, where the demonstrations are used to directly learn a mapping from observations to actions using supervised learning, potentially with interleaving learning and data collection (e.g., Pomerleau (1989); Ross et al. (2011)).", "startOffset": 114, "endOffset": 434}, {"referenceID": 31, "context": "Several recent advances in deep reinforcement learning have made this practical, including Deep Q-Networks (Mnih et al., 2015), Trust Region Policy Optimization (Schulman et al.", "startOffset": 107, "endOffset": 126}, {"referenceID": 17, "context": "As discussed in Nehaniv & Dautenhahn (2001) the \u201dwhat and how to imitate\u201d questions become significantly more challenging in this setting.", "startOffset": 16, "endOffset": 44}, {"referenceID": 7, "context": "Such a mapping is often difficult to obtain, and it typically relies on providing feature representations that captures the invariance between both environments Carpenter et al. (2002); Shon et al.", "startOffset": 161, "endOffset": 185}, {"referenceID": 7, "context": "Such a mapping is often difficult to obtain, and it typically relies on providing feature representations that captures the invariance between both environments Carpenter et al. (2002); Shon et al. (2005); Calinon et al.", "startOffset": 161, "endOffset": 205}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al.", "startOffset": 8, "endOffset": 46}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al.", "startOffset": 8, "endOffset": 69}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available.", "startOffset": 8, "endOffset": 90}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al.", "startOffset": 8, "endOffset": 288}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al.", "startOffset": 8, "endOffset": 307}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data.", "startOffset": 8, "endOffset": 332}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al.", "startOffset": 8, "endOffset": 685}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016).", "startOffset": 8, "endOffset": 774}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014).", "startOffset": 8, "endOffset": 793}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014). The problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al.", "startOffset": 8, "endOffset": 898}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014). The problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al. (2007); Mansour et al.", "startOffset": 8, "endOffset": 1073}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014). The problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al. (2007); Mansour et al. (2009); Kulis et al.", "startOffset": 8, "endOffset": 1096}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014). The problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al. (2007); Mansour et al. (2009); Kulis et al. (2011); Aytar & Zisserman (2011); Duan et al.", "startOffset": 8, "endOffset": 1117}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014). The problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al. (2007); Mansour et al. (2009); Kulis et al. (2011); Aytar & Zisserman (2011); Duan et al.", "startOffset": 8, "endOffset": 1143}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014). The problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al. (2007); Mansour et al. (2009); Kulis et al. (2011); Aytar & Zisserman (2011); Duan et al. (2012); Hoffman et al.", "startOffset": 8, "endOffset": 1163}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014). The problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al. (2007); Mansour et al. (2009); Kulis et al. (2011); Aytar & Zisserman (2011); Duan et al. (2012); Hoffman et al. (2013); Long & Wang (2015).", "startOffset": 8, "endOffset": 1186}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014). The problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al. (2007); Mansour et al. (2009); Kulis et al. (2011); Aytar & Zisserman (2011); Duan et al. (2012); Hoffman et al. (2013); Long & Wang (2015). It has also been shown that features trained in one domain can often be relevant to other domains Donahue et al.", "startOffset": 8, "endOffset": 1206}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014). The problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al. (2007); Mansour et al. (2009); Kulis et al. (2011); Aytar & Zisserman (2011); Duan et al. (2012); Hoffman et al. (2013); Long & Wang (2015). It has also been shown that features trained in one domain can often be relevant to other domains Donahue et al. (2014). The work most closely related to ours is Tzeng et al.", "startOffset": 8, "endOffset": 1327}, {"referenceID": 7, "context": "This work in turn relates to earlier work by Bromley et al. (1993); Chopra et al.", "startOffset": 45, "endOffset": 67}, {"referenceID": 7, "context": "This work in turn relates to earlier work by Bromley et al. (1993); Chopra et al. (2005), which also considers supervised training of deep feature embeddings.", "startOffset": 45, "endOffset": 89}, {"referenceID": 7, "context": "This work in turn relates to earlier work by Bromley et al. (1993); Chopra et al. (2005), which also considers supervised training of deep feature embeddings. Our approach to third-person imitation learning relies on reinforcement learning from raw sensory data in the imitator domain. Several recent advances in deep reinforcement learning have made this practical, including Deep Q-Networks (Mnih et al., 2015), Trust Region Policy Optimization (Schulman et al., 2015a), A3C Mnih et al. (2016), and Generalized Advantage Estimation (Schulman et al.", "startOffset": 45, "endOffset": 496}, {"referenceID": 6, "context": "(See Bridle et al. (1992); Barber & Agakov (2005); Krause et al.", "startOffset": 5, "endOffset": 26}, {"referenceID": 6, "context": "(See Bridle et al. (1992); Barber & Agakov (2005); Krause et al.", "startOffset": 5, "endOffset": 50}, {"referenceID": 6, "context": "(See Bridle et al. (1992); Barber & Agakov (2005); Krause et al. (2010); Chen et al.", "startOffset": 5, "endOffset": 72}, {"referenceID": 6, "context": "(See Bridle et al. (1992); Barber & Agakov (2005); Krause et al. (2010); Chen et al. (2016) for further discussion on instantiating the information term by introducing another classifier.", "startOffset": 5, "endOffset": 92}], "year": 2017, "abstractText": "Reinforcement learning (RL) makes it possible to train agents capable of achieving sophisticated goals in complex and uncertain environments. A key difficulty in reinforcement learning is specifying a reward function for the agent to optimize. Traditionally, imitation learning in RL has been used to overcome this problem. Unfortunately, hitherto imitation learning methods tend to require that demonstrations are supplied in the first-person: the agent is provided with a sequence of states and a specification of the actions that it should have taken. While powerful, this kind of imitation learning is limited by the relatively hard problem of collecting first-person demonstrations. Humans address this problem by learning from third-person demonstrations: they observe other humans perform tasks, infer the task, and accomplish the same task themselves. In this paper, we present a method for unsupervised third-person imitation learning. Here third-person refers to training an agent to correctly achieve a simple goal in a simple environment when it is provided a demonstration of a teacher achieving the same goal but from a different viewpoint; and unsupervised refers to the fact that the agent receives only these third-person demonstrations, and is not provided a correspondence between teacher states and student states. Our methods primary insight is that recent advances from domain confusion can be utilized to yield domain agnostic features which are crucial during the training process. To validate our approach, we report successful experiments on learning from third-person demonstrations in a pointmass domain, a reacher domain, and inverted pendulum.", "creator": "LaTeX with hyperref package"}}}