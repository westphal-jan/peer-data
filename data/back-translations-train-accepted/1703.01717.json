{"id": "1703.01717", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Measuring Sample Quality with Kernels", "abstract": "Approximate Markov chain Monte Carlo (MCMC) offers the promise of more rapid sampling at the cost of more biased inference. Since standard MCMC diagnostics fail to detect these biases, researchers have developed computable Stein discrepancy measures that provably determine the convergence of a sample to its target distribution. This approach was recently combined with the theory of reproducing kernels to define a closed-form kernel Stein discrepancy (KSD) computable by summing kernel evaluations across pairs of sample points. We develop a theory of weak convergence for KSDs based on Stein's method, demonstrate that commonly used KSDs fail to detect non-convergence even for Gaussian targets, and show that kernels with slowly decaying tails provably determine convergence for a large class of target distributions. The resulting convergence-determining KSDs are suitable for comparing biased, exact, and deterministic sample sequences and simpler to compute and parallelize than alternative Stein discrepancies. We use our tools to compare biased samplers, select sampler hyperparameters, and improve upon existing KSD approaches to one-sample hypothesis testing and sample quality improvement.", "histories": [["v1", "Mon, 6 Mar 2017 03:22:39 GMT  (1796kb,D)", "https://arxiv.org/abs/1703.01717v1", null], ["v2", "Mon, 12 Jun 2017 06:04:43 GMT  (1834kb,D)", "http://arxiv.org/abs/1703.01717v2", null], ["v3", "Fri, 7 Jul 2017 20:41:24 GMT  (1834kb,D)", "http://arxiv.org/abs/1703.01717v3", null], ["v4", "Tue, 11 Jul 2017 23:30:56 GMT  (1835kb,D)", "http://arxiv.org/abs/1703.01717v4", null], ["v5", "Fri, 21 Jul 2017 04:38:46 GMT  (1835kb,D)", "http://arxiv.org/abs/1703.01717v5", null], ["v6", "Thu, 3 Aug 2017 21:23:32 GMT  (1835kb,D)", "http://arxiv.org/abs/1703.01717v6", null], ["v7", "Sat, 19 Aug 2017 01:35:40 GMT  (1835kb,D)", "http://arxiv.org/abs/1703.01717v7", null], ["v8", "Wed, 13 Sep 2017 20:51:38 GMT  (1835kb,D)", "http://arxiv.org/abs/1703.01717v8", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["jackson gorham", "lester w mackey"], "accepted": true, "id": "1703.01717"}, "pdf": {"name": "1703.01717.pdf", "metadata": {"source": "META", "title": "Measuring Sample Quality with Kernels", "authors": ["Jackson Gorham", "Lester Mackey"], "emails": ["<jgorham@stanford.edu>,", "<lmackey@microsoft.com>."], "sections": [{"heading": "1. Introduction", "text": "If we consider the year 2016 as an impenetrable and opaque method of measurement, we will not have to deal with the question of whether there can ever be an improvement in quality. (...) If we perceive the year 2016 as impenetrable (...), we will not do it. (...) \"We.\" (...) \"We.\" (...) \"We.\" (...) \"We.\" (...) \"We.\" (...) \"We.\" (...) \"We.\" (...) \"We.\" (...) \"We.\" (...) \"We.\" (...) \"We.\" (...) \"We.\" (...) \"(...).\" (...). \"(...).\" (.... \"(...).\" (.... \"(...). (....\" (...). (. \"(...). (.\" We. \"(....\" (...). \"(.\" We. \"(...).\" (. \"(...).\" We. \"(.\" (...). \"(.\" (...). \"(.\" We. \"(...).\" (. \"(...).\" (. \"(...).\" (. \"(...).\" (. \"We.\" (. \"(...).\" (...). (. \"(.).\" (. (.). (.). \"(.\" (.). (. \"(.). (.). (.\" (.). (. (.). (.). \"(.). (.). (.). (. (.).\" We. (. (. \"(.). (.). (. (.). (.). (. (.). (.). (. (.). (. (.). (.). (. (.). (.). (We. (. (. (. (.). (.). (. (.). (. (.). (.). (.). (. (.). (. (.). (.). (.). (.). (We. (."}, {"heading": "2. Quality measures for samples", "text": "Consider a target distribution P with continuously differentiable (live) density p based on all samples i.e. we assume that the score function b, [log p] can be evaluated, but that direct integration under P is not feasible for most functions of interest. Therefore, we will not make assumptions about the origin of the sample points; they may be the output of a Markov chain or even deterministically generated. (Each Qn provides an approximation EQn [h) and qn a probability mass function. (xi) h (xi) h for each intractable expectation EP (h), and our goal is to compare the quality of the approximation of two samples aimed at P."}, {"heading": "3. Stein\u2019s method with kernels", "text": "Stein's Method (Stein, 1972) provides a three-step recipe for evaluating convergence in distribution: 1. Identify a stone operator T that defines the functions g: Rd \u2192 Rd from a domain G to real-rated functions T g such as EP [(T g) (Z)] = 0 for all g-G. For each such stone operator and stone set G, Gorham & Mackey (2015) defined the stone discrepancy asS (\u00b5, T, G), sup g G | E\u00b5 [(T g) (X)] | = dT G (\u00b5, P) (2), which, crucially, avoids explicit integration under P.2. Limit the stone discrepancy by an IPM dH that is known to dominate weak convergence. This can be done once for a wide class of target distributions to ensure that \u00b5m \u00b2 P is used whenever S (\u00b5m, T) \u2192 0 for a sequence of probability dimensions (Mackm)."}, {"heading": "3.1. Selecting a Stein operator and a Stein set", "text": "A standard that largely applies to stone is the density method operator (see Stein et al., 2004; Chatterjee & Shao, 2011; Chen et al., 2011; Ley et al., 2017), (T g) (x), 1p (x) d (x) g (x) g (x) g (x) g (x) b (x) g (x) g (x).The resulting Langevin stone operator (TP g) (x), 1p (x) < p (x) g (x), Gorham & Mackey (2015) generalizes this operator to multiple dimensions; the resulting Langevin stone operator (TP g) (x), 1p (x) < p (x)."}, {"heading": "3.2. Lower bounding the kernel Stein discrepancy", "text": "The next aim is to establish a joint coordination in which the KSD S (P, Gk, Gk) - only if the KSD (P, Gk) - and the KSD (P, Gk) - and the KSD (P, Gk) - and the KSD (P, Gk) - and the KSD (P, Gk) - and the KSD (P, GK) - and the KSD (P) - and the KSD (P, GK) - and the KSD (P, GK) - and the KSD (P) - and the KSD (P) - and the KSD (S) - and the KSD (S) - and the KSD (S) - and the KSD (S) - and the KSD (S) - and the KSD (S) - and the KSD (S) - and the KSD (S)."}, {"heading": "3.3. Upper bounding the kernel Stein discrepancy", "text": "The usual goal in the upper limit of the stone discrepancy is to calculate a convergence rate to P for certain approximate sequences (\u00b5m) \u221e m = 1. Since we aim to calculate the KSD directly for arbitrary samples Qn, our main purpose in this section is to ensure that the KSD S (\u00b5m, TP, Gk) converges to zero when the micrometer converges to P (desiderate (i). If the k value C (2.2) b and the p value is Lipschitz with the EP, then S (\u00b5m, TP, Gk) \u2192 0 whenever the Waterstone value implies dW value 2 (\u00b5m, P) \u2192 0, Proposition 9 for ordinary nuclei such as the Gaussian, Mate value, and IMQ value."}, {"heading": "4. Experiments", "text": "Next, we perform an empirical evaluation of the KSD quality standards recommended by our theory, recording all timings on an Intel Xeon CPU E5-2650 v2 @ 2.60GHz. Throughout the process, we point to the KSD with IMQ core k (x, y) = (c2 + x \u2212 y, 22) \u03b2, exponent \u03b2 = \u2212 1 2 and c = 1. Code reproducing all experiments can be found on the packet page of Julia (Bezanson et al., 2014) https: / / jgorham.github.io / SteinDiscrepancy.jl /."}, {"heading": "4.1. Comparing discrepancies", "text": "Our first, simple experiment is designed to illustrate several properties of the IMQ KSD and to compare its behavior with two existing variables of deviation, the Waterstone Distance dW-2, which can be calculated for simple univariate targets (Vallender, 1974), and the Stretch Chart Stone Deviation by Gorham & Mackey (2015). We take a bimodal Gaussian mixture with p (x) and a second sequence i.i.d. from one component of the mixture, N (\u2212 e1, Id) 2 x \u2212 1 \u00b2 x \u00b2 x \u00b2 \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 \u00b2 \u00b2 as our target P and generate a first sample point sequence i.i.i.d. The second sequence i.i.i.d resulted in a component of the mixture, N (\u2212 e1, Id).As shown in the left panel of Figure 1, where d = 1, the IMQ figure i.i.i.i.i.d."}, {"heading": "4.2. The importance of kernel choice", "text": "Theorem 6 found that nuclei with rapidly decaying tails yield KSDs that can be zero-driven by off-target sample sequences. Our next experiment provides an empirical demonstration of this problem for a multivariate Gaussian target P = N (0, Id) and KSDs based on popular Gaussian (k (x, y) = e \u2212 x \u2212 y \u00b2 2 / 2) and mate \u00b2 rn (k (x, y) = (1 + 3 x \u2212 y \u00b2 2) e \u2212 3 x \u2212 y \u00b2 2) radial nucleus. According to evidence theorem 6 in section F, we construct an off-target sequence (Qn) n \u00b2 n \u00b2 n \u00b2 1 that sends S (Qn, TP, Gk) to 0 for these kernel choices whenever d \u00b2 3 and whenever for each n-scan \u00b2 deviation, we leave Qn = 1n \u00b2 n \u00b2 sampel \u00b2 n where for all i and j \u00b2 deviations."}, {"heading": "4.3. Selecting sampler hyperparameters", "text": "The approximate sample sampler from DuBois et al. (2014) is a biased MCMC method designed to accelerate conclusions when the target density takes the form p (x) \u0445 \u03c0 (x) \u0445 L = 1 \u03c0 (yl | x) for \u03c0 (\u00b7) a prior distribution of the sample on Rd and \u03c0 (yl | x) assumes the probability of a datapoint yl. A standard section sampler must evaluate the probability of all L data points in order to draw each new sample point xi. To reduce these costs, the approximate sampler introduces a tuning parameter that determines the number of datapoints contributing to an approximation of the sampling step; an appropriate setting of this parameter is imperative in order to draw precise conclusions. If the sample is too small, relatively few points are generated in a given sampler, the sampling time being independent, the sampling expectation being generated by the Monte Carlo sample."}, {"heading": "4.4. Selecting samplers", "text": "Ahn et al. (2012) developed two biased MCMC samplers for accelerated posterior inference, both called Stochastic Gradient Fisher Scoring (SGFS). In the full version of SGFS (called SGFS-f), a d \u00b7 d matrix must be inverted to draw each new sampling point. As this can be costly for large d, the authors developed a second sampler (called SGFS-d) in which only a diagonal matrix must be inverted to draw each new sampling point. Both samplers can be regarded as discrete time approximations to a continuous Markov process that has target P as its stationary distribution; however, because no Metropolis-Hastings correction is used, neither sampler has the target as its stationary distribution."}, {"heading": "4.5. Beyond sample quality comparison", "text": "While our study of the KSD was motivated by the desire to develop practical, trustworthy tools for comparing the quality of samples, the cores recommended by our theory can serve as a substitute for other inferential tasks that utilize kernel discrepancies."}, {"heading": "4.5.1. ONE-SAMPLE HYPOTHESIS TESTING", "text": "Chwialkowski et al. (2016) recently used the KSD S (Qn, TP, Gk) to develop a hypotheses test to determine whether a given sample from a Markov chain was drawn from a target distribution P (see also Liu et al., 2016). However, the authors pointed out that the KSD test with its standard Gaussian nucleus k experienced a considerable loss of performance when dimension d increased. We reconstruct their experiment and show that this loss of performance can be avoided by using our standard IMQ core with \u03b2 = -12 and c = 1. Subsequently (Chwialkowski et al., 2016, Section 4) we draw zi iid-N (0, Id) and ui iid-Unif [0, 1] to compare a sample (xi) n i = 1 with xi = zi + ui e1 for n = 500 and different dimensions. Using the (modified) author code of the IMQ nuclear test, we compare the performance of the SD = 1 Gau\u00dfality test with the results of the SD = 1."}, {"heading": "4.5.2. IMPROVING SAMPLE QUALITY", "text": "Liu & Lee (2016) recently used KSD S (Qn, TP, Gk) as a means to improve the quality of a sample. Specifically, in a first sample, they minimize Qn based on x1,.., xn, S (Q, TP, Gk) across all measures supported on the same sample points to obtain a new sample that better approximates P across the class of test functions H = TPGk. In all experiments, Liu & Lee (2016) use a Gaussian kernel k (x, y) = e \u2212 1 h, x \u2212 y, 2, with bandwidth h chosen as the median of the square euclidean distance between the sample pairs. Using the authors \"code, we replicate the experiment (Liu & Lee, 2016, Fig. 2b) and execute a KSD target with an IMQ kernel (x, y) = (1 + 1h,.y)."}, {"heading": "5. Related and future work", "text": "The results of the study show that the number of people recorded and recorded is the number of people recorded and recorded who are able to identify themselves. (...) The number of people recorded and recorded who are able to identify themselves is very high. (...) The number of people recorded who are able to identify themselves is very high. (...) The number of people recorded who are able to identify themselves is very low. (...) The number of people recorded who are able to identify themselves. (...) The number of people recorded who are able to identify themselves is very high. (...) The number of people recorded who are able to identify themselves is very low. (...) The number of people recorded who are able to identify themselves. (...) The number of people recorded who are able to identify themselves."}, {"heading": "Acknowledgments", "text": "We thank Kacper Chwialkowski, Heiko Strathmann, and Arthur Gretton for sharing their hypotheses test code, Qiang Liu for sharing his sampling code for the black box, and Sebastian Vollmer and Andrew Duncan for many helpful conversations about this work. This material is based on work supported by the National Science Foundation DMS RTG Grant No. 1501767, the National Science Foundation Graduate Research Fellowship under Grant No. DGE-114747, and the Frederick E. Terman Fellowship."}, {"heading": "A. Additional appendix notation", "text": "We use f \u0445 h to denote the convolution between f and h, and, for absolutely integrable f: Rd \u2192 R, let's say f: (\u03c9), (2\u03c0) \u2212 d / 2: f (x) e \u2212 i < x, \u03c9 > dx is the Fourier transformation of f. For g: g: Kdk we define g: g: Kdk, \"d: 1: gj: 2 Kk.\" Let's leave L: 2 the banach space of the real functions f: f: f: f: L2, f: x: 2 dx < for g: Rd: 2 we overload g: g: 1: g: L2, p: 1: 2 L2 <. We define the operator norm of a vector a: Rd: a: a: p: 2 and a matrix A: pd: p: d, p: Rd: 2 < and the operator norm of a vector a: Rd: a: 2 and we use the constanx: Rd: 2."}, {"heading": "B. Proof of Proposition 1: Zero mean test functions", "text": "Since k-C (1,1), supx-Rd-k (x, x) < and supx-Rd-x-yk (x, x), op <, Cor. 4,36 of (Steinwart & Christmann, 2008) implies that M0 (gj) < and M1 (gj) < for each j < 1,..., d}. Like EP [\u30fb b (Z), 2] < \u221e, the detection of (Gorham & Mackey, 2015, Prop. 1) now implies EP [(TP g) (Z)] = 0."}, {"heading": "C. Proof of Proposition 2: KSD closed form", "text": "Our proof generalizes that (Chwialkowski et al., 2016, Thm.) (for each dimension j (1,., d) we use the operator T jP over (T j P g0) (x), 1 p (x), 1 p (x), 1 p (p (x) g0 (x), 1 (x) + bj (x) g0 (x) for g0: Rd \u2192 R. We also allow: Rd \u2192 Kk the canonical character map of Kk (p (x) g0 (x), k (x), k), k (x), k (k), k (k), k (k), k (k)."}, {"heading": "D. Proof of Proposition 3: Stein set equivalence", "text": "By sentence 2, S (\u00b5, TP, Gk, Hm.) = Hm. W and S (\u00b5, TP, Gk, Hm. 2) = Hm. 2 for any vector w, and by (Bachman & Narici, 1966, Thm. 8.7) there are constants cd, c \"d > 0 that depend only on d and Hm. \u00b7, so that cd-w-w-w-2 \u2264 c\" d-w-w. \""}, {"heading": "E. Proof of Theorem 5: Univariate KSD detects non-convergence", "text": "The statement of the Gaussian kernel kb (x), y) d\u00b5 (y), y) d\u00b5 (y), p), p), p), p), p), p), p (e), p (e), p (e), p (e), p (e), p (e), p (e), p (e), p (e), p (e), p (e), p (e), p (e), p (e), p (e), p (e), p (e), p (e), p (e), p (e), p (e), p (p), p (e), p (e), p (e), p (e), p (p), p (e), p (e, p, p, p (e, p, p, p, p, p (e, p, p, p, e, p, p (e, p, p, p, p, e, p, p (e), p (e), p (e)."}, {"heading": "F. Proof of Theorem 6: KSD fails with light kernel tails", "text": "First, we define the generalized inverse function \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - \"xi\" - xi \"- xi\" - \"xi\" - xi \"-\" xi \"- xi\" - \"xi\" - xi \"- xi\" - xi \"- xi\" - xi \"- xi\" - xi \"-\" xi \"-\" xi \"- xi -\" xi \"- xi\" - \"i\" - xi \"-\" i."}, {"heading": "G. Proof of Theorem 7: KSD detects tight non-convergence", "text": "For all probability measures \u00b5 on Rd and > 0 there is a uniform definition of its tightness rate asR (\u00b5,), inf {r \u2265 0 | \u00b5 2 > r) \u2264. \"(10) Theorem 7 results from the following result, which transforms the upper limit of the Lipschitz metric dBL (\u00b5, P) in relation to the tightness rate R (\u00b5,), the rate of decay of the generalized Fourier, and the KSD S (\u00b5, Gk). Theorem 13 (KSD tightness lower bound). (2) Theorem 13 (KSD tightness lower bound)."}, {"heading": "H. Proof of Theorem 8: IMQ KSD detects non-convergence", "text": "We first use the following theorem on the upper limit of the Lipschitz function (1). \u2212 k In addition, the function (2) is defined with respect to the KSD S (\u00b5, TP, Gk). \u2212 k In addition, the function (2) is defined with respect to the KSD S (\u00b5, TP, Gk). \u2212 k Suppose there is a 0 > 0 and a constant MP (x, y) = (c2 + x, 22) \u03b2 for c > 0, and \u03b2 (\u2212 1, 0)."}, {"heading": "I. Proof of Proposition 9: KSD detects convergence", "text": "& & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; &"}, {"heading": "J. Proof of Theorem 10: KSD fails for bounded scores", "text": "We will show that if M0 (b) is finite, S (Qn, TP, Gk) \u2192 0 as n \u00b2. We can use k0 (x, y), p = 1 k = 1 k j 0 (x, y) ask0 (x, y) ask0 (x, y) ask0 (x, y) ask0 (x, y) ask0 (x, y) ask0 (x, y) ask0 (x, y) ask0 (x) b (b), b (x, y) > k (b) > k (x) > k), p (l), p (x) xi (x, y) + < b), p (x), p (x) xi (x) (x, y), k)."}], "references": [{"title": "Bayesian posterior sampling via stochastic gradient Fisher scoring", "author": ["S. Ahn", "A. Korattikara", "M. Welling"], "venue": "In Proc. 29th ICML,", "citeRegEx": "Ahn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2012}, {"title": "Functional Analysis. Academic Press textbooks in mathematics", "author": ["G. Bachman", "L. Narici"], "venue": "Dover Publications,", "citeRegEx": "Bachman and Narici,? \\Q1966\\E", "shortCiteRegEx": "Bachman and Narici", "year": 1966}, {"title": "Integration of radial functions", "author": ["J. Baker"], "venue": "Mathematics Magazine,", "citeRegEx": "Baker,? \\Q1999\\E", "shortCiteRegEx": "Baker", "year": 1999}, {"title": "Stein\u2019s method and Poisson process convergence", "author": ["A.D. Barbour"], "venue": "J. Appl. Probab., (Special Vol. 25A):175\u2013184,", "citeRegEx": "Barbour,? \\Q1988\\E", "shortCiteRegEx": "Barbour", "year": 1988}, {"title": "Stein\u2019s method for diffusion approximations", "author": ["A.D. Barbour"], "venue": "Probab. Theory Related Fields,", "citeRegEx": "Barbour,? \\Q1990\\E", "shortCiteRegEx": "Barbour", "year": 1990}, {"title": "A consistent test for multivariate normality based on the empirical characteristic function", "author": ["L. Baringhaus", "N. Henze"], "venue": "Metrika,", "citeRegEx": "Baringhaus and Henze,? \\Q1988\\E", "shortCiteRegEx": "Baringhaus and Henze", "year": 1988}, {"title": "Julia: A fresh approach to numerical computing", "author": ["J. Bezanson", "A. Edelman", "S. Karpinski", "V.B. Shah"], "venue": "arXiv preprint arXiv:1411.1607,", "citeRegEx": "Bezanson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bezanson et al\\.", "year": 2014}, {"title": "Handbook of Markov chain Monte Carlo", "author": ["S. Brooks", "A. Gelman", "G. Jones", "Meng", "X.-L"], "venue": "CRC press,", "citeRegEx": "Brooks et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Brooks et al\\.", "year": 2011}, {"title": "Vector valued reproducing kernel hilbert spaces and universality", "author": ["C. Carmeli", "E. De Vito", "A. Toigo", "V. Umanit\u00e1"], "venue": "Analysis and Applications,", "citeRegEx": "Carmeli et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carmeli et al\\.", "year": 2010}, {"title": "Nonnormal approximation by Stein\u2019s method of exchangeable pairs with application to the Curie-Weiss model", "author": ["S. Chatterjee", "Q. Shao"], "venue": "Ann. Appl. Probab.,", "citeRegEx": "Chatterjee and Shao,? \\Q2011\\E", "shortCiteRegEx": "Chatterjee and Shao", "year": 2011}, {"title": "Normal approximation by Stein\u2019s method. Probability and its Applications", "author": ["L. Chen", "L. Goldstein", "Q. Shao"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "A kernel test of goodness of fit", "author": ["K. Chwialkowski", "H. Strathmann", "A. Gretton"], "venue": "In Proc. 33rd ICML,", "citeRegEx": "Chwialkowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chwialkowski et al\\.", "year": 2016}, {"title": "Approximate slice sampling for Bayesian posterior inference", "author": ["C. DuBois", "A. Korattikara", "M. Welling", "P. Smyth"], "venue": "In Proc. 17th AISTATS,", "citeRegEx": "DuBois et al\\.,? \\Q2014\\E", "shortCiteRegEx": "DuBois et al\\.", "year": 2014}, {"title": "Reflection couplings and contraction rates for diffusions", "author": ["A. Eberle"], "venue": "Probab. Theory Related Fields, pp", "citeRegEx": "Eberle,? \\Q2015\\E", "shortCiteRegEx": "Eberle", "year": 2015}, {"title": "Output assessment for Monte Carlo simulations via the score statistic", "author": ["Y. Fan", "S.P. Brooks", "A. Gelman"], "venue": "J. Comp. Graph. Stat.,", "citeRegEx": "Fan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2006}, {"title": "Kernel measures of conditional dependence", "author": ["K. Fukumizu", "A. Gretton", "X. Sun", "B. Sch\u00f6lkopf"], "venue": "In NIPS,", "citeRegEx": "Fukumizu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2007}, {"title": "Markov chain Monte Carlo maximum likelihood", "author": ["C.J. Geyer"], "venue": "Computer Science and Statistics: Proc. 23rd Symp. Interface,", "citeRegEx": "Geyer,? \\Q1991\\E", "shortCiteRegEx": "Geyer", "year": 1991}, {"title": "Measuring sample quality with Stein\u2019s method", "author": ["J. Gorham", "L. Mackey"], "venue": "Adv. NIPS", "citeRegEx": "Gorham and Mackey,? \\Q2015\\E", "shortCiteRegEx": "Gorham and Mackey", "year": 2015}, {"title": "Measuring sample quality with diffusions", "author": ["J. Gorham", "A. Duncan", "S. Vollmer", "L. Mackey"], "venue": null, "citeRegEx": "Gorham et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gorham et al\\.", "year": 2016}, {"title": "On the rate of convergence in the multivariate CLT", "author": ["F. G\u00f6tze"], "venue": "Ann. Probab.,", "citeRegEx": "G\u00f6tze,? \\Q1991\\E", "shortCiteRegEx": "G\u00f6tze", "year": 1991}, {"title": "A kernel two-sample test", "author": ["A. Gretton", "K. Borgwardt", "M. Rasch", "B. Sch\u00f6lkopf", "A. Smola"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "The Plancherel formula, the Plancherel theorem, and the Fourier transform of orbital integrals", "author": ["R. Herb", "P.J. Sally Jr."], "venue": "In Representation Theory and Mathematical Physics: Conference in Honor of Gregg Zuckerman\u2019s 60th Birthday,", "citeRegEx": "Herb and Jr.,? \\Q2009\\E", "shortCiteRegEx": "Herb and Jr.", "year": 2009}, {"title": "Austerity in MCMC land: Cutting the Metropolis-Hastings budget", "author": ["A. Korattikara", "Y. Chen", "M. Welling"], "venue": "In Proc. of 31st ICML,", "citeRegEx": "Korattikara et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Korattikara et al\\.", "year": 2014}, {"title": "Stein\u2019s method for comparison of univariate distributions", "author": ["C. Ley", "G. Reinert", "Y. Swan"], "venue": "Probab. Surveys,", "citeRegEx": "Ley et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ley et al\\.", "year": 2017}, {"title": "Two methods for wild variational inference", "author": ["Q. Liu", "Y. Feng"], "venue": "arXiv preprint arXiv:1612.00081,", "citeRegEx": "Liu and Feng,? \\Q2016\\E", "shortCiteRegEx": "Liu and Feng", "year": 2016}, {"title": "Variational Gradient Descent: A General Purpose", "author": ["Q. Liu", "Wang", "D. Stein"], "venue": "Bayesian Inference Algorithm", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "A kernelized Stein discrepancy for goodness-of-fit tests", "author": ["Q. Liu", "J. Lee", "M. Jordan"], "venue": "In Proc. of 33rd ICML, volume 48 of ICML,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Multivariate Stein factors for a class of strongly log-concave distributions", "author": ["L. Mackey", "J. Gorham"], "venue": "Electron. Commun. Probab.,", "citeRegEx": "Mackey and Gorham,? \\Q2016\\E", "shortCiteRegEx": "Mackey and Gorham", "year": 2016}, {"title": "Integral probability metrics and their generating classes of functions", "author": ["A. M\u00fcller"], "venue": "Ann. Appl. Probab.,", "citeRegEx": "M\u00fcller,? \\Q1997\\E", "shortCiteRegEx": "M\u00fcller", "year": 1997}, {"title": "Control functionals for QuasiMonte Carlo integration", "author": ["C. Oates", "M. Girolami"], "venue": null, "citeRegEx": "Oates and Girolami,? \\Q2015\\E", "shortCiteRegEx": "Oates and Girolami", "year": 2015}, {"title": "Convergence rates for a class of estimators based on steins method", "author": ["C. Oates", "J. Cockayne", "F. Briol", "M. Girolami"], "venue": "arXiv preprint arXiv:1603.03220,", "citeRegEx": "Oates et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oates et al\\.", "year": 2016}, {"title": "Control functionals for Monte Carlo integration", "author": ["C.J. Oates", "M. Girolami", "N. Chopin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), pp. n/a\u2013n/a,", "citeRegEx": "Oates et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oates et al\\.", "year": 2016}, {"title": "Operator variational inference", "author": ["R. Ranganath", "D. Tran", "J. Altosaar", "D. Blei"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ranganath et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2016}, {"title": "Kernel distribution embeddings: Universal kernels, characteristic kernels and kernel metrics on distributions", "author": ["C. Simon-Gabriel", "B. Sch\u00f6lkopf"], "venue": "arXiv preprint arXiv:1604.05251,", "citeRegEx": "Simon.Gabriel and Sch\u00f6lkopf,? \\Q2016\\E", "shortCiteRegEx": "Simon.Gabriel and Sch\u00f6lkopf", "year": 2016}, {"title": "On the optimal estimation of probability measures in weak and strong", "author": ["B. Sriperumbudur"], "venue": "topologies. Bernoulli,", "citeRegEx": "Sriperumbudur,? \\Q2016\\E", "shortCiteRegEx": "Sriperumbudur", "year": 2016}, {"title": "Hilbert space embeddings and metrics on probability measures", "author": ["B. Sriperumbudur", "A. Gretton", "K. Fukumizu", "B. Sch\u00f6lkopf", "G. Lanckriet"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2010}, {"title": "A bound for the error in the normal approximation to the distribution of a sum of dependent random variables", "author": ["C. Stein"], "venue": "In Proc. 6th Berkeley Symposium on Mathematical Statistics and Probability (Univ. California, Berkeley,", "citeRegEx": "Stein,? \\Q1971\\E", "shortCiteRegEx": "Stein", "year": 1971}, {"title": "Use of exchangeable pairs in the analysis of simulations. In Stein\u2019s method: expository lectures and applications, volume 46 of IMS Lecture Notes Monogr", "author": ["C. Stein", "P. Diaconis", "S. Holmes", "G. Reinert"], "venue": "Ser., pp. 1\u201326. Inst. Math. Statist., Beachwood,", "citeRegEx": "Stein et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Stein et al\\.", "year": 2004}, {"title": "Positive definite functions and generalizations, an historical survey", "author": ["J. Stewart"], "venue": "Rocky Mountain J. Math.,", "citeRegEx": "Stewart,? \\Q1976\\E", "shortCiteRegEx": "Stewart", "year": 1976}, {"title": "Calculation of the Wasserstein distance between probability distributions on the line", "author": ["S. Vallender"], "venue": "Theory Probab. Appl.,", "citeRegEx": "Vallender,? \\Q1974\\E", "shortCiteRegEx": "Vallender", "year": 1974}, {"title": "High-dimensional statistics: A non-asymptotic viewpoint. 2017. URL http: //www.stat.berkeley.edu/ \u0303wainwrig/ nachdiplom/Chap5_Sep10_2015.pdf", "author": ["M. Wainwright"], "venue": null, "citeRegEx": "Wainwright,? \\Q2015\\E", "shortCiteRegEx": "Wainwright", "year": 2015}, {"title": "Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning", "author": ["D. Wang", "Q. Liu"], "venue": null, "citeRegEx": "Wang and Liu,? \\Q2016\\E", "shortCiteRegEx": "Wang and Liu", "year": 2016}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["M. Welling", "Y. Teh"], "venue": "In ICML,", "citeRegEx": "Welling and Teh,? \\Q2011\\E", "shortCiteRegEx": "Welling and Teh", "year": 2011}, {"title": "Scattered data approximation, volume 17", "author": ["H. Wendland"], "venue": "Cambridge university press,", "citeRegEx": "Wendland,? \\Q2004\\E", "shortCiteRegEx": "Wendland", "year": 2004}, {"title": "Proof of Proposition 2: KSD closed form Our proof generalizes that of (Chwialkowski et al., 2016", "author": ["C. EP [(TP g)(Z"], "venue": "Thm. 2.1). For each dimension j \u2208 {1,", "citeRegEx": "0.,? \\Q2016\\E", "shortCiteRegEx": "0.", "year": 2016}, {"title": "x)kb(x, y) and \u2207r(x) = \u2212xr(x). Thus for any x, by (Steinwart & Christmann, 2008, Corollary 4.36) we have \u2016\u2207h(x)\u20162 \u2264 Kk\u0303b \u3008\u2207x,\u2207yk\u0303b(x", "author": ["\u2207xkb(x"], "venue": null, "citeRegEx": "\u2207xkb.x and y,? \\Q2008\\E", "shortCiteRegEx": "\u2207xkb.x and y", "year": 2008}, {"title": "When bounds on R and F are known, the final expression can be optimized over and \u03b4 to produce rates of convergence in dBL\u2016\u00b7\u20162 . Consider now a sequence of probability measures (\u03bcm)m\u22651 that is uniformly tight. This implies that lim supmR(\u03bcm, ) < \u221e for all", "author": ["Gorham"], "venue": null, "citeRegEx": "Gorham,? \\Q2016\\E", "shortCiteRegEx": "Gorham", "year": 2016}, {"title": "Taking \u2192 0 yields dBL\u2016\u00b7\u20162 (\u03bcm, P )\u2192", "author": ["\u03b8d MP"], "venue": null, "citeRegEx": "..,? \\Q2016\\E", "shortCiteRegEx": "..", "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "When Bayesian inference and maximum likelihood estimation (Geyer, 1991) demand the evaluation of intractable expectations EP [h(Z)] = \u222b p(x)h(x)dx under a target distribution P , Markov chain Monte Carlo (MCMC) methods (Brooks et al.", "startOffset": 58, "endOffset": 71}, {"referenceID": 7, "context": "When Bayesian inference and maximum likelihood estimation (Geyer, 1991) demand the evaluation of intractable expectations EP [h(Z)] = \u222b p(x)h(x)dx under a target distribution P , Markov chain Monte Carlo (MCMC) methods (Brooks et al., 2011) are often employed to approximate these integrals with asymptotically correct sample averages EQn [h(X)] = 1 n \u2211n i=1 h(xi).", "startOffset": 219, "endOffset": 240}, {"referenceID": 0, "context": "MCMC methods are computationally expensive, and recent years have seen the introduction of biased MCMC procedures (see, e.g., Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed.", "startOffset": 114, "endOffset": 190}, {"referenceID": 22, "context": "MCMC methods are computationally expensive, and recent years have seen the introduction of biased MCMC procedures (see, e.g., Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed.", "startOffset": 114, "endOffset": 190}, {"referenceID": 7, "context": "Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P .", "startOffset": 150, "endOffset": 171}, {"referenceID": 18, "context": "To address this shortcoming, we develop a theory of weak convergence for the kernel Stein discrepancies analogous to that of (Gorham & Mackey, 2015; Mackey & Gorham, 2016; Gorham et al., 2016) and design a class of kernel Stein discrepancies that provably control weak convergence for a large class of target distributions.", "startOffset": 125, "endOffset": 192}, {"referenceID": 0, "context": ", Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P .", "startOffset": 23, "endOffset": 366}, {"referenceID": 0, "context": ", Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al.", "startOffset": 23, "endOffset": 553}, {"referenceID": 0, "context": ", Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al.", "startOffset": 23, "endOffset": 577}, {"referenceID": 0, "context": ", Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al. (2016) further showed that specific members of this family \u2013 the graph Stein discrepancies \u2013 were (a) efficiently computable by solving a linear program and (b) convergence-determining for large classes of targets P .", "startOffset": 23, "endOffset": 599}, {"referenceID": 0, "context": ", Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al. (2016) further showed that specific members of this family \u2013 the graph Stein discrepancies \u2013 were (a) efficiently computable by solving a linear program and (b) convergence-determining for large classes of targets P . Building on the zero mean reproducing kernel theory of Oates et al. (2016b), Chwialkowski et al.", "startOffset": 23, "endOffset": 886}, {"referenceID": 0, "context": ", Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al. (2016) further showed that specific members of this family \u2013 the graph Stein discrepancies \u2013 were (a) efficiently computable by solving a linear program and (b) convergence-determining for large classes of targets P . Building on the zero mean reproducing kernel theory of Oates et al. (2016b), Chwialkowski et al. (2016) and Liu et al.", "startOffset": 23, "endOffset": 914}, {"referenceID": 0, "context": ", Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al. (2016) further showed that specific members of this family \u2013 the graph Stein discrepancies \u2013 were (a) efficiently computable by solving a linear program and (b) convergence-determining for large classes of targets P . Building on the zero mean reproducing kernel theory of Oates et al. (2016b), Chwialkowski et al. (2016) and Liu et al. (2016) later showed that other members of the Stein discrepancy family had a closed-form solution involving the sum of kernel evaluations over pairs of sample points.", "startOffset": 23, "endOffset": 936}, {"referenceID": 28, "context": "In this case, we call (1) an integral probability metric (IPM) (M\u00fcller, 1997).", "startOffset": 63, "endOffset": 77}, {"referenceID": 28, "context": "In this case, we call (1) an integral probability metric (IPM) (M\u00fcller, 1997). For example, when H = BL\u2016\u00b7\u20162 , {h : R \u2192 R |M0(h) + M1(h) \u2264 1}, the IPM dBL\u2016\u00b7\u20162 is called the bounded Lipschitz or Dudley metric and exactly metrizes convergence in distribution. Alternatively, when H = W\u2016\u00b7\u20162 , {h : R d \u2192 R |M1(h) \u2264 1} is the set of 1-Lipschitz functions, the IPM dW\u2016\u00b7\u2016 in (1) is known as the Wasserstein metric. An apparent practical problem with using the IPM dH as a sample quality measure is that EP [h(Z)] may not be computable for h \u2208 H. However, if H were chosen such that EP [h(Z)] = 0 for all h \u2208 H, then no explicit integration under P would be necessary. To generate such a class of test functions and to show that the resulting IPM still satisfies our desiderata, we follow the lead of Gorham & Mackey (2015) and consider Charles Stein\u2019s method for characterizing distributional convergence.", "startOffset": 64, "endOffset": 816}, {"referenceID": 36, "context": "For any such Stein operator and Stein set G, Gorham & Mackey (2015) defined the Stein discrepancy as", "startOffset": 13, "endOffset": 68}, {"referenceID": 18, "context": "While Stein\u2019s method is principally used as a mathematical tool to prove convergence in distribution, we seek, in the spirit of (Gorham & Mackey, 2015; Gorham et al., 2016), to harness the Stein discrepancy as a practical tool for measuring sample quality.", "startOffset": 128, "endOffset": 172}, {"referenceID": 10, "context": "Selecting a Stein operator and a Stein set A standard, widely applicable univariate Stein operator is the density method operator (see Stein et al., 2004; Chatterjee & Shao, 2011; Chen et al., 2011; Ley et al., 2017), (T g)(x) , 1 p(x) d dx (p(x)g(x)) = g(x)b(x) + g \u2032(x).", "startOffset": 130, "endOffset": 216}, {"referenceID": 23, "context": "Selecting a Stein operator and a Stein set A standard, widely applicable univariate Stein operator is the density method operator (see Stein et al., 2004; Chatterjee & Shao, 2011; Chen et al., 2011; Ley et al., 2017), (T g)(x) , 1 p(x) d dx (p(x)g(x)) = g(x)b(x) + g \u2032(x).", "startOffset": 130, "endOffset": 216}, {"referenceID": 18, "context": "While our work is compatible with other practical Stein operators, like the family of diffusion Stein operators defined in (Gorham et al., 2016), we will focus on the Langevin operator for the sake of brevity.", "startOffset": 123, "endOffset": 144}, {"referenceID": 3, "context": "Inspired by the generator method of Barbour (1988; 1990) and G\u00f6tze (1991), Gorham & Mackey (2015) generalized this operator to multiple dimensions.", "startOffset": 36, "endOffset": 74}, {"referenceID": 3, "context": "Inspired by the generator method of Barbour (1988; 1990) and G\u00f6tze (1991), Gorham & Mackey (2015) generalized this operator to multiple dimensions.", "startOffset": 36, "endOffset": 98}, {"referenceID": 3, "context": "Inspired by the generator method of Barbour (1988; 1990) and G\u00f6tze (1991), Gorham & Mackey (2015) generalized this operator to multiple dimensions. The resulting Langevin Stein operator (TP g)(x) , 1 p(x) \u3008\u2207, p(x)g(x)\u3009 = \u3008g(x), b(x)\u3009+ \u3008\u2207, g(x)\u3009 for functions g : R \u2192 R was independently developed, without connection to Stein\u2019s method, by Oates et al. (2016b) for the design of Monte Carlo control functionals.", "startOffset": 36, "endOffset": 360}, {"referenceID": 3, "context": "Inspired by the generator method of Barbour (1988; 1990) and G\u00f6tze (1991), Gorham & Mackey (2015) generalized this operator to multiple dimensions. The resulting Langevin Stein operator (TP g)(x) , 1 p(x) \u3008\u2207, p(x)g(x)\u3009 = \u3008g(x), b(x)\u3009+ \u3008\u2207, g(x)\u3009 for functions g : R \u2192 R was independently developed, without connection to Stein\u2019s method, by Oates et al. (2016b) for the design of Monte Carlo control functionals. Notably, the Langevin Stein operator depends on P only through its score function b = \u2207 log p and hence is computable even when the normalizing constant of p is not. While our work is compatible with other practical Stein operators, like the family of diffusion Stein operators defined in (Gorham et al., 2016), we will focus on the Langevin operator for the sake of brevity. Hereafter, we will let k : R\u00d7R \u2192 R be the reproducing kernel of a reproducing kernel Hilbert space (RKHS) Kk of functions from R \u2192 R. That is, Kk is a Hilbert space of functions such that, for all x \u2208 R, k(x, \u00b7) \u2208 Kk and f(x) = \u3008f, k(x, \u00b7)\u3009Kk whenever f \u2208 Kk. We let \u2016\u00b7\u2016Kk be the norm induced from the inner product on Kk. With this definition, we define our kernel Stein set Gk,\u2016\u00b7\u2016 as the set of vector-valued functions g = (g1, . . . , gd) such that each component function gj belongs toKk and the vector of their norms \u2016gj\u2016Kk belongs to the \u2016\u00b7\u2016 \u2217 unit ball:2 Gk,\u2016\u00b7\u2016 , {g = (g1, . . . , gd) | \u2016v\u2016 \u2217 \u2264 1 for vj , \u2016gj\u2016Kk}. The following result, proved in Section B, establishes that this is an acceptable domain for TP . Proposition 1 (Zero mean test functions). If k \u2208 C b and EP [\u2016\u2207 log p(Z)\u20162] <\u221e, then EP [(TP g)(Z)] = 0 for all g \u2208 Gk,\u2016\u00b7\u2016. Our analyses and algorithms support each gj belonging to a different RKHS Kkj , but we will not need that flexibility here. The Langevin Stein operator and kernel Stein set together define our quality measure of interest, the kernel Stein discrepancy (KSD) S(\u03bc, TP ,Gk,\u2016\u00b7\u2016). When \u2016\u00b7\u2016 = \u2016\u00b7\u20162, this definition recovers the KSD proposed by Chwialkowski et al. (2016) and Liu et al.", "startOffset": 36, "endOffset": 1996}, {"referenceID": 3, "context": "Inspired by the generator method of Barbour (1988; 1990) and G\u00f6tze (1991), Gorham & Mackey (2015) generalized this operator to multiple dimensions. The resulting Langevin Stein operator (TP g)(x) , 1 p(x) \u3008\u2207, p(x)g(x)\u3009 = \u3008g(x), b(x)\u3009+ \u3008\u2207, g(x)\u3009 for functions g : R \u2192 R was independently developed, without connection to Stein\u2019s method, by Oates et al. (2016b) for the design of Monte Carlo control functionals. Notably, the Langevin Stein operator depends on P only through its score function b = \u2207 log p and hence is computable even when the normalizing constant of p is not. While our work is compatible with other practical Stein operators, like the family of diffusion Stein operators defined in (Gorham et al., 2016), we will focus on the Langevin operator for the sake of brevity. Hereafter, we will let k : R\u00d7R \u2192 R be the reproducing kernel of a reproducing kernel Hilbert space (RKHS) Kk of functions from R \u2192 R. That is, Kk is a Hilbert space of functions such that, for all x \u2208 R, k(x, \u00b7) \u2208 Kk and f(x) = \u3008f, k(x, \u00b7)\u3009Kk whenever f \u2208 Kk. We let \u2016\u00b7\u2016Kk be the norm induced from the inner product on Kk. With this definition, we define our kernel Stein set Gk,\u2016\u00b7\u2016 as the set of vector-valued functions g = (g1, . . . , gd) such that each component function gj belongs toKk and the vector of their norms \u2016gj\u2016Kk belongs to the \u2016\u00b7\u2016 \u2217 unit ball:2 Gk,\u2016\u00b7\u2016 , {g = (g1, . . . , gd) | \u2016v\u2016 \u2217 \u2264 1 for vj , \u2016gj\u2016Kk}. The following result, proved in Section B, establishes that this is an acceptable domain for TP . Proposition 1 (Zero mean test functions). If k \u2208 C b and EP [\u2016\u2207 log p(Z)\u20162] <\u221e, then EP [(TP g)(Z)] = 0 for all g \u2208 Gk,\u2016\u00b7\u2016. Our analyses and algorithms support each gj belonging to a different RKHS Kkj , but we will not need that flexibility here. The Langevin Stein operator and kernel Stein set together define our quality measure of interest, the kernel Stein discrepancy (KSD) S(\u03bc, TP ,Gk,\u2016\u00b7\u2016). When \u2016\u00b7\u2016 = \u2016\u00b7\u20162, this definition recovers the KSD proposed by Chwialkowski et al. (2016) and Liu et al. (2016). Our next result shows that, for any \u2016\u00b7\u2016, the KSD admits a closed-form solution.", "startOffset": 36, "endOffset": 2018}, {"referenceID": 20, "context": "Each term wj in Proposition 2 can also be viewed as an instance of the maximum mean discrepancy (MMD) (Gretton et al., 2012) between \u03bc and P measured with respect to the Stein kernel k 0.", "startOffset": 102, "endOffset": 124}, {"referenceID": 29, "context": "Our Stein set choice was motivated by the work of Oates et al. (2016b) who used the sum of Stein kernels k0 = \u2211d j=1 k j 0 to develop nonparametric control variates.", "startOffset": 50, "endOffset": 71}, {"referenceID": 18, "context": "Recently, Gorham et al. (2016) showed that the Langevin graph Stein discrepancy dominates convergence in distribution whenever P belongs to the class P of distantly dissipative distributions with Lipschitz score function b:", "startOffset": 10, "endOffset": 31}, {"referenceID": 13, "context": "Definition 4 (Distant dissipativity (Eberle, 2015; Gorham et al., 2016)).", "startOffset": 36, "endOffset": 71}, {"referenceID": 18, "context": "Definition 4 (Distant dissipativity (Eberle, 2015; Gorham et al., 2016)).", "startOffset": 36, "endOffset": 71}, {"referenceID": 6, "context": "Code reproducing all experiments can be found on the Julia (Bezanson et al., 2014) package site https://jgorham.", "startOffset": 59, "endOffset": 82}, {"referenceID": 39, "context": "Comparing discrepancies Our first, simple experiment is designed to illustrate several properties of the IMQ KSD and to compare its behavior with that of two preexisting discrepancy measures, the Wasserstein distance dW\u2016\u00b7\u20162 , which can be computed for simple univariate targets (Vallender, 1974), and the spanner graph Stein discrepancy of Gorham & Mackey (2015).", "startOffset": 278, "endOffset": 295}, {"referenceID": 36, "context": "Comparing discrepancies Our first, simple experiment is designed to illustrate several properties of the IMQ KSD and to compare its behavior with that of two preexisting discrepancy measures, the Wasserstein distance dW\u2016\u00b7\u20162 , which can be computed for simple univariate targets (Vallender, 1974), and the spanner graph Stein discrepancy of Gorham & Mackey (2015). We adopt a bimodal Gaussian mixture with p(x) \u221d e\u2212 1 2\u2016x+\u2206e1\u2016 2 2 + e\u2212 1 2\u2016x\u2212\u2206e1\u2016 2 2 and \u2206 = 1.", "startOffset": 319, "endOffset": 363}, {"referenceID": 36, "context": "Comparing discrepancies Our first, simple experiment is designed to illustrate several properties of the IMQ KSD and to compare its behavior with that of two preexisting discrepancy measures, the Wasserstein distance dW\u2016\u00b7\u20162 , which can be computed for simple univariate targets (Vallender, 1974), and the spanner graph Stein discrepancy of Gorham & Mackey (2015). We adopt a bimodal Gaussian mixture with p(x) \u221d e\u2212 1 2\u2016x+\u2206e1\u2016 2 2 + e\u2212 1 2\u2016x\u2212\u2206e1\u2016 2 2 and \u2206 = 1.5 as our target P and generate a first sample point sequence i.i.d. from the target and a second sequence i.i.d. from one component of the mixture, N (\u2212\u2206e1, Id). As seen in the left panel of Figure 1 where d = 1, the IMQ KSD decays at an n\u22120.51 rate when applied to the first n points in the target sample and remains bounded away from zero when applied to the to the single component sample. This desirable behavior is closely mirrored by the Wasserstein distance and the graph Stein discrepancy. The middle panel of Figure 1 records the time consumed by the graph and kernel Stein discrepancies applied to the i.i.d. sample points from P . Each method is given access to d cores when working in d dimensions, and we use the released code of Gorham & Mackey (2015) with the default Gurobi 6.", "startOffset": 319, "endOffset": 1226}, {"referenceID": 12, "context": "Selecting sampler hyperparameters The approximate slice sampler of DuBois et al. (2014) is a biased MCMC procedure designed to accelerate inference when the target density takes the form p(x) \u221d \u03c0(x) \u220fL l=1 \u03c0(yl|x) for \u03c0(\u00b7) a prior distribution on R and \u03c0(yl|x) the likelihood of a datapoint yl.", "startOffset": 67, "endOffset": 88}, {"referenceID": 7, "context": "budget of 148000 likelihood evaluations, and plotted the median IMQ KSD and effective sample size (ESS, a standard sample quality measure based on asymptotic variance (Brooks et al., 2011)) in Figure 3.", "startOffset": 167, "endOffset": 188}, {"referenceID": 0, "context": "Selecting samplers Ahn et al. (2012) developed two biased MCMC samplers for accelerated posterior inference, both called Stochastic Gradient Fisher Scoring (SGFS).", "startOffset": 19, "endOffset": 37}, {"referenceID": 0, "context": "Selecting samplers Ahn et al. (2012) developed two biased MCMC samplers for accelerated posterior inference, both called Stochastic Gradient Fisher Scoring (SGFS). In the full version of SGFS (termed SGFS-f), a d\u00d7 d matrix must be inverted to draw each new sample point. Since this can be costly for large d, the authors developed a second sampler (termed SGFS-d) in which only a diagonal matrix must be inverted to draw each new sample point. Both samplers can be viewed as discrete-time approximations to a continuoustime Markov process that has the target P as its stationary distribution; however, because no Metropolis-Hastings correction is employed, neither sampler has the target as its stationary distribution. Hence we will use the KSD \u2013 a quality measure that accounts for asymptotic bias \u2013 to evaluate and choose between these samplers. Specifically, we evaluate the SGFS-f and SGFS-d samples produced in (Ahn et al., 2012, Sec. 5.1). The target P is a Bayesian logistic regression with a flat prior, conditioned on a dataset of 10 MNIST handwritten digit images. From each image, the authors extracted 50 random projections of the raw pixel values as covariates and a label indicating whether the image was a 7 or a 9. After discarding the first half of sample points as burn-in, we obtained regression coefficient samples with 5 \u00d7 10 points and d = 51 dimensions (including the intercept term). Figure 4 displays the IMQ KSD applied to the first n points in each sample. As external validation, we follow the protocol of Ahn et al. (2012) to find the bivariate marginal means and 95% confidence ellipses of each sample that align best and worst with those of a surrogate ground truth sample obtained from a", "startOffset": 19, "endOffset": 1553}, {"referenceID": 44, "context": "IMPROVING SAMPLE QUALITY Liu & Lee (2016) recently used the KSD S(Qn, TP ,Gk) as a means of improving the quality of a sample. Specifically, given an initial sample Qn supported on x1, . . . , xn, they minimize S(Q\u0303n, TP ,Gk) over all measures Q\u0303n supported on the same sample points to obtain a new sample that better approximates P over the class of test functions H = TPGk. In all experiments, Liu & Lee (2016) employ a Gaussian kernel k(x, y) = e\u2212 1 h\u2016x\u2212y\u2016 2 2 with bandwidth h selected to be the median of the squared Euclidean distance between pairs of sample points.", "startOffset": 37, "endOffset": 414}, {"referenceID": 14, "context": "For example, when P = N (0, 1), the score statistic (Fan et al., 2006) only monitors sample means and variances.", "startOffset": 52, "endOffset": 70}, {"referenceID": 12, "context": "The score statistic of Fan et al. (2006) and the Gibbs sampler convergence criteria of Zellner & Min (1995) detect certain forms of non-convergence but fail to detect others due to the finite number of test functions tested.", "startOffset": 23, "endOffset": 41}, {"referenceID": 12, "context": "The score statistic of Fan et al. (2006) and the Gibbs sampler convergence criteria of Zellner & Min (1995) detect certain forms of non-convergence but fail to detect others due to the finite number of test functions tested.", "startOffset": 23, "endOffset": 108}, {"referenceID": 8, "context": "3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al.", "startOffset": 41, "endOffset": 1088}, {"referenceID": 8, "context": "3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al.", "startOffset": 41, "endOffset": 1113}, {"referenceID": 8, "context": "3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al.", "startOffset": 41, "endOffset": 1172}, {"referenceID": 8, "context": "3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al.", "startOffset": 41, "endOffset": 1191}, {"referenceID": 8, "context": "3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al. (2016), and the Stein generative adversarial network approach of Wang & Liu (2016).", "startOffset": 41, "endOffset": 1216}, {"referenceID": 8, "context": "3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al. (2016), and the Stein generative adversarial network approach of Wang & Liu (2016). In the future, we aim to leverage stochastic, low-rank, and sparse approximations of the kernel matrix and score function to produce KSDs that scale better with the number of sample and data points while still guaranteeing control over weak convergence.", "startOffset": 41, "endOffset": 1292}, {"referenceID": 8, "context": "3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al. (2016), and the Stein generative adversarial network approach of Wang & Liu (2016). In the future, we aim to leverage stochastic, low-rank, and sparse approximations of the kernel matrix and score function to produce KSDs that scale better with the number of sample and data points while still guaranteeing control over weak convergence. A reader may also wonder for which distributions outside ofP the KSD dominates weak convergence. The following theorem, proved in Section J, shows that no KSD with aC0 kernel dominates weak convergence when the target has a bounded score function. Theorem 10 (KSD fails for bounded scores). If \u2207 log p is bounded and k \u2208 C 0 , then S(Qn, TP ,Gk) \u2192 0 does not imply Qn \u21d2 P . However, Gorham et al. (2016) developed convergencedetermining graph Stein discrepancies for heavy-tailed targets by replacing the Langevin Stein operator TP with diffusion Stein operators of the form (T g)(x) = 1 p(x) \u3008\u2207, p(x)(a(x) + c(x))g(x)\u3009.", "startOffset": 41, "endOffset": 1951}], "year": 2017, "abstractText": "Approximate Markov chain Monte Carlo (MCMC) offers the promise of more rapid sampling at the cost of more biased inference. Since standard MCMC diagnostics fail to detect these biases, researchers have developed computable Stein discrepancy measures that provably determine the convergence of a sample to its target distribution. This approach was recently combined with the theory of reproducing kernels to define a closed-form kernel Stein discrepancy (KSD) computable by summing kernel evaluations across pairs of sample points. We develop a theory of weak convergence for KSDs based on Stein\u2019s method, demonstrate that commonly used KSDs fail to detect non-convergence even for Gaussian targets, and show that kernels with slowly decaying tails provably determine convergence for a large class of target distributions. The resulting convergence-determining KSDs are suitable for comparing biased, exact, and deterministic sample sequences and simpler to compute and parallelize than alternative Stein discrepancies. We use our tools to compare biased samplers, select sampler hyperparameters, and improve upon existing KSD approaches to one-sample hypothesis testing and sample quality improvement.", "creator": "LaTeX with hyperref package"}}}