{"id": "1703.01720", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Sound-Word2Vec: Learning Word Representations Grounded in Sounds", "abstract": "Sound and vision are the primary modalities that influence how we perceive the world around us. Thus, it is crucial to incorporate information from these modalities into language to help machines interact better with humans. While existing works have explored incorporating visual cues into language embeddings, the task of learning word representations that respect auditory grounding remains under-explored. In this work, we propose a new embedding scheme, sound-word2vec that learns language embeddings by grounding them in sound -- for example, two seemingly unrelated concepts, leaves and paper are closer in our embedding space as they produce similar rustling sounds. We demonstrate that the proposed embeddings perform better than language-only word representations, on two purely textual tasks that require reasoning about aural cues -- sound retrieval and foley-sound discovery. Finally, we analyze nearest neighbors to highlight the unique dependencies captured by sound-w2v as compared to language-only embeddings.", "histories": [["v1", "Mon, 6 Mar 2017 04:30:12 GMT  (43kb)", "https://arxiv.org/abs/1703.01720v1", "5 pages"], ["v2", "Fri, 28 Apr 2017 06:35:16 GMT  (59kb)", "http://arxiv.org/abs/1703.01720v2", "5 pages; 3 tables"], ["v3", "Thu, 10 Aug 2017 04:26:57 GMT  (227kb,D)", "http://arxiv.org/abs/1703.01720v3", "Accepted at EMNLP 2017. Contains 6 pages; 3 tables; 1 figure"], ["v4", "Tue, 29 Aug 2017 15:54:31 GMT  (227kb,D)", "http://arxiv.org/abs/1703.01720v4", "Accepted at EMNLP 2017. Contains 6 pages; 3 tables; 1 figure"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.SD", "authors": ["ashwin k vijayakumar", "ramakrishna vedantam", "devi parikh"], "accepted": true, "id": "1703.01720"}, "pdf": {"name": "1703.01720.pdf", "metadata": {"source": "CRF", "title": "Sound-Word2Vec: Learning Word Representations Grounded in Sounds", "authors": ["Ashwin Vijayakumar", "Ramakrishna Vedantam", "Devi Parikh", "Georgia Tech", "Virgina Tech"], "emails": ["ashwinkv@gatech.edu,", "parikh@gatech.edu,", "vrama1@vt.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, language has evolved to include numerous constructions that help us to represent visual concepts. Thus, for example, we can easily form the image of a white, fearsome cat with blue eyes, while a description of the cat in terms of its visual attributes (Lampert et al., 2009; Parikh and Graumann, 2011) describes the auditory restoration of cats. While a first thought might be to use audio descriptors such as sounds, shrills, etc., the constructions are constructed on the middle level or \"conceptual,\" it is difficult to convey the sound accurately and to describe the sound of cats."}, {"heading": "2 Related Work", "text": "In recent years, the number of people who are able to stay in the city has decreased drastically; in recent years, the number of people who are in the city has multiplied; in recent years, the number of people who are in the city has multiplied; in recent years, the number of people who have settled in the city has multiplied; in recent years, the number of people who have settled in the city has multiplied; in recent years, the number of people who have settled in the city has multiplied; and in recent years, the number of people who have settled in the city has multiplied."}, {"heading": "3 Datasets", "text": "Freesound. We use the Freesound database (Font et al., 2013), which has also been used in previous work (Kiela and Clark, 2015; Lopopolo and van Miltenburg, 2015) to learn the proposed sound word2vec embeddings. Freesound is a freely available collaborative dataset consisting of user uploaded sounds that allow reuse. All uploaded sounds have human descriptions in the form of tags and captions in natural language. Freesound tags contain a large number of relevant topics for a sound (e.g. ambience, electronics, birds, city, reverb) and captions that describe the content of the sound, in addition to details regarding audio quality. For the text-based sound retrieval task, we use a subset of 234,120 sounds from this database and divide them into training (80%), validation (10%) and testing of records (10%)."}, {"heading": "4 Approach", "text": "It is a question of whether and in what form people will be able to put themselves in the world, and whether they will be able to put themselves in the world, and whether they will be able to put themselves in the world, and whether they will be able to put themselves in the world, and whether they will be able to put themselves in the world, or whether they will be able to put themselves in the world, and whether they will be able to change the world."}, {"heading": "5 Results", "text": "Ablations. In addition to the linguistic baseline word2vec (Mikolov et al., 2013), we compare with tag-word2vec - which predicts a day that uses other tags of sound as context, inspired by (Font et al., 2014). We also report on results using a randomly initialized projection matrix (soundword2vec (r) to evaluate the effectiveness of pre-training with word2vec. Preparatory work. We compare with previous work Lopopolo and van Miltenburg (2015), as well as Kiela and Clark (2015). While the former uses a standard bag of words and an SVD pipeline to arrive at a result, we have also tried to rely directly on sound functions rather than clusters, but found that it performs poorly.Distribution representations for words, the latter trains under a common goal that respects both linguistic and auditory similarity. We use the open implementation for Lopopolo Kiela and Kieltenburg (2015) as a comparison (and a)."}, {"heading": "5.1 Text-based Sound Retrieval", "text": "Given a textual description of a sound as a query, we compare it to tags that are associated with sounds in the database to retrieve the sound with the nearest matching tags. Note that this is a purely textual task, albeit one that requires an awareness of sound. In a way, this task captures exactly what we want from our model - bridging the semantic gap between language and sound. We use the training split (Sec. 3) to learn the sound word2vec vectors, validation to select the number of clusters (K), and report the results of the test split. For the query, we represent sounds by calculating the learned embeddings for the associated tags. We similarly embed the captions provided for the sound (in the Freesound database) and use them as a query. We then evaluate sounds based on the cosmic similarity between the tag and the query."}, {"heading": "5.2 Foley Sound Discovery", "text": "In this task, we evaluate how well embeddings identify matching pairs of target sounds (fluttering bird wings) and descriptions of Foley's sound production techniques (rubbing a pair of gloves). Intuitively, tone-conscious word embeddings are expected to perform better in this task than tone-agnostic ones. We create a ranking of original Foley sound pairs and bait pairs, which are formed by combining the target description with each word from the vocabulary. We evaluate embeddings based on cosmic similarity between the average word vectors in each member of the pair. A good embeddings is one in which the original Foley sound pair has the lowest rank. We use the middle rank of the Foley sound in the dataset. We transfer the embeddings of Sec. 5.1 to this task, with no additional traction. Results. We find that sound word2vec with an average rank of 4.6 seconds performs best in comparison to other baselines."}, {"heading": "5.3 Evaluation on AMEN and ASLex", "text": "AMEN and ASLex (Kiela and Clark, 2015) are subsets of the MEN and SimLex 999 datasets for word references based on sound. Table 2 shows that our embeddings perform better in both AMEN and ASLex (Kiela and Clark, 2015). These datasets have been curated by annotation concepts related to sound, but we observe that references are often confused. For example, (river, water), (car, car) are labeled as aurallyrelated terms, but do not stand out as aurallyrelated examples because they are already semantically related to each other. In contrast, we are interested in how onomatopoeic words relate to regular words (Table 3), which we examine through explicit grounding in sound."}, {"heading": "6 Discussion and Conclusion", "text": "While word2vec assigns a word (say, apple) to other semantically similar words (other fruits), similar \"sounding\" words (chips) or onomatopoeia (Munch) in our embedding space are closer. In addition, onomatopoeic words (say, boom and slam) are associated with relevant objects (explosion and door). Interestingly, parts (e.g. lock, latch) and actions (close) are also closer to the onomatopoeic weirdness - they show an understanding of the auditory scene. Conclusion. In this work, we introduce a novel word embedding scheme that respects auditory grounding. We show that our embedding provides a strong performance on text-based sound paintings, Foley sound discovery along with intuitive nearest words."}], "references": [{"title": "Shape-based spectral contrast descriptor", "author": ["Vincent Akkermans", "Joan Serr\u00e0", "Perfecto Herrera."], "venue": "Proc. of the Sound and Music Computing Conf.(SMC).", "citeRegEx": "Akkermans et al\\.,? 2009", "shortCiteRegEx": "Akkermans et al\\.", "year": 2009}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam-Khanh Tran", "Marco Baroni."], "venue": "Journal of Artificial Intelligence Research (JAIR).", "citeRegEx": "Bruni et al\\.,? 2014", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Audio word2vec: Unsupervised learning of audio segment representations using sequence-to-sequence autoencoder", "author": ["Yu-An Chung", "Chao-Chung Wu", "Chia-Hao Shen", "Hung-yi Lee", "Lin-Shan Lee."], "venue": "CoRR.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Extending tagging ontologies with domain specific knowledge", "author": ["Frederic Font", "Sergio Oramas", "Gy\u00f6rgy Fazekas", "Xavier Serra."], "venue": "Proceedings of the International Semantic Web Conference.", "citeRegEx": "Font et al\\.,? 2014", "shortCiteRegEx": "Font et al\\.", "year": 2014}, {"title": "Freesound technical demo", "author": ["Frederic Font", "Gerard Roma", "Xavier Serra."], "venue": "Proceedings of the 21st ACM International Conference on Multimedia.", "citeRegEx": "Font et al\\.,? 2013", "shortCiteRegEx": "Font et al\\.", "year": 2013}, {"title": "Comparative evaluation of various mfcc implementations on the speaker verification task", "author": ["Todor Ganchev", "Nikos Fakotakis", "George Kokkinakis."], "venue": "Proceedings of the SPECOM.", "citeRegEx": "Ganchev et al\\.,? 2005", "shortCiteRegEx": "Ganchev et al\\.", "year": 2005}, {"title": "Multi-view recurrent neural acoustic word embeddings", "author": ["Wanjia He", "Weiran Wang", "Karen Livescu."], "venue": "arXiv preprint arXiv:1611.04496.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Multi-and cross-modal semantics beyond vision: Grounding in auditory perception", "author": ["Douwe Kiela", "Stephen Clark."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Kiela and Clark.,? 2015", "shortCiteRegEx": "Kiela and Clark.", "year": 2015}, {"title": "Visual word2vec (vis-w2v): Learning visually grounded word embeddings using abstract scenes", "author": ["Satwik Kottur", "Ramakrishna Vedantam", "Jose M.F. Moura", "Devi Parikh."], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recog-", "citeRegEx": "Kottur et al\\.,? 2016", "shortCiteRegEx": "Kottur et al\\.", "year": 2016}, {"title": "Learning to detect unseen object classes by betweenclass attribute transfer", "author": ["Christoph H. Lampert", "Hannes Nickisch", "Stefan Harmeling."], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Lampert et al\\.,? 2009", "shortCiteRegEx": "Lampert et al\\.", "year": 2009}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["Angeliki Lazaridou", "Nghia The Pham", "Marco Baroni."], "venue": "arXiv preprint arXiv:1501.02598.", "citeRegEx": "Lazaridou et al\\.,? 2015", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2015}, {"title": "Sound-based distributional models", "author": ["Alessandro Lopopolo", "Emiel van Miltenburg."], "venue": "IWCS 2015.", "citeRegEx": "Lopopolo and Miltenburg.,? 2015", "shortCiteRegEx": "Lopopolo and Miltenburg.", "year": 2015}, {"title": "The role of context types and dimensionality in learning word embeddings", "author": ["Oren Melamud", "David McClosky", "Siddharth Patwardhan", "Mohit Bansal."], "venue": "arXiv preprint arXiv:1601.00893.", "citeRegEx": "Melamud et al\\.,? 2016", "shortCiteRegEx": "Melamud et al\\.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Ambient Sound Provides Supervision for Visual Learning", "author": ["A. Owens", "J. Wu", "J.H. McDermott", "W.T. Freeman", "A. Torralba."], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Owens et al\\.,? 2016", "shortCiteRegEx": "Owens et al\\.", "year": 2016}, {"title": "Relative Attributes", "author": ["Devi Parikh", "Kristen Grauman."], "venue": "Proceedings of IEEE International Conference on Computer Vision (ICCV).", "citeRegEx": "Parikh and Grauman.,? 2011", "shortCiteRegEx": "Parikh and Grauman.", "year": 2011}, {"title": "Tonal consonance and critical bandwidth", "author": ["Reinier Plomp", "Willem Johannes Maria Levelt."], "venue": "The journal of the Acoustical Society of America.", "citeRegEx": "Plomp and Levelt.,? 1965", "shortCiteRegEx": "Plomp and Levelt.", "year": 1965}, {"title": "Towards computational morphological description of sound", "author": ["Julien Ricard."], "venue": "DEA pre-thesis research work, Universitat Pompeu Fabra, Barcelona.", "citeRegEx": "Ricard.,? 2004", "shortCiteRegEx": "Ricard.", "year": 2004}, {"title": "Discriminative acoustic word embeddings: Recurrent neural network-based approaches", "author": ["Shane Settle", "Karen Livescu."], "venue": "arXiv preprint arXiv:1611.02550.", "citeRegEx": "Settle and Livescu.,? 2016", "shortCiteRegEx": "Settle and Livescu.", "year": 2016}, {"title": "Art of foley", "author": ["Philip R. Singer"], "venue": null, "citeRegEx": "Singer.,? \\Q2017\\E", "shortCiteRegEx": "Singer.", "year": 2017}, {"title": "Vector-based representation and clustering of audio using onomatopoeia words", "author": ["Shiva Sundaram", "Shrikanth Narayanan."], "venue": "In Proceedings of AAAI 2006 Fall Symposia.", "citeRegEx": "Sundaram and Narayanan.,? 2006", "shortCiteRegEx": "Sundaram and Narayanan.", "year": 2006}, {"title": "Sound retrieval with intuitive verbal expressions", "author": ["Sanae Wake", "Toshiyuki Asahi."], "venue": "Proceedings of the 5th International Conference on Auditory Display (ICAD).", "citeRegEx": "Wake and Asahi.,? 1998", "shortCiteRegEx": "Wake and Asahi.", "year": 1998}], "referenceMentions": [{"referenceID": 10, "context": "tributes (Lampert et al., 2009; Parikh and Grauman, 2011).", "startOffset": 9, "endOffset": 57}, {"referenceID": 16, "context": "tributes (Lampert et al., 2009; Parikh and Grauman, 2011).", "startOffset": 9, "endOffset": 57}, {"referenceID": 22, "context": "Indeed, Wake and Asahi (1998) find that humans first communicate sounds using \u201conomatopoeia\u201d \u2013 words that are suggestive of the phonetics of sounds while having no explicit meaning e.", "startOffset": 8, "endOffset": 30}, {"referenceID": 21, "context": "Thus, for a large number of concepts there seems to be a gap between sound and its counterpart in language (Sundaram and Narayanan, 2006).", "startOffset": 107, "endOffset": 137}, {"referenceID": 8, "context": "Aurally-relevant word relatedness assessment on AMEN and ASLex (Kiela and Clark, 2015) (Sec.", "startOffset": 63, "endOffset": 86}, {"referenceID": 1, "context": "Multiple works in the recent past (Bruni et al., 2014; Lazaridou et al., 2015; Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015; Kottur et al., 2016) have explored using perceptual modalities like vision and sound to learn language embeddings.", "startOffset": 34, "endOffset": 157}, {"referenceID": 11, "context": "Multiple works in the recent past (Bruni et al., 2014; Lazaridou et al., 2015; Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015; Kottur et al., 2016) have explored using perceptual modalities like vision and sound to learn language embeddings.", "startOffset": 34, "endOffset": 157}, {"referenceID": 8, "context": "Multiple works in the recent past (Bruni et al., 2014; Lazaridou et al., 2015; Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015; Kottur et al., 2016) have explored using perceptual modalities like vision and sound to learn language embeddings.", "startOffset": 34, "endOffset": 157}, {"referenceID": 9, "context": "Multiple works in the recent past (Bruni et al., 2014; Lazaridou et al., 2015; Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015; Kottur et al., 2016) have explored using perceptual modalities like vision and sound to learn language embeddings.", "startOffset": 34, "endOffset": 157}, {"referenceID": 1, "context": "Multiple works in the recent past (Bruni et al., 2014; Lazaridou et al., 2015; Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015; Kottur et al., 2016) have explored using perceptual modalities like vision and sound to learn language embeddings. While Lopopolo and van Miltenburg (2015) show preliminary results on using sound to learn distributional representations, Kiela and Clark (2015) build on ideas from Bruni et al.", "startOffset": 35, "endOffset": 293}, {"referenceID": 1, "context": "Multiple works in the recent past (Bruni et al., 2014; Lazaridou et al., 2015; Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015; Kottur et al., 2016) have explored using perceptual modalities like vision and sound to learn language embeddings. While Lopopolo and van Miltenburg (2015) show preliminary results on using sound to learn distributional representations, Kiela and Clark (2015) build on ideas from Bruni et al.", "startOffset": 35, "endOffset": 397}, {"referenceID": 1, "context": "Multiple works in the recent past (Bruni et al., 2014; Lazaridou et al., 2015; Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015; Kottur et al., 2016) have explored using perceptual modalities like vision and sound to learn language embeddings. While Lopopolo and van Miltenburg (2015) show preliminary results on using sound to learn distributional representations, Kiela and Clark (2015) build on ideas from Bruni et al. (2014) to learn word embeddings that respect both linguistic and auditory relationships by optimizing a joint objective.", "startOffset": 35, "endOffset": 437}, {"referenceID": 13, "context": "Similar to previous findings (Melamud et al., 2016), we observe that our specialized embeddings outperform language-only as well as other multi-modal embeddings in the downstream tasks of interest.", "startOffset": 29, "endOffset": 51}, {"referenceID": 2, "context": "In an orthogonal and interesting direction, other recent works (Chung et al., 2016; He et al., 2016; Settle and Livescu, 2016) learn word representations based on similarity in their pronunciation and not the sounds associated with them.", "startOffset": 63, "endOffset": 126}, {"referenceID": 6, "context": "In an orthogonal and interesting direction, other recent works (Chung et al., 2016; He et al., 2016; Settle and Livescu, 2016) learn word representations based on similarity in their pronunciation and not the sounds associated with them.", "startOffset": 63, "endOffset": 126}, {"referenceID": 19, "context": "In an orthogonal and interesting direction, other recent works (Chung et al., 2016; He et al., 2016; Settle and Livescu, 2016) learn word representations based on similarity in their pronunciation and not the sounds associated with them.", "startOffset": 63, "endOffset": 126}, {"referenceID": 2, "context": "In an orthogonal and interesting direction, other recent works (Chung et al., 2016; He et al., 2016; Settle and Livescu, 2016) learn word representations based on similarity in their pronunciation and not the sounds associated with them. In other words, phonetically similar words that have near identical pronunciations are brought closer in the embedding space (e.g., flower and flour). Sundaram and Narayanan (2006) study the appli-", "startOffset": 64, "endOffset": 419}, {"referenceID": 9, "context": "Kottur et al. (2016) and Owens et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "Kottur et al. (2016) and Owens et al. (2016) use a surrogate modality to induce", "startOffset": 0, "endOffset": 45}, {"referenceID": 4, "context": "We use the freesound database (Font et al., 2013), also used in prior work (Kiela and Clark, 2015; Lopopolo and van Miltenburg, 2015) to learn the proposed sound-word2vec embeddings.", "startOffset": 30, "endOffset": 49}, {"referenceID": 8, "context": ", 2013), also used in prior work (Kiela and Clark, 2015; Lopopolo and van Miltenburg, 2015) to learn the proposed sound-word2vec embeddings.", "startOffset": 33, "endOffset": 91}, {"referenceID": 8, "context": "AMEN and ASLex (Kiela and Clark, 2015) are subsets of the standard", "startOffset": 15, "endOffset": 38}, {"referenceID": 1, "context": "MEN (Bruni et al., 2014) and SimLex (Hill et al.", "startOffset": 4, "endOffset": 24}, {"referenceID": 7, "context": ", 2014) and SimLex (Hill et al., 2015) word similarity datasets consisting of word-pairs that \u201ccan be associated with a distinctive associated sound\u201d.", "startOffset": 19, "endOffset": 38}, {"referenceID": 14, "context": "We then aim to learn an embedding space for the tags that respects auditory grounding using sound information as cross-modal context \u2013 similar to word2vec (Mikolov et al., 2013) that uses neighboring words as context / supervision.", "startOffset": 155, "endOffset": 177}, {"referenceID": 5, "context": "We represent each sound s by a feature vector consisting of the mean and variance of the following audio descriptors that are readily available as part of Freesound database: \u2022 Mel-Frequency Cepstral Co-efficients: This feature represents the short-term power spectrum of an audio and closely approximates the response of the human auditory system \u2013 computed as given in (Ganchev et al., 2005).", "startOffset": 371, "endOffset": 393}, {"referenceID": 0, "context": "in the peaks and valleys of the spectrum \u2013 computed according to (Akkermans et al., 2009).", "startOffset": 65, "endOffset": 89}, {"referenceID": 17, "context": "\u2022 Dissonance: It measures the perceptual roughness of the sound (Plomp and Levelt, 1965).", "startOffset": 64, "endOffset": 88}, {"referenceID": 18, "context": "have higher values (Ricard, 2004).", "startOffset": 19, "endOffset": 33}, {"referenceID": 14, "context": "We initialize WP with word2vec embeddings (Mikolov et al., 2013) trained on the Google news corpus dataset with \u223c3M words.", "startOffset": 42, "endOffset": 64}, {"referenceID": 8, "context": "objective with language and audio context (Kiela and Clark, 2015) is driven by the fact that we are interested in embeddings for words grounded in sounds, and not better generic word similarity.", "startOffset": 42, "endOffset": 65}, {"referenceID": 14, "context": "In addition to the language-only baseline word2vec (Mikolov et al., 2013), we compare against tag-word2vec \u2013 that predicts a tag using other tags of the sound as context, inspired", "startOffset": 51, "endOffset": 73}, {"referenceID": 3, "context": "by (Font et al., 2014).", "startOffset": 3, "endOffset": 22}, {"referenceID": 3, "context": "by (Font et al., 2014). We also report results with a randomly initialized projection matrix (soundword2vec(r) to evaluate the effectiveness of pretraining with word2vec. Prior work. We compare against previous works Lopopolo and van Miltenburg (2015) and Kiela and Clark (2015).", "startOffset": 4, "endOffset": 252}, {"referenceID": 3, "context": "by (Font et al., 2014). We also report results with a randomly initialized projection matrix (soundword2vec(r) to evaluate the effectiveness of pretraining with word2vec. Prior work. We compare against previous works Lopopolo and van Miltenburg (2015) and Kiela and Clark (2015). While the former uses a standard bag of words and SVD pipeline to arrive at", "startOffset": 4, "endOffset": 279}, {"referenceID": 8, "context": "In addition, we show a comparison to word-vectors released by (Kiela and Clark, 2015) in the supplementary material.", "startOffset": 62, "endOffset": 85}, {"referenceID": 8, "context": "We use the openly available implementation for Lopopolo and van Miltenburg (2015) and re-implement Kiela and Clark (2015) and train them on our dataset for a fair comparison of the methods.", "startOffset": 99, "endOffset": 122}, {"referenceID": 8, "context": "We see that specializing the embeddings for sound using our two-stage training outperforms prior work(Kiela and Clark (2015) and Lopopolo and van Miltenburg (2015)), which did not do specialization.", "startOffset": 102, "endOffset": 125}, {"referenceID": 8, "context": "We see that specializing the embeddings for sound using our two-stage training outperforms prior work(Kiela and Clark (2015) and Lopopolo and van Miltenburg (2015)), which did not do specialization.", "startOffset": 102, "endOffset": 164}, {"referenceID": 8, "context": "(Kiela and Clark, 2015) 6.", "startOffset": 0, "endOffset": 23}, {"referenceID": 8, "context": "04 (Kiela and Clark, 2015) 0.", "startOffset": 3, "endOffset": 26}, {"referenceID": 8, "context": "Table 2: Comparison to state of the art AMEN and ASLex datasets (Kiela and Clark, 2015) (higher is better).", "startOffset": 64, "endOffset": 87}, {"referenceID": 8, "context": "Table 2: Comparison to state of the art AMEN and ASLex datasets (Kiela and Clark, 2015) (higher is better). Our approach performs better than Kiela and Clark (2015).", "startOffset": 65, "endOffset": 165}, {"referenceID": 8, "context": "Lopopolo and van Miltenburg (2015) and Kiela and Clark (2015) perform worse than tag-word2vec with a mean rank of 48.", "startOffset": 39, "endOffset": 62}, {"referenceID": 8, "context": "AMEN and ASLex (Kiela and Clark, 2015) are subsets of the MEN and SimLex-999 datasets for word relatedness grounded in sound.", "startOffset": 15, "endOffset": 38}, {"referenceID": 8, "context": "From Table 2, we can see that our embeddings outperform (Kiela and Clark, 2015) on both AMEN and ASLex.", "startOffset": 56, "endOffset": 79}], "year": 2017, "abstractText": "To be able to interact better with humans, it is crucial for machines to understand sound \u2013 a primary modality of human perception. Previous works have used sound to learn embeddings for improved generic semantic similarity assessment. In this work, we treat sound as a first-class citizen, studying downstream 6textual tasks which require aural grounding. To this end, we propose sound-word2vec \u2013 a new embedding scheme that learns specialized word embeddings grounded in sounds. For example, we learn that two seemingly (semantically) unrelated concepts, like leaves and paper are similar due to the similar rustling sounds they make. Our embeddings prove useful in textual tasks requiring aural reasoning like text-based sound retrieval and discovering Foley sound effects (used in movies). Moreover, our embedding space captures interesting dependencies between words and onomatopoeia and outperforms prior work on aurallyrelevant word relatedness datasets such as AMEN and ASLex.", "creator": "LaTeX with hyperref package"}}}