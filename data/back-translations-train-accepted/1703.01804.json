{"id": "1703.01804", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Orthogonalized ALS: A Theoretically Principled Tensor Decomposition Algorithm for Practical Use", "abstract": "The popular Alternating Least Squares (ALS) algorithm for tensor decomposition is extremely efficient, but often converges to poor local optima, particularly when the weights of the factors are non-uniform. We propose a modification of the ALS approach that is as efficient as standard ALS, but provably recovers the true factors with random initialization under standard incoherence assumptions on the factors of the tensor. We demonstrate the significant practical superiority of our approach over traditional ALS (with both random initialization and SVD-based initialization) for a variety of tasks on synthetic data - including tensor factorization on exact, noisy and over-complete tensors, as well as tensor completion - and for computing word embeddings from a third-order word tri-occurrence tensor.", "histories": [["v1", "Mon, 6 Mar 2017 10:31:00 GMT  (1958kb,D)", "http://arxiv.org/abs/1703.01804v1", "47 pages, 6 figures"], ["v2", "Sat, 23 Sep 2017 21:15:50 GMT  (1466kb,D)", "http://arxiv.org/abs/1703.01804v2", "Minor updates to presentation. Appears in ICML'17"]], "COMMENTS": "47 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["vatsal sharan", "gregory valiant"], "accepted": true, "id": "1703.01804"}, "pdf": {"name": "1703.01804.pdf", "metadata": {"source": "CRF", "title": "Orthogonalized ALS: A Theoretically Principled Tensor Decomposition Algorithm for Practical Use", "authors": ["Vatsal Sharan"], "emails": ["vsharan@stanford.edu", "valiant@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of them are able to move into another world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they are able"}, {"heading": "2 Background and Related Work", "text": "We begin the section with a brief discussion of the related work on tensor decomposition. We then review the ALS algorithm and tensor power method and discuss their basic properties. Our proposed tensor decomposition algorithm, Orth-ALS, builds on these algorithms."}, {"heading": "2.1 Related Work on Tensor Decomposition", "text": "Although it is not possible for us to do justice to the essential work on tensor decomposition, we will review three families of algorithms that differ from alternate minimization approaches such as ALS and the tensor power method. Many algorithms have been proposed for the guaranteed decomposition of orthogonal tensors, we point the reader to algorithms that use orthogonal tensors, and require the tensor to be converted into an orthogonal form (2009); Zhang and Golub (2001)."}, {"heading": "2.2 Alternating Least Squares (ALS)", "text": "ALS is the most widely used algorithm for the decomposition of tensors and has been described as a \"workhorse\" for the decomposition of tensors (Kolda and Bader, 2009).The algorithm is conceptually very simple: fixing two of the modes, the optimization problem of finding the value of the third mode, which minimizes the quadratic error of the resulting tensor, can be expressed as a linear regression problem (and can therefore be solved efficiently).As the name suggests, ALS iteratively fixes two of the three modes and solves the quadratic problem on the remaining mode. these updates continue until some stop conditions are met - typically when the quadratic error of the approximation no longer decreases, or when a fixed number of iterations has elapsed. The factors used in ALS are either randomly selected, or via an expensive initialization scheme such as SVD."}, {"heading": "2.3 Tensor Power Method", "text": "The method is then repeated several times to recover different factors, and the factors recovered in different iterations of the algorithm are then clustered to determine the set of unique factors. Different initialization strategies for the tensor power method were proposed. Anandkumar et al. (2014c) showed that the tensor power method converges locally (i.e. for an appropriately selected initialization) for random tensors ranked o (d1.5), and they also showed that an SVD-based initialization strategy provides good starting points and used this to prove global convergence for random tensors ranked O (d). However, the SVD-based initialization strategy can be costly from a computational point of view, and our experiments suggest that even SVD initialization in the setting in which the weights decay according to a power law is c. (see Figure 2)."}, {"heading": "3 Notation", "text": "In this case, it is as if it were an infinite time in which we would sow ourselves to be able to bridge the infinite time in which we would be able to be able to be able to be able to be able to be able to be able to be able to put ourselves in a position to be able to put ourselves in a position to be able to put ourselves in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be able to be in a position to be in a position to be able to be in a position to be in a position to be in a position to be able to be able to be able to be able to be able to be able to be able to be able to be able to be in a position to be able to be able to be able to be able to be able to be able to be able to be in a position to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be in a situation to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to"}, {"heading": "4 The Algorithm: Orthogonalized Alternating Least Squares", "text": "This year it is more than ever before."}, {"heading": "4.1 Performance Guarantees", "text": "The specific variant of orthogonalized ALS to which our theorems apply is a slight modification of algorithm 1 and differs in that there is periodic (each log k-step) re-randomization of factors for which our analysis does not yet have a guaranteed convergence. In our practical implementations, we observe that all factors converge within this first log k-step, and therefore subsequent re-randomization is not necessarily. Theorem 1. Consider a d-dimensional rank k-tensor T = \u2211 ki = 1wiAi Ai. Let cmax = maxi 6 = j | ATi Aj | be the incoherence between the true factors and g = wmaxwmin be the ratio between the largest and the smallest rank k-tensor."}, {"heading": "4.2 New Guarantees for the Tensor Power Method", "text": "As a consequence of our analysis of the orthogonalized ALS algorithm, we also demonstrate new guarantees for the tensor power method. As these may be of independent interest due to the broad application of the tensor power method, we summarize them in this subsection. We show a square convergence rate (in O (log d) steps) with random initialization for random tensors ranked k = o (d), in contrast to the analysis by Anandkumar et al. (2014c), which shows a linear convergence rate (log d) steps for random tensors, provided that an SVD-based initialization is present. Theorem 2. Consider a d-dimensional ranking k tensor T = 1wiAi Ai Ai with the factors Ai sampled uniformally the d-dimensional sphere."}, {"heading": "5 Experiments", "text": "We compare the performance of Orth-ALS, standard ALS (with random and SVD initialization), the tensor power method, and the classic self-decomposition approach through low-rank tensor regeneration experiments in several different parameter regimes, based on a tensor decomposition task and a tensor completion task. We also compare the factorization of Orth-ALS and standard ALS based on a large real tensor of word-tri occurrences based on the 1.5 billion word Wikipedia corpus.3"}, {"heading": "5.1 Experiments on Random Tensors", "text": "In fact, it is the case that most of us are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live."}, {"heading": "5.2 Learning Word Embeddings via Tensor Factorization", "text": "Current methods of learning word embeddings implicitly (Mikolov et al., 2013b; Levy and Goldberg, 2014) or explicitly (Pennington et al., 2014) factorize some matrices derived from the matrix of word offsets M, with Mij indicating how often word i occurs with word j. We examine tensor methods for learning word embeddings and compare the performance of standard ALS and orthogonized ALS with tasks that test the quality of embeddings."}, {"heading": "5.2.1 Methodology", "text": "We used English Wikipedia as our corpus, with 1.5 billion words. We constructed a word co-occurence tensor T of the 10,000 most common words, with the entry Tijk indicating the number of times the words i, j and k appear in a sliding length window w above the corpus. We consider two different window lengths, w = 3 and w = 5. Before including the tensor, we apply the nonlinear elemental scaling f (x) = log (1 + x) to the tensor of the tri-occurrences. This scaling is known to work well in practice for co-occurrence matrices (Pennington et al., 2014), and makes some intuitive meaning in the light of the Zipfian distribution of word frequencies. After applying these elementary nonlinear nonlinear nonlinearity, we recover a ranking of 100 approximation of the tensor matrices (Pennington al, 2014)."}, {"heading": "5.3 Evaluation: Similarity and Analogy Tasks", "text": "Word similarity tasks (Bruni et al., 2012; Finkelstein et al., 2001) contain pairs of words along with humanly assigned similarity values, and the goal is to maximize the correlation between the similarity in the embedding of the two words (according to a similarity metric such as the Dot product) and humanly assessed similarity. Word analogy tasks (Mikolov et al., 2013a; c) ask questions of the form \"a is to a as b is to?\" (e.g. \"Paris is to France like Rome?\") and human similarity. We find the answer to \"a is to a is to a is to b\" by finding the word whose embedding was most likely to be \"b \u2212 wa \u2212 wa.\""}, {"heading": "6 Proof Overview: the Orthogonal Tensor Case", "text": "In this section, we will address the question of what we will do to highlight the high-level analysis methods we apply to the more general settings. First, the analysis of the tensor-power method will go through some preliminaries for our analysis of the tensor-power method. Let's let the tensor-power method be applied on a temporary basis. The tensor-power method is described as updated equations (let's refer to Anandkumar et al. (2014c)) Zt = 1wi < Zt \u2212 1, Ai > 2Ai."}, {"heading": "7 Conclusion", "text": "Our results point to the theoretical and practical benefits of orthogonalized ALS compared to standard ALS. An interesting direction for future work would be a more in-depth study of the practical and theoretical benefits of orthogonalization for other sorter-related tasks, such as tensor completion. In addition, it seems worthwhile to investigate orthogonalized ALS or hybrid ALS in more application-specific areas such as natural language processing."}, {"heading": "A Global convergence of the tensor power method for incoherent", "text": "This is a necessary step before analyzing the tensor-power method, because we now need to analyze the results of the tensor-power method, which is independent of the Orth-ALS method, as they prove themselves under random initialization. The proof is similar to the proof of the tensor-power method in the orthogonal method, but we now need to analyze the cross-power method, because the factors are no longer orthogonal. Theorem 3. We are considering a d-dimensional ranking of tensor-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-to-ratio-ratio-ratio-ratio-ratio-ratio-to-to-ratio-to-to-ratio-to-ratio-to-to-ratio-ratio-ratio-ratio-to-to-ratio-to-ratio-ratio-to-ratio-to-ratio-ratio-to-ratio-to-ratio-ratio-to-to-to-ratio-ratio-to-ratio-to-ratio-to-ratio-to-ratio-ratio-to-to-ratio-to-ratio-to-ratio-to-to-to-to-ratio-to-ratio-to-to-to-ratio-ratio-to-to-to-to"}, {"heading": "B Global convergence of the tensor power method for random", "text": "The previous section gives guarantees for the tensor power method for incoherent tensors. Applying Theorem 3 to a tensor whose factors are uniformly randomly selected, we can say that the tensor power method converges with random initialization when the precedence k = o (d0,25). Theorem 3 also proves a linear convergence rate. However, this is quite suboptimal for random tensors. In this section, we use the randomness in the tensor to obtain much stronger convergence, which is used in this section, are very different from the rest of the paper. Instead of recursively analyzing the tensor power method by showing that the algorithms are making progress at each step by increasing their correlation with some fixed factors, we express the correlation of the factors with the estimate directly."}, {"heading": "C Proof of convergence for Orth-ALS", "text": "The proof of the convergence of the orth-ALS for the first i-factors is then updated with high probability. (...) We assume that the proof of the convergence of the orth-ALS for the first i-factors (...) for the orthogonal tensors in Section 6.Theorem (...) We will try to provide the proof of the orthogonal case as far as possible, while we will also provide evidence for the intermediate lemmas specified without proof in Section 6.Theorem. (...) We will consider the d-dimensional ratio of the largest and smallest ratio factors in relation to the largest and smallest ratio. (...) The estimates of the factors are initialized by the unit Sphere. (...) Provided that, at the i (...) th (... log) th step of the algorithms the estimates for all, but the first i-factors are re-randomized (...)."}, {"heading": "D Proof of additional Lemmas", "text": "max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max max"}], "references": [{"title": "A spectral algorithm for latent dirichlet allocation", "author": ["Animashree Anandkumar", "Yi-kai Liu", "Daniel J Hsu", "Dean P Foster", "Sham M Kakade"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "A tensor approach to learning mixed membership community models", "author": ["Animashree Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M Kakade"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "Tensor decompositions for learning latent variable models", "author": ["Animashree Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M Kakade", "Matus Telgarsky"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "Guaranteed non-orthogonal tensor decomposition via alternating rank-1 updates", "author": ["Animashree Anandkumar", "Rong Ge", "Majid Janzamin"], "venue": "arXiv preprint arXiv:1402.5180,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "Learning overcomplete latent variable models through tensor methods", "author": ["Animashree Anandkumar", "Rong Ge", "Majid Janzamin"], "venue": "In Proceedings of The 28th Conference on Learning Theory,", "citeRegEx": "Anandkumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2015}, {"title": "Reinforcement learning of POMDPs using spectral methods", "author": ["Kamyar Azizzadenesheli", "Alessandro Lazaric", "Animashree Anandkumar"], "venue": "In 29th Annual Conference on Learning Theory,", "citeRegEx": "Azizzadenesheli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Azizzadenesheli et al\\.", "year": 2016}, {"title": "Efficient MATLAB computations with sparse and factored tensors", "author": ["Brett W. Bader", "Tamara G. Kolda"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Bader and Kolda.,? \\Q2007\\E", "shortCiteRegEx": "Bader and Kolda.", "year": 2007}, {"title": "Matlab tensor toolbox version 2.5", "author": ["Brett W. Bader", "Tamara G. Kolda"], "venue": "Available online,", "citeRegEx": "Bader and Kolda,? \\Q2012\\E", "shortCiteRegEx": "Bader and Kolda", "year": 2012}, {"title": "A practical randomized CP tensor decomposition", "author": ["Casey Battaglino", "Grey Ballard", "Tamara G Kolda"], "venue": "arXiv preprint arXiv:1701.06600,", "citeRegEx": "Battaglino et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Battaglino et al\\.", "year": 2017}, {"title": "Distributional semantics in technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 136\u2013145", "author": ["Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam-Khanh Tran"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Bruni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Estimating latent-variable graphical models using moments and likelihoods", "author": ["Arun Tejasvi Chaganty", "Percy Liang"], "venue": "In ICML,", "citeRegEx": "Chaganty and Liang.,? \\Q2014\\E", "shortCiteRegEx": "Chaganty and Liang.", "year": 2014}, {"title": "SPALS: Fast alternating least squares via implicit leverage scores sampling", "author": ["Dehua Cheng", "Richard Peng", "Yan Liu", "Ioakeim Perros"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "FastMotif: spectral sequence motif", "author": ["Nicolo Colombo", "Nikos Vlassis"], "venue": "discovery. Bioinformatics,", "citeRegEx": "Colombo and Vlassis.,? \\Q2015\\E", "shortCiteRegEx": "Colombo and Vlassis.", "year": 2015}, {"title": "Tensor decomposition via joint matrix schur decomposition", "author": ["Nicolo Colombo", "Nikos Vlassis"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Colombo and Vlassis.,? \\Q2016\\E", "shortCiteRegEx": "Colombo and Vlassis.", "year": 2016}, {"title": "Tensor decompositions, alternating least squares and other tales", "author": ["Pierre Comon", "Xavier Luciani", "Andr\u00e9 LF De Almeida"], "venue": "Journal of chemometrics,", "citeRegEx": "Comon et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Comon et al\\.", "year": 2009}, {"title": "A link between the canonical decomposition in multilinear algebra and simultaneous matrix diagonalization", "author": ["Lieven De Lathauwer"], "venue": "SIAM journal on Matrix Analysis and Applications,", "citeRegEx": "Lathauwer.,? \\Q2006\\E", "shortCiteRegEx": "Lathauwer.", "year": 2006}, {"title": "Bounding standard gaussian tail probabilities", "author": ["Lutz Duembgen"], "venue": "arXiv preprint arXiv:1012.2063,", "citeRegEx": "Duembgen.,? \\Q2010\\E", "shortCiteRegEx": "Duembgen.", "year": 2010}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Decomposing overcomplete 3rd order tensors using sum-of-squares algorithms. Approximation, Randomization, and Combinatorial Optimization", "author": ["Rong Ge", "Tengyu Ma"], "venue": "Algorithms and Techniques,", "citeRegEx": "Ge and Ma.,? \\Q2015\\E", "shortCiteRegEx": "Ge and Ma.", "year": 2015}, {"title": "Learning mixtures of gaussians in high dimensions", "author": ["Rong Ge", "Qingqing Huang", "Sham M Kakade"], "venue": "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,", "citeRegEx": "Ge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2015}, {"title": "Foundations of the parafac procedure: Models and conditions for an", "author": ["Richard A Harshman"], "venue": "explanatory\u201d multi-modal factor analysis", "citeRegEx": "Harshman.,? \\Q1970\\E", "shortCiteRegEx": "Harshman.", "year": 1970}, {"title": "Tensor rank is NP-Complete", "author": ["Johan H\u030aastad"], "venue": "Journal of Algorithms,", "citeRegEx": "H\u030aastad.,? \\Q1990\\E", "shortCiteRegEx": "H\u030aastad.", "year": 1990}, {"title": "Most tensor problems are NP-Hard", "author": ["Christopher J Hillar", "Lek-Heng Lim"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Hillar and Lim.,? \\Q2013\\E", "shortCiteRegEx": "Hillar and Lim.", "year": 2013}, {"title": "Fast spectral algorithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors", "author": ["Samuel B Hopkins", "Tselil Schramm", "Jonathan Shi", "David Steurer"], "venue": "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing,", "citeRegEx": "Hopkins et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hopkins et al\\.", "year": 2016}, {"title": "Fast detection of overlapping communities via online tensor methods", "author": ["Furong Huang", "UN Niranjan", "Mohammad Umar Hakeem", "Animashree Anandkumar"], "venue": "arXiv preprint arXiv:1309.0787,", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Distributed latent dirichlet allocation via tensor factorization", "author": ["Furong Huang", "Sergiy Matusevych", "Anima Anandkumar", "Nikos Karampatziakis", "Paul Mineiro"], "venue": "In NIPS Optimization Workshop,", "citeRegEx": "Huang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2014}, {"title": "Gigatensor: scaling tensor analysis up by 100 times-algorithms and discoveries", "author": ["U Kang", "Evangelos Papalexakis", "Abhay Harpale", "Christos Faloutsos"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Kang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2012}, {"title": "Tensor decompositions and applications", "author": ["Tamara G Kolda", "Brett W Bader"], "venue": "SIAM review,", "citeRegEx": "Kolda and Bader.,? \\Q2009\\E", "shortCiteRegEx": "Kolda and Bader.", "year": 2009}, {"title": "Shifted power method for computing tensor eigenpairs", "author": ["Tamara G Kolda", "Jackson R Mayo"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Kolda and Mayo.,? \\Q2011\\E", "shortCiteRegEx": "Kolda and Mayo.", "year": 2011}, {"title": "Tensorly: Tensor learning in python", "author": ["Jean Kossaifi", "Yannis Panagakis", "Maja Pantic"], "venue": "arXiv preprint arXiv:1610.09555,", "citeRegEx": "Kossaifi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kossaifi et al\\.", "year": 2016}, {"title": "Three-way arrays: rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics", "author": ["Joseph B Kruskal"], "venue": "Linear algebra and its applications,", "citeRegEx": "Kruskal.,? \\Q1977\\E", "shortCiteRegEx": "Kruskal.", "year": 1977}, {"title": "Tensor factorization via matrix factorization", "author": ["Volodymyr Kuleshov", "Arun Tejasvi Chaganty", "Percy Liang"], "venue": "In AISTATS,", "citeRegEx": "Kuleshov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kuleshov et al\\.", "year": 2015}, {"title": "ICA with reconstruction cost for efficient overcomplete feature learning", "author": ["Quoc V Le", "Alexandre Karpenko", "Jiquan Ngiam", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Le et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Le et al\\.", "year": 2011}, {"title": "A decomposition for three-way arrays", "author": ["SE Leurgans", "RT Ross", "RB Abel"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Leurgans et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Leurgans et al\\.", "year": 1993}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levy and Goldberg.,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Polynomial-time tensor decompositions with sumof-squares", "author": ["Tengyu Ma", "Jonathan Shi", "David Steurer"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Ma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In HLT-NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Parcube: Sparse parallelizable tensor decompositions", "author": ["Evangelos E Papalexakis", "Christos Faloutsos", "Nicholas D Sidiropoulos"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Papalexakis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Papalexakis et al\\.", "year": 2012}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Sparse and low-rank tensor decomposition", "author": ["Parikshit Shah", "Nikhil Rao", "Gongguo Tang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Shah et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shah et al\\.", "year": 2015}, {"title": "DMS: Distributed sparse tensor factorization with alternating least squares", "author": ["Shaden Smith", "George Karypis"], "venue": "Technical report,", "citeRegEx": "Smith and Karypis.,? \\Q2015\\E", "shortCiteRegEx": "Smith and Karypis.", "year": 2015}, {"title": "Sublinear time orthogonal tensor decomposition", "author": ["Zhao Song", "David Woodruff", "Huan Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Song et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Song et al\\.", "year": 2016}, {"title": "Joint diagonalization: Is non-orthogonal always preferable to orthogonal", "author": ["Antoine Souloumiac"], "venue": "In 2009 3rd IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP),", "citeRegEx": "Souloumiac.,? \\Q2009\\E", "shortCiteRegEx": "Souloumiac.", "year": 2009}, {"title": "Guaranteed tensor decomposition: A moment approach", "author": ["Gongguo Tang", "Parikshit Shah"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Tang and Shah.,? \\Q2015\\E", "shortCiteRegEx": "Tang and Shah.", "year": 2015}, {"title": "Rubik: Knowledge guided tensor factorization and completion for health data analytics", "author": ["Yichen Wang", "Robert Chen", "Joydeep Ghosh", "Joshua C Denny", "Abel Kho", "You Chen", "Bradley A Malin", "Jimeng Sun"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Fast and guaranteed tensor decomposition via sketching", "author": ["Yining Wang", "Hsiao-Yu Tung", "Alexander J Smola", "Anima Anandkumar"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Learning from multiway data: Simple and efficient tensor regression", "author": ["Rose Yu", "Yan Liu"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning", "citeRegEx": "Yu and Liu.,? \\Q2016\\E", "shortCiteRegEx": "Yu and Liu.", "year": 2016}, {"title": "Rank-one approximation to high order tensors", "author": ["Tong Zhang", "Gene H Golub"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Zhang and Golub.,? \\Q2001\\E", "shortCiteRegEx": "Zhang and Golub.", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "From a theoretical perspective, tensor methods have become an incredibly useful and versatile tool for learning a wide array of popular models, including topic modeling (Anandkumar et al., 2012), mixtures of Gaussians (Ge et al.", "startOffset": 169, "endOffset": 194}, {"referenceID": 19, "context": ", 2012), mixtures of Gaussians (Ge et al., 2015), community detection (Anandkumar et al.", "startOffset": 31, "endOffset": 48}, {"referenceID": 10, "context": ", 2014a), learning graphical models with guarantees via the method of moments (Anandkumar et al., 2014b; Chaganty and Liang, 2014) and reinforcement learning (Azizzadenesheli et al.", "startOffset": 78, "endOffset": 130}, {"referenceID": 5, "context": ", 2014b; Chaganty and Liang, 2014) and reinforcement learning (Azizzadenesheli et al., 2016).", "startOffset": 62, "endOffset": 92}, {"referenceID": 30, "context": "The key property of tensors that enables these applications is that tensors have a unique decomposition (decomposition here refers to the most commonly used CANDECOMP/PARAFAC or CP decomposition), under mild conditions on the factor matrices (Kruskal, 1977); for example, tensors have a unique decomposition whenever the factor matrices are full rank.", "startOffset": 242, "endOffset": 257}, {"referenceID": 48, "context": "In fact, we are already seeing exciting applications of tensor methods for analysis of high-order spatiotemporal data (Yu and Liu, 2016), health data analysis (Wang et al.", "startOffset": 118, "endOffset": 136}, {"referenceID": 12, "context": ", 2015a) and bioinformatics (Colombo and Vlassis, 2015).", "startOffset": 28, "endOffset": 55}, {"referenceID": 22, "context": "As tensor decomposition is NP-Hard in the worst-case (Hillar and Lim, 2013; H\u030aastad, 1990), one cannot hope for algorithms which always produce the correct factorization.", "startOffset": 53, "endOffset": 90}, {"referenceID": 21, "context": "As tensor decomposition is NP-Hard in the worst-case (Hillar and Lim, 2013; H\u030aastad, 1990), one cannot hope for algorithms which always produce the correct factorization.", "startOffset": 53, "endOffset": 90}, {"referenceID": 33, "context": "Early work from the 1970\u2019s (Leurgans et al., 1993; Harshman, 1970) established a simple algorithm ar X iv :1 70 3.", "startOffset": 27, "endOffset": 66}, {"referenceID": 20, "context": "Early work from the 1970\u2019s (Leurgans et al., 1993; Harshman, 1970) established a simple algorithm ar X iv :1 70 3.", "startOffset": 27, "endOffset": 66}, {"referenceID": 35, "context": "25) is significantly worse than the best known provable recovery guarantees for polynomial-time algorithms on random tensors\u2014the recent work Ma et al. (2016) succeeds even in the over-complete setting with k = o(d1.", "startOffset": 141, "endOffset": 158}, {"referenceID": 0, "context": "Anandkumar et al. (2014c) had previously shown local convergence of the tensor power method with a linear convergence rate (and also showed global convergence via a SVD-based initialization scheme, obtaining the first guarantees for the tensor power method in non-orthogonal settings).", "startOffset": 0, "endOffset": 26}, {"referenceID": 32, "context": "However, obtaining guaranteed recovery of non-orthogonal tensors using algorithms for orthogonal tensors requires converting the tensor into an orthogonal form (known as whitening) which is ill conditioned in high dimensions (Le et al., 2011; Souloumiac, 2009), and is computationally the most expensive step (Huang et al.", "startOffset": 225, "endOffset": 260}, {"referenceID": 44, "context": "However, obtaining guaranteed recovery of non-orthogonal tensors using algorithms for orthogonal tensors requires converting the tensor into an orthogonal form (known as whitening) which is ill conditioned in high dimensions (Le et al., 2011; Souloumiac, 2009), and is computationally the most expensive step (Huang et al.", "startOffset": 225, "endOffset": 260}, {"referenceID": 24, "context": ", 2011; Souloumiac, 2009), and is computationally the most expensive step (Huang et al., 2013).", "startOffset": 74, "endOffset": 94}, {"referenceID": 13, "context": "Another very interesting line of work on tensor decompositions is to use simultaneous diagonalization and higher order SVD (Colombo and Vlassis, 2016; Kuleshov et al., 2015; De Lathauwer, 2006) but the algorithms typically have global convergence guarantees only for orthogonal tensors, and are not as computationally efficient as alternating minimization2.", "startOffset": 123, "endOffset": 193}, {"referenceID": 31, "context": "Another very interesting line of work on tensor decompositions is to use simultaneous diagonalization and higher order SVD (Colombo and Vlassis, 2016; Kuleshov et al., 2015; De Lathauwer, 2006) but the algorithms typically have global convergence guarantees only for orthogonal tensors, and are not as computationally efficient as alternating minimization2.", "startOffset": 123, "endOffset": 193}, {"referenceID": 35, "context": "Recently, there has been intriguing work on provably decomposing random tensors using the sum-of-squares approach (Ma et al., 2016; Hopkins et al., 2016; Tang and Shah, 2015; Ge and Ma, 2015).", "startOffset": 114, "endOffset": 191}, {"referenceID": 23, "context": "Recently, there has been intriguing work on provably decomposing random tensors using the sum-of-squares approach (Ma et al., 2016; Hopkins et al., 2016; Tang and Shah, 2015; Ge and Ma, 2015).", "startOffset": 114, "endOffset": 191}, {"referenceID": 45, "context": "Recently, there has been intriguing work on provably decomposing random tensors using the sum-of-squares approach (Ma et al., 2016; Hopkins et al., 2016; Tang and Shah, 2015; Ge and Ma, 2015).", "startOffset": 114, "endOffset": 191}, {"referenceID": 18, "context": "Recently, there has been intriguing work on provably decomposing random tensors using the sum-of-squares approach (Ma et al., 2016; Hopkins et al., 2016; Tang and Shah, 2015; Ge and Ma, 2015).", "startOffset": 114, "endOffset": 191}, {"referenceID": 0, "context": "Many algorithms have been proposed for guaranteed decomposition of orthogonal tensors, we refer the reader to Anandkumar et al. (2014b); Kolda and Mayo (2011); Comon et al.", "startOffset": 110, "endOffset": 136}, {"referenceID": 0, "context": "Many algorithms have been proposed for guaranteed decomposition of orthogonal tensors, we refer the reader to Anandkumar et al. (2014b); Kolda and Mayo (2011); Comon et al.", "startOffset": 110, "endOffset": 159}, {"referenceID": 0, "context": "Many algorithms have been proposed for guaranteed decomposition of orthogonal tensors, we refer the reader to Anandkumar et al. (2014b); Kolda and Mayo (2011); Comon et al. (2009); Zhang and Golub (2001).", "startOffset": 110, "endOffset": 180}, {"referenceID": 0, "context": "Many algorithms have been proposed for guaranteed decomposition of orthogonal tensors, we refer the reader to Anandkumar et al. (2014b); Kolda and Mayo (2011); Comon et al. (2009); Zhang and Golub (2001). However, obtaining guaranteed recovery of non-orthogonal tensors using algorithms for orthogonal tensors requires converting the tensor into an orthogonal form (known as whitening) which is ill conditioned in high dimensions (Le et al.", "startOffset": 110, "endOffset": 204}, {"referenceID": 0, "context": "Many algorithms have been proposed for guaranteed decomposition of orthogonal tensors, we refer the reader to Anandkumar et al. (2014b); Kolda and Mayo (2011); Comon et al. (2009); Zhang and Golub (2001). However, obtaining guaranteed recovery of non-orthogonal tensors using algorithms for orthogonal tensors requires converting the tensor into an orthogonal form (known as whitening) which is ill conditioned in high dimensions (Le et al., 2011; Souloumiac, 2009), and is computationally the most expensive step (Huang et al., 2013). Another very interesting line of work on tensor decompositions is to use simultaneous diagonalization and higher order SVD (Colombo and Vlassis, 2016; Kuleshov et al., 2015; De Lathauwer, 2006) but the algorithms typically have global convergence guarantees only for orthogonal tensors, and are not as computationally efficient as alternating minimization2. Recently, there has been intriguing work on provably decomposing random tensors using the sum-of-squares approach (Ma et al., 2016; Hopkins et al., 2016; Tang and Shah, 2015; Ge and Ma, 2015). Ma et al. (2016) show that a sum-of-squares based relaxation can decompose highly overcomplete random tensors of rank De Lathauwer (2006) prove unique recovery under very general conditions, but their algorithm is quite complex and requires solving a linear system of size O(d), which is prohibitive for large tensors.", "startOffset": 110, "endOffset": 1104}, {"referenceID": 0, "context": "Many algorithms have been proposed for guaranteed decomposition of orthogonal tensors, we refer the reader to Anandkumar et al. (2014b); Kolda and Mayo (2011); Comon et al. (2009); Zhang and Golub (2001). However, obtaining guaranteed recovery of non-orthogonal tensors using algorithms for orthogonal tensors requires converting the tensor into an orthogonal form (known as whitening) which is ill conditioned in high dimensions (Le et al., 2011; Souloumiac, 2009), and is computationally the most expensive step (Huang et al., 2013). Another very interesting line of work on tensor decompositions is to use simultaneous diagonalization and higher order SVD (Colombo and Vlassis, 2016; Kuleshov et al., 2015; De Lathauwer, 2006) but the algorithms typically have global convergence guarantees only for orthogonal tensors, and are not as computationally efficient as alternating minimization2. Recently, there has been intriguing work on provably decomposing random tensors using the sum-of-squares approach (Ma et al., 2016; Hopkins et al., 2016; Tang and Shah, 2015; Ge and Ma, 2015). Ma et al. (2016) show that a sum-of-squares based relaxation can decompose highly overcomplete random tensors of rank De Lathauwer (2006) prove unique recovery under very general conditions, but their algorithm is quite complex and requires solving a linear system of size O(d), which is prohibitive for large tensors.", "startOffset": 110, "endOffset": 1225}, {"referenceID": 0, "context": "Many algorithms have been proposed for guaranteed decomposition of orthogonal tensors, we refer the reader to Anandkumar et al. (2014b); Kolda and Mayo (2011); Comon et al. (2009); Zhang and Golub (2001). However, obtaining guaranteed recovery of non-orthogonal tensors using algorithms for orthogonal tensors requires converting the tensor into an orthogonal form (known as whitening) which is ill conditioned in high dimensions (Le et al., 2011; Souloumiac, 2009), and is computationally the most expensive step (Huang et al., 2013). Another very interesting line of work on tensor decompositions is to use simultaneous diagonalization and higher order SVD (Colombo and Vlassis, 2016; Kuleshov et al., 2015; De Lathauwer, 2006) but the algorithms typically have global convergence guarantees only for orthogonal tensors, and are not as computationally efficient as alternating minimization2. Recently, there has been intriguing work on provably decomposing random tensors using the sum-of-squares approach (Ma et al., 2016; Hopkins et al., 2016; Tang and Shah, 2015; Ge and Ma, 2015). Ma et al. (2016) show that a sum-of-squares based relaxation can decompose highly overcomplete random tensors of rank De Lathauwer (2006) prove unique recovery under very general conditions, but their algorithm is quite complex and requires solving a linear system of size O(d), which is prohibitive for large tensors. We ran the simultaneous diagonalization algorithm of Kuleshov et al. (2015) on a dimension 100, rank 30 tensor; and the algorithm needed around 30 minutes to run, whereas Orth-ALS converges in less than 5 seconds.", "startOffset": 110, "endOffset": 1482}, {"referenceID": 43, "context": "Very recently, there has been exciting work on scalable tensor decomposition algorithms using ideas such as sketching (Song et al., 2016; Wang et al., 2015b) and contraction of tensor problems to matrix problems (Shah et al.", "startOffset": 118, "endOffset": 157}, {"referenceID": 41, "context": ", 2015b) and contraction of tensor problems to matrix problems (Shah et al., 2015).", "startOffset": 63, "endOffset": 82}, {"referenceID": 8, "context": "Also worth noting are recent approaches to speedup ALS via sampling and randomized least squares (Battaglino et al., 2017; Cheng et al., 2016; Papalexakis et al., 2012).", "startOffset": 97, "endOffset": 168}, {"referenceID": 11, "context": "Also worth noting are recent approaches to speedup ALS via sampling and randomized least squares (Battaglino et al., 2017; Cheng et al., 2016; Papalexakis et al., 2012).", "startOffset": 97, "endOffset": 168}, {"referenceID": 39, "context": "Also worth noting are recent approaches to speedup ALS via sampling and randomized least squares (Battaglino et al., 2017; Cheng et al., 2016; Papalexakis et al., 2012).", "startOffset": 97, "endOffset": 168}, {"referenceID": 27, "context": "ALS is the most widely used algorithm for tensor decomposition and has been described as the \u201cworkhorse\u201d for tensor decomposition (Kolda and Bader, 2009).", "startOffset": 130, "endOffset": 153}, {"referenceID": 29, "context": "There are several publicly available optimized packages implementing ALS, such as (Kossaifi et al., 2016; Vervliet et al.; Bader et al., 2012; Bader and Kolda, 2007; Smith and Karypis; Huang et al., 2014; Kang et al., 2012).", "startOffset": 82, "endOffset": 223}, {"referenceID": 6, "context": "There are several publicly available optimized packages implementing ALS, such as (Kossaifi et al., 2016; Vervliet et al.; Bader et al., 2012; Bader and Kolda, 2007; Smith and Karypis; Huang et al., 2014; Kang et al., 2012).", "startOffset": 82, "endOffset": 223}, {"referenceID": 25, "context": "There are several publicly available optimized packages implementing ALS, such as (Kossaifi et al., 2016; Vervliet et al.; Bader et al., 2012; Bader and Kolda, 2007; Smith and Karypis; Huang et al., 2014; Kang et al., 2012).", "startOffset": 82, "endOffset": 223}, {"referenceID": 26, "context": "There are several publicly available optimized packages implementing ALS, such as (Kossaifi et al., 2016; Vervliet et al.; Bader et al., 2012; Bader and Kolda, 2007; Smith and Karypis; Huang et al., 2014; Kang et al., 2012).", "startOffset": 82, "endOffset": 223}, {"referenceID": 14, "context": "Despite the advantages, ALS does not have any global convergence guarantees and can get stuck in local optima (Comon et al., 2009; Kolda and Bader, 2009), even under very realistic settings.", "startOffset": 110, "endOffset": 153}, {"referenceID": 27, "context": "Despite the advantages, ALS does not have any global convergence guarantees and can get stuck in local optima (Comon et al., 2009; Kolda and Bader, 2009), even under very realistic settings.", "startOffset": 110, "endOffset": 153}, {"referenceID": 0, "context": "Anandkumar et al. (2014c) showed that the tensor power method converges locally (i.", "startOffset": 0, "endOffset": 26}, {"referenceID": 45, "context": "Similar to other works (Tang and Shah, 2015; Anandkumar et al., 2014c), our guarantees for tensor decomposition depend on the incoherence of the factor matrices (cmax), defined to be the maximum correlation in absolute value between any two factors, i.", "startOffset": 23, "endOffset": 70}, {"referenceID": 0, "context": "This contrasts with the analysis of Anandkumar et al. (2014c) who showed a linear rate of convergence (O(log d) steps) for random tensors, provided an SVD based initialization is employed.", "startOffset": 36, "endOffset": 62}, {"referenceID": 33, "context": "We also test the classical technique for tensor decomposition via simultaneous diagonalization (Leurgans et al., 1993; Harshman, 1970) (also known as Jennrich\u2019s algorithm, we refer to it as Sim-Diag), which first performs two random projections of the tensor, and then recovers the factors by an eigenvalue decomposition of the projected matrices.", "startOffset": 95, "endOffset": 134}, {"referenceID": 20, "context": "We also test the classical technique for tensor decomposition via simultaneous diagonalization (Leurgans et al., 1993; Harshman, 1970) (also known as Jennrich\u2019s algorithm, we refer to it as Sim-Diag), which first performs two random projections of the tensor, and then recovers the factors by an eigenvalue decomposition of the projected matrices.", "startOffset": 95, "endOffset": 134}, {"referenceID": 4, "context": "Recovering over-complete tensors: Overcomplete tensors are tensors with rank higher than the dimension, and have found numerous theoretical applications in learning latent variable models (Anandkumar et al., 2015).", "startOffset": 188, "endOffset": 213}, {"referenceID": 34, "context": "Current methods for learning word embeddings implicitly (Mikolov et al., 2013b; Levy and Goldberg, 2014) or explicitly (Pennington et al.", "startOffset": 56, "endOffset": 104}, {"referenceID": 40, "context": ", 2013b; Levy and Goldberg, 2014) or explicitly (Pennington et al., 2014) factorize some matrix derived from the matrix of word co-occurrences M , where Mij denotes how often word i appears with word j.", "startOffset": 48, "endOffset": 73}, {"referenceID": 40, "context": "This scaling is known to perform well in practice for co-occurrence matrices (Pennington et al., 2014), and makes some intuitive sense in light of the Zipfian distribution of word frequencies.", "startOffset": 77, "endOffset": 102}, {"referenceID": 9, "context": "The word similarity tasks (Bruni et al., 2012; Finkelstein et al., 2001) contain word pairs along with human assigned similarity scores, and the objective is to maximize the correlation between the similarity in the embeddings of the two words (according to a similarity metric such as the dot product) and human judged similarity.", "startOffset": 26, "endOffset": 72}, {"referenceID": 17, "context": "The word similarity tasks (Bruni et al., 2012; Finkelstein et al., 2001) contain word pairs along with human assigned similarity scores, and the objective is to maximize the correlation between the similarity in the embeddings of the two words (according to a similarity metric such as the dot product) and human judged similarity.", "startOffset": 26, "endOffset": 72}, {"referenceID": 0, "context": "The tensor power method update equations can be written as (refer to Anandkumar et al. (2014c))", "startOffset": 69, "endOffset": 95}, {"referenceID": 16, "context": "We will use the following tail bound on the standard Gaussian random variable x (refer to Duembgen (2010))-", "startOffset": 90, "endOffset": 106}], "year": 2017, "abstractText": "The popular Alternating Least Squares (ALS) algorithm for tensor decomposition is extremely efficient, but often converges to poor local optima, particularly when the weights of the factors are non-uniform. We propose a modification of the ALS approach that is as efficient as standard ALS, but provably recovers the true factors with random initialization under standard incoherence assumptions on the factors of the tensor. We demonstrate the significant practical superiority of our approach over traditional ALS (with both random initialization and SVDbased initialization) for a variety of tasks on synthetic data\u2014including tensor factorization on exact, noisy and over-complete tensors, as well as tensor completion\u2014and for computing word embeddings from a third-order word tri-occurrence tensor.", "creator": "LaTeX with hyperref package"}}}