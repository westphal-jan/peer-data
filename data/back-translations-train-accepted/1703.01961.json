{"id": "1703.01961", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Multiplicative Normalizing Flows for Variational Bayesian Neural Networks", "abstract": "We reinterpret multiplicative noise in neural networks as auxiliary random variables that augment the approximate posterior in a variational setting for Bayesian neural networks. We show that through this interpretation it is both efficient and straightforward to improve the approximation by employing normalizing flows while still allowing for local reparametrizations and a tractable lower bound. In experiments we show that with this new approximation we can significantly improve upon classical mean field for Bayesian neural networks on both predictive accuracy as well as predictive uncertainty.", "histories": [["v1", "Mon, 6 Mar 2017 16:39:16 GMT  (970kb,D)", "http://arxiv.org/abs/1703.01961v1", "Submitted to ICML 2017"], ["v2", "Mon, 12 Jun 2017 21:05:58 GMT  (1136kb,D)", "http://arxiv.org/abs/1703.01961v2", "Appearing at the International Conference on Machine Learning (ICML) 2017"]], "COMMENTS": "Submitted to ICML 2017", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["christos louizos", "max welling"], "accepted": true, "id": "1703.01961"}, "pdf": {"name": "1703.01961.pdf", "metadata": {"source": "META", "title": "Multiplicative Normalizing Flows for Variational Bayesian Neural Networks", "authors": ["Christos Louizos", "Max Welling"], "emails": ["<c.louizos@uva.nl>."], "sections": [{"heading": "1. Introduction", "text": "In fact, most of us have no idea what they are doing. (...) Most of us have no idea what they are doing. (...) Most of us have no idea what they are doing. (...) Most of us have no idea what they are doing. (...) We have no idea what they are doing. (...) Most of us have no idea what they are doing. (...) Most of us have no idea what they are doing. (...) We have no idea. (...) We have no idea what they are doing. (...) We have no idea. (...) We have no idea. \"(...) We do not know what they are doing. (...) We do not know what they are doing. (...) We do not know what they are doing. (...) We do not know what they are doing. (...) We do not know what they are doing. (...)"}, {"heading": "2. Multiplicative normalizing flows", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Variational inference for Bayesian Neural Networks", "text": "Assuming that p (Wi), q\u03c6 (Wi) are the previous and approximate values above the parameters of the i'th layer, we may have the following lower limit on the marginal log probability of record D using variable Bayes (Peterson, 1987; Hinton & Van Camp, 1993; Graves, 2011; Blundell et al., 2015; Kingma et al., 2015; Gal & Ghahramani, 2015b; Louizos & Welling, 2016): L (Ep) = Ep (x, y) [Eq\u03c6 (W1: L) [log p (y | x, W1: L) + log p (W1: L) \u2212 log oferitions (W1: L) = problem of reparation or reparation (W1: L), if we (W1: L) allow the problem of reparation (W1: L) or reparation (the indemnity) and (the indemnity)."}, {"heading": "2.2. Improving the variational approximation", "text": "While there have been attempts to improve this approach with works such as (Gal & Ghahramani, 2015b) with mixtures of delta peaks and (Louizos & Welling, 2016) with matrix Gaussians that allow for non-religious covariances among weight classes, both of the aforementioned methods are still limited in a certain sense; the true parameters posterior are limited; the actual parameters posterior are more complex than delta peaks or correlated Gaussians. There have been a lot of new possibilities to improve posterior variable models."}, {"heading": "2.3. Bounding the entropy", "text": "However, this makes the calculation of entropy \u2212 Eq (W) [log q (W)] difficult. Fortunately, we can reverse the lower limit by limiting entropy further down. It can be seen as if we are making variable inferences to the augmented probability space p (D, W1: L, z1: L), which maintains the same true posterior distribution p (W, z1: L) (W | D) (as we can always marginalize)."}, {"heading": "3. Related work", "text": "In fact, it is as if most of us are able to obey the rules that they have imposed on ourselves. (...) It is not as if they are able to obey the rules. (...) It is as if they are able to change the rules. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine the rules. (...) It is as if they were able. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...)"}, {"heading": "4. Experiments", "text": "All experiments were encoded in tensor flow (Abadi et al., 2016) and optimized using Adam (Kingma & Ba, 2015) using the standard hyperparameters. We used the LeNet 54 (LeCun et al., 1998) convolutional architecture with ReLU (Nair & Hinton, 2010) nonlinearities. Means M of conditional Gaussian q (W | z) were initialized using the scheme proposed in (He et al., 2015), while the log of variances was initialized using samples from N (\u2212 9, 0.001). Unless explicitly stated otherwise, we use flows of length 2 for q (z) and r (z | W) with 50 hidden units for each step of the flow of q (z) and 100 hidden units for each step of the flow of r (z | W)."}, {"heading": "4.1. Predictive performance and uncertainty", "text": "We trained on MNIST LeNet al., We trained on MNIST LeNet al., We trained on Dropout with the way we described them (Gal & Ghahramani, 2015a), We trained Dropout with the way we described them (Gal & Ghahramani, 2015a), The models with the Gaussian prediction, which we consider to be the standard deviation from conditional post-processing during the preliminary phase, The classification performance of each model can be seen in Table 2; while our overall focus is not the classification accuracy per se, we see that with the MNF posteriors we achieve similar accuracy with deep ensembles. notMNIST To evaluate the predictive uncertainties of each model we described (Lakshminarayanan et al, 2016)."}, {"heading": "4.2. Accuracy and uncertainty on adversarial examples", "text": "We also measure how robust our models and uncertainties are against contrary examples (Szegedy et al., 2013; Goodfellow et al., 2014) by generating examples with cleverhans for each of the previously trained architectures (Goodfellow et al., 2014). For this task, we do not include deep ensembles, as they are trained on contrary examples. MNIST In this scenario, we observe interesting results when we record the change in accuracy and entropy by varying the order of magnitude of the opposite disturbance. The resulting graph is Figure 4. Overall, Dropout seems to have better accuracies on opposite examples; they never come at an \"overconfident\" price, as the entropy of the opposite distributions is quite low, leading to predictions that on average have a higher probability for the dominant class."}, {"heading": "4.3. Regression on toy dataset", "text": "For the final experiment, we visualize the predictive distributions obtained with the various models of the toy regression task introduced to (Herna \u0301 ndez-Lobato & Adams, 2015) We generated 20 training inputs from U [\u2212 4, 4] and then achieved the corresponding targets via y = x3 +, setting probability noise to its true value and then equipping a dropout network of \u03c0 = 0.5 for the hidden layer 9, an FFLU network and an MNFG. We also built a dropout network in which we also fixed the failure probability of the hidden layer according to the layer described in Section 2.3 (corresponding to the layer described in (Gal & Ghahramani, 2015b) by describing the failure probability of the hidden layer based on the uncertain layer derivation or failure probability of the hidden layer Gregor & Mh (2014)."}, {"heading": "5. Conclusion", "text": "We have shown that this approach can significantly improve the medium field in terms of both predictive power and predictive uncertainty. We compared our uncertainty in notMNIST and CIFAR with Dropout (Srivastava et al., 2014; Gal & Ghahramani, 2015b) and Deep Ensembles (Lakshminarayanan et al., 2016) using Convolutionary Architectures and found that MNFs achieve more realistic uncertainties while providing predictive capabilities on par with Dropout. We suspect that the predictive capabilities of MNFs can be further improved by more appropriate optimizers that avoid the bad local minima in the variation target. Finally, we have also demonstrated the limitations of dropout approximations and empirically that MNFs can be overcome as preceding neutrality."}, {"heading": "Acknowledgements", "text": "We thank Klamer Schutte, Matthias Reisser and Karen Ullrich for their valuable feedback. This research is supported by TNO, Scyfer B.V., NWO, Google and Facebook."}], "references": [{"title": "An auxiliary variational method", "author": ["Agakov", "Felix V", "Barber", "David"], "venue": "In International Conference on Neural Information Processing,", "citeRegEx": "Agakov et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Agakov et al\\.", "year": 2004}, {"title": "Weight uncertainty in neural networks", "author": ["Blundell", "Charles", "Cornebise", "Julien", "Kavukcuoglu", "Koray", "Wierstra", "Daan"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "Blundell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Blundell et al\\.", "year": 2015}, {"title": "Deep gaussian processes", "author": ["Damianou", "Andreas C", "Lawrence", "Neil D"], "venue": "In Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Damianou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Damianou et al\\.", "year": 2013}, {"title": "Density estimation using real nvp", "author": ["Dinh", "Laurent", "Sohl-Dickstein", "Jascha", "Bengio", "Samy"], "venue": "arXiv preprint arXiv:1605.08803,", "citeRegEx": "Dinh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2016}, {"title": "Bayesian convolutional neural networks with bernoulli approximate variational inference", "author": ["Gal", "Yarin", "Ghahramani", "Zoubin"], "venue": "arXiv preprint arXiv:1506.02158,", "citeRegEx": "Gal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2015}, {"title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning", "author": ["Gal", "Yarin", "Ghahramani", "Zoubin"], "venue": "arXiv preprint arXiv:1506.02142,", "citeRegEx": "Gal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2015}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Practical variational inference for neural networks", "author": ["Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Graves and Alex.,? \\Q2011\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2011}, {"title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Probabilistic backpropagation for scalable learning of bayesian neural networks", "author": ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", "Adams", "Ryan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2015}, {"title": "Black-box \u03b1-divergence minimization", "author": ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", "Li", "Yingzhen", "Daniel", "Bui", "Thang", "Turner", "Richard E"], "venue": "arXiv preprint arXiv:1511.03243,", "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2015}, {"title": "Keeping the neural networks simple by minimizing the description length of the weights", "author": ["Hinton", "Geoffrey E", "Van Camp", "Drew"], "venue": "In Proceedings of the sixth annual conference on Computational learning theory,", "citeRegEx": "Hinton et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1993}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "International Conference on Learning Representations (ICLR), San Diego,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Variational dropout and the local reparametrization trick", "author": ["Kingma", "Diederik P", "Salimans", "Tim", "Welling", "Max"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Improving variational inference with inverse autoregressive flow", "author": ["Kingma", "Diederik P", "Salimans", "Tim", "Welling", "Max"], "venue": "arXiv preprint arXiv:1606.04934,", "citeRegEx": "Kingma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2016}, {"title": "Bayesian dark knowledge", "author": ["Korattikara", "Anoop", "Rathod", "Vivek", "Murphy", "Kevin", "Welling", "Max"], "venue": "arXiv preprint arXiv:1506.04416,", "citeRegEx": "Korattikara et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Korattikara et al\\.", "year": 2015}, {"title": "Simple and scalable predictive uncertainty estimation using deep ensembles", "author": ["Lakshminarayanan", "Balaji", "Pritzel", "Alexander", "Blundell", "Charles"], "venue": "arXiv preprint arXiv:1612.01474,", "citeRegEx": "Lakshminarayanan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lakshminarayanan et al\\.", "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Structured and efficient variational deep learning with matrix gaussian posteriors", "author": ["Louizos", "Christos", "Welling", "Max"], "venue": "arXiv preprint arXiv:1603.04733,", "citeRegEx": "Louizos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Louizos et al\\.", "year": 2016}, {"title": "Auxiliary deep generative models", "author": ["Maal\u00f8e", "Lars", "S\u00f8nderby", "Casper Kaae", "S\u00f8ren Kaae", "Winther", "Ole"], "venue": "arXiv preprint arXiv:1602.05473,", "citeRegEx": "Maal\u00f8e et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Maal\u00f8e et al\\.", "year": 2016}, {"title": "A practical bayesian framework for backpropagation networks", "author": ["MacKay", "David JC"], "venue": "Neural computation,", "citeRegEx": "MacKay and JC.,? \\Q1992\\E", "shortCiteRegEx": "MacKay and JC.", "year": 1992}, {"title": "Expectation propagation for approximate bayesian inference", "author": ["Minka", "Thomas P"], "venue": "In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "Minka and P.,? \\Q2001\\E", "shortCiteRegEx": "Minka and P.", "year": 2001}, {"title": "Neural variational inference and learning in belief networks", "author": ["Mnih", "Andriy", "Gregor", "Karol"], "venue": "arXiv preprint arXiv:1402.0030,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Bayesian learning for neural networks", "author": ["Neal", "Radford M"], "venue": "PhD thesis, Citeseer,", "citeRegEx": "Neal and M.,? \\Q1995\\E", "shortCiteRegEx": "Neal and M.", "year": 1995}, {"title": "Deep exploration via bootstrapped dqn", "author": ["Osband", "Ian", "Blundell", "Charles", "Pritzel", "Alexander", "Van Roy", "Benjamin"], "venue": "arXiv preprint arXiv:1602.04621,", "citeRegEx": "Osband et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2016}, {"title": "cleverhans v1.0.0: an adversarial machine learning library", "author": ["Papernot", "Nicolas", "Goodfellow", "Ian", "Sheatsley", "Ryan", "Feinman", "Reuben", "McDaniel", "Patrick"], "venue": "arXiv preprint arXiv:1610.00768,", "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "A mean field theory learning algorithm for neural networks", "author": ["Peterson", "Carsten"], "venue": "Complex systems,", "citeRegEx": "Peterson and Carsten.,? \\Q1987\\E", "shortCiteRegEx": "Peterson and Carsten.", "year": 1987}, {"title": "Hierarchical variational models", "author": ["Ranganath", "Rajesh", "Tran", "Dustin", "Blei", "David M"], "venue": "arXiv preprint arXiv:1511.02386,", "citeRegEx": "Ranganath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2015}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "arXiv preprint arXiv:1505.05770,", "citeRegEx": "Rezende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Fixed-form variational posterior approximation through stochastic linear regression", "author": ["Salimans", "Tim", "Knowles", "David A"], "venue": "Bayesian Analysis,", "citeRegEx": "Salimans et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2013}, {"title": "Markov chain monte carlo and variational inference: Bridging the gap", "author": ["Salimans", "Tim", "Kingma", "Diederik P", "Welling", "Max"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Soft weight-sharing for neural network compression", "author": ["Ullrich", "Karen", "Meeds", "Edward", "Welling", "Max"], "venue": "arXiv preprint arXiv:1702.04008,", "citeRegEx": "Ullrich et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ullrich et al\\.", "year": 2017}, {"title": "Fast dropout training", "author": ["Wang", "Sida", "Manning", "Christopher"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Bayesian learning via stochastic gradient langevin dynamics", "author": ["Welling", "Max", "Teh", "Yee W"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Welling et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2011}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}], "referenceMentions": [{"referenceID": 12, "context": "We show that through this interpretation it is both efficient and straightforward to improve the approximation by employing normalizing flows (Rezende & Mohamed, 2015) while still allowing for local reparametrizations (Kingma et al., 2015) and a tractable lower bound (Ranganath et al.", "startOffset": 218, "endOffset": 239}, {"referenceID": 29, "context": ", 2015) and a tractable lower bound (Ranganath et al., 2015; Maal\u00f8e et al., 2016).", "startOffset": 36, "endOffset": 81}, {"referenceID": 20, "context": ", 2015) and a tractable lower bound (Ranganath et al., 2015; Maal\u00f8e et al., 2016).", "startOffset": 36, "endOffset": 81}, {"referenceID": 16, "context": "Many works have considered the task of approximate Bayesian inference for neural networks using either Markov Chain Monte Carlo (MCMC) with Hamiltonian Dynamics (Neal, 1995), distilling SGD with Langevin Dynamics (Welling & Teh, 2011; Korattikara et al., 2015) or deterministic techniques such as the Laplace Approximation (MacKay, 1992), Expectation Propagation (Hern\u00e1ndez-Lobato & Adams, 2015; Hern\u00e1ndezLobato et al.", "startOffset": 213, "endOffset": 260}, {"referenceID": 1, "context": ", 2015) and variational inference (Graves, 2011; Blundell et al., 2015; Kingma et al., 2015; Gal & Ghahramani, 2015b; Louizos & Welling, 2016).", "startOffset": 34, "endOffset": 142}, {"referenceID": 12, "context": ", 2015) and variational inference (Graves, 2011; Blundell et al., 2015; Kingma et al., 2015; Gal & Ghahramani, 2015b; Louizos & Welling, 2016).", "startOffset": 34, "endOffset": 142}, {"referenceID": 31, "context": "We will adopt a stochastic gradient variational inference (Kingma & Welling, 2014; Rezende et al., 2014) procedure in order to estimate the posterior distribution over the weight matrices of the network.", "startOffset": 58, "endOffset": 104}, {"referenceID": 32, "context": "In Section 2 we will show how we can produce very flexible distributions in an efficient way by employing auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; Ranganath et al., 2015; Maal\u00f8e et al., 2016) and normalizing flows (Rezende & Mohamed, 2015).", "startOffset": 133, "endOffset": 224}, {"referenceID": 29, "context": "In Section 2 we will show how we can produce very flexible distributions in an efficient way by employing auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; Ranganath et al., 2015; Maal\u00f8e et al., 2016) and normalizing flows (Rezende & Mohamed, 2015).", "startOffset": 133, "endOffset": 224}, {"referenceID": 20, "context": "In Section 2 we will show how we can produce very flexible distributions in an efficient way by employing auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; Ranganath et al., 2015; Maal\u00f8e et al., 2016) and normalizing flows (Rezende & Mohamed, 2015).", "startOffset": 133, "endOffset": 224}, {"referenceID": 1, "context": "Assuming that p(Wi), q\u03c6(Wi) are the prior and approximate posterior over the parameters of the i\u2019th layer we can derive the following lower bound on the marginal log-likelihood of the dataset D using variational Bayes (Peterson, 1987; Hinton & Van Camp, 1993; Graves, 2011; Blundell et al., 2015; Kingma et al., 2015; Gal & Ghahramani, 2015b; Louizos & Welling, 2016):", "startOffset": 218, "endOffset": 367}, {"referenceID": 12, "context": "Assuming that p(Wi), q\u03c6(Wi) are the prior and approximate posterior over the parameters of the i\u2019th layer we can derive the following lower bound on the marginal log-likelihood of the dataset D using variational Bayes (Peterson, 1987; Hinton & Van Camp, 1993; Graves, 2011; Blundell et al., 2015; Kingma et al., 2015; Gal & Ghahramani, 2015b; Louizos & Welling, 2016):", "startOffset": 218, "endOffset": 367}, {"referenceID": 31, "context": "For continuous q(\u00b7) distributions that allow for the reparametrization trick (Kingma & Welling, 2014) or stochastic backpropagation (Rezende et al., 2014) we can reparametrize the random sampling from q(\u00b7) of the lower bound in terms of noise variables and deterministic functions f(\u03c6, ):", "startOffset": 132, "endOffset": 154}, {"referenceID": 32, "context": "There has been a lot of recent work on ways to improve the posterior approximation in latent variable models; normalizing flows (Rezende & Mohamed, 2015) and auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; Ranganath et al., 2015; Maal\u00f8e et al., 2016)", "startOffset": 185, "endOffset": 276}, {"referenceID": 29, "context": "There has been a lot of recent work on ways to improve the posterior approximation in latent variable models; normalizing flows (Rezende & Mohamed, 2015) and auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; Ranganath et al., 2015; Maal\u00f8e et al., 2016)", "startOffset": 185, "endOffset": 276}, {"referenceID": 20, "context": "There has been a lot of recent work on ways to improve the posterior approximation in latent variable models; normalizing flows (Rezende & Mohamed, 2015) and auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; Ranganath et al., 2015; Maal\u00f8e et al., 2016)", "startOffset": 185, "endOffset": 276}, {"referenceID": 31, "context": "Deep Latent Gaussian Models (Kingma & Welling, 2014; Rezende et al., 2014).", "startOffset": 28, "endOffset": 74}, {"referenceID": 12, "context": "Furthermore, by utilizing this procedure we also lose the benefits of local reparametrizations (Kingma et al., 2015; Louizos & Welling, 2016) which are possible with Gaussian approximate posteriors.", "startOffset": 95, "endOffset": 141}, {"referenceID": 32, "context": "In order to simultaneously maintain the benefits of local reparametrizations and increase the flexibility of the approximate posteriors in a Bayesian neural network we will rely on auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; 2015; Ranganath et al., 2015; Maal\u00f8e et al., 2016); more specifically we will exploit the well known \u201cmultiplicative noise\u201d concept, e.", "startOffset": 208, "endOffset": 305}, {"referenceID": 29, "context": "In order to simultaneously maintain the benefits of local reparametrizations and increase the flexibility of the approximate posteriors in a Bayesian neural network we will rely on auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; 2015; Ranganath et al., 2015; Maal\u00f8e et al., 2016); more specifically we will exploit the well known \u201cmultiplicative noise\u201d concept, e.", "startOffset": 208, "endOffset": 305}, {"referenceID": 20, "context": "In order to simultaneously maintain the benefits of local reparametrizations and increase the flexibility of the approximate posteriors in a Bayesian neural network we will rely on auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; 2015; Ranganath et al., 2015; Maal\u00f8e et al., 2016); more specifically we will exploit the well known \u201cmultiplicative noise\u201d concept, e.", "startOffset": 208, "endOffset": 305}, {"referenceID": 12, "context": "Note that we did not let z affect the variance of the Gaussian approximation; in a pilot study we found that this parametrization was prone to local optima due to large variance gradients, an effect also observed with the multiplicative parametrization of the Gaussian posterior (Kingma et al., 2015; Molchanov et al., 2017).", "startOffset": 279, "endOffset": 324}, {"referenceID": 3, "context": "For the normalizing flow of q(z) we will use the masked RealNVP (Dinh et al., 2016) using the numerically stable updates introduced in Inverse Autoregressive Flow (IAF) (Kingma et al.", "startOffset": 64, "endOffset": 83}, {"referenceID": 15, "context": ", 2016) using the numerically stable updates introduced in Inverse Autoregressive Flow (IAF) (Kingma et al., 2016):", "startOffset": 93, "endOffset": 114}, {"referenceID": 32, "context": "Fortunately we can make the lower bound tractable again by further lower bounding the entropy in terms of an auxiliary distribution r(z|W) (Agakov & Barber, 2004; Salimans et al., 2013; 2015; Ranganath et al., 2015; Maal\u00f8e et al., 2016).", "startOffset": 139, "endOffset": 236}, {"referenceID": 29, "context": "Fortunately we can make the lower bound tractable again by further lower bounding the entropy in terms of an auxiliary distribution r(z|W) (Agakov & Barber, 2004; Salimans et al., 2013; 2015; Ranganath et al., 2015; Maal\u00f8e et al., 2016).", "startOffset": 139, "endOffset": 236}, {"referenceID": 20, "context": "Fortunately we can make the lower bound tractable again by further lower bounding the entropy in terms of an auxiliary distribution r(z|W) (Agakov & Barber, 2004; Salimans et al., 2013; 2015; Ranganath et al., 2015; Maal\u00f8e et al., 2016).", "startOffset": 139, "endOffset": 236}, {"referenceID": 29, "context": "Therefore, to allow for a flexible r(z|W) we will follow (Ranganath et al., 2015) and we will parametrize it with inverse normalizing flows as follows:", "startOffset": 57, "endOffset": 81}, {"referenceID": 29, "context": "We can arrive at the bound of (Gal & Ghahramani, 2015b) if we trivially parametrize the auxiliary model r(z|W) = q(z) (which provides a less tight bound (Ranganath et al., 2015)) use a standard normal prior for W, a Bernoulli q(z) with probability of success \u03c0 and then let the variance of our conditional Gaussian q(W|z) go to zero.", "startOffset": 153, "endOffset": 177}, {"referenceID": 15, "context": "This is akin to the free bits objective described at (Kingma et al., 2016).", "startOffset": 53, "endOffset": 74}, {"referenceID": 16, "context": ") and distillation methods (Korattikara et al., 2015).", "startOffset": 27, "endOffset": 53}, {"referenceID": 1, "context": "(Blundell et al., 2015) improved upon this work with an unbiased estimator and a scale mixture prior.", "startOffset": 0, "endOffset": 23}, {"referenceID": 12, "context": "(Kingma et al., 2015) showed how Gaussian dropout can be interpreted as performing approximate inference with log-uniform priors, multiplicative Gaussian posteriors and local reparametrizations, thus allowing straightforward learning of the dropout rates.", "startOffset": 0, "endOffset": 21}, {"referenceID": 12, "context": "Similarly (Louizos & Welling, 2016) arrived at the same result through structured posterior approximations via matrix Gaussians and local reparametrizations (Kingma et al., 2015).", "startOffset": 157, "endOffset": 178}, {"referenceID": 26, "context": "It should also be mentioned that uncertainty estimation in neural networks can also be performed without the Bayesian paradigm; frequentist methods such as Bootstrap (Osband et al., 2016) and ensembles (Lakshminarayanan et al.", "startOffset": 166, "endOffset": 187}, {"referenceID": 17, "context": ", 2016) and ensembles (Lakshminarayanan et al., 2016) have shown that in certain scenarios they can provide reasonable confidence intervals.", "startOffset": 22, "endOffset": 53}, {"referenceID": 18, "context": "We used the LeNet 54 (LeCun et al., 1998) convolutional architecture with ReLU (Nair & Hinton, 2010) nonlinearities.", "startOffset": 21, "endOffset": 41}, {"referenceID": 8, "context": "The means M of the conditional Gaussian q(W|z) were initialized with the scheme proposed in (He et al., 2015), whereas the log of the variances were initialized by sampling from N (\u22129, 0.", "startOffset": 92, "endOffset": 109}, {"referenceID": 17, "context": "ble to the model used in (Lakshminarayanan et al., 2016), FFG to", "startOffset": 25, "endOffset": 56}, {"referenceID": 1, "context": "the Bayesian neural network employed in (Blundell et al., 2015),", "startOffset": 40, "endOffset": 63}, {"referenceID": 6, "context": "Ensembles use adversarial training (Goodfellow et al., 2014).", "startOffset": 35, "endOffset": 60}, {"referenceID": 17, "context": "5 for the dropout rate and for Deep Ensembles (Lakshminarayanan et al., 2016) we used 10 members and = .", "startOffset": 46, "endOffset": 77}, {"referenceID": 17, "context": "notMNIST To evaluate the predictive uncertainties of each model we performed the task described at (Lakshminarayanan et al., 2016); we estimated the entropy of the predictive distributions on notMNIST5 from the LeNet architectures trained on MNIST.", "startOffset": 99, "endOffset": 130}, {"referenceID": 17, "context": "Contrary to (Lakshminarayanan et al., 2016) we do not plot the histogram of the entropies across the images but we instead use the empirical CDF, which we think is more informative.", "startOffset": 12, "endOffset": 43}, {"referenceID": 17, "context": "Finally, whereas it was shown at (Lakshminarayanan et al., 2016) that Deep Ensembles provide good uncertainty estimates (better than Dropout) on this task using fully connected networks, this result did not seem to apply for the LeNet architecture we considered.", "startOffset": 33, "endOffset": 64}, {"referenceID": 35, "context": "We also measure how robust our models and uncertainties are against adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014) by generating examples using the fast sign method (Goodfellow et al.", "startOffset": 89, "endOffset": 136}, {"referenceID": 6, "context": "We also measure how robust our models and uncertainties are against adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014) by generating examples using the fast sign method (Goodfellow et al.", "startOffset": 89, "endOffset": 136}, {"referenceID": 6, "context": ", 2014) by generating examples using the fast sign method (Goodfellow et al., 2014) for each of the previously trained architectures using Cleverhans (Papernot et al.", "startOffset": 58, "endOffset": 83}, {"referenceID": 27, "context": ", 2014) for each of the previously trained architectures using Cleverhans (Papernot et al., 2016).", "startOffset": 74, "endOffset": 97}, {"referenceID": 17, "context": ", 2014; Gal & Ghahramani, 2015b) and Deep Ensembles (Lakshminarayanan et al., 2016) using convolutional architectures and found that MNFs achieve more realistic uncertainties while providing predictive capabilities on par with Dropout.", "startOffset": 52, "endOffset": 83}, {"referenceID": 12, "context": "One avenue would be to explore how much can MNFs sparsify and compress neural networks under either sparsity inducing priors, such as the log-uniform prior (Kingma et al., 2015; Molchanov et al., 2017), or empirical priors (Ullrich et al.", "startOffset": 156, "endOffset": 201}, {"referenceID": 36, "context": ", 2017), or empirical priors (Ullrich et al., 2017).", "startOffset": 29, "endOffset": 51}], "year": 2017, "abstractText": "We reinterpret multiplicative noise in neural networks as auxiliary random variables that augment the approximate posterior in a variational setting for Bayesian neural networks. We show that through this interpretation it is both efficient and straightforward to improve the approximation by employing normalizing flows (Rezende & Mohamed, 2015) while still allowing for local reparametrizations (Kingma et al., 2015) and a tractable lower bound (Ranganath et al., 2015; Maal\u00f8e et al., 2016). In experiments we show that with this new approximation we can significantly improve upon classical mean field for Bayesian neural networks on both predictive accuracy as well as predictive uncertainty.", "creator": "LaTeX with hyperref package"}}}