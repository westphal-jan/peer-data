{"id": "1703.01988", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Neural Episodic Control", "abstract": "Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.", "histories": [["v1", "Mon, 6 Mar 2017 17:23:27 GMT  (3234kb,D)", "http://arxiv.org/abs/1703.01988v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["alexander pritzel", "benigno uria", "sriram srinivasan", "adri\u00e0 puigdom\u00e8nech badia", "oriol vinyals", "demis hassabis", "daan wierstra", "charles blundell"], "accepted": true, "id": "1703.01988"}, "pdf": {"name": "1703.01988.pdf", "metadata": {"source": "META", "title": "Neural Episodic Control", "authors": ["Alexander Pritzel", "Benigno Uria", "Sriram Srinivasan", "Adri\u00e0 Puigdom\u00e8nech", "Oriol Vinyals", "Daan Wierstra", "Charles Blundell"], "emails": ["APRITZEL@GOOGLE.COM", "BURIA@GOOGLE.COM", "SRSRINIVASAN@GOOGLE.COM", "ADRIAP@GOOGLE.COM", "VINYALS@GOOGLE.COM", "DEMISHASSABIS@GOOGLE.COM", "WIERSTRA@GOOGLE.COM", "CBLUNDELL@GOOGLE.COM"], "sections": [{"heading": "1. Introduction", "text": "This year it is so far that it will only be a matter of time before it will be so far, until it will be so far."}, {"heading": "2. Deep Reinforcement Learning", "text": "The action value function of an enhanced learning agent (Sutton & Barto, 1998) is defined as Q\u03c0 (s, a) = E\u03c0 [\u2211 t \u03b3trt | s, a], where a is the first measure taken by the agent in the initial state, and the expectation is that the policy \u03c0 will be followed thereafter. Discount factor \u03b3 (0, 1) is about short- and long-term rewards. Agent Q-Network Agents (DQN; Mnih et al., 2015) uses Qlearning (Watkins & Dayan, 1992) to learn a value function Q (st, at), which is best for taking action in each state. The agent then executes a greedy policy based on this value function for trade-off exploration and exploitation: with probability, the agent chooses a random action that he chooses the action at = argmaxaQ, a)."}, {"heading": "3. Neural Episodic Control", "text": "Our agent consists of three components: a revolutionary neural network that processes pixel images, a set of memory modules (one per action), and a terminal network that converts read data from the action stores into Q (s, a) values. We use the same architecture as DQN for the revolutionary neural network (Mnih et al., 2015)."}, {"heading": "3.1. Differentiable Neural Dictionary", "text": "The memory module acts like an arbitrary mapping of keys to corresponding values, similar to the type of data found in programs in the dictionary. Therefore, we refer to this type of memory module as a differentiated neural dictionary (DND). There are two operations that are possible on a DND: looking up and writing, as shown in Figure 1. Performing a search on a DND forms a key h to an output value o: o = i wivi, (1) where vi is the ith element of the array Va and wi = k (h, hi) / i j k (h, hj) k (h, hj), (2), where hi is the ith element of the array d (x, y) a kernel between vectors x and y."}, {"heading": "3.2. Agent Architecture", "text": "Figure 2 shows a DND agent as part of the NEC agent for a single action. In practice, we use a uniform Q-Q method. Q-Q describes the general outline of the NEC algorithm. The pixel state s is processed by a revolutionary neural network to produce a key h. The key h is then used to search for a value from the DND, which generates weights in the process for each element of the memory arrays. The output is a weighted sum of values in the DND. The values in the DND, in the case of an NEC agent, are the Q values that correspond to the state that originally led to the corresponding key-value pair to be written into memory. Thus, this architecture produces an estimate of values in the DND. The architecture is replicated once for each action an agent can take, with the revolutionary part of the network shared by each separate DND Ma."}, {"heading": "3.4. Learning", "text": "The agent parameters are updated by minimizing the L2 loss between the predicted Q value for a given action and the Q (N) estimate on randomly sampled mini-batches from a replay buffer. Specifically, we store tuples (st, at, Rt) in the replay buffer, where N is the horizon of the N step Q rule, and Rt = Q (N) (st, a) plays the role of the target network seen in DQN (our replay buffer is significantly smaller than DQN), and these (st, at, Rt) tuples are then sampled evenly at random to form minibatches for training. Note that the architecture in Figure 2 is fully differentiable and we can minimize this loss by gradient drop."}, {"heading": "4. Experiments", "text": "We investigated whether neural control enables more efficient learning in practice. We investigated whether neural control enables more data in complex domains. We investigated whether neural control enables more efficient learning in practice in complex domains. We chose the Atari Learning Environment (ALE; Bellemare et al., 2013) as the problem domain. We tested our method on the 57 Atari games used in these domains by Schaul et al. (2015a), which form an interesting set of tasks, as they involve different challenges, such as the small number of rewards and vastly different orders of magnitude of results from A3C and DQN. Most common algorithms used in these domains, such as variants of DQN and A3C, require in the thousands of hours of play time, i.e. they are data inefficient and consider 5 variants of A3C and DQN as baseline and MEC (Blundell et al. We compare the basic Amentic and 3h)."}, {"heading": "5. Related work", "text": "There is much current work on storage architectures for neural networks (LSTM; Hochreiter & Schmidhuber, 1997), DNC (Graves et al., 2016), storage networks (Sukhbaatar et al., 2015; Miller et al., 2016), some of these models have been adapted for use in RL agents (LSTMs; Bakker et al., 2003; Hausknecht & Stone, 2015), DNCs (Graves et al.), storage networks (Oh et al., 2016), and the contents of these memories are typically reset at the beginning of each episode."}, {"heading": "6. Discussion", "text": "We have proposed Neural Episodic Control (NEC): a deep reinforcement learning agent that learns significantly faster than other baseline agents on a wide range of Atari 2600 games. At the core of NEC is a memory structure: a differentiated Neural Dictionary (DND), one for each potential action. NEC adds current state representations paired with corresponding value functions in the corresponding DND. Our experiments show that NEC requires an order of magnitude of fewer interactions with the environment than agents that were previously proposed for data efficiency, such as Prioritized Replay (Schaul et al., 2015b) and Retrace (Khalid et al., 2016). We speculate that NEC learns faster than three characteristics of the agent: the storage architecture (DND) that appreciates the use of N-step Q, and a state representation that is provided by a revolutionary neural network architecture."}], "references": [{"title": "Using fast weights to attend to the recent past", "author": ["Ba", "Jimmy", "Hinton", "Geoffrey E", "Mnih", "Volodymyr", "Leibo", "Joel Z", "Ionescu", "Catalin"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Ba et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2016}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Bellemare", "Marc G", "Naddaf", "Yavar", "Veness", "Joel", "Bowling", "Michael"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["Bentley", "Jon Louis"], "venue": "Commun. ACM,", "citeRegEx": "Bentley and Louis.,? \\Q1975\\E", "shortCiteRegEx": "Bentley and Louis.", "year": 1975}, {"title": "Model-free episodic control", "author": ["Blundell", "Charles", "Uria", "Benigno", "Pritzel", "Alexander", "Li", "Yazhe", "Ruderman", "Avraham", "Leibo", "Joel Z", "Rae", "Jack", "Wierstra", "Daan", "Hassabis", "Demis"], "venue": "arXiv preprint arXiv:1606.04460,", "citeRegEx": "Blundell et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Blundell et al\\.", "year": 2016}, {"title": "Rl: Fast reinforcement learning via slow reinforcement learning", "author": ["Duan", "Yan", "Schulman", "John", "Chen", "Xi", "Bartlett", "Peter L", "Sutskever", "Ilya", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1611.02779,", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Pathnet: Evolution channels gradient descent in super neural networks", "author": ["Fernando", "Chrisantha", "Banarse", "Dylan", "Blundell", "Charles", "Zwols", "Yori", "Ha", "David", "Rusu", "Andrei A", "Pritzel", "Alexander", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1701.08734,", "citeRegEx": "Fernando et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Fernando et al\\.", "year": 2017}, {"title": "Cbr for state value function approximation in reinforcement learning", "author": ["Gabel", "Thomas", "Riedmiller", "Martin"], "venue": "In International Conference on Case-Based Reasoning,", "citeRegEx": "Gabel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gabel et al\\.", "year": 2005}, {"title": "lambda) with off-policy corrections", "author": ["Harutyunyan", "Anna", "Bellemare", "Marc G", "Stepleton", "Tom", "Munos", "R\u00e9mi"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "Harutyunyan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Harutyunyan et al\\.", "year": 2016}, {"title": "Deep recurrent qlearning for partially observable mdps", "author": ["Hausknecht", "Matthew", "Stone", "Peter"], "venue": "arXiv preprint arXiv:1507.06527,", "citeRegEx": "Hausknecht et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hausknecht et al\\.", "year": 2015}, {"title": "Learning to play in a day: Faster deep reinforcement learning by optimality tightening", "author": ["He", "Frank S", "Liu", "Yang", "Schwing", "Alexander G", "Peng", "Jian"], "venue": "arXiv preprint arXiv:1611.01606,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Using fast weights to deblur old memories", "author": ["Hinton", "Geoffrey E", "Plaut", "David C"], "venue": "In Proceedings of the ninth annual conference of the Cognitive Science Society,", "citeRegEx": "Hinton et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1987}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Learning to remember rare", "author": ["Kaiser", "Lukasz", "Nachum", "Ofir", "Roy", "Aurko", "Bengio", "Samy"], "venue": null, "citeRegEx": "Kaiser et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kaiser et al\\.", "year": 2016}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "What learning systems do intelligent agents need? complementary learning systems theory updated", "author": ["Kumaran", "Dharshan", "Hassabis", "Demis", "McClelland", "James L"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "Kumaran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kumaran et al\\.", "year": 2016}, {"title": "Building machines that learn and think like people", "author": ["Lake", "Brenden M", "Ullman", "Tomer D", "Tenenbaum", "Joshua B", "Gershman", "Samuel J"], "venue": "arXiv preprint arXiv:1604.00289,", "citeRegEx": "Lake et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2016}, {"title": "Hippocampal contributions to control: The third way", "author": ["M. Lengyel", "P. Dayan"], "venue": "In NIPS,", "citeRegEx": "Lengyel and Dayan,? \\Q2007\\E", "shortCiteRegEx": "Lengyel and Dayan", "year": 2007}, {"title": "Catastrophic interference in connectionist networks: The sequential learning problem", "author": ["McCloskey", "Michael", "Cohen", "Neal J"], "venue": "Psychology of learning and motivation,", "citeRegEx": "McCloskey et al\\.,? \\Q1989\\E", "shortCiteRegEx": "McCloskey et al\\.", "year": 1989}, {"title": "Keyvalue memory networks for directly reading documents", "author": ["Miller", "Alexander", "Fisch", "Adam", "Dodge", "Jesse", "Karimi", "Amir-Hossein", "Bordes", "Antoine", "Weston", "Jason"], "venue": "arXiv preprint arXiv:1606.03126,", "citeRegEx": "Miller et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Badia", "Adria Puigdomenech", "Mirza", "Mehdi", "Graves", "Alex", "Lillicrap", "Timothy P", "Harley", "Tim", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Barycentric interpolators for continuous space and time reinforcement learning", "author": ["Munos", "Remi", "Moore", "Andrew W"], "venue": "In NIPS, pp", "citeRegEx": "Munos et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Munos et al\\.", "year": 1998}, {"title": "Safe and efficient off-policy reinforcement learning", "author": ["Munos", "R\u00e9mi", "Stepleton", "Tom", "Harutyunyan", "Anna", "Bellemare", "Marc"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Munos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Munos et al\\.", "year": 2016}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["Oh", "Junhyuk", "Guo", "Xiaoxiao", "Lee", "Honglak", "Lewis", "Richard L", "Singh", "Satinder"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Control of memory, active perception, and action in minecraft", "author": ["Oh", "Junhyuk", "Chockalingam", "Valliappa", "Lee", "Honglak"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Oh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2016}, {"title": "Deep exploration via bootstrapped dqn", "author": ["Osband", "Ian", "Blundell", "Charles", "Pritzel", "Alexander", "Van Roy", "Benjamin"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Osband et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2016}, {"title": "Incremental multi-step q-learning", "author": ["Peng", "Jing", "Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Peng et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Peng et al\\.", "year": 1996}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Experiments with reinforcement learning in problems with continuous state and action spaces", "author": ["Santamar\u0131\u0301a", "Juan C", "Sutton", "Richard S", "Ram", "Ashwin"], "venue": "Adaptive behavior,", "citeRegEx": "Santamar\u0131\u0301a et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Santamar\u0131\u0301a et al\\.", "year": 1997}, {"title": "Prioritized experience replay", "author": ["Schaul", "Tom", "Quan", "John", "Antonoglou", "Ioannis", "Silver", "David"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "Weston", "Jason", "Fergus", "Rob"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Sutton", "Richard S"], "venue": "Machine learning,", "citeRegEx": "Sutton and S.,? \\Q1988\\E", "shortCiteRegEx": "Sutton and S.", "year": 1988}, {"title": "Reinforcement learning: An introduction", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": "MIT press,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Lecture 6.5rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Tijmen", "Hinton", "Geoffrey"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Learning functions across many orders of magnitudes", "author": ["H. van Hasselt", "A. Guez", "M. Hessel", "D. Silver"], "venue": "ArXiv e-prints,", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Deep reinforcement learning with double q-learning", "author": ["Van Hasselt", "Hado", "Guez", "Arthur", "Silver", "David"], "venue": "In AAAI,", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["Vezhnevets", "Alexander", "Mnih", "Volodymyr", "Osindero", "Simon", "Graves", "Alex", "Vinyals", "Oriol", "Agapiou", "John"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vezhnevets et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vezhnevets et al\\.", "year": 2016}, {"title": "Matching networks for one shot learning", "author": ["Vinyals", "Oriol", "Blundell", "Charles", "Lillicrap", "Tim", "Wierstra", "Daan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2016}, {"title": "Learning to reinforcement learn", "author": ["Wang", "Jane X", "Kurth-Nelson", "Zeb", "Tirumala", "Dhruva", "Soyer", "Hubert", "Leibo", "Joel Z", "Munos", "Remi", "Blundell", "Charles", "Kumaran", "Dharshan", "Botvinick", "Matt"], "venue": "arXiv preprint arXiv:1611.05763,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Learning from delayed rewards", "author": ["Watkins", "Christopher John Cornish Hellaby"], "venue": "PhD thesis,", "citeRegEx": "Watkins and Hellaby.,? \\Q1989\\E", "shortCiteRegEx": "Watkins and Hellaby.", "year": 1989}], "referenceMentions": [{"referenceID": 20, "context": "Deep reinforcement learning agents have achieved state-ofthe-art results in a variety of complex environments (Mnih et al., 2015; 2016), often surpassing human performance (Silver et al.", "startOffset": 110, "endOffset": 135}, {"referenceID": 1, "context": "For example, in the Atari 2600 set of environments (Bellemare et al., 2013), deep Q-networks (Mnih et al.", "startOffset": 51, "endOffset": 75}, {"referenceID": 21, "context": ", 2013), deep Q-networks (Mnih et al., 2016) require more than 200 hours of gameplay in order to achieve scores similar to those", "startOffset": 25, "endOffset": 44}, {"referenceID": 16, "context": "a human player achieves after two hours (Lake et al., 2016).", "startOffset": 40, "endOffset": 59}, {"referenceID": 26, "context": "In this work we shall focus on addressing the three concerns listed above; we must note, however, that other recent advances in exploration (Osband et al., 2016), hierarchical reinforcement learning (Vezhnevets et al.", "startOffset": 140, "endOffset": 161}, {"referenceID": 37, "context": ", 2016), hierarchical reinforcement learning (Vezhnevets et al., 2016) and transfer learning (Rusu et al.", "startOffset": 45, "endOffset": 70}, {"referenceID": 6, "context": ", 2016) and transfer learning (Rusu et al., 2016; Fernando et al., 2017) also make substantial contributions to improving data efficiency in deep reinforcement learning over baseline agents.", "startOffset": 30, "endOffset": 72}, {"referenceID": 20, "context": ", stochastic gradient descent) as is the case with DQN (Mnih et al., 2015) and A3C (Mnih et al.", "startOffset": 55, "endOffset": 74}, {"referenceID": 21, "context": ", 2015) and A3C (Mnih et al., 2016).", "startOffset": 16, "endOffset": 35}, {"referenceID": 4, "context": "Our work is in part inspired by the hypothesised role of the Hippocampus in decision making (Lengyel & Dayan, 2007; Blundell et al., 2016) and also by recent work on one-shot learning (Vinyals et al.", "startOffset": 92, "endOffset": 138}, {"referenceID": 38, "context": ", 2016) and also by recent work on one-shot learning (Vinyals et al., 2016) and learning to remember rare events with neural networks (Kaiser et al.", "startOffset": 53, "endOffset": 75}, {"referenceID": 13, "context": ", 2016) and learning to remember rare events with neural networks (Kaiser et al., 2016).", "startOffset": 66, "endOffset": 87}, {"referenceID": 0, "context": "This helps alleviate the typically slow weight updates of stochastic gradient descent applied to the whole network and is reminiscent of work on fast weights (Ba et al., 2016; Hinton & Plaut, 1987), although the architecture we present is quite different.", "startOffset": 158, "endOffset": 197}, {"referenceID": 0, "context": "This helps alleviate the typically slow weight updates of stochastic gradient descent applied to the whole network and is reminiscent of work on fast weights (Ba et al., 2016; Hinton & Plaut, 1987), although the architecture we present is quite different. Another unique aspect of the memory is that unlike other memory architectures such as LSTM and the differentiable neural computer (DNC; Graves et al., 2016), our architecture does not try to learn when to write to memory, as this can be slow to learn and take a significant amount of time. Instead, we elect to write all experiences to the memory, and allow it to grow very large compared to existing memory architectures (in contrast to Oh et al. (2015); Graves et al.", "startOffset": 159, "endOffset": 711}, {"referenceID": 0, "context": "This helps alleviate the typically slow weight updates of stochastic gradient descent applied to the whole network and is reminiscent of work on fast weights (Ba et al., 2016; Hinton & Plaut, 1987), although the architecture we present is quite different. Another unique aspect of the memory is that unlike other memory architectures such as LSTM and the differentiable neural computer (DNC; Graves et al., 2016), our architecture does not try to learn when to write to memory, as this can be slow to learn and take a significant amount of time. Instead, we elect to write all experiences to the memory, and allow it to grow very large compared to existing memory architectures (in contrast to Oh et al. (2015); Graves et al. (2016) where the memory is wiped at the end of each episode).", "startOffset": 159, "endOffset": 733}, {"referenceID": 20, "context": "Deep Q-Network agents (DQN; Mnih et al., 2015) use Qlearning (Watkins & Dayan, 1992) to learn a value function Q(st, at) to rank which action at is best to take in each state st at step t.", "startOffset": 22, "endOffset": 46}, {"referenceID": 8, "context": "Several authors have proposed methods of improving reward propagation and the back up mechanism of Q learning (Harutyunyan et al., 2016; Munos et al., 2016; He et al., 2016) by incorporating on-policy rewards or by adding constraints to the optimisation.", "startOffset": 110, "endOffset": 173}, {"referenceID": 23, "context": "Several authors have proposed methods of improving reward propagation and the back up mechanism of Q learning (Harutyunyan et al., 2016; Munos et al., 2016; He et al., 2016) by incorporating on-policy rewards or by adding constraints to the optimisation.", "startOffset": 110, "endOffset": 173}, {"referenceID": 10, "context": "Several authors have proposed methods of improving reward propagation and the back up mechanism of Q learning (Harutyunyan et al., 2016; Munos et al., 2016; He et al., 2016) by incorporating on-policy rewards or by adding constraints to the optimisation.", "startOffset": 110, "endOffset": 173}, {"referenceID": 8, "context": "Q\u2217(\u03bb) (Harutyunyan et al., 2016) and Retrace(\u03bb) (Munos et al.", "startOffset": 6, "endOffset": 32}, {"referenceID": 23, "context": ", 2016) and Retrace(\u03bb) (Munos et al., 2016) change the form of the Q-learning target to incorporate on-policy samples and fluidly switch between on-policy learning and off-policy learning.", "startOffset": 23, "endOffset": 43}, {"referenceID": 8, "context": "Several authors have proposed methods of improving reward propagation and the back up mechanism of Q learning (Harutyunyan et al., 2016; Munos et al., 2016; He et al., 2016) by incorporating on-policy rewards or by adding constraints to the optimisation. Q\u2217(\u03bb) (Harutyunyan et al., 2016) and Retrace(\u03bb) (Munos et al., 2016) change the form of the Q-learning target to incorporate on-policy samples and fluidly switch between on-policy learning and off-policy learning. Munos et al. (2016) show that by incorporating on-policy samples allows an agent to learn faster in Atari environments, indicating that reward propagation is indeed a bottleneck to efficiency in deep reinforcement learning.", "startOffset": 111, "endOffset": 489}, {"referenceID": 21, "context": "A3C (Mnih et al., 2016) is another well known deep reinforcement learning algorithm that is very different from DQN.", "startOffset": 4, "endOffset": 23}, {"referenceID": 20, "context": "Interestingly, Mnih et al. (2016) also added an LSTM memory to the otherwise convolutional neural network architecture to give the agent a notion of memory, although this did not have significant impact on the performance on Atari games.", "startOffset": 15, "endOffset": 34}, {"referenceID": 20, "context": "For the convolutional neural network we use the same architecture as DQN (Mnih et al., 2015).", "startOffset": 73, "endOffset": 92}, {"referenceID": 38, "context": "It is also a generalisation to the memory and lookup schemes described in (Vinyals et al., 2016; Kaiser et al., 2016) for classification.", "startOffset": 74, "endOffset": 117}, {"referenceID": 13, "context": "It is also a generalisation to the memory and lookup schemes described in (Vinyals et al., 2016; Kaiser et al., 2016) for classification.", "startOffset": 74, "endOffset": 117}, {"referenceID": 4, "context": "Note that a DND is a differentiable version of the memory module described in Blundell et al. (2016). It is also a generalisation to the memory and lookup schemes described in (Vinyals et al.", "startOffset": 78, "endOffset": 101}, {"referenceID": 4, "context": "In Blundell et al. (2016), Monte Carlo returns were written to memory.", "startOffset": 3, "endOffset": 26}, {"referenceID": 1, "context": "As a problem domain we chose the Atari Learning Environment(ALE; Bellemare et al., 2013).", "startOffset": 59, "endOffset": 88}, {"referenceID": 1, "context": "As a problem domain we chose the Atari Learning Environment(ALE; Bellemare et al., 2013). We tested our method on the 57 Atari games used by Schaul et al. (2015a), which form an interesting set of tasks as they contain diverse challenges such as sparse rewards and vastly different magnitudes of scores across games.", "startOffset": 65, "endOffset": 163}, {"referenceID": 4, "context": "We consider 5 variants of A3C and DQN as baselines as well as MFEC (Blundell et al., 2016).", "startOffset": 67, "endOffset": 90}, {"referenceID": 21, "context": "We compare to the basic implementations of A3C (Mnih et al., 2016) and DQN (Mnih et al.", "startOffset": 47, "endOffset": 66}, {"referenceID": 20, "context": ", 2016) and DQN (Mnih et al., 2015).", "startOffset": 16, "endOffset": 35}, {"referenceID": 8, "context": "We also compare to two algorithms incorporating \u03bb returns (Sutton, 1988) aiming at more data efficiency by faster propagation of credit assignments, namely Q\u2217(\u03bb) (Harutyunyan et al., 2016) and Retrace(\u03bb) (Munos et al.", "startOffset": 162, "endOffset": 188}, {"referenceID": 23, "context": ", 2016) and Retrace(\u03bb) (Munos et al., 2016).", "startOffset": 23, "endOffset": 43}, {"referenceID": 25, "context": "We did not directly compare to DRQN (Hausknecht & Stone, 2015) nor FRMQN (Oh et al., 2016) as results were not available", "startOffset": 73, "endOffset": 90}, {"referenceID": 20, "context": "We apply the same preprocessing steps as (Mnih et al., 2015), including repeating each action four times.", "startOffset": 41, "endOffset": 60}, {"referenceID": 1, "context": "We picked the hyperparameter values that performed best on the median for this subset of games (a common cross validation procedure described by Bellemare et al. (2013), and adhered to by Mnih et al.", "startOffset": 145, "endOffset": 169}, {"referenceID": 1, "context": "We picked the hyperparameter values that performed best on the median for this subset of games (a common cross validation procedure described by Bellemare et al. (2013), and adhered to by Mnih et al. (2015)).", "startOffset": 145, "endOffset": 207}, {"referenceID": 31, "context": ", 2016), memory networks (Sukhbaatar et al., 2015; Miller et al., 2016)).", "startOffset": 25, "endOffset": 71}, {"referenceID": 19, "context": ", 2016), memory networks (Sukhbaatar et al., 2015; Miller et al., 2016)).", "startOffset": 25, "endOffset": 71}, {"referenceID": 25, "context": ", 2016), memory networks (Oh et al., 2016).", "startOffset": 25, "endOffset": 42}, {"referenceID": 39, "context": "RNNs can learn to quickly write highly rewarding states into memory and may even be able to learn entire reinforcement learning algorithms (Wang et al., 2016; Duan et al., 2016).", "startOffset": 139, "endOffset": 177}, {"referenceID": 5, "context": "RNNs can learn to quickly write highly rewarding states into memory and may even be able to learn entire reinforcement learning algorithms (Wang et al., 2016; Duan et al., 2016).", "startOffset": 139, "endOffset": 177}, {"referenceID": 24, "context": "The work of Oh et al. (2016) is also reminiscent of the ideas presented here.", "startOffset": 12, "endOffset": 29}, {"referenceID": 28, "context": "They also propose the use of the latent variable obtained from a variational autoencoder (Rezende et al., 2014) as an embedding space, but showed random projections often obtained better results.", "startOffset": 89, "endOffset": 111}, {"referenceID": 14, "context": "Kumaran et al. (2016) suggest that training on replayed experiences from the replay buffer in DQN is similar to the replay of experiences from episodic memory during sleep in animals.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": "Kumaran et al. (2016) suggest that training on replayed experiences from the replay buffer in DQN is similar to the replay of experiences from episodic memory during sleep in animals. DQN\u2019s replay buffer differs from most other work on memory for deep reinforcement learning in its sheer scale: it is common for DQN\u2019s replay buffer to hold millions of (s, a, r, s\u2032) tuples. The use of local regression techniques for Q-function approximation has been suggested before: Santamar\u0131\u0301a et al. (1997) proposed the use of k-nearest-neighbours regression with a heuristic for adding memories based on the distance to previous memories.", "startOffset": 0, "endOffset": 495}, {"referenceID": 14, "context": "Kumaran et al. (2016) suggest that training on replayed experiences from the replay buffer in DQN is similar to the replay of experiences from episodic memory during sleep in animals. DQN\u2019s replay buffer differs from most other work on memory for deep reinforcement learning in its sheer scale: it is common for DQN\u2019s replay buffer to hold millions of (s, a, r, s\u2032) tuples. The use of local regression techniques for Q-function approximation has been suggested before: Santamar\u0131\u0301a et al. (1997) proposed the use of k-nearest-neighbours regression with a heuristic for adding memories based on the distance to previous memories. Munos & Moore (1998) proposed barycentric interpolators to model the value function and proved their convergence to the optimal value function under mild conditions, but no empirical results were presented.", "startOffset": 0, "endOffset": 649}, {"referenceID": 14, "context": "Kumaran et al. (2016) suggest that training on replayed experiences from the replay buffer in DQN is similar to the replay of experiences from episodic memory during sleep in animals. DQN\u2019s replay buffer differs from most other work on memory for deep reinforcement learning in its sheer scale: it is common for DQN\u2019s replay buffer to hold millions of (s, a, r, s\u2032) tuples. The use of local regression techniques for Q-function approximation has been suggested before: Santamar\u0131\u0301a et al. (1997) proposed the use of k-nearest-neighbours regression with a heuristic for adding memories based on the distance to previous memories. Munos & Moore (1998) proposed barycentric interpolators to model the value function and proved their convergence to the optimal value function under mild conditions, but no empirical results were presented. Gabel & Riedmiller (2005) also suggested the use of local regression, under the paradigm of case-based-reasoning that included heuristics for the deletion of stored cases.", "startOffset": 0, "endOffset": 861}, {"referenceID": 23, "context": ", 2015b) and Retrace(\u03bb) (Munos et al., 2016).", "startOffset": 24, "endOffset": 44}], "year": 2017, "abstractText": "Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.", "creator": "LaTeX with hyperref package"}}}