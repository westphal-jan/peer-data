{"id": "1703.02527", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Online Learning to Rank in Stochastic Click Models", "abstract": "Online learning to rank is an important problem in machine learning, information retrieval, recommender systems, and display advertising. Many provably efficient algorithms have been developed for this problem recently, under specific click models. A click model is a model of how users click on a list of documents. Though these results are significant, the proposed algorithms have limited application because they are designed for specific click models, and do not have guarantees beyond them. To overcome this challenge, we propose a novel algorithm, which we call MergeRank, for learning to rank in a class of click models that satisfy mild technical assumptions. This class encompasses two most fundamental click models, the cascade and position-based models. We derive a gap-dependent upper bound on the expected $n$-step regret of MergeRank and evaluate it on web search queries. We observe that MergeRank performs better than ranked bandits and is more robust than CascadeKL-UCB, an existing algorithm for learning to rank in the cascade model.", "histories": [["v1", "Tue, 7 Mar 2017 18:53:58 GMT  (155kb,D)", "https://arxiv.org/abs/1703.02527v1", null], ["v2", "Tue, 20 Jun 2017 07:13:15 GMT  (157kb,D)", "http://arxiv.org/abs/1703.02527v2", "Proceedings of the 34th International Conference on Machine Learning"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["masrour zoghi", "tom\u00e1s tunys", "mohammad ghavamzadeh", "branislav kveton", "csaba szepesv\u00e1ri", "zheng wen"], "accepted": true, "id": "1703.02527"}, "pdf": {"name": "1703.02527.pdf", "metadata": {"source": "META", "title": "Online Learning to Rank in Stochastic Click Models", "authors": ["Masrour Zoghi", "Tomas Tunys", "Mohammad Ghavamzadeh", "Branislav Kveton", "Csaba Szepesvari", "Zheng Wen"], "emails": ["<kveton@adobe.com>,", "<masrour@zoghi.org>."], "sections": [{"heading": "1. Introduction", "text": "In fact, it is that it is a way in which most people are able to survive themselves, as if they see themselves able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves."}, {"heading": "2. Background", "text": "In this section, two basic click models (Chuklin et al., 2015) are considered, models of how users click on an ordered list of K-documents. The universe of all documents is represented by the base set D = [L] and we refer to the documents in D as items. The user is presented with a ranking, an ordered list of K-documents from L. We call this list R = (d1,.., dK), where \u041aK (D) and DK are the set of all K-tuples with different elements from D and dk the k-th item in R. We assume that the click model is parameterized by L item-dependent attraction probabilities \u03b1 [0, 1] L, where \u03b1 (d) is the probability that item d is attractive. The items attract the user independently. For the sake of simplicity and without loss of universality, we assume that \u03b1 (1) \u2265.. The reviewed models differ in how the user examines the clicks."}, {"heading": "2.1. Position-Based Model", "text": "The position-based model (PBM) (Richardson et al., 2007) is a model in which the probability of clicking on an element depends on both its identity and its position. Therefore, in addition to the item-dependent attraction probabilities, the PBM is parameterized by the K position-dependent examination probability (K), where \"K\" means the examination probability of the position k. The user interacts with a list of items R = (d1,..., dK) as follows. The user examines the position k [K] with probability (k) and then clicks on the item dk at that position with probability \u03b1 (dk), thus generating the expected number of clicks on ListR (R) = K \u00b2 K = K \u00b2 (k) \u03b1 (dk). In practice, it is often observed that \"K\" is the satisfaction of the user (K)."}, {"heading": "2.2. Cascade Model", "text": "In the Cascade Model (CM) (Craswell et al., 2008), the user scans a list of items R = (d1,.., dK) from the first item d1 to the last dK. If item dk is attractive, the user clicks on it and does not check the remaining items. If item dk is unattractive, the user checks items dk + 1. The first item d1 is likely to be one.The definition of the model shows that the probability that item dk is examined corresponds to the probability that none of the first k \u2212 1 items is attractive. Since each item attracts the user independently, this probability is equivalent to the probability that item dk (R, k) = k \u2212 1 (1 \u2212 \u03b1 (di))). (2) The expected number of clicks on list R is at most 1 and corresponds to the probability that the items are optimal."}, {"heading": "3. Online Learning to Rank in Click Models", "text": "The PBM and the CM (Section 2) are similar in many ways: first, both models are parameterized by L-item-dependent attraction probabilities; the items attract the user independently of each other; second, the probability of clicking on the item is a product of its attraction probability, which depends on the item's identity; and the investigative probability of its position, which is independent of the item's identity; and finally, the optimal solution in both models is the list of K-most attractive items in (1), in which the k-th most attractive item is positioned, suggesting that the optimal solution in both models can be learned by a single learning algorithm that does not know the underlying model; we propose this algorithm in Section 4. Before discussing it, we formalize our learning problem as a multi-armed bandit (Auer et al., 2002; Lai & Robbins, 1985)."}, {"heading": "3.1. Stochastic Click Bandit", "text": "We refer to our learning problem as a stochastic click-bandit. One instance of this problem is a tuple (K, L, P, P), where K is the number of items, L is the number of items, P\u03b1 is a distribution via binary vectors {0, 1} L, and Pa is a distribution via binary matrices {0, 1} K (D). The learning agent interacts with our problem as follows: Let (At, Xt) Tt = 1 be T i.e. Random variables drawn from P\u03b1, where An (D) is the attraction indicator of item d at the time t; and Xt (0, 1).K (D) \u00b7 K andXt (R, k) is the examination indicator of item k in list R."}, {"heading": "3.2. Position Bandit", "text": "The learning variant of PBM in Section 2.1 can be formulated in our setting if the probability of clicking item dtk is defined at the time t isE [ct (k) | Rt] = egg (k) \u03b1 (dt), with Section 2.1. defining the expected reward of the list Rt at the time t isE [rt | Rt] = K \u2211 k = 1 \u0430 (k) \u03b1 (dt)."}, {"heading": "3.3. Cascading Bandit", "text": "The learning variant of CM in Section 2.2 can be formulated in our setting as follows: whenXt (R, k) = k \u2212 1 \u0441i = 1 (1 \u2212 At (di)) (5) for each list R-K (D) and position k-K [K]. Under this assumption, the probability of clicking item dtk at the time t is t isE [ct (k) | Rt] = [k \u2212 1 \u0441i = 1 (1 \u2212 \u03b1 (dti))] \u03b1 (dtk).The expected reward of list Rt at the time t is E [rt | Rt] = K \u0445 k = 1 [k \u2212 1 \u0441i = 1 (1 \u2212 \u03b1 (dti))] \u03b1 (dtk)."}, {"heading": "3.4. Additional Assumptions", "text": "The above assumptions are not sufficient to guarantee that the optimal list R (1) is learnable. Therefore, we make four additional assumptions that are entirely natural. Adoption 3 (independent audit). Adoption 3 (independent audit). Adoption 3 (independent audit). Adoption 3 (independent audit). Adoption 3 (independent audit). Adoption 3 (independent audit). Adoption 3 (independent audit). Adoption 4 (regular audit). Adoption 4 (regular audit). Adoption 4 (regular audit). Adoption 4 (regular audit). Adoption 4 (regular audit). Adoption 4 (regular audit). Adoption 4 (regular audit). Adoption 4 (regular audit). Adoption 4 (regular audit). Adoption 4 (regular audit). Adoption 4 (regular audit). Adoption 4 (regular audit)."}, {"heading": "19: `b \u2190 `b + 1", "text": "20: otherwise, if s > 0, then 21: / / Split 22: A \u00b7 {bmax + 1, bmax + 2}\\ {b} 23: Ibmax + 1 \u2190 (Ib (1), Ib (1) + s \u2212 1) 24: Bbmax + 1.0 \u2190 B + s, 'bmax + 1 \u2190 0 25: Ibmax + 2 \u2190 (Ib (1) + s, Ib (2))) 26: Bbmax + 2.0 \u2190 B \u2212 s,' bmax + 2 \u2190 0 27: bmax \u2190 bmax + 2 of this batch. Finally, if Ib (2) < K, the number of articles in batch b corresponds to the number of their positions."}, {"heading": "5. Analysis", "text": "In this section, we regret BatchRank. Before doing so, we discuss our estimator of the clicks in (6). In particular, we show that (6) is the attraction probability of item d, scaled by the average examination probability at the stage \"of Lot B. The examination scale maintains the order of the attraction probabilities, and therefore BatchRank can operate with (6) instead of \u03b1 (d)."}, {"heading": "5.1. Confidence Radii", "text": "Attach batch b, positions Ib, stage b and positions Bb. \"Then we can use the estimator for each point d-Bb\" in (6) asc-b, \"(d) = 1n\" \u2211 t-T Ib (2) \u2211 k = Ib (1) ct (k) 1 {dtk = d} (7) for some n \"time steps T and its expected value isc-b,\" (d) = E [c-b, \"(d)]. (8) The key step in designing BatchRank is that we maintain confidence radii around (7). This is solid because the observations in (7), {Xt (Rt, k) At (d) t-t-T: dtk = d} (9) at each position k, i.i.d. in time. More precisely, by designing DisplayBatch, all displayed positions from batch b are randomly selected."}, {"heading": "5.2. Correct Examination Scaling", "text": "Since checking a position does not depend on the order of higher positions and does not depend at all on lower positions (assumption 3), we can express the probability that we click on any point d-Bb (R, k) 1 {dk = k}, (10) where Sd = {(e1,.., eIb (2): d \u00b2 eIb (1),.. eIb (2), (eIb (1) 1 {dk = k}, (10), where Sd = {(e1,., eIb (2): d \u00b2 eIb (1),., eIb (2), (eIb (1))), eIb (2)), eIb (2)))), eIb (2)), eIb (2), eIlen (b), (b \u00b2 d), the set of all lists with permutations of Bb \u00b2, \"on positions containing d \u00b2, some positions d \u00b2, b \u00b2, b \u00b2, b \u00b2, b, high."}, {"heading": "5.3. Regret Bound", "text": "For simplicity's sake, \u03b1 (1) >. > \u03b1 (L) > 0. Leave \u03b1max = \u03b1 (1), and \u0432 (k) = \u0432 (R *, k) for all k [K]. BatchRank regret is limited below. Theorem 1. For each stochastic click bandit in Section 3.1 that meets assumptions 1 to 6 and T \u2265 5, the expected T-step regret of the BatchRank is limited to R (T) \u2264 192K 3L (1 \u2212 \u03b1max) \u2206 min log T + 4KL (3e + K), where \u2206 min = mink [K] {\u03b1 (k) \u2212 \u03b1 (k + 1)}.Proof. The key idea is to tie the expected T-step regret in each batch (Lemma 7 in the appendix). Since the number of batches is not more than 2K, the regret of the BatchRank is at most 2K times greater than that of the batch."}, {"heading": "5.4. Discussion", "text": "Our upper limit in Theorem 1 is logarithmic in the number of steps T, linear in the number of points L and polynomic in the number of positions K. To the best of our knowledge, this is the first gap-dependent upper limit in terms of regretting a learning algorithm that exhibits a sublinear regret in both CM and PBM. The gap is characteristic of the hardness of sorting K + 1 most attractive points that is sufficient to solve our problem. In practice, the maximum attraction probability is \u03b1max of 1. Therefore, the dependence on (1 \u2212 \u03b1max) \u2212 1 is not decisive. In most queries in Section 6, \u03b1max \u2212 0.9, we believe that the cubic dependence on K is not far from being optimal. Consider in particular the problem of learning the most frequently clicked positions in PBM (Section 2.1), which is easier than our problem. This problem can be solved as stostic ranking."}, {"heading": "6. Experiments", "text": "We are experimenting with the Yandex Dataset (Yandex), a dataset of 35 million (M) searches, each of which can contain multiple searches. Each query is associated with displayed documents at positions 1 to 10 and their clicks. We select 60 frequent searches and learn their CMs and PBMs using PyClick (Chuklin et al., 2015), which is an open source library of click models for web search. In each query, our goal will be to examine unattractive articles with the aim of maximizing the expected number of clicks at the first K = 5 positions, similar to a real-world environment where the learning agent would only be allowed to search for highly attractive articles and is not allowed to research unattractive articles (Zoghi et al, 2016).BatchRank is compared with two methods, CascadeKL-UCB-UCB."}, {"heading": "7. Related Work", "text": "A popular approach to ranking online learning is the ranking bandits (Radlinski et al., 2008a; Slivkins et al., 2013). The key idea among ranking bandits is to model each position in the recommended list as an individual bandit problem, which is then solved by a simple bandit algorithm, which is typically counterproductive (Auer et al., 1995) because the distribution of clicks to lower positions is influenced by higher positions. We compare the ranking of bandits in Section 6. Online learning in click models (Craswell et al., 2008; Chuklin et al., 2015) has recently been investigated in several papers (Craklin et al., 2015a; Combes et al., 2015; Chuveton et al., 2015b; Katariya et al., 2016; Zong et al., 2016; Li et al., 2016; Sampling."}, {"heading": "8. Conclusions", "text": "We propose stochastic click bandits, a framework for online learning, to classify it into a broad class of click models that includes two basic click models, cascade and position-based models. In addition, we propose a mathematically and randomly efficient algorithm to solve our problems, BatchRank, and derive a cap on its T-step regret. Finally, we evaluate BatchRank on web search queries. Our algorithm outperforms ranked bandits (Radlinski et al., 2008a), a popular online learning-to-rank approach, and is more robust than CascadeKL-UCB (Kveton et al., 2015a), an existing algorithm for online learning to rank in the cascade model. The goal of this work is not to suggest the optimal algorithm for our setting, but to show that online learning is possible to guarantee in multiple click models, with theoretical guarantees."}, {"heading": "A. Notation", "text": "Symbol definition \u03b1 (d) attraction probability of item d \u03b1max Highest attraction probability, \u03b1 (1) A binary attraction vector, where A (d) is the attraction indicator of item d P\u03b1 distribution via binary attraction vectors. (d) Number of observed clicks on item d in stage \"testing item b c b,\" (d) Estimated probability of clicking on item d in stage \"testing item b,\" (d) Number of observed clicks on item d in stage \"testing item b,\" (d) Estimated probability of clicking on item d in stage \"testing item T,\" \"probability of clicking on item d in stage\" testing item b, \"E [c] D Ground set of items [L] of such items (1)."}, {"heading": "B. Proof of Theorem 1", "text": "Let Rb, \"the stochastic regret related to stage\" of group B. Then the expected T-step regret of the MergeRank can be dissected because the maximum number of batches is 2K. Letter b, \"(d) = c-b,\" (d) \u03b1 (d), (12) is the average check probability of point d at stage \"of group B. Let Eb,\" = {Event 1: \"d-Bb,\" \": c event,\" (d), (Lb), Ub, \"(d)], Event 2:\" Ib event [K] 2, d-Bb, \"d event Bb,\" d event \"Bb,\" \"d event\" K] s.t. (K] s.t. (K] s.t. (K event): \"Event (d) > 0: n-16K event (Ib) \u2212 d) (1 \u2212 max."}, {"heading": "C. Upper Bound on the Probability of Bad Event E", "text": "The probability that an event will not occur in Eb \"is limited as follows:\" Fix Ib and Bb, \"\" For each d, \"\" P (c), P (c), P (c), P (c), P (c), P (c), Bb, \"P (c), P (c), Bb,\" P (c), Bb, \"P (c), B,\" P (c), B, P (c), B, \"P (c), B,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"\" D, \"D,\" \"D,\" \"D,\" D, \"D."}, {"heading": "D. Upper Bound on the Regret in Individual Batches", "text": "The proof comes from two observations: First (assumption 6, assumption (k) is the lowest probability for position k '. Second, point d becomes at position k with the probability of at least 1 / K.Lemma 4. Let event E pass (assumption 6, assumption (k) the lowest probability for position k'. Second, point d becomes at position k with the probability of at least 1 / K.Lemma 4. Let event E pass (assumption 6, assumption (k) the lowest probability for position b '."}, {"heading": "E. Technical Lemmas", "text": "The other claims result from symmetrical claims. From the inequality (2,1) of Hoeffding (1963) it follows that we have the following assumptions: \"We have no evidence that the clues (2,1) of Hoeffding (1963), we have no evidence that the clues (2,1) of Hoeffding (1963), we have the clues (2,1) of Hoeffding (2,1), we have the clues (2,2) of Hoffding (2,1), we have the clues (2,2) of Hoffding (2,1), we have the clues (2,0) and the clues (2,1) of Hoff (2,1)."}], "references": [{"title": "Improving web search ranking by incorporating user behavior information", "author": ["Agichtein", "Eugene", "Brill", "Eric", "Dumais", "Susan"], "venue": "In Proceedings of the 29th Annual International ACM SIGIR Conference,", "citeRegEx": "Agichtein et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Agichtein et al\\.", "year": 2006}, {"title": "Asymptotically efficient adaptive allocation schemes for controlled i.i.d. processes: Finite parameter space", "author": ["Agrawal", "Rajeev", "Teneketzis", "Demosthenis", "Anantharam", "Venkatachalam"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Agrawal et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 1989}, {"title": "Gambling in a rigged casino: The adversarial multi-armed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Freund", "Yoav", "Schapire", "Robert"], "venue": "In Proceedings of the 36th Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Auer et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Auer et al\\.", "year": 1995}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Fischer", "Paul"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Partial monitoring with side information", "author": ["Bartok", "Gabor", "Szepesvari", "Csaba"], "venue": "In Proceedings of the 23rd International Conference on Algorithmic Learning Theory, pp", "citeRegEx": "Bartok et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bartok et al\\.", "year": 2012}, {"title": "An adaptive algorithm for finite stochastic partial monitoring", "author": ["Bartok", "Gabor", "Zolghadr", "Navid", "Szepesvari", "Csaba"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Bartok et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bartok et al\\.", "year": 2012}, {"title": "Partial monitoring - classification, regret bounds, and algorithms", "author": ["Bartok", "Gabor", "Foster", "Dean", "Pal", "David", "Rakhlin", "Alexander", "Szepesvari", "Csaba"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Bartok et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bartok et al\\.", "year": 2014}, {"title": "Modeling contextual factors of click rates", "author": ["Becker", "Hila", "Meek", "Christopher", "Chickering", "David Maxwell"], "venue": "In Proceedings of the 22nd AAAI Conference on Artificial Intelligence,", "citeRegEx": "Becker et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Becker et al\\.", "year": 2007}, {"title": "A dynamic Bayesian network click model for web search ranking", "author": ["Chapelle", "Olivier", "Zhang", "Ya"], "venue": "In Proceedings of the 18th International Conference on World Wide Web,", "citeRegEx": "Chapelle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2009}, {"title": "Click Models for Web Search", "author": ["Chuklin", "Aleksandr", "Markov", "Ilya", "de Rijke", "Maarten"], "venue": null, "citeRegEx": "Chuklin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chuklin et al\\.", "year": 2015}, {"title": "Learning to rank: Regret lower bounds and efficient algorithms", "author": ["Combes", "Richard", "Magureanu", "Stefan", "Proutiere", "Alexandre", "Laroche", "Cyrille"], "venue": "In Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Sys-", "citeRegEx": "Combes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Combes et al\\.", "year": 2015}, {"title": "An experimental comparison of click positionbias models", "author": ["Craswell", "Nick", "Zoeter", "Onno", "Taylor", "Michael", "Ramsey", "Bill"], "venue": "In Proceedings of the 1st ACM International Conference on Web Search and Data Mining,", "citeRegEx": "Craswell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Craswell et al\\.", "year": 2008}, {"title": "The KL-UCB algorithm for bounded stochastic bandits and beyond", "author": ["Garivier", "Aurelien", "Cappe", "Olivier"], "venue": "In Proceeding of the 24th Annual Conference on Learning Theory, pp", "citeRegEx": "Garivier et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Garivier et al\\.", "year": 2011}, {"title": "A comparative study of click models for web search", "author": ["Grotov", "Artem", "Chuklin", "Aleksandr", "Markov", "Ilya", "Stout", "Luka", "Xumara", "Finde", "de Rijke", "Maarten"], "venue": "In Proceedings of the 6th International Conference of the CLEF Association,", "citeRegEx": "Grotov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grotov et al\\.", "year": 2015}, {"title": "Click chain model in web search", "author": ["Guo", "Fan", "Liu", "Chao", "Kannan", "Anitha", "Minka", "Tom", "Taylor", "Michael", "Wang", "Yi Min", "Faloutsos", "Christos"], "venue": "In Proceedings of the 18th International Conference on World Wide Web,", "citeRegEx": "Guo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2009}, {"title": "Efficient multipleclick models in web search", "author": ["Guo", "Fan", "Liu", "Chao", "Wang", "Yi Min"], "venue": "In Proceedings of the 2nd ACM International Conference on Web Search and Data Mining, pp", "citeRegEx": "Guo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2009}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["Hoeffding", "Wassily"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoeffding and Wassily.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding and Wassily.", "year": 1963}, {"title": "Reusing historical interaction data for faster online learning to rank for IR", "author": ["Hofmann", "Katja", "Schuth", "Anne", "Whiteson", "Shimon", "de Rijke", "Maarten"], "venue": "In Proceedings of the 6th ACM International Conference on Web Search and Data Mining,", "citeRegEx": "Hofmann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hofmann et al\\.", "year": 2013}, {"title": "DCM bandits: Learning to rank with multiple clicks", "author": ["Katariya", "Sumeet", "Kveton", "Branislav", "Szepesvari", "Csaba", "Wen", "Zheng"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "Katariya et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Katariya et al\\.", "year": 2016}, {"title": "Bernoulli rank-1 bandits for click feedback", "author": ["Katariya", "Sumeet", "Kveton", "Branislav", "Szepesvari", "Csaba", "Vernade", "Claire", "Wen", "Zheng"], "venue": "In Proceedings of the 26th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Katariya et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Katariya et al\\.", "year": 2017}, {"title": "Stochastic rank-1 bandits", "author": ["Katariya", "Sumeet", "Kveton", "Branislav", "Szepesvari", "Csaba", "Vernade", "Claire", "Wen", "Zheng"], "venue": "In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Katariya et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Katariya et al\\.", "year": 2017}, {"title": "Cascading bandits: Learning to rank in the cascade model", "author": ["Kveton", "Branislav", "Szepesvari", "Csaba", "Wen", "Zheng", "Ashkan", "Azin"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Kveton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kveton et al\\.", "year": 2015}, {"title": "Combinatorial cascading bandits", "author": ["Kveton", "Branislav", "Wen", "Zheng", "Ashkan", "Azin", "Szepesvari", "Csaba"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Kveton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kveton et al\\.", "year": 2015}, {"title": "Multiple-play bandits in the position-based model", "author": ["Lagree", "Paul", "Vernade", "Claire", "Cappe", "Olivier"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Lagree et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lagree et al\\.", "year": 2016}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "Robbins", "Herbert"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Lai et al\\.", "year": 1985}, {"title": "Contextual combinatorial cascading bandits", "author": ["Li", "Shuai", "Wang", "Baoxiang", "Zhang", "Shengyu", "Chen", "Wei"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Learning to Rank for Information", "author": ["Liu", "Tie-Yan"], "venue": null, "citeRegEx": "Liu and Tie.Yan.,? \\Q2011\\E", "shortCiteRegEx": "Liu and Tie.Yan.", "year": 2011}, {"title": "Learning diverse rankings with multi-armed bandits", "author": ["Radlinski", "Filip", "Kleinberg", "Robert", "Joachims", "Thorsten"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Radlinski et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Radlinski et al\\.", "year": 2008}, {"title": "How does clickthrough data reflect retrieval quality", "author": ["Radlinski", "Filip", "Kurup", "Madhu", "Joachims", "Thorsten"], "venue": "In Proceedings of the 17th ACM Conference on Information and Knowledge Management, pp", "citeRegEx": "Radlinski et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Radlinski et al\\.", "year": 2008}, {"title": "Predicting clicks: Estimating the click-through rate for new ads", "author": ["Richardson", "Matthew", "Dominowska", "Ewa", "Ragno", "Robert"], "venue": "In Proceedings of the 16th International Conference on World Wide Web,", "citeRegEx": "Richardson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2007}, {"title": "Ranked bandits in metric spaces: Learning diverse rankings over large document collections", "author": ["Slivkins", "Aleksandrs", "Radlinski", "Filip", "Gollapudi", "Sreenivas"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Slivkins et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Slivkins et al\\.", "year": 2013}, {"title": "Interactively optimizing information retrieval systems as a dueling bandits problem", "author": ["Yue", "Yisong", "Joachims", "Thorsten"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "Yue et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2009}, {"title": "Cascading bandits for large-scale recommendation problems", "author": ["Zong", "Shi", "Ni", "Hao", "Sung", "Kenny", "Ke", "Nan Rosemary", "Wen", "Zheng", "Kveton", "Branislav"], "venue": "In Proceedings of the 32nd Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Zong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zong et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "These methods can be divided into two groups: learning the best ranker in a family of rankers (Yue & Joachims, 2009; Hofmann et al., 2013), and learning the best list under some model of user interaction with the list (Radlinski et al.", "startOffset": 94, "endOffset": 138}, {"referenceID": 30, "context": ", 2013), and learning the best list under some model of user interaction with the list (Radlinski et al., 2008a; Slivkins et al., 2013), such as a click model (Chuklin et al.", "startOffset": 87, "endOffset": 135}, {"referenceID": 9, "context": ", 2013), such as a click model (Chuklin et al., 2015).", "startOffset": 31, "endOffset": 53}, {"referenceID": 10, "context": "More precisely, many algorithms have been proposed and analyzed for finding the optimal ranked list in the cascade model (CM) (Kveton et al., 2015a; Combes et al., 2015; Kveton et al., 2015b; Zong et al., 2016; Li et al., 2016), the dependent-click model (DCM) (Katariya et al.", "startOffset": 126, "endOffset": 227}, {"referenceID": 32, "context": "More precisely, many algorithms have been proposed and analyzed for finding the optimal ranked list in the cascade model (CM) (Kveton et al., 2015a; Combes et al., 2015; Kveton et al., 2015b; Zong et al., 2016; Li et al., 2016), the dependent-click model (DCM) (Katariya et al.", "startOffset": 126, "endOffset": 227}, {"referenceID": 25, "context": "More precisely, many algorithms have been proposed and analyzed for finding the optimal ranked list in the cascade model (CM) (Kveton et al., 2015a; Combes et al., 2015; Kveton et al., 2015b; Zong et al., 2016; Li et al., 2016), the dependent-click model (DCM) (Katariya et al.", "startOffset": 126, "endOffset": 227}, {"referenceID": 18, "context": ", 2016), the dependent-click model (DCM) (Katariya et al., 2016), and the position-based model (PBM) (Lagree et al.", "startOffset": 41, "endOffset": 64}, {"referenceID": 23, "context": ", 2016), and the position-based model (PBM) (Lagree et al., 2016).", "startOffset": 44, "endOffset": 65}, {"referenceID": 13, "context": "This is a grave issue because it is well known that no single click model captures the behavior of an entire population of users (Grotov et al., 2015).", "startOffset": 129, "endOffset": 150}, {"referenceID": 29, "context": "We make the following contributions: \u2022 We propose stochastic click bandits, a learning framework for maximizing the expected number of clicks in online LTR in a broad class of click models, which includes both the PBM (Richardson et al., 2007) and CM (Craswell et al.", "startOffset": 218, "endOffset": 243}, {"referenceID": 11, "context": ", 2007) and CM (Craswell et al., 2008).", "startOffset": 15, "endOffset": 38}, {"referenceID": 9, "context": "This section reviews two fundamental click models (Chuklin et al., 2015), models of how users click on an ordered list of K documents.", "startOffset": 50, "endOffset": 72}, {"referenceID": 29, "context": "The position-based model (PBM) (Richardson et al., 2007) is a model where the probability of clicking on an item depends on both its identity and position.", "startOffset": 31, "endOffset": 56}, {"referenceID": 9, "context": "\u2265 \u03c7(K) (Chuklin et al., 2015), and we adopt this assumption in this work.", "startOffset": 7, "endOffset": 29}, {"referenceID": 11, "context": "In the cascade model (CM) (Craswell et al., 2008), the user scans a list of items R = (d1, .", "startOffset": 26, "endOffset": 49}, {"referenceID": 3, "context": "Before we discuss it, we formalize our learning problem as a multi-armed bandit (Auer et al., 2002; Lai & Robbins, 1985).", "startOffset": 80, "endOffset": 120}, {"referenceID": 18, "context": "This problem can be solved as a stochastic rank-1 bandit (Katariya et al., 2017b) by Rank1Elim. Now consider the following PBM. The examination probability of the first position is close to one and the examination probabilities of all other positions are close to zero. Then the T -step regret of Rank1Elim is O([K + K2L\u2206\u22121 min] log T ) because \u03bc = O(1/K), where \u03bc is defined in Katariya et al. (2017b). Note that the gapdependent term nearly matches our upper bound.", "startOffset": 58, "endOffset": 403}, {"referenceID": 9, "context": "We select 60 frequent search queries, and learn their CMs and PBMs using PyClick (Chuklin et al., 2015), which is an open-source library of click models for web search.", "startOffset": 81, "endOffset": 103}, {"referenceID": 2, "context": "RankedExp3 is a variant of ranked bandits (Section 7) where the base bandit algorithm is Exp3 (Auer et al., 1995).", "startOffset": 94, "endOffset": 113}, {"referenceID": 30, "context": "A popular approach to online learning to rank are ranked bandits (Radlinski et al., 2008a; Slivkins et al., 2013).", "startOffset": 65, "endOffset": 113}, {"referenceID": 2, "context": "This algorithm is typically adversarial (Auer et al., 1995) because the distribution of clicks on lower positions is affected by higher positions.", "startOffset": 40, "endOffset": 59}, {"referenceID": 11, "context": "Online learning to rank in click models (Craswell et al., 2008; Chuklin et al., 2015) was recently studied in several papers (Kveton et al.", "startOffset": 40, "endOffset": 85}, {"referenceID": 9, "context": "Online learning to rank in click models (Craswell et al., 2008; Chuklin et al., 2015) was recently studied in several papers (Kveton et al.", "startOffset": 40, "endOffset": 85}, {"referenceID": 10, "context": ", 2015) was recently studied in several papers (Kveton et al., 2015a; Combes et al., 2015; Kveton et al., 2015b; Katariya et al., 2016; Zong et al., 2016; Li et al., 2016; Lagree et al., 2016).", "startOffset": 47, "endOffset": 192}, {"referenceID": 18, "context": ", 2015) was recently studied in several papers (Kveton et al., 2015a; Combes et al., 2015; Kveton et al., 2015b; Katariya et al., 2016; Zong et al., 2016; Li et al., 2016; Lagree et al., 2016).", "startOffset": 47, "endOffset": 192}, {"referenceID": 32, "context": ", 2015) was recently studied in several papers (Kveton et al., 2015a; Combes et al., 2015; Kveton et al., 2015b; Katariya et al., 2016; Zong et al., 2016; Li et al., 2016; Lagree et al., 2016).", "startOffset": 47, "endOffset": 192}, {"referenceID": 25, "context": ", 2015) was recently studied in several papers (Kveton et al., 2015a; Combes et al., 2015; Kveton et al., 2015b; Katariya et al., 2016; Zong et al., 2016; Li et al., 2016; Lagree et al., 2016).", "startOffset": 47, "endOffset": 192}, {"referenceID": 23, "context": ", 2015) was recently studied in several papers (Kveton et al., 2015a; Combes et al., 2015; Kveton et al., 2015b; Katariya et al., 2016; Zong et al., 2016; Li et al., 2016; Lagree et al., 2016).", "startOffset": 47, "endOffset": 192}, {"referenceID": 1, "context": "General partial-monitoring algorithms (Agrawal et al., 1989; Bartok et al., 2012; Bartok & Szepesvari, 2012; Bartok et al., 2014) are unsuitable for our setting because their computational complexity is polynomial in the number of actions, which is exponential in K.", "startOffset": 38, "endOffset": 129}, {"referenceID": 4, "context": "General partial-monitoring algorithms (Agrawal et al., 1989; Bartok et al., 2012; Bartok & Szepesvari, 2012; Bartok et al., 2014) are unsuitable for our setting because their computational complexity is polynomial in the number of actions, which is exponential in K.", "startOffset": 38, "endOffset": 129}, {"referenceID": 6, "context": "General partial-monitoring algorithms (Agrawal et al., 1989; Bartok et al., 2012; Bartok & Szepesvari, 2012; Bartok et al., 2014) are unsuitable for our setting because their computational complexity is polynomial in the number of actions, which is exponential in K.", "startOffset": 38, "endOffset": 129}, {"referenceID": 9, "context": "The click model is a model of how the user interacts with a list of documents (Chuklin et al., 2015), and many such models have been proposed (Becker et al.", "startOffset": 78, "endOffset": 100}, {"referenceID": 11, "context": "Two fundamental click models are the CM (Craswell et al., 2008) and PBM (Richardson et al.", "startOffset": 40, "endOffset": 63}, {"referenceID": 29, "context": ", 2008) and PBM (Richardson et al., 2007).", "startOffset": 16, "endOffset": 41}], "year": 2017, "abstractText": "Online learning to rank is a core problem in information retrieval and machine learning. Many provably efficient algorithms have been recently proposed for this problem in specific click models. The click model is a model of how the user interacts with a list of documents. Though these results are significant, their impact on practice is limited, because all proposed algorithms are designed for specific click models and lack convergence guarantees in other models. In this work, we propose BatchRank, the first online learning to rank algorithm for a broad class of click models. The class encompasses two most fundamental click models, the cascade and position-based models. We derive a gap-dependent upper bound on the T -step regret of BatchRank and evaluate it on a range of web search queries. We observe that BatchRank outperforms ranked bandits and is more robust than CascadeKL-UCB, an existing algorithm for the cascade model.", "creator": "LaTeX with hyperref package"}}}