{"id": "1703.02567", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Online Learning of Optimal Bidding Strategy in Repeated Multi-Commodity Auctions", "abstract": "We study the online learning problem of a bidder who participates in repeated auctions. With the goal of maximizing his total T-period payoff, the bidder wants to determine the optimal allocation of his fixed budget among his bids for $K$ different goods at each period. As a bidding strategy, we propose a polynomial time algorithm, referred to as dynamic programming on discrete set (DPDS), which is inspired by the dynamic programming approach to Knapsack problems. We show that DPDS achieves the regret order of $O(\\sqrt{T\\log{T}})$. Also, by showing that the regret growth rate is lower bounded by $\\Omega(\\sqrt{T})$ for any bidding strategy, we conclude that DPDS algorithm is order optimal up to a $\\sqrt{\\log{T}}$ term. We also evaluate the performance of DPDS empirically in the context of virtual bidding in wholesale electricity markets by using historical data from the New York energy market.", "histories": [["v1", "Tue, 7 Mar 2017 19:33:50 GMT  (263kb,D)", "https://arxiv.org/abs/1703.02567v1", null], ["v2", "Fri, 31 Mar 2017 17:01:18 GMT  (263kb,D)", "http://arxiv.org/abs/1703.02567v2", null], ["v3", "Wed, 12 Apr 2017 12:55:06 GMT  (264kb,D)", "http://arxiv.org/abs/1703.02567v3", null], ["v4", "Fri, 28 Apr 2017 22:00:22 GMT  (264kb,D)", "http://arxiv.org/abs/1703.02567v4", null]], "reviews": [], "SUBJECTS": "cs.GT cs.LG", "authors": ["sevi baltaoglu", "lang tong", "qing zhao"], "accepted": true, "id": "1703.02567"}, "pdf": {"name": "1703.02567.pdf", "metadata": {"source": "CRF", "title": "Online Learning of Optimal Bidding Strategy in Repeated Multi-Commodity Auctions", "authors": ["Sevi Baltaoglu", "Lang Tong", "Qing Zhao"], "emails": [], "sections": [{"heading": null, "text": "By showing that the rate of regret growth in an auction is lower than in an auction, we conclude that the DPDS algorithm is optimal up to a period of \u221a log T. We also evaluate the performance of the DPDS empirically in the context of virtual bidding in the wholesale electricity markets, using historical data from the New York energy market.Index TermsRepeated Auctions, Online Learning, Dynamic Programming, Virtual Bidding I. INTRODUCTIONWe look at the problem of optimal bidding in a universal price auction (MUPA), which is used as detailed in the wholesale electricity markets as \u2212 A below as a motivating application. Specifically, a bidder K has various goods that are offered at an auction. In order to maximize his expected profit in each period, the bidder's goal is to determine how much to bid for each good bid."}, {"heading": "A. Motivating applications", "text": "The above problem is motivated by the virtual bidding problem in the dual-settlement electricity market, which consists of a day in advance (DA) and a real-time market (RT). In the DA market, the independent system operator (ISO) receives offers to sell and purchase virtual products for every hour of the next day. In order to determine the optimal daily routine and the losses of the next day, electricity prices must be set at each location, ISO solves an economic problem with the aim of maximizing social surpluses and taking into account operational constraints."}, {"heading": "B. Main results and related work", "text": "We propose an online learning approach known as the Discreet Dynamic Programming Problem (DPDS), which is inspired by a pseudo-polynomial dynamic programming approach to 0-1 scarcity problems. (DPDS shows the bidder's limited budget among K-goods optimally in polynomial time both in terms of the number of goods and the time horizon.) We show that the expected payout rate of DPDS adapts to that of the optimal strategy under known distribution, and that this convergence leads to regret in terms of both the number of goods K and the time horizon T-goods (T-Log). By showing that for each bidding strategy the regret growth rate is lower than we prove that DPDS algorithm is an optimal order up to a recording protocol. We also evaluate the performance of DempDS data from the historical bidding context in the virtual energy market."}, {"heading": "II. PROBLEM FORMULATION", "text": "Similarly, xt = [xt, 1,..., xt, K] is the vector of the bids for the period. Then, the expected total yield for the period t given bid xt can be expressed as follows: (xt) = E (ept) 1 {xt,..., xt, K], with the expectation being taken from the common distribution of (\u03c0t, \u03bbt). We assume that the payout resulting from bidding for any good k can be expressed as follows: (\u0432t, k \u2212 \u0441t, k) 1 {xt, k \u0432t, k}, which is a limited random variable with support in [l, u] for any choice of the bid."}, {"heading": "A. Optimal solution under known distribution and regret", "text": "If the common distribution f (.,.) of \u03c0t and \u03bbt is known, the optimization problem (1) is decoupled to be solved individually for each time. Since (\u03c0t, \u03bbt) is i.i.d. over t, an optimal solution within the framework of the known model is not dependent on t and is given by x * = arg max xt * F (xt) (2), where F = {x \u00b2 < K: x \u00b2 0, x \u00b2 1 \u2264 B} is the viable group of commandments. However, the optimal solution x * cannot be unique or have no closed form. The following example illustrates a case where there is no closed solution and shows that even in the case of the known distribution the problem is a combinatorial stochastic optimization and it is not easy to calculate an optimal solution."}, {"heading": "III. ONLINE LEARNING APPROACH FOR OPTIMAL BIDDING STRATEGY", "text": "The idea behind our online learning approach is to maximize the sample mean of the expected payout function. However, the direct implementation of this approach involves holistic programming that is NP-hard. In this section, we propose a polynomial time algorithm based on dynamic programming based on a discredited, practicable set. We show that this approach achieves the optimal sequence among all online learning guidelines."}, {"heading": "A. Approximate expected payoff function and its optimization", "text": "Regardless of bidding policy, the auction problem and spot prices of each period can be observed at the end of that period. (...) Therefore, the average payout amount that could be achieved by bidding x up to the current period can be calculated for each fixed value of x-F. (...) The average payout rate r-t, k (xk) for each good k as a function of the bid value k-t can be calculated at the end of the period t using observations up to t, i.e., r-t, k (xk) = (1 / t) t-value i = 1 (1, k). (...)"}, {"heading": "B. Dynamic programming on discrete set (DPDS) policy", "text": "Next, we present an approach that demonstrates feasibility based on intervals of equal length and optimizes the average payout of this new discrete sum via a dynamic program. (...) Although this approach does not solve the problem given in (7), the solution may be arbitrarily close to optimum, depending on the choice of interval length under the assumption of Lipschitz's expected payout function. (...) In order to exploit the smoothness of Lipschitz's functions, the discretization approach of the continuous feasible amount may have been previously used in the continuous MAB literature. (11) However, unlike the MAB literature, this paper uses the discretization approach to reduce the computational complexity of a problem in addition to exploiting the smoothness of functionality. (...) Let us be an integral sequence illustrated with t and Dt = {0, B / 2, B}."}, {"heading": "C. Convergence and regret of DPDS policy", "text": "Assuming the continuous payout function of Lipschitz, Theorem 1 shows that the value of the DPDS policy approaches the value of the optimal policy under the known model at a rate of faster than or equal to 270 log t / t if the DPDS algorithm parameter \u03b1t = t\u03b3 with \u03b3 \u2265 1 / 2. It follows that the regret growth rate of the DPDS is limited to O (Kt) (Kt 2). Theorem 1: Let xDPDSt + 1 denotes the requirement of the DPDS policy for the period t + 1. If r (.) Lipschitz is continuously constant to F with p-norm and Lipschitz (Kt 2). \u2212 Theorem 1: Let xDPDSt + 1 denotes the requirement of the DPDS policy for the period t + 1. If r (.) Lipschitz is continuously constant to F with p-norm and Lipschitz (KT), Theorem 1: Let-DPDS policy for the period t + 1."}, {"heading": "D. Lower bound of regret for any bidding policy", "text": "We now show that the DPDS actually achieves the slowest possible repentance growth. Theorem 2: Consider the case in which K = 1, B = 1, and Horizon T are independent random variables with distribution differences for which the repentance growth is slower than or equal to the square root of horizon T. Theorem 2: Consider the case in which K = 1, B = 1, and horizon T are independent random variables with distribution differences. Let's leave it at f (1 \u2212) = 11 {t (1 \u2212) / 2 \u2264 t \u2264 (1 +) / 2 \u2264 t \u2264 (1 +) / 2} and with any bidding policy. R\u00b5T (f) = Bernoulli (\u03c0t) = Bernoulli (\u03c0) = 1 / 16 \u2012 Bandit."}, {"heading": "IV. NUMERICAL RESULTS", "text": "In this section, we first present a simulation example illustrating the repentance growth rate of the DPDS. In the simulation example, 1000 Monte Carlo runs were used to calculate average performance, and then we present an empirical example using real historical data from the New York ISO website. In both cases, we establish the DPDS algorithm parameter \u03b1t = t."}, {"heading": "A. Simulation example", "text": "To illustrate the growth rate of regret in the DPDS, let us look at an example with K = 5. In this example, \u03c0t and \u03bbt are independent, \u03bbt is exponentially distributed with average regret = [4, 6, 8, 8, 4] and \u03c0t is evenly distributed with average regret = [5, 8, 8, 9, 3] and support in [4, 6, 8, 8, 4]. Previously, in Sec. II-A, we specified the characterization of the optimal solution for this example. On the basis of this example, we have an optimal solution and an associated budget B. As a benchmark, we consider two different algorithms. The first is the sliding window (SW), which calculates the average payout function of each good every day from the prices of the last ten days. Then, it determines the optimal solution and an associated budget B.As a benchmark, we consider two different algorithms. \u2212 The second one to which SA is referred is a 2.5-DS method with approximate result of the ChaK-4K result."}, {"heading": "B. Empirical example", "text": "This year it is so far that it will only take one year to reach an agreement."}, {"heading": "ACKNOWLEDGEMENT", "text": "We would like to thank Professor Robert Kleinberg for the insightful discussion. (5) These transaction costs are referred to on the NYISO website as installment plan 1 for virtual resources. (9) Total compensation over three years without prior training (b) Total compensation in the last two years after one year of training."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>We study the online learning problem of a bidder who participates in repeated auctions. With the goal of maximizing his total<lb>T-period payoff, the bidder wants to determine the optimal allocation of his fixed budget among his bids for K different goods<lb>at each period. As a bidding strategy, we propose a polynomial time algorithm, referred to as dynamic programming on discrete<lb>set (DPDS), which is inspired by the dynamic programming approach to Knapsack problems. We show that DPDS achieves the<lb>regret order of O(<lb>\u221a<lb>T log T ). Also, by showing that the regret growth rate is lower bounded by \u03a9(<lb>\u221a<lb>T ) for any bidding strategy,<lb>we conclude that DPDS algorithm is order optimal up to a<lb>\u221a<lb>log T term. We also evaluate the performance of DPDS empirically<lb>in the context of virtual bidding in wholesale electricity markets by using historical data from the New York energy market. Index Terms<lb>Repeated auctions, Online learning, Dynamic programming, Virtual bidding", "creator": "LaTeX with hyperref package"}}}