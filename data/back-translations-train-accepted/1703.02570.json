{"id": "1703.02570", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Regularising Non-linear Models Using Feature Side-information", "abstract": "Very often features come with their own vectorial descriptions which provide detailed information about their properties. We refer to these vectorial descriptions as feature side-information. In the standard learning scenario, input is represented as a vector of features and the feature side-information is most often ignored or used only for feature selection prior to model fitting. We believe that feature side-information which carries information about features intrinsic property will help improve model prediction if used in a proper way during learning process. In this paper, we propose a framework that allows for the incorporation of the feature side-information during the learning of very general model families to improve the prediction performance. We control the structures of the learned models so that they reflect features similarities as these are defined on the basis of the side-information. We perform experiments on a number of benchmark datasets which show significant predictive performance gains, over a number of baselines, as a result of the exploitation of the side-information.", "histories": [["v1", "Tue, 7 Mar 2017 19:47:22 GMT  (636kb,D)", "http://arxiv.org/abs/1703.02570v1", "11 page with appendix"]], "COMMENTS": "11 page with appendix", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["amina mollaysa", "pablo strasser", "alexandros kalousis"], "accepted": true, "id": "1703.02570"}, "pdf": {"name": "1703.02570.pdf", "metadata": {"source": "META", "title": "Regularising Non-linear Models Using Feature Side-information", "authors": ["Amina Mollaysa", "Pablo Strasser", "Alexandros Kalousis"], "emails": ["<maolaaisha.aminanmu@hesge.ch>,", "<pablo.strasser@hesge.ch>,", "dros.Kalousis@hesge.ch>."], "sections": [{"heading": "1. Introduction", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country and in which it is a country, in which it is a country and in which it is a country, in which it is a country and in which it is a country."}, {"heading": "2. Learning Symmetric Models with Respect to Feature Similarity", "text": "We will look at the monitored learning settings, in addition to the classical data matrix X: n \u00b7 d, which contains n instances and d characteristics, and the target matrix Y: n \u00b7 m, we will also get a matrix Z: d \u00b7 c, whose exact line is denoted by zi, which contains a description of the ith characteristics. We will call the feature side characteristics Matrix: n \u00b7 m, as the Z matrix as a whole is fixed and independent of the training instances. Since in the default setting the instances called X Rd are drawn i.d by some unobserved probability distributions P (X) and the targets associated with Y Rm, some unobserved conditional distributions P (Y | X), Y Rm will be used. In the default setting, we will learn an assignment to the output of the results: x Rd \u2192 y Rm with the help of optimizing some functions L. In this paper, we will learn the output of Y mapping."}, {"heading": "2.1. An analytical approximation", "text": "We will start by using the first order Taylor expansion to approximate the value of \u03c6 (x). (x) We will use the first order Taylor expansion to approximate the value of \u03c6 (x). (x) We will use the second set of measures (x) to take the second set of measures (x). (X) We will use the third set of measures (x) to take the second set of measures (x). (X) We will take the third set of measures. (X) We will take the third set of measures. (X) We will take the third set of measures. (X) We will take the third set of measures. (X) We will take the third set of measures. (X) We will take the third set of measures. (X) We will take the third set of measures. (X) We will take the third set of measures. (X) We will take the third set of measures. (X) We will take the third set of measures. (X) We will take the third set of measures. (X) We will take the third set of measures."}, {"heading": "2.2. A stochastic approximation", "text": "Instead of using the first order Taylor expansion to simplify the square term required by the regulator, we can use sampling to approximate it. Specifically, for a particular feature pair, i, j, and a particular instance x sample p quadrupled (l) i, \u03bb (l) j, \u03bb (l) i, \u03bb (l) j and a given instance x, which we randomly quadruple p. (l) j, l: = 1. p, which we use to generate new instance pairs as follows: x \u2192 (l) i ei + \u03bb (l) j ejx + \u03bb (l) i + \u03bb (l) j ejWe can now use the training sample and the sampling process to get an estimate of Rij (l) and the sampling process."}, {"heading": "2.3. Optimization", "text": "The objective function of the analytical approach contains the Jacobian of the model in terms of its input. Calculation of the gradient above it leads to the introduction of second-order partial derivatives of the model in terms of inputs and model parameters. Bishop, 1992, gave a back-propagation algorithm for the exact calculation of the Hessian loss of a multi-layer perceptron. We have adapted this algorithm so that we can calculate the gradient of the objective functions that contain the Jacobian in terms of input characteristics, we enter the complete gradient calculation method in the appendix. We now enter the computational complexity of each of the two methods. We will calculate the computational complexity of the individual layers (1 x x x x x x x x) with l the number of layers, m the output dimension of the network, hk the number of hidden units of the kest layer and we will define only the computational complexity of each layer (1 x x x x x x x x m)."}, {"heading": "3. Related Work", "text": "This year, it is so far that it will be able to put itself at the top, \"he says.\" But it is not yet so far that we will be able to do it, \"he says.\" But it is still too early, it is still too early to do it. \""}, {"heading": "4. Experiments", "text": "We are experimenting and evaluating our regularizers in two settings, a synthetic and a real world. We are comparing the analytical and stochastic regularizers, each designated by AN and ST, against popular regularizers used with neural networks, namely \"2 and Dropout (Srivastava et al., 2014), across different network architectures. In the real world datasets, we are also giving the results of Word Mover's Distance, WMD, (Kusner et al., 2015), which makes direct use of page information to calculate document spacing. Obviously, our regularizers and WMD have an advantage over\" 2 and Dropout, as it does not exploit the side information, the \"2 and Dropout.\" We were training both the analytical and stochastic models, as well as all baselines against which we are comparing, with Adam (Kingma & Ba, 2014). We were using the regularizers = 0,001, li\u03b21 = 0,001."}, {"heading": "4.1. Artificial datasets", "text": "We design a simple data generation process to test the performance of our regulators when the data generation mechanisms are compatible with the assumptions of our models. \"We,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,"}, {"heading": "4.2. Real world datasets", "text": "We evaluated both approaches on the eight classification datasets used in (Kusner et al., 2015): BBC Sports Articles (BBCSPORT) designated as one of the eight sports, cricket, footbal, rugby, tennis; tweets labeled with Sen-Timents are worse than positive, negative or neutral (TWITTR); prescriptions labeled by region of origin (RECIPE); medical abstracts labeled by various cardiovascular disease groups (OHSUMED); sentences from academic papers labeled as significant by the editor (CLASSIC); ratings labeled by product category (AMAZON); news datasets labeled by news topics (REUTER); news articles classified into 20 different categories (20NEWS). We removed all the words from the 1988 SMART & SMART list."}, {"heading": "5. Conclusion and Future Work", "text": "In this paper, we develop a regulator that uses exactly this information for general nonlinear models, relying on the simple intuition that characteristics that have similar properties should be treated in a similar way by the learned model. We give two ways to approximate the value of the regulator; an analytical one, which amounts to the imposition of a regulator on the side of the model, forces the model to produce similar results in cases whose characteristic values differ only on the basis of similar characteristics; we give two ways to approximate the value of the regulator; an analytical one, which amounts to the imposition of a regulator on the side of the Jacobin model, in terms of the input characteristics; and a stochastic one, which relies on sampling. We experiment with neural networks with the two approaches of the regulator and compare their performance with well-established models regulators, namely \"2 and dropout,\" based on artificial data sets and not in the actual world."}, {"heading": "6. Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Modified Backpropogation", "text": "The objektive Funktion \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2."}], "references": [{"title": "Exact calculation of the hessian matrix for the multilayer perceptron", "author": ["Bishop", "Chris"], "venue": null, "citeRegEx": "Bishop and Chris.,? \\Q1992\\E", "shortCiteRegEx": "Bishop and Chris.", "year": 1992}, {"title": "Neural networks for pattern recognition", "author": ["Bishop", "Christopher M"], "venue": "Oxford university press,", "citeRegEx": "Bishop and M.,? \\Q1995\\E", "shortCiteRegEx": "Bishop and M.", "year": 1995}, {"title": "Matrix completion with noisy side information", "author": ["Chiang", "Kai-Yang", "Hsieh", "Cho-Jui", "Dhillon", "Inderjit S"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chiang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chiang et al\\.", "year": 2015}, {"title": "Robust principal component analysis with side information", "author": ["Chiang", "Kai-Yang", "Hsieh", "Cho-Jui", "Dhillon", "EDU Inderjit S"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Chiang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chiang et al\\.", "year": 2016}, {"title": "Training invariant support vector machines", "author": ["Decoste", "Dennis", "Sch\u00f6lkopf", "Bernhard"], "venue": "Machine Learning,", "citeRegEx": "Decoste et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Decoste et al\\.", "year": 2002}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In Aistats,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "The sparse laplacian shrinkage estimator for highdimensional regression", "author": ["Huang", "Jian", "Ma", "Shuangge", "Li", "Hongzhe", "Zhang", "CunHui"], "venue": "The Annals of Statistics,", "citeRegEx": "Huang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Incorporating prior knowledge on features into learning", "author": ["Krupka", "Eyal", "Tishby", "Naftali"], "venue": "In AISTATS,", "citeRegEx": "Krupka et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Krupka et al\\.", "year": 2007}, {"title": "Learning to select features using their properties", "author": ["Krupka", "Eyal", "Navot", "Amir", "Tishby", "Naftali"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Krupka et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Krupka et al\\.", "year": 2008}, {"title": "From word embeddings to document distances", "author": ["Kusner", "Matt J", "Sun", "Yu", "Kolkin", "Nicholas I", "Weinberger", "Kilian Q"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML", "citeRegEx": "Kusner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kusner et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["Miller", "George A"], "venue": "Communications of the ACM,", "citeRegEx": "Miller and A.,? \\Q1995\\E", "shortCiteRegEx": "Miller and A.", "year": 1995}, {"title": "Collaborative filtering with graph information: Consistency and scalable methods", "author": ["Rao", "Nikhil", "Yu", "Hsiang-Fu", "Ravikumar", "Pradeep K", "Dhillon", "Inderjit S"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Rao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rao et al\\.", "year": 2015}, {"title": "Higher order contractive auto-encoder", "author": ["Rifai", "Salah", "Mesnil", "Gr\u00e9goire", "Vincent", "Pascal", "Muller", "Xavier", "Bengio", "Yoshua", "Dauphin", "Yann", "Glorot"], "venue": "In Machine Learning and Knowledge Discovery in Databases - European Conference,", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["Rifai", "Salah", "Vincent", "Pascal", "Muller", "Xavier", "Glorot", "Bengio", "Yoshua"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Nonparametric sparsity and regularization", "author": ["L Rosasco", "S Villa", "S Mosci", "M Santoro", "others"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Rosasco et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rosasco et al\\.", "year": 2013}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["Salton", "Gerard", "Buckley", "Christopher"], "venue": "Information processing & management,", "citeRegEx": "Salton et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Salton et al\\.", "year": 1988}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Learning using privileged information: Similarity control and knowledge transfer", "author": ["Vapnik", "Vladimir", "Izmailov", "Rauf"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Vapnik et al\\.,? \\Q2023\\E", "shortCiteRegEx": "Vapnik et al\\.", "year": 2023}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Manifold regularized discriminative neural networks", "author": ["Zhai", "Shuangfei", "Zhang", "Zhongfei"], "venue": "arXiv preprint arXiv:1511.06328,", "citeRegEx": "Zhai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhai et al\\.", "year": 2015}, {"title": "Improving the robustness of deep neural networks via stability training", "author": ["Zheng", "Stephan", "Song", "Yang", "Leung", "Thomas", "Goodfellow", "Ian J"], "venue": "In 2016 IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Zheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "Similar ideas also appear in tasks such as matrix completion, robust PCA and collaborative filtering (Rao et al., 2015; Chiang et al., 2016; 2015).", "startOffset": 101, "endOffset": 146}, {"referenceID": 3, "context": "Similar ideas also appear in tasks such as matrix completion, robust PCA and collaborative filtering (Rao et al., 2015; Chiang et al., 2016; 2015).", "startOffset": 101, "endOffset": 146}, {"referenceID": 6, "context": "Such regularisers have been previously used for parameter shrinkage but only in the setting of linear models where one has direct access to the model parameters (Huang et al., 2011).", "startOffset": 161, "endOffset": 181}, {"referenceID": 6, "context": "i is the ith column vector of W containing the model parameters associated with the ith feature, (Huang et al., 2011).", "startOffset": 97, "endOffset": 117}, {"referenceID": 20, "context": "Denoising autoencoders, (Vincent et al., 2010) follow the stochastic paradigm and require that small random variations in the inputs have only a limited effect on the model output.", "startOffset": 24, "endOffset": 46}, {"referenceID": 6, "context": "(Huang et al., 2011).", "startOffset": 0, "endOffset": 20}, {"referenceID": 10, "context": "In the real world datasets we also give the results of the Word Mover\u2019s Distance, WMD, (Kusner et al., 2015) which makes direct use of the side-information to compute document distances.", "startOffset": 87, "endOffset": 108}, {"referenceID": 10, "context": "We evaluated both approaches on the eight classification datasets used in (Kusner et al., 2015).", "startOffset": 74, "endOffset": 95}, {"referenceID": 11, "context": "As feature side-information we use the word2vec representation of the words which have a dimensionality of 300 (Mikolov et al., 2013); other possibilities include knowledge-based side-information, e.", "startOffset": 111, "endOffset": 133}, {"referenceID": 10, "context": "WMD results are from from (Kusner et al., 2015).", "startOffset": 26, "endOffset": 47}], "year": 2017, "abstractText": "Very often features come with their own vectorial descriptions which provide detailed information about their properties. We refer to these vectorial descriptions as feature side-information. In the standard learning scenario, input is represented as a vector of features and the feature sideinformation is most often ignored or used only for feature selection prior to model fitting. We believe that feature side-information which carries information about features intrinsic property will help improve model prediction if used in a proper way during learning process. In this paper, we propose a framework that allows for the incorporation of the feature side-information during the learning of very general model families to improve the prediction performance. We control the structures of the learned models so that they reflect features\u2019 similarities as these are defined on the basis of the side-information. We perform experiments on a number of benchmark datasets which show significant predictive performance gains, over a number of baselines, as a result of the exploitation of the side-information.", "creator": "LaTeX with hyperref package"}}}