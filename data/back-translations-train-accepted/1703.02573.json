{"id": "1703.02573", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Data Noising as Smoothing in Neural Network Language Models", "abstract": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in $n$-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.", "histories": [["v1", "Tue, 7 Mar 2017 19:56:26 GMT  (314kb,D)", "http://arxiv.org/abs/1703.02573v1", "ICLR 2017"]], "COMMENTS": "ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["ziang xie", "sida i wang", "jiwei li", "daniel l\\'evy", "aiming nie", "dan jurafsky", "rew y ng"], "accepted": true, "id": "1703.02573"}, "pdf": {"name": "1703.02573.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ziang Xie", "Sida I. Wang", "Jiwei Li", "Daniel L\u00e9vy", "Aiming Nie", "Dan Jurafsky", "Andrew Y. Ng"], "emails": ["zxie@cs.stanford.edu,", "sidaw@cs.stanford.edu,", "danilevy@cs.stanford.edu,", "anie@cs.stanford.edu,", "ang@cs.stanford.edu,", "jiweil@stanford.edu", "jurafsky@stanford.edu"], "sections": [{"heading": null, "text": "Data noise is an effective technique for regulating neural network models. While noise is widely used in applications such as vision and speech, commonly used noise primitives have not been developed for discrete sequence levels such as speech modeling. In this paper, we derive a link between input noise in neural network speech models and smoothing in gram models. Using this link, we draw on ideas of smoothing to develop effective sound schemes. We show performance gains in applying the proposed schemes to speech modeling and machine translation. Finally, we provide empirical analyses that validate the relationship between noise and smoothing."}, {"heading": "1 INTRODUCTION", "text": "A key challenge in estimating speech models is the problem of data scarcity: Due to the large vocabulary sizes and the exponential number of possible contexts, however, most possible sequences are rarely or never observed, even with very short subsequences. In other applications, data augmentation has been the key to improving the performance of neural network models in the face of insufficient data. In computer vision, however, there are established primitives for synthesizing additional image data, such as by recalculating or applying affine distortions to images (LeCun et al., 1998; Krizhevsky et al., 2012). Similarly, in speech recognition, there is transmission of audio tracks or the application of small shifts along the time dimension."}, {"heading": "2 RELATED WORK", "text": "Our work can be regarded as a form of data multiplication for which, to the best of our knowledge, there are no commonly applied schemes in neural network speech modeling; classical regulatory methods such as L2 regulation are typically applied to model parameters, while dropouts are applied to activations that can move both forward and in recurring directions (Zaremba et al., 2014; Semeniuta et al., 2015; Gal, 2015); others have introduced methods for recurring neural networks that encourage the hidden activations to remain stable in the norm, or limit the recurring weight matrix to eigenvalues that are close to one another (Krueger & Memisevic, 2015; Arjovsky et al., 2015). However, these methods consider all weights and hidden units instead of input data and are motivated by the disappearance and exploitation of gradient problems."}, {"heading": "3 METHOD", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 PRELIMINARIES", "text": "We look at language models where a sequence of indexes X = (x1, x2, \u00b7 \u00b7, xT) follows complex units, using the vocabulary V, we model (X) = 1p (xt | x < t) In n-gram models it is not possible to model the full context x < t for large t based on the exponential number of possible stories. Recurring neural sequence models (RNN) can (theoretically) model longer dependencies because they operate over distributed hidden states, rather than modelling an exponential number of discrete counts (Bengio et al., 2003; Mikolov, 2012). An L-layer recurrent neural network is modeled as h (l) t = fconventional language (h) t \u2212 1, h \u2212 1) t, where l denotes the layer index, h (0) the most uniform coding of X and its simplest form."}, {"heading": "3.2 SMOOTHING AND NOISING", "text": "Remember that for a given context length l < t, an n-gram model of the order l + 1 according to the logLikelihood criterion is optimal. Thus, in the case where a finite-context RNN achieves nearly the lowest possible cross-entropy loss, it behaves like an n-gram model. Like n-gram models, RNNNs are trained with maximum probability and can easily be overestimated (Zaremba et al., 2014). While generic regulation methods such as L2 regulation and dropout are effective, they do not use specific properties of sequence modeling. To understand sequence-specific regulations, it is helpful to examine n-gram language models whose properties are well underestimated. Smoothing of n-gram models If we model p (xt | x < t), we are able to consider the maximum probability c (x < xt) c / lt; (we recommend zero) probabilities as impermissible and therefore, we do not recommend calculations."}, {"heading": "3.3 NOISING AS SMOOTHING", "text": "We consider the maximum probability estimation of n-gram probabilities, which are estimated using the pseudo values of the data injected. (J) \"J\" (J. \"J.\") (J \"J.\") (J \"J.\") (J \"J.\") (J \"J.\") (J \"J.\") (J \"J.\") (J \"J.\") (J \"J\" J. \") (J\" J \"J.\") (J \"J.\") (J \"J\" J. \"(J\" J. \") (J\" J \"J.\") (J \"J.\") (J \"J\" J \") (J\" J \"J\") (J \"J) (J\" J \"J) (J\" J \") (J\" J) (J \"J) (J\" J) (J \"J) (J\" J \"J) (J\" J \"J) (J\" J) (J \"J) (J\" J) (J \"J) (J\" J) (J \"J) (J\" J) (J \"J\" J) (J \"J) (J\" J \"J) (J\" J) (J \"J) (J\" J) (J \"J\" J) (J) (J \"J) (J\" J \"J) (J\" J \"J\" J \"(J\" J \"J) (J) (J\" J \"J) (J\" J) (J \"J\" J \"J\" J \"(J\" J \"J) (J) (J\" J \"J) (J\" J) (J \"J) (J\" J) (J \"J) (J) (J\" J \"(J\" J \"J\" (J \"J) (J) (J) (J\" (J \"J\" J) (J \"J) (J) (J\" J) (J \"J) (J\" J) (J \"J) (J\" (J) (J) (J \"J) (J\" J \"J\" J \"(\" (J \"J) (J) (J) (J) (J\" J) (J \"(J\" J \"J\" (\"J).\" (J"}, {"heading": "3.4 BORROWING TECHNIQUES", "text": "With the link between noise and smoothing in place, we are now looking at how we can improve the two components of the noise scheme by considering: 1. Adaptively, we calculate the probability of noise \u03b3 to reflect our confidence in a particular input subsequence. 2. Choosing a supply distribution q (x) that is less naive than the feed-in sequence by using the following types of n-gram statistics. Noising Probability Although it simplifies the analysis, there is no reason why we should opt for a fixed distribution; we are now looking at the definition of an adaptive distribution q (x1: t) that depends on the input sequence. Consider the following bigrams: \"and the\" Humpty Dumpty Probability \"Although it is one of the most common distributions in the English corpora; its probability will be well estimated by hence and should not be interpolated with lower order distributions. In anticipation, however, we are using fixed results when we interpolate the lower order results."}, {"heading": "3.5 TRAINING AND TESTING", "text": "During training, the noise per batch is performed and executed online, so that each training epoch sees a different noisy version of the training data. To achieve the training goal, we should select several corrupt versions of the test data at test date and then capture the predictions on average (Srivastava et al., 2014). In practice, however, we find that using the maximum (undamaged) input sequence works well; the evaluation time remains unchanged."}, {"heading": "3.6 EXTENSIONS", "text": "The schemes described refer to the language model setting. To extend it to the sequence sequence or encoder decoder setting, we noise both x < t and y < t. While in the decoder we have y < t and yt as analogies to the context of the language model and to the target prediction, it is unclear whether the noise x < t should be beneficial. However, empirically this is the case (Table 4)."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 LANGUAGE MODELING", "text": "The PTB data set contains 929k training marks, 73k validation marks and 82k test marks. Following Zaremba et al. (2014), we use size 20 minibatches and unroll for 35 time steps if we perform backpropagation over time. All models have two hidden layers and use LSTM units. Weights are uniformly initialized in the range [\u2212 0.1, 0.1]. We look at models with hidden sizes of 512 and 1500. We train with stochastic gradient descend rate with an initial learning rate of 1.0, truncating the gradients if their norm exceeds 5.0. If the validation cross-entropy does not decrease after one training period, we halve the learning rate."}, {"heading": "4.2 MACHINE TRANSLATION", "text": "For our machine translation experiments, we look at the English-German trace of the machine translation of IWSLT 20154. IWSLT 2015 corpus consists of sentence-oriented subtitles of TED and TEDx conversations. The training set contains approximately 190K pairs of sentences with 5.4M tokens. Following Luong & Manning (2015), we use TED tst2012 as the validation set and report BLEU score results (Papineni et al., 2002) to tst2014. We limit vocabulary to the most common 50K words for each language. 2Code is made available at: http: / / deeplearn.stanford.edu / noising 3http: / mattmahoney.net / dc / text8.zip 4http: / workshop2015.iwslt.org / We train a two-layer LSTM encoder-decoder-network (Sutskal et al al al, 2014 al, al)."}, {"heading": "5 DISCUSSION", "text": "5.1 SCALING \u03b3 VIA DISCOUNTINGWe now examine whether discounting has the desired effect of noise reduction sequences according to their uncertainty. If we look at the discount factor \u03b3AD (x1) = \u03b30 N1 + (x1, \u2022) c (x1), we find that the denominator c (x1) may prevail over the counter N1 + (x1, \u2022). Frequent characters are often used in discounting to re-scale the probability of noise, while rare characters are quoted comparatively much more frequently where, in extreme cases, when a brand appears exactly once, we have GAAD = \u03b30. However, due to word frequencies following a zipeic power distribution, common tokens represent the majority of most texts, and therefore discounting results in significantly less noising.We compare the performance of models designed with a fixed efficiency 0 with the help of discounting."}, {"heading": "5.2 NOISED VERSUS UNNOISED MODELS", "text": "To confirm that the data noise for RNN models has a similar effect to the smoothing number in n-gram models, we look at three models formed with the unified noise, as described in Section 4.1 of the Penn Treebank corpus with \u03b3 = 0 (no noise), \u03b3 = 0.1 and \u03b3 = 0.25. Using the trained models, we measure the Kullback-Leibler divergence DKL (p-q) = \u2211 i pi log (pi / qi) over the validation rate between the predicted Softmax distributions, p-q and the even distribution, as well as the unigram frequency distribution. We then take the mean KL divergence across all tokens in the validation set.Consider that a weighted combination of higher and lower n-gram models is used for interpolation smoothing, whether these unigned models are used."}, {"heading": "6 CONCLUSION", "text": "In this paper, we demonstrate that data noise is effective in regulating neural network-based sequence models. By deriving an agreement between noise and smoothing, we are able to adapt advanced smoothing methods for n-gram models to the setting of neural networks, taking into account well-understood generative assumptions of language. Possible applications include studying noise to improve performance in low-resource environments, or studying how these techniques for sequence modeling are generalized in other areas."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Will Monroe for feedback on a draft of this paper, Anand Avati for helping with experiments, and Jimmy Wu for computer support. We also thank the developers of Theano (Theano Development Team, 2016) and Tensorflow (Abadi et al., 2016). Some of the GPUs used in this work were donated by NVIDIA Corporation. ZX, SW, and JL were supported by an NDSEG Fellowship, NSERC PGS-D Fellowship, and Facebook Fellowship, respectively. This project was partially funded by the DARPA MUSE Award FA8750-15-C-0242 AFRL / RIKF."}, {"heading": "A SKETCH OF NOISING ALGORITHM", "text": "We supply pseudo-codes of the noise algorithm according to the Bigram Kneser-Ney smoothing for n-grams. (In the case of sequence-to-sequence tasks, we estimate the number-based parameters separately for source and target.) To simplify this, we assume a batch size of one. The noise algorithm is applied to each data stack during the training. No noise is applied. Algorithm 1 Bigram KN Noising (Language Modeling Settings) Requires c (x), Number of unique sequences N1 + (x, \u2022), Application Distribution q (x), N1 + (\u2022, x) Input X, Y-Stack of unlisted data indexes, Scaling Factor 0Procedure NOISEBGKN (X, Y), Number of unique sequences N1 + (x1,.., xt), Y = (x2,. + 1) Input X, Y-Stack of unlisted data indexed method, NOISEBX (EBY)."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Unitary evolution recurrent neural networks", "author": ["Martin Arjovsky", "Amar Shah", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1511.06464,", "citeRegEx": "Arjovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arjovsky et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "In Journal Of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Generating sentences from a continuous space", "author": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio"], "venue": "arXiv preprint arXiv:1511.06349,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Classbased n-gram models of natural language", "author": ["Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai"], "venue": "Computational linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F Chen", "Joshua Goodman"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Chen and Goodman.,? \\Q1996\\E", "shortCiteRegEx": "Chen and Goodman.", "year": 1996}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Semi-supervised sequence learning", "author": ["Andrew M Dai", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dai and Le.,? \\Q2015\\E", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Large-vocabulary speech recognition under adverse acoustic environments", "author": ["Li Deng", "Alex Acero", "Mike Plumpe", "Xuedong Huang"], "venue": "In ICSLP,", "citeRegEx": "Deng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2000}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal"], "venue": null, "citeRegEx": "Gal.,? \\Q2015\\E", "shortCiteRegEx": "Gal.", "year": 2015}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III"], "venue": "In Association for Computatonal Linguistics (ACL),", "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Regularizing rnns by stabilizing activations", "author": ["David Krueger", "Roland Memisevic"], "venue": "arXiv preprint arXiv:1511.08400,", "citeRegEx": "Krueger and Memisevic.,? \\Q2015\\E", "shortCiteRegEx": "Krueger and Memisevic.", "year": 2015}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "arXiv preprint arXiv:1506.07285,", "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Gradient-based Learning Applied to Document Recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Stanford neural machine translation systems for spoken language domains", "author": ["Minh-Thang Luong", "Christopher D Manning"], "venue": "In Proceedings of the International Workshop on Spoken Language Translation,", "citeRegEx": "Luong and Manning.,? \\Q2015\\E", "shortCiteRegEx": "Luong and Manning.", "year": 2015}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "venue": "arXiv preprint arXiv:1609.07843,", "citeRegEx": "Merity et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "Statistical language models based on neural networks", "author": ["Tom\u00e1\u0161 Mikolov"], "venue": "PhD thesis, PhD thesis, Brno University of Technology", "citeRegEx": "Mikolov.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov.", "year": 2012}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Vu Pham", "Th\u00e9odore Bluche", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": "In Frontiers in Handwriting Recognition (ICFHR),", "citeRegEx": "Pham et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "Recurrent dropout without memory loss", "author": ["Stanislau Semeniuta", "Aliaksei Severyn", "Erhardt Barth"], "venue": "arXiv preprint arXiv:1603.05118,", "citeRegEx": "Semeniuta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Semeniuta et al\\.", "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Altitude training: Strong bounds for single-layer dropout", "author": ["S. Wager", "W. Fithian", "S.I. Wang", "P. Liang"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Wager et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2014}, {"title": "Data augmentation via levy processes", "author": ["Stefan Wager", "William Fithian", "Percy Liang"], "venue": "arXiv preprint arXiv:1603.06340,", "citeRegEx": "Wager et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2016}, {"title": "Feature noising for log-linear structured prediction", "author": ["Sida I Wang", "Mengqiu Wang", "Stefan Wager", "Percy Liang", "Christopher D Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Recurrent highway networks", "author": ["Julian Georg Zilly", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1607.03474,", "citeRegEx": "Zilly et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zilly et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "In computer vision, for example, there exist well-established primitives for synthesizing additional image data, such as by rescaling or applying affine distortions to images (LeCun et al., 1998; Krizhevsky et al., 2012).", "startOffset": 175, "endOffset": 220}, {"referenceID": 15, "context": "In computer vision, for example, there exist well-established primitives for synthesizing additional image data, such as by rescaling or applying affine distortions to images (LeCun et al., 1998; Krizhevsky et al., 2012).", "startOffset": 175, "endOffset": 220}, {"referenceID": 10, "context": "Similarly, in speech recognition adding a background audio track or applying small shifts along the time dimension has been shown to yield significant gains, especially in noisy settings (Deng et al., 2000; Hannun et al., 2014).", "startOffset": 187, "endOffset": 227}, {"referenceID": 12, "context": "Similarly, in speech recognition adding a background audio track or applying small shifts along the time dimension has been shown to yield significant gains, especially in noisy settings (Deng et al., 2000; Hannun et al., 2014).", "startOffset": 187, "endOffset": 227}, {"referenceID": 4, "context": "Neural network models, however, have no notion of discrete counts, and instead use distributed representations to combat the curse of dimensionality (Bengio et al., 2003).", "startOffset": 149, "endOffset": 170}, {"referenceID": 27, "context": "Existing regularization methods, however, are typically applied to weights or hidden units within the network (Srivastava et al., 2014; Le et al., 2015) instead of directly considering the input data.", "startOffset": 110, "endOffset": 152}, {"referenceID": 18, "context": "Existing regularization methods, however, are typically applied to weights or hidden units within the network (Srivastava et al., 2014; Le et al., 2015) instead of directly considering the input data.", "startOffset": 110, "endOffset": 152}, {"referenceID": 32, "context": "Classical regularization methods such as L2-regularization are typically applied to the model parameters, while dropout is applied to activations which can be along the forward as well as the recurrent directions (Zaremba et al., 2014; Semeniuta et al., 2016; Gal, 2015).", "startOffset": 213, "endOffset": 270}, {"referenceID": 26, "context": "Classical regularization methods such as L2-regularization are typically applied to the model parameters, while dropout is applied to activations which can be along the forward as well as the recurrent directions (Zaremba et al., 2014; Semeniuta et al., 2016; Gal, 2015).", "startOffset": 213, "endOffset": 270}, {"referenceID": 11, "context": "Classical regularization methods such as L2-regularization are typically applied to the model parameters, while dropout is applied to activations which can be along the forward as well as the recurrent directions (Zaremba et al., 2014; Semeniuta et al., 2016; Gal, 2015).", "startOffset": 213, "endOffset": 270}, {"referenceID": 1, "context": "Others have introduced methods for recurrent neural networks encouraging the hidden activations to remain stable in norm, or constraining the recurrent weight matrix to have eigenvalues close to one (Krueger & Memisevic, 2015; Arjovsky et al., 2015; Le et al., 2015).", "startOffset": 199, "endOffset": 266}, {"referenceID": 18, "context": "Others have introduced methods for recurrent neural networks encouraging the hidden activations to remain stable in norm, or constraining the recurrent weight matrix to have eigenvalues close to one (Krueger & Memisevic, 2015; Arjovsky et al., 2015; Le et al., 2015).", "startOffset": 199, "endOffset": 266}, {"referenceID": 31, "context": "Feature noising has been demonstrated to be effective for structured prediction tasks, and has been interpreted as an explicit regularizer (Wang et al., 2013).", "startOffset": 139, "endOffset": 158}, {"referenceID": 30, "context": "(2014) show that noising can inject appropriate generative assumptions into discriminative models to reduce their generalization error, but do not consider sequence models (Wager et al., 2016).", "startOffset": 172, "endOffset": 192}, {"referenceID": 1, "context": "Others have introduced methods for recurrent neural networks encouraging the hidden activations to remain stable in norm, or constraining the recurrent weight matrix to have eigenvalues close to one (Krueger & Memisevic, 2015; Arjovsky et al., 2015; Le et al., 2015). These methods, however, all consider weights and hidden units instead of the input data, and are motivated by the vanishing and exploding gradient problem. Feature noising has been demonstrated to be effective for structured prediction tasks, and has been interpreted as an explicit regularizer (Wang et al., 2013). Additionally, Wager et al. (2014) show that noising can inject appropriate generative assumptions into discriminative models to reduce their generalization error, but do not consider sequence models (Wager et al.", "startOffset": 227, "endOffset": 618}, {"referenceID": 1, "context": "Others have introduced methods for recurrent neural networks encouraging the hidden activations to remain stable in norm, or constraining the recurrent weight matrix to have eigenvalues close to one (Krueger & Memisevic, 2015; Arjovsky et al., 2015; Le et al., 2015). These methods, however, all consider weights and hidden units instead of the input data, and are motivated by the vanishing and exploding gradient problem. Feature noising has been demonstrated to be effective for structured prediction tasks, and has been interpreted as an explicit regularizer (Wang et al., 2013). Additionally, Wager et al. (2014) show that noising can inject appropriate generative assumptions into discriminative models to reduce their generalization error, but do not consider sequence models (Wager et al., 2016). The technique of randomly zero-masking input word embeddings for learning sentence representations has been proposed by Iyyer et al. (2015), Kumar et al.", "startOffset": 227, "endOffset": 945}, {"referenceID": 1, "context": "Others have introduced methods for recurrent neural networks encouraging the hidden activations to remain stable in norm, or constraining the recurrent weight matrix to have eigenvalues close to one (Krueger & Memisevic, 2015; Arjovsky et al., 2015; Le et al., 2015). These methods, however, all consider weights and hidden units instead of the input data, and are motivated by the vanishing and exploding gradient problem. Feature noising has been demonstrated to be effective for structured prediction tasks, and has been interpreted as an explicit regularizer (Wang et al., 2013). Additionally, Wager et al. (2014) show that noising can inject appropriate generative assumptions into discriminative models to reduce their generalization error, but do not consider sequence models (Wager et al., 2016). The technique of randomly zero-masking input word embeddings for learning sentence representations has been proposed by Iyyer et al. (2015), Kumar et al. (2015), and Dai & Le (2015), and adopted by others such as Bowman et al.", "startOffset": 227, "endOffset": 966}, {"referenceID": 1, "context": "Others have introduced methods for recurrent neural networks encouraging the hidden activations to remain stable in norm, or constraining the recurrent weight matrix to have eigenvalues close to one (Krueger & Memisevic, 2015; Arjovsky et al., 2015; Le et al., 2015). These methods, however, all consider weights and hidden units instead of the input data, and are motivated by the vanishing and exploding gradient problem. Feature noising has been demonstrated to be effective for structured prediction tasks, and has been interpreted as an explicit regularizer (Wang et al., 2013). Additionally, Wager et al. (2014) show that noising can inject appropriate generative assumptions into discriminative models to reduce their generalization error, but do not consider sequence models (Wager et al., 2016). The technique of randomly zero-masking input word embeddings for learning sentence representations has been proposed by Iyyer et al. (2015), Kumar et al. (2015), and Dai & Le (2015), and adopted by others such as Bowman et al.", "startOffset": 227, "endOffset": 987}, {"referenceID": 1, "context": "Others have introduced methods for recurrent neural networks encouraging the hidden activations to remain stable in norm, or constraining the recurrent weight matrix to have eigenvalues close to one (Krueger & Memisevic, 2015; Arjovsky et al., 2015; Le et al., 2015). These methods, however, all consider weights and hidden units instead of the input data, and are motivated by the vanishing and exploding gradient problem. Feature noising has been demonstrated to be effective for structured prediction tasks, and has been interpreted as an explicit regularizer (Wang et al., 2013). Additionally, Wager et al. (2014) show that noising can inject appropriate generative assumptions into discriminative models to reduce their generalization error, but do not consider sequence models (Wager et al., 2016). The technique of randomly zero-masking input word embeddings for learning sentence representations has been proposed by Iyyer et al. (2015), Kumar et al. (2015), and Dai & Le (2015), and adopted by others such as Bowman et al. (2015). However, to the best of our knowledge, no analysis has been provided besides reasoning that zeroing embeddings may result in a model ensembling effect similar to that in standard dropout.", "startOffset": 227, "endOffset": 1039}, {"referenceID": 1, "context": "Others have introduced methods for recurrent neural networks encouraging the hidden activations to remain stable in norm, or constraining the recurrent weight matrix to have eigenvalues close to one (Krueger & Memisevic, 2015; Arjovsky et al., 2015; Le et al., 2015). These methods, however, all consider weights and hidden units instead of the input data, and are motivated by the vanishing and exploding gradient problem. Feature noising has been demonstrated to be effective for structured prediction tasks, and has been interpreted as an explicit regularizer (Wang et al., 2013). Additionally, Wager et al. (2014) show that noising can inject appropriate generative assumptions into discriminative models to reduce their generalization error, but do not consider sequence models (Wager et al., 2016). The technique of randomly zero-masking input word embeddings for learning sentence representations has been proposed by Iyyer et al. (2015), Kumar et al. (2015), and Dai & Le (2015), and adopted by others such as Bowman et al. (2015). However, to the best of our knowledge, no analysis has been provided besides reasoning that zeroing embeddings may result in a model ensembling effect similar to that in standard dropout. This analysis is applicable to classification tasks involving sum-of-embeddings or bag-of-words models, but does not capture sequence-level effects. Bengio et al. (2015) also make an empirical observation that the method of randomly replacing words with fixed probability with a draw from the uniform distribution improved performance slightly for an image captioning task; however, they do not examine why performance improved.", "startOffset": 227, "endOffset": 1398}, {"referenceID": 4, "context": "Recurrent neural network (RNN) language models can (in theory) model longer dependencies, since they operate over distributed hidden states instead of modeling an exponential number of discrete counts (Bengio et al., 2003; Mikolov, 2012).", "startOffset": 201, "endOffset": 237}, {"referenceID": 23, "context": "Recurrent neural network (RNN) language models can (in theory) model longer dependencies, since they operate over distributed hidden states instead of modeling an exponential number of discrete counts (Bengio et al., 2003; Mikolov, 2012).", "startOffset": 201, "endOffset": 237}, {"referenceID": 8, "context": "As an extension, we also consider encoder-decoder or sequence-to-sequence (Cho et al., 2014; Sutskever et al., 2014) models where given an input sequence X and output sequence Y of length TY , we model p(Y |X) = TY \u220f", "startOffset": 74, "endOffset": 116}, {"referenceID": 28, "context": "As an extension, we also consider encoder-decoder or sequence-to-sequence (Cho et al., 2014; Sutskever et al., 2014) models where given an input sequence X and output sequence Y of length TY , we model p(Y |X) = TY \u220f", "startOffset": 74, "endOffset": 116}, {"referenceID": 32, "context": "Like n-gram models, RNNs are trained using maximum likelihood, and can easily overfit (Zaremba et al., 2014).", "startOffset": 86, "endOffset": 108}, {"referenceID": 17, "context": "This also serves as an alternative explanation for the gains that other related work have found with the \u201cword-dropout\u201d idea (Kumar et al., 2015; Dai & Le, 2015; Bowman et al., 2015).", "startOffset": 125, "endOffset": 182}, {"referenceID": 5, "context": "This also serves as an alternative explanation for the gains that other related work have found with the \u201cword-dropout\u201d idea (Kumar et al., 2015; Dai & Le, 2015; Bowman et al., 2015).", "startOffset": 125, "endOffset": 182}, {"referenceID": 6, "context": "However, it forms what Brown et al. (1992) term a \u201csticky pair\u201d: the unigram \u201cDumpty\u201d almost always follows the unigram \u201cHumpty\u201d, and similarly, \u201cHumpty\u201d almost always precedes \u201cDumpty\u201d.", "startOffset": 23, "endOffset": 43}, {"referenceID": 27, "context": "At test time, to match the training objective we should sample multiple corrupted versions of the test data, then average the predictions (Srivastava et al., 2014).", "startOffset": 138, "endOffset": 163}, {"referenceID": 31, "context": "4 Zaremba et al. (2014) 82.", "startOffset": 2, "endOffset": 24}, {"referenceID": 11, "context": "4 Gal (2015) variational dropout (tied weights) 77.", "startOffset": 2, "endOffset": 13}, {"referenceID": 11, "context": "4 Gal (2015) variational dropout (tied weights) 77.3 75.0 Gal (2015) (untied weights, Monte Carlo) \u2014 73.", "startOffset": 2, "endOffset": 69}, {"referenceID": 11, "context": "We also compare to the variational method of Gal (2015), who also train LSTM models with the same hidden dimension.", "startOffset": 45, "endOffset": 56}, {"referenceID": 23, "context": "Penn Treebank We train networks for word-level language modeling on the Penn Treebank dataset, using the standard preprocessed splits with a 10K size vocabulary (Mikolov, 2012).", "startOffset": 161, "endOffset": 176}, {"referenceID": 25, "context": "For regularization, we apply feed-forward dropout (Pham et al., 2014) in combination with our noising schemes.", "startOffset": 50, "endOffset": 69}, {"referenceID": 23, "context": "Penn Treebank We train networks for word-level language modeling on the Penn Treebank dataset, using the standard preprocessed splits with a 10K size vocabulary (Mikolov, 2012). The PTB dataset contains 929k training tokens, 73k validation tokens, and 82k test tokens. Following Zaremba et al. (2014), we use minibatches of size 20 and unroll for 35 time steps when performing backpropagation through time.", "startOffset": 162, "endOffset": 301}, {"referenceID": 23, "context": "Penn Treebank We train networks for word-level language modeling on the Penn Treebank dataset, using the standard preprocessed splits with a 10K size vocabulary (Mikolov, 2012). The PTB dataset contains 929k training tokens, 73k validation tokens, and 82k test tokens. Following Zaremba et al. (2014), we use minibatches of size 20 and unroll for 35 time steps when performing backpropagation through time. All models have two hidden layers and use LSTM units. Weights are initialized uniformly in the range [\u22120.1, 0.1]. We consider models with hidden sizes of 512 and 1500. We train using stochastic gradient descent with an initial learning rate of 1.0, clipping the gradient if its norm exceeds 5.0. When the validation cross entropy does not decrease after a training epoch, we halve the learning rate. We anneal the learning rate 8 times before stopping training, and pick the model with the lowest perplexity on the validation set. For regularization, we apply feed-forward dropout (Pham et al., 2014) in combination with our noising schemes. We report results in Table 2 for the best setting of the dropout rate (which we find to match the settings reported in Zaremba et al. (2014)) as well as the best setting of noising", "startOffset": 162, "endOffset": 1190}, {"referenceID": 22, "context": "Recent work (Merity et al., 2016; Zilly et al., 2016) has also achieved impressive results on this task by proposing different architectures which are orthogonal to our data augmentation schemes.", "startOffset": 12, "endOffset": 53}, {"referenceID": 33, "context": "Recent work (Merity et al., 2016; Zilly et al., 2016) has also achieved impressive results on this task by proposing different architectures which are orthogonal to our data augmentation schemes.", "startOffset": 12, "endOffset": 53}, {"referenceID": 24, "context": "Following Luong & Manning (2015), we use TED tst2012 as a validation set and report BLEU score results (Papineni et al., 2002) on tst2014.", "startOffset": 103, "endOffset": 126}, {"referenceID": 28, "context": "We train a two-layer LSTM encoder-decoder network (Sutskever et al., 2014; Cho et al., 2014) with 512 hidden units in each layer.", "startOffset": 50, "endOffset": 92}, {"referenceID": 8, "context": "We train a two-layer LSTM encoder-decoder network (Sutskever et al., 2014; Cho et al., 2014) with 512 hidden units in each layer.", "startOffset": 50, "endOffset": 92}, {"referenceID": 2, "context": "The decoder uses an attention mechanism (Bahdanau et al., 2014) with the dot alignment function (Luong et al.", "startOffset": 40, "endOffset": 63}, {"referenceID": 21, "context": ", 2014) with the dot alignment function (Luong et al., 2015).", "startOffset": 40, "endOffset": 60}, {"referenceID": 2, "context": "The decoder uses an attention mechanism (Bahdanau et al., 2014) with the dot alignment function (Luong et al., 2015). The initial learning rate is 1.0 and we start halving the learning rate when the relative difference in perplexity on the validation set between two consecutive epochs is less than 1%. We follow training protocols as described in Sutskever et al. (2014): (a) LSTM parameters and word embeddings are initialized from a uniform distribution between [\u22120.", "startOffset": 41, "endOffset": 372}, {"referenceID": 2, "context": "The decoder uses an attention mechanism (Bahdanau et al., 2014) with the dot alignment function (Luong et al., 2015). The initial learning rate is 1.0 and we start halving the learning rate when the relative difference in perplexity on the validation set between two consecutive epochs is less than 1%. We follow training protocols as described in Sutskever et al. (2014): (a) LSTM parameters and word embeddings are initialized from a uniform distribution between [\u22120.1, 0.1], (b) inputs are reversed, (c) batch size is set to 128, (d) gradient clipping is performed when the norm exceeds a threshold of 5. We set hidden unit dropout rate to 0.2 across all settings as suggested in Luong et al. (2015). We compare unigram, blank, and bigram Kneser-Ney noising.", "startOffset": 41, "endOffset": 703}], "year": 2017, "abstractText": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequencelevel settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in ngram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.", "creator": "LaTeX with hyperref package"}}}