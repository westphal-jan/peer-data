{"id": "1703.02660", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "Towards Generalization and Simplicity in Continuous Control", "abstract": "This work shows that policies with simple linear and RBF parameterizations can be trained to solve a variety of continuous control tasks, including the OpenAI gym benchmarks. The performance of these trained policies are competitive with state of the art results, obtained with more elaborate parameterizations such as fully connected neural networks. Furthermore, existing training and testing scenarios are shown to be very limited and prone to over-fitting, thus giving rise to only trajectory-centric policies. Training with a diverse initial state distribution is shown to produce more global policies with better generalization. This allows for interactive control scenarios where the system recovers from large on-line perturbations; as shown in the supplementary video.", "histories": [["v1", "Wed, 8 Mar 2017 01:33:51 GMT  (1379kb,D)", "http://arxiv.org/abs/1703.02660v1", "Project page:this https URL"]], "COMMENTS": "Project page:this https URL", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO cs.SY", "authors": ["aravind rajeswaran", "kendall lowrey", "emanuel todorov", "sham kakade"], "accepted": true, "id": "1703.02660"}, "pdf": {"name": "1703.02660.pdf", "metadata": {"source": "META", "title": "Towards Generalization and Simplicity in Continuous Control", "authors": ["Aravind Rajeswaran", "Kendall Lowrey", "Emanuel Todorov", "Sham Kakade"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2. Problem Formulation", "text": "We look at Markov decision-making processes (MDPs) in the average reward setting defined by the tuple: M = {S, A, R, T, \u03c10}. S'Rn, A'Rm, and R: S \u00b7 A \u2192 R are (continuous) states, action, and reward functions, and have the usual meaning. T: S \u00b7 A \u2192 S is the stochastic transition function, and 0 is the probability distribution over initial states. (1) Since we use finite-length simulations to estimate the target and gradient, we approach the ratios (\u03c0) using a finite T. In this finite-horizon setting, we define the value, Q, and the advantage parameters as follows: V (s, t), although we use a finite time and a gradient, using (\u03c0) a finite T (finite T), a finite T (finite T), a finite T (finite T), a finite T (finite T), and a finite T (T) finite (T)."}, {"heading": "3. Algorithm", "text": "Using the probability theorem and the Markov property of the problem, the political gradient is essentially derived as follows: \"Effectiveness\" Gradient is suboptimal, since it is not the steepest ascent direction in the metric of the parameter space (Amari, 1998; Kakade, 2001). The steepest ascent direction is achieved by solving the following local optimization problem around the iterate level, since it is not the steepest ascent direction in the metric of the parameter space (Amari, 1998; Kakade, 2001). The steepest ascent direction is achieved by solving the following local optimization problem around the iterate level. We value this metric asF level. \"Effectiveness\" - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness - Effectiveness of the following local optimization problem around the iterate level."}, {"heading": "3.1. Policy Architecture", "text": "First, we consider a linear policy that directly maps from the observations to the motor torques. We use the same observations as in the Gym v1 tasks, which include joint positions, joint velocities, and for some tasks information related to contacts. Therefore, we use the term states and observations interchangeable. Generally, the policy with observations is defined as input and therefore attempts to solve a POMDP. Second, we consider a more meaningful policy based on characteristics of the Fourier observation. Since these characteristics approximate the RKHS characteristics and observations under an RBF kernel (Rahimi & Recht, 2007), we call this policy parameterization the policy of the WyBF. These characteristics are constructed as: solid i-JIS characteristics (approximate b-JIS characteristics) are (approximate n-JIS characteristics), different distances (RBF kernel, Frame & Law, 2007)."}, {"heading": "4. Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Gym-v1 tasks", "text": "In fact, the fact is that most of them will be able to demonstrate that they are able, that they are able to achieve their goals, and that they are able to achieve their goals."}, {"heading": "4.2. Modified Tasks", "text": "This year it is more than ever before."}, {"heading": "5. Discussion and Future Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Sample Complexity and Optimization", "text": "Although linear strategies deliver surprisingly good results in Gym v1 tasks and work well even in the modified tasks, they do not seem to be favorable in terms of sample complexity when optimized with policy gradient methods. As can be seen from the learning curves in Figure 2 and Figure 3, linear politics can learn faster at first, but this benefit diminishes rapidly after a few dozen iterations. Therefore, over-parameterized representations appear to be beneficial from an optimization perspective, but the ability of these over-parameterized representations (neural networks, RBFs, etc.) to generalize is not yet fully understood and is outside the scope of this work. To understand these problems probably requires a reconsideration of problem settings and more appropriate benchmark tasks. Furthermore, it would be interesting to see whether simple strategies can be optimized more efficiently using alternative direct policy search methods such as CMA-ES and function on the basis of careful methods."}, {"heading": "5.2. Towards Harder Tasks", "text": "One explanation for the good performance of political gradient methods, even with simplistic strategies, is that the motion tasks considered are conducive to advantageous random research. In both the original Gym v1 tasks and the modified tasks, the agent does not need to find a sequence of controls with a very delayed reward, as in the case of tasks such as Mountain Car or Montezuma's Revenge. We have not been able to train a successful policy using the linear or RBF architecture on the Humanoid v1 task, which may indicate that the Humanoid v1 task presents representative challenges and probably requires targeted research. Manipulation tasks that are not considered in this work also pose similar challenges."}, {"heading": "5.3. Robustness", "text": "The generalization and robustness that we demonstrate in this work stems exclusively from the states that the agent visits during the training. No disturbances were applied during the training phase, and designing tasks with model groups (Rajeswaran et al., 2016) or disruptors during the training could potentially further enhance the robustness."}, {"heading": "5.4. Interactive Control", "text": "Interactive control tasks have been proposed and successfully solved in previous work (Tassa et al., 2012; Mordatch et al., 2015). Although, to our knowledge, the interactive tasks we are investigating are not as complex as those in Mordatch et al. (2015), this is the first work to consider model-free approaches to continuous control tasks, where a user can interact with the policies learned. As observed in previous work, interactive disturbances and the ability to set interactive goals are an interesting way to study and characterize robustness and generalization, since the human user can consciously target political weaknesses. Such approaches could also pave the way for hierarchical planning, in which the goals can be modulated by a superior agent."}], "references": [{"title": "Trajectory Optimization for Full-Body Movements with Complex Contacts", "author": ["M. Al Borno", "M. de Lasa", "A. Hertzmann"], "venue": "IEEE Transactions on Visualization and Computer Graphics,", "citeRegEx": "Borno et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Borno et al\\.", "year": 2013}, {"title": "Natural gradient works efficiently in learning", "author": ["S. Amari"], "venue": "Neural Computation,", "citeRegEx": "Amari,? \\Q1998\\E", "shortCiteRegEx": "Amari", "year": 1998}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": "In ICML,", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Infinitehorizon model predictive control for periodic tasks with contacts", "author": ["Erez", "Tom", "Tassa", "Yuval", "Todorov", "Emanuel"], "venue": "In RSS,", "citeRegEx": "Erez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Erez et al\\.", "year": 2011}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "A natural policy gradient", "author": ["S. Kakade"], "venue": "In NIPS,", "citeRegEx": "Kakade,? \\Q2001\\E", "shortCiteRegEx": "Kakade", "year": 2001}, {"title": "Continuous control with deep reinforcement learning", "author": ["T. Lillicrap", "J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": null, "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["V Mnih"], "venue": "Nature, 518,", "citeRegEx": "Mnih,? \\Q2015\\E", "shortCiteRegEx": "Mnih", "year": 2015}, {"title": "Discovery of complex behaviors through contact-invariant optimization", "author": ["I. Mordatch", "E. Todorov", "Z. Popovic"], "venue": "ACM SIGGRAPH,", "citeRegEx": "Mordatch et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mordatch et al\\.", "year": 2012}, {"title": "Interactive Control of Diverse Complex Characters with Neural Networks", "author": ["I. Mordatch", "K. Lowrey", "G. Andrew", "Z. Popovic", "E. Todorov"], "venue": "In NIPS,", "citeRegEx": "Mordatch et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mordatch et al\\.", "year": 2015}, {"title": "Random Features for Large-Scale Kernel Machines", "author": ["A. Rahimi", "B. Recht"], "venue": "In NIPS,", "citeRegEx": "Rahimi and Recht,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht", "year": 2007}, {"title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles", "author": ["A. Rajeswaran", "S. Ghotra", "B. Ravindran", "S. Levine"], "venue": "ArXiv e-prints,", "citeRegEx": "Rajeswaran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajeswaran et al\\.", "year": 2016}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Moritz", "M. Jordan", "P. Abbeel"], "venue": "In ICML,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["J. Schulman", "P. Moritz", "S. Levine", "M. Jordan", "P. Abbeel"], "venue": "In ICLR,", "citeRegEx": "Schulman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2016}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["D Silver"], "venue": "search. Nature,", "citeRegEx": "Silver,? \\Q2016\\E", "shortCiteRegEx": "Silver", "year": 2016}, {"title": "Synthesis and stabilization of complex behaviors through online trajectory optimization", "author": ["Y. Tassa", "T. Erez", "E. Todorov"], "venue": "International Conference on Intelligent Robots and Systems,", "citeRegEx": "Tassa et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tassa et al\\.", "year": 2012}, {"title": "MuJoCo: A physics engine for model-based control", "author": ["E. Todorov", "T. Erez", "Y. Tassa"], "venue": "In International Conference on Intelligent Robots and Systems,", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "Deep learning has recently achieved impressive results on a number of hard problems, including sequential decision making in game domains (Mnih, 2015; Silver, 2016).", "startOffset": 138, "endOffset": 164}, {"referenceID": 14, "context": "Deep learning has recently achieved impressive results on a number of hard problems, including sequential decision making in game domains (Mnih, 2015; Silver, 2016).", "startOffset": 138, "endOffset": 164}, {"referenceID": 6, "context": "Neural network controllers trained with Reinforcement Learning have recently produced rich motor behaviors on a number of simulated control tasks (Lillicrap et al., 2015; Schulman et al., 2016).", "startOffset": 146, "endOffset": 193}, {"referenceID": 13, "context": "Neural network controllers trained with Reinforcement Learning have recently produced rich motor behaviors on a number of simulated control tasks (Lillicrap et al., 2015; Schulman et al., 2016).", "startOffset": 146, "endOffset": 193}, {"referenceID": 15, "context": "The complexity of the systems being controlled is not yet at the level of what can be achieved through trajectory optimization in simulation (Tassa et al., 2012; Mordatch et al., 2012; Al Borno et al., 2013) or with hand-crafted controllers on physical robots (e.", "startOffset": 141, "endOffset": 207}, {"referenceID": 8, "context": "The complexity of the systems being controlled is not yet at the level of what can be achieved through trajectory optimization in simulation (Tassa et al., 2012; Mordatch et al., 2012; Al Borno et al., 2013) or with hand-crafted controllers on physical robots (e.", "startOffset": 141, "endOffset": 207}, {"referenceID": 16, "context": "(2016) released a suite of physics models and associated control tasks implemented in the MuJoCo simulator (Todorov et al., 2012).", "startOffset": 107, "endOffset": 129}, {"referenceID": 5, "context": "The best performing algorithms seem to draw upon the benefits of the Natural Gradient (Kakade, 2001) and expressive power of neural networks.", "startOffset": 86, "endOffset": 100}, {"referenceID": 2, "context": "Duan et al. (2016) surveyed various algorithms and studied their performance on these benchmark tasks.", "startOffset": 0, "endOffset": 19}, {"referenceID": 4, "context": "Recent research effort in other areas of AI such as computer vision have been devoted to finding tailored architectures for the respective problems (He et al., 2016).", "startOffset": 148, "endOffset": 165}, {"referenceID": 2, "context": "However, for control tasks, recent works have focused almost exclusively on small fully connected neural networks and occasionally recurrent networks (see Duan et al. (2016) for a survey).", "startOffset": 155, "endOffset": 174}, {"referenceID": 12, "context": "For simplicity, this work utilizes a straightforward natural policy gradient method with normalized stepsize, which is closely related to the TRPO method (Schulman et al., 2015).", "startOffset": 154, "endOffset": 177}, {"referenceID": 1, "context": "Gradient ascent using this \u201cvanilla\u201d gradient is sub-optimal since it is not the steepest ascent direction in the metric of the parameter space (Amari, 1998; Kakade, 2001).", "startOffset": 144, "endOffset": 171}, {"referenceID": 5, "context": "Gradient ascent using this \u201cvanilla\u201d gradient is sub-optimal since it is not the steepest ascent direction in the metric of the parameter space (Amari, 1998; Kakade, 2001).", "startOffset": 144, "endOffset": 171}, {"referenceID": 13, "context": "For estimating the advantage function, we use the GAE procedure (Schulman et al., 2016).", "startOffset": 64, "endOffset": 87}, {"referenceID": 2, "context": "Similar procedures have been adopted in prior work (Duan et al., 2016).", "startOffset": 51, "endOffset": 70}, {"referenceID": 16, "context": "We test the algorithm from Section 3 on the OpenAI gym-v1 benchmarks simulated in MuJoCo (Todorov et al., 2012).", "startOffset": 89, "endOffset": 111}, {"referenceID": 6, "context": "We defer comparisons to policies trained with value function based methods or actor critic methods, such as DDPG (Lillicrap et al., 2015) for future work.", "startOffset": 113, "endOffset": 137}, {"referenceID": 3, "context": "Finding global policies on these tasks with on-line model based optimization has been a challenging benchmark in literature due to under-actuation, high dimensionality, and presence of contacts (Erez et al., 2011).", "startOffset": 194, "endOffset": 213}, {"referenceID": 15, "context": "Without diverse initializations and if termination conditions are used, the learned policy is effectively only trajectory centric, albeit with potentially larger coverage that those obtained using trajectory optimization methods like iLQG (Tassa et al., 2012).", "startOffset": 239, "endOffset": 259}, {"referenceID": 11, "context": "Designing tasks with model ensembles (Rajeswaran et al., 2016) or perturbation forces during training could potentially further improve the robustness.", "startOffset": 37, "endOffset": 62}, {"referenceID": 15, "context": "Interactive control tasks have been suggested and successfully solved in prior work (Tassa et al., 2012; Mordatch et al., 2015).", "startOffset": 84, "endOffset": 127}, {"referenceID": 9, "context": "Interactive control tasks have been suggested and successfully solved in prior work (Tassa et al., 2012; Mordatch et al., 2015).", "startOffset": 84, "endOffset": 127}, {"referenceID": 8, "context": ", 2012; Mordatch et al., 2015). Even though the interactive tasks that we study are not as complex as those in Mordatch et al. (2015),", "startOffset": 8, "endOffset": 134}], "year": 2017, "abstractText": "This work shows that policies with simple linear and RBF parameterizations can be trained to solve a variety of continuous control tasks, including the OpenAI gym benchmarks. The performance of these trained policies are competitive with state of the art results, obtained with more elaborate parameterizations such as fully connected neural networks. Furthermore, existing training and testing scenarios are shown to be very limited and prone to over-fitting, thus giving rise to only trajectory centric policies. Training with a diverse initial state distribution is shown to produce more global policies with better generalization. This allows for interactive control scenarios where the system recovers from large on-line perturbations; as shown in the video. 1", "creator": "LaTeX with hyperref package"}}}