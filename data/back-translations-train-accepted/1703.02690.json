{"id": "1703.02690", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "Leveraging Sparsity for Efficient Submodular Data Summarization", "abstract": "The facility location problem is widely used for summarizing large datasets and has additional applications in sensor placement, image retrieval, and clustering. One difficulty of this problem is that submodular optimization algorithms require the calculation of pairwise benefits for all items in the dataset. This is infeasible for large problems, so recent work proposed to only calculate nearest neighbor benefits. One limitation is that several strong assumptions were invoked to obtain provable approximation guarantees. In this paper we establish that these extra assumptions are not necessary---solving the sparsified problem will be almost optimal under the standard assumptions of the problem. We then analyze a different method of sparsification that is a better model for methods such as Locality Sensitive Hashing to accelerate the nearest neighbor computations and extend the use of the problem to a broader family of similarities. We validate our approach by demonstrating that it rapidly generates interpretable summaries.", "histories": [["v1", "Wed, 8 Mar 2017 03:56:27 GMT  (24kb)", "http://arxiv.org/abs/1703.02690v1", "In NIPS 2016"]], "COMMENTS": "In NIPS 2016", "reviews": [], "SUBJECTS": "stat.ML cs.DS cs.IT cs.LG math.IT", "authors": ["erik m lindgren", "shanshan wu", "alexandros g dimakis"], "accepted": true, "id": "1703.02690"}, "pdf": {"name": "1703.02690.pdf", "metadata": {"source": "CRF", "title": "Leveraging Sparsity for Efficient Submodular Data Summarization", "authors": ["Erik M. Lindgren", "Shanshan Wu", "Alexandros G. Dimakis"], "emails": ["erikml@utexas.edu,", "shanshan@utexas.edu,", "dimakis@austin.utexas.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 3.02 690v 1 [stat.ML] 8 M"}, {"heading": "1 Introduction", "text": "In this thesis we examine the problem of selected sites: We have the problem V of size n, I of size m and a utility matrix of non-negative numbers C, RI, V, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S"}, {"heading": "2 Related Work", "text": "The use of an economical proxy function has been proven useful by Wei et al. for finding a subset to train classifiers for the nearest neighbors [41], and they also show a connection between classifiers for the nearest neighbors and the function for the location of the plant. The plant location function was also used by Mirzasoleiman et al. as part of a summary of objective function in [32], where they present a summary algorithm capable of handling a variety of limitations.The stochastic greedy algorithm received a 1 \u2212 1 / e \u2212 \u03b5 approximation with runtime O (nm log 1\u03b5), which has no dependence on k [33]. It works by selecting a sample setfrom V of size n k log 1 and adding the element of the sample with the greatest gain to the current set. In addition, there are several related algorithms for streaming setting [6, 25, 31] as the overall function is limited in terms of optimizing the plant."}, {"heading": "2.1 Benefits Functions and Nearest Neighbor Methods", "text": "x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x"}, {"heading": "4 Guarantees for Threshold-Based Sparsification", "text": "Instead of the fact that due to the approximate constant K we would have no guarantee for the temporal accessibility of the results, we would consider all entries that have a value below a threshold. Let us remember the definition of a location-dependent hash. H is one (Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth, Earth,"}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Summarizing Movies and Music from Ratings Data", "text": "(...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...).). (...).). (...). (...). (...).). (...). (...). (...). (...). (...)."}, {"heading": "5.2 Finding Influential Actors and Actresses", "text": "We have an edge between an actor or actress when they work together in a movie, weighted by the number of collaborations. Data was taken from [19] and an actor or actress was only recorded when he or she was one of the top six in the cast. First, we look at a small instance where we can see how well the frugal approach works. We build a chart based on the cast of the top thousand most-rated films. This chart has about 6,000 shortcuts and 60,000 edges. We then calculate the total PPR matrix with the style in which we work."}, {"heading": "Acknowledgements", "text": "This material is based on research supported by the National Science Foundation Graduate Research Fellowship under grant number DGE-1110007 and NSF grants CCF 1344179, 1344364, 1407278, 1422549 and ARO YIP W911NF-14-1-0258."}, {"heading": "6 Appendix: Additional Figures", "text": "Algorithm 1 Greedy algorithm with sparsity graph Input: Power matrix C, Sparsity graph G = (V, I, E) Definition N (v): Returns the neighbors of v to G for all i-I: # Cache of the current benefit given to i-Gv \u2190 0A \u2190 \u2205 for k iterations: for all V-V: # Calculation of the gain of the element v gv \u2190 for all i-N (v): # Add the gain of the element v from i-Gv \u2190 gv + max (Civ \u2212 \u03b2i, 0) v-N (v) v-N for all i-N (v): # update the cache of the current benefit for i \u03b2i \u2190 max (\u03b2i, Civ) Return A"}, {"heading": "7 Appendix: Full Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Proof of Theorem 1", "text": "The first problem limits the size of the smallest set of left vertices covering each right vertice in a t-regular bifurcated diagram. Lemma 7. For each bifurcated diagram G = (V, I, E) there is a series of vertices so that each vertice in I has a neighbor in H and V. The second problem limits the rate that the optimal solution as a function of k.Lemma 8. Let f be any normalized submodular function and let O2 and O1 be optimal solutions for their respective size. The second problem limits the rate that the optimal solution of k.Lemma 8. Let f be all submodular functions and let O2 and O1 be optimal solutions for their respective size."}, {"heading": "7.2 Proof of Proposition 3", "text": "For column n + 1, we have column i equal to 1 for positions i to i + t \u2212 1, so that we can turn the position back to the beginning if necessary, and then 0 otherwise. For column n + 1, we make all values 1 \u2212 1 / 2n. For example, column 6 (3) = 1 0 0 0 1 1 11 / 12 1 1 0 0 0 0 11 / 12 0 0 0 1 1 1 1 1 11 / 12. We show the lower limit in two parts when the column < 1 \u2212 1 and when column 1 \u2212 1 receives evidence for case \u03b1 1. F is the function of the device location defined on the utility matrix C = n (crystallization matrix n k). For t = crystallized matrix n, column C (t) has all elements except the n + 1st row."}, {"heading": "7.3 Proof of Proposition 4", "text": "The stochastic greedy algorithm works by selecting a series of elements Sj of each iteration of the quantity nk log 1 \u03b5. We assume that m = n and \u03b5 = 1 / e simplify the notation. We want to show this by using Amber's Inequality: given n i.i.d. random variables X1,.., Xn such that E (X) = 0, Var (X) = 2, and | X | \u2264 c with probability 1, we haveP (n i.i.d. = 1X1,., Xn such that E (X) = 0, Var (X) = 2, and | X | \u2264 c with probability 1, we haveP (n = 1Xn) \u2264 exp (\u2212 n\u03bb22\u04452 + 2 3 c\u043c).We use X to uniformly select the degree of the fourth element of V according to the random principle, shifted according to the mean value of t. < n If the elements according to X."}, {"heading": "7.4 Proof of Lemma 7", "text": "We now prove Lemma 7, which is a modification of theorem 1.2.2 of [1]. Proof. Choose a set of X by selecting each element of V with the probability p, with p to be decided later. Add one arbitrarily for each element of I without neighbors in X. Name this set Y. We have E (| X-Y |) \u2264 np + m (1 \u2212 p) t \u2264 np + me \u2212 pt. Optimization to p yields p = 1t ln mt n. This is a valid probability if mt n \u2265 1, which we assumed, and if m n \u2264 et t (we need not worry about the latter case, because if it does not hold, it implies an inequality weaker than the trivial one."}, {"heading": "7.5 Proof of Lemma 8", "text": "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "7.6 Proof of Theorem 5", "text": "Proof. Let O be the optimal solution to the original problem. Let F\u03c4 and F \u03c4 be the functions limited to the matrix elements, using at least \u03c4 or all remaining elements. If there is a set S of size k, so that \u00b5n elements have a neighbor in S, then we have F (O) \u2264 F\u03c4 (O) + F \u03c4 (O) \u2264 F\u03c4 (O) + n\u03c4 \u2264 F\u0442 (O) + 1\u00b5 F\u03c4 (S) \u2264 (1 + 1\u00b5) F\u043e (O\u0442), where the final inequality follows that O\u03c4 is optimal for F\u03c4."}, {"heading": "7.7 Proof of Lemma 6", "text": "Evidence. Consider the following algorithm: B \u2190 S \u2190, while B | \u2264 c\u03b4n v (argmax | N (v) | add v S add N (v) to B remove N (v) * {v} from GWe will show that after T = c (1 \u2212 2c2) \u03b4-iterations this algorithm ends. If it does so, S will satisfy the degree | N (S) | \u2265 c\u03b4n, since each element of B has a neighbor in S until we finish the number of edges occurring to B, at most | B | c\u03b4n \u2264 c2\u0441n \u2264 c\u0441n, then we end after the first iteration. Otherwise, all Vertices have degrees less than ccc\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u043d\u0438\u043d\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u043d\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u043d\u0438\u043d\u0438\u0441\u0438\u043d\u0438\u043d\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u043d\u0438\u0441\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "The facility location problem is widely used for summarizing large datasets and<lb>has additional applications in sensor placement, image retrieval, and clustering. One<lb>difficulty of this problem is that submodular optimization algorithms require the cal-<lb>culation of pairwise benefits for all items in the dataset. This is infeasible for large<lb>problems, so recent work proposed to only calculate nearest neighbor benefits. One<lb>limitation is that several strong assumptions were invoked to obtain provable approx-<lb>imation guarantees. In this paper we establish that these extra assumptions are not<lb>necessary\u2014solving the sparsified problem will be almost optimal under the standard<lb>assumptions of the problem. We then analyze a different method of sparsification that<lb>is a better model for methods such as Locality Sensitive Hashing to accelerate the<lb>nearest neighbor computations and extend the use of the problem to a broader family<lb>of similarities. We validate our approach by demonstrating that it rapidly generates<lb>interpretable summaries.", "creator": "LaTeX with hyperref package"}}}