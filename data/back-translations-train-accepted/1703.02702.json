{"id": "1703.02702", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "Robust Adversarial Reinforcement Learning", "abstract": "Deep neural networks coupled with fast simulation and improved computation have led to recent successes in the field of reinforcement learning (RL). However, most current RL-based approaches fail to generalize since: (a) the gap between simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses). Inspired from H-infinity control methods, we note that both modeling errors and differences in training and test scenarios can be viewed as extra forces/disturbances in the system. This paper proposes the idea of robust adversarial reinforcement learning (RARL), where we train an agent to operate in the presence of a destabilizing adversary that applies disturbance forces to the system. The jointly trained adversary is reinforced -- that is, it learns an optimal destabilization policy. We formulate the policy learning as a zero-sum, minimax objective function. Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah, Swimmer, Hopper and Walker2d) conclusively demonstrate that our method (a) improves training stability; (b) is robust to differences in training/test conditions; and c) outperform the baseline even in the absence of the adversary.", "histories": [["v1", "Wed, 8 Mar 2017 04:58:51 GMT  (3349kb,D)", "http://arxiv.org/abs/1703.02702v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.MA cs.RO", "authors": ["lerrel pinto", "james davidson", "rahul sukthankar", "abhinav gupta"], "accepted": true, "id": "1703.02702"}, "pdf": {"name": "1703.02702.pdf", "metadata": {"source": "META", "title": "Robust Adversarial Reinforcement Learning", "authors": ["Lerrel Pinto", "James Davidson", "Rahul Sukthankar", "Abhinav Gupta"], "emails": ["<lerrelp@cs.cmu.edu>."], "sections": [{"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "1.1. Overview of RARL", "text": "Our aim is to develop a policy that works not only on paper, but also on the streets."}, {"heading": "2. Background", "text": "Before delving into the details of RARL, we first outline our terminology, the standard amplification learning kit, and the zero-sum games for two players that inspired our newspaper."}, {"heading": "2.1. Standard reinforcement learning on MDPs", "text": "In this paper, we examine MDPs in continuous space represented by the tuple: (S, A, P, r, \u03b3, s0) where S is a series of continuous states and A is a series of continuous actions, P: S \u00b7 A \u00b7 S \u2192 R is the transition probability, r: S \u00b7 A \u2192 R is the reward function, \u03b3 is the discount factor, and s0 is the initial state distribution. Stacks of political algorithms like (Williams, 1992; Kakade, 2002; Schulman et al., 2015) try to learn a stochastic politics: S \u00b7 A \u2192 R that maximizes the cumulative discounted reward: T \u2212 1 t = 0 \u03b3tr (st, at)."}, {"heading": "2.2. Two-player zero-sum discounted games", "text": "The opposing attitude we propose can be expressed as a two-dimensional zero-sum Markov brand game (Littman, 1994; Perolat et al., 2015). This game MDP can be expressed as tuples: (S, A1, A2, P, r, \u03b3, s0) where A1 and A2 are the continuous series of actions players can perform. P: S \u00b7 A1 \u00b7 A2 \u00b7 S \u2192 R is the transition density and r: S \u00b7 A1 \u00b7 A2 \u2192 R is the reward of both players. If player 1 (protagonist) plays strategy \u00b5 and player 2 (opponent) plays strategy, the reward function can be considered player 1, which maximizes the discounted reward while player 2 minimizes it."}, {"heading": "3. Robust Adversarial RL", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Robust Control via Adversarial Agents", "text": "Our goal is to learn the policies of the protagonist (denoted by \u00b5), so that it is better (higher reward) and robust (generalizes better to deviations in test settings). In the default reinforcement learning settings, for a certain transition function P, we can learn political parameters Phenomeno, so that the expected reward is maximized, where the expected reward for the policy \u00b5 is maximized from the beginning, since the transition function defines the roll-out function of states. In the standard RL settings, the transition function is fixed (since the physics machine and parameters such as mass, friction are fixed). However, in our setting we assume that the transition function will have modeling errors and that there will be differences between training and testing conditions."}, {"heading": "3.2. Formulating Adversarial Reinforcement Learning", "text": "In our zero-sum game, the protagonist receives a reward r1t = P (st, a1t, a2t) and a reward rt = r (st, a1t, a 2 t). In our zero-sum game, the protagonist receives a reward r1t = rt, while the opponent receives a reward r 2 t = \u2212 rt. Therefore, each step of this MDP can be represented as (st, a 1 t, a 2 t, r 1 t, r 2 t, st + 1). The protagonist strives to maximize the following reward function, R1 = Es0 \u043c, a1."}, {"heading": "3.3. Proposed Method: RARL", "text": "In the first phase we learn the politics of the protagonist, while the politics of the opponent is determined. Next, the politics of the protagonist is kept constant and the politics of the opponent is learned. This sequence is repeated until convergence. Algorithm 1 outlines our approach in detail. The initial parameters for the politics of the opponent are sampled by means of a random distribution. In each of the niter iterations we perform a two-stage (alternating) optimization procedure. First, the parameters of the politics of the opponent are kept constant, while the parameters of the politics of the protagonist are kept constant. Algorithm 1 RARL (proposed algorithm) Input: Environment E; stochastic strategies \u00b5 and Proposition Initialize: Learnable parameters \u00b50 for \u00b5 and Proposition 0 for the politics 2jecter it is."}, {"heading": "4. Experimental Evaluation", "text": "We will now demonstrate the robustness of the RARL algorithm: (a) for training with different initializations; (b) for testing under different conditions; (c) for hostile interference in the test environment. But first we will describe our implementation and testing situation, followed by evaluations and results of our algorithm."}, {"heading": "4.1. Implementation", "text": "Our implementation of opposing environments relies on OpenAI gyms (Brockman et al., 2016) controlling environments using the MuJoCo (Todorov et al., 2012) physics simulator. Details of the environments and their respective opposing perturbations are (see also Figure 1): InvertedPendulum: The reverse pendulum is mounted on a pivot point on a wagon, with the wagon limited to linear movement on a plane. State space is 4D: position and speed for both the art and for the pendulum. The protagonist can apply 1D forces to keep the pendulum upright. The opponent applies a 2D force to the center of the pendulum to destabilize it. HalfCheetah: Half cheetah is a bifurcated robot with 8 rigid connections, including two legs and a torso."}, {"heading": "4.2. Evaluating Learned Policies", "text": "We evaluate the robustness of our RARL approach compared to the strong TRPO baseline. Since our policy is stochastic and the initial state is also derived from a distribution, we learn 50 strategies for each task with different seed / initializations. First, we give the mean and variance of cumulative reward (over 50 strategies) as a function of the training processes. Figure 2 shows the mean and variance of the rewards of learned strategies for the task of HalfCheetah, Swimmer, Hopper and Walker2D. We omit the graph for InvertedPendulum because the task is simple and both TRPO and RARL have similar achievements and similar rewards. As we can see from the figure, RARL learns a better policy for medium reward and variance for all four tasks. This clearly shows that the policies learned by RARL are better than the policies that TRPO has learned from these settings, even if there is no change or no change in settings."}, {"heading": "4.3. Robustness under Adversarial Disturbances", "text": "One way to measure the robustness of such effects is to measure the performance of our learned control strategies in the event of an enemy disturbance. To this end, we train an adversary to apply a disturbance while keeping the protagonist's policies constant. Again, we show the percentile diagrams as described in the section above. As the RARL's control policy has been trained on similar adversaries, it works better, as shown in Figure 4."}, {"heading": "4.4. Robustness to Test Conditions", "text": "Finally, we evaluate the robustness and generalization of the learned policies with respect to different test conditions. In this section, we train the policies based on certain mass and friction values; however, at test date, we evaluate the table 1. Comparison of the best policies learned from the RARL with the baseline (mean \u00b1 one standard deviation) InvertedPendulum HalfCheetah Swimmer Hopper Walker2dBaseline 1000 \u00b1 0.0 5093 \u00b1 44 358 \u00b1 2.4 3614 \u00b1 2.16 5418 \u00b1 87 RARL 1000 \u00b1 0.0 5444 \u00b1 97 354 \u00b1 1.5 3590 \u00b1 7.4 5854 \u00b1 159R ewar dR ewar dR ewar dR ewar dR when different mass and friction values are used in the environment. Note that we omit the assessment of the float because the policy for the swimming task is not significantly affected by a change in mass or friction."}, {"heading": "4.4.1. EVALUATION WITH CHANGING MASS", "text": "We describe the results of training with the standard mass variables in the OpenAI gym while testing it with different mass. Specifically, the mass of InvertedPendulum, HalfCheetah, Hopper and Walker2D was 4.89, 6.36, 3.53 and 3.53, respectively. At test time, we evaluated the strategies learned by changing the mass values and estimating the average cumulative rewards. Figure 5 records the average rewards and their standard deviations against a given torso mass (horizontal axis)."}, {"heading": "4.4.2. EVALUATION WITH CHANGING FRICTION", "text": "Since several of the control tasks involve contacts and friction (which are often poorly modelled), we evaluate robustness using different coefficients of friction in the test. Similar to assessing robustness against mass, the model is trained with standard variables in the OpenAI gym. Figure 6 shows the average reward values with different coefficients of friction at test time. It can be seen that the baseline guidelines do not generalise and performance falls significantly when friction in the test differs from training. On the other hand, RARL shows greater resistance to changing coefficients of friction. The increased robustness of RARL is illustrated in Figure 7, where we test with jointly varying coefficients of mass and friction. As shown in the figure, RARL leads to significantly higher reward values for most combinations of mass and friction values compared to the baseline."}, {"heading": "4.5. Visualizing the Adversarial Policy", "text": "Finally, we illustrate the hostile policy in the case of InvertedPendulum and Hopper to see if the learned strategies are human interpretable. As shown in Figure 8, the direction of the force applied by the opponent coincides with human intuition: When the car is stationary and the pole is already inclined (top row), the opponent tries to emphasize the inclination. Similarly, the car behaves quickly and the pole vertical (bottom row), the opponent exerts a force in the direction of the car movement, unless the pole continues to accelerate (which can also cause the car to spin out of control). Note that the naive policy of pushing in the opposite direction would be less effective, as the protagonist could slow down the car to stabilize the pole. Similarly, in the hopper task in Figure 9, the opponent behaves horizontal forces in order to impede the movement when the hopper is in the air while he is on the ground (while the left)."}, {"heading": "5. Related Research", "text": "In recent years, we have an even longer history in control theory (Zhou 1998). We have shown great success in a variety of tasks ranging from games (Mnih et al., 2015; Silver et al., 2016), robot control (Gu et al., 2016; Lillicrap et al., 2015; Mordatch et al., 2015), to meta-learning (Zoph & Le, 2016). An overview of recent advances in deep RL will be provided in (Li, 2017) and (Kaelbling et al., 1996; Kober & Peters, 2012) provide a comprehensive history of RL research. Scholarly strategies should be robust to uncertainty and parameter variation to ensure predictive behavior, which is essential for many practical applications of RL, including robotics. Furthermore, the process of learning policy should use safe and effective exploration with improved example efficiency to reduce the risk of costly failure."}, {"heading": "6. Conclusion", "text": "We have presented a novel framework for adversarial reinforcement, RARL, which means: (a) robust to training initializations; (b) better generalized and resilient to environmental changes between training and test conditions; (c) robust to disruptions in the test environment that are difficult to model during training; and our basic idea is that modeling errors should be considered as additional forces / disturbances in the system. Inspired by this insight, we propose to model uncertainties about an adversary applying disturbances to the system. Instead of applying a fixed strategy, the adversary is strengthened and learns an optimal strategy to optimally counter the protagonist. Our work shows that the adversary effectively selects hard examples (paths with worst rewards) that lead to a more robust control strategy."}], "references": [{"title": "H-infinity optimal control and related minimax design problems: a dynamic game approach", "author": ["Ba\u015far", "Tamer", "Bernhard", "Pierre"], "venue": "Springer Science & Business Media,", "citeRegEx": "Ba\u015far et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ba\u015far et al\\.", "year": 2008}, {"title": "Multi-agent reinforcement learning: An overview", "author": ["Bu\u015foniu", "Lucian", "Babu\u0161ka", "Robert", "De Schutter", "Bart"], "venue": "In Innovations in multi-agent systems and applications-1,", "citeRegEx": "Bu\u015foniu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bu\u015foniu et al\\.", "year": 2010}, {"title": "Transfer from simulation to real world through learning deep inverse dynamics model", "author": ["Christiano", "Paul", "Shah", "Zain", "Mordatch", "Igor", "Schneider", "Jonas", "Blackwell", "Trevor", "Tobin", "Joshua", "Abbeel", "Pieter", "Zaremba", "Wojciech"], "venue": "arXiv preprint arXiv:1610.03518,", "citeRegEx": "Christiano et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Christiano et al\\.", "year": 2016}, {"title": "Percentile optimization for Markov decision processes with parameter uncertainty", "author": ["Delage", "Erick", "Mannor", "Shie"], "venue": "Operations research,", "citeRegEx": "Delage et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Delage et al\\.", "year": 2010}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Duan", "Yan", "Chen", "Xi", "Houthooft", "Rein", "Schulman", "John", "Abbeel", "Pieter"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Adversarially learned inference", "author": ["Dumoulin", "Vincent", "Belghazi", "Ishmael", "Poole", "Ben", "Lamb", "Alex", "Arjovsky", "Martin", "Mastropietro", "Olivier", "Courville", "Aaron"], "venue": "arXiv preprint arXiv:1606.00704,", "citeRegEx": "Dumoulin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dumoulin et al\\.", "year": 2016}, {"title": "A comprehensive survey on safe reinforcement learning", "author": ["Garc\u0131a", "Javier", "Fern\u00e1ndez", "Fernando"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Garc\u0131a et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Garc\u0131a et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Goodfellow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Memory-based control with recurrent neural networks", "author": ["Heess", "Nicolas", "Hunt", "Jonathan J", "Lillicrap", "Timothy P", "Silver", "David"], "venue": "arXiv preprint arXiv:1512.04455,", "citeRegEx": "Heess et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2015}, {"title": "Deep reinforcement learning from self-play in imperfect-information games", "author": ["Heinrich", "Johannes", "Silver", "David"], "venue": "arXiv preprint arXiv:1603.01121,", "citeRegEx": "Heinrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Heinrich et al\\.", "year": 2016}, {"title": "Reinforcement learning: A survey", "author": ["Kaelbling", "Leslie Pack", "Littman", "Michael L", "Moore", "Andrew W"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Kaelbling et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1996}, {"title": "A natural policy gradient", "author": ["Kakade", "Sham"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Kakade and Sham.,? \\Q2002\\E", "shortCiteRegEx": "Kakade and Sham.", "year": 2002}, {"title": "Reinforcement learning in robotics: A survey", "author": ["Kober", "Jens", "Peters", "Jan"], "venue": "In Reinforcement Learning,", "citeRegEx": "Kober et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kober et al\\.", "year": 2012}, {"title": "Deep reinforcement learning: An overview", "author": ["Li", "Yuxi"], "venue": "arXiv preprint arXiv:1701.07274,", "citeRegEx": "Li and Yuxi.,? \\Q2017\\E", "shortCiteRegEx": "Li and Yuxi.", "year": 2017}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Markov games as a framework for multi-agent reinforcement learning", "author": ["Littman", "Michael L"], "venue": "In Proceedings of the eleventh international conference on machine learning,", "citeRegEx": "Littman and L.,? \\Q1994\\E", "shortCiteRegEx": "Littman and L.", "year": 1994}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih and Volodymyr,? \\Q2015\\E", "shortCiteRegEx": "Mnih and Volodymyr", "year": 2015}, {"title": "Interactive control of diverse complex characters with neural networks", "author": ["Mordatch", "Igor", "Lowrey", "Kendall", "Andrew", "Galen", "Popovic", "Zoran", "Todorov", "Emanuel V"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mordatch et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mordatch et al\\.", "year": 2015}, {"title": "Robust reinforcement learning", "author": ["Morimoto", "Jun", "Doya", "Kenji"], "venue": "Neural computation,", "citeRegEx": "Morimoto et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morimoto et al\\.", "year": 2005}, {"title": "Robust control of Markov decision processes with uncertain transition matrices", "author": ["Nilim", "Arnab", "El Ghaoui", "Laurent"], "venue": "Operations Research,", "citeRegEx": "Nilim et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Nilim et al\\.", "year": 2005}, {"title": "Stochastic and shortest path games: theory and algorithms", "author": ["Patek", "Stephen David"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Patek and David.,? \\Q1997\\E", "shortCiteRegEx": "Patek and David.", "year": 1997}, {"title": "Approximate dynamic programming for twoplayer zero-sum games", "author": ["Perolat", "Julien", "Scherrer", "Bruno", "Piot", "Bilal", "Pietquin", "Olivier"], "venue": "In ICML,", "citeRegEx": "Perolat et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Perolat et al\\.", "year": 2015}, {"title": "Supervision via competition: Robot adversaries for learning", "author": ["Pinto", "Lerrel", "Davidson", "James", "Gupta", "Abhinav"], "venue": "tasks. CoRR,", "citeRegEx": "Pinto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pinto et al\\.", "year": 2016}, {"title": "EPOpt: Learning robust neural network policies using model ensembles", "author": ["Rajeswaran", "Aravind", "Ghotra", "Sarvjeet", "Ravindran", "Balaraman", "Levine", "Sergey"], "venue": "arXiv preprint arXiv:1610.01283,", "citeRegEx": "Rajeswaran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajeswaran et al\\.", "year": 2016}, {"title": "Sim-toreal robot learning from pixels with progressive nets", "author": ["Rusu", "Andrei A", "Vecerik", "Matej", "Roth\u00f6rl", "Thomas", "Heess", "Nicolas", "Pascanu", "Razvan", "Hadsell", "Raia"], "venue": "arXiv preprint arXiv:1610.04286,", "citeRegEx": "Rusu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Moritz", "Philipp", "Jordan", "Michael I", "Abbeel", "Pieter"], "venue": "CoRR, abs/1502.05477,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "A robust Markov game controller for nonlinear systems", "author": ["Sharma", "Rajneesh", "Gopal", "Madan"], "venue": "Applied Soft Computing,", "citeRegEx": "Sharma et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sharma et al\\.", "year": 2007}, {"title": "Training region-based object detectors with online hard example mining", "author": ["Shrivastava", "Abhinav", "Gupta", "Girshick", "Ross B"], "venue": null, "citeRegEx": "Shrivastava et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shrivastava et al\\.", "year": 2016}, {"title": "Mastering the game of Go with deep neural networks and tree search", "author": ["Silver", "David"], "venue": null, "citeRegEx": "Silver and David,? \\Q2016\\E", "shortCiteRegEx": "Silver and David", "year": 2016}, {"title": "Learning and example selection for object and pattern detection", "author": ["K. Sung", "T. Poggio"], "venue": "MIT A.I. Memo,", "citeRegEx": "Sung and Poggio,? \\Q1994\\E", "shortCiteRegEx": "Sung and Poggio", "year": 1994}, {"title": "Optimizing the CVaR via sampling", "author": ["Tamar", "Aviv", "Glassner", "Yonatan", "Mannor", "Shie"], "venue": "arXiv preprint arXiv:1404.3862,", "citeRegEx": "Tamar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tamar et al\\.", "year": 2014}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Todorov", "Emanuel", "Erez", "Tom", "Tassa", "Yuval"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Robust Markov decision processes", "author": ["Wiesemann", "Wolfram", "Kuhn", "Daniel", "Rustem", "Ber\u00e7"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Wiesemann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wiesemann et al\\.", "year": 2013}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Preparing for the unknown: Learning a universal policy with online system identification", "author": ["Yu", "Wenhao", "Liu", "C. Karen", "Turk", "Greg"], "venue": "arXiv preprint arXiv:1702.02453,", "citeRegEx": "Yu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2017}, {"title": "Essentials of robust control, volume 104", "author": ["Zhou", "Kemin", "Doyle", "John Comstock"], "venue": "Prentice hall Upper Saddle River, NJ,", "citeRegEx": "Zhou et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 1998}, {"title": "Neural architecture search with reinforcement learning", "author": ["Zoph", "Barret", "Le", "Quoc V"], "venue": "arXiv preprint arXiv:1611.01578,", "citeRegEx": "Zoph et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "High-capacity function approximators such as deep neural networks have led to increased success in the field of reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Gu et al., 2016; Lillicrap et al., 2015; Mordatch et al., 2015).", "startOffset": 135, "endOffset": 239}, {"referenceID": 18, "context": "High-capacity function approximators such as deep neural networks have led to increased success in the field of reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Gu et al., 2016; Lillicrap et al., 2015; Mordatch et al., 2015).", "startOffset": 135, "endOffset": 239}, {"referenceID": 2, "context": "This reality gap often results in unsuccessful transfer if the learned policy isn\u2019t robust to modeling errors (Christiano et al., 2016; Rusu et al., 2016).", "startOffset": 110, "endOffset": 154}, {"referenceID": 25, "context": "This reality gap often results in unsuccessful transfer if the learned policy isn\u2019t robust to modeling errors (Christiano et al., 2016; Rusu et al., 2016).", "startOffset": 110, "endOffset": 154}, {"referenceID": 24, "context": ") and learn an ensemble of policies for different possible variations (Rajeswaran et al., 2016).", "startOffset": 70, "endOffset": 95}, {"referenceID": 26, "context": "Batch policy algorithms like (Williams, 1992; Kakade, 2002; Schulman et al., 2015) attempt to learn a stochastic policy \u03c0\u03b8 : S \u00d7 A \u2192 R that maximizes the cumulative discounted reward \u2211T\u22121 t=0 \u03b3 r(st, at).", "startOffset": 29, "endOffset": 82}, {"referenceID": 22, "context": "The adversarial setting we propose can be expressed as a two player \u03b3 discounted zero-sum Markov game (Littman, 1994; Perolat et al., 2015).", "startOffset": 102, "endOffset": 139}, {"referenceID": 31, "context": "Instead, inspired by work in robust control (Tamar et al., 2014; Rajeswaran et al., 2016), we choose to optimize for conditional value at risk (CVaR):", "startOffset": 44, "endOffset": 89}, {"referenceID": 24, "context": "Instead, inspired by work in robust control (Tamar et al., 2014; Rajeswaran et al., 2016), we choose to optimize for conditional value at risk (CVaR):", "startOffset": 44, "endOffset": 89}, {"referenceID": 24, "context": "But how do you tractably sample trajectories that are in worst \u03b1-percentile? Approaches like EP-Opt (Rajeswaran et al., 2016) sample these worst percentile trajectories by changing parameters such as friction, mass of objects, etc.", "startOffset": 100, "endOffset": 125}, {"referenceID": 28, "context": "We would also like to point out the connection between our proposed approach and the practice of hard-example mining (Sung & Poggio, 1994; Shrivastava et al., 2016).", "startOffset": 117, "endOffset": 164}, {"referenceID": 22, "context": "(Perolat et al., 2015; Patek, 1997) show that notions of minimax equilibrium and Nash equilibrium are equivalent for this game with optimal equilibrium reward:", "startOffset": 0, "endOffset": 35}, {"referenceID": 22, "context": "The complexity of this greedy solution is exponential in the cardinality of the action spaces, which makes it prohibitive (Perolat et al., 2015).", "startOffset": 122, "endOffset": 144}, {"referenceID": 32, "context": ", 2016) control environments with the MuJoCo (Todorov et al., 2012) physics simulator.", "startOffset": 45, "endOffset": 67}, {"referenceID": 4, "context": "Our implementation of RARL is built on top of rllab (Duan et al., 2016) and uses Trust Region Policy Optimization (TRPO) (Schulman et al.", "startOffset": 52, "endOffset": 71}, {"referenceID": 26, "context": ", 2016) and uses Trust Region Policy Optimization (TRPO) (Schulman et al., 2015) as the policy optimizer.", "startOffset": 57, "endOffset": 80}, {"referenceID": 15, "context": ", 2016), robot control (Gu et al., 2016; Lillicrap et al., 2015; Mordatch et al., 2015), to meta learning (Zoph & Le, 2016).", "startOffset": 23, "endOffset": 87}, {"referenceID": 18, "context": ", 2016), robot control (Gu et al., 2016; Lillicrap et al., 2015; Mordatch et al., 2015), to meta learning (Zoph & Le, 2016).", "startOffset": 23, "endOffset": 87}, {"referenceID": 11, "context": "An overview of recent advances in deep RL is presented in (Li, 2017) and (Kaelbling et al., 1996; Kober & Peters, 2012) provide a comprehensive history of RL research.", "startOffset": 73, "endOffset": 119}, {"referenceID": 26, "context": "To address this, we extend RRL\u2019s modelfree formulation using deep RL via TRPO (Schulman et al., 2015) with neural networks as the function approximator.", "startOffset": 78, "endOffset": 101}, {"referenceID": 33, "context": "(Wiesemann et al., 2013) present an enhancement to standard MDP that provides probabilistic guarantees to unknown model parameters.", "startOffset": 0, "endOffset": 24}, {"referenceID": 31, "context": "Other approaches are risk-based including (Tamar et al., 2014; Delage & Mannor, 2010), which formulate various mechanisms of percentile risk into the formulation.", "startOffset": 42, "endOffset": 85}, {"referenceID": 8, "context": "Adversarial methods have been used in other learning problems including (Goodfellow et al., 2015), which leverages adversarial examples to train a more robust classifiers and (Goodfellow et al.", "startOffset": 72, "endOffset": 97}, {"referenceID": 7, "context": ", 2015), which leverages adversarial examples to train a more robust classifiers and (Goodfellow et al., 2014; Dumoulin et al., 2016), which uses an adversarial lost function for a discriminator to train a generative model.", "startOffset": 85, "endOffset": 133}, {"referenceID": 5, "context": ", 2015), which leverages adversarial examples to train a more robust classifiers and (Goodfellow et al., 2014; Dumoulin et al., 2016), which uses an adversarial lost function for a discriminator to train a generative model.", "startOffset": 85, "endOffset": 133}, {"referenceID": 23, "context": "In (Pinto et al., 2016) two supervised agents were trained with one acting as an adversary for selfsupervised learning which showed improved robot grasping.", "startOffset": 3, "endOffset": 23}, {"referenceID": 1, "context": "Refer to (Bu\u015foniu et al., 2010) for an review of multiagent RL techniques.", "startOffset": 9, "endOffset": 31}, {"referenceID": 9, "context": "(Heess et al., 2015) use recurrent neural networks to perform direct adaptive", "startOffset": 0, "endOffset": 20}, {"referenceID": 35, "context": "Indirect adaptive control was applied in (Yu et al., 2017) for online parameter identification.", "startOffset": 41, "endOffset": 58}, {"referenceID": 24, "context": "(Rajeswaran et al., 2016) learn a robust policy by sampling the worst case trajectories from a class of parametrized models, to learn a robust policy.", "startOffset": 0, "endOffset": 25}], "year": 2017, "abstractText": "Deep neural networks coupled with fast simulation and improved computation have led to recent successes in the field of reinforcement learning (RL). However, most current RL-based approaches fail to generalize since: (a) the gap between simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses). Inspired from H\u221e control methods, we note that both modeling errors and differences in training and test scenarios can be viewed as extra forces/disturbances in the system. This paper proposes the idea of robust adversarial reinforcement learning (RARL), where we train an agent to operate in the presence of a destabilizing adversary that applies disturbance forces to the system. The jointly trained adversary is reinforced \u2013 that is, it learns an optimal destabilization policy. We formulate the policy learning as a zero-sum, minimax objective function. Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah, Swimmer, Hopper and Walker2d) conclusively demonstrate that our method (a) improves training stability; (b) is robust to differences in training/test conditions; and c) outperform the baseline even in the absence of the adversary.", "creator": "LaTeX with hyperref package"}}}