{"id": "1703.02910", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "Deep Bayesian Active Learning with Image Data", "abstract": "Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).", "histories": [["v1", "Wed, 8 Mar 2017 16:53:57 GMT  (2493kb,D)", "http://arxiv.org/abs/1703.02910v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["yarin gal", "riashat islam", "zoubin ghahramani"], "accepted": true, "id": "1703.02910"}, "pdf": {"name": "1703.02910.pdf", "metadata": {"source": "META", "title": "Deep Bayesian Active Learning with Image Data", "authors": ["Yarin Gal", "Riashat Islam", "Zoubin Ghahramani"], "emails": ["<yg279@cam.ac.uk>."], "sections": [{"heading": "1. Introduction", "text": "This can be a long, tedious and costly process that often renders the provision of ML systems uneconomic; a framework in which a system could learn from small amounts of data, and choose what data it would give the user to label, would spread machine learning much further; such a learning environment is referred to as active learning (Cohn et al., 1996) (also known as \"experimental design\" in statistical literature), and has been used successfully in areas such as medical diagnostics, microbiology and manufacturing (Tong, 2001); in active learning, a model is used at a small1University of Cambridge, UK 2The Alan Turing Institute. Correspondence to: Yarin Gal < yg279 @ cam.amount of data (the initial training set), and an acquisition function (often based on the uncertainty of the model) determines which data is based on a label."}, {"heading": "2. Related Research", "text": "Previous attempts to actively learn image data focused on kernel-based methods. Joshi et al. (2009) used ideas from previous research on active learning of low-dimensional data (Tong, 2001) and extracted probabilistic results from support vector machines (SVM) (Cortes & Vapnik, 1995), using linear, polynomial, and radial base functions (RBF) on the raw images, selecting the core that offered the best classification accuracy. Similar to SVM approaches, Li & Guo (2013) used Gaussian processes (GPs) with RBF cores to maintain model uncertainty. Li & Guo (2013), however, fed low-dimensional features (such as SIFT functions) into their RBF kernels."}, {"heading": "3. Bayesian Convolutional Neural Networks", "text": "In this paper, we focus on high-dimensional image data and need a model capable of representing predictive uncertainty on such data. Existing approaches such as (Zhu et al., 2003; Li & Guo, 2013; Joshi et al., 2009) rely on core methods and feed image pairs using linear, polynomial, and RBF methods to capture image similarity as input to an SVM model. Unlike the above models, which cannot capture spatial information in the input image, CNNs are designed to utilize this spatial information, and have been successfully used to achieve state-of-the-art results (Krizhevsky et al, 2012). To perform active learning with image data, we use the Bayesian equivalent of CNNs proposed in Gi."}, {"heading": "4. Acquisition Functions and their Approximations", "text": "Considering that a pool of data dpool and input x-troy (Shannon, 1948) is x-projected, a capture function a (x, M) is a function of x that the AL system uses to decide where to ask next: x * = argmaxx * Dpoola (x, M). Next, we examine various capture functions that are appropriate for our image data settings, and develop tractable approximations for ourselves that we can use with our Bayean CNNs. For example, in tasks that involve regression, we often use the predictive variance or quantity derived from it for our capture function (although we still need to be careful to create the query from informative areas rather than query the noise). For example, we could search for images with high predictive variance and select those that make a label in the hope that they will reduce uncertainty. However, many tasks that include image profiles will often be formulated as classifiers."}, {"heading": "5. Active Learning with Bayesian Convolutional Neural Networks", "text": "We will examine the proposed technique for active learning of image data. We will compare the various capture functions based on Bayean CNN uncertainty with a simple benchmark for image classification. We will then examine the importance of model uncertainty by evaluating the same capture functions with a deterministic CNN. Then we will compare them with a current technique for active learning with image data based on SVMs. We will compare them with the most advanced models closest to active learning with image data - semi-monitored techniques with image data. These semi-monitored techniques have access to much more data (the unlabeled data) than our active learning models, but we still work with them in a comparable way. Finally, we will demonstrate the proposed methodology with a real application of skin cancer diagnosis from a small number of lesion images, relying on fine-tuning a large CNN model."}, {"heading": "5.1. Comparison of various acquisition functions", "text": "We explore all the capture functions through our Bayesian CNN that have been trained on the MNIST dataset (LeCun & Cortes, 1998); all capture functions are evaluated using the same model structure: Convolution-reluctant convolution-relu-max pooling-dropout-dense-dropoutse-softmax, with 32 convolution cores, 4x4 kernel size, 2x2 pooling, dense layer of 128 units, and dropout probabilities 0.25 and 0.5 (using Kera's MNIST CNN implementation, 2015); all models are trained on the MNIST dataset with an (random but balanced) initial dataset of 20 data points, and a validation of 100 points on which we optimize the weight decay (this is a realistic validation of the size set compared to the standard validation)."}, {"heading": "10% 145 120 165 230 255", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5% 335 295 355 695 835", "text": "2The code for these experiments is available at http: / / mlg.eng.cam.ac.uk / yarin / publications. html # Gal2016Active."}, {"heading": "5.2. Importance of model uncertainty", "text": "We assess the importance of model uncertainty in our Bayesian CNN by evaluating three of the capture functions (BALD, Variation Ratios, and Max Entropy) with a deterministic CNN. Similar to Bayesian CNN, deterministic CNN generates a probability vector that can be used with the capture functions of \u00a7 4 (formally, by setting q \u043a (\u03c9) = \u03b4 (\u03c9 \u2212 \u03b8) as the point mass at the location of the model parameters \u03b8. Such deterministic models can capture aleatory uncertainty - the noise in the data - but not capture epistemic uncertainty - the uncertainty about the parameters of CNN that we try to minimize during active learning. Models in this experiment still use dropout, but only for regulation (i.e. we do not perform MC dropout at the test date).Comparing the Bayesian models with the BALD deterministic models, Ratio's and Max Entropy models shows a more significant effect in the overall Bayesian capture function."}, {"heading": "5.3. Comparison to current active learning techniques with image data", "text": "Next, we compare a method in the sparse literature on active learning with image data that focuses on (Zhu et al., 2003), which relies on a kernel method and continues to use the unlabeled images (which will be discussed in more detail in the next section). Zhu et al. (2003) evaluate an RBF kernel over the raw images to obtain a similarity graph that can be used to exchange information about the unlabeled data. Active learning is then performed by greedily selecting unlabeled images to be labeled to minimize an estimate of the expected classification error. This is called MBR.MBR and was formulated in the case of binary classification, so we compared MBR with the earning functions BALD, Variation Rates, Max Entropy, and Random on a binary classification (two digits from the MNIST dataset)."}, {"heading": "5.4. Comparison to semi-supervised learning", "text": "We continue the comparison with the next models (in modern literature) to our active learning with image data: semi-supervised learning with image data. In semi-supervised learning, a model receives a fixed set of marked data and a fixed set of unmarked data. The model can use the unmarked data set to learn about the distribution of inputs, in the hope that this information will also help in learning how to match the results. Several semi-supervised models for image data have been proposed in recent years (Weston et al., 2012; Kingma et al., 2014; Rasmus et al., 2015), models that have set benchmarks for the MNIST give a small number of marked images (1000 random images). These models use a (very) large set of unmarked images of 49K and a large validation set of 5K-10K-designated images to adjust the model hyper parameter and model structure (Rasmus et al., 2015). These models have access to our models to compare our marked models to the equivalent models, not to our marked 5K models."}, {"heading": "5.5. Cancer diagnosis from lesion image data", "text": "We conclude the investigation of the proposed technique with a real test case. We are experimenting with melanoma (skin cancer) diagnosis from dermoscopic reading images. In this task, we are confronted with image data of skin segments that are both malignant (cancerous) and benign (non-cancerous). Our task is to classify the images as malignant or benign (an example is shown in Fig. 4) The data used is the ISIC archive (Gutman et al., 2016). These data were collected to provide a \"large public repository of expertly annotated high quality skin images\" to provide clinical support in the identification of skin cancer and to develop algorithms for the diagnosis of skin cancer. We are using the training data of \"ISBI 2016: Skin Lesion Analysis Towards Melanoma Detection - Part 3B: Segmented Lesion Classification.\""}, {"heading": "6. Future Research", "text": "We introduced a new approach to active learning of imaging data, building on recent advances at the interface of Bayesian modeling and deep learning, and demonstrated a real-world application in medical diagnostics. We evaluated the performance of the techniques by resetting the models after each acquisition and re-training them for convergence, in order to isolate the impact of our acquisition capabilities associated with longer training times (for example, 20 hours for each melanoma experiment). We demonstrated that even with this long duration, our technology still reduces the required expert labels, thus reducing the cost of such a system. This duration can be further reduced by not resetting the system - with the potential cost of falling into local optima. We are leaving this problem to future research."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).", "creator": "LaTeX with hyperref package"}}}