{"id": "1703.02914", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "Dropout Inference in Bayesian Neural Networks with Alpha-divergences", "abstract": "To obtain uncertainty estimates with real-world Bayesian deep learning models, practical inference approximations are needed. Dropout variational inference (VI) for example has been used for machine vision and medical applications, but VI can severely underestimates model uncertainty. Alpha-divergences are alternative divergences to VI's KL objective, which are able to avoid VI's uncertainty underestimation. But these are hard to use in practice: existing techniques can only use Gaussian approximating distributions, and require existing models to be changed radically, thus are of limited use for practitioners. We propose a re-parametrisation of the alpha-divergence objectives, deriving a simple inference technique which, together with dropout, can be easily implemented with existing models by simply changing the loss of the model. We demonstrate improved uncertainty estimates and accuracy compared to VI in dropout networks. We study our model's epistemic uncertainty far away from the data using adversarial images, showing that these can be distinguished from non-adversarial images by examining our model's uncertainty.", "histories": [["v1", "Wed, 8 Mar 2017 17:00:21 GMT  (840kb,D)", "http://arxiv.org/abs/1703.02914v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yingzhen li", "yarin gal"], "accepted": true, "id": "1703.02914"}, "pdf": {"name": "1703.02914.pdf", "metadata": {"source": "META", "title": "Dropout Inference in Bayesian Neural Networks with Alpha-divergences", "authors": ["Yingzhen Li", "Yarin Gal"], "emails": ["<yl494@cam.ac.uk>."], "sections": [{"heading": "1. Introduction", "text": "This information, however, unlike the deep learning models Bayesian probabilistic models can capture parameter uncertainty and its induced effects over predictions, capturing the models' ignorance about the world, and able to convey their increased uncertainty on out-of-data examples. This information can be used, for example, if a vi-1University of Cambridge, UK 2The Alan Turing Institute, UK. Correspondence to: Yingzhen Li < yl494 @ cam.ac.sion model is given."}, {"heading": "2. Background", "text": "We will look at the background of Bayesian neural networks and more detailed conclusions of variation. In the next section we will discuss \u03b1-divergences."}, {"heading": "2.1. Bayesian Neural Networks", "text": "Considering the training inputs X = {x1,., xN} and their corresponding outputs Y = {y1,.., yN}, we would like to conclude in the Bayesian parametric regression that a distribution over parameters of a function y = f\u03c9 (x) may have generated the outputs. To find parameters that our data could have generated, we apply a prior distribution over the space of the parameters p0 (\u03c9). This distribution captures our prior belief as to which parameters our outputs were likely to have generated before considering any data. We also need to define a probability distribution over the outputs resulting from the inputs p (y | x, \u03c9). For classification tasks, we assume a soft maximum probability, p (y | x, \u03c9) = Softmax (f\u03c9 (x) = Nevis (x) or a Gaussian probability for the regression. Considering a dataset X, Y, we can then search for the subsequent or subsequent distribution of a Y function."}, {"heading": "2.2. Approximate Variational Inference in Bayesian Neural Networks", "text": "In approximate derivation, we are interested in finding the distribution of the weight matrices (parameterization of our functions) that generated our data, which is the lateral distribution relative to the weights of our observables X, Y: p (\u03c9 | X, Y), which is generally not comprehensible. Existing approaches to approximating this lateral distribution are based on variable inference (as in Hinton & Van Camp (1993); Barber & Bishop (1998); Graves (2011); Blundell et al. (2015). We need to define an approximate variable distribution q\u03b8 (\u03c9) (parameterized by varying parameters) and then minimize the KL divergence (Kullback & Leibler, 1951)."}, {"heading": "2.3. Dropout Approximate Inference", "text": "Given a (deterministic) neural network, stochastic regularization techniques in the model (such as \"drop-out\" = \"drop-out\" (Hinton et al., 2012; Srivastava et al., 2014) can be interpreted as variative Bayean approximations in a Bayesian NN with the same network structure (Gal & Ghahramani, 2016b), because the application of stochastic regularization technology equals a multiplication of the NN weight matrices Mi by random noise i (with a new noise realization for each data point).The resulting stochastic weight matrix matrix matrix-matrix-matrix-X-matrix matrix-X-matrix matrix-Qyesian matrix matrix-X-matrix-matrix-X-matrix-matrix-matrix-Qyesian matrix-matrix-X-matrix-matrix-matrix-X-matrix-X-Li-matrix-matrix-1."}, {"heading": "3. Black-box \u03b1-divergence minimisation", "text": "In this section, we offer a brief overview of the black box alpha (BB-\u03b1, BB-q = BB-1). (BB-Lobato et al. (2016) Method on which the most important derivative in this paper is based. Consider the approximation of the following distribution: p (\u03c9) = 1Z p0 (\u03c9). (BB-Lobato et al. (BB-Lobato et al. (2016) Method on which the most important derivatives in this paper are based.) These factors represent the probability concepts p (yn-xn, \u03c9) and the approximation target p (\u03c9) is the exact posteriore p (\u03c9). (X, Y) Popular methods of approximate inference include variational inference (VI) (Jordan et al., 1999) and expectation propagation (EP) (Minka, 2001), where these two algorithms are special cases of power EP (Minka, 2004), which are the minimization of Amari-divergence (1985)."}, {"heading": "4. A New Reparameterisation of BB-\u03b1 Energy", "text": "We propose a repair of the BB-\u03b1 energy in order to reduce the computing effort that uses the so-called \"cavity distributions.\" First, we designate q-q as a free cavity distribution and write the approximate rear q number (n) = 1Zq number (n) (n) (n), in which we assume the approximate constant to ensure a valid distribution. If \u03b1 / N \u2192 0, the unnormalized density is converted to q number (n) (n), and Zq number (1), in which we specify Zq < + number is the normalizing constant to ensure a valid distribution. If \u03b1 / N \u2192 0, the unnormalized density is converted to q number (n), and Zq number (1), in which we specify the n number (n), and Zq < + (n) is the normalizing density converted to (6), not to ensure a valid \u03b1 (n)."}, {"heading": "4.1. Dropout BB-\u03b1", "text": "We now provide a concrete example in which the approximate distribution by dropouts is defined. With dropouts VI, MC samples are used to approximate the expectation. In practice, this is implemented by passing stochastic forwards through the dropout network - i.e., a new dropout mask is sampled and applied at each dropout layer. This results in a stochastic output - a sample from the dropout network on the input x. A similar approach is also used in our case where the MC sample is implemented in eq. (7) We perform multiple stochastic forwards through the network.Recall the neural network f\u03c9 (x) is parameterized by the variable class."}, {"heading": "5. Experiments", "text": "We test the repaired BB-\u03b1 on Bayesian NNNs using the drop-out approach. We evaluate the proposed conclusion in regression and classification tasks on standard benchmarking datasets and compare different values of \u03b1. We also evaluate the trade-off between our technique and VI during training time and examine the properties of uncertainty in our model on out-of-distribution datasets. This last experiment leads us to suggest a technique that could be used to identify enemy image attacks."}, {"heading": "5.1. Regression", "text": "The first experiment looks at Bavarian Neural Network Regression with approximate posterior results caused by dropout. We use benchmark UCI datasets 2 tested in related literature; the model is a single-layer neural network with 50 ReLU units for all datasets except protein and year using 100 units; we look at \u03b1 {0.0, 0.5, 1.0} to investigate the effects of mass covering / zero-forcing behavior in dropout; MC approximation with K = 10 samples is also used to calculate the energy function; other initialization settings are largely taken from (Li & Turner, 2016); we summarize the negative log probabilities (LL) and RMSE with standard errors (via various random splits) for selected datasets in Figure 2 and 3; the full results are provided in the appendix. Although the optimal alpha values for measurement sets may remain different from those for measurement sets, Li VI does not have the same data sets in the full IC."}, {"heading": "5.2. Classification", "text": "Furthermore, we are experimenting with a classification task by comparing the accuracy of the various \u03b1 values on the MNIST benchmark (LeCun & Cortes, 1998), using the failure probability 0.5 and \u03b2 {0, 0.5, 1}, again using K = 10 samples at training point for all \u03b1 values and Ktest = 100 samples at test point, using the weight decay 10 \u2212 6, which corresponds to the previous longitudinal scale l2 = 0.1 (Gal & Ghahramani, 2016b), repeating each experiment three times and recording mean and standard errors, the test RMSE and the test log probability are shown in Figure 4. As you can see, the Hellinger value \u03b1 = 0.5 gives the best test RMSE, with the test log probability corresponding to the EP value \u03b1 = 1."}, {"heading": "5.3. Detecting Adversarial Examples", "text": "The third set of experiments that deal with the question of whether it is about a way in which it is about the realization of entitlements that are oriented towards the real world, and which it is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is about, is, is about, is about, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is about, is, is, is, is, is, is, is, is, is, is, is about, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is, is,"}, {"heading": "5.4. Run time trade-off", "text": "In contrast to VI, our conclusion relies on a large number of samples to reduce the distortion factor of the estimator. If a small number of samples is used (K = 1), our method breaks down to standard VI. In Figure 8, we see both the test accuracy and the test log probability for a fully connected NN with four layers of 1024 units trained on the MNIST dataset as \u03b1 = 1. The two metrics are presented as a function of the wall clock time for different values of K (1, 10, 100). As can be seen, K = 1 arrives at a test accuracy of 98.8% faster than the other values of K, which approach the same accuracy. On the other hand, both K = 1 and K = 10 reach a value \u2212 600 within 1000 seconds, but K = 10 further improves its test log probability and converges after \u2212 500 = 3000 seconds possibly to the same value of runtime."}, {"heading": "6. Conclusions", "text": "We introduced a practical extension of the BB-Alpha lens that allows us to apply the technique with dropout approximating distributions, which often replaces existing approximating inference techniques (even sparse Gaussian processes) and is easy to implement. A snippet of code for our induced loss is given in the appendix."}, {"heading": "Acknowledgements", "text": "YL thanks the Schlumberger Foundation's FFTF scholarship for supporting her doctoral thesis."}, {"heading": "A. Code Example", "text": "The following is a code snippet that shows how our conclusion can be implemented with a few lines of Keras code (Chollet, 2015). We define a new loss function bbalpha softmax cross entropy with mc logits that takes MC sampled logits as input, which is demonstrated in the case of classification. Regression can be implemented in a similar way. bbalpha _ softmax _ cross _ entropy _ with _ mc _ logits (alpha): def loss (y _ true, mc _ logits): # mc _ logits: output of GenerateMCSlist (output of GenerateMCSsamples, msoc mlogsamples of CSsoc, mmax _ logits): def loss (y _ max, mc _ logits): (mc _ log _ logits): output of GenerateMCSlist (output of GenerateMCSsamples, msoc logsamples of CSsoc, mmax, mmax _ msoftshape = K.x _ ftshape _ K.x)."}, {"heading": "B. Alpha-divergence minimisation", "text": "There are several available definitions of \u03b1 divergences, and in this paper we mainly used two of them: Amari's definition (Amari, 1985), which is adapted to the context of the EP (Minka, 2005), and Re'nyi divergence (Re'nyi, 1961), which is more widely used in information theory research. \u2022 Amari's \u03b1 divergence (Amari, 1985): D\u03b1 [p | | q] = 1\u03b1 (1 \u2212 \u03b1) (1 \u2212, 000 p (\u03c9) \u03b1q (\u03c9) 1 \u2212 \u03b1d\u03c9.These two divergences can be converted to one another, e.g. D\u03b1 [p | q] = 1\u03b1 (1 \u2212 \u03b1) = exp [p \u2212 1) p [p | q]))."}, {"heading": "C. Original Derivation of BB-\u03b1 Energy", "text": "Here we include the original formulation of the BB-\u03b1 energy for completeness. Consider the approximation of a distribution of the following form (\u03c9) = 1Z p0 (\u03c9) n, in which the previous distribution p0 (\u03c9) exhibits an exponential family form p0 (\u03c9) q = q problems. Since the factors fn may not be conjugated to the previous one, the exact posterior is no longer attributable to the same exponential family as the previous one, and therefore we need approximations of this type of approximation."}, {"heading": "D. Full Regression Results", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Concrete problems in ai safety", "author": ["Amodei", "Dario", "Olah", "Chris", "Steinhardt", "Jacob", "Christiano", "Paul", "Schulman", "John", "Mane", "Dan"], "venue": "arXiv preprint arXiv:1606.06565,", "citeRegEx": "Amodei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Amodei et al\\.", "year": 2016}, {"title": "Multi-task deep neural network to predict CpG methylation profiles from low-coverage sequencing data", "author": ["C Angermueller", "O. Stegle"], "venue": "In NIPS MLCB workshop,", "citeRegEx": "Angermueller and Stegle,? \\Q2015\\E", "shortCiteRegEx": "Angermueller and Stegle", "year": 2015}, {"title": "Ensemble learning in Bayesian neural networks", "author": ["Barber", "David", "Bishop", "Christopher M"], "venue": "NATO ASI SERIES F COMPUTER AND SYSTEMS SCIENCES,", "citeRegEx": "Barber et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Barber et al\\.", "year": 1998}, {"title": "Weight uncertainty in neural network", "author": ["Blundell", "Charles", "Cornebise", "Julien", "Kavukcuoglu", "Koray", "Wierstra", "Daan"], "venue": "In ICML,", "citeRegEx": "Blundell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Blundell et al\\.", "year": 2015}, {"title": "Deep gaussian processes for regression using approximate expectation propagation", "author": ["Bui", "Thang D", "Hern\u00e1ndez-Lobato", "Daniel", "Li", "Yingzhen", "Jos\u00e9 Miguel", "Turner", "Richard E"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "Bui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bui et al\\.", "year": 2016}, {"title": "Towards evaluating the robustness of neural networks", "author": ["Carlini", "Nicholas", "Wagner", "David"], "venue": "arXiv preprint arXiv:1608.04644,", "citeRegEx": "Carlini et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Carlini et al\\.", "year": 2016}, {"title": "Transforming neural-net output levels to probability distributions", "author": ["Denker", "John", "LeCun", "Yann"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Denker et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Denker et al\\.", "year": 1991}, {"title": "Learning and policy search in stochastic dynamical systems with bayesian neural networks", "author": ["Depeweg", "Stefan", "Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", "Doshi-Velez", "Finale", "Udluft", "Steffen"], "venue": "arXiv preprint arXiv:1605.07127,", "citeRegEx": "Depeweg et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Depeweg et al\\.", "year": 2016}, {"title": "Uncertainty in Deep Learning", "author": ["Gal", "Yarin"], "venue": "PhD thesis, University of Cambridge,", "citeRegEx": "Gal and Yarin.,? \\Q2016\\E", "shortCiteRegEx": "Gal and Yarin.", "year": 2016}, {"title": "Bayesian convolutional neural networks with Bernoulli approximate variational inference", "author": ["Gal", "Yarin", "Ghahramani", "Zoubin"], "venue": "ICLR workshop track,", "citeRegEx": "Gal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2016}, {"title": "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning", "author": ["Gal", "Yarin", "Ghahramani", "Zoubin"], "venue": null, "citeRegEx": "Gal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2016}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Practical variational inference for neural networks", "author": ["Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Graves and Alex.,? \\Q2011\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2011}, {"title": "Neue begr\u00fcndung der theorie quadratischer formen von unendlichvielen ver\u00e4nderlichen", "author": ["Hellinger", "Ernst"], "venue": "Journal fu\u0308r die reine und angewandte Mathematik,", "citeRegEx": "Hellinger and Ernst.,? \\Q1909\\E", "shortCiteRegEx": "Hellinger and Ernst.", "year": 1909}, {"title": "Probabilistic backpropagation for scalable learning of Bayesian neural networks", "author": ["Hernandez-Lobato", "Jose Miguel", "Adams", "Ryan"], "venue": "In ICML,", "citeRegEx": "Hernandez.Lobato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hernandez.Lobato et al\\.", "year": 2015}, {"title": "Black-box alpha divergence minimization", "author": ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", "Li", "Yingzhen", "Hern\u00e1ndezLobato", "Daniel", "Bui", "Thang", "Turner", "Richard E"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2016}, {"title": "Keeping the neural networks simple by minimizing the description length of the weights", "author": ["Hinton", "Geoffrey E", "Van Camp", "Drew"], "venue": "In COLT,", "citeRegEx": "Hinton et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1993}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "An introduction to variational methods for graphical models", "author": ["Jordan", "Michael I", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawrence K"], "venue": "Machine learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Nal", "Blunsom", "Phil"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Modelling uncertainty in deep learning for camera relocalization", "author": ["Kendall", "Alex", "Cipolla", "Roberto"], "venue": "IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Kendall et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kendall et al\\.", "year": 2016}, {"title": "Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding", "author": ["Kendall", "Alex", "Badrinarayanan", "Vijay", "Cipolla", "Roberto"], "venue": "arXiv preprint arXiv:1511.02680,", "citeRegEx": "Kendall et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kendall et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Information theory and statistics", "author": ["Kullback", "Solomon"], "venue": null, "citeRegEx": "Kullback and Solomon.,? \\Q1959\\E", "shortCiteRegEx": "Kullback and Solomon.", "year": 1959}, {"title": "On information and sufficiency", "author": ["Kullback", "Solomon", "Leibler", "Richard A"], "venue": "The annals of mathematical statistics,", "citeRegEx": "Kullback et al\\.,? \\Q1951\\E", "shortCiteRegEx": "Kullback et al\\.", "year": 1951}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["LeCun", "Yann", "Boser", "Bernhard", "Denker", "John S", "Henderson", "Donnie", "Howard", "Richard E", "Hubbard", "Wayne", "Jackel", "Lawrence D"], "venue": "Neural Computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "A tutorial on energy-based learning", "author": ["LeCun", "Yann", "Chopra", "Sumit", "Hadsell", "Raia", "M Ranzato", "F. Huang"], "venue": "Predicting structured data,", "citeRegEx": "LeCun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2006}, {"title": "R\u00e9nyi divergence variational inference", "author": ["Li", "Yingzhen", "Turner", "Richard E"], "venue": "In NIPS,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Stochastic expectation propagation", "author": ["Li", "Yingzhen", "Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", "Turner", "Richard E"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A practical Bayesian framework for backpropagation networks", "author": ["MacKay", "David JC"], "venue": "Neural Computation,", "citeRegEx": "MacKay and JC.,? \\Q1992\\E", "shortCiteRegEx": "MacKay and JC.", "year": 1992}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tom\u00e1\u0161", "Karafi\u00e1t", "Martin", "Burget", "Luk\u00e1\u0161", "\u010cernock\u1ef3", "Jan", "Khudanpur", "Sanjeev"], "venue": "In Eleventh Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Divergence measures and message passing", "author": ["Minka", "Tom"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "Minka and Tom.,? \\Q2005\\E", "shortCiteRegEx": "Minka and Tom.", "year": 2005}, {"title": "Expectation propagation for approximate Bayesian inference", "author": ["T.P. Minka"], "venue": "In Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Minka,? \\Q2001\\E", "shortCiteRegEx": "Minka", "year": 2001}, {"title": "Bayesian learning for neural networks", "author": ["Neal", "Radford M"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "Neal and M.,? \\Q1995\\E", "shortCiteRegEx": "Neal and M.", "year": 1995}, {"title": "cleverhans v1.0.0: an adversarial machine learning library", "author": ["Papernot", "Nicolas", "Goodfellow", "Ian", "Sheatsley", "Ryan", "Feinman", "Reuben", "McDaniel", "Patrick"], "venue": "arXiv preprint arXiv:1610.00768,", "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "On measures of entropy and information", "author": ["R\u00e9nyi", "Alfr\u00e9d"], "venue": "Fourth Berkeley symposium on mathematical statistics and probability,", "citeRegEx": "R\u00e9nyi and Alfr\u00e9d.,? \\Q1961\\E", "shortCiteRegEx": "R\u00e9nyi and Alfr\u00e9d.", "year": 1961}, {"title": "Learning internal representations by error propagation", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Rumelhart et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1985}, {"title": "Edinburgh neural machine translation systems for wmt 16", "author": ["Sennrich", "Rico", "Haddow", "Barry", "Birch", "Alexandra"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "LSTM neural networks for language modeling", "author": ["Sundermeyer", "Martin", "Schl\u00fcter", "Ralf", "Ney", "Hermann"], "venue": "In INTERSPEECH,", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Two problems with variational expectation maximisation for time-series models. Inference and Estimation in Probabilistic Time-Series", "author": ["RE Turner", "M. Sahani"], "venue": null, "citeRegEx": "Turner and Sahani,? \\Q2011\\E", "shortCiteRegEx": "Turner and Sahani", "year": 2011}, {"title": "R\u00e9nyi divergence and Kullback-Leibler divergence", "author": ["Van Erven", "Tim", "Harremo\u00ebs", "Peter"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Erven et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Erven et al\\.", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["L Wan", "M Zeiler", "S Zhang", "Y LeCun", "R. Fergus"], "venue": "In ICML-13,", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Fast predictive image registration", "author": ["Yang", "Xiao", "Kwitt", "Roland", "Niethammer", "Marc"], "venue": "arXiv preprint arXiv:1607.02504,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 22, "context": "Deep learning models have been used to obtain state-ofthe-art results on many tasks (Krizhevsky et al., 2012; Szegedy et al., 2014; Sutskever et al., 2014; Sundermeyer et al., 2012; Mikolov et al., 2010; Kalchbrenner & Blunsom, 2013), and in many pipelines these models have replaced the more traditional Bayesian probabilistic models (Sennrich et al.", "startOffset": 84, "endOffset": 233}, {"referenceID": 42, "context": "Deep learning models have been used to obtain state-ofthe-art results on many tasks (Krizhevsky et al., 2012; Szegedy et al., 2014; Sutskever et al., 2014; Sundermeyer et al., 2012; Mikolov et al., 2010; Kalchbrenner & Blunsom, 2013), and in many pipelines these models have replaced the more traditional Bayesian probabilistic models (Sennrich et al.", "startOffset": 84, "endOffset": 233}, {"referenceID": 41, "context": "Deep learning models have been used to obtain state-ofthe-art results on many tasks (Krizhevsky et al., 2012; Szegedy et al., 2014; Sutskever et al., 2014; Sundermeyer et al., 2012; Mikolov et al., 2010; Kalchbrenner & Blunsom, 2013), and in many pipelines these models have replaced the more traditional Bayesian probabilistic models (Sennrich et al.", "startOffset": 84, "endOffset": 233}, {"referenceID": 40, "context": "Deep learning models have been used to obtain state-ofthe-art results on many tasks (Krizhevsky et al., 2012; Szegedy et al., 2014; Sutskever et al., 2014; Sundermeyer et al., 2012; Mikolov et al., 2010; Kalchbrenner & Blunsom, 2013), and in many pipelines these models have replaced the more traditional Bayesian probabilistic models (Sennrich et al.", "startOffset": 84, "endOffset": 233}, {"referenceID": 31, "context": "Deep learning models have been used to obtain state-ofthe-art results on many tasks (Krizhevsky et al., 2012; Szegedy et al., 2014; Sutskever et al., 2014; Sundermeyer et al., 2012; Mikolov et al., 2010; Kalchbrenner & Blunsom, 2013), and in many pipelines these models have replaced the more traditional Bayesian probabilistic models (Sennrich et al.", "startOffset": 84, "endOffset": 233}, {"referenceID": 38, "context": ", 2010; Kalchbrenner & Blunsom, 2013), and in many pipelines these models have replaced the more traditional Bayesian probabilistic models (Sennrich et al., 2016).", "startOffset": 139, "endOffset": 162}, {"referenceID": 0, "context": "sion model is given an adversarial image (studied below), or to tackle many problems in AI safety (Amodei et al., 2016).", "startOffset": 98, "endOffset": 119}, {"referenceID": 3, "context": "Originating in the 90s (Neal, 1995; MacKay, 1992; Denker & LeCun, 1991), Bayesian neural networks (BNNs) in particular have started gaining in popularity again (Graves, 2011; Blundell et al., 2015; HernandezLobato & Adams, 2015).", "startOffset": 160, "endOffset": 228}, {"referenceID": 3, "context": "Many approximations have been proposed over the years (Denker & LeCun, 1991; Neal, 1995; Graves, 2011; Blundell et al., 2015; Hernandez-Lobato & Adams, 2015; Hern\u00e1ndez-Lobato et al., 2016), some more practical and some less practical.", "startOffset": 54, "endOffset": 188}, {"referenceID": 15, "context": "Many approximations have been proposed over the years (Denker & LeCun, 1991; Neal, 1995; Graves, 2011; Blundell et al., 2015; Hernandez-Lobato & Adams, 2015; Hern\u00e1ndez-Lobato et al., 2016), some more practical and some less practical.", "startOffset": 54, "endOffset": 188}, {"referenceID": 37, "context": "A practical approximation for inference in Bayesian neural networks should be able to scale well to large data and complex models (such as convolutional neural networks (CNNs) (Rumelhart et al., 1985; LeCun et al., 1989)).", "startOffset": 176, "endOffset": 220}, {"referenceID": 26, "context": "A practical approximation for inference in Bayesian neural networks should be able to scale well to large data and complex models (such as convolutional neural networks (CNNs) (Rumelhart et al., 1985; LeCun et al., 1989)).", "startOffset": 176, "endOffset": 220}, {"referenceID": 18, "context": "One possible solution for practical inference in BNNs is variational inference (VI) (Jordan et al., 1999), a ubiquitous technique for approximate inference.", "startOffset": 84, "endOffset": 105}, {"referenceID": 21, "context": "These have been used for machine vision and medical applications (Kendall & Cipolla, 2016; Kendall et al., 2015; Angermueller & Stegle, 2015; Yang et al., 2016).", "startOffset": 65, "endOffset": 160}, {"referenceID": 46, "context": "These have been used for machine vision and medical applications (Kendall & Cipolla, 2016; Kendall et al., 2015; Angermueller & Stegle, 2015; Yang et al., 2016).", "startOffset": 65, "endOffset": 160}, {"referenceID": 17, "context": "Dropout variational inference can be implemented by adding dropout layers (Hinton et al., 2012; Srivastava et al., 2014) before every weight layer in the NN model.", "startOffset": 74, "endOffset": 120}, {"referenceID": 15, "context": "Black-box \u03b1-divergence minimisation (Hern\u00e1ndez-Lobato et al., 2016; Li & Turner, 2016; Minka, 2005) is a class of approximate inference methods extending on VI, approximating EP\u2019s energy function (Minka, 2001) as well as the Hellinger distance (Hellinger, 1909).", "startOffset": 36, "endOffset": 99}, {"referenceID": 33, "context": ", 2016; Li & Turner, 2016; Minka, 2005) is a class of approximate inference methods extending on VI, approximating EP\u2019s energy function (Minka, 2001) as well as the Hellinger distance (Hellinger, 1909).", "startOffset": 136, "endOffset": 149}, {"referenceID": 15, "context": "To implement a complex CNN structure with the inference and code of (Hern\u00e1ndez-Lobato et al., 2016), for example, one would be required to re-implement many already-implemented software tools.", "startOffset": 68, "endOffset": 99}, {"referenceID": 3, "context": "Existing approaches to approximate this posterior are through variational inference (as was done in Hinton & Van Camp (1993); Barber & Bishop (1998); Graves (2011); Blundell et al. (2015)).", "startOffset": 165, "endOffset": 188}, {"referenceID": 17, "context": "Given a (deterministic) neural network, stochastic regularisation techniques in the model (such as dropout (Hinton et al., 2012; Srivastava et al., 2014)) can be interpreted as variational Bayesian approximations in a Bayesian NN with the same network structure (Gal & Ghahramani, 2016b).", "startOffset": 107, "endOffset": 153}, {"referenceID": 45, "context": ", 2014) or dropConnect (Wan et al., 2013)).", "startOffset": 23, "endOffset": 41}, {"referenceID": 15, "context": "In this section we provide a brief review of the black box alpha (BB-\u03b1, Hern\u00e1ndez-Lobato et al. (2016)) method upon which the main derivation in this paper is based.", "startOffset": 72, "endOffset": 103}, {"referenceID": 18, "context": "Popular methods of approximate inference include variational inference (VI) (Jordan et al., 1999) and expectation propagation (EP) (Minka, 2001), where these two algorithms are special cases of power EP (Minka, 2004) that minimises Amari\u2019s \u03b1-divergence (Amari, 1985) D\u03b1[p||q] in a local way:", "startOffset": 76, "endOffset": 97}, {"referenceID": 33, "context": ", 1999) and expectation propagation (EP) (Minka, 2001), where these two algorithms are special cases of power EP (Minka, 2004) that minimises Amari\u2019s \u03b1-divergence (Amari, 1985) D\u03b1[p||q] in a local way:", "startOffset": 41, "endOffset": 54}, {"referenceID": 29, "context": "The recently proposed stochastic EP (Li et al., 2015) and BB-\u03b1 (Hern\u00e1ndezLobato et al.", "startOffset": 36, "endOffset": 53}, {"referenceID": 15, "context": "BB-\u03b1 has been successfully applied to Bayesian neural networks for regression, classification (Hern\u00e1ndez-Lobato et al., 2016) and model-based reinforcement learning (Depeweg et al.", "startOffset": 94, "endOffset": 125}, {"referenceID": 7, "context": ", 2016) and model-based reinforcement learning (Depeweg et al., 2016).", "startOffset": 47, "endOffset": 69}, {"referenceID": 15, "context": "One could verify that this is the same energy function as presented in (Hern\u00e1ndez-Lobato et al., 2016) by considering q an exponential family distribution.", "startOffset": 71, "endOffset": 102}, {"referenceID": 15, "context": "But empirically Hern\u00e1ndez-Lobato et al. (2016) showed that the bias introduced by the MC approximation is often dominated by the variance of the samples, meaning that the effect of the bias is negligible.", "startOffset": 16, "endOffset": 47}, {"referenceID": 15, "context": "The original paper (Hern\u00e1ndez-Lobato et al., 2016) proposed a naive implementation which directly evaluates the MC estimation (4) with samples \u03c9\u0302k \u223c q(\u03c9).", "startOffset": 19, "endOffset": 50}, {"referenceID": 27, "context": "see (LeCun et al., 2006)1.", "startOffset": 4, "endOffset": 24}, {"referenceID": 7, "context": "The Hellinger value could be used to achieve a balance between reducing training error and improving predictive likelihood, which has been found to be desirable (Hern\u00e1ndezLobato et al., 2016; Depeweg et al., 2016).", "startOffset": 161, "endOffset": 213}, {"referenceID": 4, "context": "As a comparison we also include test performances of a BNN with a Gaussian approximation (VI-G) (Li & Turner, 2016), a BNN with HMC, and a sparse Gaussian process model with 50 inducing points (Bui et al., 2016).", "startOffset": 193, "endOffset": 211}, {"referenceID": 11, "context": "The first attack in consideration is the Fast Gradient Sign (FGS) method (Goodfellow et al., 2014).", "startOffset": 73, "endOffset": 98}, {"referenceID": 35, "context": "We use the single gradient step FGS implemented in Cleverhans (Papernot et al., 2016) with the stepsize \u03b7 varied between 0.", "startOffset": 62, "endOffset": 85}], "year": 2017, "abstractText": "To obtain uncertainty estimates with real-world Bayesian deep learning models, practical inference approximations are needed. Dropout variational inference (VI) for example has been used for machine vision and medical applications, but VI can severely underestimates model uncertainty. Alpha-divergences are alternative divergences to VI\u2019s KL objective, which are able to avoid VI\u2019s uncertainty underestimation. But these are hard to use in practice: existing techniques can only use Gaussian approximating distributions, and require existing models to be changed radically, thus are of limited use for practitioners. We propose a re-parametrisation of the alpha-divergence objectives, deriving a simple inference technique which, together with dropout, can be easily implemented with existing models by simply changing the loss of the model. We demonstrate improved uncertainty estimates and accuracy compared to VI in dropout networks. We study our model\u2019s epistemic uncertainty far away from the data using adversarial images, showing that these can be distinguished from non-adversarial images by examining our model\u2019s uncertainty.", "creator": "LaTeX with hyperref package"}}}