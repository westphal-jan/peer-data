{"id": "1703.02949", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning", "abstract": "People can learn a wide range of tasks from their own experience, but can also learn from observing other creatures. This can accelerate acquisition of new skills even when the observed agent differs substantially from the learning agent in terms of morphology. In this paper, we examine how reinforcement learning algorithms can transfer knowledge between morphologically different agents (e.g., different robots). We introduce a problem formulation where two agents are tasked with learning multiple skills by sharing information. Our method uses the skills that were learned by both agents to train invariant feature spaces that can then be used to transfer other skills from one agent to another. The process of learning these invariant feature spaces can be viewed as a kind of \"analogy making\", or implicit learning of partial correspondences between two distinct domains. We evaluate our transfer learning algorithm in two simulated robotic manipulation skills, and illustrate that we can transfer knowledge between simulated robotic arms with different numbers of links, as well as simulated arms with different actuation mechanisms, where one robot is torque-driven while the other is tendon-driven.", "histories": [["v1", "Wed, 8 Mar 2017 18:09:32 GMT  (1041kb,D)", "http://arxiv.org/abs/1703.02949v1", "Published as a conference paper at ICLR 2017"]], "COMMENTS": "Published as a conference paper at ICLR 2017", "reviews": [], "SUBJECTS": "cs.AI cs.RO", "authors": ["abhishek gupta", "coline devin", "yuxuan liu", "pieter abbeel", "sergey levine"], "accepted": true, "id": "1703.02949"}, "pdf": {"name": "1703.02949.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["REINFORCEMENT LEARNING", "Abhishek Gupta", "Coline Devin", "YuXuan Liu", "Pieter Abbeel", "Sergey Levine"], "emails": ["abhigupta@eecs.berkeley.edu", "coline@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu", "yuxuanliu@berkeley.edu", "pieter@openai.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is such that we are in a position, ourselves in a position to assert that we are in a position, ourselves in a position, ourselves in a position, ourselves in a position, in which we are, in which we are, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we, in which we, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we, in which we live, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, we, we, in which we, we, we, we, in which we, we, we, we, we, in which we, we, we, we, we, we, we, we, in which we, we, we, we, we, we, we, in which, we, we, we, we, we, we, we, we, in which, we, we, we, we, we, we, in which, we, we, we, we, we, in which, we, we, we, we, in which, we, we, we, we, in which, we, we, we, in which, we, we, in which, we, we, we, in which, we, we, we, in which, we, we, in which, we, we, we, in which, we, we, we, in which, we, we, in which, we, we, we, we, in which, we, we, in which, we, we, in which, we, we, in which, we, we, in which, we, we, in which, we, we, in which, we, we, in which, we, in which, we, we, we, we, in which, we, we, we, in which, we, we, we, we, in which, we, we, we, we, in which, we, we, we, we, we, in which, we, we, we, we,"}, {"heading": "2 RELATED WORK", "text": "In fact, it is the case that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is a process in"}, {"heading": "3 PROBLEM FORMULATION AND ASSUMPTIONS", "text": "We formalize our transfer problem in a general way by looking at a source domain and a target domain with the designations DS and DT, each corresponding to the Markov decision-making processes (MDPs) DS = (SS, AS, TS, RS) and DT = (ST, AT, TT, RT), each with its own state space S, action space A, dynamics or transition function T and reward function R. In general, the state and action spaces in the two domains could be completely different. Accordingly, the dynamics TS and TT also differ, often dramatically. However, we assume that the reward functions exhibit some structural similarity, since the state distribution of an optimal policy in the source domain resembles the state distribution of an optimal policy in the target domain when projected into a common feature space. Thus, for example, DS corresponds to a robotic arm with 3 connections in one of our experimental tasks, while DT with 4 connections is completely similar to the function of the arms at the end, while the dimensions of the states and the two functions are completely different."}, {"heading": "3.1 COMMON FEATURE SPACES", "text": "We can formalize this assumption of the common feature space as follows: If \u03c0S (sS) denotes the state distribution of the optimal policy in DS and \u03c0T (sT) denotes the state distribution of the optimal policy in DT, it is possible to learn two functions, f and g, so that p (f (sS) = p (g (sT))) for sS \u0445 \u03c0S and sT \u0445 \u03c0T. That is, the images of \u03c0S under f and \u03c0T under g correspond to the same distribution. This assumption is trivial if we allow lossy figures f and g (e.g. if f (sS) = g (sT) = 0 for all sS and sT). However, the less information we lose in f and g, the more informative the common feature will be for the purpose of the transfer. Thus, while we might not be able to fully recover from the image of zepS under f, we can try to learn f and g to maximize the amount of information contained in the common space."}, {"heading": "3.2 LEARNING WITH MULTIPLE SKILLS", "text": "While both agents could in principle learn a common trait space through direct exploration, in this work we assume that the agents have a prior knowledge of each other in the form of other skills they have both learned. This assumption is reasonable, since many practical use cases of transfer involve two agents who have already learned some basic manipulation behaviors, and want to transform the competence of one agent into another. For example, we could transform a particular cooking skill from one domestic robot to another, in an environment where both robots have already learned some basic manipulation behaviors that allow us to transform a common trait space between the two robots into another."}, {"heading": "3.3 ESTIMATING CORRESPONDENCES FROM PROXY SKILL", "text": "This ability is useful for learning which pairs of agent-specific states match in both domains. We want to learn a P pairing, a list of state pairs in both domains that correspond, which is then used for contrastive loss, as in Section 4. These matches could be achieved by an unattended alignment process, but in our method we will examine two simpler approaches, taking advantage of the fact that the capabilities we consider to be episodic."}, {"heading": "3.3.1 TIME-BASED ALIGNMENT", "text": "The first extremely simple approach that we are considering is to say that, with such episodic capabilities, a reasonable approximation can be achieved by assuming that the two agents complete each task roughly equally quickly, and that we can therefore simply couple the states visited at the same time in the two proxy domains."}, {"heading": "3.3.2 ALTERNATING OPTIMIZATION USING DYNAMIC TIME WARPING", "text": "To address this problem, we are formulating an alternating optimization process that is more robust than time-based alignment. This optimization alternates between learning a common attribute space using currently estimated correspondence and re-evaluating correspondence using the currently learned attribute space. We are using dynamic time warping (DTW), as described in Mu \ufffd ller (2007), a well-known method for learning correspondence between sequences that can vary in speed. Dynamic time distortion requires a metric space to compare elements in the sequences to calculate optimal alignment between sequences. In this method, we initialize the weak time-based alignment described in the previous paragraph and use it to learn a common attribute space. This attribute space serves as metric space for DTW to re-evaluate inter-area correspondence."}, {"heading": "4 LEARNING COMMON FEATURE SPACES FOR SKILL TRANSFER", "text": "In this section, we will discuss how to learn shared space using the proxy task, then describe how this shared space can be used for knowledge transfer for a new task, and finally present results that evaluate the transfer on a number of simulated robot control domains. We would like to find functions f and g so that for states sS p and sT p along the optimal guidelines \u03c0S p * and \u03c0T p *, f and g approximately satisfy p (f (sSp, r)) = p (g (sT p, r). If we can find the common attribute space by learning f and g, we can optimize \u03c0T by distributing it over f (sSp, r), where sSp, r \u0432S directly mimic."}, {"heading": "4.1 LEARNING THE EMBEDDING FUNCTIONS FROM A PROXY TASK", "text": "To approximate the request that p (sSp, r)) = p (sSp, r) = p (sSp, r) = p (sT p, r), let's assume that p (sSp, r), p (sSp, r), p (sSp, r), p (sSp, r), p (sSp, r), p (sSp, r), p (sSp, r), p (sSp, r), p (sSp), p (sp), sp (sSp), sp (sSp), p (sp), p (sp), p (sp), p (p), p (p), p (sp), p (sp), p (sp, Sp, Sp, Sp, Sp (, Sp, Sp, Sp, Sp, Sp, Sp, Sp (sp), Sp (sp), Sp (sp), Sp (sp), Sp (sp), Sp (sp), Sp (sp), Sp (sp), Sp (sp), Sp (sp), Sp (sp, Sp, Sp, Sp, Sp (sp), Sp (sp), Sp (sp, Sp, Sp, sp (sp), Sp (sp), Sp (sp, sp (sp), sp (sp, sp (sp), sp (sp, sp (sp), sp (sp, sp (sp), sp (sp, sp (sp), sp (sp, sp (sp), sp (sp, sp (sp), sp (sp), sp (sp (sp), sp (sp, sp, sp, sp), sp (sp (sp, sp), sp (sp (sp), sp (sp), sp (sp (sp, sp, sp, sp), sp (sp, s"}, {"heading": "4.1.1 USING THE COMMON EMBEDDING FOR KNOWLEDGE TRANSFER", "text": "The f and g functions learned with the above approach establish an invariant space across the two additional domains. However, since these functions do not have to be reversible, a direct mapping from a state in the source domain to a state in the target domain is progressively unfeasible. Instead of attempting a direct political transfer, we can adjust the distribution of the optimal trajectories in the target domain to match the source domains under the mappings f and g. Ideally, we want the distributions p (sS, r) and p (sT, r) to match the distribution of trajectories in the target domain as closely as possible, as the source domain under the mappings f and g. Ideally, we would like the distributions p (sT, r) and p (sT, r) to match the target domain. However, it may be necessary for the target state to learn some aspects of the skills from scratch, as not all subtleties are transferred in the present phological differences."}, {"heading": "5 EXPERIMENTS", "text": "The experiments were conducted with the physics simulator MuJoCo (Todorov et al., 2012) to explore a variety of different robots and actuation mechanisms, the embedding functions f and g. Our experiments are three-layered neural networks, each with 60 hidden units and ReLu nonlinearities, which are trained end-to-end with the ADAM optimizer (Kingma & Ba, 2015).Videos of our experiment are available at https: / / sites.google.com / site / invariantfieluretransfer / For details of the algorithm used to enhance learning, see Appendix A."}, {"heading": "5.1 METHODS USED FOR COMPARISON", "text": "In the years to come, we will put ourselves in the position we are in, in order to be able to be in the position we are in."}, {"heading": "5.2 TRANSFER BETWEEN ROBOTS WITH DIFFERENT NUMBERS OF LINKS", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "5.3 TRANSFER BETWEEN TORQUE CONTROLLED AND TENDON CONTROLLED MANIPULATORS", "text": "In order to illustrate the ability of our method to overlook the very different operating mechanisms, we must consider the transmission between a rotating arm and a tendon-driven arm, both with 3 limbs. The rotating arm must be attached to each of its joints, and the state must individually control the elbows and the wrist.The tendon-driven arm shown in Figure 6 uses three tendencies to actuate the joints, the first tendon stretching both to the shoulder and to the elbow.The second and third control takes place over the elbow and the wrist.The last tendon has a driven arm, which is shown in Figure 6."}, {"heading": "5.4 TRANSFER THROUGH IMAGE FEATURES", "text": "In this experimental setup, we evaluate our method of embedding raw pixels instead of robot states. We evaluate our method of transmission via a 3-link and a 4-link robot as in Section 5.2, but use images instead of robots. Since images from the source and target domains have the same size and \"type,\" we allow g = f. We parameterize f as 3 revolutionary layers with 5x5 filters and no pooling. A spatial softmax (Levine et al., 2016) is applied to the output of the third layer in such a way that f outputs show normalized pixel indexes of characteristics on the image."}, {"heading": "6 DISCUSSION AND FUTURE WORK", "text": "The formulation of our transmission problem corresponds to a setting in which two agents (e.g. two different robots) have each learned a collection of skills, with some skills known to only one of the agents and some of them shared by the two. A common skill can be used to learn a space that implicitly brings the agents into correspondence, without assuming that an explicit state can be constructed of space isomorphism. By then mapping a skill known to only one of the agents into this space, the other agent can significantly accelerate his learning of this skill by transferring the common structure. We present an algorithm for learning the common feature spaces by means of a common proxy task and experimentally demonstrate that we can use this method to transfer manipulation skills between different simulated robotic weapons, whereby our experiments should not include the transfer between weapons with different number of links, as well as the transfer from a rotating arm to a driven arm."}, {"heading": "7 APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 REINFORCEMENT LEARNING WITH LOCAL MODELS", "text": "Although we can use any appropriate reinforcement learning algorithm for learning strategies, in this work we use a simple, trajectory-centered reinforcement learning method that trains time-varying linear-Gaussian strategies (Levine & Abbeel, 2014).While this method produces simple strategies, it is very efficient, making it well suited for robotic learning.In order to obtain robotic trajectories for training tasks and source robots, we optimize time-varying linear-Gaussian strategies using a trajectory-centric reinforcement learning algorithm that alternates between adapting local time-varying linear dynamics models and updating time-varying linear-Gaussian strategies using the iterative linear-Gaussian regulator algorithm (Li & Todorov, 2004).This approach is simple and efficient, and is typically able to quickly locate complex high-dimensional learning by using only dozens of skills."}], "references": [{"title": "Reinforcement learning transfer via common subspaces", "author": ["Haitham Bou Ammar", "Matthew E. Taylor"], "venue": "In Adaptive and Learning Agents: International Workshop,", "citeRegEx": "Ammar and Taylor.,? \\Q2012\\E", "shortCiteRegEx": "Ammar and Taylor.", "year": 2012}, {"title": "Unsupervised cross-domain transfer in policy gradient reinforcement learning via manifold alignment", "author": ["Haitham Bou Ammar", "Eric Eaton", "Paul Ruvolo", "Matthew Taylor"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Ammar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2015}, {"title": "Unsupervised cross-domain transfer in policy gradient reinforcement learning via manifold alignment", "author": ["Haitham Bou Ammar", "Eric Eaton", "Paul Ruvolo", "Matthew E Taylor"], "venue": "In Proc. of AAAI,", "citeRegEx": "Ammar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2015}, {"title": "Exploiting task relatedness for multiple task learning", "author": ["Shai Ben-David", "Reba Schuller"], "venue": "In Learning Theory and Kernel Machines,", "citeRegEx": "Ben.David and Schuller.,? \\Q2003\\E", "shortCiteRegEx": "Ben.David and Schuller.", "year": 2003}, {"title": "Reuse of neural modules for general video game playing", "author": ["Alexander Braylan", "Mark Hollenbeck", "Elliot Meyerson", "Risto Miikkulainen"], "venue": "CoRR, abs/1512.01537,", "citeRegEx": "Braylan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Braylan et al\\.", "year": 2015}, {"title": "Multitask learning", "author": ["Rich Caruana"], "venue": "Machine Learning,", "citeRegEx": "Caruana.,? \\Q1997\\E", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Sumit Chopra", "Raia Hadsell", "Yann LeCun"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Learning transferable policies for monocular reactive MAV control", "author": ["Shreyansh Daftry", "J. Andrew Bagnell", "Martial Hebert"], "venue": "In International Symposium on Experimental Robotics (ISER),", "citeRegEx": "Daftry et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Daftry et al\\.", "year": 2016}, {"title": "Learning modular neural network policies for multi-task and multi-robot transfer", "author": ["Coline Devin", "Abhishek Gupta", "Trevor Darrell", "Pieter Abbeel", "Sergey Levine"], "venue": "arXiv preprint arXiv:1609.07088,", "citeRegEx": "Devin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Devin et al\\.", "year": 2016}, {"title": "Mirror neurons responding to observation of actions made with tools in monkey ventral premotor cortex", "author": ["P.F. Ferrari", "S. Rozzi", "L. Fogassi"], "venue": "Journal of Cognitive Neuroscience,", "citeRegEx": "Ferrari et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ferrari et al\\.", "year": 2005}, {"title": "Domain-adversarial training of neural networks", "author": ["Yaroslav Ganin", "Evgeniya Ustinova", "Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "Francois Laviolette", "Mario Marchand", "Victor Lempitsky"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ganin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ganin et al\\.", "year": 2016}, {"title": "Random projections for manifold learning", "author": ["Chinmay Hegde", "Michael Wakin", "Richard Baraniuk"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Hegde et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hegde et al\\.", "year": 2008}, {"title": "Relations between two sets of variates", "author": ["Harold Hotelling"], "venue": null, "citeRegEx": "Hotelling.,? \\Q1936\\E", "shortCiteRegEx": "Hotelling.", "year": 1936}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "A framework for transfer in reinforcement learning", "author": ["George Konidaris"], "venue": "Workshop on Structural Knowledge Transfer for Machine Learning,", "citeRegEx": "Konidaris.,? \\Q2006\\E", "shortCiteRegEx": "Konidaris.", "year": 2006}, {"title": "Autonomous shaping: knowledge transfer in reinforcement learning", "author": ["George Konidaris", "Andrew Barto"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Konidaris and Barto.,? \\Q2006\\E", "shortCiteRegEx": "Konidaris and Barto.", "year": 2006}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["Sergey Levine", "Pieter Abbeel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levine and Abbeel.,? \\Q2014\\E", "shortCiteRegEx": "Levine and Abbeel.", "year": 2014}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Iterative linear quadratic regulator design for nonlinear biological movement systems", "author": ["Weiwei Li", "Emanuel Todorov"], "venue": "ICINCO", "citeRegEx": "Li and Todorov.,? \\Q2004\\E", "shortCiteRegEx": "Li and Todorov.", "year": 2004}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P. Lillicrap", "Jonathan J. Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "CoRR, abs/1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Born to learn: What infants learn from watching", "author": ["Andrew Meltzoff"], "venue": "us. Skillman, NJ: Pediatric Institute Publication,", "citeRegEx": "Meltzoff.,? \\Q1999\\E", "shortCiteRegEx": "Meltzoff.", "year": 1999}, {"title": "Dynamic time warping. Information retrieval for music and motion, pp", "author": ["Meinard M\u00fcller"], "venue": null, "citeRegEx": "M\u00fcller.,? \\Q2007\\E", "shortCiteRegEx": "M\u00fcller.", "year": 2007}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang"], "venue": "IEEE Transactions on knowledge and data engineering,", "citeRegEx": "Pan and Yang.,? \\Q2010\\E", "shortCiteRegEx": "Pan and Yang.", "year": 2010}, {"title": "A preliminary study of transfer learning between unicycle robots", "author": ["Kaizad V Raimalwala", "Bruce A Francis", "Angela P Schoellig"], "venue": "In 2016 AAAI Spring Symposium Series,", "citeRegEx": "Raimalwala et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Raimalwala et al\\.", "year": 2016}, {"title": "The mirror neuron system", "author": ["Giacomo Rizzolatti", "Laila Craighero"], "venue": "Annual Review of Neuroscience,", "citeRegEx": "Rizzolatti and Craighero.,? \\Q2004\\E", "shortCiteRegEx": "Rizzolatti and Craighero.", "year": 2004}, {"title": "Sim-to-real robot learning from pixels with progressive nets", "author": ["Andrei A Rusu", "Matej Vecerik", "Thomas Roth\u00f6rl", "Nicolas Heess", "Razvan Pascanu", "Raia Hadsell"], "venue": "arXiv preprint arXiv:1610.04286,", "citeRegEx": "Rusu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Transfer learning via inter-task mappings for temporal difference learning", "author": ["Matthew Taylor", "Peter Stone", "Yaxin Liu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Taylor et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2007}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["Matthew E. Taylor", "Peter Stone"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Taylor and Stone.,? \\Q2009\\E", "shortCiteRegEx": "Taylor and Stone.", "year": 2009}, {"title": "Transferring instances for model-based reinforcement learning", "author": ["Matthew E. Taylor", "Nicholas K. Jong", "Peter Stone"], "venue": "In Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD),", "citeRegEx": "Taylor et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2008}, {"title": "MuJoCo: A physics engine for model-based control", "author": ["E. Todorov", "T. Erez", "Y. Tassa"], "venue": "In International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Simultaneous deep transfer across domains and tasks", "author": ["Eric Tzeng", "Judy Hoffman", "Trevor Darrell", "Kate Saenko"], "venue": "In International Conference in Computer Vision (ICCV),", "citeRegEx": "Tzeng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tzeng et al\\.", "year": 2015}, {"title": "When pliers become fingers in the monkey motor system", "author": ["M.A. Umilta", "L. Escola", "I. Intskirveli", "F. Grammont", "M. Rochat", "F. Caruana", "A. Jezzini", "V. Gallese", "G. Rizzolatti"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Umilta et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Umilta et al\\.", "year": 2008}, {"title": "Manifold alignment without correspondence", "author": ["Chang Wang", "Sridhar Mahadevan"], "venue": "In IJCAI,", "citeRegEx": "Wang and Mahadevan.,? \\Q2009\\E", "shortCiteRegEx": "Wang and Mahadevan.", "year": 2009}, {"title": "Distance metric learning, with application to clustering with side-information", "author": ["Eric P. Xing", "Andrew Y. Ng", "Michael I. Jordan", "Stuart Russell"], "venue": "In Proceedings of the 15th International Conference on Neural Information Processing Systems,", "citeRegEx": "Xing et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 20, "context": "In fact, human infants learn faster when they observe adults performing a task, even when the adult performs the task differently from the child, and even when the adult performs the task incorrectly (Meltzoff, 1999).", "startOffset": 200, "endOffset": 216}, {"referenceID": 31, "context": "Furthermore, evidence in neuroscience suggests that the parts of the brain in monkeys that respond to the pose of the hand can quickly adapt to instead respond to the pose of the end-effector of a tool held in the hand (Umilta et al., 2008).", "startOffset": 219, "endOffset": 240}, {"referenceID": 9, "context": "Mirror neurons also fire both when the animal performs a task and when it observes another animal performing it (Rizzolatti & Craighero, 2004; Ferrari et al., 2005).", "startOffset": 112, "endOffset": 164}, {"referenceID": 26, "context": "For instance, Taylor et al. (2008) find a mapping between state spaces by searching through all possible pairings.", "startOffset": 14, "endOffset": 35}, {"referenceID": 14, "context": "Konidaris & Barto (2006) learned value functions on subsets of the state representation that were shared between tasks, providing a shaping reward in the target task.", "startOffset": 0, "endOffset": 25}, {"referenceID": 14, "context": "Konidaris & Barto (2006) learned value functions on subsets of the state representation that were shared between tasks, providing a shaping reward in the target task. Taylor et al. (2007) manually construct a function to map a Q-function from one Markov decision process (MDP) to another.", "startOffset": 0, "endOffset": 188}, {"referenceID": 14, "context": "Konidaris & Barto (2006) learned value functions on subsets of the state representation that were shared between tasks, providing a shaping reward in the target task. Taylor et al. (2007) manually construct a function to map a Q-function from one Markov decision process (MDP) to another. Ammar & Taylor (2012) manually define a common feature space between the states of two MDPs, and use this feature space to learn a mapping between states.", "startOffset": 0, "endOffset": 311}, {"referenceID": 1, "context": "Later work by Ammar et al. (2015a) uses unsupervised manifold alignment to assign pairings between states for transfer.", "startOffset": 14, "endOffset": 35}, {"referenceID": 1, "context": "Later work by Ammar et al. (2015a) uses unsupervised manifold alignment to assign pairings between states for transfer. Like in our method, they aim to transfer skills between robots with different configurations and action spaces by guiding exploration in the target domain. The main difference from our work is that Ammar et al. (2015a) assume the presence of a feature mapping that provides distances between states, and use these (hand designed) features to assign correspondences between states in the different domains.", "startOffset": 14, "endOffset": 339}, {"referenceID": 1, "context": "Later work by Ammar et al. (2015a) uses unsupervised manifold alignment to assign pairings between states for transfer. Like in our method, they aim to transfer skills between robots with different configurations and action spaces by guiding exploration in the target domain. The main difference from our work is that Ammar et al. (2015a) assume the presence of a feature mapping that provides distances between states, and use these (hand designed) features to assign correspondences between states in the different domains. In contrast, we assume that good correspondences in episodic tasks can be extracted through time alignment, and focus on learning the feature mapping itself. Additionally, we do not try to learn a direct mapping between state spaces but instead try to learn nonlinear embedding functions into a common feature space, as compared to linear mappings between state spaces learned in Ammar et al. (2015a). In a similar vein, Raimalwala et al.", "startOffset": 14, "endOffset": 927}, {"referenceID": 1, "context": "Later work by Ammar et al. (2015a) uses unsupervised manifold alignment to assign pairings between states for transfer. Like in our method, they aim to transfer skills between robots with different configurations and action spaces by guiding exploration in the target domain. The main difference from our work is that Ammar et al. (2015a) assume the presence of a feature mapping that provides distances between states, and use these (hand designed) features to assign correspondences between states in the different domains. In contrast, we assume that good correspondences in episodic tasks can be extracted through time alignment, and focus on learning the feature mapping itself. Additionally, we do not try to learn a direct mapping between state spaces but instead try to learn nonlinear embedding functions into a common feature space, as compared to linear mappings between state spaces learned in Ammar et al. (2015a). In a similar vein, Raimalwala et al. (2016) consider transfer learning across linear time-invariant (LTI) systems through simple alignment based methods.", "startOffset": 14, "endOffset": 972}, {"referenceID": 5, "context": "In deep learning, Caruana (1997) show that a multitask network can leverage a shared representation of the input to learn multiple tasks more quickly together than separately.", "startOffset": 18, "endOffset": 33}, {"referenceID": 4, "context": "More recent work in deep learning has also looked at transferring policies by reusing policy parameters between environments (Rusu et al., 2016a;b; Braylan et al., 2015; Daftry et al., 2016), using either regularization or novel neural network architectures, though this work has not looked at transfer between agents with structural differences in state, such as different dimensionalities.", "startOffset": 125, "endOffset": 190}, {"referenceID": 7, "context": "More recent work in deep learning has also looked at transferring policies by reusing policy parameters between environments (Rusu et al., 2016a;b; Braylan et al., 2015; Daftry et al., 2016), using either regularization or novel neural network architectures, though this work has not looked at transfer between agents with structural differences in state, such as different dimensionalities.", "startOffset": 125, "endOffset": 190}, {"referenceID": 8, "context": "Our own recent work has looked at morphological differences in the context of multi-agent and multi-task learning (Devin et al., 2016), by reusing neural network components across agent/task combinations.", "startOffset": 114, "endOffset": 134}, {"referenceID": 7, "context": "Our own recent work has looked at morphological differences in the context of multi-agent and multi-task learning (Devin et al., 2016), by reusing neural network components across agent/task combinations. In contrast to that work, which transferred components of policies, our present work aims to learn common feature spaces in situations where we have just two agents. We do not aim to transfer parts of policies themselves, but instead look at shared structure in the states visited by optimal policies, which can be viewed as a kind of analogy making across domains. Learning feature spaces has also been studied in the domain of computer vision as a mechanism for domain adaptation and metric learning. Xing et al. (2002) finds a linear transformation of the input data to satisfy pairwise similarity contraints, while past work by Chopra et al.", "startOffset": 115, "endOffset": 727}, {"referenceID": 6, "context": "(2002) finds a linear transformation of the input data to satisfy pairwise similarity contraints, while past work by Chopra et al. (2005) used Siamese networks to learn a feature space where paired images are brought close together and unpaired images are pushed apart.", "startOffset": 117, "endOffset": 138}, {"referenceID": 6, "context": "(2002) finds a linear transformation of the input data to satisfy pairwise similarity contraints, while past work by Chopra et al. (2005) used Siamese networks to learn a feature space where paired images are brought close together and unpaired images are pushed apart. This enables a semantically meaningful metric space to be learned with only pairs as labels. Later work on domain adaptation by Tzeng et al. (2015) and Ganin et al.", "startOffset": 117, "endOffset": 418}, {"referenceID": 6, "context": "(2002) finds a linear transformation of the input data to satisfy pairwise similarity contraints, while past work by Chopra et al. (2005) used Siamese networks to learn a feature space where paired images are brought close together and unpaired images are pushed apart. This enables a semantically meaningful metric space to be learned with only pairs as labels. Later work on domain adaptation by Tzeng et al. (2015) and Ganin et al. (2016) use an adversarial approach to learn an image embedding that is useful for classification and invariant to the input image\u2019s domain.", "startOffset": 117, "endOffset": 442}, {"referenceID": 8, "context": "A similar partitioning of the state variables was previously discussed by Devin et al. (2016), and is closely related to the agent-space proposed by Konidaris (2006).", "startOffset": 74, "endOffset": 94}, {"referenceID": 8, "context": "A similar partitioning of the state variables was previously discussed by Devin et al. (2016), and is closely related to the agent-space proposed by Konidaris (2006). For simplicity, we will consider a case where there are just two skills: one proxy skill that has been learned by both agents, and one test skill that has been learned by the source agent in the domain DS and is currently being transferred to the target agent in domain DT .", "startOffset": 74, "endOffset": 166}, {"referenceID": 21, "context": "We make use of Dynamic Time Warping (DTW) as described in M\u00fcller (2007), a well known method for learning correspondences across sequences which may vary in speed.", "startOffset": 58, "endOffset": 72}, {"referenceID": 6, "context": "As f and g are parametrized as neural networks, we can optimize them using the similarity loss metric introduced by Chopra et al. (2005):", "startOffset": 116, "endOffset": 137}, {"referenceID": 29, "context": "The experiments were performed in simulation using the MuJoCo physics simulator (Todorov et al., 2012), in order to explore a variety of different robots and actuation mechanisms.", "startOffset": 80, "endOffset": 102}, {"referenceID": 11, "context": "Random projections of data have been found to provide meaningful dimensionality reduction (Hegde et al., 2008).", "startOffset": 90, "endOffset": 110}, {"referenceID": 12, "context": "CCA (Hotelling, 1936) aims to find a basis for the data in which the source data and target data are maximally correlated.", "startOffset": 4, "endOffset": 21}, {"referenceID": 9, "context": "Random projections of data have been found to provide meaningful dimensionality reduction (Hegde et al., 2008). We assign f and g be random projections into spaces of the same dimension, and transfer as described in Section 4.1.1. CCA (Hotelling, 1936) aims to find a basis for the data in which the source data and target data are maximally correlated. We use the matrices that map from state space to the learned basis as f and g. UMA (Wang & Mahadevan (2009), Ammar et al.", "startOffset": 91, "endOffset": 462}, {"referenceID": 1, "context": "UMA (Wang & Mahadevan (2009), Ammar et al. (2015b)) uses pairwise distances between states to align the manifolds of the two domains.", "startOffset": 30, "endOffset": 51}, {"referenceID": 26, "context": "This is representative of a number of prior techniques that attempt to put source and target domains into direct correspondence such as Taylor et al. (2008). In this method, we use the same pairs as we do for our method, estimated from prior experience, but try to map directly from the source domain to the target domain.", "startOffset": 136, "endOffset": 157}, {"referenceID": 19, "context": "Prior work has generally used well-shaped reward functions for tasks of this type, with terms that reward the arm for approaching the object of interest (Lillicrap et al., 2015; Devin et al., 2016).", "startOffset": 153, "endOffset": 197}, {"referenceID": 8, "context": "Prior work has generally used well-shaped reward functions for tasks of this type, with terms that reward the arm for approaching the object of interest (Lillicrap et al., 2015; Devin et al., 2016).", "startOffset": 153, "endOffset": 197}, {"referenceID": 17, "context": "A spatial softmax (Levine et al., 2016) is applied to the output of the third layer such that f outputs normalized pixel indices of feature points on the image.", "startOffset": 18, "endOffset": 39}], "year": 2017, "abstractText": "People can learn a wide range of tasks from their own experience, but can also learn from observing other creatures. This can accelerate acquisition of new skills even when the observed agent differs substantially from the learning agent in terms of morphology. In this paper, we examine how reinforcement learning algorithms can transfer knowledge between morphologically different agents (e.g., different robots). We introduce a problem formulation where two agents are tasked with learning multiple skills by sharing information. Our method uses the skills that were learned by both agents to train invariant feature spaces that can then be used to transfer other skills from one agent to another. The process of learning these invariant feature spaces can be viewed as a kind of \u201canalogy making,\u201d or implicit learning of partial correspondences between two distinct domains. We evaluate our transfer learning algorithm in two simulated robotic manipulation skills, and illustrate that we can transfer knowledge between simulated robotic arms with different numbers of links, as well as simulated arms with different actuation mechanisms, where one robot is torque-driven while the other is tendon-driven.", "creator": "LaTeX with hyperref package"}}}