{"id": "1703.03121", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2017", "title": "Coordinated Multi-Agent Imitation Learning", "abstract": "We study the problem of imitation learning from demonstrations of multiple coordinating agents. One key challenge in this setting is that learning a good model of coordination can be difficult, since coordination is often implicit in the demonstrations and must be inferred as a latent variable. We propose a joint approach that simultaneously learns a latent coordination model along with the individual policies. In particular, our method integrates unsupervised structure learning with conventional imitation learning. We illustrate the power of our approach on a difficult problem of learning multiple policies for fine-grained behavior modeling in team sports, where different players occupy different roles in the coordinated team strategy. We show that having a coordination model to infer the roles of players yields substantially improved imitation loss compared to conventional baselines.", "histories": [["v1", "Thu, 9 Mar 2017 03:45:42 GMT  (2673kb,D)", "http://arxiv.org/abs/1703.03121v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hoang m le", "yisong yue", "peter carr 0001", "patrick lucey"], "accepted": true, "id": "1703.03121"}, "pdf": {"name": "1703.03121.pdf", "metadata": {"source": "META", "title": "Coordinated Multi-Agent Imitation Learning", "authors": ["Hoang M. Le", "Yisong Yue", "Peter Carr"], "emails": ["<hmle@caltech.edu>."], "sections": [{"heading": "1. Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2. Problem Formulation", "text": "\"In coordinated multi-agent imitation, we learn the common policy that maps the common state,\" he says. \"In coordinated multi-agent imitation, we act in coordination to achieve a common goal (or sequence of goals).\" Training Data D consists of several demonstrations of K-agents. Importantly, we can change the identity (or indexing) of K-experts from one demonstration to another. Each (unstructured) series of demonstrations is designated by U \"tU1\" 1 as the context associated with each demonstration sequence. Our ultimate goal is to learn a (largely) decentralized policy, but for clarity, we first pose the problem of learning a fully centralized multi-agent policy. Following the notation of (Ross et al, 2011), we have the common policy that maps the common state mapped."}, {"heading": "3. Learning Approach", "text": "The optimization of (1) is a challenge for two reasons: First, multi-agent imitation must take into account multiple simultaneous learning agents beyond the challenges inherited from single-agent constellations, which is known to result in instationarity in multi-agent reinforcement learning (Busoniu et al., 2008); second, the latent role assignment model that forms the basis for coordination must depend on the measures of learning strategies, which in turn depend on structured role assignment. We propose an alternating optimization approach to solving (1), summarized in Figure 2. The main idea is to combine imitation learning with unattended structural learning by alternately optimizing (i) for imitation strategies and simultaneously defining a structured model (minimizing imitation losses) and (ii) retraining and reallocating the latent structural model while defining learning strategies (maximizing role assignment)."}, {"heading": "3.1. Approach Outline", "text": "We assume that the latent structural model for the calculation of role assignments is formulated as a graphical model.The multi-agent training method Learn uses a reduction approach and can effectively use powerful standard tools such as deep neural networks (Hochreiter & Schmidhuber, 1997).The structural learning method LearnStructure and role assignment Assign components are based on graphi-Algorithm 1 Coordinated Multi-Agent Imitation Learning Input: Multiple unstructured trajectory sets U \"tU1,.., UKuwith Uk,\" kuTt \"1 and context C\" tctuTt \"1. Input: Graphic model q with global / local parameters and e.g. Input: Initialized Strategies, k\" 1.,., K \"Learning sequence: Step size sequence: n.\""}, {"heading": "3.2. Joint Multi-Agent Imitation Learning", "text": "In this section, we describe the learning procedure for multi-agent imitating learning in line 3 of algorithms 1. As background, imitating learning for single agents, we can pursue a new policy that formedAlgorithm 2 Joint Multi-Agent Imitation LearnpA1, A2,., AK, Cq) where a new policy is an optimal or demonstrated action. (A new policy can formedAlgorithm 2 Joint Multi-Agent Imitation LearnpA1, A2,., AK, Cq) Input: Ordered actions Ak \"tat, kuTt\" 1 @ k, Context tctuTt \"1 Input: Initialized policy 1,.., 2) Input: Basis Routine TrainpS, Aq mapping State to actions 1: Set increasing prediction horizon j P t1,. (.) T u 2: for t\" 0, j, 2j., \"T 3: for i.\""}, {"heading": "3.3. Coordination Structure Learning", "text": "The coordination mechanism is based on a latently structured model that regulates role distribution. Training and follow-up procedures aim to solve two main problems: \u2022 LearnStructure: unsupervised learning a probabilistic role assignment model q. \u2022 Assign: how q informs the indexing mechanism so that unstructured trajectories can be mapped to structured trajectories accessible to the algorithm. \u2022 Assign: how q informs the indexing mechanism so that the coordination mechanism underlying each such U can be regulated by a true unknown model p with global parameters. We suppress the agent / policy subscript and consider a generic set of trajectories U \"rut, cts @ t. Let's leave the latent role sequence for the same agent z1: T. At all times, each agent acts according to a latent role.\""}, {"heading": "3.3.1. TRAINING TO LEARN MODEL PARAMETERS", "text": "The optimization proceeds via the alternating optimization of \u03b8 and z. \"Stochastic variational inference\" q \"q\" q \"qq.\" We little abuse notations and overload \u03b8 for the natural parameters of global parameter \u03b8 in the exponential family. Assuming the usual conjugation in the exponential family, the stochastic natural gradient is the convenient form (Hoffman et al., 2013): \u03b8n \"1\" \u03b8np1'\u03c1nq. \"bJEq \u02da pzq rtpz, xqsq, (3) where tpz, xq is the vector of adequate statistics, b is a vector of scaling factors adjusting for the relative size of the mini-batches. Here, the global update assumes that the optimal local update qqqs pzq\" computed.Algorithm 3 coordinates Structure Learning Learning tU1,."}, {"heading": "3.3.2. INFERENCE FOR ROLE AND INDEX ASSIGNMENT", "text": "We can draw two types of conclusions about an learned q: Role assignment. \"we.\" s We can draw two types of conclusions about an learned q: Role assignment. \"s This most likely role sequence supports, kuTt\" 1P t1,., K \"uT, e.g., with Viterbi (or dynamic programming-based forward transmission for graph structures).This most likely role sequence for Agent k, which is the low-dimensional representation of the coordination mechanism, can be used to train the contextual function tctuTt\" 1for each agent's policy. Algorithm 4 Multi-Agent Role Assignment Assignment tU1,.., UK \"qu\" rA1,."}, {"heading": "4. Experiments", "text": "We present empirical results from two scenarios: the first is a synthetic predator-prey setting, where the goal is to imitate a coordinating team of predators; the second is large-scale imitation training based on the trajectories of players in professional football games, where the goal is to imitate defensive team play."}, {"heading": "4.1. Predator-Prey Domain", "text": "In fact, most of us are able to play by the rules we have set ourselves in order to fulfil them."}, {"heading": "4.2. Multi-agent Imitation Learning for Soccer", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight,"}, {"heading": "5. Other Related Work", "text": "The problem of multi-agent imitation learning has not been generally considered, perhaps with the exception of (Chernova & Veloso, 2007), which focused on very different applications and technical challenges (i.e. learning a model of a common task by collecting samples from direct interaction with teleoperative human teachers).The actual learning algorithm there requires the learner to collect enough data points from human teachers to safely classify the task. It is not clear how well the proposed method could be transferred to other areas. index-free political learning is generally difficult for black box machine learning, and some recent work has drawn attention to the importance of learning when input or output are set (Vinyals et al., 2015), motivated by classical algorithmic and geometric problems such as learning to sort a set of numbers or find convex envelopes for a set of points for which no clear index mechanisms exist. Other mutation-invasive approaches (2006 are those for shivary classification)."}, {"heading": "6. Limitations and Future Work", "text": "In principle, the training and conclusion of the latent structural model can accommodate different types of graphical models, but the exact approach varies depending on the graph structure. We rely on first-order Markov models to use well-studied follow-up techniques such as Viterbi. It would be interesting to find areas that can benefit from more general graphical models. Another possible direction is the development of fully integrated, differentiated training methods that can take into account our index-free formulation of policy learning, in particular a method based on deep learning that could provide computational acceleration compared to conventional graphical models. A potential problem with the end-to-end approach is the need to deviate from a learning reduction approach. Although we have dealt with learning from demonstrations in this paper, the proposed framework can also be used for generative modelling or more efficient structured exploration to strengthen learning."}, {"heading": "B. Experimental Evaluation", "text": "B. 1. Batch version of Algorithm 2 for Predator-PreyAlgorithm 5 Multi-Agent Data Aggregation Imitation LearnpA1, A2,..., AK, C | DqInput: Ordered actions Ak \"tat, kuTt\" 1 @ k, context tctuTt \"1 Input: Aggregating data set D1,.., DK for each policy Input: base routine TrainpS, Aq mapping state to ac-tions 1: for t\" 0, 1, 2,.., T do 2: Roll-out a \"t\" 1, k \"\u03c0kps, kq @ agent k 3: cross-update for each policy k P t1,..., Ku s\" 1 \"k\" pra \"t\" 1,1,., T do 2: Roll-out a \"t\" 1, k., \"c.\""}], "references": [{"title": "Variational algorithms for approximate Bayesian inference", "author": ["Beal", "Matthew James"], "venue": "University of London United Kingdom,", "citeRegEx": "Beal and James.,? \\Q2003\\E", "shortCiteRegEx": "Beal and James.", "year": 2003}, {"title": "On optimal cooperation of knowledge sources", "author": ["Benda", "Miroslav"], "venue": "Technical Report BCS-G2010-28,", "citeRegEx": "Benda and Miroslav.,? \\Q1985\\E", "shortCiteRegEx": "Benda and Miroslav.", "year": 1985}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio", "Samy", "Vinyals", "Oriol", "Jaitly", "Navdeep", "Shazeer", "Noam"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "win at home and draw away\u201d: Automatic formation analysis highlighting the differences in home and away team behaviors", "author": ["Bialkowski", "Alina", "Lucey", "Patrick", "Carr", "Peter", "Yue", "Yisong", "Matthews", "Iain"], "venue": "In MIT Sloan Sports Analytics Conference (SSAC),", "citeRegEx": "Bialkowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bialkowski et al\\.", "year": 2014}, {"title": "A comprehensive survey of multiagent reinforcement learning", "author": ["Busoniu", "Lucian", "Babuska", "Robert", "De Schutter", "Bart"], "venue": "IEEE Transactions on Systems Man and Cybernetics Part C Applications and Reviews,", "citeRegEx": "Busoniu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Busoniu et al\\.", "year": 2008}, {"title": "Multiagent collaborative task learning through imitation", "author": ["Chernova", "Sonia", "Veloso", "Manuela"], "venue": "In Proceedings of the fourth International Symposium on Imitation in Animals and Artifacts,", "citeRegEx": "Chernova et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Chernova et al\\.", "year": 2007}, {"title": "Searchbased structured prediction", "author": ["Daum\u00e9 III", "Hal", "Langford", "John", "Marcu", "Daniel"], "venue": "Machine learning,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Performance guarantees for regularized maximum entropy density estimation", "author": ["Dudik", "Miroslav", "Phillips", "Steven J", "Schapire", "Robert E"], "venue": "In International Conference on Computational Learning Theory,", "citeRegEx": "Dudik et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dudik et al\\.", "year": 2004}, {"title": "Semi-supervised learning by entropy minimization", "author": ["Grandvalet", "Yves", "Bengio", "Yoshua"], "venue": "In NIPS,", "citeRegEx": "Grandvalet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Grandvalet et al\\.", "year": 2004}, {"title": "Coordinated reinforcement learning", "author": ["Guestrin", "Carlos", "Lagoudakis", "Michail", "Parr", "Ronald"], "venue": "In ICML,", "citeRegEx": "Guestrin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2002}, {"title": "Generative adversarial imitation learning", "author": ["Ho", "Jonathan", "Ermon", "Stefano"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ho et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Structured stochastic variational inference", "author": ["Hoffman", "Matt", "Blei", "David"], "venue": "CoRR abs/1404.4114,", "citeRegEx": "Hoffman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2014}, {"title": "Stochastic variational inference", "author": ["Hoffman", "Matthew D", "Blei", "David M", "Wang", "Chong", "Paisley", "John William"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Information theory and statistical mechanics", "author": ["Jaynes", "Edwin T"], "venue": "Physical review,", "citeRegEx": "Jaynes and T.,? \\Q1957\\E", "shortCiteRegEx": "Jaynes and T.", "year": 1957}, {"title": "Stochastic variational inference for bayesian time series models", "author": ["Johnson", "Matthew James", "Willsky", "Alan S"], "venue": "In ICML, pp", "citeRegEx": "Johnson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Index-free multi-agent systems: An eulerian approach", "author": ["Kingston", "Peter", "Egerstedt", "Magnus"], "venue": "IFAC Proceedings Volumes,", "citeRegEx": "Kingston et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kingston et al\\.", "year": 2010}, {"title": "Path planning for permutation-invariant multirobot formations", "author": ["Kloder", "Stephen", "Hutchinson", "Seth"], "venue": "IEEE Transactions on Robotics,", "citeRegEx": "Kloder et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kloder et al\\.", "year": 2006}, {"title": "Multi-robot decision making using coordination graphs", "author": ["Kok", "Jelle R", "Spaan", "Matthijs TJ", "Vlassis", "Nikos"], "venue": "In Proceedings of the 11th International Conference on Advanced Robotics, ICAR,", "citeRegEx": "Kok et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kok et al\\.", "year": 2003}, {"title": "Smooth imitation learning for online sequence prediction", "author": ["Le", "Hoang", "Kang", "Andrew", "Yue", "Yisong", "Carr", "Peter"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Le et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Le et al\\.", "year": 2016}, {"title": "Datadriven ghosting using deep imitation learning", "author": ["Le", "Hoang M", "Carr", "Peter", "Yue", "Yisong", "Lucey", "Patrick"], "venue": "In MIT Sloan Sports Analytics Conference (SSAC),", "citeRegEx": "Le et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Le et al\\.", "year": 2017}, {"title": "Cooperative multi-agent learning: The state of the art", "author": ["Panait", "Liviu", "Luke", "Sean"], "venue": "Autonomous agents and multi-agent systems,", "citeRegEx": "Panait et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Panait et al\\.", "year": 2005}, {"title": "Combinatorial optimization: algorithms and complexity", "author": ["Papadimitriou", "Christos H", "Steiglitz", "Kenneth"], "venue": "Courier Corporation,", "citeRegEx": "Papadimitriou et al\\.,? \\Q1982\\E", "shortCiteRegEx": "Papadimitriou et al\\.", "year": 1982}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross", "Stephane", "Gordon", "Geoff", "Bagnell", "J. Andrew"], "venue": "In Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Permutation invariant svms", "author": ["Shivaswamy", "Pannagadatta K", "Jebara", "Tony"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Shivaswamy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Shivaswamy et al\\.", "year": 2006}, {"title": "What\u2019s hot at RoboCup", "author": ["Stone", "Peter"], "venue": "In The AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Stone and Peter.,? \\Q2016\\E", "shortCiteRegEx": "Stone and Peter.", "year": 2016}, {"title": "Multiagent systems: A survey from a machine learning perspective", "author": ["Stone", "Peter", "Veloso", "Manuela"], "venue": "Autonomous Robots,", "citeRegEx": "Stone et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Stone et al\\.", "year": 2000}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Order matters: Sequence to sequence for sets", "author": ["Vinyals", "Oriol", "Bengio", "Samy", "Kudlur", "Manjunath"], "venue": "arXiv preprint arXiv:1511.06391,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "(Bialkowski et al., 2014), now enables the possibility of learning multi-agent policies from demonstrations, also known as multi-agent imitation learning.", "startOffset": 0, "endOffset": 25}, {"referenceID": 9, "context": "Many realistic multi-agent settings require coordination among collaborative agents to achieve some common goal (Guestrin et al., 2002; Kok et al., 2003).", "startOffset": 112, "endOffset": 153}, {"referenceID": 19, "context": "Many realistic multi-agent settings require coordination among collaborative agents to achieve some common goal (Guestrin et al., 2002; Kok et al., 2003).", "startOffset": 112, "endOffset": 153}, {"referenceID": 24, "context": "Following the notation of (Ross et al., 2011), let ~\u03c0p~sq :\u201c ~a denote the joint policy that maps the joint state, ~s \u201c rs1, .", "startOffset": 26, "endOffset": 45}, {"referenceID": 4, "context": "First, beyond the challenges inherited from single-agent settings, multi-agent imitation learning must account for multiple simultaneously learning agents, which is known to cause non-stationarity for multi-agent reinforcement learning (Busoniu et al., 2008).", "startOffset": 236, "endOffset": 258}, {"referenceID": 13, "context": "For efficient training, we employ alternating stochastic optimization (Hoffman et al., 2013; Johnson & Willsky, 2014; Beal, 2003) on the same mini-batches.", "startOffset": 70, "endOffset": 129}, {"referenceID": 2, "context": "The algorithm optionally includes a mixing step on line 5, where the rolled-out trajectories may replace the training trajectories with increasing probability approaching 1, which is similar to scheduled sampling (Bengio et al., 2015), and may help stabilize learning in the early phase of the algorithm.", "startOffset": 213, "endOffset": 234}, {"referenceID": 24, "context": ",Dn (Ross et al., 2011).", "startOffset": 4, "endOffset": 23}, {"referenceID": 2, "context": "While LSTMs are very good at sequence-to-sequence prediction tasks, they cannot naturally deal with the drifting of input state distribution drift caused by action output feedback in dynamical systems (Bengio et al., 2015).", "startOffset": 201, "endOffset": 222}, {"referenceID": 4, "context": "This non-stationarity issue also arises in multiagent reinforcement learning (Busoniu et al., 2008) when agents learn simultaneously.", "startOffset": 77, "endOffset": 99}, {"referenceID": 20, "context": "In practice, dynamic oracle feedback is very expensive to obtain and some recent work have attempted to relax this requirement (Le et al., 2016; Ho & Ermon, 2016).", "startOffset": 127, "endOffset": 162}, {"referenceID": 24, "context": "The Appendix shows how to use DAgger (Ross et al., 2011) for Algorithm 2, which we used for our synthetic experiment.", "startOffset": 37, "endOffset": 56}, {"referenceID": 13, "context": "In particular, we employ techniques from stochastic variational inference (Hoffman et al., 2013), which allows for efficient stochastic training on mini-batches that can naturally integrate with our imitation learning subroutine.", "startOffset": 74, "endOffset": 96}, {"referenceID": 13, "context": "Assuming the usual conjugacy in the exponential family, the stochastic natural gradient takes the convenient form (Hoffman et al., 2013):", "startOffset": 114, "endOffset": 136}, {"referenceID": 24, "context": "The experts thus act as dynamic oracles, which result in a multi-agent training setting analogous to DAgger (Ross et al., 2011).", "startOffset": 108, "endOffset": 127}, {"referenceID": 3, "context": "In this experiment, we aim to learn multi-agent policies for team soccer defense, based on tracking data from real-life professional soccer (Bialkowski et al., 2014).", "startOffset": 140, "endOffset": 165}, {"referenceID": 24, "context": ", 2009) and DAgger (Ross et al., 2011), we gradually increase the horizon of the rolled-out trajectories from 1 to 10 steps lookahead.", "startOffset": 19, "endOffset": 38}, {"referenceID": 21, "context": "We note that learned policies are qualitatively similar to the ground truth demonstrations, and can be useful for applications such as counterfactual replay analysis (Le et al., 2017).", "startOffset": 166, "endOffset": 183}, {"referenceID": 29, "context": "Some recent work has called attention to the importance of order to learning when input or output are sets (Vinyals et al., 2015), motivated by classic algorithmic and geometric problems such as learning to sort a set of numbers, or finding convex hull for a set of points, where no clear indexing mechanism exists.", "startOffset": 107, "endOffset": 129}], "year": 2017, "abstractText": "We study the problem of imitation learning from demonstrations of multiple coordinating agents. One key challenge in this setting is that learning a good model of coordination can be difficult, since coordination is often implicit in the demonstrations and must be inferred as a latent variable. We propose a joint approach that simultaneously learns a latent coordination model along with the individual policies. In particular, our method integrates unsupervised structure learning with conventional imitation learning. We illustrate the power of our approach on a difficult problem of learning multiple policies for finegrained behavior modeling in team sports, where different players occupy different roles in the coordinated team strategy. We show that having a coordination model to infer the roles of players yields substantially improved imitation loss compared to conventional baselines.", "creator": "LaTeX with hyperref package"}}}