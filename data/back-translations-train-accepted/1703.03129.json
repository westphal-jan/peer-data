{"id": "1703.03129", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2017", "title": "Learning to Remember Rare Events", "abstract": "Despite recent advances, memory-augmented deep neural networks are still limited when it comes to life-long and one-shot learning, especially in remembering rare events. We present a large-scale life-long memory module for use in deep learning. The module exploits fast nearest-neighbor algorithms for efficiency and thus scales to large memory sizes. Except for the nearest-neighbor query, the module is fully differentiable and trained end-to-end with no extra supervision. It operates in a life-long manner, i.e., without the need to reset it during training.", "histories": [["v1", "Thu, 9 Mar 2017 04:36:15 GMT  (89kb,D)", "http://arxiv.org/abs/1703.03129v1", "Conference paper accepted for ICLR'17"]], "COMMENTS": "Conference paper accepted for ICLR'17", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["{\\l}ukasz kaiser", "ofir nachum", "aurko roy", "samy bengio"], "accepted": true, "id": "1703.03129"}, "pdf": {"name": "1703.03129.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["REMEMBER RARE EVENTS", "\u0141ukasz Kaiser", "Ofir Nachum", "Aurko Roy", "Samy Bengio"], "emails": ["lukaszkaiser@google.com", "ofirnachum@google.com", "aurko@gatech.edu", "bengio@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, the majority of us are in a position to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "2 MEMORY MODULE", "text": "Our memory consists of a matrix K of the memory keys, a vector V of the memory values, and an additional vector A, which tracks the age of the items stored in memory. Keys can be any vector of the size of the key size, and we assume that the memory values are individual integers representing a class or token ID. We define a memory of the size memory size as triple: M = (Kmemory size, Vmemory size, Amemory size).A memory query is a vector of the size key size, which we assume to be normalized, i.e., we define the closest neighbor of the q size as one of the keys that maximize the Dot product with q: NN (q, M) = argmaxi q \u00b7 K [i]. Since the keys are normalized, the above notion corresponds to the closest neighbor in terms of cosmic similarity."}, {"heading": "2.1 USING THE MEMORY MODULE", "text": "This year it is so far that it will only be a matter of time before an agreement is reached."}, {"heading": "3 RELATED WORK", "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "4 EXPERIMENTS", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "5 DISCUSSION", "text": "We presented a long-term memory module that can be used for lifelong learning. It is versatile, so it can be added to different deep-learning models and at different levels to give networks one-shot learning capability. Several parts of the presented memory module could be optimized and studied in more detail. We could parameterize the update rule that averages the query with the right key. Instead of returning only to the nearest neighbor, we could also return a number of them to be processed by other network layers. We will leave these questions to future research. However, the main problem we have encountered is that evaluating one-shot learning is difficult because standard metrics do not focus on this scenario. In this work, we have adjusted the standard metrics to examine our approach."}], "references": [{"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "47th Annual IEEE Symposium on Foundations of Computer Science", "citeRegEx": "Andoni and Indyk.,? \\Q2006\\E", "shortCiteRegEx": "Andoni and Indyk.", "year": 2006}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Large-scale simple question answering with memory", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston"], "venue": "networks. CoRR,", "citeRegEx": "Bordes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Hierarchical memory networks", "author": ["Sarath Chandar", "Sungjin Ahn", "Hugo Larochelle", "Pascal Vincent", "Gerald Tesauro", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1605.07427,", "citeRegEx": "Chandar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chandar et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "CoRR, abs/1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "One-shot learning of object categories", "author": ["Li Fei-Fei", "Rob Fergus", "Pietro Perona"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Fei.Fei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Fei.Fei et al\\.", "year": 2006}, {"title": "Dynamic neural turing machine with soft and hard addressing schemes", "author": ["\u00c7aglar G\u00fcl\u00e7ehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "G\u00fcl\u00e7ehre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "G\u00fcl\u00e7ehre et al\\.", "year": 2016}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": "representations. CoRR,", "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George Dahl", "Abdelrahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara Sainath", "Brian Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["Piotr Indyk", "Rajeev Motwani"], "venue": "In Proceedings of the thirtieth annual ACM symposium on Theory of computing,", "citeRegEx": "Indyk and Motwani.,? \\Q1998\\E", "shortCiteRegEx": "Indyk and Motwani.", "year": 1998}, {"title": "Can active memory replace attention", "author": ["\u0141ukasz Kaiser", "Samy Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kaiser and Bengio.,? \\Q2016\\E", "shortCiteRegEx": "Kaiser and Bengio.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Siamese neural networks for one-shot image recognition", "author": ["Gregory Koch"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "Koch.,? \\Q2015\\E", "shortCiteRegEx": "Koch.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural network", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Order matters: Sequence to sequence for sets", "author": ["Manjunath Kudlur Oriol Vinyals", "Samy Bengio"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Vinyals and Bengio.,? \\Q2016\\E", "shortCiteRegEx": "Vinyals and Bengio.", "year": 2016}, {"title": "Matching networks for one shot learning", "author": ["Charles Blundell"], "venue": "CoRR, abs/1606.04080,", "citeRegEx": "Vinyals and Blundell.,? \\Q2016\\E", "shortCiteRegEx": "Vinyals and Blundell.", "year": 2016}, {"title": "Scaling memory-augmented neural networks with sparse reads and writes", "author": ["Jack W Rae", "Jonathan J Hunt", "Tim Harley", "Ivo Danihelka", "Andrew Senior", "Greg Wayne", "Alex Graves", "Timothy P Lillicrap"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rae et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rae et al\\.", "year": 2016}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["Florian Schroff", "Dmitry Kalenichenko", "James Philbin"], "venue": "In CVPR, pp", "citeRegEx": "Schroff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schroff et al\\.", "year": 2015}, {"title": "Weakly supervised memory", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": "networks. CoRR,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Matching networks for one shot learning", "author": ["Oriol Vinyals", "Charles Blundell", "Timothy P. Lillicrap", "Koray Kavukcuoglu", "Daan Wierstra"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2016}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["Kilian Q Weinberger", "Lawrence K Saul"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Weinberger and Saul.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul.", "year": 2009}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence,", "citeRegEx": "Weston et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2011}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Lei Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Reinforcement learning neural turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "CoRR, abs/1505.00521,", "citeRegEx": "Zaremba and Sutskever.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "Machine learning systems have been successful in many domains, from computer vision (Krizhevsky et al., 2012) to speech recognition (Hinton et al.", "startOffset": 84, "endOffset": 109}, {"referenceID": 8, "context": ", 2012) to speech recognition (Hinton et al., 2012) and machine translation (Sutskever et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 20, "context": ", 2012) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014).", "startOffset": 32, "endOffset": 97}, {"referenceID": 1, "context": ", 2012) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014).", "startOffset": 32, "endOffset": 97}, {"referenceID": 4, "context": ", 2012) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014).", "startOffset": 32, "endOffset": 97}, {"referenceID": 23, "context": "(2015) and similar to many other distance metric learning works (Weinberger & Saul, 2009; Weston et al., 2011).", "startOffset": 64, "endOffset": 110}, {"referenceID": 18, "context": "This definition and reasoning behind it are almost identical to the one in Schroff et al. (2015) and similar to many other distance metric learning works (Weinberger & Saul, 2009; Weston et al.", "startOffset": 75, "endOffset": 97}, {"referenceID": 20, "context": "Such sequence-to-sequence recurrent neural networks (RNNs) with long short-term memory (LSTM) cells (Hochreiter & Schmidhuber, 1997) have proven especially successful at natural language processing (NLP) tasks, including machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014).", "startOffset": 241, "endOffset": 306}, {"referenceID": 1, "context": "Such sequence-to-sequence recurrent neural networks (RNNs) with long short-term memory (LSTM) cells (Hochreiter & Schmidhuber, 1997) have proven especially successful at natural language processing (NLP) tasks, including machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014).", "startOffset": 241, "endOffset": 306}, {"referenceID": 4, "context": "Such sequence-to-sequence recurrent neural networks (RNNs) with long short-term memory (LSTM) cells (Hochreiter & Schmidhuber, 1997) have proven especially successful at natural language processing (NLP) tasks, including machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014).", "startOffset": 241, "endOffset": 306}, {"referenceID": 1, "context": "Augmenting recurrent neural networks with attention (Bahdanau et al., 2014) can be interpreted as creating a large memory component that allows content-based addressing.", "startOffset": 52, "endOffset": 75}, {"referenceID": 1, "context": "Augmenting recurrent neural networks with attention (Bahdanau et al., 2014) can be interpreted as creating a large memory component that allows content-based addressing. More generally, Graves et al. (2014) augmented a recurrent neural network with a computing-inspired memory component that can be addressed via both content- and address-based queries.", "startOffset": 53, "endOffset": 207}, {"referenceID": 1, "context": "Augmenting recurrent neural networks with attention (Bahdanau et al., 2014) can be interpreted as creating a large memory component that allows content-based addressing. More generally, Graves et al. (2014) augmented a recurrent neural network with a computing-inspired memory component that can be addressed via both content- and address-based queries. Sukhbaatar et al. (2015) present a similar augmentation and show the importance of allowing multiple reads and writes to memory between inputs.", "startOffset": 53, "endOffset": 379}, {"referenceID": 24, "context": "Some attempts have been made at making hard access queries to memory (Zaremba & Sutskever, 2015; Xu et al., 2015), but it was usually challenging to match the soft version.", "startOffset": 69, "endOffset": 113}, {"referenceID": 6, "context": "Recently, more successful training for hard queries was reported (G\u00fcl\u00e7ehre et al., 2016) that makes use of a curriculum strategy that mixes soft and hard queries at training time.", "startOffset": 65, "endOffset": 88}, {"referenceID": 2, "context": ", 2014) and later work on scaling it (Bordes et al., 2015; Chandar et al., 2016) used memory with size in the millions.", "startOffset": 37, "endOffset": 80}, {"referenceID": 3, "context": ", 2014) and later work on scaling it (Bordes et al., 2015; Chandar et al., 2016) used memory with size in the millions.", "startOffset": 37, "endOffset": 80}, {"referenceID": 2, "context": ", 2014) and later work on scaling it (Bordes et al., 2015; Chandar et al., 2016) used memory with size in the millions. The cost of doing so is that the memory must be fixed prior to training. Moreover, since during the beginning of training the model is unlikely to query the memory correctly, strong supervision is used to encourage the model to query memory locations that are useful. These hints are either given as additional supervising information by the task or determined heuristically as in Hill et al. (2015). All the work discussed so far has either used a memory that is fixed before training or used a memory that is not persistent between different examples.", "startOffset": 38, "endOffset": 520}, {"referenceID": 2, "context": ", 2014) and later work on scaling it (Bordes et al., 2015; Chandar et al., 2016) used memory with size in the millions. The cost of doing so is that the memory must be fixed prior to training. Moreover, since during the beginning of training the model is unlikely to query the memory correctly, strong supervision is used to encourage the model to query memory locations that are useful. These hints are either given as additional supervising information by the task or determined heuristically as in Hill et al. (2015). All the work discussed so far has either used a memory that is fixed before training or used a memory that is not persistent between different examples. For one-shot and lifelong learning, a memory must necessarily be both volatile during training and persistent between examples. To bridge this gap, Santoro et al. (2016) propose to partition training into distinct episodes consisting of a sequence of labelled examples {(xi, yi)}i=1.", "startOffset": 38, "endOffset": 844}, {"referenceID": 2, "context": ", 2014) and later work on scaling it (Bordes et al., 2015; Chandar et al., 2016) used memory with size in the millions. The cost of doing so is that the memory must be fixed prior to training. Moreover, since during the beginning of training the model is unlikely to query the memory correctly, strong supervision is used to encourage the model to query memory locations that are useful. These hints are either given as additional supervising information by the task or determined heuristically as in Hill et al. (2015). All the work discussed so far has either used a memory that is fixed before training or used a memory that is not persistent between different examples. For one-shot and lifelong learning, a memory must necessarily be both volatile during training and persistent between examples. To bridge this gap, Santoro et al. (2016) propose to partition training into distinct episodes consisting of a sequence of labelled examples {(xi, yi)}i=1. A network augmented with a fully-differentiable memory is trained to predict yi given the previous sequence (x1, y1, . . . , xi\u22121). This way, the model learns to store important examples with their corresponding labels in memory and later re-use this information to correctly classify new examples. This model successfully exhibits one-shot learning on Omniglot. However, this approach again requires fully-differentiable memory access and thus limits the size of the memory as well as the length of an episode. This restriction has recently been alleviated by Rae et al. (2016). Their model can utilize large memories, but unlike our work does not have an explicit cost to guide the formation of memory keys.", "startOffset": 38, "endOffset": 1537}, {"referenceID": 5, "context": "Early work utilized Bayesian methods to model data generatively (Fei-Fei et al., 2006; Lake et al., 2011).", "startOffset": 64, "endOffset": 105}, {"referenceID": 13, "context": "One early neural network approach to one-shot learning was given by Siamese networks (Koch, 2015).", "startOffset": 85, "endOffset": 97}, {"referenceID": 5, "context": "Early work utilized Bayesian methods to model data generatively (Fei-Fei et al., 2006; Lake et al., 2011). The paper that introduced the Omniglot dataset (Lake et al., 2011) approached the task with a generative model for strokes. This way, given a single character image, the probability of a different image being of the same character may be approximated via standard techniques. One early neural network approach to one-shot learning was given by Siamese networks (Koch, 2015). When our approach is applied to the Omniglot image classification dataset, the resulting training algorithm is actually similar to that of Siamese networks. The only difference is in the loss function: Siamese networks utilize a cross-entropy loss whereas our method uses a margin triplet loss. A more sophisticated neural network approach is given by Vinyals et al. (2016). The strengths of this approach are (1) the model architecture utilizes recent advances in attention-augmented neural networks for set-to-set learning (Oriol Vinyals, 2016a), and (2) the training algorithm is designed to exactly match the testing phase (given k distinct images and an additional image, the model must predict which of the k images is of the same class as the additional image).", "startOffset": 65, "endOffset": 856}], "year": 2017, "abstractText": "Despite recent advances, memory-augmented deep neural networks are still limited when it comes to life-long and one-shot learning, especially in remembering rare events. We present a large-scale life-long memory module for use in deep learning. The module exploits fast nearest-neighbor algorithms for efficiency and thus scales to large memory sizes. Except for the nearest-neighbor query, the module is fully differentiable and trained end-to-end with no extra supervision. It operates in a life-long manner, i.e., without the need to reset it during training. Our memory module can be easily added to any part of a supervised neural network. To show its versatility we add it to a number of networks, from simple convolutional ones tested on image classification to deep sequence-to-sequence and recurrent-convolutional models. In all cases, the enhanced network gains the ability to remember and do life-long one-shot learning. Our module remembers training examples shown many thousands of steps in the past and it can successfully generalize from them. We set new state-of-the-art for one-shot learning on the Omniglot dataset and demonstrate, for the first time, life-long one-shot learning in recurrent neural networks on a large-scale machine translation task.", "creator": "LaTeX with hyperref package"}}}