{"id": "1703.03365", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2017", "title": "Learning Active Learning from Data", "abstract": "In this paper, we suggest a novel data-driven approach to active learning: Learning Active Learning (LAL). The key idea behind LAL is to train a regressor that predicts the expected error reduction for a potential sample in a particular learning state. By treating the query selection procedure as a regression problem we are not restricted to dealing with existing AL heuristics; instead, we learn strategies based on experience from previous active learning experiments. We show that LAL can be learnt from a simple artificial 2D dataset and yields strategies that work well on real data from a wide range of domains. Moreover, if some domain-specific samples are available to bootstrap active learning, the LAL strategy can be tailored for a particular problem.", "histories": [["v1", "Thu, 9 Mar 2017 17:36:52 GMT  (970kb,D)", "http://arxiv.org/abs/1703.03365v1", null], ["v2", "Fri, 31 Mar 2017 07:33:28 GMT  (973kb,D)", "http://arxiv.org/abs/1703.03365v2", null], ["v3", "Fri, 14 Jul 2017 12:59:12 GMT  (803kb,D)", "http://arxiv.org/abs/1703.03365v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ksenia konyushkova", "raphael sznitman", "pascal fua"], "accepted": true, "id": "1703.03365"}, "pdf": {"name": "1703.03365.pdf", "metadata": {"source": "META", "title": "Learning Active Learning from Real and Synthetic Data", "authors": ["Ksenia Konyushkova", "Raphael Sznitman", "Pascal Fua"], "emails": ["KSENIA.KONYUSHKOVA@EPFL.CH", "RAPHAEL.SZNITMAN@ARTORG.UNIBE.CH", "PASCAL.FUA@EPFL.CH"], "sections": [{"heading": "1. Introduction", "text": "Many modern machine learning methods require large amounts of training data to reach their full potential. However, annotated data is difficult and expensive to obtain, especially in specialized areas where only experts whose time is scarce and precious can provide reliable annotations. Active Learning (AL) is an established way to simplify the data collection process by automatically deciding which instances an annotator should mark in order to train an algorithm as quickly as possible and with minimal annotation effort. Over the years, many AL strategies have been developed for different tasks without one of them clearly surpassing all others in all cases. Consequently, a number of approaches have been proposed to automatically select the best strategy. Recent examples include bandit algorithms (Baram et al., 2004; Hsu & Lin, 2015), aggregation strategies whose experience can be transferred between domains (Chu & Lin, 2016), and approaches based on reinforcement learning."}, {"heading": "2. Related Work", "text": "In fact, it is the case that you are able to play by the rules and that you are able to break the rules."}, {"heading": "3. Towards Data Driven Active Learning", "text": "In this section, we briefly present the standard framework Active Leaning (AL), together with uncertainty samples (US), the most commonly used heuristics for their implementation (Tong & Koller, 2002; Joshi et al., 2009; Settles, 2010; Yang et al., 2015).Despite its simplicity, US often works remarkably well in practice, but we will show that it only exhibits optimal behavior in a limited number of situations, thus motivating the fact that you cannot rely on a single heuristic, no matter how well."}, {"heading": "3.1. Active Learning (AL)", "text": "In practice, this means that instead of asking experts to comment on all available data, we iteratively select which data points should be commented on next..., (xN, yN)}, where xi is a d-dimensional attribute representing a datapoint and yi, {0, 1} is its binary label. We choose a classification system f, which is on some Lt, y1),., (xN, yN)}, where xi is a d-dimensional representation of a datapoint and yi, {0, 1} is its binary label. We choose a classification system f, which is on some Lt, y1), to map the characteristics on the labels fLt (xi) = y, yx, x, x, x, x, x, x, x, x, x, etc."}, {"heading": "3.2. Uncertainty Sampling", "text": "It has been reported that the US is successful in numerous scenarios and environments (Tong & Koller, 2002; Joshi et al., 2009; Settles, 2010; Yang et al., 2015; Konyushkova et al., 2015; Mosinska et al., 2016). As discussed in Section 2, it is based on the selection of initial samples about which the current classifier is least certain. There are many definitions of maximum uncertainty, but one of the most widely accepted is the selection of samples that maximize entropy H over the predicted classes. In other words, it means that x * is taken to endure maximum xi-UtH [p (yi = y | xi, Lt)]. (1)"}, {"heading": "3.3. Success, Failure, and Motivation", "text": "We start with a simple logistic regression on L0 and then test it on L0. We have the possibility that we will be able to detect a simple two-dimensional error situation, which consists of two different classes. (The data in each cloud comes from a different mean, but the same variance is assumed to be a dataset D. \"We further assume that a dataset D.\" from the same distribution has the same number of points available as shown in Figure 1 (a). (The data in each cloud comes from a different mean, but the same variance.) We assume that a dataset D. \"We can use the AL procedure of each class and their respective labels L0 = (x1, 0), (x2, 1)}. We include the remaining dataset D.\" We start with a simple logistic regression on L0 and then on L0. \""}, {"heading": "4. Approach", "text": "We are now formulating LAL as a regression problem. We are modelling the dependence between the state of learning and the expected greedy improvement of the generalization error. To this end, we are simulating a Monte Carlo-style online learning process. We are further expanding the approach to include the one provided by AL.Specifically, we divide the data set into Training D and Tests D \u2032 data, and use a small subset of labels from D to train an initial classifier whose performance is tested on D. A number of parameters that characterize the state of the classifier are evaluated, and then we include unused labels from D one by one to retrain the classifier. This allows us to correlate the increase in classifier performance or its failures with the previously calculated classifier parameters."}, {"heading": "4.1. Monte-Carlo LAL", "text": "Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings-Settings"}, {"heading": "4.2. Iterative LAL", "text": "MONTECARLOLAL strategy of Alg. 2 results in an implicit oversimplification in the way the data set D is divided into a labeled group L\u03c4 and an unlabeled group U\u03c4. Regardless of how many labeled samples we have, the labeled group consists of randomly selected samples. To take this into account, we modify the approach of Section 4.1 as follows. We learn a different AL strategy A\u03c4 with each labeling. The labeling shows us which datapoint should be selected by AL with iteration t = 2. When we perform iteration, we simulate AL procedures that start with 2 labeled samples and with labeling A2,..., when selecting 3,."}, {"heading": "5. LAL Strategies Implementation", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "6. Experiments", "text": "We now compare the performance of LAL with multiple baselines on both synthetic and real data."}, {"heading": "6.1. Baselines and Protocol", "text": "We will compare the following three versions of our approach: \u2022 LAL-MC-2D. LAL strategy learned as discussed in Section 4.1, where D synthetic 2D dataset from Section 5. \u2022 LAL-MC-2D, but D is a subset of data D on which AL is performed. Similar to Section 4.2. \u2022 Rs: random samples. \u2022 Us: uncertainty samples according to Section 3.2. \u2022 Kapoor: Approach by Kapoor et al. (2007), the exploration against exploitation by including mean and variance estimates of GP classification. In our experiments, we chose ourselves as our primary baseline because our approach uses the same information - predicted probability distribution across classes - as date characteristics. To compare a cold and variance estimate of GP classification against Kapoor, we will choose a deviation strategy for the deviation."}, {"heading": "6.2. Synthetic Data", "text": "We will first demonstrate the performance of the AL approach on the synthetic data generated with the Sec. 5 model, but not before. Next, we will test our approach on the notoriously hard XOR-like data sets. Two Gaussian cloud experiments. We will generate 1000 new invisible data sets of the type shown in Figure 2, and test the better classifiers than us heurists. This experiment shows that the average test accuracy can be recognized as a number of described samplers. In the case of both classifiers, the proposed strategy is able to construct the better classifications than us."}, {"heading": "6.3. Real Data", "text": "In fact, it is as if they are able to distinguish themselves in the areas in which they operate. \u2022 It is as if they are able to move. \u2022 It is as if they are able to move. \u2022 It is as if they are able to move. \u2022 It is as if they are able to move. \u2022 It is as if they are able to move. \u2022 It is as if they are able to move. \u2022 It is as if they are able to move. \u2022 It is as if they are able to be in a position. \u2022 It is as if they are able to hide."}, {"heading": "7. Conclusion", "text": "In this paper, we presented a new approach to AL driven by data - Learning Active Learning. We found that an LAL that faced AL experiments on simple 2D data can sometimes generalize surprisingly well to challenging new domains. The ability to learn LAL from a subset of interesting data allows us to expand the applicability of our approach further. LAL demonstrated robustness in choosing the classification type and characteristics. In future work, we would like to integrate more features into LAL that allow them to be compared with various exploration / exploitation strategies. We also want to address issues of multi-class classification and batch mode AL."}, {"heading": "Acknowledgements", "text": "We would like to thank Carlos Becker, Helge Rhodin and Lucas Maystre for their discussions and comments on the text."}], "references": [{"title": "The higgs boson machine learning challenge", "author": ["Adam-Bourdarios", "Claire", "Cowan", "Glen", "Germain", "C\u00e9cile", "Guyon", "Isabelle", "K\u00e9gl", "Bal\u00e1zs", "Rousseau", "David"], "venue": "In NIPS 2014 Workshop on High-energy Physics and Machine Learning,", "citeRegEx": "Adam.Bourdarios et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Adam.Bourdarios et al\\.", "year": 2014}, {"title": "Online Choice of Active Learning Algorithms", "author": ["Baram", "Yoram", "El-Yaniv", "Ran", "K. Luz", "Kobi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Baram et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Baram et al\\.", "year": 2004}, {"title": "Can active learning experience be transferred? 2016", "author": ["Chu", "Hong-Min", "Lin", "Hsuan-Tien"], "venue": "URL http: //arxiv.org/abs/1608.00667", "citeRegEx": "Chu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chu et al\\.", "year": 2016}, {"title": "Calibrating probability with undersampling for unbalanced classification", "author": ["Dal Pozzolo", "Andrea", "Caelen", "Olivier", "Johnson", "Reid A", "Bontempi", "Gianluca"], "venue": "In Computational Intelligence,", "citeRegEx": "Pozzolo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pozzolo et al\\.", "year": 2015}, {"title": "RALF: A Reinforced Active Learning Formulation for Object Class Recognition", "author": ["S. Ebert", "M. Fritz", "B. Schiele"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Ebert et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ebert et al\\.", "year": 2012}, {"title": "Query by Committee Made Real", "author": ["R. Gilad-bachrach", "A. Navot", "N. Tishby"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gilad.bachrach et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gilad.bachrach et al\\.", "year": 2005}, {"title": "State of the Art Survey on MRI Brain Tumor Segmentation", "author": ["N. Gordillo", "E. Montseny", "P. Sobrevilla"], "venue": "Magnetic Resonance in Medicine,", "citeRegEx": "Gordillo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gordillo et al\\.", "year": 2013}, {"title": "Batch Mode Active Learning and Its Application to Medical Image Classification", "author": ["S.C. Hoi", "R. Jin", "J. Zhu", "M.R. Lyu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Hoi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hoi et al\\.", "year": 2006}, {"title": "Bayesian active learning for classification and preference", "author": ["Houlsby", "Neil", "Husz\u00e1r", "Ferenc", "Ghahramani", "Zoubin", "Lengyel", "M\u00e1t\u00e9"], "venue": "learning. stat,", "citeRegEx": "Houlsby et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Houlsby et al\\.", "year": 2011}, {"title": "Active learning by learning", "author": ["Hsu", "Wei-Ning", "Lin", "Hsuan-Tien"], "venue": "AAAI, pp. 2659\u20132665,", "citeRegEx": "Hsu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2015}, {"title": "Combining Generative and Discriminative Models for Semantic Segmentation", "author": ["J.E. Iglesias", "E. Konukoglu", "A. Montillo", "Z. Tu", "A. Criminisi"], "venue": "In Information Processing in Medical Imaging,", "citeRegEx": "Iglesias et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Iglesias et al\\.", "year": 2011}, {"title": "Scalable Active Learning for Multiclass Image Classification", "author": ["A.J. Joshi", "F. Porikli", "N.P. Papanikolopoulos"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Joshi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2012}, {"title": "MultiClass Active Learning for Image Classification", "author": ["A.J. Joshi", "F. Porikli", "N. Papanikolopoulos"], "venue": "In Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Joshi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2009}, {"title": "Active Learning with Gaussian Processes for Object Categorization", "author": ["A. Kapoor", "K. Grauman", "R. Urtasun", "T. Darrell"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "Kapoor et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kapoor et al\\.", "year": 2007}, {"title": "Introducing Geometry into Active Learning for Image Segmentation", "author": ["K. Konyushkova", "R. Sznitman", "P. Fua"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "Konyushkova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Konyushkova et al\\.", "year": 2015}, {"title": "Fully Convolutional Networks for Semantic Segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "In Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Splice junction recognition using machine learning techniques", "author": ["Lorena", "Ana Carolina", "Batista", "Gustavo EAPA", "de Carvalho", "Andr\u00e9 Carlos Ponce Leon Ferreira", "Monard", "Maria Carolina"], "venue": "In WOB, pp", "citeRegEx": "Lorena et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lorena et al\\.", "year": 2002}, {"title": "Structured Image Segmentation Using Kernelized Features", "author": ["A. Lucchi", "Y. Li", "K. Smith", "P. Fua"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Lucchi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lucchi et al\\.", "year": 2012}, {"title": "Active Learning to Recognize Multiple Types of Plankton", "author": ["T. Luo", "K. Kramer", "S. Samson", "A. Remsen", "D.B. Goldgof", "L.O. Hall", "T. Hopkins"], "venue": "In International Conference on Pattern Recognition,", "citeRegEx": "Luo et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2004}, {"title": "The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)", "author": ["B. Menza", "A Jacas"], "venue": "IEEE Transactions on Medical Imaging,", "citeRegEx": "Menza and Jacas,? \\Q2014\\E", "shortCiteRegEx": "Menza and Jacas", "year": 2014}, {"title": "Active Learning for Delineation of Curvilinear Structures", "author": ["A. Mosinska", "R. Sznitman", "P. Glowacki", "P. Fua"], "venue": "In Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Mosinska et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mosinska et al\\.", "year": 2016}, {"title": "A Literature Survey of Active Machine Learning in the Context of Natural Language Processing", "author": ["F. Olsson"], "venue": "Swedish Institute of Computer Science,", "citeRegEx": "Olsson,? \\Q2009\\E", "shortCiteRegEx": "Olsson", "year": 2009}, {"title": "Soft margins for adaboost", "author": ["R\u00e4tsch", "Gunnar", "Onoda", "Takashi", "M\u00fcller", "K-R"], "venue": "Machine learning,", "citeRegEx": "R\u00e4tsch et al\\.,? \\Q2001\\E", "shortCiteRegEx": "R\u00e4tsch et al\\.", "year": 2001}, {"title": "Active Learning Literature Survey", "author": ["B. Settles"], "venue": "Technical report, University of Wisconsin\u2013Madison,", "citeRegEx": "Settles,? \\Q2010\\E", "shortCiteRegEx": "Settles", "year": 2010}, {"title": "An Analysis of Active Learning Strategies for Sequence Labeling Tasks", "author": ["B. Settles", "M. Craven"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Settles and Craven,? \\Q2008\\E", "shortCiteRegEx": "Settles and Craven", "year": 2008}, {"title": "Actively learning hemimetrics with applications to eliciting user preferences", "author": ["Singla", "Adish", "Tschiatschek", "Sebastian", "Krause", "Andreas"], "venue": "In ICML,", "citeRegEx": "Singla et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Singla et al\\.", "year": 2016}, {"title": "Active Testing for Face Detection and Localization", "author": ["R. Sznitman", "B. Jedynak"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Sznitman and Jedynak,? \\Q2010\\E", "shortCiteRegEx": "Sznitman and Jedynak", "year": 2010}, {"title": "Value iteration networks", "author": ["Tamar", "Aviv", "Levine", "Sergey", "Abbeel", "WU Pieter", "YI", "Thomas", "Garrett"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tamar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tamar et al\\.", "year": 2016}, {"title": "Support Vector Machine Active Learning with Applications to Text Classification", "author": ["S. Tong", "D. Koller"], "venue": "Machine Learning,", "citeRegEx": "Tong and Koller,? \\Q2002\\E", "shortCiteRegEx": "Tong and Koller", "year": 2002}, {"title": "Weakly Supervised Structured Output Learning for Semantic Segmentation", "author": ["A. Vezhnevets", "V. Ferrari", "J.M. Buhmann"], "venue": "In Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Vezhnevets et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Vezhnevets et al\\.", "year": 2012}, {"title": "Multi-Class Active Learning by Uncertainty Sampling with Diversity Maximization", "author": ["Y. Yang", "Z. Ma", "F. Nie", "X. Chang", "A.G. Hauptmann"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Recent examples include bandit algorithms (Baram et al., 2004; Hsu & Lin, 2015), aggregating strategies whose experience can be transferred between domains (Chu & Lin, 2016), and approaches based on Reinforcement Learning (Ebert et al.", "startOffset": 42, "endOffset": 79}, {"referenceID": 4, "context": ", 2004; Hsu & Lin, 2015), aggregating strategies whose experience can be transferred between domains (Chu & Lin, 2016), and approaches based on Reinforcement Learning (Ebert et al., 2012).", "startOffset": 167, "endOffset": 187}, {"referenceID": 12, "context": "They include uncertainty sampling (Tong & Koller, 2002; Joshi et al., 2009; Settles, 2010; Yang et al., 2015), ar X iv :1 70 3.", "startOffset": 34, "endOffset": 109}, {"referenceID": 23, "context": "They include uncertainty sampling (Tong & Koller, 2002; Joshi et al., 2009; Settles, 2010; Yang et al., 2015), ar X iv :1 70 3.", "startOffset": 34, "endOffset": 109}, {"referenceID": 30, "context": "They include uncertainty sampling (Tong & Koller, 2002; Joshi et al., 2009; Settles, 2010; Yang et al., 2015), ar X iv :1 70 3.", "startOffset": 34, "endOffset": 109}, {"referenceID": 5, "context": "query-by-committee (Gilad-bachrach et al., 2005; Iglesias et al., 2011), expected model change (Settles, 2010; Sznitman & Jedynak, 2010; Vezhnevets et al.", "startOffset": 19, "endOffset": 71}, {"referenceID": 10, "context": "query-by-committee (Gilad-bachrach et al., 2005; Iglesias et al., 2011), expected model change (Settles, 2010; Sznitman & Jedynak, 2010; Vezhnevets et al.", "startOffset": 19, "endOffset": 71}, {"referenceID": 23, "context": ", 2011), expected model change (Settles, 2010; Sznitman & Jedynak, 2010; Vezhnevets et al., 2012), expected error (Joshi et al.", "startOffset": 31, "endOffset": 97}, {"referenceID": 29, "context": ", 2011), expected model change (Settles, 2010; Sznitman & Jedynak, 2010; Vezhnevets et al., 2012), expected error (Joshi et al.", "startOffset": 31, "endOffset": 97}, {"referenceID": 11, "context": ", 2012), expected error (Joshi et al., 2012), and variance (Hoi et al.", "startOffset": 24, "endOffset": 44}, {"referenceID": 7, "context": ", 2012), and variance (Hoi et al., 2006) minimization, Bayesian AL (Houlsby et al.", "startOffset": 22, "endOffset": 40}, {"referenceID": 8, "context": ", 2006) minimization, Bayesian AL (Houlsby et al., 2011).", "startOffset": 34, "endOffset": 56}, {"referenceID": 1, "context": "While both examples are synthetic, analogous situations arise regularly in real data (Baram et al., 2004).", "startOffset": 85, "endOffset": 105}, {"referenceID": 13, "context": "Among AL methods designed to handle complex real-world situations, some cater to specific classifiers, such as those that rely on Gaussian Processes (Kapoor et al., 2007), and applications, such as natural language processing (Tong & Koller, 2002; Olsson, 2009), sequence labeling tasks (Settles & Craven, 2008), visual recognition (Luo et al.", "startOffset": 149, "endOffset": 170}, {"referenceID": 21, "context": ", 2007), and applications, such as natural language processing (Tong & Koller, 2002; Olsson, 2009), sequence labeling tasks (Settles & Craven, 2008), visual recognition (Luo et al.", "startOffset": 63, "endOffset": 98}, {"referenceID": 18, "context": ", 2007), and applications, such as natural language processing (Tong & Koller, 2002; Olsson, 2009), sequence labeling tasks (Settles & Craven, 2008), visual recognition (Luo et al., 2004; Long et al., 2015), semantic segmentation (Vezhnevets et al.", "startOffset": 169, "endOffset": 206}, {"referenceID": 15, "context": ", 2007), and applications, such as natural language processing (Tong & Koller, 2002; Olsson, 2009), sequence labeling tasks (Settles & Craven, 2008), visual recognition (Luo et al., 2004; Long et al., 2015), semantic segmentation (Vezhnevets et al.", "startOffset": 169, "endOffset": 206}, {"referenceID": 29, "context": ", 2015), semantic segmentation (Vezhnevets et al., 2012), foreground-background segmentation (Konyushkova et al.", "startOffset": 31, "endOffset": 56}, {"referenceID": 14, "context": ", 2012), foreground-background segmentation (Konyushkova et al., 2015), image delineation (Mosinska et al.", "startOffset": 44, "endOffset": 70}, {"referenceID": 20, "context": ", 2015), image delineation (Mosinska et al., 2016), and preference learning (Singla et al.", "startOffset": 27, "endOffset": 50}, {"referenceID": 25, "context": ", 2016), and preference learning (Singla et al., 2016).", "startOffset": 33, "endOffset": 54}, {"referenceID": 23, "context": "Moreover, various querying strategies aim to maximize different performance metrics as evidenced in the case of multi-class classification (Settles, 2010).", "startOffset": 139, "endOffset": 154}, {"referenceID": 27, "context": "As a result, meta learning algorithms have been gaining in popularity in recent years (Tamar et al., 2016), but they are rarely designed to deal with AL scenarios.", "startOffset": 86, "endOffset": 106}, {"referenceID": 1, "context": "Baram et al. (2004) combine several known heuristics during the AL execution with the help of a bandit algorithm.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Baram et al. (2004) combine several known heuristics during the AL execution with the help of a bandit algorithm. This is made possible by the use of the maximum entropy criterion, which estimates the classification performance without labels during AL. Hsu & Lin (2015) extend this line of work but move the focus from datasamples as arms to heuristics as arms in the bandit process and use a new unbiased estimator of the test error, which enables it to outperform the previous method.", "startOffset": 0, "endOffset": 271}, {"referenceID": 1, "context": "Baram et al. (2004) combine several known heuristics during the AL execution with the help of a bandit algorithm. This is made possible by the use of the maximum entropy criterion, which estimates the classification performance without labels during AL. Hsu & Lin (2015) extend this line of work but move the focus from datasamples as arms to heuristics as arms in the bandit process and use a new unbiased estimator of the test error, which enables it to outperform the previous method. Chu & Lin (2016) go further and suggest transferring the bandit-learnt combination of AL heuristics between different tasks.", "startOffset": 0, "endOffset": 505}, {"referenceID": 1, "context": "Baram et al. (2004) combine several known heuristics during the AL execution with the help of a bandit algorithm. This is made possible by the use of the maximum entropy criterion, which estimates the classification performance without labels during AL. Hsu & Lin (2015) extend this line of work but move the focus from datasamples as arms to heuristics as arms in the bandit process and use a new unbiased estimator of the test error, which enables it to outperform the previous method. Chu & Lin (2016) go further and suggest transferring the bandit-learnt combination of AL heuristics between different tasks. Another view on the problem is presented in Ebert et al. (2012), where they balance exploration and exploitation with a Markov decision process.", "startOffset": 0, "endOffset": 677}, {"referenceID": 12, "context": "In this section we briefly introduce the standard Active Leaning (AL) framework along with uncertainty sampling (US), the most frequently-used heuristic to implement it (Tong & Koller, 2002; Joshi et al., 2009; Settles, 2010; Yang et al., 2015).", "startOffset": 169, "endOffset": 244}, {"referenceID": 23, "context": "In this section we briefly introduce the standard Active Leaning (AL) framework along with uncertainty sampling (US), the most frequently-used heuristic to implement it (Tong & Koller, 2002; Joshi et al., 2009; Settles, 2010; Yang et al., 2015).", "startOffset": 169, "endOffset": 244}, {"referenceID": 30, "context": "In this section we briefly introduce the standard Active Leaning (AL) framework along with uncertainty sampling (US), the most frequently-used heuristic to implement it (Tong & Koller, 2002; Joshi et al., 2009; Settles, 2010; Yang et al., 2015).", "startOffset": 169, "endOffset": 244}, {"referenceID": 12, "context": "US has been reported to be successful in numerous scenarios and settings (Tong & Koller, 2002; Joshi et al., 2009; Settles, 2010; Yang et al., 2015; Konyushkova et al., 2015; Mosinska et al., 2016).", "startOffset": 73, "endOffset": 197}, {"referenceID": 23, "context": "US has been reported to be successful in numerous scenarios and settings (Tong & Koller, 2002; Joshi et al., 2009; Settles, 2010; Yang et al., 2015; Konyushkova et al., 2015; Mosinska et al., 2016).", "startOffset": 73, "endOffset": 197}, {"referenceID": 30, "context": "US has been reported to be successful in numerous scenarios and settings (Tong & Koller, 2002; Joshi et al., 2009; Settles, 2010; Yang et al., 2015; Konyushkova et al., 2015; Mosinska et al., 2016).", "startOffset": 73, "endOffset": 197}, {"referenceID": 14, "context": "US has been reported to be successful in numerous scenarios and settings (Tong & Koller, 2002; Joshi et al., 2009; Settles, 2010; Yang et al., 2015; Konyushkova et al., 2015; Mosinska et al., 2016).", "startOffset": 73, "endOffset": 197}, {"referenceID": 20, "context": "US has been reported to be successful in numerous scenarios and settings (Tong & Koller, 2002; Joshi et al., 2009; Settles, 2010; Yang et al., 2015; Konyushkova et al., 2015; Mosinska et al., 2016).", "startOffset": 73, "endOffset": 197}, {"referenceID": 13, "context": "\u2022 Kapoor: approach of Kapoor et al. (2007) that balances exploration against exploitation by incorporating mean and variance estimation of the GP classifier.", "startOffset": 22, "endOffset": 43}, {"referenceID": 6, "context": ", 2010), dice score (Gordillo et al., 2013), AMS score (Adam-Bourdarios et al.", "startOffset": 20, "endOffset": 43}, {"referenceID": 0, "context": ", 2013), AMS score (Adam-Bourdarios et al., 2014) and area under ROC curve.", "startOffset": 19, "endOffset": 49}, {"referenceID": 1, "context": "Some works (Baram et al., 2004) report that various AL algorithms struggle with the type of tasks that is depicted in Fig.", "startOffset": 11, "endOffset": 31}, {"referenceID": 22, "context": "Another XOR-like dataset that has more natural shape is the banana dataset from R\u00e4tsch et al. (2001). Note that these datasets do not resemble in the least the data used to train LAL, as can be seen by comparing Figs.", "startOffset": 80, "endOffset": 101}, {"referenceID": 17, "context": "The task is to detect and segment mitochondria in it (Lucchi et al., 2012; Konyushkova et al., 2015).", "startOffset": 53, "endOffset": 100}, {"referenceID": 14, "context": "The task is to detect and segment mitochondria in it (Lucchi et al., 2012; Konyushkova et al., 2015).", "startOffset": 53, "endOffset": 100}, {"referenceID": 16, "context": "\u2022 Splice: In this dataset from the domain of molecular biology, our task is to detect splice junctions in DNA sequences (Lorena et al., 2002).", "startOffset": 120, "endOffset": 141}, {"referenceID": 0, "context": "\u2022 Higgs: This dataset from the domain of high energy physics contains the data that simulates the ATLAS experiment (Adam-Bourdarios et al., 2014).", "startOffset": 115, "endOffset": 145}], "year": 2017, "abstractText": "In this paper, we suggest a novel data-driven approach to active learning: Learning Active Learning (LAL). The key idea behind LAL is to train a regressor that predicts the expected error reduction for a potential sample in a particular learning state. By treating the query selection procedure as a regression problem we are not restricted to dealing with existing AL heuristics; instead, we learn strategies based on experience from previous active learning experiments. We show that LAL can be learnt from a simple artificial 2D dataset and yields strategies that work well on real data from a wide range of domains. Moreover, if some domain-specific samples are available to bootstrap active learning, the LAL strategy can be tailored for a particular problem.", "creator": "LaTeX with hyperref package"}}}