{"id": "1703.03389", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2017", "title": "Faster Greedy MAP Inference for Determinantal Point Processes", "abstract": "Determinantal point processes (DPPs) are popular probabilistic models that arise in many machine learning tasks, where distributions of diverse sets are characterized by matrix determinants. In this paper, we develop fast algorithms to find the most likely configuration (MAP) of large-scale DPPs, which is NP-hard in general. Due to the submodular nature of the MAP objective, greedy algorithms have been used with empirical success. Greedy implementations require computation of log-determinants, matrix inverses or solving linear systems at each iteration. We present faster implementations of the greedy algorithms by utilizing the complementary benefits of two log-determinant approximation schemes: (a) first-order expansions to the matrix log-determinant function and (b) high-order expansions to the scalar log function with stochastic trace estimators. In our experiments, our algorithms are orders of magnitude faster than their competitors, while sacrificing marginal accuracy.", "histories": [["v1", "Thu, 9 Mar 2017 18:43:11 GMT  (1386kb,D)", "http://arxiv.org/abs/1703.03389v1", null], ["v2", "Wed, 14 Jun 2017 02:48:18 GMT  (1320kb,D)", "http://arxiv.org/abs/1703.03389v2", null]], "reviews": [], "SUBJECTS": "cs.DM cs.LG", "authors": ["insu han", "prabhanjan kambadur", "kyoungsoo park", "jinwoo shin"], "accepted": true, "id": "1703.03389"}, "pdf": {"name": "1703.03389.pdf", "metadata": {"source": "CRF", "title": "Faster Greedy MAP Inference for Determinantal Point Processes", "authors": ["Insu Han", "Prabhanjan Kambadur", "Kyoungsoo Park", "Jinwoo Shin"], "emails": ["hawki17@kaist.ac.kr", "badur@gmail.com", "soo@kaist.ac.kr", "woos@kaist.ac.kr"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to hide, while others are able to play by the rules. (...) Most of them are able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...)"}, {"heading": "1.1 Contribution", "text": "A greedy selection requires the calculation of marginal gains of log determinants; we consider their (linear) approximations of first order. We observe that the calculation of multiple marginal gains in a single run of a linear solver can be amortized, in addition to multiple vector internal products. We choose the popular conjugate gradient lineage (CG) [32] as the linear solver. In addition, we partition the remaining items into p-1 sets (using some cluster algorithms) and apply the approximations of first order in each partition. The resulting approximate calculation of multiple marginal gains in each greedy selection requires 2p runs of CG under the Schur completion, and the total lifetime of the proposed greedy algorithm becomes O (d3)."}, {"heading": "1.2 Related work", "text": "To the best of our knowledge, this is the first work aimed at developing faster greedy algorithms specializing in the MAP conclusion of DPP, while there have been several efforts at general submodular maximization. An accelerated greedy algorithm was first proposed by [26], which maintains the upper limits of marginal gains rather than recalculating exact values. In each iteration, only the elements with the maximum limit calculate the exact gain, which is still limited to the exact value due to submodularity. However, we have found that the approximation quality of this algorithm is extremely poor for the DPP case (see Section 5). Another natural approach is stochastic greedy selections, which calculate marginal gains of randomly selected elements."}, {"heading": "1.3 Organization", "text": "We present the necessary background in Section 2 and present the proposed algorithms in Section 3 and Section 4. Experimental results are reported in Section 5."}, {"heading": "2 Preliminaries", "text": "Our Determinant Point Processes (DPPs) algorithms select elements from the base set of d elements Y = [d]: = {1, 2,.., d} and denote the set of all subsets of Y by 2Y. For each positive semidefinitive matrix L and Rd \u00b7 d, we denote \u03bbmin and \u03bbmax as the smallest and largest eigenvalues of L. In view of the subset X, Y, and Y, we use LX, Y to denote the submatrix of L, which is obtained by entries in rows and columns indexed by X and Y, respectively. To simplify the notation, we leave LX, X = LX and LX, {i} = LX, i for i, Y. In addition, LX is defined as the average of LX, i {i} for i, Y\\ X. \"Finally, < \u00b7 > means the matrix / vector inner product or elemental product sum."}, {"heading": "2.1 Determinantal Point Processes", "text": "DPPs are probabilistic models for the subset selection of a finite base set Y = [d], which covers both quality and diversity. Formally, it defines the following distribution to 2Y: for X Y, Pr [X'Y] ched det (LX), where L-Rd \u00b7 d is a positive definitive matrix called L-ensemble core. Under the distribution, several probabilistic inference tasks are required for real-world applications, including MAP [9, 8, 35], sampling [16, 15, 21], marginalization and conditioning [9]. In particular, we are interested in MAP inference, i.e. the search for the most varied subset Y of Y that achieves the highest probability, i.e., arg maxY'Y'det (LY), possibly with certain limitations on Y. Unlike other inference tasks on DPP, MAP is known to be a NP-hard problem [19]."}, {"heading": "2.2 Greedy Submodular Maximization", "text": "A specified function f: 2Y \u2192 R is submodular if its marginal gains decrease, i.e., f (X-i) \u2212 f (X) \u2265 f (Y-i) \u2212 f (Y) \u2212 f (Y), for each X-Y value and each i-Y value. We say f is monoton if f (X) \u2264 f (Y) for each X-Y value. It is generally known that DPP has the submodular structure, i.e., f = log is submodular. We say f is monoton if f (X) \u2264 f (Y) for each X-Y value. 2 (i.e., f = log is submodular). The submodular maximization task consists in finding a submodular function f maximizes a submodular function f (2) corresponding to the MAP follow-up task in the DPP case. Therefore, it is NP-hard and a popular approximate scheme of the following greedy method [28]: Initially, X-shaped and iterative updating of X-shaped (imax i-monoform f) is dangerous (imax i-monoform f), 2, (imax) corresponding to the MAP follow-up task in the DPP case. Therefore, it is (NP-hard and a popular approximate scheme following greedy method [28]: First, X-shaped and iterative updating of X-shaped and iterative updating of X-shaped (f (f), 5, 5, 5-dangerous (imax) forimax i-monoform f, 5, 5, 5, 5, 1, 1, 1, 5-dangerous, 1, 1, 1, x-36-X-X, 5, 5, 5, 5-X-5, 5, 5, 1, 1, 5-hazardous, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1,"}, {"heading": "2.3 Na\u0131\u0308ve Implementations of Greedy Algorithm", "text": "(A)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "3 Faster Greedy DPP Inference", "text": "In this section we provide a faster submodular maximization scheme for the MAP conclusion of DPP. We explain our key ideas in Section 3.1 and then provide the formal algorithm description in Section 3.2."}, {"heading": "3.1 Key Ideas", "text": "In order to reduce the temporal complexity, we consider the following first order, i.e., linear, approach to the log determinant as: 1argmax i, Xlog detLX, (3) log detLX, (3) log detLX, (3) log detLX, (3) log detLX, (3) log detLX, (3) log detLX, (3) log detLX, (3) log detLX, (3) log detLX, (4) argmax i, (4) l, (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, \"(5), (5), (5,\" (5), (5), (5, \"(5), (5,\" (5), (5, 5, \"(5), (5, 5,\" (5), (5), (5, \"(5), (5,\" (5), (5, \"(5),\" (5, \"(5),\" (5, \"(5),\" (5, \"(5),\" (5, \"(5),\" (5, \"),\" (5, \"(5),\" (5, \"(5),\" (5, \"(5),\" (5, \"(5),\" (5, \"(5),\" (5, \"(5),\" (5, \"(5),\" (5, \"(5,\"), \"(5,\" (5), \"(5,\" (5, \"(5),\" (5, \"), (5), (5, (5, (5), (5), (5), (5, (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5, (5), (5), (5), (5,"}, {"heading": "3.2 Algorithm Description and Guarantee", "text": "2.algorithm 1.algorithm 1.algorithm 1.algorithm 1.algorithm 1.algorithm 1.algorithm 1.algorithm 1.algorithm 1.algorithm 1.algorithm 1.algorithm 1.algorithm 1.algorithm 1.algorithm 1.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm"}, {"heading": "4 Faster Batch-Greedy DPP Inference", "text": "In this section, we present an even faster greedy algorithm for the MAP inference task of DPP, especially for large tasks. In addition to the ideas described in Section 3.1, we use a batch strategy, i.e. we insert k elements instead of a single element into the current sentence, with LDAS now being used as the key component in Section 2.3. The batch strategy speeds up our algorithm. We first provide the formal description of the batch greedy algorithm in Section 4.1. In Section 4.2, we describe additional ideas for using LDAS as a subroutine of the proposed batch algorithm."}, {"heading": "4.1 Algorithm Description", "text": "In fact, it is in such a way that it acts in a way, in which people act in the world, in the world, in the world, in which they live, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in,"}, {"heading": "4.2 Sharing Randomness in Trace Estimators", "text": "In order to improve the approximate quality of algorithm 2, we propose to run LDAS with the same random vectors v (1),.., v (m) over j [p]. This is because we are interested in relative values logging detL (j) X for j [p] instead of their absolute vectors. Our intuition is that different random vectors have different biases that harm the comparison task. Figure 1 shows an experiment to estimate log detL (j) X when random vectors are divided and independent, implying that common random vectors may be worse for estimating the absolute values of log determinants, but better for comparing them. We also formally justify the idea of dividing random vectors as stated in the following theorems. Theorem 2. The stocks A, B and B are positive definitive matrices whose eigenvalues are in \u043c, 1 \u2212 3 and 2 \u2212 variants."}, {"heading": "5 Experimental Results", "text": "In this section we evaluate our proposed algorithms for MAP conclusion on synthetic and realworld DPP instances. Setups. the experiments are performed using a machine with a hexa-core Intel CPU (Core i75930K, 3.5GHz) and 32GB RAM. We compare our algorithms with the following competitors: the standard greedy algorithm (GREEDY) [28], stochastically greedy algorithm (STOCH) [27], double greedy algorithm (DOUBLE) and softmax extension (SOFTMAX) [8]. We implement GREEDY with the marginalization of DPP, which requires matrix inversion [19], which is a bit faster (maintaining the same accuracy) than its naive implementation described in Section 2.3. We use 100 samples in STOCH, regardless of the matrix dimension, where a larger number of samples makes it more precise."}, {"heading": "5.1 Synthetic Dataset", "text": "In this section, we use synthetic DPP data sets that are generated as follows: As [18, 19] suggested, a kernel matrix L can be reconfigured for DPP to measure the similarity between i and j. We use qi = exp (\u03b2xi) for quality measurement xi-R and select \u03b2 = 0.1. We select each entry of \u03c6i and xi drawn from the normal distribution N (0, 1) for all i dimensions, and then normalize Qi = exp (\u03b2xi) for quality measurement xi-R and select \u03b2 = 0.1. We select each entry of \u03c6i and xi drawn from the normal distribution N (0, 1) for all i dimensions, and then normalize the accuracy so that we should first show how much the number of clusters p and the batch size k is inaccurate for algorithm 1 and algorithm 2. Figure 3 (a) shows the accuracy of algorithm 1 with different clusters."}, {"heading": "5.2 Real Dataset", "text": "This task provides useful information for comparing texts addressed by the same speaker at different times. Suppose we have two different documents and each consists of several statements. The goal is to use DPP to search for statements that are similar to each other while summarizing the two documents (i.e., multiple). We use transcripts of debates in the 2016 U.S. Republican Party presidential election, which is attended by the following 8: Bush, Carson, Christie, Kasich, Paul, Trump, Cruz and Rubio.4We follow similar steps before processing [8]. First, each sentence is analyzed and only nouns other than the stopwords are extracted via NLTK."}, {"heading": "6 Conclusion", "text": "Our basic idea is to amortize common determinant calculations using linear algebraic techniques and newer logdeterministic approximation methods. Although we primarily focus on a specific matrix optimization, we expect that some ideas developed in this paper could be useful for other related matrix calculation problems, especially when it comes to multiple determinant calculations."}, {"heading": "A Proof of Theorem 1", "text": "If the smallest value of L is greater than 1, the smallest difference between the actual difference and the approximate difference is \"i\" (used in algorithm 1) \"i\": = \"log detLX\" \u2212 log detLX \"i: =\" (L (j) X \"i\" (used in algorithm 1) \"i\": = \"log detLX\" (log detLX) \"log detLX\" i: = \"(L (j)\" i \").\" We also use \"iOPT\" = argmaxi \"i\" and \"imax\" = argmaxi \"i. Then we have the\" maximum difference \"(imax\"). \"In addition, the\" maximum difference \"imax\" [p \"imax\"] \"(Xp\" imax \") applies.\" \u2212 \"Xp\" \u2212 \""}, {"heading": "B Proof of Theorem 2", "text": "As we have explained in section 2.3, Chebyshev in section 2.3 (1) the extension of Log x in section 2.3 (1) the extension of Log x in section 2.3 (1) the extension of Log x in section 2.3 (1) the extension of Log x (2) the extension of Log x (1) the extension of Log x (1) the extension of Log x (2) the extension of Log x (1) the extension of Log x (1) the extension of Log x (1) the extension of Log x (1) the extension of Log x (1) the extension of Log x (2) the extension of Log x (2) the extension of Log x (2) the extension of Log x (2) the extension of Log (1) the extension of Log (2) the extension of Log (2) the extension of Log (1) the extension of Log (2) the extension of Log (2) the extension of Log (1) the extension of Log (2) the extension of Log (2) the extension of Log (2) the extension of Log (2) the extension of (1) the extension of Log (2) the extension of (2) the extension of Log (2) the extension of Log (2) the extension of Log (2) the extension of Log (2) the extension of Log (2) the extension of (1 of (1) the extension of Log (2) the extension of Log (2) the extension of Log (2) the extension of Log (1 of (2) the extension of (1) the extension of Log (2) the extension of Log (1 of (2) the extension of Log (1 of (1) the extension of (1) the extension of Log x the extension of Log (2) the extension of Log (1 of (1) the extension of (2) the extension of Log x the extension of (2) the extension of Log (1 of (1) the extension of Log x the extension of (2) the extension of (1 of Log)"}], "references": [{"title": "Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix", "author": ["H. Avron", "S. Toledo"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Nltk: the natural language toolkit", "author": ["S. Bird"], "venue": "In Proceedings of the COLING/ACL on Interactive presentation sessions,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "A randomized algorithm for approximating the log determinant of a symmetric positive definite matrix. arXiv preprint arXiv:1503.00374", "author": ["C. Boutsidis", "P. Drineas", "P. Kambadur", "A. Zouzias"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "A tight linear time (1/2)approximation for unconstrained submodular maximization", "author": ["N. Buchbinder", "M. Feldman", "J. Seffi", "R. Schwartz"], "venue": "SIAM Journal on Computing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "An introduction to the theory of point processes: volume II: general theory and structure", "author": ["D.J. Daley", "D. Vere-Jones"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Vsumm: A mechanism designed to produce static video summaries and a novel evaluation method", "author": ["S.E.F. De Avila", "A.P.B. Lopes", "A. da Luz", "A. de Albuquerque Ara\u00fajo"], "venue": "Pattern Recognition Letters,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Maximizing non-monotone submodular functions", "author": ["U. Feige", "V.S. Mirrokni", "J. Vondrak"], "venue": "SIAM Journal on Computing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Near-optimal map inference for determinantal point processes", "author": ["J. Gillenwater", "A. Kulesza", "B. Taskar"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Diverse sequential subset selection for supervised video summarization", "author": ["B. Gong", "Chao", "W.-L", "K. Grauman", "F. Sha"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Iterative methods for solving linear systems", "author": ["A. Greenbaum"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Large-scale log-determinant computation through stochastic chebyshev expansions", "author": ["I. Han", "D. Malioutov", "J. Shin"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Worst case analysis of greedy type algorithms for independence systems", "author": ["D. Hausmann", "B. Korte", "T. Jenkyns"], "venue": "In Combinatorial Optimization,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1980}, {"title": "A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines", "author": ["M.F. Hutchinson"], "venue": "Communications in Statistics-Simulation and Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1990}, {"title": "Learning in graphical models, volume 89", "author": ["M.I. Jordan"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Fast determinantal point process sampling with application to clustering", "author": ["B. Kang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "On sampling and greedy map inference of constrained determinantal point processes", "author": ["T. Kathuria", "A. Deshpande"], "venue": "arXiv preprint arXiv:1607.01551", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Learning determinantal point processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "Proceedings of UAI. Citeseer", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Determinantal point processes for machine learning", "author": ["A. Kulesza", "B Taskar"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Fast greedy algorithms in mapreduce and streaming", "author": ["R. Kumar", "B. Moseley", "S. Vassilvitskii", "A. Vattani"], "venue": "ACM Transactions on Parallel Computing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Efficient sampling for k-determinantal point processes", "author": ["C. Li", "S. Jegelka", "S. Sra"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Gaussian quadrature for matrix inverse forms with applications", "author": ["C. Li", "S. Sra", "S. Jegelka"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Performance bounds for the kbatch greedy strategy in optimization problems with curvature", "author": ["Y. Liu", "Z. Zhang", "E.K. Chong", "A. Pezeshki"], "venue": "In American Control Conference (ACC),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "The coincidence approach to stochastic point processes", "author": ["O. Macchi"], "venue": "Advances in Applied Probability,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1975}, {"title": "Accelerated greedy algorithms for maximizing submodular set functions", "author": ["M. Minoux"], "venue": "In Optimization Techniques,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1978}, {"title": "Lazier than lazy greedy", "author": ["B. Mirzasoleiman", "A. Badanidiyuru", "A. Karbasi", "J. Vondr\u00e1k", "A. Krause"], "venue": "arXiv preprint arXiv:1409.7938", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "An analysis of approximations for maximizing submodular set functionsi", "author": ["G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher"], "venue": "Mathematical Programming,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1978}, {"title": "Schur complements and statistics", "author": ["D.V. Ouellette"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1981}, {"title": "Parallel double greedy submodular maximization", "author": ["X. Pan", "S. Jegelka", "J.E. Gonzalez", "J.K. Bradley", "M.I. Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Large-scale log-determinant computation via weighted l 2 polynomial approximation with prior distribution of eigenvalues", "author": ["W. Peng", "H. Wang"], "venue": "In International Conference on High Performance Computing and Applications,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Iterative methods for sparse linear systems", "author": ["Y. Saad"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "On greedy maximization of entropy", "author": ["D. Sharma", "A. Kapoor", "A. Deshpande"], "venue": "In ICML, pages 1330\u20131338", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "An online algorithm for maximizing submodular functions", "author": ["M. Streeter", "D. Golovin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2009}, {"title": "Tweet timeline generation with determinantal point processes", "author": ["Yao", "J.-g", "F. Fan", "W.X. Zhao", "X. Wan", "E. Chang", "J. Xiao"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}], "referenceMentions": [{"referenceID": 23, "context": "Determinantal point processes (DPPs) are elegant probabilistic models, first introduced by [24], who called them \u2018fermion processes\u2019.", "startOffset": 91, "endOffset": 95}, {"referenceID": 4, "context": "Since then, DPPs have been extensively studied in the fields of quantum physics and random matrices, giving rise to a beautiful theory [5].", "startOffset": 135, "endOffset": 138}, {"referenceID": 8, "context": "Recently, they have been applied in many machine learning tasks such as summarization [9], human pose detection [19], clustering [15] and tweet time-line generation [35].", "startOffset": 86, "endOffset": 89}, {"referenceID": 18, "context": "Recently, they have been applied in many machine learning tasks such as summarization [9], human pose detection [19], clustering [15] and tweet time-line generation [35].", "startOffset": 112, "endOffset": 116}, {"referenceID": 14, "context": "Recently, they have been applied in many machine learning tasks such as summarization [9], human pose detection [19], clustering [15] and tweet time-line generation [35].", "startOffset": 129, "endOffset": 133}, {"referenceID": 33, "context": "Recently, they have been applied in many machine learning tasks such as summarization [9], human pose detection [19], clustering [15] and tweet time-line generation [35].", "startOffset": 165, "endOffset": 169}, {"referenceID": 14, "context": "For example, conditioning, sampling [15] and marginalization of DPPs admit polynomialtime/efficient algorithms, while those on popular graphical models [14] do not, i.", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "For example, conditioning, sampling [15] and marginalization of DPPs admit polynomialtime/efficient algorithms, while those on popular graphical models [14] do not, i.", "startOffset": 152, "endOffset": 156}, {"referenceID": 18, "context": "One exception is the MAP inference (finding the most likely configuration), which is our main interest; this is known to be NP-hard even for DPPs [19].", "startOffset": 146, "endOffset": 150}, {"referenceID": 16, "context": "Furthermore, it has been often empirically observed that greedy algorithms provide near optimal solutions [17].", "startOffset": 106, "endOffset": 110}, {"referenceID": 18, "context": "Hence, greedy algorithms have been also applied for the DPP task [19, 35, 36].", "startOffset": 65, "endOffset": 77}, {"referenceID": 33, "context": "Hence, greedy algorithms have been also applied for the DPP task [19, 35, 36].", "startOffset": 65, "endOffset": 77}, {"referenceID": 18, "context": "Known implementations of greedy selection on DPP require computation of log-determinants, matrix inversions [19] or solving linear systems [22].", "startOffset": 108, "endOffset": 112}, {"referenceID": 21, "context": "Known implementations of greedy selection on DPP require computation of log-determinants, matrix inversions [19] or solving linear systems [22].", "startOffset": 139, "endOffset": 143}, {"referenceID": 30, "context": "We choose the popular conjugate gradient descent (CG) [32] as a linear solver.", "startOffset": 54, "endOffset": 58}, {"referenceID": 10, "context": "Now, we suggest running the recent fast log-determinant approximation scheme (LDAS) [11] p times, instead of running CG pk times under the Schur complement, where LDAS utilizes high-order, i.", "startOffset": 84, "endOffset": 88}, {"referenceID": 24, "context": "An accelerated greedy algorithm was first proposed by [26] which", "startOffset": 54, "endOffset": 58}, {"referenceID": 25, "context": "Its worst-case approximation guarantee was also studied [27], under the standard, non-batch, greedy algorithm.", "startOffset": 56, "endOffset": 60}, {"referenceID": 3, "context": "Recently, [4] proposed a \u2018one-pass\u2019 greedy algorithm where each greedy selection requires computing only a single marginal gain, i.", "startOffset": 10, "endOffset": 13}, {"referenceID": 28, "context": "There have been also several efforts to design parallel/distributed implementations of greedy algorithms: [30] use parallel strategies for the above one-pass greedy algorithm and [20] adapt a MapReduce paradigm for implementing greedy algorithms in distributed settings.", "startOffset": 106, "endOffset": 110}, {"referenceID": 19, "context": "There have been also several efforts to design parallel/distributed implementations of greedy algorithms: [30] use parallel strategies for the above one-pass greedy algorithm and [20] adapt a MapReduce paradigm for implementing greedy algorithms in distributed settings.", "startOffset": 179, "endOffset": 183}, {"referenceID": 7, "context": "Finally, we remark that a non-greedy algorithm was studied in [8] for better MAP qualities of DPP, but it is much slower than ours as reported in Section 5.", "startOffset": 62, "endOffset": 65}, {"referenceID": 8, "context": "Under the distribution, several probabilistic inference tasks are required for real-world applications, including MAP [9, 8, 35], sampling [16, 15, 21], marginalization and conditioning [9].", "startOffset": 118, "endOffset": 128}, {"referenceID": 7, "context": "Under the distribution, several probabilistic inference tasks are required for real-world applications, including MAP [9, 8, 35], sampling [16, 15, 21], marginalization and conditioning [9].", "startOffset": 118, "endOffset": 128}, {"referenceID": 33, "context": "Under the distribution, several probabilistic inference tasks are required for real-world applications, including MAP [9, 8, 35], sampling [16, 15, 21], marginalization and conditioning [9].", "startOffset": 118, "endOffset": 128}, {"referenceID": 15, "context": "Under the distribution, several probabilistic inference tasks are required for real-world applications, including MAP [9, 8, 35], sampling [16, 15, 21], marginalization and conditioning [9].", "startOffset": 139, "endOffset": 151}, {"referenceID": 14, "context": "Under the distribution, several probabilistic inference tasks are required for real-world applications, including MAP [9, 8, 35], sampling [16, 15, 21], marginalization and conditioning [9].", "startOffset": 139, "endOffset": 151}, {"referenceID": 20, "context": "Under the distribution, several probabilistic inference tasks are required for real-world applications, including MAP [9, 8, 35], sampling [16, 15, 21], marginalization and conditioning [9].", "startOffset": 139, "endOffset": 151}, {"referenceID": 8, "context": "Under the distribution, several probabilistic inference tasks are required for real-world applications, including MAP [9, 8, 35], sampling [16, 15, 21], marginalization and conditioning [9].", "startOffset": 186, "endOffset": 189}, {"referenceID": 18, "context": "Unlike other inference tasks on DPP, it is known that MAP is a NP-hard problem [19].", "startOffset": 79, "endOffset": 83}, {"referenceID": 26, "context": "Hence, it is NP-hard and a popular approximate scheme is the following greedy procedure [28]: initially, X \u2190 \u2205 and iteratively update X \u2190 X \u222a {imax} for imax = argmax i\u2208Y\\X f(X \u222a {i})\u2212 f(X), (1)", "startOffset": 88, "endOffset": 92}, {"referenceID": 26, "context": "For the monotone case, it guarantees (1\u2212 1/e)-approximation [28].", "startOffset": 60, "endOffset": 64}, {"referenceID": 6, "context": "Under some modifications of the standard greedy procedure, 2/5-approximation can be guaranteed even for non-monotone functions [7].", "startOffset": 127, "endOffset": 130}, {"referenceID": 16, "context": "Irrespectively of such theoretical guarantees, it has been empirically observed that greedy selection (1) provides near optimal solutions in practice [17, 33, 35, 36].", "startOffset": 150, "endOffset": 166}, {"referenceID": 31, "context": "Irrespectively of such theoretical guarantees, it has been empirically observed that greedy selection (1) provides near optimal solutions in practice [17, 33, 35, 36].", "startOffset": 150, "endOffset": 166}, {"referenceID": 33, "context": "Irrespectively of such theoretical guarantees, it has been empirically observed that greedy selection (1) provides near optimal solutions in practice [17, 33, 35, 36].", "startOffset": 150, "endOffset": 166}, {"referenceID": 2, "context": "In literature, several polynomial expansions, including Taylor [3], Chebyshev [11] and Legendre [31] have been used.", "startOffset": 63, "endOffset": 66}, {"referenceID": 10, "context": "In literature, several polynomial expansions, including Taylor [3], Chebyshev [11] and Legendre [31] have been used.", "startOffset": 78, "endOffset": 82}, {"referenceID": 29, "context": "In literature, several polynomial expansions, including Taylor [3], Chebyshev [11] and Legendre [31] have been used.", "startOffset": 96, "endOffset": 100}, {"referenceID": 0, "context": "For trace estimation, several random vectors have been also studied [1], e.", "startOffset": 68, "endOffset": 71}, {"referenceID": 12, "context": ", the Hutchinson method [13] chooses elements", "startOffset": 24, "endOffset": 28}, {"referenceID": 10, "context": "In this paper, we use LDAS using the Chebyshev polynomial and Hutchinson method [11], but one can also use other alternatives.", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "Log-determinant Approximation Scheme (LDAS) [11] Input: symmetric matrix A \u2208 Rd\u00d7d with eigenvalues in [\u03b4, 1 \u2212 \u03b4], sampling number m and polynomial degree n Initialize: \u0393\u2190 0 cj \u2190 j-th coefficient of Chebyshev expansion of log x on [\u03b4, 1\u2212 \u03b4] for 0 \u2264 j \u2264 n.", "startOffset": 44, "endOffset": 48}, {"referenceID": 27, "context": "An alternative way to achieve the same complexity is to use the Schur complement [29]: log detLX\u222a{i} \u2212 log detLX = log ( Li,i \u2212 Li,XL X LX,i ) .", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": "(2) This requires a linear solver to compute L\u22121 X LX,i; conjugate gradient descent (CG) [10] is a popular choice in practice.", "startOffset": 89, "endOffset": 93}, {"referenceID": 21, "context": "Recently, Gauss quadrature via Lanczos iteration is used for efficient computing of Li,XL \u22121 X LX,i [22].", "startOffset": 100, "endOffset": 104}, {"referenceID": 26, "context": "Such batch greedy algorithms have been also studied for submodular maximization [28, 12] and recently, [23] studied their theoretical guarantees showing that they can be better than their non-batch counterparts under some conditions.", "startOffset": 80, "endOffset": 88}, {"referenceID": 11, "context": "Such batch greedy algorithms have been also studied for submodular maximization [28, 12] and recently, [23] studied their theoretical guarantees showing that they can be better than their non-batch counterparts under some conditions.", "startOffset": 80, "endOffset": 88}, {"referenceID": 22, "context": "Such batch greedy algorithms have been also studied for submodular maximization [28, 12] and recently, [23] studied their theoretical guarantees showing that they can be better than their non-batch counterparts under some conditions.", "startOffset": 103, "endOffset": 107}, {"referenceID": 25, "context": "[27] first propose an uniformly random sampling to the standard non-batch greedy algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "We compare our algorithms with following competitors: the standard greedy algorithm (GREEDY) [28], stochastic greedy algorithm (STOCH) [27], double greedy algorithm (DOUBLE) [4] and softmax extension (SOFTMAX) [8].", "startOffset": 93, "endOffset": 97}, {"referenceID": 25, "context": "We compare our algorithms with following competitors: the standard greedy algorithm (GREEDY) [28], stochastic greedy algorithm (STOCH) [27], double greedy algorithm (DOUBLE) [4] and softmax extension (SOFTMAX) [8].", "startOffset": 135, "endOffset": 139}, {"referenceID": 3, "context": "We compare our algorithms with following competitors: the standard greedy algorithm (GREEDY) [28], stochastic greedy algorithm (STOCH) [27], double greedy algorithm (DOUBLE) [4] and softmax extension (SOFTMAX) [8].", "startOffset": 174, "endOffset": 177}, {"referenceID": 7, "context": "We compare our algorithms with following competitors: the standard greedy algorithm (GREEDY) [28], stochastic greedy algorithm (STOCH) [27], double greedy algorithm (DOUBLE) [4] and softmax extension (SOFTMAX) [8].", "startOffset": 210, "endOffset": 213}, {"referenceID": 18, "context": "3 We implement GREEDY using DPP marginalization requiring matrix inversion [19], which is a bit faster (preserving the same accuracy) than its na\u0131\u0308ive implementation described in Section 2.", "startOffset": 75, "endOffset": 79}, {"referenceID": 24, "context": "For boosting approximation qualities of our algorithms, we use the simple trick in our experiments: recompute top ` marginal gains exactly (using CG) 3We also run the accelerated greedy algorithm [26] for general submodular maximization, but do not report its performance since its approximation quality is extremely bad for the DPP case.", "startOffset": 196, "endOffset": 200}, {"referenceID": 24, "context": "In fact, the trick is inspired by [26] where the authors also recompute the exact marginal gain of a single element.", "startOffset": 34, "endOffset": 38}, {"referenceID": 17, "context": "As [18, 19] proposed, a kernel matrix L for DPP can be re-parameterized as Li,j = qi\u03c6 > i \u03c6jqj , where qi \u2208 R is considered as the quality of item i and \u03c6i \u2208 R is the normalized feature vector of item i so that \u03c6i \u03c6j measures the similarity between i and j.", "startOffset": 3, "endOffset": 11}, {"referenceID": 18, "context": "As [18, 19] proposed, a kernel matrix L for DPP can be re-parameterized as Li,j = qi\u03c6 > i \u03c6jqj , where qi \u2208 R is considered as the quality of item i and \u03c6i \u2208 R is the normalized feature vector of item i so that \u03c6i \u03c6j measures the similarity between i and j.", "startOffset": 3, "endOffset": 11}, {"referenceID": 3, "context": "Interestingly, we found that DOUBLE has the strong theoretical guarantee for general submodular maximization [4], but its practical performance for DPP is bad.", "startOffset": 109, "endOffset": 112}, {"referenceID": 7, "context": "We evaluate our proposed algorithms for matched summarization that is first proposed by [8].", "startOffset": 88, "endOffset": 91}, {"referenceID": 7, "context": "4 We follow similar pre-processing steps of [8].", "startOffset": 44, "endOffset": 47}, {"referenceID": 1, "context": "First, every sentence is parsed and only nouns except the stopwords are extracted via NLTK [2].", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "We use 39 videos from a Youtube dataset [6], and the trained DPP kernels from [9].", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "We use 39 videos from a Youtube dataset [6], and the trained DPP kernels from [9].", "startOffset": 78, "endOffset": 81}], "year": 2017, "abstractText": "Determinantal point processes (DPPs) are popular probabilistic models that arise in many machine learning tasks, where distributions of diverse sets are characterized by matrix determinants. In this paper, we develop fast algorithms to find the most likely configuration (MAP) of large-scale DPPs, which is NP-hard in general. Due to the submodular nature of the MAP objective, greedy algorithms have been used with empirical success. Greedy implementations require computation of log-determinants, matrix inverses or solving linear systems at each iteration. We present faster implementations of the greedy algorithms by utilizing the complementary benefits of two log-determinant approximation schemes: (a) first-order expansions to the matrix log-determinant function and (b) high-order expansions to the scalar log function with stochastic trace estimators. In our experiments, our algorithms are orders of magnitude faster than their competitors, while sacrificing marginal accuracy.", "creator": "LaTeX with hyperref package"}}}