{"id": "1703.03400", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2017", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on a few-shot image classification benchmark, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.", "histories": [["v1", "Thu, 9 Mar 2017 18:58:03 GMT  (5061kb,D)", "http://arxiv.org/abs/1703.03400v1", "Videos of the reinforcement learning results are at sites.google.com/view/maml"], ["v2", "Tue, 9 May 2017 17:14:08 GMT  (5065kb,D)", "http://arxiv.org/abs/1703.03400v2", "Videos of the reinforcement learning results are atthis https URL"], ["v3", "Tue, 18 Jul 2017 16:45:29 GMT  (5063kb,D)", "http://arxiv.org/abs/1703.03400v3", "ICML 2017. Code atthis https URL, Videos of RL results atthis https URL, Blog post atthis http URL"]], "COMMENTS": "Videos of the reinforcement learning results are at sites.google.com/view/maml", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.NE", "authors": ["chelsea finn", "pieter abbeel", "sergey levine"], "accepted": true, "id": "1703.03400"}, "pdf": {"name": "1703.03400.pdf", "metadata": {"source": "META", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", "authors": ["Chelsea Finn", "Pieter Abbeel", "Sergey Levine"], "emails": ["<cbfinn@eecs.berkeley.edu>."], "sections": [{"heading": "1. Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2. Model-Agnostic Meta-Learning", "text": "In this section we will define the problem and present the general form of our algorithm."}, {"heading": "2.1. Meta-Learning Problem Set-Up", "text": "The goal of a few-shot meta-learning is to train a model that can quickly adapt to a new task by using only a few data points and training siterations. To achieve this, the model or learner is trained on a series of tasks during a meta-learning phase, so that the trained model can quickly adapt to new tasks by using only a small number of examples or experiments. In reality, the meta-learning problem treats entire tasks as training examples. In this section, we formalize this meta-learning problem in a general way, including brief examples of different learning areas. We will detail two different learning areas in Section 3.We will consider a model called f that maps observations x to outputs a. During meta-learning, the model is trained to adapt to a large or infinite number of tasks. As we would like to apply our framework to a variety of learning problems, from classification to enhancing learning."}, {"heading": "2.2. A Model-Agnostic Meta-Learning Algorithm", "text": "In contrast to the previous work, which attempted to train recurrent neural networks that record entire data sets to make rapid progress (Santoro et al., 2016; Duan et al., 2016b) or to enable embedding that can be combined with non-parametric methods at test date (Vinyals et al., 2016; Koch, 2015), we propose a method that can learn the parameters of each standard model using meta-learning in such a way that this model can be prepared for rapid adaptation. The intuition behind this approach is that some internal representations are more transferable than others. For example, a neural network could learn internal features that are generally applicable to all tasks in p (T) rather than to a single individual task. How can we promote the emergence of such general purpose representations? We take an explicit approach to this problem: since the model will be adapted to a new task using a gradient rule-based learning rule."}, {"heading": "3. Species of MAML", "text": "In this section we will discuss specific instances of our meta-learning algorithm for supervised learning and enhanced learning. The domains differ in the form of the loss function and in the way data is generated by the task and presented to the model, but in both cases the same basic adjustment mechanism can be applied."}, {"heading": "3.1. Supervised Regression and Classification", "text": "The goal is to learn a new function from only a few input / output pairs for this task, whereby previous dataAlgorithm 1 Model-Agnostic Meta-Learning Require: p (T): Distribution via tasks Require: \u03b1, \u03b2: Step size Hyperparameter1: random initialization of tasks 2: not executed but executed tasks Ti (T) 4: for all Ti do 5: Evaluation via tasks Require: \u03b1, \u03b2: Step size Hyperparameter in relation to K Examples 6: Compute-adapted parameters with gradient descent:."}, {"heading": "3.2. Reinforcement Learning", "text": "In Enhancing Learning (RL), the goal of meta-learning or little-shot learning is to enable an agent to quickly develop a policy for a new test task, using only a small amount of experience in the test environment. This could mean adapting the agent to achieve a new goal or to succeed on a previously trained goal in a new environment. In this section, we will discuss how MAML responds to metal earning for RL. Each RL task Ti contains an initial state distribution (x1) and a transition distribution (xt + 1), and the loss LTi corresponds to the (negative) reward function R. The entire task is therefore a Markov decision process (MDP) with Horizon H, where the student may query a limited number of sample projects for a few learning tasks."}, {"heading": "4. Related Work", "text": "The idea behind it is that people in the USA and in other parts of the world in which they live, can not only learn, but that they must also learn what they want, and that they must do it in order to understand the world. (...) It is not the first time that people in the USA, in Europe, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the, in the world, in the world, in the, in the world, in the, in the world, in the, in the world, in the, in the, in the, in the world, in the, in the world, in the, in the world, in the, in the world, in the, in the world, in the, in the, in the, in the, in the, in the, in the, in the, in the world, in the, in the world, in the, in the, in the, in the, in the, in the, in the, in the world, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in"}, {"heading": "5. Experimental Evaluation", "text": "The aim of our experimental evaluation is to answer the following questions: (1) Can MAML enable rapid learning of new tasks? (2) Does MAML work for several different areas, namely supervised regression, classification and reinforcement learning? (3) Can a model learned with MAML be further improved with additional gradient updates and / or examples? To this end, we evaluate our methodology for meta-learning problems from these three areas and examine problems of varying difficulty and characteristics. All the problems we are looking at require some adjustment to a new task at the test date. Whenever possible, we compare our results with an oracle that receives the identity of the task (which represents a problem-dependent representation) as an additional input for the performance of the model. All experiments were conducted using TensorFlow (Abadi et al., 2016), which enables automatic differentiation through gradient updating () during the release of the metal value for all experiments."}, {"heading": "5.1. Regression", "text": "We start with a simple regression problem that illustrates the basic principles of MAML. In this problem, the task is to go back from input to output of a sine wave, where the amplitude and phase of the sine wave are varied between the tasks. Thus, p (T) is continuous, where the amplitude varies within [0,1, 5,0] and the phase within [0, \u03c0], and the input and output both have a dimensionality of 1. During training and testing, datapoints are uniformly sampled from the input range, x [5,0, 5,0] and the loss is the mean square error between the prediction f (x) and the true value. The regressor is a neural network model with 2 hidden layers of size 40 with ReLU nonlinearity. In training with MAML, we use a gradient update with a fixed step size = 0.01, Adam and we are trained with optimisers."}, {"heading": "5.2. Classification", "text": "In order to evaluate the results, we need to analyze our method in a way that works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it, how it works in practice, how it works in practice, how it works in practice, how it, how it works in practice, how it works in practice, how it works in practice, how it works in practice, how it is in practice, how it works in practice, how it is in practice, how it works in practice, how it is in practice, how it works in practice, how it is in practice, how it works in practice, how it is in practice, how it works in practice, how it works in practice, how it is in practice, how it works in practice, how it works in practice, how it works in"}, {"heading": "5.3. Reinforcement Learning", "text": "This year it is more than ever before."}, {"heading": "6. Discussion and Future Work", "text": "We have introduced a meta-learning algorithm based on learning easily customizable network parameters through gradient descent, but our approach has a number of advantages. It is simple and does not introduce additional learned parameters into meta-learning. It can be combined with any model that is suitable for training through gradient descent, and with any differentiable goal, including classification, regression and reinforcement learning. Finally, since our method only produces good weight initialization, the adjustment can be performed with any amount of data and any number of gradient steps, although we show that it can achieve state-of-the-art results in classification with only one or five examples per class. We also show that our method can adapt an RL agent through policy gradations using a very modest measure of experience. Although our method is simple and widely applicable, it has a number of limitations. Because the meta-learning process can actually increase the weights of these steps with a small number of adjustments, it is possible."}, {"heading": "Acknowledgements", "text": "The authors thank Xi Chen and Trevor Darrell for helpful discussions, Yan Duan and Alex Lee for technical advice, and Nikhil Mishra, Haoran Tang and Greg Kahn for feedback on an early draft of the paper. This work was partially supported by an ONR PECASE Award and an NSF GRFP Award."}, {"heading": "A. Additional Experiment Details", "text": "In this section, we provide additional details of the experimental setup and the hyperparameter.A.1. Classification For the N-Way classification, the model learned from MAML for the N-Way, 1-Shot classification was trained and evaluated using 1-Shot and 5-Shot evaluation. For the 5-Shot evaluation, we simply increased the batch size for each gradient update from N examples to 5N amp. The 5-Way Convolutionary and 5-Shot Evaluation MAML models were each evaluated using 1 gradient step with step size \u03b1 = 0.4 and a Meta Batch Size of 32 tasks. The network was evaluated using 3 gradient steps with the same step size \u03b1 = 0.4. The 20-Way Convolutionary MAML models # # were evaluated using 1 gradient steps with step size \u03b1 # 0.4 and a Meta Batch Size of 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp; amp; 160 amp # 160 amp # 160 amp; 160 amp # 160 amp; 160 amp # 160 amp; 160 amp # 160 amp; 160 amp # 160 amp; 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp; amp # 160 amp; amp # 160 amp # 160 amp; amp # 160 amp # 160 amp; amp # 160 amp # 160 amp; amp # 160 amp # 160 amp # 160 amp # 160 amp; amp # 160 amp # 160 amp # 160 amp; amp # 160 amp # 160 amp; amp # 160 amp # 160 amp # 160 amp; amp # 160 amp # 160 amp; amp; amp; amp; amp # 160 during the meta meta meta meta meta batch size was determined on 16 tasks. All 2 models were found with a single 25000 Item number of assignments on a single A.A.A.A.A."}, {"heading": "B. Additional Sinusoid Visualizations", "text": "Figure 6 shows the qualitative performance of MAML and the pre-trained baseline using randomly selected sinusoids."}], "references": [{"title": "Learning to learn by gradient descent by gradient descent", "author": ["Andrychowicz", "Marcin", "Denil", "Misha", "Gomez", "Sergio", "Hoffman", "Matthew W", "Pfau", "David", "Schaul", "Tom", "de Freitas", "Nando"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Andrychowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz et al\\.", "year": 2016}, {"title": "Using fast weights to attend to the recent past", "author": ["Ba", "Jimmy", "Hinton", "Geoffrey E", "Mnih", "Volodymyr", "Leibo", "Joel Z", "Ionescu", "Catalin"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Ba et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2016}, {"title": "On the optimization of a synaptic learning rule", "author": ["Bengio", "Samy", "Yoshua", "Cloutier", "Jocelyn", "Gecsei", "Jan"], "venue": "In Optimality in Artificial and Biological Neural Networks, pp", "citeRegEx": "Bengio et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1992}, {"title": "Learning a synaptic learning", "author": ["Bengio", "Yoshua", "Samy", "Cloutier", "Jocelyn"], "venue": "rule. Universite\u0301 de Montre\u0301al, De\u0301partement d\u2019informatique et de recherche ope\u0301rationnelle,", "citeRegEx": "Bengio et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1990}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Donahue", "Jeff", "Jia", "Yangqing", "Vinyals", "Oriol", "Hoffman", "Judy", "Zhang", "Ning", "Tzeng", "Eric", "Darrell", "Trevor"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Rl2: Fast reinforcement learning via slow reinforcement learning", "author": ["Duan", "Yan", "Schulman", "John", "Chen", "Xi", "Bartlett", "Peter L", "Sutskever", "Ilya", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1611.02779,", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Towards a neural statistician", "author": ["Edwards", "Harrison", "Storkey", "Amos"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Edwards et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Edwards et al\\.", "year": 2017}, {"title": "Using fast weights to deblur old memories", "author": ["Hinton", "Geoffrey E", "Plaut", "David C"], "venue": "In Conference of the Cognitive Science Society (CogSci),", "citeRegEx": "Hinton et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1987}, {"title": "Learning to learn using gradient descent", "author": ["Hochreiter", "Sepp", "Younger", "A Steven", "Conwell", "Peter R"], "venue": "In International Conference on Artificial Neural Networks. Springer,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Learning to remember rare", "author": ["Kaiser", "Lukasz", "Nachum", "Ofir", "Roy", "Aurko", "Bengio", "Samy"], "venue": "events. International Conference on Learning Representations (ICLR),", "citeRegEx": "Kaiser et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kaiser et al\\.", "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Siamese neural networks for one-shot image recognition", "author": ["Koch", "Gregory"], "venue": "ICML Deep Learning Workshop,", "citeRegEx": "Koch and Gregory.,? \\Q2015\\E", "shortCiteRegEx": "Koch and Gregory.", "year": 2015}, {"title": "Data-dependent initializations of convolutional neural networks", "author": ["Kr\u00e4henb\u00fchl", "Philipp", "Doersch", "Carl", "Donahue", "Jeff", "Darrell", "Trevor"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Kr\u00e4henb\u00fchl et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kr\u00e4henb\u00fchl et al\\.", "year": 2016}, {"title": "One shot learning of simple visual concepts", "author": ["Lake", "Brenden M", "Salakhutdinov", "Ruslan", "Gross", "Jason", "Tenenbaum", "Joshua B"], "venue": "In Conference of the Cognitive Science Society (CogSci),", "citeRegEx": "Lake et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2011}, {"title": "Learning to optimize", "author": ["Li", "Ke", "Malik", "Jitendra"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Li et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Evolvability search: Directly selecting for evolvability in order to study and produce", "author": ["Mengistu", "Henok", "Lehman", "Joel", "Clune", "Jeff"], "venue": null, "citeRegEx": "Mengistu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mengistu et al\\.", "year": 2016}, {"title": "Meta-neural networks that learn by learning", "author": ["Naik", "Devang K", "Mammone", "RJ"], "venue": "In International Joint Conference on Neural Netowrks (IJCNN),", "citeRegEx": "Naik et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Naik et al\\.", "year": 1992}, {"title": "Actor-mimic: Deep multitask and transfer reinforcement learning", "author": ["Parisotto", "Emilio", "Ba", "Jimmy Lei", "Salakhutdinov", "Ruslan"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Parisotto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Parisotto et al\\.", "year": 2016}, {"title": "Optimization as a model for few-shot learning", "author": ["Ravi", "Sachin", "Larochelle", "Hugo"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Ravi et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ravi et al\\.", "year": 2017}, {"title": "One-shot generalization in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Danihelka", "Ivo", "Gregor", "Karol", "Wierstra", "Daan"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Rezende et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2016}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["Salimans", "Tim", "Kingma", "Diederik P"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Meta-learning with memory-augmented neural networks", "author": ["Santoro", "Adam", "Bartunov", "Sergey", "Botvinick", "Matthew", "Wierstra", "Daan", "Lillicrap", "Timothy"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Santoro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santoro et al\\.", "year": 2016}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew", "McClelland", "James", "Ganguli", "Surya"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "Evolutionary principles in selfreferential learning. On learning how to learn: The meta-meta-.", "author": ["Schmidhuber", "Jurgen"], "venue": "hook.) Diploma thesis, Institut f. Informatik, Tech. Univ. Munich,", "citeRegEx": "Schmidhuber and Jurgen.,? \\Q1987\\E", "shortCiteRegEx": "Schmidhuber and Jurgen.", "year": 1987}, {"title": "Learning to control fast-weight memories: An alternative to dynamic recurrent networks", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1992\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1992}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael I", "Moritz", "Philipp"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Learning to learn", "author": ["Thrun", "Sebastian", "Pratt", "Lorien"], "venue": "Springer Science & Business Media,", "citeRegEx": "Thrun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Thrun et al\\.", "year": 1998}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Todorov", "Emanuel", "Erez", "Tom", "Tassa", "Yuval"], "venue": "In International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Matching networks for one shot learning", "author": ["Vinyals", "Oriol", "Blundell", "Charles", "Lillicrap", "Tim", "Wierstra", "Daan"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Vinyals et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2016}, {"title": "Learning to reinforcement learn", "author": ["Wang", "Jane X", "Kurth-Nelson", "Zeb", "Tirumala", "Dhruva", "Soyer", "Hubert", "Leibo", "Joel Z", "Munos", "Remi", "Blundell", "Charles", "Kumaran", "Dharshan", "Botvinick", "Matt"], "venue": "arXiv preprint arXiv:1611.05763,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}], "referenceMentions": [{"referenceID": 2, "context": "Unlike prior meta-learning methods that learn an update function or learning rule (Schmidhuber, 1987; Bengio et al., 1992; Andrychowicz et al., 2016; Ravi & Larochelle, 2017), our algorithm does not expand the number of learned parameters nor place constraints on the model architecture (e.", "startOffset": 82, "endOffset": 174}, {"referenceID": 0, "context": "Unlike prior meta-learning methods that learn an update function or learning rule (Schmidhuber, 1987; Bengio et al., 1992; Andrychowicz et al., 2016; Ravi & Larochelle, 2017), our algorithm does not expand the number of learned parameters nor place constraints on the model architecture (e.", "startOffset": 82, "endOffset": 174}, {"referenceID": 22, "context": "by requiring a recurrent model (Santoro et al., 2016) or a Siamese network (Koch, 2015)), and it can be readily combined with fully connected, convolutional, or recurrent neural networks.", "startOffset": 31, "endOffset": 53}, {"referenceID": 22, "context": "In contrast to prior work, which has sought to train recurrent neural networks that ingest entire datasets (Santoro et al., 2016; Duan et al., 2016b) or feature embeddings that can be combined with nonparametric methods at test time (Vinyals et al.", "startOffset": 107, "endOffset": 149}, {"referenceID": 29, "context": ", 2016b) or feature embeddings that can be combined with nonparametric methods at test time (Vinyals et al., 2016; Koch, 2015), we propose a method that can learn the parameters of any standard model via meta-learning in such a way as to prepare that model for fast adaptation.", "startOffset": 92, "endOffset": 126}, {"referenceID": 26, "context": "Practical implementations of this method may also use a variety of improvements recently proposed for policy gradient algorithms, including state or action-dependent baselines and trust regions (Schulman et al., 2015).", "startOffset": 194, "endOffset": 217}, {"referenceID": 2, "context": "One of the popular approaches for meta-learning with neural networks is to train a meta-learner that learns how to update the weights of the learner\u2019s model (Bengio et al., 1992; Schmidhuber, 1992; Bengio et al., 1990).", "startOffset": 157, "endOffset": 218}, {"referenceID": 3, "context": "One of the popular approaches for meta-learning with neural networks is to train a meta-learner that learns how to update the weights of the learner\u2019s model (Bengio et al., 1992; Schmidhuber, 1992; Bengio et al., 1990).", "startOffset": 157, "endOffset": 218}, {"referenceID": 8, "context": "This approach has been applied to learning to optimize neural networks (Hochreiter et al., 2001; Andrychowicz et al., 2016; Li & Malik, 2017), as well as for learning dynamically changing recurrent neural networks (Ha et al.", "startOffset": 71, "endOffset": 141}, {"referenceID": 0, "context": "This approach has been applied to learning to optimize neural networks (Hochreiter et al., 2001; Andrychowicz et al., 2016; Li & Malik, 2017), as well as for learning dynamically changing recurrent neural networks (Ha et al.", "startOffset": 71, "endOffset": 141}, {"referenceID": 16, "context": "Similar methods have also been proposed that use evolutionary algorithms (Mengistu et al., 2016).", "startOffset": 73, "endOffset": 96}, {"referenceID": 20, "context": "Few-shot learning methods have also been developed for specific tasks such as generative modeling (Edwards & Storkey, 2017; Rezende et al., 2016) and image recognition (Vinyals et al.", "startOffset": 98, "endOffset": 145}, {"referenceID": 29, "context": ", 2016) and image recognition (Vinyals et al., 2016).", "startOffset": 30, "endOffset": 52}, {"referenceID": 29, "context": "Siamese networks (Koch, 2015) or recurrence with attention mechanisms (Vinyals et al., 2016).", "startOffset": 70, "endOffset": 92}, {"referenceID": 22, "context": "Memory augmented networks have been applied to few-shot recognition (Santoro et al., 2016) and learning \u201cfast\u201d reinforcement learning agents with recurrent policies (Duan et al.", "startOffset": 68, "endOffset": 90}, {"referenceID": 30, "context": ", 2016) and learning \u201cfast\u201d reinforcement learning agents with recurrent policies (Duan et al., 2016b; Wang et al., 2016).", "startOffset": 82, "endOffset": 121}, {"referenceID": 1, "context": "Memory augmentation can also take the form of a recurrent network with Hebbian learning updates (Ba et al., 2016).", "startOffset": 96, "endOffset": 113}, {"referenceID": 4, "context": "In computer vision, models pretrained on large-scale image classification tasks have been shown to learn effective features for a wide range of vision problems (Donahue et al., 2014).", "startOffset": 160, "endOffset": 182}, {"referenceID": 23, "context": "A number of prior works have explored the topic of sensitivity in deep networks, often in the context of studying initialization techniques for weight matrices (Saxe et al., 2014; Kirkpatrick et al., 2016).", "startOffset": 160, "endOffset": 205}, {"referenceID": 13, "context": "Most of these works have looked at good random initializations, though a number of papers have also addressed datadependent initializers (Kr\u00e4henb\u00fchl et al., 2016; Salimans & Kingma, 2016).", "startOffset": 137, "endOffset": 187}, {"referenceID": 14, "context": "To evaluate MAML in comparison to prior meta-learning and few-shot learning algorithms, we evaluate our method on a few-shot character recognition task using the Omniglot dataset (Lake et al., 2011), which consists 20 instances of 1623 characters from 50 different alphabets.", "startOffset": 179, "endOffset": 198}, {"referenceID": 29, "context": "Omniglot is a standard benchmark for few-shot learning used in a number of prior works (Vinyals et al., 2016; Santoro et al., 2016).", "startOffset": 87, "endOffset": 131}, {"referenceID": 22, "context": "Omniglot is a standard benchmark for few-shot learning used in a number of prior works (Vinyals et al., 2016; Santoro et al., 2016).", "startOffset": 87, "endOffset": 131}, {"referenceID": 14, "context": "To evaluate MAML in comparison to prior meta-learning and few-shot learning algorithms, we evaluate our method on a few-shot character recognition task using the Omniglot dataset (Lake et al., 2011), which consists 20 instances of 1623 characters from 50 different alphabets. Each instance was drawn by a different person. Omniglot is a standard benchmark for few-shot learning used in a number of prior works (Vinyals et al., 2016; Santoro et al., 2016). We follow the experimental protocol proposed by Vinyals et al. (2016), which involves fast learning of N -way classification with 1 or 5 shots.", "startOffset": 180, "endOffset": 526}, {"referenceID": 22, "context": "5-way Accuracy 20-way Accuracy 1-shot 5-shot 1-shot 5-shot MANN, no conv (Santoro et al., 2016) 82.", "startOffset": 73, "endOffset": 95}, {"referenceID": 29, "context": "matching nets (Vinyals et al., 2016) 98.", "startOffset": 14, "endOffset": 36}, {"referenceID": 10, "context": "(Kaiser et al., 2017) 98.", "startOffset": 0, "endOffset": 21}, {"referenceID": 22, "context": "is augmented with rotations by multiples of 90 degrees, as proposed by Santoro et al. (2016).", "startOffset": 71, "endOffset": 93}, {"referenceID": 22, "context": "In order to also provide a fair comparison against memory-augmented neural networks (Santoro et al., 2016) and to test the flexibility of MAML, we also experiment with a non-convolutional network.", "startOffset": 84, "endOffset": 106}, {"referenceID": 28, "context": "Our model follows the same architecture as the embedding function used by Vinyals et al. (2016), which has 4 modules with a 3\u00d7 3 convolutions and 64 filters, followed by batch normalization (Ioffe & Szegedy, 2015), a ReLU nonlinearity, and 2\u00d72 max-pooling.", "startOffset": 74, "endOffset": 96}, {"referenceID": 28, "context": "Our model follows the same architecture as the embedding function used by Vinyals et al. (2016), which has 4 modules with a 3\u00d7 3 convolutions and 64 filters, followed by batch normalization (Ioffe & Szegedy, 2015), a ReLU nonlinearity, and 2\u00d72 max-pooling. The Omniglot images are downsampled to 28\u00d728, so the dimensionality of the last hidden layer is 64. As in the baseline classifier used by Vinyals et al. (2016), the last layer is fed into a softmax.", "startOffset": 74, "endOffset": 417}, {"referenceID": 22, "context": "Memory-augmented neural networks (Santoro et al., 2016) specifically, and recurrent meta-learning models in general, represent a more broadly applicable class of methods that can be used for other tasks such as reinforcement learning (Duan et al.", "startOffset": 33, "endOffset": 55}, {"referenceID": 30, "context": ", 2016) specifically, and recurrent meta-learning models in general, represent a more broadly applicable class of methods that can be used for other tasks such as reinforcement learning (Duan et al., 2016b; Wang et al., 2016).", "startOffset": 186, "endOffset": 225}, {"referenceID": 26, "context": "The gradient updates are computed using vanilla policy gradient (REINFORCE) (Williams, 1992), and we use trust-region policy optimization (TRPO) as the meta-optimizer (Schulman et al., 2015).", "startOffset": 167, "endOffset": 190}, {"referenceID": 5, "context": "To evaluate MAML on reinforcement learning problems, we constructed several sets of tasks based off of the simulated continuous control environments in the rllab benchmark suite (Duan et al., 2016a). We discuss the individual domains below. In all of the domains, the model trained by MAML is a neural network policy with two hidden layers of size 100, with ReLU nonlinearities. The gradient updates are computed using vanilla policy gradient (REINFORCE) (Williams, 1992), and we use trust-region policy optimization (TRPO) as the meta-optimizer (Schulman et al., 2015). In order to avoid computing third derivatives, we use finite differences to compute the Hessian-vector products for TRPO. For both learning and meta-learning updates, we use the standard linear feature baseline proposed by Duan et al. (2016a), which is fitted separately at each iteration for each sampled task in the batch.", "startOffset": 179, "endOffset": 814}, {"referenceID": 28, "context": "complex deep RL problems, we also study adaptation on high-dimensional locomotion tasks with the MuJoCo simulator (Todorov et al., 2012).", "startOffset": 114, "endOffset": 136}, {"referenceID": 18, "context": "In fact, pretraining is in some cases worse than random initialization, a fact observed in prior RL work (Parisotto et al., 2016).", "startOffset": 105, "endOffset": 129}], "year": 2017, "abstractText": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on a fewshot image classification benchmark, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.", "creator": "LaTeX with hyperref package"}}}