{"id": "1703.03633", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Mar-2017", "title": "Learning Gradient Descent: Better Generalization and Longer Horizons", "abstract": "Training deep neural networks is a highly nontrivial task, involving carefully selecting appropriate training algorithms, scheduling step sizes and tuning other hyperparameters. Trying different combinations can be quite labor-intensive and time consuming. Recently, researchers have tried to use deep learning algorithms to exploit the landscape of the loss function of the training problem of interest, and learn how to optimize over it in an automatic way. In this paper, we propose a new learning-to-learn model and some useful and practical tricks. Our optimizer outperforms generic, hand-crafted optimization algorithms and state-of-the-art learning-to-learn optimizers by DeepMind in many tasks. We demonstrate the effectiveness of our algorithms on a number of tasks, including deep MLPs, CNNs, and simple LSTMs.", "histories": [["v1", "Fri, 10 Mar 2017 11:30:03 GMT  (3751kb,D)", "https://arxiv.org/abs/1703.03633v1", "9 pages, 8 figures, 3 tables, submitted to ICML 2017"], ["v2", "Mon, 13 Mar 2017 02:45:49 GMT  (3751kb,D)", "http://arxiv.org/abs/1703.03633v2", "9 pages, 8 figures, 3 tables"], ["v3", "Sat, 10 Jun 2017 16:25:37 GMT  (3538kb,D)", "http://arxiv.org/abs/1703.03633v3", "Accepted to ICML 2017, 9 pages, 9 figures, 4 tables"]], "COMMENTS": "9 pages, 8 figures, 3 tables, submitted to ICML 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["kaifeng lv", "shunhua jiang", "jian li"], "accepted": true, "id": "1703.03633"}, "pdf": {"name": "1703.03633.pdf", "metadata": {"source": "META", "title": "Learning Gradient Descent: Better Generalization and Longer Horizons", "authors": ["Kaifeng Lv", "Shunhua Jiang", "Jian Li"], "emails": ["ing@163.com>,", "Jiang<linda6582@163.com>,", "<lijian83@mail.tsinghua.edu.cn>."], "sections": [{"heading": "1. Introduction", "text": "The formation of a neural network can be seen as a solution to an optimization problem for a highly non-convex loss function. Gradient-based algorithms are by far the most widely used algorithms for forming neural networks, such as basic SGD, Adagrad, RMSprop, Adam, etc. For a particular neural network, it is initially unclear which is the best optimization algorithm and how to set up the hyperparameters (such as learning rates). It usually takes a lot of time and skilled hands to determine the best optimization algorithm together with the best hyperparameters, and some other tricks may be necessary for the network to work. * Equal contribution \u2020 Research is partially supported by the National Basic Research Program of China."}, {"heading": "1.1. Existing Work", "text": "To address the above problem, a promising approach is the use of machine learning algorithms to replace hard-coded optimization algorithms, and hopefully the learning algorithm will be able to learn a good strategy, from experience, to explore the landscape of loss function and adaptively choose good downward steps. However, at a high level, the idea can be categorized under the umbrella of learning tolearn (or meta-learning), a broad area known to the learning community for more than two decades. In a recent paper (Andrychowicz et al., 2016), the authors proposed an optimizer that uses a coordinated long-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) that performs the optimizer's course as input and output of updates for each optimization level MMSN."}, {"heading": "1.2. Our Contributions", "text": "In this article, we propose two new training tricks and a new model to improve the results of the formation of a recurrent neural Xiv: 170 3.03 633v 3 [cs.L G] 10 Jun 2017network (RNN) to optimize the loss functions of real neural networks. The other trick is to combine the loss function of the optimizer with other simple convex functions that help speed up the training process. With the help of our new training tricks, our new model, called RNNprop, achieves remarkable improvements over previous work after being trained on a simple 1-hidden-layer MLP: 1. It can train optimizations for longer horizons, especially when RNNprop is only trained to minimize the final loss of a 100-step training process."}, {"heading": "2. Other Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Learning to Learn", "text": "The concept of learning to learn or meta-learning has been used for years to address the concept of learning from meta-knowledge about the learning process. However, there is no agreement on the exact definition of meta-learning, and various concepts have been developed by different authors (Thrun & Pratt, 1998; Vilalta & Drissi, 2002; Brazdil et al., 2008).In this paper, we consider the training process of a neural network as an optimization problem, and we use an RNN as an optimizer to train other neural networks. The use of another neural network to guide the formation of neural networks was advanced by Naik and Mammone (1992).In their early work, Cotter and Younger (1990; 1999) argued that RNNNN can be used as optimization algorithms (Prokhorov et al., 2002)."}, {"heading": "2.2. Traditional Optimization Algorithms", "text": "A large number of optimization algorithms have been proposed to improve the performance of vanilla gradients, including Momentum (Tseng, 1998), Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), RMSprop (Tieleman & Hinton, 2012), Adam (Kingma & Ba, 2015)."}, {"heading": "3. Rethinking of Optimization Problems", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Problem Formalization", "text": "We are interested in finding an optimizer that performs the optimization tasks for different optimizers. An optimizer is a function f (\u03b8) that must be minimized. In the case that the optimizer is stochastic, i.e. the value of f (\u03b8) depends on the sample d selected from a dataset D, the goal of an optimizer is to perform 1 | D | vid D fd (\u03b8) (1) via the variables \u03b8.When optimizing an optimizer on a datasetD, the behavior of an optimizer can be summarized by the following loop: 1. Taking into account the current parameters \u0445t and a sample dt-D, forward and backward propagation is performed to calculate the functional value yt = fdt (\u0442t) and the degree of optimization gt = \u0445fdt (\u0442t)."}, {"heading": "3.2. Some Insight into Adaptivity", "text": "Table 1 Summaries Optimization algorithms that are most commonly used when training neural networks. However, all of these optimization algorithms automatically have some degree of adaptability, that is, they are able to adjust the effective step size. Second class includes Adagrad, Adadelta, RMSprop, and Adam. These algorithms get the sum or moving average of past gradients g2t, because they can consider the effective step size based on the absolute size of the gradients. Second class includes Adagrad, Adadelta, RMSprop, and Adam. These algorithms get the sum or moving average of past gradients g2t, which can be considered, with a little misuse of terminology, the second raw moment (or uncentered variance). Then, these algorithms produce the effective step size only by the relative size of the gradient, namely the gradient divided by the square root of the second coordinate moment."}, {"heading": "4. Methods", "text": "Our RNN Optimizer works in a coordinated manner on the parameters \u03b8 that result directly from (Andrychowicz et al., 2016).The RNN Optimizer processes the slopes in a coordinated manner and obtains hidden states for each coordinate.The parameters of the RNN itself are divided into different coordinates.In this way, the RNN Optimizer Optimizer can train optimizers with any number of parameters."}, {"heading": "4.1. Random Scaling", "text": "Before introducing our ideas, we should consider what happens when we train an RNN Optimizer to minimize f (\u03b8) = \u03bb \u0445 \u03b8 \u0445 22 with initial parameter \u03b80. Clearly, a test of this RNN Optimizer on the same function with different approach could produce a modest or even bad result, since the lowest point can be reached in only one step. However, if the RNN Optimizer learns to follow this rule closely, we can also select a random number to scale all parameters to achieve the same goal. To further generalize this idea, we design our training trick, Random Scaling, which sets the parameters of the target function in the training day.In more detail, we are forced to choose a first stage c optimizer (we will generalize)."}, {"heading": "4.2. Combination with Convex Functions", "text": "Now we present another training trick. It is clear that we should train our RNN optimizer on optimizers implemented with neural networks. However, due to the non-convex and stochastic nature of neural networks, it can be difficult for an RNN to learn the basic idea of gradient descent. Our idea is loosely inspired by proximal algorithms (see e.g. (Parikh & Boyd, 2014). To simplify the training, we combine the original optimization function f with an n-dim-convex function g to get a new optimization function FF (x) = f (\u03b8) + g (x). (3) For each exercise of the RNN optimizer, we generate a random vector v in n-dim vector space, and the function g is defined as asg (x) = 1n function."}, {"heading": "4.3. RNNprop Model", "text": "Apart from the two tricks mentioned above, we are also designing a new model RNNprop, as shown in Figure 1. All operations in our model are coordinative, according to the idea of the DMoptimizer idea in (Andrychowicz et al., 2016).The main difference between RNNprop and DMoptimizer is the input. The input m-t and g-t are defined as follows: m-t = m-tv-1 / 2 t, (5) g-t = gtv-1 / 2 t, (6) where m-t, v-t-t like Adam in Table 1. This change in input grading has three advantages. Firstly, this input does not contain information about the absolute size of the gradients, so that our algorithm automatically belongs to the second class and is therefore more robust. Secondly, this manipulation of the gradients can be regarded as a kind of normalization, so that the input values are affected by a constant."}, {"heading": "5. Experiments", "text": "We trained two RNN Optimizers in five phases of 20 steps each. We trained two RNN Optimizers in five phases of 20 steps each. We trained two RNN Optimizers in five phases of 20 steps each. We trained the Optimizers in 100 steps. We trained them in 100 steps. We trained them in 100 steps. We trained them in 100 steps of 20 steps. We trained them in 100 steps of 20 steps. We trained the Optimizers in 100 steps of 20 steps. We trained them in 100 steps of 20 steps. We trained the Optimizers in 100 steps of 20 steps. We trained them in 100 steps of 20 steps. We trained them in 100 steps of 20 steps. We trained them in 100 steps of 20 steps."}, {"heading": "5.1. Generalization to More Steps", "text": "Both DMOptimizer and RNNprop exceed all conventional optimization algorithms. DMOptimizer has better performance possibly due to overadjustment. We then test the optimizers to run for further steps on the basis of MLP. The left chart of Figure 2 shows that RNNprop can achieve comparable performance with conventional algorithms for 2000 steps, while the DMOptimizer fails. We also test the optimizers for many more steps: 10,000 steps, as in the right chart of Figure 2. It is clear that the DMOptimizer loses the ability to reduce the loss after about 400 steps, and its loss begins to increase dramatically. RNNprop, on the other hand, is able to continuously reduce the loss, although it gradually slows down and traditional algorithms overtake it. The main reason for this is that RNNprop is trained to run only for 100 steps, and 10000 steps of the training process we can significantly complete the optimization step 100."}, {"heading": "5.2. Generalization to Different Activation Functions", "text": "We test optimizers based on MLP with different activation functions. As shown in Figure 4, when the activation function is changed from sigmoid to ReLU, RNNprop can still perform better than traditional algorithms, while DMoptimizer fails. At other activations, RNprop generalizes well as shown in Table 2."}, {"heading": "5.3. Generalization to Deeper MLP", "text": "In deep neural networks, different layers may have different optimal learning rates, but traditional algorithms only have a global learning rate for all parameters. Our RNN optimizer can achieve better performance, benefiting from a more adaptable behavior. We tested the optimizers on deeper MLPs. More hidden layers are added to the base MLP, which have all 20 hidden units and use Sigmoid as an activation function. As shown in Figure 6, RNNprop can always overtake traditional algorithms until the MLP becomes too deep and none of them can reduce its loss in 100 steps. Figure 5 shows the loss curves on the MLP with 5 hidden layers as an example."}, {"heading": "5.4. Generalization to Different Structures", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.4.1. CNN", "text": "The CNN optimizers are the cross-entropy losses of Convolutionary Neural Networks (CNN) with a structure similar to VGGNet (Simonyan & Zisserman, 2015) on the MNIST dataset or the CIFAR-10 dataset. All Convolutionary Layers use 3 x 3 filters and the window of each Max Pooling Layer is 2 x 2. We use c to designate a Convolutionary Layer, p to designate a Max Pooling Layer, and f to designate a Fully Connected Layer. In the experiments, three CNN are used: CNN with the structure c-c-p-f on MNIST, CNN with the structure c-c-p-f on MNIST, and CNN with the structure c-c-p-f on the structure c-p-f on CIFAR-10. The results are shown in Figure 7. RNNprop shows that the traditional algorithms on CNN with the structure c-c-f-traditional data can be best fixed on the NT, although the NT is the best."}, {"heading": "5.4.2. LSTM", "text": "Optimizers are also tested for the mean square loss of an LSTM with a hidden state variable of 20 in a simple task: when specifying a sequence f (0),.., f (9) with additive noise, the LSTM must predict the value of f (10). Here f (x) = A sin (\u03c9x + \u03c6). When generating the data set, we choose evenly according to the random principle A \u0445 U (0, 10), \u03c9 U (0, \u03c0 / 2), \u03c6 U (0, 2\u03c0) and draw the noise from the Gaussian distribution N (0, 0.1). Although the task is completely different from the task used for training, RNNprop still has a comparable or even better performance than conventional algorithms, which may be due to the fact that the structure within LSTM is similar to that of the basic MLP with sigmoidem in between. We also adjust the settings to the task."}, {"heading": "5.5. Control Experiment", "text": "In order to evaluate the effectiveness of each contribution individually, we have also trained three other RNN optimizers: DMoptimizer trains with the two tricks and two RNNprop, each trained with one of the two tricks. Remember that the trick of combining with convex function aims to speed up the training of RNN optimizers. We test the performance of RNNprop, whose own parameters are trained for different number of iterations, with or without this trick. The result is shown in Table 4. With this trick, RNN optimizers can achieve a good result with less iterations of the training. To evaluate the other two contributions, we select the trained optimizers in the same way as RNNprop. In Figure 9, we test their performance based on MLP with activation replaced by ReLU for 1000 steps. From the figure, we conclude that Random Scaling is the most effective trick."}, {"heading": "6. Conclusion", "text": "In this article, we present a new Learning-to-Learning model with several useful tricks. We show that our new optimizer has better generalization capabilities than modern Learning-to-Learning Optimizers. After training with a simple MLP, our new optimizer performs better or comparable with traditional optimization algorithms when training more complex neural networks or training for longer horizons. We believe it is possible to further improve the generalization capability of our optimizer. In fact, our optimizer has not exceeded the best traditional optimization algorithms in some tasks in our experiments, especially when training for a much longer horizon or training neural networks on different data sets. In the future, we aim to develop a more general optimizer with a more elaborate design so that it can perform better on a broader range of tasks analogous to the optimizer used in training."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Metalearning: Applications to Data Mining", "author": ["P. Brazdil", "C.G. Carrier", "C. Soares", "R. Vilalta"], "venue": null, "citeRegEx": "Brazdil et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Brazdil et al\\.", "year": 2008}, {"title": "Learning to learn for global optimization of black box functions", "author": ["Y. Chen", "M.W. Hoffman", "S.G. Colmenarejo", "M. Denil", "T.P. Lillicrap", "N. de Freitas"], "venue": "arXiv preprint arXiv:1611.03824,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["D.A. Clevert", "T. Unterthiner", "S. Hochreiter"], "venue": "arXiv preprint arXiv:1511.07289,", "citeRegEx": "Clevert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clevert et al\\.", "year": 2015}, {"title": "Fixed-weight networks can learn", "author": ["N.E. Cotter", "P.R. Conwell"], "venue": "In IJCNN International Joint Conference on Neural Networks,", "citeRegEx": "Cotter and Conwell,? \\Q1990\\E", "shortCiteRegEx": "Cotter and Conwell", "year": 1990}, {"title": "Learning step size controllers for robust neural network training", "author": ["C. Daniel", "J. Taylor", "S. Nowozin"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Daniel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Daniel et al\\.", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Using deep q-learning to control optimization hyperparameters", "author": ["S. Hansen"], "venue": "arXiv preprint arXiv:1602.04062,", "citeRegEx": "Hansen,? \\Q2016\\E", "shortCiteRegEx": "Hansen", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Learning to learn using gradient descent", "author": ["S. Hochreiter", "A. Younger", "P. Conwell"], "venue": "International Conference on Artificial Neural Networks,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Learning to optimize", "author": ["K. Li", "J. Malik"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Li and Malik,? \\Q2017\\E", "shortCiteRegEx": "Li and Malik", "year": 2017}, {"title": "Meta-neural networks that learn by learning", "author": ["D.K. Naik", "R.J. Mammone"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Naik and Mammone,? \\Q1992\\E", "shortCiteRegEx": "Naik and Mammone", "year": 1992}, {"title": "Adaptive behavior with fixed weights in rnn: an overview", "author": ["D.V. Prokhorov", "L.A. Feldkarnp", "I.Y. Tyukin"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Prokhorov et al\\.,? \\Q2018\\E", "shortCiteRegEx": "Prokhorov et al\\.", "year": 2018}, {"title": "Simple Principles of Metalearning", "author": ["J. Schmidhuber", "J. Zhao", "M. Wiering"], "venue": "Istituto Dalle Molle Di Studi Sull\u2019Intelligenza Artificiale,", "citeRegEx": "Schmidhuber et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Schmidhuber et al\\.", "year": 1999}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Simonyan and Zisserman,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2015}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "An incremental gradient (-projection) method with momentum term and adaptive stepsize rule", "author": ["P. Tseng"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Tseng,? \\Q1998\\E", "shortCiteRegEx": "Tseng", "year": 1998}, {"title": "A perspective view and survey of meta-learning", "author": ["R. Vilalta", "Y. Drissi"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Vilalta and Drissi,? \\Q2002\\E", "shortCiteRegEx": "Vilalta and Drissi", "year": 2002}, {"title": "Learning to reinforcement learn", "author": ["J.X. Wang", "Z. Kurthnelson", "D. Tirumala", "H. Soyer", "J.Z. Leibo", "R. Munos", "C. Blundell", "D. Kumaran", "M. Botvinick"], "venue": "arXiv preprint arXiv:1611.05763,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Fixedweight on-line learning", "author": ["A.S. Younger", "P.R. Conwell", "N.E. Cotter"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Younger et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Younger et al\\.", "year": 1999}, {"title": "Metalearning with backpropagation", "author": ["A.S. Younger", "S. Hochreiter", "P.R. Conwell"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Younger et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Younger et al\\.", "year": 2001}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler,? \\Q2012\\E", "shortCiteRegEx": "Zeiler", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "However, there is no agreement on the exact definition of meta-learning, and various concepts have been developed by different authors (Thrun & Pratt, 1998; Vilalta & Drissi, 2002; Brazdil et al., 2008).", "startOffset": 135, "endOffset": 202}, {"referenceID": 21, "context": "This idea was further developed in (Younger et al., 2001; Hochreiter et al., 2001) and gradient descent is used to train an RNN optimizer on convex problems.", "startOffset": 35, "endOffset": 82}, {"referenceID": 9, "context": "This idea was further developed in (Younger et al., 2001; Hochreiter et al., 2001) and gradient descent is used to train an RNN optimizer on convex problems.", "startOffset": 35, "endOffset": 82}, {"referenceID": 2, "context": "In another recent paper (Chen et al., 2016), an RNN is used to take current position and value as input and outputs the next position, and it works well for black-box optimization and simple RL tasks.", "startOffset": 24, "endOffset": 43}, {"referenceID": 14, "context": "From a reinforcement learning perspective, the optimizer can be viewed as a policy which takes the current state as input and output the next action (Schmidhuber et al., 1999).", "startOffset": 149, "endOffset": 175}, {"referenceID": 5, "context": "Two recent papers (Daniel et al., 2016; Hansen, 2016) trained adaptive controllers to adjust the hyperparameters (learning rate) of traditional optimization algorithms from this perspective.", "startOffset": 18, "endOffset": 53}, {"referenceID": 7, "context": "Two recent papers (Daniel et al., 2016; Hansen, 2016) trained adaptive controllers to adjust the hyperparameters (learning rate) of traditional optimization algorithms from this perspective.", "startOffset": 18, "endOffset": 53}, {"referenceID": 19, "context": "More general methods have been introduced in (Li & Malik, 2017; Wang et al., 2016) which also take the RL perspective and train a neural network to model a policy.", "startOffset": 45, "endOffset": 82}, {"referenceID": 8, "context": "The usage of another neural network to direct the training of neural networks has been put forward by Naik and Mammone (1992). In their early work, Cotter and Younger (1990; 1999) argued that RNNs can be used to model adaptive optimization algorithms (Prokhorov et al.", "startOffset": 102, "endOffset": 126}, {"referenceID": 6, "context": ", 2001; Hochreiter et al., 2001) and gradient descent is used to train an RNN optimizer on convex problems. Recently, as shown in Section 1.1, Andrychowicz et al. (2016) proposed a more general optimizer model using LSTM to learn gradient descent, and our work directly follows their work.", "startOffset": 8, "endOffset": 170}, {"referenceID": 17, "context": "A great number of optimization algorithms have been proposed to improve the performance of vanilla gradient descent, including Momentum(Tseng, 1998), Adagrad(Duchi et al.", "startOffset": 135, "endOffset": 148}, {"referenceID": 6, "context": "A great number of optimization algorithms have been proposed to improve the performance of vanilla gradient descent, including Momentum(Tseng, 1998), Adagrad(Duchi et al., 2011), Adadelta(Zeiler, 2012), RMSprop(Tieleman & Hinton, 2012), Adam(Kingma & Ba, 2015).", "startOffset": 157, "endOffset": 177}, {"referenceID": 22, "context": ", 2011), Adadelta(Zeiler, 2012), RMSprop(Tieleman & Hinton, 2012), Adam(Kingma & Ba, 2015).", "startOffset": 17, "endOffset": 31}, {"referenceID": 3, "context": "The input is preprocessed by a fully-connected layer with ELU (Exponential Linear Unit) as the activation function (Clevert et al., 2015) before being handled by the RNN.", "startOffset": 115, "endOffset": 137}, {"referenceID": 0, "context": "Their performances are compared with the best traditional optimization algorithms whose learning rates are carefully chosen and other hyperparameters are set to the default values in Tensorflow (Abadi et al., 2016).", "startOffset": 194, "endOffset": 214}], "year": 2017, "abstractText": "Training deep neural networks is a highly nontrivial task, involving carefully selecting appropriate training algorithms, scheduling step sizes and tuning other hyperparameters. Trying different combinations can be quite labor-intensive and time consuming. Recently, researchers have tried to use deep learning algorithms to exploit the landscape of the loss function of the training problem of interest, and learn how to optimize over it in an automatic way. In this paper, we propose a new learning-to-learn model and some useful and practical tricks. Our optimizer outperforms generic, hand-crafted optimization algorithms and state-of-the-art learning-to-learn optimizers by DeepMind in many tasks. We demonstrate the effectiveness of our algorithms on a number of tasks, including deep MLPs, CNNs, and simple LSTMs.", "creator": "LaTeX with hyperref package"}}}