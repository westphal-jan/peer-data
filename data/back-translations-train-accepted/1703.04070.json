{"id": "1703.04070", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2017", "title": "Prediction and Control with Temporal Segment Models", "abstract": "We introduce a method for learning the dynamics of complex nonlinear systems based on deep generative models over temporal segments of states and actions. Unlike dynamics models that operate over individual discrete timesteps, we learn the distribution over future state trajectories conditioned on past state, past action, and planned future action trajectories, as well as a latent prior over action trajectories. Our approach is based on convolutional autoregressive models and variational autoencoders. It makes stable and accurate predictions over long horizons for complex, stochastic systems, effectively expressing uncertainty and modeling the effects of collisions, sensory noise, and action delays. The learned dynamics model and action prior can be used for end-to-end, fully differentiable trajectory optimization and model-based policy optimization, which we use to evaluate the performance and sample-efficiency of our method.", "histories": [["v1", "Sun, 12 Mar 2017 04:59:15 GMT  (863kb,D)", "https://arxiv.org/abs/1703.04070v1", null], ["v2", "Thu, 13 Jul 2017 04:54:00 GMT  (787kb,D)", "http://arxiv.org/abs/1703.04070v2", "camera-ready version, ICML 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO stat.ML", "authors": ["nikhil mishra", "pieter abbeel", "igor mordatch"], "accepted": true, "id": "1703.04070"}, "pdf": {"name": "1703.04070.pdf", "metadata": {"source": "CRF", "title": "Prediction and Control with Temporal Segment Models", "authors": ["Nikhil Mishra", "Pieter Abbeel", "Igor Mordatch"], "emails": ["<nmishra@berkeley.edu>."], "sections": [{"heading": "1. Introduction", "text": "The problem of learning - where an agent learns how his actions affect the state and environment - is a key problem in robotics and enhancing learning. An agent equipped with a dynamic model can learn an approach that requires a wide range of tasks that cannot be detected in advance."}, {"heading": "2. Related Work", "text": "A number of options are available for the representation of learned dynamics models, including linear functions (Mordatch et al., 2016; Yip & Camarillo, 2014), Gaussian processes (Boedecker et al., 2014; Ko & Fox, 2009; Deisenroth & Rasmussen, 2011), predictive state representations (PSRs) (Littman et al., 2002; Rosencrantz et al., 2004), and deep neural networks (Punjani & Abbeel, 2015; Fragkiadaki et al., 2015; Agrawal et al al al., 2016). Linear functions are efficient for evaluating and resolving controls, but have limited expression possibilities. Gaussian processes (Williams & Rasmussen, 1996) provide estimates of uncertainty, but scaling to large amounts of data."}, {"heading": "3. Segment-Based Dynamics Model", "text": "Suppose we have a non-linear dynamic system with states and actions ut. The conventional approach to learning dynamics is to learn a function xt + 1 = f (xt, ut) using an approximator like a neural network (possibly recursive).We are looking at a more general formulation of the problem presented in Figure 1: predetermined segments (of length H) of past states X \u2212 = {xt \u2212 H,., xt \u2212 1} and actions U \u2212 = {ut \u2212 H,., ut \u2212 1}. If we treat these four temporal segments as random variables, then we want to predict the entire segment of future states X + = {xt,..., xt + H} that would result from actions U + = {ut,. \u2212 H \u2212 1}. If we treat these four temporal segments as random variables X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X"}, {"heading": "3.1. Model Architecture and Training", "text": "In the previous section, we discussed a conditional variable autoencoder whose generative path serves as a stochastic dynamic model. Here, we will go into some architectural details. A diagram of the entire training setup is shown in Figure 2. For further details of the architectures used in our experiments, see Appendix A. The encoder network Q (x) explicitly parameterizes a Gaussian distribution over latent codes Z with diagonal covariance. It consists of a stack of 1D-Convolutionary layers, the output of which is flattened and projected into a single vector containing a mean \u00b5Z and a variance of K2Z. We then try z-N (\u00b5Z, \u03c32Z) in a differentiated manner using the repair trick (Kingma & Welling, 2014).The decoder network P (x) attempts to model a distribution over a segment of the states P (X +)."}, {"heading": "4. Control with Segment-Based Models", "text": "Once we have learned a dynamic model, we want to use it to perform various tasks, each of which can be expressed as a reward function r (xt, ut). Track optimization and policy optimization are two settings where a dynamic model would normally be used, and provide meaningful ways to evaluate a dynamic model."}, {"heading": "4.1. Trajectory Optimization", "text": "In track optimization, we want to find a sequence of actions that can be applied to perform a particular instance of a task. Specifically, we want to use a reward function r to maximize the sum of rewards along the track resulting from the application of actions u1,..., uT, starting from an initial state x0. This can be summarized by the following optimization problem: max u1,..., uT E [T \u2211 t = 1r (xt, ut)] with xt \u0445 P (x) (xt | x0: t \u2212 1, u1: t) (2), where r (xt, ut) is the reward received at the time t, and X = {x1,..., xT} is the sequence of states that would result from performing actions U = {u1,..., uT} from the initial state x0 in the dynamic model P (x)."}, {"heading": "4.2. Latent Action Priors", "text": "If we try to solve the optimization problem as in (2), the solution will often try to apply action sequences outside the multiplicity in which the dynamic model is valid: These actions come from a very different distribution than the action distribution of the training data. This can be problematic: Optimization can find actions that achieve high rewards under the model (by exploiting it in a regime in which it is invalid) but that do not reach the target when executed in the real environment. To mitigate this problem, we propose the use of another conditional variable autoencoder, this one through segments of actions. In particular, given sequences of past actions U \u2212 = {ut \u2212 H,., ut \u2212 1} and future actions U + =., ut + H}, we want to model the conditional distribution P (U + | U)."}, {"heading": "4.3. Policy Optimization", "text": "The goal of policy optimization is to maximize the value of politics in terms of its parameters, the class of algorithms known as political gradient methods (Sutton et al., 1999a; Peters & Schaal, 2006), and we try to solve this problem without considering the dynamization of politics."}, {"heading": "5. Experiments", "text": "Our experiments examine the following questions: (i) How well do segment-based models predict dynamics? (ii) How does predictive accuracy translate into control applications? How does this scale function with the difficulty of the task and stochasticity in dynamics? (iii) How is it affected by the use of latent action priors? (iv) Is there any meaning or structure encoded by the latent space that the dynamic model learns?"}, {"heading": "5.1. Environments", "text": "For a dynamic model to be versatile enough for use in control settings, the training data must include a variety of actions that explore a diverse subset of the state space. Efficient exploration strategies are an open problem in amplification learning and are not the focus of this work. Against this background, our experiments assume a simulated 2-DOF arm that moves on a plane (as implemented in the Reacher environment in the OpenAI gym), because performing random actions in this environment leads to sufficient exploration. In each of our experiments, we consider the following environments (illustrated in Figure 4): (i) The basic, unmodified Reacher environment. (ii) A version that contains an obstacle that the arm may collide with: The obstacle cannot move, but its position is randomly chosen at the beginning of each episode. (iii) A version in which an arm's damped segment length allows us to push the space length of this object and the dimension of the 1."}, {"heading": "5.2. Baselines", "text": "We compare our method with the following basics: (i) A single-stage model: a learned function xt + 1 = f (xt, ut), where f is a fully connected neural network. It is trained using a single-stage prediction l2 loss of tuples (xt, ut, xt + 1). (ii) A single-stage model that is rolled out during training for several periods of time.The model is still a learned function xt + 1 = f (xt, ut), but it is trained with a multi-stage prediction loss over a horizon of 2H. (iii) An LSTM model that can store information about the past in a hidden state can be: xt + 1, ht + 1 = f (xt, ut, ht) and is more difficult to make a multi-stage prediction (2M) with the dynamics we expect over the same horizon."}, {"heading": "5.3. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.3.1. DYNAMICS PREDICTION", "text": "After learning a dynamic model, we evaluate it using a test set of sustained trajectories, calculating the average log probability of the test data within the model. For our method, we obtain samples from the model, match a Gaussian to the samples, and determine the log probability of the actual trajectory under the adjusted Gaussian. Since the basic methods do not express uncertainty, but are trained on the basis of an L2 loss, we interpret their predictions as the mean of a Gaussian distribution whose variance is constant across all state dimensions and time periods (since minimizing the L2 loss corresponds to maximizing that log probability). We then adjust the value of the variance constant to calculate the log probability on the testset.Figure 5 compares our method with the base values of L2 losses in each environment. The reported values are log probabilities per time averaged over a test set of 1,000 trajectories."}, {"heading": "5.3.2. CONTROL1", "text": "Next, we compare our method with the baselines for trajectories and policy optimization. Of interest is both the actual reward achieved in the environment and the difference between the true reward and the expected reward within the model. If a control algorithm uses the model to predict unrealistic behavior, it becomes larger. We consider two tasks: The arm must move its end effect to a desired position; the reward function is the negative distance between the end effector and the target position."}, {"heading": "5.3.3. SENSORY NOISE AND DELAYED ACTIONS", "text": "In order to investigate the effects of stochastic dynamics and delayed actions, we consider two other modifications of the Reacher environment, one in which there is considerable Gaussian noise in the state observations (\u03c3 = 0.25 on data in the range [\u2212 1, + 1]), and one in which actions are delayed: they do not work for \u03c4 = 5 time steps after application. These challenges usually occur in real robot applications (Atkeson et al., 2016), and therefore it is important to be able to learn a useful dynamic model in each setting. We learn a dynamic model with each method for both the environment of the loud state and for the delayed action, and then use it to learn a policy for the task to be achieved. Figure 8 shows the resulting learning curves. Our dynamic model behaves much better than the basic lines, both with and without previous measures. In particular, using the LSTM model leads to a much worse policy than our own, although it is less likely to react to the underlying elements."}, {"heading": "5.3.4. ANALYSIS OF LATENT SPACE", "text": "Variational autoencoders are known to learn lossy latent codes that maintain a high-level semantics of data, allowing the decoder to determine the details at a low level. Consequently, we are curious to see if our dynamic model learns a latent space that has similar properties. Applied to dynamic data, one might expect a latent code to provide a general description of what happens in the state trajectory X + that encodes it. Alternatively, according to Chen et al. (2016), it is conceivable that the decoder completely ignores the latent code because the segments X \u2212, U \u2212, U + provide better information than Z about X +. However, we observe that our model learns a meaningful latent space: one that encodes uncertainty about the future. A certain latent code corresponds to a certain future within the space of possible codes given X \u2212, U \u2212, U +. If the dynamics are simple and deterrent (this model expresses the uncertainty directly about the original environment)."}, {"heading": "5.3.5. EFFECT OF LATENT ACTION PRIOR", "text": "Our previous experiments have shown the advantages of latent action before: by considering only actions for which the dynamic model is valid, the discrepancy between the model and the true dynamic is minimized, resulting in higher rewards achieved in the actual environment.In this section, we qualitatively examine how the actions returned by control algorithms differ as a result of the latent action prior to that. An example is Figure 10. In the training data, the actions that the agent performs are smooth, random torques, and we observe that solutions from flight path optimization look similar. In contrast to solutions from optimization directly via actions that are sharp and discontinuous, unlike anything the dynamic model has seen before, we can conclude that the baseline performs poorly in the pressing task (as shown in Figure 6) due to large discrepancies between model prediction and actual execution."}, {"heading": "6. Conclusion and Future Work", "text": "We presented a novel approach to dynamic learning based on time segments, using a variational auto encoder to learn the distribution of future state paths based on past states, past actions, and planned future actions. We also introduced latent action before, a variational auto encoder that models a previous state path based on action segments, and demonstrated how it can be used to perform actions from the same distribution as the training data of a dynamic model. Finally, by experimenting with track optimization and model-based policy optimization, we demonstrated that the resulting method can model complex phenomena such as collisions, is robust to sensory noise and action delays, and learns a significant latent space expressing uncertainty about the future. The most prominent direction for future work we want to investigate is the data acquisition process. In our experiments, correlated random actions led us to consider sufficient exploration for the tasks we want to perform."}, {"heading": "Acknowledgements", "text": "The work done in Berkeley was partially supported by an ONR-PECASE prize."}, {"heading": "A. Appendix", "text": "Here we give a more detailed description of the architectures of the models we have introduced in the work. Both the dynamic model and the previous action were trained with Adam and the standard parameters. \u2212 A.1. Dynamic modelThe following figure shows the detailed encoder and decoder architectures for our dynamic models. The encoder uses 1D convolutions (over time) and the ReLU activation function. The decoder is authoregressive by using dilatative causal 1D convolutions and the gated activation function described in Section 3.1. The layer sizes correspond to the model we have trained for the basic Reacher environment. We used the same encoder architecture for the obstacles and pushing environments. The decoder for these environments had 64 channels in all layers and had an additional 1 \u00d7 1 convolution with 128 channels before final design."}], "references": [{"title": "Learning first-order markov models for control", "author": ["Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Abbeel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2004}, {"title": "Learning to poke by poking: Experiential learning of intuitive physics", "author": ["Agrawal", "Pulkit", "Nair", "Ashvin", "Abbeel", "Pieter", "Malik", "Jitendra", "Levine", "Sergey"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Agrawal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2016}, {"title": "What happened at the darpa robotics challenge, and why. submitted to the DRC", "author": ["Atkeson", "Christopher G", "BPW Babu", "N Banerjee", "D Berenson", "CP Bove", "X Cui", "M DeDonato", "R Du", "S Feng", "P Franklin"], "venue": "Finals Special Issue of the Journal of Field Robotics,", "citeRegEx": "Atkeson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Atkeson et al\\.", "year": 2016}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio", "Samy", "Vinyals", "Oriol", "Jaitly", "Navdeep", "Shazeer", "Noam"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Approximate real-time optimal control based on sparse gaussian process models", "author": ["Boedecker", "Joschka", "Springenberg", "Jost Tobias", "Wlfing", "Jan", "Riedmiller", "Martin"], "venue": "In Adaptive Dynamic Programming and Reinforcement Learning (ADPRL),", "citeRegEx": "Boedecker et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Boedecker et al\\.", "year": 2014}, {"title": "Variational lossy autoencoder", "author": ["Chen", "Xi", "Kingma", "Diederik P", "Salimans", "Tim", "Duan", "Yan", "Dhariwal", "Prafulla", "Schulman", "John", "Sutskever", "Ilya", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1611.02731,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Pilco: A model-based and data-efficient approach to policy search", "author": ["Deisenroth", "Marc Peter", "Rasmussen", "Carl Edward"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Deisenroth et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2011}, {"title": "Deep visual foresight for planning robot motion", "author": ["Finn", "Chelsea", "Levine", "Sergey"], "venue": "arXiv preprint arXiv:1610.00696,", "citeRegEx": "Finn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2016}, {"title": "Deep spatial autoencoders for visuomotor learning", "author": ["Finn", "Chelsea", "Tan", "Xin Yu", "Duan", "Yan", "Darrell", "Trevor", "Levine", "Sergey", "Abbeel", "Pieter"], "venue": "In International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Finn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2016}, {"title": "Learning visual predictive models of physics for playing billiards", "author": ["Fragkiadaki", "Katerina", "Agrawal", "Pulkit", "Levine", "Sergey", "Malik", "Jitendra"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Fragkiadaki et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fragkiadaki et al\\.", "year": 2015}, {"title": "One-shot learning of manipulation skills with online dynamics adaptation and neural network priors", "author": ["Fu", "Justin", "Levine", "Sergey", "Abbeel", "Pieter"], "venue": "In International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "Fu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fu et al\\.", "year": 2016}, {"title": "Learning continuous control policies by stochastic value gradients", "author": ["Heess", "Nicolas", "Wayne", "Gregory", "Silver", "David", "Lillicrap", "Tim", "Erez", "Tom", "Tassa", "Yuval"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Heess et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2015}, {"title": "Composing graphical models with neural networks for structured representations and fast inference", "author": ["Johnson", "Matthew", "Duvenaud", "David K", "Wiltschko", "Alex", "Adams", "Ryan P", "Datta", "Sandeep R"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Gp-bayesfilters: Bayesian filtering using gaussian process prediction and observation models", "author": ["Ko", "Jonathan", "Fox", "Dieter"], "venue": "Auton. Robots,", "citeRegEx": "Ko et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ko et al\\.", "year": 2009}, {"title": "Fast sparse gaussian process methods: The informative vector machine", "author": ["Lawrence", "Neil", "Seeger", "Matthias", "Herbrich", "Ralf"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lawrence et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lawrence et al\\.", "year": 2003}, {"title": "Deepmpc: Learning deep latent features for model predictive control", "author": ["Lenz", "Ian", "Knepper", "Ross", "Saxena", "Ashutosh"], "venue": "In Robotics: Science and Systems (RSS),", "citeRegEx": "Lenz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lenz et al\\.", "year": 2015}, {"title": "Predictive representations of state", "author": ["Littman", "Michael L", "Sutton", "Richard S", "Singh", "Satinder"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Littman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Littman et al\\.", "year": 2002}, {"title": "Combining model-based policy search with online model learning for control of physical humanoids", "author": ["Mordatch", "Igor", "Mishra", "Nikhil", "Eppner", "Clemens", "Abbeel", "Pieter"], "venue": "In International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Mordatch et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mordatch et al\\.", "year": 2016}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["Oh", "Junhyuk", "Guo", "Xiaoxiao", "Lee", "Honglak", "Lewis", "Richard L", "Singh", "Satinder"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Policy gradient methods for robotics", "author": ["Peters", "Jan", "Schaal", "Stefan"], "venue": "In International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "Peters et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2006}, {"title": "Capture point: A step toward humanoid push recovery", "author": ["Pratt", "Jerry", "Carff", "John", "Drakunov", "Sergey", "Goswami", "Ambarish"], "venue": "In Proceedings of the Sixth IEEE-RAS International Conference on Humanoid Robots (Humanoids", "citeRegEx": "Pratt et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pratt et al\\.", "year": 2006}, {"title": "Deep learning helicopter dynamics models", "author": ["Punjani", "Ali", "Abbeel", "Pieter"], "venue": "In International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Punjani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Punjani et al\\.", "year": 2015}, {"title": "Legged Robots that Balance", "author": ["M.H. Raibert"], "venue": "URL https://books.google.com/books? id=EXRiBnQ37RwC", "citeRegEx": "Raibert,? \\Q1986\\E", "shortCiteRegEx": "Raibert", "year": 1986}, {"title": "Learning low dimensional predictive representations", "author": ["Rosencrantz", "Matthew", "Gordon", "Geoff", "Thrun", "Sebastian"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Rosencrantz et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rosencrantz et al\\.", "year": 2004}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Sutton", "Richard S", "McAllester", "David A", "Singh", "Satinder P", "Mansour", "Yishay"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Sutton", "Richard S", "Precup", "Doina", "Singh", "Satinder"], "venue": "Artificial Intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Wavenet: A generative model for raw audio", "author": ["van den Oord", "A\u00e4ron", "Dieleman", "Sander", "Zen", "Heiga", "Simonyan", "Karen", "Vinyals", "Oriol", "Graves", "Alex", "Kalchbrenner", "Nal", "Senior", "Andrew W", "Kavukcuoglu", "Koray"], "venue": "CoRR, abs/1609.03499,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["van den Oord", "Aaron", "Kalchbrenner", "Nal", "Espeholt", "Lasse", "Vinyals", "Oriol", "Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Improving multi-step prediction of learned time series models", "author": ["Venkatraman", "Arun", "Hebert", "Martial", "Bagnell", "J Andrew"], "venue": "In AAAI,", "citeRegEx": "Venkatraman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venkatraman et al\\.", "year": 2015}, {"title": "Improved learning of dynamics models for control", "author": ["Venkatraman", "Arun", "Capobianco", "Roberto", "Pinto", "Lerrel", "Hebert", "Martial", "Nardi", "Daniele", "Bagnell", "J Andrew"], "venue": "In International Symposium on Experimental Robotics (ISER),", "citeRegEx": "Venkatraman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Venkatraman et al\\.", "year": 2016}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["Vezhnevets", "Alexander (Sasha", "Mnih", "Volodymyr", "Agapiou", "John", "Osindero", "Simon", "Graves", "Alex", "Vinyals", "Oriol", "Kavukcuoglu", "Koray"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Vezhnevets et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vezhnevets et al\\.", "year": 2016}, {"title": "Embed to control: A locally linear latent dynamics model for control from raw images", "author": ["Watter", "Manuel", "Springenberg", "Jost", "Boedecker", "Joschka", "Riedmiller", "Martin"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Watter et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Watter et al\\.", "year": 2015}, {"title": "Gaussian processes for regression", "author": ["Williams", "Christopher KI", "Rasmussen", "Carl Edward"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Williams et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Williams et al\\.", "year": 1996}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Model-Less Feedback Control of Continuum Manipulators in Constrained Environments", "author": ["Yip", "Michael C", "Camarillo", "David B"], "venue": "IEEE Transactions on Robotics,", "citeRegEx": "Yip et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yip et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 19, "context": "Related Work A number of options are available for representation of learned dynamics models, including linear functions (Mordatch et al., 2016; Yip & Camarillo, 2014), Gaussian processes (Boedecker et al.", "startOffset": 121, "endOffset": 167}, {"referenceID": 4, "context": ", 2016; Yip & Camarillo, 2014), Gaussian processes (Boedecker et al., 2014; Ko & Fox, 2009; Deisenroth & Rasmussen, 2011), predictive state representations (PSRs) (Littman et al.", "startOffset": 51, "endOffset": 121}, {"referenceID": 18, "context": ", 2014; Ko & Fox, 2009; Deisenroth & Rasmussen, 2011), predictive state representations (PSRs) (Littman et al., 2002; Rosencrantz et al., 2004), and deep neural networks (Punjani & Abbeel, 2015; Fragkiadaki et al.", "startOffset": 95, "endOffset": 143}, {"referenceID": 25, "context": ", 2014; Ko & Fox, 2009; Deisenroth & Rasmussen, 2011), predictive state representations (PSRs) (Littman et al., 2002; Rosencrantz et al., 2004), and deep neural networks (Punjani & Abbeel, 2015; Fragkiadaki et al.", "startOffset": 95, "endOffset": 143}, {"referenceID": 9, "context": ", 2004), and deep neural networks (Punjani & Abbeel, 2015; Fragkiadaki et al., 2015; Agrawal et al., 2016).", "startOffset": 34, "endOffset": 106}, {"referenceID": 1, "context": ", 2004), and deep neural networks (Punjani & Abbeel, 2015; Fragkiadaki et al., 2015; Agrawal et al., 2016).", "startOffset": 34, "endOffset": 106}, {"referenceID": 16, "context": "Gaussian processes (Williams & Rasmussen, 1996) provide uncertainty estimates, but scaling them to large datasets remains a challenge (Shen et al.; Lawrence et al., 2003).", "startOffset": 134, "endOffset": 170}, {"referenceID": 10, "context": "An alternative is to learn dynamics models in an online fashion, constantly adapting the model based on an incoming stream of observed states and actions (Fu et al., 2016; Mordatch et al., 2016; Yip & Camarillo, 2014; Lenz et al., 2015).", "startOffset": 154, "endOffset": 236}, {"referenceID": 19, "context": "An alternative is to learn dynamics models in an online fashion, constantly adapting the model based on an incoming stream of observed states and actions (Fu et al., 2016; Mordatch et al., 2016; Yip & Camarillo, 2014; Lenz et al., 2015).", "startOffset": 154, "endOffset": 236}, {"referenceID": 17, "context": "An alternative is to learn dynamics models in an online fashion, constantly adapting the model based on an incoming stream of observed states and actions (Fu et al., 2016; Mordatch et al., 2016; Yip & Camarillo, 2014; Lenz et al., 2015).", "startOffset": 154, "endOffset": 236}, {"referenceID": 20, "context": "Existing methods for video prediction (Finn & Levine, 2016; Oh et al., 2015) look at a history of previous states and actions to predict the next frame; we take this a step further by modeling a distribution over an entire segment of future states that is also conditioned on future actions.", "startOffset": 38, "endOffset": 76}, {"referenceID": 32, "context": ", 1999b) or sequencing of sub-plans (Vezhnevets et al., 2016).", "startOffset": 36, "endOffset": 61}, {"referenceID": 24, "context": "For example, there are effective and simple manually-designed control laws (Raibert, 1986), (Pratt et al.", "startOffset": 75, "endOffset": 90}, {"referenceID": 22, "context": "For example, there are effective and simple manually-designed control laws (Raibert, 1986), (Pratt et al., 2006) that formulate optimal actions as a function of the entire future trajectory rather than a single future state.", "startOffset": 92, "endOffset": 112}, {"referenceID": 1, "context": ", 2015; Agrawal et al., 2016). Linear functions are efficient to evaluate and solve controls for, but have limited expressive power. Gaussian processes (Williams & Rasmussen, 1996) provide uncertainty estimates, but scaling them to large datasets remains a challenge (Shen et al.; Lawrence et al., 2003). PSRs and variants make multi-step predictions, but suffer from the same scalability challenges as Gaussian processes. Our method combines the expressiveness and scalability of neural networks with the ability to provide sampling and uncertainty estimates, modeling entire segments to improve stability and robustness. An alternative is to learn dynamics models in an online fashion, constantly adapting the model based on an incoming stream of observed states and actions (Fu et al., 2016; Mordatch et al., 2016; Yip & Camarillo, 2014; Lenz et al., 2015). However, such approaches are slow to adapt to rapidly-changing dynamics modes (such as those arising when making or breaking contact) and may be problematic when applied on robots performing rapid motions. Several approaches exist to improve the stability of models that make sequential predictions. Abbeel & Ng (2004) and Venkatraman et al.", "startOffset": 8, "endOffset": 1180}, {"referenceID": 1, "context": ", 2015; Agrawal et al., 2016). Linear functions are efficient to evaluate and solve controls for, but have limited expressive power. Gaussian processes (Williams & Rasmussen, 1996) provide uncertainty estimates, but scaling them to large datasets remains a challenge (Shen et al.; Lawrence et al., 2003). PSRs and variants make multi-step predictions, but suffer from the same scalability challenges as Gaussian processes. Our method combines the expressiveness and scalability of neural networks with the ability to provide sampling and uncertainty estimates, modeling entire segments to improve stability and robustness. An alternative is to learn dynamics models in an online fashion, constantly adapting the model based on an incoming stream of observed states and actions (Fu et al., 2016; Mordatch et al., 2016; Yip & Camarillo, 2014; Lenz et al., 2015). However, such approaches are slow to adapt to rapidly-changing dynamics modes (such as those arising when making or breaking contact) and may be problematic when applied on robots performing rapid motions. Several approaches exist to improve the stability of models that make sequential predictions. Abbeel & Ng (2004) and Venkatraman et al. (2015) consider alternative loss functions that improve robustness over long prediction horizons.", "startOffset": 8, "endOffset": 1210}, {"referenceID": 1, "context": ", 2015; Agrawal et al., 2016). Linear functions are efficient to evaluate and solve controls for, but have limited expressive power. Gaussian processes (Williams & Rasmussen, 1996) provide uncertainty estimates, but scaling them to large datasets remains a challenge (Shen et al.; Lawrence et al., 2003). PSRs and variants make multi-step predictions, but suffer from the same scalability challenges as Gaussian processes. Our method combines the expressiveness and scalability of neural networks with the ability to provide sampling and uncertainty estimates, modeling entire segments to improve stability and robustness. An alternative is to learn dynamics models in an online fashion, constantly adapting the model based on an incoming stream of observed states and actions (Fu et al., 2016; Mordatch et al., 2016; Yip & Camarillo, 2014; Lenz et al., 2015). However, such approaches are slow to adapt to rapidly-changing dynamics modes (such as those arising when making or breaking contact) and may be problematic when applied on robots performing rapid motions. Several approaches exist to improve the stability of models that make sequential predictions. Abbeel & Ng (2004) and Venkatraman et al. (2015) consider alternative loss functions that improve robustness over long prediction horizons. Bengio et al. (2015) and Venkatraman et al.", "startOffset": 8, "endOffset": 1322}, {"referenceID": 1, "context": ", 2015; Agrawal et al., 2016). Linear functions are efficient to evaluate and solve controls for, but have limited expressive power. Gaussian processes (Williams & Rasmussen, 1996) provide uncertainty estimates, but scaling them to large datasets remains a challenge (Shen et al.; Lawrence et al., 2003). PSRs and variants make multi-step predictions, but suffer from the same scalability challenges as Gaussian processes. Our method combines the expressiveness and scalability of neural networks with the ability to provide sampling and uncertainty estimates, modeling entire segments to improve stability and robustness. An alternative is to learn dynamics models in an online fashion, constantly adapting the model based on an incoming stream of observed states and actions (Fu et al., 2016; Mordatch et al., 2016; Yip & Camarillo, 2014; Lenz et al., 2015). However, such approaches are slow to adapt to rapidly-changing dynamics modes (such as those arising when making or breaking contact) and may be problematic when applied on robots performing rapid motions. Several approaches exist to improve the stability of models that make sequential predictions. Abbeel & Ng (2004) and Venkatraman et al. (2015) consider alternative loss functions that improve robustness over long prediction horizons. Bengio et al. (2015) and Venkatraman et al. (2016) also use simple curricula for a similar effect.", "startOffset": 8, "endOffset": 1352}, {"referenceID": 1, "context": ", 2015; Agrawal et al., 2016). Linear functions are efficient to evaluate and solve controls for, but have limited expressive power. Gaussian processes (Williams & Rasmussen, 1996) provide uncertainty estimates, but scaling them to large datasets remains a challenge (Shen et al.; Lawrence et al., 2003). PSRs and variants make multi-step predictions, but suffer from the same scalability challenges as Gaussian processes. Our method combines the expressiveness and scalability of neural networks with the ability to provide sampling and uncertainty estimates, modeling entire segments to improve stability and robustness. An alternative is to learn dynamics models in an online fashion, constantly adapting the model based on an incoming stream of observed states and actions (Fu et al., 2016; Mordatch et al., 2016; Yip & Camarillo, 2014; Lenz et al., 2015). However, such approaches are slow to adapt to rapidly-changing dynamics modes (such as those arising when making or breaking contact) and may be problematic when applied on robots performing rapid motions. Several approaches exist to improve the stability of models that make sequential predictions. Abbeel & Ng (2004) and Venkatraman et al. (2015) consider alternative loss functions that improve robustness over long prediction horizons. Bengio et al. (2015) and Venkatraman et al. (2016) also use simple curricula for a similar effect. While they all consider multi-step prediction losses, they only do so in the context of training models that are intrinsically one-step. Existing methods for video prediction (Finn & Levine, 2016; Oh et al., 2015) look at a history of previous states and actions to predict the next frame; we take this a step further by modeling a distribution over an entire segment of future states that is also conditioned on future actions. In this work, we focus on demonstrating the benefits of a probabilistic segment-based approach; these methods could easily be incorporated with ours to learn dynamics from images, but we leave this to future work. Watter et al. (2015) and Johnson et al.", "startOffset": 8, "endOffset": 2064}, {"referenceID": 1, "context": ", 2015; Agrawal et al., 2016). Linear functions are efficient to evaluate and solve controls for, but have limited expressive power. Gaussian processes (Williams & Rasmussen, 1996) provide uncertainty estimates, but scaling them to large datasets remains a challenge (Shen et al.; Lawrence et al., 2003). PSRs and variants make multi-step predictions, but suffer from the same scalability challenges as Gaussian processes. Our method combines the expressiveness and scalability of neural networks with the ability to provide sampling and uncertainty estimates, modeling entire segments to improve stability and robustness. An alternative is to learn dynamics models in an online fashion, constantly adapting the model based on an incoming stream of observed states and actions (Fu et al., 2016; Mordatch et al., 2016; Yip & Camarillo, 2014; Lenz et al., 2015). However, such approaches are slow to adapt to rapidly-changing dynamics modes (such as those arising when making or breaking contact) and may be problematic when applied on robots performing rapid motions. Several approaches exist to improve the stability of models that make sequential predictions. Abbeel & Ng (2004) and Venkatraman et al. (2015) consider alternative loss functions that improve robustness over long prediction horizons. Bengio et al. (2015) and Venkatraman et al. (2016) also use simple curricula for a similar effect. While they all consider multi-step prediction losses, they only do so in the context of training models that are intrinsically one-step. Existing methods for video prediction (Finn & Levine, 2016; Oh et al., 2015) look at a history of previous states and actions to predict the next frame; we take this a step further by modeling a distribution over an entire segment of future states that is also conditioned on future actions. In this work, we focus on demonstrating the benefits of a probabilistic segment-based approach; these methods could easily be incorporated with ours to learn dynamics from images, but we leave this to future work. Watter et al. (2015) and Johnson et al. (2016) use variational autoencoders to learn a low-dimensional latent-space representation of image observations.", "startOffset": 8, "endOffset": 2090}, {"referenceID": 1, "context": ", 2015; Agrawal et al., 2016). Linear functions are efficient to evaluate and solve controls for, but have limited expressive power. Gaussian processes (Williams & Rasmussen, 1996) provide uncertainty estimates, but scaling them to large datasets remains a challenge (Shen et al.; Lawrence et al., 2003). PSRs and variants make multi-step predictions, but suffer from the same scalability challenges as Gaussian processes. Our method combines the expressiveness and scalability of neural networks with the ability to provide sampling and uncertainty estimates, modeling entire segments to improve stability and robustness. An alternative is to learn dynamics models in an online fashion, constantly adapting the model based on an incoming stream of observed states and actions (Fu et al., 2016; Mordatch et al., 2016; Yip & Camarillo, 2014; Lenz et al., 2015). However, such approaches are slow to adapt to rapidly-changing dynamics modes (such as those arising when making or breaking contact) and may be problematic when applied on robots performing rapid motions. Several approaches exist to improve the stability of models that make sequential predictions. Abbeel & Ng (2004) and Venkatraman et al. (2015) consider alternative loss functions that improve robustness over long prediction horizons. Bengio et al. (2015) and Venkatraman et al. (2016) also use simple curricula for a similar effect. While they all consider multi-step prediction losses, they only do so in the context of training models that are intrinsically one-step. Existing methods for video prediction (Finn & Levine, 2016; Oh et al., 2015) look at a history of previous states and actions to predict the next frame; we take this a step further by modeling a distribution over an entire segment of future states that is also conditioned on future actions. In this work, we focus on demonstrating the benefits of a probabilistic segment-based approach; these methods could easily be incorporated with ours to learn dynamics from images, but we leave this to future work. Watter et al. (2015) and Johnson et al. (2016) use variational autoencoders to learn a low-dimensional latent-space representation of image observations. Finn et al. (2016) takes a similar approach, but without the variational aspect.", "startOffset": 8, "endOffset": 2216}, {"referenceID": 11, "context": "Heess et al. (2015) use a model-based approach where a one-step dynamics model is learned jointly with a policy in an online manner.", "startOffset": 0, "endOffset": 20}, {"referenceID": 30, "context": ", Venkatraman et al. (2015); Abbeel & Ng (2004)).", "startOffset": 2, "endOffset": 28}, {"referenceID": 30, "context": ", Venkatraman et al. (2015); Abbeel & Ng (2004)).", "startOffset": 2, "endOffset": 48}, {"referenceID": 2, "context": "These challenges commonly arise in realworld robotics applications (Atkeson et al., 2016), and so it is important to be able to learn a useful dynamics model in either setting.", "startOffset": 67, "endOffset": 89}, {"referenceID": 5, "context": "Alternatively, per the argument made by Chen et al. (2016), it is also conceivable that the decoder would ignore the latent code entirely, because the segments X\u2212, U\u2212, U provide better information than Z about X.", "startOffset": 40, "endOffset": 59}], "year": 2017, "abstractText": "We introduce a method for learning the dynamics of complex nonlinear systems based on deep generative models over temporal segments of states and actions. Unlike dynamics models that operate over individual discrete timesteps, we learn the distribution over future state trajectories conditioned on past state, past action, and planned future action trajectories, as well as a latent prior over action trajectories. Our approach is based on convolutional autoregressive models and variational autoencoders. It makes stable and accurate predictions over long horizons for complex, stochastic systems, effectively expressing uncertainty and modeling the effects of collisions, sensory noise, and action delays. The learned dynamics model and action prior can be used for end-to-end, fully differentiable trajectory optimization and model-based policy optimization, which we use to evaluate the performance and sample-efficiency of our method.", "creator": "LaTeX with hyperref package"}}}