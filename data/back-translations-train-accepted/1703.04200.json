{"id": "1703.04200", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Mar-2017", "title": "Continual Learning Through Synaptic Intelligence", "abstract": "Deep learning has led to remarkable advances when applied to problems where the data distribution does not change over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, and solve a diversity of tasks simultaneously. Furthermore, synapses in biological neurons are not simply real-valued scalars, but possess complex molecular machinery enabling non-trivial learning dynamics. In this study, we take a first step toward bringing this biological complexity into artificial neural networks. We introduce a model of intelligent synapses that accumulate task relevant information over time, and exploit this information to efficiently consolidate memories of old tasks to protect them from being overwritten as new tasks are learned. We apply our framework to learning sequences of related classification problems, and show that it dramatically reduces catastrophic forgetting while maintaining computational efficiency.", "histories": [["v1", "Mon, 13 Mar 2017 00:02:48 GMT  (255kb,D)", "https://arxiv.org/abs/1703.04200v1", null], ["v2", "Mon, 10 Apr 2017 17:54:57 GMT  (259kb,D)", "http://arxiv.org/abs/1703.04200v2", null], ["v3", "Mon, 12 Jun 2017 19:57:42 GMT  (1060kb,D)", "http://arxiv.org/abs/1703.04200v3", "ICML 2017"]], "reviews": [], "SUBJECTS": "cs.LG q-bio.NC stat.ML", "authors": ["friedemann zenke", "ben poole", "surya ganguli"], "accepted": true, "id": "1703.04200"}, "pdf": {"name": "1703.04200.pdf", "metadata": {"source": "META", "title": "Continual Learning Through Synaptic Intelligence", "authors": ["Friedemann Zenke", "Ben Poole", "Surya Ganguli"], "emails": ["<fzenke@stanford.edu>,", "<poole@cs.stanford.edu>."], "sections": [{"heading": "1. Introduction", "text": "In fact, the fact is that most of them are able to move, to move and to move."}, {"heading": "2. Prior work", "text": "These studies can comprehensively translate Xiv: 170 3.04 200v 3 [cs.L G] 12 Jun 2017 into (1) architectural, (2) functional, and (3) structural approaches. Architectural approaches to catastrophic forgetfulness alter the architecture of the network to reduce interference between tasks without changing the objective function.The simplest form of architectural regulation is freezing certain weights in the network so that they remain exactly the same (Razavian et al., 2014).A somewhat more casual approach reduces the learning rate for layers shared with the original task while fine-tuning dramatic changes in parameters to avoid (Donahue et al., 2014; Yosinski et al., 2014).Approaches with other nonlinearities such as ReLU, MaxOut, and local winner-take-all have been shown to improve performance on altered MNIST and tasks."}, {"heading": "3. Synaptic framework", "text": "To address the problem of continuous learning in neural networks, we have tried to build a simple structural regulator that could be calculated online and implemented locally on each synapse. Specifically, we aim to provide each synapse with a local measure of \"importance\" in solving tasks that the network has been trained on in the past. As we work on a new task, we will make changes to important parameters to prevent old memories from being overwritten. To this end, we have developed a class of algorithms that reflect an important metric for past task improvement."}, {"heading": "4. Theoretical analysis of special cases", "text": "In the following we illustrate that our general approach in the case of simple and analytically traceable training scenarios (Q = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D (D = D = D = D = D = D = D = D = D = D = D = D = D - D = D = D = D = D = D = D = D = D = D (D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D"}, {"heading": "5. Experiments", "text": "We examined our approach to continuous learning using the split and permutated MNIST (LeCun et al., 1998; Goodfellow et al., 2013) and the split versions of CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009)."}, {"heading": "5.1. Split MNIST", "text": "The 5 tasks correspond to learning to distinguish between two consecutive digits from 0 to 10. However, we used a small multi-layer perceptron (MLP) with only two hidden layers, each consisting of 256 units, and a standard cross-entropy loss function plus our consolidation cost concept (with attenuation parameters from 1 to 10 \u2212 3). To avoid the complication of crosstalk between digits at the output layer, we used a multi-head approach in which the categorical cross-entropy losses at the output level were calculated."}, {"heading": "5.2. Permuted MNIST benchmark", "text": "We trained an MLP with two hidden layers, each with 2000 ReLUs and Softmax loss. We used Adam with the same parameters as before. However, unlike the split MNIST benchmark, we achieved better results by maintaining the state of the Adam Optimizer between tasks. The final test error was calculated based on data from the MNIST test set. Performance is measured by the network's ability to complete all tasks. To create a basis for comparison, we first trained a network without synaptic consolidation and without synaptic consolidation."}, {"heading": "5.3. Split CIFAR-10/CIFAR-100 benchmark", "text": "To assess whether synaptic consolidation dynamics would prevent catastrophic oblivion in more complex datasets and larger models, we experimented with a continuous learning task based on CIFAR-10 and CIFAR100. Specifically, we trained a CNN (4 convolutionary, followed by 2 dense layers with dropout; see appendix for details). First, we trained the network for 60 epochs on the full CIFAR-10 dataset (task 1) and sequentially on 5 additional tasks, each corresponding to 10 consecutive classes from the CIFAR-100 dataset."}, {"heading": "6. Discussion", "text": "We have shown that the problem of catastrophic forgetfulness, which often occurs in continuous learning scenarios, can be alleviated by allowing individual synapses to estimate their importance in solving past tasks. Then, by punishing changes to the most important synapses, new tasks can be learned with minimal interference to previously learned tasks. However, the regulatory penalty is comparable to that recently introduced by Kirkpatrick et al. (2017). However, our approach calculates the consolidation strength per synapse online and over the entire learning path in the parameter space, whereas the synaptic importance of EWC offline as Fisher information is compressed to a minimum of loss for a given task. Despite this difference, these two approaches yielded a similar performance on the permutated MNIST benchmark due to correlations between the two different synapse synapses."}, {"heading": "Acknowledgements", "text": "The authors thank Subhaneil Lahiri for the helpful discussions. FZ was supported by the SNSF (Swiss National Science Foundation) and the Wellcome Trust. BP was supported by an IGERT Fellowship from Stanford MBC and a Stanford Interdisciplinary Graduate Fellowship. SG was supported by the Burroughs Wellcome, McKnight, Simons and James S. McDonnell Foundations and the Office of Naval Research."}, {"heading": "A. Split CIFAR-10/100 CNN architecture", "text": "For our CIFAR-10 / 100 experiments we used the standard CIFAR-10 CNN from Keras:"}, {"heading": "B. Additional split CIFAR\u201310 experiments", "text": "As an additional experiment, we trained a CNN (4 convolutional, followed by 2 dense layers with dropout; see main text) on the split CIFAR-10 benchmark. We used the same multi-head setup as in the case of the split MNIST with Adam (\u03b7 = 1 \u00b7 10 \u2212 3, \u03b21 = 0.9, \u03b22 = 0.999, minibatch size 256). First, we trained the network for 60 epochs on the first 5 categories (task A), at which time the training accuracy was close to 1. Then, the optimizer was reset and the network trained for another 60 epochs on the remaining 5 categories (task B). We performed identical experiments for both the control case (c = 0) and the case where the consolidation was active (c > 0). All experiments were repeated n = 10 times to quantify the uncertainty on the validation."}, {"heading": "C. Comparison of path integral approach to other metrics", "text": "Previous approaches to measuring the sensitivity of parameters in a network have focused primarily on local metrics related to the curvature of the objective function in the final parameters (Martens et al., 2016). Hessian is a possible metric, but can be negative, and calculating even the diagonals adds more effort than standard back-propagation (Martens et al., 2012). An alternative choice is the Fisher information (see, for example, Kirk-patrick et al. (2017)): F = Ex-D, y-PTB (y | x) [(\u2202 log pTB (y | x)) 20s (\u2202 log pTB (y | x))).While the Fisher information has a number of desirable properties (Pascanu & Bengio, 2013), it requires calculating gradients using labels sampled from the model distribution instead of the data distribution, thus requiring at least an additional backup pass to propagation online."}], "references": [{"title": "Computational principles of synaptic memory consolidation", "author": ["Benna", "Marcus K", "Fusi", "Stefano"], "venue": "Nat Neurosci, advance online publication,", "citeRegEx": "Benna et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Benna et al\\.", "year": 2016}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Donahue", "Jeff", "Jia", "Yangqing", "Vinyals", "Oriol", "Hoffman", "Judy", "Zhang", "Ning", "Tzeng", "Eric", "Darrell", "Trevor"], "venue": "In International Conference in Machine Learning (ICML),", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Visual Pattern Recognition", "author": ["Fukushima", "Kunihiko", "Miyake", "Sei"], "venue": "In Competition and Cooperation in Neural Nets,", "citeRegEx": "Fukushima et al\\.,? \\Q1982\\E", "shortCiteRegEx": "Fukushima et al\\.", "year": 1982}, {"title": "Cascade models of synaptically stored memories", "author": ["Fusi", "Stefano", "Drew", "Patrick J", "Abbott", "Larry F"], "venue": null, "citeRegEx": "Fusi et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Fusi et al\\.", "year": 2005}, {"title": "An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks", "author": ["Goodfellow", "Ian J", "Mirza", "Mehdi", "Xiao", "Da", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Distilling the knowledge in a neural network", "author": ["Hinton", "Geoffrey", "Vinyals", "Oriol", "Dean", "Jeff"], "venue": "NIPS Deep Learning and Representation Learning Workshop,", "citeRegEx": "Hinton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2014}, {"title": "Less-forgetting Learning in Deep Neural Networks", "author": ["Jung", "Heechul", "Ju", "Jeongwoo", "Minju", "Kim", "Junmo"], "venue": "[cs],", "citeRegEx": "Jung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jung et al\\.", "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "[cs],", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "A memory frontier for complex synapses", "author": ["Lahiri", "Subhaneil", "Ganguli", "Surya"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lahiri et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lahiri et al\\.", "year": 2013}, {"title": "The MNIST database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher JC"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning without forgetting", "author": ["Li", "Zhizhong", "Hoiem", "Derek"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Second-order optimization for neural networks", "author": ["Martens", "James"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "Martens and James.,? \\Q2016\\E", "shortCiteRegEx": "Martens and James.", "year": 2016}, {"title": "Estimating the hessian by back-propagating curvature", "author": ["Martens", "James", "Sutskever", "Ilya", "Swersky", "Kevin"], "venue": "arXiv preprint arXiv:1206.6464,", "citeRegEx": "Martens et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Martens et al\\.", "year": 2012}, {"title": "StateDependent Heterogeneity in Synaptic Depression between", "author": ["Montgomery", "Johanna M", "Madison", "Daniel V"], "venue": "Pyramidal Cell Pairs. Neuron,", "citeRegEx": "Montgomery et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Montgomery et al\\.", "year": 2002}, {"title": "Revisiting natural gradient for deep networks", "author": ["Pascanu", "Razvan", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1301.3584,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Cnn features off-theshelf: an astounding baseline for recognition", "author": ["Razavian", "Ali Sharif", "Azizpour", "Hossein", "Sullivan", "Josephine", "Carlsson", "Stefan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,", "citeRegEx": "Razavian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Razavian et al\\.", "year": 2014}, {"title": "Making memories last: the synaptic tagging and capture hypothesis", "author": ["Redondo", "Roger L", "Morris", "Richard G. M"], "venue": "Nat Rev Neurosci,", "citeRegEx": "Redondo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Redondo et al\\.", "year": 2011}, {"title": "The perceptron: A probabilistic model for information storage and organization in the brain", "author": ["Rosenblatt", "Frank"], "venue": "Psychological review,", "citeRegEx": "Rosenblatt and Frank.,? \\Q1958\\E", "shortCiteRegEx": "Rosenblatt and Frank.", "year": 1958}, {"title": "Progressive Neural Networks", "author": ["Rusu", "Andrei A", "Rabinowitz", "Neil C", "Desjardins", "Guillaume", "Soyer", "Hubert", "Kirkpatrick", "James", "Kavukcuoglu", "Koray", "Pascanu", "Razvan", "Hadsell", "Raia"], "venue": "[cs],", "citeRegEx": "Rusu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Split CIFAR-10/100 CNN architecture For our CIFAR-10/100 experiments, we used the default CIFAR-10 CNN from Keras: Operation Kernel Stride Filters Dropout Nonlin", "author": ["A. 10.1523/JNEUROSCI"], "venue": null, "citeRegEx": "10.1523.JNEUROSCI.3989.14.2015.,? \\Q2015\\E", "shortCiteRegEx": "10.1523.JNEUROSCI.3989.14.2015.", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "To accommodate changes in the data distribution, ANNs typically have to be retrained on the entire dataset to avoid overfitting and catastrophic forgetting (Choy et al., 2006; Goodfellow et al., 2013).", "startOffset": 156, "endOffset": 200}, {"referenceID": 3, "context": "While this complexity has been surmised to serve memory consolidation (Fusi et al., 2005; Lahiri & Ganguli, 2013; Zenke et al., 2015; Ziegler et al., 2015; Benna & Fusi, 2016), few studies have illustrated how it benefits learning in ANNs.", "startOffset": 70, "endOffset": 175}, {"referenceID": 16, "context": "The simplest form of architectural regularization is freezing certain weights in the network so that they stay exactly the same (Razavian et al., 2014).", "startOffset": 128, "endOffset": 151}, {"referenceID": 1, "context": "A slightly more relaxed approach reduces the learning rate for layers shared with the original task while fine-tuning to avoid dramatic changes in the parameters (Donahue et al., 2014; Yosinski et al., 2014).", "startOffset": 162, "endOffset": 207}, {"referenceID": 4, "context": "Approaches using different nonlinearities like ReLU, MaxOut, and local winner-take-all have been shown to improve performance on permuted MNIST and sentiment analysis tasks (Srivastava et al., 2013; Goodfellow et al., 2013).", "startOffset": 173, "endOffset": 223}, {"referenceID": 4, "context": "Moreover, injecting noise to sparsify gradients using dropout also improves performance (Goodfellow et al., 2013).", "startOffset": 88, "endOffset": 113}, {"referenceID": 1, "context": "A slightly more relaxed approach reduces the learning rate for layers shared with the original task while fine-tuning to avoid dramatic changes in the parameters (Donahue et al., 2014; Yosinski et al., 2014). Approaches using different nonlinearities like ReLU, MaxOut, and local winner-take-all have been shown to improve performance on permuted MNIST and sentiment analysis tasks (Srivastava et al., 2013; Goodfellow et al., 2013). Moreover, injecting noise to sparsify gradients using dropout also improves performance (Goodfellow et al., 2013). Recent work from Rusu et al. (2016) proposed more dramatic architectural changes where the entire network for the previous task is copied and augmented with new features while solving a new task.", "startOffset": 163, "endOffset": 585}, {"referenceID": 5, "context": "In Li & Hoiem (2016), the predictions of the previous task\u2019s network and the current network are encouraged to be similar when applied to data from the new task by using a form of knowledge distillation (Hinton et al., 2014).", "startOffset": 203, "endOffset": 224}, {"referenceID": 5, "context": "In Li & Hoiem (2016), the predictions of the previous task\u2019s network and the current network are encouraged to be similar when applied to data from the new task by using a form of knowledge distillation (Hinton et al., 2014). Similarly, Jung et al. (2016) regularize the `2 distance between the final hidden activations instead of the knowledge distillation penalty.", "startOffset": 204, "endOffset": 256}, {"referenceID": 10, "context": "We evaluated our approach for continual learning on the split and permuted MNIST (LeCun et al., 1998; Goodfellow et al., 2013), and split versions of CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009).", "startOffset": 81, "endOffset": 126}, {"referenceID": 4, "context": "We evaluated our approach for continual learning on the split and permuted MNIST (LeCun et al., 1998; Goodfellow et al., 2013), and split versions of CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009).", "startOffset": 81, "endOffset": 126}], "year": 2017, "abstractText": "While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging complex molecular machinery to solve many tasks simultaneously. In this study, we introduce intelligent synapses that bring some of this biological complexity into artificial neural networks. Each synapse accumulates task relevant information over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintaining computational efficiency.", "creator": "LaTeX with hyperref package"}}}