{"id": "1703.04274", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Mar-2017", "title": "Online Learning with Local Permutations and Delayed Feedback", "abstract": "We propose an Online Learning with Local Permutations (OLLP) setting, in which the learner is allowed to slightly permute the \\emph{order} of the loss functions generated by an adversary. On one hand, this models natural situations where the exact order of the learner's responses is not crucial, and on the other hand, might allow better learning and regret performance, by mitigating highly adversarial loss sequences. Also, with random permutations, this can be seen as a setting interpolating between adversarial and stochastic losses. In this paper, we consider the applicability of this setting to convex online learning with delayed feedback, in which the feedback on the prediction made in round $t$ arrives with some delay $\\tau$. With such delayed feedback, the best possible regret bound is well-known to be $O(\\sqrt{\\tau T})$. We prove that by being able to permute losses by a distance of at most $M$ (for $M\\geq \\tau$), the regret can be improved to $O(\\sqrt{T}(1+\\sqrt{\\tau^2/M}))$, using a Mirror-Descent based algorithm which can be applied for both Euclidean and non-Euclidean geometries. We also prove a lower bound, showing that for $M&lt;\\tau/3$, it is impossible to improve the standard $O(\\sqrt{\\tau T})$ regret bound by more than constant factors. Finally, we provide some experiments validating the performance of our algorithm.", "histories": [["v1", "Mon, 13 Mar 2017 07:31:46 GMT  (470kb,D)", "http://arxiv.org/abs/1703.04274v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ohad shamir", "liran szlak"], "accepted": true, "id": "1703.04274"}, "pdf": {"name": "1703.04274.pdf", "metadata": {"source": "CRF", "title": "Online Learning with Local Permutations and Delayed Feedback", "authors": ["Ohad Shamir", "Liran Szlak"], "emails": ["ohad.shamir@weizmann.ac.il", "liran.szlak@weizmann.ac.il"], "sections": [{"heading": null, "text": "Repentance can be improved to O (\u221a T (1 + \u221a \u03c42 / M) by an algorithm based on mirror descent, which can be applied to both Euclidean and non-Euclidean geometries. We also demonstrate a lower limit and show that it is impossible for M < \u03c4 / 3 to improve the default regret O (\u221a \u03c4T), which is bound by more than constant factors. Finally, we provide some experiments to validate the performance of our algorithm."}, {"heading": "1 Introduction", "text": "While it is often possible to develop algorithms with non-trivial regrets, they have to cope with arbitrary loss sequences, which makes them conservative and in some cases inferior to algorithms that are not tailored to deal with worst-case behavior. Indeed, an emerging working timeline looks at how to achieve better online learning in a \"simple\" way, satisfying some additional assumptions. Some examples include losses that are i.e. sampled from some distributions, changes slowly over time, have a consistently powerful predictor over time, have some predictable structures, mix adversarial and stochastic losses, etc. (e.g. Sani et al. [2014], Karnin and Anava [2016], Bubeck and Slivkins [2014], Hazan and Kale] et."}, {"heading": "2 Setting and Notation", "text": "We assume that both W & # 8222; W & # 8220; W & # 8222; W & # 8220; W & # 8222; W & # 8222; W & # 8222; W & # 8220; W & # 8222; W & # 8222; W & # 8222; W & # 8222; W & # 8222; W & # 8222; W & # 8220; W & # 8222; W & # 8222; W & # 8222; W & # 8222; W & # 8222; W & # 8222; W & # 8222; W & # 8222; W & # 8222; W & # 8222; W & # 822; W & # 8211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W & # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211; W # 211 # 211; W # 211; W # 211; W # 211 & # 211; W # 211 & # 211; W # 211; W # 211 # 211; W # 211; W # 211; W # 211 & # 211 & # 211; W # 211; W # 211; W # 211 & # 211; W # 211; W # 211; W # 211 & # 211; W # 211 & # 211; W # 211 & # 211; W # 211; W # 211; W # 211 & # 211 & # 211; W # 211; W # 211;"}, {"heading": "3 Algorithm and Analysis", "text": "Our algorithmic approach is based on the established Online Mirror Descendancy Frame. So we start with a brief recollection of the Online Mirror Descendancy Algorithm (see e.g. Hazan et al. [2016] for more details). Readers familiar with the algorithm are invited to move on to Subdivision 3.1. The Online Mirror Descendancy Algorithm is a generalization of the Online Gradient Descendancy that can handle non-Euclidean geometries. \u2212 The general idea is this: We start with a certain point wt-W where W is our primal space. We map this point onto the dual space using a (strictly convex and continuously differentiable) mirror map, i.e. we perform the gradient actualization in dual space, where W is our primal space."}, {"heading": "3.1 The Delayed Permuted Mirror Descent Algorithm", "text": "Before describing the algorithm, we must point out that we focus on the case where the permutation window parameters M are greater than the delay parameters M < D, then our regret is no better than the O (\u221a) block (see algorithm 1 below and Figure 1 for a graphic illustration). First, the algorithm splits the time horizon T into M consecutive blocks and performs a uniform random permutation on the loss functions within each block. Then, it runs two online algorithms in parallel, and uses the delayed gradients to update two separate predictions - wf and ws, where wf is used for predicting."}, {"heading": "3.2 Analysis", "text": "The repentance analysis of the Delayed Permuted Mirror Descent Algorithm is based on a separate analysis of the two mirror descent sub-algorithms, in which the delay parameters of the two sub-algorithms occur multiplicatively, but do not play a significant role in the repentance of the second sub-algorithms (which take advantage of the stochastic nature of the permutations). To analyze the effect of the delay, we need a bond between the two consistent predictors wt, wt + 1 generated by the sub-algorithms. This depends on the mirror map and the Bregman divergence used for the update, and we currently have no bound attitudes in full generality."}, {"heading": "3.3 Handling Variable Delay Size", "text": "So far, we have discussed a setting where feedback arrives with a fixed delay of size \u03c4. However, in many situations, feedback with a variable delay value \u03c4t could arrive at each iteration t, which could pose some problems. First, feedback might arrive asynchronously, which causes us to update our predictor with gradients of time points further in the past after using newer gradients, making it difficult to analyze the algorithm. A second, algorithmic problem is that in certain iterations, we might get multiple feedback at the same time or no feedback at all, since the delay is of variable size. A simple solution is to use buffering and reduce the problem to a constant delay setting. Specifically, we assume that all delays are limited by a maximum delay size setting. We would like to use a gradient to update our predictor at each iteration (this is mainly for the simplicity of the analysis, with multiple losses in the predictor that could be handy)."}, {"heading": "4 Lower Bound", "text": "In this section we will give a lower limit in the setting in which M < \u03c43 with all feedback delays of exactly Z = A. We will show that in this case the regret is limited, which cannot be improved by more than one constant factor over the limit of the opposing online learning problem with a fixed delay of magnitude Z = W. We will show that this regret cannot be improved by more than one constant factor over the limit of the opposing online learning process (and not only by a fixed delay of magnitude Z = 3). However, this remains an open problem. For any (possible randomized) algorithmA with a permutation window of sizeM \u2264 0 there is a choice of linear, 1-lipschitz functions above [\u2212 1] R, so that the expected regret of A after T rounds (referring to algorithm omness), isE [T = 1 ft (wt)."}, {"heading": "5 Experiments", "text": "We look at the hostile setting described in Section 4, in which an opponent selects a sequence of functions so that each \u03c4 function is identical, resulting in blocks of size \u03c4 of identical loss functions, the form ft (wt) = \u03b1t \u00b7 wt, in which \u03b1t is randomly selected in {\u2212 1, + 1} for each block. In all experiments, we use T = 105 rounds, a delay parameter of \u03c4 = 200, determine our step sizes according to the theoretical analysis, and report the mean regret value over 1000 repetitions of the experiments. In our first experiment, we looked at the behavior of our delayed mirror drop algorithm, for window sizes M > \u03c4, which range from 1 to T. In this experiment, we chose the alpha values randomly, while ensuring a gap of 200 between the number of blocks with + 1 values and the number of blocks with \u2212 1 values (this ensures that the optimal w value is a sufficiently strong competitor, otherwise the algorithm is shown to be simple)."}, {"heading": "6 Discussion", "text": "In this paper, we focused on the problem of learning from delayed feedback in the OLLP setting, showing how it is possible to improve regret by allowing local permutations; we also had a lower limit in the situation where the permutation window is significantly smaller than the feedback delay; and we showed that permutations in this case cannot allow for any better regret than the standard, adverse setting; and we provided some experiments demonstrating both the power of the setting and the feasibility of the proposed algorithm. An interesting open question is what minimum permutation size allows for a non-trivial improvement in regret; and whether our upper limit in Theorem 1 is narrow. As suggested by our empirical experiments, it is possible that even small mutations are highly interesting enough in the worst case to partially improve the results."}, {"heading": "Acknowledgements", "text": "OS is supported in part by a Marie Curie CIG Scholarship from the 7th Research Framework Programme, the Intel ICRI-CI Institute and the Israel Science Foundation."}, {"heading": "A Proofs", "text": "We will use the well-known Pythagorean theorems for Bregman divergences, and the \"projection\" divergences, which take into account the projection step in the algorithm. \u00b7 Lemma 1. Pythagorean theorem for Bregman divergences. (u, w) Let v be the projection of a convex setW w.r.t Bregman Divergence40s: v = argminu W40s (u, w). (u, w). (u, w). (u, w). (v, w) Lemma 2. Projection Lemma LetW be a closed convex set and let v be the projection of w ontoW, namely, v = argminx. (u, w)."}], "references": [{"title": "Distributed delayed stochastic optimization", "author": ["Alekh Agarwal", "John C Duchi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Agarwal and Duchi.,? \\Q2011\\E", "shortCiteRegEx": "Agarwal and Duchi.", "year": 2011}, {"title": "The best of both worlds: Stochastic and adversarial bandits", "author": ["S\u00e9bastien Bubeck", "Aleksandrs Slivkins"], "venue": "In COLT,", "citeRegEx": "Bubeck and Slivkins.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Slivkins.", "year": 2012}, {"title": "Online optimization with gradual variations", "author": ["Chao-Kai Chiang", "Tianbao Yang", "Chia-Jung Lee", "Mehrdad Mahdavi", "Chi-Jen Lu", "Rong Jin", "Shenghuo Zhu"], "venue": "In COLT,", "citeRegEx": "Chiang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chiang et al\\.", "year": 2012}, {"title": "Extracting certainty from uncertainty: Regret bounded by variation in costs", "author": ["Elad Hazan", "Satyen Kale"], "venue": "Machine learning,", "citeRegEx": "Hazan and Kale.,? \\Q2010\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2010}, {"title": "Better algorithms for benign bandits", "author": ["Elad Hazan", "Satyen Kale"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hazan and Kale.,? \\Q2011\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2011}, {"title": "Multi-armed bandits: Competing with optimal sequences", "author": ["Zohar S Karnin", "Oren Anava"], "venue": "In NIPS,", "citeRegEx": "Karnin and Anava.,? \\Q2016\\E", "shortCiteRegEx": "Karnin and Anava.", "year": 2016}, {"title": "Slow learners are fast", "author": ["John Langford", "Alexander Smola", "Martin Zinkevich"], "venue": "arXiv preprint arXiv:0911.0491,", "citeRegEx": "Langford et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2009}, {"title": "On-demand, spot, or both: Dynamic resource allocation for executing batch jobs in the cloud", "author": ["Ishai Menache", "Ohad Shamir", "Navendu Jain"], "venue": "In 11th International Conference on Autonomic Computing (ICAC", "citeRegEx": "Menache et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Menache et al\\.", "year": 2014}, {"title": "On-line learning with delayed label feedback", "author": ["Chris Mesterharm"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "Mesterharm.,? \\Q2005\\E", "shortCiteRegEx": "Mesterharm.", "year": 2005}, {"title": "Online learning with adversarial delays", "author": ["Kent Quanrud", "Daniel Khashabi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Quanrud and Khashabi.,? \\Q2015\\E", "shortCiteRegEx": "Quanrud and Khashabi.", "year": 2015}, {"title": "Online learning with predictable sequences", "author": ["Alexander Rakhlin", "Karthik Sridharan"], "venue": "In COLT, pages 993\u20131019,", "citeRegEx": "Rakhlin and Sridharan.,? \\Q2013\\E", "shortCiteRegEx": "Rakhlin and Sridharan.", "year": 2013}, {"title": "Exploiting easy data in online optimization", "author": ["Amir Sani", "Gergely Neu", "Alessandro Lazaric"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sani et al\\.", "year": 2014}, {"title": "One practical algorithm for both stochastic and adversarial bandits", "author": ["Yevgeny Seldin", "Aleksandrs Slivkins"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Seldin and Slivkins.,? \\Q2014\\E", "shortCiteRegEx": "Seldin and Slivkins.", "year": 2014}, {"title": "Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time", "author": ["Daniel A Spielman", "Shang-Hua Teng"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Spielman and Teng.,? \\Q2004\\E", "shortCiteRegEx": "Spielman and Teng.", "year": 2004}, {"title": "Adaptivity and optimism: An improved exponentiated gradient algorithm", "author": ["Jacob Steinhardt", "Percy Liang"], "venue": "In ICML,", "citeRegEx": "Steinhardt and Liang.,? \\Q2014\\E", "shortCiteRegEx": "Steinhardt and Liang.", "year": 2014}, {"title": "On delayed prediction of individual sequences", "author": ["Marcelo J Weinberger", "Erik Ordentlich"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Weinberger and Ordentlich.,? \\Q2002\\E", "shortCiteRegEx": "Weinberger and Ordentlich.", "year": 2002}], "referenceMentions": [{"referenceID": 5, "context": "Sani et al. [2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "[2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al.", "startOffset": 8, "endOffset": 32}, {"referenceID": 1, "context": "[2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al.", "startOffset": 33, "endOffset": 60}, {"referenceID": 1, "context": "[2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al.", "startOffset": 33, "endOffset": 88}, {"referenceID": 1, "context": "[2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al.", "startOffset": 33, "endOffset": 111}, {"referenceID": 1, "context": "[2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al. [2012], Steinhardt and Liang [2014], Hazan and Kale [2011], Rakhlin and Sridharan [2013], Seldin and Slivkins [2014]).", "startOffset": 33, "endOffset": 133}, {"referenceID": 1, "context": "[2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al. [2012], Steinhardt and Liang [2014], Hazan and Kale [2011], Rakhlin and Sridharan [2013], Seldin and Slivkins [2014]).", "startOffset": 33, "endOffset": 162}, {"referenceID": 1, "context": "[2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al. [2012], Steinhardt and Liang [2014], Hazan and Kale [2011], Rakhlin and Sridharan [2013], Seldin and Slivkins [2014]).", "startOffset": 33, "endOffset": 185}, {"referenceID": 1, "context": "[2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al. [2012], Steinhardt and Liang [2014], Hazan and Kale [2011], Rakhlin and Sridharan [2013], Seldin and Slivkins [2014]).", "startOffset": 33, "endOffset": 215}, {"referenceID": 1, "context": "[2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al. [2012], Steinhardt and Liang [2014], Hazan and Kale [2011], Rakhlin and Sridharan [2013], Seldin and Slivkins [2014]).", "startOffset": 33, "endOffset": 243}, {"referenceID": 1, "context": "[2014], Karnin and Anava [2016], Bubeck and Slivkins [2012], Seldin and Slivkins [2014], Hazan and Kale [2010], Chiang et al. [2012], Steinhardt and Liang [2014], Hazan and Kale [2011], Rakhlin and Sridharan [2013], Seldin and Slivkins [2014]). In this paper, we take a related but different direction: Rather than explicitly excluding highly adversarial loss sequences, we consider how slightly perturbing them can mitigate their worst-case behavior, and lead to improved performance. Conceptually, this resembles smoothed analysis Spielman and Teng [2004], in which one considers the worst-case performance of some algorithm, after performing some perturbation to their input.", "startOffset": 33, "endOffset": 558}, {"referenceID": 11, "context": "For convex online learning with delayed feedback, in a standard adversarial setting, it is known that the attainable regret is on the order of O( \u221a \u03c4T ), and this is also the best possible in the worst case Weinberger and Ordentlich [2002], Mesterharm [2005], Langford et al.", "startOffset": 207, "endOffset": 240}, {"referenceID": 6, "context": "For convex online learning with delayed feedback, in a standard adversarial setting, it is known that the attainable regret is on the order of O( \u221a \u03c4T ), and this is also the best possible in the worst case Weinberger and Ordentlich [2002], Mesterharm [2005], Langford et al.", "startOffset": 241, "endOffset": 259}, {"referenceID": 5, "context": "For convex online learning with delayed feedback, in a standard adversarial setting, it is known that the attainable regret is on the order of O( \u221a \u03c4T ), and this is also the best possible in the worst case Weinberger and Ordentlich [2002], Mesterharm [2005], Langford et al. [2009], Joulani et al.", "startOffset": 260, "endOffset": 283}, {"referenceID": 5, "context": "For convex online learning with delayed feedback, in a standard adversarial setting, it is known that the attainable regret is on the order of O( \u221a \u03c4T ), and this is also the best possible in the worst case Weinberger and Ordentlich [2002], Mesterharm [2005], Langford et al. [2009], Joulani et al. [2013], Quanrud and Khashabi [2015].", "startOffset": 260, "endOffset": 306}, {"referenceID": 5, "context": "For convex online learning with delayed feedback, in a standard adversarial setting, it is known that the attainable regret is on the order of O( \u221a \u03c4T ), and this is also the best possible in the worst case Weinberger and Ordentlich [2002], Mesterharm [2005], Langford et al. [2009], Joulani et al. [2013], Quanrud and Khashabi [2015]. On the other hand, in a stochastic setting where the losses are sampled i.", "startOffset": 260, "endOffset": 335}, {"referenceID": 0, "context": "from some distribution, Agarwal and Duchi [2011] show that the attainable regret is much better, on the order of O( \u221a T + \u03c4).", "startOffset": 24, "endOffset": 49}, {"referenceID": 0, "context": "from some distribution, Agarwal and Duchi [2011] show that the attainable regret is much better, on the order of O( \u221a T + \u03c4). This gap between the worst-case adversarial setting, and the milder i.i.d. setting, hints that this problem is a good fit for our OLLP framework. Thus, in this paper, we focus on online learning with feedback delayed up to \u03c4 rounds, in the OLLP framework where the learner is allowed to locally permute the loss functions (up to a distance of M ). First, we devise an algorithm, denoted as Delayed Permuted Mirror Descent, and prove that it achieves an expected regret bound of order O( \u221a T (\u03c42/M + 1)) assuming M \u2265 \u03c4 . As M increases compared to \u03c4 , this regret bound interpolates between the standard adversarial \u221a \u03c4T regret, and a milder \u221a T regret, typical of i.i.d. losses. As its name implies, the algorithm is based on the well-known online mirror descent (OMD) algorithm (see Hazan et al. [2016], Shalev-Shwartz et al.", "startOffset": 24, "endOffset": 930}, {"referenceID": 0, "context": "from some distribution, Agarwal and Duchi [2011] show that the attainable regret is much better, on the order of O( \u221a T + \u03c4). This gap between the worst-case adversarial setting, and the milder i.i.d. setting, hints that this problem is a good fit for our OLLP framework. Thus, in this paper, we focus on online learning with feedback delayed up to \u03c4 rounds, in the OLLP framework where the learner is allowed to locally permute the loss functions (up to a distance of M ). First, we devise an algorithm, denoted as Delayed Permuted Mirror Descent, and prove that it achieves an expected regret bound of order O( \u221a T (\u03c42/M + 1)) assuming M \u2265 \u03c4 . As M increases compared to \u03c4 , this regret bound interpolates between the standard adversarial \u221a \u03c4T regret, and a milder \u221a T regret, typical of i.i.d. losses. As its name implies, the algorithm is based on the well-known online mirror descent (OMD) algorithm (see Hazan et al. [2016], Shalev-Shwartz et al. [2012]), and works in the same generality, involving both Euclidean and non-Euclidean geometries.", "startOffset": 24, "endOffset": 960}, {"referenceID": 0, "context": "is distinct from another delayed feedback scenario sometimes studied in the literature (Agarwal and Duchi [2011], Langford et al.", "startOffset": 88, "endOffset": 113}, {"referenceID": 0, "context": "is distinct from another delayed feedback scenario sometimes studied in the literature (Agarwal and Duchi [2011], Langford et al. [2009]), where rather than receiving ft\u2212\u03c4 the learner only receives a (sub)gradient of ft\u2212\u03c4 at wt\u2212\u03c4 .", "startOffset": 88, "endOffset": 137}, {"referenceID": 6, "context": "The proof sketch for the setting where no permutation is allowed was already provided in Langford et al. [2009], and our contribution is in providing a full formal proof.", "startOffset": 89, "endOffset": 112}, {"referenceID": 7, "context": "and so we get: \u2016w\u2032 \u2212 w\u20162 \u2264 \u2016w2 \u2212 w\u20162 \u2264 \u03b7 \u00b7G We prove a modification of Lemma 2 given in Menache et al. [2014] in order to bound the distance between two consequent predictions when using the negative entropy mirror map: Lemma 4.", "startOffset": 88, "endOffset": 110}], "year": 2017, "abstractText": "We propose an Online Learning with Local Permutations (OLLP) setting, in which the learner is allowed to slightly permute the order of the loss functions generated by an adversary. On one hand, this models natural situations where the exact order of the learner\u2019s responses is not crucial, and on the other hand, might allow better learning and regret performance, by mitigating highly adversarial loss sequences. Also, with random permutations, this can be seen as a setting interpolating between adversarial and stochastic losses. In this paper, we consider the applicability of this setting to convex online learning with delayed feedback, in which the feedback on the prediction made in round t arrives with some delay \u03c4 . With such delayed feedback, the best possible regret bound is well-known to be O( \u221a \u03c4T ). We prove that by being able to permute losses by a distance of at most M (for M \u2265 \u03c4 ), the regret can be improved to O( \u221a T (1 + \u221a \u03c42/M)), using a Mirror-Descent based algorithm which can be applied for both Euclidean and non-Euclidean geometries. We also prove a lower bound, showing that for M < \u03c4/3, it is impossible to improve the standard O( \u221a \u03c4T ) regret bound by more than constant factors. Finally, we provide some experiments validating the performance of our algorithm.", "creator": "LaTeX with hyperref package"}}}