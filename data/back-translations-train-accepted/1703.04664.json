{"id": "1703.04664", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Mar-2017", "title": "Optimal Densification for Fast and Accurate Minwise Hashing", "abstract": "Minwise hashing is a fundamental and one of the most successful hashing algorithm in the literature. Recent advances based on the idea of densification~\\cite{Proc:OneHashLSH_ICML14,Proc:Shrivastava_UAI14} have shown that it is possible to compute $k$ minwise hashes, of a vector with $d$ nonzeros, in mere $(d + k)$ computations, a significant improvement over the classical $O(dk)$. These advances have led to an algorithmic improvement in the query complexity of traditional indexing algorithms based on minwise hashing. Unfortunately, the variance of the current densification techniques is unnecessarily high, which leads to significantly poor accuracy compared to vanilla minwise hashing, especially when the data is sparse. In this paper, we provide a novel densification scheme which relies on carefully tailored 2-universal hashes. We show that the proposed scheme is variance-optimal, and without losing the runtime efficiency, it is significantly more accurate than existing densification techniques. As a result, we obtain a significantly efficient hashing scheme which has the same variance and collision probability as minwise hashing. Experimental evaluations on real sparse and high-dimensional datasets validate our claims. We believe that given the significant advantages, our method will replace minwise hashing implementations in practice.", "histories": [["v1", "Tue, 14 Mar 2017 18:49:57 GMT  (734kb,D)", "http://arxiv.org/abs/1703.04664v1", "Fast Minwise Hashing"]], "COMMENTS": "Fast Minwise Hashing", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["anshumali shrivastava"], "accepted": true, "id": "1703.04664"}, "pdf": {"name": "1703.04664.pdf", "metadata": {"source": "META", "title": "Optimal Densification for Fast and Accurate Minwise Hashing", "authors": ["Anshumali Shrivastava"], "emails": ["ANSHUMALI@RICE.EDU"], "sections": [{"heading": "1. Introduction and Motivation", "text": "Recent years have seen a dramatic increase in the dimensionality of modern datasets. (Weinberger et al., 2009) show dataset with 16 trillion (1013) unique features. Many studies have shown that the accuracy of the models climbs only slowly with exponential increase in dimensionality. (Large dictionary based on the representation of images, language and text are very popular (Broder, 1997; Fetterly et al., 2003). Representation of genomic sequences with features consisting of 32-contiguous characters (or higher) (Ondov et al., 2016) results in around 432 = 264 dimensions. To deal with overwhelming dimensionality, there is an increased emphasis on the use of hashing algorithms, such as Mindov et al. (Ondov et al., 2016) results in around 432 = 264 dimensions."}, {"heading": "2. Important Notations and Concepts", "text": "Equation 2 (i.e. the LSH property) results in an estimator of the Jacard similarity R using k-hashes, defined by: R-value = 1k-value i = 1 (hi (S1) = hi (S2). (4) Here 1 is the indicator function. In the paper, we call variance the variance of the above estimator. Notations such as V ar (h +) mean the variance of the above estimator when the h + is used as a hash function. [k] denotes the set of integers {1, 2,..., k}. n denotes the number of points (samples) in the dataset. D is used for dimensionality. We use min {S} to denote the minimum element of the set S. A permutation: A permutation between two integers is another set S (S), where x-value S, if and only if a (x) hash & ash is the smallest element of the set S."}, {"heading": "3. Background: Fast Minwise Hashing via Densification", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. 2-Universal Hashing", "text": "Definitions: A randomized function huniv: [l] \u2192 [k] is 2-universal if for all, i, j [l] with i 6 = j, we have the following property for each z1, z2 [k] Pr (huniv (i) = z1 and huniv (j) = z2) = 1k2 (5) (Carter & Wegman, 1977) showed that the easiest way to create a 2-universal hash scheme is to select a prime p \u2265 k, two random numbers a, b and computehuniv (x) = (ax + b) mod p) mod k."}, {"heading": "3.2. One Permutation Hashing and Empty Bins", "text": "It was shown in (Li et al., 2012; Dahlgaard et al., 2015) that instead of calculating the global minimum in Equation 2, i.e., h (S) = min (\u03c0 (S)), an efficient way to generate empty k-sketches, with a permutation, is to first denote the range of empty space of \u03c0, i.e. OPH, in k-disjoint and equal partitions, followed by calculating minimum in each bin (or partition).Let me denote the ith partition of the range of \u03c0, i.e. formally, the ith-a permutation hash (OPH) of a setS defines ashOPHi (S) = {min {\u03c0 (S)."}, {"heading": "3.3. The Idea of Densification", "text": "In (Shrivastava & Li, 2014a), the authors proposed a \"compression\" or rededication of values to empty containers by reusing the information in the non-empty containers to correct the bias of permutation hashing. The general procedure is fairly simple; each empty container borrows the values of the non-empty containers in the direction of the circular right (or left) 1. See Figure 1 for an illustration. Since the positions of empty and non-empty containers were random, compression (or rededication) was shown to correspond to stochastic re-selection of a hash from a set of existing informative (non-empty) hashes. This type of rededication restores the LSH property and the collision probability for two hashes, after rededication, is exactly the same as compression of mineral hashes. Compression generates mutations with the required mutation of the LSH and the only one Li-improvement each."}, {"heading": "3.4. Lack of Randomness in Densification", "text": "It was pointed out in (Shrivastava & Li, 2014c) that the compaction scheme of (Shrivastava & Li, 2014a) had an unnecessarily high variance, in particular, the probability that two empty containers would borrow the information from the same non-empty container was significantly higher, due to poor randomization (load distribution) detrimental to the variance. (Shrivastava & Li, 2014c) showed that the variance is demonstrably improved when more randomness is introduced into the redistribution process through the use of additional k random bits. See Figure 1 for an example to illustrate the method. Once again, the run time of the improved scheme was O (d + k) for calculating k-hashes. This improvement retains the required LSH property, but this time with improved variance. An improved variance resulted in significant savings in abandoning neighbourhood searches in real sparse data sets."}, {"heading": "4. Issues with Current Densification", "text": "Our careful analysis shows that the variance, even with the improved scheme, is still significantly higher. Worse, even in the extreme case, when we take k \u2192 \u221e, the variance converges to a positive constant instead of zero, meaning that even with infinite samples the variance will not be zero. This positive limit increases with the sparseness of the data set further. In particular, we have the following theorem about the boundary variances of existing techniques: theorem 1 Enter two finite sets S1, S2, with A = | S1, S2 | a = | S1, S2, S2 | 0 and | D \u2192, whereby the boundary deviation of the estimators from the condensation and improved condensation, if k = D, S2, S2, with A = S1, S2 | a = | S2, S2, S1, S1, S1, S1 >, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S5, S5, S5, S5, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S4, S2, S4, S2, S2, S4, S2, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4, S4"}, {"heading": "5. Optimal Densification", "text": "We argue that even with improved compression, there is not enough randomness (or load distribution) in the reallocation process, resulting in smaller deviations. To ensure the LSH property, the reallocation should be consistent for each given pair of S1 and S2. Specifically, as mentioned in (Shrivastava & Li, 2014a), for each arbitrary pair of S1 and S2, the reallocation of each given bin i is empty at the same time, i.e. Ei = 1, the collision probability of one of the non-empty bin j at the same time should be imitated with Ej = 0. Arbitrary allocation (or borrowing) of values will not ensure this consistency across all pairs. We would like to point out that the allocation of one of the non-empty bin j with Ej = 0 is not sufficient."}, {"heading": "5.1. Intuition: Load Balancing", "text": "In Figure 2, it should be noted that if there are many contiguous, non-empty containers (containers 2, 3, and 4), then with compaction systems h, all are forced to borrow values from the same non-empty containers (containers 5 in the example). Although there are other informative containers (containers 1, 6, and 7), their information is never used. This local compaction increases the likelihood (p) that two empty containers are bound to the same information, even if there are many other informative, non-empty containers. Adding k random bits improves this to some extent by allowing load sharing between the two ends instead of one (containers 1 and 5 instead of just 5). However, the load distribution is far from optimal, and the location of the information exchange is the main culprit in current compaction systems. Note that poor load sharing does not alter capability c, but the variance is significant."}, {"heading": "5.2. Simple 2-Universal Hashing Doesn\u2019t Help", "text": "To break the locality of information reuse and allow a non-empty recycle bin to consistently borrow information from another far-away recycle bin, it seems natural to use universal hash. The hope is to have a 2-universal hash function (Section 3.1) huniv: [k] \u2192 [k]. If a recycle bin i is empty, instead of borrowing information from neighbors, borrow information from bin huniv (i). The hash function allows consistency between two S1 and S2, thus preserving LSH property. The value of huniv (i) is uniformly distributed, so any recycle bin is equally likely. Therefore, it seems to break the locality at first thought. If huniv (i) is also empty, then we continue using huniv (i) until we reach a non-empty recycle bin whose value we reuse."}, {"heading": "5.3. The Fix: Carefully Tailored 2-Universal Hashing", "text": "Algorithm 1 Optimal Densification input k One Permutation Hashes hOPH [] of S. input huniv (.,.) Initialize h \u0445 [] = 0 for i = 1 to k doif OPH [i] 6 = E then h \u0445 [i] = hOPH [i] else try = 1 next = huniv (i, try) while hOPH [next] 6 = E do try + + + next = huniv (i, try) end while h \u0445 [i] = hOPH [next] end if end for RETURN h \u0445 [] It turns out that there is a way to use universal hashing that does not ensure cycles and optimum load balancing. We describe the complete process in algorithm 1. The key is the use of a 2-universal hashing huniv: [k] \u00d7 N \u2192 [k] which takes two argument: 1) the current bin id that must be reasonably signed and 2) the number of hfail to make a universal hashing if huniversally hashing is."}, {"heading": "5.4. Analysis and Optimality", "text": "We define the final k-hashes generated by the proposed compaction scheme of algorithm 1 using the scheme \"Nemp\" (* for optimisability). Formally, with the optimal compaction \"h\" we have the following: Theorem 2Pr (h) = h \"(S2) = | S1\" S2 | | S1 \"S2 (= 12) V ar (h) = Rk + AR k2 + B RR\" k2 \u2212 R2 (13) lim \"k\" \u2192 V \"ar (h) = 0 (14), where Nemp is the number of concurrent empty containers between S1 and S2 and the quantities\" A \"and\" B \"Nemp\" (Nemp \u2212 1) k \"k \u2212 Nemp\" Nemp. Nemp. \"B\" E \"[k \u2212 Nemp) (k \u2212 Nemp) (k \u2212 1) and\" Nemp. \"v.\" (k \u2212 1 is optimal compaction)."}, {"heading": "5.5. Running Time", "text": "We show that the expected runtime of our proposal, including all constants, is very similar to the runtime of the existing compaction schemes. Given the quantity S with | S | = d, we are interested in calculating k hash values. The first step involves calculating k-a permutation hash value (or sketches) that requires only a single pass over the elements of S. This requires max {d, k} \u2264 d + k time. Now, the compaction algorithm 1 requires a loop of size k and within each loop, if the trash is empty, it requires an additional while loop. Let Nemp be the number of empty paper bodies, and therefore k \u2212 Nempk is the probability that the while loop will end in an iteration (the next one is not empty). Therefore, the expected number of iterations that each loop will run is a binomical random variable with the expectation \u2212 kkk \u2212 Nemp."}, {"heading": "6. Evaluations", "text": "To determine this, we will focus on experiments with the following objectives: 1. Verify that our proposed scheme has significantly better accuracy (variance) than the existing compaction schemes. Validate our variance formulas. 2. Quantify empirically the effects of optimal variance in practice. How does this quantification change with similarity and conciseness? Verify that the proposal has an accuracy that is close to vanilla mint hashing. 3. Verify that there is no impact on the runtime of the proposed scheme on existing compaction schemes, and our proposal is significantly faster than vanilla mint hashing. Do you understand how the runtime changes with the change in scarcity and k?"}, {"heading": "6.1. Accuracy", "text": "For Goals 1 and 2, we have selected 9 different word pairs generated from new 20 corpus, with varying degrees of similarity and sparseness. We use the popular term doctor vector representation for each word. The statistics of these word vector pairs are in Table 1. For each word pair, we have generated k hash values using three different schemes: 1) compression h, 2) improved compression h + and the proposed compression h (algorithm 1). Using these hash values, we estimate the Jaccard similarity (equation 4). We plot the mean square error (MSE) with varying numbers of hash values. Since the process is randomized, we repeat the process 5000 times, for each k, and report the average on independent runs. We report all integer values of k in the interval [1, 214].It is worth noting that all three schemes have the Lance, the property H is the default and the MSE is the property saving."}, {"heading": "6.2. Speed", "text": "To calculate runtime, we use three publicly available text datasets: 1) RCV1, 2) URL and 3) News20. The dimensionality and scarcity of these datasets are an excellent representative of the scope and size commonly found in large data processing systems such as Google's SIBYL (Chandra et al., 2010). Statistics of these datasets are in Table 3.We have implemented three methods for calculating hashes: 1) compaction scheme h +, 2) the proposed h method (algorithm 1 and 3) Vanilla Minwise Hashing. The methods were implemented in C + +. Cheap hash functions replaced costly permutations. Clever alternatives to avoid mod operations were used. These tricks ensured that our implementations2 are as efficient as possible. We calculate the wall clock time required to calculate 100, 300 and all three datasets."}, {"heading": "A. Proofs", "text": "The limit variance of the estimators of compaction and improved compaction is when k = D \u00b2 s (A \u2212 1) + (2A \u2212 1) of: lim k \u00b2 V ar (h) = aA [A \u2212 aA (A + 1) > 0 (16) lim k \u2192 compaction V ar (h +) = aA [3 (A \u2212 1) (a \u2212 1) 2 (A + 1) (A \u2212 1) \u2212 a) Proof: When k = D \u2212 A. Substitute this value in the variance formulae of (Shrivastava & Li, 2014c) and take the limit as D = k \u00b2 s, we get the above expression after manipulation."}], "references": [{"title": "The optimality of correlated sampling", "author": ["Bavarian", "Mohammad", "Ghazi", "Badih", "Haramaty", "Elad", "Kamath", "Pritish", "Rivest", "Ronald L", "Sudan", "Madhu"], "venue": "CoRR, abs/1612.01041,", "citeRegEx": "Bavarian et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bavarian et al\\.", "year": 2016}, {"title": "Scaling up all pairs similarity search", "author": ["Bayardo", "Roberto J", "Ma", "Yiming", "Srikant", "Ramakrishnan"], "venue": "In WWW, pp", "citeRegEx": "Bayardo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bayardo et al\\.", "year": 2007}, {"title": "On the resemblance and containment of documents", "author": ["Broder", "Andrei Z"], "venue": "In the Compression and Complexity of Sequences,", "citeRegEx": "Broder and Z.,? \\Q1997\\E", "shortCiteRegEx": "Broder and Z.", "year": 1997}, {"title": "Min-wise independent permutations", "author": ["Broder", "Andrei Z", "Charikar", "Moses", "Frieze", "Alan M", "Mitzenmacher", "Michael"], "venue": "In STOC,", "citeRegEx": "Broder et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Broder et al\\.", "year": 1998}, {"title": "A scalable pattern mining approach to web graph compression with communities", "author": ["Buehrer", "Gregory", "Chellapilla", "Kumar"], "venue": "In WSDM,", "citeRegEx": "Buehrer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Buehrer et al\\.", "year": 2008}, {"title": "Universal classes of hash functions", "author": ["Carter", "J. Lawrence", "Wegman", "Mark N"], "venue": "In STOC, pp", "citeRegEx": "Carter et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Carter et al\\.", "year": 1977}, {"title": "Sibyl: a system for large scale machine learning", "author": ["Chandra", "Tushar", "Ie", "Eugene", "Goldman", "Kenneth", "Llinares", "Tomas Lloret", "McFadden", "Jim", "Pereira", "Fernando", "Redstone", "Joshua", "Shaked", "Tal", "Singer", "Yoram"], "venue": "Technical report,", "citeRegEx": "Chandra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chandra et al\\.", "year": 2010}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["Charikar", "Moses S"], "venue": "In STOC,", "citeRegEx": "Charikar and S.,? \\Q2002\\E", "shortCiteRegEx": "Charikar and S.", "year": 2002}, {"title": "Semantic similarity between search engine queries using temporal correlation", "author": ["Chien", "Steve", "Immorlica", "Nicole"], "venue": "In WWW, pp", "citeRegEx": "Chien et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chien et al\\.", "year": 2005}, {"title": "On compressing social networks", "author": ["Chierichetti", "Flavio", "Kumar", "Ravi", "Lattanzi", "Silvio", "Mitzenmacher", "Michael", "Panconesi", "Alessandro", "Raghavan", "Prabhakar"], "venue": "In KDD,", "citeRegEx": "Chierichetti et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chierichetti et al\\.", "year": 2009}, {"title": "Hashing for statistics over kpartitions", "author": ["Dahlgaard", "S\u00f8ren", "Knudsen", "Mathias B\u00e6k Tejs", "Rotenberg", "Eva", "Thorup", "Mikkel"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Dahlgaard et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dahlgaard et al\\.", "year": 2015}, {"title": "A large-scale study of the evolution of web pages", "author": ["Fetterly", "Dennis", "Manasse", "Mark", "Najork", "Marc", "Wiener", "Janet L"], "venue": "In WWW,", "citeRegEx": "Fetterly et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Fetterly et al\\.", "year": 2003}, {"title": "Finding near-duplicate web pages: a large-scale evaluation of algorithms", "author": ["Henzinger", "Monika Rauch"], "venue": "In SIGIR,", "citeRegEx": "Henzinger and Rauch.,? \\Q2006\\E", "shortCiteRegEx": "Henzinger and Rauch.", "year": 2006}, {"title": "Approximate nearest neighbors: Towards removing the curse of dimensionality", "author": ["Indyk", "Piotr", "Motwani", "Rajeev"], "venue": "In STOC,", "citeRegEx": "Indyk et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Indyk et al\\.", "year": 1998}, {"title": "0-bit consistent weighted sampling", "author": ["Li", "Ping"], "venue": "In KDD,", "citeRegEx": "Li and Ping.,? \\Q2015\\E", "shortCiteRegEx": "Li and Ping.", "year": 2015}, {"title": "Hashing algorithms for largescale learning", "author": ["Li", "Ping", "Shrivastava", "Anshumali", "Moore", "Joshua", "K\u00f6nig", "Arnd Christian"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "One permutation hashing", "author": ["Li", "Ping", "Owen", "Art B", "Zhang", "Cun-Hui"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Less is more: sampling the neighborhood graph makes salsa better and faster", "author": ["Najork", "Marc", "Gollapudi", "Sreenivas", "Panigrahy", "Rina"], "venue": "In WSDM,", "citeRegEx": "Najork et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Najork et al\\.", "year": 2009}, {"title": "Mash: fast genome and metagenome distance estimation using minhash", "author": ["Ondov", "Brian D", "Treangen", "Todd J", "Melsted", "P\u00e1ll", "Mallonee", "Adam B", "Bergman", "Nicholas H", "Koren", "Sergey", "Phillippy", "Adam M"], "venue": "Genome Biology,", "citeRegEx": "Ondov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ondov et al\\.", "year": 2016}, {"title": "Densifying one permutation hashing via rotation for fast near neighbor search", "author": ["Shrivastava", "Anshumali", "Li", "Ping"], "venue": null, "citeRegEx": "Shrivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shrivastava et al\\.", "year": 2014}, {"title": "In defense of minhash over simhash", "author": ["Shrivastava", "Anshumali", "Li", "Ping"], "venue": "In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Shrivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shrivastava et al\\.", "year": 2014}, {"title": "Improved densification of one permutation hashing", "author": ["Shrivastava", "Anshumali", "Li", "Ping"], "venue": null, "citeRegEx": "Shrivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shrivastava et al\\.", "year": 2014}, {"title": "Substituting this value in the variance formulas from (Shrivastava & Li, 2014c) and taking the limit as D = k \u2192 \u221e, we get the above expression after manipulation", "author": ["A. then Nemp = D"], "venue": null, "citeRegEx": "D and \u2212,? \\Q2014\\E", "shortCiteRegEx": "D and \u2212", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "Large dictionary based representation for images, speech, and text are quite popular (Broder, 1997; Fetterly et al., 2003).", "startOffset": 85, "endOffset": 122}, {"referenceID": 18, "context": "Representing genome sequences with features consisting of 32-contiguous characters (or higher) (Ondov et al., 2016) leads to around 4 = 2 dimensions.", "startOffset": 95, "endOffset": 115}, {"referenceID": 3, "context": "Minwise hashing belongs to the Locality Sensitive Hashing (LSH) family (Broder et al., 1998; Charikar, 2002).", "startOffset": 71, "endOffset": 108}, {"referenceID": 1, "context": "Because of this same LSH property, minwise hashing is a popular indexing technique for a variety of large-scale data processing applications, which include duplicate detection (Broder, 1997; Henzinger, 2006), all-pair similarity (Bayardo et al., 2007), temporal correlation (Chien & Immorlica, 2005), graph algorithms (Buehrer & Chellapilla, 2008; Chierichetti et al.", "startOffset": 229, "endOffset": 251}, {"referenceID": 9, "context": ", 2007), temporal correlation (Chien & Immorlica, 2005), graph algorithms (Buehrer & Chellapilla, 2008; Chierichetti et al., 2009; Najork et al., 2009), and more.", "startOffset": 74, "endOffset": 151}, {"referenceID": 17, "context": ", 2007), temporal correlation (Chien & Immorlica, 2005), graph algorithms (Buehrer & Chellapilla, 2008; Chierichetti et al., 2009; Najork et al., 2009), and more.", "startOffset": 74, "endOffset": 151}, {"referenceID": 15, "context": "It was recently shown that the LSH property of minwise hashes can be used to generate kernel features for large-scale learning (Li et al., 2011).", "startOffset": 127, "endOffset": 144}, {"referenceID": 0, "context": "Minwise hashing is known to be theoretical optimal in many scenarios (Bavarian et al., 2016).", "startOffset": 69, "endOffset": 92}, {"referenceID": 16, "context": "Other Related Fast Sketches are not LSH: Two notable techniques for estimating Jaccard Similarity are: 1) bottom-k sketches and 2) one permutation hashing (Li et al., 2012).", "startOffset": 155, "endOffset": 172}, {"referenceID": 16, "context": "It was shown in (Li et al., 2012; Dahlgaard et al., 2015) that instead of computing the global minimum in Equation 2, i.", "startOffset": 16, "endOffset": 57}, {"referenceID": 10, "context": "It was shown in (Li et al., 2012; Dahlgaard et al., 2015) that instead of computing the global minimum in Equation 2, i.", "startOffset": 16, "endOffset": 57}, {"referenceID": 16, "context": "Using the formula for Pr(Nemp = i) from (Li et al., 2012), we can precisely compute the theoretical variance.", "startOffset": 40, "endOffset": 57}, {"referenceID": 16, "context": "Number of Empty Bins per vector (rounded) generated with One Permutation Hashing (Li et al., 2012).", "startOffset": 81, "endOffset": 98}, {"referenceID": 6, "context": "The dimensionality and sparsity of these datasets are an excellent representative of the scale and the size frequently encountered in large-scale data processing systems, such as Google\u2019s SIBYL (Chandra et al., 2010).", "startOffset": 194, "endOffset": 216}], "year": 2017, "abstractText": "Minwise hashing is a fundamental and one of the most successful hashing algorithm in the literature. Recent advances based on the idea of densification (Shrivastava & Li, 2014a;c) have shown that it is possible to compute k minwise hashes, of a vector with d nonzeros, in mere (d + k) computations, a significant improvement over the classical O(dk). These advances have led to an algorithmic improvement in the query complexity of traditional indexing algorithms based on minwise hashing. Unfortunately, the variance of the current densification techniques is unnecessarily high, which leads to significantly poor accuracy compared to vanilla minwise hashing, especially when the data is sparse. In this paper, we provide a novel densification scheme which relies on carefully tailored 2-universal hashes. We show that the proposed scheme is variance-optimal, and without losing the runtime efficiency, it is significantly more accurate than existing densification techniques. As a result, we obtain a significantly efficient hashing scheme which has the same variance and collision probability as minwise hashing. Experimental evaluations on real sparse and highdimensional datasets validate our claims. We believe that given the significant advantages, our method will replace minwise hashing implementations in practice.", "creator": "LaTeX with hyperref package"}}}