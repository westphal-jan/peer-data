{"id": "1703.04730", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Mar-2017", "title": "Understanding Black-box Predictions via Influence Functions", "abstract": "How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, identifying the points most responsible for a given prediction. Applying ideas from second-order optimization, we scale up influence functions to modern machine learning settings and show that they can be applied to high-dimensional black-box models, even in non-convex and non-differentiable settings. We give a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for many different purposes: to understand model behavior, debug models and detect dataset errors, and even identify and exploit vulnerabilities to adversarial training-set attacks.", "histories": [["v1", "Tue, 14 Mar 2017 21:07:01 GMT  (4753kb,D)", "http://arxiv.org/abs/1703.04730v1", null], ["v2", "Mon, 10 Jul 2017 02:31:54 GMT  (4538kb,D)", "http://arxiv.org/abs/1703.04730v2", "International Conference on Machine Learning, 2017"]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["pang wei koh", "percy liang"], "accepted": true, "id": "1703.04730"}, "pdf": {"name": "1703.04730.pdf", "metadata": {"source": "META", "title": "Understanding Black-box Predictions via Influence Functions", "authors": ["Pang Wei Koh", "Percy Liang"], "emails": ["<pangwei@cs.stanford.edu>,", "ang@cs.stanford.edu>."], "sections": [{"heading": "1. Introduction", "text": "In fact, it is the case that most of them are able to survive themselves if they do not see themselves able to survive themselves, and that they are not able to survive themselves, \"he told the Deutsche Presse-Agentur in an interview with\" Welt am Sonntag \":\" I do not believe that we will be able to change the world. \""}, {"heading": "2. Approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Changing the weight of a training point", "text": "Our goal is to understand the effects of training points on the predictions of a model. We formalize this goal by asking the counterfactual question: How would the predictions of the model change if we did not have this training point? Let us consider an unrestricted empirical risk minimization attitude, i.e., the parameters of risk are removed and we fold ourselves into regularization terms to L. Let us assume that the empirical risk 1 n i = 1 L (zi, 2001) is doubly differentiable and strongly convex in a local neighborhood around the prediction, i.e., Hessian HTB = 1 n, n, n, n, z, z, z, z, z, exists and is positively defined (PD)."}, {"heading": "2.2. Perturbing the training input", "text": "We can develop a finer grained idea of the influence by taking a different counterfactual view: how would the model's predictions change if a training input were modified? Let's add z = (x, y) a training point and a new \"phantom point.\" Let's look at its effects, let's look at the shift of the infinitesimal weight from z to z, \u2212 let's interpret the resulting parameters to remove 1n weight from z and add it to a new \"phantom point.\" To approximate its effects, let's look at the shift of the infinitesimal weight from z to e.g."}, {"heading": "2.3. Relation to Euclidean distance", "text": "In order to find the training examples that are most relevant for a test example, it is customary to look at its nearest neighbours in the Euclidean space (e.g. Ribeiro et al. (2016); with normalized vectors, this corresponds to the choice of x with a large x \u00b7 xtest. For intuition, we compare this with iup, loss (z, ztest) on a logistic regression model and show that the influence is much more accurate when taking into account the training effect.If we leave p (y | x) = \u03c3 (y\u03b8 > x), with y (1, 1} and \u03c3 (t) = 1 + exp (\u2212 t), we try to maximize the probability of the training point. For a training point z = (x, y), L (z, \u03b8) = Log (1 + exp (\u2212 exp > x), the influence of the training model will have no influence on the training model."}, {"heading": "3. Efficiently calculating influence", "text": "There are two challenges that we seek to avoid. (...) There are two challenges that we seek to avoid. (...) There are two challenges that we seek to avoid. (...) There are two challenges that we seek to avoid. (...) There are two problems that we seek to avoid. (...) There is only one solution. (...) There is only one solution. (...) There is only one solution. (...) There is only one solution. (...) There is only one solution. (...) There is only one solution. (...) There is only one solution. (...) There is only one solution. (...) There is only one solution. (...) There is only one solution. (...) There is only one solution. (...) There is only one solution. (...) There is only one solution. (...) The first solution. (...) The first problem is well investigated. (...) There is no solution. (...)"}, {"heading": "4. Validation and extensions", "text": "Remember that influencing functions are asymptotic approximations of leave-one-out retraining, and that these approximations are only theoretically valid if (i) the model parameters \u00da minimize empirical risk, and (ii) the empirical risk in a local neighborhood is doubly differentiable and strongly convex around \u03b8. In this section, we show empirically that influencing functions are accurate approximations (Section 4.1) and that they provide useful information even if these assumptions are violated (chapters 4.2, 4.3)."}, {"heading": "4.1. Influence functions vs. leave-one-out retraining", "text": "To investigate the accuracy of the influencing functions when = 1n, i.e. when we remove a training point and retrain the model, we compared \u2212 1nIup, loss (z, ztest) with the actual execution of Leaveone-out retraining. Using a Softmax model to 10-class MNIST, 3, the predicted and actual changes closely matched (fig. 2-links).3We trained with L-BFGS (Liu & Nocedal, 1989), with a weight drop of 0.01, n = 55,000 and p = 7,840 parameters. The stochastic estimator was also accurate with r = 10 repetitions and t = 5,000 iterations (fig. 2-mid, 1989).Since each iteration requires only one HVP (zi, profile value), this is fast: in reality, we accurately estimated this case without most of the points actually being set to 1-mid."}, {"heading": "4.2. Non-convexity and non-convergence", "text": "In practice, if we obtain our model parameters using methods such as SGD, we cannot guarantee that influence functions applied to \u03b8 will still produce meaningful results in practice. If this model is a local minimum, L (z, \u00b7) is still roughly square of L (z, \u03b8). As such, influence functions are locally significant: they tell us what happens when we upgrade a training point z and retrain ourselves with a gradient drop, i.e., stay with the \"same\" local minimum. If the influence functions are close to a local minimum, they are locally significant: They tell us what happens when we upgrade a training point z and start with a gradient drop, i.e., if we stay with the \"same\" local minimum. If the influence variable is close to a local minimum and Hig value is close, then negative influence functions of 0, Iup, Iameter, Step, Parrelate to the result."}, {"heading": "4.3. Non-differentiable losses", "text": "In this section we show that influencing functions calculated on smooth approximations of non-differentiable losses can predict the behavior of the original, non-differentiable loss in the context of leave-one-out retraining. To see this, we trained a linear SVM on the same 1s vs. 7s MNIST task in Section 2.3. This involves minimizing hinge (s) = max (0, 1 \u2212 s); this simple, piecewise linear function is similar to ReLUs in neural networks, the most common source of their non-differentiability. We have the derivatives on the hinges to 0 and calculated iup, loss. As you might expect, this was inaccurate (F3b-Left: the most common source of their non-differentiability is 0)."}, {"heading": "5. Use cases of influence functions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Understanding model behavior", "text": "In this section, we show that two models can make the same correct predictions, but get there in very different ways. We compared (a) the state-of-the-art Inception v3 network (Szegedy et al., 2016) with all but the top layer frozen6 and (b) an RBF SVM dataset we built from ImageNet (Russakovsky et al., 2015), with 900 training examples for each class. Freezing neural networks is not unusual in computer visions and corresponds to training Softmax features on the bottleneck. (Donahue et al., 2014) We chose a test image that both models are correct (Fig 4-Top) and used smoothHinge (\u00b7 0.001) for the influence of SVM."}, {"heading": "5.2. Adversarial training examples", "text": "In this section, we show that models that have a lot of impact on a small number of points are surprisingly susceptible to training impressions (disorders that affect a serious6We used pre-trained weights of keras (Chollet, 2015).Security risks in real world ML systems where attackers can obviously affect training data (Huang et al., 2011).Recent work has generated counterproductive test images that are visually indistinguishable from real test images but completely confuse a classifier (Goodfellow et al., 2015).We show that influence functions can be used to create training programs that are similarly visually indistinguishable and can reverse the prediction of a model to a separate test image. This is the first evidence of our knowledge that visually indistinguishable training attacks can be executed."}, {"heading": "5.3. Debugging domain mismatch", "text": "We show that influencing functions can identify the training examples that are most responsible for the errors, helping model developers locate a domain misjudgement (Kansagara et al., 2011). As a case study, we predicted whether a patient would be admitted to a hospital again. Domain misjudgments are widely used in biomedical data; for example, different hospitals can serve very different population groups, and reading function models trained in one population can perform poorly in another (Kansagara et al., 2011). We used logistical regression to predict reassessment with a balanced training dataset of 20K diabetics from 100 + US hospitals, each represented by 127 characteristics (Strack et al., 2014).73 of the 24 children under the age of 10 in this dataset were classified as misclassification. To induce a domain mismatch, we filtered 20 children who were not admitted, with 4 of them not being admitted."}, {"heading": "5.4. Fixing mislabeled examples", "text": "Labels in the real world are often loud, especially when they are crowdsourced (Fre \u0301 nay & Verleysen, 2014), and can even be counterproductively corrupted as described in Section 5.2. Even if a human expert could detect mislabeled examples, in many applications it is impossible to manually verify all training data. We show that influencing functions can help human experts prioritize their attention and allow them to inspect only those examples that are actually important.The key idea is to identify the training points that have the greatest impact on the model. Since we do not have access to the test kit, we measure the influence of zi with \u2212 1nIup, loss (zi, zi), which is similar to the individual errors that occurred on zi when we remove zi from the training set.Our case study removes email spam classification that relies on user-provided labels and is also vulnerable to a counterattack if they have access to one of the training units with counterfaulty labels (8)."}, {"heading": "6. Related work", "text": "This year, it has come to the point that it has never been as far as this year."}, {"heading": "7. Discussion", "text": "We have discussed a variety of applications, from vulnerability detection to debugging models and data set fixation. Each of these applications is a common tool, influencing functions based on a simple idea - we can better understand the behavior of models by looking at how it was derived from its training data. At its core, however, we could measure the impact of local changes: what happens when we pass a point through an infinitesimally small one? This locality allows us to derive efficient closed-form estimates, and how we show that they can be surprisingly effective. However, we would like to think about other global changes, such as how the set of all patients from this hospital affects the model, as the accuracy of the influencing functions is based on the model that does not change too much how to tackle this is an open question.It seems inevitable that powerful, complex black box models will become increasingly dominant and important."}, {"heading": "Acknowledgements", "text": "We thank Jacob Steinhardt, Zhenghao Chen and Hongseok Namkoong for helpful discussions and comments. This work was supported by a Future of Life Research Award and a Microsoft Research Faculty Fellowship."}], "references": [{"title": "Auditing black-box models for indirect influence", "author": ["P. Adler", "C. Falk", "S.A. Friedler", "G. Rybeck", "C. Scheidegger", "B. Smith", "S. Venkatasubramanian"], "venue": "arXiv preprint arXiv:1602.07043,", "citeRegEx": "Adler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Adler et al\\.", "year": 2016}, {"title": "Second order stochastic optimization in linear time", "author": ["N. Agarwal", "B. Bullins", "E. Hazan"], "venue": "arXiv preprint arXiv:1602.03943,", "citeRegEx": "Agarwal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2016}, {"title": "Modeltracker: Redesigning performance analysis tools for machine learning", "author": ["S. Amershi", "M. Chickering", "S.M. Drucker", "B. Lee", "P. Simard", "J. Suh"], "venue": "In Conference on Human Factors in Computing Systems (CHI),", "citeRegEx": "Amershi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Amershi et al\\.", "year": 2015}, {"title": "A theory of learning from different domains", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan"], "venue": "Machine Learning,", "citeRegEx": "Ben.David et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2010}, {"title": "Support vector machines under adversarial label noise", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "ACML, 20:97\u2013112,", "citeRegEx": "Biggio et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2011}, {"title": "Poisoning attacks against support vector machines", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Biggio et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2012}, {"title": "Debugging machine learning models", "author": ["G. Cadamuro", "R. Gilad-Bachrach", "X. Zhu"], "venue": "In ICML Workshop on Reliable Machine Learning in the Wild,", "citeRegEx": "Cadamuro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cadamuro et al\\.", "year": 2016}, {"title": "Influential observations, high leverage points, and outliers in linear regression", "author": ["S. Chatterjee", "A.S. Hadi"], "venue": "Statistical Science,", "citeRegEx": "Chatterjee and Hadi,? \\Q1986\\E", "shortCiteRegEx": "Chatterjee and Hadi", "year": 1986}, {"title": "On robustness properties of convex risk minimization methods for pattern recognition", "author": ["A. Christmann", "I. Steinwart"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Christmann and Steinwart,? \\Q2004\\E", "shortCiteRegEx": "Christmann and Steinwart", "year": 2004}, {"title": "Detection of influential observation in linear regression", "author": ["R.D. Cook"], "venue": "Technometrics, 19:15\u201318,", "citeRegEx": "Cook,? \\Q1977\\E", "shortCiteRegEx": "Cook", "year": 1977}, {"title": "Assessment of local influence", "author": ["R.D. Cook"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "citeRegEx": "Cook,? \\Q1986\\E", "shortCiteRegEx": "Cook", "year": 1986}, {"title": "Characterizations of an empirical influence function for detecting influential cases in regression", "author": ["R.D. Cook", "S. Weisberg"], "venue": null, "citeRegEx": "Cook and Weisberg,? \\Q1980\\E", "shortCiteRegEx": "Cook and Weisberg", "year": 1980}, {"title": "Residuals and influence in regression", "author": ["R.D. Cook", "S. Weisberg"], "venue": null, "citeRegEx": "Cook and Weisberg,? \\Q1982\\E", "shortCiteRegEx": "Cook and Weisberg", "year": 1982}, {"title": "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems", "author": ["A. Datta", "S. Sen", "Y. Zick"], "venue": "In Security and Privacy (SP),", "citeRegEx": "Datta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Datta et al\\.", "year": 2016}, {"title": "Model selection in kernel based regression using the influence function", "author": ["M. Debruyne", "M. Hubert", "J.A. Suykens"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Debruyne et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Debruyne et al\\.", "year": 2008}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Classification in the presence of label noise: a survey", "author": ["B. Fr\u00e9nay", "M. Verleysen"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Fr\u00e9nay and Verleysen,? \\Q2014\\E", "shortCiteRegEx": "Fr\u00e9nay and Verleysen", "year": 2014}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Goodfellow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "European union regulations on algorithmic decision-making and a \u201cright to explanation", "author": ["B. Goodman", "S. Flaxman"], "venue": "arXiv preprint arXiv:1606.08813,", "citeRegEx": "Goodman and Flaxman,? \\Q2016\\E", "shortCiteRegEx": "Goodman and Flaxman", "year": 2016}, {"title": "Adversarial machine learning", "author": ["L. Huang", "A.D. Joseph", "B. Nelson", "B.I. Rubinstein", "J. Tygar"], "venue": "In Proceedings of the 4th ACM workshop on Security and artificial intelligence,", "citeRegEx": "Huang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2011}, {"title": "The infinitesimal jackknife", "author": ["L.A. Jaeckel"], "venue": "Unpublished memorandum,", "citeRegEx": "Jaeckel,? \\Q1972\\E", "shortCiteRegEx": "Jaeckel", "year": 1972}, {"title": "Risk prediction models for hospital readmission: a systematic review", "author": ["D. Kansagara", "H. Englander", "A. Salanitro", "D. Kagen", "C. Theobald", "M. Freeman", "S. Kripalani"], "venue": null, "citeRegEx": "Kansagara et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kansagara et al\\.", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Data poisoning attacks on factorization-based collaborative filtering", "author": ["B. Li", "Y. Wang", "A. Singh", "Y. Vorobeychik"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Understanding neural networks through representation erasure", "author": ["J. Li", "W. Monroe", "D. Jurafsky"], "venue": "arXiv preprint arXiv:1612.08220,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "On the limited memory BFGS method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical Programming,", "citeRegEx": "Liu and Nocedal,? \\Q1989\\E", "shortCiteRegEx": "Liu and Nocedal", "year": 1989}, {"title": "Efficient approximation of cross-validation for kernel methods using Bouligand influence function", "author": ["Y. Liu", "S. Jiang", "S. Liao"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Deep learning via hessian-free optimization", "author": ["J. Martens"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Martens,? \\Q2010\\E", "shortCiteRegEx": "Martens", "year": 2010}, {"title": "The security of latent Dirichlet allocation", "author": ["S. Mei", "X. Zhu"], "venue": "In Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Mei and Zhu,? \\Q2015\\E", "shortCiteRegEx": "Mei and Zhu", "year": 2015}, {"title": "Using machine teaching to identify optimal training-set attacks on machine learners", "author": ["S. Mei", "X. Zhu"], "venue": "In Association for the Advancement of Artificial Intelligence (AAAI),", "citeRegEx": "Mei and Zhu,? \\Q2015\\E", "shortCiteRegEx": "Mei and Zhu", "year": 2015}, {"title": "Spam filtering with naive bayes-which naive bayes", "author": ["V. Metsis", "I. Androutsopoulos", "G. Paliouras"], "venue": "In CEAS,", "citeRegEx": "Metsis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Metsis et al\\.", "year": 2006}, {"title": "Fast exact multiplication by the hessian", "author": ["B.A. Pearlmutter"], "venue": "Neural Computation,", "citeRegEx": "Pearlmutter,? \\Q1994\\E", "shortCiteRegEx": "Pearlmutter", "year": 1994}, {"title": "why should I trust you?\u201d: Explaining the predictions of any classifier", "author": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "venue": "In International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Ribeiro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ribeiro et al\\.", "year": 2016}, {"title": "ImageNet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Not just a black box: Learning important features through propagating activation differences", "author": ["A. Shrikumar", "P. Greenside", "A. Shcherbina", "A. Kundaje"], "venue": "arXiv preprint arXiv:1605.01713,", "citeRegEx": "Shrikumar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shrikumar et al\\.", "year": 2016}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1312.6034,", "citeRegEx": "Simonyan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Striving for simplicity: The all convolutional net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1412.6806,", "citeRegEx": "Springenberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "Impact of HbA1c measurement on hospital readmission rates: analysis of 70,000 clinical database patient", "author": ["B. Strack", "J.P. DeShazo", "C. Gennings", "J.L. Olmo", "S. Ventura", "K.J. Cios", "J.N. Clore"], "venue": "records. BioMed Research International,", "citeRegEx": "Strack et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Strack et al\\.", "year": 2014}, {"title": "Rethinking the Inception architecture for computer vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano D. Team"], "venue": "arXiv preprint arXiv:1605.02688,", "citeRegEx": "Team.,? \\Q2016\\E", "shortCiteRegEx": "Team.", "year": 2016}, {"title": "Assessing influence on predictions from generalized linear models", "author": ["W. Thomas", "R.D. Cook"], "venue": null, "citeRegEx": "Thomas and Cook,? \\Q1990\\E", "shortCiteRegEx": "Thomas and Cook", "year": 1990}, {"title": "Asymptotic statistics", "author": ["A.W. van der Vaart"], "venue": null, "citeRegEx": "Vaart,? \\Q1998\\E", "shortCiteRegEx": "Vaart", "year": 1998}, {"title": "Generalized leverage and its applications", "author": ["B. Wei", "Y. Hu", "W. Fung"], "venue": "Scandinavian Journal of Statistics,", "citeRegEx": "Wei et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Wei et al\\.", "year": 1998}, {"title": "Influence sketching\u201d: Finding influential samples in large-scale regressions", "author": ["M. Wojnowicz", "B. Cruz", "X. Zhao", "B. Wallace", "M. Wolff", "J. Luan", "C. Crable"], "venue": "arXiv preprint arXiv:1611.05923,", "citeRegEx": "Wojnowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wojnowicz et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "By understanding why a model does what it does, we can hope to improve the model (Amershi et al., 2015), discover new science (Shrikumar et al.", "startOffset": 81, "endOffset": 103}, {"referenceID": 36, "context": ", 2015), discover new science (Shrikumar et al., 2016), and provide end-users with explanations of actions that impact them (Goodman & Flaxman, 2016).", "startOffset": 30, "endOffset": 54}, {"referenceID": 23, "context": ", deep neural networks for image and speech recognition (Krizhevsky et al., 2012) \u2014 are complicated, blackbox models whose predictions seem hard to explain.", "startOffset": 56, "endOffset": 81}, {"referenceID": 34, "context": ", by locally fitting a simpler model around the test point (Ribeiro et al., 2016) or by perturbing the test point to see how the prediction changes (Simonyan et al.", "startOffset": 59, "endOffset": 81}, {"referenceID": 43, "context": "Despite their rich history in statistics, influence functions have not seen widespread use in machine learning; to the best of our knowledge, the work closest to ours is a recent paper from Wojnowicz et al. (2016), which introduced a method for approximating Cook\u2019s distance (a quantity related to influence) in generalized linear models.", "startOffset": 190, "endOffset": 214}, {"referenceID": 33, "context": "We address this challenge by efficiently approximating influence functions using techniques from second-order optimization (Pearlmutter, 1994; Martens, 2010; Agarwal et al., 2016).", "startOffset": 123, "endOffset": 179}, {"referenceID": 29, "context": "We address this challenge by efficiently approximating influence functions using techniques from second-order optimization (Pearlmutter, 1994; Martens, 2010; Agarwal et al., 2016).", "startOffset": 123, "endOffset": 179}, {"referenceID": 1, "context": "We address this challenge by efficiently approximating influence functions using techniques from second-order optimization (Pearlmutter, 1994; Martens, 2010; Agarwal et al., 2016).", "startOffset": 123, "endOffset": 179}, {"referenceID": 17, "context": "We show that they are a versatile tool that can be applied to a wide variety of seemingly disparate tasks: understanding model behavior; debugging models; detecting dataset errors; and creating visually-indistinguishable adversarial training examples that can flip neural network test predictions, the training set analogue of Goodfellow et al. (2015). ar X iv :1 70 3.", "startOffset": 327, "endOffset": 352}, {"referenceID": 34, "context": ", Ribeiro et al. (2016)); with normalized", "startOffset": 2, "endOffset": 24}, {"referenceID": 24, "context": "These results are from a logistic regression model trained to distinguish 1\u2019s from 7\u2019s in MNIST (LeCun et al., 1998).", "startOffset": 96, "endOffset": 116}, {"referenceID": 33, "context": "We discuss two techniques for approximating stest, both relying on the fact that the HVP of a single term in H\u03b8\u0302, [\u2207\u03b8L(zi, \u03b8\u0302)]v, can be computed for arbitrary v in roughly the same amount of time that\u2207\u03b8L(zi, \u03b8\u0302) would take, which is typically O(p) (Pearlmutter, 1994).", "startOffset": 249, "endOffset": 268}, {"referenceID": 29, "context": "exact solution takes p CG iterations, in practice we can get a good approximation with fewer iterations; see Martens (2010) for more details.", "startOffset": 109, "endOffset": 124}, {"referenceID": 1, "context": "We use a method developed by Agarwal et al. (2016) to get a stochastic estimator that only samples a single point per iteration, achieving a significant speedup.", "startOffset": 29, "endOffset": 51}, {"referenceID": 1, "context": "We note that the original method of Agarwal et al. (2016) dealt only with generalized linear models, for which [\u2207\u03b8L(zi, \u03b8\u0302)]v can be decomposed into two rank-one matrix-vector products that can be efficiently computed in O(p) time.", "startOffset": 36, "endOffset": 58}, {"referenceID": 1, "context": "We note that the original method of Agarwal et al. (2016) dealt only with generalized linear models, for which [\u2207\u03b8L(zi, \u03b8\u0302)]v can be decomposed into two rank-one matrix-vector products that can be efficiently computed in O(p) time. In our case, we rely on Pearlmutter (1994)\u2019s more general algorithm for fast HVPs, described above, to achieve the same time complexity.", "startOffset": 36, "endOffset": 275}, {"referenceID": 38, "context": "The network had 7 sets of convolutional layers with tanh(\u00b7) non-linearities, modeled after the all-convolutional network from (Springenberg et al., 2014).", "startOffset": 126, "endOffset": 153}, {"referenceID": 40, "context": "We compared (a) the state-of-the-art Inception v3 network (Szegedy et al., 2016) with all but the top layer frozen6 and (b) an RBF SVM on a dog vs.", "startOffset": 58, "endOffset": 80}, {"referenceID": 35, "context": "fish image classification dataset we built from ImageNet (Russakovsky et al., 2015), with 900 training examples for each class.", "startOffset": 57, "endOffset": 83}, {"referenceID": 15, "context": "Freezing neural networks in this way is not uncommon in computer vision applications and is equivalent to training a softmax on the bottleneck features (Donahue et al., 2014).", "startOffset": 152, "endOffset": 174}, {"referenceID": 19, "context": "security risk in real-world ML systems where attackers can influence the training data (Huang et al., 2011).", "startOffset": 87, "endOffset": 107}, {"referenceID": 17, "context": "Recent work has generated adversarial test images that are visually indistinguishable from real test images but completely fool a classifier (Goodfellow et al., 2015).", "startOffset": 141, "endOffset": 166}, {"referenceID": 17, "context": "This is a training-set analogue of the fast gradient sign method used by (Goodfellow et al., 2015) for test-set attacks.", "startOffset": 73, "endOffset": 98}, {"referenceID": 4, "context": "We note that this attack is mathematically similar to the gradient-based dataset poisoning attacks explored by Biggio et al. (2012); Mei & Zhu (2015b) and others in the context of different models; we elaborate on this connection in section 6.", "startOffset": 111, "endOffset": 132}, {"referenceID": 4, "context": "We note that this attack is mathematically similar to the gradient-based dataset poisoning attacks explored by Biggio et al. (2012); Mei & Zhu (2015b) and others in the context of different models; we elaborate on this connection in section 6.", "startOffset": 111, "endOffset": 151}, {"referenceID": 4, "context": "We note that this attack is mathematically similar to the gradient-based dataset poisoning attacks explored by Biggio et al. (2012); Mei & Zhu (2015b) and others in the context of different models; we elaborate on this connection in section 6. Biggio et al. (2012) constructed a dataset poisoning attack against a linear SVM on a two-class MNIST task, but had to modify the training points in an obviously distinguishable way to be effective.", "startOffset": 111, "endOffset": 265}, {"referenceID": 3, "context": "Domain mismatch \u2014 where the training distribution does not match the test distribution \u2014 can cause models with high training accuracy to do poorly on test data (Ben-David et al., 2010).", "startOffset": 160, "endOffset": 184}, {"referenceID": 21, "context": "Domain mismatches are common in biomedical data; for example, different hospitals can serve very different populations, and readmission models trained on one population can do poorly on another (Kansagara et al., 2011).", "startOffset": 194, "endOffset": 218}, {"referenceID": 39, "context": "We used logistic regression to predict readmission with a balanced training dataset of 20K diabetic patients from 100+ US hospitals, each represented by 127 features (Strack et al., 2014).", "startOffset": 166, "endOffset": 187}, {"referenceID": 4, "context": "Our case study is email spam classification, which relies on user-provided labels and is also vulnerable to adversarial attack (Biggio et al., 2011).", "startOffset": 127, "endOffset": 148}, {"referenceID": 9, "context": "The use of influence-based diagnostics originated in statistics in the 70s and 80s, driven by seminal papers by Cook and others (Cook, 1977; Cook & Weisberg, 1980; 1982), though similar ideas appeared even earlier in other forms, e.", "startOffset": 128, "endOffset": 169}, {"referenceID": 20, "context": ", the infinitesimal jackknife (Jaeckel, 1972).", "startOffset": 30, "endOffset": 45}, {"referenceID": 10, "context": "Earlier work focused on removing training points from linear models, with later work extending this to more general models and a wider variety of perturbations (Cook, 1986; Thomas & Cook, 1990; Chatterjee & Hadi, 1986; Wei et al., 1998).", "startOffset": 160, "endOffset": 236}, {"referenceID": 44, "context": "Earlier work focused on removing training points from linear models, with later work extending this to more general models and a wider variety of perturbations (Cook, 1986; Thomas & Cook, 1990; Chatterjee & Hadi, 1986; Wei et al., 1998).", "startOffset": 160, "endOffset": 236}, {"referenceID": 32, "context": "We divided the Enron1 spam dataset (Metsis et al., 2006) into training (n = 4147) and test (n = 1035) sets, training logistic regression on top of a bag-of-words representation.", "startOffset": 35, "endOffset": 56}, {"referenceID": 12, "context": "Christmann & Steinwart (2004); Debruyne et al. (2008); Liu et al.", "startOffset": 31, "endOffset": 54}, {"referenceID": 12, "context": "Christmann & Steinwart (2004); Debruyne et al. (2008); Liu et al. (2014) use influence functions to study model robustness and to do fast cross-validation in kernel methods.", "startOffset": 31, "endOffset": 73}, {"referenceID": 12, "context": "Christmann & Steinwart (2004); Debruyne et al. (2008); Liu et al. (2014) use influence functions to study model robustness and to do fast cross-validation in kernel methods. Wojnowicz et al. (2016) uses matrix sketching to estimate Cook\u2019s distance, which is closely related to influence; they focus on prioritizing training points for human attention and derive methods specific to generalized linear models.", "startOffset": 31, "endOffset": 198}, {"referenceID": 4, "context": "2, our training-set attack is mathematically similar to an approach first explored by Biggio et al. (2012) in the context of SVMs, with follow-up work extending the framework and applying it to linear and logistic regression (Mei & Zhu, 2015b), topic modeling (Mei & Zhu, 2015a), and collaborative filtering (Li et al.", "startOffset": 86, "endOffset": 107}, {"referenceID": 4, "context": "2, our training-set attack is mathematically similar to an approach first explored by Biggio et al. (2012) in the context of SVMs, with follow-up work extending the framework and applying it to linear and logistic regression (Mei & Zhu, 2015b), topic modeling (Mei & Zhu, 2015a), and collaborative filtering (Li et al., 2016a). These papers derived the attack directly from the KKT conditions without considering influence functions, though for continuous data the end result is equivalent to iterating Ipert,loss. Influence functions additionally allow us to consider attacks that change discrete data (section 2.2) and potentially models that are not convex or differentiable (sections 4.2, 4.3), though more work needs to be done to test the effectiveness of those attacks. Our work merges elements of this approach with the work of Goodfellow et al. (2015) and others on visually-imperceptible, adversarial test-set attacks in neural networks: we cast the dataset poisoning attack in the influence function framework and apply it in a larger-scale, neural network setting, creating a visually-imperceptible attack on the training set.", "startOffset": 86, "endOffset": 861}, {"referenceID": 6, "context": "In contrast to training-set attacks, Cadamuro et al. (2016) consider the task of taking an incorrect test prediction and", "startOffset": 37, "endOffset": 60}], "year": 2017, "abstractText": "How can we explain the predictions of a blackbox model? In this paper, we use influence functions \u2014 a classic technique from robust statistics \u2014 to trace a model\u2019s prediction through the learning algorithm and back to its training data, identifying the points most responsible for a given prediction. Applying ideas from second-order optimization, we scale up influence functions to modern machine learning settings and show that they can be applied to highdimensional black-box models, even in nonconvex and non-differentiable settings. We give a simple, efficient implementation that requires only oracle access to gradients and Hessianvector products. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for many different purposes: to understand model behavior, debug models and detect dataset errors, and even identify and exploit vulnerabilities to adversarial training-set attacks.", "creator": "LaTeX with hyperref package"}}}