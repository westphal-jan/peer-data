{"id": "1703.04783", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Mar-2017", "title": "Multichannel End-to-end Speech Recognition", "abstract": "The field of speech recognition is in the midst of a paradigm shift: end-to-end neural networks are challenging the dominance of hidden Markov models as a core technology. Using an attention mechanism in a recurrent encoder-decoder architecture solves the dynamic time alignment problem, allowing joint end-to-end training of the acoustic and language modeling components. In this paper we extend the end-to-end framework to encompass microphone array signal processing for noise suppression and speech enhancement within the acoustic encoding network. This allows the beamforming components to be optimized jointly within the recognition architecture to improve the end-to-end speech recognition objective. Experiments on the noisy speech benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system outperformed the attention-based baseline with input from a conventional adaptive beamformer.", "histories": [["v1", "Tue, 14 Mar 2017 22:28:51 GMT  (2107kb,D)", "http://arxiv.org/abs/1703.04783v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["tsubasa ochiai", "shinji watanabe", "takaaki hori", "john r hershey"], "accepted": true, "id": "1703.04783"}, "pdf": {"name": "1703.04783.pdf", "metadata": {"source": "META", "title": "Multichannel End-to-end Speech Recognition ", "authors": ["Tsubasa Ochiai", "Shinji Watanabe", "Takaaki Hori", "John R. Hershey"], "emails": ["EUP1105@MAIL4.DOSHISHA.AC.JP", "WATANABE@MERL.COM", "THORI@MERL.COM", "HERSHEY@MERL.COM"], "sections": [{"heading": "1. Introduction", "text": "In this context, we must also mention the fact that these two are people who have played an important role in politics and the economy in recent years. (...) It is not that they have pushed themselves to the forefront in politics. (...) It is not that they play a role in politics. (...) It is not that they play a role. (...) It is that they play a role. (...) It is that they play a role. (...) It is that they play a role. (...) It is not that they play a role. (...) It is that they play a role. (...) It is that they play a role. (...) It is that they play a role. (...). (...). (...). \").\" \"\". \"\"..................................... \"\". \"\".. \"\" \"..\" \""}, {"heading": "2. Overview of attention-based encoder-decoder networks", "text": "The frame consists of two RNNs, each designated as an encoder and decoder, and an attention mechanism connecting the encoder and decoder, as shown in Figure 1. (N) The sequence of input characteristics O = 1, \u00b7 \u00b7, T}, the network generates an N-length sequence of output labels Y = {yn V | n = 1, \u00b7, \u00b7, where there is a DO-dimensional feature vector (e.g., Log Mel filter bank) at input time t, and yn is a label symbol (e.g., character) at output time step n in label set V.First, given an input sequence O, the encoder network transforms it to an L-length high level."}, {"heading": "3. Neural beamformers", "text": "This section describes neural beamformer techniques integrated into the encoder decoder network (Li et al., 2016; Sainath et al., 2016). In the frequency domain representation, a filter and sum beamer receives an improved signal as follows: x-t, f = C \u2211 c = 1 gt, f, cxt, f, c, (11), where xt, f, c-C is an STFT coefficient of c-th channel noise in a time-frequency recycle bin (t, f). gt, f, c-C is a corresponding beamforming filter coefficient. x-t, f-C is an improved STFT coefficient, and C is the number of channels. In this essay, we adopt two types of neural beamformers that essentially follow Eq 11 (sections, f-structure, f-structure and scheme)."}, {"heading": "3.1. Filter estimation network approach", "text": "The filter estimates network directly estimates a time variant filter coefficient {gt, f, c} T, F, Ct = 1, f = 1, c = 1 as the output of the network, which was originally proposed in (Li et al., 2016). F is the dimension of STFT functions. This approach uses a single real value BLSTM network to predict the real and imaginary parts of the complex rated filter coefficients in each step. Therefore, we present several (2 \u00b7 C) output problems separately calculate the real and imaginary parts of the filter coefficients for each channel. Then, the network returns time variant filter coefficients gt, c = {gt, c} Ff, c} Ff for a time step t for c-th channel as follows; Z = BLSTSTSTSTSTSTM ({x, t} Tt) Tt = 1, (12) < z, c = tanh."}, {"heading": "3.2. Mask estimation network approach", "text": "The central point of the network approach to mask estimation is that it limits the estimated filters based on sound array signal processing principles. Here, the network estimates the time-frequency masks used to calculate the time-invariant filter coefficients {gf, c} F, Cf = 1, c = 1 based on MVDR formalizations, which is the main difference between this approach and the approach to filter network estimation described in Section 3.1. In addition, mask-based beamforming approaches have performed well in benchmarks for sound speech recognition (Yoshioka et al., 2015; Heymann et al., 2016; Erdogan et al., 2016).Therefore, this paper suggests the use of a mask-based MVDR beamformer formalizing the entire procedures as a differentiated network for the subsequent end-to-end speech recognition system."}, {"heading": "3.2.1. MASK-BASED MVDR FORMALIZATION", "text": "One of the MVDR formalizations calculates the time-variant filter coefficients g (f) = {gf, c} Cc = 1 \u0445 CC in Equation (11) as follows (Souden et al., 2010): g (f) = \u03a6N (f) \u2212 1\u03a6S (f) Tr (f) \u2212 1\u03a6S (f) u, (15) where \u03a6S (f) \u0445 CC \u00b7 C and \u03a6N (f) \u0445 CC \u00b7 C are the coupling power spectral density (PSD) matrices (also known as spatial covariance matrices) for speech and sound signals, correspondingly."}, {"heading": "3.2.2. MASK ESTIMATION NETWORK", "text": "In the network approach, we use two real-weighted BLSTM networks; one for a speech mask and the other for a noise mask. Each network prints the time mask as follows: ZSc = BLSTM S ({x-t, c-Tt = 1), (18) mSt, c = sigmoid (W SzSt, c + b S), (19) ZNc = BLSTM N ({x-t, c-Tt = 1), (20) mNt, c = sigmoid (W NzNt, c + b N), (21) where ZSc = {zSt, c-RDZ-t = 1, \u00b7, T} Tt = 1), (20) mNt, c = sigmoid (W NzNt, c + b N), (21) where ZSc = {zSt, c-RDZ | t = 1, \u00b7, T}, Tt = 1, c, (mc), where Nc-D, mc = (t), with the mask of the Nc, Nt motors."}, {"heading": "3.2.3. ATTENTION-BASED REFERENCE SELECTION", "text": "In order to integrate the selection of the reference microphone in a neural beam frame, we use a soft-max mask for the vector u in Equation (15), which is derived from an attention mechanism. In this approach, the reference microphone vector u is estimated from time invariant feature vectors qc and rc as follows: k = v Ttanh (VQqc + V Rrc + b), (23) uc = exp (\u03b2k \u0445c) \u2211 C = 1 exp (\u03b2k \u0445c), (24) where v channels R1 \u00b7 DV, VZ-RDV \u00b7 2DZ, VR channels RDV \u00b7 2F are detectable weight variables, b channels RDV is a detectable bias vector. \u03b2 is the sharpening factor. We use two types of offsets; 1) the time mean vector Dqc \u00b2 R2Z R2Z."}, {"heading": "4. Multichannel end-to-end ASR", "text": "This year, we are dealing with a number of projects that deal with the question of whether and to what extent the people in the individual countries are able to represent their interests. (...) We have it in our hands to represent their interests. (...) We have it in our hands to represent their interests. (...) We have it in our hands to represent their interests. (...) We have it in our hands to represent their interests. (...) We have it in our hands. (...) We have it in our hands to represent their interests. (...) We have it in our hands. (...) We have it in our hands to represent their interests. (...) We have it in our hands. \"(...) We have it in our hands.\" (...) We have it in our hands. \"(...) We have it in our hands.\" (...) We have it in our hands. \""}, {"heading": "5. Experiments", "text": "We study the effectiveness of our end-to-end multi-channel system compared to a base-end-to-end system with noisy speech or beam formats. We use the two multi-channel voice recognition benchmarks CHiME-4 (Vincent et al., 2016) and AMI (Hain et al., 2007).CHiME-4 is a speech recognition task in a noisy environment, consisting of speech recorded with a tablet device with 6-channel microphones. It consists of real and simulated data. The training kit consists of 3 hours of real speech data expressed by 4 speakers and 15 hours of simulation language data expressed by 83 speakers. The development kit consists of 2.9 hours of real and simulation language data transmitted by 4 speakers."}, {"heading": "5.1. Configuration", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1.1. ENCODER-DECODER NETWORKS", "text": "In this experiment, we used 4-layer BLSTM with 320 cells in the encoder (DH = 320) and 1-layer LSTM with 320 cells in the decoder (DS = 320). In the encoder, we examined the hidden states of the first and second layer and used every second of the hidden states for the inputs of the next layer. Therefore, the number of hidden states in the output layer of the encoder is set to L = T / 4. After each BLSTM layer, we used a linear projection layer with 320 units to combine the forward and backward LSTM outputs. For the attention mechanism, 10 centered folding filters (DF = 10) of width 100 (Df = 100) and the Cnop-1 hypothesis were used."}, {"heading": "5.1.2. NEURAL BEAMFORMERS", "text": "256 STFT coefficients and offset were calculated from a 25ms wide hammer window with 10ms displacement (F = 257). Both filter and mask estimation networking approaches used a 3-layer BLSTM with 320 cells (DZ = 320) without subsampling technique. For the attention mechanism of the reference selection we used the same dimension of the inner attention product (DV = 320) and the sharpening factor \u03b2 = 2 as that of the encoder decoder network."}, {"heading": "5.1.3. SHARED CONFIGURATIONS", "text": "We used the AdaDelta algorithm (Zeiler, 2012) with gradient circumcision (Pascanu et al., 2013) for optimization. We initialized the AdaDelta hyperparameters \u03c1 = 0.95 and = 1 \u2212 8. Once the loss via the validation set was reduced, we decreased the AdaDelta hyperparameter by multiplying by 0.01 in each subsequent epoch. Training was stopped after 15 epochs. During the training, we chose a multi-conditioning training strategy, i.e. in addition to optimizing with the advanced functions of the neural beamformers, we also used the loud multi-channel voice data as input from encoder decoder networks without the neural beamers to improve the robustness of the encoder decoder networks. All of the above networks were implemented by Chainer (Tokui et al., 2015)."}, {"heading": "5.2. Results", "text": "In fact, the fact is that most of them are able to move, to move and to move."}, {"heading": "5.3. Influence on the number and order of channels", "text": "As we discussed in Section 3.2, a unique feature of our proposed MASK NET (ATT) is robustness / invariance compared to the number and sequence of channels without retraining. Table 3 shows an influence of the accuracy of the CHiME-4 validation on the number and sequence of channels. Accuracy of the validation was calculated on the basis of the ground truth labels y 1: n \u2212 1 in equation (10) during the recursive character generation of the decoder, which has a strong correlation with the CER. The second column of the table represents the channel indices used to input the same MASK NET (ATT) network. Comparing 5 6 4 3 1 1 and 3 4 1 5 6 shows that the sequence of channels did not affect the recognition performance of MASK NET (ATT) at all as we expected."}, {"heading": "5.4. Visualization of beamformed features", "text": "To analyze the behavior of our engineered voice enhancement component with a neural beamformer, Figure 4 illustrates the spectrograms of the same CHiME-4 expression with the 5th channel noise, the improved signal with BeamformIt, and the improved signal with our proposed MASK NET (ATT). We were able to confirm that BeamformIt and MASK NET (ATT) successfully suppressed noise relative to the 5th channel signal by eliminating overall blurred red areas. Furthermore, the harmonic structure that was corrupted in the 5th channel signal was restored in BeamformIt and MASK NET (ATT), suggesting that our proposed MASK NET (ATT) successfully learned a noise suppression function similar to the conventional beamformer, although optimized based on the end-to-end ASR lens, without explicitly using clean data as a target."}, {"heading": "6. Conclusions", "text": "In this paper, we expanded an existing attention-based encoder-decoder framework by integrating a neural beamformer and proposed a multi-channel end-to-end speech recognition framework that can optimize the general conclusion in multi-channel speech recognition (i.e., from speech enhancement to speech recognition) based on the end-to-end ASR target and generalize to different numbers and configurations of microphones. Experimental results on challenging speech recognition benchmarks, CHiME-4 and AMI, show that the proposed framework exceeded the end-to-end baseline with loud and delayed and summary inputs. The current system still presents data-saving problems due to the absence of lexicon and language models, as opposed to the conventional hybrid approach. Therefore, the results reported in the paper did not reach the state of the art, but they are convincing to demonstrate the effectiveness of the proposed framework."}], "references": [{"title": "Acoustic beamforming for speaker diarization of meetings", "author": ["Anguera", "Xavier", "Wooters", "Chuck", "Hernando", "Javier"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Anguera et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Anguera et al\\.", "year": 2011}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Bahdanau", "Dzmitry", "Chorowski", "Jan", "Serdyuk", "Dmitriy", "Brakel", "Philemon", "Bengio", "Yoshua"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Bahdanau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2016}, {"title": "The third \u2019CHiME\u2019 speech separation and recognition challenge: Analysis and outcomes", "author": ["Barker", "Jon", "Marxer", "Ricard", "Vincent", "Emmanuel", "Watanabe", "Shinji"], "venue": "Computer Speech & Language,", "citeRegEx": "Barker et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Barker et al\\.", "year": 2016}, {"title": "Microphone array signal processing, volume 1", "author": ["Benesty", "Jacob", "Chen", "Jingdong", "Huang", "Yiteng"], "venue": "Springer Science & Business Media,", "citeRegEx": "Benesty et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Benesty et al\\.", "year": 2008}, {"title": "Connectionist speech recognition: A hybrid approach", "author": ["Bourlard", "Herv\u00e9", "Morgan", "Nelson"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "Bourlard et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bourlard et al\\.", "year": 1994}, {"title": "High-resolution frequency-wavenumber spectrum analysis", "author": ["Capon", "Jack"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Capon and Jack.,? \\Q1969\\E", "shortCiteRegEx": "Capon and Jack.", "year": 1969}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["Chan", "William", "Jaitly", "Navdeep", "Le", "Quoc", "Vinyals", "Oriol"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Chan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2016}, {"title": "End-to-end continuous speech recognition using attention-based recurrent NN: First results", "author": ["Chorowski", "Jan", "Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.1602,", "citeRegEx": "Chorowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2014}, {"title": "Attention-based models for speech recognition", "author": ["Chorowski", "Jan K", "Bahdanau", "Dzmitry", "Serdyuk", "Dmitriy", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Improved MVDR beamforming using single-channel mask prediction networks", "author": ["Erdogan", "Hakan", "Hershey", "John R", "Watanabe", "Shinji", "Mandel", "Michael", "Le Roux", "Jonathan"], "venue": "In Interspeech,", "citeRegEx": "Erdogan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Erdogan et al\\.", "year": 2016}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Alex", "Jaitly", "Navdeep"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Neural network based spectral mask estimation for acoustic beamforming", "author": ["Heymann", "Jahn", "Drude", "Lukas", "Haeb-Umbach", "Reinhold"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Heymann et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Heymann et al\\.", "year": 2016}, {"title": "Speech acoustic modeling from raw multichannel waveforms", "author": ["Hoshen", "Yedid", "Weiss", "Ron J", "Wilson", "Kevin W"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Hoshen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hoshen et al\\.", "year": 2015}, {"title": "Continuous speech recognition by statistical methods", "author": ["Jelinek", "Frederick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Jelinek and Frederick.,? \\Q1976\\E", "shortCiteRegEx": "Jelinek and Frederick.", "year": 1976}, {"title": "Joint CTC-attention based end-to-end speech recognition using multi-task learning", "author": ["Kim", "Suyoun", "Hori", "Takaaki", "Watanabe", "Shinji"], "venue": "arXiv preprint arXiv:1609.06773,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Neural network adaptive beamforming for robust multichannel speech recognition", "author": ["Li", "Bo", "Sainath", "Tara N", "Weiss", "Ron J", "Wilson", "Kevin W", "Bacchiani", "Michiel"], "venue": "In Interspeech,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding", "author": ["Miao", "Yajie", "Gowayyed", "Mohammad", "Metze", "Florian"], "venue": "In IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Miao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Reducing the computational complexity of multimicrophone acoustic models with integrated feature extraction", "author": ["Sainath", "Tara N", "Narayanan", "Arun", "Weiss", "Ron J", "Variani", "Ehsan", "Wilson", "Kevin W", "Bacchiani", "Michiel", "Shafran", "Izhak"], "venue": "In Interspeech,", "citeRegEx": "Sainath et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2016}, {"title": "On optimal frequency-domain multichannel linear filtering for noise reduction", "author": ["Souden", "Mehrez", "Benesty", "Jacob", "Affes", "Sofi\u00e8ne"], "venue": "IEEE Transactions on audio, speech, and language processing,", "citeRegEx": "Souden et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Souden et al\\.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems (NIPS)", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Chainer: a next-generation open source framework for deep learning", "author": ["Tokui", "Seiya", "Oono", "Kenta", "Hido", "Shohei", "Clayton", "Justin"], "venue": "In Proceedings of Workshop on Machine Learning Systems (LearningSys) in NIPS,", "citeRegEx": "Tokui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tokui et al\\.", "year": 2015}, {"title": "Beamforming: A versatile approach to spatial filtering", "author": ["Van Veen", "Barry D", "Buckley", "Kevin M"], "venue": "IEEE ASSP Magazine,", "citeRegEx": "Veen et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Veen et al\\.", "year": 1988}, {"title": "An analysis of environment, microphone and data simulation mismatches in robust speech recognition", "author": ["Vincent", "Emmanuel", "Watanabe", "Shinji", "Nugraha", "Aditya Arie", "Barker", "Jon", "Marxer", "Ricard"], "venue": "Computer Speech & Language,", "citeRegEx": "Vincent et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2016}, {"title": "Distant speech recognition", "author": ["W\u00f6lfel", "Matthias", "McDonough", "John"], "venue": null, "citeRegEx": "W\u00f6lfel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "W\u00f6lfel et al\\.", "year": 2009}, {"title": "A study of learning based beamforming methods for speech recognition", "author": ["Xiao", "Xiong", "Xu", "Chenglin", "Zhang", "Zhaofeng", "Zhao", "Shengkui", "Sun", "Sining", "Watanabe", "Shinji"], "venue": "In CHiME 2016 workshop,", "citeRegEx": "Xiao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2016}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}, {"title": "Very deep convolutional networks for end-to-end speech recognition", "author": ["Zhang", "Yu", "Chan", "William", "Jaitly", "Navdeep"], "venue": "arXiv preprint arXiv:1610.03022,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "As a simpler alternative, end-to-end speech recognition paradigm has attracted great research interest (Chorowski et al., 2014; 2015; Chan et al., 2016; Graves & Jaitly, 2014; Miao et al., 2015).", "startOffset": 103, "endOffset": 194}, {"referenceID": 6, "context": "As a simpler alternative, end-to-end speech recognition paradigm has attracted great research interest (Chorowski et al., 2014; 2015; Chan et al., 2016; Graves & Jaitly, 2014; Miao et al., 2015).", "startOffset": 103, "endOffset": 194}, {"referenceID": 16, "context": "As a simpler alternative, end-to-end speech recognition paradigm has attracted great research interest (Chorowski et al., 2014; 2015; Chan et al., 2016; Graves & Jaitly, 2014; Miao et al., 2015).", "startOffset": 103, "endOffset": 194}, {"referenceID": 7, "context": "Specifically, an attentionbased encoder-decoder framework (Chorowski et al., 2014) integrates all of those components using a set of recurrent neural networks (RNN), which map from acoustic feature sequences to character label sequences.", "startOffset": 58, "endOffset": 82}, {"referenceID": 2, "context": ", Amazon echo) and benchmark studies (Barker et al., 2016; Kinoshita et al., 2016) show that multichannel speech enhancement techniques, using beamforming methods, produce substantial improvements as a pre-processor for conventional hybrid systems, in the presence of strong background noise.", "startOffset": 37, "endOffset": 82}, {"referenceID": 3, "context": "Traditionally, beamforming techniques such as delay-and-sum and filterand-sum are optimized based on a signal-level loss function, independently of speech recognition task (Benesty et al., 2008; Van Veen & Buckley, 1988).", "startOffset": 172, "endOffset": 220}, {"referenceID": 15, "context": "Recent studies on neural beamformers can be categorized into two types: (1) beamformers with a filter estimation network (Xiao et al., 2016a; Li et al., 2016) and (2) beamformers with a mask estimation network (Heymann et al.", "startOffset": 121, "endOffset": 158}, {"referenceID": 11, "context": ", 2016) and (2) beamformers with a mask estimation network (Heymann et al., 2016; Erdogan et al., 2016).", "startOffset": 59, "endOffset": 103}, {"referenceID": 9, "context": ", 2016) and (2) beamformers with a mask estimation network (Heymann et al., 2016; Erdogan et al., 2016).", "startOffset": 59, "endOffset": 103}, {"referenceID": 11, "context": "Recently, it has been reported that the mask estimationbased approaches (Yoshioka et al., 2015; Heymann et al., 2016; Erdogan et al., 2016) achieve great performance in noisy speech recognition benchmarks (e.", "startOffset": 72, "endOffset": 139}, {"referenceID": 9, "context": "Recently, it has been reported that the mask estimationbased approaches (Yoshioka et al., 2015; Heymann et al., 2016; Erdogan et al., 2016) achieve great performance in noisy speech recognition benchmarks (e.", "startOffset": 72, "endOffset": 139}, {"referenceID": 1, "context": "To reduce the input sequence length, we apply a subsampling technique (Bahdanau et al., 2016) to some layers.", "startOffset": 70, "endOffset": 93}, {"referenceID": 8, "context": "In this work, we adopt a location-based attention mechanism (Chorowski et al., 2015), and an and cn are formalized as follows:", "startOffset": 60, "endOffset": 84}, {"referenceID": 8, "context": "sn \u2208 RS is a DS-dimensional hidden state vector obtained from an upper decoder network at n, and \u03b1 is a sharpening factor (Chorowski et al., 2015).", "startOffset": 122, "endOffset": 146}, {"referenceID": 15, "context": "This paper uses frequency-domain beamformers rather than time-domain ones, which achieve significant computational complexity reduction in multichannel neural processing (Li et al., 2016; Sainath et al., 2016).", "startOffset": 170, "endOffset": 209}, {"referenceID": 18, "context": "This paper uses frequency-domain beamformers rather than time-domain ones, which achieve significant computational complexity reduction in multichannel neural processing (Li et al., 2016; Sainath et al., 2016).", "startOffset": 170, "endOffset": 209}, {"referenceID": 15, "context": "The filter estimation network directly estimates a timevariant filter coefficients {gt,f,c} t=1,f=1,c=1 as the outputs of the network, which was originally proposed in (Li et al., 2016).", "startOffset": 168, "endOffset": 185}, {"referenceID": 11, "context": "Also, mask-based beamforming approaches have achieved great performance in noisy speech recognition benchmarks (Yoshioka et al., 2015; Heymann et al., 2016; Erdogan et al., 2016).", "startOffset": 111, "endOffset": 178}, {"referenceID": 9, "context": "Also, mask-based beamforming approaches have achieved great performance in noisy speech recognition benchmarks (Yoshioka et al., 2015; Heymann et al., 2016; Erdogan et al., 2016).", "startOffset": 111, "endOffset": 178}, {"referenceID": 19, "context": "(11) as follows (Souden et al., 2010):", "startOffset": 16, "endOffset": 37}, {"referenceID": 11, "context": "Based on (Yoshioka et al., 2015; Heymann et al., 2016), the PSD matrices are robustly estimated using the expectation with respect to time-frequency masks as follows:", "startOffset": 9, "endOffset": 54}, {"referenceID": 11, "context": "The MVDR beamformer through this BLSTM mask estimation is originally proposed in (Heymann et al., 2016), but our neural beamformer further extends it with attention-based reference selection, which is described in the next subsection.", "startOffset": 81, "endOffset": 103}, {"referenceID": 15, "context": "There have been several related studies of neural beamformers based on the filter estimation (Li et al., 2016; Xiao et al., 2016a) and the mask estimation (Heymann et al.", "startOffset": 93, "endOffset": 130}, {"referenceID": 11, "context": ", 2016a) and the mask estimation (Heymann et al., 2016; Erdogan et al., 2016; Xiao et al., 2016b).", "startOffset": 33, "endOffset": 97}, {"referenceID": 9, "context": ", 2016a) and the mask estimation (Heymann et al., 2016; Erdogan et al., 2016; Xiao et al., 2016b).", "startOffset": 33, "endOffset": 97}, {"referenceID": 12, "context": "Speech recognition with raw multichannel waveforms (Hoshen et al., 2015; Sainath et al., 2016) can also be seen as using a neural beamformer, where the filter coefficients are represented as network parameters, but again these methods are still based on the hybrid approach.", "startOffset": 51, "endOffset": 94}, {"referenceID": 18, "context": "Speech recognition with raw multichannel waveforms (Hoshen et al., 2015; Sainath et al., 2016) can also be seen as using a neural beamformer, where the filter coefficients are represented as network parameters, but again these methods are still based on the hybrid approach.", "startOffset": 51, "endOffset": 94}, {"referenceID": 7, "context": "(Chorowski et al., 2014; Graves & Jaitly, 2014; Chorowski et al., 2015; Chan et al., 2016; Miao et al., 2015; Zhang et al., 2016; Kim et al., 2016; Lu et al., 2016).", "startOffset": 0, "endOffset": 164}, {"referenceID": 8, "context": "(Chorowski et al., 2014; Graves & Jaitly, 2014; Chorowski et al., 2015; Chan et al., 2016; Miao et al., 2015; Zhang et al., 2016; Kim et al., 2016; Lu et al., 2016).", "startOffset": 0, "endOffset": 164}, {"referenceID": 6, "context": "(Chorowski et al., 2014; Graves & Jaitly, 2014; Chorowski et al., 2015; Chan et al., 2016; Miao et al., 2015; Zhang et al., 2016; Kim et al., 2016; Lu et al., 2016).", "startOffset": 0, "endOffset": 164}, {"referenceID": 16, "context": "(Chorowski et al., 2014; Graves & Jaitly, 2014; Chorowski et al., 2015; Chan et al., 2016; Miao et al., 2015; Zhang et al., 2016; Kim et al., 2016; Lu et al., 2016).", "startOffset": 0, "endOffset": 164}, {"referenceID": 27, "context": "(Chorowski et al., 2014; Graves & Jaitly, 2014; Chorowski et al., 2015; Chan et al., 2016; Miao et al., 2015; Zhang et al., 2016; Kim et al., 2016; Lu et al., 2016).", "startOffset": 0, "endOffset": 164}, {"referenceID": 14, "context": "(Chorowski et al., 2014; Graves & Jaitly, 2014; Chorowski et al., 2015; Chan et al., 2016; Miao et al., 2015; Zhang et al., 2016; Kim et al., 2016; Lu et al., 2016).", "startOffset": 0, "endOffset": 164}, {"referenceID": 23, "context": "We use the two multichannel speech recognition benchmarks, CHiME-4 (Vincent et al., 2016) and AMI (Hain et al.", "startOffset": 67, "endOffset": 89}, {"referenceID": 14, "context": "CTC-attention multi-task loss function (Kim et al., 2016), and set the CTC loss weight as 0.", "startOffset": 39, "endOffset": 57}, {"referenceID": 20, "context": "For decoding, we used a beam search algorithm similar to (Sutskever et al., 2014) with the beam size 20 at each output step to reduce the computation cost.", "startOffset": 57, "endOffset": 81}, {"referenceID": 8, "context": "We adopted a length penalty term (Chorowski et al., 2015) to the decoding objective and set the penalty weight as 0.", "startOffset": 33, "endOffset": 57}, {"referenceID": 17, "context": "We used the AdaDelta algorithm (Zeiler, 2012) with gradient clipping (Pascanu et al., 2013) for optimization.", "startOffset": 69, "endOffset": 91}, {"referenceID": 21, "context": "All the above networks are implemented by using Chainer (Tokui et al., 2015).", "startOffset": 56, "endOffset": 76}, {"referenceID": 23, "context": "by following a conventional multi-condition training strategy (Vincent et al., 2016).", "startOffset": 62, "endOffset": 84}], "year": 2017, "abstractText": "The field of speech recognition is in the midst of a paradigm shift: end-to-end neural networks are challenging the dominance of hidden Markov models as a core technology. Using an attention mechanism in a recurrent encoder-decoder architecture solves the dynamic time alignment problem, allowing joint end-to-end training of the acoustic and language modeling components. In this paper we extend the end-to-end framework to encompass microphone array signal processing for noise suppression and speech enhancement within the acoustic encoding network. This allows the beamforming components to be optimized jointly within the recognition architecture to improve the end-to-end speech recognition objective. Experiments on the noisy speech benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system outperformed the attention-based baseline with input from a conventional adaptive beamformer.", "creator": "LaTeX with hyperref package"}}}