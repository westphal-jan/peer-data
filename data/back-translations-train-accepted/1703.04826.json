{"id": "1703.04826", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Mar-2017", "title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling", "abstract": "Semantic role labeling (SRL) is the task of identifying the predicate-argument structure of a sentence. It is typically regarded as an important step in the standard natural language processing pipeline, providing information to downstream tasks such as information extraction and question answering. As the semantic representations are closely related to syntactic ones, we exploit syntactic information in our model. We propose a version of graph convolutional networks (GCNs), a recent class of multilayer neural networks operating on graphs, suited to modeling syntactic dependency graphs. GCNs over syntactic dependency trees are used as sentence encoders, producing latent feature representations of words in a sentence and capturing information relevant to predicting the semantic representations. We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009) both for Chinese and English.", "histories": [["v1", "Tue, 14 Mar 2017 23:25:34 GMT  (431kb,D)", "http://arxiv.org/abs/1703.04826v1", null], ["v2", "Tue, 23 May 2017 09:47:59 GMT  (436kb,D)", "http://arxiv.org/abs/1703.04826v2", null], ["v3", "Wed, 24 May 2017 09:48:05 GMT  (437kb,D)", "http://arxiv.org/abs/1703.04826v3", null], ["v4", "Sun, 30 Jul 2017 17:24:38 GMT  (437kb,D)", "http://arxiv.org/abs/1703.04826v4", "To appear in EMNLP 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["diego marcheggiani", "ivan titov"], "accepted": true, "id": "1703.04826"}, "pdf": {"name": "1703.04826.pdf", "metadata": {"source": "CRF", "title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling", "authors": ["Diego Marcheggiani", "Ivan Titov"], "emails": ["marcheggiani@uva.nl", "titov@uva.nl"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive themselves, and they are able to survive themselves, \"he said in an interview with The New York Times, in which he addressed the\" New York Times \"and the\" New York Times. \""}, {"heading": "2 Graph Convolutional Networks", "text": "In this section, we will describe GCNs introduced by Kipf and Welling (2016). GCNs are neural networks based on graphs and inducing characteristics of nodes (i.e. real rated vectors / embeddings), based on characteristics of their neighborhoods. In Kipf and Welling (2016), they were shown very effectively for the node classification task: The classifier was estimated together with a GCN, so the induced node characteristics were informative for the node classification problem. Depending on how many layers of folding are used, GCNs can gather information only about immediate neighbors (with a layer of folding) or about arbitrary nodes at most k hops aways (when k layers are stacked on top of each other). Formally, we will consider an undirected graph G = (V, E), where V (| V | n) and E are sets of nodes and edges."}, {"heading": "3 Syntactic GCNs", "text": "Since GCNs are designed for undirected, unlabeled graphs, while syntactic dependency trees are steered and labeled (we call the dependency labels syntactical functions), we must first modify the calculation to include label information (Section 3.1). In the following section, we will integrate gates into GCNs so that the model can decide which edges are more relevant to the task in question. Having gates is also important as we can rely on automatically predicted syntactic representations and detect and weigh the gates for potentially faulty edges. We will also discuss the shortcomings of GCNs and how they are handled when GCNs are used with LSTMs (Section 3.3)."}, {"heading": "3.1 Incorporating directions and labels", "text": "Now we introduce a generalization of the GCNs that is suitable for directed labeled graphs, such as syntactic dependency trees. First, there is no reason to assume that information only flows along the syntactic dependency arcs (e.g. from tags to sequa), so we allow it to flow in the opposite direction as1 We refer the reader to Kipf and Welling (2016) for details and potential alternatives, including other normalization options. So we use a graph G = (V, E) in which the edge set contains all pairs of nodes (i.e. words) adjacent to the dependency tree. In our example, both (sequa, power) and (power, sequa) belong to the edge set. The graph is labeled, and the labeled synchronization L (u, v) contains both pieces of information about the syntactic function and indicates whether the edge in the same or opposite direction is called a syntactical dependency."}, {"heading": "3.2 Edge-wise gating", "text": "Both the uniform normalization, as in Kipf and Welling (2016), and the dropping of normalization (cv = 1), as we proposed in the previous section, are problematic. Apart from the fact that grade information is not properly captured (as we discussed above), uniform normalization would cause the activation of high-degree nodes to fizzle out, and these nodes (e.g. many of them are verbs central to SRL) carry important information. Overall, the uniform adoption of information from all adjacent nodes may not be suitable for SRL setting. Thus, in Figure 1, many semantic arcs only reflect their syntactic counterparts, so they may need to be upweighted. 3Chinese and English CoNLL 2009 datasets used 41 and 48 different syntactic functions, which would result in 83 and 97 different matrices in each layer, such as the syntactic form, being visible."}, {"heading": "3.3 Complementarity of GCNs and LSTMs", "text": "The inability of GCNs to grasp dependencies between distant nodes in the graph, together with the lack of shared parameters across folding layers, may seem like a serious problem, especially in the context of SRL: paths between predicates and arguments often contain many dependency arcs (Roth and Lapata, 2016). However, when graph convolution is executed across LSTM states (i.e. LSTM states serve as input xv = h (0) v to GCN) rather than as static word embeddings, GCN may not need to grasp more than a few hops. To explain this in more detail, let us speculate what role GCNs would play in combination with LSTMs, since LSTMs for SRL have already proven to be very effective (Zhou and Xu, 2015; Marcheggiani et al., 2017). Although LSTMs are able to grasp at least some degree of syntactic syntax with us, we will be able to grasp the 2016, without RM cases (13)."}, {"heading": "4 Syntax-Aware Neural SRL Encoder", "text": "In this thesis, we build our semantic predictive power on top of the syntax-agnostic LSTM-based SRL model by Marcheggiani et al. (2017), which already achieves state-of-the-art results on the English data set CoNLL-2009. Following this approach, we use the same bidirectional (BiLSTM) encoder and enrich it with a syntactical GCN.The CoNLL 2009 benchmark assumes that predicate positions are already marked in the test set (e.g. we would know that brands, repairs and motors in Figure 1 are predictors), so no predicate identification is required. As we focus exclusively on identifying arguments and their labeling with semantic roles, for predicate disambiguation (i.e. the marking makes as make.01) the laboratory work, we use a laboratory to disambiguate a Roth Word Lapet (2016, 2009, and B\u00c3 \u00bc kelet)."}, {"heading": "4.1 Word representations", "text": "We represent each word w as a concatenation of four vectors: 4 a randomly initialized word to embed xre-Rdw, a pre-trained word to embed xpos-Rdp, and a randomly initialized problem to embed xle-Rdl (active only if the word is a predicate). The randomly initialized embeddings xre, xpos, and xle are fine-tuned during the training, while the pre-trained ones are held firmly. The final word representation is given by x = xre-xpe-xpos-xle, representing the chain operator."}, {"heading": "4.2 Bidirectional LSTM layer", "text": "One of the most popular and effective methods of representing sequences such as sentences (Mikolov et al., 2010) is the use of recursive neural networks (RNN) (Elman, 1990). Specifically, their gated versions, Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Units (GRU) (Cho et al., 2014), have proven effective in modelling long sequences (Chiu and Nichols, 2016; Sutskever et al., 2014). Formally, an LSTM can be defined as a function LSTM (x1: i) that inputs the sequence x1: i and returns a hidden state hi Rdh. This state can be considered as a representation of the sentence from beginning to position i, or, in other words, it encodes the word LSTM at position i together with its left context. However, the right context is also important, so that M-bidirectional layer STM (STM) forms another STM (STM) layer."}, {"heading": "4.3 Graph convolutional layer", "text": "The representation calculated with the BiLSTM encoder is fed as input into a GCN of the form defined in Equation (5). The adjacent nodes of a node v, namely N (v), are predicted by an external syntactic dependency parser. GCNs allow us to easily integrate syntactic information into our neural SRL model and provide an effective and fast method."}, {"heading": "4.4 Semantic role classifier", "text": "The classifier predicts semantic roles of words associated with the predicate, while relying on word representations provided by the GCN; we concatenate hidden states of the candidate's argument and the predicate word, and use them as input to a classifier (Figure 3, above).The Softmax classifier calculates the probability of the role (including special \"NULL\" roles, encoding that the word is not an argument of the predicate): p (r | ti, tp, l) and exp (Wl, r (ti, tp)), (6) where ti and tp are representations generated by the graph's revolutionary encoder, l is the problem of the predicate p and the symbol means proportionality. 5 Like FitzGerald et al. (2015) and Marchgiani et al. (2017), where ti and tp are representations, instead of a fixed predicate, we base them on a matrix or qr that we simply use."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Datasets and parameters", "text": "We tested the proposed SRL model based on the English and Chinese CoNLL 2009 dataset with standard differences in training, testing and development sets. For English, we used external embeddings from Dyer et al. (2015), which we learned using the structured approach from Ling et al. (2015). For Chinese, we used external embeddings created using the neural language model from Bengio et al. (2003). As in (Kiperwasser and Goldberg, 2016), we applied word suspensions at the word representation level (Iyyer et al., 2015): a word is misused by a special unknown5We misuse notation and define both the predicate word and its position in the sentence by p. Token UNK with probability \u03b1fr (w) + \u03b1, where \u03b1 is a hyperparameter and fr (w) the frequency of the word w."}, {"heading": "5.2 Results and discussion", "text": "To show that GCN layers are effective, we first compare our model against its version, which has multiple layers of the GCN. Importantly, we measure the actual contribution of the GCNs by first comparing this syntax agnostic model (e.g. the number of LSTM layers) to achieve the best possible performance on the basis of constellation. 6We compare the syntax agnostic model with the syntax-conscious person we rely on with a layer of graph convolution via syntax (j = 1) and with two layers of graph constellation (j = 2), since we rely on the same way as disambiguator does for all versions of the model, in Table 2 and 3, which we use only the results of SRL disambiguation."}, {"heading": "5.3 Analysis of syntactic GCNs", "text": "In this section, we conduct an ablation study of the English CoNLL 2009 development set; we discuss the effects of the predicted syntax and the behavior of various GCN architectures; Table 7 clearly shows that the quality of the paraparser is reflected in performance gains from the use of GCNs. While the use of the predicted syntax resulted in a 0.6% improvement based on the gold standard syntax (i.e. \"perfectly correct\"), the syntax resulted in a further 3.1% improvement. However, to confirm this hypothesis, we first reduce the number of BiLSTM layers to 1 (k = 1) and then even replace it with a fully connected nonlinear layer (k = 0)."}, {"heading": "6 Related Work", "text": "In fact, it is that we see ourselves as being able to assert ourselves, that we are able, that we are able, that we are in a position, that we are in a position."}, {"heading": "7 Conclusions and Future Work", "text": "We demonstrated how GCNs can be used to integrate syntactical information into neural models and specifically to construct a syntax-conscious SRL model, resulting in state-of-the-art results for Chinese and English. There are relatively simple steps that can further improve SRL results. For example, we have relied independently on the labeling of arguments, while the use of a common model is likely to significantly improve performance. In addition, in this paper we consider the dependency version of the SRL task, although the model can be generalized to the span-based version (i.e. relaxing arguments with rollers instead of syntactical argument heads), relatively straightforward. More generally, given the simplicity of GCNs and their applicability to general diagram structures (not necessarily trees), we believe that there are many NLP tasks where GCNs can be used to integrate linguistic structures (e.g., syntactical and manual references to diagram structures and discurse documents)."}, {"heading": "Acknowledgements", "text": "We thank Anton Frolov, Michael Schlichtkrull, Thomas Kipf, Michael Roth, Max Welling and Wilker Aziz for their suggestions and comments. The project was supported by the European Research Council (ERC StG BroadSem 678254), the Dutch National Science Foundation (NWO VIDI 639.022.518) and an Amazon Web Services (AWS) grant."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Multilingual semantic role labeling", "author": ["Anders Bj\u00f6rkelund", "Love Hafdell", "Pierre Nugues."], "venue": "Proceedings of CoNLL Shared Task.", "citeRegEx": "Bj\u00f6rkelund et al\\.,? 2009", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2009}, {"title": "A high-performance syntactic and semantic dependency parser", "author": ["Anders Bj\u00f6rkelund", "Bernd Bohnet", "Love Hafdell", "Pierre Nugues."], "venue": "Proceedings of COLING: Demonstrations.", "citeRegEx": "Bj\u00f6rkelund et al\\.,? 2010", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2010}, {"title": "Named entity recognition with bidirectional LSTM-CNNs", "author": ["Jason P.C. Chiu", "Eric Nichols."], "venue": "TACL, 4:357\u2013370.", "citeRegEx": "Chiu and Nichols.,? 2016", "shortCiteRegEx": "Chiu and Nichols.", "year": 2016}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Language modeling with gated convolutional networks", "author": ["Yann N. Dauphin", "Angela Fan", "Michael Auli", "David Grangier."], "venue": "CoRR, abs/1612.08083.", "citeRegEx": "Dauphin et al\\.,? 2016", "shortCiteRegEx": "Dauphin et al\\.", "year": 2016}, {"title": "Convolutional networks on graphs for learning molecular fingerprints", "author": ["David K Duvenaud", "Dougal Maclaurin", "Jorge Iparraguirre", "Rafael Bombarell", "Timothy Hirzel", "Alan Aspuru-Guzik", "Ryan P Adams."], "venue": "NIPS.", "citeRegEx": "Duvenaud et al\\.,? 2015", "shortCiteRegEx": "Duvenaud et al\\.", "year": 2015}, {"title": "Transitionbased dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proceedings of ACL.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman."], "venue": "Cognitive Science, 14(2):179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Learning to parse and translate improves neural machine translation", "author": ["Akiko Eriguchi", "Yoshimasa Tsuruoka", "Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1702.03525.", "citeRegEx": "Eriguchi et al\\.,? 2017", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2017}, {"title": "Semantic role labeling with neural network factors", "author": ["Nicholas FitzGerald", "Oscar T\u00e4ckstr\u00f6m", "Kuzman Ganchev", "Dipanjan Das."], "venue": "Proceedings of EMNLP.", "citeRegEx": "FitzGerald et al\\.,? 2015", "shortCiteRegEx": "FitzGerald et al\\.", "year": 2015}, {"title": "Dependencybased semantic role labeling using convolutional neural networks", "author": ["William Foland", "James Martin."], "venue": "Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics.", "citeRegEx": "Foland and Martin.,? 2015", "shortCiteRegEx": "Foland and Martin.", "year": 2015}, {"title": "Latent variable model of synchronous syntactic-semantic parsing for multiple languages", "author": ["Andrea Gesmundo", "James Henderson", "Paola Merlo", "Ivan Titov."], "venue": "Proceedings of CoNLL 2009 Shared Task.", "citeRegEx": "Gesmundo et al\\.,? 2009", "shortCiteRegEx": "Gesmundo et al\\.", "year": 2009}, {"title": "Automatic labeling of semantic roles", "author": ["Daniel Gildea", "Daniel Jurafsky."], "venue": "Computational linguistics, 28(3):245\u2013288.", "citeRegEx": "Gildea and Jurafsky.,? 2002", "shortCiteRegEx": "Gildea and Jurafsky.", "year": 2002}, {"title": "Supervised sequence labelling with recurrent neural networks", "author": ["Alex Graves."], "venue": "Ph.D. thesis, M\u00fcnchen, Techn. Univ., Diss., 2008.", "citeRegEx": "Graves.,? 2008", "shortCiteRegEx": "Graves.", "year": 2008}, {"title": "A latent variable model of synchronous parsing for syntactic and semantic dependencies", "author": ["James Henderson", "Paola Merlo", "Gabriele Musillo", "Ivan Titov."], "venue": "Proceedings of CoNLL Shared Task.", "citeRegEx": "Henderson et al\\.,? 2008", "shortCiteRegEx": "Henderson et al\\.", "year": 2008}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III."], "venue": "Proceedings of ACL.", "citeRegEx": "Iyyer et al\\.,? 2015", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "The effect of syntactic representation on semantic role labeling", "author": ["Richard Johansson", "Pierre Nugues."], "venue": "COLING 2008, 22nd International Conference on Computational Linguistics, Proceedings of the Conference, 18-22 August 2008, Manchester, UK, pages", "citeRegEx": "Johansson and Nugues.,? 2008", "shortCiteRegEx": "Johansson and Nugues.", "year": 2008}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proceedings of ICLR.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Simple and accurate dependency parsing using bidirectional lstm feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "Transactions of the Association for Computational Linguistics, 4:313\u2013327.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Semisupervised classification with graph convolutional networks", "author": ["Thomas N. Kipf", "Max Welling."], "venue": "CoRR, abs/1609.02907.", "citeRegEx": "Kipf and Welling.,? 2016", "shortCiteRegEx": "Kipf and Welling.", "year": 2016}, {"title": "The insideoutside recursive neural network model for dependency parsing", "author": ["Phong Le", "Willem Zuidema."], "venue": "EMNLP.", "citeRegEx": "Le and Zuidema.,? 2014", "shortCiteRegEx": "Le and Zuidema.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner."], "venue": "Intelligent Signal Processing, pages 306\u2013351. IEEE Press.", "citeRegEx": "LeCun et al\\.,? 2001", "shortCiteRegEx": "LeCun et al\\.", "year": 2001}, {"title": "High-order lowrank tensors for semantic role labeling", "author": ["Tao Lei", "Yuan Zhang", "Llu\u0131\u0301s M\u00e0rquez", "Alessandro Moschitti", "Regina Barzilay"], "venue": "In Proceedings of NAACL", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "English verb classes and alternations: A preliminary investigation", "author": ["Beth Levin."], "venue": "University of Chicago press.", "citeRegEx": "Levin.,? 1993", "shortCiteRegEx": "Levin.", "year": 1993}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso."], "venue": "NAACL.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Assessing the ability of lstms to learn syntaxsensitive dependencies", "author": ["Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg."], "venue": "TACL, 4:521\u2013535.", "citeRegEx": "Linzen et al\\.,? 2016", "shortCiteRegEx": "Linzen et al\\.", "year": 2016}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "CoRR, abs/1511.06114.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "A simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling", "author": ["Diego Marcheggiani", "Anton Frolov", "Ivan Titov."], "venue": "CoRR, abs/1701.02593.", "citeRegEx": "Marcheggiani et al\\.,? 2017", "shortCiteRegEx": "Marcheggiani et al\\.", "year": 2017}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur."], "venue": "Proceedings of INTERSPEECH.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Tree kernels for semantic role labeling", "author": ["Alessandro Moschitti", "Daniele Pighin", "Roberto Basili."], "venue": "Computational Linguistics, 34(2):193\u2013224.", "citeRegEx": "Moschitti et al\\.,? 2008", "shortCiteRegEx": "Moschitti et al\\.", "year": 2008}, {"title": "Discriminative neural sentence modeling by tree-based convolution", "author": ["Lili Mou", "Hao Peng", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Mou et al\\.,? 2015", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "Column networks for collective classification", "author": ["Trang Pham", "Truyen Tran", "Dinh Q. Phung", "Svetha Venkatesh."], "venue": "Proceedings AAAI.", "citeRegEx": "Pham et al\\.,? 2017", "shortCiteRegEx": "Pham et al\\.", "year": 2017}, {"title": "Semantic role chunking combining complementary syntactic views", "author": ["Sameer Pradhan", "Kadri Hacioglu", "Wayne H. Ward", "James H. Martin", "Daniel Jurafsky."], "venue": "Proceedings of CoNLL.", "citeRegEx": "Pradhan et al\\.,? 2005", "shortCiteRegEx": "Pradhan et al\\.", "year": 2005}, {"title": "The importance of syntactic parsing and inference", "author": ["Vasin Punyakanok", "Dan Roth", "Wen-tau Yih"], "venue": null, "citeRegEx": "Punyakanok et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2008}, {"title": "Neural semantic role labeling with dependency path embeddings", "author": ["Michael Roth", "Mirella Lapata."], "venue": "Proceedings of ACL.", "citeRegEx": "Roth and Lapata.,? 2016", "shortCiteRegEx": "Roth and Lapata.", "year": 2016}, {"title": "Linguistic input features improve neural machine translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "Proceedings of WMT.", "citeRegEx": "Sennrich and Haddow.,? 2016", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2016}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Greedy, joint syntacticsemantic parsing with stack LSTMs", "author": ["Swabha Swayamdipta", "Miguel Ballesteros", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of CoNLL.", "citeRegEx": "Swayamdipta et al\\.,? 2016", "shortCiteRegEx": "Swayamdipta et al\\.", "year": 2016}, {"title": "A generative model for semantic role labeling", "author": ["Cynthia A. Thompson", "Roger Levy", "Christopher D. Manning."], "venue": "Proceedings of ECML.", "citeRegEx": "Thompson et al\\.,? 2003", "shortCiteRegEx": "Thompson et al\\.", "year": 2003}, {"title": "Online projectivisation for synchronous parsing of semantic and syntactic dependencies", "author": ["Ivan Titov", "James Henderson", "Paola Merlo", "Gabriele Musillo."], "venue": "Proceedings of IJCAI.", "citeRegEx": "Titov et al\\.,? 2009", "shortCiteRegEx": "Titov et al\\.", "year": 2009}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["A\u00e4ron van den Oord", "Nal Kalchbrenner", "Lasse Espeholt", "Koray Kavukcuoglu", "Oriol Vinyals", "Alex Graves."], "venue": "NIPS.", "citeRegEx": "Oord et al\\.,? 2016", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Multilingual dependency learning: Exploiting rich features for tagging syntactic and semantic dependencies", "author": ["Hai Zhao", "Wenliang Chen", "Jun\u2019ichi Kazama", "Kiyotaka Uchimoto", "Kentaro Torisawa"], "venue": "Proceedings of CoNLL", "citeRegEx": "Zhao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2009}, {"title": "End-to-end learning of semantic role labeling using recurrent neural networks", "author": ["Jie Zhou", "Wei Xu."], "venue": "Proceedings of ACL.", "citeRegEx": "Zhou and Xu.,? 2015", "shortCiteRegEx": "Zhou and Xu.", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": "Semantic role labeling (SRL) (Gildea and Jurafsky, 2002) can be informally described as the task of discovering who did what to whom.", "startOffset": 29, "endOffset": 56}, {"referenceID": 25, "context": "The semantic representations are closely related to syntactic ones, even though the syntax-semantics interface is far from trivial (Levin, 1993).", "startOffset": 131, "endOffset": 144}, {"referenceID": 42, "context": "Though historically most SRL approaches did rely on syntax (Thompson et al., 2003; Pradhan et al., 2005; Punyakanok et al., 2008; Johansson and Nugues, 2008), the last generation of SRL models put syntax aside in favour of neural sequence modar X iv :1 70 3.", "startOffset": 59, "endOffset": 157}, {"referenceID": 35, "context": "Though historically most SRL approaches did rely on syntax (Thompson et al., 2003; Pradhan et al., 2005; Punyakanok et al., 2008; Johansson and Nugues, 2008), the last generation of SRL models put syntax aside in favour of neural sequence modar X iv :1 70 3.", "startOffset": 59, "endOffset": 157}, {"referenceID": 36, "context": "Though historically most SRL approaches did rely on syntax (Thompson et al., 2003; Pradhan et al., 2005; Punyakanok et al., 2008; Johansson and Nugues, 2008), the last generation of SRL models put syntax aside in favour of neural sequence modar X iv :1 70 3.", "startOffset": 59, "endOffset": 157}, {"referenceID": 18, "context": "Though historically most SRL approaches did rely on syntax (Thompson et al., 2003; Pradhan et al., 2005; Punyakanok et al., 2008; Johansson and Nugues, 2008), the last generation of SRL models put syntax aside in favour of neural sequence modar X iv :1 70 3.", "startOffset": 59, "endOffset": 157}, {"referenceID": 46, "context": "els, namely LSTMs (Zhou and Xu, 2015; Marcheggiani et al., 2017), and outperformed syntacticallydriven methods on standard benchmarks.", "startOffset": 18, "endOffset": 64}, {"referenceID": 29, "context": "els, namely LSTMs (Zhou and Xu, 2015; Marcheggiani et al., 2017), and outperformed syntacticallydriven methods on standard benchmarks.", "startOffset": 18, "endOffset": 64}, {"referenceID": 6, "context": "Specifically, we rely on graph convolutional networks (GCNs) (Duvenaud et al., 2015; Kipf and Welling, 2016), a recent class of multilayer neural networks operating on graphs.", "startOffset": 61, "endOffset": 108}, {"referenceID": 21, "context": "Specifically, we rely on graph convolutional networks (GCNs) (Duvenaud et al., 2015; Kipf and Welling, 2016), a recent class of multilayer neural networks operating on graphs.", "startOffset": 61, "endOffset": 108}, {"referenceID": 6, "context": "Specifically, we rely on graph convolutional networks (GCNs) (Duvenaud et al., 2015; Kipf and Welling, 2016), a recent class of multilayer neural networks operating on graphs. For every node in the graph (in our case a word in a sentence), GCN encodes relevant information about its neighborhood as a real-valued feature vector. GCNs have been studied largely in the context of undirected unlabeled graphs and we are not aware of any previous application of GCNs to NLP. We introduce a generalization of GCNs of Kipf and Welling (2016) applicable to labeled directed graphs, as necessary for modeling syntactic dependency structures.", "startOffset": 62, "endOffset": 536}, {"referenceID": 8, "context": "This contrasts with recurrent and recursive neural networks (Elman, 1990; Socher et al., 2013) which, at least in theory, can capture statistical dependencies across unbounded paths in a trees or in a sequence.", "startOffset": 60, "endOffset": 94}, {"referenceID": 39, "context": "This contrasts with recurrent and recursive neural networks (Elman, 1990; Socher et al., 2013) which, at least in theory, can capture statistical dependencies across unbounded paths in a trees or in a sequence.", "startOffset": 60, "endOffset": 94}, {"referenceID": 21, "context": "In this section we describe GCNs introduced by Kipf and Welling (2016). GCNs are neural networks operating on graphs and inducing features of nodes (i.", "startOffset": 47, "endOffset": 71}, {"referenceID": 21, "context": "In this section we describe GCNs introduced by Kipf and Welling (2016). GCNs are neural networks operating on graphs and inducing features of nodes (i.e., real-valued vectors / embeddings) based on properties of their neighborhoods. In Kipf and Welling (2016), they were shown very effective for the node classification task: the classifier was estimated jointly with a GCN, so that the induced node features were informative for the node classification problem.", "startOffset": 47, "endOffset": 260}, {"referenceID": 21, "context": "In this section we describe GCNs introduced by Kipf and Welling (2016). GCNs are neural networks operating on graphs and inducing features of nodes (i.e., real-valued vectors / embeddings) based on properties of their neighborhoods. In Kipf and Welling (2016), they were shown very effective for the node classification task: the classifier was estimated jointly with a GCN, so that the induced node features were informative for the node classification problem. Depending on how many layers of convolution are used, GCNs can capture information only about immediate neighbors (with one layer of convolution) or any nodes at most k hops aways (if k layers are stacked on top of each other). More formally, consider an undirected graph G = (V, E), where V (|V | = n) and E are sets of nodes and edges, respectively. Kipf and Welling (2016) assume that edges contain all the self-loops, i.", "startOffset": 47, "endOffset": 839}, {"referenceID": 23, "context": "As in standard convolutional networks (LeCun et al., 2001), by stacking GCN layers one can incorporate higher degree neighborhoods:", "startOffset": 38, "endOffset": 58}, {"referenceID": 21, "context": "We refer the reader to Kipf and Welling (2016) for details and potential alternatives, including other normalization choices.", "startOffset": 23, "endOffset": 47}, {"referenceID": 21, "context": "Both uniform normalization, as in Kipf and Welling (2016), and dropping normalization (cv = 1), as we suggested in the preceding section, are problematic.", "startOffset": 34, "endOffset": 58}, {"referenceID": 5, "context": "In order to address the above issues, inspired by recent literature (van den Oord et al., 2016; Dauphin et al., 2016), we calculate for each edge node pair a scalar gate of the form", "startOffset": 68, "endOffset": 117}, {"referenceID": 37, "context": "The inability of GCNs to capture dependencies between nodes far away from each other in the graph, together with no parameter sharing across convolution layers, may seem like a serious problem, especially in the context of SRL: paths between predicates and arguments often include many dependency arcs (Roth and Lapata, 2016).", "startOffset": 302, "endOffset": 325}, {"referenceID": 46, "context": "To elaborate on this, let us speculate what role GCNs would play when used in combinations with LSTMs, given that LSTMs have already been shown very effective for SRL (Zhou and Xu, 2015; Marcheggiani et al., 2017).", "startOffset": 167, "endOffset": 213}, {"referenceID": 29, "context": "To elaborate on this, let us speculate what role GCNs would play when used in combinations with LSTMs, given that LSTMs have already been shown very effective for SRL (Zhou and Xu, 2015; Marcheggiani et al., 2017).", "startOffset": 167, "endOffset": 213}, {"referenceID": 27, "context": "Though LSTMs are capable of capturing at least some degree of syntax (Linzen et al., 2016) without explicit syntactic", "startOffset": 69, "endOffset": 90}, {"referenceID": 37, "context": "01) we use of an offthe-shelf disambiguation model (Roth and Lapata, 2016; Bj\u00f6rkelund et al., 2009).", "startOffset": 51, "endOffset": 99}, {"referenceID": 1, "context": "01) we use of an offthe-shelf disambiguation model (Roth and Lapata, 2016; Bj\u00f6rkelund et al., 2009).", "startOffset": 51, "endOffset": 99}, {"referenceID": 27, "context": "In this work, we build our semantic role labeler on top of the syntax-agnostic LSTM-based SRL model of Marcheggiani et al. (2017), which already achieves state-of-the-art results on the CoNLL-2009 English dataset.", "startOffset": 103, "endOffset": 130}, {"referenceID": 1, "context": "01) we use of an offthe-shelf disambiguation model (Roth and Lapata, 2016; Bj\u00f6rkelund et al., 2009). As in Marcheggiani et al. (2017) and in most previous work, we process individual predicates in isolation, so for each predicate, our tasks reduces to a sequence labeling problem.", "startOffset": 75, "endOffset": 134}, {"referenceID": 31, "context": "One of the most popular and effective ways to represent sequences, such as sentences (Mikolov et al., 2010), is to use recurrent neural networks (RNN) (Elman, 1990).", "startOffset": 85, "endOffset": 107}, {"referenceID": 8, "context": ", 2010), is to use recurrent neural networks (RNN) (Elman, 1990).", "startOffset": 51, "endOffset": 64}, {"referenceID": 16, "context": "In particular their gated versions, Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Units (GRU) (Cho et al.", "startOffset": 75, "endOffset": 109}, {"referenceID": 4, "context": "In particular their gated versions, Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Units (GRU) (Cho et al., 2014), have proven effective in modeling long sequences (Chiu and Nichols, 2016; Sutskever et al.", "startOffset": 142, "endOffset": 160}, {"referenceID": 3, "context": ", 2014), have proven effective in modeling long sequences (Chiu and Nichols, 2016; Sutskever et al., 2014).", "startOffset": 58, "endOffset": 106}, {"referenceID": 40, "context": ", 2014), have proven effective in modeling long sequences (Chiu and Nichols, 2016; Sutskever et al., 2014).", "startOffset": 58, "endOffset": 106}, {"referenceID": 14, "context": "However, the right context is also important, so Bidirectional LSTMs (Graves, 2008) use two LSTMs: one for the forward pass, and another for the backward pass, LSTMF and LSTMB , respectively.", "startOffset": 69, "endOffset": 83}, {"referenceID": 14, "context": "However, the right context is also important, so Bidirectional LSTMs (Graves, 2008) use two LSTMs: one for the forward pass, and another for the backward pass, LSTMF and LSTMB , respectively. By concatenating the states of both LSTMs, we create a complete context-aware representation of a word BiLSTM(x1:n, i) = LSTMF (x1:i) \u25e6 LSTMB(xn:i). We follow Marcheggiani et al. (2017) and stack k layers of bidirectional LSTMs, where each layer takes the lower layer as its input.", "startOffset": 70, "endOffset": 378}, {"referenceID": 10, "context": "5 As FitzGerald et al. (2015) and Marcheggiani et al.", "startOffset": 5, "endOffset": 30}, {"referenceID": 10, "context": "5 As FitzGerald et al. (2015) and Marcheggiani et al. (2017), instead of using a fixed matrix Wl,r or simply assuming that Wl,r = Wr, we jointly embed the role r and predicate lemma l using a non-linear transformation:", "startOffset": 5, "endOffset": 61}, {"referenceID": 20, "context": "As in (Kiperwasser and Goldberg, 2016), we applied word dropout at the word representation level (Iyyer et al.", "startOffset": 6, "endOffset": 38}, {"referenceID": 17, "context": "As in (Kiperwasser and Goldberg, 2016), we applied word dropout at the word representation level (Iyyer et al., 2015): a word is replaced with a special unknown", "startOffset": 97, "endOffset": 117}, {"referenceID": 6, "context": "For English, we used external embeddings of Dyer et al. (2015), learned using the structured skip ngram approach of Ling et al.", "startOffset": 44, "endOffset": 63}, {"referenceID": 6, "context": "For English, we used external embeddings of Dyer et al. (2015), learned using the structured skip ngram approach of Ling et al. (2015). For Chinese we used external embeddings produced with the neural language model of Bengio et al.", "startOffset": 44, "endOffset": 135}, {"referenceID": 0, "context": "For Chinese we used external embeddings produced with the neural language model of Bengio et al. (2003). As in (Kiperwasser and Goldberg, 2016), we applied word dropout at the word representation level (Iyyer et al.", "startOffset": 83, "endOffset": 104}, {"referenceID": 20, "context": "We parsed English sentences with the BIST Parser (Kiperwasser and Goldberg, 2016), whereas for Chinese we used automatically predicted parses provided by the CoNLL-2009 shared-task organizers.", "startOffset": 49, "endOffset": 81}, {"referenceID": 34, "context": "For the predicate disambiguator we used the ones from Roth and Lapata (2016) for English and from Bj\u00f6rkelund et al.", "startOffset": 54, "endOffset": 77}, {"referenceID": 1, "context": "For the predicate disambiguator we used the ones from Roth and Lapata (2016) for English and from Bj\u00f6rkelund et al. (2009) for Chinese.", "startOffset": 98, "endOffset": 123}, {"referenceID": 19, "context": "Adam (Kingma and Ba, 2015) was used as an optimizer.", "startOffset": 5, "endOffset": 26}, {"referenceID": 10, "context": "6 FitzGerald et al. (2015) (local) - - 86.", "startOffset": 2, "endOffset": 27}, {"referenceID": 29, "context": "7 Marcheggiani et al. (2017) (local) 88.", "startOffset": 2, "endOffset": 29}, {"referenceID": 1, "context": "7 Bj\u00f6rkelund et al. (2009) (global) 82.", "startOffset": 2, "endOffset": 27}, {"referenceID": 1, "context": "7 Bj\u00f6rkelund et al. (2009) (global) 82.4 75.1 78.6 Roth and Lapata (2016) (global) 83.", "startOffset": 2, "endOffset": 74}, {"referenceID": 37, "context": "When we study the Chinese results (Table 5), we can see that our best model outperforms the stateof-the-art model of Roth and Lapata (2016) by even larger margin of 3.", "startOffset": 117, "endOffset": 140}, {"referenceID": 10, "context": "As seen in Table 4, labelers of FitzGerald et al. (2015) and Roth and Lapata (2016) gained 0.", "startOffset": 32, "endOffset": 57}, {"referenceID": 10, "context": "As seen in Table 4, labelers of FitzGerald et al. (2015) and Roth and Lapata (2016) gained 0.", "startOffset": 32, "endOffset": 84}, {"referenceID": 30, "context": "The training portion of the English CoNLL data (originating from the WSJ portion of Penn Treebank (Marcus et al., 1993)) consists of newswire, whereas the out-of-domain test set (based on the Brown corpus portion) contains such genres as fiction and humor.", "startOffset": 98, "endOffset": 119}, {"referenceID": 29, "context": "As expected though, our model does not outperform the syntax-agnostic model of Marcheggiani et al. (2017). Nevertheless, the ensemble version obtains achieves a substantial improvement over the best previous approach (+1.", "startOffset": 79, "endOffset": 106}, {"referenceID": 10, "context": "6 FitzGerald et al. (2015) (local) - - 75.", "startOffset": 2, "endOffset": 27}, {"referenceID": 10, "context": "6 FitzGerald et al. (2015) (local) - - 75.2 Roth and Lapata (2016) (local) 76.", "startOffset": 2, "endOffset": 67}, {"referenceID": 10, "context": "6 FitzGerald et al. (2015) (local) - - 75.2 Roth and Lapata (2016) (local) 76.9 73.8 75.3 Marcheggiani et al. (2017) (local) 78.", "startOffset": 2, "endOffset": 117}, {"referenceID": 21, "context": "Still, when combining LSTMs and GCNs, we do not observe improvements from deep GCNs, unlike the nodel labeled task considered in Kipf and Welling (2016). We hypothesized that this has to do with the expressive power of BiLSTMs.", "startOffset": 129, "endOffset": 153}, {"referenceID": 15, "context": "Perhaps the earliest methods modeling syntaxsemantics interface with RNNs are due to (Henderson et al., 2008; Titov et al., 2009; Gesmundo et al., 2009), they used shift-reduce parsers for joint SRL and syntactic parsing, and relied on RNNs to model statistical dependencies across syntactic and semantic parsing actions.", "startOffset": 85, "endOffset": 152}, {"referenceID": 43, "context": "Perhaps the earliest methods modeling syntaxsemantics interface with RNNs are due to (Henderson et al., 2008; Titov et al., 2009; Gesmundo et al., 2009), they used shift-reduce parsers for joint SRL and syntactic parsing, and relied on RNNs to model statistical dependencies across syntactic and semantic parsing actions.", "startOffset": 85, "endOffset": 152}, {"referenceID": 12, "context": "Perhaps the earliest methods modeling syntaxsemantics interface with RNNs are due to (Henderson et al., 2008; Titov et al., 2009; Gesmundo et al., 2009), they used shift-reduce parsers for joint SRL and syntactic parsing, and relied on RNNs to model statistical dependencies across syntactic and semantic parsing actions.", "startOffset": 85, "endOffset": 152}, {"referenceID": 32, "context": "integrating syntax while minimizing the amount of feature engineering), focused on tree kernels and their applications to SRL (Moschitti et al., 2008).", "startOffset": 126, "endOffset": 150}, {"referenceID": 10, "context": ", 2009; Gesmundo et al., 2009), they used shift-reduce parsers for joint SRL and syntactic parsing, and relied on RNNs to model statistical dependencies across syntactic and semantic parsing actions. A more modern (e.g., based on LSTMs) and effective reincarnation of this line of research has been proposed in Swayamdipta et al. (2016). Other recent work which considered incorporation of syntactic information in neural SRL models include: FitzGerald et al.", "startOffset": 8, "endOffset": 337}, {"referenceID": 10, "context": "Other recent work which considered incorporation of syntactic information in neural SRL models include: FitzGerald et al. (2015) who use standard syntactic features within an MLP calculating potentials of a CRF model; Roth and Lapata (2016) who enriched standard features for SRL with LSTM representations of syntactic paths between arguments and predicates; Lei et al.", "startOffset": 104, "endOffset": 129}, {"referenceID": 10, "context": "Other recent work which considered incorporation of syntactic information in neural SRL models include: FitzGerald et al. (2015) who use standard syntactic features within an MLP calculating potentials of a CRF model; Roth and Lapata (2016) who enriched standard features for SRL with LSTM representations of syntactic paths between arguments and predicates; Lei et al.", "startOffset": 104, "endOffset": 241}, {"referenceID": 10, "context": "Other recent work which considered incorporation of syntactic information in neural SRL models include: FitzGerald et al. (2015) who use standard syntactic features within an MLP calculating potentials of a CRF model; Roth and Lapata (2016) who enriched standard features for SRL with LSTM representations of syntactic paths between arguments and predicates; Lei et al. (2015) who relied on low-rank tensor factorizations for modeling syntax.", "startOffset": 104, "endOffset": 377}, {"referenceID": 10, "context": "Other recent work which considered incorporation of syntactic information in neural SRL models include: FitzGerald et al. (2015) who use standard syntactic features within an MLP calculating potentials of a CRF model; Roth and Lapata (2016) who enriched standard features for SRL with LSTM representations of syntactic paths between arguments and predicates; Lei et al. (2015) who relied on low-rank tensor factorizations for modeling syntax. Also Foland and Martin (2015) used (nongraph) convolutional neural networks and provided syntactic features (e.", "startOffset": 104, "endOffset": 473}, {"referenceID": 28, "context": "Beyond SRL, there have been many proposals on how to incorporate syntactic information in RNN models, for example, in the context of neural machine translation (Luong et al., 2015; Eriguchi et al., 2017; Sennrich and Haddow, 2016).", "startOffset": 160, "endOffset": 230}, {"referenceID": 9, "context": "Beyond SRL, there have been many proposals on how to incorporate syntactic information in RNN models, for example, in the context of neural machine translation (Luong et al., 2015; Eriguchi et al., 2017; Sennrich and Haddow, 2016).", "startOffset": 160, "endOffset": 230}, {"referenceID": 38, "context": "Beyond SRL, there have been many proposals on how to incorporate syntactic information in RNN models, for example, in the context of neural machine translation (Luong et al., 2015; Eriguchi et al., 2017; Sennrich and Haddow, 2016).", "startOffset": 160, "endOffset": 230}, {"referenceID": 39, "context": "the most popular and attractive approaches is to use tree-structured recursive neural networks (Socher et al., 2013), which, though originally introduced for constituent syntax, can also be applied in the dependency parsing context (Le and Zuidema, 2014; Dyer et al.", "startOffset": 95, "endOffset": 116}, {"referenceID": 22, "context": ", 2013), which, though originally introduced for constituent syntax, can also be applied in the dependency parsing context (Le and Zuidema, 2014; Dyer et al., 2015).", "startOffset": 123, "endOffset": 164}, {"referenceID": 7, "context": ", 2013), which, though originally introduced for constituent syntax, can also be applied in the dependency parsing context (Le and Zuidema, 2014; Dyer et al., 2015).", "startOffset": 123, "endOffset": 164}, {"referenceID": 6, "context": "(2015) to sentiment analysis and question classification, introduced even before GCNs became popular in the machine learning community (Duvenaud et al., 2015; Kipf and Welling, 2016), is related to graph convolution.", "startOffset": 135, "endOffset": 182}, {"referenceID": 21, "context": "(2015) to sentiment analysis and question classification, introduced even before GCNs became popular in the machine learning community (Duvenaud et al., 2015; Kipf and Welling, 2016), is related to graph convolution.", "startOffset": 135, "endOffset": 182}, {"referenceID": 6, "context": ", 2013), which, though originally introduced for constituent syntax, can also be applied in the dependency parsing context (Le and Zuidema, 2014; Dyer et al., 2015). An approach of Mou et al. (2015) to sentiment analysis and question classification, introduced even before GCNs became popular in the machine learning community (Duvenaud et al.", "startOffset": 146, "endOffset": 199}, {"referenceID": 34, "context": "Though most previous work have focused on undirected unlabeled graphs, the model of Pham et al. (2017) used directed graphs for the node classification task.", "startOffset": 84, "endOffset": 103}], "year": 2017, "abstractText": "Semantic role labeling (SRL) is the task of identifying the predicate-argument structure of a sentence. It is typically regarded as an important step in the standard natural language processing pipeline, providing information to downstream tasks such as information extraction and question answering. As the semantic representations are closely related to syntactic ones, we exploit syntactic information in our model. We propose a version of graph convolutional networks (GCNs), a recent class of multilayer neural networks operating on graphs, suited to modeling syntactic dependency graphs. GCNs over syntactic dependency trees are used as sentence encoders, producing latent feature representations of words in a sentence and capturing information relevant to predicting the semantic representations. We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already stateof-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009) both for Chinese and English.", "creator": "LaTeX with hyperref package"}}}