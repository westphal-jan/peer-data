{"id": "1703.04933", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "Sharp Minima Can Generalize For Deep Nets", "abstract": "Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter &amp; Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.", "histories": [["v1", "Wed, 15 Mar 2017 05:12:25 GMT  (147kb,D)", "http://arxiv.org/abs/1703.04933v1", "8.5 pages of main content, 2.5 of bibliography and 1 page of appendix"], ["v2", "Mon, 15 May 2017 23:33:19 GMT  (148kb,D)", "http://arxiv.org/abs/1703.04933v2", "8.5 pages of main content, 2.5 of bibliography and 1 page of appendix"]], "COMMENTS": "8.5 pages of main content, 2.5 of bibliography and 1 page of appendix", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["laurent dinh", "razvan pascanu", "samy bengio", "yoshua bengio"], "accepted": true, "id": "1703.04933"}, "pdf": {"name": "1703.04933.pdf", "metadata": {"source": "META", "title": "Sharp Minima Can Generalize For Deep Nets", "authors": ["Laurent Dinh", "Razvan Pascanu", "Samy Bengio", "Yoshua Bengio"], "emails": ["<laurent.dinh@umontreal.ca>."], "sections": [{"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "3 Properties of Deep Rectified Networks", "text": "Most of our results for clarity will relate to the deep, rectified output networks we use for a linear output layer, which we describe below, although they can easily be extended to other architectures (e.g. the behavior of a deeply rectified network with a linear output layer is)."}, {"heading": "4 Deep Rectified networks and flat minima", "text": "In this section, we will use the resulting strong unidentifiability to highlight some shortcomings of some definitions of flatness. Although the \u03b1 scale transformation does not affect the function displayed, it allows us to significantly reduce several flatness measures. For another definition of flatness, the \u03b1 scale transformation shows that all minima are equally flat."}, {"heading": "4.1 Volume -flatness", "text": "and. and. and...................................................................................................................................... and.................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4.2 Hessian-based measures", "text": "The non-euclidean geometry of the space parameter, coupled with the manifolds of the observationally similar behavior of the model, allows one to move from one region of the parameter space to another (e.g. Desjardins et al., 2015; Salimans & Kingma, 2016).In this section, we look at two widely used measurements of the Hessian, the spectral radius and the track, which show that one of these values can be manipulated without actually changing the behavior of the function. If the flatness of a minimum is defined by one of these values, then it could also be easily manipulated."}, {"heading": "5 Allowing reparametrizations", "text": "In this section we show a simple observation: if we are allowed to change the parameterization of a function f, we can obtain arbitrarily different geometries without affecting the evaluation of the function on the basis of invisible data. The same applies to the repair of the input space. It follows that the correlation between the geometry of the parameter space (and thus the error area) and the behavior of a given function is meaningless if it is not dependent on the specific parameterization of the model."}, {"heading": "5.1 Model reparametrization", "text": "(2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) (2016) () (2016) (2016) () (2016) (2016) () (2016) () (2016) () (2016) () (2016) () () (2016) () () (2016) () () (2016) () () (2016) () () () (2016) () () () (2016) () () () (2016) () () (2016) () () () () () (2016) () () () () (2016) () () () (2016) () () () () (2016) () () () () (2016) () () (2016) () () () (2016) () () () () () () (2016) () () () () () (2016) () () () (2016) () () (2016) () () () () () () (2016) () () () () (2016) ("}, {"heading": "5.2 Input representation", "text": "Since we come to the conclusion that the notion of flatness for a minimum of the loss function alone is not sufficient to determine its generalization capability in the general case, we can instead concentrate on the properties of the prediction function. Motivated by some work in contrary examples (Szegedy et al., 2014; Goodfellow et al., 2015) for deep neural networks, one could decide on their generalization capability by analyzing the gradient of the prediction function on the basis of examples. If the gradient at typical points of the distribution is small or has a small Lipschitz constant, then a small change in the input should not result in a large change in the prediction function 1. However, this infinitesimal reasoning is again very dependent on the local geometry of the input space. For an invertable pre-processing \u2212 1, for example, the feature standardization, brightening or Gaussianization (Chen & Gopinath 2001, we could prepare the prefunction 1)."}, {"heading": "6 Discussion", "text": "It has been empirically observed that minimums found by standard deep learning algorithms that generalize well tend to be flatter than found minimums that do not generalize well (Chaudhari et al., 2017; Keskar et al., 2017). However, if we follow several definitions of flatness, we have shown that the conclusion that flat minimums should be generalized better than sharp minimums cannot be applied without wider context. Previously used definitions do not take into account the complex geometry of some frequently used depth architectures. In particular, the non-identifiability of the model induced by symmetries allows the flatness of a minimum to be changed without affecting the function it represents. Furthermore, the entire geometry of the fault surface can be arbitrarily modified in terms of parameters under different parameters. In the sense of (Swircz et al, 2016), our work may be related to the need of a certain range or variety of our work."}, {"heading": "Acknowledgements", "text": "The authors thank Grzegorz S'wirszcz for an insightful discussion of the paper, Harm De Vries, Yann Dauphin, Jascha Sohl-Dickstein and C\u00e9sar Laurent for useful discussions on optimization, Danilo Rezende for explaining the universal approach by means of normalizing currents and Kyle Kastner, Adriana Romero, Junyoung Chung, Nicolas Ballas, Aaron Courville, George Dahl, Yaroslav Ganin, Prajit Ramachandran, \u00c7ag lar G\u00fcl\u00e7ehre and Ahmed Touati for useful feedback."}, {"heading": "A Radial transformations", "text": "We show an elementary transformation in the direction of a local disturbance of the geometry of a finite vector space and therefore influence the relative flatness between finite numerical minima, at least with respect to the spectral standard of Hessen. We define the function: Surface area > 0, Surface area + 0, Surface area [, r, r], Surface area + + + 0, Surface area (r, r) 0, Surface area (r) + 1 (r), Surface area + 1 (r), Surface area + 1 (r), Surface area + 1 (r), Surface area + 1 (r), Surface area (r), Surface dimension + 1 (r), Surface area (r), (r), Surface area (r), (), (r), (), (r), (), (r), (), (), (r), (), (r), (r), (r), (r), (r), (r), (r), (r), (r), (r), (r), (r), (r), (r), (r), (r), (r), (r), (r), (r), (r), (r), (r), (r), (), (r), (r), (r), (r), (), (r), (r), (), (), (r), (r), (), (r), (), (r), (), (r), (), (r), (), (), (), (), (r), (), (), (), (), (), (), (r), (), (), (), (), (), ((), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (),), (), (), (),),"}, {"heading": "B Considering the bias parameter", "text": "If we consider the bias parameter for a single (hidden) neural network, the non-negative homogeneity property intoy = \u03c6rect (x \u00b7 \u03b81 + b1) \u00b7 \u03b82 + b2 = \u03c6rect (x \u00b7 \u03b1\u03b81 + \u03b1b1) \u00b7 \u03b1 \u2212 1\u03b82 + b2 gives rise to conclusions similar to those in Section 4.For a deeper rectified neural network, this property iny = \u03c6rect (\u03c6rect (\u00b7 \u03c6rect (x \u00b7 \u03b81 + b1) \u00b7 \u00b7 \u00b7) \u00b7 \u03b8K \u2212 1 + bK \u2212 1) \u00b7 \u03b8K + bK = \u03c6rect (\u00b7 accelerrect (x \u00b7 \u03b11\u03b81 + \u03b11b1) \u00b7 \u00b7 \u00b7 \u00b7) \u00b7 \u03b1K \u2212 1\u03b8K \u2212 1 + K \u2212 1 \u041aK = 1 \u03b1kbK = 1) \u00b7 \u03b1KKKfor \u0445k = 1 \u03b1k = 1. This can reduce the amount of eigenvalues that can be influenced by Hessively."}, {"heading": "C Euclidean distance and input representation", "text": "A natural consequence of Section 5.2 is that metrics based on Euclidean metrics such as mean quadratic errors or earthmover distances are evaluated very differently depending on the input representation chosen. Therefore, the choice of input representation is crucial when evaluating different models based on these metrics. Indeed, such simple bijective transformations as feature standardization or whitening can significantly change the metric meaning.On the contrary, rankings resulting from metrics such as divergence and log probability are not affected by bijective transformations due to the change in the variable formula."}], "references": [{"title": "Natural gradient works efficiently in learning", "author": ["Amari", "Shun-Ichi"], "venue": "Neural Comput.,", "citeRegEx": "Amari and Shun.Ichi.,? \\Q1998\\E", "shortCiteRegEx": "Amari and Shun.Ichi.", "year": 1998}, {"title": "Normalization propagation: A parametric technique for removing internal covariate shift in deep networks", "author": ["Arpit", "Devansh", "Zhou", "Yingbo", "Kota", "Bhargava U", "Govindaraju", "Venu"], "venue": "arXiv preprint arXiv:1603.01431,", "citeRegEx": "Arpit et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arpit et al\\.", "year": 2016}, {"title": "Understanding symmetries in deep networks", "author": ["Badrinarayanan", "Vijay", "Mishra", "Bamdev", "Cipolla", "Roberto"], "venue": "arXiv preprint arXiv:1511.01029,", "citeRegEx": "Badrinarayanan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Badrinarayanan et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In ICLR\u20192015,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["Bottou", "L\u00e9on"], "venue": "In Proceedings of COMPSTAT\u20192010,", "citeRegEx": "Bottou and L\u00e9on.,? \\Q2010\\E", "shortCiteRegEx": "Bottou and L\u00e9on.", "year": 2010}, {"title": "The tradeoffs of large scale learning", "author": ["Bottou", "L\u00e9on", "Bousquet", "Olivier"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Bottou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2008}, {"title": "On-line learning for very large datasets", "author": ["Bottou", "L\u00e9on", "LeCun", "Yann"], "venue": "Applied Stochastic Models in Business and Industry,", "citeRegEx": "Bottou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2005}, {"title": "Optimization methods for large-scale machine learning", "author": ["Bottou", "L\u00e9on", "Curtis", "Frank E", "Nocedal", "Jorge"], "venue": "arXiv preprint arXiv:1606.04838,", "citeRegEx": "Bottou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2016}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["Chan", "William", "Jaitly", "Navdeep", "Le", "Quoc V", "Vinyals", "Oriol"], "venue": "In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Chan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2016}, {"title": "Entropy-sgd: Biasing gradient descent into wide valleys", "author": ["Chaudhari", "Pratik", "Choromanska", "Anna", "Soatto", "Stefano", "LeCun", "Yann"], "venue": "In ICLR\u20192017,", "citeRegEx": "Chaudhari et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Chaudhari et al\\.", "year": 2017}, {"title": "The loss surfaces of multilayer networks", "author": ["Choromanska", "Anna", "Henaff", "Mikael", "Mathieu", "Micha\u00ebl", "Arous", "G\u00e9rard Ben", "LeCun", "Yann"], "venue": "In AISTATS,", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Attention-based models for speech recognition", "author": ["Chorowski", "Jan K", "Bahdanau", "Dzmitry", "Serdyuk", "Dmitriy", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Wav2letter: an end-to-end convnet-based speech recognition system", "author": ["Collobert", "Ronan", "Puhrsch", "Christian", "Synnaeve", "Gabriel"], "venue": "arXiv preprint arXiv:1609.03193,", "citeRegEx": "Collobert et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2016}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Dauphin", "Yann N", "Pascanu", "Razvan", "G\u00fcl\u00e7ehre", "\u00c7aglar", "Cho", "KyungHyun", "Ganguli", "Surya", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Nice: Nonlinear independent components estimation", "author": ["Dinh", "Laurent", "Krueger", "David", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1410.8516,", "citeRegEx": "Dinh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2014}, {"title": "Density estimation using real nvp", "author": ["Dinh", "Laurent", "Sohl-Dickstein", "Jascha", "Bengio", "Samy"], "venue": "In ICLR\u20192017,", "citeRegEx": "Dinh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A convolutional encoder model for neural machine translation", "author": ["Gehring", "Jonas", "Auli", "Michael", "Grangier", "David", "Dauphin", "Yann N"], "venue": "arXiv preprint arXiv:1611.02344,", "citeRegEx": "Gehring et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gehring et al\\.", "year": 2016}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In Aistats,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "In ICLR\u20192015", "citeRegEx": "Goodfellow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "(icassp), 2013 ieee international conference on,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Hannun", "Awni Y", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan", "Diamos", "Greg", "Elsen", "Erich", "Prenger", "Ryan", "Satheesh", "Sanjeev", "Sengupta", "Shubho", "Coates", "Adam", "Ng", "Andrew Y"], "venue": "CoRR, abs/1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Hardt", "Moritz", "Recht", "Ben", "Singer", "Yoram"], "venue": "Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "Hardt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2016}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE international conference on computer vision,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Keeping the neural networks simple by minimizing the description length of the weights", "author": ["Hinton", "Geoffrey E", "Van Camp", "Drew"], "venue": "In Proceedings of the sixth annual conference on Computational learning theory,", "citeRegEx": "Hinton et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1993}, {"title": "Nonlinear independent component analysis: Existence and uniqueness results", "author": ["Hyv\u00e4rinen", "Aapo", "Pajunen", "Petteri"], "venue": "Neural Networks,", "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 1999}, {"title": "An empirical analysis of deep network loss surfaces", "author": ["Im", "Daniel Jiwoong", "Tao", "Michael", "Branson", "Kristin"], "venue": "arXiv preprint arXiv:1612.04010,", "citeRegEx": "Im et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Im et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In Bach & Blei", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Jarrett", "Kevin", "Kavukcuoglu", "Koray", "LeCun", "Yann"], "venue": "In Computer Vision,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "On largebatch training for deep learning: Generalization gap and sharp minima", "author": ["Keskar", "Nitish Shirish", "Mudigere", "Dheevatsa", "Nocedal", "Jorge", "Smelyanskiy", "Mikhail", "Tang", "Ping Tak Peter"], "venue": "In ICLR\u20192017,", "citeRegEx": "Keskar et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Keskar et al\\.", "year": 2017}, {"title": "Random walks on symmetric spaces and inequalities for matrix spectra", "author": ["Klyachko", "Alexander A"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Klyachko and A.,? \\Q2000\\E", "shortCiteRegEx": "Klyachko and A.", "year": 2000}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "About diagonal rescaling applied to neural nets", "author": ["Lafond", "Jean", "Vasilache", "Nicolas", "Bottou", "L\u00e9on"], "venue": "ICML Workshop on Optimization Methods for the Next Generation of Machine Learning,", "citeRegEx": "Lafond et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lafond et al\\.", "year": 2016}, {"title": "Autoencoding beyond pixels using a learned similarity", "author": ["Larsen", "Anders Boesen Lindbo", "S\u00f8nderby", "S\u00f8ren Kaae", "Winther", "Ole"], "venue": "metric. CoRR,", "citeRegEx": "Larsen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Larsen et al\\.", "year": 2015}, {"title": "On the number of linear regions of deep neural networks", "author": ["Montufar", "Guido F", "Pascanu", "Razvan", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Confidence level solutions for stochastic programming", "author": ["Nesterov", "Yu", "Vial", "J-Ph"], "venue": null, "citeRegEx": "Nesterov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nesterov et al\\.", "year": 2008}, {"title": "Path-sgd: Path-normalized optimization in deep neural networks", "author": ["Neyshabur", "Behnam", "Salakhutdinov", "Ruslan R", "Srebro", "Nati"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Neyshabur et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neyshabur et al\\.", "year": 2015}, {"title": "Revisiting natural gradient for deep networks", "author": ["Pascanu", "Razvan", "Bengio", "Yoshua"], "venue": "ICLR,", "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "On the expressive power of deep neural networks", "author": ["Raghu", "Maithra", "Poole", "Ben", "Kleinberg", "Jon", "Ganguli", "Surya", "Sohl-Dickstein", "Jascha"], "venue": "arXiv preprint arXiv:1606.05336,", "citeRegEx": "Raghu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Raghu et al\\.", "year": 2016}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "In Bach & Blei", "citeRegEx": "Rezende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2015}, {"title": "Singularity of the hessian in deep learning", "author": ["Sagun", "Levent", "Bottou", "L\u00e9on", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1611.07476,", "citeRegEx": "Sagun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sagun et al\\.", "year": 2016}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["Salimans", "Tim", "Kingma", "Diederik P"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Improved techniques for training gans", "author": ["Salimans", "Tim", "Goodfellow", "Ian", "Zaremba", "Wojciech", "Cheung", "Vicki", "Radford", "Alec", "Chen", "Xi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In ICLR\u20192015,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Local minima in training of deep", "author": ["Swirszcz", "Grzegorz", "Czarnecki", "Wojciech Marian", "Pascanu", "Razvan"], "venue": "networks. CoRR,", "citeRegEx": "Swirszcz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Swirszcz et al\\.", "year": 2016}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian", "Fergus", "Rob"], "venue": "In ICLR\u20192014,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "A note on the evaluation of generative models", "author": ["Theis", "Lucas", "Oord", "A\u00e4ron van den", "Bethge", "Matthias"], "venue": "In ICLR\u20192016", "citeRegEx": "Theis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2016}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Wu", "Yonghui", "Schuster", "Mike", "Chen", "Zhifeng", "Le", "Quoc V", "Norouzi", "Mohammad", "Macherey", "Wolfgang", "Krikun", "Maxim", "Cao", "Yuan", "Gao", "Qin", "Klaus"], "venue": "arXiv preprint arXiv:1609.08144,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["Zhang", "Chiyuan", "Bengio", "Samy", "Hardt", "Moritz", "Recht", "Benjamin", "Vinyals", "Oriol"], "venue": "In ICLR\u20192017,", "citeRegEx": "Zhang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 30, "context": "Hochreiter & Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization.", "startOffset": 33, "endOffset": 54}, {"referenceID": 49, "context": "Deep learning techniques have been very successful in several domains, like object recognition in images (e.g Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016), machine translation (e.", "startOffset": 105, "endOffset": 201}, {"referenceID": 24, "context": "Deep learning techniques have been very successful in several domains, like object recognition in images (e.g Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016), machine translation (e.", "startOffset": 105, "endOffset": 201}, {"referenceID": 46, "context": ", 2016), machine translation (e.g. Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Gehring et al., 2016) and speech recognition (e.", "startOffset": 29, "endOffset": 138}, {"referenceID": 3, "context": ", 2016), machine translation (e.g. Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Gehring et al., 2016) and speech recognition (e.", "startOffset": 29, "endOffset": 138}, {"referenceID": 51, "context": ", 2016), machine translation (e.g. Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Gehring et al., 2016) and speech recognition (e.", "startOffset": 29, "endOffset": 138}, {"referenceID": 17, "context": ", 2016), machine translation (e.g. Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Gehring et al., 2016) and speech recognition (e.", "startOffset": 29, "endOffset": 138}, {"referenceID": 21, "context": ", 2016) and speech recognition (e.g. Graves et al., 2013; Hannun et al., 2014; Chorowski et al., 2015; Chan et al., 2016; Collobert et al., 2016).", "startOffset": 31, "endOffset": 145}, {"referenceID": 11, "context": ", 2016) and speech recognition (e.g. Graves et al., 2013; Hannun et al., 2014; Chorowski et al., 2015; Chan et al., 2016; Collobert et al., 2016).", "startOffset": 31, "endOffset": 145}, {"referenceID": 8, "context": ", 2016) and speech recognition (e.g. Graves et al., 2013; Hannun et al., 2014; Chorowski et al., 2015; Chan et al., 2016; Collobert et al., 2016).", "startOffset": 31, "endOffset": 145}, {"referenceID": 12, "context": ", 2016) and speech recognition (e.g. Graves et al., 2013; Hannun et al., 2014; Chorowski et al., 2015; Chan et al., 2016; Collobert et al., 2016).", "startOffset": 31, "endOffset": 145}, {"referenceID": 10, "context": "Other works (e.g Dauphin et al., 2014; Choromanska et al., 2015) have looked at the structure of the error surface to analyze how trainable these models are.", "startOffset": 12, "endOffset": 64}, {"referenceID": 30, "context": "Finally, another point of discussion is how well these models can generalize (Nesterov & Vial, 2008; Keskar et al., 2017; Zhang et al., 2017).", "startOffset": 77, "endOffset": 141}, {"referenceID": 52, "context": "Finally, another point of discussion is how well these models can generalize (Nesterov & Vial, 2008; Keskar et al., 2017; Zhang et al., 2017).", "startOffset": 77, "endOffset": 141}, {"referenceID": 10, "context": ", 2014; Choromanska et al., 2015) have looked at the structure of the error surface to analyze how trainable these models are. Finally, another point of discussion is how well these models can generalize (Nesterov & Vial, 2008; Keskar et al., 2017; Zhang et al., 2017). These correspond, respectively, to low approximation, optimization and estimation error as described by Bottou (2010).", "startOffset": 8, "endOffset": 388}, {"referenceID": 30, "context": "Another conjecture that was recently (Keskar et al., 2017) explored, but that could be traced back to Hochreiter & Schmidhuber (1997), relies on the geometry of the loss function around a given solution.", "startOffset": 37, "endOffset": 58}, {"referenceID": 13, "context": "For example, Bottou & LeCun (2005); Bottou & Bousquet (2008); Duchi et al. (2011); Nesterov & Vial (2008); Hardt et al.", "startOffset": 62, "endOffset": 82}, {"referenceID": 13, "context": "For example, Bottou & LeCun (2005); Bottou & Bousquet (2008); Duchi et al. (2011); Nesterov & Vial (2008); Hardt et al.", "startOffset": 62, "endOffset": 106}, {"referenceID": 13, "context": "For example, Bottou & LeCun (2005); Bottou & Bousquet (2008); Duchi et al. (2011); Nesterov & Vial (2008); Hardt et al. (2016); Bottou et al.", "startOffset": 62, "endOffset": 127}, {"referenceID": 5, "context": "(2016); Bottou et al. (2016) rely on the concept of stochastic approximation.", "startOffset": 8, "endOffset": 29}, {"referenceID": 5, "context": "(2016); Bottou et al. (2016) rely on the concept of stochastic approximation. Another conjecture that was recently (Keskar et al., 2017) explored, but that could be traced back to Hochreiter & Schmidhuber (1997), relies on the geometry of the loss function around a given solution.", "startOffset": 8, "endOffset": 212}, {"referenceID": 9, "context": "Chaudhari et al. (2017) relies, in contrast, on the curvature of the second order structure around the minimum, while Keskar et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 9, "context": "Chaudhari et al. (2017) relies, in contrast, on the curvature of the second order structure around the minimum, while Keskar et al. (2017) looks at the maximum loss in a bounded neighbourhood of the minimum.", "startOffset": 0, "endOffset": 139}, {"referenceID": 9, "context": "Chaudhari et al. (2017); Keskar et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 9, "context": "Chaudhari et al. (2017); Keskar et al. (2017) suggest that this information is encoded in the eigenvalues of the Hessian.", "startOffset": 0, "endOffset": 46}, {"referenceID": 30, "context": "Additionally Keskar et al. (2017) defines the notion of sharpness.", "startOffset": 13, "endOffset": 34}, {"referenceID": 29, "context": "\u2022 \u03c6rect is the rectified elementwise activation function (Jarrett et al., 2009; Nair & Hinton, 2010; Glorot et al., 2011), which is the positive part (zi)i 7\u2192 (max(zi, 0))i.", "startOffset": 57, "endOffset": 121}, {"referenceID": 18, "context": "\u2022 \u03c6rect is the rectified elementwise activation function (Jarrett et al., 2009; Nair & Hinton, 2010; Glorot et al., 2011), which is the positive part (zi)i 7\u2192 (max(zi, 0))i.", "startOffset": 57, "endOffset": 121}, {"referenceID": 38, "context": "Let us redefine, for convenience, the non-negative homogeneity property (Neyshabur et al., 2015; Lafond et al., 2016) below.", "startOffset": 72, "endOffset": 117}, {"referenceID": 33, "context": "Let us redefine, for convenience, the non-negative homogeneity property (Neyshabur et al., 2015; Lafond et al., 2016) below.", "startOffset": 72, "endOffset": 117}, {"referenceID": 32, "context": "Note that beside this property, the reason for studying the rectified linear activation is for its widespread adoption (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016).", "startOffset": 119, "endOffset": 211}, {"referenceID": 49, "context": "Note that beside this property, the reason for studying the rectified linear activation is for its widespread adoption (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016).", "startOffset": 119, "endOffset": 211}, {"referenceID": 24, "context": "Note that beside this property, the reason for studying the rectified linear activation is for its widespread adoption (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016).", "startOffset": 119, "endOffset": 211}, {"referenceID": 23, "context": "Other models like leaky recitifers (He et al., 2015) or maxout networks (Goodfellow et al.", "startOffset": 35, "endOffset": 52}, {"referenceID": 9, "context": "Many directions However, some notion of sharpness might take into account the entire eigenspectrum of the Hessian as opposed to its largest eigenvalue, for instance, Chaudhari et al. (2017) describe the notion of wide valleys, allowing the presence of very few large eigenvalues.", "startOffset": 166, "endOffset": 190}, {"referenceID": 42, "context": "Since Sagun et al. (2016) seems to suggests that rank deficiency in the Hessian is due to over-parametrization of the model, one could conjecture that ( r\u2212mink\u2264K(nk) )", "startOffset": 6, "endOffset": 26}, {"referenceID": 30, "context": "We have redefined for > 0 the -sharpness of Keskar et al. (2017) as follow", "startOffset": 44, "endOffset": 65}, {"referenceID": 29, "context": "This also applies when using the full-space -sharpness used by Keskar et al. (2017). We can prove this similarly using the equivalence of norms in finite dimensional vector spaces and the fact that for c > 0, > 0, \u2264 (c + 1) (see Keskar et al.", "startOffset": 63, "endOffset": 84}, {"referenceID": 29, "context": "This also applies when using the full-space -sharpness used by Keskar et al. (2017). We can prove this similarly using the equivalence of norms in finite dimensional vector spaces and the fact that for c > 0, > 0, \u2264 (c + 1) (see Keskar et al. (2017)).", "startOffset": 63, "endOffset": 250}, {"referenceID": 29, "context": "This also applies when using the full-space -sharpness used by Keskar et al. (2017). We can prove this similarly using the equivalence of norms in finite dimensional vector spaces and the fact that for c > 0, > 0, \u2264 (c + 1) (see Keskar et al. (2017)). We have not been able to show a similar problem with random subspace -sharpness used by Keskar et al. (2017), i.", "startOffset": 63, "endOffset": 361}, {"referenceID": 9, "context": "a restriction of the maximization to a random subspace, which could relate to the notion of wide valleys described by Chaudhari et al. (2017).", "startOffset": 118, "endOffset": 142}, {"referenceID": 14, "context": "Several practical (Dinh et al., 2014; Rezende & Mohamed, 2015; Kingma et al., 2016; Dinh et al., 2016) and theoretical works (Hyv\u00e4rinen & Pajunen, 1999) show how powerful bijections can be.", "startOffset": 18, "endOffset": 102}, {"referenceID": 15, "context": "Several practical (Dinh et al., 2014; Rezende & Mohamed, 2015; Kingma et al., 2016; Dinh et al., 2016) and theoretical works (Hyv\u00e4rinen & Pajunen, 1999) show how powerful bijections can be.", "startOffset": 18, "endOffset": 102}, {"referenceID": 43, "context": "Instances of commonly used reparametrization are batch normalization (Ioffe & Szegedy, 2015), or the virtual batch normalization variant (Salimans et al., 2016), and weight normalization (Badrinarayanan et al.", "startOffset": 137, "endOffset": 160}, {"referenceID": 2, "context": ", 2016), and weight normalization (Badrinarayanan et al., 2015; Salimans & Kingma, 2016; Arpit et al., 2016).", "startOffset": 34, "endOffset": 108}, {"referenceID": 1, "context": ", 2016), and weight normalization (Badrinarayanan et al., 2015; Salimans & Kingma, 2016; Arpit et al., 2016).", "startOffset": 34, "endOffset": 108}, {"referenceID": 1, "context": ", 2015; Salimans & Kingma, 2016; Arpit et al., 2016). Im et al. (2016) have plotted how the loss function landscape was affected by batch normalization.", "startOffset": 33, "endOffset": 71}, {"referenceID": 48, "context": "Motivated by some work in adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015) for deep neural networks, one could decide on its generalization property by analyzing the gradient of the prediction function on examples.", "startOffset": 47, "endOffset": 94}, {"referenceID": 19, "context": "Motivated by some work in adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015) for deep neural networks, one could decide on its generalization property by analyzing the gradient of the prediction function on examples.", "startOffset": 47, "endOffset": 94}, {"referenceID": 34, "context": "This remark applies in applications involving images, sound or other signals with invariances (Larsen et al., 2015).", "startOffset": 94, "endOffset": 115}, {"referenceID": 34, "context": "This remark applies in applications involving images, sound or other signals with invariances (Larsen et al., 2015). For example, Theis et al. (2016) show for images how a small drift of one to four pixels can incur a large difference in terms of L2 norm.", "startOffset": 95, "endOffset": 150}, {"referenceID": 9, "context": "It has been observed empirically that minima found by standard deep learning algorithms that generalize well tend to be flatter than found minima that did not generalize well (Chaudhari et al., 2017; Keskar et al., 2017).", "startOffset": 175, "endOffset": 220}, {"referenceID": 30, "context": "It has been observed empirically that minima found by standard deep learning algorithms that generalize well tend to be flatter than found minima that did not generalize well (Chaudhari et al., 2017; Keskar et al., 2017).", "startOffset": 175, "endOffset": 220}, {"referenceID": 47, "context": "In the spirit of (Swirszcz et al., 2016), our", "startOffset": 17, "endOffset": 40}], "year": 2017, "abstractText": "Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter & Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.", "creator": "LaTeX with hyperref package"}}}