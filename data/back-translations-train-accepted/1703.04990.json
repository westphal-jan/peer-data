{"id": "1703.04990", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "Neural Programming by Example", "abstract": "Programming by Example (PBE) targets at automatically inferring a computer program for accomplishing a certain task from sample input and output. In this paper, we propose a deep neural networks (DNN) based PBE model called Neural Programming by Example (NPBE), which can learn from input-output strings and induce programs that solve the string manipulation problems. Our NPBE model has four neural network based components: a string encoder, an input-output analyzer, a program generator, and a symbol selector. We demonstrate the effectiveness of NPBE by training it end-to-end to solve some common string manipulation problems in spreadsheet systems. The results show that our model can induce string manipulation programs effectively. Our work is one step towards teaching DNN to generate computer programs.", "histories": [["v1", "Wed, 15 Mar 2017 07:57:51 GMT  (255kb,D)", "http://arxiv.org/abs/1703.04990v1", "7 pages, Association for the Advancement of Artificial Intelligence (AAAI)"]], "COMMENTS": "7 pages, Association for the Advancement of Artificial Intelligence (AAAI)", "reviews": [], "SUBJECTS": "cs.AI cs.NE cs.SE", "authors": ["chengxun shu", "hongyu zhang"], "accepted": true, "id": "1703.04990"}, "pdf": {"name": "1703.04990.pdf", "metadata": {"source": "CRF", "title": "Neural Programming by Example", "authors": ["Chengxun Shu", "Hongyu Zhang"], "emails": ["shuchengxun@163.com", "hongyu.zhang@newcastle.edu.au", "john@example.com", "james@company.com", "@),", "jacob@test.com,"], "sections": [{"heading": "Introduction", "text": "An example of this is the fact that users (often non-professional programmers) provide a machine with input-output examples for a task they would like to perform, and that the machine automatically starts a program to accomplish the task. PBE's concept has been successfully used for string manipulations in spreadsheet systems such as Microsoft Excel (Gulwani et al. 2015), computer-aided training (Gulwani 2014), and data extraction (Le and Gulwani 2014) when a user provides the following input and output examples: john @ example.com, john james @ company.com, jamesA PBE system should understand that the user wants to automatically synthesize the username from the email address."}, {"heading": "Related Work", "text": "It treats programs as embedded and uses neural networks to generate functions and arguments for executing programs. Its model is trained to monitor execution tracks and can be used in many different scenarios. However, its model cannot be directly applied to the problem of PBE as input for its model, while our work is dedicated to PBE and the input-output examples are. Neural programmers (Neelakantan, Le, and Sutskever 2015) is a neural network that is encoded with the task, while our work on our model is input-output-output-output examples. Neural programmers (Neelakantan, and Sutskever 2015) is a neural network that can be accessed through multiple steps. It is trained to output the result of executing programs, while our model is trained to output the program through symbols."}, {"heading": "The NPBE Model", "text": "Problem wording: Let S denote the set of strings. In the case of an input string x-S and an output string y-S, our model is to induce a function f-SS so that y = f (x). The input to our model is the input / output pair z: = (x, y) and S2, and we want to get the correct function f so that if the user enters another string x-x-S, we can generate the desired output y-f (x-S) without the user having to explicitly specify the function."}, {"heading": "Program Generator", "text": "The proposed NPBE model (Figure 1) consists of four modules: \u2022 a string encoder for encoding input and output strings; \u2022 an input-output analyzer that generates the transformation embedding that describes the relationship between input and output strings; \u2022 a program generator that generates the function and argument embedding in a few steps; \u2022 a symbol selector to decrypt the function / argument embedding and generate human-readable symbols.The program generator runs through a few steps, accessing the outputs of previous steps at each step, enabling the model to compose powerful programs with only a few predefined atomic functions. While traditional deep learning models attempt to generate the output y = f (x) and finally adjust the function f = f (x), our model learns to directly match the function f and generate the function y (x)."}, {"heading": "String Encoder", "text": "In view of a string consisting of a sequence of characters {i1, i2,..., iL}, the string encoder outputs a string embedding and a list of character embedding. First, each character ik is mapped in the sequence to the 8-dimensional raw embedding e (ik) via a randomly initialized and trainable embedding matrix. To better represent and maintain a character, the context of each character is merged with the raw embedding of the character in order to build the embedding of the character. Let l (ik) define the left context of the character ik and r (ik) as the right context of ic. l (ik) and r (ik) are each denoted by Equation (1) and Equation (2) with l (L + 1) C. In our implementation f l and fr are the update functions of LSTM (?).l ()"}, {"heading": "Input/output Analyzer", "text": "The input / output analyzer converts the embedding of input and output string into the embedding of transformations, which describes the relationship between input and output strings. Let's characterize the embedding of transformations by t-RT. sI-RS, sO-RS are the embedding of input and output strings. The input / output analyzer can be represented as equation (4). In our implementation, fIO is only a fully networked neural network with activation function.t = fIO ([sI; sO]) (4)"}, {"heading": "Program Generator", "text": "The function's embedding at time t is calculated as equation (5), which embedding t a fully connected neural network as input of transformation, and the execution history of the program generator ht (h0 = t). Similarly, the function reasoning embedding at time t is calculated as equation (6). However, the function arguments are often very complex and difficult to predict. Thus, the attention mechanism (similar to that used in neural machine translation models (?) is to embedding ar, t with attention to input and output strings strings. This is summarized in equation (7), the function embedding ft ft, the refined arguments embedding at, and the previous history embedding."}, {"heading": "Symbol Selector", "text": "The symbol selector uses the function for embedding ft and the arguments generated by the program generator to select a correct function and the corresponding arguments of that function. The probability distribution \u03b1func, t [0, 1] P via P atomic functions is generated by Equation (16), where Uf-RP \u00b7 F is the matrix that stores the representations of atomic functions. The arguments embedded in (represent the summary of arguments) are decrypted by an RNN (?) conditioned and attached to, as shown in Equation (17). st, 0,... st, i \u2212 1 are concealed states of LSTM with st, 0 = [0] H. In this way, the argument sequence is generated by the RNN. The probability distribution of the i-th argument at the time is not possible arguments, 1, 2, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5."}, {"heading": "Training", "text": "Any program Pi: {f1i, a1i,..., f T i, a T i} can transform Xi into Yi, where i is the i'th training example. For each input pair, we can generate a sequence of T functions, and each function has an argument list in which A is the set of possible arguments, M is the maximum number of arguments a function can take on. In our implementation, T = 5, M = 5. The training is performed by directly maximizing the log probability of the correct program P by specifying {X, Y}: \u03b8 = Argmax-Duration-LogP (X, Y, P) LogP (P | X, Y; KIS) (19) in which are the parameters of our model. Random Gaussian noise (?) is embedded in the transformation and the arguments for improving the generalization and generalization of NBE are embedded."}, {"heading": "Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Experimental Design", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves, and that they are able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. \""}, {"heading": "Experimental Results", "text": "RQ1: What is the accuracy of the NPBE when generating programs? Table 4 gives the evaluation results of the NPBE when predicting programs. The average Top1 accuracy of the NPBE is 74.1%, which means that for 74.1% of the input pairs in the test, NPBE successfully generates the corresponding program. We found that the model prediction will most likely occur on the integer argument of the Select, because neural networks do not count well. So let the model make 3 or 5 predictions when it tries to predict the integer arguments of the Select. The average Top3 and Top5 precision results are 85.8% and 91.0%, respectively, which means that the input-output-output-output-output pairs are under test."}, {"heading": "Discussions and Future Work", "text": "The intention behind NPBE is to get the model to automatically learn related characteristics from input-output strings and use the learned characteristics to induce correct programs. The purpose of this work is not to compete directly with existing PBE systems. Instead, we show that the use of DNN can detect characteristics in string transformations and learn precise programs through input-output pairs. Currently, NPBE cannot be generalized to completely invisible tasks (such as split, join, join, concatenate) that never occurred in the training set. In our future work, we will try to develop a model that truly \"understands\" the meaning of ofatomic functions in order to make it possible to generalize the invisible tasks."}, {"heading": "Conclusion", "text": "In this paper, we propose NPBE, a DNN-based Programming by Example (PBE) model. NPBE can trigger string manipulation programs based on simple input-output pairs by deriving a composition of functions and corresponding arguments. We have shown that the novel use of DNN can be successfully applied to the development of Programming by Example systems. Our work also examines the way higher-value functions are learned in deep learning, and is a step toward DNN for generating computer programs."}], "references": [{"title": "F", "author": ["W. Banzhaf", "P. Nordin", "R.E. Keller", "Francone"], "venue": "D.", "citeRegEx": "Banzhaf et al. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert"], "venue": "Journal of Machine Learning Research 12(Aug):2493\u20132537", "citeRegEx": "Collobert,? \\Q2011\\E", "shortCiteRegEx": "Collobert", "year": 2011}, {"title": "D", "author": ["A. Cypher", "Halbert"], "venue": "C.", "citeRegEx": "Cypher and Halbert 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "K", "author": ["Grefenstette, E.", "Hermann"], "venue": "M.; Suleyman, M.; and Blunsom, P.", "citeRegEx": "Grefenstette et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "S", "author": ["S. Gulwani", "J. Hernandez-Orallo", "E. Kitzelmann", "Muggleton"], "venue": "H.; Schmid, U.; and Zorn, B.", "citeRegEx": "Gulwani et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "G", "author": ["A. Krizhevsky", "I. Sutskever", "Hinton"], "venue": "E.", "citeRegEx": "Krizhevsky. Sutskever. and Hinton 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "S", "author": ["B.M. Lake", "T.D. Ullman", "J.B. Tenenbaum", "Gershman"], "venue": "J.", "citeRegEx": "Lake et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "D", "author": ["T. Lau", "S.A. Wolfman", "P. Domingos", "Weld"], "venue": "S.", "citeRegEx": "Lau et al. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "and Gulwani", "author": ["V. Le"], "venue": "S.", "citeRegEx": "Le and Gulwani 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "M", "author": ["Liang, P.", "Jordan"], "venue": "I.; and Klein, D.", "citeRegEx": "Liang. Jordan. and Klein 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "K", "author": ["W. Ling", "E. Grefenstette", "Hermann"], "venue": "M.; Kocisky, T.; Senior, A.; Wang, F.; and Blunsom, P.", "citeRegEx": "Ling et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "B", "author": ["A.K. Menon", "O. Tamuz", "S. Gulwani", "Lampson"], "venue": "W.; and Kalai, A.", "citeRegEx": "Menon et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "G", "author": ["Mohamed, A.-r.", "Dahl"], "venue": "E.; and Hinton, G.", "citeRegEx": "Mohamed. Dahl. and Hinton 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Q", "author": ["Neelakantan, A.", "Le"], "venue": "V.; and Sutskever, I.", "citeRegEx": "Neelakantan. Le. and Sutskever 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and de Freitas", "author": ["S. Reed"], "venue": "N.", "citeRegEx": "Reed and de Freitas 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end memory networks. In Advances in neural information processing systems, 2440\u20132448", "author": ["Sukhbaatar"], "venue": null, "citeRegEx": "Sukhbaatar,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar", "year": 2015}, {"title": "and Hinton", "author": ["T. Tieleman"], "venue": "G.", "citeRegEx": "Tieleman and Hinton 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Neural enquirer: Learning to query tables with natural", "author": ["Yin"], "venue": "arXiv preprint arXiv:1512.00965", "citeRegEx": "Yin,? \\Q2015\\E", "shortCiteRegEx": "Yin", "year": 2015}, {"title": "Learning simple algorithms from examples. arXiv preprint arXiv:1511.07275", "author": ["Zaremba"], "venue": null, "citeRegEx": "Zaremba,? \\Q2015\\E", "shortCiteRegEx": "Zaremba", "year": 2015}], "referenceMentions": [], "year": 2017, "abstractText": "Programming by Example (PBE) targets at automatically inferring a computer program for accomplishing a certain task from sample input and output. In this paper, we propose a deep neural networks (DNN) based PBE model called Neural Programming by Example (NPBE), which can learn from input-output strings and induce programs that solve the string manipulation problems. Our NPBE model has four neural network based components: a string encoder, an input-output analyzer, a program generator, and a symbol selector. We demonstrate the effectiveness of NPBE by training it end-toend to solve some common string manipulation problems in spreadsheet systems. The results show that our model can induce string manipulation programs effectively. Our work is one step towards teaching DNN to generate computer programs.", "creator": "LaTeX with hyperref package"}}}