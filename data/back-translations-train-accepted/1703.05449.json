{"id": "1703.05449", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2017", "title": "Minimax Regret Bounds for Reinforcement Learning", "abstract": "We consider the problem of efficient exploration in finite horizon MDPs.We show that an optimistic modification to model-based value iteration, can achieve a regret bound $\\tilde{O}( \\sqrt{HSAT} + H^2S^2A+H\\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states, $A$ the number of actions and $T$ the time elapsed. This result improves over the best previous known bound $\\tilde{O}(HS \\sqrt{AT})$ achieved by the UCRL2 algorithm.The key significance of our new results is that when $T\\geq H^3S^3A$ and $SA\\geq H$, it leads to a regret of $\\tilde{O}(\\sqrt{HSAT})$ that matches the established lower bounds of $\\Omega(\\sqrt{HSAT})$ up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in $S$), and we use \"exploration bonuses\" based on Bernstein's inequality, together with using a recursive -Bellman-type- Law of Total Variance (to improve scaling in $H$).", "histories": [["v1", "Thu, 16 Mar 2017 01:31:33 GMT  (33kb)", "http://arxiv.org/abs/1703.05449v1", null], ["v2", "Sat, 1 Jul 2017 13:00:06 GMT  (30kb)", "http://arxiv.org/abs/1703.05449v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["mohammad gheshlaghi azar", "ian osband", "r\u00e9mi munos"], "accepted": true, "id": "1703.05449"}, "pdf": {"name": "1703.05449.pdf", "metadata": {"source": "CRF", "title": "Minimax Regret Bounds for Reinforcement Learning", "authors": ["Mohammad Gheshlaghi Azar", "Ian Osband"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 3.05 449v 1 [stat.ML] 1 6M arWe consider the problem of efficient exploration in finite horizons MDP. We show that an optimistic modification to model-based value titeration can achieve a pitiful O, with H being the time horizon, S the number of states, A the number of actions, and T the elapsed time. This result improves over the best known bound O-value (HS-AT) achieved by the UCRL2 algorithm Jaksch et al. (2010a). The central importance of our new results is that if T \u2265 H3S3A and SA-H, it leads to a regret about O-value (\u221a HSAT), which corresponds to the established lower limits of the HAT algorithm, up to a logarithmic factor. Our analysis contains two key findings: We use a cautious application of concentration inequalities to the optimal value function as a basis for improving the law and not to it."}, {"heading": "1 Introduction", "text": "We consider the enhancement of learning (RL) as a problem that interacts with an environment to maximize its cumulative rewards over time (Burnetas and Katehakis, 1997; Sutton and Barto, 1998). We model the environment as a Markov decision-making process (MDP), but the agent is uncertain about the true dynamics of the MDP. How the agent interacts with the environment, he observes the states, actions and rewards generated by system dynamics. This leads to a fundamental trade-off: Should the agent explore poorly understood states and actions in order to improve his future performance, or optimize his existing knowledge in order to optimize short-term rewards, that is the process of estimating and optimizing. In this paradigmatism, point estimates, point estimates of unknown quantities are used to relate the unknown parameters and a plan to these estimates."}, {"heading": "2 Problem formulation", "text": "In this section we briefly review some notation, as well as some standard concepts and definitions from the theory of Markov decision-making processes (MDPs). We consider the problem of non-discounted episode amplification learning (RL) (Bertsekas and Tsitsiklis, 1996), where an RL agent interacts with a stochastic environment and this interaction is modeled as discrete-temporal MDP. An MDP is a quintuple < S, A, P, R, H >, where S and A are the composition of states and actions, P is the transitional state distribution, the function R: S \u00b7 A \u2192 A is a real-valued function on the action space of state H is the length of the episode. We denote P (x, a) and R (x, a) the probability distribution over the next state and the immediate reward of action in state x."}, {"heading": "3 Upper confidence bound value iteration", "text": "In this section, we will introduce the two variants of the algorithm we are examining in this paper, called the Upper Confidence Iteration (UCBVI) algorithm, which is very similar to the model-based Interval Estimation (MBIE-EB) algorithms that guarantee more than the resulting value function (Strehl and Littman, 2005). Our key contribution is the precise design of the upper trust values, and the analysis that leads to narrow regrets. Algorithm 1 UCB-VIInitialize dataH = 1, 2, doQk, h = UCB \u2212 values (H) for step h = 1, H doTake Action ak, h = argmaxa Qk, h (xk, h), h (xk, h, h, h) UpdateH = H (h, h, h, h, h, h, h, h)."}, {"heading": "4 Main results", "text": "In this section we present the main results of the paper, which represent upper limits of regretting the UCBVI _ 1 and UCBVI _ 2 algorithms. We assume that assumption 1 holds. Theorem 1 (Regret bound for UCBVI _ 1). Consider a parameter \u03b4 > 0. Then the regret of UCBVI _ 1 is limited. (Theorem 1 is significant in that it improves the regret dependence on S \u2011 ichance, compared to the best known limit of Jaksch et al. (2010b). The most important intuition for this improved S dependence is that we link the estimation error of the next-state function directly to the transition probabilities. (Theorem 1, compared to the best known limit of Jaksch et al. (2010b)."}, {"heading": "4.1 Computational efficiency", "text": "The results of theorems 1 and 2 provide performance guarantees for the statistical efficiency of the respective UCBVI algorithms. Importantly, both UCBVI _ 1 and UCBVI _ 2 result in mathematically tractable algorithms. In each episode, both algorithms perform an optimistic value iteration at calculation costs of the same order of magnitude as solving a known MDP. In fact, the calculation costs of this algorithm are in the same order of magnitude as those of the standard model-based value iteration that occurs with each update of O (HSAmin (T, S)) (Kearns and Singh, 1999). This corresponds to the total calculation costs of the O value (SAT min (T, S), as we update UCK = T / H times after T steps. In fact, it is possible to recalculate the O-scalings of both theorems 1 and 2 with the variants of UCBVI, which only selectively recalculate optimistic values."}, {"heading": "5 Proof sketch", "text": "Here is the sketchy proof of our results. The complete proof will be moved to the appendix."}, {"heading": "5.1 Sketch Proof of Theorem 1", "text": "To simplify the notations in this sketch of evidence, we also use simplified notations, e.g. L to represent the logarithmic constant, which is characterized by a numerical constant that can vary from line to line. Cumulative repentance in time T is Regret (k) def = k V, 1) \u2212 V, which is available in the full collection of evidence. We also use simplified notations, e.g. the use of L to represent the logarithmic constant L = log (SAT / 6)."}, {"heading": "5.2 Sketch Proof of Theorem 2", "text": "The detection of the Theorem 1 is not necessarily the case that the empirical variance of the VK-K 1 is based on a proof that the VK-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K"}, {"heading": "6 Conclusion", "text": "In this paper, we present a new variant of the well-known concept of optimism in the face of uncertainty. Our central contribution is the design and analysis of the UCBVI _ 2 algorithm, which addresses two major deficiencies in existing algorithms for the optimistic exploration of finite MDPs. First, we apply a focus on the value as a whole and not on the transition estimates. This leads to a reduction from S to \u221a S. Next, we apply a recursive law of total variance to pair estimates within an episode, ratherthane at each step, which leads to a reduction from H to \u221a H. Theorem 2 provides the first limits of regret, which for sufficiently large T correspond to the lower limits of the problem O-value (H2S2A) to logarithmic factors. It remains an open problem whether we can reach the lower limit with this approach for small values of T. In fact, we believe that the higher order concept can be improved from O-value (H2S2A) to O-value (HS2A)."}, {"heading": "Appendices", "text": "We begin by introducing a notation in Section A. Then we provide the full analysis of the UCBVI in Section B."}, {"heading": "A Notation", "text": "Let us define the total number of visits we make in state x, a at step h of all episodes up to episode k by N \u2032 k, h (x, a). We also use the notation N \u2032 k, h (x) = \u2211 A N \u2032 k, h (x, a) for the total number of visits in state x at time step h up to episode k. Let us also define the empirical variance V \u0432\u0430k, h, the variance of the optimal value function V \u0445 h (x, a) the empirical variance of the optimal value function V \u0432 k, h (x, a) and the variance of V \u0432\u0430k, h (x, a) def = Vary \u0432\u0430k (\u00b7 x, a), a) (Vk + 1 (y), h (Vk), h (Vk), k (k)."}, {"heading": "A.1 \u201cTypical\u201d state-actions and steps", "text": "In our analysis, we divide the episodes into 2 groups: the total number of \"typical\" episodes in which the number of visits to the encountered state acts is large and the rest of the episodes. Since the total number of other episodes is limited, this technique delivers the desired result. The total number of typical state acts for each episode k is defined as follows: [(x, a) k def = (x, a) k def = (x, a): (x, a) x S \u00d7 A, Nh (x, a) \u2265 H, N \u2032 k, h (x) \u2265 H}. Based on the definition of [(x, a) k def = (x, h, h) type, we define the group of typical episodes and the group of typical state-dependent episodes as follows [k] type def = {i: i [k], i [k] [k], i] k [k], y [k], y [k], y [n], y, h, h] (h)."}, {"heading": "A.2 Surrogate regrets", "text": "Our ultimate goal is to be bound to regret regret (k). However, in our analysis we mainly focus on limiting replacement regret. Let's limit the upper regret R (k), h (x) def = Vk, h (x) \u2212 V \u03c0kh (x) for each x-S, h [H] and k [K]. Then the upper regret R (k) def = k (i) = 1\u03b4 i, 1.R (k) egret (k) is useful in our analysis, as it provides an upper limit for true regret (k). Thus, we can limit R (k) as a substitute for regret (k). We also define the corresponding regret per state step and upper regret for each state x (X) and step h (H), or as a consequence Regret (k, x, h) def = 1I (xi, h), h (i), c (c), c (c), c (), H (), H)."}, {"heading": "A.3 Martingale difference sequences", "text": "In our analysis, we rely heavily on the theory of martyr sequences to prove that they are tied to the regret caused by the occurrence of a random sequence of states. We now provide some definitions and notations in this regard. We define the following martyr operator for each k-K, h-H, and F-S-S. If we also leave t = (k-1) H + h. If we designate the timestamp at step h of the episode k, then MtF def = P \u03c0kh is F-F (xk, h + 1). If G is also a real value function, it depends on Ht + s for some integer s > 0. Then, we generalize our definition of the operator Mt asMtG def = E (Ht + s).Ht \u00b7 k function for each integer s > 0."}, {"heading": "A.4 High probability events", "text": "We present the highly probable events E and P, among which the regret is no longer pronounced. (...) We use the abbreviation L def = log (5S). (...) Also for each v > 0, p = 0, 1] and n > 0 we leave the trusted events c1, c2 and c3, each as sequence: c1 (v, n) def = 2, vLn + 14HL3n, c2 (p, n) def = 2, p = 2, p (1 \u2212 p) Ln + 2L 3n, c3 (n) def = 2, SLn. (...) Let P define the set of all probability distributions on S. Define the following security for each k = 1,. (...) K, n > 0 and (...) p: A: P (k, h, n), n, n, n (...) def = 2, y) def (...)"}, {"heading": "A.4.1 UCB Events", "text": "Leave k [K] and h [H]. Specify the series of steps for which the value functions before Vk, h are obtained as [k, h] hist = {(i, j): i [K], j [H], i < k [i = k], i < k (i = k = j > h). Leave the calculation after Vi, h = {Vi, j \u2265 V \u0445 h, i (i, j), i [k, h] hist the event under which Vi, j before Vk, h are the upper limits for the optimal value functions. By reverse induction to h (and standard concentration imbalances) we will prove that k, h is valid under event E (see Lem. 19)."}, {"heading": "A.5 Other useful notation", "text": "We define first zk, h, for each h [H] and k [K], as a sequence of c4, k, h = 4H2SALnk, h.for each k [K], h [H] and x [x] we also introduce the following notation, which we use later when we summarize the regret: Ck, h def = k [K] type H \u2212 1, j [K] type, i, j, Bk, h def = k [H] type, h def = h [K] type) H \u2212 1, j = hbi, j, Ck, h, x def = k [K] type, i [K] type, k def = k [K] type, k, k def, h def [H] type, h def = h, h def] type, h def = h, h def] type, H \u2212 1, j = hbi, j, k, Ck, h, x def = hbi, k, k, k, k, k, k, k, k, k, k, k def (H) type, k, k, k, k def (H), k, k, k, k def, k def [H] type, h def [H] type, h def [H] type, h def [H] type, h def, h def [H] type, h def, h def [H] type, h def = h, h def = h, h def = h, h def = h [H] type, H, h def = h def = h, H, H, h def = h def] type, H, H, h def = h def = h, H, H, h def = h type, H \u2212 1, H \u2212 1, H \u2212 1, H \u2212 1, j, j = hbi, j, j = hbi, j, j, j, j, k, k, h def = 1I, k, k, k, h def = k, k, k, k, k, k, k, k, k def = k, k, k, k, k def (H"}, {"heading": "B Proof of the Regret Bounds", "text": "Before we start with the main analysis, we point out the following useful problem, which is frequently used in the analysis: h = h = h h = h h h = h h = h h h = h h h h = h h h = h h h h (h) h h (h) h h (h) h (h) h (h) h (h) h h h (h) h h) h (h) h h h (h) h h h h (h) h h h (h) h h h (h) h h (h h) h (h h) h (h h) h (h h) h (h) h (h) h h (h) h h (h h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h) h (h (h) h (h) h (h) h (h) h (h (h) h) h (h) h (h) h (h) h (h (h) h (h) h (h (h) h) h (h (h) h (h) h (h (h) h (h h) h (h) h (h (h h h) h (h h) h (h h (h h h) h (h h h h) h (h) h h h (h h (h h) h h h h (h h h h) h h h h (h (h h h h) h h h h (h h (h"}, {"heading": "B.1 Proof of Thm. 1", "text": "The result is a direct consequence of Lem. 18 and Lem. 15 and the fact that the high probability event E w.p. 1 \u2212 \u03b4 holds."}, {"heading": "B.2 Proof of Thm. 2", "text": "The result is a direct consequence of Lem. 19 and Lem. 14 and the fact that the high probability event E w.p. 1 \u2212 \u03b4 holds."}], "references": [{"title": "Minimax pac bounds on the sample complexity of reinforcement learning", "author": ["M.G. Azar", "R. Munos", "H.J. Kappen"], "venue": null, "citeRegEx": "Azar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2013}, {"title": "REGAL: A regularization based algorithm for reinforcement learning in weakly commu", "author": ["P.L. Bartlett", "A. Tewari"], "venue": null, "citeRegEx": "Bartlett and Tewari,? \\Q2009\\E", "shortCiteRegEx": "Bartlett and Tewari", "year": 2009}, {"title": "R-max - a general polynomial time algorithm for near-optimal reinforcement", "author": ["D.P. edition. Bertsekas", "J.N. Tsitsiklis"], "venue": "Neuro-Dynamic Programming", "citeRegEx": "Bertsekas and Tsitsiklis,? \\Q1996\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1996}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bubeck and Cesa.Bianchi,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi", "year": 2012}, {"title": "Optimal adaptive policies for markov decision processes", "author": ["A.N. 12:1587\u20131627. Burnetas", "M.N. Katehakis"], "venue": null, "citeRegEx": "Burnetas and Katehakis,? \\Q1997\\E", "shortCiteRegEx": "Burnetas and Katehakis", "year": 1997}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Operations Research,", "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "On tail probabilities for martingales", "author": ["D.A. TBA. Freedman"], "venue": "Information Processing Systems,", "citeRegEx": "Freedman,? \\Q1975\\E", "shortCiteRegEx": "Freedman", "year": 1975}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["T. Jaksch", "R. Ortner", "P. Auer"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["T. Jaksch", "R. Ortner", "P. Auer"], "venue": "Learning Research,", "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "Finite-sample convergence rates for Q-learning and indirect algorithms", "author": ["M. Kearns", "S. Singh"], "venue": "Learning Research,", "citeRegEx": "Kearns and Singh,? \\Q1999\\E", "shortCiteRegEx": "Kearns and Singh", "year": 1999}, {"title": "PAC bounds for discounted MDPs", "author": ["T. 3):209\u2013232. Lattimore", "M. Hutter"], "venue": null, "citeRegEx": "Lattimore and Hutter,? \\Q2012\\E", "shortCiteRegEx": "Lattimore and Hutter", "year": 2012}, {"title": "more) efficient reinforcement learning via posterior sampling", "author": ["I. Osband", "D. Russo", "B. Van Roy"], "venue": null, "citeRegEx": "Osband et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2013}, {"title": "On lower bounds for regret in reinforcement learning. stat, 1050:9", "author": ["I. Osband", "B. Van Roy"], "venue": "Neural Information Processing Systems,", "citeRegEx": "Osband and Roy,? \\Q2016\\E", "shortCiteRegEx": "Osband and Roy", "year": 2016}, {"title": "PAC model-free reinforcement learning", "author": ["A.L. Strehl", "L. Li", "E. Wiewiora", "J. Langford", "M.L. Littman"], "venue": null, "citeRegEx": "Strehl et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2006}, {"title": "A theoretical analysis of model-based interval estimation", "author": ["A.L. Strehl", "M.L. Littman"], "venue": null, "citeRegEx": "Strehl and Littman,? \\Q2005\\E", "shortCiteRegEx": "Strehl and Littman", "year": 2005}, {"title": "A Bayesian framework for reinforcement learning", "author": ["M.J.A. ACM. Strens"], "venue": "international conference on Machine learning,", "citeRegEx": "Strens,? \\Q2000\\E", "shortCiteRegEx": "Strens", "year": 2000}, {"title": "Inequalities for the l1 deviation", "author": ["T. Weissman", "E. Ordentlich", "G. Seroussi", "S. Verdu", "M.J. Weinberger"], "venue": null, "citeRegEx": "Weissman et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Weissman et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 7, "context": "This result improves over the best previous known bound \u00d5(HS \u221a AT ) achieved by the UCRL2 algorithm Jaksch et al. (2010a). The key significance of our new results is that when T \u2265 HSA and SA \u2265 H , it leads to a regret of \u00d5( \u221a HSAT ) that matches the established lower bounds of \u03a9( \u221a HSAT ) up to a logarithmic factor.", "startOffset": 100, "endOffset": 122}, {"referenceID": 4, "context": "1 Introduction We consider the reinforcement learning (RL) problem of an agent interacting with an environment in order to maximize its cumulative rewards through time (Burnetas and Katehakis, 1997; Sutton and Barto, 1998).", "startOffset": 168, "endOffset": 222}, {"referenceID": 13, "context": "Almost all reinforcement learning algorithms with polynomial bounds on sample complexity employ optimism to guide exploration (Kearns and Singh, 2002; Brafman and Tennenholtz, 2002; Strehl et al., 2006; Jaksch et al., 2010a).", "startOffset": 126, "endOffset": 224}, {"referenceID": 15, "context": "The algorithm posterior sampling for reinforcement learning (PSRL) maintains a posterior distribution for MDPs and, at each episode of interaction, follows a policy which is optimal for a single random sample (Strens, 2000).", "startOffset": 209, "endOffset": 223}, {"referenceID": 11, "context": "Recent work has established Bayesian regret bounds for PSRL and even argues for the potential benefits of sampling-based methods over existing optimistic approaches (Osband et al., 2013; Osband and Van Roy, 2016b).", "startOffset": 165, "endOffset": 213}, {"referenceID": 14, "context": "Our algorithm, upper confidence bound value iteration (UCBVI) is similar to model-based interval estimation (MBIE-EB) (Strehl and Littman, 2005) with a delicate alteration to the form of the \u201cexploration bonus\u201d.", "startOffset": 118, "endOffset": 144}, {"referenceID": 1, "context": "This positive result is the first of its kind and helps to address an ongoing question about where the fundamental lower bounds lie for reinforcement learning in finite horizon MDPs (Bartlett and Tewari, 2009; Dann and Brunskill, 2015; Osband and Van Roy, 2016a).", "startOffset": 182, "endOffset": 262}, {"referenceID": 6, "context": "Careful application of the Bernstein and Freedman inequalities (Bernstein, 1927; Freedman, 1975) to the concentration of the optimal value function directly, rather than building confidence sets for the transitions probabilities and rewards, like in UCRL2 (Jaksch et al.", "startOffset": 63, "endOffset": 96}, {"referenceID": 1, "context": "First, we study the setting of episodic, finite horizon MDPs and not the more general setting of weakly communicating systems (Bartlett and Tewari, 2009; Jaksch et al., 2010a).", "startOffset": 126, "endOffset": 175}, {"referenceID": 0, "context": "The use of exploration bonuses based on Bernstein\u2019s inequality, and a recursive Bellman-type Law of Total Variance (LTV) to prove tight bounds on the expected sum of the variances of the value estimates, in a similar spirit to the analysis from Azar et al. (2013); Lattimore and Hutter (2012).", "startOffset": 245, "endOffset": 264}, {"referenceID": 0, "context": "The use of exploration bonuses based on Bernstein\u2019s inequality, and a recursive Bellman-type Law of Total Variance (LTV) to prove tight bounds on the expected sum of the variances of the value estimates, in a similar spirit to the analysis from Azar et al. (2013); Lattimore and Hutter (2012). At a high level, this work addresses the noted shortcomings of existing algorithms for optimistic RL (Osband and Van Roy, 2016b) and demonstrates (contrary to previous assertions) that it is possible to design a simple and computationally efficient optimistic algorithm that does not suffer from these flaws when T is sufficiently large.", "startOffset": 245, "endOffset": 293}, {"referenceID": 2, "context": "MarkovDecision ProblemsWe consider the problem of undiscounted episodic reinforcement learning (RL) (Bertsekas and Tsitsiklis, 1996), where an RL agent interacts with a stochastic environment and this interaction is modeled as a discrete-time MDP.", "startOffset": 100, "endOffset": 132}, {"referenceID": 14, "context": "This algorithm is very related to the model based interval estimation (MBIE-EB) algorithms (Strehl and Littman, 2005).", "startOffset": 91, "endOffset": 117}, {"referenceID": 0, "context": "We then use the fact that the sum of these variances is bounded by the variance of the return (in a spirit similar to (Munos and Moore, 1999; Azar et al., 2013; Lattimore and Hutter, 2012)), which proves that the estimation errors accumulate as \u221a H instead of linearly inH , thus implying the improvedH-dependence.", "startOffset": 118, "endOffset": 188}, {"referenceID": 10, "context": "We then use the fact that the sum of these variances is bounded by the variance of the return (in a spirit similar to (Munos and Moore, 1999; Azar et al., 2013; Lattimore and Hutter, 2012)), which proves that the estimation errors accumulate as \u221a H instead of linearly inH , thus implying the improvedH-dependence.", "startOffset": 118, "endOffset": 188}, {"referenceID": 6, "context": "Theorem 1 is significant in that, for large T , it improves the regret dependence from S to \u221a S, compared to the best known bound of Jaksch et al. (2010b). The main intuition for this improved S-dependence is that we bound the estimation error of the next-state value function directly, instead of the transition probabilities.", "startOffset": 133, "endOffset": 155}, {"referenceID": 6, "context": "Theorem 1 is significant in that, for large T , it improves the regret dependence from S to \u221a S, compared to the best known bound of Jaksch et al. (2010b). The main intuition for this improved S-dependence is that we bound the estimation error of the next-state value function directly, instead of the transition probabilities. More precisely, instead of bounding the estimation error (P\u0302 \u03c0k k \u2212P k)Vk,h+1 by \u2016P\u0302 \u03c0k k \u2212P k\u20161\u2016Vk,h+1\u2016\u221e (as is done in Jaksch et al. (2010b) for example), we bound (P\u0302 \u03c0k k \u2212 P k)V \u2217 h+1 instead (for which a bound with no dependence on S can be achieved since V \u2217 is deterministic) and handle carefully the correction term (P\u0302 \u03c0k k \u2212 P k)(Vk,h+1 \u2212 V \u2217 h+1).", "startOffset": 133, "endOffset": 471}, {"referenceID": 9, "context": "In fact the computational cost of this algorithm is of the same order as that of standard model-based value iteration, which is of \u00d5(HSAmin(T, S)) at every update (Kearns and Singh, 1999).", "startOffset": 163, "endOffset": 187}, {"referenceID": 7, "context": "This technique is common to the literature involves extending the period of fixed optimistic estimates as more data is gathered Jaksch et al. (2010b); Dann and Brunskill (2015).", "startOffset": 128, "endOffset": 150}, {"referenceID": 7, "context": "This technique is common to the literature involves extending the period of fixed optimistic estimates as more data is gathered Jaksch et al. (2010b); Dann and Brunskill (2015). The computational cost of this variant of UCBVI then amounts to \u00d5(SA min(T, S)) as it only needs to update the model \u00d5(SA) times (Jaksch et al.", "startOffset": 128, "endOffset": 177}, {"referenceID": 7, "context": "This technique is common to the literature involves extending the period of fixed optimistic estimates as more data is gathered Jaksch et al. (2010b); Dann and Brunskill (2015). The computational cost of this variant of UCBVI then amounts to \u00d5(SA min(T, S)) as it only needs to update the model \u00d5(SA) times (Jaksch et al., 2010a). We can use similar techniques to further improve the computational complexity of both UCBVI_1 and UCBVI_2 but omit these modifications for clarity in an (already complicated) analysis. In comparison with UCRL2 both variants of our algorithms are up to S-times less computationally expensive than the previous state of the art , despite the improvement in regret scaling for large T . The reason for this improved computational efficiency comes from the structure of UCBVI. Since both algorithms design confidence sets upon the optimal value function directly, rather than the underlying estimates for rewards and transitions, they avoid the need for the computationally intensive extended value iteration Jaksch et al. (2010b).", "startOffset": 128, "endOffset": 1058}, {"referenceID": 7, "context": "In Jaksch et al. (2010b), this issue is addressed by bounding it as \u2016P\u0302 \u03c0k k \u2212 P k\u20161\u2016Vk,h+1\u2016\u221e at the price of an additional \u221a S.", "startOffset": 3, "endOffset": 25}, {"referenceID": 7, "context": "In Jaksch et al. (2010b), this issue is addressed by bounding it as \u2016P\u0302 \u03c0k k \u2212 P k\u20161\u2016Vk,h+1\u2016\u221e at the price of an additional \u221a S. The main contribution of our \u00d5(H \u221a SAT ) bound (which removes a \u221a S factor compared to the previous bound of Jaksch et al. (2010b)) is to handle this term more properly.", "startOffset": 3, "endOffset": 260}, {"referenceID": 7, "context": "Using similar argument as those used in Jaksch et al. (2010b), we have that ak,h \u2264 H2\u2016P\u0302 \u03c0k \u2212 P k\u20161 \u2264 H \u221a SL/nk,h (where nk,h def = Nk(xk,h, \u03c0k(xk,h))).", "startOffset": 40, "endOffset": 62}, {"referenceID": 0, "context": "A recursive application of the law of total variance (see e.g., Munos and Moore, 1999; Azar et al., 2013; Lattimore and Hutter, 2012) shows that this quantity is nothing else than the variance of the return (sum of h rewards) under policy \u03c0k: V (\u2211 h r(xk,h, \u03c0k(xk,h)) ) , which is thus bounded by H.", "startOffset": 53, "endOffset": 133}, {"referenceID": 10, "context": "A recursive application of the law of total variance (see e.g., Munos and Moore, 1999; Azar et al., 2013; Lattimore and Hutter, 2012) shows that this quantity is nothing else than the variance of the return (sum of h rewards) under policy \u03c0k: V (\u2211 h r(xk,h, \u03c0k(xk,h)) ) , which is thus bounded by H.", "startOffset": 53, "endOffset": 133}, {"referenceID": 1, "context": "These results are particularly significant because they help to settle an ongoing debate over whether such bounds were even attainable, or whether the true lower bound should be \u03a9(H \u221a SAT ) Bartlett and Tewari (2009); Osband and Van Roy (2016a).", "startOffset": 190, "endOffset": 217}, {"referenceID": 1, "context": "These results are particularly significant because they help to settle an ongoing debate over whether such bounds were even attainable, or whether the true lower bound should be \u03a9(H \u221a SAT ) Bartlett and Tewari (2009); Osband and Van Roy (2016a).", "startOffset": 190, "endOffset": 245}, {"referenceID": 16, "context": "The following result also holds on l1-normed estimation error of the transition distribution (Weissman et al., 2003), combined with a union bound on Nk(x, a) \u2208 [T ] implies w.", "startOffset": 93, "endOffset": 116}, {"referenceID": 6, "context": "When the sum of the variances \u2211n i=1 Var(Xi|Fi) \u2264 w then the following sharper bound due to Freedman (1975) holds w.", "startOffset": 92, "endOffset": 108}], "year": 2017, "abstractText": "We consider the problem of efficient exploration in finite horizon MDPs. We show that an optimistic modification to modelbased value iteration, can achieve a regret bound \u00d5( \u221a HSAT+HSA+H \u221a T )whereH is the time horizon, S the number of states, A the number of actions and T the time elapsed. This result improves over the best previous known bound \u00d5(HS \u221a AT ) achieved by the UCRL2 algorithm Jaksch et al. (2010a). The key significance of our new results is that when T \u2265 HSA and SA \u2265 H , it leads to a regret of \u00d5( \u221a HSAT ) that matches the established lower bounds of \u03a9( \u221a HSAT ) up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in S), and we use \"exploration bonuses\" based on Bernstein\u2019s inequality, together with using a recursive -Bellman-typeLaw of Total Variance (to improve scaling in H).", "creator": "LaTeX with hyperref package"}}}