{"id": "1703.05851", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2017", "title": "Temporal Information Extraction for Question Answering Using Syntactic Dependencies in an LSTM-based Architecture", "abstract": "In this paper, we propose to use a set of simple, uniform in architecture LSTM-based models to recover different kinds of temporal relations from text. Using the shortest dependency path between entities as input, the same architecture is used to extract intra-sentence, cross-sentence, and document creation time relations. A \"double-checking\" technique reverses entity pairs in classification, boosting the recall of positive cases and reducing misclassifications between opposite classes. An efficient pruning algorithm resolves conflicts globally. Evaluated on QA-TempEval (SemEval2015 Task 5), our proposed technique outperforms state-of-the-art methods by a large margin.", "histories": [["v1", "Fri, 17 Mar 2017 00:02:42 GMT  (327kb,D)", "http://arxiv.org/abs/1703.05851v1", null], ["v2", "Thu, 5 Oct 2017 21:38:08 GMT  (298kb,D)", "http://arxiv.org/abs/1703.05851v2", "EMNLP 2017"]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["yuanliang meng", "anna rumshisky", "alexey romanov"], "accepted": true, "id": "1703.05851"}, "pdf": {"name": "1703.05851.pdf", "metadata": {"source": "CRF", "title": "Temporal Information Extraction for Question Answering Using Syntactic Dependencies in an LSTM-based Architecture", "authors": ["Yuanliang Meng", "Anna Rumshisky", "Alexey Romanov"], "emails": ["ymeng@cs.uml.edu", "arum@cs.uml.edu", "aromanov@cs.uml.edu"], "sections": [{"heading": "1 Introduction", "text": "Restoring temporal information from the text is essential for many word processing tasks that require deep linguistic understanding, such as answering questions about the timeline of events or automatically generating text summaries. This paper presents intermediate results of an effort to build a temporal reasoning framework using contemporary deep learning techniques. Until recently, there have been remarkably few attempts to evaluate temporal methods of extracting information (TemporalIE) in the context of downstream applications that require time representation considerations. A more recent attempt to perform such an evaluation was SemEval2015 task 5, also known as QA-TempEval (Llorens et al., 2015a), in which answering questions (QA-TempEval) was used as a target application. QA-TempEval-rated systems that produce TimeML (Pustejovsky et al., 2003) notes that can be based on how well their results are used in architecture."}, {"heading": "2 Related Work", "text": "A variety of TemporalIE systems have been developed over the past decade, both in response to the series of common tasks organized by the community (Verhagen et al., 2007, 2010; UzZaman et al., 2012; Sun et al., 2013; Bethard et al., 2015; Llorens et al., 2015b; Minard et al., 2015), and in standalone efforts (Chambers et al., 2014b; Mirza, 2016). The best methods used by TemporalIE systems to date tend to be based on sophisticated task-ar Xiv: 170 3.05 851v 1 [cs.I R] 17 Mar 201 7 specific models using traditional static learning, which is usually applied successively (Sun et al., 2013; Chambers et al., 2014a). In a recent QA TempEval joint task, for example, the participants routinely used a set of classifiers (such as SVM or SVM supportcobrid) in the SVM."}, {"heading": "3 Dataset", "text": "We used QA-TempEval (SemEval 2015 Task 5) 1 data and evaluation methods in our experiments. The training set contains 276 commented TimeML files, most of which contain news articles from major agencies or wikinews from the late 1990s to the early 2000s. The test set contains uncommented files in three genres: 10 news articles from 2014, 10 Wikipedia articles on world history and 8 blog entries from the early 2000s. In QA-TempEval, evaluation is done with a QA toolkit that contains yes / no questions about two events or one event and a TIMEX. As the test set contains uncommented files, QA is the only way to measure performance. Some statistics of the test data and their QA sets can be found in Table 5. QA toolkit is also available for 25 files from training data that we used as a validation set."}, {"heading": "4 Timex and Event Extraction", "text": "The first task in our TemporalIE pipeline (TEA) is the identification of time expressions (TIMEX) and occurrences1http: / / alt.qcri.org / semeval2015 / task5 / in text. We used the HeidelTime package (Stretgen and Gertz, 2013) to identify TIMEX. We trained a neural network model to identify event mentions. Unlike current practice in TemporalIE, our TLINK models do not rely on event attributes, and therefore we have not attempted to identify them. Our pre-processing includes tokenization, part of language tagging, and parsing dependency trees performed with NewsReader (Agerri et al., 2014) for preprocessing. Each token is represented with a set of attributes. The attributes used to identify events are listed in Table 1."}, {"heading": "5 TLINK Classification", "text": "Our TLINK classifier consists of four components: an LSTM-based model for intra-sentence entity relationships, an LSTM-based model for cross-sentence relationships, another LSTM-based model for relationships at the time of document creation, and a rules-based component for time print pairs. The four models perform TLINK classifications independently, and the combined results are fed into a cutting module to remove the conflicting TLINKs."}, {"heading": "5.1 Intra-Sentence Model", "text": "A TLINK extraction model should be able to learn the patterns that indicate temporal relationships, such as phrases with temporal prepositions or clauses with specific conjunctions. This suggests that such models may benefit from encoding syntactic relationships rather than from a linear sequence of lexical elements. Following an idea used by Xu et al. (2015) with respect to extraction, we use the shortest path between units in a dependency tree to capture the essential context. Using the NewsReader pipeline, we identify the shortest path and use the word embedding for all tokens in the path as input into a neural network. Similar to Xu et al. (2015), we use two branches where the left branch processes the path from the source unit to the least common ancestor (LCA), and the right branch processes the path from the target unit to the LCA."}, {"heading": "5.2 Cross-Sentence Model", "text": "TLINKs between entities in successive sentences can often be identified without any external context or prior knowledge. Thus, the order relationships can be displayed by discourse connections as then or in the meantime, or events can follow natural orders that are potentially encoded in their word embeddings. To restore such relationships, we use a model similar to the model used for intra-sentence relationships as described in Section 5.1. Since there is no common root between entities in different sentences, we use the path between an entity and the sentence root to construct input data."}, {"heading": "5.3 Time Expressions Model", "text": "Time expressions explicitly denote a time or a time interval. Once labeled, the time relationships between time pairs can be identified using rules-based techniques. Without the timex units, which serve as \"turntables,\" many events would be isolated from each other. In this component, we focus on the timex tag DATE class. TIME class tags, which contain more information, are converted to DATE. Each DATE value is mapped to a tupel of real values (START, END). The \"value\" attribute of the timex tags follows the ISO 8601 standard, making the illustration simple. We set the minimum time interval to a tag. Practically, such a treatment is sufficient for our data. After mapping DATE values to tuples of real numbers, we can start 5 relationships between timex units T1 = (1, end1) and TLIN2 (defining BERS), when starting 1 and 2 (starting RELATE), such as > START, > START, > START and START."}, {"heading": "5.4 Relations to DCT", "text": "The identification of DCT and its association with events and time expressions in the text is an important step in understanding the content, and for our purpose it is also part of the TLINK classification. The relationship between DCT and other time expressions is just one special case of the timex-TLINK classification discussed in Section 5.3. In this section we will discuss how to identify the temporal relationship between an event and DCT. Assumption here is an event and its local context can often reveal its relationship to DCT. Linguistically, this is broadly correct. In English, verbs must be flexed in a tense clause and use tools to express different aspects. The model we use is similar to that in Section 5.2, but only one branch should suffice, because only one unit is mentioned in the text."}, {"heading": "5.5 Pruning TLINKs", "text": "The four models deal with TLINK in different states. They only produce pairs of relationships and do not overlap, and therefore will not cause conflicts if we only look at the results of the TLINK labels alone. However, some temporal relationships are transitive in nature, so the derived relationships from given TLINK can be turned into conflicts. Most conflicts are potentially dependent on two types of relationships. One is the BEFORE / AFTER relationship, and the other is the INCLUDES / IS INCLUDED relationship. Of course, we can relate them all to each other. If we use a direct diagram to represent the BEFORE relationships between all entities, it should be acyclic.Sun (2014) a strategy that prefers \"the edges that can be derived from other edges in the graph, and which are least like that.\" Another strategy (Chambers et al) based on results of individual classifications (see the results \"in their rankings) is to it."}, {"heading": "6 Experiments", "text": "Each model includes a number of hyperparameters. It is impossible to perform a full grid search, but we refer to previous research as a starting point and vary the hyperparameters to see the effects. We use the word2vec-GoogleNews vectors4 for all models that require word embedding. It has been pre-trained on the Google News corpus (3 billion running words) with the word2vec model. It contains 3 million English words and each word vector has a dimension of 300. Our neural network models are written in Keras on Theano. Training and testing run on a GPU-enabled workstation."}, {"heading": "6.1 Timex and Event Annotation", "text": "The LSTM layer of the event extraction model contains 128 LSTM units. The hidden layer above it has 30 neurons. The input layer corresponding to the 4 token characteristics is connected to a hidden layer of 3 neurons. The combined hidden layer is then connected to a single neuron output layer. We set a failure rate of 0.5 on the input layer and another failure rate of 0.5 on the hidden layer before output.4https: / / github.com / mmihaltz / word2vec-GoogleNews vectorsThe evaluation rate is also shown in Table 2. We intentionally increase the recall rate, even if precision has to be sacrificed.To answer questions about time relationships, it is not particularly harmful to have redundant events, but the absence of an event makes it impossible to answer questions related to them."}, {"heading": "6.2 Intra-Sentence Model", "text": "We identified 12 classes of temporal relationships plus one NO-LINK class. As we did not deal with the event correlation, we combined SIMULTANEOUS and IDENTITY. For the training, we sampled the NO-LINK class down to 50% of the number of positive instances. In addition to the usual problems of class imbalance, downsampling also takes into account the fact that TimeML-style annotations are de facto sparse, commenting only a tiny fraction of the positive instances. LSTM layer of the intra-sentence model contains 256 LSTM units on each branch. The hidden layer above has 100 neurons. We set a drop-out rate of 0.6 on the input layer and another drop-out rate of 0.5 on the hidden layer before output."}, {"heading": "6.2.1 Double-checking", "text": "We introduce a technique to promote the recall of positive classes and to reduce misclassifications between opposing classes. Since entity pairs are always classified in both sequences when both results are positive (not NO-LINK), we adopt the label with a higher probability value, as assigned by the Softmax classifier. We call this technique \"Doublecheck.\" One effect of this is the reduction of errors that are fundamentally harmful, e.g. BEFORE misclassification than NACH, and vice versa. In addition, we also allow a positive class the \"veto\" against a NO-LINK class. For example, if our model (e1, e2) \u2192 NO \u2212 LINK predicts, but (e2, e1) \u2192 then we adopt the result (e2, e1) \u2192 NACH. As illustrated in Table 3, stomping the NO-LINK cases can largely promote the recall of positive classes."}, {"heading": "6.3 Cross-Sentence Model", "text": "The LSTM layer of the cross set model contains 256 LSTM units on each branch, and the hidden layer above that has 100 neurons. We set a failure rate of 0.6 on the input layer and a further failure rate of 0.5 on the hidden layer in front of the exit. The training and evaluation procedures are very similar to what we have done for intra set models. Now, the vast majority of entity pairs do not have TLINs explicitly marked in the training data. However, unlike the intra set scenario, in most cases a NO-LINK label is really sufficient. Often, the two sets alone mean that a reader cannot interpret the temporal relationships between entities across set boundaries. Nevertheless, there are still many false negs and it is not possible to evaluate the systems in a simple way."}, {"heading": "6.4 DCT Model", "text": "The hyperparameters for the DCT model are the same as we use for intraplacement and crosscentence models. Again, the training files do not comment on TLINKs with DCT sufficiently, even if the relationships are clear, so there are many false negatives. We select the NO-LINK instances down so that they are four times higher than positive instances."}, {"heading": "7 Results and Discussion", "text": "This section presents the QA-based evaluation results. First, we will present the results of the validation data with different model configurations and then present the results on the basis of test data."}, {"heading": "7.1 Evaluation on Validation data", "text": "The aim of the evaluation here is to select the model. It is impossible to directly represent all intermediate results, so we will selectively highlight those of interest. The QA toolkit contains 79 yes / no questions about entity temporal relationships in the validation data. Originally, only 6 questions have \"no\" as the correct answer, and 1 question has \"unknown.\" However, after examining the questions and answers, we found some errors and typos. After determining the errors, there are 7 no questions and 72 yes questions in total. All evaluations are performed after the errors have been corrected. We report results in two decimals, because the previous publication (Llorens et al., 2015a) did so. The evaluation tool draws answers only from the comments. If an entity (event or timex) involved in a question is not commented on, or the TLINeX system cannot be found, then the answer is not found. The question is then not answered explicitly."}, {"heading": "7.2 Evaluation on Test Data", "text": "As we see, the vast majority of questions should be answered with yes. In general, it is much more difficult to validate a specific relationship (answer yes) than to reject it (answer no) when we have as many as 12 types of relationships in addition to the vague NO-LINK classes. The first is exactly the same as we have done with validation data. the second includes questions that are in the same sentence or in consecutive sentences. dist + means that the entities are farther off. the organizer had two ways to evaluate systems; the first is exactly the same as we have done with validation data. the second is a so-called Time Expression Reasoner (TREFL) to add relationships between time systems and evaluate the extended outcomes. The goal of such an extra round is to \"analyze how a general temporal expression might improve the results."}, {"heading": "8 Conclusion", "text": "We have proposed a method based on a relatively simple LSTM-based architecture that uses the shortest dependency paths as input and translates them into a series of sub-tasks required to extract temporal relationships from text. We also introduce (1) a \"double-checking\" technique that reverses pairs in classification, thereby promoting recall of positives and reducing misclassifications between opposing classes, and (2) an efficient truncation algorithm to resolve conflicting TLINKs. In a QA-based evaluation, our proposed method, which does not take event references into account, far exceeds the state of the art."}], "references": [{"title": "Ixa pipeline: Efficient and ready to use multilingual nlp tools", "author": ["Rodrigo Agerri", "Josu Bermudez", "German Rigau."], "venue": "Proceedings of the 9th Language Resources and Evaluation Conference (LREC2014). pages 26\u201331. http://ixa2.si.ehu.es/ixa-", "citeRegEx": "Agerri et al\\.,? 2014", "shortCiteRegEx": "Agerri et al\\.", "year": 2014}, {"title": "Semeval-2015 task 6: Clinical tempeval", "author": ["Steven Bethard", "Leon Derczynski", "James Pustejovsky", "Marc Verhagen."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015). Association for Computational Lin-", "citeRegEx": "Bethard et al\\.,? 2015", "shortCiteRegEx": "Bethard et al\\.", "year": 2015}, {"title": "Dense event ordering with a multipass architecture", "author": ["Nathanael Chambers", "Taylor Cassidy", "Steven Bethard."], "venue": "Transactions of the Association for Computational Linguistics 2:273\u2013284.", "citeRegEx": "Chambers et al\\.,? 2014a", "shortCiteRegEx": "Chambers et al\\.", "year": 2014}, {"title": "Dense event ordering with a multi-pass architecture", "author": ["Nathanael Chambers", "Taylor Cassidy", "Bill McDowell", "Steven Bethard."], "venue": "Transactions of the Association for Computational Linguistics 2:273\u2013 284.", "citeRegEx": "Chambers et al\\.,? 2014b", "shortCiteRegEx": "Chambers et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Reducibility among combinatorial problems", "author": ["Richard Karp."], "venue": "Complexity of Computer Computations, Proc. Sympos.. pages 85\u2013103.", "citeRegEx": "Karp.,? 1972", "shortCiteRegEx": "Karp.", "year": 1972}, {"title": "Semeval-2015 task 5: Qa tempeval - evaluating temporal information understanding with question answering", "author": ["Hector Llorens", "Nathanael Chambers", "Naushad UzZaman", "Nasrin Mostafazadeh", "James Allen", "James Pustejovsky."], "venue": "Proceedings of", "citeRegEx": "Llorens et al\\.,? 2015a", "shortCiteRegEx": "Llorens et al\\.", "year": 2015}, {"title": "Semeval-2015 task 5: Qa tempeval-evaluating temporal information understanding with question answering", "author": ["Hector Llorens", "Nathanael Chambers", "Naushad UzZaman", "Nasrin Mostafazadeh", "James Allen", "James Pustejovsky."], "venue": "Proceed-", "citeRegEx": "Llorens et al\\.,? 2015b", "shortCiteRegEx": "Llorens et al\\.", "year": 2015}, {"title": "Semeval-2015 task 4: Timeline: Cross-document event ordering", "author": ["Anne-Lyse Minard", "Manuela Speranza", "Eneko Agirre", "Itziar Aldabe", "Marieke van Erp", "Bernardo Magnini", "German Rigau", "Rub\u00e9n Urizar", "Fondazione Bruno Kessler."], "venue": "In", "citeRegEx": "Minard et al\\.,? 2015", "shortCiteRegEx": "Minard et al\\.", "year": 2015}, {"title": "Extracting temporal and causal relations between events", "author": ["Paramita Mirza."], "venue": "CoRR abs/1604.08120. http://arxiv.org/abs/1604.08120.", "citeRegEx": "Mirza.,? 2016", "shortCiteRegEx": "Mirza.", "year": 2016}, {"title": "Timeml: Robust specification", "author": ["ham Katz"], "venue": null, "citeRegEx": "Katz.,? \\Q2003\\E", "shortCiteRegEx": "Katz.", "year": 2003}, {"title": "Time Well Tell: Temporal Reason", "author": [], "venue": null, "citeRegEx": "Sun.,? \\Q2014\\E", "shortCiteRegEx": "Sun.", "year": 2014}, {"title": "Classifying relations via long", "author": ["Zhi Jin"], "venue": null, "citeRegEx": "Jin.,? \\Q2015\\E", "shortCiteRegEx": "Jin.", "year": 2015}, {"title": "Relation classification via", "author": ["Jun Zhao"], "venue": null, "citeRegEx": "Zhao.,? \\Q2014\\E", "shortCiteRegEx": "Zhao.", "year": 2014}, {"title": "Relation classification via recurrent neural network", "author": ["Dongxu Zhang", "Dong Wang."], "venue": "CoRR abs/1508.01006. http://arxiv.org/abs/1508.01006.", "citeRegEx": "Zhang and Wang.,? 2015", "shortCiteRegEx": "Zhang and Wang.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "QA-TempEval (Llorens et al., 2015a), which used question answering (QA) as the target application.", "startOffset": 12, "endOffset": 35}, {"referenceID": 1, "context": "A multitude of TemporalIE systems have been developed over the past decade both in response to the series of shared tasks organized by the community (Verhagen et al., 2007, 2010; UzZaman et al., 2012; Sun et al., 2013; Bethard et al., 2015; Llorens et al., 2015b; Minard et al., 2015) and in standalone efforts (Chambers et al.", "startOffset": 149, "endOffset": 284}, {"referenceID": 7, "context": "A multitude of TemporalIE systems have been developed over the past decade both in response to the series of shared tasks organized by the community (Verhagen et al., 2007, 2010; UzZaman et al., 2012; Sun et al., 2013; Bethard et al., 2015; Llorens et al., 2015b; Minard et al., 2015) and in standalone efforts (Chambers et al.", "startOffset": 149, "endOffset": 284}, {"referenceID": 8, "context": "A multitude of TemporalIE systems have been developed over the past decade both in response to the series of shared tasks organized by the community (Verhagen et al., 2007, 2010; UzZaman et al., 2012; Sun et al., 2013; Bethard et al., 2015; Llorens et al., 2015b; Minard et al., 2015) and in standalone efforts (Chambers et al.", "startOffset": 149, "endOffset": 284}, {"referenceID": 3, "context": ", 2015) and in standalone efforts (Chambers et al., 2014b; Mirza, 2016).", "startOffset": 34, "endOffset": 71}, {"referenceID": 9, "context": ", 2015) and in standalone efforts (Chambers et al., 2014b; Mirza, 2016).", "startOffset": 34, "endOffset": 71}, {"referenceID": 2, "context": "specific models using traditional statistical learning, typically used in succession (Sun et al., 2013; Chambers et al., 2014a).", "startOffset": 85, "endOffset": 127}, {"referenceID": 14, "context": ", 2014) and recurrent neural networks both have been used for argument relation classification and similar tasks (Zhang and Wang, 2015; Xu et al., 2015; Vu et al., 2016).", "startOffset": 113, "endOffset": 169}, {"referenceID": 14, "context": ", 2014) and recurrent neural networks both have been used for argument relation classification and similar tasks (Zhang and Wang, 2015; Xu et al., 2015; Vu et al., 2016). We take inspiration from some of this work, including specifically the approach proposed proposed by Xu et al. (2015).", "startOffset": 114, "endOffset": 289}, {"referenceID": 0, "context": "Our preprocessing includes tokenization, part of speech tagging, and dependency tree parsing, which is done with NewsReader (Agerri et al., 2014) for preprocessing.", "startOffset": 124, "endOffset": 145}, {"referenceID": 4, "context": "The extraction model uses long short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) which is a popular RNN architecture to process time series.", "startOffset": 56, "endOffset": 90}, {"referenceID": 11, "context": "Sun (Sun, 2014) proposed a strategy that \u201cprefers the edges that can be inferred by other edges in the graph and remove the ones that are least so\u201d.", "startOffset": 4, "endOffset": 15}, {"referenceID": 2, "context": "Another strategy (Chambers et al., 2014a), based on results from separate classifiers (\u201csieves\u201d in their term), is to rank results according to their precisions.", "startOffset": 17, "endOffset": 41}, {"referenceID": 5, "context": "This problem is actually an extension of the minimum feedback arc set problem and is NP-hard (Karp, 1972).", "startOffset": 93, "endOffset": 105}, {"referenceID": 6, "context": "We report results in two decimal digits because the previous publication (Llorens et al., 2015a) did so.", "startOffset": 73, "endOffset": 96}, {"referenceID": 6, "context": "Adapted from (Llorens et al., 2015a) Table 1.", "startOffset": 13, "endOffset": 36}, {"referenceID": 6, "context": "These systems were not optimized for the task (Llorens et al., 2015a).", "startOffset": 46, "endOffset": 69}], "year": 2017, "abstractText": "In this paper, we propose to use a set of simple, uniform in architecture LSTMbased models to recover different kinds of temporal relations from text. Using the shortest dependency path between entities as input, the same architecture is used to extract intra-sentence, crosssentence, and document creation time relations. A \u201cdouble-checking\u201d technique reverses entity pairs in classification, boosting the recall of positive cases and reducing misclassifications between opposite classes. An efficient pruning algorithm resolves conflicts globally. Evaluated on QA-TempEval (SemEval2015 Task 5), our proposed technique outperforms state-ofthe-art methods by a large margin.", "creator": "LaTeX with hyperref package"}}}