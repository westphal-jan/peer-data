{"id": "1703.06182", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2017", "title": "Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under Partial Observability", "abstract": "Many real-world tasks involve multiple agents with partial observability and limited communication. Learning is challenging in these settings due to local viewpoints of agents, which perceive the world as non-stationary due to concurrently-exploring teammates. Approaches that learn specialized policies for individual tasks face major problems when applied to the real world: not only do agents have to learn and store a distinct policy for each task, but in practice the identity of the task is often non-observable, making these algorithms inapplicable. This paper formalizes and addresses the problem of multi-task multi-agent reinforcement learning under partial observability. We introduce a decentralized single-task learning approach that is robust to concurrent interactions of teammates, and present an approach for distilling single-task policies into a unified policy that performs well across multiple related tasks, without explicit provision of task identity.", "histories": [["v1", "Fri, 17 Mar 2017 19:32:38 GMT  (5933kb,D)", "https://arxiv.org/abs/1703.06182v1", null], ["v2", "Sat, 25 Mar 2017 15:54:36 GMT  (5933kb,D)", "http://arxiv.org/abs/1703.06182v2", null], ["v3", "Wed, 14 Jun 2017 16:09:39 GMT  (6467kb,D)", "http://arxiv.org/abs/1703.06182v3", null], ["v4", "Thu, 13 Jul 2017 17:34:34 GMT  (6467kb,D)", "http://arxiv.org/abs/1703.06182v4", "Accepted to ICML 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.MA", "authors": ["shayegan omidshafiei", "jason pazis", "christopher amato", "jonathan p how", "john vian"], "accepted": true, "id": "1703.06182"}, "pdf": {"name": "1703.06182.pdf", "metadata": {"source": "META", "title": "Deep Decentralized Multi-task Multi-Agent Reinforcement Learningunder Partial Observability", "authors": ["Shayegan Omidshafiei", "Jason Pazis", "Christopher Amato", "Jonathan P. How", "John Vian"], "emails": ["<shayegan@mit.edu>."], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "2. Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Reinforcement Learning", "text": "The individual actors RL under full observation are typically formalized using Markov Decision Processes (MDPs) Q-Q-Processes (Q-Q-Processes) Q-Processes (Q-Processes) Q-Processes (Sutton & Barto, 1998), defined as Tupel < S, T, R-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-A-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-A-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-A-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-Process-A-Process-Process-Process-Proc"}, {"heading": "2.2. Multi-agent RL", "text": "Our work focuses on cooperative settings in which the agents share a common return. Claus & Boutilier (1998) divides MARL agents into two classes: Joint Action Learners (JALs) and Independent Learners (ILs). JALs observe the actions taken by all agents, while ILs observe only local actions. Since the observability of joint actions is a strong assumption in partially observable areas, ILs are typically more practical, although they must solve a more difficult problem (Claus & Boutilier, 1998). Our approach uses ILs that perform both learning and execution in decentralized manners.Unique challenges arise in MARL due to the agents \"interactions during learning (Bus, oniu et al., 2010; Matignon et al., 2012)."}, {"heading": "2.3. Transfer and Multi-Task Learning", "text": "Transfer Learning (TL) aims to generalize knowledge from a set of source tasks to a target task (Pan & Yang, 2010).While TL assumes a sequential transfer where source tasks have already been learned and may not be related to the target task (Taylor & Stone, 2009), Multi-Task Reinforcement Learning (MTRL) aims to learn a policy that works well for related tasks from an underlying task distribution (Caruana, 1998; Pan & Yang, 2010).MTRL tasks can be learned simultaneously or sequentially (Taylor & Stone, 2009).MTRL draws the agent's attention to powerful training signals that are learned for individual tasks, allowing a consistent policy to generalize well across all tasks."}, {"heading": "3. Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Multi-agent RL", "text": "Most existing IL approaches rely on full observability. (2012) Partially observable MARL approaches received limited attention. (Wu et al., 2012) These include model-free gradient ascent-based methods (Peshkin et al., 2000; Dutech et al., 2001), simulator-based methods to improve strategies using a series of linear programs (Wu et al., 2012), and model-based approaches where agents learn in an interleaved manner to reduce destabilization caused by concurrent learning (Banerjee et al., 2012). Recent scalable methods use Expectation Maximization to learn finite state controller (FSC) policies (Wu et al., 2013; Liu et al., 2016).Our approach is most closely related to IL algorithms that learn Qvalues because their power of representation is more conducive than transfer between tasks, as opposed to political tables or full-majority SCIL approaches."}, {"heading": "3.2. Transfer and Multi-task RL", "text": "Taylor & Stone (2009) and Torrey & Shavlik (2009) offer excellent surveys of transfer and multi-task RLs that almost exclusively target individual agents with fully observable settings. Tanaka & Yamamura (2003) use first and secondary statistics to calculate a prioritized blanket metric for MTRL that allows an agent to maximize lifetime reward across task flows. Ferna'ndez & Veloso (2006) introduce an MDP comparison metric, and learn a policy library that generalizes tasks well within a common area. Wilson et al. (2007) consider TL for MDPs by learning a Dirichlet Process Mixture Model over source MDPs that is used as an informative precursor to a target MDP. They then expand the work to include multi-agent MDPs by reducing characteristic agent roles for MDPs (2013) and MDP negative agency roles (2013)."}, {"heading": "4. Multi-task Multi-agent RL", "text": "This section presents MT-MARL under partial observation probability. We formalize the single task MARL (< P = > reward) using the decentralized, partially observable Markov decision-making process (Dec-POMDP), defined as < I, S, A, T, R, kin, O, \u03b3 >, where I am a series of n agents, S is the state space, A = \u00b7 iA (i) is the common action space, and (n) > causes the common observation space (Bernstein et al., 2002). Each actor i executes actions a (i), where joint action a = < a (1), one (n) > causes the ambient state a with probability P (s)."}, {"heading": "5. Approach", "text": "This section introduces a two-phase approach to partially detectable MT-MARL, which first specializes individual tasks and then combines task-specific DRQNs into a common policy that performs well in all tasks."}, {"heading": "5.1. Phase I: Dec-POMDP Single-Task MARL", "text": "Since Dec-POMDP RL is notoriously complex (and the solution for optimal policy is NEXP-complete even with a known model (Bernstein et al., 2002)), we first introduce an approach for a stable single MARL task, which allows agents to learn coordination while simultaneously learning Q values required to calculate a uniform MT-MARL policy."}, {"heading": "5.1.1. DECENTRALIZED HYSTERETIC DEEP RECURRENT Q-NETWORKS (DEC-HDRQNS)", "text": "Due to partial observability and local non-stationarity, the model-based Dec-POMDP MARL is extremely challenging (Banerjee et al., 2012).Our approach is model-free and decentralized, with Q values determined for each actor. Unlike policy tables or FSCs, Q values are accessible to the multi-task distillation process because they by their nature measure the quality of all measures and not just the optimal implementation. Overly optimistic MARL approaches (e.g. Distributed Qlearning (Lauer & Riedmiller, 2000) ignore low yields caused by exploratory measures, leading to a strong overestimation of Q values in stochastic areas. Hysteretic Qlearning (Matignon et al., 2007) instead uses the finding that low yields can also be caused by negative yields."}, {"heading": "5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS)", "text": "In fact, this is a very complex and complex matter, in which both sides are in a very difficult situation, in which both sides have agreed on a common denominator."}, {"heading": "5.1.3. TRAINING DEC-HDRQNS USING CERTS", "text": "This ensures that our approach is fully decentralized and explicitly communicated. < p (i) < p (i), p (i), p (i), p (i), p (i), p (i), p (i), p (i), p (i), p (i), p (i), p (i), p (i), p (i), p (i), p (i), p (i), p (i), p (i), p (i), p), p (i), p (i), p (i), p (i), p (i), p (i), p (i), p (i), p (i), p (i), p (i)."}, {"heading": "5.2. Phase II: Dec-POMDP MT-MARL", "text": "Following task specialization, the second phase involves distillation of DRQNs = > Q > Q into a unified DRQN that performs well in all tasks without explicit provision of task ID. With DRQNs, our approach expands the single-agent, fully observable MTRL method proposed by Rusu et al. (2015). < o (i), a (i), r, o (i) > in Eqs. (3) to (6). Each task can be treated as a regression problem over Q values. During multi-task learning, our approach leads iteratively to QQQi values and regression. For data collection, the agents use each specialized DRQN (from Phase I) to execute measures that lead to corresponding tasks."}, {"heading": "6. Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Task Specialization using Dec-HDRQN", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "6.2. Multi-tasking using Distilled Dec-HDRQN", "text": "We are now evaluating the distillation of specialized Dec-HDRQN strategies (as learned in Section 6.1) for MT-MARL. A first approach is to refrain from specialization and learn directly a Dec-HDRQN by using a pool of experience from all tasks. This approach, called Multi-DQN by Rusu et al. (2015), is prone to convergence problems, even in a single agent, fully observable constellations. In Fig. 5, we compare these approaches (calling ours \"distilled\" and MultiHDRQN \"multi\"), both approaches have been trained to perform multi-tasking on 2-agent MAMT tasks ranging from 3 x-3 to 6 x-6, with Pf = 0.3. Our distillation approach does not use task-specific MLP layers, unlike Rusu et al. (2015), due to our increased task-time observability and lack of task identification."}, {"heading": "7. Contribution", "text": "Our approach combines hysterical learners, DRQNs, CERTs and distillation and has been proven to achieve multi-agent coordination with a single common strategy across a range of Dec-POMDP tasks with scant rewards, although they are not provided with task identities during execution. The parametric nature of the capture tasks used for evaluation (e.g. variations in grid size, target assignments and dynamics, sensor failure probabilities) makes them good candidates for ongoing benchmarks of multi-agent multitask learning. Future work will examine the inclusion of skills (macro actions) in the framework, the extension to areas with heterogeneous agents, and evaluation into more complex areas with a much larger number of tasks."}, {"heading": "Acknowledgements", "text": "The authors thank the anonymous reviewers for their insightful feedback and suggestions, supported by Boeing Research & Technology, ONR MURI Grant N000141110688 and BRC Grant N000141712072."}, {"heading": "A. Multi-agent Multi-target (MAMT) Domain Overview", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B. Empirical Results: Learning on Multi-agent Single-Target (MAST) Domain", "text": "Multi-Agent Single-Target (MAST) domain results for Dec-DRQN and Dec-HDRQN, with 2 agents and Pf = 0.0 (observation flickers disabled) These results mainly illustrate that Dec-DRQN sometimes has some empirical success in low-noise, low-state domains. Note that Dec-HDRQN significantly outperforms Dec-DRQN in the 8 x 8 task, which converges to a sub-optimal policy despite the simplicity of the domain."}, {"heading": "C. Empirical Results: Learning on MAMT Domain", "text": "Multi-agent single-target (MAMT) domain results, with 2 agents and Pf = 0.3 (observation flickering disabled). We also evaluated the performance of an inter-agent parameter sharing (a centralized approach) in Dec-DRQN (which we called Dec-DRQN-PS). Additionally, the performance of a double DQN was classified as negligible (Dec-DRQN).0.0 10K 20K 30K 40KTraining Epoch0.00.20.40.60.81.0E mp iric a lR e turn3 \u00d7 3 (Dec-DRQN) 4 \u00b7 4 (Dec-DRQN) 3 \u00b7 3 (Dec-DRQN) 4 \u00d7 4 (Dec-Dec-Dec) domlyinitialized games.0.0 10K 30K 40KTraining Epoch0.00.40.6Q (Dec-HDRQN) (a) Empirical yields during training."}, {"heading": "E. Empirical Results: Learning Sensitivity to Dec-HDRQN Recurrent Training Tracelength Parameter \u03c4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D. Empirical Results: Learning Sensitivity to Dec-HDRQN Negative Learning Rate \u03b2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "F. Empirical Results: Multi-tasking Performance Comparison", "text": "The following diagrams show the multitasking performance of both the distillation and multi-HDRQN approaches. Both approaches were trained on the 3 x 3 to 6 x 6 MAMT tasks. Multi-DRQN failed to achieve a specific performance for all tasks despite 500K training periods. In contrast, the proposed MT-MARL distillation approach achieves a nominal performance after 100K periods."}], "references": [{"title": "Incremental policy generation for finitehorizon DEC-POMDPs", "author": ["Amato", "Christopher", "Dibangoye", "Jilles Steeve", "Zilberstein", "Shlomo"], "venue": "In ICAPS,", "citeRegEx": "Amato et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Amato et al\\.", "year": 2009}, {"title": "Sample bounded distributed reinforcement learning for decentralized POMDPs", "author": ["Banerjee", "Bikramjit", "Lyle", "Jeremy", "Kraemer", "Landon", "Yellamraju", "Rajesh"], "venue": "In AAAI,", "citeRegEx": "Banerjee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2012}, {"title": "A robust approach for multi-agent natural resource allocation based on stochastic optimization algorithms", "author": ["Barbalios", "Nikos", "Tzionas", "Panagiotis"], "venue": "Applied Soft Computing,", "citeRegEx": "Barbalios et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Barbalios et al\\.", "year": 2014}, {"title": "Practical recommendations for gradientbased training of deep architectures", "author": ["Bengio", "Yoshua"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "Bengio and Yoshua.,? \\Q2012\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2012}, {"title": "The complexity of decentralized control of markov decision processes", "author": ["Bernstein", "Daniel S", "Givan", "Robert", "Immerman", "Neil", "Zilberstein", "Shlomo"], "venue": "Mathematics of operations research,", "citeRegEx": "Bernstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2002}, {"title": "Multiagent learning using a variable learning rate", "author": ["Bowling", "Michael", "Veloso", "Manuela"], "venue": "Artificial Intelligence,", "citeRegEx": "Bowling et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bowling et al\\.", "year": 2002}, {"title": "Sample complexity of multi-task reinforcement learning", "author": ["Brunskill", "Emma", "Li", "Lihong"], "venue": "arXiv preprint arXiv:1309.6821,", "citeRegEx": "Brunskill et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Brunskill et al\\.", "year": 2013}, {"title": "Multi-agent reinforcement learning: An overview", "author": ["Bu\u015foniu", "Lucian", "Babu\u0161ka", "Robert", "De Schutter", "Bart"], "venue": "In Innovations in multi-agent systems and applications-1,", "citeRegEx": "Bu\u015foniu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bu\u015foniu et al\\.", "year": 2010}, {"title": "Multitask learning. In Learning to learn, pp. 95\u2013133", "author": ["Caruana", "Rich"], "venue": null, "citeRegEx": "Caruana and Rich.,? \\Q1998\\E", "shortCiteRegEx": "Caruana and Rich.", "year": 1998}, {"title": "The dynamics of reinforcement learning in cooperative multiagent systems", "author": ["Claus", "Caroline", "Boutilier", "Craig"], "venue": "AAAI/IAAI, 1998:746\u2013752,", "citeRegEx": "Claus et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Claus et al\\.", "year": 1998}, {"title": "Multi-agent systems by incremental gradient reinforcement learning", "author": ["Dutech", "Alain", "Buffet", "Olivier", "Charpillet", "Fran\u00e7ois"], "venue": "In Proc. of the International Joint Conf. on Artificial Intelligence,", "citeRegEx": "Dutech et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Dutech et al\\.", "year": 2001}, {"title": "Probabilistic policy reuse in a reinforcement learning agent", "author": ["Fern\u00e1ndez", "Fernando", "Veloso", "Manuela"], "venue": "In Proc. of the fifth international joint conf. on Autonomous agents and multiagent sys.,", "citeRegEx": "Fern\u00e1ndez et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Fern\u00e1ndez et al\\.", "year": 2006}, {"title": "Stabilising experience replay for deep multi-agent reinforcement learning", "author": ["Foerster", "Jakob", "Nardelli", "Nantas", "Farquhar", "Gregory", "Torr", "Philip", "Kohli", "Pushmeet", "Whiteson", "Shimon"], "venue": "arXiv preprint arXiv:1702.08887,", "citeRegEx": "Foerster et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Foerster et al\\.", "year": 2017}, {"title": "Learning to communicate to solve riddles with deep distributed recurrent Q-networks", "author": ["Foerster", "Jakob N", "Assael", "Yannis M", "de Freitas", "Nando", "Whiteson", "Shimon"], "venue": "arXiv preprint arXiv:1602.02672,", "citeRegEx": "Foerster et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Foerster et al\\.", "year": 2016}, {"title": "Predicting and preventing coordination problems in cooperative Q-learning systems", "author": ["Fulda", "Nancy", "Ventura", "Dan"], "venue": "In IJCAI,", "citeRegEx": "Fulda et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fulda et al\\.", "year": 2007}, {"title": "Stable function approximation in dynamic programming", "author": ["Gordon", "Geoffrey J"], "venue": "In Proc. of the twelfth international conf. on machine learning,", "citeRegEx": "Gordon and J.,? \\Q1995\\E", "shortCiteRegEx": "Gordon and J.", "year": 1995}, {"title": "Deep recurrent Qlearning for partially observable MDPs", "author": ["Hausknecht", "Matthew", "Stone", "Peter"], "venue": "arXiv preprint arXiv:1507.06527,", "citeRegEx": "Hausknecht et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hausknecht et al\\.", "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["Hinton", "Geoffrey", "Vinyals", "Oriol", "Dean", "Jeff"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural comp.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["Kaelbling", "Leslie Pack", "Littman", "Michael L", "Cassandra", "Anthony R"], "venue": "Artificial intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Reinforcement learning of coordination in cooperative multi-agent systems", "author": ["Kapetanakis", "Spiros", "Kudenko", "Daniel"], "venue": null, "citeRegEx": "Kapetanakis et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kapetanakis et al\\.", "year": 2002}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "An algorithm for distributed reinforcement learning in cooperative multiagent systems", "author": ["Lauer", "Martin", "Riedmiller"], "venue": "In Proc. of the Seventeenth International Conf. on Machine Learning. Citeseer,", "citeRegEx": "Lauer et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lauer et al\\.", "year": 2000}, {"title": "The world of independent learners is not markovian", "author": ["Laurent", "Guillaume J", "Matignon", "La\u00ebtitia", "Fort-Piat", "Le"], "venue": "International Journal of Knowledge-based and Intelligent Engineering Systems,", "citeRegEx": "Laurent et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Laurent et al\\.", "year": 2011}, {"title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "author": ["Lin", "Long-Ji"], "venue": "Machine learning,", "citeRegEx": "Lin and Long.Ji.,? \\Q1992\\E", "shortCiteRegEx": "Lin and Long.Ji.", "year": 1992}, {"title": "Stick-breaking policy learning in Dec-POMDPs", "author": ["Liu", "Miao", "Amato", "Christopher", "Liao", "Xuejun", "Carin", "Lawrence", "How", "Jonathan P"], "venue": "In Proc. of the International Joint Conf. on Artificial Intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Learning for decentralized control of multiagent systems in large partially observable stochastic environments", "author": ["Liu", "Miao", "Amato", "Christopher", "Anesta", "Emily", "Griffith", "J. Daniel", "How", "Jonathan P"], "venue": "In AAAI,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Hysteretic Q-learning: an algorithm for decentralized reinforcement learning in cooperative multiagent teams", "author": ["Matignon", "La\u00ebtitia", "Laurent", "Guillaume J", "Le Fort-Piat", "Nadine"], "venue": "In IROS,", "citeRegEx": "Matignon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Matignon et al\\.", "year": 2007}, {"title": "Independent reinforcement learners in cooperative markov games: a survey regarding coordination problems", "author": ["Matignon", "Laetitia", "Laurent", "Guillaume J", "Le Fort-Piat", "Nadine"], "venue": "The Knowledge Engineering Review,", "citeRegEx": "Matignon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Matignon et al\\.", "year": 2012}, {"title": "A Concise Introduction to Decentralized POMDPs", "author": ["Oliehoek", "Frans A", "Amato", "Christopher"], "venue": null, "citeRegEx": "Oliehoek et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oliehoek et al\\.", "year": 2016}, {"title": "Optimal and approximate q-value functions for decentralized POMDPs", "author": ["Oliehoek", "Frans A", "Spaan", "Matthijs TJ", "Vlassis", "Nikos A"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Oliehoek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Oliehoek et al\\.", "year": 2008}, {"title": "A survey on transfer learning", "author": ["Pan", "Sinno Jialin", "Yang", "Qiang"], "venue": "IEEE Transactions on knowledge and data engineering,", "citeRegEx": "Pan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2010}, {"title": "Learning to cooperate via policy search", "author": ["Peshkin", "Leonid", "Kim", "Kee-Eung", "Meuleau", "Nicolas", "Kaelbling", "Leslie Pack"], "venue": "In Proc. of the Sixteenth conf. on Uncertainty in artificial intelligence,", "citeRegEx": "Peshkin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Peshkin et al\\.", "year": 2000}, {"title": "Policy distillation", "author": ["Rusu", "Andrei A", "Colmenarejo", "Sergio Gomez", "Gulcehre", "Caglar", "Desjardins", "Guillaume", "Kirkpatrick", "James", "Pascanu", "Razvan", "Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Hadsell", "Raia"], "venue": "arXiv preprint arXiv:1511.06295,", "citeRegEx": "Rusu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2015}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Multi-agent reinforcement learning: Independent vs. cooperative agents", "author": ["Tan", "Ming"], "venue": "In Proc. of the tenth international conf. on machine learning,", "citeRegEx": "Tan and Ming.,? \\Q1993\\E", "shortCiteRegEx": "Tan and Ming.", "year": 1993}, {"title": "Multitask reinforcement learning on the distribution of mdps", "author": ["Tanaka", "Fumihide", "Yamamura", "Masayuki"], "venue": "In Computational Intelligence in Robotics and Automation,", "citeRegEx": "Tanaka et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Tanaka et al\\.", "year": 2003}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["Taylor", "Matthew E", "Stone", "Peter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Taylor et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2009}, {"title": "Transfer learning. Handbook of Research on Machine Learning Applications and Trends: Algs", "author": ["Torrey", "Lisa", "Shavlik", "Jude"], "venue": "Methods, and Techniques,", "citeRegEx": "Torrey et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Torrey et al\\.", "year": 2009}, {"title": "Solving deep memory POMDPs with recurrent policy gradients", "author": ["Wierstra", "Daan", "Foerster", "Alexander", "Peters", "Jan", "Schmidhuber", "Juergen"], "venue": "In International Conf. on Artificial Neural Networks,", "citeRegEx": "Wierstra et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wierstra et al\\.", "year": 2007}, {"title": "Multi-task reinforcement learning: a hierarchical bayesian approach", "author": ["Wilson", "Aaron", "Fern", "Alan", "Ray", "Soumya", "Tadepalli", "Prasad"], "venue": "In Proc. of the 24th international conf. on Machine learning,", "citeRegEx": "Wilson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2007}, {"title": "Learning and transferring roles in multi-agent reinforcement", "author": ["Wilson", "Aaron", "Fern", "Alan", "Ray", "Soumya", "Tadepalli", "Prasad"], "venue": "In Proc. AAAI-08 Workshop on Transfer Learning for Complex Tasks,", "citeRegEx": "Wilson et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2008}, {"title": "Rollout sampling policy iteration for decentralized POMDPs", "author": ["Wu", "Feng", "Zilberstein", "Shlomo", "Chen", "Xiaoping"], "venue": "arXiv preprint arXiv:1203.3528,", "citeRegEx": "Wu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2012}, {"title": "Monte-carlo expectation maximization for decentralized POMDPs", "author": ["Wu", "Feng", "Zilberstein", "Shlomo", "Jennings", "Nicholas R"], "venue": "In Proc. of the International Joint Conf. on Artificial Intelligence,", "citeRegEx": "Wu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2013}, {"title": "Multiagent-based reinforcement learning for optimal reactive power dispatch", "author": ["Xu", "Yinliang", "Zhang", "Wei", "Liu", "Wenxin", "Ferrese", "Frank"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),", "citeRegEx": "Xu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "Each MTMARL task is formalized as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Bernstein et al., 2002), a general formulation for cooperative decision-making under uncertainty.", "startOffset": 107, "endOffset": 131}, {"referenceID": 33, "context": "Using CERTs and optimistic learners, well-performing distilled policies (Rusu et al., 2015) are learned for multi-agent domains.", "startOffset": 72, "endOffset": 91}, {"referenceID": 19, "context": "Such domains are formalized as Partially Observable Markov Decision Processes (POMDPs), defined as \u3008S,A, T ,R,\u03a9,O, \u03b3\u3009 (Kaelbling et al., 1998).", "startOffset": 118, "endOffset": 142}, {"referenceID": 39, "context": "As Recurrent Neural Networks (RNNs) inherently maintain an internal state ht to compress input history until timestep t, they have been demonstrated to be effective for learning POMDP policies (Wierstra et al., 2007).", "startOffset": 193, "endOffset": 216}, {"referenceID": 7, "context": "Multi-agent RL (MARL) involves a set of agents in a shared environment, which must learn to maximize their individual returns (Bu\u015foniu et al., 2010).", "startOffset": 126, "endOffset": 148}, {"referenceID": 7, "context": "Multi-agent RL (MARL) involves a set of agents in a shared environment, which must learn to maximize their individual returns (Bu\u015foniu et al., 2010). Our work focuses on cooperative settings, where agents share a joint return. Claus & Boutilier (1998) dichotomize MARL agents into two classes: Joint Action Learners (JALs) and Independent Learners (ILs).", "startOffset": 127, "endOffset": 252}, {"referenceID": 7, "context": "Unique challenges arise in MARL due to agent interactions during learning (Bu\u015foniu et al., 2010; Matignon et al., 2012).", "startOffset": 74, "endOffset": 119}, {"referenceID": 28, "context": "Unique challenges arise in MARL due to agent interactions during learning (Bu\u015foniu et al., 2010; Matignon et al., 2012).", "startOffset": 74, "endOffset": 119}, {"referenceID": 28, "context": "Another desired characteristic is robustness to alter-exploration, or drastic changes in policies due to exploratory actions of teammates (Matignon et al., 2012).", "startOffset": 138, "endOffset": 161}, {"referenceID": 40, "context": "MTRL is most beneficial when target tasks share common features (Wilson et al., 2007), and most challenging when the task ID is not explicitly specified to agents during execution \u2013 the setting addressed in this paper.", "startOffset": 64, "endOffset": 85}, {"referenceID": 32, "context": "Works include model-free gradient-ascent based methods (Peshkin et al., 2000; Dutech et al., 2001), simulator-supported methods to improve policies using a series of linear programs (Wu et al.", "startOffset": 55, "endOffset": 98}, {"referenceID": 10, "context": "Works include model-free gradient-ascent based methods (Peshkin et al., 2000; Dutech et al., 2001), simulator-supported methods to improve policies using a series of linear programs (Wu et al.", "startOffset": 55, "endOffset": 98}, {"referenceID": 42, "context": ", 2001), simulator-supported methods to improve policies using a series of linear programs (Wu et al., 2012), and modelbased approaches where agents learn in an interleaved fashion to reduce destabilization caused by concurrent learning (Banerjee et al.", "startOffset": 91, "endOffset": 108}, {"referenceID": 1, "context": ", 2012), and modelbased approaches where agents learn in an interleaved fashion to reduce destabilization caused by concurrent learning (Banerjee et al., 2012).", "startOffset": 136, "endOffset": 159}, {"referenceID": 43, "context": "Recent scalable methods use Expectation Maximization to learn finite state controller (FSC) policies (Wu et al., 2013; Liu et al., 2015; 2016).", "startOffset": 101, "endOffset": 142}, {"referenceID": 25, "context": "Recent scalable methods use Expectation Maximization to learn finite state controller (FSC) policies (Wu et al., 2013; Liu et al., 2015; 2016).", "startOffset": 101, "endOffset": 142}, {"referenceID": 28, "context": "This simple approach has some empirical success (Matignon et al., 2012).", "startOffset": 48, "endOffset": 71}, {"referenceID": 27, "context": "Hysteretic Q-learning (Matignon et al., 2007) addresses miscoordination using cautious optimism to stabilize policies while teammates explore.", "startOffset": 22, "endOffset": 45}, {"referenceID": 44, "context": "Its track record of empirical success against complex methods (Xu et al., 2012; Matignon et al., 2012; Barbalios & Tzionas, 2014) leads us to use it as a foundation for our MT-MARL approach.", "startOffset": 62, "endOffset": 129}, {"referenceID": 28, "context": "Its track record of empirical success against complex methods (Xu et al., 2012; Matignon et al., 2012; Barbalios & Tzionas, 2014) leads us to use it as a foundation for our MT-MARL approach.", "startOffset": 62, "endOffset": 129}, {"referenceID": 25, "context": "Matignon et al. (2012) survey these approaches, the most straightforward being Decentralized Qlearning (Tan, 1993), where each agent performs independent Q-learning.", "startOffset": 0, "endOffset": 23}, {"referenceID": 25, "context": "Matignon et al. (2012) survey these approaches, the most straightforward being Decentralized Qlearning (Tan, 1993), where each agent performs independent Q-learning. This simple approach has some empirical success (Matignon et al., 2012). Distributed Q-learning (Lauer & Riedmiller, 2000) is an optimal algorithm for deterministic domains; it updates Q-values only when they are guaranteed to increase, and the policy only for actions that are no longer greedy with respect to Q-values. Bowling & Veloso (2002) conduct Policy Hill Climbing using the Win-or-Learn Fast heuristic to decrease (increase) each agent\u2019s learning rate when it performs well (poorly).", "startOffset": 0, "endOffset": 511}, {"referenceID": 12, "context": "Foerster et al. (2016) present architectures to learn communication protocols for DecPOMDP RL, noting best performance using a centralized approach with inter-agent backpropagation and parameter sharing.", "startOffset": 0, "endOffset": 23}, {"referenceID": 12, "context": "Foerster et al. (2016) present architectures to learn communication protocols for DecPOMDP RL, noting best performance using a centralized approach with inter-agent backpropagation and parameter sharing. They also evaluate a model combining Decentralized Q-learning with DRQNs, which they call Reinforced Inter-Agent Learning. Given the decentralized nature of this latter model (called Dec-DRQN herein for clarity), we evaluate our method against it. Concurrent to our work, Foerster et al. (2017) investigated an alternate means of stabilizing experience replay for the centralized learning case.", "startOffset": 0, "endOffset": 499}, {"referenceID": 41, "context": "They extend the work to multi-agent MDPs by learning characteristic agent roles (Wilson et al., 2008).", "startOffset": 80, "endOffset": 101}, {"referenceID": 17, "context": "Recent work extends the notion of neural network distillation (Hinton et al., 2015) to DQNs for single-agent, fully-observable MTRL, first learning a set of specialized teacher DQNs, then distilling teachers to a single multi-task network (Rusu et al.", "startOffset": 62, "endOffset": 83}, {"referenceID": 33, "context": ", 2015) to DQNs for single-agent, fully-observable MTRL, first learning a set of specialized teacher DQNs, then distilling teachers to a single multi-task network (Rusu et al., 2015).", "startOffset": 163, "endOffset": 182}, {"referenceID": 37, "context": "Wilson et al. (2007) consider TL for MDPs, learning a Dirichlet Process Mixture Model over source MDPs, used as an informative prior for a target MDP.", "startOffset": 0, "endOffset": 21}, {"referenceID": 37, "context": "Wilson et al. (2007) consider TL for MDPs, learning a Dirichlet Process Mixture Model over source MDPs, used as an informative prior for a target MDP. They extend the work to multi-agent MDPs by learning characteristic agent roles (Wilson et al., 2008). Brunskill & Li (2013) introduce an MDP clustering approach that reduces negative transfer in MTRL, and prove reduction of sample complexity of exploration using transfer.", "startOffset": 0, "endOffset": 276}, {"referenceID": 35, "context": "Taylor et al. (2013) introduce parallel transfer to accelerate multi-agent learning using interagent transfer.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "\u00d7iA is the joint action space, and \u03a9 = \u00d7i\u03a9 is the joint observation space (Bernstein et al., 2002).", "startOffset": 74, "endOffset": 98}, {"referenceID": 30, "context": "For simplicity, we consider only pure joint policies, as finite-horizon Dec-POMDPs have at least one pure joint optimal policy (Oliehoek et al., 2008).", "startOffset": 127, "endOffset": 150}, {"referenceID": 1, "context": "This assumption is consistent with prior work in MARL (Banerjee et al., 2012; Peshkin et al., 2000).", "startOffset": 54, "endOffset": 99}, {"referenceID": 32, "context": "This assumption is consistent with prior work in MARL (Banerjee et al., 2012; Peshkin et al., 2000).", "startOffset": 54, "endOffset": 99}, {"referenceID": 23, "context": "However, the domain appears non-stationary from the perspective of each Dec-POMDP agent, a property we formalize by extending the definition by Laurent et al. (2011). Definition 1.", "startOffset": 144, "endOffset": 166}, {"referenceID": 4, "context": "As Dec-POMDP RL is notoriously complex (and solving for the optimal policy is NEXP-complete even with a known model (Bernstein et al., 2002)), we first introduce an approach for stable single-task MARL.", "startOffset": 116, "endOffset": 140}, {"referenceID": 1, "context": "Due to partial observability and local non-stationarity, model-based Dec-POMDP MARL is extremely challenging (Banerjee et al., 2012).", "startOffset": 109, "endOffset": 132}, {"referenceID": 27, "context": "Hysteretic Qlearning (Matignon et al., 2007), instead, uses the insight that low returns may also be caused by domain stochasticity, which should not be ignored.", "startOffset": 21, "endOffset": 44}, {"referenceID": 44, "context": "Hysteretic Q-learning has enjoyed a strong empirical track record in fully-observable MARL (Xu et al., 2012; Matignon et al., 2012; Barbalios & Tzionas, 2014), exhibiting similar performance as more complex approaches.", "startOffset": 91, "endOffset": 158}, {"referenceID": 28, "context": "Hysteretic Q-learning has enjoyed a strong empirical track record in fully-observable MARL (Xu et al., 2012; Matignon et al., 2012; Barbalios & Tzionas, 2014), exhibiting similar performance as more complex approaches.", "startOffset": 91, "endOffset": 158}, {"referenceID": 13, "context": "Despite the benefits in single-agent settings, existing MARL approaches have found it necessary to disable experience replay (Foerster et al., 2016).", "startOffset": 125, "endOffset": 148}, {"referenceID": 33, "context": "Using DRQNs, our approach extends the single-agent, fully-observable MTRL method proposed by Rusu et al. (2015) to Dec-POMDP MT-MARL.", "startOffset": 93, "endOffset": 112}, {"referenceID": 33, "context": "We refer readers to Rusu et al. (2015) for additional analysis of the distillation loss.", "startOffset": 20, "endOffset": 39}, {"referenceID": 0, "context": "Performance is evaluated on both multi-agent single-target (MAST) and multi-agent multi-target (MAMT) capture domains, variations of the existing meeting-in-a-grid DecPOMDP benchmark (Amato et al., 2009).", "startOffset": 183, "endOffset": 203}, {"referenceID": 0, "context": "Performance is evaluated on both multi-agent single-target (MAST) and multi-agent multi-target (MAMT) capture domains, variations of the existing meeting-in-a-grid DecPOMDP benchmark (Amato et al., 2009). Agents i \u2208 {1, . . . , n} in an m \u00d7 m toroidal grid receive +1 terminal reward only when they simultaneously capture moving targets (1 target in MAST, and n targets in MAMT). Each agent always observes its own location, but only sometimes observes targets\u2019 locations. Target dynamics are unknown to agents and vary across tasks. Similar to the Pong POMDP domain of Hausknecht & Stone (2015), our domains include observation flickering: in each timestep, observations of targets are sometimes obscured, with probability Pf .", "startOffset": 184, "endOffset": 596}, {"referenceID": 30, "context": "icy space for agent i at timestep t is O(|A| |\u03a9(i)|t\u22121 |\u03a9(i)|\u22121 ) (Oliehoek et al., 2008), where |A| = 5, |\u03a9| = m for MAST, and |\u03a9| = m for MAMT.", "startOffset": 66, "endOffset": 89}, {"referenceID": 12, "context": "A centralized-learning variation of Dec-DRQN with interagent parameter sharing (similar to RIAL-PS in Foerster et al. (2016)) was also tested, but was not found to improve performance (see supplementary material).", "startOffset": 102, "endOffset": 125}, {"referenceID": 33, "context": "This approach, called Multi-DQN by Rusu et al. (2015), is susceptible to convergence issues even in single-agent, fully-observable settings.", "startOffset": 35, "endOffset": 54}, {"referenceID": 33, "context": "This approach, called Multi-DQN by Rusu et al. (2015), is susceptible to convergence issues even in single-agent, fully-observable settings. In Fig. 5, we compare these approaches (where we label ours as \u2018Distilled\u2019, and MultiHDRQN as \u2018Multi\u2019). Both approaches were trained to perform multi-tasking on 2-agent MAMT tasks ranging from 3\u00d73 to 6\u00d76, with Pf = 0.3. Our distillation approach uses no task-specific MLP layers, unlike Rusu et al. (2015), due to our stronger assumptions on task relatedness and lack of execution-time observability of task identity.", "startOffset": 35, "endOffset": 447}], "year": 2017, "abstractText": "Many real-world tasks involve multiple agents with partial observability and limited communication. Learning is challenging in these settings due to local viewpoints of agents, which perceive the world as non-stationary due to concurrentlyexploring teammates. Approaches that learn specialized policies for individual tasks face problems when applied to the real world: not only do agents have to learn and store distinct policies for each task, but in practice identities of tasks are often non-observable, making these approaches inapplicable. This paper formalizes and addresses the problem of multi-task multi-agent reinforcement learning under partial observability. We introduce a decentralized single-task learning approach that is robust to concurrent interactions of teammates, and present an approach for distilling single-task policies into a unified policy that performs well across multiple related tasks, without explicit provision of task identity.", "creator": "LaTeX with hyperref package"}}}