{"id": "1703.06217", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2017", "title": "Deciding How to Decide: Dynamic Routing in Artificial Neural Networks", "abstract": "We propose and systematically evaluate three strategies for training dynamically-routed artificial neural networks: graphs of learned transformations through which different input signals may take different paths. Though some approaches have advantages over others, the resulting networks are often qualitatively similar. We find that, in dynamically-routed networks trained to classify images, layers and branches become specialized to process distinct categories of images. Additionally, given a fixed computational budget, dynamically-routed networks tend to perform better than comparable statically-routed networks.", "histories": [["v1", "Fri, 17 Mar 2017 23:52:14 GMT  (3551kb,D)", "http://arxiv.org/abs/1703.06217v1", "Submitted to ICML 2017"], ["v2", "Tue, 12 Sep 2017 22:14:36 GMT  (2257kb,D)", "http://arxiv.org/abs/1703.06217v2", "ICML 2017. Code atthis https URLVideo abstract atthis https URL"]], "COMMENTS": "Submitted to ICML 2017", "reviews": [], "SUBJECTS": "stat.ML cs.CV cs.LG cs.NE", "authors": ["mason mcgill", "pietro perona"], "accepted": true, "id": "1703.06217"}, "pdf": {"name": "1703.06217.pdf", "metadata": {"source": "META", "title": "Deciding How to Decide:  Dynamic Routing in Artificial Neural Networks", "authors": ["Mason McGill", "Pietro Perona"], "emails": ["@caltech.edu>."], "sections": [{"heading": "1. Introduction", "text": "Some decisions are easier to make than others - for example, large, unlocked objects are easier to detect. > In addition, different difficult decisions require different expertise - an avid bird watcher may know very little about identifying cars. We assume that complex decision-making techniques such as visual classification can be meaningfully divided into specialized subtasks, and that a system that performs a complex task should first try to identify the subtask that is submitted to it, and then use that information to select the most appropriate algorithm for its solution. This approach - dynamically transmitting signals through an inference system based on their content - has already been integrated into Machine Vision Pipelines, using methods such as increasing Viola et al., 2005), coarse to fine cascades (Zhou et al., 2013), and random decision-making forests (Ho, 1995)."}, {"heading": "2. Related work", "text": "More recently, Kontschieder et al. (2015) conducted a joint optimization of ANN and decision tree parameters, and Bulo & Kontschieder (2014) used randomized multi-layer networks to calculate decision tree splitting functionality. To our knowledge, the family of inference systems we discussed was first described by Denoyer & Gallinari (2014). Furthermore, Bengio et al. (2015) investigated dynamically skipped layers in neural networks and Ioannou et al. (2016) examined dynamic routing systems in uniform-length networks. Some recently developed visual recognition systems perform cascaded evaluations of revolutionary neural network layers (Li et al., 2015; Cai et al., 2015; Girick, 2015; these methods of cost and cost recognition cannot be efficiently applied."}, {"heading": "3. Setup", "text": "In a statically routed, forward-facing artificial neural network, each layer transforms a single input feature vector into a single output feature vector. The output feature vector is then used as input to the following layer (which we will call the sink of the current layer) if it exists, or as output to the network as a whole if it does not. We consider networks in which layers may have more than one sink. In such a network, the network must make a decision for each n-way intersection that reaches a signal, i.e. that the signal propagates through the ith sink if and only if dj = i (shown in fig. 2). We calculate dj as the argmax of the score vector sj, a learned function of the last feature vector that is calculated before we reach j."}, {"heading": "3.1. Multipath architectures for convolutional networks", "text": "It is unreasonable to expect that such a feature vector can explicitly encode the global information needed to decide how to route the entire signal (e.g., in the case of object recognition, whether the image was taken indoors, whether the image contains an animal or the frequency of occlusion in the scene).To achieve this, instead of a two-dimensional array of local features at each level, we calculate a feature pyramid (similar to the pyramids described by Ke et al. (2016)) with local descriptors at the bottom and global descriptors at the top. At each node j, the score vector sj is calculated by a small routing network operating on the most recently calculated global descriptor. Our multi-path architecture is illustrated in Fig. 3."}, {"heading": "3.2. Balancing accuracy and efficiency", "text": "For a given input, a network directive, and a series of routing decisions d, we define the cost of performing inferences: cinf (\u03bd, d) = cerr (\u03bd, d) + ccpt (\u03bd, d), (1) where cerr (\u03bd, d) is the cost of the inference errors caused by the network, and ccpt (\u03bd, d) is the cost of the calculation. In our experiments, cerr, unless otherwise specified, is the crossentropy loss, andccpt (\u03bd, d) = kcptnops (\u03bd, d), (2) where nops (\u03bd, d) is the number of multiple operations and kcpt is a scalar hyperparameter. This definition assumes a time or energy-limited system - each operation consumes approximately the same amount of time and energy, so that each operation is equally consuming. ccpt can be defined differently under other constraints (e.g. memory bandwidth)."}, {"heading": "4. Training", "text": "We propose three approaches to train dynamically routed networks, as well as complementary approaches to regulation and optimization and a method to adapt to changes in the cost of calculation. 4 x 48 x 816 x 1632 x 32 Folding, Batch Normalization, RectificationLinear Transformation, Batch Normalization, RectificationLinear Transformation, SoftmaxActive Path \"Horse\" Linear Transformation, Argmax"}, {"heading": "4.1. Training strategy I: actor learning", "text": "Since d is discrete, cinf (\u03bd, d) cannot be minimized by gradient-based methods. However, if d is replaced by a stochastic approximation d during training, we can reduce the gradient of E [cinf (\u03bd, d)] to a value not equal to zero. We can then simultaneously learn the routing parameters and classification parameters by minimizing the lossy Lac = E [cinf (\u03bd, d)]. (3) In our experiments, the training routing strategies were based on d parameters such as Pr (d-j = i) = Softmax (sj / \u03c4) i, (4) where \u03c4 is the network \"temperature\": a scalar hyperparameter that decays during training, with the training convergence-routing policy converging with the inference-routing policy."}, {"heading": "4.2. Training strategy II: pragmatic critic learning", "text": "Alternatively, we can try to predict the estimated cost of each route decision. In this case, we minimize the lossLcr = E cinf (\u03bd, d) + \u2211 j-J cjcre, (5) where J is the number of intersections that occur when deciding on the route, and ccre is the cost return error that is defined: cjcre = kcre-sj-uj-2, (6) where J = \u2212 cinf (\u03bdij, d), (7) kcre is a scalar hyperparameter, and \u03bdij is the subnetwork that consists of the child of \u03bdj and all his offspring. Since we want to learn the guidelines indirectly (via cost forecast), d is treated as constant in terms of optimization."}, {"heading": "4.3. Training strategy III: optimistic critic learning", "text": "In order to improve the stability of the loss and hopefully speed up the training, we can adjust the routing utility function u so that for each node j is independent of the routing parameters downstream of j. Instead of predicting the cost of routing decisions in light of current downstream routing policy, we can predict the cost of routing decisions in light of the optimal downstream routing policy. In this optimistic variant of the critique method uij = \u2212 mind \u2032 (cinf (\u03bdij, d \u2032)). (8)"}, {"heading": "4.4. Regularization", "text": "Many regulatory techniques involve adding a model complexity term, cmod, to the loss function in order to influence learning, effectively imposing soft constraints on network parameters (Hoerl & Kennard, 1970; Rudin et al., 1992; Tibshirani, 1996). However, if such a term affects layers in a way that is independent of the amount of signal they transmit, it will restrict commonly used layers to either underused or overused layers. To support both frequently and rarely used layers, we regulate subnetworks as they are through d rather than directly regulating the entire network. For example, to apply L2 regulation to critical networks, we define cmod: cmod = E [kL2-w-W w2], (9) where W is the amount of weights activated by the layers through d-mod, and Lk2 is a hyperparameter."}, {"heading": "4.5. Adjusting learning rates to compensate for throughput variations", "text": "Both training techniques attempt to minimize the expected cost of conducting conclusions with the network, via the training routing guidelines. (With this setup, if we use a constant learning rate for each layer on the network, then layers through which the policy route examples more often receive larger parameter updates, as they contribute more to the expected costs. To allow each layer to learn as quickly as possible, we scale the learning rate of each layer \"dynamically by a factor \u03b1,\" so that the elementary variance of the loss gradient in relation to \"s parameters is independent of the amount of probability density routed through it. To\" derive from this, \"we consider an alternative routing policy that\" routes \"all signals though, then routes through subsequential layers that are d-based.\" With this policy, we can link the minibatch manufacturing parameters of the vector with the d'i-based ones in each training intervention."}, {"heading": "4.6. Responding to changes in the cost of computation", "text": "s routing behavior to dynamic ccpt, we can append its known parameters - in our case {kcpt} - to the input of each routing subnet to allow it to modulate routing policies. To adjust the scope of image functions and facilitate optimization, we express kcpt in cost units per ten million operations."}, {"heading": "4.7. Hyperparameters", "text": "In all our experiments, we use a mini-batch size nex of 128 and perform 80,000 training iterations. The weights of the last layers of routing networks are zero-initialized, and all other weights we initialize using the Xavier initialization method (Glorot & Bengio, 2010). All distortions are zero-initialized. We perform batch normalization (Ioffe & Szegedy, 2015) before each rectification process with a resolution of 1 \u00d7 10 \u2212 6 and an exponential shift of the average decay constant from 0.9 to 1.0 for actor networks and 0.1 for critic networks, and decays with a half-life of 10,000 iterations. kdec = 0.01, kcre = 0.001 \u00d7 10 \u2212 We improve these values by hybrid iteration, not hybrid iteration (IQ values)."}, {"heading": "4.8. Data augmentation", "text": "We expand our data with an approach that is popular for use with CIFAR-10 (Lin et al., 2013) (Srivastava et al., 2015) (Clevert et al., 2015). We expand each image by applying vertical and horizontal shifts that are evenly scanned out of the range [-4.4], and if the image originates from CIFAR-10, we rotate it horizontally with a probability of 0.5. We fill empty pixels resulting from shifts with the middle color of the image (after gamma decoding)."}, {"heading": "5. Experiments", "text": "We compare approaches to dynamic routing by training 153 networks to classify small images, varying strategies for policy learning, regulatory strategy, optimization strategy, architecture, calculation costs, and task details. Results of these experiments are presented in Figure 5-10."}, {"heading": "5.1. Comparing policy-learning strategies", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "5.2. Comparing regularization strategies", "text": "Based on our hybrid dataset experiments, regulating the flow of data, as described in Section 4.4, discourages networks from routing data along deep paths, thereby reducing peak accuracy. Moreover, a mechanism to promote exploration (in our case a kdec non-zero) seems to be necessary to train effective stakeholder networks."}, {"heading": "5.3. Comparing optimization strategies", "text": "The Throughput Adjustment of Learning Rates (TALR) described in Section 4.5 improves the performance of hybrid datasets of stakeholder and critic networks in computationally resource-rich, high-precision contexts."}, {"heading": "5.4. Comparing architectures", "text": "With a given computational budget, 2- and 3-way node architectures have a higher capacity than sub-trees with only 2-way nodes. On the hybrid dataset, we find, under tight computational constraints, that trees with a higher degree of branching reach higher accuracy rates, but without limitation, they tend to overmatch. In dynamically routed networks, early classification layers tend to exhibit high accuracy rates and push difficult decisions downstream. Even without energy contracts, end layers specialize in recognizing in recognizing certain image classes. These classes are usually related (they either all originate from MNIST or all originate from CIFAR-10.) In 2- and 3-way node networks, branches specialize to an even greater extent. (See Figures 6 and 7.)"}, {"heading": "5.5. Comparing specialized and adaptive networks", "text": "We train a single stakeholder network to classify images from the hybrid dataset under various computational constraints, using the approach described in Section 4.6 and selecting kcpt randomly for each training example from the set mentioned in Fig. 5. This network works in a similar way to a collection of 8 stakeholder networks trained with different static values from kcpt, over a significant central area of the accuracy / efficiency curve with an eight-fold reduction in memory consumption and training time."}, {"heading": "5.6. Exploring the effects of the decision difficulty distribution", "text": "To study the effects of the difficulty distribution of the inference task on the performance of dynamically routed networks, we train networks to classify images from CIFAR-10 and adjust the classification task to vary the frequency of difficult decisions (see Fig. 9). We call these variants CIFAR-2 - which label images as \"horse\" or \"other\" - and CIFAR-5 - which label images as \"cat,\" \"dog,\" \"deer,\" \"horse\" or \"other.\" In this experiment, we compare stakeholder networks (the most powerful networks in the first series of experiments) with architecturally customized static routed networks. We find that dynamic routing is more advantageous when the task involves many low-difficulty decisions, allowing the network to route more data over shorter distances. While dynamic routing offers only a small advantage over CIFAR-10, dynamic routing achieves peak static accuracy on a third-party IFAR network."}, {"heading": "5.7. Exploring the effects of model capacity", "text": "To test whether dynamic routing is beneficial in higher capacity situations, we train stakeholder networks and architecturally matched static routed networks to classify images from CIFAR-10 while varying the width of the networks (see Figure 10). Increasing model capacity either increases or does not affect the relative advantage of dynamically routed networks, suggesting that our approach is applicable to more complicated tasks."}, {"heading": "6. Discussion", "text": "Our experiments suggest that dynamically routed networks trained under mild computational constraints can operate two to three times more efficiently than comparable statistically routed networks without sacrificing performance. In addition, dynamically routed networks, despite their higher capacity, appear to be less susceptible to overfits. When designing a multi-path architecture, we suggest supporting early decision-making wherever possible, as cheap, simple routing networks appear to be better suited to very large networks (trained by decision sampling to conserve memory) or networks designed for applications with less frictionless cost-inference features - such as one where kcpt units exhibit error / operation. Adapting learning rates to compensate for throughput variations, as described in Section 4.5, may improve the performance of networks where individual components are trained with specialized network components."}, {"heading": "Acknowledgements", "text": "This work was financed by a generous donation from Google Inc. We would also like to thank Krzysztof Chalupka, Cristina Segalin and Oisin Mac Aodha for their thoughtful comments."}], "references": [{"title": "Conditional computation in neural networks for faster models", "author": ["Bengio", "Emmanuel", "Bacon", "Pierre-Luc", "Pineau", "Joelle", "Precup", "Doina"], "venue": "arXiv preprint arXiv:1511.06297,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Neural decision forests for semantic image labelling", "author": ["Bulo", "Samuel", "Kontschieder", "Peter"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Bulo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bulo et al\\.", "year": 2014}, {"title": "Learning complexity-aware cascades for deep pedestrian detection", "author": ["Cai", "Zhaowei", "Saberian", "Mohammad", "Vasconcelos", "Nuno"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Cai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2015}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["Clevert", "Djork-Arn\u00e9", "Unterthiner", "Thomas", "Hochreiter", "Sepp"], "venue": "arXiv preprint arXiv:1511.07289,", "citeRegEx": "Clevert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clevert et al\\.", "year": 2015}, {"title": "Deep sequential neural network", "author": ["Denoyer", "Ludovic", "Gallinari", "Patrick"], "venue": "arXiv preprint arXiv:1410.0510,", "citeRegEx": "Denoyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denoyer et al\\.", "year": 2014}, {"title": "Fast r-cnn", "author": ["Girshick", "Ross"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Girshick and Ross.,? \\Q2015\\E", "shortCiteRegEx": "Girshick and Ross.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In Aistats,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Separate visual pathways for perception and action", "author": ["Goodale", "Melvyn A", "Milner", "A David"], "venue": "Trends in neurosciences,", "citeRegEx": "Goodale et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Goodale et al\\.", "year": 1992}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Random decision forests", "author": ["Ho", "Tin Kam"], "venue": "In Document Analysis and Recognition,", "citeRegEx": "Ho and Kam.,? \\Q1995\\E", "shortCiteRegEx": "Ho and Kam.", "year": 1995}, {"title": "Ridge regression: Biased estimation for nonorthogonal problems", "author": ["Hoerl", "Arthur E", "Kennard", "Robert W"], "venue": null, "citeRegEx": "Hoerl et al\\.,? \\Q1970\\E", "shortCiteRegEx": "Hoerl et al\\.", "year": 1970}, {"title": "Decision forests, convolutional networks and the models in-between", "author": ["Ioannou", "Yani", "Robertson", "Duncan", "Zikic", "Darko", "Kontschieder", "Peter", "Shotton", "Jamie", "Brown", "Matthew", "Criminisi", "Antonio"], "venue": "arXiv preprint arXiv:1603.01250,", "citeRegEx": "Ioannou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ioannou et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Deep neural decision forests", "author": ["Kontschieder", "Peter", "Fiterau", "Madalina", "Criminisi", "Antonio", "Rota Bulo", "Samuel"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "Kontschieder et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kontschieder et al\\.", "year": 2015}, {"title": "A network for scene processing in the macaque temporal lobe", "author": ["Kornblith", "Simon", "Cheng", "Xueqi", "Ohayon", "Shay", "Tsao", "Doris Y"], "venue": null, "citeRegEx": "Kornblith et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kornblith et al\\.", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher JC"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "A convolutional neural network cascade for face detection", "author": ["Li", "Haoxiang", "Lin", "Zhe", "Shen", "Xiaohui", "Brandt", "Jonathan", "Hua", "Gang"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Patches with links: a unified system for processing faces in the macaque temporal lobe", "author": ["Moeller", "Sebastian", "Freiwald", "Winrich A", "Tsao", "Doris Y"], "venue": null, "citeRegEx": "Moeller et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Moeller et al\\.", "year": 2008}, {"title": "Stacked hourglass networks for human pose estimation", "author": ["Newell", "Alejandro", "Yang", "Kaiyu", "Deng", "Jia"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Newell et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Newell et al\\.", "year": 2016}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["Ren", "Shaoqing", "He", "Kaiming", "Girshick", "Ross", "Sun", "Jian"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Nonlinear total variation based noise removal algorithms", "author": ["Rudin", "Leonid I", "Osher", "Stanley", "Fatemi", "Emad"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "Rudin et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Rudin et al\\.", "year": 1992}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Neural trees: a new tool for classification", "author": ["JA Sirat", "Nadal", "JP"], "venue": "Network: Computation in Neural Systems,", "citeRegEx": "Sirat et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Sirat et al\\.", "year": 1990}, {"title": "Training very deep networks", "author": ["Srivastava", "Rupesh K", "Greff", "Klaus", "Schmidhuber", "J\u00fcrgen"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Tibshirani", "Robert"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "citeRegEx": "Tibshirani and Robert.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani and Robert.", "year": 1996}, {"title": "Perceptron trees: A case study in hybrid concept representations", "author": ["Utgoff", "Paul E"], "venue": "Connection Science,", "citeRegEx": "Utgoff and E.,? \\Q1989\\E", "shortCiteRegEx": "Utgoff and E.", "year": 1989}, {"title": "Detecting pedestrians using patterns of motion and appearance", "author": ["Viola", "Paul", "Jones", "Michael J", "Snow", "Daniel"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Viola et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Viola et al\\.", "year": 2005}, {"title": "Extensive facial landmark localization with coarse-to-fine convolutional network cascade", "author": ["Zhou", "Erjin", "Fan", "Haoqiang", "Cao", "Zhimin", "Jiang", "Yuning", "Yin", "Qi"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision Workshops,", "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 27, "context": "This approach\u2014dynamically routing signals through an inference system, based on their content\u2014has already been incorporated into machine vision pipelines via methods such as boosting (Viola et al., 2005), coarse-to-fine cascades (Zhou et al.", "startOffset": 183, "endOffset": 203}, {"referenceID": 28, "context": ", 2005), coarse-to-fine cascades (Zhou et al., 2013), and random decision forests (Ho, 1995).", "startOffset": 33, "endOffset": 52}, {"referenceID": 18, "context": "Dynamic routing is also performed in the primate visual system: spatial information is processed somewhat separately from object identity information (Goodale & Milner, 1992), and faces and other behaviorally-relevant stimuli ellicit responses in anatomically distinct, specialized regions (Moeller et al., 2008; Kornblith et al., 2013).", "startOffset": 290, "endOffset": 336}, {"referenceID": 14, "context": "Dynamic routing is also performed in the primate visual system: spatial information is processed somewhat separately from object identity information (Goodale & Milner, 1992), and faces and other behaviorally-relevant stimuli ellicit responses in anatomically distinct, specialized regions (Moeller et al., 2008; Kornblith et al., 2013).", "startOffset": 290, "endOffset": 336}, {"referenceID": 8, "context": "However, state-of-the-art artificial neural networks (ANNs) for visual inference are routed statically (Simonyan & Zisserman, 2014; He et al., 2016; Dosovitskiy et al., 2015; Newell et al., 2016); every input triggers an identical sequence of operations.", "startOffset": 103, "endOffset": 195}, {"referenceID": 19, "context": "However, state-of-the-art artificial neural networks (ANNs) for visual inference are routed statically (Simonyan & Zisserman, 2014; He et al., 2016; Dosovitskiy et al., 2015; Newell et al., 2016); every input triggers an identical sequence of operations.", "startOffset": 103, "endOffset": 195}, {"referenceID": 16, "context": "We propose three approaches to training these networks, test them on small image datasets synthesized from MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky & Hinton, 2009), and quantify the accuracy/efficiency trade-off that occurs when the network parameters are tuned to yield more aggressive early classification policies.", "startOffset": 113, "endOffset": 133}, {"referenceID": 17, "context": "Some recently-developed visual detection systems perform cascaded evaluation of convolutional neural network layers (Li et al., 2015; Cai et al., 2015; Girshick, 2015; Ren et al., 2015); though highly specialized for the task of visual detection, these modifications can radically improve efficiency.", "startOffset": 116, "endOffset": 185}, {"referenceID": 2, "context": "Some recently-developed visual detection systems perform cascaded evaluation of convolutional neural network layers (Li et al., 2015; Cai et al., 2015; Girshick, 2015; Ren et al., 2015); though highly specialized for the task of visual detection, these modifications can radically improve efficiency.", "startOffset": 116, "endOffset": 185}, {"referenceID": 20, "context": "Some recently-developed visual detection systems perform cascaded evaluation of convolutional neural network layers (Li et al., 2015; Cai et al., 2015; Girshick, 2015; Ren et al., 2015); though highly specialized for the task of visual detection, these modifications can radically improve efficiency.", "startOffset": 116, "endOffset": 185}, {"referenceID": 10, "context": "More recently, Kontschieder et al. (2015) performed joint optimization of ANN and decision tree parameters, and Bulo & Kontschieder (2014) used randomized multi-layer networks to compute decision tree split functions.", "startOffset": 15, "endOffset": 42}, {"referenceID": 10, "context": "More recently, Kontschieder et al. (2015) performed joint optimization of ANN and decision tree parameters, and Bulo & Kontschieder (2014) used randomized multi-layer networks to compute decision tree split functions.", "startOffset": 15, "endOffset": 139}, {"referenceID": 10, "context": "More recently, Kontschieder et al. (2015) performed joint optimization of ANN and decision tree parameters, and Bulo & Kontschieder (2014) used randomized multi-layer networks to compute decision tree split functions. To our knowledge, the family of inference systems we discuss was first described by Denoyer & Gallinari (2014). Additionally, Bengio et al.", "startOffset": 15, "endOffset": 329}, {"referenceID": 0, "context": "Additionally, Bengio et al. (2015) explored dynamically skipping layers in neural networks, and Ioannou et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 0, "context": "Additionally, Bengio et al. (2015) explored dynamically skipping layers in neural networks, and Ioannou et al. (2016) explored dynamic routing in networks with equallength paths.", "startOffset": 14, "endOffset": 118}, {"referenceID": 21, "context": "Regularization Many regularization techniques involve adding a modelcomplexity term, cmod, to the loss function to influence learning, effectively imposing soft constraints upon the network parameters (Hoerl & Kennard, 1970; Rudin et al., 1992; Tibshirani, 1996).", "startOffset": 201, "endOffset": 262}, {"referenceID": 24, "context": ", 2013) (Srivastava et al., 2015) (Clevert et al.", "startOffset": 8, "endOffset": 33}, {"referenceID": 3, "context": ", 2015) (Clevert et al., 2015).", "startOffset": 8, "endOffset": 30}, {"referenceID": 16, "context": "Comparing policy-learning strategies To compare routing strategies in the context of a simple dataset with a high degree of difficulty variation, we train networks to classify images from a small-image dataset synthesized from MNIST (LeCun et al., 1998) and CIFAR10 (Krizhevsky & Hinton, 2009) (see Fig.", "startOffset": 233, "endOffset": 253}], "year": 2017, "abstractText": "We propose and systematically evaluate three strategies for training dynamically-routed artificial neural networks: graphs of learned transformations through which different input signals may take different paths. Though some approaches have advantages over others, the resulting networks are often qualitatively similar. We find that, in dynamically-routed networks trained to classify images, layers and branches become specialized to process distinct categories of images. Additionally, given a fixed computational budget, dynamically-routed networks tend to perform better than comparable staticallyrouted networks.", "creator": "LaTeX with hyperref package"}}}