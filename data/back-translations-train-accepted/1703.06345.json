{"id": "1703.06345", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2017", "title": "Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks", "abstract": "Recent papers have shown that neural networks obtain state-of-the-art performance on several different sequence tagging tasks. One appealing property of such systems is their generality, as excellent performance can be achieved with a unified architecture and without task-specific feature engineering. However, it is unclear if such systems can be used for tasks without large amounts of training data. In this paper we explore the problem of transfer learning for neural sequence taggers, where a source task with plentiful annotations (e.g., POS tagging on Penn Treebank) is used to improve performance on a target task with fewer available annotations (e.g., POS tagging for microblogs). We examine the effects of transfer learning for deep hierarchical recurrent networks across domains, applications, and languages, and show that significant improvement can often be obtained. These improvements lead to improvements over the current state-of-the-art on several well-studied tasks.", "histories": [["v1", "Sat, 18 Mar 2017 20:21:44 GMT  (590kb,D)", "http://arxiv.org/abs/1703.06345v1", "Accepted as a conference paper at ICLR 2017. This is an extended version of the original paper (this https URL). The original paper proposes a new architecture, while this version focuses on transfer learning for a general model class"]], "COMMENTS": "Accepted as a conference paper at ICLR 2017. This is an extended version of the original paper (this https URL). The original paper proposes a new architecture, while this version focuses on transfer learning for a general model class", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["zhilin yang", "ruslan salakhutdinov", "william w cohen"], "accepted": true, "id": "1703.06345"}, "pdf": {"name": "1703.06345.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Zhilin Yang", "Ruslan Salakhutdinov", "William W. Cohen"], "emails": ["zhiliny@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "wcohen@cs.cmu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, it is so far that it will be able to erenie.n mention the aforementioned lcihsrc\u00fcehncS."}, {"heading": "2 RELATED WORK", "text": "There are two common paradigms for teaching natural language processing tasks (NLP), resource-based transfer and model-based transfer. Resource-based transfer uses additional linguistic annotations as weak supervision for transfer-specific learning processes, such as translingual dictionaries (Zirikly & Hagiwara, 2015), corporate (Wang & Manning, 2014) and word alignments (Yarowsky et al., 2001). Resource-based methods show considerable success in transferral transfer, but are highly sensitive to the scope and quality of the additional resources. Resource-based transfer is largely limited to transfer-specific in previous work, and there is no comprehensive research on extending resource-based methods to transfer-specific and cross-application settings."}, {"heading": "3 APPROACH", "text": "In this section, we present our approach to transfer learning. First, we present an abstract framework for marking neural sequences that summarizes previous work, and then discuss three different architectures of transfer learning."}, {"heading": "3.1 BASE MODEL", "text": "Although many different variants of neural networks have been suggested for the problem of sequence marking, we note that most models can be described with the hierarchical framework shown in Figure 1 (a). A character level takes a string (represented as embedding) as input and prints a representation that encodes the morphological information at character level. A character level then combines the character level with a word embedding and inserts the contextual information further to output a new feature representation. After two levels of featurextraction (encoding), the feature representation is fed through the character level to a conditional random level (CRF) that outputs the label sequence. Both the word level and the character level can be implemented as conventional neural networks (CNNs) or recursive neural networks (RNNNs) (Collobert et al., 2011; Chiu & Nichols, 2015; et)."}, {"heading": "3.2 TRANSFER LEARNING ARCHITECTURES", "text": "We are developing three architectures for transfer learning, T-A, T-B and T-C, which are shown in Figures 1 (b), 1 (c) and 1 (d), respectively. These three architectures are all extensions of the basic model discussed in the previous section with different parameter distribution schemes. We will now discuss the use cases for the different architectures."}, {"heading": "3.2.1 CROSS-DOMAIN TRANSFER", "text": "Because different domains are \"sublanguages\" that have domain-specific regularities, sequence taggers trained on one domain may not perform optimally in another domain. The goal of cross-domain transfer is to learn a sequence tagger that can transfer knowledge from a source domain to a target domain. We assume that there are few labels available in the target domain.There are two cases of cross-domain transfer: the two domains can have label sets that can be mapped to each other, or different label sets. For example, POS tags in Genia's biomedical corpus can be mapped to Penn Treebank tags (Barrett & Weber-Jahnke, 2014), while some POS tags in Twitter (e.g. \"URL\") cannot be mapped to Penn Treebank tags (Ritter et al., 2011).If the two domains have Cappable word parameters at the top level, we share the RB word parameter and model parameters at the top level."}, {"heading": "3.2.2 CROSS-APPLICATION TRANSFER", "text": "Similar to the motivation in (Collobert et al., 2011), it is generally desirable to exploit the underlying similarities and regularities of different applications and improve the performance of one application by sharing training with another. Furthermore, transfer between multiple applications can be helpful if the labels are limited. In the cross-application setting, we assume that several applications are in the same language. Since different applications use the same alphabet, the case is similar to cross-sector transfer with different sets of labels. We adopt the architecture of the T-B model for cross-application transfer learning, where only the CRF layers for different applications are separated from each other."}, {"heading": "3.2.3 CROSS-LINGUAL TRANSFER", "text": "In this paper, instead, we explore a complementary method that exploits the linguistic regularities exclusively at the model level (Zirikly & Hagiwara, 2015); model-level transfer learning is achieved by exploiting the morphologies common to both languages: for example, \"Canada\" in English and \"Canada\" in Spanish refer to the same named entity, and the morphological similarities can be used for NER- and POS-tagging with nouns."}, {"heading": "3.3 TRAINING", "text": "In the sections above, we have introduced three neural architectures with different parameter distribution schemes designed for different transfer learning settings. Now, we describe how we train the neural networks together for two tasks. Let's say we move from a source task s to a target task t, with the training instances being Xs and Xt. Let's have Ws and Wt specify the set of model parameters for the source and target task, respectively. Let's say the model parameters are divided into two sets, task-specific parameters and common parameters, i.e. Ws = Ws, spec = Wshared, Wt = Wshared, spec = Wshared, with the common parameters Wshared being jointly optimized by the two tasks, while task-specific parameters Ws, spec and Wt, spec are trained separately for each task. The training procedure is as follows. At each iteration, we try a task (i.e., either s or {s) based on a dual function, where we pre-distribute the}."}, {"heading": "3.4 MODEL IMPLEMENTATION", "text": "In this section we describe our implementation of the base model. Both the character level and the word level of the neural networks are implemented as RNNs. Specifically, we use gated recurrent units (GRUs) (Cho et al., 2014). Let (x1, x2, \u00b7 \u00b7, xT) be a sequence of input factors that can be embedded or hidden states of other levels. Let the GRU be hidden state at time step. Formally, a GRU unit at time step t can be expressed as margin (Wrxxt + Wrhht \u2212 1) zt = \u03c3 (Wzxxt + Wzhht \u2212 1) h = tanh (Whxxt \u2212 1) h type type type type type \u2212 1 + (1 \u2212 zt) h, h-level, where W's are model of each unit, h, h-type, h-type, h-level, h, h-h, h-h, h-h, h-h, h-level, h-h-h, h-h-h, h-h-level, h-h-h, h-h-h, h-level, h-h-h-h, h-h, h-h-level."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 DATASETS", "text": "We use the following benchmark datasets in our experiments: Penn Treebank (PTB) POS tagging, CoNLL 2000 chunking, CoNLL 2003 English NER, CoNLL 2002 Dutch NER, CoNLL 2002 Spanish NER, the Genia biomedical corpus (Kim et al., 2003), and a Twitter corpus (Ritter et al., 2011).The statistics of the datasets are in Table 1. We construct the POS tagging dataset with the instructions described in Toutanova et al. (2003). Note that the POS tags are extracted by default from the cut trees. For the CoNLL 2003 English NER dataset, we follow previous work (Collobert et al., 2011) to append one-sided gazetteer features to the input of the CRF layer in order to allow a fair comparison. As there is no standard training / development test data / JOS for the previous work, we follow the PTB tag for the PTB and the PTB tag for the previous work, and the PTB tag for the PTB."}, {"heading": "4.2 TRANSFER LEARNING PERFORMANCE", "text": "This year, it has reached the point where it is all about one person who is able to move, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to understand the world, to travel the world, to see the world, to travel the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to understand the world and to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to the world, to the world, to the world, to the world, to the world, to the world, to the world, to the world, to the world, to the world, to the world, to the world, to the world, to the world, to the world, to the world around the world, to the world, to the world around the world, to the world around the world, to the world around the world, to the world around the world, to the world around the world, to the world around the world, around the world around the world, around the world, around the world, around the world around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world"}, {"heading": "4.3 COMPARISON WITH STATE-OF-THE-ART RESULTS", "text": "In the section above, we examine the effects of different transfer learning architectures. Now, we compare our approach with modern TB systems on these datasets. We use publicly available, preschooled word embeddings as initialization. On the English datasets, we experiment with both the 50-dimensional SENNA embeddings (Collobert et al., 2011; Huang et al., 2015; Chiu & Nichols, 2015; Ma & Hovy, 2016) and the 100-dimensional GloVe embeddings (Pennington et al., 2014), using both the 50-dimensional SENNA embeddings (Collobert et al., 2011), and the development set to select the embeddings for different tasks and settings. For Spanish and Dutch, we use the 64-dimensional polyglot embeddings (Al-Rfou et al., 2013). We set the hidden state data embeddings to 300 for the word-level-da01, which is set to the initial learning rate (GRU)."}, {"heading": "5 CONCLUSION", "text": "In this paper, we develop a transfer-learning approach to sequence marking that takes advantage of the generality that deep neural networks have demonstrated in previous work. We design three neural network architectures for configuring cross-domain, cross-application and cross-language transfer. Our transfer-learning approach achieves significant improvements in different sets of data under resource-constrained conditions, as well as new state-of-the-art results in some of the benchmarks. In thorough experiments, we observe that the following factors are critical to the performance of our transfer-learning approach: a) the abundance of labels for the target task, b) the relationship between source and target task, and c) the number of parameters that can be shared. In the future, it will be interesting to combine model-based transfer (as in this paper) with resource-based transfer for linguistic transfer learning."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was funded by NVIDIA, the Office of Naval Research Grant N000141512791, the ADeLAIDE Grant FA8750-16C-0130-001, the NSF Grant IIS1250956 and Google Research."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": "In ACL,", "citeRegEx": "Al.Rfou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Rie Kubota Ando", "Tong Zhang"], "venue": "JMLR, 6:1817\u20131853,", "citeRegEx": "Ando and Zhang.,? \\Q2005\\E", "shortCiteRegEx": "Ando and Zhang.", "year": 2005}, {"title": "A token centric part-of-speech tagger for biomedical text", "author": ["Neil Barrett", "Jens Weber-Jahnke"], "venue": "Artificial intelligence in medicine,", "citeRegEx": "Barrett and Weber.Jahnke.,? \\Q2014\\E", "shortCiteRegEx": "Barrett and Weber.Jahnke.", "year": 2014}, {"title": "Co-training for domain adaptation", "author": ["Minmin Chen", "Kilian Q Weinberger", "John Blitzer"], "venue": "In NIPS,", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["Jason PC Chiu", "Eric Nichols"], "venue": "arXiv preprint arXiv:1511.08308,", "citeRegEx": "Chiu and Nichols.,? \\Q2015\\E", "shortCiteRegEx": "Chiu and Nichols.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "In ACL,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "JMLR, 12:2121\u20132159,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Hierarchical bayesian domain adaptation", "author": ["Jenny Rose Finkel", "Christopher D Manning"], "venue": "In HLT,", "citeRegEx": "Finkel and Manning.,? \\Q2009\\E", "shortCiteRegEx": "Finkel and Manning.", "year": 2009}, {"title": "Multilingual language processing from bytes", "author": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "venue": "arXiv preprint arXiv:1512.00103,", "citeRegEx": "Gillick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Softmax-margin crfs: Training log-linear models with cost functions", "author": ["Kevin Gimpel", "Noah A Smith"], "venue": "In NAACL, pp", "citeRegEx": "Gimpel and Smith.,? \\Q2010\\E", "shortCiteRegEx": "Gimpel and Smith.", "year": 2010}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991,", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Genia corpusa semantically annotated corpus for bio-textmining", "author": ["J-D Kim", "Tomoko Ohta", "Yuka Tateisi", "Junichi Tsujii"], "venue": "Bioinformatics, 19(suppl 1):i180\u2013i182,", "citeRegEx": "Kim et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2003}, {"title": "New transfer learning techniques for disparate label sets", "author": ["Young-Bum Kim", "Karl Stratos", "Ruhi Sarikaya", "Minwoo Jeong"], "venue": "In ACL,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Neural architectures for named entity recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "venue": "In NAACL,", "citeRegEx": "Lample et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": "In EMNLP,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Joint named entity recognition and disambiguation", "author": ["Gang Luo", "Xiaojiang Huang", "Chin-Yew Lin", "Zaiqing Nie"], "venue": "In ACL,", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "End-to-end sequence labeling via bi-directional lstm-cnns-crf", "author": ["Xuezhe Ma", "Eduard Hovy"], "venue": "In ACL,", "citeRegEx": "Ma and Hovy.,? \\Q2016\\E", "shortCiteRegEx": "Ma and Hovy.", "year": 2016}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "Pan and Yang.,? \\Q2010\\E", "shortCiteRegEx": "Pan and Yang.", "year": 2010}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["Alexandre Passos", "Vineet Kumar", "Andrew McCallum"], "venue": "In HLT,", "citeRegEx": "Passos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Improving named entity recognition for chinese social media with word segmentation representation learning", "author": ["Nanyun Peng", "Mark Dredze"], "venue": "In ACL,", "citeRegEx": "Peng and Dredze.,? \\Q2016\\E", "shortCiteRegEx": "Peng and Dredze.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Lev Ratinov", "Dan Roth"], "venue": "In CoNLL, pp", "citeRegEx": "Ratinov and Roth.,? \\Q2009\\E", "shortCiteRegEx": "Ratinov and Roth.", "year": 2009}, {"title": "Named entity recognition in tweets: an experimental study", "author": ["Alan Ritter", "Sam Clark", "Oren Etzioni"], "venue": "In EMNLP, pp", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Flors: Fast and simple domain adaptation for part-of-speech tagging", "author": ["Tobias Schnabel", "Hinrich Sch\u00fctze"], "venue": "TACL, 2:15\u201326,", "citeRegEx": "Schnabel and Sch\u00fctze.,? \\Q2014\\E", "shortCiteRegEx": "Schnabel and Sch\u00fctze.", "year": 2014}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In NAACL, pp", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Cross-lingual pseudo-projected expectation regularization for weakly supervised learning", "author": ["Mengqiu Wang", "Christopher D Manning"], "venue": "TACL,", "citeRegEx": "Wang and Manning.,? \\Q2014\\E", "shortCiteRegEx": "Wang and Manning.", "year": 2014}, {"title": "Inducing multilingual text analysis tools via robust projection across aligned corpora", "author": ["David Yarowsky", "Grace Ngai", "Richard Wicentowski"], "venue": "In HLT, pp", "citeRegEx": "Yarowsky et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yarowsky et al\\.", "year": 2001}, {"title": "Cross-lingual transfer of named entity recognizers without parallel corpora", "author": ["Ayah Zirikly", "Masato Hagiwara"], "venue": "In ACL,", "citeRegEx": "Zirikly and Hagiwara.,? \\Q2015\\E", "shortCiteRegEx": "Zirikly and Hagiwara.", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "Transfer learning can be used in several settings, notably for low-resource languages (Zirikly & Hagiwara, 2015; Wang & Manning, 2014) and low-resource domains such as biomedical corpora (Kim et al., 2003) and Twitter corpora (Ritter et al.", "startOffset": 187, "endOffset": 205}, {"referenceID": 23, "context": ", 2003) and Twitter corpora (Ritter et al., 2011)).", "startOffset": 28, "endOffset": 49}, {"referenceID": 6, "context": "Even on datasets with relatively abundant labels, multi-task transfer can sometimes achieve improvement over state-of-the-art results (Collobert et al., 2011).", "startOffset": 134, "endOffset": 158}, {"referenceID": 6, "context": "Recently, a number of approaches based on deep neural networks have addressed the problem of sequence tagging in an end-to-end manner (Collobert et al., 2011; Lample et al., 2016; Ling et al., 2015; Ma & Hovy, 2016).", "startOffset": 134, "endOffset": 215}, {"referenceID": 14, "context": "Recently, a number of approaches based on deep neural networks have addressed the problem of sequence tagging in an end-to-end manner (Collobert et al., 2011; Lample et al., 2016; Ling et al., 2015; Ma & Hovy, 2016).", "startOffset": 134, "endOffset": 215}, {"referenceID": 15, "context": "Recently, a number of approaches based on deep neural networks have addressed the problem of sequence tagging in an end-to-end manner (Collobert et al., 2011; Lample et al., 2016; Ling et al., 2015; Ma & Hovy, 2016).", "startOffset": 134, "endOffset": 215}, {"referenceID": 27, "context": "Resource-based transfer utilizes additional linguistic annotations as weak supervision for transfer learning, such as cross-lingual dictionaries (Zirikly & Hagiwara, 2015), corpora (Wang & Manning, 2014), and word alignments (Yarowsky et al., 2001).", "startOffset": 225, "endOffset": 248}, {"referenceID": 3, "context": "Techniques in cross-domain transfer include the design of robust feature representations (Schnabel & Sch\u00fctze, 2014), co-training (Chen et al., 2011), hierarchical Bayesian prior (Finkel & Manning, 2009), and canonical component analysis (Kim et al.", "startOffset": 129, "endOffset": 148}, {"referenceID": 13, "context": ", 2011), hierarchical Bayesian prior (Finkel & Manning, 2009), and canonical component analysis (Kim et al., 2015).", "startOffset": 96, "endOffset": 114}, {"referenceID": 6, "context": "Later architectures based on different combinations of convolutional networks and recurrent networks have achieved state-of-the-art results on many tasks (Collobert et al., 2011; Huang et al., 2015; Chiu & Nichols, 2015; Lample et al., 2016; Ma & Hovy, 2016).", "startOffset": 154, "endOffset": 258}, {"referenceID": 11, "context": "Later architectures based on different combinations of convolutional networks and recurrent networks have achieved state-of-the-art results on many tasks (Collobert et al., 2011; Huang et al., 2015; Chiu & Nichols, 2015; Lample et al., 2016; Ma & Hovy, 2016).", "startOffset": 154, "endOffset": 258}, {"referenceID": 14, "context": "Later architectures based on different combinations of convolutional networks and recurrent networks have achieved state-of-the-art results on many tasks (Collobert et al., 2011; Huang et al., 2015; Chiu & Nichols, 2015; Lample et al., 2016; Ma & Hovy, 2016).", "startOffset": 154, "endOffset": 258}, {"referenceID": 21, "context": "Resource-based transfer utilizes additional linguistic annotations as weak supervision for transfer learning, such as cross-lingual dictionaries (Zirikly & Hagiwara, 2015), corpora (Wang & Manning, 2014), and word alignments (Yarowsky et al., 2001). Resource-based methods demonstrate considerable success in cross-lingual transfer, but are quite sensitive to the scale and quality of the additional resources. Resource-based transfer is mostly limited to cross-lingual transfer in previous works, and there is not extensive research on extending resource-based methods to cross-domain and cross-application settings. Model-based transfer, on the other hand, does not require additional resources. Model-based transfer exploits the similarity and relatedness between the source task and the target task by adaptively modifying the model architectures, training algorithms, or feature representation. For example, Ando & Zhang (2005) proposed a transfer learning framework that shares structural parameters across multiple tasks, and improve the performance on various tasks including NER; Collobert et al.", "startOffset": 226, "endOffset": 933}, {"referenceID": 5, "context": "For example, Ando & Zhang (2005) proposed a transfer learning framework that shares structural parameters across multiple tasks, and improve the performance on various tasks including NER; Collobert et al. (2011) presented a task-independent convolutional neural network and employed joint training to transfer knowledge from NER and POS tagging to chunking; Peng & Dredze (2016) studied transfer learning between named entity recognition and word segmentation in Chinese based on recurrent neural networks.", "startOffset": 189, "endOffset": 213}, {"referenceID": 5, "context": "For example, Ando & Zhang (2005) proposed a transfer learning framework that shares structural parameters across multiple tasks, and improve the performance on various tasks including NER; Collobert et al. (2011) presented a task-independent convolutional neural network and employed joint training to transfer knowledge from NER and POS tagging to chunking; Peng & Dredze (2016) studied transfer learning between named entity recognition and word segmentation in Chinese based on recurrent neural networks.", "startOffset": 189, "endOffset": 380}, {"referenceID": 3, "context": "Techniques in cross-domain transfer include the design of robust feature representations (Schnabel & Sch\u00fctze, 2014), co-training (Chen et al., 2011), hierarchical Bayesian prior (Finkel & Manning, 2009), and canonical component analysis (Kim et al., 2015). While our approach falls into the paradigm of model-based transfer, in contrast to the above methods, our method focuses on exploiting the generality of deep recurrent neural networks and is applicable to transfer between domains, applications, and languages. Our work builds on previous work on sequence tagging based on deep neural networks. Collobert et al. (2011) develop end-to-end neural networks for sequence tagging without hand-engineered features.", "startOffset": 130, "endOffset": 625}, {"referenceID": 6, "context": "Both of the word-level layer and the character-level layer can be implemented as convolutional neural networks (CNNs) or recurrent neural networks (RNNs) (Collobert et al., 2011; Chiu & Nichols, 2015; Lample et al., 2016; Ma & Hovy, 2016).", "startOffset": 154, "endOffset": 238}, {"referenceID": 14, "context": "Both of the word-level layer and the character-level layer can be implemented as convolutional neural networks (CNNs) or recurrent neural networks (RNNs) (Collobert et al., 2011; Chiu & Nichols, 2015; Lample et al., 2016; Ma & Hovy, 2016).", "startOffset": 154, "endOffset": 238}, {"referenceID": 23, "context": ", \u201cURL\u201d) cannot be mapped to Penn Treebank tags (Ritter et al., 2011).", "startOffset": 48, "endOffset": 69}, {"referenceID": 6, "context": "Similar to the motivation in (Collobert et al., 2011), it is usually desirable to exploit the underlying similarities and regularities of different applications, and improve the performance of one application via joint training with another.", "startOffset": 29, "endOffset": 53}, {"referenceID": 27, "context": "Though cross-lingual transfer is usually accomplished with additional multi-lingual resources, these methods are sensitive to the size and quality of the additional resources (Yarowsky et al., 2001; Wang & Manning, 2014).", "startOffset": 175, "endOffset": 220}, {"referenceID": 7, "context": "We adopt AdaGrad (Duchi et al., 2011) to dynamically compute the learning rates for each iteration.", "startOffset": 17, "endOffset": 37}, {"referenceID": 5, "context": "More specifically, we employ gated recurrent units (GRUs) (Cho et al., 2014).", "startOffset": 58, "endOffset": 76}, {"referenceID": 6, "context": "We note that our transfer learning framework does not make assumptions about specific model implementation, and could be applied to other neural architectures (Collobert et al., 2011; Chiu & Nichols, 2015; Lample et al., 2016; Ma & Hovy, 2016) as well.", "startOffset": 159, "endOffset": 243}, {"referenceID": 14, "context": "We note that our transfer learning framework does not make assumptions about specific model implementation, and could be applied to other neural architectures (Collobert et al., 2011; Chiu & Nichols, 2015; Lample et al., 2016; Ma & Hovy, 2016) as well.", "startOffset": 159, "endOffset": 243}, {"referenceID": 13, "context": "Our base model is similar to Lample et al. (2016), but in contrast to their model, we employ GRUs for the character-level and word-level networks instead of Long Short-Term Memory (LSTM) units, and define the objective function based on the max-margin principle.", "startOffset": 29, "endOffset": 50}, {"referenceID": 12, "context": "We use the following benchmark datasets in our experiments: Penn Treebank (PTB) POS tagging, CoNLL 2000 chunking, CoNLL 2003 English NER, CoNLL 2002 Dutch NER, CoNLL 2002 Spanish NER, the Genia biomedical corpus (Kim et al., 2003), and a Twitter corpus (Ritter et al.", "startOffset": 212, "endOffset": 230}, {"referenceID": 23, "context": ", 2003), and a Twitter corpus (Ritter et al., 2011).", "startOffset": 30, "endOffset": 51}, {"referenceID": 6, "context": "For the CoNLL 2003 English NER dataset, we follow previous works (Collobert et al., 2011) to append one-hot gazetteer features to the input of the CRF layer for fair comparison.", "startOffset": 65, "endOffset": 89}, {"referenceID": 24, "context": "We construct the POS tagging dataset with the instructions described in Toutanova et al. (2003). Note that as a standard practice, the POS tags are extracted from the parsed trees.", "startOffset": 72, "endOffset": 96}, {"referenceID": 6, "context": "On the English datasets, following previous works that are based on neural networks (Collobert et al., 2011; Huang et al., 2015; Chiu & Nichols, 2015; Ma & Hovy, 2016), we experiment with both the 50-dimensional SENNA embeddings (Collobert et al.", "startOffset": 84, "endOffset": 167}, {"referenceID": 11, "context": "On the English datasets, following previous works that are based on neural networks (Collobert et al., 2011; Huang et al., 2015; Chiu & Nichols, 2015; Ma & Hovy, 2016), we experiment with both the 50-dimensional SENNA embeddings (Collobert et al.", "startOffset": 84, "endOffset": 167}, {"referenceID": 6, "context": ", 2015; Chiu & Nichols, 2015; Ma & Hovy, 2016), we experiment with both the 50-dimensional SENNA embeddings (Collobert et al., 2011) and the 100-dimensional GloVe embeddings (Pennington et al.", "startOffset": 108, "endOffset": 132}, {"referenceID": 21, "context": ", 2011) and the 100-dimensional GloVe embeddings (Pennington et al., 2014) and use the development set to choose the embeddings for different tasks and settings.", "startOffset": 49, "endOffset": 74}, {"referenceID": 0, "context": "For Spanish and Dutch, we use the 64-dimensional Polyglot embeddings (Al-Rfou et al., 2013).", "startOffset": 69, "endOffset": 91}], "year": 2017, "abstractText": "Recent papers have shown that neural networks obtain state-of-the-art performance on several different sequence tagging tasks. One appealing property of such systems is their generality, as excellent performance can be achieved with a unified architecture and without task-specific feature engineering. However, it is unclear if such systems can be used for tasks without large amounts of training data. In this paper we explore the problem of transfer learning for neural sequence taggers, where a source task with plentiful annotations (e.g., POS tagging on Penn Treebank) is used to improve performance on a target task with fewer available annotations (e.g., POS tagging for microblogs). We examine the effects of transfer learning for deep hierarchical recurrent networks across domains, applications, and languages, and show that significant improvement can often be obtained. These improvements lead to improvements over the current state-ofthe-art on several well-studied tasks.1", "creator": "LaTeX with hyperref package"}}}