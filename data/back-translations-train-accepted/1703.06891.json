{"id": "1703.06891", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "Dance Dance Convolution", "abstract": "Dance Dance Revolution (DDR) is a popular rhythm-based video game. Players perform steps on a dance platform in synchronization with music as directed by on-screen step charts. While many step charts are available in standardized packs, users may grow tired of existing charts, or wish to dance to a song for which no chart exists. We introduce the task of learning to choreograph. Given a raw audio track, the goal is to produce a new step chart. This task decomposes naturally into two subtasks: deciding when to place steps and deciding which steps to select. For the step placement task, we combine recurrent and convolutional neural networks to ingest spectrograms of low-level audio features to predict steps, conditioned on chart difficulty. For step selection, we present a conditional LSTM generative model that substantially outperforms n-gram and fixed-window approaches.", "histories": [["v1", "Mon, 20 Mar 2017 18:00:13 GMT  (1518kb,D)", "https://arxiv.org/abs/1703.06891v1", null], ["v2", "Wed, 22 Mar 2017 07:44:55 GMT  (1518kb,D)", "http://arxiv.org/abs/1703.06891v2", null], ["v3", "Wed, 21 Jun 2017 00:45:51 GMT  (1881kb,D)", "http://arxiv.org/abs/1703.06891v3", "Published as a conference paper at ICML 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.MM cs.NE cs.SD stat.ML", "authors": ["chris donahue", "zachary c lipton", "julian mcauley"], "accepted": true, "id": "1703.06891"}, "pdf": {"name": "1703.06891.pdf", "metadata": {"source": "META", "title": "Dance Dance Convolution", "authors": ["Chris Donahue", "Zachary C. Lipton", "Julian McAuley"], "emails": ["<cdonahue@ucsd.edu>."], "sections": [{"heading": "1. Introduction", "text": "In fact, it is such that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process"}, {"heading": "1.1. Contributions", "text": "In short, our paper offers the following contributions: \u2022 We define Learning to Choreography, a new task with real utility and strong connections to fundamental problems in ME. \u2022 We present two large, curated datasets for benchmarking GDR choreography algorithms, which are an underestimated source of music commentary. \u2022 We present an effective pipeline for learning choreography using deep neural networks.2"}, {"heading": "2. Data", "text": "The first set of data contains 90 songs choreographed by a single prolific author working under the name Fraxtil. This set of data contains five charts per song, corresponding to increasing difficulty levels. We find that these charts significantly overlap, the lower difficulty levels do not include strict subsets of the higher difficulty levels (Figure 3). The 1https: / / github.com / chrisdonahue / ddc 2Demonstration, which shows human choreography alongside the output of our method: https: / / youtu.be / yUc3O237p9Msecond dataset is a larger, multilingual dataset called In The Groove (ITG); this dataset contains 133 songs with one chart per difficulty level, except for 13 songs that do not contain charts for the highest difficulty. Both datasets contain electronic music with constant tempo and a strong beat, characteristic of music preferred by the GDR community."}, {"heading": "3. Problem Definition", "text": "One step can occur at up to 192 different locations (subdivisions) within each measurement. Therefore, in order to make the automatic DDR choreography comprehensible, we divide it into two subtasks: step placement and step selection.In step placement, our goal is to determine at what exact time steps are to be placed. A step placement algorithm records raw audio functions and issues timestamps corresponding to the steps. In addition to the audio signal, we offer step placement algorithms with a uniform representation of the intended difficulty level for the diagram. In step selection, a discredited list of step times is computed during step placement, and each of these steps is mapped to a DDR step. Our approach to solving this problem includes modelling the probability distribution P (mn | m1,.)."}, {"heading": "4. Methods", "text": "We will now describe our specific solutions to the problems of step placement and selection. Our basic pipeline works as follows: (1) extract an audio feature representation; (2) feed this representation into a step placement algorithm that estimates the probability that a ground truth step is within this frame; (3) use a peak picking process on this probability sequence to identify the exact timestamps at which the steps should be placed; and (4) use a font selection algorithm to determine which steps should be placed based on a sequence of timestamps."}, {"heading": "4.1. Audio Representation", "text": "Music files arrive as lossy encodings at 44.1 kHz. We decode the audio files into stereo PCM audio and calculate the two channels on average into a monophonic representation. We then calculate a multi-time short-term Fourier transformation (STFT) using window lengths of 23ms, 46ms and 93ms and a step of 10ms. Shorter window sizes retain low characteristics such as pitch and timbre, while larger window sizes provide more context for high-level features such as melody and rhythm (Hamel et al., 2012). Using the ESSENTIA library (Bogdanov et al., 2013) we reduce the dimensionality of the STFT size spectrum to 80 frequency bands by using a Mel scale (Stevens et al., 1937) filter bank. We scale the filter outputs logarithmically to better define the human perception of Stevens's past loudness, using seven frames and seven."}, {"heading": "4.2. Step Placement", "text": "In fact, most of them will be able to put themselves in a different world, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live."}, {"heading": "4.3. Peak Picking", "text": "Following standard practice for detecting insertion, we convert sequences of step probabilities into a discrete set of 3For LogReg and MLP and add difficulty to the input level. Selected placements are made using a peak-picking process. First, we execute our step-placement algorithm over an entire song to assign the probabilities of a step within each 10ms frame. 4 We then combine this sequence of predicted probabilities with a hamming window, flatten the predictions and suppress duplicate peaks from a short distance. Finally, we set a constant threshold to choose which peaks are high enough (Figure 6). Since the number of peaks varies according to the difficulty of the chart, we choose a different threshold per difficulty level. We consider predicted placements to be true positives if they lie within a window of \u00b1 20ms of a truth."}, {"heading": "4.4. Step Selection", "text": "We treat the step selection task as a sequence generation problem. Our approach follows similar work in speech modeling, where RNNs are known to produce a coherent text that captures far-reaching relationships (Mikolov et al., 2010; Sutskever et al., 2011; Sundermeyer et al., 2012).Our LSTM model goes beyond the basic truth steps and predicts the next token based on the previous sequence of tokens. The output is a softmax distribution across the 256 possible steps. As input, we use a more compact bag-of-arrows representation that contains 16 features (4 per arrow) to represent the previous step. For each arrow, the 4 corresponding features represent the states on, off, hold and resolve. We found the bag of arrows to be equivalent4In DDR, the results depend on the accuracy of the step."}, {"heading": "5. Experiments", "text": "For both fraxtile and ITG data sets, we use 80%, 10%, 10% splits for training, validation, and test data. Because of the correlation between charts for the same song with varying degrees of difficulty, we ensure that all charts for a particular song are grouped in the same split."}, {"heading": "5.1. Step Placement", "text": "We evaluate the performance of our step placement methods using the methodology outlined below. Baselines To establish reasonable baselines for step placement, we first report the results of a logistics regressor (LogReg) with flattened audio features. We also report the performance of an MLP. Our MLP architecture includes two fully interconnected layers, sizes 256 and 128, applying an rectifier algorithm to each layer. Metrics We report the perplexity of each model (PL) with a 50% probability after each fully interconnected layer during training. We model our CNN baseline using the Schl\u00fcter & Bo \ufffd ck (2014) method, a state-of-the-art algorithm for detecting the deployment date. The average perplexity of each model (PL) is averaged in each frame in each chart."}, {"heading": "5.2. Step Selection", "text": "In order to protect the n-gram models from unlimited losses of previously unseen n-grams, we use modified Kneser-Ney smoothing (Chen & Goodman, 1998) and follow best practices in speech modeling (Mikolov et al., 2010; Sutskever et al., 2011), in particular we train a smoothed 5-gram model with backoff (KN5) as implemented in Stolcke (2002). Following the work of Bengio et al. (2003), we also compare against a 5-gram MLP containing 4 bag arrow pocket coded steps as input and prediction of the next step."}, {"heading": "6. Discussion", "text": "In fact, it is so that we are in a position to assert ourselves in a position to assert that we are in a position to assert ourselves in the world, and that we are in a position to assert ourselves in the world, that we are in a position to remain in the world, and that we are in a position to assert ourselves in the world, that we are in a position to assert ourselves in the world, that we are in a position to remain in the world, and that we are in a position to remain in the world, in the world, in the world in which we live, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world in the world, in the world, in the world in the world, in the world, in the world in the world, in the world in the world, in the world, in the world in the world, in the world, in the world in the world, in the world, in the world in the world, in the world, in the world, in the world in the world, in the world, in the world in the world, in the world, in the world in the world, in the world, in the world, in the world in the world, in the world, in the world in the world, in the world, in the world in the world, in the world in the world, in the world in the world, in the world, in the world in the world, in the world in the world, in the world in the world, in the world, in the world in the world, in the world, in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world in the world, in the world in the world, in the world, in the world in the world, in the world in the world, in the"}, {"heading": "7. Related Work", "text": "Several scientific papers deal with multi-layered representation, including anthropological studies (Hoysniemi, 2006; Behrenshausen, 2007) and two papers that describe approaches to automated choreography; the first, called Dancing Monkeys, uses rule-based methods for both step placement and step placement (O'Keeffe, 2003); the second deals with genetic algorithms for step placement that optimize an ad-hoc fitness function (Nogaj, 2005; Dixon, 2006), both establishing reproducible evaluation methods or learning the semantics of steps from data.Our step placement task is very similar to the classical problem of recognizing pieces of music (Bello et al., 2005; Dixon, 2006). Several recording papers examine the modern method of deep learning or deal with the semantics of steps from data.Our step placement task is very similar to the classical problem of identifying very classical pieces of music (Bello)."}, {"heading": "8. Conclusions", "text": "By combining insights from the detection of musical intrusion and the modelling of statistical language, we have developed and evaluated a number of methods that deepen the learning of choreographic learning. We have introduced standardised data sets and reproducible evaluation methods in the hope of fostering a wider investigation of this and related issues. We stress that the sheer volume of available step diagrams represents a rare opportunity for ME: access to large amounts of high-quality annotated data. This data could help foster innovation for several MIR tasks, including detecting intrusion, tracking the beat and detecting tempo."}, {"heading": "Acknowledgements", "text": "The authors thank Jeff Donahue, Shlomo Dubnov, Jennifer Hsu, Mohsen Malmir, Miller Puckette, Adith Swaminathan, and Sharad Vikram for their helpful feedback on this work. For this work, the Extreme Science and Engineering Discovery Environment (XSEDE) (Towns et al., 2014) was used, which is supported by the National Science Foundation with grant number ACI-1548562. GPUs used for this research were kindly donated by NVIDIA Corporation."}], "references": [{"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Amodei", "Dario", "Anubhai", "Rishita", "Battenberg", "Eric", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan", "Chen", "Jingdong", "Chrzanowski", "Mike", "Coates", "Adam", "Diamos", "Greg"], "venue": "In ICML,", "citeRegEx": "Amodei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Amodei et al\\.", "year": 2015}, {"title": "Toward a (kin) aesthetic of video gaming the case of dance dance revolution", "author": ["Behrenshausen", "Bryan G"], "venue": "Games and Culture,", "citeRegEx": "Behrenshausen and G.,? \\Q2007\\E", "shortCiteRegEx": "Behrenshausen and G.", "year": 2007}, {"title": "A tutorial on onset detection in music signals", "author": ["Bello", "Juan Pablo", "Daudet", "Laurent", "Abdallah", "Samer", "Duxbury", "Chris", "Davies", "Mike", "Sandler", "Mark B"], "venue": "IEEE Transactions on speech and audio processing,", "citeRegEx": "Bello et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bello et al\\.", "year": 2005}, {"title": "A neural probabilistic language model", "author": ["Bengio", "Yoshua", "Ducharme", "R\u00e9jean", "Vincent", "Pascal", "Jauvin", "Christian"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Modeling the perception of tonal structure with neural nets", "author": ["Bharucha", "Jamshed J", "Todd", "Peter M"], "venue": "Computer Music Journal,", "citeRegEx": "Bharucha et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Bharucha et al\\.", "year": 1989}, {"title": "Polyphonic piano note transcription with recurrent neural networks", "author": ["B\u00f6ck", "Sebastian", "Schedl", "Markus"], "venue": "In ICASSP,", "citeRegEx": "B\u00f6ck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "B\u00f6ck et al\\.", "year": 2012}, {"title": "Essentia: An audio analysis library for music information retrieval", "author": ["Bogdanov", "Dmitry", "Wack", "Nicolas", "G\u00f3mez", "Emilia", "Gulati", "Sankalp", "Herrera", "Perfecto", "Mayor", "Oscar", "Roma", "Gerard", "Salamon", "Justin", "Zapata", "Jos\u00e9 R", "Serra", "Xavier"], "venue": "In ISMIR,", "citeRegEx": "Bogdanov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bogdanov et al\\.", "year": 2013}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["Boulanger-Lewandowski", "Nicolas", "Bengio", "Yoshua", "Vincent", "Pascal"], "venue": "In ICML,", "citeRegEx": "Boulanger.Lewandowski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2012}, {"title": "Audio chord recognition with recurrent neural networks", "author": ["Boulanger-Lewandowski", "Nicolas", "Bengio", "Yoshua", "Vincent", "Pascal"], "venue": "In ISMIR,", "citeRegEx": "Boulanger.Lewandowski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2013}, {"title": "High-dimensional sequence transduction", "author": ["Boulanger-Lewandowski", "Nicolas", "Bengio", "Yoshua", "Vincent", "Pascal"], "venue": "In ICASSP,", "citeRegEx": "Boulanger.Lewandowski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2013}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Chen", "Stanley F", "Goodman", "Joshua"], "venue": "Technical Report TR-10-98,", "citeRegEx": "Chen et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1998}, {"title": "Song from pi: A musically plausible network for pop music generation", "author": ["Chu", "Hang", "Urtasun", "Raquel", "Fidler", "Sanja"], "venue": null, "citeRegEx": "Chu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chu et al\\.", "year": 2016}, {"title": "Semi-supervised sequence learning", "author": ["Dai", "Andrew M", "Le", "Quoc V"], "venue": "In NIPS,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Onset detection revisited", "author": ["Dixon", "Simon"], "venue": "In Proceedings of the 9th International Conference on Digital Audio Effects,", "citeRegEx": "Dixon and Simon.,? \\Q2006\\E", "shortCiteRegEx": "Dixon and Simon.", "year": 2006}, {"title": "A first look at music composition using lstm recurrent neural networks", "author": ["Eck", "Douglas"], "venue": "Technical Report IDSIA-0702,", "citeRegEx": "Eck and Douglas.,? \\Q2002\\E", "shortCiteRegEx": "Eck and Douglas.", "year": 2002}, {"title": "Universal onset detection with bidirectional long short-term memory neural networks", "author": ["Eyben", "Florian", "B\u00f6ck", "Sebastian", "Schuller", "Bj\u00f6rn W", "Graves", "Alex"], "venue": "In ISMIR,", "citeRegEx": "Eyben et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Eyben et al\\.", "year": 2010}, {"title": "Recurrent nets that time and count", "author": ["Gers", "Felix", "Schmidhuber", "J\u00fcrgen"], "venue": "In International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Alex", "Jaitly", "Navdeep"], "venue": "In ICML,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Graves", "Alex", "Fern\u00e1ndez", "Santiago", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In ICML,", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "In ICASSP,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Lstm: A search space odyssey", "author": ["Greff", "Klaus", "Srivastava", "Rupesh K", "Koutn\u0131\u0301k", "Jan", "Steunebrink", "Bas R", "Schmidhuber", "J\u00fcrgen"], "venue": "IEEE transactions on neural networks and learning systems,", "citeRegEx": "Greff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2016}, {"title": "Deepbach: a steerable model for bach chorales generation", "author": ["Hadjeres", "Ga\u00ebtan", "Pachet", "Fran\u00e7ois"], "venue": null, "citeRegEx": "Hadjeres et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hadjeres et al\\.", "year": 2016}, {"title": "Building musically-relevant audio features through multiple timescale representations", "author": ["Hamel", "Philippe", "Bengio", "Yoshua", "Eck", "Douglas"], "venue": "In ISMIR,", "citeRegEx": "Hamel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hamel et al\\.", "year": 2012}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "International survey on the dance dance revolution game", "author": ["Hoysniemi", "Johanna"], "venue": "Computers in Entertainment (CIE),", "citeRegEx": "Hoysniemi and Johanna.,? \\Q2006\\E", "shortCiteRegEx": "Hoysniemi and Johanna.", "year": 2006}, {"title": "Rethinking automatic chord recognition with convolutional neural networks", "author": ["Humphrey", "Eric J", "Bello", "Juan Pablo"], "venue": "In ICMLA,", "citeRegEx": "Humphrey et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Humphrey et al\\.", "year": 2012}, {"title": "Visualizing and understanding recurrent networks", "author": ["Karpathy", "Andrej", "Johnson", "Justin", "Fei-Fei", "Li"], "venue": null, "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Character-aware neural language models", "author": ["Kim", "Yoon", "Jernite", "Yacine", "Sontag", "David", "Rush", "Alexander M"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "A critical review of recurrent neural networks for sequence learning", "author": ["Lipton", "Zachary C", "Berkowitz", "John", "Elkan", "Charles"], "venue": null, "citeRegEx": "Lipton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lipton et al\\.", "year": 2015}, {"title": "Learning to diagnose with LSTM recurrent neural networks", "author": ["Lipton", "Zachary C", "Kale", "David C", "Elkan", "Charles", "Wetzell", "Randall"], "venue": "In ICLR,", "citeRegEx": "Lipton et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lipton et al\\.", "year": 2016}, {"title": "Multi-resolution linear prediction based features for audio onset detection with bidirectional lstm neural networks", "author": ["Marchi", "Erik", "Ferroni", "Giacomo", "Eyben", "Florian", "Gabrielli", "Leonardo", "Squartini", "Stefano", "Schuller", "Bjorn"], "venue": null, "citeRegEx": "Marchi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marchi et al\\.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tomas", "Karafi\u00e1t", "Martin", "Burget", "Lukas", "Cernock\u1ef3", "Jan", "Khudanpur", "Sanjeev"], "venue": "In Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "A genetic algorithm for determining optimal step patterns in dance dance revolution", "author": ["Nogaj", "Adam"], "venue": "Technical report, State University of New York at Fredonia,", "citeRegEx": "Nogaj and Adam.,? \\Q2005\\E", "shortCiteRegEx": "Nogaj and Adam.", "year": 2005}, {"title": "Dancing monkeys (automated creation of step files for dance dance revolution)", "author": ["O\u2019Keeffe", "Karl"], "venue": "Technical report, Imperial College London,", "citeRegEx": "O.Keeffe and Karl.,? \\Q2003\\E", "shortCiteRegEx": "O.Keeffe and Karl.", "year": 2003}, {"title": "Two decades of statistical language modeling: Where do we go from here", "author": ["Rosenfeld", "Ronald"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Rosenfeld and Ronald.,? \\Q2000\\E", "shortCiteRegEx": "Rosenfeld and Ronald.", "year": 2000}, {"title": "Learning the speech frontend with raw waveform cldnns", "author": ["Sainath", "Tara N", "Weiss", "Ron J", "Senior", "Andrew", "Wilson", "Kevin W", "Vinyals", "Oriol"], "venue": "In Interspeech,", "citeRegEx": "Sainath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2015}, {"title": "Improved musical onset detection with convolutional neural networks", "author": ["Schl\u00fcter", "Jan", "B\u00f6ck", "Sebastian"], "venue": "In ICASSP,", "citeRegEx": "Schl\u00fcter et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Schl\u00fcter et al\\.", "year": 2014}, {"title": "An end-to-end neural network for polyphonic piano music transcription", "author": ["Sigtia", "Siddharth", "Benetos", "Emmanouil", "Dixon", "Simon"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Sigtia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sigtia et al\\.", "year": 2016}, {"title": "A scale for the measurement of the psychological magnitude pitch", "author": ["Stevens", "Stanley Smith", "Volkmann", "John", "Newman", "Edwin B"], "venue": "The Journal of the Acoustical Society of America,", "citeRegEx": "Stevens et al\\.,? \\Q1937\\E", "shortCiteRegEx": "Stevens et al\\.", "year": 1937}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["Stolcke", "Andreas"], "venue": "In Interspeech,", "citeRegEx": "Stolcke and Andreas.,? \\Q2002\\E", "shortCiteRegEx": "Stolcke and Andreas.", "year": 2002}, {"title": "Lstm neural networks for language modeling", "author": ["Sundermeyer", "Martin", "Schl\u00fcter", "Ralf", "Ney", "Hermann"], "venue": "In Interspeech,", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Generating text with recurrent neural networks", "author": ["Sutskever", "Ilya", "Martens", "James", "Hinton", "Geoffrey E"], "venue": "In ICML,", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Xsede: accelerating scientific discovery", "author": ["Towns", "John", "Cockerill", "Timothy", "Dahan", "Maytal", "Foster", "Ian", "Gaither", "Kelly", "Grimshaw", "Andrew", "Hazlewood", "Victor", "Lathrop", "Scott", "Lifka", "Dave", "Peterson", "Gregory D"], "venue": "Computing in Science & Engineering,", "citeRegEx": "Towns et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Towns et al\\.", "year": 2014}, {"title": "Boundary detection in music structure analysis using convolutional neural networks", "author": ["Ullrich", "Karen", "Schl\u00fcter", "Jan", "Grill", "Thomas"], "venue": "In ISMIR,", "citeRegEx": "Ullrich et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ullrich et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 24, "context": "Shorter window sizes preserve low-level features such as pitch and timbre while larger window sizes provide more context for high-level features such as melody and rhythm (Hamel et al., 2012).", "startOffset": 171, "endOffset": 191}, {"referenceID": 6, "context": "Using the ESSENTIA library (Bogdanov et al., 2013), we reduce the dimensionality of the STFT magnitude spectra to 80 frequency bands by applying a Mel-scale (Stevens et al.", "startOffset": 27, "endOffset": 50}, {"referenceID": 40, "context": ", 2013), we reduce the dimensionality of the STFT magnitude spectra to 80 frequency bands by applying a Mel-scale (Stevens et al., 1937) filterbank.", "startOffset": 114, "endOffset": 136}, {"referenceID": 18, "context": "use rectified linear units (ReLU) (Glorot et al., 2011).", "startOffset": 34, "endOffset": 55}, {"referenceID": 46, "context": "For LSTM layers, we apply dropout in the input to output but not temporal directions, following best practices from (Zaremba et al., 2014; Lipton et al., 2016; Dai & Le, 2015).", "startOffset": 116, "endOffset": 175}, {"referenceID": 31, "context": "For LSTM layers, we apply dropout in the input to output but not temporal directions, following best practices from (Zaremba et al., 2014; Lipton et al., 2016; Dai & Le, 2015).", "startOffset": 116, "endOffset": 175}, {"referenceID": 33, "context": "Our approach follows related work in language modeling where RNNs are well-known to produce coherent text that captures long-range relationships (Mikolov et al., 2010; Sutskever et al., 2011; Sundermeyer et al., 2012).", "startOffset": 145, "endOffset": 217}, {"referenceID": 43, "context": "Our approach follows related work in language modeling where RNNs are well-known to produce coherent text that captures long-range relationships (Mikolov et al., 2010; Sutskever et al., 2011; Sundermeyer et al., 2012).", "startOffset": 145, "endOffset": 217}, {"referenceID": 42, "context": "Our approach follows related work in language modeling where RNNs are well-known to produce coherent text that captures long-range relationships (Mikolov et al., 2010; Sutskever et al., 2011; Sundermeyer et al., 2012).", "startOffset": 145, "endOffset": 217}, {"referenceID": 33, "context": "To protect the n-gram models against unbounded loss on previously unseen n-grams, we use modified Kneser-Ney smoothing (Chen & Goodman, 1998), following best practices in language modeling (Mikolov et al., 2010; Sutskever et al., 2011).", "startOffset": 189, "endOffset": 235}, {"referenceID": 43, "context": "To protect the n-gram models against unbounded loss on previously unseen n-grams, we use modified Kneser-Ney smoothing (Chen & Goodman, 1998), following best practices in language modeling (Mikolov et al., 2010; Sutskever et al., 2011).", "startOffset": 189, "endOffset": 235}, {"referenceID": 33, "context": "To protect the n-gram models against unbounded loss on previously unseen n-grams, we use modified Kneser-Ney smoothing (Chen & Goodman, 1998), following best practices in language modeling (Mikolov et al., 2010; Sutskever et al., 2011). Specifically, we train a smoothed 5-gram model with backoff (KN5) as implemented in Stolcke (2002).", "startOffset": 190, "endOffset": 336}, {"referenceID": 3, "context": "Following the work of Bengio et al. (2003) we also compare against a fixed-window 5-gram MLP which takes 4 bag-of-arrows-encoded steps as input and predicts the next step.", "startOffset": 22, "endOffset": 43}, {"referenceID": 0, "context": "For step placement, the best performing model is an LSTM with CNN encoder, an approach which has been used for speech recognition (Amodei et al., 2015), but, to our knowledge, never for music-related tasks.", "startOffset": 130, "endOffset": 151}, {"referenceID": 2, "context": "Our step placement task closely resembles the classic problem of musical onset detection (Bello et al., 2005; Dixon, 2006).", "startOffset": 89, "endOffset": 122}, {"referenceID": 2, "context": "Our step placement task closely resembles the classic problem of musical onset detection (Bello et al., 2005; Dixon, 2006). Several onset detection papers investigate modern deep learning methodology. Eyben et al. (2010) employ bidirectional LSTMs (BLSTMs) for onset detection; Marchi et al.", "startOffset": 90, "endOffset": 221}, {"referenceID": 2, "context": "Our step placement task closely resembles the classic problem of musical onset detection (Bello et al., 2005; Dixon, 2006). Several onset detection papers investigate modern deep learning methodology. Eyben et al. (2010) employ bidirectional LSTMs (BLSTMs) for onset detection; Marchi et al. (2014) improve upon this work, developing a rich multi-resolution feature representation; Schl\u00fcter & B\u00f6ck (2014) demonstrate a CNN-based approach (against", "startOffset": 90, "endOffset": 299}, {"referenceID": 2, "context": "Our step placement task closely resembles the classic problem of musical onset detection (Bello et al., 2005; Dixon, 2006). Several onset detection papers investigate modern deep learning methodology. Eyben et al. (2010) employ bidirectional LSTMs (BLSTMs) for onset detection; Marchi et al. (2014) improve upon this work, developing a rich multi-resolution feature representation; Schl\u00fcter & B\u00f6ck (2014) demonstrate a CNN-based approach (against", "startOffset": 90, "endOffset": 405}, {"referenceID": 45, "context": ", 2013a) and boundary detection (Ullrich et al., 2014), another transient audio phenomenon.", "startOffset": 32, "endOffset": 54}, {"referenceID": 33, "context": "More recently, RNNs have demonstrated superior performance to fixed-window approaches (Mikolov et al., 2010; Sundermeyer et al., 2012; Sutskever et al., 2011).", "startOffset": 86, "endOffset": 158}, {"referenceID": 42, "context": "More recently, RNNs have demonstrated superior performance to fixed-window approaches (Mikolov et al., 2010; Sundermeyer et al., 2012; Sutskever et al., 2011).", "startOffset": 86, "endOffset": 158}, {"referenceID": 43, "context": "More recently, RNNs have demonstrated superior performance to fixed-window approaches (Mikolov et al., 2010; Sundermeyer et al., 2012; Sutskever et al., 2011).", "startOffset": 86, "endOffset": 158}, {"referenceID": 28, "context": "LSTMs are also capable of modeling language at the character level (Karpathy et al., 2015; Kim et al., 2016).", "startOffset": 67, "endOffset": 108}, {"referenceID": 29, "context": "LSTMs are also capable of modeling language at the character level (Karpathy et al., 2015; Kim et al., 2016).", "startOffset": 67, "endOffset": 108}, {"referenceID": 30, "context": "While a thorough explanation of modern RNNs exceeds the scope of this paper, we point to two comprehensive reviews of the literature (Lipton et al., 2015; Greff et al., 2016).", "startOffset": 133, "endOffset": 174}, {"referenceID": 22, "context": "While a thorough explanation of modern RNNs exceeds the scope of this paper, we point to two comprehensive reviews of the literature (Lipton et al., 2015; Greff et al., 2016).", "startOffset": 133, "endOffset": 174}, {"referenceID": 11, "context": "Several papers investigate neural networks for single-note melody generation (Bharucha & Todd, 1989; Eck, 2002; Chu et al., 2016; Hadjeres & Pachet, 2016) and polyphonic melody generation (Boulanger-Lewandowski et al.", "startOffset": 77, "endOffset": 154}, {"referenceID": 7, "context": ", 2016; Hadjeres & Pachet, 2016) and polyphonic melody generation (Boulanger-Lewandowski et al., 2012).", "startOffset": 66, "endOffset": 102}, {"referenceID": 3, "context": "Bengio et al. (2003) demonstrate an approach to language modeling using neural networks with fixed-length context.", "startOffset": 0, "endOffset": 21}, {"referenceID": 39, "context": "RNNs currently yield state-of-the-art performance for musical transcription (B\u00f6ck & Schedl, 2012; Boulanger-Lewandowski et al., 2013b; Sigtia et al., 2016).", "startOffset": 76, "endOffset": 155}, {"referenceID": 20, "context": "RNNs are widely used for speech recognition (Graves & Jaitly, 2014; Graves et al., 2006; 2013; Sainath et al., 2015), and the state-of-the-art method (Amodei et al.", "startOffset": 44, "endOffset": 116}, {"referenceID": 37, "context": "RNNs are widely used for speech recognition (Graves & Jaitly, 2014; Graves et al., 2006; 2013; Sainath et al., 2015), and the state-of-the-art method (Amodei et al.", "startOffset": 44, "endOffset": 116}, {"referenceID": 0, "context": ", 2015), and the state-of-the-art method (Amodei et al., 2015) combines convolutional and recurrent networks.", "startOffset": 41, "endOffset": 62}, {"referenceID": 44, "context": "This work used the Extreme Science and Engineering Discovery Environment (XSEDE) (Towns et al., 2014), which is supported by National Science Foundation grant number ACI-1548562.", "startOffset": 81, "endOffset": 101}], "year": 2017, "abstractText": "Dance Dance Revolution (DDR) is a popular rhythm-based video game. Players perform steps on a dance platform in synchronization with music as directed by on-screen step charts. While many step charts are available in standardized packs, players may grow tired of existing charts, or wish to dance to a song for which no chart exists. We introduce the task of learning to choreograph. Given a raw audio track, the goal is to produce a new step chart. This task decomposes naturally into two subtasks: deciding when to place steps and deciding which steps to select. For the step placement task, we combine recurrent and convolutional neural networks to ingest spectrograms of low-level audio features to predict steps, conditioned on chart difficulty. For step selection, we present a conditional LSTM generative model that substantially outperforms n-gram and fixed-window approaches.", "creator": "LaTeX with hyperref package"}}}