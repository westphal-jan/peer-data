{"id": "1703.07326", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "One-Shot Imitation Learning", "abstract": "Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning.", "histories": [["v1", "Tue, 21 Mar 2017 17:22:29 GMT  (4955kb,D)", "http://arxiv.org/abs/1703.07326v1", null], ["v2", "Wed, 22 Mar 2017 00:24:03 GMT  (4926kb,D)", "http://arxiv.org/abs/1703.07326v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE cs.RO", "authors": ["yan duan", "marcin", "rychowicz", "bradly c stadie", "jonathan ho", "jonas schneider", "ilya sutskever", "pieter abbeel", "wojciech zaremba"], "accepted": true, "id": "1703.07326"}, "pdf": {"name": "1703.07326.pdf", "metadata": {"source": "META", "title": "One-Shot Imitation Learning", "authors": ["Yan Duan", "Marcin Andrychowicz", "Bradly Stadie", "Jonathan Ho", "Jonas Schneider", "Ilya Sutskever", "Pieter Abbeel", "Wojciech Zaremba"], "emails": ["<rocky@openai.com>."], "sections": [{"heading": null, "text": "In particular, we consider the setting in which there is a very large (perhaps infinite) set of tasks, and each task has many instances. For example, one task could be to stack all blocks on a table in a single tower, another task could be to place all blocks on a table in two-block towers, etc. In any case, different instances of the task would consist of different blocks with different starting states. In the training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural network is trained that takes as input a demonstration and the current state (which is initially the initial state of the other demonstration of the pair), and issues an action to ensure that the resulting sequence of states and actions coincides as closely as possible with the second demonstration. In the test time, a single instance of a new task is presented, and the current state (which is the initial state of the other demonstration of the pair)."}, {"heading": "1. Introduction", "text": "In fact, it is the case that we will be able to be in a position without being able to manoeuvre ourselves into such a situation."}, {"heading": "2. Related Work", "text": "This means that we estimate the ability of learning (e.g., Pomerleau (1989); Ross et al. (2011); and inverse enhancement of learning (Ng & Russell, 2000), where a reward function (Abbeel & Ng, 2004; Ziebart et al., 2008; Levine et al., 2011; Finn et al., 2016; Ho & Ermon, 2016) is explained as (almost) optimal behavior. While this previous work has yielded a wide range of impressive robotic results, it looks at each skill individually and has learned to imitate a skill."}, {"heading": "3. One Shot Imitation Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Problem Formalization", "text": "We designate a division of tasks according to T, an individual task according to t \u0445 T, and a distribution of the demonstrations for task t according to D. A policy is symbolized by \u03c0\u03b8 (a | o, d), where a is an action, o is an observation, d is a demonstration, and \u03b8 are the parameters of politics. A demonstration d \u00b2 D (t) is a sequence of observations and actions: d = [(o1, a1), (o2, a2),..., (oT, aT). We assume that the division of tasks T is given and that we can obtain successful demonstrations for each task in t \u00b2 T. We assume that for each task there is a scalable evaluation function Rt (d) (e.g. a binary value indicating success), although this is not necessary during the training. The goal is to maximize the expected performance of the policy, with the expectation of the tasks t \u00b2 T (d) and the demonstrations d \u00b2 t (t)."}, {"heading": "3.2. Example Settings", "text": "In order to clarify the problem, we describe two concrete examples, which we will examine later in the experiments."}, {"heading": "3.2.1. PARTICLE REACHING", "text": "As shown in Fig. 2, one task could be to reach the orange square and another task could be to reach the green triangle. An agent gets his own 2D location and the 2D locations of the individual boundary stones. Within each task, the starting position of the agent as well as the positions of all boundary stones in different instances of the task can vary. Without demonstration, the robot does not know which boundary sign to reach and will not be able to accomplish the task. Therefore, this setting already gets to the core of a unique imitation, namely the communication of the task through a demonstration. After learning, the agent should be able to identify the target from the demonstration and reach the same boundary stone in a new instance of the task."}, {"heading": "3.2.2. BLOCK STACKING", "text": "In the group of block stacking tasks, the goal is to control a 7 DOF fetch robot arm to stack different numbers of cubes - 0http: / / fetchrobotics.com / shaped blocks in user-defined configurations. Each configuration consists of a list of blocks arranged in towers of different heights, which can be identified by a string such as ghij or ab cd ef gh, as shown in Fig. 3. Each of these configurations corresponds to a different task. In a typical task, an observation consists of a list (x, y, z) of object positions relative to the gripper and information on whether the gripper is opened or closed. The number of objects may vary according to the task."}, {"heading": "3.3. Algorithm", "text": "To train the politics of neural networks, we can use arbitrary algorithms for political learning in sequential decision-making problems. If, for example, rewards are available in the training tasks, we can use reinforcement learning to optimize the policy. The only change required is to link the policy to a randomly selected demonstration at the beginning of each episode. In this paper, we focus on learning algorithms such as behavioral cloning and DAGGER (Ross et al., 2011), where only demon strations need to be specified instead of reward functions. [1] This has the potential to be more scalable, as it is often easier to prove a task than to specify a well-formed reward function (Ng et al., 1999). We start by collecting a series of demonstrations for each task where we add sounds to the actions in order to have broader trajectory coverage."}, {"heading": "4. Architectures", "text": "Although a generic neural network could, in principle, learn to map demonstration and current observation to appropriate actions, we felt it was important to use an appropriate architecture. Our architecture for learning block stacks is one of the main contributions of this paper, and we believe it is representative of what architectures might look like for the one-time imitation of more complex tasks in the future. Although the particle task is simpler, we also found architectural decisions important, and we will consider several options that should be evaluated in Section 5.1 below."}, {"heading": "4.1. Architecture for Particle Reaching", "text": "We look at three architectures for this problem: \u2022 Plain LSTM: The first architecture is a simple LSTM (Hochreiter & Schmidhuber, 1997) with 512 hidden units. It reads the demonstration process, whose output is then linked to the current state and fed to a multi-layer perceptron (MLP) to generate the action. \u2022 Attention LSTM: In this architecture, the LSTM issues a weighting across the various boundaries from the demonstration sequence, then applies this weighting in the test scene and produces a weighted combination over boundary positions given the current state. This 2D output is then linked to the current position of the agent and fed to an MLP to generate the action."}, {"heading": "4.2. Architecture for Block Stacking", "text": "For the block stacking task, it is desirable that the political architecture has the following characteristics: 1. It should be easy to apply to task instances that have different block numbers; 2. It should, of course, be applied to different permutations of the same task. For example, the policy should perform well on task dcba, even if it only applies to task abc.3. It should accommodate demonstrations of different lengths. Our proposed architecture consists of three modules: the demonstration network, the context network, and the manipulation network. The modules indispensable use an operation to attract attention in the neighborhood. We will first describe this operation in more detail, followed by the description of each of the three modules."}, {"heading": "4.2.1. NEIGHBORHOOD ATTENTION", "text": "Since our neural network must handle demonstrations with variable number of blocks, it must have modules that can handle variable dimensional input. Soft attention is a natural operation that maps variable dimensional inputs to fixed dimensional outputs, but this may cause it to lose information relative to its input. This is undesirable because the amount of information contained in a demonstration grows as the number of blocks increases. Therefore, we need an operation that can block variable dimensional inputs to outputs with comparable dimensions. Intuitively, instead of having a single output as a result of participating in all inputs, we have as many outputs as inputs and have each output that is related to all other inputs in relation to their own corresponding inputs.We start by describing the soft attention module as specified in (Bahdanau et al., 2014). Attention input includes a query q, a list of context vectors {cj}, and a list of ectors."}, {"heading": "N Attentions i-th attention is applied to i-th block vs others", "text": "In practice, we use multiple headers per block, so the size of each result is proportional to the number of headers."}, {"heading": "4.2.2. DEMONSTRATION NETWORK", "text": "The size of this embedding grows linearly depending on the length of the demonstration and the number of blocks in the environment. In block stacking, the demonstrations can range from hundreds to thousands of time steps, and training with such long sequences can be challenging both in time and memory usage. Therefore, we randomly discard a subset of time steps during training, an operation we call time stoppers, analogous to (Srivastava et al., 2014; Krueger et al., 2016). In our experiments, we use p = 0.95, which reduces the length of the demonstrations by a factor of 20. During the test period, we can prick several time-sampled trajectories to calculate the downstream results and use these average results to estimate the total cyclicality (as shown in Fig. 5)."}, {"heading": "4.2.3. CONTEXT NETWORK", "text": "The context network is the crux of our model. In Fig. 6, illustrated, it processes both the current state and the embedding produced by the demonstration network, and outputs a context embedding whose dimension does not depend on the length of the demonstration or the number of blocks in the environment. Therefore, it is forced to capture only the relevant information used by the manipulation network. The context network begins by calculating a query vector as a function of the current state, which is then used to take care of the different time steps in the demonstration embedding. Attention weights across different blocks within the same time step are summarized to generate a single weight per time step. The result of this time attention is a vector whose size is proportional to the number of blocks in the environment. We then apply neighborhood attention to spread the information about the embedding of each block to the single block size, this process is repeated several times, with the single STM generating a single block size."}, {"heading": "4.2.4. MANIPULATION NETWORK", "text": "The manipulation network is the simplest component. In Fig. 7, after extracting information from source and target blocks, it calculates the measures required to complete the current phase of stacking one block over another using a simple MLP network.2 This division of labor opens up the possibility of modular training: the manipulation network can be trained to perform this simple procedure without knowing about demonstrations or more than two blocks in the vicinity. We leave this possibility to future work."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Particle Reaching", "text": "To demonstrate the key concepts underlying the Unified Learning Framework, we are conducting experiments with the simple 2D particle achievement task described in Section 3.2.1. We are looking at an increasingly difficult group of task families, in which the number of boundary stones increases from 2 to 10. For each task family, we collect 10,000 trajectories for training, in which the positions of boundary stones and the starting position of the point robot are randomized. We are using a hard-coded expert module to efficiently generate demonstrations. We are adding noise to the trajectories by disrupting the calculated actions before applying them to the environment, and we are using simple behavioral cloning to replace The2In principle, this module can be replaced by an RNN module, but for the tasks we are looking at, we did not consider this necessary."}, {"heading": "5.1.1. RESULTS", "text": "The results are shown in Fig. 8. We observe that we achieve a much better generalization performance with increasing specialization of the architecture. In this simple task, it seems that conditioning on the entire demonstration affects the generalization performance and conditioning on only the final state delivers the best performance even without explicit regularization. This makes intuitive sense, since the final state already sufficiently characterizes the task. However, this conclusion does not seem to apply, as the task becomes more complicated, as the next series of experiments shows."}, {"heading": "5.2. Block Stacking", "text": "In fact, most of them are not just a problem, but a problem that has come to a head in recent years, \"he told the German Press Agency.\" It's a problem that we have to solve, \"he said,\" but it's not that we have to solve it. \""}, {"heading": "5.2.1. PERFORMANCE EVALUATION", "text": "This is because tasks requiring more levels tend to be more demanding. In fact, even our scripted measures often fail due to the most difficult tasks within the same group. Note that there are no tasks with 8 levels in the training tasks, and no tasks with 1 or 3 levels in the test tasks, and the corresponding interventions are omitted. Figure 1 shows that the average success rate for all tasks within the same group is. Note that there are no tasks with 8 levels in the training tasks, and that the tasks are performed with 1 or 3 levels in the test tasks."}, {"heading": "5.2.2. EVALUATING PERMUTATION INVARIANCE", "text": "During training and in previous assessments, we select only one task per equivalence class, with two tasks considered equivalent if they are equal to the permutation of different blocks, based on the assumption that our architecture is invariant against permutations between blocks. For example, if the policy is trained only on the abcd task, it should perform well on the dcba task in a single demonstration of the dcba task. We are now testing this property experimentally by defining a training task and evaluating the performance of the policy among all equivalent permutations of the same. As Figure 10 shows, even though the policy has seen the task only abcd, it achieves the same level of performance for all other equivalent tasks."}, {"heading": "5.2.3. EFFECT OF ENSEMBLING", "text": "We are now evaluating the importance of scanning multiple sampled demonstrations during the evaluation process introduced in Section 4.2.2. Figure 11 shows the performance of all training and test tasks as the number of ensembles varies from 1 to 20. We observe that more ensembles help the most for tasks with fewer levels. On the other hand, it continuously improves performance on the more difficult tasks, although the gap is smaller. We suspect that this is due to the fact that politicians have learned to take care of frames in the demonstration process in which the blocks are already stacked. For example, for tasks with only one level, it is very easy to drop these frames in a single sampled demonstration. On the other hand, it becomes more resistant to missing frames for tasks with more levels. Using more than 10 ensembles does not seem to bring significant improvements, so we have used 10 ensembles in our main evaluation."}, {"heading": "5.2.4. BREAKDOWN OF FAILURE CASES", "text": "To understand the limitations of the current approach, we conduct an analysis of the failures. We consider three failure scenarios: \"Wrong move\" means that the policy has arranged a layout incompatible with the desired layout, which could be because the policy has misinterpreted the demonstration, or an accidental misstep in which the blocks accidentally end up in the wrong layout. \"Manipulation failure\" means that the policy has made an irretrievable mistake, for example, if the block is shaken off the table, which the current hard-coded policy cannot handle. \"Reproducible failure\" means that the policy runs out of time before the task is completed, which may be due to an accidental failure during the operation that would have been remediable in the face of more time. As shown in Fig. 12, conditioning only the final state leads to more wrong movements compared to other architectures. Apart from that, most failures are actually due to manipulation errors, most of which are unworkable."}, {"heading": "5.2.5. VISUALIZATION", "text": "Finally, we visualize the attention mechanisms that underlie the main architecture of politics. There are two types of attention that we are primarily interested in, one in which politics takes care of different time steps in the demonstration, and the other in which politics takes care of different blocks in the current step to filter out irrelevant signals. Figure 13 shows a subset of attention breathmaps and the complete set of visualizations, along with key frames of the neural Network4 indicating that the actual ratio of misinterpreted demonstrations may be different, since the runs that caused a manipulation error could later lead to a wrong step if successfully executed. On the other hand, by visualizing the videos, we observed that most of the trajectories categorized as \"wrongly moved\" are actually due to manipulation failures (with the exception of the political conditioning of the final state that occasionally appears to perform an actual wrong move). 12. Illustration of the attention folding and the corresponding failure pattern can represent the corresponding area of attention."}, {"heading": "6. Conclusions", "text": "In this paper, we presented a simple model that assigns a single successful demonstration of a task to an effective policy that solves the task in a new situation. We demonstrated the effectiveness of this approach in two areas: particle achievement and block stacking. There are a lot of exciting directions for future work. We plan to expand the scope to demonstrations in the form of image data that allow more end-to-end learning without requiring a separate perception module. We are also interested in enabling policy makers to attach multiple demonstrations to conditions in case a demonstration does not fully resolve the ambiguities in the object. 5 Furthermore, and most importantly, we hope to expand our method to a much larger and broader task distribution and to reach its potential toward a general robotic imitation learning system5 Many tasks that include abstract concepts fall into this category. For example, when sorting and grouping different objects, whether the goal was to group objects based on colors or based on shapes, the ability to perform tasks would be overwhelming in diversity."}, {"heading": "7. Acknowledgements", "text": "We would like to thank our colleagues at Berkeley and OpenAI for their insightful discussions, especially the OpenAI Robotics team for supporting the infrastructure. This research was partly funded by ONR through a PECASE Prize. Yan Duan was also supported by a Huawei Fellowship."}, {"heading": "A. More Details on Particle Reaching", "text": "A.1. Learning CurvesFig. 14 shows the learning curves for the three architectures developed for the tasks that reach the particle, as the number of landmarks varies by executing the guidelines over 100 different configurations, and success rates are calculated using both training and test data. We can clearly observe that both LSTM-based architectures match as the number of landmarks increases. On the other hand, the use of attention clearly improves generalization performance, and by conditioning only to the final state, a perfect generalization is achieved in all scenarios. It is also interesting to observe that learning goes through a phase transition. Intuitively, this may be the case that the network learns to derive the task from the demonstration."}, {"heading": "B. More Details on Block Stacking", "text": "These learning curves do not reflect final performance: for each evaluation point, we collect tasks and demonstrations from the training data, place the environment at the starting point of a certain level (so that some blocks are already stacked), and implement the policy only up to one level."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Abbeel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2004}, {"title": "Learning to learn by gradient descent by gradient descent", "author": ["Andrychowicz", "Marcin", "Denil", "Misha", "Gomez", "Sergio", "Hoffman", "Matthew W", "Pfau", "David", "Schaul", "Tom", "de Freitas", "Nando"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Andrychowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz et al\\.", "year": 2016}, {"title": "A survey of robot learning from demonstration", "author": ["Argall", "Brenna D", "Chernova", "Sonia", "Veloso", "Manuela", "Browning", "Brett"], "venue": "Robotics and autonomous systems,", "citeRegEx": "Argall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Argall et al\\.", "year": 2009}, {"title": "Using fast weights to attend to the recent past", "author": ["Ba", "Jimmy", "Hinton", "Geoffrey E", "Mnih", "Volodymyr", "Leibo", "Joel Z", "Ionescu", "Catalin"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Ba et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Interaction networks for learning about objects, relations and physics", "author": ["Battaglia", "Peter", "Pascanu", "Razvan", "Lai", "Matthew", "Rezende", "Danilo Jimenez"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Battaglia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Battaglia et al\\.", "year": 2016}, {"title": "On the optimization of a synaptic learning rule", "author": ["Bengio", "Samy", "Yoshua", "Cloutier", "Jocelyn", "Gecsei", "Jan"], "venue": "In Optimality in Artificial and Biological Neural Networks, pp", "citeRegEx": "Bengio et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1992}, {"title": "Neurodynamic programming: an overview", "author": ["Bertsekas", "Dimitri P", "Tsitsiklis", "John N"], "venue": "In Decision and Control,", "citeRegEx": "Bertsekas et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Bertsekas et al\\.", "year": 1995}, {"title": "Robot programming by demonstration", "author": ["Calinon", "Sylvain"], "venue": "EPFL Press,", "citeRegEx": "Calinon and Sylvain.,? \\Q2009\\E", "shortCiteRegEx": "Calinon and Sylvain.", "year": 2009}, {"title": "A compositional object-based approach to learning physical dynamics", "author": ["Chang", "Michael B", "Ullman", "Tomer", "Torralba", "Antonio", "Tenenbaum", "Joshua B"], "venue": "In Int. Conf. on Learning Representations (ICLR),", "citeRegEx": "Chang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2017}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Donahue", "Jeff", "Jia", "Yangqing", "Vinyals", "Oriol", "Hoffman", "Judy", "Zhang", "Ning", "Tzeng", "Eric", "Darrell", "Trevor"], "venue": "In ICML, pp", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Rl: Fast reinforcement learning via slow reinforcement learning", "author": ["Duan", "Yan", "Schulman", "John", "Chen", "Xi", "Bartlett", "Peter L", "Sutskever", "Ilya", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1611.02779,", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Towards a neural statistician", "author": ["Edwards", "Harrison", "Storkey", "Amos"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Edwards et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Edwards et al\\.", "year": 2017}, {"title": "Guided cost learning: Deep inverse optimal control via policy optimization", "author": ["Finn", "Chelsea", "Levine", "Sergey", "Abbeel", "Pieter"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "Finn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2016}, {"title": "Learning invariant feature spaces to transfer skills with reinforcement learning", "author": ["Gupta", "Abhishek", "Devin", "Coline", "Liu", "YuXuan", "Abbeel", "Pieter", "Levine", "Sergey"], "venue": "In Int. Conf. on Learning Representations (ICLR),", "citeRegEx": "Gupta et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2017}, {"title": "Learning continuous control policies by stochastic value gradients", "author": ["Heess", "Nicolas", "Wayne", "Gregory", "Silver", "David", "Lillicrap", "Tim", "Erez", "Tom", "Tassa", "Yuval"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Heess et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2015}, {"title": "Generative adversarial imitation learning", "author": ["Ho", "Jonathan", "Ermon", "Stefano"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ho et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2016}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Siamese neural networks for one-shot image recognition", "author": ["Koch", "Gregory"], "venue": "ICML Deep Learning Workshop,", "citeRegEx": "Koch and Gregory.,? \\Q2015\\E", "shortCiteRegEx": "Koch and Gregory.", "year": 2015}, {"title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["Kulis", "Brian", "Saenko", "Kate", "Darrell", "Trevor"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Kulis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulis et al\\.", "year": 2011}, {"title": "Nonlinear inverse reinforcement learning with gaussian processes", "author": ["S. Levine", "Z. Popovic", "V. Koltun"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Levine et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2011}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Wierstra and Daan.,? \\Q2015\\E", "shortCiteRegEx": "Wierstra and Daan.", "year": 2015}, {"title": "Meta-neural networks that learn by learning", "author": ["Naik", "Devang K", "Mammone", "RJ"], "venue": "In International Joint Conference on Neural Netowrks (IJCNN),", "citeRegEx": "Naik et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Naik et al\\.", "year": 1992}, {"title": "Algorithms for inverse reinforcement learning", "author": ["Ng", "Andrew", "Russell", "Stuart"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Ng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2000}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["Ng", "Andrew Y", "Harada", "Daishi", "Russell", "Stuart"], "venue": "In ICML,", "citeRegEx": "Ng et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "Autonomous helicopter flight via reinforcement learning", "author": ["Ng", "Andrew Y", "Kim", "H Jin", "Jordan", "Michael I", "Sastry", "Shankar", "Ballianda", "Shiv"], "venue": "In NIPS,", "citeRegEx": "Ng et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2003}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["Peters", "Jan", "Schaal", "Stefan"], "venue": "Neural networks,", "citeRegEx": "Peters et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2008}, {"title": "Alvinn: An autonomous land vehicle in a neural network", "author": ["Pomerleau", "Dean A"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Pomerleau and A.,? \\Q1989\\E", "shortCiteRegEx": "Pomerleau and A.", "year": 1989}, {"title": "Optimization as a model for few-shot learning", "author": ["Ravi", "Sachin", "Larochelle", "Hugo"], "venue": "In Under Review,", "citeRegEx": "Ravi et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ravi et al\\.", "year": 2017}, {"title": "One-shot generalization in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Danihelka", "Ivo", "Gregor", "Karol", "Wierstra", "Daan"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Rezende et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2016}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross", "St\u00e9phane", "Gordon", "Geoffrey J", "Bagnell", "Drew"], "venue": "In AISTATS,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "cad) rl: Real single-image flight without a single real", "author": ["Sadeghi", "Fereshteh", "Levine", "Sergey"], "venue": null, "citeRegEx": "Sadeghi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sadeghi et al\\.", "year": 2016}, {"title": "Meta-learning with memory-augmented neural networks", "author": ["Santoro", "Adam", "Bartunov", "Sergey", "Botvinick", "Matthew", "Wierstra", "Daan", "Lillicrap", "Timothy"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Santoro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santoro et al\\.", "year": 2016}, {"title": "Is imitation learning the route to humanoid robots", "author": ["Schaal", "Stefan"], "venue": "Trends in cognitive sciences,", "citeRegEx": "Schaal and Stefan.,? \\Q1999\\E", "shortCiteRegEx": "Schaal and Stefan.", "year": 1999}, {"title": "Evolutionary principles in selfreferential learning. On learning how to learn: The meta-meta-.", "author": ["Schmidhuber", "Jurgen"], "venue": "hook.) Diploma thesis, Institut f. Informatik, Tech. Univ. Munich,", "citeRegEx": "Schmidhuber and Jurgen.,? \\Q1987\\E", "shortCiteRegEx": "Schmidhuber and Jurgen.", "year": 1987}, {"title": "Learning to control fast-weight memories: An alternative to dynamic recurrent networks", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1992\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1992}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael I", "Moritz", "Philipp"], "venue": "In ICML, pp", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Third person imitation learning", "author": ["Stadie", "Bradlie", "Abbeel", "Pieter", "Sutskever", "Ilya"], "venue": "In Int. Conf. on Learning Representations (ICLR),", "citeRegEx": "Stadie et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Stadie et al\\.", "year": 2017}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Temporal difference learning and tdgammon", "author": ["Tesauro", "Gerald"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro and Gerald.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro and Gerald.", "year": 1995}, {"title": "Learning to learn", "author": ["Thrun", "Sebastian", "Pratt", "Lorien"], "venue": "Springer Science & Business Media,", "citeRegEx": "Thrun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Thrun et al\\.", "year": 1998}, {"title": "Deep domain confusion: Maximizing for domain invariance", "author": ["Tzeng", "Eric", "Hoffman", "Judy", "Zhang", "Ning", "Saenko", "Kate", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1412.3474,", "citeRegEx": "Tzeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tzeng et al\\.", "year": 2014}, {"title": "Towards adapting deep visuomotor representations from simulated to real environments", "author": ["Tzeng", "Eric", "Devin", "Coline", "Hoffman", "Judy", "Finn", "Chelsea", "Peng", "Xingchao", "Abbeel", "Pieter", "Levine", "Sergey", "Saenko", "Kate", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1511.07111,", "citeRegEx": "Tzeng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tzeng et al\\.", "year": 2015}, {"title": "Matching networks for one shot learning", "author": ["Vinyals", "Oriol", "Blundell", "Charles", "Lillicrap", "Tim", "Wierstra", "Daan"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Vinyals et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2016}, {"title": "Learning to reinforcement learn", "author": ["Wang", "Jane X", "Kurth-Nelson", "Zeb", "Tirumala", "Dhruva", "Soyer", "Hubert", "Leibo", "Joel Z", "Munos", "Remi", "Blundell", "Charles", "Kumaran", "Dharshan", "Botvinick", "Matt"], "venue": "arXiv preprint arXiv:1611.05763,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron C", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Bengio", "Yoshua"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Cross-domain video concept detection using adaptive svms", "author": ["Yang", "Jun", "Yan", "Rong", "Hauptmann", "Alexander G"], "venue": "In Proceedings of the 15th ACM international conference on Multimedia,", "citeRegEx": "Yang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2007}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["Yu", "Fisher", "Koltun", "Vladlen"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["B. Ziebart", "A. Maas", "J.A. Bagnell", "A.K. Dey"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Ziebart et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 4, "context": "To make this model work, we made essential use of soft attention (Bahdanau et al., 2014) for processing both the (potentially long) sequence of states and action that correspond to the demonstration, and for processing the components of the vector specifying the locations of the various blocks in our environment.", "startOffset": 65, "endOffset": 88}, {"referenceID": 2, "context": "Survey articles include (Schaal, 1999; Calinon, 2009; Argall et al., 2009).", "startOffset": 24, "endOffset": 74}, {"referenceID": 53, "context": "(2011)); and inverse reinforcement learning (Ng & Russell, 2000), where a reward function (Abbeel & Ng, 2004; Ziebart et al., 2008; Levine et al., 2011; Finn et al., 2016; Ho & Ermon, 2016) is estimated that explains the demonstrations as (near) optimal behavior.", "startOffset": 90, "endOffset": 189}, {"referenceID": 22, "context": "(2011)); and inverse reinforcement learning (Ng & Russell, 2000), where a reward function (Abbeel & Ng, 2004; Ziebart et al., 2008; Levine et al., 2011; Finn et al., 2016; Ho & Ermon, 2016) is estimated that explains the demonstrations as (near) optimal behavior.", "startOffset": 90, "endOffset": 189}, {"referenceID": 14, "context": "(2011)); and inverse reinforcement learning (Ng & Russell, 2000), where a reward function (Abbeel & Ng, 2004; Ziebart et al., 2008; Levine et al., 2011; Finn et al., 2016; Ho & Ermon, 2016) is estimated that explains the demonstrations as (near) optimal behavior.", "startOffset": 90, "endOffset": 189}, {"referenceID": 30, "context": ", Pomerleau (1989); Ross et al. (2011)); and inverse reinforcement learning (Ng & Russell, 2000), where a reward function (Abbeel & Ng, 2004; Ziebart et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 48, "context": "One-shot and few-shot learning has been studied for image recognition (Vinyals et al., 2016; Koch, 2015; Santoro et al., 2016; Ravi & Larochelle, 2017), generative modeling (Edwards & Storkey, 2017; Rezende et al.", "startOffset": 70, "endOffset": 151}, {"referenceID": 35, "context": "One-shot and few-shot learning has been studied for image recognition (Vinyals et al., 2016; Koch, 2015; Santoro et al., 2016; Ravi & Larochelle, 2017), generative modeling (Edwards & Storkey, 2017; Rezende et al.", "startOffset": 70, "endOffset": 151}, {"referenceID": 32, "context": ", 2016; Ravi & Larochelle, 2017), generative modeling (Edwards & Storkey, 2017; Rezende et al., 2016), learning \u201cfast\u201d reinforcement learning agents with recurrent policies (Duan et al.", "startOffset": 54, "endOffset": 101}, {"referenceID": 12, "context": ", 2016), learning \u201cfast\u201d reinforcement learning agents with recurrent policies (Duan et al., 2016; Wang et al., 2016).", "startOffset": 79, "endOffset": 117}, {"referenceID": 49, "context": ", 2016), learning \u201cfast\u201d reinforcement learning agents with recurrent policies (Duan et al., 2016; Wang et al., 2016).", "startOffset": 79, "endOffset": 117}, {"referenceID": 3, "context": "Fast adaptation has also been achieved through fast-weights (Ba et al., 2016).", "startOffset": 60, "endOffset": 77}, {"referenceID": 6, "context": "Metalearning has also been studied to discover neural network weight optimization algorithms (Bengio et al., 1992; Schmidhuber, 1992; Andrychowicz et al., 2016).", "startOffset": 93, "endOffset": 160}, {"referenceID": 1, "context": "Metalearning has also been studied to discover neural network weight optimization algorithms (Bengio et al., 1992; Schmidhuber, 1992; Andrychowicz et al., 2016).", "startOffset": 93, "endOffset": 160}, {"referenceID": 28, "context": "Reinforcement learning has had many successes, including Backgammon (Tesauro, 1995), helicopter control (Ng et al., 2003), Atari (Mnih et al.", "startOffset": 104, "endOffset": 121}, {"referenceID": 39, "context": ", 2016), continuous control in simulation (Schulman et al., 2015; Heess et al., 2015; Lillicrap et al., 2015) and on real robots (Peters & Schaal, 2008; Levine et al.", "startOffset": 42, "endOffset": 109}, {"referenceID": 16, "context": ", 2016), continuous control in simulation (Schulman et al., 2015; Heess et al., 2015; Lillicrap et al., 2015) and on real robots (Peters & Schaal, 2008; Levine et al.", "startOffset": 42, "endOffset": 109}, {"referenceID": 23, "context": ", 2015) and on real robots (Peters & Schaal, 2008; Levine et al., 2016).", "startOffset": 27, "endOffset": 71}, {"referenceID": 51, "context": "Success stories include domain adaptation in computer vision (Yang et al., 2007; Kulis et al., 2011; Tzeng et al., 2014; Donahue et al., 2014) and control (Tzeng et al.", "startOffset": 61, "endOffset": 142}, {"referenceID": 21, "context": "Success stories include domain adaptation in computer vision (Yang et al., 2007; Kulis et al., 2011; Tzeng et al., 2014; Donahue et al., 2014) and control (Tzeng et al.", "startOffset": 61, "endOffset": 142}, {"referenceID": 46, "context": "Success stories include domain adaptation in computer vision (Yang et al., 2007; Kulis et al., 2011; Tzeng et al., 2014; Donahue et al., 2014) and control (Tzeng et al.", "startOffset": 61, "endOffset": 142}, {"referenceID": 11, "context": "Success stories include domain adaptation in computer vision (Yang et al., 2007; Kulis et al., 2011; Tzeng et al., 2014; Donahue et al., 2014) and control (Tzeng et al.", "startOffset": 61, "endOffset": 142}, {"referenceID": 47, "context": ", 2014) and control (Tzeng et al., 2015; Rusu et al., 2016; Sadeghi & Levine, 2016; Gupta et al., 2017; Stadie et al., 2017).", "startOffset": 20, "endOffset": 124}, {"referenceID": 15, "context": ", 2014) and control (Tzeng et al., 2015; Rusu et al., 2016; Sadeghi & Levine, 2016; Gupta et al., 2017; Stadie et al., 2017).", "startOffset": 20, "endOffset": 124}, {"referenceID": 41, "context": ", 2014) and control (Tzeng et al., 2015; Rusu et al., 2016; Sadeghi & Levine, 2016; Gupta et al., 2017; Stadie et al., 2017).", "startOffset": 20, "endOffset": 124}, {"referenceID": 4, "context": "We use the soft attention model proposed in (Bahdanau et al., 2014) for machine translations, and which has also been successful in image captioning (Xu et al.", "startOffset": 44, "endOffset": 67}, {"referenceID": 50, "context": ", 2014) for machine translations, and which has also been successful in image captioning (Xu et al., 2015).", "startOffset": 89, "endOffset": 106}, {"referenceID": 5, "context": "The interaction networks proposed in (Battaglia et al., 2016; Chang et al., 2017) also leverage locality of physical interaction in learning.", "startOffset": 37, "endOffset": 81}, {"referenceID": 9, "context": "The interaction networks proposed in (Battaglia et al., 2016; Chang et al., 2017) also leverage locality of physical interaction in learning.", "startOffset": 37, "endOffset": 81}, {"referenceID": 42, "context": "Our model is also related to the sequence to sequence model (Sutskever et al., 2014; Cho et al., 2014), as in both cases we consume a very long demonstration sequence and, effectively, emit a long sequence of actions.", "startOffset": 60, "endOffset": 102}, {"referenceID": 10, "context": "Our model is also related to the sequence to sequence model (Sutskever et al., 2014; Cho et al., 2014), as in both cases we consume a very long demonstration sequence and, effectively, emit a long sequence of actions.", "startOffset": 60, "endOffset": 102}, {"referenceID": 33, "context": "In this paper, we focus on imitation learning algorithms such as behavioral cloning and DAGGER (Ross et al., 2011), which only require demonstrations rather than reward functions to be specified.", "startOffset": 95, "endOffset": 114}, {"referenceID": 27, "context": "1 This has the potential to be more scalable, since it is often easier to demonstrate a task than specifying a well-shaped reward function (Ng et al., 1999).", "startOffset": 139, "endOffset": 156}, {"referenceID": 4, "context": "We start by describing the soft attention module as specified in (Bahdanau et al., 2014).", "startOffset": 65, "endOffset": 88}, {"referenceID": 27, "context": "This is related to the way in which reward shaping can significantly affect performance in reinforcement learning (Ng et al., 1999).", "startOffset": 114, "endOffset": 131}], "year": 2017, "abstractText": "Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a metalearning framework for achieving such capability, which we call one-shot imitation learning. Specifically, we consider the setting where there is a very large (maybe infinite) set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. Our experiments show that the use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks. OpenAI University of California, Berkeley. Correspondence to: Yan Duan <rocky@openai.com>. Copyright 2017 by the author(s). One-shot policy. A single policy trained to solve many tasks. (left) Task-specific policy. This policy is trained to stack blocks into two towers, each of height 3. (right) A separate task-specific policy. This policy is trained to stack blocks into three towers, each of height 2. Figure 1. Traditionally, policies are task-specific. For example, a policy might have been trained (through imitation or reinforcement learning) to stack blocks into towers of height 3, and then another policy would be trained to stack blocks into towers of height 2, etc. In this paper, we are interested in policies that are not specific to one task, but rather can be told (through a single demonstration) what the current new task is, and be successful at this new task. As illustrative examples, we would want to be able to provide a single demonstration of each task, and from that the one-shot policy would know what to do when faced with a new situation of the task, where the blocks are randomly rearranged. Videos of the illustrated tasks are available at http://bit.ly/one-shot-imitation.", "creator": "LaTeX with hyperref package"}}}