{"id": "1703.07345", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "On The Projection Operator to A Three-view Cardinality Constrained Set", "abstract": "The cardinality constraint is an intrinsic way to restrict the solution structure in many domains, for example, sparse learning, feature selection, and compressed sensing. To solve a cardinality constrained problem, the key challenge is to solve the projection onto the cardinality constraint set, which is NP-hard in general when there exist multiple overlapped cardiaiality constraints. In this paper, we consider the scenario where overlapped cardinality constraints satisfy a Three-view Cardinality Structure (TVCS), which reflects the natural restriction in many applications, such as identification of gene regulatory networks and task-worker assignment problem. We cast the projection onto the TVCS set into a linear programming, and prove that its solution can be obtained by finding an integer solution to such linear programming. We further prove that such integer solution can be found with the complexity proportional to the problem scale. We finally use synthetic experiments and two interesting applications in bioinformatics and crowdsourcing to validate the proposed TVCS model and method.", "histories": [["v1", "Tue, 21 Mar 2017 17:58:03 GMT  (680kb,D)", "https://arxiv.org/abs/1703.07345v1", null], ["v2", "Wed, 14 Jun 2017 17:05:57 GMT  (108kb,D)", "http://arxiv.org/abs/1703.07345v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["haichuan yang", "shupeng gui", "chuyang ke", "daniel stefankovic", "ryohei fujimaki", "ji liu"], "accepted": true, "id": "1703.07345"}, "pdf": {"name": "1703.07345.pdf", "metadata": {"source": "CRF", "title": "On The Projection Operator to A Three-view Cardinality Constrained Set", "authors": ["Haichuan Yang", "Shupeng Gui", "Chuyang Ke", "Daniel Stefankovic", "Ryohei Fujimaki", "Ji Liu"], "emails": ["h.yang@rochester.edu,", "shupenggui@gmail.com,", "cke@u.rochester.edu", "stefanko@cs.rochester.edu,", "rfujimaki@nec-labs.com,", "ji.liu.uwisc@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it is so far that it is not even a year ago."}, {"heading": "2 Related Works", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush."}, {"heading": "3 Preliminary: GradMP and IHT Frameworks", "text": "In this case, it is as if it were an \"unforeseen\" situation, in which there would be an \"unforeseen\" situation."}, {"heading": "4 Projection Operator", "text": "This section introduces the solution of the essential projection step. Note that the projection onto a non-convex set is generally NP-hard. By using the special structure of TVCS, we show that the projection can be solved efficiently. Due to the side constraint, all the evidence is included in the supplementary material."}, {"heading": "4.1 LP Relaxation", "text": "First, we can transfer the projection problem (2) to an equivalent integer linear programming problem (ILP) according to Lemma 1. Projection problem (2) corresponds to the following integer linear programming problem (ILP): max x < v2, x > (3) is subject to Ax \u2264 s x-solution (0, 1} pwhere v2 is apply element-wise square operation on vector v. A is a | G | \u00d7 p matrix, which is defined as: A = [1 > C] (4), where C \u00b2 {0, 1} | G1} | G1, whose rows represent the indicator vector of each group g-G1 and G2. Each line in A corresponds to a group g of G. For example, Cij = 1 if the j-th coordinate is in the i-th group TVx, otherwise Cij-x-group, Cij-x-group, Cij-x-group,.e."}, {"heading": "4.2 Linearly Convergent Algorithm for Projection Operator onto TVCS", "text": "To find a solution on the vertex, one can use the Simplex Method. Although the Simplex Method guarantees to find an optimal solution on the vertex and could be very efficient in practice, it cannot have deterministic complexity. < < < p > p > p > p > p > p > p > p > p is only a sub-procedure in an iteration. In this section, we usually assume that the integer solution can solve many instances of the problem (3). Simplex may be efficient in practice, but its worst case may lead to exponential time complexity [Papadimitriou and Steiglitz, 1982]."}, {"heading": "5 Empirical Study", "text": "In this section, the proposed method is validated for both synthetic data and two practical applications: crowdsourcing and identification of gene regulatory networks."}, {"heading": "5.1 Linear Regression and Classification on Synthetic Data", "text": "In this section, we validate the proposed method with the linear regression object and the squared hinge objective (classification) on synthetic data. Let's define the proposed method with the linear regression object and the squared hinge loss as groups with all the rows and all the columns. < Xi, w > \u2212 yi) 2 and the squared hinge loss is defined as groups with all the rows and all the columns. < Xi, w >) 2, where n is the total number of training samples defined. Xi and yi are the features and labels of the i-th sample or the squared hinge losses are defined as converged points that can be on the face of the polytope in some cases instead of the vertex. However, we can add a small random perturbation to ensure the optimal point to be compared with the probability."}, {"heading": "5.2 Application in Crowdsourcing", "text": "The goal is to maximize the expected predictive accuracy based on the assigned task. Let's select the predicted task for each picture of the assigned workers and the quality of each worker based on the assigned task. Let's select the predicted task for each picture of the assigned workers and the quality of each employee based on the assigned task. Let's solve the assigned task by the assigned workers and the assigned task by the assigned workers. Let's take the assigned task by the assigned workers and the assigned task by the assigned workers. Let's take the assigned task by the assigned workers and the assigned task by the assigned workers. Let's apply the assigned task by the assigned workers to the assigned task. Let's take the assigned task by the assigned quality matrix, which is usually estimated from the gold standard test."}, {"heading": "5.3 Application in Identification of Gene Regulatory Networks", "text": "In this section, we will apply the number of genes related to the identification of gene regulatory networks (GRN). We will illustrate the relationships between different genes that play important roles in biological processes and activities by controlling the expression level of RNAs. There is a well-known biological competition called the DREAM Challenge for the identification of GRN. Based on the time series expression data that are RNAs' level along the time sequence, we must restore the entire gene network of the given size. A gene in the network is related to only a small number of genes, and we already know that there is no relationship between some genes. Therefore, the amount of edges that connect to a vertex is far less than the dimension of the GRN: it is a practical case of sequence and column-wise."}, {"heading": "6 Conclusion", "text": "This paper looks at TVCS-limited optimization, motivated by the intrinsic constraints on many important applications, such as bioinformatics, referral systems, and crowdsourcing. To solve the problem of cardinality, the key step is projection on cardinality constraints. Although projection on overlapping cardinality constraints is generally NP-hard, we prove that when the TVCS condition is met, the projection can be reduced to linear programming. Furthermore, we prove that there is an iterative algorithm that finds a holistic solution for linear programming within the time complexity O (((p + | G |) log\u03b1 1R, with R being the distance from the starting point to the optimization solution and \u03b1 < 1 the convergence rate. Finally, we use synthetic experiments and two interesting applications in bioinformatics and crowdsourcing to validate the proposed TVCS model."}, {"heading": "Acknowledgements", "text": "This project is partially supported by the NSF grant CNS-1548078 and the NEC grant."}, {"heading": "1 Proof of Lemma 1", "text": "First, we show how to turn the projection problem (2) into a selection problem for columns. For each vector w, the vector x (0, 1) p should indicate the nonzero positions of w, then we can claim that the vector v-vx (2) is a vector that has the same dimension with v, and it holds elements at positions where x \"1\" has, and fills zeros at positions where x \"0\" has. Furthermore, the vector w (G, s) can be defined in (4) if and only if its column indicator vector x Ax \u2264 s is met. Thus, the problem (2) can be converted into integral programming: min x (0.1) p, v \u2212 vx (9) subject to the Ax sands, the goal can be further simplified: The vector x (vx), which is 2 = < v2.1 x \u2212 > integral programming p. (0.1)"}, {"heading": "2 Proof of Theorem 2", "text": "To prove that theorem 2 is either 1 or 0, we use the concept of the completely unimodular matrix. Definition 2. Perfectly unimodular (TU) matrix. An integer matrix is TU, if the determinant of each square submatrix 3 in the sentence {\u2212 1, 0, 1}. Proposition 1. If A is the determinant, then A > is the TU, and its concatenation with identity matrices (i.e. [A >, I], [A >, I] >) are still TU.Proof. Since the determinant of the determinant is not changed, it is obvious that A > is TU, and its concatenation with identity matrix I maintains the TU property. First, we show that submatrix with size 1 is always determinant, because each element of I is either 1 or 0."}, {"heading": "If it is constructed from the TVCS model, then A is a TU matrix.", "text": "In other words: There are only three possible forms of such a submatrix p. We will all have their determinants in {\u2212 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 2, 1, 1, 2, 1, 1, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4"}, {"heading": "3 Proof of Theorem 3", "text": "To prove Theorem 3 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s s s \u00b2 s \u00b2 s \u00b2 s s s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s s \u00b2 s s \u00b2 s s \u00b2 s s s s s \u00b2 s s s \u00b2 s s s s \u00b2 s s s \u00b2 s s \u00b2 s s s s s s \u00b2 s s s s s \u00b2 s s s s s s \u00b2 s s s s \u00b2 s s \u00b2 s s s s s s s \u00b2 s s s s"}, {"heading": "4 Formulation of the Expected Accuracy in Crowdsourcing Task", "text": "AssignmentIn crowdsourcing task assignment task assignment problem, remember the objective function of problem (8): 1m m \u00b2 j = 1 Eacc (Q \u00b7, j \u00b7 \u2212 j) for the j-th task, Eacc (Q \u00b7, j, X \u00b7, j) is defined in the following: Eacc (Q \u00b7, j, X \u00b7, j) = P (y \u2212 j = 1) P (yj = 1) P (yj = 1) + P (y, j = 0 | yj = 0) P (yj = 0) P (yj = 0) (yj = 0) (12) where I (\u00b7) is the indicator (I (y, j = 1)."}], "references": [{"title": "Structured sparsity through convex optimization", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "Statistical Science,", "citeRegEx": "Bach et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2012}, {"title": "Model-based compressive sensing", "author": ["R.G. Baraniuk", "V. Cevher", "M.F. Duarte", "C. Hegde"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Baraniuk et al\\.,? \\Q1982\\E", "shortCiteRegEx": "Baraniuk et al\\.", "year": 1982}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "author": ["E.J. Candes", "J.K. Romberg", "T. Tao"], "venue": "Communications on pure and applied mathematics,", "citeRegEx": "Candes et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2006}, {"title": "Recovery of clustered sparse signals from compressive measurements", "author": ["V. Cevher", "P. Indyk", "C. Hegde", "R.G. Baraniuk"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Cevher et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cevher et al\\.", "year": 2009}, {"title": "A totally unimodular view of structured sparsity", "author": ["M. El Halabi", "V. Cevher"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Halabi and Cevher.,? \\Q2015\\E", "shortCiteRegEx": "Halabi and Cevher.", "year": 2015}, {"title": "Large-scale mapping and validation of escherichia coli transcriptional regulation from a compendium of expression profiles", "author": ["J.J. Faith", "B. Hayete", "J.T. Thaden", "I. Mogno", "J. Wierzbowski", "G. Cottarel", "S. Kasif", "J.J. Collins", "T.S. Gardner"], "venue": "PLoS biol,", "citeRegEx": "Faith et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Faith et al\\.", "year": 2007}, {"title": "Tigress: trustful inference of gene regulation using stability selection", "author": ["A.-C. Haury", "F. Mordelet", "P. Vera-Licona", "J.-P. Vert"], "venue": "BMC systems biology,", "citeRegEx": "Haury et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Haury et al\\.", "year": 2012}, {"title": "Compressive sensing recovery of spike trains using a structured sparsity model", "author": ["C. Hegde", "M.F. Duarte", "V. Cevher"], "venue": "In SPARS\u201909-Signal Processing with Adaptive Sparse Structured Representations,", "citeRegEx": "Hegde et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hegde et al\\.", "year": 2009}, {"title": "Approximation algorithms for model-based compressive sensing", "author": ["C. Hegde", "P. Indyk", "L. Schmidt"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Hegde et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hegde et al\\.", "year": 2015}, {"title": "A nearly-linear time framework for graph-structured sparsity", "author": ["C. Hegde", "P. Indyk", "L. Schmidt"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Hegde et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hegde et al\\.", "year": 2015}, {"title": "Adaptive task assignment for crowdsourced classification", "author": ["C.-J. Ho", "S. Jabbari", "J.W. Vaughan"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Ho et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2013}, {"title": "On approximate solutions of systems of linear inequalities", "author": ["A.J. Hoffman"], "venue": "In Selected Papers Of Alan J Hoffman: With Commentary,", "citeRegEx": "Hoffman.,? \\Q2003\\E", "shortCiteRegEx": "Hoffman.", "year": 2003}, {"title": "Inferring regulatory networks from expression data using tree-based methods", "author": ["V.A. Huynh-Thu", "A. Irrthum", "L. Wehenkel", "P. Geurts"], "venue": "PloS one,", "citeRegEx": "Huynh.Thu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huynh.Thu et al\\.", "year": 2010}, {"title": "Proximal methods for hierarchical sparse coding", "author": ["R. Jenatton", "J. Mairal", "G. Obozinski", "F. Bach"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jenatton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jenatton et al\\.", "year": 2011}, {"title": "Exclusive feature learning on arbitrary structures via l1,2-norm", "author": ["D. Kong", "R. Fujimaki", "J. Liu", "F. Nie", "C. Ding"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2014}, {"title": "Asynchronous stochastic coordinate descent: Parallelism and convergence properties", "author": ["J. Liu", "S.J. Wright"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Liu and Wright.,? \\Q2015\\E", "shortCiteRegEx": "Liu and Wright.", "year": 2015}, {"title": "Generating realistic in silico gene networks for performance assessment of reverse engineering methods", "author": ["D. Marbach", "T. Schaffter", "C. Mattiussi", "D. Floreano"], "venue": "Journal of computational biology,", "citeRegEx": "Marbach et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Marbach et al\\.", "year": 2009}, {"title": "Aracne: an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context", "author": ["A.A. Margolin", "I. Nemenman", "K. Basso", "C. Wiggins", "G. Stolovitzky", "R.D. Favera", "A. Califano"], "venue": "BMC bioinformatics, 7(Suppl 1):S7,", "citeRegEx": "Margolin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Margolin et al\\.", "year": 2006}, {"title": "minet: Ar/bioconductor package for inferring large transcriptional networks using mutual information", "author": ["P.E. Meyer", "F. Lafitte", "G. Bontempi"], "venue": "BMC bioinformatics,", "citeRegEx": "Meyer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Meyer et al\\.", "year": 2008}, {"title": "Cosamp: Iterative signal recovery from incomplete and inaccurate samples", "author": ["D. Needell", "J.A. Tropp"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Needell and Tropp.,? \\Q2009\\E", "shortCiteRegEx": "Needell and Tropp.", "year": 2009}, {"title": "A unified iterative greedy algorithm for sparsity constrained optimization", "author": ["N. Nguyen", "S. Chin", "T.D. Tran"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2012}, {"title": "Linear convergence of stochastic iterative greedy algorithms with sparse constraints", "author": ["N. Nguyen", "D. Needell", "T. Woolf"], "venue": "arXiv preprint arXiv:1407.0088,", "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by v1", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Vision research,", "citeRegEx": "Olshausen and Field.,? \\Q1997\\E", "shortCiteRegEx": "Olshausen and Field.", "year": 1997}, {"title": "Combinatorial optimization: algorithms and complexity", "author": ["C.H. Papadimitriou", "K. Steiglitz"], "venue": "Courier Corporation,", "citeRegEx": "Papadimitriou and Steiglitz.,? \\Q1982\\E", "shortCiteRegEx": "Papadimitriou and Steiglitz.", "year": 1982}, {"title": "Genenetweaver: in silico benchmark generation and performance profiling of network inference methods", "author": ["T. Schaffter", "D. Marbach", "D. Floreano"], "venue": null, "citeRegEx": "Schaffter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Schaffter et al\\.", "year": 2011}, {"title": "On benefits of selection diversity via bilevel exclusive sparsity", "author": ["H. Yang", "Y. Huang", "L. Tran", "J. Liu", "S. Huang"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Yuan and Lin.,? \\Q2006\\E", "shortCiteRegEx": "Yuan and Lin.", "year": 2006}, {"title": "Gradient hard thresholding pursuit for sparsity-constrained optimization", "author": ["X. Yuan", "P. Li", "T. Zhang"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Yuan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2014}, {"title": "Exact recovery of hard thresholding pursuit", "author": ["X. Yuan", "P. Li", "T. Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yuan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2016}, {"title": "On the consistency of feature selection using greedy least squares regression", "author": ["T. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang.,? \\Q2009\\E", "shortCiteRegEx": "Zhang.", "year": 2009}, {"title": "Exclusive lasso for multi-task feature selection", "author": ["Y. Zhou", "R. Jin", "S.C. Hoi"], "venue": "In AISTATS,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 22, "context": "The cardinality constraint is an intrinsic way to restrict the solution structure in many real problems, for example, sparse learning [Olshausen and Field, 1997], feature selection [Zhang, 2009], and compressed sensing [Candes et al.", "startOffset": 134, "endOffset": 161}, {"referenceID": 29, "context": "The cardinality constraint is an intrinsic way to restrict the solution structure in many real problems, for example, sparse learning [Olshausen and Field, 1997], feature selection [Zhang, 2009], and compressed sensing [Candes et al.", "startOffset": 181, "endOffset": 194}, {"referenceID": 2, "context": "The cardinality constraint is an intrinsic way to restrict the solution structure in many real problems, for example, sparse learning [Olshausen and Field, 1997], feature selection [Zhang, 2009], and compressed sensing [Candes et al., 2006].", "startOffset": 219, "endOffset": 240}, {"referenceID": 27, "context": "Some efficient iterative methods such as IHT [Yuan et al., 2014], CoSaMP [Needell and Tropp, 2009], GradMP [Nguyen et al.", "startOffset": 45, "endOffset": 64}, {"referenceID": 19, "context": ", 2014], CoSaMP [Needell and Tropp, 2009], GradMP [Nguyen et al.", "startOffset": 16, "endOffset": 41}, {"referenceID": 20, "context": ", 2014], CoSaMP [Needell and Tropp, 2009], GradMP [Nguyen et al., 2012], and their variants can guarantee to solve the original problem under some mild conditions.", "startOffset": 50, "endOffset": 71}, {"referenceID": 14, "context": "In [Kong et al., 2014], the authors discussed the overlapping situation and tried to solve the problem using convex relaxation, which is different from our approach.", "startOffset": 3, "endOffset": 22}, {"referenceID": 15, "context": "Yuan and Lin [2006] introduced the group LASSO, which pursues group-wise sparsity that restricts the number of groups for the selected variables.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "Jenatton et al. [2011] construct a hierarchical structure over the variables and use group LASSO with overlapped groups to solve it.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "Bach et al. [2012] extended the usage of L1-norm relaxation to several different categories of structures.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "Bach et al. [2012] extended the usage of L1-norm relaxation to several different categories of structures. And recently, another generalization work [El Halabi and Cevher, 2015] proposed convex envelopes for various sparsity structures. They built the framework by defining a totally unimodular penalty, and showed how to formulate different sparsity structures using the penalty. The work above concentrated on using convex relaxation to control the sparsity. Besides using convex relaxation, there are also some works focusing on projection-based methods. When the exact projection operator was provided, Baraniuk et al. [2010] extended the traditional IHT and CoSaMP methods to general sparsity structures.", "startOffset": 0, "endOffset": 630}, {"referenceID": 0, "context": "Bach et al. [2012] extended the usage of L1-norm relaxation to several different categories of structures. And recently, another generalization work [El Halabi and Cevher, 2015] proposed convex envelopes for various sparsity structures. They built the framework by defining a totally unimodular penalty, and showed how to formulate different sparsity structures using the penalty. The work above concentrated on using convex relaxation to control the sparsity. Besides using convex relaxation, there are also some works focusing on projection-based methods. When the exact projection operator was provided, Baraniuk et al. [2010] extended the traditional IHT and CoSaMP methods to general sparsity structures. In this work, the authors also introduced the projection operator for block sparsity and tree sparsity. Cevher et al. [2009] investigated cluster sparsity and they applied dynamic programming to solve the projection operator for their sparsity model.", "startOffset": 0, "endOffset": 835}, {"referenceID": 0, "context": "Bach et al. [2012] extended the usage of L1-norm relaxation to several different categories of structures. And recently, another generalization work [El Halabi and Cevher, 2015] proposed convex envelopes for various sparsity structures. They built the framework by defining a totally unimodular penalty, and showed how to formulate different sparsity structures using the penalty. The work above concentrated on using convex relaxation to control the sparsity. Besides using convex relaxation, there are also some works focusing on projection-based methods. When the exact projection operator was provided, Baraniuk et al. [2010] extended the traditional IHT and CoSaMP methods to general sparsity structures. In this work, the authors also introduced the projection operator for block sparsity and tree sparsity. Cevher et al. [2009] investigated cluster sparsity and they applied dynamic programming to solve the projection operator for their sparsity model. Hegde et al. [2009] introduced a \u201cspike trains\u201d signal model, which is also related to exclusive group sparsity.", "startOffset": 0, "endOffset": 981}, {"referenceID": 0, "context": "Bach et al. [2012] extended the usage of L1-norm relaxation to several different categories of structures. And recently, another generalization work [El Halabi and Cevher, 2015] proposed convex envelopes for various sparsity structures. They built the framework by defining a totally unimodular penalty, and showed how to formulate different sparsity structures using the penalty. The work above concentrated on using convex relaxation to control the sparsity. Besides using convex relaxation, there are also some works focusing on projection-based methods. When the exact projection operator was provided, Baraniuk et al. [2010] extended the traditional IHT and CoSaMP methods to general sparsity structures. In this work, the authors also introduced the projection operator for block sparsity and tree sparsity. Cevher et al. [2009] investigated cluster sparsity and they applied dynamic programming to solve the projection operator for their sparsity model. Hegde et al. [2009] introduced a \u201cspike trains\u201d signal model, which is also related to exclusive group sparsity. Its groups always have consecutive coordinates, and each group cannot contain more than one nonzero element. To solve the projection problem of their model, they showed the basic feasible solutions of the relaxed linear programming (LP) are always integer points. In our work, we also use LP to solve the projection problem. But our model defines the group structure differently and aims at different applications. In addition, there are some works for the cases without an efficient exact projection operator [Hegde et al., 2015a,b, Nguyen et al., 2014]. This is meaningful since the projection operator for complex structured sparsity often involves solving complicated combinatorial optimization problems. Hegde et al. [2015a] discussed how to guarantee convergence if using approximate projection in IHT and CoSaMP for compressive sensing.", "startOffset": 0, "endOffset": 1804}, {"referenceID": 0, "context": "Bach et al. [2012] extended the usage of L1-norm relaxation to several different categories of structures. And recently, another generalization work [El Halabi and Cevher, 2015] proposed convex envelopes for various sparsity structures. They built the framework by defining a totally unimodular penalty, and showed how to formulate different sparsity structures using the penalty. The work above concentrated on using convex relaxation to control the sparsity. Besides using convex relaxation, there are also some works focusing on projection-based methods. When the exact projection operator was provided, Baraniuk et al. [2010] extended the traditional IHT and CoSaMP methods to general sparsity structures. In this work, the authors also introduced the projection operator for block sparsity and tree sparsity. Cevher et al. [2009] investigated cluster sparsity and they applied dynamic programming to solve the projection operator for their sparsity model. Hegde et al. [2009] introduced a \u201cspike trains\u201d signal model, which is also related to exclusive group sparsity. Its groups always have consecutive coordinates, and each group cannot contain more than one nonzero element. To solve the projection problem of their model, they showed the basic feasible solutions of the relaxed linear programming (LP) are always integer points. In our work, we also use LP to solve the projection problem. But our model defines the group structure differently and aims at different applications. In addition, there are some works for the cases without an efficient exact projection operator [Hegde et al., 2015a,b, Nguyen et al., 2014]. This is meaningful since the projection operator for complex structured sparsity often involves solving complicated combinatorial optimization problems. Hegde et al. [2015a] discussed how to guarantee convergence if using approximate projection in IHT and CoSaMP for compressive sensing. They proved that the convergence needs a \u201chead approximation\u201d to project the update (gradient) before applying it. Hegde et al. [2015b] proposed a general framework to formulate a series of models as a weighted graph, and designed an efficient approximate projection operator for the models.", "startOffset": 0, "endOffset": 2054}, {"referenceID": 0, "context": "Bach et al. [2012] extended the usage of L1-norm relaxation to several different categories of structures. And recently, another generalization work [El Halabi and Cevher, 2015] proposed convex envelopes for various sparsity structures. They built the framework by defining a totally unimodular penalty, and showed how to formulate different sparsity structures using the penalty. The work above concentrated on using convex relaxation to control the sparsity. Besides using convex relaxation, there are also some works focusing on projection-based methods. When the exact projection operator was provided, Baraniuk et al. [2010] extended the traditional IHT and CoSaMP methods to general sparsity structures. In this work, the authors also introduced the projection operator for block sparsity and tree sparsity. Cevher et al. [2009] investigated cluster sparsity and they applied dynamic programming to solve the projection operator for their sparsity model. Hegde et al. [2009] introduced a \u201cspike trains\u201d signal model, which is also related to exclusive group sparsity. Its groups always have consecutive coordinates, and each group cannot contain more than one nonzero element. To solve the projection problem of their model, they showed the basic feasible solutions of the relaxed linear programming (LP) are always integer points. In our work, we also use LP to solve the projection problem. But our model defines the group structure differently and aims at different applications. In addition, there are some works for the cases without an efficient exact projection operator [Hegde et al., 2015a,b, Nguyen et al., 2014]. This is meaningful since the projection operator for complex structured sparsity often involves solving complicated combinatorial optimization problems. Hegde et al. [2015a] discussed how to guarantee convergence if using approximate projection in IHT and CoSaMP for compressive sensing. They proved that the convergence needs a \u201chead approximation\u201d to project the update (gradient) before applying it. Hegde et al. [2015b] proposed a general framework to formulate a series of models as a weighted graph, and designed an efficient approximate projection operator for the models. Nguyen et al. [2014] applied the approximate projection-based IHT and CoSaMP to general convex functions and stochastic gradients.", "startOffset": 0, "endOffset": 2231}, {"referenceID": 19, "context": ", 2012, 2014] (the general version of CoSaMP [Needell and Tropp, 2009]) for solving cardinality constrained problem.", "startOffset": 45, "endOffset": 70}, {"referenceID": 28, "context": "Other methods like hard thresholding pursuit (HTP) also follows similar steps and has been shown to be effective both empirically and theoretically [Yuan et al., 2016].", "startOffset": 148, "endOffset": 167}, {"referenceID": 21, "context": "To avoid the expensive computation of the gradient, GradMP and IHT can be extended to the stochastic versions [Nguyen et al., 2014] by assigning g the stochastic gradient at the gradient computation step.", "startOffset": 110, "endOffset": 131}, {"referenceID": 21, "context": "Both Algorithms 1 and 2 (and their stochastic variants) guarantee some nice properties: the iterate converges to a small ball surrounding the true solution at a linear rate under certain RIPtype conditions [Nguyen et al., 2014] and the radius of such ball converges to zero when the number of samples goes to infinity.", "startOffset": 206, "endOffset": 227}, {"referenceID": 25, "context": "If all the groups except [p] in G do not overlap each other, the projection problem can be easily solved by sequential projections [Yang et al., 2016].", "startOffset": 131, "endOffset": 150}, {"referenceID": 23, "context": "The proof basically shows that matrix A (for TVCS) is a totally unimodular matrix [Papadimitriou and Steiglitz, 1982].", "startOffset": 82, "endOffset": 117}, {"referenceID": 23, "context": "Simplex might be efficient practically, but its worst case may lead to exponential time complexity [Papadimitriou and Steiglitz, 1982].", "startOffset": 99, "endOffset": 134}, {"referenceID": 11, "context": "Theorem 3 mainly applies Hoffman\u2019s Theorem [Hoffman, 2003] to show that f is an optimal strongly convex function [Liu and Wright, 2015].", "startOffset": 43, "endOffset": 58}, {"referenceID": 15, "context": "Theorem 3 mainly applies Hoffman\u2019s Theorem [Hoffman, 2003] to show that f is an optimal strongly convex function [Liu and Wright, 2015].", "startOffset": 113, "endOffset": 135}, {"referenceID": 11, "context": "The convergence rate \u03b1 = 1/(1+ \u03bb L ), where \u03bb is the Hoffman constant [Hoffman, 2003] that depends on A,B and is always positive.", "startOffset": 70, "endOffset": 85}, {"referenceID": 25, "context": "We compare the proposed methods to bilevel exclusive sparsity with non-overlapped groups (row-wise or column-wise) [Yang et al., 2016], overall sparsity [Needell and Tropp, 2009], and exclusive LASSO [Kong et al.", "startOffset": 115, "endOffset": 134}, {"referenceID": 19, "context": ", 2016], overall sparsity [Needell and Tropp, 2009], and exclusive LASSO [Kong et al.", "startOffset": 26, "endOffset": 51}, {"referenceID": 14, "context": ", 2016], overall sparsity [Needell and Tropp, 2009], and exclusive LASSO [Kong et al., 2014].", "startOffset": 73, "endOffset": 92}, {"referenceID": 10, "context": "Q \u2208 [0, 1]n\u00d7m is the corresponding quality matrix, which is usually estimated from the golden standard test [Ho et al., 2013].", "startOffset": 108, "endOffset": 125}, {"referenceID": 21, "context": "To avoid evaluating the expectation term, we apply the stochastic iterative hard thresholding framework [Nguyen et al., 2014].", "startOffset": 104, "endOffset": 125}, {"referenceID": 10, "context": "Besides the proposed formulation (8), we evaluate the random assignment algorithm and the Q-based linear programming [Ho et al., 2013].", "startOffset": 117, "endOffset": 134}, {"referenceID": 17, "context": "method with six representative algorithms, including PCC, ARACNE [Margolin et al., 2006], CLR [Faith et al.", "startOffset": 65, "endOffset": 88}, {"referenceID": 5, "context": ", 2006], CLR [Faith et al., 2007], MINET [Meyer et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 18, "context": ", 2007], MINET [Meyer et al., 2008], GENIE3 [Huynh-Thu et al.", "startOffset": 15, "endOffset": 35}, {"referenceID": 12, "context": ", 2008], GENIE3 [Huynh-Thu et al., 2010], TIGRESS [Haury et al.", "startOffset": 16, "endOffset": 40}, {"referenceID": 6, "context": ", 2010], TIGRESS [Haury et al., 2012].", "startOffset": 17, "endOffset": 37}], "year": 2017, "abstractText": "The cardinality constraint is an intrinsic way to restrict the solution structure in many domains, for example, sparse learning, feature selection, and compressed sensing. To solve a cardinality constrained problem, the key challenge is to solve the projection onto the cardinality constraint set, which is NP-hard in general when there exist multiple overlapped cardinality constraints. In this paper, we consider the scenario where the overlapped cardinality constraints satisfy a Three-view Cardinality Structure (TVCS), which reflects the natural restriction in many applications, such as identification of gene regulatory networks and task-worker assignment problem. We cast the projection into a linear programming, and show that for TVCS, the vertex solution of this linear programming is the solution for the original projection problem. We further prove that such solution can be found with the complexity proportional to the number of variables and constraints. We finally use synthetic experiments and two interesting applications in bioinformatics and crowdsourcing to validate the proposed TVCS model and method.", "creator": "LaTeX with hyperref package"}}}