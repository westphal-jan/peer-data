{"id": "1703.07469", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "RobustFill: Neural Program Learning under Noisy I/O", "abstract": "The problem of automatically generating a computer program from some specification has been studied since the early days of AI. Recently, two competing approaches for automatic program learning have received significant attention: (1) neural program synthesis, where a neural network is conditioned on input/output (I/O) examples and learns to generate a program, and (2) neural program induction, where a neural network generates new outputs directly using a latent program representation.", "histories": [["v1", "Tue, 21 Mar 2017 23:29:47 GMT  (1571kb,D)", "http://arxiv.org/abs/1703.07469v1", "8 pages + 9 pages of supplementary material"]], "COMMENTS": "8 pages + 9 pages of supplementary material", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jacob devlin", "jonathan uesato", "surya bhupatiraju", "rishabh singh", "abdel-rahman mohamed", "pushmeet kohli"], "accepted": true, "id": "1703.07469"}, "pdf": {"name": "1703.07469.pdf", "metadata": {"source": "META", "title": "RobustFill: Neural Program Learning under Noisy I/O", "authors": ["Jacob Devlin", "Jonathan Uesato", "Surya Bhupatiraju", "Rishabh Singh", "Abdel-rahman Mohamed", "Pushmeet Kohli"], "emails": ["<jdevlin@microsoft.com>."], "sections": [{"heading": null, "text": "Our neural models use a modified attention RNN to encode variably large sets of I / O pairs. Our best synthesis model achieves 92% accuracy on a real test set, compared to 34% accuracy on the best neural synthesis approach to date. The synthesis model also outperforms a comparable induction model in this task, but we mainly show that the strength of each approach depends to a large extent on the assessment metric and the application by the user. Finally, we show that we can train our neural models to be very robust to the kind of noise expected in real data (e.g. typing errors), while an advanced rules-based system fails completely."}, {"heading": "1. Introduction", "text": "In fact, most of them will be able to move to another world, in which they will be able to integrate, and in which they will be able to change the world."}, {"heading": "2. Related Work", "text": "There is a wealth of recent work on neural program induction and synthesis. Neural Program Induction: Neural Turing Machine (NTM) (Graves et al., 2014) uses a neural controller to read and write an external tape with soft attention and is able to learn simple algorithmic tasks such as copying and sorting. Stack RNs (Joulin & Mikolov, 2015) presents a turn-complete model similar to a neural controller with an external memory and is able to learn algorithmic patterns. Neural GPU (Kaiser & Sutskever, 2015) presents a turn-complete model that designs NTM and a flat design similar to an external one."}, {"heading": "3. Problem Overview", "text": "We now formally define the problem definition and the domain-specific language of string transformations."}, {"heading": "3.1. Problem Formulation", "text": "rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "3.2. The Domain Specific Language", "text": "The Domain Specific Language (DSL) used here to represent P models a rich set of string transformations based on substring extractions, string conversions, and constant strings.The DSL is similar to the DSL described in Figure 2, but is enhanced by nested expressions, arbitrary constant strings, and a powerful regex-based substring extraction function.The syntax of the DSL is represented in Figure 2, and formal semantics are represented in the supplementary material.A program P: string \u21d2 string in the DSL takes a string as input and returns another string as output, and the uppermost operator in the DSL is the Concat operator, which concatenates a finite list of string expressions. A string expression e can be either a substring expression f, a nested expression n, or a constant string expression."}, {"heading": "3.3. Training Data and Test Sets", "text": "Since there are only a few hundred real FlashFill instances, the data used to form the neural networks were automatically synthesized using a random sampling and generation strategy. First, random programs from our DSL, up to a maximum length (10 expressions). In a sampled program, we calculate a simple set of heuristic requirements on InStr so that the program can be executed without exception. For example, if a printout in the program retrieves the fourth number, the InStr must have at least 4 numbers. Then, each InStr is generated as a random sequence of ASCII characters that must meet the requirements. The corresponding OutStr is synthesized by running the program on InStr. To evaluate the trained models, we use FlashFillTest, a set of 205 real examples collected from Microsoft Excel spreadsheets and made available to us by the authors of Gulwani al. (2012 and parisotal test), the individual examples for the first FlashFillum 2017 will be used."}, {"heading": "4. Program Synthesis Model Architecture", "text": "We model program synthesis as a sequence-to-sequence generation task, similar to machine translation in the past (Bahdanau et al., 2014), image captioning (Xu et al., 2015), and program induction (Zaremba & Sutskever, 2014). In the most general description, we encode observed I / Os using a series of recursive neural networks (RNN), generating P with a different RNN symbol each. The central challenge here is that in typical sequence modeling, input into the model is a single sequence. In this case, input is a variably long, disordered series of sequence pairs in which each pair (i.e. an I / O example) has an internal conditional dependency. We describe and evaluate several multilateral variants of the observant RNN architecture (Bahdanau et al., 2014) to model this scenario."}, {"heading": "4.1. Single-Example Representation", "text": "We will first consider a model that uses only a single observed example (I, O) as input and produces a program P as output. Note that this model is not bound to the valuation input Iy. In all models described here, P is generated with a sequential RNN rather than a hierarchical RNN (Parisotto et al., 2017; Tai et al., 2015).3 As shown in Vinyals et al. (2015), sequential RNNs can be surprisingly strong in representing hierarchical structures. We will examine four increasingly complex model architectures that are visually demonstrated in Figure 3: \u2022 Basic Seq-to-Seq: Each sequence is encoded with a non-attentive LSTM, and the final hidden state is used as the initial hidden state of the next LSTM. \u2022 Attention-A: O and P are attentive LSTMs, without O 2In cases where less archaic examples than the 4 are used to justify only the general examples 6."}, {"heading": "4.2. Double Attention", "text": "A typical attention layer takes the following form: si = attention (hi \u2212 1, xi, S) hi = LSTM (hi \u2212 1, xi, si) Where S is the set of vectors to be looked after, hi \u2212 1 is the previous recurring state and xi is the current input. The Attention () function takes the form of the \"general\" model by Luong et al. (2015). Double attention takes the form: sAi = attention (hi \u2212 1, xi, S A) sBi = attention (hi \u2212 1, xi, s A i, S B) hi = LSTM (hi \u2212 1, xi, s A i, s B i) Note that sAi is associated with hi \u2212 1 when calculating attention to SB, so there is a directed dependence between the two attention layers."}, {"heading": "4.3. Multi-Example Pooling", "text": "The previous section describes only one architecture for the encoding of a single I / O example, but in general we assume that the input consists of several I / O examples; the number of I / O examples can vary between the test instances, and the examples are disordered, suggesting a pooling-based approach. Previous work (Parisotto et al., 2017) focused on the final hidden states of the encoder, but this 4A variant, where O and I are swapped, works much worse; the approach cannot be used for attention-based models. Instead, we choose an approach that we call late pooling. Here, each I / O example has its own layers for I, O and P (with shared weights between examples), but the hidden states of P1,..., Pn are pooled at each time step before we are fed into a single output-soft-max layer."}, {"heading": "4.4. Hyperparameters and Training", "text": "All models were trained for 2 million mini-batch updates, with each mini-batch containing 128 training instances (i.e. 128 programs with four I / O examples each), each mini-batch being re-sampled so that the model saw 256 million random programs and 1024 million random I / O examples during the training, training took approximately 24 hours from 2 Titan X GPUs using an in-house toolkit, and a small amount of hyperparameter tuning was performed on a synthetic validation set that was generated like the training."}, {"heading": "5. Program Synthesis Results", "text": "Once the training is complete, the synthesis models can be decrypted with a beam search decoder (Sutskever et al., 2014). Unlike a typical sequence generation task, where the model is decrypted with a beam k and then only the 1-best output is taken, here all k-best candidates are executed one-to-one to determine consistency. If several program candidates match all the observed examples, the program with the highest model score is called Output.5 This program is called P \u0445 during the search. In addition to the standard beam search, we also propose a variant called the \"DP beam,\" which adds a search effort similar to the dynamic programming algorithm mentioned in Section 3.3. Here, an expression is completed each time during the search, the subprogram is executed in a black box manner. If any resulting partial OutStr variant is not a string prefix selection of the beam observed, the subprogram is removed from the OutStr."}, {"heading": "5.1. Comparison to Past Work", "text": "Prior to this work, the strongest statistical model for solving FlashFillTest was Parisotto et al. (2017). Generalization accuracy is shown below: System Beam 100 1000 Parisotto et al. (2017) 23% 34% Basic Seq-to-Seq 51% 56% Attention-C 83% 86% Attention-C-DP 89% 92% We believe that this improvement in accuracy is due to several reasons. First, late pooling enables us to effectively integrate effective attention mechanisms into our model. As the architecture in Parisotto et al. (2017) performed pooling at the I / O coding level, it was unable to exploit the attention mechanisms that we show are critical to achieving high accuracy. Second, the DSL used here is more meaningful, especially the GetSpan () function that was required to solve about 20% of the test cases."}, {"heading": "5.2. Consistency vs. Generalization Results", "text": "Interestingly, the consistency is relatively constant as the number of observed examples increases. There was no expectation from the outset whether the consistency would increase or decrease, as more examples with fewer overall programs are consistent, but also give the network a stronger input signal. Finally, we can see that decoding Beam = 1 produces consistent output only about 50% of the time, implying that the latent functional semantics learned from the model are far from perfect."}, {"heading": "6. Program Induction Results", "text": "An alternative approach to solving the FlashFill problem is program induction, where the output string is generated directly by the neural network without DSL. Specifically, we can train a neural network that generates a set of n observed examples (I1, O1),... (In, On) as input, as well as an unpaired InStr, Iy, and the corresponding OutStr, Oy. Both approaches have the same end goal - determine the Oy that corresponds to Iy - but have several important conceptual differences. The first major difference is that the induction model does not use program P, but only model P. Both approaches have the same end goal - determine the Oy that corresponds to Iy - but have several important conceptual differences. The first major difference is that the induction model does not use program P anywhere."}, {"heading": "6.1. Comparison of Induction and Synthesis Models", "text": "Despite these differences, it is possible to model both approaches using almost identical network architectures as the previous sections - six examples must be compared exactly with the synthesis. The induction model evaluated here is identical to the synthesis of Attention-A with late pooling, with the exception of the following two modifications: 1. Instead of generating P, the system generates the new OutStr Oy character by character. 2. There is an additional LSTM for encoding Iy. The decoder layer Oy uses double attention to Oj and Iy. The induction network diagram is specified in the supplementary material. Each pair (Iy, Oy) is independent, but conditioned by all observed examples. Attention, pooling, hidden sizes, training details and decoders are otherwise identical to the synthesis. The induction model was trained on the same synthetic data as the synthesis model. The results are shown in Figure 6. The induction model is compared with the previous synthesis-A, where the accuracy is compared with the previous sections."}, {"heading": "6.2. Average-Example Accuracy", "text": "In all previous sections, a strict definition of \"generalization accuracy\" was used, according to which all six evaluation examples must be exactly correct. We are talking about an all-out-of-the-box accuracy, but another useful measurement is the measurement of the total percentage of correct evaluation examples averaged across all instances. 8 This measurement generalizes 5-out-of-6 evaluation examples to get more credit than 0. We8For example, if a user wants to automatically fill an entire column into a spreadsheet, he can prioritize the all-out accuracy - if the system proposes a solution, he can be sure that it is correct for all rows. If the application instead offers automatic completion suggestions based on an average model, then a more accurate model might be preferred."}, {"heading": "7. Handling Noisy I/O Examples", "text": "In the FlashFill task, the real I / O examples are typically compiled manually by the user, so noise (e.g. typos) is expected and should be handled well. An example is given in Figure 1.Since neural networking methods (1) are inherently more likely and (2) operate in an approximate spatial representation, it is reasonable to believe that they can learn to be robust against this type of noise. To explicitly take noise into account, we have made only two small modifications: First, noise was synthetically injected into the training data using random character transformations; 9 Second, the best program P * was selected by using the character processing rate (CER) (Marzal & Vidal, 1993) among the observed examples, rather9This did not degrade the results on the noise-free test environment."}, {"heading": "8. Conclusions", "text": "We have presented a novel variant of an attentive RNN architecture for program synthesis that achieves 92% accuracy for a real Programming By Example task, which is equivalent to the performance of a handmade system and outperforms the previously best neural synthesis model by 58%. Furthermore, we have shown that our model remains robust against moderate noise in the I / O examples, while the handmade system fails even at low levels of noise. Furthermore, we have carefully contrasted our 10 standard beam, which is also used in place of DP beam. 11FlashFill was developed manually on this basis. The neural program synthesis system with a neural program induction system showed that although the synthesis system performs better in this task, both approaches have their own strength under certain evaluation conditions. In particular, synthesis systems have an advantage in assessing whether all outputs are correct, while induction systems perform better in assessing which system shows the outputs correctly."}, {"heading": "A. DSL Extended Description", "text": "The formal semantics of this language are defined in Figure 9 below. The program takes a string v as input and generates a string as output (result of the Concat operator). As an implementing detail, we note that after scanning a grammar program, we place calls to nesting functions (as defined in Figure 2 of the paper) in a single token. For example, the GetToken (t, i) function would symbolize a single token, not 3 separate tokens. This is possible because for nesting functions the size of the entire parameter space is small. For all other functions, the parameter space is too large to flatten function calls without dramatically increasing the vocabulary, so we treat parameters as separate tokens."}, {"heading": "B. Synthetic Evaluation Details", "text": "The results of synthetically generated examples are largely omitted from the work, since the synthetic dataset in a vacuum can be interpreted as easily or with difficulty by different generating methods, making summary statistics difficult to interpret. Instead, we report results from an external real dataset to verify that the model has learned functional semantics that are at least as meaningful as programs observed in real data. Nevertheless, we include additional details about our experiments with synthetically generated programs for readers interested in the details of our approach. As described in the paper, programs were randomly generated from the DSL by first determining a program length up to a maximum of 10 expressions and then independently sampling each expression. We used a simple set of heuristics to limit potential inputs to strings that do not generate empty outputs (e.g., any program that references the third occurrence of a number leads to samples containing at least three)."}, {"heading": "C. Examples of Synthesized Programs", "text": "Figure 13 shows several randomly sampled (anonymized) examples from the FlashFill test set, along with their predicted programs issued by the synthesis model. Figure 14 shows several examples that were hand-selected to demonstrate interesting limitations of the model. In the case of the first example, the task is to reformat international phone numbers. In this case, given the input-output examples observed, the task is underdone, as there are many different programs that match the observed examples. Note that in order to extract the first two digits, there are many other possible functions that would generate the correct output in the observed examples, some of which would generalize and others would not: for example, capturing the second and third digits, obtaining the first two digits, or obtaining the first two digits. In this case, the predicted program extracts the country code by taking the first two digits, a strategy that does not generalize the country code on different examples."}], "references": [{"title": "Syntax-guided synthesis", "author": ["Alur", "Rajeev", "Bodik", "Rastislav", "Juniwal", "Garvit", "Martin", "Milo MK", "Raghothaman", "Mukund", "Seshia", "Sanjit A", "Singh", "Rishabh", "Solar-Lezama", "Armando", "Torlak", "Emina", "Udupa", "Abhishek"], "venue": null, "citeRegEx": "Alur et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Alur et al\\.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Deepcoder: Learning to write programs", "author": ["Balog", "Matej", "Gaunt", "Alexander L", "Brockschmidt", "Marc", "Nowozin", "Sebastian", "Tarlow", "Daniel"], "venue": "arXiv preprint arXiv:1611.01989,", "citeRegEx": "Balog et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Balog et al\\.", "year": 2016}, {"title": "Terpret: A probabilistic programming language for program induction", "author": ["Gaunt", "Alexander L", "Brockschmidt", "Marc", "Singh", "Rishabh", "Kushman", "Nate", "Kohli", "Pushmeet", "Taylor", "Jonathan", "Tarlow", "Daniel"], "venue": null, "citeRegEx": "Gaunt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gaunt et al\\.", "year": 2016}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Hybrid computing using a neural network with dynamic external memory", "author": ["Graves", "Alex", "Wayne", "M Greg", "T Reynolds", "I Harley", "A Danihelka", "SG Grabska-Barwiska", "E Colmenarejo", "T Grefenstette", "J Ramalho", "Agapiou", "AP", "Badia"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2016}, {"title": "Automating string processing in spreadsheets using input-output examples", "author": ["Gulwani", "Sumit"], "venue": "In ACM SIGPLAN Notices. ACM,", "citeRegEx": "Gulwani and Sumit.,? \\Q2011\\E", "shortCiteRegEx": "Gulwani and Sumit.", "year": 2011}, {"title": "Spreadsheet data manipulation using examples", "author": ["Gulwani", "Sumit", "Harris", "William R", "Singh", "Rishabh"], "venue": "Communications of the ACM,", "citeRegEx": "Gulwani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gulwani et al\\.", "year": 2012}, {"title": "Attention-based multimodal neural machine translation", "author": ["Huang", "Po-Yao", "Liu", "Frederick", "Shiang", "Sz-Rung", "Oh", "Jean", "Dyer", "Chris"], "venue": "In Proceedings of the First Conference on Machine Translation, Berlin,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "In NIPS, pp", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong", "Minh-Thang", "Pham", "Hieu", "Manning", "Christopher D"], "venue": "arXiv preprint arXiv:1508.04025,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Knowledge and reasoning in program synthesis", "author": ["Manna", "Zohar", "Waldinger", "Richard"], "venue": "Artificial intelligence,", "citeRegEx": "Manna et al\\.,? \\Q1975\\E", "shortCiteRegEx": "Manna et al\\.", "year": 1975}, {"title": "A deductive approach to program synthesis", "author": ["Manna", "Zohar", "Waldinger", "Richard"], "venue": "ACM Transactions on Programming Languages and Systems (TOPLAS),", "citeRegEx": "Manna et al\\.,? \\Q1980\\E", "shortCiteRegEx": "Manna et al\\.", "year": 1980}, {"title": "Computation of normalized edit distance and applications", "author": ["Marzal", "Andres", "Vidal", "Enrique"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Marzal et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marzal et al\\.", "year": 1993}, {"title": "A machine learning framework for programming by example", "author": ["Menon", "Aditya Krishna", "Tamuz", "Omer", "Gulwani", "Sumit", "Lampson", "Butler W", "Kalai", "Adam"], "venue": "In ICML, pp", "citeRegEx": "Menon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Menon et al\\.", "year": 2013}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Neelakantan", "Arvind", "Le", "Quov V", "Sutskever", "Ilya"], "venue": null, "citeRegEx": "Neelakantan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "Neuro-symbolic program synthesis", "author": ["Parisotto", "Emilio", "Mohamed", "Abdel-rahman", "Singh", "Rishabh", "Li", "Lihong", "Zhou", "Dengyong", "Kohli", "Pushmeet"], "venue": null, "citeRegEx": "Parisotto et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Parisotto et al\\.", "year": 2017}, {"title": "Flashmeta: a framework for inductive program synthesis", "author": ["Polozov", "Oleksandr", "Gulwani", "Sumit"], "venue": "In OOPSLA,", "citeRegEx": "Polozov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Polozov et al\\.", "year": 2015}, {"title": "Programming with a differentiable forth", "author": ["Riedel", "Sebastian", "Bosnjak", "Matko", "Rockt\u00e4schel", "Tim"], "venue": "interpreter. CoRR,", "citeRegEx": "Riedel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from treestructured long short-term memory networks", "author": ["Tai", "Kai Sheng", "Socher", "Richard", "Manning", "Christopher D"], "venue": "arXiv preprint arXiv:1503.00075,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Vinyals", "Oriol", "Kaiser", "\u0141ukasz", "Koo", "Terry", "Petrov", "Slav", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "In NIPS,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "A step toward automatic program writing", "author": ["Waldinger", "Richard J", "Lee", "Richard C.T. Prow"], "venue": "In IJCAI,", "citeRegEx": "Waldinger et al\\.,? \\Q1969\\E", "shortCiteRegEx": "Waldinger et al\\.", "year": 1969}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron C", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Bengio", "Yoshua"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Learning to execute", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "Modern rule-based synthesis methods are typically centered around hand-crafted function semantics and pruning rules to search for programs consistent with the I/O examples (Gulwani et al., 2012; Alur et al., 2013).", "startOffset": 172, "endOffset": 213}, {"referenceID": 0, "context": "Modern rule-based synthesis methods are typically centered around hand-crafted function semantics and pruning rules to search for programs consistent with the I/O examples (Gulwani et al., 2012; Alur et al., 2013).", "startOffset": 172, "endOffset": 213}, {"referenceID": 2, "context": "This work has fallen into two overarching categories: (1) neural program synthesis, where the program is generated by a neural network conditioned on the I/O examples (Balog et al., 2016; Parisotto et al., 2017; Gaunt et al., 2016; Riedel et al., 2016), and (2) neural program induction, where network learns to generate the output directly using a latent program representation (Graves et al.", "startOffset": 167, "endOffset": 252}, {"referenceID": 16, "context": "This work has fallen into two overarching categories: (1) neural program synthesis, where the program is generated by a neural network conditioned on the I/O examples (Balog et al., 2016; Parisotto et al., 2017; Gaunt et al., 2016; Riedel et al., 2016), and (2) neural program induction, where network learns to generate the output directly using a latent program representation (Graves et al.", "startOffset": 167, "endOffset": 252}, {"referenceID": 3, "context": "This work has fallen into two overarching categories: (1) neural program synthesis, where the program is generated by a neural network conditioned on the I/O examples (Balog et al., 2016; Parisotto et al., 2017; Gaunt et al., 2016; Riedel et al., 2016), and (2) neural program induction, where network learns to generate the output directly using a latent program representation (Graves et al.", "startOffset": 167, "endOffset": 252}, {"referenceID": 18, "context": "This work has fallen into two overarching categories: (1) neural program synthesis, where the program is generated by a neural network conditioned on the I/O examples (Balog et al., 2016; Parisotto et al., 2017; Gaunt et al., 2016; Riedel et al., 2016), and (2) neural program induction, where network learns to generate the output directly using a latent program representation (Graves et al.", "startOffset": 167, "endOffset": 252}, {"referenceID": 4, "context": ", 2016), and (2) neural program induction, where network learns to generate the output directly using a latent program representation (Graves et al., 2014; 2016; Kurach et al., 2016; Kaiser & Sutskever, 2015; Joulin & Mikolov, 2015; Reed & de Freitas, 2016; Neelakantan et al., 2016).", "startOffset": 134, "endOffset": 283}, {"referenceID": 15, "context": ", 2016), and (2) neural program induction, where network learns to generate the output directly using a latent program representation (Graves et al., 2014; 2016; Kurach et al., 2016; Kaiser & Sutskever, 2015; Joulin & Mikolov, 2015; Reed & de Freitas, 2016; Neelakantan et al., 2016).", "startOffset": 134, "endOffset": 283}, {"referenceID": 7, "context": "The primary task evaluated for this work is a Programming By Example (PBE) system for string transformations similar to FlashFill (Gulwani et al., 2012; Gulwani, 2011).", "startOffset": 130, "endOffset": 167}, {"referenceID": 1, "context": "For modeling, we develop novel variants of the attentional RNN architecture (Bahdanau et al., 2014) to encode a variable-length unordered set of input-output examples.", "startOffset": 76, "endOffset": 99}, {"referenceID": 1, "context": "For modeling, we develop novel variants of the attentional RNN architecture (Bahdanau et al., 2014) to encode a variable-length unordered set of input-output examples. For program representation, we have developed a domain-specific language (DSL), similar to that of Gulwani et al. (2012), that defines an expressive class of regular expression-based string transformations.", "startOffset": 77, "endOffset": 289}, {"referenceID": 7, "context": "We compare our neural induction model, neural synthesis model, and the rule-based architecture of Gulwani et al. (2012) on a real-world FlashFill test set.", "startOffset": 98, "endOffset": 120}, {"referenceID": 4, "context": "Neural Program Induction: Neural Turing Machine (NTM) (Graves et al., 2014) uses a neural controller to read and write to an external memory tape using soft attention and is able to learn simple algorithmic tasks such as array copying and sorting.", "startOffset": 54, "endOffset": 75}, {"referenceID": 16, "context": "Neural Program Synthesis: The most closely related work to ours uses a Recursive-Reverse-Recursive neural network (R3NN) to learn string transformation programs from examples (Parisotto et al., 2017), and is directly compared in Section 5.", "startOffset": 175, "endOffset": 199}, {"referenceID": 2, "context": "DeepCoder (Balog et al., 2016) trains a neural network to predict a distribution over possible functions useful for a given task from input-output examples, which is used to augment an external search algorithm.", "startOffset": 10, "endOffset": 30}, {"referenceID": 3, "context": "Terpret (Gaunt et al., 2016) and Neural Forth (Riedel et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 18, "context": ", 2016) and Neural Forth (Riedel et al., 2016) allow programmers to write sketches of partial programs to express prior procedural knowledge, which are then completed by training neural networks on examples.", "startOffset": 25, "endOffset": 46}, {"referenceID": 7, "context": "DSL-based synthesis: Non-statistical DSL-based synthesis approaches (Gulwani et al., 2012) exploit independence properties of DSL operators to develop a divide-and-", "startOffset": 68, "endOffset": 90}, {"referenceID": 14, "context": "There is also some work on using learnt clues to guide the search in DSL expansions (Menon et al., 2013), but this requires handcoded textual features of examples.", "startOffset": 84, "endOffset": 104}, {"referenceID": 16, "context": "The DSL is similar to the DSL described in Parisotto et al. (2017), but is extended to include nested expressions, arbitrary constant strings, and a powerful regexbased substring extraction function.", "startOffset": 43, "endOffset": 67}, {"referenceID": 7, "context": "For evaluating the trained models, we use FlashFillTest, a set of 205 real-world examples collected from Microsoft Excel spreadsheets, and provided to us by the authors of Gulwani et al. (2012) and Parisotto et al.", "startOffset": 172, "endOffset": 194}, {"referenceID": 7, "context": "For evaluating the trained models, we use FlashFillTest, a set of 205 real-world examples collected from Microsoft Excel spreadsheets, and provided to us by the authors of Gulwani et al. (2012) and Parisotto et al. (2017). Each FlashFillTest instance has ten I/O examples, of which the first four are used as observed examples and the remaining six are used as assessment examples.", "startOffset": 172, "endOffset": 222}, {"referenceID": 1, "context": "We model program synthesis as a sequence-to-sequence generation task, along the lines of past work in machine translation (Bahdanau et al., 2014), image captioning (Xu et al.", "startOffset": 122, "endOffset": 145}, {"referenceID": 23, "context": ", 2014), image captioning (Xu et al., 2015), and program induction (Zaremba & Sutskever, 2014).", "startOffset": 26, "endOffset": 43}, {"referenceID": 1, "context": "We describe and evaluate several multi-attentional variants of the attentional RNN architecture (Bahdanau et al., 2014) to model this scenario.", "startOffset": 96, "endOffset": 119}, {"referenceID": 16, "context": "In all models described here, P is generated using a sequential RNN, rather than a hierarchical RNN (Parisotto et al., 2017; Tai et al., 2015).", "startOffset": 100, "endOffset": 142}, {"referenceID": 20, "context": "In all models described here, P is generated using a sequential RNN, rather than a hierarchical RNN (Parisotto et al., 2017; Tai et al., 2015).", "startOffset": 100, "endOffset": 142}, {"referenceID": 16, "context": "In all models described here, P is generated using a sequential RNN, rather than a hierarchical RNN (Parisotto et al., 2017; Tai et al., 2015).3 As demonstrated in Vinyals et al. (2015), sequential RNNs can be surprisingly strong at representing hierarchical structures.", "startOffset": 101, "endOffset": 186}, {"referenceID": 8, "context": "Double attention is a straightforward extension to the standard attentional architecture, similar to the multimodal attention described in Huang et al. (2016). A typical attentional layer takes the following form:", "startOffset": 139, "endOffset": 159}, {"referenceID": 10, "context": "The Attention() function takes the form of the \u201cgeneral\u201d model from Luong et al. (2015). Double attention takes the form:", "startOffset": 68, "endOffset": 88}, {"referenceID": 16, "context": "Previous work (Parisotto et al., 2017) has pooled on the final encoder hidden states, but this", "startOffset": 14, "endOffset": 38}, {"referenceID": 19, "context": "Once training is complete, the synthesis models can be decoded with a beam search decoder (Sutskever et al., 2014).", "startOffset": 90, "endOffset": 114}, {"referenceID": 16, "context": "Prior to this work, the strongest statistical model for solving FlashFillTest was Parisotto et al. (2017). The generalization accuracy is shown below:", "startOffset": 82, "endOffset": 106}, {"referenceID": 16, "context": "System Beam 100 1000 Parisotto et al. (2017) 23% 34% Basic Seq-to-Seq 51% 56% Attention-C 83% 86% Attention-C-DP 89% 92%", "startOffset": 21, "endOffset": 45}, {"referenceID": 16, "context": "Because the architecture in Parisotto et al. (2017) performed pooling at the I/O encoding level, it could not exploit the attention mechanisms which we show our critical to achieving high accuracy.", "startOffset": 28, "endOffset": 52}, {"referenceID": 7, "context": "We compare the models in this paper to the actual FlashFill implementation found in Microsoft Excel, as described in Gulwani et al. (2012). An overview of this model is described in Section 2.", "startOffset": 117, "endOffset": 139}, {"referenceID": 7, "context": "\u201d This result is expected, since the efficiency of their algorithm is critically centered around exact string matching (Gulwani et al., 2012).", "startOffset": 119, "endOffset": 141}], "year": 2017, "abstractText": "The problem of automatically generating a computer program from some specification has been studied since the early days of AI. Recently, two competing approaches for automatic program learning have received significant attention: (1) neural program synthesis, where a neural network is conditioned on input/output (I/O) examples and learns to generate a program, and (2) neural program induction, where a neural network generates new outputs directly using a latent program representation. Here, for the first time, we directly compare both approaches on a large-scale, real-world learning task. We additionally contrast to rule-based program synthesis, which uses hand-crafted semantics to guide the program generation. Our neural models use a modified attention RNN to allow encoding of variable-sized sets of I/O pairs. Our best synthesis model achieves 92% accuracy on a real-world test set, compared to the 34% accuracy of the previous best neural synthesis approach. The synthesis model also outperforms a comparable induction model on this task, but we more importantly demonstrate that the strength of each approach is highly dependent on the evaluation metric and end-user application. Finally, we show that we can train our neural models to remain very robust to the type of noise expected in real-world data (e.g., typos), while a highlyengineered rule-based system fails entirely.", "creator": "LaTeX with hyperref package"}}}