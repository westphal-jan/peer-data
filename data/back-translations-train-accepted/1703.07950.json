{"id": "1703.07950", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2017", "title": "Failures of Gradient-Based Deep Learning", "abstract": "In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four families of problems for which some of the commonly used existing algorithms fail or suffer significant difficulty. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied.", "histories": [["v1", "Thu, 23 Mar 2017 07:16:37 GMT  (316kb,D)", "http://arxiv.org/abs/1703.07950v1", null], ["v2", "Wed, 26 Apr 2017 05:23:26 GMT  (406kb,D)", "http://arxiv.org/abs/1703.07950v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["shai shalev-shwartz", "ohad shamir", "shaked shammah"], "accepted": true, "id": "1703.07950"}, "pdf": {"name": "1703.07950.pdf", "metadata": {"source": "CRF", "title": "Failures of Deep Learning", "authors": ["Shai Shalev-Shwartz", "Ohad Shamir"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "2 Failure due to Non-Informative Gradients", "text": "Most existing deep learning algorithms are gradient-based methods, namely algorithms that optimize a target by accessing its gradient w.r.t. or any weight vector w or estimates of the gradient. We consider a setting in which the goal of this optimization process is to learn some underlying hypothesis Class H, one of which, h-H, is responsible for the labeling of the data. This results in an optimization problem of the shape w Fh (w).The underlying assumption is that the gradient of the object w.r.t. w, Fh (w), contains useful information about the target function h and will help us make advance.Below we will discuss a family of problems for which the gradient, with high probability, is essentially the same at any fixed point, and regardless of the underlying target function h (w)."}, {"heading": "2.1 Experiment", "text": "We start with the simple problem of learning random parities: After selecting some v * {0, 1} d evenly at random, our goal is to learn a predictor mapping x * {0, 1} d to y = (\u2212 1) < x, v * > where x is evenly distributed. In words, y indicates whether the number of ones in a certain subset of x (indicated by v *) is odd or eve.1See http: / / joelgrus.com / 2016 / 05 / 23 / fizz-buzz-in-sorflow / for the inspiration behind this humorous quote. For our experiments, we use the hinge loss and a simple network architecture of a fully connected layer width 10d > 3d2 with ReLU activations and a fully connected output layer with linear activation and a single unit. Note that this class realizes the parity function corresponding to each v \u00b2 function (see Annex 3)."}, {"heading": "2.2 Analysis", "text": "To formally explain the failure from a geometric perspective, let us consider the stochastic optimization problem associated with learning a target function. (< min w Fh (w): = 1 x (pw (x), h (x))], (1), where \"there is a loss function, x), are the stochastic input factors (assuming that vectors will be in euclidean space), and pw is any predictor parameterized by a parameter vector. We assume that F is differentiable. A key variable we will be interested in is the variance of the gradient of F in relation to h when h is uniformly drawn by a collection of target functions. (H, F) = E h.\""}, {"heading": "3 Decomposition vs. End-to-end", "text": "Many practical learning problems, and more generally algorithmic problems, can be seen as a structured set of partial problems. Applicable solutions can either be to address the problem in an end-to-end manner or by decomposition. While for a traditional algorithmic solution, the \"divide-and-conquer\" strategy is an obvious choice, the ability of deep learning to use big data and expressive architectures has made the \"end-to-end approach\" an attractive alternative. Is it such a bad idea to let a network \"learn by itself\"? When is it necessary or worth the effort to \"help\" it? There are various aspects that can be taken into account in this context. For example, [25] the difference between the approaches of sample analysis \"learning alone\" analyzes such a bad idea? When is it necessary, or is it worth the effort to \"help\"?"}, {"heading": "3.1 Experiment I", "text": "Our first experiment compares the two approaches in a computer vision environment where the problem is closest. \"We have the most common and successful algorithmic approach (v = 1). We define a family of problems, parameterized by k-N, and show that the gap between an\" end-to-end \"approach and a\" decomposition \"one.Let X1 denotes the space of 28 x-28 binary images, with a distribution D defined by the following scanning method: \u2022 For example, the gap between an\" end-to-end \"approach and a\" fragmentation \"(x, 28 \u2212 5]), (x, y) is set to 0 everywhere except for a straight line of length l, centered on (x, y), and rotated at an angle."}, {"heading": "3.2 Experiment II", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "3.3 Analysis", "text": "\"We are approaching the analysis in which our estimate goes beyond the random introduction of the network,\" he says. \"We have to ask the question of what we should do.\" \"We have to ask what we should do.\" \"We have to ask the question of what we should do.\" \"We have to ask the question of what we should do.\" \"We have to ask the question of what we should do.\" \"We have to ask the question of what we should do.\" \"We have to ask the question of what we should do.\" \"\" We have to ask the question of what we should do. \"\" \"We have to ask the question of what we should do.\" \"\" We have to ask the question of what we should do. \"\" \"\" We have to ask the question of what we should do. \".\" \"\". \"\" \"\".. \"\" \"\" \"\".. \"\". \"\" \"\" \"\".. \"\". \"\" \"\". \"\" \"\" \"\" \"..\". \"\". \"\" \"\". \"\" \"\". \"\" \"\" \"\". \"..\" \".\" \".\" \"\".. \"\"..... \"\" \"\" \"\" \"......\" \"\" \"\" \"\" \"\"...... \"\" \"\" \"\" \".........\" \"\" \"\" \"\" \"\" \"\" \".......\" \"\" \"\" \"\"....... \"\" \"\" \"\" \"\" \"\" \"\"............... \"\" \"\" \"\" \"\" \"\" \"\".......... \"\" \"\" \"\" \"\" \"\" \"..............\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\".. \"\" \"\" \"\" \"\" \".............\" \"\" \"\" \"\" \"\" \"\"... \"\" \"\" \"\" \"\" \"\" \".............\" \"\" \"\" \"\" \"\" \"\""}, {"heading": "4 Architecture and Conditioning", "text": "The choice of network architecture is a crucial element for the success of deep learning. New variants and the development of new architectures are one of the most important tools for achieving practical breakthroughs [11, 29]. When selecting an architecture, one consideration is how to incorporate existing knowledge into the existing problem, thereby improving the network's relevance to this problem without dramatically increasing the complexity of the samples. Another aspect concerns improving the computational complexity of the training. In this section, we formally show how the choice of architecture influences training time through the lens of the conditional number of the problem. The learning problem we are looking at in this section is that of coding one-dimensional piecemeal linear curves. We show how different architectures, all of sufficient significance for solving the problem, can have better conditioning techniques."}, {"heading": "4.1 Experiments and Analysis", "text": "We are experimenting with various in-depth learning solutions for encoding the structure of one-dimensional, continuous, piecemeal linear (PWL) curves. Each PWL curve with k pieces can be described as follows: f (x) = b + \u2211 k i = 1 ai [x \u2212 \u03b8i] +, where ai is the difference between the slope at the i'th segment and the (i \u2212 1) th segment. For example, the following curve can be parameterized by b = 1, a = (1, \u2212 2, 3), \u03b8 = (0, 2, 6). The problem we are looking at is that we can get a vector of the values from f to x \u00b2 {0,.,., n \u2212 1}, namely f: = (0), f (1), f (n \u2212 1), and the output of the values of b, {ai, \u03b8i} ki = 1. We can consider this problem as a coding problem because we would start from b = 1, we would assume that sample in = 1."}, {"heading": "4.1.1 Convex Problem, Large Condition Number", "text": "Since we assume that each of these layers is an integral U, we will not solve the problem. (...) We assume that each of these layers is an integral U. (...) We assume that each layer is an integral U. (...) We assume that each layer is an integral U. (...) We assume that each layer is an integral U. (...) We assume that each layer is an integral U. (...) We assume that we are directly learning this linear transformation. (...) We assume that the layer of a layer is explicitly labeled, with n source channels. (...) We assume that the layer of this layer is explicitly labeled. (...) We begin to learn this linear transformation directly. (...) We assume that the layer of this layer is explicitly labeled. (...)"}, {"heading": "4.1.2 Improved Condition Number through Convolutional Architecture", "text": "Looking at the explicit expression for U given in term 1, we find that U f can be written as a one-dimensional fusion of f with the core [1, \u2212 2, 1]. Therefore, the mapping from f to p is feasible with the help of a revolutionary layer. Empirically, convergence to a precise solution with this architecture is faster. Figure 6b illustrates a few examples. In order to theoretically understand the advantage of using a merger, we will look at the state number of the new problem from the perspective of the number of iterations required for training, thus creating an understanding of the gap in the training time. In the previous section, we saw that GD requires iterations to learn the full matrix U. In the appendix (sections A.5 and A.6), we show that under some mild assumptions, the state number consists only of BA (n3) and GD requires only this order of iterations to learn the optimal filter, [1] \u2212 1."}, {"heading": "4.1.3 Additional Improvement through Explicit Conditioning", "text": "In Section 4.1.2, despite observing an improvement through the fully interconnected architecture, we saw that GD requires approximate (n3) iterations even for the simple problem of learning the filter [1, \u2212 2, 1], which motivates the application of additional conditioning techniques in the hope of additional performance gains. First, we explicitly present the revolutionary architecture as a linear regression problem. We perform the Vec2Row operation on f as follows: If we give an example f conversion, we construct a matrix, F, the size n \u00d7 3, so that the t'te series of F [ft \u2212 1, ft, ft + 1] is. Then we get a vanilla linear regression problem in R3, with the filter [1, \u2212 2, 1] as the solution. Faced with a sample f, we can now approximate the correlation matrix of F, which is called the C matrix."}, {"heading": "4.1.4 Perhaps I should use a deeper network?", "text": "The solution found in Section 4.1.3 indicates that a suitable selection and conditioning scheme for the architecture can allow training time accelerations of several orders of magnitude. Furthermore, the benefit of reducing the number of parameters in the transition from a fully networked architecture to a convex one proves helpful in terms of convergence time. However, we should not exclude that a deeper, broader network does not suffer from the deficiencies analyzed above for the convex case. Motivated by the success of deep auto-encoders, we are experimenting with a deeper architecture for coding f. Namely, we minimize minv1, v2 Ef [(f \u2212 Mv2 (Nv1 (f))) 2], where Nv1, Mv2 are deep networks parameterized by their weight v1, v2, where the output of N is 2k, enough for realizing the encoding problem of 100 points, each of these two layers having no intuition for the 500 points."}, {"heading": "5 Flat Activations", "text": "We are now examining another aspect of gradient-based learning that poses difficulties for optimization: the flatness of the loss area due to the saturation of activation functions, which leads to disappearing gradients and a slowdown in the training process. This problem is exacerbated in deeper architectures, since it is likely that the back-propagated message to lower layers of architecture would disappear somewhere along the way due to saturated activation. This is a major problem when sigmoids are used as gating mechanisms in recursive neural networks such as LSTMS and GRUs [10, 3]. While non-local search-based optimization seems out of reach for large-scale problems, variants of gradient updating, whether by adding dynamics, higher-order methods or normalized gradients, are quite successful, leading to considerations about updating programs that differ from \"vanilla\" grades updating."}, {"heading": "5.1 Experimental Setup", "text": "Consider the following optimization setup: The sample space X-Rd is symmetrically distributed; the target function y: Rd \u2192 R has the form y (x) = u (v * > x + b *), where v * Rd, b * * * *, and u: R \u2192 R is a function that does not decrease monotonously; the goal of the optimization problem is given by: min w E x ['(u (Nw (x)), y (x)], where Nw is any neural network parameterized by w, and \"any loss function (e.g. the square or absolute difference); for the experiments we use u in the form: u (r) = z0 + 0 [55] 1 [r > zi \u2212 1] \u00b7 (zi \u2212 1), where z0 < z1 < z55 are identical; in other words: r is given the function rounds to the next zi."}, {"heading": "5.2 Non-Flat Approximation Experiment", "text": "We begin by trying to approximate u by means of a non-flat function, where c is a constant, and \u03c3 is the sigmoid function \u03c3 (z) = (1 + exp (\u2212 z) \u2212 1. Intuitively, we approach the \"steps\" in u by means of a sum of sigmoids, each of which corresponds to the amplitude of the step height and is centered in the step position. This is similar to the motivation for using sigmoids as activation functions and as gates in LSTM cells - a non-flat approximation of the step function. Below is an example of u and its approximation and u The goal is the expected square loss that spreads through u-shaped, namelymin v, b E x [(u-shaped (v > x + b) \u2212 y (x)."}, {"heading": "5.3 End-to-End Experiment", "text": "Next, we try to solve the problem by improper learning, with the goal now being: min w E x [(Nw (x) \u2212 y (x) 2], where Nw is a network parameterized by its weight vector w. We use a simple architecture of four fully connected levels, the first three with ReLU activations and 100 output channels, and the last, with only one output channel and no activation function. As described in Section 4, difficulties arise when resorting to non-smooth functions. In this case, when u is not even continuous, the inaccuracies in detecting the non-continuity points come to the fore. Furthermore, this solution comes at an additional cost in terms of sample complexity, training time and testing time, since a much larger than necessary network is used. An advantage, of course, is the minimal prior knowledge of u that is required."}, {"heading": "5.4 Multi-Class Experiment", "text": "In this experiment, we approach the problem as a general multi-class classification problem, treating each value of the image of u as its own class. We use an architecture similar to that of the end-to-end experiment, with a less hidden layer and a final layer that provides 55 outputs, each corresponding to one of the steps defined by the Zis. One problem is the inaccuracy at the class boundaries due to the lack of structure imposed on the predictor. The fact that the linear link between x and the input to u is not imposed by the architecture leads to \"blurred\" boundaries. Moreover, the fact that we rely on an \"improper\" approach in the sense that we ignore the order imposed by u leads to higher sample complexity."}, {"heading": "5.5 The \u201cForward-Only\u201d Update Rule", "text": "Returning to a direct formulation of the problem, in the form of the objective function min w F (w) = E x [(u (w > x) \u2212 y (x)) 2], where y (x) = u (v * > x), the gradient update rule in this case is w (t + 1) = w (t) \u2212 \u03b7 F (w (t))), where we want to replace the gradient with the following for our goal: F (w) = E x [(u (w > x) \u2212 y (x)) \u00b7 u (w > x) \u00b7 x] Since u \u2032 s zero a.e., the gradient update is meaningless. [17, 16] It has been proposed to replace the gradient with the following: F (w) = E x [(u (w > x) \u2212 y (x)) \u00b7 x] (5) With regard to the repropagation algorithm, this type of update can be interpreted as replacing the reset message for the activation message with an identification message."}, {"heading": "A Proofs", "text": "To prove the result, it is sufficient to show that Eh has chosen an expectation with respect to h."}, {"heading": "B Technical Lemmas", "text": "Lemma 3 Each parity function via d variables is feasible through a network with a fully connected layer of width d- > 3d2 with ReLU activations and a fully connected output layer with linear activation and a single unit. Proof Have the weights entering each of the first 3d2 hidden units set to v * and the rest to 0. Further, assume that the distortions of the first 3i + {1, 2, 3} units are set to \u2212 (2i \u2212 12), \u2212 2i, \u2212 (2i + 1 2) or to i, and that their weights are set to 1, \u2212 2, and 1 in the output layer. It is not difficult to see that the weighted sum of these triads of neurons is 1 2 if < x, v \u00b2 > = 2i, and 0 otherwise. Note that there is such a triad defined for each even number in the range [d]."}, {"heading": "C Command Lines for Experiments", "text": "Our experiments _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _"}], "references": [{"title": "Weakly learning dnf and characterizing statistical query learning using fourier analysis", "author": ["Avrim Blum", "Merrick Furst", "Jeffrey Jackson", "Michael Kearns", "Yishay Mansour", "Steven Rudich"], "venue": "In Proceedings of the twentysixth annual ACM symposium on Theory of computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "Multitask learning. In Learning to learn, pages 95\u2013133", "author": ["Rich Caruana"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Statistical query algorithms for stochastic convex optimization", "author": ["Vitaly Feldman", "Cristobal Guzman", "Santosh Vempala"], "venue": "arXiv preprint arXiv:1512.09170,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Stochastic first-and zeroth-order methods for nonconvex stochastic programming", "author": ["Saeed Ghadimi", "Guanghui Lan"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "(icassp), 2013 ieee international conference on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Efficient learning of generalized linear and single index models with isotonic regression", "author": ["Sham M Kakade", "Varun Kanade", "Ohad Shamir", "Adam Kalai"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "The isotron algorithm: High-dimensional isotonic regression", "author": ["Adam Tauman Kalai", "Ravi Sastry"], "venue": "In COLT,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Efficient noise-tolerant learning from statistical queries", "author": ["Michael Kearns"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["Florian Schroff", "Dmitry Kalenichenko", "James Philbin"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Pieter Abbeel", "Michael I Jordan", "Philipp Moritz"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Large-scale convex minimization with a low-rank constraint", "author": ["Shai Shalev-Shwartz", "Alon Gonen", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1106.1622,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "On the sample complexity of end-to-end training vs. semantic abstraction training", "author": ["Shai Shalev-Shwartz", "Amnon Shashua"], "venue": "arXiv preprint arXiv:1604.06915,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Distribution-specific hardness of learning neural networks", "author": ["Ohad Shamir"], "venue": "arXiv preprint arXiv:1609.01037,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George E Dahl", "Geoffrey E Hinton"], "venue": "ICML (3),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning", "author": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke", "Alex Alemi"], "venue": "arXiv preprint arXiv:1602.07261,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Deepface: Closing the gap to humanlevel performance in face verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lior Wolf"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}], "referenceMentions": [{"referenceID": 19, "context": "The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-of-the-art performances, ranging the fields of computer vision [20, 11, 22, 30], audio and natural language processing and generation [4, 12, 9, 31], as well as robotics [21, 23], to name just a few.", "startOffset": 167, "endOffset": 183}, {"referenceID": 10, "context": "The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-of-the-art performances, ranging the fields of computer vision [20, 11, 22, 30], audio and natural language processing and generation [4, 12, 9, 31], as well as robotics [21, 23], to name just a few.", "startOffset": 167, "endOffset": 183}, {"referenceID": 21, "context": "The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-of-the-art performances, ranging the fields of computer vision [20, 11, 22, 30], audio and natural language processing and generation [4, 12, 9, 31], as well as robotics [21, 23], to name just a few.", "startOffset": 167, "endOffset": 183}, {"referenceID": 29, "context": "The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-of-the-art performances, ranging the fields of computer vision [20, 11, 22, 30], audio and natural language processing and generation [4, 12, 9, 31], as well as robotics [21, 23], to name just a few.", "startOffset": 167, "endOffset": 183}, {"referenceID": 3, "context": "The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-of-the-art performances, ranging the fields of computer vision [20, 11, 22, 30], audio and natural language processing and generation [4, 12, 9, 31], as well as robotics [21, 23], to name just a few.", "startOffset": 238, "endOffset": 252}, {"referenceID": 11, "context": "The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-of-the-art performances, ranging the fields of computer vision [20, 11, 22, 30], audio and natural language processing and generation [4, 12, 9, 31], as well as robotics [21, 23], to name just a few.", "startOffset": 238, "endOffset": 252}, {"referenceID": 8, "context": "The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-of-the-art performances, ranging the fields of computer vision [20, 11, 22, 30], audio and natural language processing and generation [4, 12, 9, 31], as well as robotics [21, 23], to name just a few.", "startOffset": 238, "endOffset": 252}, {"referenceID": 20, "context": "The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-of-the-art performances, ranging the fields of computer vision [20, 11, 22, 30], audio and natural language processing and generation [4, 12, 9, 31], as well as robotics [21, 23], to name just a few.", "startOffset": 274, "endOffset": 282}, {"referenceID": 22, "context": "The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-of-the-art performances, ranging the fields of computer vision [20, 11, 22, 30], audio and natural language processing and generation [4, 12, 9, 31], as well as robotics [21, 23], to name just a few.", "startOffset": 274, "endOffset": 282}, {"referenceID": 26, "context": "The list of success stories can be matched and surpassed by a list of practical \u201ctips and tricks\u201d, from different optimization algorithms, parameter tuning methods [27, 19], initialization schemes [8], architecture designs [28], loss functions, data augmentation [20] and so on.", "startOffset": 164, "endOffset": 172}, {"referenceID": 18, "context": "The list of success stories can be matched and surpassed by a list of practical \u201ctips and tricks\u201d, from different optimization algorithms, parameter tuning methods [27, 19], initialization schemes [8], architecture designs [28], loss functions, data augmentation [20] and so on.", "startOffset": 164, "endOffset": 172}, {"referenceID": 7, "context": "The list of success stories can be matched and surpassed by a list of practical \u201ctips and tricks\u201d, from different optimization algorithms, parameter tuning methods [27, 19], initialization schemes [8], architecture designs [28], loss functions, data augmentation [20] and so on.", "startOffset": 197, "endOffset": 200}, {"referenceID": 27, "context": "The list of success stories can be matched and surpassed by a list of practical \u201ctips and tricks\u201d, from different optimization algorithms, parameter tuning methods [27, 19], initialization schemes [8], architecture designs [28], loss functions, data augmentation [20] and so on.", "startOffset": 223, "endOffset": 227}, {"referenceID": 19, "context": "The list of success stories can be matched and surpassed by a list of practical \u201ctips and tricks\u201d, from different optimization algorithms, parameter tuning methods [27, 19], initialization schemes [8], architecture designs [28], loss functions, data augmentation [20] and so on.", "startOffset": 263, "endOffset": 267}, {"referenceID": 25, "context": "Indeed, in [26], it was shown that this also holds in a more general setup, when the output y corresponds to a linear function composed with a periodic one, and the input x is sampled from a smooth distribution: Theorem 2 (Shamir 2016) Let \u03c8 be a fixed periodic function, and let H = {x 7\u2192 \u03c8(v\u2217>x) : \u2016v\u2217\u2016 = r} for some r > 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": ", 1), this problem is known to be solvable with an appropriate LSTM network [14]).", "startOffset": 76, "endOffset": 80}, {"referenceID": 17, "context": "Finally, we remark that the connection between parities, difficulty of learning and orthogonal functions is not new, and has already been made in the context of statistical query learning [18, 1].", "startOffset": 188, "endOffset": 195}, {"referenceID": 0, "context": "Finally, we remark that the connection between parities, difficulty of learning and orthogonal functions is not new, and has already been made in the context of statistical query learning [18, 1].", "startOffset": 188, "endOffset": 195}, {"referenceID": 5, "context": "Recently, [6] have shown that gradient-based methods with an approximate gradient oracle can be implemented as a statistical query algorithm, which implies that gradient-based methods are indeed unlikely to solve learning problems which are known to be hard in the statistical queries framework, in particular parities.", "startOffset": 10, "endOffset": 13}, {"referenceID": 20, "context": "Prior results of End-to-end [21, 9] and decomposition and added feedback [13, 29, 2] approaches show success in both directions.", "startOffset": 28, "endOffset": 35}, {"referenceID": 8, "context": "Prior results of End-to-end [21, 9] and decomposition and added feedback [13, 29, 2] approaches show success in both directions.", "startOffset": 28, "endOffset": 35}, {"referenceID": 12, "context": "Prior results of End-to-end [21, 9] and decomposition and added feedback [13, 29, 2] approaches show success in both directions.", "startOffset": 73, "endOffset": 84}, {"referenceID": 28, "context": "Prior results of End-to-end [21, 9] and decomposition and added feedback [13, 29, 2] approaches show success in both directions.", "startOffset": 73, "endOffset": 84}, {"referenceID": 1, "context": "Prior results of End-to-end [21, 9] and decomposition and added feedback [13, 29, 2] approaches show success in both directions.", "startOffset": 73, "endOffset": 84}, {"referenceID": 24, "context": "For example, [25] analyzed the difference between the approaches from the sample complexity point of view.", "startOffset": 13, "endOffset": 17}, {"referenceID": 26, "context": "Let X1 denote the space of 28 \u00d7 28 binary images, with a distribution D defined by the following sampling procedure: \u2022 Sample \u03b8 \u223c U([0, \u03c0]), l \u223c U([5, 28\u2212 5]), (x, y) \u223c U([0, 27]).", "startOffset": 171, "endOffset": 178}, {"referenceID": 0, "context": "\u2022 Concatenate the \u201cscores\u201d of a tuple\u2019s entries, transform them to the range [0, 1] using a sigmoid function, and feed the resulting vector into another network, N (2) w2 , of a similar architecture to the one defined in Section 2, outputting a single \u201ctuple-score\u201d, which can then be thresholded for obtaining the binary prediction.", "startOffset": 77, "endOffset": 83}, {"referenceID": 6, "context": "In our context, [7] showed that when running SGD (even on non-convex objectives), arriving at a point where \u2016\u2207wL(w)\u2016 \u2264 requires order of \u03bd\u0304/ 2 iterations, where \u03bd\u0304 = max t E x,q \u2016\u2207w(x, q)\u2016 \u2212 \u2016\u2207wL(w)\u2016,", "startOffset": 16, "endOffset": 19}, {"referenceID": 10, "context": "New variants and development of novel architectures are one of the main tools for achieving practical breakthroughs [11, 29].", "startOffset": 116, "endOffset": 124}, {"referenceID": 28, "context": "New variants and development of novel architectures are one of the main tools for achieving practical breakthroughs [11, 29].", "startOffset": 116, "endOffset": 124}, {"referenceID": 14, "context": ", [15, 19, 5, 24]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 18, "context": ", [15, 19, 5, 24]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 4, "context": ", [15, 19, 5, 24]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 23, "context": ", [15, 19, 5, 24]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 9, "context": "This is a major problem when using sigmoids as a gating mechanisms in Recurrent Neural Networks such as LSTMS and GRUs [10, 3].", "startOffset": 119, "endOffset": 126}, {"referenceID": 2, "context": "This is a major problem when using sigmoids as a gating mechanisms in Recurrent Neural Networks such as LSTMS and GRUs [10, 3].", "startOffset": 119, "endOffset": 126}, {"referenceID": 16, "context": "Using a different variant of a local search-based update, based on [17, 16] , we arrive at an efficient solution.", "startOffset": 67, "endOffset": 75}, {"referenceID": 15, "context": "Using a different variant of a local search-based update, based on [17, 16] , we arrive at an efficient solution.", "startOffset": 67, "endOffset": 75}, {"referenceID": 16, "context": "[17, 16] proposed to replace the gradient with the following: \u2207\u0303F (w) = E x [ (u(w>x)\u2212 y(x)) \u00b7 x ] (5)", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[17, 16] proposed to replace the gradient with the following: \u2207\u0303F (w) = E x [ (u(w>x)\u2212 y(x)) \u00b7 x ] (5)", "startOffset": 0, "endOffset": 8}, {"referenceID": 16, "context": "As mentioned before, the method is due to [17, 16], where it is proven to converge to an -optimal solution in O(L/ ), under the additional assumptions that the function u is L-Lipschitz, and that w is constrained to have bounded norm.", "startOffset": 42, "endOffset": 50}, {"referenceID": 15, "context": "As mentioned before, the method is due to [17, 16], where it is proven to converge to an -optimal solution in O(L/ ), under the additional assumptions that the function u is L-Lipschitz, and that w is constrained to have bounded norm.", "startOffset": 42, "endOffset": 50}], "year": 2017, "abstractText": "In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four families of problems for which some of the commonly used existing algorithms fail or suffer significant difficulty. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied.", "creator": "LaTeX with hyperref package"}}}