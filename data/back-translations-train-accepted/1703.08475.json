{"id": "1703.08475", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2017", "title": "Overcoming Catastrophic Forgetting by Incremental Moment Matching", "abstract": "Catastrophic forgetting is a problem which refers to losing the information of the first task after training from the second task in continual learning of neural networks. To resolve this problem, we propose the incremental moment matching (IMM), which uses the Bayesian neural network framework. IMM assumes that the posterior distribution of parameters of neural networks is approximated with Gaussian distribution and incrementally matches the moment of the posteriors, which are trained for the first and second task, respectively. To make our Gaussian assumption reasonable, the IMM procedure utilizes various transfer learning techniques including weight transfer, L2-norm of old and new parameters, and a newly proposed variant of dropout using old parameters. We analyze our methods on the MNIST and CIFAR-10 datasets, and then evaluate them on a real-world life-log dataset collected using Google Glass. Experimental results show that IMM produces state-of-the-art performance in a variety of datasets.", "histories": [["v1", "Fri, 24 Mar 2017 15:43:39 GMT  (1187kb,D)", "http://arxiv.org/abs/1703.08475v1", null], ["v2", "Thu, 7 Sep 2017 13:01:46 GMT  (1290kb,D)", "http://arxiv.org/abs/1703.08475v2", "Selected for a spotlight presentation at NIPS, 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["sang-woo lee", "jin-hwa kim", "jaehyun jun", "jung-woo ha", "byoung-tak zhang"], "accepted": true, "id": "1703.08475"}, "pdf": {"name": "1703.08475.pdf", "metadata": {"source": "META", "title": "Overcoming Catastrophic Forgetting by Incremental Moment Matching", "authors": ["Sang-Woo Lee", "Jin-Hwa Kim", "Jung-Woo Ha", "Byoung-Tak Zhang"], "emails": ["<btzhang@bi.snu.ac.kr>."], "sections": [{"heading": "1. Introduction", "text": "In fact, most people who live and work in the US have the same problems as most people who live and work in the US."}, {"heading": "2. Previous Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Catastrophic Forgetting", "text": "One of the most important approaches to catastrophic forgetting is the use of an ensemble of neural networks. When a new task arrives, the algorithm creates a new network and divides the representation between the tasks (Lee et al., 2016; Rusu et al., 2016). This approach often improves the performance of the entire task and is used practically in continuous learning. However, this approach has a complexity problem, especially in the conclusion, because the number of networks increases with the number of new tasks that need to be learned. Another approach investigates methods that implicitly distribute information, in typical stochastic gradient descent (SGD) learning. These methods use the idea of exit or maxout for the distribution of information for each task, using the large capacity of the neural network (Srivastava et al., 2013) Most studies that follow this approach had limited success and failed to maintain performance on the old task."}, {"heading": "2.2. Bayesian Neural Networks", "text": "Previous studies have argued that BNN is better regulated than NN and provides a confidence interval for estimating the output of each instance. To our knowledge, the entire study on BNN uses Gaussian distribution as the rear of the parameters. However, the Gaussian assumption also proposes full covariance because tracking all the information of the covariance matrix is too expensive. However, in order to estimate the covariance matrix, researchers usually only use the diagonal term for the covariance matrix, in which case the rear distribution is fully factored for each parameter. However, methods that use full covariance have recently been proposed (Louizos & Welling, 2016)."}, {"heading": "3. Incremental Moment Matching", "text": "In incremental moment matching (IMM), the moments of posterior distributions are compared incrementally (Rashwan et al., 2016). Although sum product-related networks are not normally scalable to large data sets, their online learning method is useful and achieves a similar performance as the batch learner. Our method, which uses moment matching, focuses on continuous learning, unlike the previous method, on significantly different statistics between tasks. In our work, we use a Gaussian distribution to approximate the posterior distribution of parameters. We want to find the optimal parameter \u00b5 1: 2 and vice versa 1: 2 of the Gaussian approximation function q for the entire task, starting from the parameters of the first and second task, (\u00b51, \u04411) and (\u00b52, \u04452) of the peripheral distribution of q1 and qerip1: 2 and vice versa."}, {"heading": "3.1. Mean-based Incremental Moment Matching (mean-IMM)", "text": "The first method is the mean IMM, which calculates the parameters of two networks in each layer using a mixing ratio \u03b1. The objective function of the mean IMM is to minimize the KL divergence between q1: 2 and the mixture of q1 and q2, \u00b5 * 1: 2, namely 1: 2 = argmin\u00b51: 2, 1: 2KL (q1: 2 | (1 \u2212 \u03b1) q1 + \u03b1q2) (4) According to Zhang and Kwok (2010) the solution is as follows: \u00b5 1: 2 = (1 \u2212 \u03b1) \u00b51 + \u03b1\u00b52 (5) \u03a3 1: 2 = (1 \u2212 \u03b1) (\u03941 + (\u00b51 \u2212 \u03bc 1: 2) (\u00b51 \u2212 \u00b5 \u0445 1: 2) (\u00b51 \u2212 \u00b5 \u0445 1: 2) (\u00b51) T) + \u03b1 (\u00b51 \u2212 4)."}, {"heading": "3.2. Mode-based Incremental Moment Matching (mode-IMM)", "text": "The second method is mode-IMM, a variant of mean-IMM, which uses the covariance information of the posterior part of the Gaussian distribution. Although mean-IMM minimizes the CLD divergence for a mixture of Gaussian (MoG), a weighted average of two mean vectors of Gaussian distributions does not normally correspond to MoG mode. In discriminatory learning, the maximum distribution is of primary interest. Critical points of MoG with two clusters fulfill the following equations (Ray & Lindsay, 2005): (1 \u2212 \u03b1) \u03a3 \u2212 11 + 2) \u2212 1 ((1 \u2212 \u03b1) \u03a3 \u2212 11 \u2212 1 \u2212 1 \u2212 2 \u2212 2), where \u03b1 = q2q1 + q2. Note that this approach is a function of Gaussian verse verse verse-11, i.e. it cannot be calculated in a closed form."}, {"heading": "3.3. Comparative Models", "text": "We also compare the work of Kirkpatrick et al. (2017) with the results of our framework. The mechanism of EWC follows a sequential Bayesian estimate. EWC maximizes the following terms by gradient descent. Log p1: 2 \u2248 log p (y2 | X2, \u03b8) + \u03bb \u00b7 log q1 + C = log p (y2 | X2, \u03b8) \u2212 \u03bb2 (\u03b8 \u2212 \u00b51) T\u03a3 \u2212 11 (\u03b8 \u2212 \u00b51) + C \u2032 (13) In EWC, also \u03a3 \u2212 11 is approximated by the diagonal term of F1. Although it is a weakness to use only the diagonal term of the covariance matrix, this assumption works reasonably in the experiments of your paper. We also compare the work of Li and Hoiem (2016). Although LwF does not explicitly start from Bayesian, the approach can still be presented as follows: Log p1: 2 \u2248 log p (y2 | 2, Celsius, + log from the Xyesian protocol)."}, {"heading": "4. Transfer Techniques for Incremental Moment Matching", "text": "In general, the loss function of neural networks is not convex. If the parameters of two independently initialized neural networks are averaged, poor performance may occur. This is because there may be high cost barriers between these two parameters (Goodfellow et al., 2014). However, we will show that various transfer learning techniques can be used to solve this problem and make the assumption of a Gaussian distribution for neural networks reasonable. In this section, we present three practical techniques for IMM procedures, weight transfer, L2 transfer and drop transfer."}, {"heading": "4.1. Weight-Transfer", "text": "The first and most important idea is the determination of the weight transfer. The parameters for the new task \u00b52 are initialized with the parameters for the previous task \u00b51 (Yosinski et al., 2014). Note that this method of weight transfer was also applied to an incremental ensemble approach to continuous learning (Lee et al., 2016). In our experiments, the use of weight transfer was crucial for the continuous learning performance. Therefore, the experiments at IMM in this paper use weight transfer technology as standard. Weight transfer technology is motivated by the geometric property of neural networks discovered in the previous work (Goodfellow et al., 2014). They found that there is a straight path from the starting point to the solution without a high cost barrier, in various types of neural networks and datasets. This discovery could indicate that the weight transfer is transferred from the previous task to the new one."}, {"heading": "4.2. L2-transfer", "text": "The second idea is the L2 transfer, which is a variant of the L2 regularization. In the L2 transfer, a regularization concept of the distance between \u00b51 and \u00b52 is added to the following objective function, where \u03bb is a hyperparameter: log p (y2 | X2, \u00b52) \u2212 \u03bb \u00b7 | \u00b51 \u2212 \u00b52 | | 22 (15) Usually, the L2 transfer is used in continuous learning with large \u03bb and can also be interpreted as a special case of the \u00d6WK, in which the previous distribution is Gauss with \u03bbI as the covariance matrix. Two studies (Li & Hoiem, 2016; Kirkpatrick et al., 2017) compare their proposed model with L2transfer in its transmission and continued learning. In contrast to the previous use of large surface losses, we use small \u03bb with \u03bbI as the covariance matrix for the IMM procedure. In other words: \u00b52 is first trained by Equation 15 with small \u03bb, and \u00b51 and then becomes \u00b52 between our UV-0.1."}, {"heading": "4.3. Drop-transfer", "text": "The third idea is drop-transfer, a novel method developed in the work. Drop-transfer is a variant of drop-out, where the zero point of the drop-out method is \u00b51. In the training phase, for the weight vector that follows the ith node \u00b52i with the drop-out ratio p: \u00b5 2i = {\u00b51i, if the ith node is rejected, 1 1 \u2212 p \u00b7 \u00b52i \u2212 p 1 \u2212 p \u00b7 \u00b51i, otherwise (16) Note that the expectation of \u00b5-2i \u00b52i is intractable. There are studies (Srivastava et al., 2014; Baldi & Sadowski, 2013) that interpret the drop-out as an exponential interaction of weak learners. In this perspective, since the marginalization of the output distribution over the entire weak learner is intractable, the parameters multiplied by the reverse test-point drop-out method is simply used for the weak learning method in front of the other drop-out method."}, {"heading": "5. Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Disjoint MNIST Experiment", "text": "In this experiment, the MNIST data set is divided into two data sets: the first data set consists of only digits {0, 1, 2, 3, 4} and the second data set consists of the remaining digits {5, 6, 7, 8, 9}. The first data set contains 51.39% of the test data, and the second data set contains 48.61%. This implies that if the model misclassifies any image of the first data set, the accuracy can be at most 48.61%. Our task is the 10-class categorization, unlike in the previous work, which considers two independent tasks of 5-class categorization. Therefore, our task is more difficult than the task of the previous work. In our experiment, all models use only a 10-way softmax initial layer, except for LwF."}, {"heading": "5.2. Shuffled MNIST Experiment", "text": "The second experiment is the mixed MNIST experiment (Goodfellow et al., 2013; Kirkpatrick et al., 2017). In this experiment, the first dataset is the same as the original MNIST dataset. However, in the second dataset, the input pixels of all images are mixed with a fixed, random permutation. Therefore, the difficulty of the two datasets is the same, although a different solution is required for each dataset. However, in previous work, EWC reaches the level of performance of the learner, and it is argued that EWC overcomes catastrophic formatting in some areas. In our setting, the training data is divided into groups of 30,000 cases each. The experimental details are similar to the fragmented MNIST experiment, except all models are allowed to use the dropout regulation. In middle IMM, mode IMM, mode IMM, mode IMM, natural hyperparameter is used where for EWC, an optimal result is achieved."}, {"heading": "5.3. Lifelog Dataset", "text": "The Lifelog dataset consists of 660,000 seconds of egocentric video stream data collected over 46 days from three participants using Google Glass (Lee et al., 2016). Under Lee et al. (2016), the network can be updated every day, but a new network can be created for the 3rd, 7th, and 10th days, with training data of 3, 4, and 3 days, respectively. Following this framework, our network is created on the 3rd, 7th, and 10th days, while it is then merged into previously trained networks. We used AlexNet prepared by the ImageNet dataset (Krizhevsky et al., 2012)."}, {"heading": "6. Conclusion", "text": "Catastrophic forgetting is a fundamental challenge for continuous learning tasks. To solve this problem, we introduced incremental moment matching (IMM) and corresponding transfer techniques that improve the performance of IMM. We discussed not only how moment matching works, but also how three transfer techniques in the work make our Gaussian assumption reasonable. Three experimental results showed that IMM provides state-of-the-art performance in a variety of data sets. It was also shown that our IMM method works better by applying the methods suggested in the previous work, including dropout regulation and L2 transfer. We believe that our IMM could also be used in conjunction with other methods, including EWC and PathNet, to further enhance performance."}, {"heading": "Acknowledgments", "text": "Sang-Woo Lee would like to thank Jiseob Kim, Min-Oh Heo, Christina Baek and Heidi Tessmer for their helpful comments and edits."}, {"heading": "APPENDIX A. Modes in the Mixture of Gaussian", "text": "According to Ray and Lindsay (2005) are all critical points of a mixture of Gaussian (MoG) with two components in a curve like the following equation with 0 < \u03b1 < 1, \u03b8 = (1 \u2212 \u03b1) \u03a3 \u2212 11 + \u03b1\u03a3 \u2212 1 2) \u2212 1 (1 \u2212 \u03b1) \u03a3 \u2212 11 \u00b51 + \u03b1\u03a3 \u2212 1 2 \u00b52) (17) The proof is as follows. Imagine two Gaussian distribution q1 and q2, as in the equations 2 and 3, respectively q1 \u2261 q1 (\u03b8; \u00b51, \u04481) = 1 \u221a (2\u03c0) d | 1 | exp (\u2212 12 (\u03b8 \u2212 \u00b51) T\u03a3 \u2212 11 (\u03b8 \u2212 \u00b51)) q2 2 zip2 2 q5 q5 q5 q5 = 5 5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5 q5"}, {"heading": "APPENDIX B. Example Algorithms of Incremental Moment Matching", "text": "Algorithm 1 Mean IMM with weight transfer Input: Data {(X1, y1),..., (X \u221e, y \u221e), Weight w0 Output: wt repeat wt \u043a \u2190 wt \u2212 1 Pull (wt \u0445, Xt, yt) with L (wt, Xt, yt) + R (wt \u043a) wt \u2190 + (1 \u2212 \u03b1) wt \u2212 1until foreverAlgorithm 2 Mode IMM with weight transfer, L2 transmission and Fisher matrix input: Data {(X1, yt) + R (wt, y)}, Weight w0 Output: wt \u2212 1 Pull (wt, Xt, yt) with L (wt, Xt, yt) Transmission techniques and Fisher matrix input: Data {(X1, yt) + R (wt, y), Weight wt Output: wt \u2212 1 Pull (wt, Xt, yt, yt) with weight transfer, Xx, yt, yt ()."}], "references": [{"title": "Maximum number of modes of gaussian mixtures", "author": ["Am\u00e9ndola", "Carlos", "Engstr\u00f6m", "Alexander", "Haase", "Christian"], "venue": "arXiv preprint arXiv:1702.05066,", "citeRegEx": "Am\u00e9ndola et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Am\u00e9ndola et al\\.", "year": 2017}, {"title": "Understanding dropout", "author": ["Baldi", "Pierre", "Sadowski", "Peter J"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Baldi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Baldi et al\\.", "year": 2013}, {"title": "Weight uncertainty in neural network", "author": ["Blundell", "Charles", "Cornebise", "Julien", "Kavukcuoglu", "Koray", "Wierstra", "Daan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Blundell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Blundell et al\\.", "year": 2015}, {"title": "Streaming variational bayes", "author": ["Broderick", "Tamara", "Boyd", "Nicholas", "Wibisono", "Andre", "Wilson", "Ashia C", "Jordan", "Michael I"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Broderick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Broderick et al\\.", "year": 2013}, {"title": "Pathnet: Evolution channels gradient descent in super neural networks", "author": ["Fernando", "Chrisantha", "Banarse", "Dylan", "Blundell", "Charles", "Zwols", "Yori", "Ha", "David", "Rusu", "Andrei A", "Pritzel", "Alexander", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1701.08734,", "citeRegEx": "Fernando et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Fernando et al\\.", "year": 2017}, {"title": "Online variational bayesian learning", "author": ["Ghahramani", "Zoubin"], "venue": "In NIPS workshop on Online Learning,", "citeRegEx": "Ghahramani and Zoubin.,? \\Q2000\\E", "shortCiteRegEx": "Ghahramani and Zoubin.", "year": 2000}, {"title": "An empirical investigation of catastrophic forgetting in gradient-based neural networks", "author": ["Goodfellow", "Ian J", "Mirza", "Mehdi", "Xiao", "Da", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1312.6211,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Qualitatively characterizing neural network optimization problems", "author": ["Goodfellow", "Ian J", "Vinyals", "Oriol", "Saxe", "Andrew M"], "venue": "arXiv preprint arXiv:1412.6544,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Learning without forgetting", "author": ["Li", "Zhizhong", "Hoiem", "Derek"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Structured and efficient variational deep learning with matrix gaussian posteriors", "author": ["Louizos", "Christos", "Welling", "Max"], "venue": "arXiv preprint arXiv:1603.04733,", "citeRegEx": "Louizos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Louizos et al\\.", "year": 2016}, {"title": "A practical bayesian framework for backpropagation networks", "author": ["MacKay", "David JC"], "venue": "Neural computation,", "citeRegEx": "MacKay and JC.,? \\Q1992\\E", "shortCiteRegEx": "MacKay and JC.", "year": 1992}, {"title": "Catastrophic interference in connectionist networks: The sequential learning problem", "author": ["McCloskey", "Michael", "Cohen", "Neal J"], "venue": "Psychology of learning and motivation,", "citeRegEx": "McCloskey et al\\.,? \\Q1989\\E", "shortCiteRegEx": "McCloskey et al\\.", "year": 1989}, {"title": "Revisiting natural gradient for deep networks", "author": ["Pascanu", "Razvan", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1301.3584,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Online and distributed bayesian moment matching for parameter learning in sum-product networks", "author": ["Rashwan", "Abdullah", "Zhao", "Han", "Poupart", "Pascal"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Rashwan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rashwan et al\\.", "year": 2016}, {"title": "The topography of multivariate normal mixtures", "author": ["Ray", "Surajit", "Lindsay", "Bruce G"], "venue": "Annals of Statistics,", "citeRegEx": "Ray et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ray et al\\.", "year": 2005}, {"title": "On the upper bound of the number of modes of a multivariate normal mixture", "author": ["Ray", "Surajit", "Ren", "Dan"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Ray et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ray et al\\.", "year": 2012}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Compete to compute", "author": ["Srivastava", "Rupesh K", "Masci", "Jonathan", "Kazerounian", "Sohrob", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Srivastava et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2013}, {"title": "How transferable are features in deep neural networks", "author": ["Yosinski", "Jason", "Clune", "Jeff", "Bengio", "Yoshua", "Lipson", "Hod"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}, {"title": "Simplifying mixture models through function approximation", "author": ["Zhang", "Kai", "Kwok", "James T"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "Recently, this classical problem has resurfaced with the renaissance of deep learning research (Goodfellow et al., 2013; Kirkpatrick et al., 2017; Fernando et al., 2017).", "startOffset": 95, "endOffset": 169}, {"referenceID": 4, "context": "Recently, this classical problem has resurfaced with the renaissance of deep learning research (Goodfellow et al., 2013; Kirkpatrick et al., 2017; Fernando et al., 2017).", "startOffset": 95, "endOffset": 169}, {"referenceID": 2, "context": "that an uncertainty is assumed for each parameter in neural networks, and that the posterior distribution is calculated (Blundell et al., 2015).", "startOffset": 120, "endOffset": 143}, {"referenceID": 18, "context": "These methods use the idea of dropout or maxout for distributively storing the information for each task by making use of the large capacity of the neural network (Srivastava et al., 2013).", "startOffset": 163, "endOffset": 188}, {"referenceID": 6, "context": "Unfortunately, most studies following this approach had limited success and failed to preserve performance on the old task when an extreme change to the environment occurred (Goodfellow et al., 2013).", "startOffset": 174, "endOffset": 199}, {"referenceID": 4, "context": "Alternatively, Fernando et al. (2017) proposed PathNet, which extends the idea of the ensemble approach for parameter reuse within a single network.", "startOffset": 15, "endOffset": 38}, {"referenceID": 3, "context": "Sequential Bayes was also used to learn topic models from stream data in Broderick et al. (2013).", "startOffset": 73, "endOffset": 97}, {"referenceID": 2, "context": "Bayesian neural networks (BNN) assume an uncertainty for the whole parameter in neural networks so that the posterior distribution can be obtained (Blundell et al., 2015).", "startOffset": 147, "endOffset": 170}, {"referenceID": 14, "context": "Similarly to our method, Bayesian moment matching is used for sum-product networks, a kind of deep hierarchical probabilistic model (Rashwan et al., 2016).", "startOffset": 132, "endOffset": 154}, {"referenceID": 7, "context": "This is because there might be high cost barriers between the two parameters (Goodfellow et al., 2014).", "startOffset": 77, "endOffset": 102}, {"referenceID": 19, "context": "The parameters for new task \u03bc2 are initialized with the parameters for previous task \u03bc1 (Yosinski et al., 2014).", "startOffset": 88, "endOffset": 111}, {"referenceID": 7, "context": "The weight-transfer technique is motivated by the geometrical property of neural networks discovered in the previous work (Goodfellow et al., 2014).", "startOffset": 122, "endOffset": 147}, {"referenceID": 6, "context": "To empirically validate the concept of weight-transfer, we use the linear path analysis proposed by Goodfellow et al. (2014). We randomly chose 18,000 instances from the training dataset of CIFAR-10, and divided them into three subsets of 6,000 instances, respectively.", "startOffset": 100, "endOffset": 125}, {"referenceID": 18, "context": "We first evaluate our models on the disjoint MNIST experiment (Srivastava et al., 2013).", "startOffset": 62, "endOffset": 87}, {"referenceID": 6, "context": "Model Explanation of Natural Tuned Hyperparam Hyperparam Accuracy Hyperparam Accuracy SGD (Goodfellow et al., 2013) epoch per dataset 10 47.", "startOffset": 90, "endOffset": 115}, {"referenceID": 6, "context": "In SGD, dropout is used as proposed in Goodfellow et al. (2013).", "startOffset": 39, "endOffset": 64}, {"referenceID": 6, "context": "Model Accuracy SGD (Goodfellow et al., 2013) 96.", "startOffset": 19, "endOffset": 44}, {"referenceID": 6, "context": "The second experiment is the shuffled MNIST experiment (Goodfellow et al., 2013; Kirkpatrick et al., 2017).", "startOffset": 55, "endOffset": 106}], "year": 2017, "abstractText": "Catastrophic forgetting is a problem which refers to losing the information of the first task after training from the second task in continual learning of neural networks. To resolve this problem, we propose the incremental moment matching (IMM), which uses the Bayesian neural network framework. IMM assumes that the posterior distribution of parameters of neural networks is approximated with Gaussian distribution and incrementally matches the moment of the posteriors, which are trained for the first and second task, respectively. To make our Gaussian assumption reasonable, the IMM procedure utilizes various transfer learning techniques including weight transfer, L2-norm of old and new parameters, and a newly proposed variant of dropout using old parameters. We analyze our methods on the MNIST and CIFAR-10 datasets, and then evaluate them on a real-world life-log dataset collected using Google Glass. Experimental results show that IMM produces state-of-the-art performance in a variety of datasets.", "creator": "LaTeX with hyperref package"}}}