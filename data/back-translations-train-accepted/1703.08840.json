{"id": "1703.08840", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Mar-2017", "title": "Inferring The Latent Structure of Human Decision-Making from Raw Visual Inputs", "abstract": "The goal of imitation learning is to match example expert behavior, without access to a reinforcement signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are not explicitly modeled. We introduce an extension to the Generative Adversarial Imitation Learning method that can infer the latent structure of human decision-making in an unsupervised way. Our method can not only imitate complex behaviors, but also learn interpretable and meaningful representations. We demonstrate that the approach is applicable to high-dimensional environments including raw visual inputs. In the highway driving domain, we show that a model learned from demonstrations is able to both produce different styles of human-like driving behaviors and accurately anticipate human actions. Our method surpasses various baselines in terms of performance and functionality.", "histories": [["v1", "Sun, 26 Mar 2017 16:20:36 GMT  (8149kb,D)", "http://arxiv.org/abs/1703.08840v1", "10 pages, 6 figures"]], "COMMENTS": "10 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["yunzhu li", "jiaming song", "stefano ermon"], "accepted": true, "id": "1703.08840"}, "pdf": {"name": "1703.08840.pdf", "metadata": {"source": "META", "title": "Inferring The Latent Structure of Human Decision-Making  from Raw Visual Inputs", "authors": ["Yunzhu Li", "Jiaming Song", "Stefano Ermon"], "emails": ["<leo.liyunzhu@pku.edu.cn>,", "mon@cs.stanford.edu>."], "sections": [{"heading": "1. Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2. Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Preliminaries", "text": "We use the tuple (S, A, P, r, \u03c10, \u03b3) to define an infinite horizon, discounted Markov decision-making process (MDP), where S represents the state space, A the action space, P: S \u00b7 A \u00b7 S \u2192 R the distribution of the transition probability, r: S \u2192 R the reward function, \u03c10: S \u2192 R the distribution of the initial state s0 and ig (0, 1) the discount factor. Let us denote \u03c0 a stochastic policy \u03c0: S \u00b7 A \u2192 [0, 1] and \u03c0E the expert policy to which we have only access to demonstrations. Expert demonstrations \u03c4E are a series of trajectories generated by the \u03c0E policy, each consisting of a sequence of state-action pairs."}, {"heading": "2.2. Imitation learning", "text": "Typically, there are two approaches to imitation acquisition: 1) behavioral cloning (BC), which learns a policy through supervised learning through the state action pairs from the expert paths; and 2) apprenticeship learning (AL), which assumes that expert policy is optimal under an unknown reward and learns a policy by regaining the reward and solving the corresponding planning problem; BC tends to have poor generalization characteristics due to compounding errors and covariant shifts (Ross & Bagnell, 2010; Ross et al., 2011); AL, on the other hand, has the advantage of learning a reward function that can be used to evaluate entire trajectories (Abbeel & Ng, 2004; Syed et al., 2008; Ho et al., 2016), but is typically costly to implement because it requires solving a reinforcement problem (RL) within a learning loop."}, {"heading": "2.3. Generative Adversarial Imitation Learning", "text": "In the GAIL framework, the agent mimics the behavior of an expert policy \u03c0E by matching the distributions generated by government policies with the distributions of experts, achieving the optimum when these two distributions match perfectly. However, measuring the similarity between high-dimensional distributions is complicated, so GAIL introduces a neural network to roughly minimize the divergence between Jensen and Shannon. Intuitively, the neural network is a distinguishing feature that attempts to distinguish the two distributions, and the formal GAIL goal is called minimizing."}, {"heading": "3. Visual InfoGAIL", "text": "In this section, we propose an approach that can 1) discover and untangle prominent latent factors underlying human decision-making without oversight. 2) We learn strategies that correspond to these latent factors, and 3) use visual input as the only external perceptual information. Formally, we assume that expert policy is a mixture of experts, and we define the generative process of expert development \u03c4E as: s0, c, p (c), p (\u03c0 | c), at (at | st), st + 1, p (st + 1 | at, st), where c is a latent variable that selects a prior problem of the distribution of AIC (st + 1 | at, st) from the mix of expert policies through p (\u03c0 | c), p (appell), and p (appell)."}, {"heading": "3.1. Interpretable Imitation Learning", "text": "In order to establish a link between latent codes c and the behavior of \u03c0\u03b8, we use the information theory regularization that there should be a high level of mutual information between c and the state action pairs in the trajectory generated. This concept is introduced by InfoGAN (Chen et al., 2016), where latent codes are used to discover the prominent semantic features of data distribution and to guide the generation process. Specifically, regularization seeks to maximize the reciprocal information between latent codes and state action pairs, which are called I (c; s, a), which is difficult to directly maximize as it requires access to the rear. Therefore, we introduce a varying lower limit, LI (prev., Q), which maximizes the mutual information between latent codes and state action pairs, which is referred to as I (c; s, a)."}, {"heading": "3.2. Utilizing Raw Visual Inputs via Transfer Learning", "text": "Although our approach is general, we will focus on scenarios in which states are represented by images. Despite recent successes in visual perception, this is a very difficult scenario, as raw visual inputs are typically very high-dimensional. As a result, learning a policy that maps high-dimensional visual raw inputs for actions has particular difficulties. Intuitively, policymakers must simultaneously learn how to identify meaningful visual features and how to use them to achieve desired behavior. Conventional neural networks (CNNs) have led to dramatic improvements in many computer vision tasks (Krizhevsky et al., 2012). Unfortunately, they must simultaneously learn how to identify meaningful visual features and how to use them to achieve desired behavior. Conventional neural networks (CNNs) have led to dramatic improvements in many computer vision tasks (Krizhevsky et al., 2012)."}, {"heading": "4. Improved Optimization", "text": "While GAIL succeeds in low-dimensional input tasks (in Ho & Ermon (2016), the largest observation has 376 continuous variables), few have studied tasks where the input dimension is very high (such as 110 x 200 x 3 pixels as in our experiments), even with pre-trained functions of Residual Networks. To effectively learn a policy based solely on visual input, we are making the following improvements over the original GAIL framework."}, {"heading": "4.1. Reward Augmentation", "text": "In complex and less well-specified environments, imitation learning methods have the potential to perform better than supplementary reward learning methods because they do not require manual specification of an appropriate reward function. Assuming that the expert has an optimal policy under an unknown reward and that the expert is optimal, the reward function learned from the expert trails should match the desired reward signal. However, if the expert works suboptimally, the reward functions recovered by the expert are imperfect. Therefore, any policy that is trained under these rewards is also suboptimal; in other words, the potential of the imitator is limited by the expert's skills. In many cases, it is very difficult to fully define a suitable reward function for a given task, but it is relatively easy to impose constraints that we wish to enforce through policy."}, {"heading": "4.2. Wasserstein GAN", "text": "Wasserstein GAN (WGAN, Arjovsky et al. (2017) is a recently proposed framework for generative mutual training. Contrary to its traditional GAN counterpart, the discriminator network in WGAN solves a regression problem within a classification problem by assigning values to its input factors and attempting to maximize the score of real data and simultaneously minimize the score of generated data. We are expanding the use of WGAN to the generative learning framework to imitate adversarial imitations and define a new objective W # # # # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp # 160 amp # 160 amp; amp; amp # 160 amp; amp; amp; amp # 160 amp # 160 amp; amp # 160 amp; amp # 160 amp # 160 amp # 160 amp; amp # 160 amp; amp # 160 amp; amp # 160 amp # 160 amp; amp # 160 amp; amp # 160 amp # 160 amp # 160 amp; amp; amp # 160 amp # 160 amp; amp # 160 amp # 160 amp; amp # 160 amp # 160 amp; amp # 160 amp # 160 amp # 160 amp # 160 amp; amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp; amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp; amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp; (5; 5; 5) amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp; amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp; amp # 160 amp # 160 amp; amp; amp # 160 amp # 160 amp;"}, {"heading": "4.3. Variance Reduction", "text": "Political gradients are notorious for suffering from high variance gradients, as it is mathematically expensive to get enough rollouts from the simulator that match the political distribution. Therefore, we apply various techniques to reduce variance, such as replay buffers (Schaul et al., 2015) and base methods (Williams, 1992)."}, {"heading": "4.4. Algorithm", "text": "Apart from the baseline, we have three networks that need to be updated under InfoGAIL: the discrimination network DN (s, a), the political network DN (a), and the downstream estimator network QN (c). We are updating DN (as proposed in the original WGAN paper) and updating QN (a) and the downstream estimator network QN (c). The training process is presented in Algorithm 1. To speed up the training, we are initializing our policy based on a behavioral cloning policy, as in Ho & Ermon (2016). The discrimination network DN and the downstream approximation network QN are treated as distinct networks, as opposed to the InfoGAN approach, where they share the same network parameters until the final output layer. This is because DN (1) requires weight circumcision and short-term optimization methods, which would require a QN update in the current WAN framework (if the updating of a QN)."}, {"heading": "5. Experiments", "text": "We demonstrate the performance of our method by applying it to complex autonomous driving from the field of visual input. By conducting experiments on an auto racing simulator, we show that our learned strategies can mimic human behavior with only a handful of expert demonstrations 1) using raw visual inputs, 2) group human behavior into different and semantically significant categories, and 3) reproduce different types of human driving behavior by setting latent variables at the highest level."}, {"heading": "5.1. Environment Setup", "text": "The Open Racing Car Simulator (TORCS, Wymann et al. (2000)) is a popular simulation environment for the exploration of autonomous vehicles. We have packed it into a client-server framework with APIs similar to OpenAI Gym (Brockman et al., 2016). Our framework generates a realistic dashboard view and vehicle-related information and communicates with the policy (the client) via TCP packages so that the guidelines can be written in languages other than C + +. Specifically, we have implemented our policies using the TensorFlow Python API (Abadi et al., 2016)."}, {"heading": "128 fc 128 fc", "text": "All of our experiments are conducted in the TORCS environment. Demonstrations are collected by human experts by manually driving along the track and demonstrating typical behaviors such as staying in lanes, avoiding collisions with other cars and overtaking other cars. Politics accepts raw visual input as the only external input for the state and produces a three-dimensional action consisting of steering, acceleration and braking."}, {"heading": "5.2. Network Structure", "text": "In addition, our policy requires certain auxiliary information as an internal input in order to serve as a short-term memory, which can be accessed along with the visual input. In our experiments, the auxiliary information for the policy at the moment t consists of: 1) velocity at the time t, which is a three-dimensional vector; 2) actions at the time t \u2212 1 and t \u2212 2, which are both three-dimensional vectors; 3) damage to the car, which is a real value. The auxiliary input has a total of 10 dimensions. For the political network, the entered visual characteristics are guided through two revolutionary layers and then combined with the auxiliary information vector and (in the case of InfoGAIL) the latent code c. The exact architecture for e-commerce is shown in Figure 1. In addition, the merging of latent codes at the higher levels would have less impact on the actions as the visual characteristics have much larger dimensions."}, {"heading": "5.3. Inferring The Latent Structure of Human Decision-Making", "text": "In this experiment, we look at two subsets of human driving behavior: Turning, where the agent takes a curve with either the inside lane or the outside lane; and passing, where the agent passes another vehicle either from the left or the right. In both cases, the expert policy has two significant modes. Our goal is to capture the two modes of expert demonstrations. We consider the use of a discrete latent code, which is a unified encrypted vector with two possible states. In both cases, there are 80 expert paths in total, with 100 frames in each orbit. The performance of a learned policy is quantified using two metrics: the average distance is determined by the distance the agent travels before a collision (and is limited by the length of the simulation horizon), and the accuracy is defined as the classification accuracy of the expert model, which we define as the accuracy of the expert model."}, {"heading": "5.4. Ablation Experiments", "text": "We conduct a series of ablation experiments to demonstrate that our proposed techniques are indeed critical to learning effective policy.The experiments take into account a long-term attitude: Our policy drives a car on the race track along with other cars, while the human expert provides trajectories by trying to drive as fast as possible without collision.Reward multiplication occurs by adding a reward that encourages the car to drive faster to the learning goal of imitation.The performance of the policy is determined by the average distance. Therefore, a longer average rollout distance points to a better policy.In our ablation experiments, we remove parts of the proven optimization methods in Section 4. InfoGAIL (Ours) includes all optimization techniques; InfoGAIL\\ WGAN switches the WGAN target with the GAN target; InfoGAIL\\ RA removes the reward term augmentation enhancement from the target; InfoGAIL\\ Replay method significantly removes the most recent cyclone sample and only the worst one sample for the CRB buffer."}, {"heading": "6. Related work", "text": "There are two major paradigms for vision-based driving systems (Chen et al., 2015), whereas mediated perception is a two-step approach that first obtains scene information and then makes a driving decision (Aly, 2008; Lenz et al., 2011), whereas the behavioral reflex takes a direct approach by mapping visual entrances to driving actions (Pomerleau, 1989; 1991). Many of the current autonomous driving methods rely on the two-step approach that requires manual features such as recognition of road markings and cars (Geiger et al., 2013; Chen et al., 2015). Our approach is to learn these features through end-to-end training. While mediated perception approaches are currently more common, we believe that end-to-end-to-end learning methods are more scalable and may lead to better performance in the long run."}, {"heading": "7. Conclusion", "text": "In this paper, we present a method for mimicking complex behaviors while simultaneously identifying latent variation factors in human decision-making; discovering these latent factors does not require direct monitoring beyond expert demonstrations, and the entire process can be trained end-to-end using standard policy optimization algorithms; we also introduce several techniques to successfully perform end-to-end imitation acquisition using visual inputs, including transfer learning and reward multiplication; and our experimental results in the TORCS simulator clearly show that our methods can automatically distinguish certain behaviors in human driving while learning a policy that can mimic and even surpass expert behavior, with visual information being the only external input; and we hope that our work can inspire us to end-to-end learning approaches for autonomous driving under more realistic scenarios; another compelling direction for future work is to explore how to combine current perceptual approaches with perceptual ones."}, {"heading": "Acknowledgements", "text": "The Toyota Research Institute (TRI) has provided resources to assist the authors in their research, but this article reflects only the opinions and conclusions of its authors and not TRI or any other Toyota company."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "Abbeel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2004}, {"title": "Real time detection of lane markers in urban streets", "author": ["Aly", "Mohamed"], "venue": "In Intelligent Vehicles Symposium,", "citeRegEx": "Aly and Mohamed.,? \\Q2008\\E", "shortCiteRegEx": "Aly and Mohamed.", "year": 2008}, {"title": "Infinite time horizon maximum causal entropy inverse reinforcement learning", "author": ["Bloem", "Michael", "Bambos", "Nicholas"], "venue": "In Decision and Control (CDC),", "citeRegEx": "Bloem et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bloem et al\\.", "year": 2014}, {"title": "End to end learning for self-driving cars", "author": ["Bojarski", "Mariusz", "Del Testa", "Davide", "Dworakowski", "Daniel", "Firner", "Bernhard", "Flepp", "Beat", "Goyal", "Prasoon", "Jackel", "Lawrence D", "Monfort", "Mathew", "Muller", "Urs", "Zhang", "Jiakai"], "venue": "arXiv preprint arXiv:1604.07316,", "citeRegEx": "Bojarski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bojarski et al\\.", "year": 2016}, {"title": "Deepdriving: Learning affordance for direct perception in autonomous driving", "author": ["Chen", "Chenyi", "Seff", "Ari", "Kornhauser", "Alain", "Xiao", "Jianxiong"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Vision meets robotics: The kitti dataset", "author": ["Geiger", "Andreas", "Lenz", "Philip", "Stiller", "Christoph", "Urtasun", "Raquel"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Geiger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Geiger et al\\.", "year": 2013}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Generative adversarial imitation learning", "author": ["Ho", "Jonathan", "Ermon", "Stefano"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ho et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2016}, {"title": "Model-free imitation learning with policy optimization", "author": ["Ho", "Jonathan", "Gupta", "Jayesh K", "Ermon", "Stefano"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "Ho et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Imitating driver behavior with generative adversarial networks", "author": ["Kuefler", "Alex", "Morton", "Jeremy", "Wheeler", "Tim", "Kochenderfer", "Mykel"], "venue": "arXiv preprint arXiv:1701.06699,", "citeRegEx": "Kuefler et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kuefler et al\\.", "year": 2017}, {"title": "Sparse scene flow segmentation for moving object detection in urban environments", "author": ["Lenz", "Philip", "Ziegler", "Julius", "Geiger", "Andreas", "Roser", "Martin"], "venue": "In Intelligent Vehicles Symposium (IV), 2011 IEEE,", "citeRegEx": "Lenz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lenz et al\\.", "year": 2011}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Alvinn, an autonomous land vehicle in a neural network", "author": ["Pomerleau", "Dean A"], "venue": "Technical report,", "citeRegEx": "Pomerleau and A.,? \\Q1989\\E", "shortCiteRegEx": "Pomerleau and A.", "year": 1989}, {"title": "Efficient training of artificial neural networks for autonomous navigation", "author": ["Pomerleau", "Dean A"], "venue": "Neural Computation,", "citeRegEx": "Pomerleau and A.,? \\Q1991\\E", "shortCiteRegEx": "Pomerleau and A.", "year": 1991}, {"title": "Learning to search: Functional gradient techniques for imitation learning", "author": ["Ratliff", "Nathan D", "Silver", "David", "Bagnell", "J Andrew"], "venue": "Autonomous Robots,", "citeRegEx": "Ratliff et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2009}, {"title": "Efficient reductions for imitation learning", "author": ["Ross", "St\u00e9phane", "Bagnell", "Drew"], "venue": "In AISTATS,", "citeRegEx": "Ross et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2010}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross", "St\u00e9phane", "Gordon", "Geoffrey J", "Bagnell", "Drew"], "venue": "In AISTATS,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Prioritized experience replay", "author": ["Schaul", "Tom", "Quan", "John", "Antonoglou", "Ioannis", "Silver", "David"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael I", "Moritz", "Philipp"], "venue": "In ICML, pp", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["Schulman", "John", "Moritz", "Philipp", "Levine", "Sergey", "Jordan", "Michael", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1506.02438,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Unsupervised perceptual rewards for imitation learning", "author": ["Sermanet", "Pierre", "Xu", "Kelvin", "Levine", "Sergey"], "venue": "arXiv preprint arXiv:1612.06699,", "citeRegEx": "Sermanet et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2016}, {"title": "Cnn features off-theshelf: an astounding baseline for recognition", "author": ["Sharif Razavian", "Ali", "Azizpour", "Hossein", "Sullivan", "Josephine", "Carlsson", "Stefan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,", "citeRegEx": "Razavian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Razavian et al\\.", "year": 2014}, {"title": "Third person imitation learning", "author": ["Stadie", "Bradly", "Abbeel", "Pieter", "Sutskever", "Ilya"], "venue": "In ICLR,", "citeRegEx": "Stadie et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Stadie et al\\.", "year": 2017}, {"title": "Apprenticeship learning using linear programming", "author": ["Syed", "Umar", "Bowling", "Michael", "Schapire", "Robert E"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Syed et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Syed et al\\.", "year": 2008}, {"title": "Value iteration networks", "author": ["Tamar", "Aviv", "Levine", "Sergey", "Abbeel", "WU Pieter", "YI", "Thomas", "Garrett"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tamar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tamar et al\\.", "year": 2016}, {"title": "Lecture 6.5rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Tijmen", "Hinton", "Geoffrey"], "venue": "COURSERA: Neural networks for machine learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Torcs, the open racing car simulator. Software available at http://torcs", "author": ["Wymann", "Bernhard", "Espi\u00e9", "Eric", "Guionneau", "Christophe", "Dimitrakakis", "Christos", "Coulom", "R\u00e9mi", "Sumner", "Andrew"], "venue": "sourceforge. net,", "citeRegEx": "Wymann et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Wymann et al\\.", "year": 2000}, {"title": "How transferable are features in deep neural networks", "author": ["Yosinski", "Jason", "Clune", "Jeff", "Bengio", "Yoshua", "Lipson", "Hod"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Ziebart", "Brian D", "Maas", "Andrew L", "Bagnell", "J Andrew", "Dey", "Anind K"], "venue": "In AAAI,", "citeRegEx": "Ziebart et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 14, "context": "A key limitation of reinforcement learning (RL) is that it involves the optimization of a predefined reward function or reinforcement signal (Levine & Koltun, 2013; Schulman et al., 2015a; Lillicrap et al., 2015; Schulman et al., 2015b; Silver et al., 2016; Tamar et al., 2016).", "startOffset": 141, "endOffset": 277}, {"referenceID": 27, "context": "A key limitation of reinforcement learning (RL) is that it involves the optimization of a predefined reward function or reinforcement signal (Levine & Koltun, 2013; Schulman et al., 2015a; Lillicrap et al., 2015; Schulman et al., 2015b; Silver et al., 2016; Tamar et al., 2016).", "startOffset": 141, "endOffset": 277}, {"referenceID": 32, "context": "problems (Ziebart et al., 2008; Ratliff et al., 2009; Stadie et al., 2017).", "startOffset": 9, "endOffset": 74}, {"referenceID": 17, "context": "problems (Ziebart et al., 2008; Ratliff et al., 2009; Stadie et al., 2017).", "startOffset": 9, "endOffset": 74}, {"referenceID": 25, "context": "problems (Ziebart et al., 2008; Ratliff et al., 2009; Stadie et al., 2017).", "startOffset": 9, "endOffset": 74}, {"referenceID": 17, "context": ", 2008; Ratliff et al., 2009; Stadie et al., 2017). Among them, Generative Adversarial Imitation Learning (GAIL, Ho & Ermon (2016)) is a modelfree imitation learning method that is highly effective and scales to relatively high dimensional models.", "startOffset": 8, "endOffset": 131}, {"referenceID": 30, "context": "We demonstrate an application to autonomous highway driving using the TORCS driving simulator (Wymann et al., 2000).", "startOffset": 94, "endOffset": 115}, {"referenceID": 19, "context": "BC tends to have poor generalization properties due to compounding errors and covariate shift (Ross & Bagnell, 2010; Ross et al., 2011).", "startOffset": 94, "endOffset": 135}, {"referenceID": 26, "context": "AL, on the other hand, has the advantage of learning a reward function that can be used to score entire trajectories (Abbeel & Ng, 2004; Syed et al., 2008; Ho et al., 2016), but is typically expensive to run because it requires solving a reinforcement learning (RL) problem inside a learning loop.", "startOffset": 117, "endOffset": 172}, {"referenceID": 8, "context": "AL, on the other hand, has the advantage of learning a reward function that can be used to score entire trajectories (Abbeel & Ng, 2004; Syed et al., 2008; Ho et al., 2016), but is typically expensive to run because it requires solving a reinforcement learning (RL) problem inside a learning loop.", "startOffset": 117, "endOffset": 172}, {"referenceID": 6, "context": "In particular, Generative Adversarial Imitation Learning (GAIL, Ho & Ermon (2016) ) is a recent AL method inspired by Generative Adversarial Networks (GAN, Goodfellow et al. (2014)).", "startOffset": 156, "endOffset": 181}, {"referenceID": 21, "context": "Optimization over the GAIL objective is performed by alternating between an Adam (Kingma & Ba, 2014) gradient step on \u03c9 to increase V (\u03b8, \u03c9) with respect toD, and a Trust Region Policy Optimization (TRPO, Schulman et al. (2015a)) step on \u03b8 to decrease V (\u03b8, \u03c9) with respect to \u03c0.", "startOffset": 205, "endOffset": 229}, {"referenceID": 11, "context": "Convolutional neural networks (CNNs) have led to dramatic improvements across many computer vision tasks (Krizhevsky et al., 2012).", "startOffset": 105, "endOffset": 130}, {"referenceID": 31, "context": "Features extracted using a CNN pre-trained on ImageNet contain highlevel information about the input images, which can be adapted to new vision tasks via transfer learning (Yosinski et al., 2014).", "startOffset": 172, "endOffset": 195}, {"referenceID": 7, "context": "In particular, we use a Deep Residual Network (He et al., 2016) pre-trained on the ImageNet classification task (Russakovsky et al.", "startOffset": 46, "endOffset": 63}, {"referenceID": 20, "context": "Therefore, we apply several variance-reduction techniques, such as replay buffer (Schaul et al., 2015) and baseline methods (Williams, 1992).", "startOffset": 81, "endOffset": 102}, {"referenceID": 30, "context": "The Open Racing Car Simulator (TORCS, Wymann et al. (2000)) is a popular simulator environment for research in autonomous vehicles.", "startOffset": 38, "endOffset": 59}, {"referenceID": 4, "context": "There are two major paradigms for vision-based driving systems (Chen et al., 2015).", "startOffset": 63, "endOffset": 82}, {"referenceID": 13, "context": "Mediated perception is a two-step approach that first obtains scene information and then makes a driving decision (Aly, 2008; Lenz et al., 2011); behavior reflex, on the other hand, adopts a direct approach by mapping visual inputs to driving actions (Pomerleau, 1989; 1991).", "startOffset": 114, "endOffset": 144}, {"referenceID": 5, "context": "driving methods rely on the two-step approach, which requires hand-crafting features such as the detection of lane markings and cars (Geiger et al., 2013; Chen et al., 2015).", "startOffset": 133, "endOffset": 173}, {"referenceID": 4, "context": "driving methods rely on the two-step approach, which requires hand-crafting features such as the detection of lane markings and cars (Geiger et al., 2013; Chen et al., 2015).", "startOffset": 133, "endOffset": 173}, {"referenceID": 8, "context": "Most imitation learning methods for end-to-end driving rely heavily on LIDAR-like inputs to obtain precise distance measurements (Ho et al., 2016; Kuefler et al., 2017).", "startOffset": 129, "endOffset": 168}, {"referenceID": 12, "context": "Most imitation learning methods for end-to-end driving rely heavily on LIDAR-like inputs to obtain precise distance measurements (Ho et al., 2016; Kuefler et al., 2017).", "startOffset": 129, "endOffset": 168}, {"referenceID": 8, "context": "Most imitation learning methods for end-to-end driving rely heavily on LIDAR-like inputs to obtain precise distance measurements (Ho et al., 2016; Kuefler et al., 2017). These inputs are not usually available to humans during driving. In particular, Kuefler et al. (2017) applies GAIL to the task of modeling human driving behavior on highways.", "startOffset": 130, "endOffset": 272}, {"referenceID": 8, "context": "Most imitation learning methods for end-to-end driving rely heavily on LIDAR-like inputs to obtain precise distance measurements (Ho et al., 2016; Kuefler et al., 2017). These inputs are not usually available to humans during driving. In particular, Kuefler et al. (2017) applies GAIL to the task of modeling human driving behavior on highways. Their policy is modeled using a recurrent neural network, which is supposed to maintain sufficient statistics of the past observations. In contrast, our policy requires only raw visual information as external input, which in practice is all the information humans need in order to drive. Sermanet et al. (2016) have also introduced a pre-trained deep neural network to achieve better performance in imitation learning with relatively few demonstrations.", "startOffset": 130, "endOffset": 656}], "year": 2017, "abstractText": "The goal of imitation learning is to match example expert behavior, without access to a reinforcement signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are not explicitly modeled. We introduce an extension to the Generative Adversarial Imitation Learning method that can infer the latent structure of human decision-making in an unsupervised way. Our method can not only imitate complex behaviors, but also learn interpretable and meaningful representations. We demonstrate that the approach is applicable to high-dimensional environments including raw visual inputs. In the highway driving domain, we show that a model learned from demonstrations is able to both produce different styles of human-like driving behaviors and accurately anticipate human actions. Our method surpasses various baselines in terms of performance and functionality.", "creator": "LaTeX with hyperref package"}}}