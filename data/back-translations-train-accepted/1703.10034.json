{"id": "1703.10034", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "Probabilistic Line Searches for Stochastic Optimization", "abstract": "In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user-controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent.", "histories": [["v1", "Wed, 29 Mar 2017 13:43:52 GMT  (3736kb,D)", "https://arxiv.org/abs/1703.10034v1", "Extended version of the NIPS '15 conference paper, includes detailed pseudo-code, 51 pages, 30 figures"], ["v2", "Fri, 30 Jun 2017 16:18:08 GMT  (4490kb,D)", "http://arxiv.org/abs/1703.10034v2", "Extended version of the NIPS '15 conference paper, includes detailed pseudo-code, 59 pages, 35 figures"]], "COMMENTS": "Extended version of the NIPS '15 conference paper, includes detailed pseudo-code, 51 pages, 30 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["maren mahsereci", "philipp hennig"], "accepted": true, "id": "1703.10034"}, "pdf": {"name": "1703.10034.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Line Searches for Stochastic Optimization", "authors": ["Maren Mahsereci", "Philipp Hennig"], "emails": ["mmahsereci@tue.mpg.de", "phennig@tue.mpg.de"], "sections": [{"heading": null, "text": "Keywords: stochastic optimization, learning rates, line search, Gaussian processes, Bayesian optimization"}, {"heading": "1. Introduction", "text": "This work essentially extends to the work of Mahsereci and Hennig (2015), which were published at NIPS 2015 (2016). Stochastic gradient descent (sgd, Robbins and Monro, 1951) is currently the standard in machine learning for optimizing highly multivariate functions when their gradient is initially corrupted by noise, including online or mini-batch formation of neural networks, logistic regression (Zhang, 2004; Bottou, 2010) and variable models (e.g. Hoffman et al, 2013; Hensman et al, 2012; Broderick et al al al., 2013). In all of these cases, noisy gradients arise because a replaceable gradient function range L (x) is the optimization parameter x x x x x, across a large dataset {di} i = 1..., M, is evaluated only on a subset {j}, 1: L (x)."}, {"heading": "2. Connections", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Deterministic Line Searches", "text": "There are a variety of existing line search variants (Nocedal and Wright, 1999, \u00a7 3), but essentially these methods examine a univariate domain \"to the right\" of a starting point until an \"acceptable\" point is reached (Figure 1). Specifically, one looks at the problem of minimizing L (x): RD _ R, with access to L (x): RD _ RD. In iteration i, any \"outer loop\" at position xi selects a search direction si (e.g. according to the BFGS rule or simply si = \u2212 L (xi) for gradient descend. It is not assumed that si has a standard. The line search works along the univariate domain x (t) = xi + tsi for t-R +. Along this direction, it collects scalar function values and projected gradients that f (t) = L (t) and f (t).i = svall phases of a search for t + R."}, {"heading": "2.1.1 The Wolfe Conditions for Termination", "text": "The Wolfe Conditions (Wolfe, 1969) are a widely accepted formalization of this term; they consider it unacceptable if it is f (t) \u2264 f (0) + c1tf \"(0) (W-I) and f\" (t) \u2265 c2f \"(0) (W-II), (2) using two constants 0 \u2264 c1 < c2 \u2264 1 chosen by the line search designer, not the user. W-I is the armijo or sufficient decrease state (Armijo, 1966). It encodes that acceptable functional values below a linear extrapolation line of the slope line c1f\" (0) and the strong extrapolation line of the slope line c1f. \""}, {"heading": "2.2 Bayesian Optimization", "text": "A recently emerging sample-efficient approach to global optimization revolves around modeling object f with a probability measurement p (f); usually a Gaussian process (gp); in the search for extreme values, the evaluation points are then selected by a useful functional u [p (f)]. Our line search draws on the idea of a Gaussian process and a popular acquisition function, expected improvements (Jones et al., 1998). Bavarian optimization methods (bo) are often computationally expensive and therefore unsuitable for a cost-sensitive task such as line search. However, since line searches are more than information extractors, the kind of sample efficiency expected from a Bajesian optimizer is not required. In the following sections, a light algorithm is developed that adds little computational effort to stochastic optimization."}, {"heading": "3. A Probabilistic Line Search", "text": "We consider the minimization of f (t) = L (x (t)) by Eq. 1. That is, the algorithm can only access noisy function values and gradients yt, y (t) t at site t, with Gaussian likelihoodp (yt, y) t = N (yt y) t; [f) f \u2032 (t)], [\u03c32f 0 \u03c32f \u2032]). (3) The Gaussian form is assumed by the Central Limit argument at Eq. 1. The function value yt and the gradient y \u2032 t is assumed regardless of simplicity; see \u00a7 3.4 and Appendix A for the deviations \u03c32f \u2032, and some other references to the independence of the adoption of y and y optimization. Each assessment of f (t) uses a newly drawn mini-batch."}, {"heading": "3.1 Lightweight Gaussian Process Surrogate", "text": "We model information about the target in a probability measurement p (f). There are two requirements for such a measurement: First, it must be robust against irregularity (low and high variability) of the target."}, {"heading": "3.2 Choosing Among Candidates", "text": "In the previous section, the construction of < N + 1 discrete candidate points for the next evaluation was described. To decide which of the candidates should actually be called f and f \u2032, we use a popular acquisition function from Bayesian optimization. Expected improvement (Jones et al., 1998) is the expected amount among the gp surrogate sample by which the function f (t) could be smaller than a \"current best value\" \u03b7 (we set \u03b7 = mini = 0,..., N {\u00b5 (ti)}, where tiare contain observed locations), uEI (t) = Ep (ft | y, y \u2032) [min {0, \u03b7 \u2212 f (t)} = \u03b7 \u2212 \u00b5 (t) 2 (1 + erf\u03b7 \u2212 \u00b5 (t)) + \u221a V (t) 2\u03c0 exp (\u2212 t) = Ep (p), up (t)."}, {"heading": "3.3 Probabilistic Wolfe Conditions for Termination", "text": "The most important observation for a probable extension of the Wolfe conditions W-I and W-II is that they are positivity constraints on two variables at, bt, which are both linear projections of the (common Gaussian) variables f and f: [at bt] = [1 c1t \u2212 1 0 \u2212 c2 0 \u2212 1] f (0) f (t) f (t) p (t) 0. (10) The gp of Equation (5) to f thus implies for each value of t, a bivariate Gaussian distribution (at, bt) = N (at) f (t) f (t)), [mbt], [Caat C ab t Cbat C bb]))))), (11) to f (t), a bivariate Gaussian distribution (at) to (at)."}, {"heading": "3.3.1 Approximation for Strong Conditions:", "text": "As mentioned in Section 2.1.1, deterministic optimizers tend to use the strong Wolfe conditions that use | f \"(0) | and | f\" (t) |. Extending these conditions precisely to the probability setting is numerically burdensome, since the distribution via | f \"| is a non-central distribution that requires custom calculations. However, a simple variation to 14 captures the spirit of strong Wolfe conditions that large positive derivatives should not be accepted: (15) The value \u2212 2c2f\" (0) is limited to 95% of confidence by \u2212 2c2f \"(0) \u2212 c2f\" (0) \u2212 2c2f \"(0) \u2212 2c2f\" (0)."}, {"heading": "3.4 Eliminating Hyper-parameters", "text": "The previous section introduced six previously undefined parameters: c1, c2, c3, c4, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5, c5,"}, {"heading": "3.4.2 Scale \u03b8", "text": "The parameter \u03b8 of Eq.4 merely scales the previous variance. It can be eliminated by scaling the optimization target: We set \u03b8 = 1 and scale yi ^ (yi \u2212 y0) / | y \u2032 0 |, y \u2032 i ^ y \u2032 i / | y \u2032 0 | within the line search code. This yields y (0) = 0 and y \u2032 (0) = \u2212 1 and typically ensures that the lens ranges in the single digits are above 0 < t < t < 10, where most line searches take place."}, {"heading": "3.4.3 Noise Scales \u03c3f , \u03c3f \u2032", "text": "Probability 3 requires standard deviations for noise for both function values (\u03c3f) and gradients (\u03c3f), which could be learned over several line searches. However, in interchangeable models such as those covered by Eq. 1, the variance of loss and its gradient can be estimated directly for the mini-batch at low cost - an approach already advocated by Schaul et al. (2013). We collect the empirical statistical estimate S (x): = 1 m \u00b2 j '2 (x, yj) and estimate at the beginning of a line search of xk, \u03c32f = 1 \u2212 1 (xk) \u2212 1 (xk) \u2212 2 (where 2 denotes the element-wise square) and the estimate at the beginning of a line search of xk, \u03c32f = 1 \u2212 1 (xk) \u2212 1 (xk) \u2212 2)."}, {"heading": "3.4.4 Propagating Step Sizes Between Line Searches", "text": "As shown in paragraph 4, the line search can find good step sizes even if the length of the direction si is incorrectly scaled. Since such scale problems typically persist over time, it would be wasteful to have the algorithm switch to a good scale in each line search. Instead, we propagate step lengths from one iteration of the search to the next: We set the initial search direction to s0 = - \u03b10 - L (x0) with an initial learning rate \u03b10. Then, after each line search ending with xi = xi \u2212 1 + t \u0445 si, the next search direction is set to si + 1 = \u2212 \u03b1ext \u00b7 t \u0445 \u03b10 - L (xi) (with an extrapolation = 1.3). Thus, the next line search begins its extrapolation with 1.3 times the step size of its predecessor (Section 4.2.2 for details)."}, {"heading": "3.5 Relation to Bayesian Optimization and Noise-Free Limit", "text": "The probable line search algorithm is closely related to Bayesian optimization (bo), since it roughly minimizes a 1D target among potentially noisy function evaluations, so it uses terms like bo (e.g. a Gp surrogate for the target, and an acquisition function to distinguish locations for the next assessment of the loss), but there are some differences in goal, computing power requirements, and termination status that are briefly discussed here. (i) Performance metric in bo is usually the lowest value of the objective function found. Line searches are subroutines within a greedy, iterative optimization engine that typically performs several thousand steps (and line searches); many very safe steps often perform better than fewer, but more precise steps than the term capture. (ii) Termination: The termination state of a line search is imposed from the outside in the form of wolf conditions."}, {"heading": "3.6 Computational Time Overhead", "text": "The line scanner itself has little memory and time, but most importantly it is independent of the dimensionality of the optimization problem. After each call to the objective function, the gp (\u00a7 3.1) must be updated, at most at the expense of reversing a 2N \u00d7 2N matrix, where N is usually equal to 1, 2 or 3, but never > 10. In addition, the bivariate normal integral pWolfet of Eq. 14 must be calculated no more than N times. On a laptop, a pWolfet evaluation costs about 100 microseconds. For the selection among the proposed candidates (\u00a7 3.2), we must again evaluate no more than N for each call pWolfet and uEI (Eq. 9), the latter at the expense of evaluating two error functions. Since all these calculations have fixed costs (a total of a few milliseconds on a laptop), the relative millihead will result in the more expensive evaluation (Eq. 9), with the latter resulting in the total cost of all these laptop evaluation functions (some miniature functions)."}, {"heading": "3.7 Memory Requirement", "text": "Vanilla sgd always adheres to the current optimization parameters x-RD and the gradient vector vector RD-RD. In addition, the probabilistic line search must store the estimated gradient deviations \u03a3 (x) = (1 \u2212 m) \u2212 1 (\u043e\u043d\u044bS (x) \u2212 \u0432 L (x) 2) (Eq. 18) of the same size. Therefore, the memory requirement of sgd + probLS is comparable to AdaGrad or Adam. In combination with a search direction other than sgd, an additional vector of size D must always be stored."}, {"heading": "4. Experiments", "text": "The totality of findings from these tests is that the search works well and that it is relatively insensitive to the choice of its internal hyperparameters. \u2022 N-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O, O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O, O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O, O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O, O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O, O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O"}, {"heading": "4.1 Varying Mini-batch Sizes", "text": "The noise level of the WDBC Gradient Estimate consists of only 400 data sets that we discussed with the WDBC; the noise level of the WDBC Gradient Estimate is determined only by 400 data sets that depend on the mini batch rate; we compare the cost of line searching with the standard hyperparameter setting (see Sections 3.4 and 4.2) on four different mini batch sizes: \u2022 m = 10, 100 and 1000 (for MNIST, CIFAR-10 and EPSILON) \u2022 m = 10, 50, 100 and 400 (for WDBC and GISETTE) that correspond to increasing signal-to-to-noise ratios."}, {"heading": "4.2 Sensitivity to Design Parameters", "text": "Most, if not all, numerical methods make implicit or explicit decisions about their hyperparameters, most of which are never seen by the user, since they are either estimated at runtime or set by design to a fixed, approximately insensitive value. Well-known examples are the discount factor in ordinary differential equation solvers (Hairer et al., 1987, \u00a7 2.4) or the Wolfe parameters c1 and c2 of the classic line search (\u00a7 3.4.1). The probabilistic line search inherits the Wolfe parameters c1 and c2 from their classical counterpart and introduces two more: the Wolfe threshold cW and the extrapolation factor \u03b1ext. cW does not occur in the classical formulation, because the objective function can be evaluated accurately and the Wolfe probability is binary (either fulfilled or not). Thus, while cW is a natural consequence of enabling the line search to explicitly model noise, the extrapolation factor will be prominently the preferred result of the extrapolation factor."}, {"heading": "4.2.1 Wolfe II Parameter c2 and Wolfe Threshold cW", "text": "As described in Section 3.4, c2 encodes the severity of the curvature condition W-II. Figuratively speaking, a larger c2 widens the range of acceptable gradients (shaded green in the lower part of Figure 5) and leads to a lenient line search, while a smaller value of c2 shrinks this range, leading to a stricter line search. cW controls how certain we want to be that the Wolfe conditions are actually met. In the extreme case of complete uncertainty about the collected gradients and functional values (\u03c3f \u2032) pWolfe is always < 0.25 when the strong wolf conditions are imposed, is that the limits of certain observations are reached."}, {"heading": "4.2.2 Extrapolation Factor \u03b1ext", "text": "The extrapolation parameter \u03b1ext, introduced in Section 3.4.4, urges the line search to first try a higher learning rate than the one accepted in the previous step. Figure 9 is structured like Figure 10, but this time the line search sensitivity is examined in the c2 \u03b1ext parameter space (abscissa and lines respectively), while cW is fixed at 0.3. Unless we opt for \u03b1ext = 1.0 (no increment between steps) in combination with a yielding choice of c2, the line search works well. For now we take \u03b1ext = 1.3 as the default value, which in turn is a dark red vertical line in Figure 9.The introduction of \u03b1ext is a necessity and a well-functioning solution due to some shortcomings of the current design. First, the curvature condition W-II is the only condition that prevents too small steps and pushes optimization progress.On the other hand, both W-I and W-II are disadvantaged at the same time (see large steps)."}, {"heading": "4.2.3 Full Hyper-Parameter Search: cW -c2-\u03b1ext", "text": "A comprehensive performance evaluation of the entire cW-c2-\u03b1ext grid is in Appendix C in Figures 20-24 and 25-35. As discussed above, it demonstrates the need to introduce the extrapolation parameter \u03b1ext and shows slightly less efficient performance in apparently undesirable parameter combinations. In a large area of parameter space, and especially near the chosen design parameters, line search performance is stable and comparable to carefully hand-tuned learning rates."}, {"heading": "4.2.4 Safeguarding Mis-scaled gps: \u03b8reset", "text": "For the sake of completeness, an additional experiment has been carried out with the threshold value referred to in the pseudo-code (Appendix D) as a bail reset, which provides protection against Gp misscaling. Introduction of noisy observations requires modelling the variability of the 1D function, which is described by the kernel scaling parameter \u03b8 (\u00a7 3.4.2).Setting this hyper parameter is implicitly done by scaling the observation input using a scale similar to the previous line search (\u00a7 3.4.2).If for some reason the previous line search has accepted an unexpectedly large or small step (which means it is encoded in the bailreset), the Gp scale for the next line search will be reset to an exponential run average of previous scales (standstill in the pseudo-code).This is very rare (for the default value Bailreset = 100, the restart occurs in all line searches, 0.02%, but is necessary to avoid a partial or an error)."}, {"heading": "4.3 Candidate Selection and Learning Rate Traces", "text": "In the current implementation of probabilistic line search, the selection among applicants for evaluation is made by evaluating an acquisition function uEI (t cand i) \u00b7 pWolfe (tcandi) at each applicant point tcandi; then the selection of the candidate with the highest value for evaluating the object appears (\u00a7 3.2). The Wolfe probability pWolfe actually encodes exactly what kind of point we want to find and take into account both (W-I and W-II) conditions regarding the functional value and gradient (\u00a7 3.3). However, pWolfe does not have very desirable exploration properties. As the uncertainty of the gp grows to \"the right\" of the last observation, the Wolfe probability quickly drops to a low, approximately constant value there (Figure 4). AlsopWolfe is partially allowing us for undesirable short steps (\u00a7 4.2.2). The expected improvement uEI, on the other hand, is a well-studied acquisition function of Bayesisian."}, {"heading": "5. Conclusion", "text": "The line search paradigm, widely accepted in deterministic optimization, can be extended to noisy environments, and our design combines existing principles from noise-free fall with Bayesian optimization ideas that focus on efficiency. We have arrived at a lightweight black box algorithm that does not disclose any parameters to the user. Empirical evaluations to date have shown compatibility with the sgd search direction and feasibility for logistical regression and multi-layered perceptions. Line search effectively frees the user from worries about choosing a learning rate: any reasonable first choice is quickly adapted and results in near-optimal performance. Our Matlab implementation can be found at http: / / tinyurl.com / probLineSearch."}, {"heading": "Acknowledgments", "text": "Thanks to Jonas Jaszkowic, who prepared the basis of the pseudo-code."}, {"heading": "Appendix A. \u2013 Noise Estimation", "text": "The basic assumption is that L (x) and L (x) after [L (x)) and L (x) after [L (x) and L (x) after [L), (x) and L (x) after [L), (x) and L (x) after [L), (x) and L (x) after [L), (x), (x) and L (x) after [L), (x) and L (x) after [L), (20) after [L (x) and (x) after [L)."}, {"heading": "Appendix D. \u2013 Pseudocode", "text": "It is based on the code used for the experiments in this essay. A Matlab implementation that contains a minimal example can be found at http: / / tinyurl.com / probLineSearch. The actual line search routine is referred to here as probLineSearch and is quite short. To improve the readability of the pseudo-code, we use the following color coding: \u2022 blue: comments \u2022 green: variables of the integrated Vienna process \u2022 red: most recently evaluated observations (noisy loss and gradient)."}], "references": [{"title": "Adaptive method of realizing natural gradient learning", "author": ["R.J. Adler"], "venue": "The Geometry of Random Fields. Wiley,", "citeRegEx": "Adler.,? \\Q1981\\E", "shortCiteRegEx": "Adler.", "year": 1981}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "e-prints,", "citeRegEx": "Bottou.,? \\Q2016\\E", "shortCiteRegEx": "Bottou.", "year": 2016}, {"title": "Result analysis of the nips 2003 feature selection", "author": ["deeplearningbook.org. I. Guyon", "S. Gunn", "A. Ben-Hur", "G.n Dror"], "venue": null, "citeRegEx": "Guyon et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Guyon et al\\.", "year": 2003}, {"title": "Learning multiple layers of features from tiny", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "Neural Information Processing Systems", "citeRegEx": "Martens.,? \\Q2015\\E", "shortCiteRegEx": "Martens.", "year": 2015}, {"title": "Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design", "author": ["N. Srinivas", "A. Krause", "S. Kakade", "M. Seeger"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Srinivas et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2010}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML-13),", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Spline models for observational data. Number 59 in CBMS-NSF Regional Conferences series in applied mathematics", "author": ["G. Wahba"], "venue": null, "citeRegEx": "Wahba.,? \\Q1990\\E", "shortCiteRegEx": "Wahba.", "year": 1990}, {"title": "UCI Machine Learning Repository: Breast Cancer Wisconsin (Diagnostic) Data Set, January 2011. URL http://archive.ics.uci.edu/ ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)", "author": ["W.H. Wolberg", "W.N Street", "O.L. Mangasarian"], "venue": null, "citeRegEx": "Wolberg et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wolberg et al\\.", "year": 2011}, {"title": "Convergence conditions for ascent methods", "author": ["P. Wolfe"], "venue": "SIAM Review, pages 226\u2013235,", "citeRegEx": "Wolfe.,? \\Q1969\\E", "shortCiteRegEx": "Wolfe.", "year": 1969}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["T. Zhang"], "venue": "In Twenty-first International Conference on Machine Learning", "citeRegEx": "Zhang.,? \\Q2004\\E", "shortCiteRegEx": "Zhang.", "year": 2004}], "referenceMentions": [{"referenceID": 10, "context": "This includes the online or mini-batch training of neural networks, logistic regression (Zhang, 2004; Bottou, 2010) and variational models (e.", "startOffset": 88, "endOffset": 115}, {"referenceID": 9, "context": "The Wolfe conditions (Wolfe, 1969) are a widely accepted formalization of this notion; they consider t acceptable if it fulfills f(t) \u2264 f(0) + c1tf \u2032(0) (W-I) and f \u2032(t) \u2265 c2f \u2032(0) (W-II), (2) using two constants 0 \u2264 c1 < c2 \u2264 1 chosen by the designer of the line search, not the user.", "startOffset": 21, "endOffset": 34}, {"referenceID": 7, "context": "3, this prior gives rise to a gp posterior whose mean function is a cubic spline3 (Wahba, 1990).", "startOffset": 82, "endOffset": 95}, {"referenceID": 5, "context": "the upper-confidence bound, gp-ucb (Srinivas et al., 2010)) are possible, which might have a stronger explorative behavior; we opted for uEI since exploration is less crucial for line searches than for general bo and some (e.", "startOffset": 35, "endOffset": 58}, {"referenceID": 9, "context": "To motivate this value, first note that in the noise-free limit, all values 0 < cW < 1 are equivalent, because p Wolfe then switches discretely between 0 and 1 upon observation of the function. A back-of-the-envelope computation, assuming only two evaluations at t = 0 and t = t1 and the same fixed noise level on f and f \u2032 (which then cancels out), shows that function values barely fulfilling the conditions, i.e. at1 = bt1 = 0, can have pWolfe \u223c 0.2 while function values at at1 = bt1 = \u2212 for _ 0 with \u2018unlucky\u2019 evaluations (both function and gradient values one standard-deviation from true value) can achieve pWolfe \u223c 0.4. The choice cW = 0.3 balances the two competing desiderata for precision and recall. Empirically (Fig. 6), we rarely observed values of pWolfe close to this threshold. Even at high evaluation noise, a function evaluation typically either clearly rules out the Wolfe conditions, or lifts pWolfe well above the threshold. A more in-depth analysis of c1, c2, and cW is done in the experimental Section 4.2.1. 3.4.2 Scale \u03b8 The parameter \u03b8 of Eq. 4 simply scales the prior variance. It can be eliminated by scaling the optimization objective: We set \u03b8 = 1 and scale yi ^ (yi\u2212y0)/|y\u2032 0|, y\u2032 i ^ y\u2032 i/|y\u2032 0| within the code of the line search. This gives y(0) = 0 and y\u2032(0) = \u22121, and typically ensures the objective ranges in the single digits across 0 < t < 10, where most line searches take place. The division by |y\u2032 0| causes a non-Gaussian disturbance, but this does not seem to have notable empirical effect. 3.4.3 Noise Scales \u03c3f , \u03c3f \u2032 The likelihood 3 requires standard deviations for the noise on both function values (\u03c3f ) and gradients (\u03c3f \u2032). One could attempt to learn these across several line searches. However, in exchangeable models, as captured by Eq. 1, the variance of the loss and its gradient can be estimated directly for the mini-batch, at low computational overhead\u2014an approach already advocated by Schaul et al. (2013). We collect the empirical statistics", "startOffset": 113, "endOffset": 1967}, {"referenceID": 3, "context": "\u2022 CIFAR-10 (Krizhevsky and Hinton, 2009): multi-class classification task with 10 classes: color images of natural objects (horse, dog, frog,.", "startOffset": 11, "endOffset": 40}, {"referenceID": 8, "context": "In addition we train logistic regressors with sigmoidal output (N-III) on the following binary classification tasks: \u2022 Wisconsin Breast Cancer Dataset (WDBC) (Wolberg et al., 2011): binary classification of tumors as either \u2018malignant\u2019 or \u2018benign\u2019.", "startOffset": 158, "endOffset": 180}, {"referenceID": 3, "context": "Similar nets were also used for example in Martens (2010) and Sutskever et al.", "startOffset": 43, "endOffset": 58}, {"referenceID": 3, "context": "Similar nets were also used for example in Martens (2010) and Sutskever et al. (2013). \u2022 MNIST (LeCun et al.", "startOffset": 43, "endOffset": 86}, {"referenceID": 9, "context": "4), or the Wolfe parameters c1 and c2 of classic line searches (\u00a73.4.1). The probabilistic line search inherits the Wolfe parameters c1 and c2 from its classical counterpart as well as introducing two more: The Wolfe threshold cW and the extrapolation factor \u03b1ext. cW does not appear in the classical formulation since the objective function can be evaluated exactly and the Wolfe probability is binary (either fulfilled or not). While cW is thus a natural consequence of allowing the line search to model noise explicitly, the extrapolation factor \u03b1ext is the result of the line search favoring shorter steps, which we will discuss below in more detail, but most prominently because of bias in the line search\u2019s first gradient observation. In the following sections we will give an intuition about the task of the most influential design parameters c2, cW , and \u03b1ext, discuss how they affect the probabilistic line search, and validate good design choices through exploring the parameter space and showing insensitivity to most of them. All experiments on hyper-parameter sensitivity were performed training N-II on MNIST with mini-batch size m = 200. For a full search of the parameter space cW -c2-\u03b1ext we performed 4950 runs in total with 495 different parameter combinations. All results are reported. 4.2.1 Wolfe II Parameter c2 and Wolfe Threshold cW As described in Section 3.4, c2 encodes the strictness of the curvature condition W-II. Pictorially speaking, a larger c2 extends the range of acceptable gradients (green shaded are in the lower part of Figure 5) and leads to a lenient line search while a smaller value of c2 shrinks this area, leading to a stricter line search. cW controls how certain we want to be, that the Wolfe conditions are actually fulfilled. In the extreme case of complete uncertainty about the collected gradients and function values (\u03c3f , \u03c3f \u2032 \u2192\u221e) pWolfe will always be < 0.25, if the strong Wolfe conditions are imposed. In the limit of certain observations (\u03c3f , \u03c3f \u2032 \u2192 0) 10. An example of annealed step size performance can be found in Mahsereci and Hennig (2015).", "startOffset": 11, "endOffset": 2106}, {"referenceID": 9, "context": "pWolfe is partially allowing for undesirably short steps (\u00a74.2.2). The expected improvement uEI, on the other hand, is a well studied acquisition function of Bayesian optimization trading off exploration and exploitation. It aims to globally find a point with a function value lower than a current best guess. Though this is a desirable property also for the probabilistic line search, it is lacking the information that we are seeking a point that also fulfills the W-II curvature condition. This is evident in Figure 4 where pWolfe significantly drops at points where the objective function is already evaluated but uEI does not. In addition, we do not need to explore the positive t space to an extend, the expected improvement suggests, since the aim of a line search is just to find a good, acceptable point at positive t and not the globally best one. The product of both acquisition function uEI \u00b7 pWolfe is thus a trade-off between exploring enough, but still preventing too much exploitation in obviously undesirable regions. In practice, though, we found that all three choices ((i) uEI \u00b7 pWolfe, (ii) uEI only, (iii) p Wolfe only) perform comparable. The following experiments were all performed training N-II on MNIST; only the minibatch size might vary as indicated. Figure 11 compares all three choices for mini-batch size m = 200 and default design parameters. The top plot shows the evolution of the logarithmic test and train set error (for plot and color description see Figure caption). All test and train set error curves respectively bundle up (only lastly plotted clearly visible). The choice of acquisition function thus does not change the performance here. Rows 2-4 of Figure 11 show learning rate traces of a single seed. All three curves show very similar global behavior. First the learning rate grows, then drops again, and finally settles around the best found constant learning rate. This is intriguing since on average a larger learning rate seems to be better at the beginning of the optimization process, then later dropping again to a smaller one. This might also explain why sgd+probLS in the first part of the optimization progress outperforms vanilla sgd (Figure 7). Runs, that use just slightly larger constant learning rates than the best performing constant one (above the gray horizontal lines in Figure 11) were failing after a few steps. This shows that there is some non-trivial adaptation going on, not just globally, but locally at every step. Figure 12 shows traces of accepted learning rates for different mini-batch sizes m = 100, 200, 1000. Again the global behavior is qualitatively similar for all three mini-batch sizes on the given architecture. For the largest mini-batch size m = 1000 (last row of Figure 12) the probabilistic line search accepts a larger learning rate (on average and in absolute value) than for the smaller mini-batch sizes m = 100 and 200, which is in agreement with practical experience and theoretical findings (Hinton (2012, \u00a74 and 7), Goodfellow et al. (2016, \u00a79.1.3), Balles et al. (2016)).", "startOffset": 1, "endOffset": 3071}], "year": 2017, "abstractText": "In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user-controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent.", "creator": "LaTeX with hyperref package"}}}