{"id": "1703.10931", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2017", "title": "Sentence Simplification with Deep Reinforcement Learning", "abstract": "Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model brings significant improvements over the state of the art.", "histories": [["v1", "Fri, 31 Mar 2017 15:05:45 GMT  (38kb,D)", "http://arxiv.org/abs/1703.10931v1", null], ["v2", "Sun, 16 Jul 2017 02:28:14 GMT  (45kb,D)", "http://arxiv.org/abs/1703.10931v2", "to appear in EMNLP 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["xingxing zhang", "mirella lapata"], "accepted": true, "id": "1703.10931"}, "pdf": {"name": "1703.10931.pdf", "metadata": {"source": "CRF", "title": "Sentence Simplification with Deep Reinforcement Learning", "authors": ["Xingxing Zhang", "Mirella Lapata"], "emails": ["x.zhang@ed.ac.uk", "mlap@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "Most recent approaches rely on machine translation insights to learn simplifications from monolingual corpora of complex and simple sentences. We solve the simplification problem using an encoder decoder model coupled with a deep reinforcement learning framework. Our model explores the space for possible simplification while learning to optimize a reward function that encourages simple, fluent results and preserves the importance of input. Experiments on three sets of data show that our model brings significant improvements over the level of Art. 1."}, {"heading": "1 Introduction", "text": "The idea behind it is that people who stand up for people's rights put themselves and their rights and benefits at the centre. (...) The idea behind it is that people who stand up for people's rights stand up against people's rights. (...) The idea behind it is that people who stand up for people's rights stand up against people's rights. (...) The idea behind it is that people who stand up for people's rights stand up for people's rights. (...) The idea behind it is that people who stand up for people's rights stand up for people's rights. \"(...) The idea behind it is that people who stand up for people's rights stand up for people's rights.\" (...) The idea behind it is that people who stand up for people's rights stand up for people's rights. \"(...) The idea behind it is that people who stand up for people's rights stand up for people's rights."}, {"heading": "2 Attention-based Encoder-Decoder Model", "text": "Considering a (complex) source set X = (x1, x2,., x | X |), our model learns to predict its simplified target Y = (y1, y2,.) (y1, y2,.). Inferring from this target Y is a typical sequence of sequence learning problems that can be modelled using attention-based encoder decoder models (1,.). Sentence simplification is slightly different from related sequence creation tasks (e.g., compression) in which it can involve splitting operations. For example, a long source set (because he had to work at night to support his family, Paco often fell asleep in class.) can be simplified as several shorter sentences (Paco had to earn money for his family. Paco worked at night."}, {"heading": "3 Reinforcement Learning for Sentence Simplification", "text": "Despite successful application in numerous sequence transduction tasks (Jean et al., 2015; Chopra et al., 2016; Xu et al., 2015a), a vanilla encoder decoder model is not ideal for simplifying the sentence. Although a number of paraphrase operations (e.g. copying, deleting, replacing, reordering words) can be used to simplify text, copying is by far the most common. We have found empirically that 73% of the target words are copied from the source in the Newsela dataset, a figure that increases further to 83% when looking at Wikipedia-based datasets (we provide details of these datasets in Section 4.1). As a result, a generic encoder decoder model would learn to copy only too well at the expense of other paraphrase operations, which often parrots the source or makes few changes."}, {"heading": "3.1 Reward", "text": "In fact, we see ourselves in a position to find a solution that is capable, in which it is able, in which it is able to move."}, {"heading": "3.2 The REINFORCE Algorithm", "text": "The minimum goal of the REINFORCE algorithm is to find an agent that maximizes the expected reward. Loss of training for a sequence is the negative expected reward: L (\u03b8) = \u2212 E (y-1,..., y-2, Y-2) [r (y-1,..., y-1,..] (11), where PRL is our policy, i.e. the distribution generated by the encoder decoder model (see Equation (2) and r (\u00b7) is the reward function of an action sequence Y = (y-1,..), i.e. a generated simplification. Unfortunately, the calculation of the expectation concept is prohibitive as there is an unlimited number of possible action sequences. In practice, we approach this expectation with a single sample from the distribution of actions implemented by the encoder decoder decoder models."}, {"heading": "3.3 Learning", "text": "Presented in its original form, the REINFORCE algorithm begins to learn with a random approach, which can make model training challenging for generational tasks such as ours with large vocabulary (i.e. scope for action). We address this problem by pre-training our agent (i.e. the encoder model) with a negative log probability target (see Section 2) to ensure that it can produce reasonable simplifications, starting with a procedure that is better than random. We follow the previous work (Ranzato et al., 2016) in introducing a curriculum learning strategy. At the beginning of the training, we give our agent little freedom by allowing him to predict the last words for each target sentence. For each sequence, we use negative log probability to train the first L characters (originally L = 24) and apply the reinforcement learning algorithm to the (L + 1) th characters. After each two training periods, we set L = 3 and end L = 3."}, {"heading": "3.4 Lexical Simplification", "text": "The lexical substitution, the replacement of complex words with simpler alternatives, is an integral part of sentence simplification. Although very intuitive, the task is difficult as the substitution must take place in context, simplifying both the original meaning and grammar of the sentence. The simplification model presented so far only learns lexical substitutions indirectly. More importantly, there is no guarantee that lexical simplifications, when they occur, will be meaningless; words can often be replaced by substitutions that appear natural in their context but do not reflect the content of the source. In this section, we propose a model that explicitly learns lexical simplifications and discusses how they can be integrated with the sentence simplification framework presented in the previous sections."}, {"heading": "4 Experimental Setup", "text": "In this section, we present our experimental setup to evaluate the performance of the simplification model described above. We provide details of the data sets we use, the model training, the evaluation protocol, and the systems used to compare with our approach."}, {"heading": "4.1 Datasets", "text": "To compare the state of the art and assess whether our model performs well across the board, we conducted experiments on three existing simplification datasets, which we describe below. WikiSmall This parallel corpus was developed by Zhu et al. (2010) It automatically contains complex and simple sentences from the ordinary English Wikipedia and its simple version. The test set consists of 100 complex and simple sentence pairs. The training set contains 89.042 sentence pairs (after removing duplicates and sentences in the test set). It automatically contains complex and simple sentences from the ordinary English Wikipedia and its simple version. The test set consists of 100 complex and simple sentence pairs. The training set contains 89.042 sentence pairs and sentences in the test set."}, {"heading": "4.2 Training Details", "text": "We trained all our models on an Nvidia GPU card with 4G RAM. We used the same hyperparameters across datasets. We first trained an encoder decoder model and then did a fastening training (Section 3). Finally, we trained the lexical simplification model (Section 3.4). Encoder decoder parameters were uniformly initialized to [\u2212 0.1,0.1]. We used Adam (Kingma and Ba, 2014) to optimize the model with learning rate 0.001; the first dynamic coefficient was set to 0.9 and the second dynamic coefficient to 0.999. Gradient was recalibrated when Standard 5 (Pascanu et al., 2013) was exceeded. Both the encoder and the decoder LSTMs have two layers with 256 hidden neurons in each layer. We regulated all LSTMs with a drop rate of 0.001 (we were trained in 2014, 0.001)."}, {"heading": "4.3 Evaluation", "text": "After previous work (Woodsend and Lapata, 2011; Xu et al., 2016), we evaluated the system output automatically. To measure the readability of the output, we used BLEU4 (Papineni et al., 2002) to assess the degree of simplification generated from the gold standard references and the Flesch-Kincaid Grade Level index5 (FKGL) (lower FKGL implies simpler output) 6. In addition, we used SARI (Xu et al., 2016), which evaluates the quality of the output by comparing it with the source and the references. We also evaluated the simplifications generated by evoking human judgments. Specifically, we invited 26 commentators (all native English speakers themselves) to evaluate the output of our systems (and comparison systems), asking them to rate the simplifications on three levels: grammaticality (is the output grammatical and well-designed? does it retain the meaning of the fifth edition?)"}, {"heading": "5 Results", "text": "This year it is more than ever before in the history of the city."}, {"heading": "6 Conclusions", "text": "In this thesis, we developed a text simplification model based on deep reinforcement learning that can jointly model simplicity, grammar and semantic fidelity to input. We also proposed a lexical simplification model that further improves performance. Overall, we find that reinforcement learning is a great way to bring prior knowledge to the simplification task in order to achieve state-of-the-art results across three sets of data (see Table 2). In the future, we would like to explicitly integrate sentence division into our model. We are also interested in simplifying entire documents (not individual sentences)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR 2015.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Text simplification for informationseeking applications", "author": ["Beata Beigman Klebanov", "Kevin Knight", "Daniel Marcu."], "venue": "Proceedings of ODBASE. Springer, Agia Napa, Cyprus, volume 3290 of Lecture Notes in Computer Science, pages 735\u2013747.", "citeRegEx": "Klebanov et al\\.,? 2004", "shortCiteRegEx": "Klebanov et al\\.", "year": 2004}, {"title": "Simplifying text for languageimpaired readers", "author": ["J. Carroll", "G. Minnen", "D. Pearce", "Y. Canning", "S. Devlin", "J Tait."], "venue": "Proceedings of the 9th EACL. Bergen, Norway, pages 269\u2013270.", "citeRegEx": "Carroll et al\\.,? 1999", "shortCiteRegEx": "Carroll et al\\.", "year": 1999}, {"title": "Motivations and methods for text simplification", "author": ["R. Chandrasekar", "C. Doran", "B. Srinivas."], "venue": "Proceedings of the 16th COLING. Copenhagen, Denmark, pages 1041\u20131044.", "citeRegEx": "Chandrasekar et al\\.,? 1996", "shortCiteRegEx": "Chandrasekar et al\\.", "year": 1996}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "author": ["Sumit Chopra", "Michael Auli", "Alexander M. Rush."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Com-", "citeRegEx": "Chopra et al\\.,? 2016", "shortCiteRegEx": "Chopra et al\\.", "year": 2016}, {"title": "Linguistically motivated large-scale nlp with c&c and boxer", "author": ["James Curran", "Stephen Clark", "Johan Bos."], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and", "citeRegEx": "Curran et al\\.,? 2007", "shortCiteRegEx": "Curran et al\\.", "year": 2007}, {"title": "Semi-supervised sequence learning", "author": ["Andrew M Dai", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems. pages 3079\u20133087.", "citeRegEx": "Dai and Le.,? 2015", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Simplifying Natural Language for Aphasic Readers", "author": ["Siobhan Devlin."], "venue": "Ph.D. thesis, University of Sunderland.", "citeRegEx": "Devlin.,? 1999", "shortCiteRegEx": "Devlin.", "year": 1999}, {"title": "An evaluation of syntactic simplification rules for people with autism", "author": ["Richard Evans", "Constantin Or asan", "Iustin Dornescu."], "venue": "Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR).", "citeRegEx": "Evans et al\\.,? 2014", "shortCiteRegEx": "Evans et al\\.", "year": 2014}, {"title": "PPDB: The paraphrase database", "author": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch."], "venue": "Proceedings of NAACLHLT . Association for Computational Linguistics, Atlanta, Georgia, pages 758\u2013764.", "citeRegEx": "Ganitkevitch et al\\.,? 2013", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Text simplification for reading assistance: A project note", "author": ["Kentaro Inui", "Atsushi Fujita", "Tetsuro Takahashi", "Ryu Iida", "Tomoya Iwakura."], "venue": "Proceedings of the Second International Workshop on Paraphrasing. Association for Computa-", "citeRegEx": "Inui et al\\.,? 2003", "shortCiteRegEx": "Inui et al\\.", "year": 2003}, {"title": "Montreal neural machine translation systems for wmt15", "author": ["S\u00e9bastien Jean", "Orhan Firat", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation. pages 134\u2013140.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Verb paraphrase based on case frame alignment", "author": ["Nobuhiro Kaji", "Daisuke Kawahara", "Sadao Kurohashi", "Satoshi Sato."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguistics. Philadelphia, Pennsylvania, USA, pages", "citeRegEx": "Kaji et al\\.,? 2002", "shortCiteRegEx": "Kaji et al\\.", "year": 2002}, {"title": "Improving text simplification language modeling using unsimplified text data", "author": ["David Kauchak."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Com-", "citeRegEx": "Kauchak.,? 2013", "shortCiteRegEx": "Kauchak.", "year": 2013}, {"title": "Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel", "author": ["J Peter Kincaid", "Robert P Fishburne Jr", "Richard L Rogers", "Brad S Chissom."], "venue": "Technical report, DTIC", "citeRegEx": "Kincaid et al\\.,? 1975", "shortCiteRegEx": "Kincaid et al\\.", "year": 1975}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Dan Jurafsky", "Michel Galley", "Jianfeng Gao."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic", "author": ["Jing Zhu"], "venue": null, "citeRegEx": "Zhu.,? \\Q2002\\E", "shortCiteRegEx": "Zhu.", "year": 2002}, {"title": "A survey of automated text simplification", "author": ["Matthew Shardlow."], "venue": "International Journal of Advanced Computer Science and Applications pages 581\u2013701. Special Issue on Natural Language Processing.", "citeRegEx": "Shardlow.,? 2014", "shortCiteRegEx": "Shardlow.", "year": 2014}, {"title": "Syntactic simplification and text cohesion", "author": ["Advaith Siddharthan."], "venue": "research on language and computation. Research on Language and Computation 4(1):77\u2013109.", "citeRegEx": "Siddharthan.,? 2004", "shortCiteRegEx": "Siddharthan.", "year": 2004}, {"title": "A survey of research on text simplification", "author": ["Advaith Siddharthan."], "venue": "International Journal of Applied Linguistics 165(2):259\u2013298.", "citeRegEx": "Siddharthan.,? 2014", "shortCiteRegEx": "Siddharthan.", "year": 2014}, {"title": "Quasisynchronous grammars: Alignment by soft projection of syntactic dependencies", "author": ["David A Smith", "Jason Eisner."], "venue": "Proceedings of the Workshop on Statistical Machine Translation. Association for Computational Linguistics, pages", "citeRegEx": "Smith and Eisner.,? 2006", "shortCiteRegEx": "Smith and Eisner.", "year": 2006}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems, Curran Associates, Inc., pages 3104\u2013 3112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Sentence simplification for semantic role labeling", "author": ["D. Vickrey", "D. Koller."], "venue": "Proceedings of ACL-08: HLT . Columbus, OH, pages 344\u2013352.", "citeRegEx": "Vickrey and Koller.,? 2008", "shortCiteRegEx": "Vickrey and Koller.", "year": 2008}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Learning to simplify sentences with quasi-synchronous grammar and integer programming", "author": ["Kristian Woodsend", "Mirella Lapata."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Woodsend and Lapata.,? 2011", "shortCiteRegEx": "Woodsend and Lapata.", "year": 2011}, {"title": "Text rewriting improves semantic role labeling", "author": ["Kristian Woodsend", "Mirella Lapata."], "venue": "Journal of Artificial Intelligence Research 51:133\u2013164.", "citeRegEx": "Woodsend and Lapata.,? 2014", "shortCiteRegEx": "Woodsend and Lapata.", "year": 2014}, {"title": "Sentence simplification by monolingual machine translation", "author": ["Sander Wubben", "Antal Van Den Bosch", "Emiel Krahmer."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Asso-", "citeRegEx": "Wubben et al\\.,? 2012", "shortCiteRegEx": "Wubben et al\\.", "year": 2012}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1502.03044", "citeRegEx": "Xu et al\\.,? 2015a", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Problems in current text simplification research: New data can help", "author": ["Wei Xu", "Chris Callison-Burch", "Courtney Napoles."], "venue": "Transactions of the Association for Computational Linguistics 3:283\u2013297.", "citeRegEx": "Xu et al\\.,? 2015b", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Optimizing statistical machine translation for text simplification", "author": ["Wei Xu", "Courtney Napoles", "Ellie Pavlick", "Quanze Chen", "Chris Callison-Burch."], "venue": "Transactions of the Association for Computational Linguistics 4:401\u2013415.", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "A syntaxbased statistical translation model", "author": ["Kenji Yamada", "Kevin Knight."], "venue": "Proceedings of the 39th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, pages 523\u2013530.", "citeRegEx": "Yamada and Knight.,? 2001", "shortCiteRegEx": "Yamada and Knight.", "year": 2001}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329 .", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "A monolingual tree-based translation model for sentence simplification", "author": ["Zhemin Zhu", "Delphine Bernhard", "Iryna Gurevych."], "venue": "Proceedings of the 23rd international conference on computational linguistics. Association for Computational Linguistics,", "citeRegEx": "Zhu et al\\.,? 2010", "shortCiteRegEx": "Zhu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 21, "context": "The simplification task has been the subject of several modeling efforts in recent years due to its relevance for NLP applications and individuals alike (Siddharthan, 2014; Shardlow, 2014).", "startOffset": 153, "endOffset": 188}, {"referenceID": 19, "context": "The simplification task has been the subject of several modeling efforts in recent years due to its relevance for NLP applications and individuals alike (Siddharthan, 2014; Shardlow, 2014).", "startOffset": 153, "endOffset": 188}, {"referenceID": 3, "context": "For instance, a simplification component could be used as a preprocessing step to improve the performance of parsers (Chandrasekar et al., 1996), summarizers (Beigman Klebanov et al.", "startOffset": 117, "endOffset": 144}, {"referenceID": 25, "context": ", 2004) and semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014).", "startOffset": 35, "endOffset": 88}, {"referenceID": 28, "context": ", 2004) and semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014).", "startOffset": 35, "endOffset": 88}, {"referenceID": 8, "context": "autism (Evans et al., 2014), aphasia (Carroll et al.", "startOffset": 7, "endOffset": 27}, {"referenceID": 2, "context": ", 2014), aphasia (Carroll et al., 1999), or dyslexia (Rello et al.", "startOffset": 17, "endOffset": 39}, {"referenceID": 21, "context": "and deleting elements of the original text (Siddharthan, 2014).", "startOffset": 43, "endOffset": 62}, {"referenceID": 2, "context": "splitting (Carroll et al., 1999; Chandrasekar et al., 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al.", "startOffset": 10, "endOffset": 104}, {"referenceID": 3, "context": "splitting (Carroll et al., 1999; Chandrasekar et al., 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al.", "startOffset": 10, "endOffset": 104}, {"referenceID": 25, "context": "splitting (Carroll et al., 1999; Chandrasekar et al., 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al.", "startOffset": 10, "endOffset": 104}, {"referenceID": 20, "context": "splitting (Carroll et al., 1999; Chandrasekar et al., 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al.", "startOffset": 10, "endOffset": 104}, {"referenceID": 7, "context": ", 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002).", "startOffset": 180, "endOffset": 232}, {"referenceID": 11, "context": ", 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002).", "startOffset": 180, "endOffset": 232}, {"referenceID": 13, "context": ", 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002).", "startOffset": 180, "endOffset": 232}, {"referenceID": 18, "context": "For example, Zhu et al. (2010) draw inspiration from syntax-based translation and propose a model similar to Yamada and Knight (2001) which additionally per-", "startOffset": 13, "endOffset": 31}, {"referenceID": 18, "context": "For example, Zhu et al. (2010) draw inspiration from syntax-based translation and propose a model similar to Yamada and Knight (2001) which additionally per-", "startOffset": 13, "endOffset": 134}, {"referenceID": 22, "context": "Woodsend and Lapata (2011) formulate simplification in the framework of Quasi-synchronous grammar (Smith and Eisner, 2006) and use integer linear programming to score the candidate translations/simplifications.", "startOffset": 98, "endOffset": 122}, {"referenceID": 26, "context": "Woodsend and Lapata (2011) formulate simplification in the framework of Quasi-synchronous grammar (Smith and Eisner, 2006) and use integer linear programming to score the candidate translations/simplifications.", "startOffset": 0, "endOffset": 27}, {"referenceID": 22, "context": "Woodsend and Lapata (2011) formulate simplification in the framework of Quasi-synchronous grammar (Smith and Eisner, 2006) and use integer linear programming to score the candidate translations/simplifications. Wubben et al. (2012) propose a two-stage model: initially, a standard phrase-based machine translation (PBMT) model is trained on complex-simple sentence pairs.", "startOffset": 99, "endOffset": 232}, {"referenceID": 5, "context": "Initially, a probabilistic model performs sentence splitting and deletion operations over discourse representation structures assigned by Boxer (Curran et al., 2007).", "startOffset": 144, "endOffset": 165}, {"referenceID": 9, "context": "(2016) train a syntax-based machine translation model on a large scale paraphrase dataset (Ganitkevitch et al., 2013) using simplification-specific objective functions and features to encourage simpler output.", "startOffset": 90, "endOffset": 117}, {"referenceID": 29, "context": "Xu et al. (2016) train a syntax-based machine translation model on a large scale paraphrase dataset (Ganitkevitch et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 0, "context": "translation, a new approach to machine translation based purely on neural networks (Bahdanau et al., 2015; Sutskever et al., 2014).", "startOffset": 83, "endOffset": 130}, {"referenceID": 24, "context": "translation, a new approach to machine translation based purely on neural networks (Bahdanau et al., 2015; Sutskever et al., 2014).", "startOffset": 83, "endOffset": 130}, {"referenceID": 26, "context": "Our model uses the encoder-decoder architecture as its backbone coupled with a REINFORFCE-style (Williams, 1992) training algorithm.", "startOffset": 96, "endOffset": 112}, {"referenceID": 17, "context": ", 2016), dialogue generation (Li et al., 2016), machine translation and image caption generation (Ranzato et al.", "startOffset": 29, "endOffset": 46}, {"referenceID": 17, "context": "To train our network, we use a policy gradients method (Ranzato et al., 2016; Li et al., 2016) which we adapt to the simplification problem so as to optimize a reward function which jointly reflects the grammaticality and simplicity of the output, while preserving the meaning of the input.", "startOffset": 55, "endOffset": 94}, {"referenceID": 35, "context": "We evaluate our system on three publicly available datasets collated automatically from Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011) and human-authored news articles (Xu et al.", "startOffset": 98, "endOffset": 143}, {"referenceID": 27, "context": "We evaluate our system on three publicly available datasets collated automatically from Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011) and human-authored news articles (Xu et al.", "startOffset": 98, "endOffset": 143}, {"referenceID": 31, "context": ", 2010; Woodsend and Lapata, 2011) and human-authored news articles (Xu et al., 2015b).", "startOffset": 68, "endOffset": 86}, {"referenceID": 0, "context": "which can be modeled with attention-based encoder-decoder models (Bahdanau et al., 2015; Luong et al., 2015).", "startOffset": 65, "endOffset": 108}, {"referenceID": 12, "context": "Despite successful application in numerous sequence transduction tasks (Jean et al., 2015; Chopra et al., 2016; Xu et al., 2015a), a vanilla encoder-decoder model is not ideal for sentence", "startOffset": 71, "endOffset": 129}, {"referenceID": 4, "context": "Despite successful application in numerous sequence transduction tasks (Jean et al., 2015; Chopra et al., 2016; Xu et al., 2015a), a vanilla encoder-decoder model is not ideal for sentence", "startOffset": 71, "endOffset": 129}, {"referenceID": 30, "context": "Despite successful application in numerous sequence transduction tasks (Jean et al., 2015; Chopra et al., 2016; Xu et al., 2015a), a vanilla encoder-decoder model is not ideal for sentence", "startOffset": 71, "endOffset": 129}, {"referenceID": 26, "context": "A reward r is then received and we use the REINFORCE algorithm (Williams, 1992) to update the agent.", "startOffset": 63, "endOffset": 79}, {"referenceID": 32, "context": "Simplicity To encourage the model to apply a wide range of simplification operations, we use SARI (Xu et al., 2016), a recently proposed metric which compares System output Against References and against the Input sentence.", "startOffset": 98, "endOffset": 115}, {"referenceID": 30, "context": "In experimental evaluation Xu et al. (2016) demonstrate that SARI correlates well with human judgments of simplicity, whilst correctly rewarding systems that both make changes and simplify the input.", "startOffset": 27, "endOffset": 44}, {"referenceID": 30, "context": "In experimental evaluation Xu et al. (2016) demonstrate that SARI correlates well with human judgments of simplicity, whilst correctly rewarding systems that both make changes and simplify the input. One caveat with using SARI as a reward is the fact that it relies on the availability of multiple references which are rare for sentence simplification. Xu et al. (2016) provide 8 reference simplifications for 2,350 sentences, but these are primarily for system tuning and evaluation rather than training.", "startOffset": 27, "endOffset": 370}, {"referenceID": 30, "context": "Fluency Xu et al. (2016) observe that SARI has relatively low correlation with fluency compared to other metrics such as BLEU (Papineni et al.", "startOffset": 8, "endOffset": 25}, {"referenceID": 26, "context": "the reader to Williams (1992) for the full derivation of the gradients.", "startOffset": 14, "endOffset": 30}, {"referenceID": 29, "context": "(2010) has been previously used as a benchmark for evaluating automatic text simplification systems (Wubben et al., 2012; Woodsend and Lapata, 2011; Narayan and Gardent, 2014; Zhu et al., 2010).", "startOffset": 100, "endOffset": 193}, {"referenceID": 27, "context": "(2010) has been previously used as a benchmark for evaluating automatic text simplification systems (Wubben et al., 2012; Woodsend and Lapata, 2011; Narayan and Gardent, 2014; Zhu et al., 2010).", "startOffset": 100, "endOffset": 193}, {"referenceID": 35, "context": "(2010) has been previously used as a benchmark for evaluating automatic text simplification systems (Wubben et al., 2012; Woodsend and Lapata, 2011; Narayan and Gardent, 2014; Zhu et al., 2010).", "startOffset": 100, "endOffset": 193}, {"referenceID": 14, "context": "Specifically, we aggregated the aligned sentence pairs in Kauchak (2013), the aligned and revision sentence pairs in Woodsend and Lapata (2011), and Zhu\u2019s (2010) WikiSmall dataset described above.", "startOffset": 58, "endOffset": 73}, {"referenceID": 14, "context": "Specifically, we aggregated the aligned sentence pairs in Kauchak (2013), the aligned and revision sentence pairs in Woodsend and Lapata (2011), and Zhu\u2019s (2010) WikiSmall dataset described above.", "startOffset": 58, "endOffset": 144}, {"referenceID": 14, "context": "Specifically, we aggregated the aligned sentence pairs in Kauchak (2013), the aligned and revision sentence pairs in Woodsend and Lapata (2011), and Zhu\u2019s (2010) WikiSmall dataset described above.", "startOffset": 58, "endOffset": 162}, {"referenceID": 30, "context": "Xu et al. (2016) report results on this test with a paraphrase-based model outperforming related SMT-based models trained on monolingual data.", "startOffset": 0, "endOffset": 17}, {"referenceID": 30, "context": "Newsela Xu et al. (2015b) argue that Wikipedia is a suboptimal simplification data resource for at least three reasons: a) the automatic sentence alignment unavoidably introduces errors; b) a large number of purported simplifications are not actually simpler sentences; and c) models trained on Wikipedia generalize poorly to other text genres.", "startOffset": 8, "endOffset": 26}, {"referenceID": 16, "context": "We used Adam (Kingma and Ba, 2014) to optimize the model with learning rate 0.", "startOffset": 13, "endOffset": 34}, {"referenceID": 23, "context": "2 (Srivastava et al., 2014; Zaremba et al., 2014).", "startOffset": 2, "endOffset": 49}, {"referenceID": 34, "context": "2 (Srivastava et al., 2014; Zaremba et al., 2014).", "startOffset": 2, "endOffset": 49}, {"referenceID": 27, "context": "Following previous work (Woodsend and Lapata, 2011; Xu et al., 2016) we evaluated system output automatically.", "startOffset": 24, "endOffset": 68}, {"referenceID": 32, "context": "Following previous work (Woodsend and Lapata, 2011; Xu et al., 2016) we evaluated system output automatically.", "startOffset": 24, "endOffset": 68}, {"referenceID": 32, "context": "In addition, we used SARI (Xu et al., 2016), which evaluates the quality of the output by comparing it against the source and the references7.", "startOffset": 26, "endOffset": 43}, {"referenceID": 15, "context": "5See Kincaid et al. (1975) for more details on FKGL.", "startOffset": 5, "endOffset": 27}, {"referenceID": 15, "context": "5See Kincaid et al. (1975) for more details on FKGL. 6We used the implementation of FKGL from https:// github.com/mmautner/readability. 7We used the implementation of SARI in Xu et al. (2016). Newsela Grammar Mean Simple All", "startOffset": 5, "endOffset": 192}, {"referenceID": 29, "context": "a strong baseline, the simplification model introduced in Wubben et al. (2012): PBMT-R, is a monolingual phrase-based machine translation system with a reranking post-processing step.", "startOffset": 58, "endOffset": 79}, {"referenceID": 29, "context": "8We made a good-faith effort to re-implement their system following closely the details in Wubben et al. (2012).", "startOffset": 91, "endOffset": 112}, {"referenceID": 34, "context": "ral network models fare well on Grammaticality, which may not be entirely surprising given the recent success of LSTM models in language modeling and neural machine translation (Zaremba et al., 2014; Jean et al., 2015).", "startOffset": 177, "endOffset": 218}, {"referenceID": 12, "context": "ral network models fare well on Grammaticality, which may not be entirely surprising given the recent success of LSTM models in language modeling and neural machine translation (Zaremba et al., 2014; Jean et al., 2015).", "startOffset": 177, "endOffset": 218}, {"referenceID": 29, "context": "Besides PBMT-R (Wubben et al., 2012) and different versions of our model, we also compared against Narayan and", "startOffset": 15, "endOffset": 36}, {"referenceID": 29, "context": "We compare our models with PBMT-R (Wubben et al., 2012) and SBMR-SARI (Xu et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 32, "context": ", 2012) and SBMR-SARI (Xu et al., 2016)10, a syntax-based translation model trained on the PPDB dataset (Ganitkevitch et al.", "startOffset": 22, "endOffset": 39}, {"referenceID": 9, "context": ", 2016)10, a syntax-based translation model trained on the PPDB dataset (Ganitkevitch et al., 2013) and tuned with SARI.", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": ", 2016)10, a syntax-based translation model trained on the PPDB dataset (Ganitkevitch et al., 2013) and tuned with SARI. PPDB, which contains 106 million sentence pairs with 2 billion words, is much larger than our WkiLarge dataset. The FKGL follows a similar patten as in the previous datasets. Our models and PBMT-R are best in terms of BLEU while SBMTSARI outperforms all other systems on SARI. In human evaluation, we again elicited judgments for 100 randomly sampled test sentences. EncDecARF-LS is significantly better than SBMT-SARI and PBMT-R on all dimensions including \u201dAll\u201d (p< 0.05) except for Meaning. We used more data to train PBMT-R and maybe that is why PBMT-R performs better than Xu et al. (2016) reported.", "startOffset": 73, "endOffset": 716}], "year": 2017, "abstractText": "Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model brings significant improvements over the state of the art.1", "creator": "LaTeX with hyperref package"}}}