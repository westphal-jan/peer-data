{"id": "1703.11000", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2017", "title": "Learning Visual Servoing with Deep Features and Fitted Q-Iteration", "abstract": "Visual servoing involves choosing actions that move a robot in response to observations from a camera, in order to reach a goal configuration in the world. Standard visual servoing approaches typically rely on manually designed features and analytical dynamics models, which limits their generalization capability and often requires extensive application-specific feature and model engineering. In this work, we study how learned visual features, learned predictive dynamics models, and reinforcement learning can be combined to learn visual servoing mechanisms. We focus on target following, with the goal of designing algorithms that can learn a visual servo using low amounts of data of the target in question, to enable quick adaptation to new targets. Our approach is based on servoing the camera in the space of learned visual features, rather than image pixels or manually-designed keypoints. We demonstrate that standard deep features, in our case taken from a model trained for object classification, can be used together with a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions. A key component of our approach is to use a sample-efficient fitted Q-iteration algorithm to learn which features are best suited for the task at hand. We show that we can learn an effective visual servo on a complex synthetic car following benchmark using just 20 training trajectory samples for reinforcement learning. We demonstrate substantial improvement over a conventional approach based on image pixels or hand-designed keypoints, and we show an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. Videos are available at \\url{", "histories": [["v1", "Fri, 31 Mar 2017 17:45:53 GMT  (4575kb,D)", "https://arxiv.org/abs/1703.11000v1", "ICLR 2017"], ["v2", "Tue, 11 Jul 2017 00:26:55 GMT  (4575kb,D)", "http://arxiv.org/abs/1703.11000v2", "ICLR 2017"]], "COMMENTS": "ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO", "authors": ["alex x lee", "sergey levine", "pieter abbeel"], "accepted": true, "id": "1703.11000"}, "pdf": {"name": "1703.11000.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["FITTED Q-ITERATION", "Alex X. Lee", "Sergey Levine", "Pieter Abbeel"], "emails": ["gk@cs.berkeley.edu", "svlevine@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is such that most of them will be able to put themselves in a different world, in which they are able, in which they are able, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they are able to put themselves into a different world, in which they are able, in which they are in which they live, in which they live in which they are able, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in"}, {"heading": "2 RELATED WORK", "text": "In fact, it is not that one sees oneself in a position to trump oneself, but that one sees oneself in a position to trump oneself and to trump oneself. (...) It is not that one is in a position to trump oneself. (...) It is not that one is in a position to trump oneself. (...) It is also not that one is in a position to trump oneself. (...) It is not that one is in a position to trump oneself. (...) It is not that one is in a position to trump oneself. (...) It is not that one is in a position to trump oneself. (...). (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. () It is. (...) It is. () It is. (... It is. () It is. () It is. () It is. (... It is. () It is. () It is. It is. It is. () It is. () It is. (... it is. It is. () It is. It is. It is. It is. (. () It is. It is. (. It is. It is. It is. It is. It is. (). It is. (. It is. It is. It is. (). It is. (. It is. It is. It is. It is. It is. (). (. It is. It is. It is. It is. It is. It is. It is. (). (. It is. It is. It is. It is. It is. It is. (. () It is. It is. It is. It is. It is. It is. It is. It is. It is. (. It is.). It is. It is. It is. It is. (. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. (). It is. It is. It is. It is. It is. It is. It is. It is. ("}, {"heading": "3 PROBLEM STATEMENT", "text": "For the purposes of this work, we define visual servo-ing as the problem of selecting control functions for a fixed number of discrete time steps t in order to minimize the error between the target tag and the single-stage prediction. We use a relatively simple gradient-based servo-policy that uses a single-stage characteristic dynamic, f: {yt, ut} \u2192 yt + 1. The policy chooses the control that minimizes the distance between the target tag and the single-stage prediction: \u03c0 (xt, x) = argmin u-map of the state - f (yt, u).2. (1) Learning this policy amounts to learning the robot dynamics and distance measurement. To learn the robot dynamics, we assume that we have access to a dataset of paired observations and controls, xt, ut, xt + 1. This data is relatively easy to obtain as it involves capturing a current of the robot dynamics."}, {"heading": "4 VISUAL FEATURES DYNAMICS", "text": "We learn a two-dimensional model to predict the visual properties of the next image based on the current image of the robot camera and the robot's action. An overview of the model is shown in Figure 1. The learned dynamics can then be used for visual servo-ing as described in Section 5."}, {"heading": "4.1 VISUAL FEATURES", "text": "We look at both pixels and semantic characteristics for visual representation; we define the function h to relate the image x to its attribute y = h (x); our choice of semantic characteristics is derived from the VGG-16 network (Simonyan & Zisserman, 2015), a revolutionary neural network trained for large-scale image recognition on the ImageNet dataset (Deng et al., 2009); since spatial invariance is undesirable for servo-ing, we remove some of the max-pooling layers and replace the subsequent volumes with extended ones, such as Yu & Koltun (2016); the modified VGG network is shown in Figure 2; we use the model weights of the original VGG-16 network, which are publicly available as a caffe model (Jia et al., 2014); the characteristics we use are the outputs of some of the intermediate layers, which have been downloaded to a resolution of 32 times."}, {"heading": "4.2 BILINEAR DYNAMICS", "text": "The characteristics y (l) t are used to predict the characteristics of the corresponding plane y (l) t + 1 in the next time step, due to the action ut, according to a predictive function f (l) (y (l) t, ut) = y (l) t + 1. We use a bilinear model to represent this dynamic, motivated by previous work (Censi & Murray, 2015). To serve on different scales, we learn a bilinear dynamic model on each scale. We consider two variants of the bilinear model in previous work to reduce the number of model parameters. The first variant uses fully interconnected dynamics as in previous work, but models the dynamics of each channel independently. When semantic characteristics are used, this model interprets the characteristic maps as abstract images with spatial information within a channel c, as well as different units or variation factors in different channels."}, {"heading": "4.3 TRAINING VISUAL FEATURE DYNAMICS MODELS", "text": "The loss that we use for the training of bilinear dynamics is the sum of the losses of the predicted characteristics at each level, \u2211 L = 0 '(l), where the loss for each level l is the square \"-2 norm between the predicted characteristics and the actual characteristics of that level,\" (l) = \u0192y (l) t + 1 \u2212 y (l) t + 1 \u0445 2. We optimize the dynamics while maintaining the representation of the characteristics. This is a supervised learning problem that we solve with ADAM (Kingma & Ba, 2015). The training set, consisting of the triplets xt, ut, xt + 1, was achieved by implementing a hand-coded policy that moves the robot around the target with some Gaussian noise."}, {"heading": "5 LEARNING VISUAL SERVOING WITH REINFORCEMENT LEARNING", "text": "The challenge in introducing multiple scales and multi-channel characteristic scales for serving is that the characteristics do not necessarily match the optimal action when the goal is unattainable or the robot is far from the goal. To achieve this, it is important to use a good balance of the individual terms in the goal. As there are many weights, it would be impractically time-consuming to set them by hand, so we resort to learning. We want the weighted one-step outlook goal to promote good long-term behavior, so we want this goal to correspond to the Q state value function. Therefore, we propose a method for learning the weights based on customized Q iteration."}, {"heading": "5.1 SERVOING WITH WEIGHTED MULTISCALE FEATURES", "text": "Instead of trying to create an accurate predictive model for multi-stage planning, in Equation (1) we use the simple, greedy servo method, in which we minimize the error between the target and the predicted characteristics for all scales. Typically, only a few objects are relevant in the scene, so the errors of some channels should be punished more than others. Similarly, features on different scales may need to be weighted differently. Thus, we use a weighting w (l) c (l) c (y) c (l) t, c (c) t, u)."}, {"heading": "5.2 Q-FUNCTION APPROXIMATION FOR THE WEIGHTED SERVOING POLICY", "text": "We choose an approximator of the Q-values function, which can represent the servo target in such a way that the greedy policy with respect to the Q-values leads to the politics of the equation (3). In particular, we use a function approximator which is linear in the weight parameters \u03b8 > = [w > \u03bb >]: Q\u03b8, b (st, u) = \u03c6 (st, u) > \u03b8 + b, \u03c6 (st, u) > = [1 | y (l) \u00b7, c | s (l), c \u2212 f (l) c (y (l) t, c, u).2 2] > c, l [u2j] > j. We call the state of the MDP st = (xt, x) and add a bias b to the Q function. Servopolitics is then simply ZIP (st) = argminu Qhabi, b (st, u).To enhance learning, we optimize the weights, but keep the characteristic and its fixed dynamics."}, {"heading": "5.3 LEARNING THE Q-FUNCTION WITH FITTED Q-ITERATION", "text": "Reinforcement Learning Methods that learn a Q-function do this by minimizing the Bellman error: whether Q (st, ut) \u2212 (ct + \u03b3minu Q (st + 1, u). \u2212 (4) In customized Q iteration, the agent iteratively collects a dataset {s (i) t, u (i) t, c (i) t, s (i) t, s (i) t + 1} Ni of the N samples according to an exploration policy and then minimizes the Bellman error using this dataset. We use the term sampling iteration to refer to each iteration j of this procedure. At the beginning of each sampling iteration, the current policy with added Gaussian noise is applied as an exploration policy.It is typically hard or unstable to optimize the two Q functions that appear in the Bellman error of the equation (4), so it is optimized by optimizing the function during the current Q-optimization rule."}, {"heading": "6 EXPERIMENTS", "text": "We evaluate the performance of the model for visual servo in a simulated environment. The simulated quadrocopter is subject to rigid body dynamics. The robot has 4 degrees of freedom, corresponding to a translation along three axes and a yaw angle. This simulation is inspired by tasks in which an autonomous quadrocopter flies over a city with the aim of following a target (e.g. a car)."}, {"heading": "6.1 LEARNING FEATURE DYNAMICS AND WEIGHTS WITH FQI", "text": "The dynamics for each of the characteristics were trained with ADAM (Kingma & Ba, 2015) using a data set of 10,000 samples (corresponding to 100 trajectories); for each functional representation for all the training cars, a single dynamic model was learned (Figure 3); this training set was generated by executing a hand-coded policy that navigates the quadcopter around a car for 100 time steps per trajectory as the car moves through the city; we used the proposed FQI algorithm to learn the weighting of the characteristics and the controller; each sampling iteration executed the current policy with Gaussian noise to gather data from 10 trajectories; all trajectories in our experiments were up to 100 time steps long; the immediate cost received by the agent codes the error of the target in the image coordinates (details in Appendix B); and then the parameters were updated iteratively by executing 10 K = Q."}, {"heading": "6.2 COMPARISON OF FEATURE REPRESENTATIONS FOR SERVOING", "text": "We execute the learned strategies on the basis of 100 test runs and report on the average cost of introducing the test runs on the basis of Figure 5. The cost of a single method is the (non-discounted) sum of costs. We test the strategies both with cars seen during training and with a number of novel cars (Figure 4) to evaluate the generalization of learned dynamics and optimized strategies. However, the test runs were determined by random samples of 100 cars (with replacements) from one of the two groups of cars, and by random samples of starting states (which differ from those used for validation). To ensure consistency and reproducibility, the same sample-like cars and starting states were used in all test experiments, and the same starting states were used for both groups of cars."}, {"heading": "6.3 COMPARISON OF WEIGHTINGS FROM OTHER OPTIMIZATION METHODS", "text": "We compare our policies with the help of Conv4 3 Feature Dynamics, with weights optimized by FQI, with policies that use this dynamic, but either without feature weighting or with weights optimized by other algorithms. In the case of non-weighting, we use a single feature weight w, but optimize the relative weighting of controls \u03bb with the Cross-Entropy Method (CEM) (De Boer et al., 2005).For the other cases, we learn the weights with Trust Region Policy Optimization (TRPO) (Schulman et al., 2015).Since the servo policy is to minimize a square target (Equation (3), we represent politics as a neural network that has an inverse matrix operation at the output. We train this network for 2 and 50 sampling iterations and use a batch size of 4000 samples per iteration. All of these methods use the same weights as our weights, wherein the only difference is S and Q."}, {"heading": "6.4 COMPARISON TO PRIOR METHODS", "text": "We also consider other methods that do not use the dynamics-based servo strategy that we propose. We report on their average performance on the left side of Figure 6. For one of the previous methods, we train a Convolutionary Neural Network (CNN) policy end-to-end with TRPO. The policy is parameterized as 5-layer CNN, consisting of 2 conventional and 3 fully connected layers, with ReLU activations except the output layer; the conventional layers use 16 filters (4 x 4, strict 2), with the first 2 fully connected layers each using 32 hidden layers. The policy assumes a modest performance in terms of raw pixel intensities and outputs controls (although even worse than the policy based on Conv4 feature dynamics), but it requires significantly more training samples than the others."}, {"heading": "7 DISCUSSION", "text": "We have described an approach that combines learned visual features with predictive dynamic models and reinforcement models to learn visual servo mechanisms. Our experiments show that standard deep features, in our case from a model trained for object classification, can be used together with a bilinear prediction model to learn an effective visual servo that is robust to visual variations, changes in viewing angle and appearance, and occlusions. For control, we suggest learning Q values based on customized Q iteration, which allows one-step predictive calculations to optimize long-term goals at execution time. Our method can learn an effective visual servo on a complex synthetic car by benchmarking only 20 training samples for reinforcement learning."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This research was partially funded by the Army Research Office through the MAST program, the Berkeley DeepDrive Consortium, and NVIDIA. Alex Lee was also funded by NSF GRFP.5The term interaction matrix, or Jacobian feature, is used in the visual servo literature to characterize the Jacobian of features in relation to control. 6https: / / github.com / martin-danelljan / Continuous-ConvOp"}, {"heading": "A LINEARIZATION OF THE BILINEAR DYNAMICS", "text": "The optimization of equation (3) can be efficiently solved by using a linearization of the dynamics, f (l) c (y (l) t, c, u) = f (l) c (y (l) t, c, u) + J (l) t, c (u \u2212 u) = f (l) c (y (l) t, c, 0) + J (l) t, cu, (7) where J (l) t, c is the jacobic matrix with partial derivatives. Since the bilinear dynamics are linear in relation to the controls, this linearization is accurate and the jacobic matrix does not depend on u (l). Without loss of generality, we set u = 0, furthermore, the bilinear dynamics so that the jacobic matrix can be efficiently calculated."}, {"heading": "B SERVOING COST FUNCTION FOR REINFORCEMENT LEARNING", "text": "The goal of reinforcement learning is to find a policy that maximizes the expected sum of rewards, or equivalent a policy that minimizes the expected sum of costs. Costs should be one that quantifies the progress toward the goal. We define the cost function by the position of the target object (in the local camera image) after the action has been performed, c (st, ut, st + 1) = \u221a (pxt + 1 pzt + 1) 2 + (pyt + 1 pzt + 1) 2 + (1 pzt + 1) 2 if the camera is too close to the car (less than a distance between the two) or the starting point of the car outside the camera field (FOV) c (\u00b7, \u00b7 \u00b7, st) 2 + (9), where T is the maximum trajectory length. The episode ends early if the camera is too close to the car (less than a distance from the camera) or the starting point of the car outside the camera field of view (FOV) c (the position of the car is 4t = p), the camera position of the time (p), V = p (the time)."}, {"heading": "C EXPERIMENT DETAILS", "text": "The robot has 4 degrees of freedom corresponding to the gear ratio and the yawn angle. In our simulations, the quadrocopter follows a car driven in 1 million s-1 along city streets during training and testing. The speed of the quadrocopter is limited to 10 million s-1 for each translational degree of freedom, and its angular speed is limited to within 1 million s-1. The simulator runs at 10 Hz. For each trajectory, a car is randomly selected from a series of cars and placed randomly on one of the roads.The quadrocopter is initialized directly behind the car, in the desired relative position for the following. The image observed at the beginning of the trajectory is considered the goal of observation.C.2 LEARNING"}], "references": [{"title": "SURF: Speeded up robust features", "author": ["Herbert Bay", "Tinne Tuytelaars", "Luc Van Gool"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Bay et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bay et al\\.", "year": 2006}, {"title": "Photometric visual servoing for omnidirectional cameras", "author": ["Guillaume Caron", "Eric Marchand", "El Mustapha Mouaddib"], "venue": "Autonomous Robots,", "citeRegEx": "Caron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Caron et al\\.", "year": 2013}, {"title": "Bootstrapping bilinear models of simple vehicles", "author": ["Andrea Censi", "Richard M Murray"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Censi and Murray.,? \\Q2015\\E", "shortCiteRegEx": "Censi and Murray.", "year": 2015}, {"title": "Visual servo control. I", "author": ["Francois Chaumette", "Seth Hutchinson"], "venue": "Basic approaches. IEEE Robotics & Automation Magazine,", "citeRegEx": "Chaumette and Hutchinson.,? \\Q2006\\E", "shortCiteRegEx": "Chaumette and Hutchinson.", "year": 2006}, {"title": "Homography-based visual servo tracking control of a wheeled mobile robot", "author": ["Jian Chen", "Warren E Dixon", "M Dawson", "Michael McIntyre"], "venue": "IEEE Transactions on Robotics,", "citeRegEx": "Chen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2006}, {"title": "Photometric visual servoing", "author": ["Christophe Collewet", "Eric Marchand"], "venue": "IEEE Transactions on Robotics,", "citeRegEx": "Collewet and Marchand.,? \\Q2011\\E", "shortCiteRegEx": "Collewet and Marchand.", "year": 2011}, {"title": "Visual servoing set free from image processing", "author": ["Christophe Collewet", "Eric Marchand", "Francois Chaumette"], "venue": "In IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Collewet et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collewet et al\\.", "year": 2008}, {"title": "Visual control of robot manipulators \u2013 A review", "author": ["Peter I Corke"], "venue": "Visual servoing,", "citeRegEx": "Corke.,? \\Q1993\\E", "shortCiteRegEx": "Corke.", "year": 1993}, {"title": "Beyond correlation filters: Learning continuous convolution operators for visual tracking", "author": ["Martin Danelljan", "Andreas Robinson", "Fahad Shahbaz Khan", "Michael Felsberg"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Danelljan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Danelljan et al\\.", "year": 2016}, {"title": "A tutorial on the cross-entropy method", "author": ["Pieter-Tjerk De Boer", "Dirk P Kroese", "Shie Mannor", "Reuven Y Rubinstein"], "venue": "Annals of operations research,", "citeRegEx": "Boer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Boer et al\\.", "year": 2005}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Vision for mobile robot navigation: A survey", "author": ["Guilherme N DeSouza", "Avinash C Kak"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "DeSouza and Kak.,? \\Q2002\\E", "shortCiteRegEx": "DeSouza and Kak.", "year": 2002}, {"title": "DeCAF: A deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Tree-based batch mode reinforcement learning", "author": ["Damien Ernst", "Pierre Geurts", "Louis Wehenkel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ernst et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ernst et al\\.", "year": 2005}, {"title": "A new approach to visual servoing in robotics", "author": ["Bernard Espiau", "Francois Chaumette", "Patrick Rives"], "venue": "IEEE Transactions on Robotics and Automation,", "citeRegEx": "Espiau et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Espiau et al\\.", "year": 2002}, {"title": "Regularized fitted Q-iteration for planning in continuous-space Markovian decision problems", "author": ["Amir Massoud Farahmand", "Mohammad Ghavamzadeh", "Csaba Szepesv\u00e1ri", "Shie Mannor"], "venue": "In American Control Conference (ACC),", "citeRegEx": "Farahmand et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Farahmand et al\\.", "year": 2009}, {"title": "Vision-guided servoing with feature-based trajectory generation (for robots)", "author": ["John T Feddema", "Owen Robert Mitchell"], "venue": "IEEE Transactions on Robotics and Automation,", "citeRegEx": "Feddema and Mitchell.,? \\Q1989\\E", "shortCiteRegEx": "Feddema and Mitchell.", "year": 1989}, {"title": "Stable function approximation in dynamic programming", "author": ["Geoffrey J Gordon"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Gordon.,? \\Q1995\\E", "shortCiteRegEx": "Gordon.", "year": 1995}, {"title": "Versatile visual servoing without knowledge of true Jacobian", "author": ["Koh Hosoda", "Minoru Asada"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Hosoda and Asada.,? \\Q1994\\E", "shortCiteRegEx": "Hosoda and Asada.", "year": 1994}, {"title": "A tutorial on visual servo control", "author": ["Seth Hutchinson", "Gregory D Hager", "Peter I Corke"], "venue": "IEEE transactions on robotics and automation,", "citeRegEx": "Hutchinson et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Hutchinson et al\\.", "year": 1996}, {"title": "Experimental evaluation of uncalibrated visual servoing for precision manipulation", "author": ["Martin Jagersand", "Olac Fuentes", "Randal Nelson"], "venue": "In IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Jagersand et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Jagersand et al\\.", "year": 1997}, {"title": "Dynamic filter networks", "author": ["Xu Jia", "Bert De Brabandere", "Tinne Tuytelaars", "Luc V Gool"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Jia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "In 22nd ACM International Conference on Multimedia,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Survey on visual servoing for manipulation", "author": ["Danica Kragic", "Henrik I Christensen"], "venue": "Computational Vision and Active Perception Laboratory, Fiskartorpsv,", "citeRegEx": "Kragic and Christensen.,? \\Q2002\\E", "shortCiteRegEx": "Kragic and Christensen.", "year": 2002}, {"title": "Acquiring visual servoing reaching and grasping skills using neural reinforcement learning", "author": ["Thomas Lampe", "Martin Riedmiller"], "venue": "In International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "Lampe and Riedmiller.,? \\Q2013\\E", "shortCiteRegEx": "Lampe and Riedmiller.", "year": 2013}, {"title": "Autonomous reinforcement learning on raw visual input data in a real world application", "author": ["Sascha Lange", "Martin Riedmiller", "Arne Voigtlander"], "venue": "In International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "Lange et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lange et al\\.", "year": 2012}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P. Lillicrap", "Jonathan J. Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Lillicrap et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2016}, {"title": "Deep predictive coding networks for video prediction and unsupervised learning", "author": ["William Lotter", "Gabriel Kreiman", "David Cox"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Lotter et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Lotter et al\\.", "year": 2017}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["David G Lowe"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Lowe.,? \\Q2004\\E", "shortCiteRegEx": "Lowe.", "year": 2004}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["Micha\u00ebl Mathieu", "Camille Couprie", "Yann LeCun"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Mathieu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2016}, {"title": "Playing Atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin A. Riedmiller"], "venue": "CoRR, abs/1312.5602,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Vision-based control of a quadrotor for perching on lines", "author": ["Kartik Mohta", "Vijay Kumar", "Kostas Daniilidis"], "venue": "In IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Mohta et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mohta et al\\.", "year": 2014}, {"title": "Action-conditional video prediction using deep networks in Atari games", "author": ["Junhyuk Oh", "Xiaoxiao Guo", "Honglak Lee", "Richard L Lewis", "Satinder Singh"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Neural fitted Q iteration \u2013 First experiences with a data efficient neural reinforcement learning method", "author": ["Martin Riedmiller"], "venue": "In European Conference on Machine Learning,", "citeRegEx": "Riedmiller.,? \\Q2005\\E", "shortCiteRegEx": "Riedmiller.", "year": 2005}, {"title": "ORB: An efficient alternative to SIFT or SURF", "author": ["Ethan Rublee", "Vincent Rabaud", "Kurt Konolige", "Gary Bradski"], "venue": "In IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Rublee et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rublee et al\\.", "year": 2011}, {"title": "Self-learning visual servoing of robot manipulator using explanation-based fuzzy neural networks and Q-learning", "author": ["Mehdi Sadeghzadeh", "David Calvert", "Hussein A Abdullah"], "venue": "Journal of Intelligent & Robotic Systems,", "citeRegEx": "Sadeghzadeh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sadeghzadeh et al\\.", "year": 2015}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Pieter Abbeel", "Michael I Jordan", "Philipp Moritz"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Simonyan and Zisserman.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2015}, {"title": "Generating videos with scene dynamics", "author": ["Carl Vondrick", "Hamed Pirsiavash", "Antonio Torralba"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Vondrick et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vondrick et al\\.", "year": 2016}, {"title": "An uncertain future: Forecasting from static images using variational autoencoders", "author": ["Jacob Walker", "Carl Doersch", "Abhinav Gupta", "Martial Hebert"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Walker et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2016}, {"title": "Embed to control: A locally linear latent dynamics model for control from raw images", "author": ["Manuel Watter", "Jost Springenberg", "Joschka Boedecker", "Martin Riedmiller"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Watter et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Watter et al\\.", "year": 2015}, {"title": "Dynamic sensor-based control of robots with visual feedback", "author": ["Lee E Weiss", "Arthur C Sanderson", "Charles P Neuman"], "venue": "IEEE Journal on Robotics and Automation,", "citeRegEx": "Weiss et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 1987}, {"title": "Relative end-effector control using cartesian position based visual servoing", "author": ["William J Wilson", "Carol C Williams Hulls", "Graham S Bell"], "venue": "IEEE Transactions on Robotics and Automation,", "citeRegEx": "Wilson et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 1996}, {"title": "Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks", "author": ["Tianfan Xue", "Jiajun Wu", "Katherine Bouman", "Bill Freeman"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Xue et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2016}, {"title": "Active, uncalibrated visual servoing", "author": ["Billibon H Yoshimi", "Peter K Allen"], "venue": "In IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Yoshimi and Allen.,? \\Q1994\\E", "shortCiteRegEx": "Yoshimi and Allen.", "year": 1994}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["Fisher Yu", "Vladlen Koltun"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Yu and Koltun.,? \\Q2016\\E", "shortCiteRegEx": "Yu and Koltun.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "Many robot control tasks that combine perception and action can be posed as visual servoing, including navigation (DeSouza & Kak, 2002; Chen et al., 2006), where a robot must follow a desired path; manipulation, where the robot must servo an end-effector or a camera to a target object to grasp or manipulate it (Malis et al.", "startOffset": 114, "endOffset": 154}, {"referenceID": 7, "context": ", 2006), where a robot must follow a desired path; manipulation, where the robot must servo an end-effector or a camera to a target object to grasp or manipulate it (Malis et al., 1999; Corke, 1993; Hashimoto, 1993; Hosoda & Asada, 1994; Kragic & Christensen, 2002); and various other problems, as surveyed in Hutchinson et al.", "startOffset": 165, "endOffset": 265}, {"referenceID": 6, "context": "Most visual servoing methods assume access to good geometric image features (Chaumette & Hutchinson, 2006; Collewet et al., 2008; Caron et al., 2013) and require knowledge of their dynamics, which are typically obtained from domain knowledge about the system.", "startOffset": 76, "endOffset": 149}, {"referenceID": 1, "context": "Most visual servoing methods assume access to good geometric image features (Chaumette & Hutchinson, 2006; Collewet et al., 2008; Caron et al., 2013) and require knowledge of their dynamics, which are typically obtained from domain knowledge about the system.", "startOffset": 76, "endOffset": 149}, {"referenceID": 3, "context": "Many robot control tasks that combine perception and action can be posed as visual servoing, including navigation (DeSouza & Kak, 2002; Chen et al., 2006), where a robot must follow a desired path; manipulation, where the robot must servo an end-effector or a camera to a target object to grasp or manipulate it (Malis et al., 1999; Corke, 1993; Hashimoto, 1993; Hosoda & Asada, 1994; Kragic & Christensen, 2002); and various other problems, as surveyed in Hutchinson et al. (1996). Most visual servoing methods assume access to good geometric image features (Chaumette & Hutchinson, 2006; Collewet et al.", "startOffset": 136, "endOffset": 482}, {"referenceID": 10, "context": "Prior work has shown that the features learned by large convolutional neural networks on large image datasets, such as ImageNet classification (Deng et al., 2009), tend to be useful for a wide range of other visual tasks (Donahue et al.", "startOffset": 143, "endOffset": 162}, {"referenceID": 12, "context": ", 2009), tend to be useful for a wide range of other visual tasks (Donahue et al., 2014).", "startOffset": 66, "endOffset": 88}, {"referenceID": 14, "context": "Ideal features for servoing should be stable and discriminative, and much of the work on visual servoing focuses on designing stable and convergent controllers under the assumption that such features are available (Espiau et al., 2002; Mohta et al., 2014; Wilson et al., 1996).", "startOffset": 214, "endOffset": 276}, {"referenceID": 33, "context": "Ideal features for servoing should be stable and discriminative, and much of the work on visual servoing focuses on designing stable and convergent controllers under the assumption that such features are available (Espiau et al., 2002; Mohta et al., 2014; Wilson et al., 1996).", "startOffset": 214, "endOffset": 276}, {"referenceID": 44, "context": "Ideal features for servoing should be stable and discriminative, and much of the work on visual servoing focuses on designing stable and convergent controllers under the assumption that such features are available (Espiau et al., 2002; Mohta et al., 2014; Wilson et al., 1996).", "startOffset": 214, "endOffset": 276}, {"referenceID": 20, "context": "Some visual servoing methods do not require camera calibration (Jagersand et al., 1997; Yoshimi & Allen, 1994), and some recent methods operate directly on image intensities (Caron et al.", "startOffset": 63, "endOffset": 110}, {"referenceID": 1, "context": ", 1997; Yoshimi & Allen, 1994), and some recent methods operate directly on image intensities (Caron et al., 2013), but generally do not use learning to exploit statistical regularities in the world and improve robustness to distractors.", "startOffset": 94, "endOffset": 114}, {"referenceID": 37, "context": "Several methods have been proposed that apply ideas from reinforcement learning to directly acquire visual servoing controllers (Lampe & Riedmiller, 2013; Sadeghzadeh et al., 2015).", "startOffset": 128, "endOffset": 180}, {"referenceID": 26, "context": "Though more standard deep reinforcement learning methods (Lange et al., 2012; Mnih et al., 2013; Levine et al., 2016; Lillicrap et al., 2016) could in principle be applied to directly learn visual servoing policies, such methods tend to require large numbers of samples to learn task-specific behaviors, making them poorly suited for a flexible visual servoing algorithm that can be quickly repurposed to new tasks (e.", "startOffset": 57, "endOffset": 141}, {"referenceID": 32, "context": "Though more standard deep reinforcement learning methods (Lange et al., 2012; Mnih et al., 2013; Levine et al., 2016; Lillicrap et al., 2016) could in principle be applied to directly learn visual servoing policies, such methods tend to require large numbers of samples to learn task-specific behaviors, making them poorly suited for a flexible visual servoing algorithm that can be quickly repurposed to new tasks (e.", "startOffset": 57, "endOffset": 141}, {"referenceID": 27, "context": "Though more standard deep reinforcement learning methods (Lange et al., 2012; Mnih et al., 2013; Levine et al., 2016; Lillicrap et al., 2016) could in principle be applied to directly learn visual servoing policies, such methods tend to require large numbers of samples to learn task-specific behaviors, making them poorly suited for a flexible visual servoing algorithm that can be quickly repurposed to new tasks (e.", "startOffset": 57, "endOffset": 141}, {"referenceID": 28, "context": "Though more standard deep reinforcement learning methods (Lange et al., 2012; Mnih et al., 2013; Levine et al., 2016; Lillicrap et al., 2016) could in principle be applied to directly learn visual servoing policies, such methods tend to require large numbers of samples to learn task-specific behaviors, making them poorly suited for a flexible visual servoing algorithm that can be quickly repurposed to new tasks (e.", "startOffset": 57, "endOffset": 141}, {"referenceID": 10, "context": "We use visual features trained for ImageNet (Deng et al., 2009) classification, though any pre-trained features could in principle be applicable for our method, so long as they provide a suitable degree of invariance to visual distractors such as lighting, occlusion, and changes in viewpoint.", "startOffset": 44, "endOffset": 63}, {"referenceID": 34, "context": "General video prediction is an active research area, with a number of complex but data-hungry models proposed in recent years (Oh et al., 2015; Watter et al., 2015; Mathieu et al., 2016; Xue et al., 2016; Lotter et al., 2017; Jia et al., 2016; Walker et al., 2016; Vondrick et al., 2016).", "startOffset": 126, "endOffset": 287}, {"referenceID": 42, "context": "General video prediction is an active research area, with a number of complex but data-hungry models proposed in recent years (Oh et al., 2015; Watter et al., 2015; Mathieu et al., 2016; Xue et al., 2016; Lotter et al., 2017; Jia et al., 2016; Walker et al., 2016; Vondrick et al., 2016).", "startOffset": 126, "endOffset": 287}, {"referenceID": 31, "context": "General video prediction is an active research area, with a number of complex but data-hungry models proposed in recent years (Oh et al., 2015; Watter et al., 2015; Mathieu et al., 2016; Xue et al., 2016; Lotter et al., 2017; Jia et al., 2016; Walker et al., 2016; Vondrick et al., 2016).", "startOffset": 126, "endOffset": 287}, {"referenceID": 45, "context": "General video prediction is an active research area, with a number of complex but data-hungry models proposed in recent years (Oh et al., 2015; Watter et al., 2015; Mathieu et al., 2016; Xue et al., 2016; Lotter et al., 2017; Jia et al., 2016; Walker et al., 2016; Vondrick et al., 2016).", "startOffset": 126, "endOffset": 287}, {"referenceID": 29, "context": "General video prediction is an active research area, with a number of complex but data-hungry models proposed in recent years (Oh et al., 2015; Watter et al., 2015; Mathieu et al., 2016; Xue et al., 2016; Lotter et al., 2017; Jia et al., 2016; Walker et al., 2016; Vondrick et al., 2016).", "startOffset": 126, "endOffset": 287}, {"referenceID": 21, "context": "General video prediction is an active research area, with a number of complex but data-hungry models proposed in recent years (Oh et al., 2015; Watter et al., 2015; Mathieu et al., 2016; Xue et al., 2016; Lotter et al., 2017; Jia et al., 2016; Walker et al., 2016; Vondrick et al., 2016).", "startOffset": 126, "endOffset": 287}, {"referenceID": 41, "context": "General video prediction is an active research area, with a number of complex but data-hungry models proposed in recent years (Oh et al., 2015; Watter et al., 2015; Mathieu et al., 2016; Xue et al., 2016; Lotter et al., 2017; Jia et al., 2016; Walker et al., 2016; Vondrick et al., 2016).", "startOffset": 126, "endOffset": 287}, {"referenceID": 40, "context": "General video prediction is an active research area, with a number of complex but data-hungry models proposed in recent years (Oh et al., 2015; Watter et al., 2015; Mathieu et al., 2016; Xue et al., 2016; Lotter et al., 2017; Jia et al., 2016; Walker et al., 2016; Vondrick et al., 2016).", "startOffset": 126, "endOffset": 287}, {"referenceID": 17, "context": "This method draws on ideas from regularized fitted Q-iteration (Gordon, 1995; Ernst et al., 2005; Farahmand et al., 2009) and neural fitted Q-iteration (Riedmiller, 2005) to develop a sample-efficient algorithm that can directly estimate the expected return of the visual servo without the use of any additional function approximator.", "startOffset": 63, "endOffset": 121}, {"referenceID": 13, "context": "This method draws on ideas from regularized fitted Q-iteration (Gordon, 1995; Ernst et al., 2005; Farahmand et al., 2009) and neural fitted Q-iteration (Riedmiller, 2005) to develop a sample-efficient algorithm that can directly estimate the expected return of the visual servo without the use of any additional function approximator.", "startOffset": 63, "endOffset": 121}, {"referenceID": 15, "context": "This method draws on ideas from regularized fitted Q-iteration (Gordon, 1995; Ernst et al., 2005; Farahmand et al., 2009) and neural fitted Q-iteration (Riedmiller, 2005) to develop a sample-efficient algorithm that can directly estimate the expected return of the visual servo without the use of any additional function approximator.", "startOffset": 63, "endOffset": 121}, {"referenceID": 35, "context": ", 2009) and neural fitted Q-iteration (Riedmiller, 2005) to develop a sample-efficient algorithm that can directly estimate the expected return of the visual servo without the use of any additional function approximator.", "startOffset": 38, "endOffset": 56}, {"referenceID": 10, "context": "Our choice of semantic features are derived from the VGG-16 network (Simonyan & Zisserman, 2015), which is a convolutional neural network trained for large-scale image recognition on the ImageNet dataset (Deng et al., 2009).", "startOffset": 204, "endOffset": 223}, {"referenceID": 22, "context": "We use the model weights of the original VGG-16 network, which are publicly available as a Caffe model (Jia et al., 2014).", "startOffset": 103, "endOffset": 121}, {"referenceID": 10, "context": "Our choice of semantic features are derived from the VGG-16 network (Simonyan & Zisserman, 2015), which is a convolutional neural network trained for large-scale image recognition on the ImageNet dataset (Deng et al., 2009). Since spatial invariance is undesirable for servoing, we remove some of the max-pooling layers and replace the convolutions that followed them with dilated convolutions, as done by Yu & Koltun (2016). The modified VGG network is shown in Figure 2.", "startOffset": 205, "endOffset": 425}, {"referenceID": 38, "context": "For the other cases, we learn the weights with Trust Region Policy Optimization (TRPO) (Schulman et al., 2015).", "startOffset": 87, "endOffset": 110}, {"referenceID": 36, "context": "The other two prior methods use classical image-based visual servoing (IBVS) (Chaumette & Hutchinson, 2006) with respect to Oriented FAST and Rotated BRIEF (ORB) feature points (Rublee et al., 2011), or feature points extracted from a visual tracker.", "startOffset": 177, "endOffset": 198}, {"referenceID": 8, "context": "For the tracker-based method, we use the Continuous Convolution Operator Tracker (C-COT) (Danelljan et al., 2016) (the current state-of-the-art visual tracker) to get bounding boxes around the car and use the four corners of the box as the feature points for servoing.", "startOffset": 89, "endOffset": 113}], "year": 2017, "abstractText": "Visual servoing involves choosing actions that move a robot in response to observations from a camera, in order to reach a goal configuration in the world. Standard visual servoing approaches typically rely on manually designed features and analytical dynamics models, which limits their generalization capability and often requires extensive application-specific feature and model engineering. In this work, we study how learned visual features, learned predictive dynamics models, and reinforcement learning can be combined to learn visual servoing mechanisms. We focus on target following, with the goal of designing algorithms that can learn a visual servo using low amounts of data of the target in question, to enable quick adaptation to new targets. Our approach is based on servoing the camera in the space of learned visual features, rather than image pixels or manually-designed keypoints. We demonstrate that standard deep features, in our case taken from a model trained for object classification, can be used together with a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions. A key component of our approach is to use a sample-efficient fitted Q-iteration algorithm to learn which features are best suited for the task at hand. We show that we can learn an effective visual servo on a complex synthetic car following benchmark using just 20 training trajectory samples for reinforcement learning. We demonstrate substantial improvement over a conventional approach based on image pixels or hand-designed keypoints, and we show an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. Videos are available at http://rll.berkeley.edu/visual_servoing.", "creator": "LaTeX with hyperref package"}}}