{"id": "1704.00051", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2017", "title": "Reading Wikipedia to Answer Open-Domain Questions", "abstract": "This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval - finding the relevant articles - with that of machine comprehension of text - identifying the answer spans from those articles. Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.", "histories": [["v1", "Fri, 31 Mar 2017 20:39:10 GMT  (1365kb,D)", "http://arxiv.org/abs/1704.00051v1", null], ["v2", "Fri, 28 Apr 2017 03:53:14 GMT  (3133kb,D)", "http://arxiv.org/abs/1704.00051v2", "ACL2017, 10 pages"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["danqi chen", "adam fisch", "jason weston", "antoine bordes"], "accepted": true, "id": "1704.00051"}, "pdf": {"name": "1704.00051.pdf", "metadata": {"source": "CRF", "title": "Reading Wikipedia to Answer Open-Domain Questions", "authors": ["Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes"], "emails": ["danqi@cs.stanford.edu", "afisch@fb.com", "jase@fb.com", "abordes@fb.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to put their skills into practice. (...) Most of them are very well able to put their skills into practice. (...) Most of them are able to put their skills into practice. (...) Most of them are able to put their skills into practice. (...) Most of them are able to put their skills into practice. (...) Most of them are able to put their skills into practice. (...) Most of them are able to put their skills into practice. (...) Most of them are able to put their skills into practice. (...) Most of them are able to put their skills into practice. (...)"}, {"heading": "2 Related Work", "text": "In fact, it is in such a way that it is a matter of a way in which people are able to understand and understand what they are doing to change the world, to change it, to change it, to change it, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen, to widen"}, {"heading": "3 Our System: DrQA", "text": "In the following, we describe our DrQA for MRS system, which consists of two components: (1) the Document Retriever module for finding relevant articles and (2) a machine understanding model, the Document Reader, for extracting answers from a single document or a small collection of documents."}, {"heading": "3.1 Document Retriever", "text": "Following classic QA systems, we use an efficient (non-machine learning) document retrieval system to first narrow down our search space and focus on reading only articles that are likely to be relevant. Compared to the built-in ElasticSearch-based Wikipedia search API (Gormley and Tong, 2015), a simple index search followed by term vector model scoring performs fairly well in this task for many question types. Articles and questions are compared as TF-IDF-weighted bag vectors. We continue to improve our system by incorporating local word sequences with n-gram features. Our most powerful system uses bigram numbers while maintaining speed and storage efficiency by using the hashing of (Weinberger et al., 2009) to reset the bigram to 224 containers with an unsigned murmurmur3 hash. We use Document Retriever as the first part of our full model, resetting each question to Wikipedia article."}, {"heading": "3.2 Document Reader", "text": "Our method works as follows: We first represent each token pi in a paragraph p as attribute vector p, i and pass it as the input to a multi-layer recurrent neural network and so get: {1,.}. We are an RNN model that we apply to each paragraph and then map the predicted answers."}, {"heading": "4 Data", "text": "Our work is based on three types of data: (1) Wikipedia, which serves as a source of knowledge for finding answers; (2) the SQuAD dataset, which is our most important resource for training document readers; and (3) three additional QA datasets (CuratedTREC, WebQuestions, and WikiMovies), which are used in addition to SQuAD, to test the open domain QA capabilities of our entire system and to evaluate our model's ability to learn from multitasking and remote monitoring. Data set statistics are listed in Table 2."}, {"heading": "4.1 Wikipedia (Knowledge Source)", "text": "We use Wikipedia's 2016-12-21 dump2 for all of our extensive experiments as a source of knowledge to answer questions.2https: / / dumps.wikimedia.org / enwiki / newest For each page, only the plain text is extracted and all structured data sections such as lists and illustrations are stripped3. After discarding internal ambiguities, lists, index and sketch pages, we retain 5,075,182 articles consisting of 9,008,962 unique unpacked token types."}, {"heading": "4.2 SQuAD", "text": "The Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) is a machine understanding data set based on Wikipedia. The data set contains 90k / 10k train / development examples with a large hidden test set that only the SQuAD creators can access. Each example consists of a paragraph extracted from a Wikipedia article and an associated human-generated question. The answer is always a span from that paragraph and a model receives recognition if its predicted answer matches it. Two evaluation metrics are used: Exact string match (EM) and F1 score, which measure the weighted average of precision and recall at the token level. Below, we use SQuAD for training and rating our Document Reader for the standard machine understanding task that defines the relevant paragraph in terms of (Rajpurkar et al., 2016)."}, {"heading": "4.3 Open-domain QA Evaluation Resources", "text": "SQuAD is one of the largest universal QA datasets currently available. SQuAD questions were originally collected through a process where each human commentator is shown a paragraph and asked to write a question. As a result, their distribution is quite specific. We therefore propose to train and evaluate our system on other datasets developed for open domain QA, which have been constructed in different ways (not necessarily related to the Wikipedia answer). CuratedTREC This dataset is based on the benchmarks from the TREC QA tasks curated by Baudis and S-Edivy (2015). We use the large version, which contains a total of 2,180 questions from the datasets of TREC 1999, 2000, 2001 and 2002.4WebQuestions Introduced in (Berant et al., 2013), this dataset is designed to answer questions from the KB freebase."}, {"heading": "4.4 Distantly Supervised Data", "text": "All of the QA datasets shown above contain training parts, but CuratedTREC, WebQuestions and WikiMovies only contain question-answers4This dataset is available at https: / / github.com / brmson / dataset-factoid-curated.pairs and not an associated document or paragraph as in SQuAD, and therefore cannot be used directly for training document readers. Following previous work on relationship extraction (Mintz et al., 2009), we use a process to automatically assign paragraphs to such training examples, and then add these examples to our training set.We use the following process for each question-answer pair to build our training set. First, we run Document Retrievers to the question to retrieve the top 5 Wikipedia articles. All paragraphs from these articles without an exact match of the known answer are discarded directly. All paragraphs shorter than 25 or longer than 1500 characters in the AD retriever will be deleted from the DS page if we also delete all of the paragraphs from the page of the retrieters."}, {"heading": "5 Experiments", "text": "In this section, the evaluations of our Document Retrievers and Document Reader modules are presented separately and then tests of their combination, DrQA, for open domain QA are described on the complete Wikipedia."}, {"heading": "5.1 Finding Relevant Articles", "text": "First, we examine the performance of our Document Retriever module on all QA datasets. Table 3 compares the performance of the two approaches described in Section 3.1 with that of Wikipedia Search Engine5 for the task of finding articles that contain the given answer. Specifically, we calculate the ratio of the questions for which the text span of one of the associated answers appears on at least one of the five relevant pages returned by each system. Results on all datasets indicate that our simple approach outperforms Wikipedia searches, especially in terms of Bigram hashing. We also compared retrieval with Okapi BM25 or the use of cosmic distance in Word Embedding Space (by coding questions and articles as Bag-of-Embedients), both of which we found to perform worse."}, {"heading": "5.2 Reader Evaluation on SQuAD", "text": "Next, we evaluate our Document Reader component on the standard SQuAD evaluation (Rajpurkar et al., 2016).Implementation details We use a 3-layer bidirectional LSTM with h = 128 hidden units for both paragraph and question encoding. We apply the Stanford CoreNLP toolkit (Manning et al., 2014) for tokenization and also generate lemmas, part-of-speech and named entity manual features. Finally, all training examples are sorted by length of paragraph and divided into minibatches of 32 examples each. We use Adamax for optimization as described in (Kingma and Ba, 2014). Dropout with p = 0.3 is applied to word embedding and all hidden units of LSTMs.5We use the Wikipedia Search API https: / / www."}, {"heading": "5.3 Full Wikipedia Question Answering", "text": "This year it's at the stage where it will be able to put itself at the top, \"he said."}, {"heading": "6 Conclusion", "text": "Our findings suggest that MRS is a key task that researchers need to focus on, and machine understanding systems alone cannot solve the whole problem; our method integrates search, remote monitoring and multi-task learning to provide an effective overall system; the evaluation of each component and the entire system across multiple benchmarks showed the effectiveness of our approach; future work should aim to improve our DrQA system; two obvious points of attack are: (i) the fact that Document Reader aggregates across multiple paragraphs and documents are directly involved in the training, as it currently trains independently on paragraphs; and (ii) conducting endless training across the Document Retriever and Document Reader pipelines, rather than independent systems."}, {"heading": "Acknowledgments", "text": "The authors thank Pranav Rajpurkar for testing Document Reader on the hidden SQuAD test kit."}], "references": [{"title": "Using wikipedia at the trec qa track", "author": ["David Ahn", "Valentin Jijkoun", "Gilad Mishne", "Karin Mller", "Maarten de Rijke", "Stefan Schlobach."], "venue": "Proceedings of TREC 2004.", "citeRegEx": "Ahn et al\\.,? 2004", "shortCiteRegEx": "Ahn et al\\.", "year": 2004}, {"title": "Dbpedia: A nucleus for a web of open data", "author": ["S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives."], "venue": "The semantic web, Springer, pages 722\u2013735.", "citeRegEx": "Auer et al\\.,? 2007", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "YodaQA: a modular question answering system pipeline", "author": ["Petr Baudi\u0161."], "venue": "POSTER 2015-19th International Student Conference on Electrical Engineering. pages 1156\u20131165.", "citeRegEx": "Baudi\u0161.,? 2015", "shortCiteRegEx": "Baudi\u0161.", "year": 2015}, {"title": "Modeling of the question answering task in the YodaQA system", "author": ["Petr Baudi\u0161", "Jan \u0160ediv\u1ef3."], "venue": "International Conference of the CrossLanguage Evaluation Forum for European Languages. Springer, pages 222\u2013228.", "citeRegEx": "Baudi\u0161 and \u0160ediv\u1ef3.,? 2015", "shortCiteRegEx": "Baudi\u0161 and \u0160ediv\u1ef3.", "year": 2015}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1533\u20131544.", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1506.02075 .", "citeRegEx": "Bordes et al\\.,? 2015", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "An analysis of the AskMSR question-answering system", "author": ["Eric Brill", "Susan Dumais", "Michele Banko."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 257\u2013264.", "citeRegEx": "Brill et al\\.,? 2002", "shortCiteRegEx": "Brill et al\\.", "year": 2002}, {"title": "Mining knowledge from Wikipedia for the question answering task", "author": ["Davide Buscaldi", "Paolo Rosso."], "venue": "International Conference on Language Resources and Evaluation (LREC). pages 727\u2013730.", "citeRegEx": "Buscaldi and Rosso.,? 2006", "shortCiteRegEx": "Buscaldi and Rosso.", "year": 2006}, {"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Learning to learn, Springer, pages 95\u2013133.", "citeRegEx": "Caruana.,? 1998", "shortCiteRegEx": "Caruana.", "year": 1998}, {"title": "A thorough examination of the CNN/Daily Mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D Manning."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "International Conference on Machine Learning (ICML).", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni."], "venue": "ACM SIGKDD international conference on Knowledge discovery and data mining. pages 1156\u20131165.", "citeRegEx": "Fader et al\\.,? 2014", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "Building Watson: An overview of the DeepQA project. AI magazine 31(3):59\u201379", "author": ["David Ferrucci", "Eric Brown", "Jennifer Chu-Carroll", "James Fan", "David Gondek", "Aditya A Kalyanpur", "Adam Lally", "J William Murdock", "Eric Nyberg", "John Prager"], "venue": null, "citeRegEx": "Ferrucci et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ferrucci et al\\.", "year": 2010}, {"title": "Elasticsearch: The Definitive Guide", "author": ["Clinton Gormley", "Zachary Tong."], "venue": "\u201d O\u2019Reilly Media, Inc.\u201d.", "citeRegEx": "Gormley and Tong.,? 2015", "shortCiteRegEx": "Gormley and Tong.", "year": 2015}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka."], "venue": "arXiv preprint arXiv:1410.5401 .", "citeRegEx": "Graves et al\\.,? 2014", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Wikireading: A novel large-scale language understanding task over wikipedia", "author": ["Daniel Hewlett", "Alexandre Lacoste", "Llion Jones", "Illia Polosukhin", "Andrew Fandrianto", "Jay Han", "Matthew Kelcey", "David Berthelot."], "venue": "Association for Computational Lin-", "citeRegEx": "Hewlett et al\\.,? 2016", "shortCiteRegEx": "Hewlett et al\\.", "year": 2016}, {"title": "The Goldilocks Principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "What makes ImageNet good for transfer learning", "author": ["Minyoung Huh", "Pulkit Agrawal", "Alexei A Efros"], "venue": null, "citeRegEx": "Huh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huh et al\\.", "year": 2016}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Mohit Iyyer", "Jordan L Boyd-Graber", "Leonardo Max Batista Claudino", "Richard Socher", "Hal Daum\u00e9 III."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Iyyer et al\\.,? 2014", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "From particular to general: A preliminary case study of transfer learning in reading comprehension", "author": ["Rudolf Kadlec", "Ondrej Bajgar", "Jan Kleindienst."], "venue": "Machine Intelligence Workshop, NIPS .", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Learning recurrent span representations for extractive question answering", "author": ["Kenton Lee", "Tom Kwiatkowski", "Ankur Parikh", "Dipanjan Das."], "venue": "arXiv preprint arXiv:1611.01436 .", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Christopher D Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J Bethard", "David McClosky."], "venue": "Association for Computational Linguistics (ACL). pages 55\u201360.", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Key-value memory networks for directly reading documents", "author": ["Alexander H. Miller", "Adam Fisch", "Jesse Dodge", "AmirHossein Karimi", "Antoine Bordes", "Jason Weston."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1400\u2013", "citeRegEx": "Miller et al\\.,? 2016", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Daniel Jurafsky."], "venue": "Association for Computational Linguistics and International Joint Conference on Natural Language Processing", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "SQuAD: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Open domain question answering using Wikipedia-based knowledge model", "author": ["Pum-Mo Ryu", "Myung-Gil Jang", "Hyun-Ki Kim."], "venue": "Information Processing & Management 50(5):683\u2013692.", "citeRegEx": "Ryu et al\\.,? 2014", "shortCiteRegEx": "Ryu et al\\.", "year": 2014}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."], "venue": "arXiv preprint arXiv:1611.01603 .", "citeRegEx": "Seo et al\\.,? 2016", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Open domain question answering via semantic enrichment", "author": ["Huan Sun", "Hao Ma", "Wen-tau Yih", "Chen-Tse Tsai", "Jingjing Liu", "Ming-Wei Chang."], "venue": "Proceedings of the 24th International Conference on World Wide Web. ACM, pages 1045\u20131055.", "citeRegEx": "Sun et al\\.,? 2015", "shortCiteRegEx": "Sun et al\\.", "year": 2015}, {"title": "Multi-perspective context matching for machine comprehension", "author": ["Zhiguo Wang", "Haitao Mi", "Wael Hamza", "Radu Florian."], "venue": "arXiv preprint arXiv:1612.04211 .", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Feature hashing for large scale multitask learning", "author": ["Kilian Weinberger", "Anirban Dasgupta", "John Langford", "Alex Smola", "Josh Attenberg."], "venue": "International Conference on Machine Learning (ICML). pages 1113\u20131120.", "citeRegEx": "Weinberger et al\\.,? 2009", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Dynamic coattention networks for question answering", "author": ["Caiming Xiong", "Victor Zhong", "Richard Socher."], "venue": "arXiv preprint arXiv:1611.01604 .", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "Unlike knowledge bases (KBs) such as Freebase (Bollacker et al., 2008) or DBPedia (Auer et al.", "startOffset": 46, "endOffset": 70}, {"referenceID": 1, "context": ", 2008) or DBPedia (Auer et al., 2007), which are easier for computers to process but too sparsely populated for open-domain question answering (Miller et al.", "startOffset": 19, "endOffset": 38}, {"referenceID": 26, "context": ", 2007), which are easier for computers to process but too sparsely populated for open-domain question answering (Miller et al., 2016), Wikipedia contains up-to-date knowledge", "startOffset": 113, "endOffset": 134}, {"referenceID": 14, "context": "Large-scale QA systems like IBM\u2019s DeepQA (Ferrucci et al., 2010) rely on multiple sources to answer: Wikipedia can be one of", "startOffset": 41, "endOffset": 64}, {"referenceID": 29, "context": "This challenge thus encourages research in the ability of a machine to read, a key motivation for the machine comprehension subfield and the creation of datasets such as SQuAD (Rajpurkar et al., 2016), CNN/Daily Mail (Hermann et al.", "startOffset": 176, "endOffset": 200}, {"referenceID": 17, "context": ", 2016), CNN/Daily Mail (Hermann et al., 2015) and CBT (Hill et al.", "startOffset": 24, "endOffset": 46}, {"referenceID": 19, "context": ", 2015) and CBT (Hill et al., 2016).", "startOffset": 16, "endOffset": 35}, {"referenceID": 29, "context": "outperforms the built-in Wikipedia search engine and that Document Reader reaches state-of-theart results on the very competitive SQuAD benchmark (Rajpurkar et al., 2016).", "startOffset": 146, "endOffset": 170}, {"referenceID": 5, "context": "With the development of KBs, many recent innovations have occurred in the context of QA from KBs with the creation of resources like WebQuestions (Berant et al., 2013) and SimpleQuestions (Bordes et al.", "startOffset": 146, "endOffset": 167}, {"referenceID": 7, "context": ", 2013) and SimpleQuestions (Bordes et al., 2015) based on the Freebase KB (Bollacker et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 6, "context": ", 2015) based on the Freebase KB (Bollacker et al., 2008), or on automatically extracted KBs, e.", "startOffset": 33, "endOffset": 57}, {"referenceID": 13, "context": ", OpenIE triples and NELL (Fader et al., 2014).", "startOffset": 26, "endOffset": 46}, {"referenceID": 21, "context": ", 2014) and the release of new training and evaluation datasets like QuizBowl (Iyyer et al., 2014), CNN/Daily Mail based on news articles (Hermann et al.", "startOffset": 78, "endOffset": 98}, {"referenceID": 17, "context": ", 2014), CNN/Daily Mail based on news articles (Hermann et al., 2015), CBT based on children books (Hill et al.", "startOffset": 47, "endOffset": 69}, {"referenceID": 19, "context": ", 2015), CBT based on children books (Hill et al., 2016), or SQuAD (Rajpurkar et al.", "startOffset": 37, "endOffset": 56}, {"referenceID": 29, "context": ", 2016), or SQuAD (Rajpurkar et al., 2016) and WikiReading (Hewlett et al.", "startOffset": 18, "endOffset": 42}, {"referenceID": 18, "context": ", 2016) and WikiReading (Hewlett et al., 2016), both based on Wikipedia.", "startOffset": 24, "endOffset": 46}, {"referenceID": 30, "context": "The authors of (Ryu et al., 2014) perform open-domain QA using a Wikipedia-based knowledge model.", "startOffset": 15, "endOffset": 33}, {"referenceID": 0, "context": "Similarly (Ahn et al., 2004) also combine Wikipedia as a text resource with other resources,", "startOffset": 10, "endOffset": 28}, {"referenceID": 9, "context": "Buscaldi and Rosso (2006) also mine knowledge from Wikipedia for QA.", "startOffset": 0, "endOffset": 26}, {"referenceID": 32, "context": "There are a number of highly developed full pipeline QA approaches using either the Web, as does QuASE (Sun et al., 2015), or Wikipedia as a resource, as do Microsoft\u2019s AskMSR (Brill et al.", "startOffset": 103, "endOffset": 121}, {"referenceID": 8, "context": ", 2015), or Wikipedia as a resource, as do Microsoft\u2019s AskMSR (Brill et al., 2002), IBM\u2019s DeepQA (Ferrucci et al.", "startOffset": 62, "endOffset": 82}, {"referenceID": 14, "context": ", 2002), IBM\u2019s DeepQA (Ferrucci et al., 2010) and YodaQA (Baudi\u0161, 2015; Baudi\u0161 and \u0160ediv\u1ef3, 2015) \u2013 the latter of which is open source and hence reproducible for comparison purposes.", "startOffset": 22, "endOffset": 45}, {"referenceID": 3, "context": ", 2010) and YodaQA (Baudi\u0161, 2015; Baudi\u0161 and \u0160ediv\u1ef3, 2015) \u2013 the latter of which is open source and hence reproducible for comparison purposes.", "startOffset": 19, "endOffset": 58}, {"referenceID": 4, "context": ", 2010) and YodaQA (Baudi\u0161, 2015; Baudi\u0161 and \u0160ediv\u1ef3, 2015) \u2013 the latter of which is open source and hence reproducible for comparison purposes.", "startOffset": 19, "endOffset": 58}, {"referenceID": 10, "context": "Multitask learning (Caruana, 1998) and task transfer (e.", "startOffset": 19, "endOffset": 34}, {"referenceID": 20, "context": ", in the computer vision community using ImageNet (Huh et al., 2016)) have a rich history in machine learning, as well as in NLP", "startOffset": 50, "endOffset": 68}, {"referenceID": 12, "context": "in particular (Collobert and Weston, 2008).", "startOffset": 14, "endOffset": 42}, {"referenceID": 22, "context": "(2015) combined WebQuestions and SimpleQuestions using distant supervision with Freebase as the KB to give slight improvements on both datasets, although poor performance was reported when training on only one dataset and testing on the other, showing that task transfer is indeed a challenging subject; see also (Kadlec et al., 2016) for a similar conclusion.", "startOffset": 313, "endOffset": 334}, {"referenceID": 11, "context": "in particular (Collobert and Weston, 2008). Several works have attempted to combine multiple QA training datasets via multitask learning to (i) achieve improvement across the datasets via task transfer; and (ii) to provide a single general system capable of asking different kinds of questions due to the inevitably different data distributions across the source datasets. Fader et al. (2014) used WebQuestions, TREC and WikiAnswers with four KBs as knowledge sources and reported improvement on the latter two datasets through multitasking.", "startOffset": 15, "endOffset": 393}, {"referenceID": 7, "context": "Bordes et al. (2015) combined WebQuestions and SimpleQuestions using distant supervision with Freebase as the KB to give slight improvements on both datasets, although poor performance was reported when training on only one dataset and testing on the other, showing that task transfer is indeed a challenging subject; see also (Kadlec et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 15, "context": "A simple inverted index lookup followed by term vector model scoring performs quite well on this task for many question types, compared to the built-in ElasticSearch based Wikipedia Search API (Gormley and Tong, 2015).", "startOffset": 193, "endOffset": 217}, {"referenceID": 34, "context": "Our best performing system uses bigram counts while preserving speed and memory efficiency by using the hashing of (Weinberger et al., 2009) to map the bigrams to 224 bins with an unsigned murmur3 hash.", "startOffset": 115, "endOffset": 140}, {"referenceID": 17, "context": "Our Document Reader model is inspired by the recent success of neural network models on machine comprehension tasks, in a similar spirit to the AttentiveReader described in (Hermann et al., 2015; Chen et al., 2016).", "startOffset": 173, "endOffset": 214}, {"referenceID": 11, "context": "Our Document Reader model is inspired by the recent success of neural network models on machine comprehension tasks, in a similar spirit to the AttentiveReader described in (Hermann et al., 2015; Chen et al., 2016).", "startOffset": 173, "endOffset": 214}, {"referenceID": 28, "context": "We use the 300-dimensional Glove word embeddings trained from 840B Web crawl data (Pennington et al., 2014).", "startOffset": 82, "endOffset": 107}, {"referenceID": 24, "context": "\u2022 Aligned question embedding: following (Lee et al., 2016) and other recent works, the last part we incorporate is an aligned question embedding", "startOffset": 40, "endOffset": 58}, {"referenceID": 29, "context": "The Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) is a dataset for machine comprehension based on Wikipedia.", "startOffset": 48, "endOffset": 72}, {"referenceID": 29, "context": "In the following, we use SQuAD for training and evaluating our Document Reader for the standard machine comprehension task given the relevant paragraph as defined in (Rajpurkar et al., 2016).", "startOffset": 166, "endOffset": 190}, {"referenceID": 3, "context": "benchmarks from the TREC QA tasks that have been curated by Baudi\u0161 and \u0160ediv\u1ef3 (2015). We use the large version, which contains a total of 2,180 questions extracted from the datasets from TREC 1999, 2000, 2001 and 2002.", "startOffset": 60, "endOffset": 85}, {"referenceID": 5, "context": "WebQuestions Introduced in (Berant et al., 2013), this dataset is built to answer questions from the Freebase KB.", "startOffset": 27, "endOffset": 48}, {"referenceID": 26, "context": "WikiMovies This dataset, introduced in (Miller et al., 2016), contains 96k question-answer pairs in the domain of movies.", "startOffset": 39, "endOffset": 60}, {"referenceID": 27, "context": "Following previous work on distant supervision (DS) for relation extraction (Mintz et al., 2009), we use a", "startOffset": 76, "endOffset": 96}, {"referenceID": 36, "context": "Dynamic Coattention Networks (Xiong et al., 2016) 65.", "startOffset": 29, "endOffset": 49}, {"referenceID": 31, "context": "9 BiDAF (Seo et al., 2016) 67.", "startOffset": 8, "endOffset": 26}, {"referenceID": 33, "context": "3 Multi-Perspective Matching (Wang et al., 2016)\u2020 66.", "startOffset": 29, "endOffset": 48}, {"referenceID": 29, "context": "Next we evaluate our Document Reader component on the standard SQuAD evaluation (Rajpurkar et al., 2016).", "startOffset": 80, "endOffset": 104}, {"referenceID": 25, "context": "We apply the Stanford CoreNLP toolkit (Manning et al., 2014) for tokenization and also generating lemma, part-of-speech, and named entity manual features.", "startOffset": 38, "endOffset": 60}, {"referenceID": 23, "context": "We use Adamax for optimization as described in (Kingma and Ba, 2014).", "startOffset": 47, "endOffset": 68}, {"referenceID": 3, "context": "daQA (Baudi\u0161, 2015), giving results which were previously reported on CuratedTREC and WebQuestions.", "startOffset": 5, "endOffset": 19}], "year": 2017, "abstractText": "This paper proposes to tackle opendomain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval \u2013 finding the relevant articles \u2013 with that of machine comprehension of text \u2013 identifying the answer spans from those articles. Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.", "creator": "LaTeX with hyperref package"}}}