{"id": "1704.00109", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2017", "title": "Snapshot Ensembles: Train 1, get M for free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "histories": [["v1", "Sat, 1 Apr 2017 02:42:55 GMT  (1952kb,D)", "http://arxiv.org/abs/1704.00109v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gao huang", "yixuan li", "geoff pleiss", "zhuang liu", "john e hopcroft", "kilian q weinberger"], "accepted": true, "id": "1704.00109"}, "pdf": {"name": "1704.00109.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "emails": ["yl2363}@cornell.edu,", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu,", "kqw4@cornell.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves."}, {"heading": "2 RELATED WORK", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "3 SNAPSHOT ENSEMBLING", "text": "At the core of Snapshot Ensembling is an optimization process that visits multiple local minimums before leading to a definitive solution. We take model snapshots of these different minimums and average their predictions of test time.Ensembles work best when the individual models (1) have low test errors and (2) do not overlap in the number of learning examples they misclassify. Along most of the optimization paths, neural network weight allocations tend not to correspond with low test errors. In fact, it is often observed that the validation error falls significantly only after the learning rate, which typically occurs after several hundred epochs. Our approach is inspired by the observation that neural network formation for fewer epochs and the deposition of the learning rate earlier is low (Loshchilov & Hutter, 2016)."}, {"heading": "4 EXPERIMENTS", "text": "We show the effectiveness of snapshot ensembles using several benchmark datasets compared to competitive baselines. We perform all experiments with Torch 7 (Collobert et al., 2011) 2."}, {"heading": "4.1 DATASETS", "text": "The two CIFAR datasets (Krizhevsky & Hinton, 2009) consist of 32 x 32 pixel color nature images. CIFAR-10 (C10) and CIFAR-100 (C100) images are from 10 and 100 classes, respectively. For each dataset, there are 50,000 training images and 10,000 images reserved for testing. We use a standard data augmentation scheme (Lin et al., 2013; Romero et al., 2014; Lee et al., 2015; Springenberg et al., 2014; Srivastava et al., 2015; Huang et al., 2016b; Larsson et al., 2016) in which the images are padded with 4 pixels on each side, randomly cropped to produce 32 x 32 images and mirrored horizontally."}, {"heading": "4.2 TRAINING SETTING", "text": "We test several state-of-the-art architectures, including the residual networks (ResNet) (He et al., 2016a), Wide ResNet (Zagoruyko & Komodakis, 2016) and DenseNet (Huang et al., 2016a). For ResNet, we follow the same setup as (Huang et al., 2016a), with depth L = 100 and growth rate. In addition, we also rate our method as default ResNet at a small DenseNet level. For DenseNet, our large model follows the same setup as (Huang et al., 2016a), with depth L = 100 and growth rate."}, {"heading": "4.4 DIVERSITY OF MODEL ENSEMBLES", "text": "Parameter Space. We assume that the cyclical learning rate plan generates smooth parameter interpolations that are not only precise, but also diverse in terms of model predictions. We measure this diversity qualitatively by visualizing the local minima to which the models converge. To do this, we can interpolate snapshot models linearly, as described by Goodfellow et al. (2014). Let J (\u03b8) convert the test of a model using parameters converted (2 \u2212), given that the parameters from models 1 and 2 - we can calculate the loss for a convex combination of model parameters: J (1) + (1 \u2212) is the test of a model converted with parameters (2 \u2212), at which a mixing coefficient is given. Setting to 1 results in a parameter that is fully analog 1, while setting to 0 the parameters are implicit, there are the parameters dependent 1, while setting J gives the minimum diversibility to 1 (0)."}, {"heading": "5 DISCUSSION", "text": "We introduce Snapshot Ensembling, a simple way to obtain neural network ensembles at no additional training cost. Our method leverages the SGD's ability to approach local minimums and escape them when the learning rate is lowered, allowing the model to visit multiple weight assignments that lead to increasingly accurate predictions over the course of the training. We use this force with the cyclical learning rate schedule proposed by Loshchilov & Hutter (2016) and store model snapshots at each point of convergence. We demonstrate in multiple experiments that all snapshots are accurate but produce different predictions from each other and are therefore well suited for test time ensembles. Ensembles of these snapshots greatly improve the state of the art on CIFAR-10, CIFAR-100 and SVHN."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We would like to thank Ilya Loshchilov and Frank Hutter for their insightful comments on the cyclical cosinesformed learning rate. The authors are supported in part by the scholarships III-1618134, III-1526012, IIS1149882 of the National Science Foundation, the US Army Research Office W911NF-141-0477 and the Bill and Melinda Gates Foundation."}, {"heading": "A. Single model and Snapshot Ensemble performance over time", "text": "In Figures 7-9, we compare the test error of snapshots ensembles with the error of individual model snapshots. (The blue curve shows the test error of a single model snapshot based on a cyclic cosine learning rate. The green curve shows the test error of a single model trained for 300 epochs using a standard learning rate plan. (Note that, unlike Figure 3, we construct these ensembles using the earliest snapshots.) For reference, the red dashed line in each panel represents the test error of a single model trained for 300 epochs using a standard learning rate plan. Without snapshot ensembles, the test error of the final model using a cyclic learning rate - the correct point in the blue curve - does not exceed the individual model baseline in about half of the cases. In many cases, ensembles of only 2 or 3 model snapshots are able to achieve the performance of individual model snapshots at surprisingly high speeds."}], "references": [{"title": "Cristian Bucilu, Rich Caruana, and Alexandru Niculescu-Mizil", "author": ["L\u00e9on Bottou. Large-scale machine learning with stochastic gradient descent. In COMPSTAT."], "venue": "Model compression. In KDD, 2006. Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew, and Alex Ksikes. Ensemble selection from libraries of models. In ICML, 2004.", "citeRegEx": "COMPSTAT.,? 2010", "shortCiteRegEx": "COMPSTAT.", "year": 2010}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Yann N Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Qualitatively characterizing neural network optimization problems", "author": ["Ian J Goodfellow", "Oriol Vinyals", "Andrew M Saxe"], "venue": "arXiv preprint arXiv:1412.6544,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Neural network ensembles", "author": ["Lars Kai Hansen", "Peter Salamon"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Hansen and Salamon.,? \\Q1990\\E", "shortCiteRegEx": "Hansen and Salamon.", "year": 1990}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In CVPR,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In ECCV,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Densely connected convolutional networks", "author": ["Gao Huang", "Zhuang Liu", "Kilian Q Weinberger"], "venue": "arXiv preprint arXiv:1608.06993,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger"], "venue": "In ECCV,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In ICCV,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Sbastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.2007,", "citeRegEx": "Jean et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2014}, {"title": "Deep learning without poor local minima", "author": ["Kenji Kawaguchi"], "venue": "arXiv preprint arXiv:1605.07110,", "citeRegEx": "Kawaguchi.,? \\Q2016\\E", "shortCiteRegEx": "Kawaguchi.", "year": 2016}, {"title": "On large-batch training for deep learning: Generalization gap and sharp minima", "author": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "venue": "arXiv preprint arXiv:1609.04836,", "citeRegEx": "Keskar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Keskar et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Neural network ensembles, cross validation, and active learning", "author": ["Anders Krogh", "Jesper Vedelsby"], "venue": "In NIPS,", "citeRegEx": "Krogh and Vedelsby,? \\Q1995\\E", "shortCiteRegEx": "Krogh and Vedelsby", "year": 1995}, {"title": "Zoneout: Regularizing rnns by randomly preserving hidden activations", "author": ["David Krueger", "Tegan Maharaj", "J\u00e1nos Kram\u00e1r", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh Goyal", "Yoshua Bengio", "Hugo Larochelle", "Aaron Courville"], "venue": "arXiv preprint arXiv:1606.01305,", "citeRegEx": "Krueger et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krueger et al\\.", "year": 2016}, {"title": "Temporal ensembling for semi-supervised learning", "author": ["Samuli Laine", "Timo Aila"], "venue": "arXiv preprint arXiv:1610.02242,", "citeRegEx": "Laine and Aila.,? \\Q2016\\E", "shortCiteRegEx": "Laine and Aila.", "year": 2016}, {"title": "Fractalnet: Ultra-deep neural networks without residuals", "author": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "venue": "arXiv preprint arXiv:1605.07648,", "citeRegEx": "Larsson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Larsson et al\\.", "year": 2016}, {"title": "Sgdr: Stochastic gradient descent with restarts", "author": ["Ilya Loshchilov", "Frank Hutter"], "venue": "arXiv preprint arXiv:1608.03983,", "citeRegEx": "Loshchilov and Hutter.,? \\Q2016\\E", "shortCiteRegEx": "Loshchilov and Hutter.", "year": 2016}, {"title": "Boosted convolutional neural networks. 2016", "author": ["Mohammad Moghimi", "Mohammad Saberian", "Jian Yang", "Li-Jia Li", "Nuno Vasconcelos", "Serge Belongie"], "venue": null, "citeRegEx": "Moghimi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Moghimi et al\\.", "year": 2016}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.6550,", "citeRegEx": "Romero et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2014}, {"title": "Edinburgh neural machine translation systems for wmt 16", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "arXiv preprint arXiv:1606.02891,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Convolutional neural networks applied to house numbers digit classification", "author": ["Pierre Sermanet", "Soumith Chintala", "Yann LeCun"], "venue": "In ICPR,", "citeRegEx": "Sermanet et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2012}, {"title": "Swapout: Learning an ensemble of deep architectures", "author": ["Saurabh Singh", "Derek Hoiem", "David Forsyth"], "venue": "arXiv preprint arXiv:1605.06465,", "citeRegEx": "Singh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2016}, {"title": "No more pesky learning rate guessing games", "author": ["Leslie N. Smith"], "venue": "CoRR, abs/1506.01186,", "citeRegEx": "Smith.,? \\Q2016\\E", "shortCiteRegEx": "Smith.", "year": 2016}, {"title": "Striving for simplicity: The all convolutional net", "author": ["Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller"], "venue": "arXiv preprint arXiv:1412.6806,", "citeRegEx": "Springenberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Fast committee learning: Preliminary results", "author": ["A Swann", "N Allinson"], "venue": "Electronics Letters,", "citeRegEx": "Swann and Allinson.,? \\Q1998\\E", "shortCiteRegEx": "Swann and Allinson.", "year": 1998}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In ICML,", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Horizontal and vertical ensemble with deep representation for classification", "author": ["Jingjing Xie", "Bing Xu", "Zhang Chuang"], "venue": "arXiv preprint arXiv:1306.2759,", "citeRegEx": "Xie et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2013}, {"title": "Wide residual networks", "author": ["Sergey Zagoruyko", "Nikos Komodakis"], "venue": "arXiv preprint arXiv:1605.07146,", "citeRegEx": "Zagoruyko and Komodakis.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko and Komodakis.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "Stochastic Gradient Descent (SGD) (Bottou, 2010) and its accelerated variants (Kingma & Ba, 2014; Duchi et al., 2011) have become the de-facto approaches for optimizing deep neural networks.", "startOffset": 78, "endOffset": 117}, {"referenceID": 2, "context": "The popularity of SGD can be attributed to its ability to avoid and even escape spurious saddle-points and local minima (Dauphin et al., 2014).", "startOffset": 120, "endOffset": 142}, {"referenceID": 15, "context": "SGD tends to avoid sharper local minima because gradients are computed from small mini-batches and are therefore inexact (Keskar et al., 2016).", "startOffset": 121, "endOffset": 142}, {"referenceID": 14, "context": "It is well established (Kawaguchi, 2016) that the number of possible local minima grows exponentially with the number of parameters\u2014of which modern neural networks can have millions.", "startOffset": 23, "endOffset": 40}, {"referenceID": 2, "context": "The popularity of SGD can be attributed to its ability to avoid and even escape spurious saddle-points and local minima (Dauphin et al., 2014). Although avoiding these spurious solutions is generally considered positive, in this paper we argue that these local minima contain useful information that may in fact improve model performance. Although deep networks typically never converge to a global minimum, there is a notion of \u201cgood\u201d and \u201cbad\u201d local minima with respect to generalization. Keskar et al. (2016) argue that local minima with flat basins tend to generalize better.", "startOffset": 121, "endOffset": 512}, {"referenceID": 3, "context": "Imagenet (Deng et al., 2009) or Kaggle1, are won by ensembles of deep learning architectures.", "startOffset": 9, "endOffset": 28}, {"referenceID": 3, "context": "Imagenet (Deng et al., 2009) or Kaggle1, are won by ensembles of deep learning architectures. Despite its obvious advantages, the use of ensembling for deep networks is not nearly as widespread as it is for other algorithms. One likely reason for this lack of adaptation may be the cost of learning multiple neural networks. Training deep networks can last for weeks, even on high performance hardware with GPU acceleration. As the training cost for ensembles increases linearly, ensembles can quickly becomes uneconomical for most researchers without access to industrial scale computational resources. In this paper we focus on the seemingly-contradictory goal of learning an ensemble of multiple neural networks without incurring any additional training costs. We achieve this goal with a training method that is simple and straight-forward to implement. Our approach leverages the non-convex nature of neural networks and the ability of SGD to converge to and escape from local minima on demand. Instead of training M neural networks independently from scratch, we let SGD converge M times to local minima along its optimization path. Each time the model converges, we save the weights and add the corresponding network to our ensemble. We then restart the optimization with a large learning rate to escape the current local minimum. More specifically, we adopt the cycling procedure suggested by Loshchilov & Hutter (2016), in which the learning rate is abruptly raised and then quickly lowered to follow a cosine function.", "startOffset": 10, "endOffset": 1428}, {"referenceID": 34, "context": "As an alternative to traditional ensembles, so-called \u201cimplicit\u201d ensembles have high efficiency during both training and testing (Srivastava et al., 2014; Wan et al., 2013; Huang et al., 2016b; Singh et al., 2016; Krueger et al., 2016).", "startOffset": 129, "endOffset": 235}, {"referenceID": 29, "context": "As an alternative to traditional ensembles, so-called \u201cimplicit\u201d ensembles have high efficiency during both training and testing (Srivastava et al., 2014; Wan et al., 2013; Huang et al., 2016b; Singh et al., 2016; Krueger et al., 2016).", "startOffset": 129, "endOffset": 235}, {"referenceID": 20, "context": "As an alternative to traditional ensembles, so-called \u201cimplicit\u201d ensembles have high efficiency during both training and testing (Srivastava et al., 2014; Wan et al., 2013; Huang et al., 2016b; Singh et al., 2016; Krueger et al., 2016).", "startOffset": 129, "endOffset": 235}, {"referenceID": 34, "context": "DropConnect (Wan et al., 2013) uses a similar trick to create ensembles at test time by dropping connections (weights) during training instead of nodes.", "startOffset": 12, "endOffset": 30}, {"referenceID": 29, "context": "Finally, Swapout (Singh et al., 2016) is a stochastic training method that generalizes Dropout and Stochastic Depth.", "startOffset": 17, "endOffset": 37}, {"referenceID": 9, "context": "Several recent publications focus on reducing the test time cost of ensembles, by transferring the \u201cknowledge\u201d of cumbersome ensembles into a single model (Bucilu et al., 2006; Hinton et al., 2015).", "startOffset": 155, "endOffset": 197}, {"referenceID": 9, "context": ", 2006; Hinton et al., 2015). Hinton et al. (2015) propose to use an ensemble of multiple networks as the target of a single (smaller) network.", "startOffset": 8, "endOffset": 51}, {"referenceID": 9, "context": ", 2006; Hinton et al., 2015). Hinton et al. (2015) propose to use an ensemble of multiple networks as the target of a single (smaller) network. Our proposed method is complementary to these works as we aim to reduce the training cost of ensembles rather than the test-time cost. Perhaps most similar to our work is that of Swann & Allinson (1998) and Xie et al.", "startOffset": 8, "endOffset": 347}, {"referenceID": 9, "context": ", 2006; Hinton et al., 2015). Hinton et al. (2015) propose to use an ensemble of multiple networks as the target of a single (smaller) network. Our proposed method is complementary to these works as we aim to reduce the training cost of ensembles rather than the test-time cost. Perhaps most similar to our work is that of Swann & Allinson (1998) and Xie et al. (2013), who explore creating ensembles from slices of the learning trajectory.", "startOffset": 8, "endOffset": 369}, {"referenceID": 9, "context": ", 2006; Hinton et al., 2015). Hinton et al. (2015) propose to use an ensemble of multiple networks as the target of a single (smaller) network. Our proposed method is complementary to these works as we aim to reduce the training cost of ensembles rather than the test-time cost. Perhaps most similar to our work is that of Swann & Allinson (1998) and Xie et al. (2013), who explore creating ensembles from slices of the learning trajectory. Xie et al. introduce the horizontal and vertical ensembling method, which combines the output of networks within a range of training epochs. More recently, Jean et al. (2014) and Sennrich et al.", "startOffset": 8, "endOffset": 616}, {"referenceID": 9, "context": ", 2006; Hinton et al., 2015). Hinton et al. (2015) propose to use an ensemble of multiple networks as the target of a single (smaller) network. Our proposed method is complementary to these works as we aim to reduce the training cost of ensembles rather than the test-time cost. Perhaps most similar to our work is that of Swann & Allinson (1998) and Xie et al. (2013), who explore creating ensembles from slices of the learning trajectory. Xie et al. introduce the horizontal and vertical ensembling method, which combines the output of networks within a range of training epochs. More recently, Jean et al. (2014) and Sennrich et al. (2016) show improvement by ensembling the intermediate stages of model training.", "startOffset": 8, "endOffset": 643}, {"referenceID": 9, "context": ", 2006; Hinton et al., 2015). Hinton et al. (2015) propose to use an ensemble of multiple networks as the target of a single (smaller) network. Our proposed method is complementary to these works as we aim to reduce the training cost of ensembles rather than the test-time cost. Perhaps most similar to our work is that of Swann & Allinson (1998) and Xie et al. (2013), who explore creating ensembles from slices of the learning trajectory. Xie et al. introduce the horizontal and vertical ensembling method, which combines the output of networks within a range of training epochs. More recently, Jean et al. (2014) and Sennrich et al. (2016) show improvement by ensembling the intermediate stages of model training. Laine & Aila (2016) propose a temporal ensembling method for semi-supervised learning, which achieves consensus among models trained with different regularization and augmentation conditions for better generalization performance.", "startOffset": 8, "endOffset": 737}, {"referenceID": 9, "context": ", 2006; Hinton et al., 2015). Hinton et al. (2015) propose to use an ensemble of multiple networks as the target of a single (smaller) network. Our proposed method is complementary to these works as we aim to reduce the training cost of ensembles rather than the test-time cost. Perhaps most similar to our work is that of Swann & Allinson (1998) and Xie et al. (2013), who explore creating ensembles from slices of the learning trajectory. Xie et al. introduce the horizontal and vertical ensembling method, which combines the output of networks within a range of training epochs. More recently, Jean et al. (2014) and Sennrich et al. (2016) show improvement by ensembling the intermediate stages of model training. Laine & Aila (2016) propose a temporal ensembling method for semi-supervised learning, which achieves consensus among models trained with different regularization and augmentation conditions for better generalization performance. Finally, Moghimi et al. (2016) show that boosting can be applied to convolutional neural networks to create strong ensembles.", "startOffset": 8, "endOffset": 978}, {"referenceID": 9, "context": ", 2006; Hinton et al., 2015). Hinton et al. (2015) propose to use an ensemble of multiple networks as the target of a single (smaller) network. Our proposed method is complementary to these works as we aim to reduce the training cost of ensembles rather than the test-time cost. Perhaps most similar to our work is that of Swann & Allinson (1998) and Xie et al. (2013), who explore creating ensembles from slices of the learning trajectory. Xie et al. introduce the horizontal and vertical ensembling method, which combines the output of networks within a range of training epochs. More recently, Jean et al. (2014) and Sennrich et al. (2016) show improvement by ensembling the intermediate stages of model training. Laine & Aila (2016) propose a temporal ensembling method for semi-supervised learning, which achieves consensus among models trained with different regularization and augmentation conditions for better generalization performance. Finally, Moghimi et al. (2016) show that boosting can be applied to convolutional neural networks to create strong ensembles. Our work differs from these prior works in that we force the model to visit multiple local minima, and we take snapshots only when the model reaches a minimum. We believe this key insight allows us to leverage more power from our ensembles. Our work is inspired by the recent findings of Loshchilov & Hutter (2016) and Smith (2016), who show that cyclic learning rates can be effective for training convolutional neural networks.", "startOffset": 8, "endOffset": 1388}, {"referenceID": 9, "context": ", 2006; Hinton et al., 2015). Hinton et al. (2015) propose to use an ensemble of multiple networks as the target of a single (smaller) network. Our proposed method is complementary to these works as we aim to reduce the training cost of ensembles rather than the test-time cost. Perhaps most similar to our work is that of Swann & Allinson (1998) and Xie et al. (2013), who explore creating ensembles from slices of the learning trajectory. Xie et al. introduce the horizontal and vertical ensembling method, which combines the output of networks within a range of training epochs. More recently, Jean et al. (2014) and Sennrich et al. (2016) show improvement by ensembling the intermediate stages of model training. Laine & Aila (2016) propose a temporal ensembling method for semi-supervised learning, which achieves consensus among models trained with different regularization and augmentation conditions for better generalization performance. Finally, Moghimi et al. (2016) show that boosting can be applied to convolutional neural networks to create strong ensembles. Our work differs from these prior works in that we force the model to visit multiple local minima, and we take snapshots only when the model reaches a minimum. We believe this key insight allows us to leverage more power from our ensembles. Our work is inspired by the recent findings of Loshchilov & Hutter (2016) and Smith (2016), who show that cyclic learning rates can be effective for training convolutional neural networks.", "startOffset": 8, "endOffset": 1405}, {"referenceID": 10, "context": "\u2217 indicates numbers which we take directly from Huang et al. (2016a).", "startOffset": 48, "endOffset": 69}, {"referenceID": 1, "context": "We run all experiments with Torch 7 (Collobert et al., 2011)2.", "startOffset": 36, "endOffset": 60}, {"referenceID": 26, "context": "We use a standard data augmentation scheme (Lin et al., 2013; Romero et al., 2014; Lee et al., 2015; Springenberg et al., 2014; Srivastava et al., 2015; Huang et al., 2016b; Larsson et al., 2016), in which the images are zero-padded with 4 pixels on each side, randomly cropped to produce 32\u00d732 images, and horizontally mirrored with probability 0.", "startOffset": 43, "endOffset": 195}, {"referenceID": 31, "context": "We use a standard data augmentation scheme (Lin et al., 2013; Romero et al., 2014; Lee et al., 2015; Springenberg et al., 2014; Srivastava et al., 2015; Huang et al., 2016b; Larsson et al., 2016), in which the images are zero-padded with 4 pixels on each side, randomly cropped to produce 32\u00d732 images, and horizontally mirrored with probability 0.", "startOffset": 43, "endOffset": 195}, {"referenceID": 22, "context": "We use a standard data augmentation scheme (Lin et al., 2013; Romero et al., 2014; Lee et al., 2015; Springenberg et al., 2014; Srivastava et al., 2015; Huang et al., 2016b; Larsson et al., 2016), in which the images are zero-padded with 4 pixels on each side, randomly cropped to produce 32\u00d732 images, and horizontally mirrored with probability 0.", "startOffset": 43, "endOffset": 195}, {"referenceID": 25, "context": "The Street View House Numbers (SVHN) dataset (Netzer et al., 2011) contains 32 \u00d7 32 colored digit images from Google Street View, with one class for each digit.", "startOffset": 45, "endOffset": 66}, {"referenceID": 28, "context": "Following common practice (Sermanet et al., 2012; Goodfellow et al., 2013; Huang et al., 2016a), we withhold 6,000 training images for validation, and train on the remaining images without data augmentation.", "startOffset": 26, "endOffset": 95}, {"referenceID": 3, "context": "The Tiny ImageNet dataset3 consists of a subset of ImageNet images (Deng et al., 2009).", "startOffset": 67, "endOffset": 86}, {"referenceID": 18, "context": "Each image is resized to 64\u00d764 and augmented with random crops, horizontal mirroring, and RGB intensity scaling (Krizhevsky et al., 2012).", "startOffset": 112, "endOffset": 137}, {"referenceID": 3, "context": "The ILSVRC 2012 classification dataset (Deng et al., 2009) consists of 1000 images classes, with a total of 1.", "startOffset": 39, "endOffset": 58}, {"referenceID": 10, "context": "DenseNet Baseline (Huang et al. 2016)", "startOffset": 18, "endOffset": 37}, {"referenceID": 10, "context": "DenseNet Baseline (Huang et al. 2016)", "startOffset": 18, "endOffset": 37}, {"referenceID": 10, "context": "DenseNet Baseline (Huang et al. 2016)", "startOffset": 18, "endOffset": 37}, {"referenceID": 10, "context": "DenseNet Baseline (Huang et al. 2016)", "startOffset": 18, "endOffset": 37}, {"referenceID": 7, "context": "We test several state-of-the-art architectures, including residual networks (ResNet) (He et al., 2016a), Wide ResNet (Zagoruyko & Komodakis, 2016) and DenseNet (Huang et al., 2016a). For ResNet, we use the original 110-layer network introduced by He et al. (2016a). Wide-ResNet is a 32-layer ResNet with 4 times as many convolutional features per layer as a standard ResNet.", "startOffset": 86, "endOffset": 265}, {"referenceID": 5, "context": "To do so, we linearly interpolate snapshot models, as described by Goodfellow et al. (2014). Let J (\u03b8) be the test error of a model using parameters \u03b8.", "startOffset": 67, "endOffset": 92}], "year": 2017, "abstractText": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "creator": "LaTeX with hyperref package"}}}