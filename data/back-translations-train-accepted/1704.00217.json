{"id": "1704.00217", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2017", "title": "Adversarial Connective-exploiting Networks for Implicit Discourse Relation Classification", "abstract": "Implicit discourse relation classification is of great challenge due to the lack of connectives as strong linguistic cues, which motivates the use of annotated implicit connectives to improve the recognition. We propose a feature imitation framework in which an implicit relation network is driven to learn from another neural network with access to connectives, and thus encouraged to extract similarly salient features for accurate classification. We develop an adversarial model to enable an adaptive imitation scheme through competition between the implicit network and a rival feature discriminator. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark.", "histories": [["v1", "Sat, 1 Apr 2017 19:29:21 GMT  (1676kb,D)", "http://arxiv.org/abs/1704.00217v1", "To appear in ACL2017"]], "COMMENTS": "To appear in ACL2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG stat.ML", "authors": ["lianhui qin", "zhisong zhang", "hai zhao", "zhiting hu", "eric p xing"], "accepted": true, "id": "1704.00217"}, "pdf": {"name": "1704.00217.pdf", "metadata": {"source": "CRF", "title": "Adversarial Connective-exploiting Networks for Implicit Discourse Relation Classification", "authors": ["Lianhui Qin", "Zhisong Zhang", "Hai Zhao", "Zhiting Hu", "Eric P. Xing"], "emails": ["zzs2011}@sjtu.edu.cn,", "zhaohai@cs.sjtu.edu.cn,", "epxing}@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "There are a number of applications that answer the question (Liakata et al., 2013) that implicitly implicitly implicitly implicitly implicitly implicate, 2014), a simple summary (Gerani et al., 2014), and so forth.Connective (e.g., but so, etc.) are one of the most important linguistic terms for the identification of discourse relationships. If explicit connectives are present in the text, a simple mapping is sufficient to achieve more than 85% accuracy (Xue et al., 2016). In contrast, implicit discourse relationship recognition has long been a problem with the best accuracy previously lower than 50%. In the implicit case, discourse relationships are not lexicalizedv, but derived from relevant judgments."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Implicit Discourse Relation Recognition", "text": "Since the publication of PDTB (Prasad et al., 2008), the first major discourse corpus to distinguish implicit from explicit examples, interest in implicit discourses has grown. A large group of papers focuses on direct classification based on observed sentences, including structured methods with linguistically informed characteristics (Lin et al., 2009; Pitler et al., 2009; Zhou et al., 2010), on end-to-end neural models (Qin et al., 2016b, c; Chen et al., 2016a; Liu and Li, 2016) and combined approaches (Ji and Eisenstein, 2015; Ji et al., 2016). However, the lack of implicit references makes learning from contextual semantics fraught with challenges. Prior work has tried to use connective information. Zhou et al. (2010) include implicit connections, but also implicit deals with implicit first by explicitly predicting implicit with an implicit system."}, {"heading": "2.2 Adversarial Networks", "text": "Adversarial methods have achieved impressive successes in profound generative modeling (Goodfellow et al., 2014) and domain adaptation (Ganin et al., 2016). Generative adversarial networks (Goodfellow et al., 2014) learn to produce realistic images by competing between an image generator and a real / fake discriminator. Professor Forcing (Lamb et al., 2016) applies a similar idea to improve the long-term generation of a recurring neural language model. Other approaches (Chen et al., 2016b; Hu et al., 2017) expand the framework for controllable image / text generation. Li et al. (2015); Salimans et al. (2016) propose a feature matching that makes generators match the statistics of real / fake examples. Their properties are extracted by the discriminator and not by the classifier networks, as in our case. Our work differs from the domain-based modeling we use to differentiate the above-mentioned characteristics from the structural discriminator."}, {"heading": "3 Adversarial Method", "text": "Discourse connectors are key indicators of discourse relationships. In the annotation process of the PDTB Implicit Relations Benchmark, annotators inserted implicit connecting expressions between adjacent sentences to lexicalize abstract relationships and help make final decisions. Our model aims to make full use of the implicit connectives provided in training to regulate the recognition of implicit relations, promote the extraction of highly discriminatory semantics from raw arguments, and improve generalization at test time.Our method provides a novel, contradictory framework that uses connective information in a flexible adaptive manner and is efficiently trained end-to-end by standard feedback propagation. The basic idea of the proposed approach is simple. We want to have our implicit acknowledgement mechanism that predicts the underlying relationship of sentence arguments without a disconnecting effect to convective convective convectives, in addition to convective modulating the contractiveness of a contractile."}, {"heading": "3.1 Model Architecture", "text": "Let (x, y) be a pair of input and output implicit relation classification, where x = (x1, x2) is a pair of sentence arguments and y is the underlying discourse relationship. Each training example also contains a commented implicit connective c that best expresses the relationship. Figure 1 shows the architecture of our framework. The neural model of implicit relation classification (i-CNN in the figure) extracts latent representation from the arguments referred to as HI (x1, x2), and feeds the feature into a classifier C for final prediction (HI (x1, x2). To simplify notation, we also use HI (x) to imply the latent feature on data x.The second relation network (a-CNN) takes the sentence arguments along with an implicit connecting C as inputs to create the implicit connectivity capability to a network x2, and the implicit connectivity to both networks (c) is the implicit connectivity to a network (x1)."}, {"heading": "3.2 Training Procedure", "text": "In this section, we first present the training target for each component and then give the general training algorithms. Let \u03b8D specify the parameters of the implicit discriminator. D's training target is simple, i.e. we maximize the likelihood of correctly distinguishing the input characteristics: maximum implication characteristics LD = E (x, c, y) - implication data [logD (x, c) - implication data. We denounce the parameters of the implicit network i-CNN (x, c) - implication arguments (1 \u2212 D (HI (x); implication characteristics of X)) - implication factors of the implicit network i-CNN and classifier C as implication characteristics. The model is then trained to correctly classify (a) relationships in training data and (b) generate."}, {"heading": "3.3 Component Structures", "text": "Different roles of the modules within the frame lead to different modeling selections.Relation Classification Networks Figure 2 illustrates the structure of the implicit relationship network i-CNN. We use a revolutionary network, as it is a common architectural choice for discourse analysis.The network inputs the word vectors of the tokens in each sentence argument and maps each argument to intermediary characteristics through a common revolutionary layer. The resulting representations are then linked together and fed into a maximum pooling layer to select the most distinctive characteristics as the final representation.The final classifier C is a simple, fully connected layer, followed by a softmax-augmented network a-CNN."}, {"heading": "4 Experiments", "text": "We demonstrate the effectiveness of our approach both quantitatively and qualitatively by conducting extensive experiments. We evaluate the predictive performance of the PDTB benchmark in different constellations. Our method improves significantly compared to a variety of previous models, especially in the practical multi-class classification task. We conduct an in-depth analysis of the model behavior and show that our contradictory framework successfully enables the implicit relationship model to mimic and learn discriminatory characteristics."}, {"heading": "4.1 Experiment Setup", "text": "We use PDTB 2.01, one of the largest manually annotated discourse corpus. The data set contains a total of 16,224 implicit relation cases with three senses levels: level 1 class, level 2 type, and level 3 subtypes. Level 1 consists of four important relation classes: COMPARI-1http: / / www.seas.upenn.edu / \u0445 pdtb / SON, CONTINGENCY, EXPANSION, and TEMPORAL. Level 2 contains 16 types. To make a comprehensive comparison with previous work on implicit discourse relation classification, we evaluate two popular experimental settings: 1) multi-class classification for second-level types (Lin et al., 2009; Ji and Eisenstein, 2015), and 2) a comparison with other binary 1st-level classifications (Pitler et al., 2009). We describe the detailed configurations in the following sections."}, {"heading": "4.2 Implicit Relation Classification", "text": "We will focus mainly on the general multi-class classification problem in two alternative constellations adopted in previous work, which demonstrate the superiority of our model over the previous state of the art. We will perform a detailed comparison with carefully designed foundations, providing empirical insights into the working mechanism of the proposed framework. For more comprehensive comparisons, we will also report on performance in the one-against-all constellation. Multi-class classifications will first take part in the standard PDTB convention that follows (Lin et al., 2009), which is referred to as PDTB-Lin, using sections 2-21, 22 and 23 as training. The most common types of relationships are selected in the task set. Instances with more than one annotated relationship type are considered as multiple instances, each of which has one of the notes."}, {"heading": "4.3 Qualitative Analysis", "text": "We will now take a closer look at the modeling behavior of our framework by examining the process of opposing play during training, as well as the imitation effects of features. Figure 4 shows the training progress of various components. The a-CNN network maintains a high predictive accuracy as implicit connectivity is given, showing the importance of connectivity. The ascent and fall patterns in the discriminator's accuracy clearly show its competition with the i-CNN implicit relationship network during training. At the first iterations, the discriminator's accuracy quickly increases to above 0.9, while late-stage accuracy drops to about 0.6, indicating that the discriminator is confused by i-CNN (an accuracy of 0.5 indicates complete confusion). The i-CNN network improves in terms of implicit attribution accuracy as it gradually adapts to the data and at the same time learns increasingly discriminatory characteristics from i-CNN, by mimicking the CNN's inherent stability."}, {"heading": "5 Discussions", "text": "We have developed a hostile neural framework that enables an implicit relationship network to extract highly discriminatory traits by mimicking a connective-extended network. Our method achieved state-of-the-art performance in classifying implicit discourse relationships. In addition to implicit connective examples, our model can, of course, provide enormous explicit connective data to further improve discourse sparsing.The proposed hostile feature imitation scheme is generally applicable to other contexts to include indicative ancillary information available during training for improved inference. Our framework shares a similar spirit of iterative knowledge distillation (Hu et al., 2016a, b), which trains a \"student network\" to mimic the classification behavior of a scientifically informed \"teacher network. Our approach encourages imitation at the trait level rather than the definitive pre-context, allowing us to apply it to our more interesting regressive approach."}, {"heading": "Appendix A Model architectures and training configurations", "text": "In this section, we present the detailed architectural configurations of each component that we used in the experiments. \u2022 Table 3 lists the filter configurations of the convolutional layer in i-CNN and a-CNN in various developer sets. \u2022 As described in Section 3.3 in the paper, the convolutional layer is followed by a maximum pooling layer in i-CNN and an average k-max pooling layer with k = 2 in a-CNN. \u2022 The final single-layer classifier C contains 512 neurons and uses \"tanh\" as an activation function. \u2022 The discriminator D consists of 4 fully connected layers with 2 gated paths from layer 1 to layer 3 and layer 4 (see Figure 3 in the essay). The size of each layer is set to 1024 and is fixed in all experiments. \u2022 We set the dimension of the entered word vectors to 300 and initialize it with pre-trained word2vec (Mikolov et al, 2013)."}, {"heading": "Appendix B One-vs-all Classifications", "text": "Table 4 lists the statistics of data.2https: / / www.tensorflow.org"}], "references": [{"title": "Aggregated word pair features for implicit discourse relation disambiguation", "author": ["Or Biran", "Kathleen McKeown."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Sofia, Bulgaria,", "citeRegEx": "Biran and McKeown.,? 2013", "shortCiteRegEx": "Biran and McKeown.", "year": 2013}, {"title": "Comparing word representations for implicit discourse relation classification", "author": ["Chlo\u00e9 Braud", "Pascal Denis."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal, pages 2201\u20132211.", "citeRegEx": "Braud and Denis.,? 2015", "shortCiteRegEx": "Braud and Denis.", "year": 2015}, {"title": "Learning connective-based word representations for implicit discourse relation identification", "author": ["Chlo\u00e9 Braud", "Pascal Denis."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas, pages 203\u2013", "citeRegEx": "Braud and Denis.,? 2016", "shortCiteRegEx": "Braud and Denis.", "year": 2016}, {"title": "Implicit discourse relation detection via a deep architecture with gated relevance network", "author": ["Jifan Chen", "Qi Zhang", "Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Chen et al\\.,? 2016a", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel."], "venue": "Advances in Neural Information Processing Sys-", "citeRegEx": "Chen et al\\.,? 2016b", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Adversarial deep averaging networks for cross-lingual sentiment classification", "author": ["Xilun Chen", "Ben Athiwaratkun", "Yu Sun", "Kilian Weinberger", "Claire Cardie."], "venue": "arXiv preprint arXiv:1606.01614 .", "citeRegEx": "Chen et al\\.,? 2016c", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research 12(Jul):2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Domain-adversarial training of neural networks", "author": ["Yaroslav Ganin", "Evgeniya Ustinova", "Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "Fran\u00e7ois Laviolette", "Mario Marchand", "Victor Lempitsky."], "venue": "Journal of Machine Learning Research", "citeRegEx": "Ganin et al\\.,? 2016", "shortCiteRegEx": "Ganin et al\\.", "year": 2016}, {"title": "Abstractive summarization of product reviews using discourse", "author": ["Shima Gerani", "Yashar Mehdad", "Giuseppe Carenini", "T. Raymond Ng", "Bita Nejat"], "venue": null, "citeRegEx": "Gerani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gerani et al\\.", "year": 2014}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."], "venue": "Advances in Neural Information Processing Systems. pages 2672\u20132680.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Harnessing deep neural networks with logic rules", "author": ["Zhiting Hu", "Xuezhe Ma", "Zhengzhong Liu", "Eduard Hovy", "Eric P Xing."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL). Berlin, Germany,", "citeRegEx": "Hu et al\\.,? 2016a", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "Controllable text generation", "author": ["Zhiting Hu", "Zichao Yang", "Xiaodan Liang", "Ruslan Salakhutdinov", "Eric P Xing."], "venue": "arXiv preprint arXiv:1703.00955 .", "citeRegEx": "Hu et al\\.,? 2017", "shortCiteRegEx": "Hu et al\\.", "year": 2017}, {"title": "Deep neural networks with massive learned knowledge", "author": ["Zhiting Hu", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P Xing."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP). Austin, USA.", "citeRegEx": "Hu et al\\.,? 2016b", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "One vector is not enough: Entity-augmented distributed semantics for discourse relations", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "Transactions of the Association for Computational Linguistics (TACL) 3:329\u2013 344.", "citeRegEx": "Ji and Eisenstein.,? 2015", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2015}, {"title": "A latent variable recurrent neural network for discourse-driven language models", "author": ["Yangfeng Ji", "Gholamreza Haffari", "Jacob Eisenstein."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Ji et al\\.,? 2016", "shortCiteRegEx": "Ji et al\\.", "year": 2016}, {"title": "Closing the gap: Domain adaptation from explicit to implicit discourse relations", "author": ["Yangfeng Ji", "Gongbo Zhang", "Jacob Eisenstein."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal,", "citeRegEx": "Ji et al\\.,? 2015", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "Professor forcing: A new algorithm for training recurrent networks", "author": ["Alex M Lamb", "Anirudh Goyal", "Ying Zhang", "Saizheng Zhang", "Aaron C Courville", "Yoshua Bengio."], "venue": "Advances In Neural Information Processing Systems. pages 4601\u20134609.", "citeRegEx": "Lamb et al\\.,? 2016", "shortCiteRegEx": "Lamb et al\\.", "year": 2016}, {"title": "Leveraging synthetic discourse data via multi-task learning for implicit discourse relation recognition", "author": ["Man Lan", "Yu Xu", "Zhengyu Niu."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-", "citeRegEx": "Lan et al\\.,? 2013", "shortCiteRegEx": "Lan et al\\.", "year": 2013}, {"title": "Assessing the discourse factors that influence the quality of machine translation", "author": ["Junyi Jessy Li", "Marine Carpuat", "Ani Nenkova."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Pa-", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Implicit discourse relation classifica", "author": ["Sui"], "venue": null, "citeRegEx": "2016.,? \\Q2016\\E", "shortCiteRegEx": "2016.", "year": 2016}, {"title": "The penn discourse treebank", "author": ["nie L Webber"], "venue": null, "citeRegEx": "Webber.,? \\Q2008\\E", "shortCiteRegEx": "Webber.", "year": 2008}, {"title": "Shallow discourse parsing using convolutional neural network", "author": ["Lianhui Qin", "Zhisong Zhang", "Hai Zhao."], "venue": "Proceedings of the CoNLL-16 shared task. Berlin, Germany, pages 70\u201377.", "citeRegEx": "Qin et al\\.,? 2016b", "shortCiteRegEx": "Qin et al\\.", "year": 2016}, {"title": "A stacking gated neural architecture for implicit discourse relation classification", "author": ["Lianhui Qin", "Zhisong Zhang", "Hai Zhao."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas, pages 2263\u2013", "citeRegEx": "Qin et al\\.,? 2016c", "shortCiteRegEx": "Qin et al\\.", "year": 2016}, {"title": "Improving the inference of implicit discourse relations via classifying explicit discourse connectives", "author": ["Attapol Rutherford", "Nianwen Xue."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational", "citeRegEx": "Rutherford and Xue.,? 2015", "shortCiteRegEx": "Rutherford and Xue.", "year": 2015}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen."], "venue": "Advances in Neural Information Processing Systems. pages 2226\u20132234.", "citeRegEx": "Salimans et al\\.,? 2016", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Highway networks", "author": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "arXiv preprint arXiv:1505.00387 .", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Connective prediction using machine learning for implicit discourse relation", "author": ["Yu Xu", "Man Lan", "Yue Lu", "Zheng Yu Niu", "Chew Lim Tan"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2012}, {"title": "The CoNLL-2015 shared task on shallow discourse parsing", "author": ["Nianwen Xue", "Hwee Tou Ng", "Sameer Pradhan", "Rashmi Prasad", "Christopher Bryant", "Attapol Rutherford."], "venue": "Proceedings of the Nineteenth Conference on Computational Natural Lan-", "citeRegEx": "Xue et al\\.,? 2015", "shortCiteRegEx": "Xue et al\\.", "year": 2015}, {"title": "The CoNLL-2016 shared task on shallow discourse parsing", "author": ["Nianwen Xue", "Hwee Tou Ng", "Sameer Pradhan", "Bonnie Webber", "Attapol Rutherford", "Chuan Wang", "Hongmin Wang."], "venue": "Proceedings of the Twentieth Conference on Computational Natural", "citeRegEx": "Xue et al\\.,? 2016", "shortCiteRegEx": "Xue et al\\.", "year": 2016}, {"title": "Variational neural discourse relation recognizer", "author": ["Biao Zhang", "Deyi Xiong", "jinsong su", "Qun Liu", "Rongrong Ji", "Hong Duan", "Min Zhang"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Predicting discourse connectives for implicit discourse relation recognition", "author": ["Zhi-Min Zhou", "Yu Xu", "Zheng-Yu Niu", "Man Lan", "Jian Su", "Chew Lim Tan."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (Coling", "citeRegEx": "Zhou et al\\.,? 2010", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 18, "context": ", 2013), machine translation (Li et al., 2014), text summarization (Gerani et al.", "startOffset": 29, "endOffset": 46}, {"referenceID": 8, "context": ", 2014), text summarization (Gerani et al., 2014), and so forth.", "startOffset": 28, "endOffset": 49}, {"referenceID": 28, "context": "When explicit connectives are present in the text, a simple frequency-based mapping is sufficient to achieve over 85% classification accuracy (Xue et al., 2016).", "startOffset": 142, "endOffset": 160}, {"referenceID": 3, "context": ", 2009) to the very recent end-to-end neural models (Chen et al., 2016a; Qin et al., 2016c).", "startOffset": 52, "endOffset": 91}, {"referenceID": 22, "context": ", 2009) to the very recent end-to-end neural models (Chen et al., 2016a; Qin et al., 2016c).", "startOffset": 52, "endOffset": 91}, {"referenceID": 30, "context": "Zhou et al. (2010) developed a two-step approach by first predicting implicit connectives whose sense is then disambiguated to obtain the relation.", "startOffset": 0, "endOffset": 19}, {"referenceID": 23, "context": "Other research leveraged explicit connective examples for data augmentation (Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015; Braud and Denis, 2016).", "startOffset": 76, "endOffset": 165}, {"referenceID": 1, "context": "Other research leveraged explicit connective examples for data augmentation (Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015; Braud and Denis, 2016).", "startOffset": 76, "endOffset": 165}, {"referenceID": 15, "context": "Other research leveraged explicit connective examples for data augmentation (Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015; Braud and Denis, 2016).", "startOffset": 76, "endOffset": 165}, {"referenceID": 2, "context": "Other research leveraged explicit connective examples for data augmentation (Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015; Braud and Denis, 2016).", "startOffset": 76, "endOffset": 165}, {"referenceID": 9, "context": "The adversarial mechanism has been an emerging method in different context, especially for image generation (Goodfellow et al., 2014) and domain adaptation (Ganin et al.", "startOffset": 108, "endOffset": 133}, {"referenceID": 7, "context": ", 2014) and domain adaptation (Ganin et al., 2016; Chen et al., 2016c).", "startOffset": 30, "endOffset": 70}, {"referenceID": 5, "context": ", 2014) and domain adaptation (Ganin et al., 2016; Chen et al., 2016c).", "startOffset": 30, "endOffset": 70}, {"referenceID": 30, "context": "Compared to previous connective exploiting work (Zhou et al., 2010; Xu et al., 2012), our method provides a new integration paradigm and an end-to-end procedure that avoids inefficient feature engineering and error propagation.", "startOffset": 48, "endOffset": 84}, {"referenceID": 26, "context": "Compared to previous connective exploiting work (Zhou et al., 2010; Xu et al., 2012), our method provides a new integration paradigm and an end-to-end procedure that avoids inefficient feature engineering and error propagation.", "startOffset": 48, "endOffset": 84}, {"referenceID": 30, "context": "large set of work has focused on direct classification based on observed sentences, including structured methods with linguistically-informed features (Lin et al., 2009; Pitler et al., 2009; Zhou et al., 2010), end-to-end neural models (Qin et al.", "startOffset": 151, "endOffset": 209}, {"referenceID": 13, "context": ", 2016a; Liu and Li, 2016), and combined approaches (Ji and Eisenstein, 2015; Ji et al., 2016).", "startOffset": 52, "endOffset": 94}, {"referenceID": 14, "context": ", 2016a; Liu and Li, 2016), and combined approaches (Ji and Eisenstein, 2015; Ji et al., 2016).", "startOffset": 52, "endOffset": 94}, {"referenceID": 0, "context": "Another notable line aims at adapting explicit examples for data synthesis (Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015), multi-task learning (Lan et al.", "startOffset": 75, "endOffset": 166}, {"referenceID": 23, "context": "Another notable line aims at adapting explicit examples for data synthesis (Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015), multi-task learning (Lan et al.", "startOffset": 75, "endOffset": 166}, {"referenceID": 1, "context": "Another notable line aims at adapting explicit examples for data synthesis (Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015), multi-task learning (Lan et al.", "startOffset": 75, "endOffset": 166}, {"referenceID": 15, "context": "Another notable line aims at adapting explicit examples for data synthesis (Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015), multi-task learning (Lan et al.", "startOffset": 75, "endOffset": 166}, {"referenceID": 17, "context": ", 2015), multi-task learning (Lan et al., 2013; Liu et al., 2016), and word representation (Braud and Denis, 2016).", "startOffset": 29, "endOffset": 65}, {"referenceID": 2, "context": ", 2016), and word representation (Braud and Denis, 2016).", "startOffset": 33, "endOffset": 56}, {"referenceID": 22, "context": "Zhou et al. (2010) also incorporate implicit connectives, but in a pipeline manner by first predicting the implicit connective with a language model and determining discourse relation accordingly.", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": "2014) and domain adaptation (Ganin et al., 2016).", "startOffset": 28, "endOffset": 48}, {"referenceID": 9, "context": "Generative adversarial nets (Goodfellow et al., 2014) learn to produce realistic images through competition between an image generator and a real/fake discriminator.", "startOffset": 28, "endOffset": 53}, {"referenceID": 4, "context": "Other approaches (Chen et al., 2016b; Hu et al., 2017) extend the framework for controllable image/text generation.", "startOffset": 17, "endOffset": 54}, {"referenceID": 11, "context": "Other approaches (Chen et al., 2016b; Hu et al., 2017) extend the framework for controllable image/text generation.", "startOffset": 17, "endOffset": 54}, {"referenceID": 3, "context": "Other approaches (Chen et al., 2016b; Hu et al., 2017) extend the framework for controllable image/text generation. Li et al. (2015); Sali-", "startOffset": 18, "endOffset": 133}, {"referenceID": 25, "context": "formation flow (Srivastava et al., 2015; Qin et al., 2016c), as shown in Figure 3.", "startOffset": 15, "endOffset": 59}, {"referenceID": 22, "context": "formation flow (Srivastava et al., 2015; Qin et al., 2016c), as shown in Figure 3.", "startOffset": 15, "endOffset": 59}, {"referenceID": 13, "context": "To make extensive comparison with prior work of implicit discourse relation classification, we evaluate on two popular experimental settings: 1) multi-class classification for 2nd-level types (Lin et al., 2009; Ji and Eisenstein, 2015), and 2) oneversus-others binary classifications for 1st-level classes (Pitler et al.", "startOffset": 192, "endOffset": 235}, {"referenceID": 6, "context": "The neural parameters are trained using AdaGrad (Duchi et al., 2011) with an initial learning rate of 0.", "startOffset": 48, "endOffset": 68}, {"referenceID": 13, "context": "66 8 Ji and Eisenstein (2015) 44.", "startOffset": 5, "endOffset": 30}, {"referenceID": 13, "context": "66 8 Ji and Eisenstein (2015) 44.59 9 Qin et al. (2016a) 43.", "startOffset": 5, "endOffset": 57}, {"referenceID": 13, "context": "An alternative, slightly different multi-class setting is used in (Ji and Eisenstein, 2015), denoted", "startOffset": 66, "endOffset": 91}, {"referenceID": 22, "context": ", 2009), pure neural methods (Qin et al., 2016c), and combined approach (Ji and Eisenstein, 2015).", "startOffset": 29, "endOffset": 48}, {"referenceID": 13, "context": ", 2016c), and combined approach (Ji and Eisenstein, 2015).", "startOffset": 32, "endOffset": 57}, {"referenceID": 13, "context": ", 2009), where first 8 epochs are for initialization stage (thus the discriminator is fixed and not shown); Bottom: the PDTB-Ji setting (Ji and Eisenstein, 2015), where first 3 epochs are for initialization.", "startOffset": 136, "endOffset": 161}, {"referenceID": 17, "context": "76 Qin et al. (2016c) 41.", "startOffset": 3, "endOffset": 22}, {"referenceID": 16, "context": "(2016c) 41.55 57.32 71.50 35.43 Zhang et al. (2016) 35.", "startOffset": 1, "endOffset": 52}, {"referenceID": 16, "context": "(2016c) 41.55 57.32 71.50 35.43 Zhang et al. (2016) 35.88 50.56 71.48 29.54 Zhou et al. (2010) 31.", "startOffset": 1, "endOffset": 95}, {"referenceID": 16, "context": "(2016c) 41.55 57.32 71.50 35.43 Zhang et al. (2016) 35.88 50.56 71.48 29.54 Zhou et al. (2010) 31.79 47.16 70.11 20.30 Liu and Li (2016) 36.", "startOffset": 1, "endOffset": 137}, {"referenceID": 3, "context": "84 Chen et al. (2016a) 40.", "startOffset": 3, "endOffset": 23}, {"referenceID": 30, "context": "Notably, our feature imitation scheme greatly improves over (Zhou et al., 2010) which leverages implicit connectives as an intermediate prediction task.", "startOffset": 60, "endOffset": 79}], "year": 2017, "abstractText": "Implicit discourse relation classification is of great challenge due to the lack of connectives as strong linguistic cues, which motivates the use of annotated implicit connectives to improve the recognition. We propose a feature imitation framework in which an implicit relation network is driven to learn from another neural network with access to connectives, and thus encouraged to extract similarly salient features for accurate classification. We develop an adversarial model to enable an adaptive imitation scheme through competition between the implicit network and a rival feature discriminator. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark.", "creator": "LaTeX with hyperref package"}}}