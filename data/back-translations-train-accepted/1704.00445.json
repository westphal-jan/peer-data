{"id": "1704.00445", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "On Kernelized Multi-armed Bandits", "abstract": "We consider the stochastic bandit problem with a continuous set of arms, with the expected reward function over the arms assumed to be fixed but unknown. We provide two new Gaussian process-based algorithms for continuous bandit optimization-Improved GP-UCB (IGP-UCB) and GP-Thomson sampling (GP-TS), and derive corresponding regret bounds. Specifically, the bounds hold when the expected reward function belongs to the reproducing kernel Hilbert space (RKHS) that naturally corresponds to a Gaussian process kernel used as input by the algorithms. Along the way, we derive a new self-normalized concentration inequality for vector- valued martingales of arbitrary, possibly infinite, dimension. Finally, experimental evaluation and comparisons to existing algorithms on synthetic and real-world environments are carried out that highlight the favorable gains of the proposed strategies in many cases.", "histories": [["v1", "Mon, 3 Apr 2017 06:47:42 GMT  (127kb,D)", "http://arxiv.org/abs/1704.00445v1", null], ["v2", "Wed, 17 May 2017 09:04:40 GMT  (127kb,D)", "http://arxiv.org/abs/1704.00445v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sayak ray chowdhury", "aditya gopalan"], "accepted": true, "id": "1704.00445"}, "pdf": {"name": "1704.00445.pdf", "metadata": {"source": "CRF", "title": "On Kernelized Multi-armed Bandits", "authors": ["Sayak Ray Chowdhury", "Aditya Gopalan"], "emails": ["SRCHOWDHURY@ECE.IISC.ERNET.IN", "ADITYA@ECE.IISC.ERNET.IN"], "sections": [{"heading": null, "text": "We provide two new Gaussian process-based algorithms for continuous bandit optimization - Improved GP-UCB (IGP-UCB) and GP-Thomson sampling (GP-TS), and deduce corresponding remorse limits. Especially if the expected reward function belongs to the reproducing Hilbert kernel (RKHS), which naturally corresponds to a Gaussian process core used by the algorithms as input. Besides, we deduce a new self-normalized concentration imbalance for vector-rated martyrs of arbitrary, possibly infinite dimensions. Finally, experimental evaluations and comparisons with existing algorithms are performed in synthetic and real environments, highlighting the beneficial benefits of the proposed strategies in many cases."}, {"heading": "1. Introduction", "text": "In fact, most of us are in a position to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live, to live in, to live, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live, to live in, to live in, to live in, to live in, to live, to live in, to live in, to live, to live in, to live, to live in, to live in, to live in, to live, to live in, to live in, to live, to live in, to live in, to live, to live in, to live in, to live in, to live in, to live, to live in, to live in, to live, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live, in, to live in, to live, to live in, in, in, to live in, to live, to live, to live"}, {"heading": "2. Problem Statement", "text": "We look at the problem of sequentially maximizing a fixed but unknown reward function f: D > R over a (potentially infinite) series of decisions D + Rd, also referred to as actions or weapons. Causally, the action is chosen depending on the rewards played and received in each turn, those of the story Ht \u2212 1 = {s, ys): s = 1,.. t \u2212 1} The action is causally determined according to the rewards played and received in the round, those of the story Ht \u2212 1 = {s, ys): s = 1,., t \u2212 1}. We1, for rewards and rewards in [\u2212 1], these algorithms achieve optimal repentance O (d), where O () hides polylog (T) factors."}, {"heading": "3. Algorithms", "text": "The two algorithms we propose use Gaussian probability models for observations, and Gaussian process (GP) priories for uncertainty about reward functions. A Gaussian process via D, denoted by GPD (\u00b5 (\u00b7), k (\u00b7 \u00b7), is a collection of random variables (f (x)) and covariance E (f (xi) \u2212 D, so any finite sub-capture of random variables (f (xi)) mi = 1 common to Gaussian with mean E [f (xi)]] and covariance E [f (xi) \u2212 \u00b5))) (f (f (xj) \u2212 \u00b5 (xi) \u2212 x distribution of random variables (xi)] = k (1 \u2264 i, j \u2264 m).N. The algorithms used also assume this."}, {"heading": "3.1 Improved GP-UCB (IGP-UCB) algorithm", "text": "We present the IGP-UCB algorithm (algorithm 1), which uses a combination of current posterior mean \u00b5t \u2212 1 (x) and standard deviation v\u03c3t \u2212 1 (x) to (a) construct an upper confidence limit (UCB) for the actual function f over D, and (b) select an action to maximize it. Specifically, in each round t, it selects the action text = argmax x. (UCB) + \u03b2t\u03c3t \u2212 1 (x), (5) with the scale parameter v set to be 1. Such rule wheels from exploration (pick points with high uncertainty \u03c3t \u2212 1 (x))) with exploitation (pick points with high reward \u00b5t \u2212 1 (x))), with \u03b2t = B + R."}, {"heading": "3.2 Gaussian Process Thompson sampling (GP-TS)", "text": "Our second algorithm, GP-TS (algorithm 2), inspired by the success of Thompson's sample of standard and parametric bandits (Agrawal and Goyal, 2012; Kaufmann et al., 2012; Gopalan et al., 2014; Agrawal and Goyal, 2013), uses the time-varying scale parameter vt = B + R \u221a 2 (\u03b3t \u2212 1 + 1 + ln (2 / \u03b4) and works as follows. In each round GP-TS tries a random function ft (\u00b7) from the midrange GP \u00b5t \u2212 1 (\u00b7) and covariance function v2t kt \u2212 1 (\u00b7). Next, it selects a decision set Dt-D and plays the arm xt-Dt that maximizes ft7. We call it GP-Thompson sampling because it falls within the general framework of Thompson sampling, i.e. (a) a previous parameter of the reward distribution, i.e., the sampling is GP-GP sampling."}, {"heading": "4. Main Results", "text": "We begin by presenting two central unweights of concentration, which are essential for limiting the duration of the proposed algorithms. (Theorem 1) We begin by presenting two central unweights of concentration, which are essential for the duration of the proposed algorithms (Theorem 1 + 1). (Theorem 1 + 1) We begin by presenting two central unweights of concentration, which are essential for the duration of the proposed algorithms (Theorem 1 + 1). (Theorem 1 + 1) We begin by presenting two central unweights of concentration (Theorem 1 + 1). (Theorem 1 + 1) We begin by showing the desired size of the desired size of the desired size of the desired size of the desired size of the desired size of the desired size of the desired size of the desired size of the desired size of the desired size of the desired size of the desired size of the desired size of the desired size of the desired size of the desired size of the desired size of the desired size of the desired size of the desired size of the desired size of the desired size of the desired size of the desired size."}, {"heading": "4.1 Regret Bound of IGP-UCB", "text": "Theorem 3 If we regret for a function that lies in RKHS Hk (D), we receive a regret that is bound to O (ENG (ENG) with high probability. More precisely, with probability at least 1 \u2212 \u03b4, RT = O (ENG + ENG (D))). Improvement via GP-UCB. Srinivas et al. (2009) show in the course of the analysis of the GP-UCB algorithm that if the reward function lies in RKHS Hk (D), GP-UCB receives regret O (ENG + ENG). Improvement via GP-UCB. Srinivas et al. (T), in the course of the analysis of the GP-UCB algorithm, show that if the reward function lies in RKHS Hk (D), GP-UCB receives regret O (ENG + ENG) and EG (ENG) with high probability (ENG)."}, {"heading": "4.2 Regret Bound of GP-TS", "text": "For technical reasons, we will analyze the following version of GP-TS (1 / 2): The decision applied by GP-TS (1 / 2) is a one-time discreditation Dt of D with the property that it (x) -F (x) -D (x) -D (2) -D (2) -D (2) -D (2) -D (2) -D (2) -D (2) -D (2) -D (2) -D (2) -D (2) -D) -D (2) -D (2) -D (2) -D (2) -D) -D (2) -D (2) -D) -D (3) -D (2) -D (2) -D (3) -D (3) (D) -D) (3) -D (1 / 2)."}, {"heading": "5. Overview of techniques", "text": "It is convenient to assume that Kt, the induced kernel matrix in time t, is invertable, since this is the crux of the argument, where the crux of the argument lies. First, we show that for each function g: D \u2192 R and for all t: 0, thanks to the sub-Gaussian property (1), the process {Mgt: = exp (\u03b5 T 1: 12) lies the crux of the argument, that there is a non-negative supermartingale in relation to the filtration Ft, where g1: t: = [g (x1), g (xt)] T and indeed satisfies E [Mgt]."}, {"heading": "6. Experiments", "text": "This year we have it in hand, as we have experienced it in recent years: in the USA, in Europe, in Europe, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in"}, {"heading": "7. Conclusion", "text": "For bandit optimization, we have improved the existing GP-UCB algorithm and introduced a new GPTS algorithm. In practice, the proposed algorithms work well on both synthetic and real data. An interesting case is when the core function is also not a priori known to the algorithms and needs to be learned adaptively. In addition, one can consider time classes that differ from the RKHS, and general reinforcement learning with GP techniques. There are also important questions about computer aspects in optimizing functions derived from GPs."}, {"heading": "A. Proof of Theorem 1", "text": "For a function g \u2192 R and a sequence of realities n \u00b2 t = 1, defined for each t \u2265 0Mg, nt = exp (\u03b5T1: tg1: t, n \u2212 R22, g1: t, n = 2), where the vector g1: t, n: [g (x1) + n1,., g (xt) + nt] T: We first determine the following technical result, which resembles Abbasi-Yadkori and al. (2011, Lemma 8).Lemma 2 For fixed g and n, {Mg, nt}. t = 0 is a supermartinale in relation to filtration {Ft}."}, {"heading": "C. Proof of Theorem 2", "text": "First definition (x) as k (x), second definition (x), where the definition of the internal product < g, h > k as gTh and the RKHS standard (g) as gT g. Well, as unknown reward function f rests in the RKHS Hk (D), these definitions along with the reproducibility of the property of RKHS imply f (x) = < f, k (x, \u00b7 k), k (x), k (x), k (x), k (k), k (x), k (x), k), k (x), k (x), k (x), k (x), k (x), k (x), k (x), k (x), k (x)."}, {"heading": "D. Analysis of IGP-UCB (Theorem 3)", "text": "Note that for each round, we have t \u2265 1 by selecting xt in the algorithm 1 \u00b5t \u2212 1 (xt) + \u03b2t\u03c3t \u2212 1 (xt) \u2265 \u00b5t \u2212 1 (x?) + \u03b2t\u03c3t \u2212 1 (x?) and for the term 2 f (x?) \u2264 \u00b5t \u2212 1 (x?) + \u03b2t\u03c3t \u2212 1 (x?) and \u00b5t \u2212 1 (xt) \u2212 f (xt) \u2264 \u03b2t\u03c3t \u2212 1 (xt). Therefore, for all t \u2265 1 with a probability of at least 1 \u2212 \u03b4, rt = f (x?) \u2212 f (xt) \u2264 \u03b2t\u03c3t \u2212 1 (xt) \u2212 f (xt) \u2264 2\u03b2t\u03c3t \u2212 1 (xt), and therefore T \u0445 t = 1 rt \u2264 2\u03b2T T \u00b2 (T \u00b2 T \u00b2) (T \u00b2 T \u00b2 T) and T \u00b2 T (T \u00b2 T) with a high T \u00b2 and T \u00b2 (T \u00b2)."}, {"heading": "E. Analysis of GP-TS (Theorem 4)", "text": "& Lemma 5, and now the application of Unions (x). \u2212 f & # 8222; f & # 8220; f & # 8220; f & # 8222; f & # 8222; f & # 8222; f & # 8222; f & # 8222; f & # 8222; f & # 8222; f & # 8222; f & # 8222; f & # 8222; f & # 252; f & # 252; f & # 252; f & # 252; f & # 252 f # 252 f; f & 252 f # 252 f # 252 f; f & 252 f; f & 252 f # 252 f; f # 252 f # 252 f; f # 252 f # 252 f; f # 252 f # 252 f; f # 252 f # 252 f; f # 252 f # 252 f; f # 252 f # 252 f; f & 252 f # 252 f # 252 f # 252 f; f & 252 f # 252 f # 252 f # 252 f # 252 f # 252 f # 252 f # 252 f; f # 252 f # 252 f # 252 f # 252 f # 252 f # 252 f # 252 f # 252; f & # 252 f # 252 f # 160 & 160 & 160; f & 160 & 160; f & 160; f & 160; f & 160 & 160; f & 160; f & 160; f & 252; f & 252 & 252; f & 252 f # 252; f # 252 f # 252 f # 252 f # 252 f # 252 f # 252 f; f # 252 f # 252 f # 252 f # 252 f; 252 f # 252 f # 252 f # 252 f; f # 252 f # 252 f # 252 f; f # 252 f # 252 f # 252 f; f # 252 f # 252 f # 252 f # 252 f; f # 252 f # 252 f # 252 f # 252 f # 252 f; f # 252 f # 252 f # 252 f # 252 f # 252 f; f # 252 f # 252 f # 252 f # 252 f # 252 f # 252 f # 252 f # 252 f # 252 f # 252 f # 252 f # 252 f # 252 f # 252 f # 252 f # 252 f # 252 f # 252 f # 252 f # 252 f #"}, {"heading": "F. Recursive Updates of Posterior Mean and Covariance", "text": "We now describe a procedure for updating the posterior mean and covariance function in a recursive manner = \u03b2 = \u03b2\u03b2\u03b2A = (\u03b2 =) by the properties of the shear complement (Zhang (2006) instead of evaluating equation 2 and 3 in each round. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 Specifically for all t \u2265 1 we show the following: \u00b5t (x) = \u03b2t \u2212 1 (x, x) \u2212 1 (x, xt) (x, xt) \u2212 T (x, x) \u2212 1 (T), (19) kt (x, x) kt \u2212 1 (x, x \u2032) kt \u2212 1 (x, x) \u2212 kt \u2212 1 (x) kt \u2212 1 (x, xt) kt \u2212 1 (xt) kt \u2212 1 (xt, x \u2032) \u03bb (xt) \u2212 t (x) \u2212 t (x) \u2212 2 t (x) \u2212 1 (x) t (x) \u2212 t (x) (x) \u2212 t (x) (x), x \u00b2 (x), x \u00b2 (xt), x \u00b2 (x) \u2212 t (x)."}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["Yasin Abbasi-Yadkori", "D\u00e1vid P\u00e1l", "Csaba Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2011}, {"title": "Analysis of thompson sampling for the multi-armed bandit problem", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In COLT,", "citeRegEx": "Agrawal and Goyal.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2012}, {"title": "Thompson sampling for contextual bandits with linear payoffs", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In ICML,", "citeRegEx": "Agrawal and Goyal.,? \\Q2013\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2013}, {"title": "Online stochastic optimization under correlated bandit feedback", "author": ["Mohammad Gheshlaghi Azar", "Alessandro Lazaric", "Emma Brunskill"], "venue": "In ICML,", "citeRegEx": "Azar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2014}, {"title": "Hybrid batch bayesian optimization", "author": ["Javad Azimi", "Ali Jalali", "Xiaoli Fern"], "venue": "arXiv preprint arXiv:1202.5597,", "citeRegEx": "Azimi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Azimi et al\\.", "year": 2012}, {"title": "Dynamic pricing without knowing the demand function: Risk bounds and near-optimal algorithms", "author": ["Omar Besbes", "Assaf Zeevi"], "venue": "Operations Research,", "citeRegEx": "Besbes and Zeevi.,? \\Q2009\\E", "shortCiteRegEx": "Besbes and Zeevi.", "year": 2009}, {"title": "Time-varying gaussian process bandit optimization", "author": ["Ilija Bogunovic", "Jonathan Scarlett", "Volkan Cevher"], "venue": "arXiv preprint arXiv:1601.06650,", "citeRegEx": "Bogunovic et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bogunovic et al\\.", "year": 2016}, {"title": "Simple regret for infinitely many armed bandits", "author": ["Alexandra Carpentier", "Michal Valko"], "venue": "In ICML,", "citeRegEx": "Carpentier and Valko.,? \\Q2015\\E", "shortCiteRegEx": "Carpentier and Valko.", "year": 2015}, {"title": "Power control in wireless cellular networks", "author": ["Mung Chiang", "Prashanth Hande", "Tian Lan", "Chee Wei Tan"], "venue": "Foundations and Trends in Networking,", "citeRegEx": "Chiang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chiang et al\\.", "year": 2008}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Varsha Dani", "Thomas P Hayes", "Sham M Kakade"], "venue": "In COLT,", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Exponential regret bounds for gaussian process bandits with deterministic observations", "author": ["Nando De Freitas", "Alex Smola", "Masrour Zoghi"], "venue": "arXiv preprint arXiv:1206.6457,", "citeRegEx": "Freitas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Freitas et al\\.", "year": 2012}, {"title": "High-dimensional gaussian process bandits", "author": ["Josip Djolonga", "Andreas Krause", "Volkan Cevher"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Djolonga et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Djolonga et al\\.", "year": 2013}, {"title": "Probability: Theory and Examples", "author": ["Rick Durrett"], "venue": null, "citeRegEx": "Durrett.,? \\Q2005\\E", "shortCiteRegEx": "Durrett.", "year": 2005}, {"title": "Thompson sampling for complex online problems", "author": ["Aditya Gopalan", "Shie Mannor", "Yishay Mansour"], "venue": "In ICML,", "citeRegEx": "Gopalan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2014}, {"title": "Safe exploration for optimization with gaussian processes", "author": ["Alkis Gotovos", "ETHZ CH", "Joel W Burdick"], "venue": null, "citeRegEx": "Gotovos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gotovos et al\\.", "year": 2015}, {"title": "Regret bounds for gaussian process bandit problems", "author": ["Steffen Gr\u00fcnew\u00e4lder", "Jean-Yves Audibert", "Manfred Opper", "John Shawe-Taylor"], "venue": "In AISTATS,", "citeRegEx": "Gr\u00fcnew\u00e4lder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gr\u00fcnew\u00e4lder et al\\.", "year": 2010}, {"title": "Exploiting correlation and budget constraints in bayesian multi-armed bandit optimization", "author": ["Matthew W Hoffman", "Bobak Shahriari", "Nando de Freitas"], "venue": "arXiv preprint arXiv:1303.6746,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Reinforcement learning: A survey", "author": ["Leslie Pack Kaelbling", "Michael L Littman", "Andrew W Moore"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Kaelbling et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1996}, {"title": "Thompson sampling: An asymptotically optimal finite-time analysis", "author": ["Emilie Kaufmann", "Nathaniel Korda", "R\u00e9mi Munos"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "Kaufmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "Multi-armed bandits in metric spaces", "author": ["Robert Kleinberg", "Aleksandrs Slivkins", "Eli Upfal"], "venue": "In Proceedings of the fortieth annual ACM symposium on Theory of computing,", "citeRegEx": "Kleinberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2008}, {"title": "Contextual gaussian process bandit optimization", "author": ["Andreas Krause", "Cheng S Ong"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krause and Ong.,? \\Q2011\\E", "shortCiteRegEx": "Krause and Ong.", "year": 2011}, {"title": "A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise", "author": ["Harold J Kushner"], "venue": "Journal of Basic Engineering,", "citeRegEx": "Kushner.,? \\Q1964\\E", "shortCiteRegEx": "Kushner.", "year": 1964}, {"title": "Self-normalization techniques for streaming confident regression. 2016", "author": ["Odalric-Ambrym Maillard"], "venue": null, "citeRegEx": "Maillard.,? \\Q2016\\E", "shortCiteRegEx": "Maillard.", "year": 2016}, {"title": "On bayesian methods for seeking the extremum", "author": ["J Mo\u010dkus"], "venue": "In Optimization Techniques IFIP Technical Conference,", "citeRegEx": "Mo\u010dkus.,? \\Q1975\\E", "shortCiteRegEx": "Mo\u010dkus.", "year": 1975}, {"title": "Gaussian processes for machine learning", "author": ["Carl Edward Rasmussen", "Christopher KI Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams.", "year": 2006}, {"title": "Linearly parameterized bandits", "author": ["Paat Rusmevichientong", "John N. Tsitsiklis"], "venue": "Math. Oper. Res.,", "citeRegEx": "Rusmevichientong and Tsitsiklis.,? \\Q2010\\E", "shortCiteRegEx": "Rusmevichientong and Tsitsiklis.", "year": 2010}, {"title": "Learning to optimize via posterior sampling", "author": ["Daniel Russo", "Benjamin Van Roy"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Russo and Roy.,? \\Q2014\\E", "shortCiteRegEx": "Russo and Roy.", "year": 2014}, {"title": "Learning with kernels: support vector machines, regularization, optimization, and beyond", "author": ["Bernhard Sch\u00f6lkopf", "Alexander J Smola"], "venue": "MIT press,", "citeRegEx": "Sch\u00f6lkopf and Smola.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola.", "year": 2002}, {"title": "Practical reinforcement learning in continuous spaces", "author": ["William D Smart", "Leslie Pack Kaelbling"], "venue": "In ICML,", "citeRegEx": "Smart and Kaelbling.,? \\Q2000\\E", "shortCiteRegEx": "Smart and Kaelbling.", "year": 2000}, {"title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "author": ["Niranjan Srinivas", "Andreas Krause", "Sham M Kakade", "Matthias Seeger"], "venue": "arXiv preprint arXiv:0912.3995,", "citeRegEx": "Srinivas et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2009}, {"title": "Finite-time analysis of kernelised contextual bandits", "author": ["Michal Valko", "Nathaniel Korda", "R\u00e9mi Munos", "Ilias Flaounas", "Nelo Cristianini"], "venue": "arXiv preprint arXiv:1309.6869,", "citeRegEx": "Valko et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Valko et al\\.", "year": 2013}, {"title": "Optimization as estimation with gaussian processes in bandit settings", "author": ["Zi Wang", "Bolei Zhou", "Stefanie Jegelka"], "venue": "In International Conf. on Artificial and Statistics (AISTATS),", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Bayesian optimization in high dimensions via random embeddings", "author": ["Ziyu Wang", "Masrour Zoghi", "Frank Hutter", "David Matheson", "N Freitas"], "venue": "AAAI Press/International Joint Conferences on Artificial Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "The Schur complement and its applications, volume 4", "author": ["Fuzhen Zhang"], "venue": "Springer Science & Business Media,", "citeRegEx": "Zhang.,? \\Q2006\\E", "shortCiteRegEx": "Zhang.", "year": 2006}], "referenceMentions": [{"referenceID": 4, "context": "Introduction Optimization over large domains under uncertainty is an important subproblem arising in a variety of sequential decision making problems, such as dynamic pricing in economics Besbes and Zeevi (2009), reinforcement learning with continuous state/action spaces Kaelbling et al.", "startOffset": 188, "endOffset": 212}, {"referenceID": 4, "context": "Introduction Optimization over large domains under uncertainty is an important subproblem arising in a variety of sequential decision making problems, such as dynamic pricing in economics Besbes and Zeevi (2009), reinforcement learning with continuous state/action spaces Kaelbling et al. (1996); Smart and Kaelbling (2000), and power control in wireless communication Chiang et al.", "startOffset": 188, "endOffset": 296}, {"referenceID": 4, "context": "Introduction Optimization over large domains under uncertainty is an important subproblem arising in a variety of sequential decision making problems, such as dynamic pricing in economics Besbes and Zeevi (2009), reinforcement learning with continuous state/action spaces Kaelbling et al. (1996); Smart and Kaelbling (2000), and power control in wireless communication Chiang et al.", "startOffset": 188, "endOffset": 324}, {"referenceID": 4, "context": "Introduction Optimization over large domains under uncertainty is an important subproblem arising in a variety of sequential decision making problems, such as dynamic pricing in economics Besbes and Zeevi (2009), reinforcement learning with continuous state/action spaces Kaelbling et al. (1996); Smart and Kaelbling (2000), and power control in wireless communication Chiang et al. (2008). A typical feature of such problems is a large, or potentially infinite, domain of decision points or covariates (prices, actions, transmit powers), together with only partial and noisy observability of the associated outcomes (demand, state/reward, communication rate); reward/loss information is revealed only for decisions that are chosen.", "startOffset": 188, "endOffset": 390}, {"referenceID": 4, "context": "Introduction Optimization over large domains under uncertainty is an important subproblem arising in a variety of sequential decision making problems, such as dynamic pricing in economics Besbes and Zeevi (2009), reinforcement learning with continuous state/action spaces Kaelbling et al. (1996); Smart and Kaelbling (2000), and power control in wireless communication Chiang et al. (2008). A typical feature of such problems is a large, or potentially infinite, domain of decision points or covariates (prices, actions, transmit powers), together with only partial and noisy observability of the associated outcomes (demand, state/reward, communication rate); reward/loss information is revealed only for decisions that are chosen. This often makes it hard to balance exploration and exploitation, as available knowledge must be transferred efficiently from a finite set of observations so far to estimates of the values of infinitely many decisions. A classic case in point is that of the canonical stochastic MAB with finitely many arms, where the effort to optimize scales with the total number of arms or decisions; the effect of this is catastrophic for large or infinite arm sets. With suitable structure in the values or rewards of arms, however, the challenge of sequential optimization can be efficiently addressed. Parametric bandits, especially linearly parameterized bandits Rusmevichientong and Tsitsiklis (2010), represent a well-studied class of structured decision making settings.", "startOffset": 188, "endOffset": 1427}, {"referenceID": 4, "context": "Introduction Optimization over large domains under uncertainty is an important subproblem arising in a variety of sequential decision making problems, such as dynamic pricing in economics Besbes and Zeevi (2009), reinforcement learning with continuous state/action spaces Kaelbling et al. (1996); Smart and Kaelbling (2000), and power control in wireless communication Chiang et al. (2008). A typical feature of such problems is a large, or potentially infinite, domain of decision points or covariates (prices, actions, transmit powers), together with only partial and noisy observability of the associated outcomes (demand, state/reward, communication rate); reward/loss information is revealed only for decisions that are chosen. This often makes it hard to balance exploration and exploitation, as available knowledge must be transferred efficiently from a finite set of observations so far to estimates of the values of infinitely many decisions. A classic case in point is that of the canonical stochastic MAB with finitely many arms, where the effort to optimize scales with the total number of arms or decisions; the effect of this is catastrophic for large or infinite arm sets. With suitable structure in the values or rewards of arms, however, the challenge of sequential optimization can be efficiently addressed. Parametric bandits, especially linearly parameterized bandits Rusmevichientong and Tsitsiklis (2010), represent a well-studied class of structured decision making settings. Here, every arm corresponds to a known, finite dimensional vector (its feature vector), and its expected reward is assumed to be an unknown linear function of its feature vector. This allows for a large, or even infinite, set of arms all lying in space of finite dimension, say d, and a rich line of work gives algorithms that attain sublinear regret with a polynomial dependence on the dimension, e.g., Confidence Ball Dani et al. (2008), OFUL Abbasi-Yadkori et al.", "startOffset": 188, "endOffset": 1938}, {"referenceID": 0, "context": "(2008), OFUL Abbasi-Yadkori et al. (2011) (a strengthening of Confidence Ball) and Thompson sampling for linear bandits", "startOffset": 13, "endOffset": 42}, {"referenceID": 0, "context": "The inequality generalizes a corresponding self-normalized bound for martingales in finite dimension proven by Abbasi-Yadkori et al. (2011). \u2022 Empirical comparisons of the algorithms developed above, with other GP-based algorithms, are presented, over both synthetic and real-world setups, demonstrating performance improvements of the proposed algorithms, as well as their performance under misspecification.", "startOffset": 111, "endOffset": 140}, {"referenceID": 0, "context": "This is a mild assumption on the noise (it holds, for instance, for distributions bounded in [\u2212R,R]) and is standard in the bandit literature Abbasi-Yadkori et al. (2011); Agrawal and Goyal (2013).", "startOffset": 142, "endOffset": 171}, {"referenceID": 0, "context": "This is a mild assumption on the noise (it holds, for instance, for distributions bounded in [\u2212R,R]) and is standard in the bandit literature Abbasi-Yadkori et al. (2011); Agrawal and Goyal (2013). Regret.", "startOffset": 142, "endOffset": 197}, {"referenceID": 0, "context": "This is a mild assumption on the noise (it holds, for instance, for distributions bounded in [\u2212R,R]) and is standard in the bandit literature Abbasi-Yadkori et al. (2011); Agrawal and Goyal (2013). Regret. The goal of an algorithm is to maximize its cumulative reward or alternatively minimize its cumulative regret \u2013 the loss incurred due to not knowing f \u2019s maximum point beforehand. Let x \u2208 argmaxx\u2208D f(x) be a maximum point of f (assuming the maximum is attained). The instantaneous regret incurred at time t is rt = f(x )\u2212f(xt), and the cumulative regret in a time horizon T (not necessarily known a priori) is defined to be RT = \u2211T t=1 rt. A sub-linear growth of RT in T signifies that RT /T \u2192 0 as T \u2192 \u221e, or vanishing per-round regret. Regularity Assumptions. Attaining sub-linear regret is impossible in general for arbitrary reward functions f and domains D, and thus some regularity assumptions are in order. In what follows, we assume that D is compact. The smoothness assumption we make on the reward function f is motivated by Gaussian processes4 and their associated reproducing kernel Hilbert spaces (RKHSs, see Sch\u00f6lkopf and Smola (2002)).", "startOffset": 142, "endOffset": 1154}, {"referenceID": 1, "context": "These assumptions are required to make the regret bounds scale-free and are standard in the literature Agrawal and Goyal (2013). Instead if k(x, x) \u2264 c or \u2016f\u2016k \u2264 cB, then our regret bounds would increase by a factor of c.", "startOffset": 103, "endOffset": 128}, {"referenceID": 29, "context": "In general, thus, this represents a misspecified prior and noise model, also termed the agnostic setting by Srinivas et al. (2009). The proposed algorithms, to follow, assume the knowledge of only the sub-Gaussianity parameter R, kernel function k and upper bound B on the RKHS norm of f .", "startOffset": 108, "endOffset": 131}, {"referenceID": 29, "context": "For a compact subset D of R, \u03b3T is O((lnT )) and O(T d(d+1)/(2\u03bd+d(d+1)) lnT ), respectively, for the Squared Exponential and Mat\u00e9rn kernels (Srinivas et al. (2009)), depending only polylogarithmically on the time T .", "startOffset": 141, "endOffset": 164}, {"referenceID": 29, "context": "Srinivas et al. (2009) have proposed the GP-UCB algorithm, and Valko et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 29, "context": "Srinivas et al. (2009) have proposed the GP-UCB algorithm, and Valko et al. (2013) the KernelUCB algorithm, for sequentially optimizing reward functions lying in the RKHS Hk(D).", "startOffset": 0, "endOffset": 83}, {"referenceID": 29, "context": "Srinivas et al. (2009) have proposed the GP-UCB algorithm, and Valko et al. (2013) the KernelUCB algorithm, for sequentially optimizing reward functions lying in the RKHS Hk(D). Both algorithms play an arm at time t using the rule: xt = argmaxx\u2208D \u03bct\u22121(x) + \u03b2\u0303t\u03c3t\u22121(x). GP-UCB uses the exploration parameter \u03b2\u0303t = \u221a 2B2 + 300\u03b3t\u22121 ln (t/\u03b4), with \u03bb set to \u03c3, where \u03c3 is additionally assumed to be a known, uniform (i.e., almost-sure) upper bound on all noise variables \u03b5t (see Theorem 3 in Srinivas et al. (2009)).", "startOffset": 0, "endOffset": 510}, {"referenceID": 1, "context": "2 Gaussian Process Thompson sampling (GP-TS) Our second algorithm, GP-TS (Algorithm 2), inspired by the success of Thompson sampling for standard and parametric bandits (Agrawal and Goyal, 2012; Kaufmann et al., 2012; Gopalan et al., 2014; Agrawal and Goyal, 2013), uses the time-varying scale parameter vt = B +R \u221a 2(\u03b3t\u22121 + 1 + ln(2/\u03b4)) and operates as follows.", "startOffset": 169, "endOffset": 264}, {"referenceID": 18, "context": "2 Gaussian Process Thompson sampling (GP-TS) Our second algorithm, GP-TS (Algorithm 2), inspired by the success of Thompson sampling for standard and parametric bandits (Agrawal and Goyal, 2012; Kaufmann et al., 2012; Gopalan et al., 2014; Agrawal and Goyal, 2013), uses the time-varying scale parameter vt = B +R \u221a 2(\u03b3t\u22121 + 1 + ln(2/\u03b4)) and operates as follows.", "startOffset": 169, "endOffset": 264}, {"referenceID": 13, "context": "2 Gaussian Process Thompson sampling (GP-TS) Our second algorithm, GP-TS (Algorithm 2), inspired by the success of Thompson sampling for standard and parametric bandits (Agrawal and Goyal, 2012; Kaufmann et al., 2012; Gopalan et al., 2014; Agrawal and Goyal, 2013), uses the time-varying scale parameter vt = B +R \u221a 2(\u03b3t\u22121 + 1 + ln(2/\u03b4)) and operates as follows.", "startOffset": 169, "endOffset": 264}, {"referenceID": 2, "context": "2 Gaussian Process Thompson sampling (GP-TS) Our second algorithm, GP-TS (Algorithm 2), inspired by the success of Thompson sampling for standard and parametric bandits (Agrawal and Goyal, 2012; Kaufmann et al., 2012; Gopalan et al., 2014; Agrawal and Goyal, 2013), uses the time-varying scale parameter vt = B +R \u221a 2(\u03b3t\u22121 + 1 + ln(2/\u03b4)) and operates as follows.", "startOffset": 169, "endOffset": 264}, {"referenceID": 0, "context": "Theorem 1 represents the kernelized generalization of the finite-dimensional result of Abbasi-Yadkori et al. (2011), and we recover their result under the special case of a linear kernel: \u03c6(x) = x for all x \u2208 R.", "startOffset": 87, "endOffset": 116}, {"referenceID": 0, "context": "Theorem 1 represents the kernelized generalization of the finite-dimensional result of Abbasi-Yadkori et al. (2011), and we recover their result under the special case of a linear kernel: \u03c6(x) = x for all x \u2208 R. We remark that when \u03c6 is a mapping to a finite-dimensional Hilbert space, the argument of AbbasiYadkori et al. (2011, Theorem 1) can be lifted to establish Theorem 1, but it breaks down in the generalized, infinite-dimensional RKHS setting, as the self-normalized bound in their paper has an explicit, growing dependence on the feature dimension. Specifically, the method of mixtures (de la Pena et al., 2009) or Laplace method, as dubbed by Maillard (2016) (Lemma 5.", "startOffset": 87, "endOffset": 670}, {"referenceID": 24, "context": ", Rasmussen and Williams (2006). 9.", "startOffset": 2, "endOffset": 32}, {"referenceID": 22, "context": "5 of Maillard (2016) states a similar result on the estimation of the unknown reward function from the RKHS.", "startOffset": 5, "endOffset": 21}, {"referenceID": 29, "context": "Srinivas et al. (2009), in the course of analyzing the GP-UCB algorithm, show that when the reward function lies in the RKHS Hk(D), GP-UCB obtains regret O (\u221a T (B \u221a \u03b3T + \u03b3T ln (T )) ) with high probability (see Theorem 3 therein for the exact bound).", "startOffset": 0, "endOffset": 23}, {"referenceID": 29, "context": "Srinivas et al. (2009), in the course of analyzing the GP-UCB algorithm, show that when the reward function lies in the RKHS Hk(D), GP-UCB obtains regret O (\u221a T (B \u221a \u03b3T + \u03b3T ln (T )) ) with high probability (see Theorem 3 therein for the exact bound). Furthermore, they assume that the noise \u03b5t is uniformly bounded by \u03c3, while our sub-Gaussianity assumption (see Equation 1) is slightly more general, and we are able to obtain a O(ln T ) multiplicative factor improvement in the final regret bound thanks to the new self-normalized inequality (Theorem 1). Additionally, in our numerical experiments, we observe a significantly improved performance of IGP-UCB over GP-UCB, both on synthetically generated function, and on real-world sensor measurement data (see Section 6). Comparison with KernelUCB. Valko et al. (2013) show that the cumulative regret of KernelUCB is \u00d5( \u221a d\u0303T ), where d\u0303, defined as the effective dimension, measures, in a sense, the number of principal directions over which the projection of the data in the RKHS is spread.", "startOffset": 0, "endOffset": 821}, {"referenceID": 0, "context": "For this kernel, \u03b3T = O(d lnT ), and the regret scaling of IGP-UCB is \u00d5(d \u221a T ) and that of GP-TS is \u00d5(d \u221a T ), which recovers the regret bounds of their linear, parametric analogues OFUL (Abbasi-Yadkori et al., 2011) and Linear Thompson sampling (Agrawal and Goyal, 2013), respectively.", "startOffset": 188, "endOffset": 217}, {"referenceID": 2, "context": ", 2011) and Linear Thompson sampling (Agrawal and Goyal, 2013), respectively.", "startOffset": 37, "endOffset": 62}, {"referenceID": 0, "context": "For this kernel, \u03b3T = O(d lnT ), and the regret scaling of IGP-UCB is \u00d5(d \u221a T ) and that of GP-TS is \u00d5(d \u221a T ), which recovers the regret bounds of their linear, parametric analogues OFUL (Abbasi-Yadkori et al., 2011) and Linear Thompson sampling (Agrawal and Goyal, 2013), respectively. Moreover, in this case d\u0303 = d, thus the regret of IGP-UCB is \u221a d factor away from that of KernelUCB. But the regret bound of KernelUCB also depends on the number of arms N , and if N is exponential in d, then it also suffers \u00d5(d \u221a T ) regret. We remark that a similar O(ln T ) factor improvement, as obtained by IGP-UCB over GP-UCB, was achieved in the linear parametric setting by Abbasi-Yadkori et al. (2011) in the OFUL algorithm, over its predecessor ConfidenceBall Dani et al.", "startOffset": 189, "endOffset": 699}, {"referenceID": 0, "context": "For this kernel, \u03b3T = O(d lnT ), and the regret scaling of IGP-UCB is \u00d5(d \u221a T ) and that of GP-TS is \u00d5(d \u221a T ), which recovers the regret bounds of their linear, parametric analogues OFUL (Abbasi-Yadkori et al., 2011) and Linear Thompson sampling (Agrawal and Goyal, 2013), respectively. Moreover, in this case d\u0303 = d, thus the regret of IGP-UCB is \u221a d factor away from that of KernelUCB. But the regret bound of KernelUCB also depends on the number of arms N , and if N is exponential in d, then it also suffers \u00d5(d \u221a T ) regret. We remark that a similar O(ln T ) factor improvement, as obtained by IGP-UCB over GP-UCB, was achieved in the linear parametric setting by Abbasi-Yadkori et al. (2011) in the OFUL algorithm, over its predecessor ConfidenceBall Dani et al. (2008). Finally we see that the for linear bandit problem with infinitely many actions, IGP-UCB attains the information theoretic lower bound of \u03a9(d \u221a T ) (see Dani et al.", "startOffset": 189, "endOffset": 777}, {"referenceID": 0, "context": "For this kernel, \u03b3T = O(d lnT ), and the regret scaling of IGP-UCB is \u00d5(d \u221a T ) and that of GP-TS is \u00d5(d \u221a T ), which recovers the regret bounds of their linear, parametric analogues OFUL (Abbasi-Yadkori et al., 2011) and Linear Thompson sampling (Agrawal and Goyal, 2013), respectively. Moreover, in this case d\u0303 = d, thus the regret of IGP-UCB is \u221a d factor away from that of KernelUCB. But the regret bound of KernelUCB also depends on the number of arms N , and if N is exponential in d, then it also suffers \u00d5(d \u221a T ) regret. We remark that a similar O(ln T ) factor improvement, as obtained by IGP-UCB over GP-UCB, was achieved in the linear parametric setting by Abbasi-Yadkori et al. (2011) in the OFUL algorithm, over its predecessor ConfidenceBall Dani et al. (2008). Finally we see that the for linear bandit problem with infinitely many actions, IGP-UCB attains the information theoretic lower bound of \u03a9(d \u221a T ) (see Dani et al. (2008)), but GP-TS is a factor of \u221a d away from it.", "startOffset": 189, "endOffset": 949}, {"referenceID": 1, "context": "We follow a similar approach given in Agrawal and Goyal (2013) to prove the regret bound of GP-TS.", "startOffset": 38, "endOffset": 63}, {"referenceID": 4, "context": "We consider 2 well-known synthetic benchmark functions for Bayesian Optimization: Rosenbrock and Hartman3 (see Azimi et al. (2012) for exact analytical expressions).", "startOffset": 111, "endOffset": 131}, {"referenceID": 17, "context": "Following Srinivas et al. (2009), we calculate the empirical covariance matrix of the sensor measurements and use it as the kernel matrix in the algorithms.", "startOffset": 10, "endOffset": 33}, {"referenceID": 11, "context": "An alternative line of work pertaining toX -armed bandits Kleinberg et al. (2008); Bubeck et al.", "startOffset": 58, "endOffset": 82}, {"referenceID": 11, "context": "An alternative line of work pertaining toX -armed bandits Kleinberg et al. (2008); Bubeck et al. (2011); Carpentier and Valko (2015); Azar et al.", "startOffset": 58, "endOffset": 104}, {"referenceID": 5, "context": "(2011); Carpentier and Valko (2015); Azar et al.", "startOffset": 8, "endOffset": 36}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure.", "startOffset": 37, "endOffset": 56}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R.", "startOffset": 37, "endOffset": 150}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al.", "startOffset": 37, "endOffset": 400}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al.", "startOffset": 37, "endOffset": 422}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al.", "startOffset": 37, "endOffset": 449}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al.", "startOffset": 37, "endOffset": 508}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al. (2013); Wang et al.", "startOffset": 37, "endOffset": 549}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al. (2013); Wang et al. (2013), time-varying Bogunovic et al.", "startOffset": 37, "endOffset": 569}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al. (2013); Wang et al. (2013), time-varying Bogunovic et al. (2016) safety-aware Gotovos et al.", "startOffset": 37, "endOffset": 607}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al. (2013); Wang et al. (2013), time-varying Bogunovic et al. (2016) safety-aware Gotovos et al. (2015), budget-constraint Hoffman et al.", "startOffset": 37, "endOffset": 642}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al. (2013); Wang et al. (2013), time-varying Bogunovic et al. (2016) safety-aware Gotovos et al. (2015), budget-constraint Hoffman et al. (2013) and noise-free De Freitas et al.", "startOffset": 37, "endOffset": 683}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al. (2013); Wang et al. (2013), time-varying Bogunovic et al. (2016) safety-aware Gotovos et al. (2015), budget-constraint Hoffman et al. (2013) and noise-free De Freitas et al. (2012) settings.", "startOffset": 37, "endOffset": 723}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al. (2013); Wang et al. (2013), time-varying Bogunovic et al. (2016) safety-aware Gotovos et al. (2015), budget-constraint Hoffman et al. (2013) and noise-free De Freitas et al. (2012) settings. Other relevant work focuses on best arm identification problem in the Bayesian setup considering pure exploration Gr\u00fcnew\u00e4lder et al. (2010). For Thompson sampling (TS), Russo and Van Roy (2014) analyze the Bayesian", "startOffset": 37, "endOffset": 873}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al. (2013); Wang et al. (2013), time-varying Bogunovic et al. (2016) safety-aware Gotovos et al. (2015), budget-constraint Hoffman et al. (2013) and noise-free De Freitas et al. (2012) settings. Other relevant work focuses on best arm identification problem in the Bayesian setup considering pure exploration Gr\u00fcnew\u00e4lder et al. (2010). For Thompson sampling (TS), Russo and Van Roy (2014) analyze the Bayesian", "startOffset": 37, "endOffset": 927}, {"referenceID": 12, "context": "By the convergence theorem for nonnegative super-martingales (Durrett, 2005), M \u221e = lim t\u2192\u221e M t exists almost surely, and thus M g \u03c4 is well-defined.", "startOffset": 61, "endOffset": 76}, {"referenceID": 12, "context": "By Fatou\u2019s lemma (Durrett, 2005), E [M \u03c4 ] = E [ lim t\u2192\u221e Qgt ] = E [ lim inf t\u2192\u221e Qgt ]", "startOffset": 17, "endOffset": 32}, {"referenceID": 12, "context": "t is also a super-martingale (Durrett, 2005).", "startOffset": 29, "endOffset": 44}, {"referenceID": 0, "context": "(9) To complete the proof, we now employ a stopping time construction as in Abbasi-Yadkori et al. (2011). For each t \u2265 0, define the \u2018bad\u2019 event", "startOffset": 76, "endOffset": 105}, {"referenceID": 16, "context": "Thus using Lemma B4 of Hoffman et al. (2013), for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4 |ft(x)\u2212 \u03bct\u22121(x)| \u2264 \u221a 2 ln(1/\u03b4) vt\u03c3t\u22121(x), and now applying union bound,", "startOffset": 23, "endOffset": 45}, {"referenceID": 33, "context": "Recursive Updates of Posterior Mean and Covariance We now describe a procedure to update the posterior mean and covariance function in a recursive fashion through the properties of Schur complement (Zhang (2006)) rather than evaluating Equation 2 and 3 at each round.", "startOffset": 199, "endOffset": 212}], "year": 2017, "abstractText": "We consider the stochastic bandit problem with a continuous set of arms, with the expected reward function over the arms assumed to be fixed but unknown. We provide two new Gaussian process-based algorithms for continuous bandit optimization \u2013 Improved GP-UCB (IGP-UCB) and GP-Thomson sampling (GP-TS), and derive corresponding regret bounds. Specifically, the bounds hold when the expected reward function belongs to the reproducing kernel Hilbert space (RKHS) that naturally corresponds to a Gaussian process kernel used as input by the algorithms. Along the way, we derive a new self-normalized concentration inequality for vectorvalued martingales of arbitrary, possibly infinite, dimension. Finally, experimental evaluation and comparisons to existing algorithms on synthetic and real-world environments are carried out that highlight the favorable gains of the proposed strategies in many cases.", "creator": "LaTeX with hyperref package"}}}