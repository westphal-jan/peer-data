{"id": "1704.00514", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Multi-Task Learning of Keyphrase Boundary Classification", "abstract": "Keyphrase boundary classification (KBC) is the task of detecting keyphrases in scientific articles and labelling them with respect to predefined types. Although important in practice, this task is so far underexplored, partly due to the lack of labelled data. To overcome this, we explore several auxiliary tasks, including semantic super-sense tagging and identification of multi-word expressions, and cast the task as a multi-task learning problem with deep recurrent neural networks. Our multi-task models perform significantly better than previous state of the art approaches on two scientific KBC datasets, particularly for long keyphrases.", "histories": [["v1", "Mon, 3 Apr 2017 10:25:22 GMT  (19kb)", "https://arxiv.org/abs/1704.00514v1", "ACL 2017"], ["v2", "Wed, 26 Apr 2017 16:48:49 GMT  (25kb)", "http://arxiv.org/abs/1704.00514v2", "ACL 2017"]], "COMMENTS": "ACL 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI stat.ML", "authors": ["isabelle augenstein", "anders s\u00f8gaard"], "accepted": true, "id": "1704.00514"}, "pdf": {"name": "1704.00514.pdf", "metadata": {"source": "CRF", "title": "Multi-Task Learning of Keyphrase Boundary Classification", "authors": ["Isabelle Augenstein", "Anders S\u00f8gaard"], "emails": ["i.augenstein@ucl.ac.uk", "soegaard@di.ku.dk"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.00 514v 2 [cs.C L] 26 Apr 201 7is the task of identifying keyphrases in scientific articles and marking them in terms of predefined types. Although this task is important in practice, it is so far little researched, partly due to the lack of marked data. To overcome this, we examine several auxiliary tasks, including semantic super-sense marking and the identification of multiword expressions, and consider the task to be a multi-task learning problem with deeply recurring neural networks. Our multi-task models perform significantly better on two scientific KBC datasets than previous state-of-the-art approaches, especially on long keyphrases."}, {"heading": "1 Introduction", "text": "The fact is that we will be able to be in a position, and that we will be able, we will be able to abide by the rules that we have set ourselves in order to comply with them."}, {"heading": "2 Keyphrase Boundary Classification", "text": "Consider the following sentence from a scientific paper: (1) We note that simple interpolation methods such as log-linear and linear interpolation improve performance but fall short of the performance of an oracle. This sentence occurs in the ACL RD-TEC 2.0 corpus, where interpolation methods as well as loglinear and linear interpolation are referred to as technical keyphrases, performance as a keyphrase related to measurements, and oracle as a keyphrase referred to as miscellaneous. In the following, we are interested in predicting the boundaries and types of all keyphrases."}, {"heading": "3 Multi-Task Learning", "text": "In fact, it is not as if it were an attempt to solve the problem, but rather an attempt to solve it. (...) It is not as if it were an attempt. (...) It is not as if it were an attempt. (...) It is not as if it were an attempt. (...) It is not as if it were an attempt. (...) It is as if it were an attempt. (...) It is not as if it were an attempt. (...) It is not as if it were an attempt. (...) It is as if it were an attempt. (...) It is not as if it were an attempt. (...) It is not as if it were an attempt. (...) It is not as if it were an attempt. (...) It is not as if it were an attempt. (...) It is not as if it were an attempt. (...) It is not as if it were an attempt."}, {"heading": "4 Experiments", "text": "asD \"i\" s. \"i\" s \"i\" s. \"W\" i \"s.\" i \"s.\" D \"i\" c \"h, i\" s \"i\" s. \"s.\" s \"s.\" s \"s.\" i \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s \"s.\" s. \"s.\" s. \"s.\" s \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s \"s.\" s. \"s.\" s \"s.\" s. \"s\" s. \"s.\" s \"s.\" s \"s\" s. \"s\" s \"s.\" s \"s\" s. \"s\" s \"s\" s \"s.\" s \"s\" s. \"s\" s \"s\" s. \"s\" s \"s\" s. \"s\" s \"s.\" s \"s\" s \"s.\" s \"s\" s \"s\" s. \"s\" s \"s\" s. \"s\" s \"s.\" s \"s\" s. \"s\" s \"s\" s. \"s\" s \"s\" s. \"s\" s \"s\" s. \"s\" s \"s\" s. \"s.\" s \"s.\" s \"s\" s. \"s.\" s \"s.\" s \"s\" s. \"s\" s \"s\" s. \"s.\" s \"s.\" s. \"s\" s. \"s.\" s \"s.\" s \"s\" s \"s.\" s \"s.\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s. \"s\" s \"s"}, {"heading": "5 Results and Analysis", "text": "The results for Semeal 2017 Task 10 Corpus are presented in Table 2, and for the ACL RD-TEC Corpus in Table 3. For Semeval Corpus, all five labeled multi-task models, both examples of previous work, as well as our singletask BiLSTM Baseline, surpass by some distance. For ACL1 http: / / ronan.collobert.com / senna / 2http: / / nlp.stanford.edu / CRF-Baseline 3 https: / github.com / stack / stack / stack-TEC, three of five labeled multi-tasks that are better than the Single-Task BiLSTM Baseline."}, {"heading": "6 Related Work", "text": "Several variants have been introduced, including the hard splitting of selected layers (S\u00f8gaard and Goldberg, 2016) and the splitting of parts (sub-spaces) of layers (Liu et al., 2015). S\u00f8gaard and Goldberg (2016) show that hard parameter splitting is an effective regulator, even for heterogeneous tasks such as those considered here. Hard parameter splitting has been investigated for several tasks, including CCG supertagging (S\u00f8gaard and Goldberg, 2016), text normalization (Bollman and S\u00f8gaard, 2016), neural machine translation (Dong et al., 2015; Luong et al., 2016) and supersense tagging (Mart\u00ed \u00ed nez Alonso and Plank, 2017)."}, {"heading": "7 Conclusions and Future Work", "text": "We present a state-of-the-art approach to classifying keyword boundaries using data from related complementary tasks, in particular super sense marking and identification of multi-word expressions. Multi-task learning is significantly improved compared to previous approaches to KBC, with an error reduction of up to 9.64%, mainly due to better identification and labelling of long keywords. In future work, we plan to explore alternative multi-task learning systems for hard-sharing parameters and experiment with additional auxiliary tasks. The auxiliary tasks considered here are standard NLP tasks, apart from hyperlink predictions. Other tasks may be more directly relevant, such as predicting the layout of calls to scientific conferences or predicting hashtags in tweets from scientists, as both data sources contain scientific keywords."}, {"heading": "Acknowledgments", "text": "We would like to thank Elsevier for supporting this work."}], "references": [{"title": "SemEval-2017 Task 10 : Extracting Keyphrases and Relations from Scientific Publications", "author": ["Isabelle Augenstein", "Mrinal Das", "Sebastian Riedel", "Lakshmi Vikraman", "Andrew McCallum."], "venue": "Proceedings of SemEval, to appear.", "citeRegEx": "Augenstein et al\\.,? 2017", "shortCiteRegEx": "Augenstein et al\\.", "year": 2017}, {"title": "A model of inductive bias learning", "author": ["Jonathan Baxter."], "venue": "Journal of Artificial Intelligence Research 12:149\u2013198.", "citeRegEx": "Baxter.,? 2000", "shortCiteRegEx": "Baxter.", "year": 2000}, {"title": "Improving historical spelling normalization with bi-directional LSTMs and multi-task learning", "author": ["Marcel Bollman andAnders S\u00f8gaard."], "venue": "Proceedings of COLING.", "citeRegEx": "S\u00f8gaard.,? 2016", "shortCiteRegEx": "S\u00f8gaard.", "year": 2016}, {"title": "Multitask Learning: A Knowledge-Based Source of Inductive Bias", "author": ["Rich Caruana."], "venue": "Proceedings of ICML.", "citeRegEx": "Caruana.,? 1993", "shortCiteRegEx": "Caruana.", "year": 1993}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011b", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Frame-semantic parsing", "author": ["Dipanjan Das", "Desai Chen", "Andre Martins", "Nathan Schneider", "Noah Smith."], "venue": "Computational linguistics 40(1):9\u201356.", "citeRegEx": "Das et al\\.,? 2014", "shortCiteRegEx": "Das et al\\.", "year": 2014}, {"title": "Multi-Task Learning for Multiple Language Translation", "author": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."], "venue": "Proceedings of ACL.", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Incorporating non-local information into information extraction systems by Gibbs sampling", "author": ["Jenny Finkel", "Trond Grenager", "Christopher Manning."], "venue": "Proceedings of ACL.", "citeRegEx": "Finkel et al\\.,? 2005", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Framewise Phoneme Classification with Bidirectional LSTM and other Neural Network Architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks 18(5):602\u2013610.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Automatic Keyphrase Extraction: A Survey of the State of the Art", "author": ["Kazi Saidul Hasan", "Vincent Ng."], "venue": "Proceedings of ACL.", "citeRegEx": "Hasan and Ng.,? 2014", "shortCiteRegEx": "Hasan and Ng.", "year": 2014}, {"title": "More or less supervised supersense tagging of Twitter", "author": ["Anders Johannsen", "Dirk Hovy", "H\u00e9ctor Mart\u0131\u0301nez", "Barbara Plank", "Anders S\u00f8gaard"], "venue": "In Proceedings of *SEM", "citeRegEx": "Johannsen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Johannsen et al\\.", "year": 2014}, {"title": "SemEval-2010 Task 5 : Automatic Keyphrase Extraction from Scientific Articles", "author": ["Su Nam Kim", "Olena Medelyan", "Min-Yen Kan", "Timothy Baldwin."], "venue": "Proceedings of SemEval.", "citeRegEx": "Kim et al\\.,? 2010", "shortCiteRegEx": "Kim et al\\.", "year": 2010}, {"title": "Neural Architectures for Named Entity Recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."], "venue": "Proceedings of NAACL-HLT. pages 260\u2013270.", "citeRegEx": "Lample et al\\.,? 2016", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Finegrained Opinion Mining with Recurrent Neural Networks and Word Embeddings", "author": ["Pengfei Liu", "Shafiq Joty", "Helen Meng."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Deep Multi-Task Learning with Shared Memory for Text Classification", "author": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Multi-task Sequence to Sequence Learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "Proceedings of ICLR.", "citeRegEx": "Luong et al\\.,? 2016", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "When is multitask learning effective? Semantic sequence prediction under varying data conditions", "author": ["H\u00e9ctor Mart\u0131\u0301nez Alonso", "Barbara Plank"], "venue": "In Proceedings of EACL", "citeRegEx": "Alonso and Plank.,? \\Q2017\\E", "shortCiteRegEx": "Alonso and Plank.", "year": 2017}, {"title": "Bounds for Linear Multi Task Learning", "author": ["Andreas Maurer."], "venue": "Journal of Machine Learning Research 7:117\u2013139.", "citeRegEx": "Maurer.,? 2007", "shortCiteRegEx": "Maurer.", "year": 2007}, {"title": "The ACL RD-TEC 2.0: A Language Resource for Evaluating Term Extraction and Entity Recognition Methods", "author": ["Behrang QasemiZadeh", "Anne-Kathrin Schumann"], "venue": "In Proceedings of LREC", "citeRegEx": "QasemiZadeh and Schumann.,? \\Q2016\\E", "shortCiteRegEx": "QasemiZadeh and Schumann.", "year": 2016}, {"title": "Semi-supervisedMultitask Learning for Sequence Labeling", "author": ["Marek Rei."], "venue": "Proceedings of ACL, to appear.", "citeRegEx": "Rei.,? 2017", "shortCiteRegEx": "Rei.", "year": 2017}, {"title": "A Corpus and Model Integrating Multiword Expressions and Supersenses", "author": ["Nathan Schneider", "Noah Smith."], "venue": "Proceedings of NAACL-HLT .", "citeRegEx": "Schneider and Smith.,? 2015", "shortCiteRegEx": "Schneider and Smith.", "year": 2015}, {"title": "Deep multi-task learning with low level tasks supervised at lower layers", "author": ["Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proceedings of ACL.", "citeRegEx": "S\u00f8gaard and Goldberg.,? 2016", "shortCiteRegEx": "S\u00f8gaard and Goldberg.", "year": 2016}, {"title": "Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing", "author": ["Valentin Spitkovsky", "Daniel Jurafsky", "Hiyan Alshawi."], "venue": "Proceedings of ACL.", "citeRegEx": "Spitkovsky et al\\.,? 2010", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2010}, {"title": "Supervised Keyphrase Extraction as Positive Unlabeled Learning", "author": ["Lucas Sterckx", "Cornelia Caragea", "Thomas Demeester", "Chris Develder."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Sterckx et al\\.,? 2016", "shortCiteRegEx": "Sterckx et al\\.", "year": 2016}, {"title": "Online Segment to Segment Neural Transduction", "author": ["Lei Yu", "Jan Buys", "Phil Blunsom."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "\u22c6Both authors contributed equally available recently (QasemiZadeh and Schumann, 2016; Augenstein et al., 2017).", "startOffset": 53, "endOffset": 110}, {"referenceID": 0, "context": "\u22c6Both authors contributed equally available recently (QasemiZadeh and Schumann, 2016; Augenstein et al., 2017).", "startOffset": 53, "endOffset": 110}, {"referenceID": 9, "context": "Typical KBC approaches therefore rely on hand-crafted gazetteers (Hasan and Ng, 2014) or reduce the task to extracting a list of keyphrases for each document (Kim et al.", "startOffset": 65, "endOffset": 85}, {"referenceID": 11, "context": "Typical KBC approaches therefore rely on hand-crafted gazetteers (Hasan and Ng, 2014) or reduce the task to extracting a list of keyphrases for each document (Kim et al., 2010) instead of identifying mentions of keyphrases in sentences.", "startOffset": 158, "endOffset": 176}, {"referenceID": 12, "context": "For related more common NLP tasks such as named entity recognition and identification of multi-word expressions, neural sequence labelling methods have been shown to be useful (Lample et al., 2016).", "startOffset": 176, "endOffset": 197}, {"referenceID": 1, "context": "This approach to multi-task learning has three advantages: a) It significantly reduces Rademacher complexity (Baxter, 2000; Maurer, 2007), i.", "startOffset": 109, "endOffset": 137}, {"referenceID": 17, "context": "This approach to multi-task learning has three advantages: a) It significantly reduces Rademacher complexity (Baxter, 2000; Maurer, 2007), i.", "startOffset": 109, "endOffset": 137}, {"referenceID": 1, "context": "be cast as a regulariser as studies show reductions in Rademacher complexity in multi-task architectures over single-task architectures (Baxter, 2000; Maurer, 2007).", "startOffset": 136, "endOffset": 164}, {"referenceID": 17, "context": "be cast as a regulariser as studies show reductions in Rademacher complexity in multi-task architectures over single-task architectures (Baxter, 2000; Maurer, 2007).", "startOffset": 136, "endOffset": 164}, {"referenceID": 1, "context": "be cast as a regulariser as studies show reductions in Rademacher complexity in multi-task architectures over single-task architectures (Baxter, 2000; Maurer, 2007). Here, we follow the probably most common approach to multi-task learning, known as hard parameter sharing. This was introduced in Caruana (1993) in the context of deep neural networks, in which hidden layers can be shared among tasks.", "startOffset": 137, "endOffset": 311}, {"referenceID": 20, "context": "(2010), (4) identification of multi-word expressions using the Streusle corpus (Schneider and Smith, 2015); and (5) semantic super-sense tagging using the Semcor dataset, following Johannsen et al.", "startOffset": 79, "endOffset": 106}, {"referenceID": 2, "context": "Auxiliary tasks We experiment with five auxiliary tasks: (1) syntactic chunking using annotations extracted from the English Penn Treebank, following S\u00f8gaard and Goldberg (2016); (2) frame target annotations from FrameNet 1.", "startOffset": 150, "endOffset": 178}, {"referenceID": 2, "context": "Auxiliary tasks We experiment with five auxiliary tasks: (1) syntactic chunking using annotations extracted from the English Penn Treebank, following S\u00f8gaard and Goldberg (2016); (2) frame target annotations from FrameNet 1.5 (corresponding to the target identification and classification tasks in Das et al. (2014)); (3) hyperlink prediction using the dataset from Spitkovsky et al.", "startOffset": 150, "endOffset": 316}, {"referenceID": 2, "context": "Auxiliary tasks We experiment with five auxiliary tasks: (1) syntactic chunking using annotations extracted from the English Penn Treebank, following S\u00f8gaard and Goldberg (2016); (2) frame target annotations from FrameNet 1.5 (corresponding to the target identification and classification tasks in Das et al. (2014)); (3) hyperlink prediction using the dataset from Spitkovsky et al. (2010), (4) identification of multi-word expressions using the Streusle corpus (Schneider and Smith, 2015); and (5) semantic super-sense tagging using the Semcor dataset, following Johannsen et al.", "startOffset": 150, "endOffset": 391}, {"referenceID": 2, "context": "Auxiliary tasks We experiment with five auxiliary tasks: (1) syntactic chunking using annotations extracted from the English Penn Treebank, following S\u00f8gaard and Goldberg (2016); (2) frame target annotations from FrameNet 1.5 (corresponding to the target identification and classification tasks in Das et al. (2014)); (3) hyperlink prediction using the dataset from Spitkovsky et al. (2010), (4) identification of multi-word expressions using the Streusle corpus (Schneider and Smith, 2015); and (5) semantic super-sense tagging using the Semcor dataset, following Johannsen et al. (2014). We train our models on the main task with one auxiliary task", "startOffset": 150, "endOffset": 589}, {"referenceID": 0, "context": "Datasets We evaluate on the SemEval 2017 Task 10 dataset (Augenstein et al., 2017) and the the ACL RD-TEC 2.", "startOffset": 57, "endOffset": 82}, {"referenceID": 18, "context": "0 dataset (QasemiZadeh and Schumann, 2016).", "startOffset": 10, "endOffset": 42}, {"referenceID": 8, "context": "Models Our single- and multi-task networks are three-layer, bi-directional LSTMs (Graves and Schmidhuber, 2005) with pre-trained SENNA embeddings.", "startOffset": 81, "endOffset": 111}, {"referenceID": 2, "context": "The dimensionality of the embeddings is 50, and we follow S\u00f8gaard and Goldberg (2016) in using the same dimensionality for the hidden layers.", "startOffset": 58, "endOffset": 86}, {"referenceID": 7, "context": "Baselines Our baselines are Finkel et al. (2005) and Lample et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 7, "context": "Baselines Our baselines are Finkel et al. (2005) and Lample et al. (2016), in order to compare to a lexicalised and a state-of-the-art neural method.", "startOffset": 28, "endOffset": 74}, {"referenceID": 7, "context": "The lexicalised Finkel et al. (2005) model shows a surprisingly competitive performance on the ACL RD-TEC corpus, where it is only 2 points in F1 behind our best performing labelled model and on par with our best-performing unlabelled model.", "startOffset": 16, "endOffset": 37}, {"referenceID": 7, "context": "The lexicalised Finkel et al. (2005) model shows a surprisingly competitive performance on the ACL RD-TEC corpus, where it is only 2 points in F1 behind our best performing labelled model and on par with our best-performing unlabelled model. Results with Lample et al. (2016), on the other hand, are lower than the Finkel et al.", "startOffset": 16, "endOffset": 276}, {"referenceID": 7, "context": "The lexicalised Finkel et al. (2005) model shows a surprisingly competitive performance on the ACL RD-TEC corpus, where it is only 2 points in F1 behind our best performing labelled model and on par with our best-performing unlabelled model. Results with Lample et al. (2016), on the other hand, are lower than the Finkel et al. (2005) baseline.", "startOffset": 16, "endOffset": 336}, {"referenceID": 3, "context": "Multi-Task Learning Hard sharing of all hidden layers was introduced in Caruana (1993), and popularised in NLP by Collobert et al.", "startOffset": 72, "endOffset": 87}, {"referenceID": 3, "context": "Multi-Task Learning Hard sharing of all hidden layers was introduced in Caruana (1993), and popularised in NLP by Collobert et al. (2011a). Several variants have been introduced, including hard sharing of selected", "startOffset": 72, "endOffset": 139}, {"referenceID": 21, "context": "layers (S\u00f8gaard and Goldberg, 2016) and sharing of parts (subspaces) of layers (Liu et al.", "startOffset": 7, "endOffset": 35}, {"referenceID": 13, "context": "layers (S\u00f8gaard and Goldberg, 2016) and sharing of parts (subspaces) of layers (Liu et al., 2015).", "startOffset": 79, "endOffset": 97}, {"referenceID": 2, "context": "layers (S\u00f8gaard and Goldberg, 2016) and sharing of parts (subspaces) of layers (Liu et al., 2015). S\u00f8gaard and Goldberg (2016) show that hard parameter sharing is an effective regulariser, also on heterogeneous tasks such as the ones considered here.", "startOffset": 8, "endOffset": 127}, {"referenceID": 21, "context": "ing CCG super tagging (S\u00f8gaard and Goldberg, 2016), text normalisation (Bollman and S\u00f8gaard, 2016), neural machine translation (Dong et al.", "startOffset": 22, "endOffset": 50}, {"referenceID": 6, "context": "ing CCG super tagging (S\u00f8gaard and Goldberg, 2016), text normalisation (Bollman and S\u00f8gaard, 2016), neural machine translation (Dong et al., 2015; Luong et al., 2016), and super-sense tagging (Mart\u0131\u0301nez Alonso and Plank, 2017).", "startOffset": 127, "endOffset": 166}, {"referenceID": 15, "context": "ing CCG super tagging (S\u00f8gaard and Goldberg, 2016), text normalisation (Bollman and S\u00f8gaard, 2016), neural machine translation (Dong et al., 2015; Luong et al., 2016), and super-sense tagging (Mart\u0131\u0301nez Alonso and Plank, 2017).", "startOffset": 127, "endOffset": 166}, {"referenceID": 14, "context": "Sharing of information can further be achieved by extending LSTMs with an external memory shared across tasks (Liu et al., 2016).", "startOffset": 110, "endOffset": 128}, {"referenceID": 2, "context": "ing CCG super tagging (S\u00f8gaard and Goldberg, 2016), text normalisation (Bollman and S\u00f8gaard, 2016), neural machine translation (Dong et al., 2015; Luong et al., 2016), and super-sense tagging (Mart\u0131\u0301nez Alonso and Plank, 2017). Sharing of information can further be achieved by extending LSTMs with an external memory shared across tasks (Liu et al., 2016). A further instance of multi-task learning is to optimise a supervised training objective jointly with an unsupervised training objective, as shown in Yu et al. (2016) for natural language generation and auto-encoding, and in Rei (2017) for different sequence labelling tasks and language modelling.", "startOffset": 23, "endOffset": 525}, {"referenceID": 2, "context": "ing CCG super tagging (S\u00f8gaard and Goldberg, 2016), text normalisation (Bollman and S\u00f8gaard, 2016), neural machine translation (Dong et al., 2015; Luong et al., 2016), and super-sense tagging (Mart\u0131\u0301nez Alonso and Plank, 2017). Sharing of information can further be achieved by extending LSTMs with an external memory shared across tasks (Liu et al., 2016). A further instance of multi-task learning is to optimise a supervised training objective jointly with an unsupervised training objective, as shown in Yu et al. (2016) for natural language generation and auto-encoding, and in Rei (2017) for different sequence labelling tasks and language modelling.", "startOffset": 23, "endOffset": 594}, {"referenceID": 4, "context": "Deep neural networks have been applied to NER in Collobert et al. (2011b); Lample et al.", "startOffset": 49, "endOffset": 74}, {"referenceID": 4, "context": "Deep neural networks have been applied to NER in Collobert et al. (2011b); Lample et al. (2016). Other successful methods rely on conditional random fields, thereby modelling the probability of each output label conditioned on the label at the previous time step.", "startOffset": 49, "endOffset": 96}, {"referenceID": 4, "context": "Deep neural networks have been applied to NER in Collobert et al. (2011b); Lample et al. (2016). Other successful methods rely on conditional random fields, thereby modelling the probability of each output label conditioned on the label at the previous time step. Lample et al. (2016),", "startOffset": 49, "endOffset": 285}, {"referenceID": 9, "context": "Keyphrase detection methods specific to the scientific domain often use keyphrase gazetteers as features or exploit citation graphs (Hasan and Ng, 2014).", "startOffset": 132, "endOffset": 152}, {"referenceID": 11, "context": "for mention-level identification (Kim et al., 2010; Sterckx et al., 2016).", "startOffset": 33, "endOffset": 73}, {"referenceID": 23, "context": "for mention-level identification (Kim et al., 2010; Sterckx et al., 2016).", "startOffset": 33, "endOffset": 73}], "year": 2017, "abstractText": "Keyphrase boundary classification (KBC) is the task of detecting keyphrases in scientific articles and labelling them with respect to predefined types. Although important in practice, this task is so far underexplored, partly due to the lack of labelled data. To overcome this, we explore several auxiliary tasks, including semantic super-sense tagging and identification of multi-word expressions, and cast the task as a multi-task learning problem with deep recurrent neural networks. Our multi-task models perform significantly better than previous state of the art approaches on two scientific KBC datasets, particularly for long keyphrases.", "creator": "LaTeX with hyperref package"}}}