{"id": "1704.00559", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Neural Lattice-to-Sequence Models for Uncertain Inputs", "abstract": "The input to a neural sequence-to-sequence model is often determined by an up-stream system, e.g. a word segmenter, part of speech tagger, or speech recognizer. These up-stream models are potentially error-prone. Representing inputs through word lattices allows making this uncertainty explicit by capturing alternative sequences and their posterior probabilities in a compact form. In this work, we extend the TreeLSTM (Tai et al., 2015) into a LatticeLSTM that is able to consume word lattices, and can be used as encoder in an attentional encoder-decoder model. We integrate lattice posterior scores into this architecture by extending the TreeLSTM's child-sum and forget gates and introducing a bias term into the attention mechanism. We experiment with speech translation lattices and report consistent improvements over baselines that translate either the 1-best hypothesis or the lattice without posterior scores.", "histories": [["v1", "Mon, 3 Apr 2017 13:03:40 GMT  (415kb,D)", "http://arxiv.org/abs/1704.00559v1", null], ["v2", "Fri, 21 Jul 2017 13:31:07 GMT  (420kb,D)", "http://arxiv.org/abs/1704.00559v2", "EMNLP 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["matthias sperber", "graham neubig", "jan niehues", "alex waibel"], "accepted": true, "id": "1704.00559"}, "pdf": {"name": "1704.00559.pdf", "metadata": {"source": "CRF", "title": "Neural Lattice-to-Sequence Models for Uncertain Inputs", "authors": ["Matthias Sperber", "Graham Neubig", "Jan Niehues", "Alex Waibel"], "emails": ["matthias.sperber@kit.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to"}, {"heading": "2 Background", "text": "Our work extends the basic work on the models of attentive encoder decoders that we examine in this section (Kalchburner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).For an input sequence x = (x1, x2,.., xN), the goal is to generate an appropriate output sequence y = (y1, y2,.., yM).The conditional probability p (y | x) = M \u0448t = 1 p (yt | y < t, x).The training is performed in an end-to-end mode. This probability is factored as the product of the conditional probabilities of each token to be generated: p (y | x) = M \u0448t = 1 p (yt | y < t, x).The training is performed in an end-to-end mode. The training goal is to estimate the parameter J, the probability of the protocol to maximize the saturation."}, {"heading": "2.1 Encoder", "text": "In our base model, the encoder is a bidirectional recursive neural network (RNN), followed by (Bahdanau et al., 2015), processing the source record in both forward and backward directions with two separate RNNNs. For each xi input, two hidden states are generated as \u2212 \u2192 h i = LSTM (Efwd (xi), \u2212 \u2192 h i \u2212 1) (1) \u2190 \u2212 h i = LSTM (Ebwd (xi), \u2190 \u2212 h i + 1), (2), in which Efwd and Ebwd embed reference tables. We opt for long-term short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) recursive units due to their high performance, and to later exploit the TreeLSTM extension (Tai et al., 2015)."}, {"heading": "2.2 Attention", "text": "We use an attention mechanism (Luong et al., 2015) to summarize the encoder outputs in a fixed size representation. At each decoding step j, a context vector cj is calculated as a weighted average of the hidden states of the source: cj = N \u2211 i = 1 \u03b1ijhi. Normalized attention weights \u03b1ij measure the relative meaning of the source words for the current decoding step and are calculated as Softmax, summing the normalization factor Z over i: \u03b1ij = 1Z exp (s (sj \u2212 1, hi) (\u00b7) s is an upstream neural network with a single layer that estimates the significance of the hidden source state hi for the production of the next target symbol yj, due to the previous decoder state sj \u2212 1."}, {"heading": "2.3 Decoder", "text": "The decoder generates output symbols that are conditioned to the encoder states via the attention mechanism. It contains another LSTM that is initialized using the final hidden state of the encoder: s0 = hN. The decoding in step j is done as follows, assuming a special sequence start symbol y0: sj = LSTM (Etrg (yj \u2212 1), sj \u2212 1) s-t = tanh (Whs [sj; cj] + bhs) The conditional probability that the j-th target word is generated is: p (yj | y < j, x) = softmax (Wsos \u0442t + bso) Here Etrg is the target embedding table, Whs and Wso are weight matrices, and bhs and bso are bias vectors. When decoding, the bar search is used to find an output sequence with a high probability of conditionality."}, {"heading": "3 Attentional Lattice-to-Sequence Model", "text": "The seq2seq model described above assumes sequential input and is therefore limited to taking a single output from an upstream model as input. Instead, we would like to use grids to transfer uncertainty from an upstream model."}, {"heading": "3.1 Lattices", "text": "Grids (e.g. Figure 1) represent several ambiguous or competing sequences in a compact form. They are a more efficient alternative to enumerating all hypotheses as the n-best list, since they allow redundant calculation of subsequences divided between multiple hypotheses. Grids can be generated either directly, e.g. by ASR dumping of its truncated search space (Post et al., 2013), or by merging multiple nbest sequences (Dyer et al., 2008; Su et al., 2016). A word grid G = < V, E > is a directed, connected, and acyclic graph with the nodes V and edges E. V-N is a node order, and (k, i) E is an edge that denotes the node k with the node i. C (i) denotes the set of preceding nodes nodes for the node i. We assume that all nodes are topologically ordered with one word < each one."}, {"heading": "3.2 Lattices and the TreeLSTM", "text": "One thing to note here is that grid nodes can have multiple precursor states. In contrast, hidden states in LSTMs and other sequential RNNs are bound to only one precursor state (h-j in the left column of Table 1), making standard RNNNs unsuitable for modeling grid states. Fortunately, Tai et al. (2015)'s TreeLSTM, designed to compose tree encodings, can also be easily applied to grid states; the TreeLSTM composes multiple child states into a parent state that can also be applied to grid states to compose multiple precursor states into a succession state. Table 1, middle column, shows TreeLSTM in its parental variant, which supports an arbitrary number of precursor states."}, {"heading": "3.3 Node-labeled Lattices", "text": "At this point, we are taking a step back to motivate our decision to assign grid nodes to word labels, which is in contrast to the previous work by Ladhak et al. (2016) and Su et al. (2016), which assigned grid nodes to word labels. Recurring states in edge-labeled grid nodes are not only bound to multiple preceding states, but also have to group words from multiple incoming edges, meaning that hidden units can represent more than one word in the grid. Furthermore, our approach to encoding word-labeled grids is similar in spirit to that of Eriguchi et al. (2016), which used the TreeLSTM in an attentive tree-to-sequence model."}, {"heading": "4 Integration of Lattice Scores", "text": "This section describes the most important technical contribution of our work: the integration of lattice numbers encoding input uncertainty into the lat2seq framework. These lattice numbers assign different probabilities to competing paths and are often provided by upstream statistical models. For example, an ASR can append posterior probabilities that capture acoustic evidence and linguistic plausibility of words in the lattice. In this section, we describe our method by first explaining how to normalize values to a format that is easy to use in our method, and then introducing our methods for incorporating these values into our encoder calculations."}, {"heading": "4.1 Lattice Score Normalization", "text": "Grid point numbers derived from upstream systems (such as ASR) are typically given in a forward normalized way, interpreted as the probability of a node versus its predecessor. Here, the outgoing edges add up to one, like the most left-leaning values in Figure 3. However, in some of our methods it will be necessary to normalize the values in the backward direction, so that the weights from incoming connections add up to one or are globally normalized, so that the probability of the node is the marginal probability of all paths containing this node. Letwf, i, wm, i, wb, i, i, i, i, i are the forward normalized, marginal, and backward normalized values for the node i, respectively, illustrated in Figure 3. Given wf, i, we can reconstruct these marginal probabilities using the wm model."}, {"heading": "4.2 Integration Approaches", "text": "We propose three methods to integrate these values into our lat2seq model, with equations shown in the right-hand column of Table 1. These methods can optionally be combined, and we are conducting an ablation study to assess the effectiveness of each method in isolation (\u00a7 5,3). The first method is to calculate a weighted child sum (WCS) using grid values as weights in the composition of the hidden state, based on the intuition that hidden predecessor states with high grid weights should have a greater influence on their successor than states with low weights. The exact formulas for WCS are shown in (5). The second method distorts the gating gate for each predecessor cell state so that predecessor states with high grid weights are more likely to pass the gating gate (BFG). The intuition for this is similar to WCS; the composed cell state is more strongly influenced by high cell states."}, {"heading": "4.3 Pre-training", "text": "Finally, in order to reduce the computational effort, we perform a two-step training process where the model is first pre-trained using sequential data and then refined using grid data. Pre-training, such as the standard Neural Machine Translation (NMT) training, allows efficient training with minibatches and also training with standard text corpora for which we may not have grid data available. Fine-tuning is then done using parallel data with grid on the source side. This is much slower than pre-training, as the network structure changes from sentence to sentence and prevents us from using efficient mini-batch calculations. However, fine-tuning is sufficient for only a small number of iterations in general, as the model is already relatively accurate. In practice, we found it important to use minibatches when fine-tuning, accumulating progressions across multiple examples is possible before performing parameter updates. This led to negligible accelerations, but it is also possible to improve the model significantly in practice."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Setting", "text": "We are conducting experiments on the Fisher and Callhome Spanish-English Speech Translation Corpus4With our implementation, we could process sequential inputs about 75 times faster than lattice inputs during training, and general fine-tuning convergence was 15 times faster. Decoding time was only slightly increased by a factor of 1.2 when using lattice inputs. (Post et al., 2013) This is a corpus of Spanish telephone calls that include automatic transcripts and grids. Fisher's share consists of phone calls between strangers, while the callhome part includes phone calls between relatives or friends. Training data are 138,819 sets (Fisher / Train), and 15,000 sets (Callhome / Train). Heldout test data are shown in Table 2. ASR word error rates (WHO) are pre high, due to the spontaneous speaking style and challenging acoustic conditions. We are puntice contain on average 3.4 (Fisher / Train) or 4.5 (Callhome / Train times) more than corresponding their reference."}, {"heading": "5.2 Main Results", "text": "We compare 4 systems: Performing pre-training on the sequential reference transcripts only (R), fine-tuning on 1-best transcripts (R + 1), fine-tuning on lattices without scores (R + L), and fine-tuning on lattices including lattice scores (R + L + S). At test date, we are trying references, grid oracles, 6 1-best transcripts, and lattices as inputs to all 4 systems. The former 2 experiments give upper limits to achievable translation accuracy, while the latter 2 correspond to a realistic setting. Table 3 shows the results on Fisher / Dev2 and Fisher / Test.Before even considering lattices, we can see that 1-best-tuning boosted BLEU scores are quite impressive (1-best / R vs. 1-best / R + 1), with gains of 1.3 and 0.7 BLEU points. This is in contrast to Post et al (2013), who finds the 1-best scripts here to be unhelpful to translate."}, {"heading": "5.3 Ablation Experiments", "text": "Next, we conduct an ablation study to evaluate the impact of the three proposed extensions on the integration of grid values (Section 4.2). We train models with different peakiness coefficients S by either ignoring grid values by fixing S = 0, by using grid values as they are, or by optimizing S during training. Table 4 shows the results. Overall, the joint training of S delivers similar results to fixing S = 1, but both clearly exceed the fixation of S = 0. Removing confidence (setting S = 0) in one place at a time shows that the attention mechanism is clearly the most important point of integration, while the gains from integration into child sum and forget gates are smaller and not always consistent. We also analyzed what peakiness values were actually learned. We found that all three models we trained for average purposes converged to Sa = 0.62."}, {"heading": "5.4 Callhome Experiments", "text": "In this experiment, we are testing a situation where we have a reasonable amount of sequential data available for pre-training, but only a limited amount of grid training data for the fine-tuning step. This could be a more realistic situation, as language translation corpora are scarce. To investigate this scenario, we train our mods again on Fisher / Train, and then adjust them to the 9 times smaller Callhome / Train portion of the body. We are tuning to 10 epochs, all other settings are as before. We are using Callhome / Evltest for testing. Table 5 shows the results. Trends agree with Section 5.2: The proposed model (Grid / R + L + S) exceeds the 1-best baseline (1-best / R + 1) by 0.8 BLEU points, which in turn exceeds the pre-trained system (1-best / R) by 1.5 BLEU points."}, {"heading": "5.5 Impact of Lattice Quality", "text": "Next, we analyze the effects of using grids and grating values in the course of ASR-WER changes. To this end, we combine all test data sets from Table 2 and divide the result into containers by the 1-best WER. We sample 1000 sets from each container and compare BLEU values between several models. The results are in Figure 4. For very good WERs, grids do not improve compared to the 1-best inputs, which is not surprising. In all other cases, grid values are helpful. Grid values are most advantageous for moderate WERs and not advantageous for very high WERs. We speculate that grid values tend to be less reliable at high WERs than at lower WERs."}, {"heading": "6 Conclusion", "text": "We investigated the translation of insecure input from an error prone component upstream using a neural grid-to-sequence model. Our proposed model takes word grids as input and is able to use grid values. In our experiments in a language translation task, we find consistent improvements over the translation of 1-best transcriptions, and that the consideration of grid values, especially in the attention mechanism, is crucial for these improvements."}, {"heading": "Acknowledgments", "text": "We thank Paul Michel and Austin Matthews for their helpful comments on earlier drafts of this paper."}, {"heading": "A Appendix: Cherry-Picked Examples", "text": "It is not the first time that they are in a country where they live, but in a country where they live and work, where they live and work.... \"it is the first time that the world is changing...\" \"It is the second time that they are changing...\" \"It is the first time that they are changing...\" \"It is the second time that they are changing...\" \"It is the first time that they are changing...\" \"It is the second time that they are changing...\" \"It is the first time that they are changing.\" \"It is the third time that they are changing.\" \"It is the third time that they are changing...\" \"It is the second time that they are changing.\" \"It is the second time that they are changing.\" \"It is the third time that they are changing.\""}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Representation Learning (ICLR). San Diego, USA.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Some approaches to statistical and finite-state speech-to", "author": ["F. Casacuberta", "H. Ney", "F.J. Och", "E. Vidal", "J.M. Vilar", "S. Barrachina", "I. Garcia-Varea", "D. Llorens", "C. Martinez", "S. Molau", "F. Nevado", "M. Pastor", "D. Picco", "A. Sanchis", "C. Tillmann"], "venue": null, "citeRegEx": "Casacuberta et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Casacuberta et al\\.", "year": 2004}, {"title": "Learning Phrase Representations Using RNN EncoderDecoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Generalizing Word Lattice Translation", "author": ["Christopher Dyer", "Smaranda Muresan", "Philip Resnik."], "venue": "Technical Report LAMP-TR-149, University of Maryland, Institute For Advanced Computer Studies.", "citeRegEx": "Dyer et al\\.,? 2008", "shortCiteRegEx": "Dyer et al\\.", "year": 2008}, {"title": "Tree-to-Sequence Attentional Neural Machine Translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Association for Computational Linguistic (ACL). Berlin, Germany, pages 823\u2013833.", "citeRegEx": "Eriguchi et al\\.,? 2016", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "Line graphs and line digraphs", "author": ["R.L. Hemminger", "L.W. Beineke."], "venue": "Selected Topics in Graph Theory, Academic Press Inc., pages 271\u2013305.", "citeRegEx": "Hemminger and Beineke.,? 1978", "shortCiteRegEx": "Hemminger and Beineke.", "year": 1978}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "An Empirical Exploration of Recurrent Network Architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "International Conference on Machine Learning (ICML). Lille, France.", "citeRegEx": "Jozefowicz et al\\.,? 2015", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Recurrent Continuous Translation Models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). Seattle, Washington, USA, pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "LatticeRnn: Recurrent Neural Networks over Lattices", "author": ["Faisal Ladhak", "Ankur Gandhe", "Markus Dreyer", "Lambert Mathias", "Ariya Rastrow", "Bj\u00f6rn Hoffmeister."], "venue": "Annual Conference of the International Speech Communication Association (Inter-", "citeRegEx": "Ladhak et al\\.,? 2016", "shortCiteRegEx": "Ladhak et al\\.", "year": 2016}, {"title": "Effective Approaches to Attentionbased Neural Machine Translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). Lisbon, Portugal, pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "ASR word lattice translation with exhaustive reordering is possible", "author": ["Evgeny Matusov", "Bj\u00f6rn Hoffmeister", "Hermann Ney."], "venue": "Annual Conference of the International Speech Communication Association (InterSpeech). Brisbane, Australia, pages", "citeRegEx": "Matusov et al\\.,? 2008", "shortCiteRegEx": "Matusov et al\\.", "year": 2008}, {"title": "lamtram: A Toolkit for Language and Translation Modeling using Neural Networks", "author": ["Graham Neubig."], "venue": "http://www.github.com/neubig/lamtram.", "citeRegEx": "Neubig.,? 2015", "shortCiteRegEx": "Neubig.", "year": 2015}, {"title": "DyNet: The Dynamic Neural Network Toolkit", "author": ["Lingpeng Kong", "Adhiguna Kuncoro", "Gaurav Kumar", "Chaitanya Malaviya", "Paul Michel", "Yusuke Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin."], "venue": "arXiv", "citeRegEx": "Kong et al\\.,? 2017", "shortCiteRegEx": "Kong et al\\.", "year": 2017}, {"title": "Speech Translation: Coupling of Recognition and Translation", "author": ["Hermann Ney."], "venue": "International Conference on Acoustics, Speech, and Signal Processing (ICASSP). Phoenix, USA, pages 517\u2013520.", "citeRegEx": "Ney.,? 1999", "shortCiteRegEx": "Ney.", "year": 1999}, {"title": "Improved Speech-to-Text Translation with the Fisher and Callhome Spanish\u2013English Speech Translation Corpus", "author": ["Matt Post", "Gaurav Kumar", "Adam Lopez", "Damianos Karakos", "Chris Callison-Burch", "Sanjeev Khudanpur."], "venue": "International Work-", "citeRegEx": "Post et al\\.,? 2013", "shortCiteRegEx": "Post et al\\.", "year": 2013}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["Lawrence R. Rabiner."], "venue": "Proceedings of the IEEE .", "citeRegEx": "Rabiner.,? 1989", "shortCiteRegEx": "Rabiner.", "year": 1989}, {"title": "Using Word Lattice Information for a Tighter Coupling in Speech Translation Systems", "author": ["Shirin Saleem", "Szu-Chen (Stan) Jou", "Stephan Vogel", "Tanja Schultz"], "venue": "In International Conference of Spoken Language Processing (ICSLP). Jeju Island, Korea,", "citeRegEx": "Saleem et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Saleem et al\\.", "year": 2004}, {"title": "Lattice-Based Recurrent Neural Network Encoders for Neural Machine Translation", "author": ["Jinsong Su", "Zhixing Tan", "Deyi Xiong", "Yang Liu."], "venue": "arXiv preprint arXiv:1609.07730 .", "citeRegEx": "Su et al\\.,? 2016", "shortCiteRegEx": "Su et al\\.", "year": 2016}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems (NIPS). Montr\u00e9al, Canada, pages 3104\u2013 3112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Association for Computational Linguistic (ACL). Beijing, China, pages 1556\u20131566.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "A Decoding Algorithm for Word Lattice Translation in Speech Translation", "author": ["Ruiqiang Zhang", "Genichiro Kikui", "Hirofumi Yamamoto", "Wai-Kit Lo."], "venue": "International Workshop on Spoken Language Translation (IWSLT). Pittsburgh, USA, pages", "citeRegEx": "Zhang et al\\.,? 2005", "shortCiteRegEx": "Zhang et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 21, "context": "In this work, we extend the TreeLSTM (Tai et al., 2015) into a LatticeLSTM that is able to consume word lattices, and can be used as encoder in an attentional encoderdecoder model.", "startOffset": 37, "endOffset": 55}, {"referenceID": 15, "context": "(Ney, 1999; Casacuberta et al., 2004).", "startOffset": 0, "endOffset": 37}, {"referenceID": 1, "context": "(Ney, 1999; Casacuberta et al., 2004).", "startOffset": 0, "endOffset": 37}, {"referenceID": 18, "context": "Examples include translating lattice representations of ASR output (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), multiple word segmentations, and morphological alternatives (Dyer et al.", "startOffset": 67, "endOffset": 130}, {"referenceID": 22, "context": "Examples include translating lattice representations of ASR output (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), multiple word segmentations, and morphological alternatives (Dyer et al.", "startOffset": 67, "endOffset": 130}, {"referenceID": 12, "context": "Examples include translating lattice representations of ASR output (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), multiple word segmentations, and morphological alternatives (Dyer et al.", "startOffset": 67, "endOffset": 130}, {"referenceID": 3, "context": ", 2008), multiple word segmentations, and morphological alternatives (Dyer et al., 2008).", "startOffset": 69, "endOffset": 88}, {"referenceID": 8, "context": "Recently, neural sequence-to-sequence (seq2seq) models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) have often been preferred over the traditional methods for their strong empirical results and appealing end-to-end modeling.", "startOffset": 55, "endOffset": 134}, {"referenceID": 20, "context": "Recently, neural sequence-to-sequence (seq2seq) models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) have often been preferred over the traditional methods for their strong empirical results and appealing end-to-end modeling.", "startOffset": 55, "endOffset": 134}, {"referenceID": 0, "context": "Recently, neural sequence-to-sequence (seq2seq) models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) have often been preferred over the traditional methods for their strong empirical results and appealing end-to-end modeling.", "startOffset": 55, "endOffset": 134}, {"referenceID": 2, "context": "This is achieved by extending the encoder\u2019s Gated Recurrent Units (GRUs) (Cho et al., 2014) to be conditioned on multiple predecessor paths.", "startOffset": 73, "endOffset": 91}, {"referenceID": 18, "context": "As a remedy, Su et al. (2016) proposed replacing the sequential encoder by a lattice encoder to obtain a lattice-to-sequence (lat2seq) model.", "startOffset": 13, "endOffset": 30}, {"referenceID": 21, "context": "\u2022 We employ the popular child-sum TreeLSTM (Tai et al., 2015) to derive a lattice encoder that replaces the sequential encoder in an attentional encoder-decoder model.", "startOffset": 43, "endOffset": 61}, {"referenceID": 18, "context": "This finding stands in contrast to the positive results by Su et al. (2016), as well as by Ladhak et al.", "startOffset": 59, "endOffset": 76}, {"referenceID": 10, "context": "(2016), as well as by Ladhak et al. (2016) on a lattice classification task, and suggests higher learning complexity of our speech translation task.", "startOffset": 22, "endOffset": 43}, {"referenceID": 10, "context": "This is reminiscent of the weighted pooling strategy by Ladhak et al. (2016) for spoken utterance classification.", "startOffset": 56, "endOffset": 77}, {"referenceID": 16, "context": "We conduct experiments on the Fisher and Callhome Spanish\u2013English Speech Translation Corpus (Post et al., 2013) and report improvements of 1.", "startOffset": 92, "endOffset": 111}, {"referenceID": 8, "context": "Our work extends the seminal work on attentional encoder-decoder models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) which we survey in this section.", "startOffset": 72, "endOffset": 151}, {"referenceID": 20, "context": "Our work extends the seminal work on attentional encoder-decoder models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) which we survey in this section.", "startOffset": 72, "endOffset": 151}, {"referenceID": 0, "context": "Our work extends the seminal work on attentional encoder-decoder models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) which we survey in this section.", "startOffset": 72, "endOffset": 151}, {"referenceID": 0, "context": "In our baseline model, the encoder is a bidirectional recurrent neural network (RNN), following (Bahdanau et al., 2015).", "startOffset": 96, "endOffset": 119}, {"referenceID": 6, "context": "We opt for long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997)", "startOffset": 41, "endOffset": 75}, {"referenceID": 21, "context": "recurrent units because of their high performance and in order to later take advantage of the TreeLSTM extension (Tai et al., 2015).", "startOffset": 113, "endOffset": 131}, {"referenceID": 11, "context": "We use an attention mechanism (Luong et al., 2015) to summarize the encoder outputs into a fixed-size representation.", "startOffset": 30, "endOffset": 50}, {"referenceID": 16, "context": "by an ASR dumping its pruned search space (Post et al., 2013), or can be obtained by merging several nbest sequences (Dyer et al.", "startOffset": 42, "endOffset": 61}, {"referenceID": 3, "context": ", 2013), or can be obtained by merging several nbest sequences (Dyer et al., 2008; Su et al., 2016).", "startOffset": 63, "endOffset": 99}, {"referenceID": 19, "context": ", 2013), or can be obtained by merging several nbest sequences (Dyer et al., 2008; Su et al., 2016).", "startOffset": 63, "endOffset": 99}, {"referenceID": 21, "context": "Luckily Tai et al. (2015)\u2019s TreeLSTM, which was designed to compose encodings in trees, is also straightforward to apply to lattices; the TreeLSTM composes multiple child states into a parent state, which can also be applied to lattices to compose multiple predecessor states into a successor state.", "startOffset": 8, "endOffset": 26}, {"referenceID": 10, "context": "At this point we take a step back to motivate our choice of assigning word labels to lattice nodes, which is in contrast to the prior work by Ladhak et al. (2016) and Su et al.", "startOffset": 142, "endOffset": 163}, {"referenceID": 10, "context": "At this point we take a step back to motivate our choice of assigning word labels to lattice nodes, which is in contrast to the prior work by Ladhak et al. (2016) and Su et al. (2016) who assign word labels to edges.", "startOffset": 142, "endOffset": 184}, {"referenceID": 4, "context": "This is similar in spirit to Eriguchi et al. (2016) who used the TreeLSTM in an attentional tree-to-sequence model.", "startOffset": 29, "endOffset": 52}, {"referenceID": 5, "context": "We also note that it is easy to convert an edge-labeled lattice into a node-labeled lattice using the line-graph algorithm (Hemminger and Beineke, 1978), which we utilize in this work.", "startOffset": 123, "endOffset": 152}, {"referenceID": 21, "context": "Table 1: Formulas for sequential and TreeLSTM encoders according to Tai et al. (2015), the proposed LatticeLSTM encoder, and conventional vs.", "startOffset": 68, "endOffset": 86}, {"referenceID": 17, "context": "by using the forward algorithm (Rabiner, 1989).", "startOffset": 31, "endOffset": 46}, {"referenceID": 16, "context": "(Post et al., 2013).", "startOffset": 0, "endOffset": 19}, {"referenceID": 13, "context": "Our implementation is based on lamtram (Neubig, 2015) and DyNet (Neubig et al.", "startOffset": 39, "endOffset": 53}, {"referenceID": 7, "context": "Forget gates were initialized to 1, following Jozefowicz et al. (2015).", "startOffset": 46, "endOffset": 71}, {"referenceID": 9, "context": "We used Adam (Kingma and Ba, 2014) for training, with an empirically determined initial learning rate of 0.", "startOffset": 13, "endOffset": 34}, {"referenceID": 16, "context": "This stands in contrast to Post et al. (2013) who find the 1-best transcripts not to be helpful for training a hierarchical machine translation system.", "startOffset": 27, "endOffset": 46}, {"referenceID": 16, "context": "This stands in contrast to Post et al. (2013) who find the 1-best transcripts not to be helpful for training a hierarchical machine translation system. Possible explanations are learning from repeating error patterns, and improved robustness to erroneous inputs. On top of these gains, our proposed set-up (lattice/R+L+S) improve BLEU scores by another 1.4. Removing the lattice scores (lattice/R+L) diminishes the results and performs worse than the 1-best baseline (1-best/R+1), indicating that the proposed lattice score integration is crucial for good performance. This demonstrates a clear advantage of our proposed method over that of Su et al. (2016).", "startOffset": 27, "endOffset": 658}], "year": 2017, "abstractText": "The input to a neural sequence-tosequence model is often determined by an up-stream system, e.g. a word segmenter, part of speech tagger, or speech recognizer. These up-stream models are potentially error-prone. Representing inputs through word lattices allows making this uncertainty explicit by capturing alternative sequences and their posterior probabilities in a compact form. In this work, we extend the TreeLSTM (Tai et al., 2015) into a LatticeLSTM that is able to consume word lattices, and can be used as encoder in an attentional encoderdecoder model. We integrate lattice posterior scores into this architecture by extending the TreeLSTM\u2019s child-sum and forget gates and introducing a bias term into the attention mechanism. We experiment with speech translation lattices and report consistent improvements over baselines that translate either the 1-best hypothesis or the lattice without posterior scores.", "creator": "LaTeX with hyperref package"}}}