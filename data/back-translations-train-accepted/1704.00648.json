{"id": "1704.00648", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations", "abstract": "In this work we present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives state-of-the-art results for both.", "histories": [["v1", "Mon, 3 Apr 2017 15:39:56 GMT  (5688kb,D)", "http://arxiv.org/abs/1704.00648v1", "Supplementary visual examples available at:this http URL"], ["v2", "Thu, 8 Jun 2017 09:18:22 GMT  (1366kb,D)", "http://arxiv.org/abs/1704.00648v2", null]], "COMMENTS": "Supplementary visual examples available at:this http URL", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["eirikur agustsson", "fabian mentzer", "michael tschannen", "lukas cavigelli", "radu timofte", "luca benini", "luc van gool"], "accepted": true, "id": "1704.00648"}, "pdf": {"name": "1704.00648.pdf", "metadata": {"source": "META", "title": "Soft-to-Hard Vector Quantization for End-to-End Learned Compression of Images and Neural Networks", "authors": ["Eirikur Agustsson", "Fabian Mentzer", "Michael Tschannen", "Lukas Cavigelli", "Radu Timofte", "Luca Benini", "Luc Van Gool"], "emails": ["<aeirikur@vision.ee.ethz.ch>."], "sections": [{"heading": "1. Introduction", "text": "In recent years, various neural networks (DNNs) have led to many groundbreaking results in machine learning and computer vision (Krizhevsky et al., 2012; Silver et al., 2016; Esteva et al., 2017), and are now widely used in industry. However, modern DNN models often have millions or tens of millions of parameters that lead to highly redundant structures, both in the intermedia functional representations they generate and in the model itself. Although the over-parameterization of DNN models can have a beneficial effect on education, it is often desirable to compress DNN models in practice, e.g. when used on mobile or embedded devices with limited memory. On the other hand, the ability to learn compressible features has great potential for developing (data-adaptive) compression algorithms for different data types such as images, audio, video, and text, for all of which different DNN architectures are now available."}, {"heading": "2. Related Work", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "3. Proposed Soft-to-Hard Vector Quantization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Problem Formulation", "text": "We consider the standard model for DNNs in which we have an architecture F: Rd1 7 / RdK + 1 / 2 / 2 / 2 / 2 / 2 / 3 / 3 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / / 4 / / 4 / 4 / / 4 / / 5 / / / / 4 / / / 4 / / / / 4 / / 5 / / / 4 / / / / 4 / / 4 / / / 4 / / / 4 / / / 4 / / / / 4 / / / / / / 4 / / 4 / / 4 / / 4 / / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4"}, {"heading": "3.2. Our Method", "text": "The encoders E: [L] m: [L] m: [L] m: [L] m: [L] m: [L] s: [L] s: [L] s: [L] s: [L] s: [L] s: [L] s: [L) s: [L) s: [L) s: [L) s: [L] s: [S) s: [L] s: [L] s: [L] s: [L] s: [L) s: [L) s: [L) s: [L) s: [S) s: [S) s: [S) s: [S) s: S) s: S) s: S) s: S (S) s: S) s (S) s: S (S) s: S (S) s: S (S) s: S (S) s: S (S) s (S (S) s: S (S) s (S (S) s: S (S) s (S (S) s: S (S (S) s (S (S) s: S (S) s (S (S) s: S (S (S) s) s (S (S (S) s) s: S (S (S (S) s) s (S (S (S) s) s: S (S (S (S (S) s) s: S (S (S (S (S) s) s) s (S (S (S (S (S) s) s) s: S (S (S (S (S (S (S (S) s) s) s) s) s: S (S (S (S (S (S (S (S) s) s) s) s) s: S (S (S (S (S (S (S (S) s) s) s) s: S (S (S (S (S (S (S) s) s) s) s: S (S (S (S (S (S (S) s) s) s (S (S (S (S (S (S) s) s) s) s) s (S (S (S (S (S (S (S ("}, {"heading": "4. Image Compression", "text": "We will show how we can use our framework to realize an image compression system."}, {"heading": "4.1. Experimental setup", "text": "To evaluate the image compression performance of our Soft-to-Hard Autoencoder (SHA) method, we use four datasets (Kodak, B100, Urban100, ImageNet100) and three standard quality scales (PSNR, SSIM, MS-SSIM).Kodak 6 is the most widely used dataset for analyzing image compression performance in recent years. It contains 24 photographic quality images covering a variety of subjects, locations and lighting conditions. B100 (Timofte et al., 2015) is a set of 100 content-diversified color 481 x 321 test images from the Berkeley segmentation dataset (Martin et al., 2001).Urban100 (Huang et al., 2015) has 100 color images selected from Flickr labels such as urban, city, architecture, and structure. The images are a few times larger than the images from B100 or 100 Urban100."}, {"heading": "4.2. Results", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to hide ourselves, and that we are able, that we will be able to hide ourselves, \"he said."}, {"heading": "4.3. Detailed Discussion of with Related Methods", "text": "We point out that the recent work of (Theis et al., 2017; Balle \u0301 et al., 2016b) also provides a competitive performance with JPEG2000.While we use the architecture of (Theis et al., 2017), there are strong differences between the works: For quantization, we perform vector quantization techniques while rounding on integers; for differentiation, we use our soft relaxation of quantization, while Theis et al. redefine the gradients in reverse as the gradients of identity mapping; for minimizing entropy, they use a Gaussian blending model that promotes thrift of coefficients while directly minimizing a soft relativization of sample entropy. Thus, our end-to-end histograms can be directly incorporated into arithmetic coding during training, whereas Theis et al."}, {"heading": "5. DNN Compression", "text": "We take the same setting as (Choi et al., 2016) and look at a 32-layer architecture developed for the CIFAR-10 dataset (Krizhevsky & Hinton, 2009). As in (Choi et al., 2016), our goal is to learn a compressible representation for all 464,154 trainable parameters of the model. We link the parameters into a vector W, R464,154 and set z = W. Our goal is then to minimize the target in (7). For a direct comparison with (Choi et al., 2016), we limit ourselves to a scalar quantization. That is, we set m = d so that E (z) is a symbol vector of length with indices taking value in [L]."}, {"heading": "6. Conclusions", "text": "In this paper, we proposed a unified framework for end-to-end learning of compressed representations for deep architectures. By training with a soft to hard annealing scheme that progressively shifts from a gentle relaxation of the sample entropy and network discretization process to the actual, non-differentiated quantization process, we are able to optimize the distortion trade-off between the original network loss and entropy. Our framework can elegantly capture a variety of compression tasks, achieving state-of-the-art results for both image compression and DNN compression. The simplicity of our approach opens up different directions for future work as our framework can easily be adapted to other architectures where a compressed representation is desired."}], "references": [{"title": "Numerical continuation methods: an introduction, volume 13", "author": ["Allgower", "Eugene L", "Georg", "Kurt"], "venue": "Springer Science & Business Media,", "citeRegEx": "Allgower et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Allgower et al\\.", "year": 2012}, {"title": "End-to-end optimization of nonlinear transform codes for perceptual quality", "author": ["Ball\u00e9", "Johannes", "Laparra", "Valero", "Simoncelli", "Eero P"], "venue": "arXiv preprint arXiv:1607.05006,", "citeRegEx": "Ball\u00e9 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ball\u00e9 et al\\.", "year": 2016}, {"title": "End-to-end optimized image compression", "author": ["Ball\u00e9", "Johannes", "Laparra", "Valero", "Simoncelli", "Eero P"], "venue": "arXiv preprint arXiv:1611.01704,", "citeRegEx": "Ball\u00e9 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ball\u00e9 et al\\.", "year": 2016}, {"title": "Towards the limit of network quantization", "author": ["Choi", "Yoojin", "El-Khamy", "Mostafa", "Lee", "Jungwon"], "venue": "arXiv preprint arXiv:1612.01543,", "citeRegEx": "Choi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2016}, {"title": "JeanPierre. Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Courbariaux", "Matthieu", "Bengio", "Yoshua", "David"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Elements of information theory", "author": ["Cover", "Thomas M", "Thomas", "Joy A"], "venue": null, "citeRegEx": "Cover et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cover et al\\.", "year": 2012}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. FeiFei"], "venue": "In CVPR09,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Dermatologist-level classification of skin cancer", "author": ["Esteva", "Andre", "Kuprel", "Brett", "Novoa", "Roberto A", "Ko", "Justin", "Swetter", "Susan M", "Blau", "Helen M", "Thrun", "Sebastian"], "venue": null, "citeRegEx": "Esteva et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Esteva et al\\.", "year": 2017}, {"title": "Dynamic network surgery for efficient dnns", "author": ["Guo", "Yiwen", "Yao", "Anbang", "Chen", "Yurong"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Guo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Han", "Song", "Pool", "Jeff", "Tran", "John", "Dally", "William"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Single image super-resolution from transformed selfexemplars", "author": ["Huang", "Jia-Bin", "Singh", "Abhishek", "Ahuja", "Narendra"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Quantized neural networks: Training neural networks with low precision weights and activations", "author": ["Hubara", "Itay", "Courbariaux", "Matthieu", "Soudry", "Daniel", "ElYaniv", "Ran", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1609.07061,", "citeRegEx": "Hubara et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hubara et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Using very deep autoencoders for content-based image retrieval", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey E"], "venue": "In ESANN,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "nuation-based learning algorithm for discrete-time cellular neural networks", "author": ["Magnussen", "Holger", "Papoutsis", "Georgios", "Nossek", "Josef A. Conti"], "venue": "In Cellular Neural Networks and their Applications,", "citeRegEx": "Magnussen et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Magnussen et al\\.", "year": 1994}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["D. Martin", "C. Fowlkes", "D. Tal", "J. Malik"], "venue": "In Proc. Int\u2019l Conf. Computer Vision,", "citeRegEx": "Martin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Martin et al\\.", "year": 2001}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["Rastegari", "Mohammad", "Ordonez", "Vicente", "Redmon", "Joseph", "Farhadi", "Ali"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Vector quantization by deterministic annealing", "author": ["Rose", "Kenneth", "Gurewitz", "Eitan", "Fox", "Geoffrey C"], "venue": "IEEE Transactions on Information theory,", "citeRegEx": "Rose et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Rose et al\\.", "year": 1992}, {"title": "Is the deconvolution layer the same as a convolutional layer", "author": ["Shi", "Wenzhe", "Caballero", "Jose", "Theis", "Lucas", "Huszar", "Ferenc", "Aitken", "Andrew", "Ledig", "Christian", "Wang", "Zehan"], "venue": "arXiv preprint arXiv:1609.07009,", "citeRegEx": "Shi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "Lossy image compression with compressive autoencoders", "author": ["Theis", "Lucas", "Shi", "Wenzhe", "Cunningham", "Andrew", "Huszar", "Ferenc"], "venue": "In ICLR", "citeRegEx": "Theis et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2017}, {"title": "A+: Adjusted Anchored Neighborhood Regression for Fast Super-Resolution, pp. 111\u2013126", "author": ["Timofte", "Radu", "De Smet", "Vincent", "Van Gool", "Luc"], "venue": null, "citeRegEx": "Timofte et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Timofte et al\\.", "year": 2015}, {"title": "Variable rate image compression with recurrent neural networks", "author": ["Toderici", "George", "O\u2019Malley", "Sean M", "Hwang", "Sung Jin", "Vincent", "Damien", "Minnen", "David", "Baluja", "Shumeet", "Covell", "Michele", "Sukthankar", "Rahul"], "venue": "arXiv preprint arXiv:1511.06085,", "citeRegEx": "Toderici et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Toderici et al\\.", "year": 2015}, {"title": "Full resolution image compression with recurrent neural networks", "author": ["Toderici", "George", "Vincent", "Damien", "Johnston", "Nick", "Hwang", "Sung Jin", "Minnen", "David", "Shor", "Joel", "Covell", "Michele"], "venue": "arXiv preprint arXiv:1608.05148,", "citeRegEx": "Toderici et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Toderici et al\\.", "year": 2016}, {"title": "Soft weight-sharing for neural network compression", "author": ["Ullrich", "Karen", "Meeds", "Edward", "Welling", "Max"], "venue": "arXiv preprint arXiv:1702.04008,", "citeRegEx": "Ullrich et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ullrich et al\\.", "year": 2017}, {"title": "Multiscale structural similarity for image quality assessment", "author": ["Z. Wang", "E.P. Simoncelli", "A.C. Bovik"], "venue": "In Asilomar Conference on Signals, Systems Computers,", "citeRegEx": "Wang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2003}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Wang", "Zhou", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}, {"title": "Learning structured sparsity in deep neural networks", "author": ["Wen", "Wei", "Wu", "Chunpeng", "Wang", "Yandan", "Chen", "Yiran", "Li", "Hai"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Arithmetic coding for data compression", "author": ["Witten", "Ian H", "Neal", "Radford M", "Cleary", "John G"], "venue": "Commun. ACM,", "citeRegEx": "Witten et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Witten et al\\.", "year": 1987}, {"title": "Optimizing 1nearest prototype classifiers", "author": ["Wohlhart", "Paul", "Kostinger", "Martin", "Donoser", "Michael", "Roth", "Peter M", "Bischof", "Horst"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Wohlhart et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wohlhart et al\\.", "year": 2013}, {"title": "Competitive learning and soft competition for vector quantizer design", "author": ["Yair", "Eyal", "Zeger", "Kenneth", "Gersho", "Allen"], "venue": "IEEE transactions on Signal Processing,", "citeRegEx": "Yair et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Yair et al\\.", "year": 1992}, {"title": "Incremental network quantization: Towards lossless cnns with low-precision weights", "author": ["Zhou", "Aojun", "Yao", "Anbang", "Guo", "Yiwen", "Xu", "Lin", "Chen", "Yurong"], "venue": "arXiv preprint arXiv:1702.03044,", "citeRegEx": "Zhou et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 18, "context": "In recent years, deep neural networks (DNNs) have led to many breakthrough results in machine learning and computer vision (Krizhevsky et al., 2012; Silver et al., 2016; Esteva et al., 2017), and are now widely deployed in industry.", "startOffset": 123, "endOffset": 190}, {"referenceID": 7, "context": "In recent years, deep neural networks (DNNs) have led to many breakthrough results in machine learning and computer vision (Krizhevsky et al., 2012; Silver et al., 2016; Esteva et al., 2017), and are now widely deployed in industry.", "startOffset": 123, "endOffset": 190}, {"referenceID": 4, "context": "Among the most popular ones are stochastic approximations (Williams, 1992; Krizhevsky & Hinton, 2011; Courbariaux et al., 2015; Toderici et al., 2015; Ball\u00e9 et al., 2016b) and rounding with a smooth derivative approximation (Hubara et al.", "startOffset": 58, "endOffset": 171}, {"referenceID": 26, "context": "Among the most popular ones are stochastic approximations (Williams, 1992; Krizhevsky & Hinton, 2011; Courbariaux et al., 2015; Toderici et al., 2015; Ball\u00e9 et al., 2016b) and rounding with a smooth derivative approximation (Hubara et al.", "startOffset": 58, "endOffset": 171}, {"referenceID": 13, "context": ", 2016b) and rounding with a smooth derivative approximation (Hubara et al., 2016; Theis et al., 2017).", "startOffset": 61, "endOffset": 102}, {"referenceID": 24, "context": ", 2016b) and rounding with a smooth derivative approximation (Hubara et al., 2016; Theis et al., 2017).", "startOffset": 61, "endOffset": 102}, {"referenceID": 24, "context": "and to model the marginal symbol distribution with a parametric model, such as a Gaussian mixture model (Theis et al., 2017; Ullrich et al., 2017), a piecewise linear model (Ball\u00e9 et al.", "startOffset": 104, "endOffset": 146}, {"referenceID": 28, "context": "and to model the marginal symbol distribution with a parametric model, such as a Gaussian mixture model (Theis et al., 2017; Ullrich et al., 2017), a piecewise linear model (Ball\u00e9 et al.", "startOffset": 104, "endOffset": 146}, {"referenceID": 27, "context": ", 2016b), or a Bernoulli distribution (Toderici et al., 2016) (in the case of binary symbols).", "startOffset": 38, "endOffset": 61}, {"referenceID": 11, "context": "\u2022 We apply our method to DNN model compression for a 32-layer ResNet model (He et al., 2016) and fullresolution image compression using a variant of the compressive autoencoder proposed recently in (Theis et al.", "startOffset": 75, "endOffset": 92}, {"referenceID": 24, "context": ", 2016) and fullresolution image compression using a variant of the compressive autoencoder proposed recently in (Theis et al., 2017) (see Figure 1 for an overview).", "startOffset": 113, "endOffset": 133}, {"referenceID": 24, "context": "In both cases, we obtain state-of-the-art performance, while making fewer model assumptions and significantly simplifying the training procedure compared to the original works (Theis et al., 2017; Choi et al., 2016).", "startOffset": 176, "endOffset": 215}, {"referenceID": 3, "context": "In both cases, we obtain state-of-the-art performance, while making fewer model assumptions and significantly simplifying the training procedure compared to the original works (Theis et al., 2017; Choi et al., 2016).", "startOffset": 176, "endOffset": 215}, {"referenceID": 26, "context": "There has been a surge of interest in DNN models for full-resolution image compression, most notably (Toderici et al., 2015; 2016; Ball\u00e9 et al., 2016a;b; Theis et al., 2017), all of which outperform JPEG and some even JPEG2000.", "startOffset": 101, "endOffset": 173}, {"referenceID": 24, "context": "There has been a surge of interest in DNN models for full-resolution image compression, most notably (Toderici et al., 2015; 2016; Ball\u00e9 et al., 2016a;b; Theis et al., 2017), all of which outperform JPEG and some even JPEG2000.", "startOffset": 101, "endOffset": 173}, {"referenceID": 3, "context": "In the context of DNN model compression, the line of works (Han et al., 2015b;a; Choi et al., 2016) adopts a multi-step procedure in which the weights of a pretrained DNN are first pruned and the remaining parameters are quantized using a k-means like algorithm, the DNN is then retrained, and finally the quantized DNN model is encoded using arithmetic coding.", "startOffset": 59, "endOffset": 99}, {"referenceID": 8, "context": "To overcome this issue, (Guo et al., 2016) propose a dynamic network surgery technique that allows to reactivate pruned weights.", "startOffset": 24, "endOffset": 42}, {"referenceID": 28, "context": "A notable different approach is taken by (Ullrich et al., 2017), where the DNN compression task is tackled using the minimum description length principle, which has a solid information-theoretic foundation.", "startOffset": 41, "endOffset": 63}, {"referenceID": 13, "context": ", (Hubara et al., 2016; Rastegari et al., 2016; Wen et al., 2016; Zhou et al., 2017).", "startOffset": 2, "endOffset": 84}, {"referenceID": 21, "context": ", (Hubara et al., 2016; Rastegari et al., 2016; Wen et al., 2016; Zhou et al., 2017).", "startOffset": 2, "endOffset": 84}, {"referenceID": 31, "context": ", (Hubara et al., 2016; Rastegari et al., 2016; Wen et al., 2016; Zhou et al., 2017).", "startOffset": 2, "endOffset": 84}, {"referenceID": 36, "context": ", (Hubara et al., 2016; Rastegari et al., 2016; Wen et al., 2016; Zhou et al., 2017).", "startOffset": 2, "endOffset": 84}, {"referenceID": 16, "context": "(2016a); Theis et al. (2017) both rely on convolutional autoencoder architectures.", "startOffset": 9, "endOffset": 29}, {"referenceID": 1, "context": "Ball\u00e9 et al. (2016a) model the quantization error by uniform additive noise and incorporate generalized divisive normalization (GDN) into their architecture to reduce higher-order statistical dependencies among the features to be compressed.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Ball\u00e9 et al. (2016a) model the quantization error by uniform additive noise and incorporate generalized divisive normalization (GDN) into their architecture to reduce higher-order statistical dependencies among the features to be compressed. Their optimization framework has a close relation to variational autoencoders (Kingma & Welling, 2013). In contrast, Theis et al. (2017) quantize the representation in the bottleneck of the autoencoder to integer values, employing the identity mapping for gradient backpropagation.", "startOffset": 0, "endOffset": 379}, {"referenceID": 22, "context": ", (Rose et al., 1992; Yair et al., 1992).", "startOffset": 2, "endOffset": 40}, {"referenceID": 35, "context": ", (Rose et al., 1992; Yair et al., 1992).", "startOffset": 2, "endOffset": 40}, {"referenceID": 34, "context": "Arguably most related to our approach is (Wohlhart et al., 2013), which also employs continuation for nearest neighbor assignments, but in the context of learning a supervised prototype classifier.", "startOffset": 41, "endOffset": 64}, {"referenceID": 19, "context": "In the context of neural networks, Magnussen et al. (1994) optimize the parameters of a discrete-time cellular neural network by gradually moving from a sigmoid to a sign function.", "startOffset": 35, "endOffset": 59}, {"referenceID": 33, "context": ", arithmetic coding (Witten et al., 1987).", "startOffset": 20, "endOffset": 41}, {"referenceID": 24, "context": "In terms of network architecture, we rely on a variant of the compressive autoencoder proposed recently in (Theis et al., 2017), using convolutional neural networks for the image encoder and image decoder3.", "startOffset": 107, "endOffset": 127}, {"referenceID": 24, "context": ", 2016a;b) used in (Theis et al., 2017), we use standard deconvolutions for simplicity.", "startOffset": 19, "endOffset": 39}, {"referenceID": 24, "context": "We note that while we use the architecture of (Theis et al., 2017), we train it using our soft-to-hard entropy minimization method, which differs significantly from the approach in (Theis et al.", "startOffset": 46, "endOffset": 66}, {"referenceID": 24, "context": ", 2017), we train it using our soft-to-hard entropy minimization method, which differs significantly from the approach in (Theis et al., 2017).", "startOffset": 122, "endOffset": 142}, {"referenceID": 24, "context": "(Theis et al., 2017) use 64 and 96 channels.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "We used a subset of 90,000 images from ImageNet (Deng et al., 2009), which we downsampled by a factor 0.", "startOffset": 48, "endOffset": 67}, {"referenceID": 24, "context": "Similar to (Theis et al., 2017) we gradually unfreeze the channels in the bottleneck during training.", "startOffset": 11, "endOffset": 31}, {"referenceID": 25, "context": "B100 (Timofte et al., 2015) is a set of 100 content diverse color 481\u00d7321 test images from the Berkeley Segmentation Dataset (Martin et al.", "startOffset": 5, "endOffset": 27}, {"referenceID": 20, "context": ", 2015) is a set of 100 content diverse color 481\u00d7321 test images from the Berkeley Segmentation Dataset (Martin et al., 2001).", "startOffset": 105, "endOffset": 126}, {"referenceID": 12, "context": "Urban100 (Huang et al., 2015) has 100 color images selected from Flickr with labels such as urban, city, architecture, and structure.", "startOffset": 9, "endOffset": 29}, {"referenceID": 30, "context": "SSIM and MSSSIM are the structural similarity index (Wang et al., 2004) and its multi-scale SSIM computed variant (Wang et al.", "startOffset": 52, "endOffset": 71}, {"referenceID": 29, "context": ", 2004) and its multi-scale SSIM computed variant (Wang et al., 2003) proposed to measure the similarity of two images.", "startOffset": 50, "endOffset": 69}, {"referenceID": 24, "context": "Detailed Discussion of with Related Methods We note that recent works of (Theis et al., 2017; Ball\u00e9 et al., 2016b) also showed competitive performance with JPEG2000.", "startOffset": 73, "endOffset": 114}, {"referenceID": 24, "context": "While we use the architecture of (Theis et al., 2017), there are stark differences between the works: For quantization, we perform vector quantization while they perform rounding to integers.", "startOffset": 33, "endOffset": 53}, {"referenceID": 6, "context": "As training material, we restrict ourselves to a subset of the ImageNet database (Deng et al., 2009), while Theis et al collect a set of high quality photos from Flickr.", "startOffset": 81, "endOffset": 100}, {"referenceID": 24, "context": "Since neither raw bpp/PSNR/SSIM numbers nor codes are available and training procedures and architectures differ significantly, a direct comparison with (Theis et al., 2017; Ball\u00e9 et al., 2016b) is hard.", "startOffset": 153, "endOffset": 194}, {"referenceID": 27, "context": ", 2017) builds on the architecture proposed in (Toderici et al., 2016), and shows that impressive performance on the perceptual MS-SSIM metric (Wang et al.", "startOffset": 47, "endOffset": 70}, {"referenceID": 29, "context": ", 2016), and shows that impressive performance on the perceptual MS-SSIM metric (Wang et al., 2003) can be obtained by incorporating it into the optimization (and using sophisticated coding techniques).", "startOffset": 80, "endOffset": 99}, {"referenceID": 24, "context": "In contrast, our work adheres to the setting of (Theis et al., 2017; Ball\u00e9 et al., 2016b; Toderici et al., 2016; 2015), simply optimizing for mean squared error, while evaluating using PSNR, SSIM and MS-SSIM.", "startOffset": 48, "endOffset": 118}, {"referenceID": 27, "context": "In contrast, our work adheres to the setting of (Theis et al., 2017; Ball\u00e9 et al., 2016b; Toderici et al., 2016; 2015), simply optimizing for mean squared error, while evaluating using PSNR, SSIM and MS-SSIM.", "startOffset": 48, "endOffset": 118}, {"referenceID": 3, "context": "The pruning based results are from (Choi et al., 2016).", "startOffset": 35, "endOffset": 54}, {"referenceID": 11, "context": "For DNN compression, we investigate the ResNet (He et al., 2016) architecture for image classification.", "startOffset": 47, "endOffset": 64}, {"referenceID": 3, "context": "We adopt the same setting as (Choi et al., 2016) and consider a 32-layer architecture trained for the CIFAR-10 dataset (Krizhevsky & Hinton, 2009).", "startOffset": 29, "endOffset": 48}, {"referenceID": 3, "context": "As in (Choi et al., 2016), our goal is to learn a compressible representation for all 464,154 trainable parameters of the model.", "startOffset": 6, "endOffset": 25}, {"referenceID": 3, "context": "For a straightforward comparison with (Choi et al., 2016), we constrain ourselves to scalar quantization.", "startOffset": 38, "endOffset": 57}, {"referenceID": 3, "context": "Table 1 compares our results with state-of-the-art approaches reported by (Choi et al., 2016).", "startOffset": 74, "endOffset": 93}, {"referenceID": 3, "context": "This happens automatically in the learning process, as opposed to the methods of (Han et al., 2015b;a; Choi et al., 2016), which manually impose 0 as the most frequent weight by pruning the network.", "startOffset": 81, "endOffset": 121}, {"referenceID": 28, "context": "We note that the recent works by (Ullrich et al., 2017), where the compression task is approached using the minimum description length principle, also manages to tackle the problem in a single training procedure.", "startOffset": 33, "endOffset": 55}], "year": 2017, "abstractText": "In this work we present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives state-of-the-art results for both.", "creator": "LaTeX with hyperref package"}}}