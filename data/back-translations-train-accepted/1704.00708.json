{"id": "1704.00708", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis", "abstract": "In this paper we develop a new framework that captures the common landscape underlying the common non-convex low-rank matrix problems including matrix sensing, matrix completion and robust PCA. In particular, we show for all above problems (including asymmetric cases): 1) all local minima are also globally optimal; 2) no high-order saddle points exists. These results explain why simple algorithms such as stochastic gradient descent have global converge, and efficiently optimize these non-convex objective functions in practice. Our framework connects and simplifies the existing analyses on optimization landscapes for matrix sensing and symmetric matrix completion. The framework naturally leads to new results for asymmetric matrix completion and robust PCA.", "histories": [["v1", "Mon, 3 Apr 2017 17:49:02 GMT  (50kb)", "http://arxiv.org/abs/1704.00708v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["rong ge 0001", "chi jin", "yi zheng"], "accepted": true, "id": "1704.00708"}, "pdf": {"name": "1704.00708.pdf", "metadata": {"source": "CRF", "title": "No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis", "authors": ["Rong Ge", "Chi Jin", "Yi Zheng"], "emails": ["rongge@cs.duke.edu", "chijin@cs.berkeley.edu", "sheng.zheng@duke.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.00 708v 1 [cs.L G] 3A pr2 01In this work, we develop a new framework that captures the common landscape underlying the common low-level nonconvex matrix problems, including matrix scanning, matrix completion, and robust PCA. In particular, we show for all of the above problems (including asymmetric cases): 1) All local minimums are also globally optimal; 2) there are no high-order saddle points. These results explain why simple algorithms such as stochastic gradient descent converge globally and efficiently optimize these nonconvex lens functions in practice. Our framework combines and simplifies existing analyses on optimization landscapes for matrix scapes and symmetric matrix completion. The framework naturally leads to new results for asymmetric matrix completion and robust PCA."}, {"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to reform themselves, and that they are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...)"}, {"heading": "1.1 Related Works", "text": "The landscape of low rank matrix problems have recently received a lot of attention. Ge et al. [2016] showed symmetrical matrix completion has no false local minimum. At the same time, Bhojanapalli et al. [2016] proved a similar result for symmetrical matrix scanning. Park et al. [2016] extended the matrix scanning result to asymmetrical case. All of this work guarantees global convergence with the right solution. However, there has been a lot of work on local convergence analysis for different algorithms and problems. For Matrix scanning or matrix completion, the works [Keshavan et al., 2010a, Hardt and Wootters, 2014, Jain et al., 2013, Chen and Wainwright, 2015, Sun and Luo, 2015, Zhao et al., 2015, Zheng and Lafferty, 2016."}, {"heading": "2 Preliminaries", "text": "In this section we present notations and basic optimum conditions."}, {"heading": "2.1 Notations", "text": "We use bold letters for matrices and vectors. For a vector v, we use the vector v > to denote its \"2\" standard. For a matrix M, we use \"M\" to denote its spectral standard, and \"M\" to denote its \"r-th singular value,\" and \"M\" to denote the trace of \"MN.\" We will always use \"M\" to denote the optimal low-level solution. Furthermore, we will use \"L\" 1 to denote its largest singular value, and \"R\" to denote its r-th singular value, and \"Z\" = 1 / 3 is the conditional number. We will always use \"f\" to denote the gradient, and \"L\" to denote the \"function\" and \"L\" to denote the \"Hesse,\" since \"L\" can be used frequently. \""}, {"heading": "2.2 Optimality Conditions", "text": "For a point x to be a local minimum, it must meet the necessary conditions of first and second order. That is, we must have f (x) = 0 and b (x) 2f (x) 0.Definition 1 (optimality condition). Suppose x is a local minimum of f (x), then we have f (x) = 0, b (x) 0.Intuitively, if one of these conditions is violated, then it is possible to find a direction that reduces the functional value. Ge et al. [2015] characterized the following strict saddle property, which is a quantitative version of the optimality conditions, and can lead to efficient algorithms to local minima.Definition 2. We say that the function f (\u00b7) is closely related to the local optimality condition."}, {"heading": "3 Low Rank Problems and Our Results", "text": "In this section, we present matrix scanning, matrix completion, and robust PCAs. For each problem, we provide the results obtained through our framework. Evidence ideas will be explained later in sections 4 and 5."}, {"heading": "3.1 Matrix Sensing", "text": "In the matrix matrix matrix matrix matrix matrix matrix matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix"}, {"heading": "3.2 Matrix Completion", "text": "Matrix completion is a popular technique in recommendation systems and collaborative filters [Koren, 2009, Rennie and Srebro, 2005]. In this problem we again have an unknown low matrix M +. We observe each entry of the matrix M + independently with the probability p. Let's [d1] \u00b7 [d2] be a series of observed entries. For each matrix M + we use the matrix whose entries outside the matrix are set to 0. That is, [Memphis] i, j if (i, j), and [Mempor] i, j = 0 otherwise. We use matrix completion can be considered a special case of matrix completion where we assume that the sensitive matrices have only one unequal entry. However, such matrices do not meet the RIP conditions."}, {"heading": "3.3 Robust PCA", "text": "In a sense, the goal is to break the matrixM into these two components. There are many models of how many entries can be broken through, and how they are distributed. In this paper, we work in the setting where M + S is incoherent, and the rows / columns of the S + S column can have a fraction of 0 at most. To express robust PCA as an optimization problem, we need constraints on bothM and S: min 12% M + S \u2212 Mo \u00b2 2F. (5) s.t."}, {"heading": "4 Framework for Symmetric Positive Definite Problems", "text": "In this section we describe our objectives with respect to the parameters U, which we have previously defined. (U) In this section we describe our objectives with respect to the parameters U. (U) In this section we describe our objectives. (U) In this section we describe our objectives. (U) In this section we describe our objectives. (U) In this section we describe our objectives. (U) In this section we describe the objectives. (U) in relation to the objectives. (U) in relation to the objectives. (U) We call these objectives f (M). (U) in relation to the objectives. (U) in relation to the objectives. (U) in relation to the objectives. (U). (U) in relation to the objectives. (U). (U) in relation to the objectives. (U) In this section we describe our objectives with respect to the parameters. (U) In this section we describe our objectives with respect to the parameters. (U) In this section we describe our objectives. (U) In this section we describe our objectives with respect to the objectives. (U) In this section we describe our objectives. (U) In this section we describe our objectives. (U) In this section we describe our objectives with respect to the objectives. (U) In this section we describe our objectives. (U) in relation to the objectives. (U) in relation to the objectives. (U) in relation to the objectives. (U). (U) in relation to the objectives. (U. (U) in relation to the. (U. (U). (U) in relation to the. (U."}, {"heading": "4.1 Matrix Sensing", "text": "The Matrix-Sensorik is the ideal environment for this frame. For the symmetrical Matrix-Sensorik is the objective function ismin U-Rd-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-R126mm-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-R12mm-12mm-Rmm-12mm-Rmm-12mm-Rmm-12mm Rmm-12mm-Rmm-12mm-Rmm-12mm-Rmm-12mm Rmm-12mm-Rmm-12mm-Rmm-12mm-R12mm-Rmm-12mm-Rmm-12mm-Rmm-12mm-Rmm-12mm-Rmm-12mm-12mm-Rmm-12mm-Rmm-12mm-12mm-Rmm-12mm-Rmm-12mm-Rmm-12mm-12mm-Rmm-12mm-Rmm-12mm-R12mm-12mm-Rmm-12mm-12mm-Rmm-Rmm-12mm-12mm-Rmm-12mm-Rmm-12mm-Rmm-12mm-Rmm-12mm-12mm-Rmm-12mm-12mm-Rmm-12mm-Rmm-Rmm-12mm-12mm-12mm Rmm-12mm-Rmm-12mm-12mm Rmm-12mm-Rmm-12mm-12mm Rmm-12mm Rmm-12mm-12mm Rmm-Rmm-12mm-Rmm-12mm Rmm-12mm-12mm Rmm-12mm-12mm Rmm-12mm-12mm Rmm-12mm-12mm-Rmm-Rmm-12mm-12mm-12mm Rmm-12mm-12mm"}, {"heading": "4.2 Matrix Completion", "text": "To do this, we add a regulator Q (U), which penalizes the objective function when a set of U properties is too large. (11) We select the same regulator as Ge and al. (11) Using our framework, we must first show that the regulator ensures that all rows of U are small (step 1). Lemma 9. There is an absolute constant c p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p"}, {"heading": "4.3 Robust PCA", "text": "In the robust PCA task, for each given matrix M, we can solve the objective task of finding the optimal sparse error. In the symmetric PSD case, we have a constant PSD number in each column / row, and the objective PSD number in each column / row, and the objective PSD number in each column / row, and the objective PSD number in each column / row, in the f (U): = min S number 2 d). Note the projection on which to calculate in polynomial time (using a maximum flow algorithm). We assume that the objective PSD number can be written in each column / row, in the f (U): = min S number 2 x."}, {"heading": "5 Handling Asymmetric Matrices", "text": "In this section we show how we can solve the problems on the way to the future: \"W\" (W), \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \",\" \"W,\" \"W,\" \"W,\" \"W,\" \"\" W, \"\", \"\" W, \"\" W, \"\" W, \"\" \"W,\" \"\" W, \"\", \"\" W, \"\" W, \"\", \"\" W, \"\" W, \"\" W, \",\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"\" W, \"\" W, \"\" W, \"\" W, \"\" W, \"\" W, \"\" W, \"\" W, \"\" W, \"\" W, \"\" W, \"\" \"W,\" \"W,\" \"\" W, \"\" \"W,\" \"\" W, \"\""}, {"heading": "6 Runtime", "text": "In this section we give the precise statement of Corollary 2: the runtime of the algorithms implied by the geometric properties that we prove. (2016) To translate the geometric result into runtime guarantees, many algorithms need additional smooth conditions. (2017) We say that a function f (x) for all x, y, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o,"}, {"heading": "7 Conclusions", "text": "In this paper, we provide a framework that explains the recent success in understanding optimization landscapes for low-ranking matrix problems. Our framework combines and simplifies the existing evidence and generalizes to new settings such as asymmetric matrix completion and robust PCA. The most important observation is that if the Hessian operator maintains the norm of certain matrices, one can use the same directions of improvement to prove similar optimization landscapes. We show that the regulator 14, U-V-V-2F is exactly what it needs to maintain the norm of ownership in the asymmetrical case. Our analysis also allows the interaction between regulator and Hesse to handle difficult settings like this.For low-ranking matrix problems, there are generalizations such as weighted matrix factorization [Li et al., 2016] and 1-bit matrix sensing [Davenet al., 2014, where the Hessian operator can behave differently]."}, {"heading": "A Proofs for Symmetric Positive Definite Problems", "text": "In this section we provide the missing evidence for the symmetric matrix."}, {"heading": "B Proofs for Asymmetric Problems", "text": "In this section, we provide evidence for the asymmetric settings. (\u2212 N: for the target (15), we allow N to be defined as in definition 7. (\u2212 N: for the definition (N \u2212 N): H: (N \u2212 N): (N \u2212 N): H: (N \u2212 N): (N \u2212 N): (N \u2212 N) + 4 < F (W): F (), F (): F (), F (): 2Q (), F (): 2Q (), F: F (), F: 2Q (), F: 2Q (W), W (N (W): N (N (W): N \u2212 Z): N (N \u2212 Z): N (N \u2212 Z): N (N \u2212 Z): 3 (N \u2212 N): (N \u2212 N)."}, {"heading": "C Matrix Sensing with Noise", "text": "In this section we will show how to deal with noise using our framework. < M (originally the regulator), to also consider noise as a disturbance of the original objective function and use of Q (U) (originally the regulator). < M (originally the regulator), to also consider the Noise.C.1 Symmetrical CaseHere we assume that in each consideration we have the exact value bi = < Ai, M (1), M (2), M (26), Define Q (U) = f (0, 2). Consider the objective function in Section 4, we now have: f (M) = 1mm (M), I (< M \u2212 M), M (A) > + ni (26), Define Q (U) \u2212 12 (UU)."}, {"heading": "D Proof Sketch for Running Time", "text": "In this section, we outline the evidence for Corollarie 17: \"We are only able to find a solution to a global problem if we can find a solution to a global problem.\" (3), \"We are only able to find a solution to a global problem if we are looking for a solution to a global problem.\" (4), \"We are not able to find a solution to a global problem.\" (4), \"We are not able to find a solution to a global problem.\" (4), \"We are not able to find a solution to a global optimum.\" (4), \"We are not able to find a solution to a global optimum.\" (4), \"We are not able to find a solution to a global optimum.\" (4), \"We are more dependent on a global optimum.\" (4)"}, {"heading": "E Concentrations", "text": "A1, A2, A2, A2, A2, A2, A2, A2, A1, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, 2, A2, 2, A2, 2, 2, A2, 2, A2, 2, 2, A2, 2, 2, A2, 2, 2, A2, 2, 2, 2, 2, 2, A2, 2, 2, 2, 2, A2, 2, 2, 2, 2, A2, 2, 2, 2, A2, 2, 2, A2, 2, 2, A2, 2, 2, 2, 2, A2, 2, 2, 2, 2, 2, 2, A2, 2, 2, 2, 2, 2, 2, 2, 2, A2, 2, A2, 2, 2, 2, 2, A2, 2, A2, 2, 2, 2, A2, 2, 2, A2, 2, 2, 2, 2, 2, 2, 2, 2, A2, 2, A2, A2, A2, 2, 2, 2,"}, {"heading": "F Auxiliary Inequalities", "text": "In this section we provide some commonly used Lemmas related to matrices. Our first two Lemmas are subject to Inequality (UU)."}], "references": [{"title": "Basis learning as an algorithmic primitive", "author": ["2016. Mikhail Belkin", "Luis Rademacher", "James Voss"], "venue": null, "citeRegEx": "Belkin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2016}, {"title": "Global optimality of local search for low rank matrix", "author": ["Srinadh Bhojanapalli", "Behnam Neyshabur", "Nathan Srebro"], "venue": null, "citeRegEx": "Bhojanapalli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bhojanapalli et al\\.", "year": 2015}, {"title": "Tight oracle inequalities for low-rank matrix recovery from a minimal number", "author": ["Emmanuel J Candes", "Yaniv Plan"], "venue": null, "citeRegEx": "Candes and Plan.,? \\Q2012\\E", "shortCiteRegEx": "Candes and Plan.", "year": 2012}, {"title": "Stable signal recovery from incomplete and inaccurate", "author": ["Emmanuel J Candes", "Justin K Romberg", "Terence Tao"], "venue": null, "citeRegEx": "Candes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2009}, {"title": "Matrix rank minimization with applications", "author": ["Maryam Fazel"], "venue": "PhD thesis,", "citeRegEx": "Fazel.,? \\Q2002\\E", "shortCiteRegEx": "Fazel.", "year": 2002}, {"title": "Matrix completion has no spurious local minimum", "author": ["Rong Ge", "Jason D Lee", "Tengyu Ma"], "venue": null, "citeRegEx": "Ge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2015}, {"title": "Understanding alternating minimization for matrix completion", "author": ["Moritz Hardt"], "venue": "Information Processing Systems,", "citeRegEx": "Hardt.,? \\Q2014\\E", "shortCiteRegEx": "Hardt.", "year": 2014}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": null, "citeRegEx": "Jain et al\\.,? \\Q1933\\E", "shortCiteRegEx": "Jain et al\\.", "year": 1933}, {"title": "How to escape saddle points efficiently", "author": ["2013. Chi Jin", "Rong Ge", "Praneeth Netrapalli", "ShamMKakade", "andMichael I Jordan"], "venue": "Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Jin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2013}, {"title": "Matrix completion from a few entries", "author": ["2017. Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": null, "citeRegEx": "Keshavan et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Keshavan et al\\.", "year": 2017}, {"title": "Matrix completion from noisy entries", "author": ["Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": "Theory, IEEE Transactions on,", "citeRegEx": "Keshavan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Keshavan et al\\.", "year": 2010}, {"title": "The nonconvex geometry of low-rank matrix optimizations with general objective", "author": ["Qiuwei Li", "Gongguo Tang"], "venue": null, "citeRegEx": "Li and Tang.,? \\Q2005\\E", "shortCiteRegEx": "Li and Tang.", "year": 2005}, {"title": "Non-squarematrix sensing without", "author": ["2014. Dohyung Park", "Anastasios Kyrillidis", "Constantine Caramanis", "Sujay Sanghavi"], "venue": "pca. In Advances in Neural Information Processing Systems,", "citeRegEx": "Park et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Park et al\\.", "year": 2014}, {"title": "Complete dictionary recovery over the sphere I: Overview and the geometric", "author": ["Ju Sun", "Qing Qu", "John Wright"], "venue": "Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Sun et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2005}, {"title": "When are nonconvex problems not scary", "author": ["2015a. Ju Sun", "Qing Qu", "John Wright"], "venue": "arXiv preprint arXiv:1510.06096,", "citeRegEx": "Sun et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2015}, {"title": "Guaranteed matrix completion via nonconvex factorization", "author": ["2015b. Ruoyu Sun", "Zhi-Quan Luo"], "venue": null, "citeRegEx": "Sun and Luo.,? \\Q2015\\E", "shortCiteRegEx": "Sun and Luo.", "year": 2015}, {"title": "Low-rank solutions of linear matrix equations", "author": ["Stephen Tu", "Ross Boczar", "Mahdi Soltanolkotabi", "Benjamin Recht"], "venue": "Science (FOCS),", "citeRegEx": "Tu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2015}, {"title": "A nonconvex free lunch for low-rank plus sparse matrix recovery", "author": ["Xiao Zhang", "Lingxiao Wang", "Quanquan Gu"], "venue": "arXiv preprint arXiv:1702.06525,", "citeRegEx": "Zhang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}, {"title": "A nonconvex optimization framework for low rank matrix estimation", "author": ["Tuo Zhao", "Zhaoran Wang", "Han Liu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "Convergence analysis for rectangular matrix completion using burer-monteiro factorization and gradient descent", "author": ["Qinqing Zheng", "John Lafferty"], "venue": "arXiv preprint arXiv:1605.07051,", "citeRegEx": "Zheng and Lafferty.,? \\Q2016\\E", "shortCiteRegEx": "Zheng and Lafferty.", "year": 2016}, {"title": "Lemma follows from direct calculation using linear algebra and calculus. In the first step of our framework, we hope to show that the regularizer forces the matrixU to not have large rows. This is formalized and proved below (the Lemma is similar to Lemma", "author": ["Ge"], "venue": null, "citeRegEx": "Ge,? \\Q2016\\E", "shortCiteRegEx": "Ge", "year": 2016}, {"title": "2016] stated the symmetric version, and we need the asymmetric version here, the proof in Ge et al", "author": ["Although Ge"], "venue": null, "citeRegEx": "Ge,? \\Q2016\\E", "shortCiteRegEx": "Ge", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "Recently, a line of works showed that several natural problems including tensor decomposition [Ge et al., 2015], dictionary learning [Sun et al.", "startOffset": 94, "endOffset": 111}, {"referenceID": 1, "context": ", 2015a], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016] and matrix completion [Ge et al., 2016] have well-behaved optimization landscape: all local optima are also globally optimal. Combined with recent results (e.g. Ge et al. [2015], Carmon et al.", "startOffset": 26, "endOffset": 250}, {"referenceID": 1, "context": ", 2015a], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016] and matrix completion [Ge et al., 2016] have well-behaved optimization landscape: all local optima are also globally optimal. Combined with recent results (e.g. Ge et al. [2015], Carmon et al. [2016], Agarwal et al.", "startOffset": 26, "endOffset": 272}, {"referenceID": 1, "context": ", 2015a], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016] and matrix completion [Ge et al., 2016] have well-behaved optimization landscape: all local optima are also globally optimal. Combined with recent results (e.g. Ge et al. [2015], Carmon et al. [2016], Agarwal et al. [2016], Jin et al.", "startOffset": 26, "endOffset": 295}, {"referenceID": 1, "context": ", 2015a], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016] and matrix completion [Ge et al., 2016] have well-behaved optimization landscape: all local optima are also globally optimal. Combined with recent results (e.g. Ge et al. [2015], Carmon et al. [2016], Agarwal et al. [2016], Jin et al. [2017]) that are guaranteed to find a local minimum for many non-convex functions, such problems can be efficiently solved by basic optimization algorithms such as stochastic gradient descent.", "startOffset": 26, "endOffset": 314}, {"referenceID": 1, "context": ", 2015a], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016] and matrix completion [Ge et al., 2016] have well-behaved optimization landscape: all local optima are also globally optimal. Combined with recent results (e.g. Ge et al. [2015], Carmon et al. [2016], Agarwal et al. [2016], Jin et al. [2017]) that are guaranteed to find a local minimum for many non-convex functions, such problems can be efficiently solved by basic optimization algorithms such as stochastic gradient descent. In this paper we focus on optimization problems that look for low rank matrices using partial or corrupted observations. Such problems are studied extensively [Fazel, 2002, Rennie and Srebro, 2005, Cand\u00e8s and Recht, 2009] and has many applications in recommendation systems [Koren, 2009], see survey by Davenport and Romberg [2016]. These optimization problems can be formalized as follows: min M\u2208Rd1\u00d7d2 f(M), (1) s.", "startOffset": 26, "endOffset": 832}, {"referenceID": 4, "context": "Ge et al. [2016] showed symmetric matrix completion has no spurious local minimum.", "startOffset": 0, "endOffset": 17}, {"referenceID": 1, "context": "At the same time, Bhojanapalli et al. [2016] proved similar result for symmetric matrix sensing.", "startOffset": 18, "endOffset": 45}, {"referenceID": 1, "context": "At the same time, Bhojanapalli et al. [2016] proved similar result for symmetric matrix sensing. Park et al. [2016] extended the matrix sensing result to asymmetric case.", "startOffset": 18, "endOffset": 116}, {"referenceID": 12, "context": "Sun and Luo [2015], Zheng and Lafferty [2016]) accomplished this by showing a geometric property which is very similar to strong convexity holds in the neighborhood of optimal solution.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "Sun and Luo [2015], Zheng and Lafferty [2016]) accomplished this by showing a geometric property which is very similar to strong convexity holds in the neighborhood of optimal solution.", "startOffset": 0, "endOffset": 46}, {"referenceID": 0, "context": "Bhojanapalli et al. [2015] gave a framework for local analysis for these low rank problems.", "startOffset": 0, "endOffset": 27}, {"referenceID": 0, "context": "Belkin et al. [2014] showed a framework of learning basis functions, which generalizes tensor decompositions.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Belkin et al. [2014] showed a framework of learning basis functions, which generalizes tensor decompositions. Their techniques imply the optimization landscape for all such problems are very similar. For problems looking for a symmetric PSD matrix, Li and Tang [2016] showed for objective similar to (2) (but in the symmetric setting), restricted smoothness/strong convexity on the function f suffices for local analysis.", "startOffset": 0, "endOffset": 268}, {"referenceID": 5, "context": "Ge et al. [2015] characterized the following strict-saddle property, which is a quantitative version of the optimality conditions, and can lead to efficient algorithms to find local minima.", "startOffset": 0, "endOffset": 17}, {"referenceID": 5, "context": "We choose the same regularizer as Ge et al. [2016]: Q(U) = \u03bb \u2211d i=1(\u2016Ui\u2016 \u2212 \u03b1)+.", "startOffset": 34, "endOffset": 51}, {"referenceID": 5, "context": "7 in Ge et al. [2016]. Next we show under this regularizer, we can still select the direction\u2206, and the first part of Equation (9) is significantly negative when\u2206 is large (step 2):", "startOffset": 5, "endOffset": 22}, {"referenceID": 5, "context": "Notice that our proof is different from Ge et al. [2016], as we focus on the direction \u2206 for both first and second order conditions while they need to select different directions for the Hessian.", "startOffset": 40, "endOffset": 57}, {"referenceID": 5, "context": "Existing results show many algorithms are saddle-avoiding, including cubic regularization [Nesterov and Polyak, 2006], stochastic gradient descent [Ge et al., 2015], trust-region algorithms [Sun et al.", "startOffset": 147, "endOffset": 164}, {"referenceID": 1, "context": "Towards faster convergence For many low-rank matrices problems, in the neighborhood of local minima, objective function satisfies conditions similar to strong convexity [Zheng and Lafferty, 2016, Bhojanapalli et al., 2016] (more precisely, the (\u03b1, \u03b2)-regularity condition as Assumption A3.b in [Jin et al., 2017]). Jin et al. [2017] showed a principle way of how to combine these strong local structures with saddle-avoiding algorithm to give global linear convergence.", "startOffset": 196, "endOffset": 333}, {"referenceID": 5, "context": "7 in Ge et al. [2016], but we get a stronger guarantee here): Lemma 9.", "startOffset": 5, "endOffset": 22}, {"referenceID": 2, "context": "(Candes and Plan [2011], Theorem 2.", "startOffset": 1, "endOffset": 24}, {"referenceID": 5, "context": "Ge et al. [2016] Let d = max{d1, d2}, then with at least probability 1\u2212 e over random choice of \u03a9, we have for any rank 2r matricesA \u2208 Rd1\u00d7d2:", "startOffset": 0, "endOffset": 17}, {"referenceID": 5, "context": "Although Ge et al. [2016] stated the symmetric version, and we need the asymmetric version here, the proof in Ge et al.", "startOffset": 9, "endOffset": 26}, {"referenceID": 5, "context": "Although Ge et al. [2016] stated the symmetric version, and we need the asymmetric version here, the proof in Ge et al. [2016] works directly.", "startOffset": 9, "endOffset": 127}], "year": 2017, "abstractText": "In this paper we develop a new framework that captures the common landscape underlying the common nonconvex low-rank matrix problems including matrix sensing, matrix completion and robust PCA. In particular, we show for all above problems (including asymmetric cases): 1) all local minima are also globally optimal; 2) no highorder saddle points exists. These results explain why simple algorithms such as stochastic gradient descent have global converge, and efficiently optimize these non-convex objective functions in practice. Our framework connects and simplifies the existing analyses on optimization landscapes for matrix sensing and symmetric matrix completion. The framework naturally leads to new results for asymmetric matrix completion and robust PCA.", "creator": "LaTeX with hyperref package"}}}