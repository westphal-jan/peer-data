{"id": "1704.00784", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Online and Linear-Time Attention by Enforcing Monotonic Alignments", "abstract": "Recurrent neural network models with an attention mechanism have proven to be extremely effective on a wide variety of sequence-to-sequence problems. However, the fact that soft attention mechanisms perform a pass over the entire input sequence when producing each element in the output sequence precludes their use in online settings and results in a quadratic time complexity. Based on the insight that the alignment between input and output sequence elements is monotonic in many problems of interest, we propose an end-to-end differentiable method for learning monotonic alignments which, at test time, enables computing attention online and in linear time. We validate our approach on sentence summarization, machine translation, and online speech recognition problems and achieve results competitive with existing sequence-to-sequence models.", "histories": [["v1", "Mon, 3 Apr 2017 19:45:27 GMT  (253kb,D)", "https://arxiv.org/abs/1704.00784v1", "19 pages, 5 figures (three full-page), including 4 pages of supplementary material"], ["v2", "Thu, 29 Jun 2017 21:14:58 GMT  (773kb,D)", "http://arxiv.org/abs/1704.00784v2", "ICML camera-ready version; 10 pages + 9 page appendix"]], "COMMENTS": "19 pages, 5 figures (three full-page), including 4 pages of supplementary material", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["colin raffel", "minh-thang luong", "peter j liu", "ron j weiss", "douglas eck"], "accepted": true, "id": "1704.00784"}, "pdf": {"name": "1704.00784.pdf", "metadata": {"source": "META", "title": "Online and Linear-Time Attention by Enforcing Monotonic Alignments", "authors": ["Colin Raffel", "Minh-Thang Luong", "Peter J. Liu", "Ron J. Weiss", "Douglas Eck"], "emails": ["<craffel@gmail.com>."], "sections": [{"heading": "1. Introduction", "text": "In fact, it is a way in which people are able to determine for themselves how they want to behave."}, {"heading": "2. Online and Linear-Time Attention", "text": "To motivate our approach, we first point out that softmaxbased attention calculates the expected performance of a simple stochastic process. Then, we explain an alternative process that enables online and linear time decoding. As this process is indistinguishable, we derive an algorithm to calculate its expected performance that allows us to train a model with standard back propagation while applying our online and linear time process at test time. Finally, we propose an alternative energy function motivated by the differences between monotonous attention and softmax-based attention."}, {"heading": "2.1. Soft Attention", "text": "First, we review the commonly used form of soft attention originally proposed in (Bahdanau et al., 2015). Broadly speaking, a sequence-to-sequence model yields a sequence of outputs based on a processed input sequence, consisting of two RNNs called \"encoders\" and \"decoders\"; the RNN encoder processes the input sequence x = {x1,., xT} to produce a sequence of hidden states h = {h1,., hT}; we refer to h as \"memory\" to emphasize its connection to memory-augmented neural networks (Graves et al., 2014; Sukhbaatar et al., 2015); the RNN decoder then produces an output sequence y = {y1,.,., yU} that is conditioned on memory, until a special-end sequence is produced."}, {"heading": "2.2. A Hard Monotonic Attention Process", "text": "The discussion above makes it clear that softmax-based attention requires a run over the entire memory to calculate the terms \u03b1i, j that are required to produce each element of the output sequence, which precludes their use in online settings and leads to a complexity of O (TU) to generate the output sequence. In addition, we address these shortcomings by first formulating a stochastic process that explicitly processes the memory in a left-right manner, in which the output sequences between the following elements are applied. Specifically, we start processing memory entries from index ti \u2212 1, where ti is the index of the memory entry we selected at output, i (for convenience, so that t0 = 1)."}, {"heading": "2.3. Training in Expectation", "text": "In analogy to softmax-based attention, we therefore propose a training on the expected value of ci, which can be calculated directly as follows. However, we first calculate ei, j and pi, j exactly as in eqs. (6) and (7), where pi, j is interpreted as the probability of choosing the memory element j in the output time. Attention distribution over the memory is then determined by (see appendix C for a derivative) \u03b1i, j, j, k = 1 (\u03b1i \u2212 1, kj \u2212 l = k) (9) = pi, j (1 \u2212 1), j (1 \u2212 1), the derivative Ci, j \u2212 i, j \u2212 1, j, j, j \u2212 k =."}, {"heading": "2.4. Modified Energy Function", "text": "While various \"energy functions\" a (\u00b7) have been proposed, the most common of our knowledge is those proposed in (Bahdanau et al., 2015): a (si \u2212 1, hj) = v > tanh (Wsi \u2212 1 + V hj + b) (15), where W and V are weight matrices, b is a bias vector, 1 and v is a weight vector. We make two modifications to eq. (15) for use with our monotonous decoder: first, while the Softmax is invariable to offset, 2 is not the logistic sigmoid. Consequently, we make the simple modification of adding a scalar variable r after the tanh function, which allows the model to learn the corresponding offset for the sigmoid activations. Note that eq. (13) tends to fall exponentially over memory, because 1, 0, [pi]."}, {"heading": "2.5. Encouraging Discreteness", "text": "As mentioned above, for our mechanism to exhibit a similar behavior when we train in anticipation and apply the hard monotonous attention process at test date, we need this pi-j-0 or pi-j-1. One simple way to encourage this behavior is to add the noise in front of the sigmoid in eq. (12), as has been done, for example, in all our experiments. This approach is similar to the recently proposed Gumbel Softmax trick (Jang et al., 2016; Maddison et al., 2016), except that we did not consider it necessary to set the temperature as in (Jang et al., 2016)."}, {"heading": "3. Related Work", "text": "In fact, it is the case that most of them are able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they, in which they are able to move, in which they are able to move, in which they are able to"}, {"heading": "4. Experiments", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5. Discussion", "text": "Our results show that our differentiated approach to enforcing monotonous alignments can produce models that, following the decoding process of Section 2.2, allow for efficient online decoding at test time without sacrificing significant performance on a variety of tasks. We believe that our framework provides a promising environment for future work on online and linear time sequence-to-sequence models. We are interested in exploring various enhancements to this approach, which we outline in Appendix E. To facilitate experimentation with our proposed attention mechanism, we have made an example of the implementation of our approach available online TensorFlow (Abadi et al., 2016) and added a reference implementation to the tf.contrib.seq2seq module of TensorFlow."}, {"heading": "Acknowledgements", "text": "We thank Jan Chorowski, Mark Daoust, Pietro Kreitlon Carolino, Dieterich Lawson, Navdeep Jaitly, George Tucker, Quoc V. Le, Kelvin Xu, Cinjon Resnick, Melody Guan, Matthew D. Hoffman, Jeffrey Dean, Kevin Swersky, Ashish Vaswani and members of the Google Brain team for helpful discussions and insights."}, {"heading": "A. Algorithms", "text": "Following are the algorithms for the hard monotonic decoding process, which we used at test points (algorithm 1 = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = 0 = i, i = i = i = i = i = i = i = i = i, i = i = i = i = i = i = i, i = i = i = i = i = i = i = i = i = i, 0 = i = i = i = i = i = i = i = i = i, i = i = i = i = i = i = i, i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i"}, {"heading": "B. Figures", "text": "Below are examples of hard monotonous and soft attention alignments for each of the different tasks we have included in our experiments. Attention matrices are represented in such a way that black equals 1 and white equals 0."}, {"heading": "C. Monotonic Attention Distribution", "text": "In order to achieve this, we derive an expression for the probability that \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "D. Experiment Details", "text": "This year, the time has come to put yourself in a position to be at the forefront in order to find the way to the future."}, {"heading": "E. Future Work", "text": "We believe that there are a number of promising extensions to our monotonous attention mechanism, which we outline briefly below. \u2022 The primary drawback of training in anticipation is that it maintains the square complexity during training. One idea would be to replace the cumulative equivalent product (9) with the threshold residual method of (Graves, 2016) and (Grefenstette et al., 2015), but in preliminary experiments we have not been able to successfully learn alignments with this approach. Alternatively, we could continue our investigation of gradient estimators for discrete decisions (such as REINFORCE or straightthrough) instead of conducting the training in expectation (Bengio et al., 2013). \u2022 As we point out in Section 2.4, our method may fail if attention ei, j is poorly scaled. This is primarily due to the strict enforcement of monotonism. One way to mitigate this would be to regulate the monotonous model, rather than to soften it with a monetary penalty."}, {"heading": "F. How much faster is linear-time decoding?", "text": "In fact, so far we have not tried to quantify how much of an acceleration of this process will come to fruition in practice. To this end, we have conducted an additional experiment to measure the speed of the efficiently implemented softmax-based and hard monotonic attention mechanisms. We have focused exclusively on the speed of the acceleration mechanisms and not on a complete acceleration of the acceleration processes."}, {"heading": "G. Practitioner\u2019s Guide", "text": "As we propose a novel attention mechanism, here we share some insights we have learned from the application in various settings to help practitioners try it out on their own problems: \u2022 The recursive structure of computing \u03b1i, j in eq. (9) can lead to exploding gradients. We found it important to apply gradient clipping in all our experiments, as described in Appendix D. \u2022 Many automatic differentiation packages can create numerically unstable gradients if we use their cumulative product functions. (67) Our simple solution was to calculate the product in logspace, i.e. to replace XN = Exp (XN). (XN) In addition, the product in the denominator of eq. (29) can become negligibly small because the terms (1 \u2212 pi, k \u2212 1) all fall within the range [0, 1]."}], "references": [{"title": "TensorFlow: A system for large-scale machine learning", "author": ["Tucker", "Paul", "Vasudevan", "Vijay", "Warden", "Pete", "Wicke", "Martin", "Yu", "Yuan", "Zheng", "Xiaoqiang"], "venue": "In Operating Systems Design and Implementation,", "citeRegEx": "Tucker et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tucker et al\\.", "year": 2016}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Bengio", "Yoshua", "L\u00e9onard", "Nicholas", "Courville", "Aaron"], "venue": "arXiv preprint arXiv:1308.3432,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Towards better decoding and language model integration in sequence to sequence models", "author": ["Chorowski", "Jan", "Jaitly", "Navdeep"], "venue": "arXiv preprint arXiv:1612.02695,", "citeRegEx": "Chorowski et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2017}, {"title": "Adaptive computation time for recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1603.08983,", "citeRegEx": "Graves and Alex.,? \\Q2016\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2016}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Parallel prefix computation", "author": ["Ladner", "Richard E", "Fischer", "Michael J"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Ladner et al\\.,? \\Q1980\\E", "shortCiteRegEx": "Ladner et al\\.", "year": 1980}, {"title": "Text summarization with TensorFlow", "author": ["Liu", "Peter J", "Pan", "Xin"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Stanford neural machine translation systems for spoken language domain", "author": ["Luong", "Minh-Thang", "Manning", "Christopher D"], "venue": "In International Workshop on Spoken Language Translation,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Pham", "Vu", "Bluche", "Th\u00e9odore", "Kermorvant", "Christopher", "Louradour", "J\u00e9r\u00f4me"], "venue": "In International Conference on Frontiers in Handwriting Recognition,", "citeRegEx": "Pham et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Rush", "Alexander M", "Chopra", "Sumit", "Weston", "Jason"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "Recently, the \u201csequence-to-sequence\u201d framework (Sutskever et al., 2014; Cho et al., 2014) has facilitated the use of recurrent neural networks (RNNs) on sequence transduction problems such as machine translation and speech recognition.", "startOffset": 47, "endOffset": 89}, {"referenceID": 9, "context": "Attention-based sequence-tosequence models have proven to be extremely effective on a wide variety of problems, including machine translation (Bahdanau et al., 2015; Luong et al., 2015), image captioning (Xu et al.", "startOffset": 142, "endOffset": 185}, {"referenceID": 11, "context": ", 2016), and sentence summarization (Rush et al., 2015).", "startOffset": 36, "endOffset": 55}, {"referenceID": 9, "context": "Typically, a(\u00b7) is a single-layer neural network using a tanh nonlinearity, but other functions such as a simple dot product between si\u22121 and hj have been used (Luong et al., 2015; Graves et al., 2014).", "startOffset": 160, "endOffset": 201}, {"referenceID": 9, "context": "In a similar vein, (Luong et al., 2015) explore only computing attention over a small window of the memory.", "startOffset": 19, "endOffset": 39}, {"referenceID": 11, "context": "ROUGE F-measure scores for sentence summarization on the Gigaword test set of (Rush et al., 2015).", "startOffset": 78, "endOffset": 97}, {"referenceID": 11, "context": "(Rush et al., 2015) reports ROUGE recall scores, so we report the F-1 scores computed for that approach from (Chopra et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 11, "context": "01 (Rush et al., 2015) 29.", "startOffset": 3, "endOffset": 22}, {"referenceID": 11, "context": "For data preparation and evaluation, we followed the approach of (Rush et al., 2015), measuring performance using the ROUGE metric.", "startOffset": 65, "endOffset": 84}, {"referenceID": 9, "context": "Our baseline neural machine translation (NMT) system is the softmax attention-based sequence-to-sequence model described in (Luong et al., 2015).", "startOffset": 124, "endOffset": 144}, {"referenceID": 9, "context": "In (Luong et al., 2015), the authors demonstrated that under their proposed architecture, a dot product-based energy function worked better than eq.", "startOffset": 3, "endOffset": 23}, {"referenceID": 9, "context": "Since our architecture is based on that of (Luong et al., 2015), to facilitate comparison we also tested the following variant:", "startOffset": 43, "endOffset": 63}], "year": 2017, "abstractText": "Recurrent neural network models with an attention mechanism have proven to be extremely effective on a wide variety of sequence-tosequence problems. However, the fact that soft attention mechanisms perform a pass over the entire input sequence when producing each element in the output sequence precludes their use in online settings and results in a quadratic time complexity. Based on the insight that the alignment between input and output sequence elements is monotonic in many problems of interest, we propose an end-to-end differentiable method for learning monotonic alignments which, at test time, enables computing attention online and in linear time. We validate our approach on sentence summarization, machine translation, and online speech recognition problems and achieve results competitive with existing sequence-tosequence models.", "creator": "LaTeX with hyperref package"}}}