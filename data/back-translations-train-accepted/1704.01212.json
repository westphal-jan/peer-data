{"id": "1704.01212", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2017", "title": "Neural Message Passing for Quantum Chemistry", "abstract": "Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation function to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark, results we believe are strong enough to justify retiring this benchmark.", "histories": [["v1", "Tue, 4 Apr 2017 23:00:44 GMT  (140kb,D)", "http://arxiv.org/abs/1704.01212v1", "13 pages"], ["v2", "Mon, 12 Jun 2017 20:52:56 GMT  (118kb,D)", "http://arxiv.org/abs/1704.01212v2", "14 pages"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["justin gilmer", "samuel s schoenholz", "patrick f riley", "oriol vinyals", "george e dahl"], "accepted": true, "id": "1704.01212"}, "pdf": {"name": "1704.01212.pdf", "metadata": {"source": "META", "title": "Neural Message Passing for Quantum Chemistry", "authors": ["Justin Gilmer", "Samuel S. Schoenholz", "Patrick F. Riley", "Oriol Vinyals", "George E. Dahl"], "emails": ["<gilmer@google.com>,", "<gdahl@google.com>."], "sections": [{"heading": "1. Introduction", "text": "This year it has come to the point where it only takes a few days for it to come to a conclusion."}, {"heading": "2. Message Passing Neural Networks", "text": "There are at least seven notable examples of models from the literature, namely (Duvenaud et al). (D) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S (S) S (S) S (S (S) S (S) S (S (S) S (S (S) S (S (S) S (S (S) S (S (S (S) S (S (S) S (S (S (S) S (S (S) S (S (S (S (S) S (S (S (S) S (S (S (S) S (S (S) S (S (S) S (S (S (S (S) S (S (S (S) S (S (S (S (S) S (S (S) S (S (S) S (S (S (S) S (S (S (S) S (S (S) S (S (S) S (S (S (S) S) S (S (S) S) S (S (S (S) S) S (S (S) S (S (S) S (S) S (S (S (S) S (S (S) S"}, {"heading": "2.1. Moving Forward", "text": "Given how many MPNs appear in the literature, we should focus on pushing this generic family as far as possible in a specific application of significant practical importance, so that we can determine the key implementation details and potentially reach the limits of these models to guide us toward future modeling improvements. A drawback of all these approaches is computational time. Recent work has adapted the GG-NN architecture to larger graphs by passing only subsets of the graph at each step (Marino et al., 2016). In this paper, we also present an MPNN modification that can improve computational costs."}, {"heading": "3. Related Work", "text": "This year it is more than ever before."}, {"heading": "4. QM9 Dataset", "text": "To investigate the success of MPNs in predicting chemical properties, we use the publicly available QM9 dataset (Ramakrishnan et al., 2014). The molecules in the dataset consist of hydrogen (H), carbon (C), oxygen (O), nitrogen (N) and fluorine (F) atoms and contain up to 9 heavy (non-hydrogen) atoms. In total, this results in about 134k druglike organic molecules covering a wide range of chemistry. DFT is used for each molecule to find a reasonable low-energy structure and therefore atomic \"positions\" are available. In addition, a wide range of interesting and fundamental chemical properties are calculated. Given the fundamental properties of some QM9 properties, it is difficult to imagine that success in more demanding chemical tasks will result in molecules U and molecules U (U) atoms, if we do not make statistical predictions for the Q9."}, {"heading": "5. MPNN Variants", "text": "We started our research on MPNNs around the GG-NN model, which we believe to be a strong baseline. We focused on trying out different message functions, output functions, finding the right input representation and tuning hyperparameters. For the rest of the paper, we use d to mark the dimension of the internal hidden representation of each node in the graph, and n to indicate the number of nodes in the graph. Our implementation of MPNNs generally works on undirected chemical graphs with a separate message channel for incoming and outgoing edges, where the incoming message mv is the concatenation of minv and m out v, this was also used in Li et al. (2016) If we apply this to undirected chemical graphs, we treat the graph as directed, with each original edge being both an incoming and nothing outgoing, just looking at it with the outgoing direction."}, {"heading": "5.1. Message Functions", "text": "Matrix multiplication: We started with the message function used in GG-NN, which is represented by the equation M (hv, hw, evw) = Aevwhw.Edge network: To allow vector-weighted edge characteristics, we propose the message function M (hv, hw, evw) = A (evw) hw, where A (evw) is a neural network that maps the edge vector evw to a d \u00b7 d matrix. Couple message: One property of the matrix multiplication rule is that the message from node w to node v is only a function of the hidden state hw and the edge evw. Specifically, it does not depend on the hidden state htv. Theoretically, a network can use the message channel more efficiently if the messages of the node may depend on both the source and the destination node v."}, {"heading": "5.2. Virtual Graph Elements", "text": "The simplest modification is to add a separate \"virtual\" edge type for unconnected nodes, which can be implemented as a data pre-processing step and allows information to travel long distances during the propagation phase. We also experimented with using a latent \"master\" node to the graph, which is connected to each input node in the graph with a special edge type. The master node serves as a global scratch place from which each node both reads and writes at each step of message transmission. We allow the master node a separate node dimension dmaster as well as separate weights for the internal update function (in our case a GRU). The master node then allows for large distances during the propagation phase. Theoretically, it also allows additional model capacity (e.g. large values of the dmaster) without achieving a significant hit in the performance, as the master model complexity is the master-2 | E | node."}, {"heading": "5.3. Readout Functions", "text": "We have experimented with two read-out functions: First, with the read-out function used in GG-NN, which is defined by Equation 4; second, with a set2set model by Vinyals et al. (2015). The set2set model is specifically designed for the operation of sets and should have a more general significance than simply summing up the final node states. This model first applies a linear projection to each tuple (hTv, xv) and then takes as input the set of projected tuples T = {(hTv, xv)}. Then, after M calculation steps, the set2set model generates a graph plane that is invariant in the order of tuples T."}, {"heading": "5.4. Multiple Towers", "text": "One problem with MPNNs is scalability. In particular, a single step of the message transmission phase requires floating-point multiplications for a dense graph O (n2d2). As n or d grow large, this can be computationally expensive. To solve this problem, we suggest the d-dimensional embedding htv into k different d / k dimensional embedding ht, kv. Then we perform a propagation step for each of the k copies individually to obtain temporary embedding {h, kv, v, G} using separate message and update functions for each copy. The k-temporary embedding of each node is then mixed according to the equation (ht, 1v, h t, 2 v, g)."}, {"heading": "6. Input Representation", "text": "There are a number of features available for each atom in a molecule that capture both the properties of the electrons in the atom and the bonds in which the atom is involved. For a list of all features see Table 1. We experimented with making the hydrogen atom binding nodes explicit in the diagram (as opposed to simply including the number as a node characteristic), with diagrams having up to 29 nodes. Note that larger diagrams significantly slow down the training time by a factor of about 10. For the adjustment matrix, there are three edge representations, depending on the model. Chemical Graph: In the absence of distance information, adjustment matrix entries are discrete binding types: single, double, triple, aromatic or unbound. Distance bins: The matrix function multiplies the message functions for discrete edge types, so that we divide the distance information into 10 containers."}, {"heading": "7. Training", "text": "The number of set2set calculations M was selected from the range 1 \u2264 M \u2264 12. All models were trained with the ADAM Optimizer (Kingma & Ba (2014)), with batch size 20 for 2 million steps (360 epochs).The initial learning rate was uniformly between 1e \u2212 5 and 5e \u2212 4. We used a linear learning decay that began between 10% and 90% of the way through the training, and the initial learning rate l decayed into a final learning rate l \u0445 F, using a decay factor F in the range [.01, 1].The QM-9 dataset contains 130462 molecules therein. We randomly selected 10,000 samples for validation and used the rest of the training for the exam. We use validation to falsify the selection and reduce the averages and reduce all results to 1."}, {"heading": "8. Results", "text": "In all our tables, we report on the ratio of absolute error rates (MAE) of our models with the best estimate of chemical accuracy for that goal. Thus, any model with a failure rate of less than one percent is able to achieve chemical accuracy for that goal. We have conducted numerous experiments to find the best possible nodes in this diagram. Equally important, it is to choose the right input factors for that goal. In our experiments, we found that the complete edge features vector (bond, spatial distance) and the treatment of hydrogen atoms in the diagram are very important for a number of targets. We also found that the formation of one model per target is executed consistently. In some cases, the improvement was up to 40%. Our best MPNN variant used the edge network messaging function, and we operated on graphs with explicit hyperformations. We were able to further improve performance by comparing the predictions of the five models."}, {"heading": "9. Conclusions and Future Work", "text": "Our results show that MPNNs with the appropriate message, update, and output capabilities have a useful inductive bias for predicting molecular properties, outperforming multiple strong baselines, and eliminating the need for complicated feature engineering. Our results also show the importance of enabling far-reaching interactions between nodes in the graph, with both the master node and set2set output achieving this efficiently. Tower variation makes these models more scalable, but additional improvements are needed to scale them to much larger graphs. An important future direction is to design MPNNs that can effectively generalize to larger graphs than those that appear in the training set, or at least to work with benchmarks designed to highlight problems with generalizing graph sizes."}, {"heading": "Acknowledgements", "text": "We would like to thank \u0141ukasz Kaiser, Geoffrey Irving, Alex Graves, Steven Kearnes and Yujia Li for their helpful discussions and O. Anatole von Lilienfeld and his entire group at the University of Basel for their cooperation with Faber et al. (2017)."}, {"heading": "10. Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10.1. Interpretation of Laplacian Based Models as MPNNs", "text": "Another family of models defined in Defferrard et al. (2016), Bruna et al. (2013), Kipf & Welling (2016) can be interpreted as MPNNs. These models generalize the notion of convolutions applied to images as convolution to a generic GwithN graphical node and are typically applied to much larger graphs (w w w w w w w w w). We closely follow the notation defined in Bruna et al. (2013) Equation (3.2). Let V define the eigenvectors of L, ordered by eigenvalue. Let us define a real weighted nonlinearity (such as ReLU). We will now define an operation that transforms an input vector x of size N x d1 into a vector of L, ordered by eigenvalue."}, {"heading": "10.2. The special case of Kipf & Welling (2016)", "text": "Motivated as an approximation of the first order of the diagram, Kipf & Welling (2016) propose the following layer-by-layer propagation rule: H l + 1 = \u03c3 (D-1 / 2A-1 / 2H-lW l) (7) Here A-A + IN, where A is the real adjacence matrix for an undirected diagram G. The addition of the identity matrix IN corresponds to the addition of self-loops to the diagram. D-ii = \u2211 j A-ij also denotes the degree matrix for the diagram with self-loops, W l-RD-D-D is a layer-specific, detectable weight matrix (\u00b7) representing a real nonlinearity. Each H l is an RN-D dimensional matrix indicating the state of the D measurement node (W-1)."}, {"heading": "10.3. A More Detailed Description of the Quantum Properties", "text": "First, there are four variables related to the atomisation energy. \u2022 Atomisation energy at 0K U0 (eV): This is the energy required to break down the molecule into all its components. \u2022 This calculation assumes that the molecules are kept at room temperature H (eV). \u2022 Atomisation energy at room temperature U (eV): Like U0, this is the energy required to break up the molecule when it is at room temperature. \u2022 Enthalpy of atomisation at room temperature H (eV): The enthalpy of atomisation is similar in spirit to the energy of atomisation, U. In contrast to the energy of this calculation, however, it is assumed that the constituent molecules are kept at fixed pressure. \u2022 Free energy of atomisation G (eV): Again, this is similar to U and H, but assumes that the system is kept at fixed temperature and pressure."}, {"heading": "10.4. Additional Results", "text": "In Table 5 we compare the performance of several models formed without spatial information. In the left 4 columns we show the results of 4 experiments, one in which we train the GG-NN model on the sparse diagram, one in which we add virtual edges (ve), one in which we add a master node (mn), and one in which we change the output of the diagram to a set2set output (s2s). Generally, we find it important to allow the model to record extensive interactions in these diagrams. In Table 6 we compare GG-NN + towers + set2set output (tow8) against a baseline GG-NN + set2set output (GG-NN) when using distance tanks. We make this comparison both in the common training regime (j) and in the training of one model per target (i). For the common training of the baseline we used 100 experiments with 200 randomised trials, as well as in which d was selected."}], "references": [{"title": "Interaction networks for learning about objects, relations and physics", "author": ["Battaglia", "Peter", "Pascanu", "Razvan", "Lai", "Matthew", "Rezende", "Danilo Jimenez", "Kavukcuoglu", "Koray"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Battaglia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Battaglia et al\\.", "year": 2016}, {"title": "Density-functional thermochemistry. iii. the role of exact exchange", "author": ["Becke", "Axel D"], "venue": "The Journal of Chemical Physics,", "citeRegEx": "Becke and D.,? \\Q1993\\E", "shortCiteRegEx": "Becke and D.", "year": 1993}, {"title": "Generalized neural-network representation of high-dimensional potential-energy surfaces", "author": ["Behler", "J\u00f6rg", "Parrinello", "Michele"], "venue": "Phys. Rev. Lett.,", "citeRegEx": "Behler et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Behler et al\\.", "year": 2007}, {"title": "Spectral networks and locally connected networks on graphs", "author": ["Bruna", "Joan", "Zaremba", "Wojciech", "Szlam", "Arthur", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1312.6203,", "citeRegEx": "Bruna et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bruna et al\\.", "year": 2013}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Convolutional neural networks on graphs with fast localized spectral filtering", "author": ["Defferrard", "Micha\u00ebl", "Bresson", "Xavier", "Vandergheynst", "Pierre"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Defferrard et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Defferrard et al\\.", "year": 2016}, {"title": "Inhomogeneous electron gas", "author": ["P. Hohenberg", "W. Kohn"], "venue": null, "citeRegEx": "Hohenberg and Kohn,? \\Q2012\\E", "shortCiteRegEx": "Hohenberg and Kohn", "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Semi-Supervised Classification with Graph Convolutional Networks", "author": ["T.N. Kipf", "M. Welling"], "venue": "ArXiv eprints,", "citeRegEx": "Kipf and Welling,? \\Q2016\\E", "shortCiteRegEx": "Kipf and Welling", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gated graph sequence neural networks", "author": ["Li", "Yujia", "Tarlow", "Daniel", "Brockschmidt", "Marc", "Zemel", "Richard"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Deep architectures and deep learning in chemoinformatics: the prediction of aqueous solubility for drug-like molecules", "author": ["Lusci", "Alessandro", "Pollastri", "Gianluca", "Baldi", "Pierre"], "venue": "Journal of chemical information and modeling,", "citeRegEx": "Lusci et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lusci et al\\.", "year": 2013}, {"title": "The more you know: Using knowledge graphs for image classification", "author": ["Marino", "Kenneth", "Salakhutdinov", "Ruslan", "Gupta", "Abhinav"], "venue": "arXiv preprint arXiv:1612.04844,", "citeRegEx": "Marino et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Marino et al\\.", "year": 2016}, {"title": "Automatic generation of complementary descriptors with molecular graph networks", "author": ["Merkwirth", "Christian", "Lengauer", "Thomas"], "venue": "Journal of chemical information and modeling,", "citeRegEx": "Merkwirth et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Merkwirth et al\\.", "year": 2005}, {"title": "Neural network for graphs: A contextual constructive approach", "author": ["Micheli", "Alessio"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Micheli and Alessio.,? \\Q2009\\E", "shortCiteRegEx": "Micheli and Alessio.", "year": 2009}, {"title": "Learning convolutional neural networks for graphs", "author": ["Niepert", "Mathias", "Ahmed", "Mohamed", "Kutzkov", "Konstantin"], "venue": "In Proceedings of the 33rd annual international conference on machine learning", "citeRegEx": "Niepert et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Niepert et al\\.", "year": 2016}, {"title": "Quantum chemistry structures and properties of 134 kilo molecules", "author": ["Ramakrishnan", "Raghunathan", "Dral", "Pavlo O", "Rupp", "Matthias", "Von Lilienfeld", "O Anatole"], "venue": "Scientific data,", "citeRegEx": "Ramakrishnan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ramakrishnan et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": ", 2012), and infer features from real-world images and videos (Krizhevsky et al., 2012).", "startOffset": 62, "endOffset": 87}, {"referenceID": 11, "context": "While neural networks have been applied in a variety of situations (Merkwirth & Lengauer, 2005; Micheli, 2009; Lusci et al., 2013; Duvenaud et al., 2015), they have yet to become widely adopted.", "startOffset": 67, "endOffset": 153}, {"referenceID": 16, "context": "Here we focus on the QM9 dataset as such a benchmark (Ramakrishnan et al., 2014).", "startOffset": 53, "endOffset": 80}, {"referenceID": 9, "context": "Indeed convolutional neural networks existed for decades before careful empirical work applying them to image classification (Krizhevsky et al., 2012) helped them displace SVMs on top of handengineered features for a host of computer vision problems.", "startOffset": 125, "endOffset": 150}, {"referenceID": 10, "context": "There are at least seven notable examples of models from the literature, namely (Duvenaud et al., 2015; Kearnes et al., 2016; Li et al., 2016; Battaglia et al., 2016), that we can describe using our Message Passing Neural Networks (MPNN) framework.", "startOffset": 80, "endOffset": 166}, {"referenceID": 0, "context": "There are at least seven notable examples of models from the literature, namely (Duvenaud et al., 2015; Kearnes et al., 2016; Li et al., 2016; Battaglia et al., 2016), that we can describe using our Message Passing Neural Networks (MPNN) framework.", "startOffset": 80, "endOffset": 166}, {"referenceID": 10, "context": "Gated Graph Neural Networks (GG-NN), Li et al. (2016)", "startOffset": 37, "endOffset": 54}, {"referenceID": 4, "context": "The update function is Ut = GRU(hv,m t+1 v ), where GRU is the Gated Recurrent Unit introduced in Cho et al. (2014). This work used weight tying, so the same update function is used at each time step t.", "startOffset": 98, "endOffset": 116}, {"referenceID": 0, "context": "Interaction Networks, Battaglia et al. (2016)", "startOffset": 22, "endOffset": 46}, {"referenceID": 3, "context": "Laplacian Based Methods, Bruna et al. (2013); Defferrard et al.", "startOffset": 25, "endOffset": 45}, {"referenceID": 3, "context": "Laplacian Based Methods, Bruna et al. (2013); Defferrard et al. (2016); Kipf & Welling (2016)", "startOffset": 25, "endOffset": 71}, {"referenceID": 3, "context": "Laplacian Based Methods, Bruna et al. (2013); Defferrard et al. (2016); Kipf & Welling (2016)", "startOffset": 25, "endOffset": 94}, {"referenceID": 3, "context": "For example, the operations defined in Bruna et al. (2013); Defferrard et al.", "startOffset": 39, "endOffset": 59}, {"referenceID": 3, "context": "For example, the operations defined in Bruna et al. (2013); Defferrard et al. (2016) result in message functions of the form Mt(hv, h t w) = C t vwh t w, where the matrices C vw are parameterized by the eigenvectors of the graph laplacian L, and the learned parameters of the model.", "startOffset": 39, "endOffset": 85}, {"referenceID": 12, "context": "Recent work has adapted the GG-NN architecture to larger graphs by passing messages on only subsets of the graph at each time step (Marino et al., 2016).", "startOffset": 131, "endOffset": 152}, {"referenceID": 15, "context": "Examples in this family include Niepert et al. (2016) and Rupp et al.", "startOffset": 32, "endOffset": 54}, {"referenceID": 15, "context": "Examples in this family include Niepert et al. (2016) and Rupp et al. (2012). Finally Scarselli et al.", "startOffset": 32, "endOffset": 77}, {"referenceID": 15, "context": "Examples in this family include Niepert et al. (2016) and Rupp et al. (2012). Finally Scarselli et al. (2009) define a message passing process on graphs which is run until convergence, instead of for a finite number of time steps as in MPNNs.", "startOffset": 32, "endOffset": 110}, {"referenceID": 16, "context": "To investigate the success of MPNNs on predicting chemical properties, we use the publicly available QM9 dataset (Ramakrishnan et al., 2014).", "startOffset": 113, "endOffset": 140}, {"referenceID": 4, "context": "All of our experiments used weight tying at each time step t, and a GRU (Cho et al., 2014) for the update function as in the GG-NN family.", "startOffset": 72, "endOffset": 90}, {"referenceID": 9, "context": "Our implementation of MPNNs in general operates on directed graphs with a separate message channel for incoming and outgoing edges, in which case the incoming message mv is the concatenation of min v and m out v , this was also used in Li et al. (2016). When we apply this to undirected chemical graphs we treat the graph as directed, where each original edge becomes both an incoming and outgoing edge with the same label.", "startOffset": 236, "endOffset": 253}, {"referenceID": 0, "context": "Thus we also tried using a variant on the message function as described in (Battaglia et al., 2016).", "startOffset": 75, "endOffset": 99}, {"referenceID": 10, "context": "(2016), and the original GG-NN model Li et al. (2016) trained with distance bins.", "startOffset": 37, "endOffset": 54}], "year": 2017, "abstractText": "Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation function to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark, results we believe are strong enough to justify retiring this benchmark.", "creator": "LaTeX with hyperref package"}}}