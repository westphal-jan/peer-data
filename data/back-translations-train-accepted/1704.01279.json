{"id": "1704.01279", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2017", "title": "Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders", "abstract": "Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.", "histories": [["v1", "Wed, 5 Apr 2017 06:34:22 GMT  (4465kb,D)", "http://arxiv.org/abs/1704.01279v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.SD", "authors": ["jesse engel", "cinjon resnick", "adam roberts", "sander dieleman", "mohammad norouzi", "douglas eck", "karen simonyan"], "accepted": true, "id": "1704.01279"}, "pdf": {"name": "1704.01279.pdf", "metadata": {"source": "META", "title": "Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders", "authors": ["Jesse Engel", "Cinjon Resnick", "Adam Roberts", "Sander Dieleman", "Douglas Eck", "Karen Simonyan", "Mohammad Norouzi"], "emails": ["<jesseengel@google.com>."], "sections": [{"heading": "1. Introduction", "text": "It is important for a wide range of applications, including text-to-speech (TTS) systems and music generation from 1973. Audio generation algorithms known as vocoders in TTS and synthesizers in music have a long history of being, of accepting control signals such as \"pitch,\" \"see,\" \"see,\" \"see,\" \"see,\" \"see,\" \"\" see, \"\" \"see,\" \"\" see, \"\" \"see,\" \"\" \"see,\" \"\" \"\" see, \"\" \"\" \"\", \"\" \"\" \"\" \"\", \"\" \"\" \"\" \",\" \"\" \"\" \",\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \",\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\" \",\" \"\" \"\" \"\", \"\" \"\" \"\" \"\", \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\""}, {"heading": "2. Models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. WaveNet Autoencoder", "text": "In fact, it is a way in which people are able to determine for themselves how they want to behave."}, {"heading": "2.2. Baseline: Spectral Autoencoder", "text": "As a point of comparison, we have created a simple but strong baseline for our neural audio synthesis experiments. Inspired by image models (Vincent et al., 2010), we examine Convolutionary Autoencoder Structures with a bottleneck that forces the model to find a compressed representation for an entire note. Figure 1a shows a block diagram of our basic architecture. The Convolutionary Encoder and Decoder are each 10 layers deep with 2x2 steps and 4x4 cores. Each layer is followed by a leaky ReLU (0.1) Nonlinearity and Batch Normalization (Ioffe & Szegedy, 2015). The number of channels grows from 128 to 1024 before a linear fully connected layer adds a single 19841 dimensional hidden vector (Z) to match that with WaveNet autoencoding."}, {"heading": "2.3. Training", "text": "We train all models with stochastic gradient with an Adam Optimizer (Kingma & Ba, 2014). Base models typically use a learning rate of 1e-4, while WaveNet models use a schedule starting at 2e-4 and descending to 6e-5, 2e-5 and 6e-6 for iterations of 120k, 180k and 240k respectively. Base models train asynchronously for 1800k iterations with a batch size of 8. WaveNet models train synchronously for 250k iterations with a batch size of 32."}, {"heading": "3. The NSynth Dataset", "text": "In order to evaluate our WaveNet autoencoder model, we wanted an audio dataset that would allow us to examine the learned embeddings. Music notes are an ideal framework for this study, as we assume that the embeddings capture structure such as pitch, dynamics, and timbre. While there are currently several smaller datasets (Goto et al., 2003; Romani Picas et al., 2015), deep networks train better on abundant, high-quality data, which motivates the development of a new dataset."}, {"heading": "3.1. A Dataset of Musical Notes", "text": "NSynth consists of 306,043 notes, each with a unique pitch, timbre and envelope. For 1006 instruments from commercial sample libraries, we created four second, 16 kHz monophonic audio snippets, known as notes, varying over each pitch of a standard MIDI piano (21-108) and five different Velocities 2 (25, 50, 75, 100, 127), which were held for the first three seconds and decayed at the last second. Some instruments are unable to control 2MIDI velocity, and they have a direct relationship. For physical intuition, higher velocity equals pressing a piano key harder. All 88 pitches in this range are produced, resulting in an average of 65.4 pitches per instrument. In addition, commercial sample packs occasionally contain double sounds across multiple velocities, resulting in an average of 4.75 unique velocities per pitch."}, {"heading": "3.2. Annotations", "text": "We also commented on each note with three additional pieces of information based on a combination of human evaluation and heuristic algorithms: \u2022 Source: The method of sound generation for the instrument of the note. This can be an \"acoustic\" or \"electronic\" one for instruments recorded by acoustic or electronic instruments, or \"synthetic\" one for synthetic instruments. \u2022 Family: The high-level family to which the instrument of the note belongs. Each instrument belongs to exactly one family. See Annex for the full list. \u2022 Properties: Sound qualities of the note. See Annex for the full list of classes and their ancillary occurrences. Each note is provided with zero or more qualities."}, {"heading": "3.2.1. Availability", "text": "The full NSynth dataset can be downloaded from https: / / magenta.tensorflow.org / datasets / nsynth as TFRecord files split into training and holdout sets. Each note is represented by a serialized TensorFlow Example protocol buffer containing the note and annotations. See README for details on the format."}, {"heading": "4. Evaluation", "text": "We evaluate and analyze our models based on the tasks of note reconstruction, instrument interpolation and pitch interpolation. Audio is notoriously difficult to visualize. Magnitude spectrograms capture many aspects of a signal for analysis, but two spectrograms that seem very similar to the eye can match audio that differs drastically due to phase differences. We have included complementary audio examples for each plot and encourage the reader to listen to how they read. That is, in our analysis we present examples as diagrams of constant q transformation (CQT) (Brown, 1991), which is useful because it is immutable to changes in the fundamental frequency."}, {"heading": "4.1. Reconstruction", "text": "This year, it has come to the point that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "4.1.1. Quantitative Comparison", "text": "Inspired by the use of the Inception Score for images (Salimans et al., 2016), we train a multifunctional classification network to perform a quantitative comparison of model reconstructions by predicting pitches and quality markers on the NSynth dataset (details in the appendix).The network configuration is the same as the base encoder and tests are performed on reconstructions of a randomly selected subset of 4,096 examples from the set provided. The results in Table 1 confirm our qualifying observation that the WaveNet reconstructions are of superior quality.The classifier extracts pitches from the reconstructed WaveNet samples about 70% more successfully than the baseline and is a few points higher for predicting qualitative information, giving an accuracy roughly equivalent to the original audio quality."}, {"heading": "4.2. Interpolation in Timbre and Dynamics", "text": "Given the limited factors of variation in the dataset, we know that successful embedding in space (Z) should include the range of timbre and dynamics in its reconstructions. In Figure 3, we show reconstructions of linear interpolations (0.5: 0.5) in space Z and compare them with interpolations in space. The latter are simple superpositions of the individual instruments in their rainbow diagrams. This is, in perspective, equivalent to the two instruments played simultaneously. In contrast, we find that the generative models merge aspects of the instruments."}, {"heading": "4.3. Entanglement of Pitch and Timbre", "text": "By conditioning on pitch during training, we assume that we should be able to generate multiple pitches from a single Z vector that preserve the identity of timbre and dynamics. Our initial attempts were unsuccessful, as our models seem to have learned to ignore the conditioning variable."}, {"heading": "4.3.1. Pitch Classification from Z", "text": "One way to study the interweaving of pitch and Z is to consider the accuracy of pitch classification of embedding. If the training with pitch conditioning unravels the representation of pitch and timbre, then we expect a linear pitch classifier to be trained on the embedding to reduce accuracy. To test this, we train a series of autoencoder models with different embedding sizes, both with and without pitch conditioning. For each model, we then train a logistic regression classifier on its embedding and test it on a random sample of 4,096 embedded embedding sizes. The first two lines of Table 2 show that the embedding sizes and WaveNet models decrease the classification accuracy by 13-30% when adding pitch conditioning during training. In addition, this is an indication of a decreased timing of 12 and therefore a significant presence of pitch information in the 5latency code."}, {"heading": "4.3.2. Z Correlation across Pitch", "text": "Figure 5 shows correlations for several instruments over their entire 88-tone range at velocity 127. We see that each instrument has a unique division into two or more stops in which notes of different pitches have similar embeddings. Even the average across all instruments shows a broad distinction between high and low stops. On reflection, this is not surprising, as the sound and dynamics of an instrument can vary dramatically in its range."}, {"heading": "4.4. Generalization of Temporal Encodings", "text": "The WaveNet autoencoder model has some unique features that allow it to generalize situations that are not included in the dataset. As the model learns embedding that distorts an auto-regressive decoder, it effectively acts as a \"driving force\" for a nonlinear oscillator / infinite impulse response filter. This is illustrated by Figure 6, where the embedding follows a size contour similar to that of the rainbow diagrams of their corresponding sounds in Figures 2 and 3. In addition, the embedding, similar to a spectrogram, only captures a local context, generating them over time. So far, the model has only seen single notes with a tone lasting up to three seconds, and yet Figure 7 shows that it can successfully reconstruct an entire series of notes, as well as notes that are played for more than three seconds. While the WaveNet autoencoder model has seen only single notes with a tone lasting up to three seconds, it clearly follows the original timbre of the important organ's harmonic, as it adds several octaves between the basic octaves, while clearly following two notes of the basic frequency."}, {"heading": "5. Conclusion and Future Directions", "text": "In this paper, we presented a WaveNet autoencoder model that captures long-term structures without external conditioning and its effectiveness using the new NSynth dataset for generative audio modeling. The WaveNet autoencoder we describe is a powerful representation for which there are still several ways to explore. It builds on the fine-grained local understanding of the original WaveNet work and provides access to a useful hidden space. However, due to memory constraints, it is unable to fully grasp the global context. In fact, overcoming this constraint is an important open problem. NSynth was inspired by image recognition datasets that were critical to recent advances in deep learning. Similar to how many image datasets focus on a single object per example, NSynth sharpens the dynamics with respect to a single note. In fact, much of modern music production uses such factorization, using MIDI timers and music synthesizer sequences."}, {"heading": "A. Phase Representation for the Baseline Model", "text": "Each representation uses MSE costs and always includes the order of magnitude of the STFT spectrogram. We found that the training of the maximum normalized log magnitude of the power spectrum correlates better with perceptual distortions. When using the phase in the lens, we regress back to the phase angle. We can assume a normal circular distribution (Bishop, 2006) for the phase with a log probability loss proportional to cos. Figure 8 shows CQT spectrograms of reconstructions of a trumpet sound from models trained on each input representation. We also include audio of each reconstruction, which is essential to hear the improvement in perceptible weighting."}, {"heading": "B. Description of Quality Tags", "text": "None of the tags are by definition mutually exclusive, with the exception of light and dark. However, it is possible that a note is neither light nor dark. \u2022 Bright: A large amount of high frequency content and strong overtones. \u2022 Dark: a distinct lack of high frequency content, resulting in a muted and bass sound. \u2022 Distortion: waveshaping, which produces a distinctively crisp sound and the presence of many overtones. Sometimes paired with non-harmonic noise. \u2022 Fast drop: amplitude envelope of all overtones disintegrates well before the \"note-off\" point at 3 seconds. \u2022 Long release: amplitude envelope disintegrates slowly after the \"note-off\" point, sometimes even at the end of the sample at 4 seconds."}, {"heading": "C. Details of Pitch and Quality Classifier", "text": "We use the encoder structure of the base model, except that there is no bottleneck (see Figure 10). We use a Softmax crossentropy loss for the pitch labels as they are mutually exclusive and a Sigmoid crossentropy loss for the quality labels as they are not. As the architecture uses only size spectra, it cannot take advantage of the improved phase coherence of the WaveNet samples."}], "references": [{"title": "The million song dataset", "author": ["Bertin-Mahieux", "Thierry", "Ellis", "Daniel PW", "Whitman", "Brian", "Lamere", "Paul"], "venue": "In ISMIR,", "citeRegEx": "Bertin.Mahieux et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bertin.Mahieux et al\\.", "year": 2011}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Bishop", "Christopher M"], "venue": null, "citeRegEx": "Bishop and M.,? \\Q2006\\E", "shortCiteRegEx": "Bishop and M.", "year": 2006}, {"title": "Estimating and interpreting the instantaneous frequency of a signal", "author": ["Boashash", "Boualem"], "venue": "i. fundamentals. Proceedings of the IEEE,", "citeRegEx": "Boashash and Boualem.,? \\Q1992\\E", "shortCiteRegEx": "Boashash and Boualem.", "year": 1992}, {"title": "Calculation of a constant q spectral transform", "author": ["Brown", "Judith C"], "venue": "The Journal of the Acoustical Society of America,", "citeRegEx": "Brown and C.,? \\Q1991\\E", "shortCiteRegEx": "Brown and C.", "year": 1991}, {"title": "Variational lossy autoencoder", "author": ["Chen", "Xi", "Kingma", "Diederik P", "Salimans", "Tim", "Duan", "Yan", "Dhariwal", "Prafulla", "Schulman", "John", "Sutskever", "Ilya", "Abbeel", "Pieter"], "venue": "CoRR, abs/1611.02731,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "The synthesis of complex audio spectra by means of frequency modulation", "author": ["Chowning", "John M"], "venue": "Journal of the audio engineering society,", "citeRegEx": "Chowning and M.,? \\Q1973\\E", "shortCiteRegEx": "Chowning and M.", "year": 1973}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. Fei-Fei"], "venue": "In CVPR09,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Rwc music database: Music genre database and musical instrument sound database", "author": ["Goto", "Masataka", "Hashiguchi", "Hiroki", "Nishimura", "Takuichi", "Oka", "Ryuichi"], "venue": null, "citeRegEx": "Goto et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Goto et al\\.", "year": 2003}, {"title": "Signal estimation from modified short-time fourier transform", "author": ["Griffin", "Daniel", "Lim", "Jae"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Griffin et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Griffin et al\\.", "year": 1984}, {"title": "Pixelvae: A latent variable model for natural images", "author": ["Gulrajani", "Ishaan", "Kumar", "Kundan", "Ahmed", "Faruk", "Taiga", "Adrien Ali", "Visin", "Francesco", "V\u00e1zquez", "David", "Courville", "Aaron C"], "venue": "CoRR, abs/1611.05013,", "citeRegEx": "Gulrajani et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gulrajani et al\\.", "year": 2016}, {"title": "Minst, a collection of musical sound datasets, 2016", "author": ["Humphrey", "Eric J"], "venue": "URL https://github.com/ ejhumphrey/minst-dataset/", "citeRegEx": "Humphrey and J.,? \\Q2016\\E", "shortCiteRegEx": "Humphrey and J.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "shift. CoRR,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "The blizzard challenge", "author": ["King", "Simon", "Clark", "Robert AJ", "Mayo", "Catherine", "Karaiskos", "Vasilis"], "venue": null, "citeRegEx": "King et al\\.,? \\Q2008\\E", "shortCiteRegEx": "King et al\\.", "year": 2008}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher JC"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Sound texture synthesis via filter statistics", "author": ["McDermott", "Josh H", "Oxenham", "Andrew J", "Simoncelli", "Eero P"], "venue": "In Applications of Signal Processing to Audio and Acoustics,", "citeRegEx": "McDermott et al\\.,? \\Q2009\\E", "shortCiteRegEx": "McDermott et al\\.", "year": 2009}, {"title": "Samplernn: An unconditional end-to-end neural audio generation model", "author": ["Mehri", "Soroush", "Kumar", "Kundan", "Gulrajani", "Ishaan", "Rithesh", "Jain", "Shubham", "Sotelo", "Jose", "Courville", "Aaron C", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Mehri et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mehri et al\\.", "year": 2016}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Analog days: The invention and impact of the Moog synthesizer", "author": ["Pinch", "Trevor J", "Trocco", "Frank", "TJ"], "venue": null, "citeRegEx": "Pinch et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pinch et al\\.", "year": 2009}, {"title": "Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching", "author": ["Raffel", "Colin"], "venue": "PhD thesis, COLUMBIA UNIVERSITY,", "citeRegEx": "Raffel and Colin.,? \\Q2016\\E", "shortCiteRegEx": "Raffel and Colin.", "year": 2016}, {"title": "Improved techniques for training gans", "author": ["Salimans", "Tim", "Goodfellow", "Ian J", "Zaremba", "Wojciech", "Cheung", "Vicki", "Radford", "Alec", "Chen", "Xi"], "venue": "CoRR, abs/1606.03498,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Musical audio synthesis using autoencoding neural nets", "author": ["Sarroff", "Andy M", "Casey", "Michael A"], "venue": "In ICMC,", "citeRegEx": "Sarroff et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sarroff et al\\.", "year": 2014}, {"title": "A note on the evaluation of generative models", "author": ["Theis", "Lucas", "Oord", "A\u00e4ron van den", "Bethge", "Matthias"], "venue": "arXiv preprint arXiv:1511.01844,", "citeRegEx": "Theis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2015}, {"title": "Learning features of music from scratch", "author": ["Thickstun", "John", "Harchaoui", "Zaid", "Kakade", "Sham"], "venue": "In preprint, https: // arxiv. org/ abs/ 1611", "citeRegEx": "Thickstun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Thickstun et al\\.", "year": 2016}, {"title": "Wavenet: A generative model for raw audio", "author": ["van den Oord", "A\u00e4ron", "Dieleman", "Sander", "Zen", "Heiga", "Simonyan", "Karen", "Vinyals", "Oriol", "Graves", "Alex", "Kalchbrenner", "Nal", "Senior", "Andrew W", "Kavukcuoglu", "Koray"], "venue": null, "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Pixel recurrent neural networks. CoRR, abs/1601.06759, 2016b", "author": ["van den Oord", "A\u00e4ron", "Kalchbrenner", "Nal", "Kavukcuoglu", "Koray"], "venue": "URL http: //arxiv.org/abs/1601.06759", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Sampling generative networks: Notes on a few effective techniques", "author": ["White", "Tom"], "venue": "CoRR, abs/1609.04468,", "citeRegEx": "White and Tom.,? \\Q2016\\E", "shortCiteRegEx": "White and Tom.", "year": 2016}], "referenceMentions": [{"referenceID": 21, "context": "Synthesizers have a long history of being hand-designed instruments, accepting control signals such as \u2018pitch\u2019, \u2018velocity\u2019, and filter parameters to shape the tone, timbre, and dynamics of a sound (Pinch et al., 2009).", "startOffset": 197, "endOffset": 217}, {"referenceID": 19, "context": ", 2016a) and SampleRNN (Mehri et al., 2016).", "startOffset": 23, "endOffset": 43}, {"referenceID": 7, "context": "Recent breakthroughs in generative modeling of images (Kingma & Welling, 2013; Goodfellow et al., 2014; van den Oord et al., 2016b) have been predicated on ar X iv :1 70 4.", "startOffset": 54, "endOffset": 131}, {"referenceID": 17, "context": "the availability of high-quality and large-scale datasets such as MNIST (LeCun et al., 1998), SVHN (Netzer et al.", "startOffset": 72, "endOffset": 92}, {"referenceID": 20, "context": ", 1998), SVHN (Netzer et al., 2011), CIFAR (Krizhevsky & Hinton, 2009) and ImageNet (Deng et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 6, "context": ", 2011), CIFAR (Krizhevsky & Hinton, 2009) and ImageNet (Deng et al., 2009).", "startOffset": 56, "endOffset": 75}, {"referenceID": 25, "context": "While generative models are notoriously hard to evaluate (Theis et al., 2015), these datasets provide a common test bed for consistent qualitative and quantitative evaluation, such as with the use of the Inception score (Salimans et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 23, "context": ", 2015), these datasets provide a common test bed for consistent qualitative and quantitative evaluation, such as with the use of the Inception score (Salimans et al., 2016).", "startOffset": 150, "endOffset": 173}, {"referenceID": 0, "context": "Audio signals found in the wild contain multi-scale dependencies that prove particularly difficult to model (Raffel, 2016; Bertin-Mahieux et al., 2011; King et al., 2008; Thickstun et al., 2016), leading many previous efforts at data-driven audio synthesis to focus on more constrained domains such as texture synthesis or training small parametric models (Sarroff & Casey, 2014; McDermott et al.", "startOffset": 108, "endOffset": 194}, {"referenceID": 13, "context": "Audio signals found in the wild contain multi-scale dependencies that prove particularly difficult to model (Raffel, 2016; Bertin-Mahieux et al., 2011; King et al., 2008; Thickstun et al., 2016), leading many previous efforts at data-driven audio synthesis to focus on more constrained domains such as texture synthesis or training small parametric models (Sarroff & Casey, 2014; McDermott et al.", "startOffset": 108, "endOffset": 194}, {"referenceID": 26, "context": "Audio signals found in the wild contain multi-scale dependencies that prove particularly difficult to model (Raffel, 2016; Bertin-Mahieux et al., 2011; King et al., 2008; Thickstun et al., 2016), leading many previous efforts at data-driven audio synthesis to focus on more constrained domains such as texture synthesis or training small parametric models (Sarroff & Casey, 2014; McDermott et al.", "startOffset": 108, "endOffset": 194}, {"referenceID": 18, "context": ", 2016), leading many previous efforts at data-driven audio synthesis to focus on more constrained domains such as texture synthesis or training small parametric models (Sarroff & Casey, 2014; McDermott et al., 2009).", "startOffset": 169, "endOffset": 216}, {"referenceID": 10, "context": "We could parameterize Z as a latent variable p(Z|x) that we would have to marginalize over (Gulrajani et al., 2016), but in practice we have found this to be less effective.", "startOffset": 91, "endOffset": 115}, {"referenceID": 4, "context": "As discussed in (Chen et al., 2016), this may be due to the decoder being so powerful that it can ignore the latent variables unless they encode a much larger context that\u2019s otherwise inaccessible.", "startOffset": 16, "endOffset": 35}, {"referenceID": 8, "context": "While several smaller datasets currently exist (Goto et al., 2003; Romani Picas et al., 2015), deep networks train better on abundant, high-quality data, motivating the development of a new dataset.", "startOffset": 47, "endOffset": 93}, {"referenceID": 23, "context": "Inspired by the use of the Inception Score for images (Salimans et al., 2016), we train a multi-task classification network to perform a quantitative comparison of the model reconstructions by predicting pitch and quality labels on the NSynth dataset (details in the Appendix).", "startOffset": 54, "endOffset": 77}], "year": 2017, "abstractText": "Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.", "creator": "LaTeX with hyperref package"}}}