{"id": "1704.01691", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2017", "title": "Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction", "abstract": "Labeled sequence transduction is a task of transforming one sequence into another sequence that satisfies desiderata specified by a set of labels. In this paper we propose multi-space variational encoder-decoders, a new model for labeled sequence transduction with semi-supervised learning. The generative model can use neural networks to handle both discrete and continuous latent variables to exploit various features of data. Experiments show that our model provides not only a powerful supervised framework but also can effectively take advantage of the unlabeled data. On the SIGMORPHON morphological inflection benchmark, our model outperforms single-model state-of-art results by a large margin for the majority of languages.", "histories": [["v1", "Thu, 6 Apr 2017 02:36:56 GMT  (863kb,D)", "https://arxiv.org/abs/1704.01691v1", "Accepted by ACL 2017"], ["v2", "Thu, 17 Aug 2017 03:22:23 GMT  (863kb,D)", "http://arxiv.org/abs/1704.01691v2", "Accepted by ACL 2017"]], "COMMENTS": "Accepted by ACL 2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["chunting zhou", "graham neubig"], "accepted": true, "id": "1704.01691"}, "pdf": {"name": "1704.01691.pdf", "metadata": {"source": "CRF", "title": "Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction", "authors": ["Chunting Zhou", "Graham Neubig"], "emails": ["ctzhou@cs.cmu.edu", "gneubig@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is so that most people who are able to survive themselves, to survive themselves and to survive themselves, must stand in the way of themselves and their fellow human beings. (...) Indeed, it is so that most people are able to survive themselves. (...) It is so that people are able to survive themselves. (...) \"It is so.\" (...) \"It is so.\" (...) \"It is so.\" (...) \"It is so.\" (...) \"It is so.\" (...) \"It is so.\" (...) \"It is so.\" (...) \"It is so.\" (...) \"It is so.\" (...). \"(...\" It. \"(...).\" It. \"(...).\" It. \"(...).\" It. \"(...\" It. \"(...).\" It. \"(...).\" It is so. \"(...\" It. \"It.\" (...). \"It.\" It. \"It.\" (... \").\" It. \"It.\" (... \"It.\"). \"It.\" It. \"(...\"). \"It.\" It. \"().\" It. \"It.\" (... \").\" (). \"It.\" (). \"It.\" (). \"It.\" (). \"It.\" (). \"(It.\" (... \").\" (It. \").\" (It. \").\" (It. \").\" (It. \"(It.\"). \"(It.\" (). \"(It.\"). \"(It.\"). \"(It.\" (It. \"). ().\" (It. \").\" (It. (It. (). \"). (It.\"). (It. (It. \").\" (). (It. \").\" (It. (It. \").\"). (. (It. (It. \").\"). \"(It. (It.\"). (. \").\"). (It. \"). (It. (It.\"). (It. \"). (It."}, {"heading": "2 Labeled Sequence Transduction", "text": "In this section, we will first present some notations relating to problems of sequence transduction in general, then describe a specific instance for morphological reflection. Notation: Sequence transduction problems described involve the transformation of a source sequence x (s) into a target sequence x (t), with some labels describing the respective transformation variety. We will use discrete variables y (t) 1, y (t) 2, \u00b7 \u00b7 \u00b7, y (t) K to denote the labels associated with each target sequence, where K is the total number of labels. Let us let y (t) = [y (t) 1, y (t) 2, \u00b7 \u00b7 \u00b7, y (t) K] denote a vector of this discrete variable. Each discrete variable y (t) k represents a categorical feature referring to the target sequence (t), and has a possible set of labels to denote this discrete variable."}, {"heading": "3 Proposed Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Preliminaries: Variational Autoencoder", "text": "As mentioned above, our proposed model uses probabilistic latent variables in a model based on neural networks; the variable autoencoder (Kingma and Welling, 2014) is an efficient method of handling (continuous) latent variables in neural models; we describe it briefly here, and interested readers can refer to Doersch (2016) for details; the UAE learns a generative model of the probability of p (x | z) observed data x using a latent variable z, and simultaneously uses a detection model q (z | x) at the time of learning to estimate z for a particular observation x (Fig. 2 (a); q (\u00b7) and p (\u00b7) are modeled using neural networks, each parameterized by a latent variable z, and these parameters are learned by maximizing the variable lower limit at the marginal log probability of the data (x)."}, {"heading": "3.2 Multi-space Variational Autoencoders", "text": "As an intermediate step to our full model, we next describe a generative model for a single sequence with both continuous and discrete latent variables, each with different distribution problems (MSVAE). MSVAEs are a combination of two threads from previous work: profound generative models with both continuous and discrete latent variables for classification problems (Kingma et al., 2014; Maal\u00f8e et al., 2016) and VAEs with sequential data only variables (Bowman et al., 2016; Chung et al., 2015; Zhang et al., 2016; Fabius et al., 2014; Bayer and Osendorfer, 2014). In MSVAEs, we have an observed sequence x, continuous variables such as the VAE, and discrete variables y.In the case of morphology, x can be interpreted as an inflected word."}, {"heading": "3.3 Multi-space Variational Encoder-Decoders", "text": "Finally, we will discuss the whole proposed method: the variable multi-space encoder decoder (MSVED) y (MSVED) y, which creates the target x (t) from source x (s) and labels y (t). Again, we will discuss two cases of this model: target sequence labels are observed and not observed. MSVED: The graphical model for the MSVED is given in Figure 2 (e). Since the labels of the target sequence are not observed, we will again treat them as discrete latent variables and draw conclusions from these labels (SVED). The generative process for the MSVED is very similar to the x \u2212 labeling method for the target sequence, with one important exception: While the standard MSVAE attaches x conditions to the detection model q (z | x), the MSVED generates the detection model x (s)."}, {"heading": "4 Learning MSVED", "text": "Now that we have outlined our overall model, we are discussing details of the learning process that contributes to its success."}, {"heading": "4.1 Learning Discrete Latent Variables", "text": "A challenge in forming our model is that it is not trivial to perform repropagation using discrete random variables, and therefore it is difficult to learn the recently proposed Gumbel Softmax trick (Maddison et al., 2014; Gumbel and Lieblein, 1954; Jang et al., 2017; J. et al., 2017) in models with discrete tags such as MSVAE or MSVED.4 to create a differentiated estimator for categorical variables. Gumbel Max Trick (Gumbel and Lieblein, 1954) offers an easy way to draw samples from a categorical distribution with class probabilities \u03c01, \u03c02, \u00b7 \u00b7 \u00b7 \u00b7 by using the argmax operation as follows: a hot (argmaxi [gi + log \u03c0i]), where g1, g2, \u00b7 \u00b7 \u00b7 \u00b7 are i.e. samples taken from the rubber distribution (0.1)."}, {"heading": "4.2 Learning Continuous Latent Variables", "text": "This means that the encoder must generate an informative representation z that encodes the contents of the x (s). However, the varying lower limit in our loss function is the KL divergence between the approximate rear q\u03c6 (z | x) and the previous p (z), which is relatively easy to learn in order to generate production from a latent representation. We observe that the KL costs quickly drop to almost zero, the setting of q\u03c6 (z | x) equals the standard normal distribution. (2014) Let's solve this problem by marginalizing all labels, but this is impracticable in our case where we have an exponential number of label combinations. (The Gumbel (0.1) distribution can be achieved by the first drawing of the u-uniform (0.1) and the calculation g = \u2212 (2)."}, {"heading": "5 Architecture for Morphological", "text": "We are testing three variants of our model, which was designed with different types of data and different loss functions. Firstly, the one-sided supervised model (SDSup) is exclusively supervised: only the target word from the predefined source word is supervised with the loss function Ll (x (t), y (s))) from Eq. 12. Secondly, the bidirectional parent model (BDSup) is trained in both directions: the decoding of the target word and the decoding of the target word corresponding to the loss function Ll (t) (t), y (s), x (s) + Lu (s) | x (t) with Eqs."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Background: SIGMORPHON 2016", "text": "SIGMORPHON 2016 is a common task for morphological diffraction across 10 different morphologically rich languages. There are a total of three tasks, the most difficult of which is task 3, in which the system will output the diffraction of a flexed word.6 The training data format in task 3 consists of three parts: (source word, target name, target word). During the test phase, the system is asked to generate the target word, specifying a source word and target names.For each task, there are a total of three tracks, divided by the amount of monitored data that can be used to solve the problem, with track 2 having the most severe limitation of using only data for the corresponding task. As this is an ideal test bed for our method, which can learn from unlabeled data, we select track 2 and task 3 to test our model's ability to utilize this data."}, {"heading": "6.2 Results and Analysis", "text": "From the results in Tab. 1, we can deduce a number of observations. First, by comparing the results of our complete semi-sup model, we can see that it achieves accuracies for all languages except Spanish better than the single MED system, often by a large margin. Even compared to the MED-like model, our single-model system is quite competitive in that it achieves higher accuracies for Hungarian, 6Task-1 is the diffraction of a lemma word and the task 2 is reflection, but also provides the source word Labels.Navajo, Maltese, and Arabic, as well as achieving average accuracies that the state-of-the-art. Next, by comparing the different varieties of our proposed models, we can see that the semi-supervised model consistently exceeds the bi-directional model for all languages."}, {"heading": "6.3 Analysis on Tag Attention", "text": "To analyze how the decoder takes into account the linguistic terms associated with the target word, we randomly select two words from the Arabic and Navajo test sentences and mark the attention weight in Fig. 5. The Arabic word \"al-'ima'ima bay, tiyya, tu\" is an adjective that means \"emirati,\" and its source word in the test data is \"'ima, ra, tiyyin\" 7. Both are declinations of \"ima, ra, tiyy.\" The source word is 7https: / / en.wiktionary.org / wiki /% D8% A5% D9% 85% D8% A7% B1% D8% A7% D8% AA% D9% 8Asingular, male, genial and indefinite, while the required diffraction is plural, female, nominative and unambiguous."}, {"heading": "6.4 Visualization of Latent Lemmas", "text": "In order to examine the learned latent representations, we visualize the z-vectors in this section and examine whether the latent space groups words with the same problem. Each sample in SIGMORPHON 2016 contains word and target words with the same problem. We perform a heuristic process to assign word pairs to groups that are likely to share a problem by grouping word pairs for which at least one of the words in each pair has a surface shape. This process is not error-free - errors can occur when multiple lemmats share the same surface shape - but in general, the groups will generally reflect Lemmata, except in those rare erroneous cases, so that we will call each of these groups a pseudo-Lemma.In Fig. 6, we randomly select 1,500 words from Maltese and visualize the continuous latent vectors of these words. We calculate the latent vectors as microreceptors (x) in the posterior representation form (Eq. 6), without adding the variance that belong to the two words assigned to the space."}, {"heading": "6.5 Analyzing Effects of Size of Unlabeled Data", "text": "In this section, we examine the extent to which the size of unlabeled data can help with performance. We process a German corpus from a 2017 Wikipedia dump and get more than 100,000 German words; these words are ranked in Wikipedia in order of frequency; the data contains some level of noise because we did not use special processing; we mix all unlabeled data from Wikipedia and the data provided in the common task used in previous experiments; and increase the number of unlabeled words used in learning by 10,000 each time, and finally use all unlabeled data (more than 150,000 words) to train the model. Fig. 7 shows that the performance of the test data improves as the amount of unlabeled data increases, implying that unlabeled learning continues to improve the model's ability to represent the latent, even if we add a loud word to a small, real, relatively large volume of data, which is the growth rate of most unlabeled data."}, {"heading": "6.6 Case Study on Reinflected Words", "text": "In Tab. 3, we examine some model outputs of the test data of the MED system or our model. It can be seen that most errors of MED and our models are due to either overcopies or undercopies of characters. In particular, we observe from the complete results that our model tends to be more aggressive in its changes, which leads to more complicated transformations, both successful (like \"ndammhomli\" in \"tindammhiex\" in Maltese) and unsuccessful (\"tqoz-z-z-x\" in \"qaz-z-ejtx\"). In contrast, the attention-oriented encoder model is more conservative in its changes, probably because it is less effective to learn an abstract representation of the problem and instead copies characters directly from the input."}, {"heading": "7 Conclusion and Future Work", "text": "In this paper, we propose a multi-space variation encoder decoder framework for the problem of labeled sequence transduction. MSVED does a good job of morphological reflection, surpasses the state of the art, and improves by adding external, unlabeled data. Future work will adapt this framework to other sequence transduction scenarios such as machine translation, dialogue generation, and question answering, where continuous and discrete latent variables can be abstracted to guide sequence generation."}, {"heading": "Acknowledgments", "text": "The authors thank Jiatao Gu, Xuezhe Ma, Zihang Dai and Pengcheng Yin for their helpful discussions. This work was partially supported by an Amazon Academic Research Award."}], "references": [{"title": "Ehu at the sigmorphon 2016 shared task", "author": ["I\u00f1aki Alegria", "Izaskun Etxeberria."], "venue": "a simple proposal: Grapheme-to-phoneme for inflection. In Proceedings of the 2016 Meeting of SIGMORPHON .", "citeRegEx": "Alegria and Etxeberria.,? 2016", "shortCiteRegEx": "Alegria and Etxeberria.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "The International Conference on Learning Representations .", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning stochastic recurrent networks", "author": ["Justin Bayer", "Christian Osendorfer."], "venue": "arXiv preprint arXiv:1411.7610 .", "citeRegEx": "Bayer and Osendorfer.,? 2014", "shortCiteRegEx": "Bayer and Osendorfer.", "year": 2014}, {"title": "Generating sentences from a continuous space", "author": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio."], "venue": "Proceedings of CoNLL .", "citeRegEx": "Bowman et al\\.,? 2016", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Translating into morphologically rich languages with synthetic phrases", "author": ["Victor Chahuneau", "Eva Schlinger", "Noah A Smith", "Chris Dyer."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Chahuneau et al\\.,? 2013", "shortCiteRegEx": "Chahuneau et al\\.", "year": 2013}, {"title": "A recurrent latent variable model for sequential data", "author": ["Junyoung Chung", "Kyle Kastner", "Laurent Dinh", "Kratarth Goel", "Aaron C Courville", "Yoshua Bengio."], "venue": "Advances in neural information processing systems. pages 2980\u20132988.", "citeRegEx": "Chung et al\\.,? 2015", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "The sigmorphon 2016 shared taskmorphological reinflection", "author": ["R. Cotterell", "C. Kirov", "J. Sylak-Glassman", "D. Yarowsky", "J. Eisner", "M. Hulden."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational", "citeRegEx": "Cotterell et al\\.,? 2016", "shortCiteRegEx": "Cotterell et al\\.", "year": 2016}, {"title": "Adapting morphology for arabic information retrieval", "author": ["Kareem Darwish", "Douglas W Oard."], "venue": "Arabic Computational Morphology, Springer, pages 245\u2013262.", "citeRegEx": "Darwish and Oard.,? 2007", "shortCiteRegEx": "Darwish and Oard.", "year": 2007}, {"title": "Tutorial on variational autoencoders", "author": ["Carl Doersch."], "venue": "arXiv preprint arXiv:1606.05908 .", "citeRegEx": "Doersch.,? 2016", "shortCiteRegEx": "Doersch.", "year": 2016}, {"title": "Supervised learning of complete morphological paradigms", "author": ["Greg Durrett", "John DeNero."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.", "citeRegEx": "Durrett and DeNero.,? 2013", "shortCiteRegEx": "Durrett and DeNero.", "year": 2013}, {"title": "Variational recurrent auto-encoders", "author": ["Otto Fabius", "Joost R van Amersfoort."], "venue": "arXiv preprint arXiv:1412.6581 .", "citeRegEx": "Fabius and Amersfoort.,? 2014", "shortCiteRegEx": "Fabius and Amersfoort.", "year": 2014}, {"title": "Morphological inflection generation using character sequence to sequence learning", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Graham Neubig", "Chris Dyer."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for", "citeRegEx": "Faruqui et al\\.,? 2016", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Statistical theory of extreme values and some practical applications: a series of lectures", "author": ["Emil Julius Gumbel", "Julius Lieblein."], "venue": "US Government Printing Office Washington .", "citeRegEx": "Gumbel and Lieblein.,? 1954", "shortCiteRegEx": "Gumbel and Lieblein.", "year": 1954}, {"title": "The concrete distribution: A continuous relaxation of discrete random variables", "author": ["Maddison Chris J.", "Andriy Mnih", "Yee Whye Teh."], "venue": "The International Conference on Learning Representations..", "citeRegEx": "J. et al\\.,? 2017", "shortCiteRegEx": "J. et al\\.", "year": 2017}, {"title": "Categorical reparameterization with gumbel-softmax", "author": ["Eric Jang", "Shixiang Gu", "Ben Poole."], "venue": "The International Conference on Learning Representations..", "citeRegEx": "Jang et al\\.,? 2017", "shortCiteRegEx": "Jang et al\\.", "year": 2017}, {"title": "Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation", "author": ["Melvin Johnson", "Mike Schuster", "Quoc V Le", "Maxim Krikun", "Yonghui Wu", "Zhifeng Chen", "Nikhil Thorat", "Fernanda Vi\u00e9gas", "Martin Wattenberg", "Greg Corrado"], "venue": null, "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Neural multi-source morphological reinflection", "author": ["Katharina Kann", "Ryan Cotterell", "Hinrich Sch\u00fctze."], "venue": "arXiv preprint arXiv:1612.06027 .", "citeRegEx": "Kann et al\\.,? 2016", "shortCiteRegEx": "Kann et al\\.", "year": 2016}, {"title": "Med: The lmu system for the sigmorphon 2016 shared task on morphological reinflection", "author": ["Katharina Kann", "Hinrich Sch\u00fctze."], "venue": "In Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Mor-", "citeRegEx": "Kann and Sch\u00fctze.,? 2016a", "shortCiteRegEx": "Kann and Sch\u00fctze.", "year": 2016}, {"title": "Singlemodel encoder-decoder with explicit morphological representation for reinflection", "author": ["Katharina Kann", "Hinrich Sch\u00fctze."], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Berlin, Germany.", "citeRegEx": "Kann and Sch\u00fctze.,? 2016b", "shortCiteRegEx": "Kann and Sch\u00fctze.", "year": 2016}, {"title": "Controlling output length in neural encoder-decoders", "author": ["Yuta Kikuchi", "Graham Neubig", "Ryohei Sasano", "Hiroya Takamura", "Manabu Okumura."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Associ-", "citeRegEx": "Kikuchi et al\\.,? 2016", "shortCiteRegEx": "Kikuchi et al\\.", "year": 2016}, {"title": "Semi-supervised learning with deep generative models", "author": ["Diederik P Kingma", "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling."], "venue": "Advances in Neural Information Processing Systems. Montr\u00e9al, Canada, pages 3581\u20133589.", "citeRegEx": "Kingma et al\\.,? 2014", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling."], "venue": "The International Conference on Learning Representations.", "citeRegEx": "Kingma and Welling.,? 2014", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "Semantic parsing with semi-supervised sequential autoencoders", "author": ["Tom\u00e1\u0161 Ko\u010disk\u1ef3", "G\u00e1bor Melis", "Edward Grefenstette", "Chris Dyer", "Wang Ling", "Phil Blunsom", "Karl Moritz Hermann."], "venue": "the 2016 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Ko\u010disk\u1ef3 et al\\.,? 2016", "shortCiteRegEx": "Ko\u010disk\u1ef3 et al\\.", "year": 2016}, {"title": "Auxiliary deep generative models", "author": ["Lars Maal\u00f8e", "Casper Kaae S\u00f8nderby", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther."], "venue": "Proceedings of the 33rd International Conference on Machine Learning .", "citeRegEx": "Maal\u00f8e et al\\.,? 2016", "shortCiteRegEx": "Maal\u00f8e et al\\.", "year": 2016}, {"title": "A* sampling", "author": ["Chris J Maddison", "Daniel Tarlow", "Tom Minka."], "venue": "Advances in Neural Information Processing Systems. pages 3086\u20133094.", "citeRegEx": "Maddison et al\\.,? 2014", "shortCiteRegEx": "Maddison et al\\.", "year": 2014}, {"title": "Language as a latent variable: Discrete generative models for sentence compression", "author": ["Yishu Miao", "Phil Blunsom."], "venue": "the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP) .", "citeRegEx": "Miao and Blunsom.,? 2016", "shortCiteRegEx": "Miao and Blunsom.", "year": 2016}, {"title": "Morphological reinflection via discriminative string transduction", "author": ["Garrett Nicolai", "Bradley Hauer", "Adam St. Arnaud", "Grzegorz Kondrak."], "venue": "Proceedings of the 2016 Meeting of SIGMORPHON .", "citeRegEx": "Nicolai et al\\.,? 2016", "shortCiteRegEx": "Nicolai et al\\.", "year": 2016}, {"title": "Morphological reinflection with convolutional neural networks", "author": ["Robert Ostling."], "venue": "Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology page 23.", "citeRegEx": "Ostling.,? 2016", "shortCiteRegEx": "Ostling.", "year": 2016}, {"title": "Controlling politeness in neural machine translation via side constraints", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 2016 Conference of The North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "The columbia university - new york university abu dhabi sigmorphon 2016 morphological reinflection shared task submission", "author": ["Dima Taji", "Ramy Eskander", "Nizar Habash", "Owen Rambow."], "venue": "Proceedings of the 2016 Meeting of SIGMORPHON .", "citeRegEx": "Taji et al\\.,? 2016", "shortCiteRegEx": "Taji et al\\.", "year": 2016}, {"title": "Multi-space probability distribution hmm", "author": ["Keiichi Tokuda", "Takashi Masuko", "Noboru Miyazaki", "Takao Kobayashi."], "venue": "IEICE TRANSACTIONS on Information and Systems 85(3):455\u2013464.", "citeRegEx": "Tokuda et al\\.,? 2002", "shortCiteRegEx": "Tokuda et al\\.", "year": 2002}, {"title": "Applying morphology generation models to machine translation", "author": ["Kristina Toutanova", "Hisami Suzuki", "Achim Ruopp."], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics. pages 514\u2013522.", "citeRegEx": "Toutanova et al\\.,? 2008", "shortCiteRegEx": "Toutanova et al\\.", "year": 2008}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701 .", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Variational neural machine translation", "author": ["Biao Zhang", "Deyi Xiong", "Jinsong Su", "Hong Duan", "Min Zhang."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics .", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 28, "context": "Several examples of these tasks exist in prior work: using labels to moderate politeness in machine translation results (Sennrich et al., 2016), modifying the output language of a machine translation system (Johnson et al.", "startOffset": 120, "endOffset": 143}, {"referenceID": 15, "context": ", 2016), modifying the output language of a machine translation system (Johnson et al., 2016), or controlling the length of a summary in summarization (Kikuchi et al.", "startOffset": 71, "endOffset": 93}, {"referenceID": 19, "context": ", 2016), or controlling the length of a summary in summarization (Kikuchi et al., 2016).", "startOffset": 65, "endOffset": 87}, {"referenceID": 4, "context": "The ability to accurately analyze and generate morphological forms is crucial to creating applications such as machine translation (Chahuneau et al., 2013; Toutanova et al., 2008) or information retrieval (Darwish and Oard, 2007) in these languages.", "startOffset": 131, "endOffset": 179}, {"referenceID": 31, "context": "The ability to accurately analyze and generate morphological forms is crucial to creating applications such as machine translation (Chahuneau et al., 2013; Toutanova et al., 2008) or information retrieval (Darwish and Oard, 2007) in these languages.", "startOffset": 131, "endOffset": 179}, {"referenceID": 7, "context": ", 2008) or information retrieval (Darwish and Oard, 2007) in these languages.", "startOffset": 33, "endOffset": 57}, {"referenceID": 29, "context": "Approaches to this task include those utilizing hand-crafted linguistic rules and heuristics (Taji et al., 2016), as well as learning-based approaches using alignment and extracted transduction rules (Durrett and DeNero, 2013; Alegria and Etxeberria, 2016; Nicolai et al.", "startOffset": 93, "endOffset": 112}, {"referenceID": 9, "context": ", 2016), as well as learning-based approaches using alignment and extracted transduction rules (Durrett and DeNero, 2013; Alegria and Etxeberria, 2016; Nicolai et al., 2016).", "startOffset": 95, "endOffset": 173}, {"referenceID": 0, "context": ", 2016), as well as learning-based approaches using alignment and extracted transduction rules (Durrett and DeNero, 2013; Alegria and Etxeberria, 2016; Nicolai et al., 2016).", "startOffset": 95, "endOffset": 173}, {"referenceID": 26, "context": ", 2016), as well as learning-based approaches using alignment and extracted transduction rules (Durrett and DeNero, 2013; Alegria and Etxeberria, 2016; Nicolai et al., 2016).", "startOffset": 95, "endOffset": 173}, {"referenceID": 11, "context": "There have also been methods proposed using neural sequenceto-sequence models (Faruqui et al., 2016; Kann et al., 2016; Ostling, 2016), and currently ensembles of attentional encoder-decoder models (Kann and Sch\u00fctze, 2016a,b) have achieved state-of-art results on this task.", "startOffset": 78, "endOffset": 134}, {"referenceID": 16, "context": "There have also been methods proposed using neural sequenceto-sequence models (Faruqui et al., 2016; Kann et al., 2016; Ostling, 2016), and currently ensembles of attentional encoder-decoder models (Kann and Sch\u00fctze, 2016a,b) have achieved state-of-art results on this task.", "startOffset": 78, "endOffset": 134}, {"referenceID": 27, "context": "There have also been methods proposed using neural sequenceto-sequence models (Faruqui et al., 2016; Kann et al., 2016; Ostling, 2016), and currently ensembles of attentional encoder-decoder models (Kann and Sch\u00fctze, 2016a,b) have achieved state-of-art results on this task.", "startOffset": 78, "endOffset": 134}, {"referenceID": 6, "context": "Experiments on the SIGMORPHON morphological reinflection task (Cotterell et al., 2016) find that our model beats the state-of-the-art for a single model in the majority of languages, and is particularly effective in languages with more complicated inflectional phenomena.", "startOffset": 62, "endOffset": 86}, {"referenceID": 30, "context": "Analogous to multi-space hidden Markov models (Tokuda et al., 2002) 2 Labeled Sequence Transduction", "startOffset": 46, "endOffset": 67}, {"referenceID": 21, "context": "The variational autoencoder (Kingma and Welling, 2014) is an efficient way to handle (continuous) latent variables in neural", "startOffset": 28, "endOffset": 54}, {"referenceID": 8, "context": "We describe it briefly here, and interested readers can refer to Doersch (2016) for details.", "startOffset": 65, "endOffset": 80}, {"referenceID": 21, "context": "To optimize the parameters with gradient descent, Kingma and Welling (2014) introduce a reparameterization trick that allows for training using simple backpropagation w.", "startOffset": 50, "endOffset": 76}, {"referenceID": 20, "context": "MSVAEs are a combination of two threads of previous work: deep generative models with both continuous/discrete latent variables for classification problems (Kingma et al., 2014; Maal\u00f8e et al., 2016) and VAEs with only continuous variables for sequential data (Bowman et al.", "startOffset": 156, "endOffset": 198}, {"referenceID": 23, "context": "MSVAEs are a combination of two threads of previous work: deep generative models with both continuous/discrete latent variables for classification problems (Kingma et al., 2014; Maal\u00f8e et al., 2016) and VAEs with only continuous variables for sequential data (Bowman et al.", "startOffset": 156, "endOffset": 198}, {"referenceID": 3, "context": ", 2016) and VAEs with only continuous variables for sequential data (Bowman et al., 2016; Chung et al., 2015; Zhang et al., 2016; Fabius and van Amersfoort, 2014; Bayer and Osendorfer, 2014).", "startOffset": 68, "endOffset": 190}, {"referenceID": 5, "context": ", 2016) and VAEs with only continuous variables for sequential data (Bowman et al., 2016; Chung et al., 2015; Zhang et al., 2016; Fabius and van Amersfoort, 2014; Bayer and Osendorfer, 2014).", "startOffset": 68, "endOffset": 190}, {"referenceID": 33, "context": ", 2016) and VAEs with only continuous variables for sequential data (Bowman et al., 2016; Chung et al., 2015; Zhang et al., 2016; Fabius and van Amersfoort, 2014; Bayer and Osendorfer, 2014).", "startOffset": 68, "endOffset": 190}, {"referenceID": 2, "context": ", 2016) and VAEs with only continuous variables for sequential data (Bowman et al., 2016; Chung et al., 2015; Zhang et al., 2016; Fabius and van Amersfoort, 2014; Bayer and Osendorfer, 2014).", "startOffset": 68, "endOffset": 190}, {"referenceID": 24, "context": "4 To alleviate this problem, we use the recently proposed Gumbel-Softmax trick (Maddison et al., 2014; Gumbel and Lieblein, 1954; Jang et al., 2017; J. et al., 2017) to create a differentiable estimator for categorical variables.", "startOffset": 79, "endOffset": 165}, {"referenceID": 12, "context": "4 To alleviate this problem, we use the recently proposed Gumbel-Softmax trick (Maddison et al., 2014; Gumbel and Lieblein, 1954; Jang et al., 2017; J. et al., 2017) to create a differentiable estimator for categorical variables.", "startOffset": 79, "endOffset": 165}, {"referenceID": 14, "context": "4 To alleviate this problem, we use the recently proposed Gumbel-Softmax trick (Maddison et al., 2014; Gumbel and Lieblein, 1954; Jang et al., 2017; J. et al., 2017) to create a differentiable estimator for categorical variables.", "startOffset": 79, "endOffset": 165}, {"referenceID": 13, "context": "4 To alleviate this problem, we use the recently proposed Gumbel-Softmax trick (Maddison et al., 2014; Gumbel and Lieblein, 1954; Jang et al., 2017; J. et al., 2017) to create a differentiable estimator for categorical variables.", "startOffset": 79, "endOffset": 165}, {"referenceID": 12, "context": "The Gumbel-Max trick (Gumbel and Lieblein, 1954) offers a simple way to draw samples from a categorical distribution with class probabilities \u03c01, \u03c02, \u00b7 \u00b7 \u00b7 by using the argmax operation as follows: one hot(argmaxi[gi + log \u03c0i]), where g1, g2, \u00b7 \u00b7 \u00b7 are i.", "startOffset": 21, "endOffset": 48}, {"referenceID": 19, "context": "4 Kingma et al. (2014) solve this problem by marginalizing over all labels, but this is infeasible in our case where we have an exponential number of label combinations.", "startOffset": 2, "endOffset": 23}, {"referenceID": 3, "context": "To force the decoder to use the latent variables, we take the following two approaches which are similar to Bowman et al. (2016).", "startOffset": 108, "endOffset": 129}, {"referenceID": 22, "context": "This technique can also be seen in (Ko\u010disk\u1ef3 et al., 2016; Miao and Blunsom, 2016).", "startOffset": 35, "endOffset": 81}, {"referenceID": 25, "context": "This technique can also be seen in (Ko\u010disk\u1ef3 et al., 2016; Miao and Blunsom, 2016).", "startOffset": 35, "endOffset": 81}, {"referenceID": 1, "context": "In decoding, we use 3 types of information in calculating the probability of the next character : (1) the current decoder state, (2) a tag context vector using attention (Bahdanau et al., 2015) over the tag embeddings, and (3) the latent variable z.", "startOffset": 170, "endOffset": 193}, {"referenceID": 32, "context": "We train the model with Adadelta (Zeiler, 2012) and use earlystop with a patience of 10.", "startOffset": 33, "endOffset": 47}, {"referenceID": 17, "context": "As a baseline, we compare our results with the MED system (Kann and Sch\u00fctze, 2016a) which achieved state-of-the-art results in the shared task.", "startOffset": 58, "endOffset": 83}, {"referenceID": 6, "context": "Table 2: Percentage of inflected word forms that have modified each part of the lemma (Cotterell et al., 2016) (some words can be inflected zero or multiple times, thus sums may not add to 100%).", "startOffset": 86, "endOffset": 110}, {"referenceID": 6, "context": "Cotterell et al. (2016) estimate how often the inflection process involves prefix changes, stem-internal changes or suffix changes, the results of which are shown in Tab.", "startOffset": 0, "endOffset": 24}], "year": 2017, "abstractText": "Labeled sequence transduction is a task of transforming one sequence into another sequence that satisfies desiderata specified by a set of labels. In this paper we propose multi-space variational encoderdecoders, a new model for labeled sequence transduction with semi-supervised learning. The generative model can use neural networks to handle both discrete and continuous latent variables to exploit various features of data. Experiments show that our model provides not only a powerful supervised framework but also can effectively take advantage of the unlabeled data. On the SIGMORPHON morphological inflection benchmark, our model outperforms single-model state-ofart results by a large margin for the majority of languages.1", "creator": "LaTeX with hyperref package"}}}