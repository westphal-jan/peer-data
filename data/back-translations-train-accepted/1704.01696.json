{"id": "1704.01696", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2017", "title": "A Syntactic Neural Model for General-Purpose Code Generation", "abstract": "We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.", "histories": [["v1", "Thu, 6 Apr 2017 03:13:46 GMT  (818kb,D)", "http://arxiv.org/abs/1704.01696v1", "To appear in ACL 2017"]], "COMMENTS": "To appear in ACL 2017", "reviews": [], "SUBJECTS": "cs.CL cs.PL cs.SE", "authors": ["pengcheng yin", "graham neubig"], "accepted": true, "id": "1704.01696"}, "pdf": {"name": "1704.01696.pdf", "metadata": {"source": "CRF", "title": "A Syntactic Neural Model for General-Purpose Code Generation", "authors": ["Pengcheng Yin", "Graham Neubig"], "emails": ["pcyin@cs.cmu.edu", "gneubig@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "It is indeed the case that we are able to go in search of a solution that will enable us to find a solution that is capable, that we are able to find a solution."}, {"heading": "2 The Code Generation Problem", "text": "We address this problem by first generating the underlying AST. We define a probabilistic grammar model to generate an AST y given to x: p (y | x). The best possible AST y is then given by y = argmax yp (y | x). (1) y is then deterministically converted to the corresponding surface code c.1 While this paper uses examples from the Python code, our method is PLagnostic.Before detailing our approach, we first present a brief introduction to the Python AST and its underlying grammar. The abstract Python grammar contains a set of production rules, and an AST is generated by applying multiple production rules composed of a head node and multiple child nodes. For example, the first rule in Tab. 1 is used to sort the function call (\u00b7)."}, {"heading": "3 Grammar Model", "text": "Before explaining our neural code generation method, we first introduce the grammar model at its core. Our probabilistic grammar model defines the generative history of an AST derivative. We factorize the generation process of an AST into the sequential application of actions of two types: \u2022 APPLYRULE [r] applies a production rule r to the current derivative tree; 1We use astor libraries to convert ASTs to Python code. 2bool, float, int, int, str. \u2022 GENTOKEN [v] fills a variable terminal node by appending a terminal token v.Abb1 (b), showing the generation process of the target AST to Fig. 1 (a). Each node in Fig.) indicates an action node. Action nodes are connected by fixed arrows representing the chronological sequence of action sequence < < < < The < < < The < < <"}, {"heading": "3.1 APPLYRULE Actions", "text": "APPLYRULE actions create a program structure and expand the current node (the boundary node in the time step t: nft) in a depth traverse of the tree running from left to right. Faced with a fixed set of production rules, APPLYRULE selects a rule r from the subset that has a header corresponding to type nft, and uses r to expand nft by appending all child nodes specified by the selected production. As an example, in Figure 1 (b) the rule Call 7 \u2192 expr... extends the boundary nodes Call at a time step t4, and adds its three child nodes expr, expr *, and keyword * to the derivative. APPLYRULE actions expand the derivative AST by appending nodes. If a variable termination node (e.g. str) is added to the derivative, the boundary node is added and the grammatical model explicitly."}, {"heading": "3.2 GENTOKEN Actions", "text": "Once we reach a limit node nft that corresponds to a variable type (e.g. str), GENTOKEN actions are used to fill that node with values. For universally valid PLs like Python, variables and constants have values with one or more tokens. For example, a node that stores the name of a function (e.g. sorts) has a single token, while a node that denotes a string constant (e.g. a = \"Hello World\") could have multiple tokens. Our model handles both scenarios by triggering GENTOKEN actions in one or more time steps. At each time step, GENTOKEN appends a terminal token to the current limit variable node. A special < / n > token code is used to \"close\" the node. The grammar model then leads to the new limit nodes."}, {"heading": "4 Estimating Action Probabilities", "text": "We estimate the probabilities of action in Equation (2) using attentive neural encoder decoder models with an information flow structured by the syntax trees."}, {"heading": "4.1 Encoder", "text": "For an NL description x consisting of n words {wi} ni = 1, the encoder calculates a context-sensitive embedding hi for each wi using a bi-directional long-short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997), similar to the setting in (Bahdanau et al., 2014). See supplementary materials for detailed equations."}, {"heading": "4.2 Decoder", "text": "The decoder uses an RNN to model the sequential generation process of an AST, which is defined as Equation (2). Each action step in the grammar model is, of course, based on a time step in the RNN decoder. Therefore, the action sequence in Fig. 1 (b) can be interpreted as rolling RNN time steps, with solid arrows pointing to RNN connections. The RNN maintains an internal state to track the generation process (\u00a7 4.2.1), which is then used to calculate the probabilities of action p (at | x, a < t) (\u00a7 4.2.2)."}, {"heading": "4.2.1 Tracking Generation States", "text": "In fact, we are going to be able to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in the position we are in."}, {"heading": "4.2.2 Calculating Action Probabilities", "text": "In this section, we explain how to calculate the probabilities of action p (at | x, a < t) + p (hi \u00b7 q) based on st. APPLYRULE. The probability of applying rule r as current action at is given by a softmax5: p (at = APPLYRULE [r] | x, a < t) = softmax (WR \u00b7 g (st) e (4), where g (\u00b7) is a nonlinear word tanh (W \u00b7 st + b), and e (r) is the most uniform vector for rule r. GENTOKEN As in \u00a7 3.2, a token v can be generated from a predefined vocabulary or copied from input, defined as a marginal probability: p (at = GENTOKEN] | x, a hot vector for rule r. GENTOKEN (t) = p (gene | x, a < t) (p, a < v, < we copy a < p (p), a < p; p (p)."}, {"heading": "4.3 Training and Inference", "text": "Using a dataset of pairs of NL descriptions xi and snippets of code ci, we analyze ci into its AST yi and break yi into a sequence of oracle actions under the grammar model. The model is then optimized by maximizing the log probability of the oracle action sequence. At the time of inference, we use the bar search to determine the best AST y value in equivalents (1). See supplemental materials for the pseudo code of the inference algorithm."}, {"heading": "5 Experimental Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Datasets and Metrics", "text": "The HEARTHSTONE (HS) dataset (Ling et al., 2016) is a collection of Python classes that implement cards for the card game HearthStone. Each card comes with a series of fields (e.g. name, cost, and description) that we associate to create the input sequence. This dataset is relatively difficult: input descriptions are short, while the target code consists of complex class structures, with each AST averaging 137 nodes. DJANGO dataset (Oda et al., 2015) is a collection of code lines from the Django Web Framework, each with a manually annotated NL description. Compared with the HS dataset, in which map implementations are reasonably homogeneous, examples in DJANGO et al., include a wide variety of real-world use cases such as string manipulation, IO operations, and exception handling. IFTTP dataset (IFTT-Specification, we are a variety of IFTT-Fixation, IFTO) is a variety of applications."}, {"heading": "5.2 Setup", "text": "For DJANGO, we perform simple canonization, such as replacing quoted strings in the input with placeholders. See supplemental materials for details. We extract simple closures with frequencies greater than a threshold k (k = 30 for HS and 50 for DJANGO). Configuration The size of all embedding is 128, except for node-type embedding, which is 64. The dimensions of the RNN states and hidden layers are 256 and 50, respectively. Since our data sets are relatively small for a data-hungry neural model, we impose strong regulation using recurring suspenders (Gal and Ghahramani, 2016), along with standard failure layers added to the inputs and outputs of the RNN decoder. We validate the failure probability of {0, 0.2, 0.3, 0.4}. For decoding, we use a beam of size 15."}, {"heading": "5.3 Results", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "6 Related Work", "text": "Most existing work on code generation focuses on generating code for domain-specific languages (DSLs) (Kushman and Barzilay, 2013; Raza et al., 2015; Manshadi et al., 2013), using neural network-based approaches that have recently been studied (Parisotto et al., 2016; Balog et al., 2016) A similar line is the use of NL code generation, in addition to the general framework of Ling et al. (2016), existing methods often use language and task-specific rules and strategies (Lei et al., 2013; Raghothaman et al al al al al., 2016. A similar line is the use of NL code generation for code recovery (Wei et al., 2015; Allamanis et al., 2015). The reverse task of generating NL summaries from the source code is also relevant (Oda, et al., 2015, al.)."}, {"heading": "7 Conclusion", "text": "This paper proposes a syntax-driven approach to neural code generation that generates an abstract syntax tree by sequentially applying actions from a grammar model. Experiments on both code generation and semantic parsing tasks demonstrate the effectiveness of our proposed approach."}, {"heading": "Acknowledgment", "text": "We are grateful to Wang Ling for his generous help with the LPN and setting up the benchmark. We also thank Li Dong for his help with SEQ2TREE and insightful discussions."}, {"heading": "A Encoder LSTM Equations", "text": "Suppose the input language description x consists of n words {wi} ni = 1. Let wi denote the embedding of wi. We use two LSTMs to process x in forward and backward order, and get the sequence of hidden states {~ hi} ni = 1 and {~ hi} ni = 1 in both directions: ~ hi = f \u2192 LSTM (wi, ~ hi \u2212 1) ~ hi = f \u2190 LSTM (wi, ~ hi + 1), where f \u2192 LSTM and f \u2190 LSTM are standard LSTM update functions. The representation of the i-th word, hi, results from the concatenation of ~ hi and ~ hi.B inference algorithm. In the face of an NL description, we approach the best AST y in equation. 1 by means of beam search. The inference procedure is listed in algorithm 1. We maintain a beam of size K. The beam is initialized with a single AST hypothesis."}, {"heading": "C Dataset Preprocessing", "text": "Rare words We replace word types whose frequency is lower than d with a special token (d = 3 for DJANGO, 3 for HS, and 2 for IFTTT). Canonization We perform a simple canonization of the DJANGO dataset: (1) We observe that input descriptions are often accompanied by citation letters (e.g. the literal name is a \"cache entry\"). Therefore, we replace citation strings with indexed placeholders with regular expressions. After decoding, we perform a post-processing step to replace all placeholders with their actual values. (2) For descriptions with cascading variable references (e.g. self.makekey calling method), we hang the entire variable name. \"(e.g., we attach ourselves and makekey to self.makekey. This gives the pointer network the flexibility to either copy parts or whole variable names."}, {"heading": "D Additional Decoding Examples", "text": "We provide additional decoding examples from the DJANGO and HS datasets listed in Table 6 and Table 7, respectively, which rely heavily on the pointer network to copy variable names and constants from input descriptions, and the error source in DJANGO is more varied, with most false examples resulting from missing arguments and erroneous words copied from the pointer network. Errors in HS are largely due to partial or incorrectly implemented effects. Also, note that the first example in Table 6 is semantically correct, although it was considered incorrect by our exact match metric, suggesting a more advanced evaluation metric that takes into account the execution results in future studies. Algorithm 1: Inference Algorithm Input: NL description x Output: Code snippet c1 Encoder to encoder x 2 Q = {y0 (root)}."}], "references": [{"title": "Bimodal modelling of source code and natural language", "author": ["Miltiadis Allamanis", "Daniel Tarlow", "Andrew D. Gordon", "Yi Wei."], "venue": "Proceedings of ICML. volume 37.", "citeRegEx": "Allamanis et al\\.,? 2015", "shortCiteRegEx": "Allamanis et al\\.", "year": 2015}, {"title": "Tree-structured decoding with doubly recurrent neural networks", "author": ["David Alvarez-Melis", "Tommi S. Jaakkola."], "venue": "Proceedings of ICLR.", "citeRegEx": "Alvarez.Melis and Jaakkola.,? 2017", "shortCiteRegEx": "Alvarez.Melis and Jaakkola.", "year": 2017}, {"title": "Broad-coverage CCG semantic parsing with AMR", "author": ["Yoav Artzi", "Kenton Lee", "Luke Zettlemoyer."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Artzi et al\\.,? 2015", "shortCiteRegEx": "Artzi et al\\.", "year": 2015}, {"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions", "author": ["Yoav Artzi", "Luke Zettlemoyer."], "venue": "Transaction of ACL 1(1).", "citeRegEx": "Artzi and Zettlemoyer.,? 2013", "shortCiteRegEx": "Artzi and Zettlemoyer.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Deepcoder: Learning to write programs", "author": ["Matej Balog", "Alexander L. Gaunt", "Marc Brockschmidt", "Sebastian Nowozin", "Daniel Tarlow."], "venue": "CoRR abs/1611.01989.", "citeRegEx": "Balog et al\\.,? 2016", "shortCiteRegEx": "Balog et al\\.", "year": 2016}, {"title": "A 15 year perspective on automatic programming", "author": ["Robert Balzer."], "venue": "IEEE Trans. Software Eng. 11(11).", "citeRegEx": "Balzer.,? 1985", "shortCiteRegEx": "Balzer.", "year": 1985}, {"title": "Abstract meaning representation for sembanking", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider."], "venue": "Proceedings of the 7th Linguis-", "citeRegEx": "Banarescu et al\\.,? 2013", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Improved semantic parsers for if-then statements", "author": ["I. Beltagy", "Chris Quirk."], "venue": "Proceedings of ACL.", "citeRegEx": "Beltagy and Quirk.,? 2016", "shortCiteRegEx": "Beltagy and Quirk.", "year": 2016}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Example-centric programming: integrating web search into the development environment", "author": ["Joel Brandt", "Mira Dontcheva", "Marcos Weskamp", "Scott R. Klemmer."], "venue": "Proceedings of CHI.", "citeRegEx": "Brandt et al\\.,? 2010", "shortCiteRegEx": "Brandt et al\\.", "year": 2010}, {"title": "Two studies of opportunistic programming: interleaving web foraging, learning, and writing code", "author": ["Joel Brandt", "Philip J. Guo", "Joel Lewenstein", "Mira Dontcheva", "Scott R. Klemmer."], "venue": "Proceedings of CHI.", "citeRegEx": "Brandt et al\\.,? 2009", "shortCiteRegEx": "Brandt et al\\.", "year": 2009}, {"title": "Widecoverage efficient statistical parsing with CCG and log-linear models", "author": ["Stephen Clark", "James R. Curran."], "venue": "Computational Linguistics 33(4).", "citeRegEx": "Clark and Curran.,? 2007", "shortCiteRegEx": "Clark and Curran.", "year": 2007}, {"title": "Driving semantic parsing from the world\u2019s response", "author": ["James Clarke", "Dan Goldwasser", "Ming-Wei Chang", "Dan Roth."], "venue": "Proceedings of CoNLL.", "citeRegEx": "Clarke et al\\.,? 2010", "shortCiteRegEx": "Clarke et al\\.", "year": 2010}, {"title": "Language to logical form with neural attention", "author": ["Li Dong", "Mirella Lapata."], "venue": "Proceedings of ACL.", "citeRegEx": "Dong and Lapata.,? 2016", "shortCiteRegEx": "Dong and Lapata.", "year": 2016}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal", "Zoubin Ghahramani."], "venue": "Proceedings of NIPS.", "citeRegEx": "Gal and Ghahramani.,? 2016", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li."], "venue": "Proceedings of ACL.", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Interactive synthesis using free-form queries", "author": ["Tihomir Gvero", "Viktor Kuncak."], "venue": "Proceedings of ICSE.", "citeRegEx": "Gvero and Kuncak.,? 2015", "shortCiteRegEx": "Gvero and Kuncak.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8).", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Summarizing source code using a neural attention model", "author": ["Srinivasan Iyer", "Ioannis Konstas", "Alvin Cheung", "Luke Zettlemoyer."], "venue": "Proceedings of ACL.", "citeRegEx": "Iyer et al\\.,? 2016", "shortCiteRegEx": "Iyer et al\\.", "year": 2016}, {"title": "Data recombination for neural semantic parsing", "author": ["Robin Jia", "Percy Liang."], "venue": "Proceedings of ACL.", "citeRegEx": "Jia and Liang.,? 2016", "shortCiteRegEx": "Jia and Liang.", "year": 2016}, {"title": "Semantic parsing with semi-supervised sequential autoencoders", "author": ["Tom\u00e1s Kocisk\u00fd", "G\u00e1bor Melis", "Edward Grefenstette", "Chris Dyer", "Wang Ling", "Phil Blunsom", "Karl Moritz Hermann."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Kocisk\u00fd et al\\.,? 2016", "shortCiteRegEx": "Kocisk\u00fd et al\\.", "year": 2016}, {"title": "Semantic parsing to probabilistic programs for situated question answering", "author": ["Jayant Krishnamurthy", "Oyvind Tafjord", "Aniruddha Kembhavi."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Krishnamurthy et al\\.,? 2016", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2016}, {"title": "Using semantic unification to generate regular expressions from natural language", "author": ["Nate Kushman", "Regina Barzilay."], "venue": "Proceedings of NAACL.", "citeRegEx": "Kushman and Barzilay.,? 2013", "shortCiteRegEx": "Kushman and Barzilay.", "year": 2013}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["Tom Kwiatkowski", "Eunsol Choi", "Yoav Artzi", "Luke S. Zettlemoyer."], "venue": "Proceedings of the EMNLP.", "citeRegEx": "Kwiatkowski et al\\.,? 2013", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "From natural language specifications to program input parsers", "author": ["Tao Lei", "Fan Long", "Regina Barzilay", "Martin C. Rinard."], "venue": "Proceedings of ACL.", "citeRegEx": "Lei et al\\.,? 2013", "shortCiteRegEx": "Lei et al\\.", "year": 2013}, {"title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision", "author": ["Chen Liang", "Jonathan Berant", "Quoc Le", "Kenneth D. Forbus", "Ni Lao."], "venue": "CoRR abs/1611.00020.", "citeRegEx": "Liang et al\\.,? 2016", "shortCiteRegEx": "Liang et al\\.", "year": 2016}, {"title": "Learning dependency-based compositional semantics", "author": ["Percy Liang", "Michael I. Jordan", "Dan Klein."], "venue": "Proceedings of ACL.", "citeRegEx": "Liang et al\\.,? 2011", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "Latent predictor networks for code generation", "author": ["Wang Ling", "Phil Blunsom", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Fumin Wang", "Andrew Senior."], "venue": "Proceedings of ACL.", "citeRegEx": "Ling et al\\.,? 2016", "shortCiteRegEx": "Ling et al\\.", "year": 2016}, {"title": "Keyword programming in java", "author": ["Greg Little", "Robert C. Miller."], "venue": "Autom. Softw. Eng. 16(1).", "citeRegEx": "Little and Miller.,? 2009", "shortCiteRegEx": "Little and Miller.", "year": 2009}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of ACL.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Structured generative models of natural source code", "author": ["Chris J. Maddison", "Daniel Tarlow."], "venue": "Proceedings of ICML. volume 32.", "citeRegEx": "Maddison and Tarlow.,? 2014", "shortCiteRegEx": "Maddison and Tarlow.", "year": 2014}, {"title": "Integrating programming by example and natural language programming", "author": ["Mehdi Hafezi Manshadi", "Daniel Gildea", "James F. Allen."], "venue": "Proceedings of AAAI.", "citeRegEx": "Manshadi et al\\.,? 2013", "shortCiteRegEx": "Manshadi et al\\.", "year": 2013}, {"title": "Listen, attend, and walk: Neural mapping of navigational instructions to action sequences", "author": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter."], "venue": "Proceedings of AAAI.", "citeRegEx": "Mei et al\\.,? 2016", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Neural shiftreduce CCG semantic parsing", "author": ["Dipendra K. Misra", "Yoav Artzi."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Misra and Artzi.,? 2016", "shortCiteRegEx": "Misra and Artzi.", "year": 2016}, {"title": "Environment-driven lexicon induction for high-level instructions", "author": ["Dipendra Kumar Misra", "Kejia Tao", "Percy Liang", "Ashutosh Saxena."], "venue": "Proceedings of ACL.", "citeRegEx": "Misra et al\\.,? 2015", "shortCiteRegEx": "Misra et al\\.", "year": 2015}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V. Le", "Ilya Sutskever."], "venue": "Proceedings of ICLR.", "citeRegEx": "Neelakantan et al\\.,? 2016", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "lamtram: A toolkit for language and translation modeling using neural networks", "author": ["Graham Neubig."], "venue": "http://www.github.com/neubig/lamtram.", "citeRegEx": "Neubig.,? 2015", "shortCiteRegEx": "Neubig.", "year": 2015}, {"title": "A statistical semantic language model for source code", "author": ["Tung Thanh Nguyen", "Anh Tuan Nguyen", "Hoan Anh Nguyen", "Tien N. Nguyen."], "venue": "Proceedings of ACM SIGSOFT .", "citeRegEx": "Nguyen et al\\.,? 2013", "shortCiteRegEx": "Nguyen et al\\.", "year": 2013}, {"title": "Learning to generate pseudo-code from source code using statistical machine translation (T)", "author": ["Yusuke Oda", "Hiroyuki Fudaba", "Graham Neubig", "Hideaki Hata", "Sakriani Sakti", "Tomoki Toda", "Satoshi Nakamura."], "venue": "Proceedings of ASE.", "citeRegEx": "Oda et al\\.,? 2015", "shortCiteRegEx": "Oda et al\\.", "year": 2015}, {"title": "Neuro-symbolic program synthesis", "author": ["Emilio Parisotto", "Abdel-rahman Mohamed", "Rishabh Singh", "Lihong Li", "Dengyong Zhou", "Pushmeet Kohli."], "venue": "CoRR abs/1611.01855.", "citeRegEx": "Parisotto et al\\.,? 2016", "shortCiteRegEx": "Parisotto et al\\.", "year": 2016}, {"title": "Compositional semantic parsing on semi-structured tables", "author": ["Panupong Pasupat", "Percy Liang."], "venue": "Proceedings of ACL.", "citeRegEx": "Pasupat and Liang.,? 2015", "shortCiteRegEx": "Pasupat and Liang.", "year": 2015}, {"title": "Python abstract grammar", "author": ["Python Software Foundation."], "venue": "https://docs.python.org/2/library/ast.html.", "citeRegEx": "Foundation.,? 2016", "shortCiteRegEx": "Foundation.", "year": 2016}, {"title": "Language to code: Learning semantic parsers for if-this-then-that recipes", "author": ["Chris Quirk", "Raymond J. Mooney", "Michel Galley."], "venue": "Proceedings of ACL.", "citeRegEx": "Quirk et al\\.,? 2015", "shortCiteRegEx": "Quirk et al\\.", "year": 2015}, {"title": "SWIM: synthesizing what i mean: code search and idiomatic snippet synthesis", "author": ["Mukund Raghothaman", "Yi Wei", "Youssef Hamadi."], "venue": "Proceedings of ICSE.", "citeRegEx": "Raghothaman et al\\.,? 2016", "shortCiteRegEx": "Raghothaman et al\\.", "year": 2016}, {"title": "Compositional program synthesis from natural language and examples", "author": ["Mohammad Raza", "Sumit Gulwani", "Natasa MilicFrayling."], "venue": "Proceedings of IJCAI.", "citeRegEx": "Raza et al\\.,? 2015", "shortCiteRegEx": "Raza et al\\.", "year": 2015}, {"title": "Using multiple clause constructors in inductive logic programming for semantic parsing", "author": ["Lappoon R. Tang", "Raymond J. Mooney."], "venue": "Proceedings of ECML.", "citeRegEx": "Tang and Mooney.,? 2001", "shortCiteRegEx": "Tang and Mooney.", "year": 2001}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Proceedings of NIPS.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Building bing developer assistant", "author": ["Yi Wei", "Nirupama Chandrasekaran", "Sumit Gulwani", "Youssef Hamadi."], "venue": "Technical report. https://www.microsoft.com/enus/research/publication/building-bing-developer-", "citeRegEx": "Wei et al\\.,? 2015", "shortCiteRegEx": "Wei et al\\.", "year": 2015}, {"title": "Sequence-based structured prediction for semantic parsing", "author": ["Chunyang Xiao", "Marc Dymetman", "Claire Gardent."], "venue": "Proceedings of ACL.", "citeRegEx": "Xiao et al\\.,? 2016", "shortCiteRegEx": "Xiao et al\\.", "year": 2016}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "author": ["Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao."], "venue": "Proceedings of ACL.", "citeRegEx": "Yih et al\\.,? 2015", "shortCiteRegEx": "Yih et al\\.", "year": 2015}, {"title": "Neural enquirer: Learning to query tables in natural language", "author": ["Pengcheng Yin", "Zhengdong Lu", "Hang Li", "Ben Kao."], "venue": "Proceedings of IJCAI.", "citeRegEx": "Yin et al\\.,? 2016", "shortCiteRegEx": "Yin et al\\.", "year": 2016}, {"title": "Learning to map sentences to logical form structured classification with probabilistic categorial grammars", "author": ["Luke Zettlemoyer", "Michael Collins."], "venue": "Proceedings of UAI.", "citeRegEx": "Zettlemoyer and Collins.,? 2005", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2005}], "referenceMentions": [{"referenceID": 6, "context": "However, this is time-consuming, and thus the software engineering literature is ripe with methods to directly generate code from NL descriptions, mostly with hand-engineered methods highly tailored to specific programming languages (Balzer, 1985; Little and Miller, 2009; Gvero and Kuncak, 2015).", "startOffset": 233, "endOffset": 296}, {"referenceID": 29, "context": "However, this is time-consuming, and thus the software engineering literature is ripe with methods to directly generate code from NL descriptions, mostly with hand-engineered methods highly tailored to specific programming languages (Balzer, 1985; Little and Miller, 2009; Gvero and Kuncak, 2015).", "startOffset": 233, "endOffset": 296}, {"referenceID": 17, "context": "However, this is time-consuming, and thus the software engineering literature is ripe with methods to directly generate code from NL descriptions, mostly with hand-engineered methods highly tailored to specific programming languages (Balzer, 1985; Little and Miller, 2009; Gvero and Kuncak, 2015).", "startOffset": 233, "endOffset": 296}, {"referenceID": 12, "context": "These logical forms can be general-purpose meaning representations (Clark and Curran, 2007; Banarescu et al., 2013), formalisms for querying knowledge bases (Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Berant et al.", "startOffset": 67, "endOffset": 115}, {"referenceID": 7, "context": "These logical forms can be general-purpose meaning representations (Clark and Curran, 2007; Banarescu et al., 2013), formalisms for querying knowledge bases (Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Berant et al.", "startOffset": 67, "endOffset": 115}, {"referenceID": 46, "context": ", 2013), formalisms for querying knowledge bases (Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Berant et al., 2013) and instructions for robots or personal assistants (Artzi and Zettlemoyer, 2013; Quirk et al.", "startOffset": 49, "endOffset": 124}, {"referenceID": 52, "context": ", 2013), formalisms for querying knowledge bases (Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Berant et al., 2013) and instructions for robots or personal assistants (Artzi and Zettlemoyer, 2013; Quirk et al.", "startOffset": 49, "endOffset": 124}, {"referenceID": 9, "context": ", 2013), formalisms for querying knowledge bases (Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Berant et al., 2013) and instructions for robots or personal assistants (Artzi and Zettlemoyer, 2013; Quirk et al.", "startOffset": 49, "endOffset": 124}, {"referenceID": 3, "context": ", 2013) and instructions for robots or personal assistants (Artzi and Zettlemoyer, 2013; Quirk et al., 2015), among others.", "startOffset": 59, "endOffset": 108}, {"referenceID": 43, "context": ", 2013) and instructions for robots or personal assistants (Artzi and Zettlemoyer, 2013; Quirk et al., 2015), among others.", "startOffset": 59, "endOffset": 108}, {"referenceID": 3, "context": ", 2013) and instructions for robots or personal assistants (Artzi and Zettlemoyer, 2013; Quirk et al., 2015), among others. While these methods have the advantage of being learnable from data, compared to the programming languages (PLs) in use by programmers, the domainspecific languages targeted by these works have a schema and syntax that is relatively simple. Recently, Ling et al. (2016) have proposed a data-driven code generation method for high-level, general-purpose PLs like Python and Java.", "startOffset": 60, "endOffset": 394}, {"referenceID": 28, "context": "3% absolute improvements in accuracy against the state-of-the-art system (Ling et al., 2016).", "startOffset": 73, "endOffset": 92}, {"referenceID": 48, "context": "Xiao et al. (2016) have noted that this imposition of structure on neural models is useful for semantic parsing, and we expect this to be even more important for general-purpose PLs where the syntax trees are larger and more complex.", "startOffset": 0, "endOffset": 19}, {"referenceID": 18, "context": "For an NL description x consisting of n words {wi}i=1, the encoder computes a context sensitive embedding hi for each wi using a bidirectional Long Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997), similar to the setting in (Bahdanau et al.", "startOffset": 181, "endOffset": 215}, {"referenceID": 4, "context": "For an NL description x consisting of n words {wi}i=1, the encoder computes a context sensitive embedding hi for each wi using a bidirectional Long Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997), similar to the setting in (Bahdanau et al., 2014).", "startOffset": 243, "endOffset": 266}, {"referenceID": 4, "context": "We follow Bahdanau et al. (2014) and use a Deep Neural Network (DNN) with a single hidden layer to compute attention weights.", "startOffset": 10, "endOffset": 33}, {"referenceID": 4, "context": "We follow Bahdanau et al. (2014) and use a Deep Neural Network (DNN) with a single hidden layer to compute attention weights. Parent Feeding pt Our decoder RNN uses additional neural connections to directly pass information from parent actions. For instance, when computing s9, the information from its parent action step t4 will be used. Formally, we define the parent action step pt as the time step at which the frontier node nft is generated. As an example, for t9, its parent action step p9 is t4, since nf9 is the node ?, which is generated at t4 by the APPLYRULE[Call7\u2192. . .] action. We model parent information pt from two sources: (1) the hidden state of parent action spt , and (2) the embedding of parent action apt . pt is the concatenation. The parent feeding schema enables the model to utilize the information of parent code segments to make more confident predictions. Similar approaches of injecting parent information were also explored in the SEQ2TREE model in Dong and Lapata (2016)4.", "startOffset": 10, "endOffset": 1003}, {"referenceID": 16, "context": "To model the copy probability, we follow recent advances in modeling copying mechanism in neural networks (Gu et al., 2016; Jia and Liang, 2016; Ling et al., 2016), and use a pointer network (Vinyals et al.", "startOffset": 106, "endOffset": 163}, {"referenceID": 20, "context": "To model the copy probability, we follow recent advances in modeling copying mechanism in neural networks (Gu et al., 2016; Jia and Liang, 2016; Ling et al., 2016), and use a pointer network (Vinyals et al.", "startOffset": 106, "endOffset": 163}, {"referenceID": 28, "context": "To model the copy probability, we follow recent advances in modeling copying mechanism in neural networks (Gu et al., 2016; Jia and Liang, 2016; Ling et al., 2016), and use a pointer network (Vinyals et al.", "startOffset": 106, "endOffset": 163}, {"referenceID": 47, "context": ", 2016), and use a pointer network (Vinyals et al., 2015) to compute the probability of copying the i-th word from the input by attending to input representations {hi}:", "startOffset": 35, "endOffset": 57}, {"referenceID": 28, "context": "HEARTHSTONE (HS) dataset (Ling et al., 2016) is a collection of Python classes that implement cards for the card game HearthStone.", "startOffset": 25, "endOffset": 44}, {"referenceID": 39, "context": "DJANGO dataset (Oda et al., 2015) is a collection of lines of code from the Django web framework, each with a manually annotated NL description.", "startOffset": 15, "endOffset": 33}, {"referenceID": 43, "context": "IFTTT dataset (Quirk et al., 2015) is a domainspecific benchmark that provides an interesting side comparison.", "startOffset": 14, "endOffset": 34}, {"referenceID": 8, "context": "Like Beltagy and Quirk (2016), we strip function param-", "startOffset": 5, "endOffset": 30}, {"referenceID": 28, "context": "However, because generating an exact match for complex code structures is nontrivial, we follow Ling et al. (2016), and use tokenlevel BLEU-4 with as a secondary metric, defined as the averaged BLEU scores over all examples.", "startOffset": 96, "endOffset": 115}, {"referenceID": 15, "context": "Since our datasets are relatively small for a data-hungry neural model, we impose strong regularization using recurrent dropouts (Gal and Ghahramani, 2016), together with standard dropout layers added to the inputs and outputs of the decoder RNN.", "startOffset": 129, "endOffset": 155}, {"referenceID": 28, "context": "\u2020Results previously reported in Ling et al. (2016).", "startOffset": 32, "endOffset": 51}, {"referenceID": 28, "context": "We compare primarily with two approaches: (1) Latent Predictor Network (LPN), a state-of-the-art sequenceto-sequence code generation model (Ling et al., 2016), and (2) SEQ2TREE, a neural semantic parsing model (Dong and Lapata, 2016).", "startOffset": 139, "endOffset": 158}, {"referenceID": 14, "context": ", 2016), and (2) SEQ2TREE, a neural semantic parsing model (Dong and Lapata, 2016).", "startOffset": 59, "endOffset": 82}, {"referenceID": 30, "context": "We test both the original SEQ2TREE model released by the authors and our revised one (SEQ2TREE\u2013UNK) that uses unknown word replacement to handle rare words (Luong et al., 2015).", "startOffset": 156, "endOffset": 176}, {"referenceID": 37, "context": "For completeness, we also compare with a strong neural machine translation (NMT) system (Neubig, 2015) using a standard encoder-decoder architecture with attention and unknown word replacement8, and include numbers from other baselines used in Ling et al.", "startOffset": 88, "endOffset": 102}, {"referenceID": 14, "context": ", 2016), and (2) SEQ2TREE, a neural semantic parsing model (Dong and Lapata, 2016). SEQ2TREE generates trees one node at a time, and the target grammar is not explicitly modeled a priori, but implicitly learned from data. We test both the original SEQ2TREE model released by the authors and our revised one (SEQ2TREE\u2013UNK) that uses unknown word replacement to handle rare words (Luong et al., 2015). For completeness, we also compare with a strong neural machine translation (NMT) system (Neubig, 2015) using a standard encoder-decoder architecture with attention and unknown word replacement8, and include numbers from other baselines used in Ling et al. (2016). On the HS dataset, which has relatively large ASTs, we use unary closure for our model and SEQ2TREE, and for DJANGO we do not.", "startOffset": 60, "endOffset": 663}, {"referenceID": 43, "context": "Classical Methods posclass (Quirk et al., 2015) 81.", "startOffset": 27, "endOffset": 47}, {"referenceID": 8, "context": "0 LR (Beltagy and Quirk, 2016) 88.", "startOffset": 5, "endOffset": 30}, {"referenceID": 8, "context": "7 NN (Beltagy and Quirk, 2016) 88.", "startOffset": 5, "endOffset": 30}, {"referenceID": 14, "context": "3 SEQ2TREE (Dong and Lapata, 2016) 89.", "startOffset": 11, "endOffset": 34}, {"referenceID": 1, "context": "2 (Alvarez-Melis and Jaakkola, 2017)", "startOffset": 2, "endOffset": 36}, {"referenceID": 8, "context": "4 shows the results, following the evaluation protocol in (Beltagy and Quirk, 2016) for accuracies at both channel and full parse tree (channel + function) levels.", "startOffset": 58, "endOffset": 83}, {"referenceID": 23, "context": "Code Generation and Analysis Most existing works on code generation focus on generating code for domain specific languages (DSLs) (Kushman and Barzilay, 2013; Raza et al., 2015; Manshadi et al., 2013), with neural network-based approaches recently explored (Parisotto et al.", "startOffset": 130, "endOffset": 200}, {"referenceID": 45, "context": "Code Generation and Analysis Most existing works on code generation focus on generating code for domain specific languages (DSLs) (Kushman and Barzilay, 2013; Raza et al., 2015; Manshadi et al., 2013), with neural network-based approaches recently explored (Parisotto et al.", "startOffset": 130, "endOffset": 200}, {"referenceID": 32, "context": "Code Generation and Analysis Most existing works on code generation focus on generating code for domain specific languages (DSLs) (Kushman and Barzilay, 2013; Raza et al., 2015; Manshadi et al., 2013), with neural network-based approaches recently explored (Parisotto et al.", "startOffset": 130, "endOffset": 200}, {"referenceID": 40, "context": ", 2013), with neural network-based approaches recently explored (Parisotto et al., 2016; Balog et al., 2016).", "startOffset": 64, "endOffset": 108}, {"referenceID": 5, "context": ", 2013), with neural network-based approaches recently explored (Parisotto et al., 2016; Balog et al., 2016).", "startOffset": 64, "endOffset": 108}, {"referenceID": 25, "context": "(2016), existing methods often use language and task-specific rules and strategies (Lei et al., 2013; Raghothaman et al., 2016).", "startOffset": 83, "endOffset": 127}, {"referenceID": 44, "context": "(2016), existing methods often use language and task-specific rules and strategies (Lei et al., 2013; Raghothaman et al., 2016).", "startOffset": 83, "endOffset": 127}, {"referenceID": 48, "context": "A similar line is to use NL queries for code retrieval (Wei et al., 2015; Allamanis et al., 2015).", "startOffset": 55, "endOffset": 97}, {"referenceID": 0, "context": "A similar line is to use NL queries for code retrieval (Wei et al., 2015; Allamanis et al., 2015).", "startOffset": 55, "endOffset": 97}, {"referenceID": 39, "context": "The reverse task of generating NL summaries from source code has also been explored (Oda et al., 2015; Iyer et al., 2016).", "startOffset": 84, "endOffset": 121}, {"referenceID": 19, "context": "The reverse task of generating NL summaries from source code has also been explored (Oda et al., 2015; Iyer et al., 2016).", "startOffset": 84, "endOffset": 121}, {"referenceID": 4, "context": ", 2016; Balog et al., 2016). For general-purpose code generation, besides the general framework of Ling et al. (2016), existing methods often use language and task-specific rules and strategies (Lei et al.", "startOffset": 8, "endOffset": 118}, {"referenceID": 31, "context": "source code (Maddison and Tarlow, 2014; Nguyen et al., 2013).", "startOffset": 12, "endOffset": 60}, {"referenceID": 38, "context": "source code (Maddison and Tarlow, 2014; Nguyen et al., 2013).", "startOffset": 12, "endOffset": 60}, {"referenceID": 24, "context": "The parsing process is often guided by grammatical formalisms like combinatory categorical grammars (Kwiatkowski et al., 2013; Artzi et al., 2015), dependency-based syntax (Liang et al.", "startOffset": 100, "endOffset": 146}, {"referenceID": 2, "context": "The parsing process is often guided by grammatical formalisms like combinatory categorical grammars (Kwiatkowski et al., 2013; Artzi et al., 2015), dependency-based syntax (Liang et al.", "startOffset": 100, "endOffset": 146}, {"referenceID": 27, "context": ", 2015), dependency-based syntax (Liang et al., 2011; Pasupat and Liang, 2015) or task-specific formalisms (Clarke et al.", "startOffset": 33, "endOffset": 78}, {"referenceID": 41, "context": ", 2015), dependency-based syntax (Liang et al., 2011; Pasupat and Liang, 2015) or task-specific formalisms (Clarke et al.", "startOffset": 33, "endOffset": 78}, {"referenceID": 13, "context": ", 2011; Pasupat and Liang, 2015) or task-specific formalisms (Clarke et al., 2010; Yih et al., 2015; Krishnamurthy et al., 2016; Misra et al., 2015; Mei et al., 2016).", "startOffset": 61, "endOffset": 166}, {"referenceID": 50, "context": ", 2011; Pasupat and Liang, 2015) or task-specific formalisms (Clarke et al., 2010; Yih et al., 2015; Krishnamurthy et al., 2016; Misra et al., 2015; Mei et al., 2016).", "startOffset": 61, "endOffset": 166}, {"referenceID": 22, "context": ", 2011; Pasupat and Liang, 2015) or task-specific formalisms (Clarke et al., 2010; Yih et al., 2015; Krishnamurthy et al., 2016; Misra et al., 2015; Mei et al., 2016).", "startOffset": 61, "endOffset": 166}, {"referenceID": 35, "context": ", 2011; Pasupat and Liang, 2015) or task-specific formalisms (Clarke et al., 2010; Yih et al., 2015; Krishnamurthy et al., 2016; Misra et al., 2015; Mei et al., 2016).", "startOffset": 61, "endOffset": 166}, {"referenceID": 33, "context": ", 2011; Pasupat and Liang, 2015) or task-specific formalisms (Clarke et al., 2010; Yih et al., 2015; Krishnamurthy et al., 2016; Misra et al., 2015; Mei et al., 2016).", "startOffset": 61, "endOffset": 166}, {"referenceID": 34, "context": "Recently, there are efforts in designing neural network-based semantic parsers (Misra and Artzi, 2016; Dong and Lapata, 2016; Neelakantan et al., 2016; Yin et al., 2016).", "startOffset": 79, "endOffset": 169}, {"referenceID": 14, "context": "Recently, there are efforts in designing neural network-based semantic parsers (Misra and Artzi, 2016; Dong and Lapata, 2016; Neelakantan et al., 2016; Yin et al., 2016).", "startOffset": 79, "endOffset": 169}, {"referenceID": 36, "context": "Recently, there are efforts in designing neural network-based semantic parsers (Misra and Artzi, 2016; Dong and Lapata, 2016; Neelakantan et al., 2016; Yin et al., 2016).", "startOffset": 79, "endOffset": 169}, {"referenceID": 51, "context": "Recently, there are efforts in designing neural network-based semantic parsers (Misra and Artzi, 2016; Dong and Lapata, 2016; Neelakantan et al., 2016; Yin et al., 2016).", "startOffset": 79, "endOffset": 169}, {"referenceID": 21, "context": "Several approaches have be proposed to utilize grammar knowledge in a neural parser, such as augmenting the training data by generating examples guided by the grammar (Kocisk\u00fd et al., 2016; Jia and Liang, 2016).", "startOffset": 167, "endOffset": 210}, {"referenceID": 20, "context": "Several approaches have be proposed to utilize grammar knowledge in a neural parser, such as augmenting the training data by generating examples guided by the grammar (Kocisk\u00fd et al., 2016; Jia and Liang, 2016).", "startOffset": 167, "endOffset": 210}, {"referenceID": 0, "context": "The most relevant work is Allamanis et al. (2015), which uses a factorized model to measure semantic relatedness between NL and ASTs for code retrieval, while our model tackles the more challenging generation task.", "startOffset": 26, "endOffset": 50}, {"referenceID": 0, "context": "The most relevant work is Allamanis et al. (2015), which uses a factorized model to measure semantic relatedness between NL and ASTs for code retrieval, while our model tackles the more challenging generation task. Semantic Parsing Our work is related to the general topic of semantic parsing, where the target logical forms can be viewed as DSLs. The parsing process is often guided by grammatical formalisms like combinatory categorical grammars (Kwiatkowski et al., 2013; Artzi et al., 2015), dependency-based syntax (Liang et al., 2011; Pasupat and Liang, 2015) or task-specific formalisms (Clarke et al., 2010; Yih et al., 2015; Krishnamurthy et al., 2016; Misra et al., 2015; Mei et al., 2016). Recently, there are efforts in designing neural network-based semantic parsers (Misra and Artzi, 2016; Dong and Lapata, 2016; Neelakantan et al., 2016; Yin et al., 2016). Several approaches have be proposed to utilize grammar knowledge in a neural parser, such as augmenting the training data by generating examples guided by the grammar (Kocisk\u00fd et al., 2016; Jia and Liang, 2016). Liang et al. (2016) used a neural decoder which constrains the space of next valid tokens in the query language for question answering.", "startOffset": 26, "endOffset": 1104}, {"referenceID": 0, "context": "The most relevant work is Allamanis et al. (2015), which uses a factorized model to measure semantic relatedness between NL and ASTs for code retrieval, while our model tackles the more challenging generation task. Semantic Parsing Our work is related to the general topic of semantic parsing, where the target logical forms can be viewed as DSLs. The parsing process is often guided by grammatical formalisms like combinatory categorical grammars (Kwiatkowski et al., 2013; Artzi et al., 2015), dependency-based syntax (Liang et al., 2011; Pasupat and Liang, 2015) or task-specific formalisms (Clarke et al., 2010; Yih et al., 2015; Krishnamurthy et al., 2016; Misra et al., 2015; Mei et al., 2016). Recently, there are efforts in designing neural network-based semantic parsers (Misra and Artzi, 2016; Dong and Lapata, 2016; Neelakantan et al., 2016; Yin et al., 2016). Several approaches have be proposed to utilize grammar knowledge in a neural parser, such as augmenting the training data by generating examples guided by the grammar (Kocisk\u00fd et al., 2016; Jia and Liang, 2016). Liang et al. (2016) used a neural decoder which constrains the space of next valid tokens in the query language for question answering. Finally, the structured prediction approach proposed by Xiao et al. (2016) is closely related to our model in using the underlying grammar as prior knowledge to constrain the generation process of derivation trees, while our method is based on a unified grammar model which jointly captures production rule application and terminal symbol generation, and scales to general purpose code generation tasks.", "startOffset": 26, "endOffset": 1295}], "year": 2017, "abstractText": "We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing datadriven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.", "creator": "LaTeX with hyperref package"}}}