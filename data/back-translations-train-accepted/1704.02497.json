{"id": "1704.02497", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Apr-2017", "title": "On the Linearity of Semantic Change: Investigating Meaning Variation via Dynamic Graph Models", "abstract": "We consider two graph models of semantic change. The first is a time-series model that relates embedding vectors from one time period to embedding vectors of previous time periods. In the second, we construct one graph for each word: nodes in this graph correspond to time points and edge weights to the similarity of the word's meaning across two time points. We apply our two models to corpora across three different languages. We find that semantic change is linear in two senses. Firstly, today's embedding vectors (= meaning) of words can be derived as linear combinations of embedding vectors of their neighbors in previous time periods. Secondly, self-similarity of words decays linearly in time. We consider both findings as new laws/hypotheses of semantic change.", "histories": [["v1", "Sat, 8 Apr 2017 13:56:55 GMT  (40kb)", "http://arxiv.org/abs/1704.02497v1", "Published at ACL 2016, Berlin (short papers)"]], "COMMENTS": "Published at ACL 2016, Berlin (short papers)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["steffen eger", "alexander mehler"], "accepted": true, "id": "1704.02497"}, "pdf": {"name": "1704.02497.pdf", "metadata": {"source": "CRF", "title": "On the Linearity of Semantic Change: Investigating Meaning Variation via Dynamic Graph Models", "authors": ["Steffen Eger"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 4.02 497v 1 [cs.C L] 8A pr2 017We consider two graph models of semantic change: The first is a time series model that relates the embedding of vectors from a period of time to the embedding of vectors from earlier periods of time. In the second, we construct a graph for each word: nodes in this graph correspond to time points and edge weights of the similarity of the meaning of the word over two periods of time. We apply our two models to corpora in three different languages. We find that semantic changes are linear in two respects. First, today's embedding vectors (= meaning) of words can be derived as linear combinations of the embedding of vectors of their neighbors in earlier periods. Second, the similarity of words decays linearly in time. We consider both results as new laws / hypotheses of semantic change."}, {"heading": "1 Introduction", "text": "In fact, the meaning of the word \"gay\" has shifted in the 1970s from an adjectivist meaning for the beginning of the 20th century to its current meaning for gay marriage (Kulkarni et al., 2015a). Similarly, technological progress has led to a semantic expansion of terms such as transmission, mouse, or appeal. In this work, we look at two graph models of semantic change. Our first model is a dynamic model in which the underlying paradise is a (time) series of graphs that we relate to time."}, {"heading": "2 Related work", "text": "Broadly speaking, two current NLP approaches to analyzing changes in meaning can be distinguished: On the one hand, coarser-grained trend analyses compare the semantics of a word within a period of time with the meaning of the word in the preceding period of time (Jatowt and Duh, 2014; Kulkarni et al., 2015a). Such coarser-grained models do not on their own determine how a word has changed (e.g. semantic expansion or narrowing), but merely aim to determine whether a change in meaning has occurred. On the other hand, finer-grained analyses typically target word occurrences in corpora and then examine changes in the corresponding distributions of meaning (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015)."}, {"heading": "3 Graph models", "text": "Let us call V = {w1,.., w | V |} the common vocabulary (intersection) of all words in all periods of time. Here, T is a set of time indices. Call the embedding of a word wi in the span of time t wi (t), wi (t) for two different periods of time s, t, since they can lie in different coordinate systems, let us consider the vectors w-i (t) = (sim (wi (t), w1 (t))),..., sim (wi (t), w-V | (t))))), each of which is in R | V | and where sim is a similarity function, like the cosine. We note that our structuralist definition of w-i (t) is not unproblematic because the vectors w1 (t),."}, {"heading": "3.1 A linear model of semantic change", "text": "We postulate and test the following model of dynamics of meaning, which describes the temporal variation of words wi: wi (t) = p \u2211 n = 1 \u2211 wj \u0394V \u0394N (wi) \u03b1nwjwj (t \u2212 n) (2), where \u03b1nwj \u0394R, for n = 1,.., p, meaning vectors wj (t \u2212 n) and p \u2265 1 are the order of the model. The sentence N (wi) V denotes a series of \"neighbours\" of the word wi. 2 This model states that the meaning of a word wi at a given time is determined by the meaning of its \"neighbours\" in earlier periods and that the underlying functional relationship is linear. 1An alternative to our second-order embedding is to project vectors from different periods in a common space (Mikolov et al., 2013a; Faruqui and Dyer, 2014), whereby it is necessary to find corresponding terms over time."}, {"heading": "3.2 Time-indexed self-similarity graphs", "text": "The nodes of G (w) are the time indices T, and there is an undirected connection between any two s, t \u0394T, the weight of which is given by sim (w (s), w (t). We call the graphs G (w) time-indexed self-similarity (TISS) graphs because they indicate the (semantic) similarity of a given word to itself over different time periods. Of particular interest may lie in weak linkages in these graphs, since they indicate slight similarities between two different time periods, i.e. semantic changes over time."}, {"heading": "4 Experiments", "text": "Data as a corpus for English is used for the Corpus of Historical American (COHA).3 This includes texts from the period 1810 to 2000. We extract two disks: the years 1900-2000 and 1810-2000. For both disks, each time span contains t a decade, e.g. T = {1810, 1820, 1830,..}.4 For each disk, we only use words that are associated with the word classes nouns, adjectives and verbs. For computational and estimation purposes, we also consider words that occur at least 100 times in each time span. To induce word embeddings w Rd for each word w V, we use word2vec (Mikolov et al., 2013b) with standard parameterizations. We do this for each time span t T independently."}, {"heading": "4.1 TISS graphs", "text": "Let Dt0 represent how semantically similar a word is over two periods, on average, if the distance between periods is t0: Dt0 = 1 C + 1 C + 0 = 0 sim (w (s), w (t))), where C = | V | | {s (s, t) | s-t-t-t-t (s) is a normalizer. Figure 1 shows the values Dt0 for the period from 1810 to 2000, for the English data. We notice a clear trend: self-similarity of a word tends to (almost perfect) linear decrease with time distance. Table 1 below shows that this trend applies across all our companies, i.e., for different time scales and different languages: the linear \"decay\" model fits very well with the Dt0 curves, with the adjustedR 2 values well above 90% and clearly negative coefficients."}, {"heading": "4.2 Meaning dynamics network models", "text": "Finally, we estimate the importance of dynamic models as in Eq. (2), i.e., we estimate the coefficients \u03b1nwj from our data sources. We leave the neighbors N (w) of a word as in Eq. (2) the union (w.r.t. t) over sentences Nt (w; n) that denote the n \u2265 1 most semantically similar words (estimated by cosmic similarity on the original word2vec vectors) of the word w in the span of time. 7 In Table 3, we show two measures: Adjusted R2, which indicate the quality fit of a model, and Prediction error. By Prediction error, we measure the average euclidean distance between the true semantic vector of a word in the last time span tN against the predicted semantic vector, which is estimated by the linear model in Eq. (2), estimated on the data without prediction error."}, {"heading": "5 Concluding remarks", "text": "We proposed two new models of semantic change: first, TISS diagrams allow for the detection of gradual, non-sequential changes in meaning; second, they allow for the detection of statistical trends, possibly of a general nature; and second, our time series models allow us to study negative trends in meaning change (the \"law of differentiation\") and predict the future that we leave to future work. Both models suggest a linear behavior of semantic change that requires further investigation; we note that this linearity affects the core vocabulary of languages (in our case, words that have occurred at least 100 times in each period) and, in the case of TISS diagrams, represents an average result; certain words may exhibit drastic, non-linear changes in meaning over time (e.g. proper names that relate to completely different units)."}], "references": [{"title": "Opinion dynamics and learning in social networks", "author": ["Acemoglu", "Ozdaglar2011] Daron Acemoglu", "Asuman Ozdaglar"], "venue": "Dynamic Games and Applications,", "citeRegEx": "Acemoglu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Acemoglu et al\\.", "year": 2011}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni et al.2014] Marco Baroni", "GeorgianaDinu", "Germ\u00e1n Kruszewski"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Chen et al.2014] Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Towards semantic language classification: Inducing and clustering semantic association networks from europarl", "author": ["Eger et al.2015] Steffen Eger", "Niko Schenk", "Alexander Mehler"], "venue": "In Proceedings of the Fourth Joint Conference on Lexi-", "citeRegEx": "Eger et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Eger et al\\.", "year": 2015}, {"title": "Opinion dynamics and wisdom under out-group discrimination", "author": ["Steffen Eger"], "venue": "Mathematical Social Sciences,", "citeRegEx": "Eger.,? \\Q2016\\E", "shortCiteRegEx": "Eger.", "year": 2016}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Faruqui", "Dyer2014] Manaal Faruqui", "Chris Dyer"], "venue": "In Proceedings of EACL", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Nave learning in social networks and the wisdom of crowds", "author": ["Golub", "Jackson2010] Benjamin Golub", "Matthew O. Jackson"], "venue": "American Economic Journal: Microeconomics,", "citeRegEx": "Golub et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Golub et al\\.", "year": 2010}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "A framework for analyzing semantic change of words across time", "author": ["Jatowt", "Duh2014] Adam Jatowt", "Kevin Duh"], "venue": "In Proceedings of the Joint JCDL/TPDL Digital Libraries Conference", "citeRegEx": "Jatowt et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jatowt et al\\.", "year": 2014}, {"title": "Statistically significant detection of linguistic change", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": "In Proceedings of the 24th International Conference on World Wide Web,", "citeRegEx": "Kulkarni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "2015b. Freshman or fresher? quantifying the geographic variation of internet", "author": ["Bryan Perozzi", "Steven Skiena"], "venue": null, "citeRegEx": "Kulkarni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "Word sense induction for novel sense detection", "author": ["Lau et al.2012] Jey Han Lau", "Paul Cook", "Diana McCarthy", "David Newman", "Timothy Baldwin"], "venue": "In Proceedings of the 13th Conference of the European Chapter of the Association", "citeRegEx": "Lau et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lau et al\\.", "year": 2012}, {"title": "Exploiting similarities among languages for machine translation. CoRR, abs/1309.4168", "author": ["Quoc V. Le", "Ilya Sutskever"], "venue": null, "citeRegEx": "TomasMikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "TomasMikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "That\u2019s sick dude!: Automatic identification of word sense change across different timescales", "author": ["Mitra et al.2014] Sunny Mitra", "Ritwik Mitra", "Martin Riedl", "Chris Biemann", "Animesh Mukherjee", "Pawan Goyal"], "venue": "In Proceedings of the 52nd Annual", "citeRegEx": "Mitra et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mitra et al\\.", "year": 2014}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space", "author": ["Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "venue": "In Proceedings of the 2014 Conference on Empirical", "citeRegEx": "Neelakantan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Investigation of word senses over time using linguistic corpora", "author": ["Thomas Bartz", "Katharina Morik", "Angelika Strrer"], "venue": "In Pavel Krl and Vclav Matouek,", "citeRegEx": "Plitz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Plitz et al\\.", "year": 2015}, {"title": "Semiotic cognitive information processing: Learning to understand discourse. A systemic model of meaning constitution", "author": ["Burghard B. Rieger"], "venue": null, "citeRegEx": "Rieger.,? \\Q2003\\E", "shortCiteRegEx": "Rieger.", "year": 2003}, {"title": "Towards tracking semantic change by visual analytics", "author": ["Annette Hautli", "Thomas Mayer", "Miriam Butt", "Daniel A. Keim", "Frans Plank"], "venue": "In The 49th Annual Meeting of the Association", "citeRegEx": "Rohrdantz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rohrdantz et al\\.", "year": 2011}, {"title": "Lexicostatistic dating of prehistoric ethnic contacts", "author": ["Morris Swadesh"], "venue": "Proceedings of the American Philosophical Society,", "citeRegEx": "Swadesh.,? \\Q1952\\E", "shortCiteRegEx": "Swadesh.", "year": 1952}, {"title": "A computational evaluation of two laws of semantic change", "author": ["Xu", "Kemp2015] Y. Xu", "C. Kemp"], "venue": "In Proceedings of the 37th Annual Conference of the Cognitive Science Society", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Omnia mutantur, nihil interit: Connecting past with present by finding corresponding terms across time", "author": ["Zhang et al.2015] Yating Zhang", "Adam Jatowt", "Sourav S. Bhowmick", "Katsumi Tanaka"], "venue": "In Proceedings of the 53rd Annual Meeting", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "tend to exhibit different polysemous associations for corresponding terms (Eger et al., 2015; Kulkarni et al., 2015b).", "startOffset": 74, "endOffset": 117}, {"referenceID": 1, "context": "tics modeling (Baroni et al., 2014).", "startOffset": 14, "endOffset": 35}, {"referenceID": 17, "context": "means of their semantic similarity relations to all other words in the vocabulary (de Saussure, 1916; Rieger, 2003).", "startOffset": 82, "endOffset": 115}, {"referenceID": 18, "context": "In contrast, more finegrained analyses typically sense-label word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015).", "startOffset": 161, "endOffset": 245}, {"referenceID": 14, "context": "In contrast, more finegrained analyses typically sense-label word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015).", "startOffset": 161, "endOffset": 245}, {"referenceID": 16, "context": "In contrast, more finegrained analyses typically sense-label word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015).", "startOffset": 161, "endOffset": 245}, {"referenceID": 21, "context": "In contrast, more finegrained analyses typically sense-label word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015).", "startOffset": 161, "endOffset": 245}, {"referenceID": 7, "context": "Senselabeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al.", "startOffset": 76, "endOffset": 141}, {"referenceID": 2, "context": "Senselabeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al.", "startOffset": 76, "endOffset": 141}, {"referenceID": 15, "context": "Senselabeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al.", "startOffset": 76, "endOffset": 141}, {"referenceID": 18, "context": ", 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al., 2011; Lau et al., 2012).", "startOffset": 133, "endOffset": 175}, {"referenceID": 11, "context": ", 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al., 2011; Lau et al., 2012).", "startOffset": 133, "endOffset": 175}, {"referenceID": 21, "context": "time (words with similar meanings/roles in two time periods but potentially different lexical forms) (Zhang et al., 2015).", "startOffset": 101, "endOffset": 121}, {"referenceID": 13, "context": "An alternative to our second-order embeddings is to project vectors from different time periods in a common space (Mikolov et al., 2013a; Faruqui and Dyer, 2014), which requires to find corresponding terms across time. Further, one could also consider a \u2018core\u2019 vocabulary of semantically stable words, e.g., in the spirit of Swadesh (1952), instead of using all vocabulary words as reference.", "startOffset": 115, "endOffset": 340}, {"referenceID": 4, "context": "The model may also be seen in the socio-economic context of so-called \u201copinion dynamic models\u201d (Golub and Jackson, 2010; Acemoglu and Ozdaglar, 2011; Eger, 2016).", "startOffset": 95, "endOffset": 161}, {"referenceID": 9, "context": "We notice that the presented method can account for gradual, accumulating change, which is not possible for models that compare two succeeding time points such as the model of Kulkarni et al. (2015a).", "startOffset": 176, "endOffset": 200}], "year": 2016, "abstractText": "We consider two graph models of semantic change. The first is a time-series model that relates embedding vectors from one time period to embedding vectors of previous time periods. In the second, we construct one graph for each word: nodes in this graph correspond to time points and edge weights to the similarity of the word\u2019s meaning across two time points. We apply our two models to corpora across three different languages. We find that semantic change is linear in two senses. Firstly, today\u2019s embedding vectors (= meaning) of words can be derived as linear combinations of embedding vectors of their neighbors in previous time periods. Secondly, self-similarity of words decays linearly in time. We consider both findings as new laws/hypotheses of semantic change.", "creator": "gnuplot 4.6 patchlevel 4"}}}