{"id": "1704.02788", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Entity Linking for Queries by Searching Wikipedia Sentences", "abstract": "We present a simple yet effective approach for linking entities in queries. The key idea is to search sentences similar to a query from Wikipedia articles and directly use the human-annotated entities in the similar sentences as candidate entities for the query. Then, we employ a rich set of features, such as link-probability, context-matching, word embeddings, and relatedness among candidate entities as well as their related entities, to rank the candidates under a regression based framework. The advantages of our approach lie in two aspects, which contribute to the ranking process and final linking result. First, it can greatly reduce the number of candidate entities by filtering out irrelevant entities with the words in the query. Second, we can obtain the query sensitive prior probability in addition to the static link-probability derived from all Wikipedia articles. We conduct experiments on two benchmark datasets on entity linking for queries, namely the ERD14 dataset and the GERDAQ dataset. Experimental results show that our method outperforms state-of-the-art systems and yields 75.0% in F1 on the ERD14 dataset and 56.9% on the GERDAQ dataset.", "histories": [["v1", "Mon, 10 Apr 2017 10:19:53 GMT  (699kb,D)", "http://arxiv.org/abs/1704.02788v1", null], ["v2", "Tue, 18 Apr 2017 06:59:56 GMT  (702kb,D)", "http://arxiv.org/abs/1704.02788v2", null], ["v3", "Thu, 18 May 2017 08:03:49 GMT  (702kb,D)", "http://arxiv.org/abs/1704.02788v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chuanqi tan", "furu wei", "pengjie ren", "weifeng lv", "ming zhou 0001"], "accepted": true, "id": "1704.02788"}, "pdf": {"name": "1704.02788.pdf", "metadata": {"source": "CRF", "title": "Entity Linking for Queries by Searching Wikipedia Sentences", "authors": ["Chuanqi Tan", "Furu Wei", "Pengjie Ren", "Weifeng Lv", "Ming Zhou"], "emails": ["tanchuanqi@nlsde.buaa.edu.cn", "jay.ren@outlook.com", "mingzhou}@microsoft.com", "lwf@buaa.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "An essential part of this problem is linking information that aims to comment on entities in the query and link them to a knowledge base such as Freebase. However, the most common methods of entity linking for queries can be summarized in three steps: Mention of candidate generation, and Entity Disambiguation. The first step is to recognize candidate mentions in the query. The most common method of recognizing mentions is to search for a dictionary collected by the entity Alias in a knowledge base, and the information maintained by humans in Wikipedia (such as titles and diversions); the second step is to search for a dictionary collected by the entities."}, {"heading": "2 Related Work", "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "3 Our Approach", "text": "As illustrated in Figure 1, we introduce our approach with the \"Blake Shelton austin lyrics\" query. Our approach consists of three main phases: Sentence search, candidate generation and candidate ranking. First, we search the query in all Wikipedia articles to obtain the similar phrases. Second, we extract human commented units from these phrases. We retain the units whose corresponding anchor texts appear in the query as candidates and treat others as related entities. Specifically, in this example, we get three candidates, namely \"Blake Shelton,\" \"Austin, Texas\" and \"Austin (song).\" Finally, we use a regression-based model to classify the candidate units. We get the final annotations of \"Blake Shelton\" and \"Austin (song),\" whose values are higher than the threshold selected on the development set. In the following sections, we describe these three phases in detail."}, {"heading": "3.1 Sentence Search", "text": "Sentences in Wikipedia articles usually contain anchors that link to entities. Therefore, we are motivated to generate candidate entities based on sentence search, rather than the usual method of entity search. Secondly, there are some problems in the original annotations due to the annotation regulation. Firstly, entities on their own pages do not usually contain annotations. Therefore, we comment on these entities with text and page title consistency. Secondly, we use the Stanford CoreNLP toolkit (Manning et al., 2014) to comment on these entities when they are commented on in previous sentences on the page. Furthermore, we use the content in the Disambiguity page and in the Infobox. Although these two types of information may have an incomplete grammatical structure, it contains enough context information for sentence search in our entities."}, {"heading": "3.2 Candidate Generation", "text": "We specify the number of sentences that contain the pair (a, e), then we truncate the candidate pairs according to the following rules. First, we only keep the pair whose corresponding anchor text a appears in the query as the candidate used in previous work (Ferragina and Scaiella, 2010). Second, we follow the strategy of the long-term string set. If we have two pairs (a1, e1) and (a2, e2), while a1 is a subline of a2, we fall (a1, e1), if w (a1, e1) < w (a2, e2); this is because a2 is usually less ambiguous than a1. For example, we can ririririca (a1, e1), if w (a1, e1) < w (e2, e2), and e2. \""}, {"heading": "3.3 Candidate Ranking", "text": "We are building a regression system that leads the candidate list. (...) We are the only ones who are able to ask the candidate question. (...) We are the only ones who are able to ask the candidate question. (...) We are the only ones who are able to ask the candidate question. (...) We are the only ones who are able to ask the candidate question. (...) We are the only ones who are able to ask the candidate question. (...) We are the only ones who are able to ask the candidate question. (...) We are the only ones who are able to ask the candidate question. (...) We are the only ones who are able to ask the candidate question. (...)"}, {"heading": "4 Experiment", "text": "We conduct experiments with the ERD14 and GERDAQ datasets. We compare with several base commentators and the experimental results show that our method exceeds the baseline of these two datasets. Furthermore, we report on the parameter selection on each dataset and analyze the quality of the candidates using different methods."}, {"heading": "4.1 Dataset", "text": "ERD143 is a benchmark data set in the ERD Challenge (Carmel et al., 2014), which contains both3http: / / web-ngram.research.microsoft. com / erd2014 / Datasets.aspxlong-text track and short-text track. In this work, we focus only on the short-text track, which contains 500 queries as a development set and 500 queries as a test set. Due to the lack of training units between two knowledge bases, we use the development set to conduct model training and tuning, which can be evaluated by both Freebase and Wikipedia, as the ERD Challenge organizers provide the freebase Wikipedia mapping with 1: 1 correspondence of units between two knowledge bases. We use Wikipedia to evaluate our outcomes. GERDAQ4 is a benchmark data set to comment on entities created by Cornolti et al. (2016)."}, {"heading": "4.2 Evaluation Metric", "text": "We use average F1, designed by ERD Challenge (Carmel et al., 2014) as evaluation metrics. Specifically, we define the measure of a set of hypothetical interpretations A = {E1,..., Em} as follows: Precision = | A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-2-Precision = 2-Precision \u00b7 Recall Precision + Recall (3) The average F1 value of the evaluation set is the average of F1 for each query: AverageF1 = 1N N N N-Q = 1 F1 (qi) (4) Following the evaluation guideline in ERD14 and GERDAQ, we define the value of the recall to 1.0 if the gold binding of a query is empty and the hypothetical one is 1.0."}, {"heading": "4.3 Baseline Methods", "text": "We compare with several baselines and use the results reported by the ERD organizer and Cornolti et al. (2016). 4http: / / acube.di.unipi.it / datasetsAIDA (Hoffart et al., 2011) searches the mention using Stanford's YAGO2-based NER tagger. We select AIDA as a representative system aimed at linking documents after work in Cornolti et al. (2016). WAT (Piccinno and Ferragina, 2014) is the improved version of TagME (Ferragina and Scaiella, 2010). Magnetic IISAS (Laclavik et al., 2014) retrieves the index from Wikipedia, Freebase and Dbpedia. Then it uses Wikipedia link diagram to evaluate the similarity of candidates for disambiguation and filtering snippets. Seznam (Eckhardt et al., 2014) uses Wikipedia and Beripedia to derive candidates."}, {"heading": "4.4 Result", "text": "We report on the results of the ERD dataset and the GERDAQ dataset in Table 4 and Table 5, respectively. On the ERD14 dataset, WAT AIDA is superior, but it is still up to 10% higher than SMAPH-1, which wins the ERD challenge. SMAPH-2 improves by 2% compared to the state-of-the-art SMAPH-1 annotator. Our system performs significantly worse in this dataset than the ERD SMAPH-2 dataset by 4.2%. On the GERDAQ dataset, our system is 2.5% better than the state-of-the-art SMAPH-2 annotator. The F1 value in this dataset is much lower than the ERD dataset, because common concepts such as \"Week\" and \"Game,\" which are not recorded in the ERD dataset, are 2.5% better than the state-of-the-the-art SMAPH-2 annotator. The F1 datasets in this dataset are much better than the ERD dataset because they do not record the ERD dataset."}, {"heading": "4.5 Parameter Selection", "text": "In our framework, there are two parameters, the number of search records and the threshold for final output. We select these two parameters on the development set. In Figure 2 and Figure 3, we show the F1 score with different number of search records and thresholds. In the ERD development set, better results occur in the search numbers between 600 and 800 and the threshold between 0.55 and 0.6. In the GERDAQ development set, better results occur in the search numbers between 700 and 1000 and the threshold between 0.45 and 0.5. In our experiment, we set the number of records to 700 and the threshold to 0.56 on the ERD data set and 800 and 0.48 on the GERDAQ data set according to the F1 scores on the development set."}, {"heading": "4.6 Model Analysis", "text": "The main difference between our method and most previous work is that we create entity entity-entity entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-"}, {"heading": "5 Conclusion", "text": "In this paper, we deal with the problem of linking entities for open domain queries. We introduce a novel approach to generating candidates by searching sentences in Wikipedia for the query, then extracting the human-commented entities as candidates. We implement a regression model to classify these candidates for final output. Two experiments with the ERD dataset and the GERDAQ dataset show that our approach outperforms the base systems. In this paper, we directly use the standard marker in Lucene for similar sentences that can be improved in future work."}, {"heading": "Acknowledgments", "text": "The first author and the fourth author are supported by the National Natural Science Foundation of China (grant no. 61421003)."}], "references": [{"title": "Erd\u201914: entity recognition and disambiguation challenge", "author": ["David Carmel", "Ming-Wei Chang", "Evgeniy Gabrilovich", "Bo-June Paul Hsu", "Kuansan Wang."], "venue": "ACM SIGIR Forum. ACM, volume 48, pages 63\u201377.", "citeRegEx": "Carmel et al\\.,? 2014", "shortCiteRegEx": "Carmel et al\\.", "year": 2014}, {"title": "Ntunlp approaches to recognizing and disambiguating entities in long and short text at the erd challenge 2014", "author": ["Yen-Pin Chiu", "Yong-Siang Shih", "Yang-Yin Lee", "ChihChieh Shao", "Ming-Lun Cai", "Sheng-Lun Wei", "Hsin-Hsi Chen."], "venue": "Proceedings of the", "citeRegEx": "Chiu et al\\.,? 2014", "shortCiteRegEx": "Chiu et al\\.", "year": 2014}, {"title": "A piggyback system for joint entity mention detection and linking in web queries", "author": ["Marco Cornolti", "Paolo Ferragina", "Massimiliano Ciaramita", "Stefan R\u00fcd", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 25th International Conference on World Wide Web.", "citeRegEx": "Cornolti et al\\.,? 2016", "shortCiteRegEx": "Cornolti et al\\.", "year": 2016}, {"title": "The smaph system for query entity recognition and disambiguation", "author": ["Marco Cornolti", "Paolo Ferragina", "Massimiliano Ciaramita", "Hinrich Sch\u00fctze", "Stefan R\u00fcd."], "venue": "Proceedings of the first international workshop on Entity recognition & disam-", "citeRegEx": "Cornolti et al\\.,? 2014", "shortCiteRegEx": "Cornolti et al\\.", "year": 2014}, {"title": "Query representation and understanding workshop", "author": ["W Bruce Croft", "Michael Bendersky", "Hang Li", "Gu Xu."], "venue": "SIGIR Forum. volume 44, pages 48\u201353.", "citeRegEx": "Croft et al\\.,? 2010", "shortCiteRegEx": "Croft et al\\.", "year": 2010}, {"title": "Large-scale named entity disambiguation based on wikipedia data", "author": ["Silviu Cucerzan."], "venue": "EMNLPCoNLL. volume 7, pages 708\u2013716.", "citeRegEx": "Cucerzan.,? 2007", "shortCiteRegEx": "Cucerzan.", "year": 2007}, {"title": "Entity linking based on the cooccurrence graph and entity probability", "author": ["Alan Eckhardt", "Juraj Hre\u0161ko", "Jan Proch\u00e1zka", "Otakar Smrf."], "venue": "Proceedings of the first international workshop on Entity recognition & disambiguation. ACM, pages 37\u201344.", "citeRegEx": "Eckhardt et al\\.,? 2014", "shortCiteRegEx": "Eckhardt et al\\.", "year": 2014}, {"title": "Liblinear: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "XiangRui Wang", "Chih-Jen Lin."], "venue": "Journal of machine learning research 9(Aug):1871\u20131874.", "citeRegEx": "Fan et al\\.,? 2008", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Tagme: on-the-fly annotation of short text fragments (by wikipedia entities)", "author": ["Paolo Ferragina", "Ugo Scaiella."], "venue": "Proceedings of the 19th ACM international conference on Information and knowledge management. ACM, pages 1625\u20131628.", "citeRegEx": "Ferragina and Scaiella.,? 2010", "shortCiteRegEx": "Ferragina and Scaiella.", "year": 2010}, {"title": "Learning to link", "author": ["ICLR . David Milne", "Ian H Witten"], "venue": null, "citeRegEx": "Milne and Witten.,? \\Q2008\\E", "shortCiteRegEx": "Milne and Witten.", "year": 2008}, {"title": "Aida: An online tool for accurate disambiguation of named entities in text and tables", "author": ["Mohamed Amir Yosef", "Johannes Hoffart", "Ilaria Bordino", "Marc Spaniol", "Gerhard Weikum."], "venue": "Proceedings of the VLDB Endowment 4(12):1450\u20131453.", "citeRegEx": "Yosef et al\\.,? 2011", "shortCiteRegEx": "Yosef et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 4, "context": "Query understanding has been an important research area in information retrieval and natural language processing (Croft et al., 2010).", "startOffset": 113, "endOffset": 133}, {"referenceID": 0, "context": "This problem has been extensively studied over the recent years (Carmel et al., 2014; Usbeck et al., 2015; Cornolti et al., 2016).", "startOffset": 64, "endOffset": 129}, {"referenceID": 2, "context": "This problem has been extensively studied over the recent years (Carmel et al., 2014; Usbeck et al., 2015; Cornolti et al., 2016).", "startOffset": 64, "endOffset": 129}, {"referenceID": 1, "context": "Most work uses the knowledge base including Freebase (Chiu et al., 2014), YAGO (Yosef et al.", "startOffset": 53, "endOffset": 72}, {"referenceID": 10, "context": ", 2014), YAGO (Yosef et al., 2011) and Dbpedia (Olieman et al.", "startOffset": 14, "endOffset": 34}, {"referenceID": 1, "context": "Most work uses the knowledge base including Freebase (Chiu et al., 2014), YAGO (Yosef et al., 2011) and Dbpedia (Olieman et al., 2014). Wikify (Mihalcea and Csomai, 2007) is the very early work on linking anchor texts to Wikipedia pages. It extracts all ngrams that match Wikipedia concepts such as anchors and titles as candidates. They implement a voting scheme based on the knowledge-based and data-driven method to disambiguate candidates. Cucerzan (2007) uses four recourses to generate candidates, namely entity pages, redirecting pages, disambiguation pages, and list pages.", "startOffset": 54, "endOffset": 460}, {"referenceID": 1, "context": "Most work uses the knowledge base including Freebase (Chiu et al., 2014), YAGO (Yosef et al., 2011) and Dbpedia (Olieman et al., 2014). Wikify (Mihalcea and Csomai, 2007) is the very early work on linking anchor texts to Wikipedia pages. It extracts all ngrams that match Wikipedia concepts such as anchors and titles as candidates. They implement a voting scheme based on the knowledge-based and data-driven method to disambiguate candidates. Cucerzan (2007) uses four recourses to generate candidates, namely entity pages, redirecting pages, disambiguation pages, and list pages. Then they disambiguate candidates by calculating the similarity between the contextual information and the document as well as category tags on Wikipedia pages. Milne and Witten (2008) generate candidates by gathering all n-grams in the document, and retaining those whose probability exceeds a low threshold.", "startOffset": 54, "endOffset": 767}, {"referenceID": 8, "context": "TagME (Ferragina and Scaiella, 2010) is a very early work on entity linking in queries.", "startOffset": 6, "endOffset": 36}, {"referenceID": 8, "context": "TagME (Ferragina and Scaiella, 2010) is a very early work on entity linking in queries. It generates candidates by searching Wikipedia page titles, anchors and redirects. Then disambiguation exploits the structure of the Wikipedia graph, according to a voting scheme based on a relatedness measure inspired by Milne and Witten (2008). The improved version of TagME, named WAT (Piccinno and Ferragina, 2014), uses Jaccard-similarity between two pages\u2019 in-links as a measure of relatedness and uses PageRank to rank the candidate entities.", "startOffset": 7, "endOffset": 334}, {"referenceID": 0, "context": "Unlike the work which revolves around ranking entities for query spans, the Entity Recognition and Disambiguation (ERD) Challenge (Carmel et al., 2014) views entity linking in queries as the problem of finding multiple query interpretations.", "startOffset": 130, "endOffset": 151}, {"referenceID": 3, "context": "The SMAPH system (Cornolti et al., 2014) which wins the short-text track works in three phases: fetching, candidate-entity generation and pruning.", "startOffset": 17, "endOffset": 40}, {"referenceID": 2, "context": "They further extend SMAPH-1 to SMAPH-2 (Cornolti et al., 2016).", "startOffset": 39, "endOffset": 62}, {"referenceID": 8, "context": "ing anchor text a occurs in the query as a candidate, which has been used in previous work (Ferragina and Scaiella, 2010).", "startOffset": 91, "endOffset": 121}, {"referenceID": 7, "context": "We use LIBLINEAR (Fan et al., 2008) with L2-regularized L2loss support vector regression to train the regression model.", "startOffset": 17, "endOffset": 35}, {"referenceID": 8, "context": "Moreover, inspired by TagME (Ferragina and Scaiella, 2010), we denote freq(a) as the number of times the text a occurs in Wikipedia.", "startOffset": 28, "endOffset": 58}, {"referenceID": 0, "context": "ERD143 is a benchmark dataset in the ERD Challenge (Carmel et al., 2014), which contains both", "startOffset": 51, "endOffset": 72}, {"referenceID": 2, "context": "GERDAQ4 is a benchmark dataset to annotate entities to Wikipedia built by Cornolti et al. (2016). It contains 500 queries for training, 250 for development, and 250 for test.", "startOffset": 74, "endOffset": 97}, {"referenceID": 0, "context": "We use average F1 designed by ERD Challenge (Carmel et al., 2014) as the evaluation metrics.", "startOffset": 44, "endOffset": 65}, {"referenceID": 2, "context": "We compare with several baselines and use the results reported by the ERD organizer and Cornolti et al. (2016).", "startOffset": 88, "endOffset": 111}, {"referenceID": 8, "context": "WAT (Piccinno and Ferragina, 2014) is the improved version of TagME (Ferragina and Scaiella, 2010).", "startOffset": 68, "endOffset": 98}, {"referenceID": 6, "context": "Seznam (Eckhardt et al., 2014) uses Wikipedia and DBpedia to generate candidates.", "startOffset": 7, "endOffset": 30}, {"referenceID": 1, "context": "NTUNLP (Chiu et al., 2014) searches the query to match Freebase surface forms.", "startOffset": 7, "endOffset": 26}, {"referenceID": 3, "context": "SMAPH-1 (Cornolti et al., 2014) is the winner in the short-text track in the ERD14 Challenge.", "startOffset": 8, "endOffset": 31}, {"referenceID": 2, "context": "SMAPH-2 (Cornolti et al., 2016) is the improved version of SMAPH-1.", "startOffset": 8, "endOffset": 31}, {"referenceID": 1, "context": "We select AIDA as a representative system aiming to entity linking for documents following the work in Cornolti et al. (2016). WAT (Piccinno and Ferragina, 2014) is the improved version of TagME (Ferragina and Scaiella, 2010).", "startOffset": 103, "endOffset": 126}, {"referenceID": 0, "context": "(2016) and reported by the ERD organizer (Carmel et al., 2014).", "startOffset": 41, "endOffset": 62}, {"referenceID": 1, "context": "Results of the baseline systems are taken from Table 8 in Cornolti et al. (2016) and reported by the ERD organizer (Carmel et al.", "startOffset": 58, "endOffset": 81}, {"referenceID": 2, "context": "Results of the baseline systems are taken from Table 10 in Cornolti et al. (2016).", "startOffset": 59, "endOffset": 82}, {"referenceID": 6, "context": "Unlike the work on entity linking for documents (Eckhardt et al., 2014; Witten and Milne, 2008) that features derived from entity relations get promising results, Figure 2: F1 scores with different search numbers and thresholds on the ERD development set", "startOffset": 48, "endOffset": 95}], "year": 2017, "abstractText": "We present a simple yet effective approach for linking entities in queries. The key idea is to search sentences similar to a query from Wikipedia articles and directly use the human-annotated entities in the similar sentences as candidate entities for the query. Then, we employ a rich set of features, such as link-probability, contextmatching, word embeddings, and relatedness among candidate entities as well as their related entities, to rank the candidates under a regression based framework. The advantages of our approach lie in two aspects, which contribute to the ranking process and final linking result. First, it can greatly reduce the number of candidate entities by filtering out irrelevant entities with the words in the query. Second, we can obtain the query sensitive prior probability in addition to the static linkprobability derived from all Wikipedia articles. We conduct experiments on two benchmark datasets on entity linking for queries, namely the ERD14 dataset and the GERDAQ dataset. Experimental results show that our method outperforms state-of-the-art systems and yields 75.0% in F1 on the ERD14 dataset and 56.9% on the GERDAQ dataset.", "creator": "LaTeX with hyperref package"}}}