{"id": "1704.02801", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes", "abstract": "We consider the problem of obtaining individualized estimates for the effect of a certain treatment given observational data. The problem differs fundamentally from classical supervised learning since for each individual subject, we either observe the response with or without the treatment but never both. Hence, estimating the effect of a treatment entails a causal inference task in which we need to estimate counterfactual outcomes. To address this problem, we propose a novel multi-task learning framework in which the individuals' responses with and without the treatment are modeled as a vector-valued function that belongs to a reproducing kernel Hilbert space. Unlike previous methods for causal inference that use the G-computation formula, our approach does not obtain separate estimates for the treatment and control response surfaces, but rather obtains a joint estimate that ensures data efficiency in scenarios where the selection bias is strong. In order to be able to provide individualized measures of uncertainty in our estimates, we adopt a Bayesian approach for learning this vector-valued function using a multi-task Gaussian process prior; uncertainty is quantified via posterior credible intervals. We develop a novel risk based empirical Bayes approach for calibrating the Gaussian process hyper-parameters in a data-driven fashion based on gradient descent in which the update rule is itself learned from the data using a recurrent neural network. Experiments conducted on semi-synthetic data show that our algorithm significantly outperforms state-of-the-art causal inference methods.", "histories": [["v1", "Mon, 10 Apr 2017 11:03:36 GMT  (13kb)", "http://arxiv.org/abs/1704.02801v1", null], ["v2", "Sun, 28 May 2017 13:29:58 GMT  (204kb,D)", "http://arxiv.org/abs/1704.02801v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ahmed m alaa", "mihaela van der schaar"], "accepted": true, "id": "1704.02801"}, "pdf": {"name": "1704.02801.pdf", "metadata": {"source": "CRF", "title": "Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes", "authors": ["Ahmed M. Alaa", "Mihaela van der Schaar"], "emails": ["ahmedmalaa@ucla.edu", "mihaela.vanderschaar@eng.ox.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.02 801v 1"}, {"heading": "1 A Multi-task Learning Approach to Causal", "text": "Conclusion"}, {"heading": "1.1 Causal Inference from Observational Data", "text": "We consider an environment in which a specific treatment (e.g. a drug or an operation) is applied to a population of subjects (e.g. patients) in which each person possesses a d-dimensional characteristic Xi-X and two potential outcomes Y (1) i, Y (0) i, Y (2) i, Y (2) y which correspond to the response of the subjects with and without treatment (each). The potential outcomes Y (0) i and Y (1) i for the subjects are random variables which depend on the characteristic Xi, i.e. Yi (6) i, i) i (4) i i (i) i i (4) i (4) i (4) i) i i (4) i) i (4) i (4) i i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i i (4) i i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4 (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) i (4) (4) i (4) (4) (4) (4) (4) (4) (4)"}, {"heading": "1.2 A Multi-task Learning Framework for Causal Inference", "text": "We consider the following signal-in-white noise model for the potential results: Y (Wi) i = > fWi (Xi) x, (2) x, (4) x, (4) x, (4) x, (4) x, (4) x, (4) x, (4) x, (4) x, (4) x, (4) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, (5) x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,"}, {"heading": "1.3 Causal Multi-task Gaussian Processes (CMGPs)", "text": "The causal inference approach in (8) can be implemented through a frequentistic approach, which is defined as a backbone regression problem with tikhonov regulation. However, such an approach will not allow a form of pointwise (individualized) quantification for T (x), which is crucial for applications such as precision medicine. However, we know from representation theory that the Bavarian interpretation for (8) allows quantification of uncertainty about posterior credible intervals. (9) The construction of kernel K should take into account that the two response faces f0 (x) and f1 (x) can have different levels of heterogeneity and have different relevant characteristics."}, {"heading": "2 Risk-based Empirical Bayes Estimation of In-", "text": "An arbitrary selection for the CMGPs hyperparameters can lead to unsatisfactory performance, especially for small datasets. This can be mitigated by adjusting the previous data using empirical bayes: using a regularity class (i.e. a selection for the kernel K), which is provided by the dataset D.Denote the CMGP kernel as a function of the hyperparameters such as K (.The standard approach to implementing empirical bayes is to find an estimate for the hyperparameters that maximize the marginal probability class."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 The Dataset", "text": "We evaluated the performance of our algorithm using the semi-simulated dataset based on the Infant Health and Development Program (IHDP) introduced in [12]. The semi-simulated dataset in [12] is based on covariants from a real randomized experiment in which the effects of IHDP on subjects \"IQ test scores were evaluated at three years of age: selection bias is introduced by removing a subset of the treated population; all results (response surfaces) are simulated; the process of generating response surfaces was not developed in favor of our method: we used the non-linear default\" Response Surface B \"in [12] (also used in [15]) to generate the response surfaces; the dataset includes 747 subjects (608 control and 139 treated subjects), and there are 25 covariates associated with each subject."}, {"heading": "3.2 Benchmarks", "text": "We compared our algorithm with several state-of-the-art methods, including BART [12] (winner of the Causal Inference Data Analysis Challenge at the Atlantic Causal Inference Conference 2016), in addition to two recently developed algorithms for estimating individualized treatment effects: Causal Forests (CF) [24, 33] and Balancing Neural Networks (BNN) with the BNN 2-2 configuration (i.e. 2 initial layers and 2 representation layers) [15]. We also compared our method with a standard matching approach, k-next neighbor (k-NN) [26] and classic direct modeling approaches that use separate regression models for the two potential outcomes using Random Forests (RF) [17] and Gaussian Processes (GP) [19]."}, {"heading": "3.3 Evaluation Methodology and Criteria", "text": "We performed 10 sustained experiments to select the hyperparameters of all eligible algorithms, and 1000 experiments to evaluate the performance of the algorithms. In each experiment, we draw new values for the two potential outcomes of all subjects according to the Response Surface B model in [12]. (The same evaluation structure was used in [12] and [15].) For BART, we use the default [12]. We evaluated the performance of each algorithm by measuring its precision in estimating heterogeneous effects (PEHE) introduced in [12]. This measure reflects the accuracy of an algorithm in estimating the \"heterogeneity\" of a treatment effect; it measures the accuracy of the estimates for both the actual and the contrafactual results. The PEHE value is calculated as the root-mean-square error of the estimates for the treatment effect that follows Pi-1 (n)."}, {"heading": "3.4 Results", "text": "As expected, the k-NN algorithm performs poorly because it relies on a fixed, non-adaptive distance metric that does not cope with the selection bias. GPs \"gain in terms of tree-based algorithms (RF, BART, and CF) results from the fact that GPs assign a prior distribution to a space of smooth functions (RHKS), while tree-based algorithms calculate their estimates by averaging many non-smooth functions. That is, trees, on average, undergo many discontinuous functions, and these functions are all very coarse zero-order hold approximations for the true function, so that they need a large number of samples to be able to converge with the true function, since the true reaction function is actually smooth in a given practical environment. In the Bayesian context, this means that the ceramic hold rate of the GP is faster than the previous function, so that it requires a large number of functions to converge with the real function in a given environment."}], "references": [{"title": "Matching on the Estimated Propensity Score", "author": ["A. Abadie", "G.W. Imbens"], "venue": "Econometrica, 84(2):781-807", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Kernels for Vector-valued Functions: A Review", "author": ["M.A. Alvarez", "L. Rosasco", "N.D. Lawrence"], "venue": "Foundations and Trends R  \u00a9in Machine Learning, 4(3):195-266", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Recursive Partitioning for Heterogeneous Causal Effects", "author": ["S. Athey", "G. Imbens"], "venue": "Proceedings of the National Academy of Sciences, 113(27):7353-7360", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Doubly Robust Estimation in Missing Data and Causal Inference Models", "author": ["H. Bang", "J.M. Robins"], "venue": "Biometrics, 61(4):962-973", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Multi-task Gaussian Process Prediction", "author": ["E.V. Bonilla", "K.M. Chai", "C. Williams"], "venue": "NIPS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "and E", "author": ["L. Bottou", "J. Peters", "J. Candela", "J. Quinonero", "D. Charles", "M. Chickering", "E. Portugaly", "D. Ray", "P. Simard"], "venue": "Snelson,. Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising. JMLR, 14(1):3207-3260", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Some Practical Guidance for the Implementation of Propensity Score Matching", "author": ["M. Caliendo", "S. Kopeinig"], "venue": "Journal of Economic Surveys, 22(1):31-72", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "et al", "author": ["H.A. Chipman", "E.I. George", "R.E. McCulloch"], "venue": "BART: Bayesian Additive Regression Trees. The Annals of Applied Statistics, 4(1):266-298", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Doubly robust policy evaluation and learning", "author": ["M. Dudk", "J. Langford", "L. Li"], "venue": "ICML", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Doubly Robust Estimation of Causal Effects", "author": ["M.J. Funk", "D. Westreich", "C. Wiesen", "T. Strmer", "M.A. Brookhart", "M. Davidian"], "venue": "American Journal of Epidemiology, 173(7):761-767", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Counterfactual Prediction with Deep Instrumental Variables Networks", "author": ["J. Hartford", "G. Lewis", "K. Leyton-Brown", "M. Taddy"], "venue": "arXiv preprint arXiv:1612.09596,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Bayesian Nonparametric Modeling for Causal Inference", "author": ["J.L. Hill"], "venue": "Journal of Computational and Graphical Statistics", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Nonlinear Causal Discovery with Additive Noise Models", "author": ["P.O. Hoyer", "D. Janzing", "J.M. Mooij", "J. Peters", "B. Schlkopf"], "venue": "NIPS", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Causal Inference without Balance Checking: Coarsened Exact Matching", "author": ["S.M. Iacus", "G. King", "G. Porro"], "venue": "Political Analysis, 1-24", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning Representations for Counter-factual Inference", "author": ["F.D. Johansson", "U. Shalit", "D. Sontag"], "venue": "ICML", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "ADAM: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Estimating Individual Treatment Effect in Observational Data using Random Forest Methods", "author": ["M. Lu", "S. Sadiq", "D.J. Feaster", "H. Ishwaran"], "venue": "arXiv preprint arXiv:1701.05306", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "M", "author": ["K.E. Porter", "S. Gruber"], "venue": "J. Van Der Laan, and J. S. Sekhon. The Relative Performance of Targeted Maximum Likelihood Estimators. The International Journal of Biostatistics, 7(1):1-34", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Gaussian Processes for Machine Learning", "author": ["Carl Edward Rasmussen"], "venue": "Citeseer,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Optimal Matching for Observational Studies", "author": ["P.R. Rosenbaum"], "venue": "Journal of the American Statistical Association, 84(408): 1024-1032", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1989}, {"title": "The Central Role of the Propensity Score in Observational Studies for Causal Effects", "author": ["P.R. Rosenbaum", "D.B. Rubin"], "venue": "Biometrika, 70(1):41-55", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1983}, {"title": "Bayesian Inference for Causal Effects: The Role of Randomization", "author": ["D. B Rubin"], "venue": "The Annals of statistics, 34-58", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1978}, {"title": "Causal Inference using Potential Outcomes", "author": ["D. B Rubin"], "venue": "Journal of the American Statistical Association", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Estimating Individual Treatment Effect: Generalization Bounds and Algorithms", "author": ["U. Shalit", "F. Johansson", "D. Sontag"], "venue": "arXiv preprint arXiv:1606.03976", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "A", "author": ["S. Sniekers"], "venue": "van der Vaart. Adaptive Bayesian Credible Sets in Regression with a Gaussian Process Prior. Electronic Journal of Statistics, 9(2):2475-2527", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Matching Methods for Causal Inference: A Review and a Look Forward", "author": ["E.A. Stuart"], "venue": "Statistical Science, 25(1):1", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Batch Learning from Logged Bandit Feedback Through Counter-factual Risk Minimization", "author": ["A. Swaminathan", "T. Joachims"], "venue": "JMLR, 16(1): 1731-1755", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "A", "author": ["B. Szabo"], "venue": "van der Vaart, and H. van Zanten. Honest Bayesian Confidence Sets for the l2-norm. Journal of Statistical Planning and Inference, 166:36-51", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "A", "author": ["B. Szabo"], "venue": "van der Vaart, J. van Zanten. Frequentist Coverage of Adaptive Nonparametric Bayesian Credible Sets. The Annals of Statistics, 43(4): 1391-1428", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "A Nonparametric Bayesian Analysis of Heterogeneous Treatment Effects in Digital Experimentation", "author": ["M. Taddy", "M. Gardner", "L. Chen", "D. Draper"], "venue": "Journal of Business and Economic Statistics", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "A Simple Method for Estimating Interactions Between a Treatment and a Large Number of Covariates", "author": ["L. Tian", "A.A. Alizadeh", "A.J. Gentles", "R. Tibshirani"], "venue": "Journal of the American Statistical Association, 109(508):1517-1532", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Model Criticism for Bayesian Causal Inference", "author": ["D. Tran", "F.J. Ruiz", "S. Athey", "D.M. Blei"], "venue": "arXiv preprint arXiv:1610.09037", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Estimation and Inference of Heterogeneous Treatment Effects using Random Forests", "author": ["S. Wager", "S. Athey"], "venue": "arXiv preprint arXiv:1510.04342", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Estimating Heterogeneous Treatment Effects with Observational Data", "author": ["Y. Xie", "J.E. Brand", "B. Jann"], "venue": "Sociological Methodology, 42(1):314-347", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Multivariate Geostatistics: an Introduction with Applications", "author": ["H. Wackernagel"], "venue": "Springer Science and Business Media", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "On the Application of Probability Theory to Agricultural Experiments", "author": ["J. Splawa-Neyman", "D.M. Dabrowska", "T.P. Speed"], "venue": "Statistical Science, 5(4): 465-472", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1990}, {"title": "Learning to Learn by Gradient Descent by Gradient Descent", "author": ["M. Andrychowicz", "M. Denil", "S. Gomez", "M.W. Hoffman", "D. Pfau", "T. Schaul", "N. de Freitas"], "venue": "NIPS 2016", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}], "referenceMentions": [{"referenceID": 25, "context": "a retrospective cohort study [26, 34] or a hospital\u2019s electronic health record [15].", "startOffset": 29, "endOffset": 37}, {"referenceID": 33, "context": "a retrospective cohort study [26, 34] or a hospital\u2019s electronic health record [15].", "startOffset": 29, "endOffset": 37}, {"referenceID": 14, "context": "a retrospective cohort study [26, 34] or a hospital\u2019s electronic health record [15].", "startOffset": 79, "endOffset": 83}, {"referenceID": 5, "context": "The outcomes Y (Wi) i and Y (1\u2212Wi) i are known as the factual and the counterfactual outcomes respectively [6, 11, 15, 27].", "startOffset": 107, "endOffset": 122}, {"referenceID": 10, "context": "The outcomes Y (Wi) i and Y (1\u2212Wi) i are known as the factual and the counterfactual outcomes respectively [6, 11, 15, 27].", "startOffset": 107, "endOffset": 122}, {"referenceID": 14, "context": "The outcomes Y (Wi) i and Y (1\u2212Wi) i are known as the factual and the counterfactual outcomes respectively [6, 11, 15, 27].", "startOffset": 107, "endOffset": 122}, {"referenceID": 26, "context": "The outcomes Y (Wi) i and Y (1\u2212Wi) i are known as the factual and the counterfactual outcomes respectively [6, 11, 15, 27].", "startOffset": 107, "endOffset": 122}, {"referenceID": 0, "context": ") The conditional distribution P(Wi = 1|Xi = x), also known as the propensity score of subject i [1, 7, 21], reflects the underlying (unknown) policy for assigning the treatment to the subjects.", "startOffset": 97, "endOffset": 107}, {"referenceID": 6, "context": ") The conditional distribution P(Wi = 1|Xi = x), also known as the propensity score of subject i [1, 7, 21], reflects the underlying (unknown) policy for assigning the treatment to the subjects.", "startOffset": 97, "endOffset": 107}, {"referenceID": 20, "context": ") The conditional distribution P(Wi = 1|Xi = x), also known as the propensity score of subject i [1, 7, 21], reflects the underlying (unknown) policy for assigning the treatment to the subjects.", "startOffset": 97, "endOffset": 107}, {"referenceID": 0, "context": "Throughout this paper, we respect the standard assumptions of unconfoundedness (or ignorability) and overlap [1, 3, 8, 12, 15, 32-34].", "startOffset": 109, "endOffset": 133}, {"referenceID": 2, "context": "Throughout this paper, we respect the standard assumptions of unconfoundedness (or ignorability) and overlap [1, 3, 8, 12, 15, 32-34].", "startOffset": 109, "endOffset": 133}, {"referenceID": 7, "context": "Throughout this paper, we respect the standard assumptions of unconfoundedness (or ignorability) and overlap [1, 3, 8, 12, 15, 32-34].", "startOffset": 109, "endOffset": 133}, {"referenceID": 11, "context": "Throughout this paper, we respect the standard assumptions of unconfoundedness (or ignorability) and overlap [1, 3, 8, 12, 15, 32-34].", "startOffset": 109, "endOffset": 133}, {"referenceID": 14, "context": "Throughout this paper, we respect the standard assumptions of unconfoundedness (or ignorability) and overlap [1, 3, 8, 12, 15, 32-34].", "startOffset": 109, "endOffset": 133}, {"referenceID": 31, "context": "Throughout this paper, we respect the standard assumptions of unconfoundedness (or ignorability) and overlap [1, 3, 8, 12, 15, 32-34].", "startOffset": 109, "endOffset": 133}, {"referenceID": 32, "context": "Throughout this paper, we respect the standard assumptions of unconfoundedness (or ignorability) and overlap [1, 3, 8, 12, 15, 32-34].", "startOffset": 109, "endOffset": 133}, {"referenceID": 33, "context": "Throughout this paper, we respect the standard assumptions of unconfoundedness (or ignorability) and overlap [1, 3, 8, 12, 15, 32-34].", "startOffset": 109, "endOffset": 133}, {"referenceID": 21, "context": "The setting described above is known in the literature as the (potential outcomes) Rubin-Neyman causal model [22, 23, 36].", "startOffset": 109, "endOffset": 121}, {"referenceID": 22, "context": "The setting described above is known in the literature as the (potential outcomes) Rubin-Neyman causal model [22, 23, 36].", "startOffset": 109, "endOffset": 121}, {"referenceID": 35, "context": "The setting described above is known in the literature as the (potential outcomes) Rubin-Neyman causal model [22, 23, 36].", "startOffset": 109, "endOffset": 121}, {"referenceID": 1, "context": "We model the outcomes function f(x) as belonging to a vector-valued Reproducing Kernel Hilbert Space (vvRKHS) H [2], with a reproducing kernel K : X \u00d7X \u2192 R, where K is a symmetric matrix-valued function such that for any x, x \u2208 X ,K(x, x) is a positive semi-definite matrix.", "startOffset": 112, "endOffset": 115}, {"referenceID": 1, "context": "Using the reproducing property of the vvRKHS [2], \u3008f ,K(.", "startOffset": 45, "endOffset": 48}, {"referenceID": 1, "context": "The formulation in (3) casts the causal inference problem as a multi-task learning problem [2, 5].", "startOffset": 91, "endOffset": 97}, {"referenceID": 4, "context": "The formulation in (3) casts the causal inference problem as a multi-task learning problem [2, 5].", "startOffset": 91, "endOffset": 97}, {"referenceID": 32, "context": "Lipschitz continuity is required for the consistency of causal forests in [33]), or to ensure the adaptivity of the posterior credible sets in the Bayesian framework (e.", "startOffset": 74, "endOffset": 78}, {"referenceID": 27, "context": "f0(x) and f1(x) are assumed to belong to a Sobolev space in [28, 29]).", "startOffset": 60, "endOffset": 68}, {"referenceID": 28, "context": "f0(x) and f1(x) are assumed to belong to a Sobolev space in [28, 29]).", "startOffset": 60, "endOffset": 68}, {"referenceID": 4, "context": "Standard intrinsic correlation models for constructing vector-valued kernels impose the same covariance parameters for all outputs [5], which may lead to slow contraction rates for the posterior distribution sandwiching T (x) over the feature space.", "startOffset": 131, "endOffset": 134}, {"referenceID": 1, "context": "To that end, we construct a linear model of coregionalization (LMC) [2,35], which mixes two intrinsic correlation models as follows", "startOffset": 68, "endOffset": 74}, {"referenceID": 34, "context": "To that end, we construct a linear model of coregionalization (LMC) [2,35], which mixes two intrinsic correlation models as follows", "startOffset": 68, "endOffset": 74}, {"referenceID": 36, "context": "Since different update rules may work well for different regularity classes for the outcomes function f , we follow the approach in [37], and we \u201clearn to learn\u201d the update rule by assuming a general functional form for the hyper-parameters update as follows:", "startOffset": 132, "endOffset": 136}, {"referenceID": 11, "context": "We evaluated the performance of our algorithm through the semi-simulated dataset based on the Infant Health and Development Program (IHDP) introduced in [12].", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "The semi-simulated dataset in [12] is based on covariates from", "startOffset": 30, "endOffset": 34}, {"referenceID": 11, "context": "The response surface data generation process was not designed to favor our method: we used the standard non-linear \u201cResponse Surface B\u201d setting in [12] (also used in [15]) to generate the response surfaces.", "startOffset": 147, "endOffset": 151}, {"referenceID": 14, "context": "The response surface data generation process was not designed to favor our method: we used the standard non-linear \u201cResponse Surface B\u201d setting in [12] (also used in [15]) to generate the response surfaces.", "startOffset": 166, "endOffset": 170}, {"referenceID": 11, "context": "We compared our algorithm to various state-of-the-art methods including BART [12] (the winner of the Causal Inference Data Analysis Challenge at the 2016 Atlantic Causal Inference Conference), in addition to two recently developed algorithms for estimating individualized treatment effects: Causal Forests (CF) [24, 33] and Balancing Neural Networks (BNN) with the BNN-2-2 configuration (i.", "startOffset": 77, "endOffset": 81}, {"referenceID": 23, "context": "We compared our algorithm to various state-of-the-art methods including BART [12] (the winner of the Causal Inference Data Analysis Challenge at the 2016 Atlantic Causal Inference Conference), in addition to two recently developed algorithms for estimating individualized treatment effects: Causal Forests (CF) [24, 33] and Balancing Neural Networks (BNN) with the BNN-2-2 configuration (i.", "startOffset": 311, "endOffset": 319}, {"referenceID": 32, "context": "We compared our algorithm to various state-of-the-art methods including BART [12] (the winner of the Causal Inference Data Analysis Challenge at the 2016 Atlantic Causal Inference Conference), in addition to two recently developed algorithms for estimating individualized treatment effects: Causal Forests (CF) [24, 33] and Balancing Neural Networks (BNN) with the BNN-2-2 configuration (i.", "startOffset": 311, "endOffset": 319}, {"referenceID": 14, "context": "2 output layers and 2 representation layers) [15].", "startOffset": 45, "endOffset": 49}, {"referenceID": 25, "context": "We also compare our method with a standard matching approach, k-nearest neighbor (k-NN) [26], and classical direct modeling approaches that fit separate regression models for the two potential outcomes using Random Forests (RF) [17] and Gaussian Processes (GP) [19].", "startOffset": 88, "endOffset": 92}, {"referenceID": 16, "context": "We also compare our method with a standard matching approach, k-nearest neighbor (k-NN) [26], and classical direct modeling approaches that fit separate regression models for the two potential outcomes using Random Forests (RF) [17] and Gaussian Processes (GP) [19].", "startOffset": 228, "endOffset": 232}, {"referenceID": 18, "context": "We also compare our method with a standard matching approach, k-nearest neighbor (k-NN) [26], and classical direct modeling approaches that fit separate regression models for the two potential outcomes using Random Forests (RF) [17] and Gaussian Processes (GP) [19].", "startOffset": 261, "endOffset": 265}, {"referenceID": 11, "context": "In each experiment, we draw new values for the two potential outcomes of all subjects according to the \u201cResponse Surface B\u201d model in [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "(The same evaluation setup was used in [12] and [15].", "startOffset": 39, "endOffset": 43}, {"referenceID": 14, "context": "(The same evaluation setup was used in [12] and [15].", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": ") For BART, we use the default prior as in [12].", "startOffset": 43, "endOffset": 47}, {"referenceID": 11, "context": "We evaluated the performance of every algorithm by measuring its Precision in Estimating Heterogeneous Effects (PEHE) metric introduced in [12].", "startOffset": 139, "endOffset": 143}, {"referenceID": 24, "context": "In the Bayesian context, this translates in the GP\u2019s posterior contraction rate being faster than that for treebased algorithms [25, 28, 29], and hence the estimated treatment effect function T\u0302 (x) converges more quickly to the true function T (x) for a given x.", "startOffset": 128, "endOffset": 140}, {"referenceID": 27, "context": "In the Bayesian context, this translates in the GP\u2019s posterior contraction rate being faster than that for treebased algorithms [25, 28, 29], and hence the estimated treatment effect function T\u0302 (x) converges more quickly to the true function T (x) for a given x.", "startOffset": 128, "endOffset": 140}, {"referenceID": 28, "context": "In the Bayesian context, this translates in the GP\u2019s posterior contraction rate being faster than that for treebased algorithms [25, 28, 29], and hence the estimated treatment effect function T\u0302 (x) converges more quickly to the true function T (x) for a given x.", "startOffset": 128, "endOffset": 140}], "year": 2017, "abstractText": "We consider the problem of obtaining individualized estimates for the effect of a certain treatment given observational data. The problem differs fundamentally from classical supervised learning since for each individual subject, we either observe the response with or without the treatment but never both. Hence, estimating the effect of a treatment entails a causal inference task in which we need to estimate counterfactual outcomes. To address this problem, we propose a novel multi-task learning framework in which the individuals\u2019 responses with and without the treatment are modeled as a vector-valued function that belongs to a reproducing kernel Hilbert space. Unlike previous methods for causal inference that use the G-computation formula, our approach does not obtain separate estimates for the treatment and control response surfaces, but rather obtains a joint estimate that ensures data efficiency in scenarios where the selection bias is strong. In order to be able to provide individualized measures of uncertainty in our estimates, we adopt a Bayesian approach for learning this vector-valued function using a multi-task Gaussian process prior; uncertainty is quantified via posterior credible intervals. We develop a novel risk based empirical Bayes approach for calibrating the Gaussian process hyper-parameters in a data-driven fashion based on gradient descent in which the update rule is itself learned from the data using a recurrent neural network. Experiments conducted on semi-synthetic data show that our algorithm significantly outperforms state-of-the-art causal inference methods.", "creator": "LaTeX with hyperref package"}}}