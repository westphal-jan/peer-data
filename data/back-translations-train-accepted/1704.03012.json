{"id": "1704.03012", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Stochastic Neural Networks for Hierarchical Reinforcement Learning", "abstract": "Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.", "histories": [["v1", "Mon, 10 Apr 2017 18:41:28 GMT  (6737kb,D)", "http://arxiv.org/abs/1704.03012v1", "Published as a conference paper at ICLR 2017"]], "COMMENTS": "Published as a conference paper at ICLR 2017", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE cs.RO", "authors": ["carlos florensa", "yan duan", "pieter abbeel"], "accepted": true, "id": "1704.03012"}, "pdf": {"name": "1704.03012.pdf", "metadata": {"source": "CRF", "title": "STOCHASTIC NEURAL NETWORKS FOR HIERARCHICAL REINFORCEMENT LEARNING", "authors": ["Carlos Florensa", "Yan Duan", "Pieter Abbeel"], "emails": ["florensa@berkeley.edu,", "rocky@openai.com", "pieter@openai.com"], "sections": [{"heading": null, "text": "To address these important issues, we propose a general framework that first learns useful skills in a pre-training environment and then harnesses the acquired skills for faster learning in downstream tasks. Our approach combines some of the strengths of intrinsic motivation and hierarchical methods: learning useful skills is guided by a single substitutional reward that requires very minimal expertise in the design of downstream tasks. Subsequently, a high-level policy is trained in addition to these skills, which significantly improves exploration and allows for sparse rewards in downstream tasks. To efficiently pre-train a wide range of skills, we use stochastic neural networks combined with an information theory regulator. Our experiments 1 show that this combination is effective in learning a wide range of interpretable skills in a random manner, and can enhance learning performance in combination with a wide range of unified tasks."}, {"heading": "1 INTRODUCTION", "text": "In fact, the fact is that most of them are able to move to another world in which they are able to live."}, {"heading": "2 RELATED WORK", "text": "In fact, it is a pure disinfectant capable of defeating the disease."}, {"heading": "3 PRELIMINARIES", "text": "We define a discrete finite time horizon with a discounted Markov decision-making process (MDP) by a tuple M = (S, A, P, r, \u03c10, \u03b3, T), in which S is a state set, A is a plot set, P: S \u00b7 A \u00b7 S \u2192 R + is a probability distribution for the transition, r: S \u00b7 A \u2192 [\u2212 Rmax, Rmax] is a limited reward function, 0: S \u2192 R + is an initial state distribution, \u03b3 [0, 1] is a discount factor, and T is the horizon. If necessary, we add a suffix to the symbols to resolve ambiguities, such as SM. In political search methods, we typically optimize a stochastic policy approach: S \u00b7 A \u2192 R + parameterized by the arrangement. The goal is to maximize the expected discounted return."}, {"heading": "4 PROBLEM STATEMENT", "text": "Our main interest is in solving a set of downstream tasks specified by a collection of MDPs M. If these tasks do not share a common structure, we cannot expect to acquire a set of skills that will accelerate learning for all of them. On the other hand, we want the structural assumptions to be minimal in order to make our problem more generally applicable to all MDPs. For each MDPs, we assume that the state space SM can be included in two components, agent and SMrest, which interact only weakly with each other. The agent should be the same for all MDPs inM. We also assume that all MDPs share the same action space. Intuitively, we see a robot facing a collection of tasks where the dynamics of the robot are shared across tasks and covered by Sagent, but there may be other components in a task-specific state space that Srest calls a structural task."}, {"heading": "5 METHODOLOGY", "text": "In this section, we describe our formulation for solving a cluster of tasks using the structural assumption articulated above. In paragraph 5.1, we describe the pre-training environment in which we learn a useful range of skills based on proxy rewards. In paragraph 5.2, we motivate the use of stochastic neural networks (SNNs) and discuss the architectural design decisions that are made to adapt them to learning skills for RL. In paragraph 5.3, we describe an information theoretical regulator that further improves the range of skills acquired by SNNs. In paragraph 5.4, we describe the architecture of high-level strategies regarding acquired skills and the training process for downstream tasks with low rewards rewards rewards rewards rewards rewards rewards. Finally, in paragraph 5.5, we describe the policy optimization for both phases of training."}, {"heading": "5.1 CONSTRUCTING THE PRE-TRAINING ENVIRONMENT", "text": "Considering a collection of tasks, we would like to create a pre-training environment in which the agent can learn a range of skills that is useful to improve exploration in downstream tasks. We achieve this by letting the agent interact freely with the environment in a minimal setup. For a mobile robot, this can be a spacious environment in which the robot can first learn the necessary locomotion skills; for a manipulator arm used for object manipulation tasks, this can be an environment with many objects that the robot can interact with. Skills learned in this environment depend on the reward given to the agent. Instead of specifying different rewards in the pre-training environment corresponding to the desired skills, we require a generic proxy reward as the only reward signal for leadership skills learning. The design of the proxy reward should promote the existence of optimal solutions locally that should meet the different skills."}, {"heading": "5.2 STOCHASTIC NEURAL NETWORKS FOR SKILL LEARNING", "text": "This year is the highest in the history of the country."}, {"heading": "5.3 INFORMATION-THEORETIC REGULARIZATION", "text": "It is desirable to have direct control over the diversity of skills to be learned. To achieve this, we must introduce an information-theoretical regulator that reflects the recent success of similar goals in promoting interpretative skills in InfoGAN Chen et al. (2016).Specifically, we add an additional reward bonus corresponding to the mutual information (MI) between the latent variable and the current state. We measure the MI only in relation to a relevant subset of the state. For a mobile robot, we choose this to be the c = (x, y) coordinates of its mass center (CoM). Formally, we let C be a random variable indicating the current CoM coordinate of the agent, and let Z be the variable of the variable."}, {"heading": "5.4 LEARNING HIGH-LEVEL POLICIES", "text": "Given the range of K skills we learned during the pre-training task, we now describe how we can use them as basic building blocks for solving tasks where there are scant reward signals available. Instead of learning the low controls from scratch, we use the skills offered by freezing them and training a high-level policy (Manager Neural Network) that works by selecting a skill and committing to a certain number of steps. The imposed time consistency and quality of the skills (in our case by optimizing the proxy reward in the pre-train environment) lead to improved exploration that allows us to solve the downstream tasks with sparse rewards. For each given task, we train a new NN manager in addition to the common skills. Given the factorized representation of the state space SM as an agent and SMrest, the high-level policy we choose to allocate to the state as the available options, and the allocation of the input accordingly gives a full range of options."}, {"heading": "5.5 POLICY OPTIMIZATION", "text": "We choose TRPO because of its excellent empirical performance and because it does not require excessive optimization of hyperparameters. Training in downstream tasks does not require any changes, except that the scope of action is now the set of skills that can be used. In the pre-school phase, it can even be difficult to calculate them due to the presence of categorical latent variables. To avoid this problem, we consider the latent code as part of the observation. Given that the latent distribution of \u03c0 (a | s) is a mixture of Gaussian instead of a simple Gaussian, TRPO can be applied without any modification. To avoid this problem, we consider the latent code as part of the observation. Given that \u03c0 (a | s, z) is still a Gaussian, TRPO can be applied without any modification."}, {"heading": "6 EXPERIMENTS", "text": "We have applied our framework to the two hierarchical tasks described in the benchmark by Duan et al. (2016): Locomotion + Maze and Locomotion + Food Collection (Gather). The observation space of these tasks naturally breaks down into Sagent, the robot, and SMrest the task-specific attributes such as walls, targets, and sensor measurements. Here, we report on the results obtained with the swimmer robot, which is also described in the benchmark work. In fact, the float locomotion task described therein corresponds exactly to our pretrain task, as we also exclusively reward speed in a simple environment. We report the results with more complex robots in Appendix C-D. To increase the variety of downstream tasks, we have constructed four different labyrinths. Maze 0 is the same as the task described in the benchmark (Duan et al., 2016) and Maze 1 is its reflection, where the robot must go backwards-left instead of-right."}, {"heading": "7 RESULTS", "text": "We evaluate each step of the skill learning process and show the relevance of the different parts of our architecture and how they affect the exploration achieved in hierarchical use. We then report on the results5 to the sparse environments described above. We try to answer the following questions: \u2022 Can the multimodality of SNNs and the MI bonus consistently produce a wide range of skills? \u2022 Can the pre-training experience improve exploration in downstream environments? \u2022 Does improved exploration help to efficiently solve sparse complex tasks?"}, {"heading": "7.1 SKILL LEARNING IN PRETRAIN", "text": "To evaluate the variety of skills learned, we use \"visitation plots,\" which show the (x, y) position of the robotic center of mass (CoM) during 100 rollouts of 500 time steps each. At the beginning of each rollout, we reset the robot to its origin, always in the same orientation (as during training). In Fig. 4 (a), we show the visitation plot of six different feed-forward strategies, each of which is trained from the ground up in our pre-training environment. In order to better graphically interpret and compare it with the next plots of SNN strategies, Fig. 4 (b) overlays a series of 50 rollouts for each of the 6 strategies, each with a different color. Given the morphology of the swimmer, it has a natural preference for forward and backward movements. Therefore, if no additional stimulus is added, the visitation focuses heavily on the direction in which it is always initialized."}, {"heading": "7.2 HIERARCHICAL USE OF SKILLS", "text": "The hierarchical architectures we propose have a direct impact on the areas covered by random exploration. We will illustrate it with plots showing the visit to the (x \u2212 y) position of the Center for Mass (CoM) during individual rollouts of a million steps, each generated with a different architecture. On the one hand, in Fig. 5 (a) we show the exploration obtained with actions from a Gaussian with sizes 0 and 4, similar to what is achieved in the first iteration of a multi-layer perceptron with normalized random initializations of weights. This noise is relatively large because the floating robot has actions that are truncated on [\u2212 1, 1] it does not produce good exploration as achieved by the robot during the one million steps."}, {"heading": "7.3 MAZES AND GATHER TASKS", "text": "In Fig. 6, we evaluate the learning curves of the various hierarchical architectures that are proposed. Due to the small number of these tasks, none can be correctly solved by standard reinforcement algorithms (Duan et al., 2016). Therefore, we compare our methods against a better starting point: Adding to the downstream task the same center of mass (CoM) proxy reward that was granted to the robot in the upstream task. This baseline performs quite poorly in all labyrinths, Fig. 6 (a) -6 (c). This is due to the long time horizon needed to achieve the goal and the associated credit problem. Moreover, the proxy reward alone does not promote the variety of actions as the MI bonus for SNN. The proposed hierarchical architectures are able to learn much faster in each new MDP, since they effectively shrink the time horizon by agging time steps at useful prices."}, {"heading": "8 DISCUSSION AND FUTURE WORK", "text": "We propose a framework in which first a range of different skills are learned using stochastic neural networks, equipped with a minimum of monitoring, and then use these skills in a hierarchical architecture to solve challenging tasks with sparse rewards. Our framework successfully combines two parts: first, an unsupervised process to learn a wide range of skills by using proxy rewards, and second, a hierarchical structure that encompasses the latter range of skills and allows them to be reused in future tasks. Second, the range of skills learning can be greatly improved by using stochastic neural networks as measures and their additional expressiveness and multimodality. Bilinear integration and mutual information bonuses are the key to achieving a broad, interpretable range of skills."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported in part by DARPA, the Berkeley Vision and Learning Center (BVLC), the Berkeley Artificial Intelligence Research (BAIR) Laboratory, the Berkeley Deep Drive (BDD) and the ONR through a PECASE Award. Carlos Florensa was also supported by a La Caixa Ph.D. Fellowship and Yan Duan by a BAIR Fellowship and a Huawei Fellowship."}, {"heading": "A HYPERPARAMETERS", "text": "All neural networks (each of the multi-policy networks, the SNN and the Manager Network) have 2 layers of 32 hidden units. For SNN training, the mesh density used to grid the (x, y) space and grant the MI bonus is 10 divisions / units. The number of skills trained (i.e. the dimension of the latent variables in the SNN or the number of independently trained policies in the multi-policy setup) is 6. The batch size and the maximum path length for the lecture paper are also used in the benchmark (Duan et al., 2016): 50,000 and 500 respectively. For downstream tasks see Tab. 1."}, {"heading": "B RESULTS FOR GATHER WITH BENCHMARK SETTINGS", "text": "This year, it has reached the point where it will be able to retaliate."}], "references": [{"title": "The option-critic architecture", "author": ["Pierre-Luc Bacon", "Doina Precup"], "venue": "arXiv preprint arXiv:1609.05140v2,", "citeRegEx": "Bacon and Precup.,? \\Q2016\\E", "shortCiteRegEx": "Bacon and Precup.", "year": 2016}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["Marc G Bellemare", "Sriram Srinivasan", "Georg Ostrovski", "Tom Schaul", "David Saxton", "Remi Munos"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Intrinsically motivated reinforcement learning", "author": ["Nuttapong Chentanez", "Andrew G Barto", "Satinder P Singh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chentanez et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Chentanez et al\\.", "year": 2004}, {"title": "Gaussian-bernoulli deep boltzmann machine", "author": ["Kyung Hyun Cho", "Tapani Raiko", "Alexander Ilin"], "venue": "In International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "Cho et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2013}, {"title": "Hierarchical relative entropy policy search", "author": ["Christian Daniel", "Gerhard Neumann", "Jan Peters"], "venue": "In AISTATS, pp", "citeRegEx": "Daniel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Daniel et al\\.", "year": 2012}, {"title": "Autonomous reinforcement learning with hierarchical reps", "author": ["Christian Daniel", "Gerhard Neumann", "Jan Peters"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Daniel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Daniel et al\\.", "year": 2013}, {"title": "Learning modular neural network policies for multi-task and multi-robot transfer", "author": ["Coline Devin", "Abhishek Gupta", "Trevor Darrell", "Pieter Abbeel", "Sergey Levine"], "venue": "In International Conference on Robotics and Automation,", "citeRegEx": "Devin et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Devin et al\\.", "year": 2017}, {"title": "Hierarchical reinforcement learning with the maxq value function decomposition", "author": ["Thomas G Dietterich"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich.", "year": 2000}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Yan Duan", "Xi Chen", "Rein Houthooft", "John Schulman", "Pieter Abbeel"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach"], "venue": null, "citeRegEx": "Fukui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fukui et al\\.", "year": 2016}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Deep learning for real-time atari game play using offline monte-carlo tree search planning", "author": ["Xiaoxiao Guo", "Satinder Singh", "Honglak Lee", "Richard L Lewis", "Xiaoshi Wang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Learning continuous control policies by stochastic value gradients", "author": ["Nicolas Heess", "Gregory Wayne", "David Silver", "Tim Lillicrap", "Tom Erez", "Yuval Tassa"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Heess et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2015}, {"title": "Learning and transfer of modulated locomotor controllers", "author": ["Nicolas Heess", "Greg Wayne", "Yuval Tassa", "Timothy Lillicrap", "Martin Riedmiller", "David Silver"], "venue": "arXiv preprint arXiv:1610.05182,", "citeRegEx": "Heess et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2016}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Geoffrey E Hinton"], "venue": "Neural Computation,", "citeRegEx": "Hinton.,? \\Q2002\\E", "shortCiteRegEx": "Hinton.", "year": 2002}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Variational information maximizing exploration", "author": ["Rein Houthooft", "Xi Chen", "Yan Duan", "John Schulman", "Filip De Turck", "Pieter Abbeel"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Houthooft et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Houthooft et al\\.", "year": 2016}, {"title": "Categorical reparameterization with gumbel-softmax", "author": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Jang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Jang et al\\.", "year": 2017}, {"title": "Building portable options: Skill transfer in reinforcement learning", "author": ["George Konidaris", "Andrew G Barto"], "venue": "In IJCAI,", "citeRegEx": "Konidaris and Barto.,? \\Q2007\\E", "shortCiteRegEx": "Konidaris and Barto.", "year": 2007}, {"title": "Autonomous skill acquisition on a mobile manipulator", "author": ["George Konidaris", "Scott Kuindersma", "Roderic A Grupen", "Andrew G Barto"], "venue": "In AAAI,", "citeRegEx": "Konidaris et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Konidaris et al\\.", "year": 2011}, {"title": "Bayesian multi-task reinforcement learning", "author": ["Alessandro Lazaric", "Mohammad Ghavamzadeh"], "venue": "In 27th International Conference on Machine Learning,", "citeRegEx": "Lazaric and Ghavamzadeh.,? \\Q2010\\E", "shortCiteRegEx": "Lazaric and Ghavamzadeh.", "year": 2010}, {"title": "Representational power of restricted boltzmann machines and deep belief networks", "author": ["Nicolas Le Roux", "Yoshua Bengio"], "venue": "Neural Computation,", "citeRegEx": "Roux and Bengio.,? \\Q2008\\E", "shortCiteRegEx": "Roux and Bengio.", "year": 2008}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "The concrete distribution: A continuous relaxation of discrete random variables", "author": ["Chris J Maddison", "Andriy Mnih", "Yee Whye Teh"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Maddison et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Maddison et al\\.", "year": 2017}, {"title": "Dynamic abstraction in reinforcement learning via clustering", "author": ["Shie Mannor", "Ishai Menache", "Amit Hoze", "Uri Klein"], "venue": "In 21st International Conference on Machine Learning,", "citeRegEx": "Mannor et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2004}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["Volodymyr Mnih", "John Agapiou", "Simon Osindero", "Alex Graves", "Oriol Vinyals", "Koray Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Learning stochastic feedforward networks", "author": ["Radford M Neal"], "venue": "Department of Computer Science, University of Toronto,", "citeRegEx": "Neal.,? \\Q1990\\E", "shortCiteRegEx": "Neal.", "year": 1990}, {"title": "Connectionist learning of belief networks", "author": ["Radford M Neal"], "venue": "Artificial intelligence,", "citeRegEx": "Neal.,? \\Q1992\\E", "shortCiteRegEx": "Neal.", "year": 1992}, {"title": "Generalization and exploration via randomized value functions", "author": ["Ian Osband", "Benjamin Van Roy", "Zheng Wen"], "venue": "arXiv preprint arXiv:1402.0635,", "citeRegEx": "Osband et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2014}, {"title": "Reinforcement learning with hierarchies of machines", "author": ["Ronald Parr", "Stuart Russell"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Parr and Russell.,? \\Q1998\\E", "shortCiteRegEx": "Parr and Russell.", "year": 1998}, {"title": "Nonparametric bayesian reward segmentation for skill discovery using inverse reinforcement learning", "author": ["Pravesh Ranchod", "Benjamin Rosman", "George Konidaris"], "venue": "In International Conference on Intelligent Robots and Systems,", "citeRegEx": "Ranchod et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranchod et al\\.", "year": 2015}, {"title": "Learning movement primitives", "author": ["Stefan Schaal", "Jan Peters", "Jun Nakanishi", "Auke Ijspeert"], "venue": "In Robotics Research. The Eleventh International Symposium,", "citeRegEx": "Schaal et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Schaal et al\\.", "year": 2005}, {"title": "Curious model-building control systems", "author": ["J\u00fcrgen Schmidhuber"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q1991\\E", "shortCiteRegEx": "Schmidhuber.", "year": 1991}, {"title": "Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010)", "author": ["J\u00fcrgen Schmidhuber"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Schmidhuber.,? \\Q2010\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2010}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Abbeel", "M.I. Jordan", "P. Moritz"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Schulman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2016}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Identifying useful subgoals in reinforcement learning by local graph partitioning", "author": ["\u00d6zg\u00fcr \u015eim\u015fek", "Alicia P Wolfe", "Andrew G Barto"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "\u015eim\u015fek et al\\.,? \\Q2005\\E", "shortCiteRegEx": "\u015eim\u015fek et al\\.", "year": 2005}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["Paul Smolensky"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Smolensky.,? \\Q1986\\E", "shortCiteRegEx": "Smolensky.", "year": 1986}, {"title": "Learning options in reinforcement learning", "author": ["Martin Stolle", "Doina Precup"], "venue": "In International Symposium on Abstraction, Reformulation, and Approximation,", "citeRegEx": "Stolle and Precup.,? \\Q2002\\E", "shortCiteRegEx": "Stolle and Precup.", "year": 2002}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Learning stochastic feedforward neural networks", "author": ["Yichuan Tang", "Ruslan R Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Tang and Salakhutdinov.,? \\Q2013\\E", "shortCiteRegEx": "Tang and Salakhutdinov.", "year": 2013}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["Matthew E Taylor", "Peter Stone"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Taylor and Stone.,? \\Q2009\\E", "shortCiteRegEx": "Taylor and Stone.", "year": 2009}, {"title": "A generalized iterative lqg method for locally-optimal feedback control of constrained nonlinear stochastic systems", "author": ["Emanuel Todorov", "Weiwei Li"], "venue": "In American Control Conference,", "citeRegEx": "Todorov and Li.,? \\Q2005\\E", "shortCiteRegEx": "Todorov and Li.", "year": 2005}, {"title": "Intrinsically motivated hierarchical skill learning in structured environments", "author": ["Christopher M Vigorito", "Andrew G Barto"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Vigorito and Barto.,? \\Q2010\\E", "shortCiteRegEx": "Vigorito and Barto.", "year": 2010}, {"title": "Embed to control: A locally linear latent dynamics model for control from raw images", "author": ["Manuel Watter", "Jost Springenberg", "Joschka Boedecker", "Martin Riedmiller"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Watter et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Watter et al\\.", "year": 2015}, {"title": "Multi-task reinforcement learning: a hierarchical bayesian approach", "author": ["Aaron Wilson", "Alan Fern", "Soumya Ray", "Prasad Tadepalli"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Wilson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2007}, {"title": "On multiplicative integration with recurrent neural networks", "author": ["Yuhuai Wu", "Saizheng Zhang", "Ying Zhang", "Yoshua Bengio", "Ruslan Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "2016): maximum path length of 500 and batch-size of 50k. Our SNN hierarchical approach outperforms state-of-the-art intrinsic motivation results like VIME (Houthooft et al., 2016). The baseline of having a Center of Mass speed intrinsic reward in the task happens to be stronger than expected", "author": ["Duan"], "venue": null, "citeRegEx": "Duan,? \\Q2016\\E", "shortCiteRegEx": "Duan", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "In recent years, deep reinforcement learning has achieved many impressive results, including playing Atari games from raw pixel inputs (Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015), mastering the game of Go (Silver et al.", "startOffset": 135, "endOffset": 195}, {"referenceID": 27, "context": "In recent years, deep reinforcement learning has achieved many impressive results, including playing Atari games from raw pixel inputs (Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015), mastering the game of Go (Silver et al.", "startOffset": 135, "endOffset": 195}, {"referenceID": 37, "context": "In recent years, deep reinforcement learning has achieved many impressive results, including playing Atari games from raw pixel inputs (Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015), mastering the game of Go (Silver et al.", "startOffset": 135, "endOffset": 195}, {"referenceID": 39, "context": ", 2015), mastering the game of Go (Silver et al., 2016), and acquiring advanced manipulation and locomotion skills from raw sensory inputs (Schulman et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 37, "context": ", 2016), and acquiring advanced manipulation and locomotion skills from raw sensory inputs (Schulman et al., 2015; Lillicrap et al., 2015; Watter et al., 2015; Schulman et al., 2016; Heess et al., 2015; Levine et al., 2016).", "startOffset": 91, "endOffset": 223}, {"referenceID": 24, "context": ", 2016), and acquiring advanced manipulation and locomotion skills from raw sensory inputs (Schulman et al., 2015; Lillicrap et al., 2015; Watter et al., 2015; Schulman et al., 2016; Heess et al., 2015; Levine et al., 2016).", "startOffset": 91, "endOffset": 223}, {"referenceID": 48, "context": ", 2016), and acquiring advanced manipulation and locomotion skills from raw sensory inputs (Schulman et al., 2015; Lillicrap et al., 2015; Watter et al., 2015; Schulman et al., 2016; Heess et al., 2015; Levine et al., 2016).", "startOffset": 91, "endOffset": 223}, {"referenceID": 38, "context": ", 2016), and acquiring advanced manipulation and locomotion skills from raw sensory inputs (Schulman et al., 2015; Lillicrap et al., 2015; Watter et al., 2015; Schulman et al., 2016; Heess et al., 2015; Levine et al., 2016).", "startOffset": 91, "endOffset": 223}, {"referenceID": 13, "context": ", 2016), and acquiring advanced manipulation and locomotion skills from raw sensory inputs (Schulman et al., 2015; Lillicrap et al., 2015; Watter et al., 2015; Schulman et al., 2016; Heess et al., 2015; Levine et al., 2016).", "startOffset": 91, "endOffset": 223}, {"referenceID": 23, "context": ", 2016), and acquiring advanced manipulation and locomotion skills from raw sensory inputs (Schulman et al., 2015; Lillicrap et al., 2015; Watter et al., 2015; Schulman et al., 2016; Heess et al., 2015; Levine et al., 2016).", "startOffset": 91, "endOffset": 223}, {"referenceID": 9, "context": "Despite these success stories, these deep RL algorithms typically employ naive exploration strategies such as -greedy or uniform Gaussian exploration noise, which have been shown to perform poorly in tasks with sparse rewards (Duan et al., 2016; Houthooft et al., 2016; Bellemare et al., 2016).", "startOffset": 226, "endOffset": 293}, {"referenceID": 17, "context": "Despite these success stories, these deep RL algorithms typically employ naive exploration strategies such as -greedy or uniform Gaussian exploration noise, which have been shown to perform poorly in tasks with sparse rewards (Duan et al., 2016; Houthooft et al., 2016; Bellemare et al., 2016).", "startOffset": 226, "endOffset": 293}, {"referenceID": 1, "context": "Despite these success stories, these deep RL algorithms typically employ naive exploration strategies such as -greedy or uniform Gaussian exploration noise, which have been shown to perform poorly in tasks with sparse rewards (Duan et al., 2016; Houthooft et al., 2016; Bellemare et al., 2016).", "startOffset": 226, "endOffset": 293}, {"referenceID": 31, "context": "The challenge is further complicated by long horizons, where naive exploration strategies can lead to exponentially large sample complexity (Osband et al., 2014).", "startOffset": 140, "endOffset": 161}, {"referenceID": 43, "context": "To tackle these challenges, two main strategies have been pursued: The first strategy is to design a hierarchy over the actions (Parr & Russell, 1998; Sutton et al., 1999; Dietterich, 2000).", "startOffset": 128, "endOffset": 189}, {"referenceID": 8, "context": "To tackle these challenges, two main strategies have been pursued: The first strategy is to design a hierarchy over the actions (Parr & Russell, 1998; Sutton et al., 1999; Dietterich, 2000).", "startOffset": 128, "endOffset": 189}, {"referenceID": 35, "context": "The second strategy uses intrinsic rewards to guide exploration (Schmidhuber, 1991; 2010; Houthooft et al., 2016; Bellemare et al., 2016).", "startOffset": 64, "endOffset": 137}, {"referenceID": 17, "context": "The second strategy uses intrinsic rewards to guide exploration (Schmidhuber, 1991; 2010; Houthooft et al., 2016; Bellemare et al., 2016).", "startOffset": 64, "endOffset": 137}, {"referenceID": 1, "context": "The second strategy uses intrinsic rewards to guide exploration (Schmidhuber, 1991; 2010; Houthooft et al., 2016; Bellemare et al., 2016).", "startOffset": 64, "endOffset": 137}, {"referenceID": 29, "context": "To learn the span of skills, we propose to use Stochastic Neural Networks (SNNs) (Neal, 1990; 1992; Tang & Salakhutdinov, 2013), a class of neural networks with stochastic units in the computation graph.", "startOffset": 81, "endOffset": 127}, {"referenceID": 43, "context": "One of the main appealing aspects of hierarchical reinforcement learning (HRL) is to use skills to reduce the search complexity of the problem (Parr & Russell, 1998; Sutton et al., 1999; Dietterich, 2000).", "startOffset": 143, "endOffset": 204}, {"referenceID": 8, "context": "One of the main appealing aspects of hierarchical reinforcement learning (HRL) is to use skills to reduce the search complexity of the problem (Parr & Russell, 1998; Sutton et al., 1999; Dietterich, 2000).", "startOffset": 143, "endOffset": 204}, {"referenceID": 3, "context": "Prior work on automatic learning of skills has largely focused on learning skills in discrete domains (Chentanez et al., 2004; Vigorito & Barto, 2010).", "startOffset": 102, "endOffset": 150}, {"referenceID": 26, "context": "A popular approach there is to use statistics about state transitions to identify bottleneck states (Stolle & Precup, 2002; Mannor et al., 2004; \u015eim\u015fek et al., 2005).", "startOffset": 100, "endOffset": 165}, {"referenceID": 40, "context": "A popular approach there is to use statistics about state transitions to identify bottleneck states (Stolle & Precup, 2002; Mannor et al., 2004; \u015eim\u015fek et al., 2005).", "startOffset": 100, "endOffset": 165}, {"referenceID": 11, "context": "(2016) propose a DRAW-like (Gregor et al., 2015) recurrent neural network architecture that can learn temporally extended macro actions.", "startOffset": 27, "endOffset": 48}, {"referenceID": 34, "context": "There has also been work on learning skills in tasks with continuous actions (Schaal et al., 2005; Konidaris et al., 2011; Daniel et al., 2013; Ranchod et al., 2015).", "startOffset": 77, "endOffset": 165}, {"referenceID": 20, "context": "There has also been work on learning skills in tasks with continuous actions (Schaal et al., 2005; Konidaris et al., 2011; Daniel et al., 2013; Ranchod et al., 2015).", "startOffset": 77, "endOffset": 165}, {"referenceID": 6, "context": "There has also been work on learning skills in tasks with continuous actions (Schaal et al., 2005; Konidaris et al., 2011; Daniel et al., 2013; Ranchod et al., 2015).", "startOffset": 77, "endOffset": 165}, {"referenceID": 33, "context": "There has also been work on learning skills in tasks with continuous actions (Schaal et al., 2005; Konidaris et al., 2011; Daniel et al., 2013; Ranchod et al., 2015).", "startOffset": 77, "endOffset": 165}, {"referenceID": 3, "context": "Prior work on automatic learning of skills has largely focused on learning skills in discrete domains (Chentanez et al., 2004; Vigorito & Barto, 2010). A popular approach there is to use statistics about state transitions to identify bottleneck states (Stolle & Precup, 2002; Mannor et al., 2004; \u015eim\u015fek et al., 2005). It is however not clear how these techniques can be applied to settings with high-dimensional continuous spaces. More recently, Mnih et al. (2016) propose a DRAW-like (Gregor et al.", "startOffset": 103, "endOffset": 466}, {"referenceID": 3, "context": "Prior work on automatic learning of skills has largely focused on learning skills in discrete domains (Chentanez et al., 2004; Vigorito & Barto, 2010). A popular approach there is to use statistics about state transitions to identify bottleneck states (Stolle & Precup, 2002; Mannor et al., 2004; \u015eim\u015fek et al., 2005). It is however not clear how these techniques can be applied to settings with high-dimensional continuous spaces. More recently, Mnih et al. (2016) propose a DRAW-like (Gregor et al., 2015) recurrent neural network architecture that can learn temporally extended macro actions. However, the learning needs to be guided by external rewards as supervisory signals, and hence the algorithm cannot be straightforwardly applied in sparse reward settings. There has also been work on learning skills in tasks with continuous actions (Schaal et al., 2005; Konidaris et al., 2011; Daniel et al., 2013; Ranchod et al., 2015). These methods extract useful skills from successful trajectories for the same (or closely related) tasks, and hence require first solving a comparably challenging task or demonstrations. Guided Policy Search (GPS) Levine et al. (2016) leverages access to training in a simpler setting.", "startOffset": 103, "endOffset": 1170}, {"referenceID": 3, "context": "Prior work on automatic learning of skills has largely focused on learning skills in discrete domains (Chentanez et al., 2004; Vigorito & Barto, 2010). A popular approach there is to use statistics about state transitions to identify bottleneck states (Stolle & Precup, 2002; Mannor et al., 2004; \u015eim\u015fek et al., 2005). It is however not clear how these techniques can be applied to settings with high-dimensional continuous spaces. More recently, Mnih et al. (2016) propose a DRAW-like (Gregor et al., 2015) recurrent neural network architecture that can learn temporally extended macro actions. However, the learning needs to be guided by external rewards as supervisory signals, and hence the algorithm cannot be straightforwardly applied in sparse reward settings. There has also been work on learning skills in tasks with continuous actions (Schaal et al., 2005; Konidaris et al., 2011; Daniel et al., 2013; Ranchod et al., 2015). These methods extract useful skills from successful trajectories for the same (or closely related) tasks, and hence require first solving a comparably challenging task or demonstrations. Guided Policy Search (GPS) Levine et al. (2016) leverages access to training in a simpler setting. GPS trains with iLQG Todorov & Li (2005) in state space, and in parallel trains a neural net that only receives raw sensory inputs that has to agree with the iLQG controller.", "startOffset": 103, "endOffset": 1262}, {"referenceID": 3, "context": "Prior work on automatic learning of skills has largely focused on learning skills in discrete domains (Chentanez et al., 2004; Vigorito & Barto, 2010). A popular approach there is to use statistics about state transitions to identify bottleneck states (Stolle & Precup, 2002; Mannor et al., 2004; \u015eim\u015fek et al., 2005). It is however not clear how these techniques can be applied to settings with high-dimensional continuous spaces. More recently, Mnih et al. (2016) propose a DRAW-like (Gregor et al., 2015) recurrent neural network architecture that can learn temporally extended macro actions. However, the learning needs to be guided by external rewards as supervisory signals, and hence the algorithm cannot be straightforwardly applied in sparse reward settings. There has also been work on learning skills in tasks with continuous actions (Schaal et al., 2005; Konidaris et al., 2011; Daniel et al., 2013; Ranchod et al., 2015). These methods extract useful skills from successful trajectories for the same (or closely related) tasks, and hence require first solving a comparably challenging task or demonstrations. Guided Policy Search (GPS) Levine et al. (2016) leverages access to training in a simpler setting. GPS trains with iLQG Todorov & Li (2005) in state space, and in parallel trains a neural net that only receives raw sensory inputs that has to agree with the iLQG controller. The neural net policy is then able to generalize to new situations. Another line of work on skill discovery particularly relevant to our approach is the HiREPS algorithm by Daniel et al. (2012) where, instead of having a mutual information bonus like in our proposed approach, they introduce a constraint on an equivalent metric.", "startOffset": 103, "endOffset": 1590}, {"referenceID": 3, "context": "Prior work on automatic learning of skills has largely focused on learning skills in discrete domains (Chentanez et al., 2004; Vigorito & Barto, 2010). A popular approach there is to use statistics about state transitions to identify bottleneck states (Stolle & Precup, 2002; Mannor et al., 2004; \u015eim\u015fek et al., 2005). It is however not clear how these techniques can be applied to settings with high-dimensional continuous spaces. More recently, Mnih et al. (2016) propose a DRAW-like (Gregor et al., 2015) recurrent neural network architecture that can learn temporally extended macro actions. However, the learning needs to be guided by external rewards as supervisory signals, and hence the algorithm cannot be straightforwardly applied in sparse reward settings. There has also been work on learning skills in tasks with continuous actions (Schaal et al., 2005; Konidaris et al., 2011; Daniel et al., 2013; Ranchod et al., 2015). These methods extract useful skills from successful trajectories for the same (or closely related) tasks, and hence require first solving a comparably challenging task or demonstrations. Guided Policy Search (GPS) Levine et al. (2016) leverages access to training in a simpler setting. GPS trains with iLQG Todorov & Li (2005) in state space, and in parallel trains a neural net that only receives raw sensory inputs that has to agree with the iLQG controller. The neural net policy is then able to generalize to new situations. Another line of work on skill discovery particularly relevant to our approach is the HiREPS algorithm by Daniel et al. (2012) where, instead of having a mutual information bonus like in our proposed approach, they introduce a constraint on an equivalent metric. The solution approach is nevertheless very different as they cannot use policy gradients. Furthermore, although they do achieve multimodality like us, they only tried the episodic case where a single option is active during the rollout. Hence the hierarchical use of the learned skills is less clear. Similarly, the Option-critic architecture (Bacon & Precup, 2016) can learn interpretable skills, but whether they can be reuse across complex tasks is still an open question. Recently, Heess et al. (2016) have independently proposed to learn a range of skills in a pre-training environment that will be useful for the downstream tasks, which is similar to our framework.", "startOffset": 103, "endOffset": 2232}, {"referenceID": 49, "context": "This has been more commonly studied in the past in sequential settings, where the agent is exposed to a sequence of tasks, and should learn to make use of experience gathered from solving earlier tasks to help solve later tasks (Taylor & Stone, 2009; Wilson et al., 2007; Lazaric & Ghavamzadeh, 2010; Devin et al., 2017).", "startOffset": 228, "endOffset": 320}, {"referenceID": 7, "context": "This has been more commonly studied in the past in sequential settings, where the agent is exposed to a sequence of tasks, and should learn to make use of experience gathered from solving earlier tasks to help solve later tasks (Taylor & Stone, 2009; Wilson et al., 2007; Lazaric & Ghavamzadeh, 2010; Devin et al., 2017).", "startOffset": 228, "endOffset": 320}, {"referenceID": 41, "context": "There has been a large body of prior work on special classes of SNNs, such as Restricted Boltzmann Machines (RBMs) (Smolensky, 1986; Hinton, 2002), Deep Belief Networks (DBNs) (Hinton et al.", "startOffset": 115, "endOffset": 146}, {"referenceID": 15, "context": "There has been a large body of prior work on special classes of SNNs, such as Restricted Boltzmann Machines (RBMs) (Smolensky, 1986; Hinton, 2002), Deep Belief Networks (DBNs) (Hinton et al.", "startOffset": 115, "endOffset": 146}, {"referenceID": 16, "context": "There has been a large body of prior work on special classes of SNNs, such as Restricted Boltzmann Machines (RBMs) (Smolensky, 1986; Hinton, 2002), Deep Belief Networks (DBNs) (Hinton et al., 2006), and Sigmoid Belief Networks (SBNs) (Neal, 1990; Tang & Salakhutdinov, 2013).", "startOffset": 176, "endOffset": 197}, {"referenceID": 29, "context": ", 2006), and Sigmoid Belief Networks (SBNs) (Neal, 1990; Tang & Salakhutdinov, 2013).", "startOffset": 44, "endOffset": 84}, {"referenceID": 4, "context": "They have rich representation power and can in fact approximate any well-behaved probability distributions (Le Roux & Bengio, 2008; Cho et al., 2013).", "startOffset": 107, "endOffset": 149}, {"referenceID": 10, "context": "Richer forms of integrations, such as multiplicative integrations and bilinear pooling, have been shown to have greater representation power and improve the optimization landscape, achieving better results when complex interactions are needed (Fukui et al., 2016; Wu et al., 2016).", "startOffset": 243, "endOffset": 280}, {"referenceID": 50, "context": "Richer forms of integrations, such as multiplicative integrations and bilinear pooling, have been shown to have greater representation power and improve the optimization landscape, achieving better results when complex interactions are needed (Fukui et al., 2016; Wu et al., 2016).", "startOffset": 243, "endOffset": 280}, {"referenceID": 2, "context": "To achieve this, we introduce an information-theoretic regularizer, inspired by recent success of similar objectives in encouraging interpretable representation learning in InfoGAN Chen et al. (2016). Concretely, we add an additional reward bonus, proportional to the mutual information (MI) between the latent variable and the current state.", "startOffset": 181, "endOffset": 200}, {"referenceID": 18, "context": "This end-to-end training of a policy with discrete latent variables in the Stochastic Computation Graph could be done using straight-through estimators like the one proposed by Jang et al. (2017) or Maddison et al.", "startOffset": 177, "endOffset": 196}, {"referenceID": 18, "context": "This end-to-end training of a policy with discrete latent variables in the Stochastic Computation Graph could be done using straight-through estimators like the one proposed by Jang et al. (2017) or Maddison et al. (2017). Nevertheless, we show in our experiments that frozen low-level policies are already sufficient to achieve good performance in the studied downstream tasks, so these directions are left as future research.", "startOffset": 177, "endOffset": 222}, {"referenceID": 37, "context": "For both the pre-training phase and the training of the high-level policies, we use Trust Region Policy Optimization (TRPO) as the policy optimization algorithm (Schulman et al., 2015).", "startOffset": 161, "endOffset": 184}, {"referenceID": 9, "context": "We have applied our framework to the two hierarchical tasks described in the benchmark by Duan et al. (2016): Locomotion + Maze and Locomotion + Food Collection (Gather).", "startOffset": 90, "endOffset": 109}, {"referenceID": 9, "context": "Maze 0 is the same as the one described in the benchmark (Duan et al., 2016) and Maze 1 is its reflection, where the robot has to go backwards-right-right instead of forward-left-left.", "startOffset": 57, "endOffset": 76}, {"referenceID": 9, "context": "In the benchmark of continuous control problems (Duan et al., 2016) it was shown that algorithms that employ naive exploration strategies could not solve them.", "startOffset": 48, "endOffset": 67}, {"referenceID": 17, "context": "More advanced intrinsically motivated explorations (Houthooft et al., 2016) do achieve some progress, and we report our stronger results with the exact same setting in Appendix B.", "startOffset": 51, "endOffset": 75}, {"referenceID": 9, "context": "This exploration is not enough to reach the first reward in most of the downstream sparse reward environments and will never learn, as already reported by Duan et al. (2016).", "startOffset": 155, "endOffset": 174}, {"referenceID": 9, "context": "Due to the sparsity of these tasks, none can be properly solved by standard reinforcement algorithms (Duan et al., 2016).", "startOffset": 101, "endOffset": 120}, {"referenceID": 18, "context": "The first issue can be alleviated by introducing end-to-end training, for example using the new straight-through gradient estimators for Stochastic Computations Graphs with discrete latent variables (Jang et al., 2017; Maddison et al., 2017).", "startOffset": 199, "endOffset": 241}, {"referenceID": 25, "context": "The first issue can be alleviated by introducing end-to-end training, for example using the new straight-through gradient estimators for Stochastic Computations Graphs with discrete latent variables (Jang et al., 2017; Maddison et al., 2017).", "startOffset": 199, "endOffset": 241}], "year": 2017, "abstractText": "Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments1 show2 that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.", "creator": "LaTeX with hyperref package"}}}