{"id": "1704.03084", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning", "abstract": "In a composite-domain task-completion dialogue system, a conversation agent often switches among multiple sub-domains before it successfully completes the task. Given such a scenario, a standard deep reinforcement learning based dialogue agent may suffer to find a good policy due to the issues such as: increased state and action spaces, high sample complexity demands, sparse reward and long horizon, etc. In this paper, we propose to use hierarchical deep reinforcement learning approach which can operate at different temporal scales and is intrinsically motivated to attack these problems. Our hierarchical network consists of two levels: the top-level meta-controller for subgoal selection and the low-level controller for dialogue policy learning. Subgoals selected by meta-controller and intrinsic rewards can guide the controller to effectively explore in the state-action space and mitigate the spare reward and long horizon problems. Experiments on both simulations and human evaluation show that our model significantly outperforms flat deep reinforcement learning agents in terms of success rate, rewards and user rating.", "histories": [["v1", "Mon, 10 Apr 2017 23:24:46 GMT  (1370kb,D)", "http://arxiv.org/abs/1704.03084v1", "13 pages, 7 figures"], ["v2", "Mon, 17 Apr 2017 19:36:30 GMT  (1884kb,D)", "http://arxiv.org/abs/1704.03084v2", "11 pages, 8 figures"], ["v3", "Sat, 22 Jul 2017 22:23:53 GMT  (1596kb,D)", "http://arxiv.org/abs/1704.03084v3", "12 pages, 8 figures"]], "COMMENTS": "13 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["baolin peng", "xiujun li", "lihong li", "jianfeng gao", "asli \u00e7elikyilmaz", "sungjin lee", "kam-fai wong"], "accepted": true, "id": "1704.03084"}, "pdf": {"name": "1704.03084.pdf", "metadata": {"source": "CRF", "title": "Composite Task-Completion Dialogue System via Hierarchical Deep Reinforcement Learning", "authors": ["Baolin Peng", "Xiujun Li", "Lihong Li", "Jianfeng Gao", "Asli Celikyilmaz", "Sungjin Lee", "Kam-Fai Wong"], "emails": ["kfwong}@se.cuhk.edu.hk", "xiul@microsoft.com", "lihongli@microsoft.com", "jfgao@microsoft.com", "aslicel@microsoft.com", "sule@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people are able to survive on their own by focusing on themselves and their environment."}, {"heading": "2 Related work", "text": "There are numerous research efforts that have encouraged the user dialogue system to integrate domain-specific knowledge and optimize learning algorithms; there are ways to optimize dialogue policy over time (Scheffler and Young, 2000; Levin and al., 2000; Young et al., 2013; Williams et al., 2017); recent advances in the field of in-depth learning have inspired many in-depth deepenings based on dialogue systems: Cuaya \u0301 huitl (2017) proposed to develop a simple in-depth learning dialogue that leads directly from the raw text to dialogue policy. Li et al. (2017a) proposed an end-to-end-to-end-to-completion of dialogue that has learned dialogue policy, and demonstrated that superior performance through rule-based agent was described. (2016) a two-phase approach for task-oriented spoken dialogue systems, first trained with enhanced learning, then optimized learning."}, {"heading": "3 Neural Dialogue System", "text": "As illustrated in Figure 1, a typical task completion dialog system consists of the following components: Natural Language Understanding (NLU): Given the expressions of free text (typed or spoken), the main task of NLU is to automatically classify the domain of a user query along with domain-specific intentions and fill in a series of slots to form a structured semantic framework. \"My city of departure is San Francisco, and the destination is Seattle.,\" Maps to the dialogue action form \"inform (or city = San Francisco, dst city = Seattle).\" Dialogue Management (DM): The symbolic LU output is passed to the DM in a dialog action form (or semantic framework). The classic DM includes two levels, dialogue status tracking and political learning. In view of the symbolic output of LU, a query is mapped to interact with the database to retrieve the available result."}, {"heading": "4 Policy Learning Methodology", "text": "In this work, we focus on political learning. We formalize the task as a problem of the Markov decision-making process (MDP), which consists of a set of states and measures a, a set of measures a, a, A, A, a transitional state function T (s, a, s), which indicates the probability of the next state s in light of the current state and action a, a reward function R (s, a, s), which means the scalable reward that the actor received from the environment, based on the selection of measures a from states leading to a new state s, \"and \u03b3 [0, 1] as a discount factor. The goal is to learn a political function that can maximize a certain reward function over a long period of time. In order to learn the interaction policy of the dialogue system, we apply reinforced learning algorithms to political training in an end-to-end fashion."}, {"heading": "4.1 Deep Q-Network", "text": "First, the policy is presented as a deep QNetwork (DQN) (Mnih et al., 2015) that maps the state of st from the state tracker as input and output, the estimated Q (st, a) for all actions a). The agent aims to find an optimal reward by maximizing the following reward factors in each time step. Q-learning algorithm solves the amplification problems by using the optimal Q function Q (st, at)."}, {"heading": "5 Experiments and Results", "text": "In order to evaluate the proposed model, we conduct experiments with a task completion dialog setting that helps the user to book airline tickets and hotel reservations."}, {"heading": "5.1 User Simulator", "text": "In reality, however, it is time-consuming and costly to obtain such a large corpus of dialogues (Pietquin et al., 2011). It is common to use simulators not only in the game agent learning scenario (Mnih et al., 2015), but also in dialog research (Asri et al., 2016; Schatzmann et al., 2007).In this work, we extend a public user simulator (Li et al., 2016b) to a composite domain task completion dialog. During the training, the simulator gives the agent a reward signal both at each round and at the end of a dialog. The dialog is considered successful only if the tickets for flight and hotel are booked and the information provided by the agent satisfies the limitations of the user. At the end of each dialog, the agent receives a positive reward equal to 2% of the maximum turn for success and a negative reward for max use for the other say of 1 is included."}, {"heading": "5.2 Dataset", "text": "The raw data we use in this research comes from a public multi-domain dialog corpus1 (El Asri et al., 2017), which consists of 1369 human-human dialogs collected in a wizard-of-oz setting. We have made some changes to this multi-domain dialog data to convert it into a composite domain dialog."}, {"heading": "5.3 Dialogue Policy", "text": "In addition to the proposed Q-Network hierarchical policy, we also present two basic guidelines: a custom rule and DQN.1. Rule Agent uses sophisticated custom dialog policies that request and inform the required slots and then inform the user ticks.2. DQN agent is a standard Deep Q Network model that learns dialog policies only with extrinsic rewards. 3. h-DQN agent is a two-tier hierarchical QNetwork that learns the dialog policies with extrinsic and intrinsic rewards."}, {"heading": "5.4 Implementation", "text": "For DQN, we set a hidden layer size of 80. For h-DQN, both meta-controllers and controllers set a hidden layer size of 80. RMSprop with standard hyperparameters is used to optimize both DQN and h-DQN parameters, and we set a batch size of 16. During the training, we use - greedy strategy for exploration. For each simulation period, we simulate 100 dialogs and store these state transition stages in experience buffers. At the end of each simulation period, models are updated with all tuples in the buffers in batch manner. Experience buffering strategy plays a crucial role in the success of learning in deep amplification. In our experiments, we use rule agents at the beginning to perform N (N = 100) dialogs to fill the experience buffers, which is a kind of imitation buffers to warm up the RL."}, {"heading": "5.5 Simulated User Evaluation", "text": "In this compound task completion dialog task, we compare the proposed agent with the base agent in terms of three metrics: success rate 2, average rewards, and average number of spins. Figure 3 shows the learning curves for all three agents trained on different user types. Each learning curve is calculated by an average of 10 spins. For all three user types, reinforcement learning agents find better dialog strategies than custom rule agents: DQN and h-DQN achieve a higher success rate and require fewer spins to reach the user's goal than rule agents. For users of type B and C who may need to rework some slots during the dialog, the performance of the DQN decreases by a large distance as task complexity increases. It requires more dialog spins, which poses a credit challenge. In all three types of user simulations, 1) these user simulations can significantly exceed the QDN tasks suggested by the structure of the proposed QN."}, {"heading": "5.6 Real User Evaluation", "text": "Due to budget constraints, we conducted the study on only two types of users: Type A, which has no preferences for domains, and Type B, which has preferences for flight domains. In total, we have four agents that we have to test in this study: DQN A, h-DQN A and DQN B, h-DQN B. For each dialogue, one of the agents was randomly selected to talk to the user. During the call, the user was confronted with \"No available ticket.\" At the end of the dialogue, the user was asked to rate on a scale of 1 to 5 based on the naturalness, coherence of the dialogue. (1 is the worst rating, 5 is the best)."}, {"heading": "6 Discussion and Conclusion", "text": "This work represents an end-to-end composite hierarchical dialog system for task completion; we show that hierarchical learning agents with deep gain (hDQN) can do significantly more than flat learning agents with deep gain (DQN) and rule-based learning agents in both simulations and real-user evaluation; in addition, hierarchical learning algorithms show strong capabilities to reduce sample complexity in order to accelerate training on complicated tasks with intrinsic hierarchical structure; and also show strong adaptability to adapt to different user types, which would be a good direction to explore the personalization of hierarchical reinforcement learning agents in dialogues with compound tasks. Both DQN and hDQN agents in this work use forward-looking neural networks, as we have occasionally found that agents may ask the repeated questions, while the introduction of a recursive architecture for this hierarchical problem may contribute to solving other hierarchical problems at the hierarchical level."}, {"heading": "A User Simulator", "text": "In general, a user target is defined with two types of slots: slots where the user does not know the value and expects the user to provide it through the conversation; slots inform that the slot-value pairs that the user knows in his mind serve as soft / hard constraints in the dialog; slots that have multiple values are referred to as soft constraints, meaning that the user has preferences, and the user could change their value if there is no result returned by the agent based on the current value; otherwise, slots that have only one value serve as hard constraints. Table 2 shows an example of user goals in the compound task fulfillment dialogs. First User Act This work focuses on user-initiated dialogs so that we randomly generate a user action as the first curve (a user rotation). To make the first user action more reasonable, we add some constraints in the generation process."}, {"heading": "B Sample Dialogues", "text": "Table 3 shows a sample dialog created by DQN and h-DQN agents who interact with real users. To be informative, we explicitly show the user the destination at the top of the dialog, which is to help the user reach that destination and book the right tickets; in fact, the agent does not know anything about the user's destination during the call."}, {"heading": "C Algorithms", "text": "Algorithm 1 outlines the complete procedure for training hierarchical DQN in this composite task fulfillment dialog system.Algorithm 1: Learning algorithm for h-DQN agents in the composite task fulfillment dialog 1: Experience replay buffer D1 for meta controllers and D2 for controllers initialize. 2: Initialize Q1 and Q2 network with random weights. 3: Load dialog simulator and knowledge base. 4: for episode = 1: N do 5: restart dialog simulator and get state description s. 6: while s is not a terminal is 7: Extrinsic reward: = 0 8: s0: = s 9: Choosing a sub-target based on the probability distribution g (s) and exploration probability g 10: while s is not a terminal and subtarget g is not reached. 11: Choosing an action based on plurality probability get a conditionality: (Explurality probability: 12 | a) conditionality:"}], "references": [{"title": "A sequence-to-sequence model for user simulation in spoken dialogue systems", "author": ["Layla El Asri", "Jing He", "Kaheer Suleman."], "venue": "Nelson Morgan, editor, Interspeech 2016. ISCA, pages 1151\u20131155. https://doi.org/10.21437/Interspeech.2016-1175.", "citeRegEx": "Asri et al\\.,? 2016", "shortCiteRegEx": "Asri et al\\.", "year": 2016}, {"title": "Recent advances in hierarchical reinforcement learning", "author": ["Andrew G. Barto", "Sridhar Mahadevan."], "venue": "Discrete Event Dynamic Systems 13(1-2):41\u2013", "citeRegEx": "Barto and Mahadevan.,? 2003", "shortCiteRegEx": "Barto and Mahadevan.", "year": 2003}, {"title": "Hierarchical reinforcement learning for spoken dialogue systems", "author": ["Heriberto Cuay\u00e1huitl"], "venue": null, "citeRegEx": "Cuay\u00e1huitl.,? \\Q2009\\E", "shortCiteRegEx": "Cuay\u00e1huitl.", "year": 2009}, {"title": "SimpleDS: A simple deep reinforcement learning dialogue system", "author": ["Heriberto Cuay\u00e1huitl."], "venue": "Dialogues with Social Robots, Springer, pages 109\u2013118.", "citeRegEx": "Cuay\u00e1huitl.,? 2017", "shortCiteRegEx": "Cuay\u00e1huitl.", "year": 2017}, {"title": "Deep reinforcement learning for multi-domain dialogue systems", "author": ["Heriberto Cuay\u00e1huitl", "Seunghak Yu", "Ashley Williamson", "Jacob Carse."], "venue": "arXiv preprint arXiv:1611.08675 https://arxiv.org/abs/1611.08675.", "citeRegEx": "Cuay\u00e1huitl et al\\.,? 2016", "shortCiteRegEx": "Cuay\u00e1huitl et al\\.", "year": 2016}, {"title": "Learning cooperative visual dialog agents with deep reinforcement learning", "author": ["Abhishek Das", "Satwik Kottur", "Jos\u00e9 MF Moura", "Stefan Lee", "Dhruv Batra."], "venue": "arXiv preprint arXiv:1703.06585 https://arxiv.org/abs/1703.06585.", "citeRegEx": "Das et al\\.,? 2017", "shortCiteRegEx": "Das et al\\.", "year": 2017}, {"title": "End-to-end reinforcement learning of dialogue agents for information access", "author": ["Bhuwan Dhingra", "Lihong Li", "Xiujun Li", "Jianfeng Gao", "Yun-Nung Chen", "Faisal Ahmed", "Li Deng."], "venue": "arXiv preprint arXiv:1609.00777 https://arxiv.org/abs/1609.00777.", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Hierarchical reinforcement learning with the MAXQ value function decomposition", "author": ["Thomas G. Dietterich."], "venue": "J. Artif. Intell. Res. (JAIR) 13:227\u2013 303. https://doi.org/10.1613/jair.639.", "citeRegEx": "Dietterich.,? 2000", "shortCiteRegEx": "Dietterich.", "year": 2000}, {"title": "Frames: A corpus for adding memory to goal-oriented dialogue systems", "author": ["Layla El Asri", "Hannes Schulz", "Shikhar Sharma", "Jeremie Zumer", "Justin Harris", "Emery Fine", "Rahul Mehrotra", "Kaheer Suleman."], "venue": "arXiv preprint arXiv:1704.00057", "citeRegEx": "Asri et al\\.,? 2017", "shortCiteRegEx": "Asri et al\\.", "year": 2017}, {"title": "Distributed dialogue policies for multi-domain statistical dialogue management", "author": ["Milica Gasic", "Dongho Kim", "Pirros Tsiakoulis", "Steve J. Young."], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Gasic et al\\.,? 2015a", "shortCiteRegEx": "Gasic et al\\.", "year": 2015}, {"title": "Policy committee for adaptation in multidomain spoken dialogue systems", "author": ["Milica Gasic", "Nikola Mrksic", "Pei-hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve J. Young."], "venue": "2015 IEEE Workshop on Automatic Speech Recognition and", "citeRegEx": "Gasic et al\\.,? 2015b", "shortCiteRegEx": "Gasic et al\\.", "year": 2015}, {"title": "Deep reinforcement learning with a natural language action space", "author": ["Ji He", "Jianshu Chen", "Xiaodong He", "Jianfeng Gao", "Lihong Li", "Li Deng", "Mari Ostendorf."], "venue": "Proceedings of the 54th Annual Meeting of the Associ-", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Tejas D. Kulkarni", "Karthik Narasimhan", "Ardavan Saeedi", "Josh Tenenbaum."], "venue": "Daniel D. Lee, Masashi Sugiyama, Ulrike von", "citeRegEx": "Kulkarni et al\\.,? 2016", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "A stochastic model of human-machine interaction for learning dialog strategies", "author": ["Esther Levin", "Roberto Pieraccini", "Wieland Eckert."], "venue": "IEEE Trans. Speech and Audio Processing 8(1):11\u201323. https://doi.org/10.1109/89.817450.", "citeRegEx": "Levin et al\\.,? 2000", "shortCiteRegEx": "Levin et al\\.", "year": 2000}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel."], "venue": "Journal of Machine Learning Research 17(39):1\u201340.", "citeRegEx": "Levine et al\\.,? 2016", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Dan Jurafsky", "Michel Galley", "Jianfeng Gao."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Li et al\\.,? 2016a", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "End-to-end task-completion neural dialogue systems", "author": ["Xiujun Li", "Yun-Nung Chen", "Lihong Li", "Jianfeng Gao."], "venue": "arXiv preprint arXiv:1703.01008 https://arxiv.org/abs/1703.01008.", "citeRegEx": "Li et al\\.,? 2017a", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Investigation of language understanding impact for reinforcement learning based dialogue systems", "author": ["Xiujun Li", "Yun-Nung Chen", "Lihong Li", "Jianfeng Gao", "Asli Celikyilmaz."], "venue": "arXiv preprint arXiv:1703.07055 https://arxiv.org/abs/1703.07055.", "citeRegEx": "Li et al\\.,? 2017b", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "A user simulator for task-completion dialogues", "author": ["Xiujun Li", "Zachary C Lipton", "Bhuwan Dhingra", "Lihong Li", "Jianfeng Gao", "Yun-Nung Chen."], "venue": "arXiv preprint arXiv:1612.05688 https://arxiv.org/abs/1612.05688.", "citeRegEx": "Li et al\\.,? 2016b", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Multi-policy dialogue management", "author": ["Pierre Lison."], "venue": "Proceedings of the SIGDIAL 2011. The Association for Computer Linguistics, pages 294\u2013 300. http://www.aclweb.org/anthology/W11-2033.", "citeRegEx": "Lison.,? 2011", "shortCiteRegEx": "Lison.", "year": 2011}, {"title": "Human-level control through deep reinforcement learning", "author": ["Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis."], "venue": "Nature 518(7540):529\u2013533. https://doi.org/10.1038/nature14236.", "citeRegEx": "Antonoglou et al\\.,? 2015", "shortCiteRegEx": "Antonoglou et al\\.", "year": 2015}, {"title": "Language understanding for textbased games using deep reinforcement learning", "author": ["Karthik Narasimhan", "Tejas D. Kulkarni", "Regina Barzilay."], "venue": "Llu\u0131\u0301s M\u00e0rquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton, editors, Proceed-", "citeRegEx": "Narasimhan et al\\.,? 2015", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Reinforcement learning with hierarchies of machines", "author": ["Ronald Parr", "Stuart J. Russell."], "venue": "Michael I. Jordan, Michael J. Kearns, and Sara A. Solla, editors, Advances in Neural Information Processing Systems 10, [NIPS Conference, Denver,", "citeRegEx": "Parr and Russell.,? 1997", "shortCiteRegEx": "Parr and Russell.", "year": 1997}, {"title": "Sampleefficient batch reinforcement learning for dialogue management optimization", "author": ["Olivier Pietquin", "Matthieu Geist", "Senthilkumar Chandramohan", "Herv\u00e9 Frezza-Buet."], "venue": "TSLP 7(3):7:1\u20137:21. https://doi.org/10.1145/1966407.1966412.", "citeRegEx": "Pietquin et al\\.,? 2011", "shortCiteRegEx": "Pietquin et al\\.", "year": 2011}, {"title": "Agendabased user simulation for bootstrapping a POMDP dialogue system", "author": ["Jost Schatzmann", "Blaise Thomson", "Karl Weilhammer", "Hui Ye", "Steve J. Young."], "venue": "Candace L. Sidner, Tanja Schultz, Matthew Stone, and ChengXiang Zhai, ed-", "citeRegEx": "Schatzmann et al\\.,? 2007", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2007}, {"title": "The hidden agenda user simulation model", "author": ["Jost Schatzmann", "Steve Young."], "venue": "IEEE transactions on audio, speech, and language processing 17(4):733\u2013747.", "citeRegEx": "Schatzmann and Young.,? 2009", "shortCiteRegEx": "Schatzmann and Young.", "year": 2009}, {"title": "Probabilistic simulation of human-machine dialogues", "author": ["Konrad Scheffler", "Steve J. Young."], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing. ICASSP 2000, 59 June, 2000, Hilton Hotel and Convention Cen-", "citeRegEx": "Scheffler and Young.,? 2000", "shortCiteRegEx": "Scheffler and Young.", "year": 2000}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["Kalchbrenner", "Ilya Sutskever", "Timothy P. Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis."], "venue": "Nature 529(7587):484\u2013489.", "citeRegEx": "Kalchbrenner et al\\.,? 2016", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Reinforcement learning with a hierarchy of abstract models", "author": ["Satinder P. Singh."], "venue": "William R. Swartout, editor, Proceedings of the 10th National Conference on Artificial Intelligence. San Jose, CA, July 12-16, 1992..", "citeRegEx": "Singh.,? 1992", "shortCiteRegEx": "Singh.", "year": 1992}, {"title": "End-to-end optimization of goal-driven and visually grounded dialogue systems", "author": ["Florian Strub", "Harm de Vries", "Jeremie Mary", "Bilal Piot", "Aaron Courville", "Olivier Pietquin."], "venue": "arXiv preprint arXiv:1703.05423 https://arxiv.org/abs/1703.05423.", "citeRegEx": "Strub et al\\.,? 2017", "shortCiteRegEx": "Strub et al\\.", "year": 2017}, {"title": "Continuously learning neural dialogue management", "author": ["Pei-Hao Su", "Milica Gasic", "Nikola Mrksic", "Lina Rojas-Barahona", "Stefan Ultes", "David Vandyke", "Tsung-Hsien Wen", "Steve Young."], "venue": "arXiv preprint arXiv:1606.02689", "citeRegEx": "Su et al\\.,? 2016", "shortCiteRegEx": "Su et al\\.", "year": 2016}, {"title": "Intra-option learning about temporally abstract actions", "author": ["Richard S. Sutton", "Doina Precup", "Satinder P. Singh."], "venue": "Proceedings of the Fifteenth International Conference on Machine Learning (ICML 1998), Madison, Wisconsin, USA, July 24-27, 1998.", "citeRegEx": "Sutton et al\\.,? 1998", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S. Sutton", "Doina Precup", "Satinder P. Singh."], "venue": "Artif. Intell. 112(1-2):181\u2013211. https://doi.org/10.1016/S0004-3702(99)00052-1.", "citeRegEx": "Sutton et al\\.,? 1999", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Peihao Su", "David Vandyke", "Steve J. Young."], "venue": "Proceedings of the 2015 Con-", "citeRegEx": "Wen et al\\.,? 2015", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning", "author": ["Jason D Williams", "Kavosh Asadi", "Geoffrey Zweig."], "venue": "arXiv preprint arXiv:1702.03274 .", "citeRegEx": "Williams et al\\.,? 2017", "shortCiteRegEx": "Williams et al\\.", "year": 2017}, {"title": "Partially observable markov decision processes for spoken dialog systems", "author": ["Jason D. Williams", "Steve J. Young."], "venue": "Computer Speech & Language 21(2):393\u2013422. https://doi.org/10.1016/j.csl.2006.06.008.", "citeRegEx": "Williams and Young.,? 2007", "shortCiteRegEx": "Williams and Young.", "year": 2007}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Still talking to machines (cognitively speaking)", "author": ["Steve J. Young."], "venue": "Takao Kobayashi, Keikichi Hirose, and Satoshi Nakamura, editors, INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Young.,? 2010", "shortCiteRegEx": "Young.", "year": 2010}, {"title": "Pomdp-based statistical spoken dialog systems: A review", "author": ["Steve J. Young", "Milica Gasic", "Blaise Thomson", "Jason D. Williams."], "venue": "Proceedings of the IEEE 101(5):1160\u20131179. https://doi.org/10.1109/JPROC.2012.2225812.", "citeRegEx": "Young et al\\.,? 2013", "shortCiteRegEx": "Young et al\\.", "year": 2013}, {"title": "Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning", "author": ["Tiancheng Zhao", "Maxine Esk\u00e9nazi."], "venue": "Proceedings of the SIGDIAL 2016 Conference, The 17th Annual Meeting of the Special", "citeRegEx": "Zhao and Esk\u00e9nazi.,? 2016", "shortCiteRegEx": "Zhao and Esk\u00e9nazi.", "year": 2016}], "referenceMentions": [{"referenceID": 37, "context": "However, these personal assistants can only accomplish simple tasks, still far behind being able to handle complex tasks (Young, 2010; Williams and Young, 2007).", "startOffset": 121, "endOffset": 160}, {"referenceID": 35, "context": "However, these personal assistants can only accomplish simple tasks, still far behind being able to handle complex tasks (Young, 2010; Williams and Young, 2007).", "startOffset": 121, "endOffset": 160}, {"referenceID": 21, "context": ", 2016), text games (Narasimhan et al., 2015; He et al., 2016) and robotics (Schulman et al.", "startOffset": 20, "endOffset": 62}, {"referenceID": 11, "context": ", 2016), text games (Narasimhan et al., 2015; He et al., 2016) and robotics (Schulman et al.", "startOffset": 20, "endOffset": 62}, {"referenceID": 6, "context": "info-bot for information access (Dhingra et al., 2016), end-to-end task completion agent (Zhao and Esk\u00e9nazi, 2016; Li et al.", "startOffset": 32, "endOffset": 54}, {"referenceID": 39, "context": ", 2016), end-to-end task completion agent (Zhao and Esk\u00e9nazi, 2016; Li et al., 2017a), and open domain dialogue generation (Li et al.", "startOffset": 42, "endOffset": 85}, {"referenceID": 16, "context": ", 2016), end-to-end task completion agent (Zhao and Esk\u00e9nazi, 2016; Li et al., 2017a), and open domain dialogue generation (Li et al.", "startOffset": 42, "endOffset": 85}, {"referenceID": 15, "context": ", 2017a), and open domain dialogue generation (Li et al., 2016a), Visual Dialogue (Das et al.", "startOffset": 46, "endOffset": 64}, {"referenceID": 5, "context": ", 2016a), Visual Dialogue (Das et al., 2017; Strub et al., 2017).", "startOffset": 26, "endOffset": 64}, {"referenceID": 29, "context": ", 2016a), Visual Dialogue (Das et al., 2017; Strub et al., 2017).", "startOffset": 26, "endOffset": 64}, {"referenceID": 36, "context": ", 2015) or policy gradient (Williams, 1992) algorithms.", "startOffset": 27, "endOffset": 43}, {"referenceID": 2, "context": "In fact, several work have investigated along this direction (Cuay\u00e1huitl, 2009; Gasic et al., 2015b).", "startOffset": 61, "endOffset": 100}, {"referenceID": 10, "context": "In fact, several work have investigated along this direction (Cuay\u00e1huitl, 2009; Gasic et al., 2015b).", "startOffset": 61, "endOffset": 100}, {"referenceID": 2, "context": "Recent advances in deep learning have inspired many deep reinforcement learning based dialogue systems: Cuay\u00e1huitl (2017) proposed a simple deep reinforcement learning dialogue system which learns to map di-", "startOffset": 104, "endOffset": 122}, {"referenceID": 15, "context": "Li et al. (2017a) proposed an end-to-end task-completion dialogue system, which learned dialogue policy via deep Q-Network and demonstrated superior performance over rule-based agent.", "startOffset": 0, "endOffset": 18}, {"referenceID": 34, "context": "Williams et al. (2017) provided hybrid code networks which allow the user to input", "startOffset": 0, "endOffset": 23}, {"referenceID": 19, "context": "Multi-policy model for multi-domain dialogue has been studied in (Lison, 2011) where several policies are concurrently yield actions and a heuristic algorithm is used to determinate which action should be taken.", "startOffset": 65, "endOffset": 78}, {"referenceID": 9, "context": "Gasic et al. (2015a) investigated a distributed architecture which firstly trains a generic policy on data from all the domains and then specializes with only in-domain data.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "Gasic et al. (2015a) investigated a distributed architecture which firstly trains a generic policy on data from all the domains and then specializes with only in-domain data. Gasic et al. (2015b) presented policy committee for adaptation in multi-domain dialogue", "startOffset": 0, "endOffset": 196}, {"referenceID": 2, "context": "Cuay\u00e1huitl et al. (2016) presented Network of Deep Q-Networks for multi-domain dia-", "startOffset": 0, "endOffset": 25}, {"referenceID": 28, "context": "Hierarchical reinforcement learning methods alleviate the curse of dimensionality by integrating hierarchies, exponentially reduce computation cost, sample complexity and have better chance to find good polices (Sutton et al., 1999, 1998; Singh, 1992; Dietterich, 2000; Barto and Mahadevan, 2003).", "startOffset": 211, "endOffset": 296}, {"referenceID": 7, "context": "Hierarchical reinforcement learning methods alleviate the curse of dimensionality by integrating hierarchies, exponentially reduce computation cost, sample complexity and have better chance to find good polices (Sutton et al., 1999, 1998; Singh, 1992; Dietterich, 2000; Barto and Mahadevan, 2003).", "startOffset": 211, "endOffset": 296}, {"referenceID": 1, "context": "Hierarchical reinforcement learning methods alleviate the curse of dimensionality by integrating hierarchies, exponentially reduce computation cost, sample complexity and have better chance to find good polices (Sutton et al., 1999, 1998; Singh, 1992; Dietterich, 2000; Barto and Mahadevan, 2003).", "startOffset": 211, "endOffset": 296}, {"referenceID": 31, "context": "Options framework exploits temporal abstraction that does not necessarily make decision at each time but rather invoke temporally extended actions until termination (Sutton et al., 1998).", "startOffset": 165, "endOffset": 186}, {"referenceID": 7, "context": "archical decomposition of value functions of the original problem into a set of value functions of subproblems (Dietterich, 2000).", "startOffset": 111, "endOffset": 129}, {"referenceID": 12, "context": "Our work is motivated by the hierarchical-DQN (Kulkarni et al., 2016) which integrates hierarchi-", "startOffset": 46, "endOffset": 69}, {"referenceID": 7, "context": "archical decomposition of value functions of the original problem into a set of value functions of subproblems (Dietterich, 2000). Parr and Russell (1997) presented Hierarchies of Machines where policies are constrained by hierarchies of partially specified machines, allowing for the use of prior knowledge to reduce search spaces.", "startOffset": 112, "endOffset": 155}, {"referenceID": 4, "context": "Compared with (Cuay\u00e1huitl et al., 2016) which performs the similar task, we have two levels of DQN, the top-", "startOffset": 14, "endOffset": 39}, {"referenceID": 33, "context": "The current state-of-the-art NLG model is a semantically conditioned LSTM-based generator (Wen et al., 2015)", "startOffset": 90, "endOffset": 108}, {"referenceID": 12, "context": "Thus, inspired by the work of hierarchical deep reinforcement learning(Kulkarni et al., 2016), we investigate hierarchical reinforcement learning algorithms in the composite task-completion dialogue setting.", "startOffset": 70, "endOffset": 93}, {"referenceID": 23, "context": "However, in reality it is time-consuming and costly to obtain such a large dialogue corpus (Pietquin et al., 2011).", "startOffset": 91, "endOffset": 114}, {"referenceID": 18, "context": "In this work, we extend a public user simulator (Li et al., 2016b) to composite-domain task-completion dialogue task.", "startOffset": 48, "endOffset": 66}], "year": 2017, "abstractText": "In a composite-domain task-completion dialogue system, a conversation agent often switches among multiple sub-domains before it successfully completes the task. Given such a scenario, a standard deep reinforcement learning based dialogue agent may suffer to find a good policy due to the issues such as: increased state and action spaces, high sample complexity demands, sparse reward and long horizon, etc. In this paper, we propose to use hierarchical deep reinforcement learning approach which can operate at different temporal scales and is intrinsically motivated to attack these problems. Our hierarchical network consists of two levels: the top-level meta-controller for subgoal selection and the low-level controller for dialogue policy learning. Subgoals selected by metacontroller and intrinsic rewards can guide the controller to effectively explore in the state-action space and mitigate the spare reward and long horizon problems. Experiments on both simulations and human evaluation show that our model significantly outperforms flat deep reinforcement learning agents in terms of success rate, rewards and user rating.", "creator": "LaTeX with hyperref package"}}}