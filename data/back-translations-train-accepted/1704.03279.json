{"id": "1704.03279", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2017", "title": "Unfolding and Shrinking Neural Machine Translation Ensembles", "abstract": "Ensembling is a well-known technique in neural machine translation (NMT). Instead of a single neural net, multiple neural nets with the same topology are trained separately, and the decoder generates predictions by averaging over the individual models. Ensembling often improves the quality of the generated translations drastically. However, it is not suitable for production systems because it is cumbersome and slow. This work aims to reduce the runtime to be on par with a single system without compromising the translation quality. First, we show that the ensemble can be unfolded into a single large neural network which imitates the output of the ensemble system. We show that unfolding can already improve the runtime in practice since more work can be done on the GPU. We proceed by describing a set of techniques to shrink the unfolded network by reducing the dimensionality of layers. On Japanese-English we report that the resulting network has the size and decoding speed of a single NMT network but performs on the level of a 3-ensemble system.", "histories": [["v1", "Tue, 11 Apr 2017 13:27:00 GMT  (131kb,D)", "https://arxiv.org/abs/1704.03279v1", "Submitted to EMNLP 2017"], ["v2", "Fri, 21 Jul 2017 13:04:22 GMT  (260kb,D)", "http://arxiv.org/abs/1704.03279v2", "Accepted at EMNLP 2017"]], "COMMENTS": "Submitted to EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["felix stahlberg", "bill byrne"], "accepted": true, "id": "1704.03279"}, "pdf": {"name": "1704.03279.pdf", "metadata": {"source": "CRF", "title": "Unfolding and Shrinking Neural Machine Translation Ensembles", "authors": ["Felix Stahlberg"], "emails": ["fs439@cam.ac.uk", "wjb31@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the\" New York Times, \"in which he said the role of the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\""}, {"heading": "2 Unfolding K Networks into a Single Large Neural Network", "text": "The first concept of our approach is called unfolding. Unfolding is an alternative to networking multiple neural networks with the same topology. Instead of calculating their predictions, the unfolding constructs a single large neural network from the individual networks, which have the same number of input and output neurons, but larger inner layers. Our main motivation for unfolding is to obtain a single network with ensemble power, which can be shrunk with techniques in secs. 3. Provided we have two individual layers of tailored neural networks as in Fig. 1. Normally, the assembling is implemented by merging both networks into a single large network (Fig. 1 (a)), another isolated forward traversing of the second network (Fig. 1), and avoiding the activities in the output layers of both networks by fusing both networks to merge into a single large network, as in Fig. 1."}, {"heading": "3 Shrinking the Unfolded Network", "text": "After constructing the weight matrices of the unfolded network, we reduce its size by iteratively shrinking layer sizes. In this section, however, we refer to the incoming weight matrix of the layer as U-Rmin-m and the outgoing weight matrix as V-Rm-mout. However, our method is inspired by the method of the Srinivas and Babu (2015). They propose a criterion for taking neurons from the inner layers of the network on the basis of two intuitions. First, similar to Hebb's learning rule, they recognize redundancy by the principle of the neurons that fire together, wired together. If the incoming weight vectors U:, i and U:, j are exactly the same for two neurons i and j, we can remove the neuron j and its outgoing connections to neurons i (Vi,: Babinix-Vi,: + Vj,:) starting from neurons that are starting from neurons."}, {"heading": "3.1 Data-Free Neuron Removal", "text": "Srinivas and Babu (2015) propose to add the outgoing weights of j to the weights of a similar neuron j q to compensate for the distance from j. However, we have found that this approach does not work well on NMT networks. Instead, we propose to compensate for the removal of a neuron by a linear combination of the remaining neurons in the layer. Data-free shrinkage assumes that, for reasons of deriving the update rule, the neuron activation function is linear. We now ask the following question: How can we compensate the loss of neurons j as best as possible so that the effects on the output of the entire network are minimized? Data-free shrinkage represents the incoming weight vector of neurons j (U:, j) as a linear combination of the incoming weight vectors of the other neurons we use. The linear factors can be found by satisfying the following linear veto the system of neurons j: U: j = jj _ where the method is:"}, {"heading": "3.2 Data-Bound Neuron Removal", "text": "Although we note that our data-free approach represents a significant improvement over the methods of Srinivas and Babu (2015) on the NMT networks, it still leads to a significant decrease in BLEU scores when applied to recurring GRU layers. Our data-free method uses incoming weights to identify similar neurons, i.e. neurons that are expected to have similar activity. This works well enough for simple layers, but the interdependencies between states and the gates within gated layers such as GRUs or LSTMs are complex enough that redundancies cannot be easily found by looking for similar weights. In the spirit of Babaeizadeh et al. (2016), our data-based version picks up neuron activity during training to compensate for the removal of the j-th neurons by using a linear combination of the output of the remaining neurons with similar activity patterns."}, {"heading": "3.3 Shrinking Embedding Layers with SVD", "text": "The standard attention-based NMT network architecture (Bahdanau et al., 2015) includes three linear layers: the embedding layer in the encoder and the output and feedback embedding layer in the decoder. We found that linear layers shrink particularly easily when using a low matrix approximation. As before, we refer to the incoming weight matrix as U-Rmin-m and the outgoing weight matrix as V-Rm-mout. Since the layer is linear, we could directly connect the previous layer to the next layer by using the product of the two weight matrices X = U-V. However, X can be very large. Therefore, we approach X as the product of two low matrices Y-Rmin-m-m-m \"and Z-Rm-mout (X-Rm-\u00d7 mout), with m being the desired layer size."}, {"heading": "4 Results", "text": "In fact, it is the case that we are able to go in search of a solution that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution. \""}, {"heading": "5 Related Work", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "6 Conclusion", "text": "Our approach involves unfolding an ensemble of multiple systems into a single large neural network and shrinking that network by eliminating redundant neurons. Our best results in Japanese-English result in either a gain of 2.2 BLEU compared to the original single NMT network at roughly the same decryption speed or a decryption speed of 3.4 \u00d7 CPU with only a slight decrease in BLEU. The current formulation of the unfolding work for networks of the same topology as the concatenation of layers is only possible for analog layers in different networks. Unfolding and shrinking of different networks could be possible, for example, by applying the technique only to the input and output layers, or by a different scheme to find associations between units in different models, but we leave this investigation to future models in NMT ensembles in current research. (Bojar et al., 2016; Sennal rich, 2016; Dural, 2016; Neual, Wal, 2017)."}, {"heading": "Acknowledgments", "text": "This work was supported by the UK Engineering and Physical Sciences Research Council (EPSRC grant EP / L027623 / 1)."}, {"heading": "Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking", "text": "Data-free and data-bound shrinkage can be interpreted as reducing the expected difference between network outputs before and after removal to zero (among other assumptions). (Simplicity) We focus our likely shrinkage treatment on individual layer forward networks. (Such a network maps an input x x x x x x Rmin to an output y y Rmout.) The l output is calculated using the following equation: (1, m) l (xuTk) Vk, l (7) where uk Rmin is the incoming weight vector of k-th hidden neurons (referred to as U:, k in the main paper) and V (Rm) mout of the outgoing weight matrix of the m-dimensional hidden layer. We now remove the j-th neurons in the hidden layer and modify the outgoing weights to compensate for the distance."}], "references": [{"title": "Pruning algorithms of neural networks \u2013 a comparative study", "author": ["M. Gethsiyal Augasta", "Thangairulappan Kathirvalavakumar."], "venue": "Central European Journal of Computer Science, 3(3):105\u2013115.", "citeRegEx": "Augasta and Kathirvalavakumar.,? 2013", "shortCiteRegEx": "Augasta and Kathirvalavakumar.", "year": 2013}, {"title": "NoiseOut: A simple way to prune neural networks", "author": ["Mohammad Babaeizadeh", "Paris Smaragdis", "Roy H. Campbell."], "venue": "Proceedings of the 1st International Workshop on Efficient Methods for Deep Neural Networks (EMDNN).", "citeRegEx": "Babaeizadeh et al\\.,? 2016", "shortCiteRegEx": "Babaeizadeh et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR, Toulon, France.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio."], "venue": "NIPS, South Lake Tahoe, Nevada,", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Speedconstrained tuning for statistical machine translation using Bayesian optimization", "author": ["Daniel Beck", "Adri\u00e0 de Gispert", "Gonzalo Iglesias", "Aurelien Waite", "Bill Byrne."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of", "citeRegEx": "Beck et al\\.,? 2016", "shortCiteRegEx": "Beck et al\\.", "year": 2016}, {"title": "Findings of the 2016 conference on machine translation", "author": ["Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri."], "venue": "Proceedings of the First Conference on Machine Translation, pages 131\u2013", "citeRegEx": "Post et al\\.,? 2016", "shortCiteRegEx": "Post et al\\.", "year": 2016}, {"title": "Model compression", "author": ["Cristian Bucilu", "Rich Caruana", "Alexandru Niculescu-Mizil."], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535\u2013541. ACM.", "citeRegEx": "Bucilu et al\\.,? 2006", "shortCiteRegEx": "Bucilu et al\\.", "year": 2006}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Kyoto university participation to WAT 2016", "author": ["Fabien Cromieres", "Chenhui Chu", "Toshiaki Nakazawa", "Sadao Kurohashi."], "venue": "Proceedings of the 3rd Workshop on Asian Translation (WAT2016), pages 166\u2013174, Osaka, Japan. The COLING 2016 Orga-", "citeRegEx": "Cromieres et al\\.,? 2016", "shortCiteRegEx": "Cromieres et al\\.", "year": 2016}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Nando de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Emily L. Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus."], "venue": "Advances in Neural Information Processing Systems, pages 1269\u20131277.", "citeRegEx": "Denton et al\\.,? 2014", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Ensemble methods in machine learning", "author": ["Thomas G. Dietterich."], "venue": "International workshop on multiple classifier systems, pages 1\u201315. Springer.", "citeRegEx": "Dietterich.,? 2000", "shortCiteRegEx": "Dietterich.", "year": 2000}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research, pages 2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "QCRI machine translation systems for IWSLT 16", "author": ["Nadir Durrani", "Fahim Dalvi", "Hassan Sajjad", "Stephan Vogel."], "venue": "arXiv preprint arXiv:1701.03924.", "citeRegEx": "Durrani et al\\.,? 2017", "shortCiteRegEx": "Durrani et al\\.", "year": 2017}, {"title": "Ensemble distillation for neural machine translation", "author": ["Markus Freitag", "Yaser Al-Onaizan", "Baskaran Sankaran."], "venue": "arXiv preprint arXiv:1702.01802.", "citeRegEx": "Freitag et al\\.,? 2017", "shortCiteRegEx": "Freitag et al\\.", "year": 2017}, {"title": "Maxout networks", "author": ["Ian Goodfellow", "David Warde-Farley", "Mehdi Mirza", "Aaron Courville", "Yoshua Bengio."], "venue": "ICML, pages 1319\u20131327.", "citeRegEx": "Goodfellow et al\\.,? 2013", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally."], "venue": "Advances in Neural Information Processing Systems, pages 1135\u20131143.", "citeRegEx": "Han et al\\.,? 2015", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Neural network ensembles", "author": ["Lars Kai Hansen", "Peter Salamon."], "venue": "IEEE transactions on pattern analysis and machine intelligence, 12(10):993\u2013 1001.", "citeRegEx": "Hansen and Salamon.,? 1990", "shortCiteRegEx": "Hansen and Salamon.", "year": 1990}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["Babak Hassibi", "David G. Stork"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Hassibi and Stork,? \\Q1993\\E", "shortCiteRegEx": "Hassibi and Stork", "year": 1993}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean."], "venue": "NIPS Deep Learning Workshop.", "citeRegEx": "Hinton et al\\.,? 2014", "shortCiteRegEx": "Hinton et al\\.", "year": 2014}, {"title": "Is neural machine translation ready for deployment? A case study on 30 translation directions", "author": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Hieu Hoang."], "venue": "International Workshop on Spoken Language Translation IWSLT.", "citeRegEx": "Junczys.Dowmunt et al\\.,? 2016a", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "The AMU-UEDIN submission to the WMT16 news translation task: Attention-based NMT models as feature functions in phrase-based SMT", "author": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Rico Sennrich."], "venue": "Proceedings of the First Conference on", "citeRegEx": "Junczys.Dowmunt et al\\.,? 2016b", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Sequencelevel knowledge distillation", "author": ["Yoon Kim", "Alexander M. Rush."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317\u20131327, Austin, Texas. Association for Computational Linguistics.", "citeRegEx": "Kim and Rush.,? 2016", "shortCiteRegEx": "Kim and Rush.", "year": 2016}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S. Denker", "Sara A. Solla", "Richard E. Howard", "Lawrence D. Jackel."], "venue": "NIPS, volume 2, pages 598\u2013605.", "citeRegEx": "LeCun et al\\.,? 1989", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Learning compact recurrent neural networks", "author": ["Zhiyun Lu", "Vikas Sindhwani", "Tara N. Sainath."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on, pages 5960\u20135964. IEEE.", "citeRegEx": "Lu et al\\.,? 2016", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Blocks and Fuel: Frameworks for deep learning", "author": ["Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley", "Jan Chorowski", "Yoshua Bengio."], "venue": "CoRR.", "citeRegEx": "Merri\u00ebnboer et al\\.,? 2015", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "ASPEC: Asian scientific paper excerpt corpus", "author": ["Toshiaki Nakazawa", "Manabu Yaguchi", "Kiyotaka Uchimoto", "Masao Utiyama", "Eiichiro Sumita", "Sadao Kurohashi", "Hitoshi Isahara."], "venue": "LREC, pages 2204\u20132208, Portoroz, Slovenia.", "citeRegEx": "Nakazawa et al\\.,? 2016", "shortCiteRegEx": "Nakazawa et al\\.", "year": 2016}, {"title": "Lexicons and minimum risk training for neural machine translation: NAISTCMU at WAT2016", "author": ["Graham Neubig."], "venue": "Proceedings of the 3rd Workshop on Asian Translation (WAT2016), pages 119\u2013 125, Osaka, Japan. The COLING 2016 Organizing", "citeRegEx": "Neubig.,? 2016", "shortCiteRegEx": "Neubig.", "year": 2016}, {"title": "Neural reranking improves subjective quality of machine translation: NAIST at WAT2015", "author": ["Graham Neubig", "Makoto Morishita", "Satoshi Nakamura."], "venue": "Workshop on Asian Translation, pages 35\u201341.", "citeRegEx": "Neubig et al\\.,? 2015", "shortCiteRegEx": "Neubig et al\\.", "year": 2015}, {"title": "On the compression of recurrent neural networks with an application to LVCSR acoustic modeling for embedded speech recognition", "author": ["Rohit Prabhavalkar", "Ouais Alsharif", "Antoine Bruguier", "Lan McGraw."], "venue": "Acoustics, Speech and Signal Pro-", "citeRegEx": "Prabhavalkar et al\\.,? 2016", "shortCiteRegEx": "Prabhavalkar et al\\.", "year": 2016}, {"title": "Compression of neural machine translation models via pruning", "author": ["Abigail See", "Minh-Thang Luong", "D. Christopher Manning."], "venue": "Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 291\u2013301. Asso-", "citeRegEx": "See et al\\.,? 2016", "shortCiteRegEx": "See et al\\.", "year": 2016}, {"title": "Edinburgh neural machine translation systems for WMT 16", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the First Conference on Machine Translation, pages 371\u2013 376, Berlin, Germany. Association for Computa-", "citeRegEx": "Sennrich et al\\.,? 2016a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u2013", "citeRegEx": "Sennrich et al\\.,? 2016b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Data-free parameter pruning for deep neural networks", "author": ["Suraj Srinivas", "R. Venkatesh Babu."], "venue": "Proceedings of the British Machine Vision Conference (BMVC), pages 31.1\u201331.12. BMVA Press.", "citeRegEx": "Srinivas and Babu.,? 2015", "shortCiteRegEx": "Srinivas and Babu.", "year": 2015}, {"title": "SGNMT \u2013 A flexible NMT decoding platform for quick prototyping of new models and search strategies", "author": ["Felix Stahlberg", "Eva Hasler", "Danielle Saunders", "Bill Byrne."], "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Stahlberg et al\\.,? 2017", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2017}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems, pages 3104\u20133112. MIT Press.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Sequence student-teacher training of deep neural networks", "author": ["Jeremy HM. Wong", "Mark JF. Gales."], "venue": "Interspeech 2016, pages 2761\u20132765.", "citeRegEx": "Wong and Gales.,? 2016", "shortCiteRegEx": "Wong and Gales.", "year": 2016}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["Jian Xue", "Jinyu Li", "Yifan Gong."], "venue": "Interspeech, pages 2365\u20132369.", "citeRegEx": "Xue et al\\.,? 2013", "shortCiteRegEx": "Xue et al\\.", "year": 2013}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 32, "context": "use ensembles of a number of NMT systems (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Cromieres et al., 2016; Durrani et al., 2017).", "startOffset": 41, "endOffset": 182}, {"referenceID": 8, "context": "use ensembles of a number of NMT systems (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Cromieres et al., 2016; Durrani et al., 2017).", "startOffset": 41, "endOffset": 182}, {"referenceID": 28, "context": "use ensembles of a number of NMT systems (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Cromieres et al., 2016; Durrani et al., 2017).", "startOffset": 41, "endOffset": 182}, {"referenceID": 9, "context": "use ensembles of a number of NMT systems (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Cromieres et al., 2016; Durrani et al., 2017).", "startOffset": 41, "endOffset": 182}, {"referenceID": 14, "context": "use ensembles of a number of NMT systems (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Cromieres et al., 2016; Durrani et al., 2017).", "startOffset": 41, "endOffset": 182}, {"referenceID": 12, "context": "Ensembling (Dietterich, 2000; Hansen and Salamon, 1990) of neural networks is a simple yet very effective technique to improve the accuracy of NMT.", "startOffset": 11, "endOffset": 55}, {"referenceID": 18, "context": "Ensembling (Dietterich, 2000; Hansen and Salamon, 1990) of neural networks is a simple yet very effective technique to improve the accuracy of NMT.", "startOffset": 11, "endOffset": 55}, {"referenceID": 36, "context": "The decoder makes use of K NMT networks which are either trained independently (Sutskever et al., 2014; Chung et al., 2016; Neubig, 2016; Wu et al., 2016) or share some amount of training iterations (Sennrich et al.", "startOffset": 79, "endOffset": 154}, {"referenceID": 8, "context": "The decoder makes use of K NMT networks which are either trained independently (Sutskever et al., 2014; Chung et al., 2016; Neubig, 2016; Wu et al., 2016) or share some amount of training iterations (Sennrich et al.", "startOffset": 79, "endOffset": 154}, {"referenceID": 28, "context": "The decoder makes use of K NMT networks which are either trained independently (Sutskever et al., 2014; Chung et al., 2016; Neubig, 2016; Wu et al., 2016) or share some amount of training iterations (Sennrich et al.", "startOffset": 79, "endOffset": 154}, {"referenceID": 9, "context": ", 2016) or share some amount of training iterations (Sennrich et al., 2016b,a; Cromieres et al., 2016; Durrani et al., 2017).", "startOffset": 52, "endOffset": 124}, {"referenceID": 14, "context": ", 2016) or share some amount of training iterations (Sennrich et al., 2016b,a; Cromieres et al., 2016; Durrani et al., 2017).", "startOffset": 52, "endOffset": 124}, {"referenceID": 36, "context": "computes predictions from each of the individual models which are then combined using the arithmetic average (Sutskever et al., 2014) or the geometric average (Cromieres et al.", "startOffset": 109, "endOffset": 133}, {"referenceID": 9, "context": ", 2014) or the geometric average (Cromieres et al., 2016).", "startOffset": 33, "endOffset": 57}, {"referenceID": 6, "context": "Therefore, a recent line of research transfers the idea of knowledge distillation (Bucilu et al., 2006; Hinton et al., 2014) to NMT and trains a smaller network (the student) by minimizing the cross-entropy to the output of the ensemble system", "startOffset": 82, "endOffset": 124}, {"referenceID": 20, "context": "Therefore, a recent line of research transfers the idea of knowledge distillation (Bucilu et al., 2006; Hinton et al., 2014) to NMT and trains a smaller network (the student) by minimizing the cross-entropy to the output of the ensemble system", "startOffset": 82, "endOffset": 124}, {"referenceID": 23, "context": "(the teacher) (Kim and Rush, 2016; Freitag et al., 2017).", "startOffset": 14, "endOffset": 56}, {"referenceID": 15, "context": "(the teacher) (Kim and Rush, 2016; Freitag et al., 2017).", "startOffset": 14, "endOffset": 56}, {"referenceID": 24, "context": "This idea is justified by the fact that ensembled neural networks are often over-parameterized and have a large degree of redundancy (LeCun et al., 1989; Hassibi et al., 1993; Srinivas and Babu, 2015).", "startOffset": 133, "endOffset": 200}, {"referenceID": 34, "context": "This idea is justified by the fact that ensembled neural networks are often over-parameterized and have a large degree of redundancy (LeCun et al., 1989; Hassibi et al., 1993; Srinivas and Babu, 2015).", "startOffset": 133, "endOffset": 200}, {"referenceID": 2, "context": ", 2014, GRUs) and attention (Bahdanau et al., 2015).", "startOffset": 28, "endOffset": 51}, {"referenceID": 34, "context": "Our procedure is inspired by the method of Srinivas and Babu (2015). They propose a criterion for removing neurons in inner layers of the network based on two intu-", "startOffset": 43, "endOffset": 68}, {"referenceID": 34, "context": "The second intuition of the criterion used by Srinivas and Babu (2015) is that neurons with small outgoing weights contribute very little overall.", "startOffset": 46, "endOffset": 71}, {"referenceID": 34, "context": "1 generalizes the criterion of Srinivas and Babu (2015) to multiple outgoing weights.", "startOffset": 31, "endOffset": 56}, {"referenceID": 34, "context": "1 generalizes the criterion of Srinivas and Babu (2015) to multiple outgoing weights. Also note that Srinivas and Babu (2015) propose some heuristic improvements to this criterion.", "startOffset": 31, "endOffset": 126}, {"referenceID": 34, "context": "details we refer to Srinivas and Babu (2015).", "startOffset": 20, "endOffset": 45}, {"referenceID": 34, "context": "Note that we recover the update rule of Srinivas and Babu (2015) by setting \u03bb to the i-th unit vector.", "startOffset": 40, "endOffset": 65}, {"referenceID": 7, "context": "GRU layers The terminology of neurons needs some further elaboration for GRU layers which rather consist of update and reset gates and states (Cho et al., 2014).", "startOffset": 142, "endOffset": 160}, {"referenceID": 34, "context": "Although we find our data-free approach to be a substantial improvement over the methods of Srinivas and Babu (2015) on NMT networks, it", "startOffset": 92, "endOffset": 117}, {"referenceID": 1, "context": "In the spirit of Babaeizadeh et al. (2016), our data-bound version records neuron", "startOffset": 17, "endOffset": 43}, {"referenceID": 13, "context": "We use the AdaGrad (Duchi et al., 2011) step rule, a small learning rate of 0.", "startOffset": 19, "endOffset": 39}, {"referenceID": 2, "context": "The standard attention-based NMT network architecture (Bahdanau et al., 2015) includes three lin-", "startOffset": 54, "endOffset": 77}, {"referenceID": 39, "context": "The individual NMT systems we use as source for constructing the unfolded networks are trained using AdaDelta (Zeiler, 2012) on the Blocks/Theano implementation (van Merri\u00ebnboer et al.", "startOffset": 110, "endOffset": 124}, {"referenceID": 2, "context": ", 2012) of the standard attentionbased NMT model (Bahdanau et al., 2015) with: 1000 dimensional GRU layers (Cho et al.", "startOffset": 49, "endOffset": 72}, {"referenceID": 7, "context": ", 2015) with: 1000 dimensional GRU layers (Cho et al., 2014) in both the decoder and bidrectional encoder; a single maxout output layer (Goodfellow et al.", "startOffset": 42, "endOffset": 60}, {"referenceID": 16, "context": ", 2014) in both the decoder and bidrectional encoder; a single maxout output layer (Goodfellow et al., 2013); and 620 dimensional embedding layers.", "startOffset": 83, "endOffset": 108}, {"referenceID": 35, "context": "Our SGNMT decoder (Stahlberg et al., 2017)4 with a beam size of 12 is used in all experiments.", "startOffset": 18, "endOffset": 42}, {"referenceID": 27, "context": "Our primary corpus is the Japanese-English (Ja-En) ASPEC data set (Nakazawa et al., 2016).", "startOffset": 66, "endOffset": 89}, {"referenceID": 2, "context": ", 2012) of the standard attentionbased NMT model (Bahdanau et al., 2015) with: 1000 dimensional GRU layers (Cho et al., 2014) in both the decoder and bidrectional encoder; a single maxout output layer (Goodfellow et al., 2013); and 620 dimensional embedding layers. We follow Sennrich et al. (2016b) and use subword units based on byte pair encoding rather than words as modelling units.", "startOffset": 50, "endOffset": 300}, {"referenceID": 2, "context": ", 2012) of the standard attentionbased NMT model (Bahdanau et al., 2015) with: 1000 dimensional GRU layers (Cho et al., 2014) in both the decoder and bidrectional encoder; a single maxout output layer (Goodfellow et al., 2013); and 620 dimensional embedding layers. We follow Sennrich et al. (2016b) and use subword units based on byte pair encoding rather than words as modelling units. Our SGNMT decoder (Stahlberg et al., 2017)4 with a beam size of 12 is used in all experiments. Our primary corpus is the Japanese-English (Ja-En) ASPEC data set (Nakazawa et al., 2016). We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. (2015). We report cased BLEU scores calculated with Moses\u2019", "startOffset": 50, "endOffset": 673}, {"referenceID": 34, "context": "plied the method of Srinivas and Babu (2015) to", "startOffset": 20, "endOffset": 45}, {"referenceID": 34, "context": "Results with the original method of Srinivas and Babu (2015) are not included in Tab.", "startOffset": 36, "endOffset": 61}, {"referenceID": 4, "context": "This is of great practical use: batch decoding with only two CPU threads surpasses production speed which is often set to 2000 words per minute (Beck et al., 2016).", "startOffset": 144, "endOffset": 163}, {"referenceID": 24, "context": "The idea of pruning neural networks to improve the compactness of the models dates back more than 25 years (LeCun et al., 1989).", "startOffset": 107, "endOffset": 127}, {"referenceID": 0, "context": "The literature is therefore vast (Augasta and Kathirvalavakumar, 2013).", "startOffset": 33, "endOffset": 70}, {"referenceID": 24, "context": "The connections can be selected for deletion based on the secondderivative of the training error with respect to the weight (LeCun et al., 1989; Hassibi et al., 1993),", "startOffset": 124, "endOffset": 166}, {"referenceID": 17, "context": "or by a threshold criterion on its magnitude (Han et al., 2015).", "startOffset": 45, "endOffset": 63}, {"referenceID": 17, "context": "or by a threshold criterion on its magnitude (Han et al., 2015). See et al. (2016) confirmed a high degree of weight redundancy in NMT networks.", "startOffset": 46, "endOffset": 83}, {"referenceID": 10, "context": "Using low rank matrices for neural network compression, particularly approximations via SVD, has been studied widely in the literature (Denil et al., 2013; Denton et al., 2014; Xue et al., 2013; Prabhavalkar et al., 2016; Lu et al., 2016).", "startOffset": 135, "endOffset": 238}, {"referenceID": 11, "context": "Using low rank matrices for neural network compression, particularly approximations via SVD, has been studied widely in the literature (Denil et al., 2013; Denton et al., 2014; Xue et al., 2013; Prabhavalkar et al., 2016; Lu et al., 2016).", "startOffset": 135, "endOffset": 238}, {"referenceID": 38, "context": "Using low rank matrices for neural network compression, particularly approximations via SVD, has been studied widely in the literature (Denil et al., 2013; Denton et al., 2014; Xue et al., 2013; Prabhavalkar et al., 2016; Lu et al., 2016).", "startOffset": 135, "endOffset": 238}, {"referenceID": 30, "context": "Using low rank matrices for neural network compression, particularly approximations via SVD, has been studied widely in the literature (Denil et al., 2013; Denton et al., 2014; Xue et al., 2013; Prabhavalkar et al., 2016; Lu et al., 2016).", "startOffset": 135, "endOffset": 238}, {"referenceID": 25, "context": "Using low rank matrices for neural network compression, particularly approximations via SVD, has been studied widely in the literature (Denil et al., 2013; Denton et al., 2014; Xue et al., 2013; Prabhavalkar et al., 2016; Lu et al., 2016).", "startOffset": 135, "endOffset": 238}, {"referenceID": 29, "context": "1, our data-free method is an extension of the approach by Srinivas and Babu (2015); our extension performs significantly better on NMT networks.", "startOffset": 59, "endOffset": 84}, {"referenceID": 1, "context": "2) is inspired by Babaeizadeh et al. (2016) as we combine neurons with similar activities during training, but we use linear combinations of multiple neurons to compensate for the loss of a neuron rather than merging pairs of neurons.", "startOffset": 18, "endOffset": 44}, {"referenceID": 6, "context": "teacher) to generate soft training labels for the smaller student network (Bucilu et al., 2006; Hinton et al., 2014).", "startOffset": 74, "endOffset": 116}, {"referenceID": 20, "context": "teacher) to generate soft training labels for the smaller student network (Bucilu et al., 2006; Hinton et al., 2014).", "startOffset": 74, "endOffset": 116}, {"referenceID": 37, "context": "This idea has been applied to sequence modelling tasks such as machine translation and speech recognition (Wong and Gales, 2016; Kim and Rush, 2016; Freitag et al., 2017).", "startOffset": 106, "endOffset": 170}, {"referenceID": 23, "context": "This idea has been applied to sequence modelling tasks such as machine translation and speech recognition (Wong and Gales, 2016; Kim and Rush, 2016; Freitag et al., 2017).", "startOffset": 106, "endOffset": 170}, {"referenceID": 15, "context": "This idea has been applied to sequence modelling tasks such as machine translation and speech recognition (Wong and Gales, 2016; Kim and Rush, 2016; Freitag et al., 2017).", "startOffset": 106, "endOffset": 170}, {"referenceID": 32, "context": "Unfolding and shrinking diverse networks could be possible, for example by applying the technique only to the input and output layers or by some other scheme of finding associations between units in different models, but we leave this investigation to future work as models in NMT ensembles in current research usually have the same topology (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Durrani et al., 2017).", "startOffset": 342, "endOffset": 459}, {"referenceID": 8, "context": "Unfolding and shrinking diverse networks could be possible, for example by applying the technique only to the input and output layers or by some other scheme of finding associations between units in different models, but we leave this investigation to future work as models in NMT ensembles in current research usually have the same topology (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Durrani et al., 2017).", "startOffset": 342, "endOffset": 459}, {"referenceID": 28, "context": "Unfolding and shrinking diverse networks could be possible, for example by applying the technique only to the input and output layers or by some other scheme of finding associations between units in different models, but we leave this investigation to future work as models in NMT ensembles in current research usually have the same topology (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Durrani et al., 2017).", "startOffset": 342, "endOffset": 459}, {"referenceID": 14, "context": "Unfolding and shrinking diverse networks could be possible, for example by applying the technique only to the input and output layers or by some other scheme of finding associations between units in different models, but we leave this investigation to future work as models in NMT ensembles in current research usually have the same topology (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Durrani et al., 2017).", "startOffset": 342, "endOffset": 459}], "year": 2017, "abstractText": "Ensembling is a well-known technique in neural machine translation (NMT) to improve system performance. Instead of a single neural net, multiple neural nets with the same topology are trained separately, and the decoder generates predictions by averaging over the individual models. Ensembling often improves the quality of the generated translations drastically. However, it is not suitable for production systems because it is cumbersome and slow. This work aims to reduce the runtime to be on par with a single system without compromising the translation quality. First, we show that the ensemble can be unfolded into a single large neural network which imitates the output of the ensemble system. We show that unfolding can already improve the runtime in practice since more work can be done on the GPU. We proceed by describing a set of techniques to shrink the unfolded network by reducing the dimensionality of layers. On JapaneseEnglish we report that the resulting network has the size and decoding speed of a single NMT network but performs on the level of a 3-ensemble system.", "creator": "LaTeX with hyperref package"}}}