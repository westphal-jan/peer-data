{"id": "1704.03471", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2017", "title": "What do Neural Machine Translation Models Learn about Morphology?", "abstract": "Neural machine translation (MT) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture. However, little is known about what these models learn about source and target languages during the training process. In this work, we analyze the representations learned by neural MT models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks. We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects in the neural MT system and its ability to capture word structure.", "histories": [["v1", "Tue, 11 Apr 2017 18:01:07 GMT  (3628kb,D)", "http://arxiv.org/abs/1704.03471v1", "Accepted to ACL 2017"], ["v2", "Mon, 15 May 2017 13:38:20 GMT  (1418kb,D)", "http://arxiv.org/abs/1704.03471v2", "ACL 2017 camera-ready"]], "COMMENTS": "Accepted to ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yonatan belinkov", "nadir durrani", "fahim dalvi", "hassan sajjad", "james r glass"], "accepted": true, "id": "1704.03471"}, "pdf": {"name": "1704.03471.pdf", "metadata": {"source": "CRF", "title": "What do Neural Machine Translation Models Learn about Morphology?", "authors": ["Yonatan Belinkov", "Nadir Durrani", "Fahim Dalvi", "Hassan Sajjad", "James Glass"], "emails": ["glass}@mit.edu", "hsajjad}@qf.org.qa"], "sections": [{"heading": null, "text": "In this paper, we analyze the representations acquired through neural MT models at different levels of granularity and evaluate empirically the quality of representations for learning morphology through extrinsic parts of language and morphological tagging tasks. We conduct a thorough investigation using several parameters: word-based vs. sign-based representations, depth of coding layer, identity of target language and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects of the neural MT system and its ability to capture word structure."}, {"heading": "1 Introduction", "text": "rE \"s tis, so rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc"}, {"heading": "2 Methodology", "text": "It is indeed the case that we are able to correct the errors that have been mentioned, and that we are able to correct them in order to correct them."}, {"heading": "3 Data", "text": "Language pairs We are experimenting with several language pairs, including morphologically rich languages, which have received relatively high attention in the MT community, including Arabic, German, French and Czech-English pairs. To expand our analysis and investigate the effects of morphologically rich languages on both the source and target sites, we are also including Arabic-Hebrew, two languages with rich and similar morphological systems, and Arabic-German, two languages with rich but different morphologies. Our translation models are trained on the WIT3 corpus of the TED conversations (Cettolo et al., 2012; Cettolo, 2016) provided for IWSLT 2016. This allows comparable and cross-lingual analysis. Statistics on each language pair are given in Table 1 (under Pred)."}, {"heading": "4 Encoder Analysis", "text": "Remember that after training the NMT system, we freeze its parameters and use them only to generate characteristics for the POS / Morphology Classifier. Faced with a trained ENC encoder and a set of POS / Morphology annotations, we generate word characteristics ENCi (s) for each word in the sentence. We then train a classifier that uses the characteristics ENCi (s) to predict POS or morphological tags."}, {"heading": "4.1 Effect of word representation", "text": "In this year it has come to the point that we will be able to see ourselves entering another world, in which we are able to understand the world in which we live, and in which we are able to change the world in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we, in which we live, in which we, in which we live, in which we, in which we live, in which we live, in which we live, in which we live, in which we, in which we live, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, we, in which we, we, in which we, we, we, we, we, we, we, we, we, we, we, we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, we, in which we, we, we, we, we, we, we, we, we, in which, we, we, we, we, we, in which, we, we, we, we, we, we, we, we, we, in which, we, we, we, we, we, we, we, we, we, we, we, we, in, we, we, we, we, we, we, we, we, we,"}, {"heading": "4.2 Effect of encoder depth", "text": "Modern NMT systems use very deep architectures with up to 8 or 16 layers (Wu et al., 2016; Zhou et al., 2016). We would like to understand what kind of information different layers capture. Givena trained a multi-layer model to extract representations from the different layers in the encoder. Let's name ENCli (s) the encoded representation of the word wi after the l-th layer. We vary l and train different classifiers to predict POS or morphological tags. Here, we focus on the case of a 2-layer encoder decoder for simplicity. Figure 6 shows POS tagging results using representations from different coding layers across five language pairs. The general trend is that the passing of word vectors by the encoder improves POS tagging, which can be explained by contextual information contained in the representations by one layer."}, {"heading": "4.3 Effect of target language", "text": "As we move away from the morphologically rich languages, translation into these languages is even more difficult. Thus, our basic system achieves a value of 24.69 / 23.2 in Arabic / Czech, but only 13.37 / 13.9 in English / Czech. How can the target language influence the source languages learned? Is translation into a morphologically rich language necessary? These target languages represent a morphologically poor language (English), in which we fix the source language and apply NMT models to different target languages."}, {"heading": "5 Decoder Analysis", "text": "So far, we have only looked at the encoder. However, the DEC decoder plays a crucial role in an MT system with access to source and target sentences. To investigate what the decoder learns about morphology, we first train an NMT system on the target page. Then, we use the trained model to encode a source set and extract attributes for words in the target sentence. These attributes are used to train a classifier on the target page or morphological marking on the target page. 4 Note that in this case, the decoder is given oneby-one the correct target words, similar to the usual NMT training regime.Table 3 (first row) shows the results of using representations extracted from the Arabic-English and English-Arabic models with ENC and DEC."}, {"heading": "5.1 Effect of attention", "text": "Consider the role of the attention mechanism in learning useful representations: During decoding, the attention weights are combined with the hidden states of the decoder to produce the current translation, and these two sources of information must collectively point to the most relevant source word (s) and predict the most likely word (s). Therefore, the decoder attaches great importance to mapping the source set, which may come at the expense of a meaningful representation of the current word. We suspect that the attention mechanism harms the quality of the target representations learned by the decoder. To test this hypothesis, we train NMT models with and without attention and compare the quality of their learned representations. As shown in Table 3 (compare 1st and 2nd line), eliminating the attention mechanism reduces the quality of the encoding representations, but improves the quality of the decoder representations."}, {"heading": "5.2 Effect of word representation", "text": "By character representation we mean a CNN character on the input words. The decoder predictions are still made at the word level, which allows us to use their hidden states as word representations.Table 4 shows the accuracy of the POS representation of word representations as opposed to character representations in the encoder and decoder. Character representations improve the encoder, but do not help the decoder. BLEU values behave similarly: The character model leads to better translations in Arabic into English, but not in English into Arabic. One possible explanation for this is that the predictions of the decoder are still made at the word level, even with the character model (which encodes the target input, but not the output).In practice, this can lead to the generation of unknown words. In fact, the character model in Arabic into English reduces the number of unknowns generated in English by about 25%, while the number in Arabic remains unchanged between the models set."}, {"heading": "6 Related Work", "text": "The opacity of neural networks has motivated researchers to analyze such models in different ways (2016). A workline visualizes hidden unit activations in recursive neural networks trained for a specific task (Elman, 1991; Karpathy et al., 2015; Ka \u0301 da \u0301 r et al., 2016; Qian et al., 2016a). While such visualizations illuminate the inner workings of the network, they are often qualitative in nature and somewhat anecdotal. Another approach attempts to provide quantitative analysis by correlating parts of the neural network with linguistic properties, for example by training a classifier to predict characteristics of interest. Different units have been used, from word embedding, 2015; Qian et al al al al al al al., 2016b) by correlating parts of the neural network with linguistic properties (Qian et al., 2016, to Atta)."}, {"heading": "7 Conclusion", "text": "Neural networks have become ubiquitous in machine translation due to their elegant architecture and good performance, and the representations they use for linguistic units are crucial for high-quality translation. In this paper, we examined how neural MT models learn word structure. We evaluated their display quality at the POS and morphological marking in a number of languages. Our results lead to the following conclusions: \u2022 Character-based representations are better for learning word structure than word-based representations, especially in rare and invisible words. \u2022 Lower layers of the neural network are better at capturing morphology, while deeper networks improve translation performance. \u2022 We suspect that lower layers are more focused on word structure, while higher layers are focused on word meaning. \u2022 Translation into morphologically poorer languages leads to better source-side representations. This is partially correlated with BLEU. \u2022 Attention-oriented analyses may not lead to more morphological information or better understanding of these morphological systems."}, {"heading": "A Supplementary Material", "text": "The POS / POS labeling for all prediction tasks is a feed-forward network with a hidden layer in which the hidden layers are identical to the size of the hidden state (usually 500 dimensions). We use Adam (Kingma and Ba, 2014) with default parameters to minimize the cross-entropy target. Training is performed with size 16 mini batches and terminated as soon as the loss on the dev set stops improving. We allow a patience of 5 epochs. Neural MT system We train a 2-layer LSTM encoder decoder decoder with attention. We use the seq2seq-attn implemented languages (Kim, 2016) with the following default settings: word vectors and LSTM indicators."}], "references": [{"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "author": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg."], "venue": "arXiv preprint arXiv:1608.04207 .", "citeRegEx": "Adi et al\\.,? 2016", "shortCiteRegEx": "Adi et al\\.", "year": 2016}, {"title": "Segmentation for English-to-Arabic Statistical Machine Translation", "author": ["Ibrahim Badr", "Rabih Zbib", "James Glass."], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Lan-", "citeRegEx": "Badr et al\\.,? 2008", "shortCiteRegEx": "Badr et al\\.", "year": 2008}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Large-Scale Machine Translation between Arabic and Hebrew: Available Corpora and Initial Results", "author": ["Yonatan Belinkov", "James Glass."], "venue": "Proceedings of the Workshop on Semitic Machine Translation. Association for Computational Linguistics,", "citeRegEx": "Belinkov and Glass.,? 2016", "shortCiteRegEx": "Belinkov and Glass.", "year": 2016}, {"title": "Neural versus PhraseBased Machine Translation Quality: a Case Study", "author": ["Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo", "Marcello Federico."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Associa-", "citeRegEx": "Bentivogli et al\\.,? 2016", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2016}, {"title": "An Arabic-Hebrew parallel corpus of TED talks", "author": ["Mauro Cettolo."], "venue": "Proceedings of the AMTA Workshop on Semitic Machine Translation (SeMaT). Austin, US-TX.", "citeRegEx": "Cettolo.,? 2016", "shortCiteRegEx": "Cettolo.", "year": 2016}, {"title": "WIT: Web Inventory of Transcribed and Translated Talks", "author": ["Mauro Cettolo", "Christian Girardi", "Marcello Federico."], "venue": "Proceedings of the 16 Conference of the European Association for Machine Translation (EAMT). Trento, Italy, pages 261\u2013", "citeRegEx": "Cettolo et al\\.,? 2012", "shortCiteRegEx": "Cettolo et al\\.", "year": 2012}, {"title": "Character-based Neural Machine Translation", "author": ["Marta R. Costa-juss\u00e0", "Jos\u00e9 A.R. Fonollosa."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computa-", "citeRegEx": "Costa.juss\u00e0 and Fonollosa.,? 2016", "shortCiteRegEx": "Costa.juss\u00e0 and Fonollosa.", "year": 2016}, {"title": "Distributed representations, simple recurrent networks, and grammatical structure", "author": ["Jeffrey L Elman."], "venue": "Machine learning 7(2-3):195\u2013225.", "citeRegEx": "Elman.,? 1991", "shortCiteRegEx": "Elman.", "year": 1991}, {"title": "From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning", "author": ["Lieke Gelderloos", "Grzegorz Chrupa\u0142a."], "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational", "citeRegEx": "Gelderloos and Chrupa\u0142a.,? 2016", "shortCiteRegEx": "Gelderloos and Chrupa\u0142a.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Exploring the Limits of Language Modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."], "venue": "arXiv preprint arXiv:1602.02410 .", "citeRegEx": "Jozefowicz et al\\.,? 2016", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Representation of linguistic form and function in recurrent neural networks", "author": ["\u00c1kos K\u00e1d\u00e1r", "Grzegorz Chrupa\u0142a", "Afra Alishahi."], "venue": "arXiv preprint arXiv:1602.08952 .", "citeRegEx": "K\u00e1d\u00e1r et al\\.,? 2016", "shortCiteRegEx": "K\u00e1d\u00e1r et al\\.", "year": 2016}, {"title": "Visualizing and Understanding Recurrent Networks", "author": ["Andrej Karpathy", "Justin Johnson", "Fei-Fei Li."], "venue": "arXiv preprint arXiv:1506.02078 .", "citeRegEx": "Karpathy et al\\.,? 2015", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Seq2seq-attn", "author": ["Yoon Kim."], "venue": "https:// github.com/harvardnlp/seq2seq-attn.", "citeRegEx": "Kim.,? 2016", "shortCiteRegEx": "Kim.", "year": 2016}, {"title": "Character-aware Neural Language Models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."], "venue": "arXiv preprint arXiv:1508.06615 .", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Factored Translation Models", "author": ["Philipp Koehn", "Hieu Hoang."], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-", "citeRegEx": "Koehn and Hoang.,? 2007", "shortCiteRegEx": "Koehn and Hoang.", "year": 2007}, {"title": "Empirical Methods for Compound Splitting", "author": ["Philipp Koehn", "Kevin Knight."], "venue": "10th Conference of the European Chapter of the Association for Computational Linguistics. pages 187\u2013194. http://www.aclweb.org/anthology/E03-1076.", "citeRegEx": "Koehn and Knight.,? 2003", "shortCiteRegEx": "Koehn and Knight.", "year": 2003}, {"title": "What\u2019s in an Embedding? Analyzing Word Embeddings through Multilingual Evaluation", "author": ["Arne K\u00f6hn."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational", "citeRegEx": "K\u00f6hn.,? 2015", "shortCiteRegEx": "K\u00f6hn.", "year": 2015}, {"title": "Stanford Neural Machine Translation Systems for Spoken Language Domains", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "Proceedings of the International Workshop on Spoken Language Translation. Da Nang, Vietnam.", "citeRegEx": "Luong and Manning.,? 2015", "shortCiteRegEx": "Luong and Manning.", "year": 2015}, {"title": "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models", "author": ["Minh-Thang Luong", "D. Christopher Manning."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Luong and Manning.,? 2016", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "A Hybrid Morpheme-Word Representation for Machine Translation of Morphologically Rich Languages", "author": ["Minh-Thang Luong", "Preslav Nakov", "Min-Yen Kan."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Luong et al\\.,? 2010", "shortCiteRegEx": "Luong et al\\.", "year": 2010}, {"title": "Efficient Higher-Order CRFs for Morphological Tagging", "author": ["Thomas Mueller", "Helmut Schmid", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational", "citeRegEx": "Mueller et al\\.,? 2013", "shortCiteRegEx": "Mueller et al\\.", "year": 2013}, {"title": "Improving SMT quality with morpho-syntactic analysis", "author": ["Sonja Nieflen", "Hermann Ney."], "venue": "COLING 2000 Volume 2: The 18th International Conference on Computational Linguistics. http://www.aclweb.org/anthology/C00-2162.", "citeRegEx": "Nieflen and Ney.,? 2000", "shortCiteRegEx": "Nieflen and Ney.", "year": 2000}, {"title": "MADAMIRA: A Fast, Comprehensive Tool for Morphological Analysis and Disambiguation", "author": ["Arfath Pasha", "Mohamed Al-Badrashiny", "Mona Diab", "Ahmed El Kholy", "Ramy Eskander", "Nizar Habash", "Manoj Pooleery", "Owen Rambow", "Ryan Roth"], "venue": null, "citeRegEx": "Pasha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pasha et al\\.", "year": 2014}, {"title": "Analyzing Linguistic Knowledge in Sequential Model of Sentence", "author": ["Peng Qian", "Xipeng Qiu", "Xuanjing Huang."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Compu-", "citeRegEx": "Qian et al\\.,? 2016a", "shortCiteRegEx": "Qian et al\\.", "year": 2016}, {"title": "Investigating Language Universal and Spe", "author": ["Peng Qian", "Xipeng Qiu", "Xuanjing Huang"], "venue": null, "citeRegEx": "Qian et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Qian et al\\.", "year": 2016}, {"title": "Maximum Entropy Models for Natural Language Ambiguity Resolution", "author": ["Adwait Ratnaparkhi."], "venue": "Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.", "citeRegEx": "Ratnaparkhi.,? 1998", "shortCiteRegEx": "Ratnaparkhi.", "year": 1998}, {"title": "Part-of-Speech Tagging with Neural Networks", "author": ["Helmut Schmid."], "venue": "Proceedings of the 15th International Conference on Computational Linguistics (Coling 1994). Coling 1994 Organizing Committee, Kyoto, Japan, pages 172\u2013176.", "citeRegEx": "Schmid.,? 1994", "shortCiteRegEx": "Schmid.", "year": 1994}, {"title": "LoPar: Design and Implementation", "author": ["Helmut Schmid."], "venue": "Bericht des Sonderforschungsbereiches \u201cSprachtheoretische Grundlagen fr die Computerlinguistik\u201d 149, Institute for Computational Linguistics, University of Stuttgart.", "citeRegEx": "Schmid.,? 2000", "shortCiteRegEx": "Schmid.", "year": 2000}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Does String-Based Neural MT Learn Source Syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "author": ["Xing Shi", "Inkit Padhi", "Kevin Knight."], "venue": "Association for Computational", "citeRegEx": "Shi et al\\.,? 2016", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Word Representation Models for Morphologically Rich Languages in Neural Machine Translation", "author": ["Ekaterina Vylomova", "Trevor Cohn", "Xuanli He", "Gholamreza Haffari."], "venue": "arXiv preprint arXiv:1606.04217 .", "citeRegEx": "Vylomova et al\\.,? 2016", "shortCiteRegEx": "Vylomova et al\\.", "year": 2016}, {"title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation", "author": ["Jie Zhou", "Ying Cao", "Xuguang Wang", "Peng Li", "Wei Xu."], "venue": "Transactions of the Association for Computational Linguistics 4:371\u2013383.", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Moreover, NMT systems have become competitive with, or better than, the previous state-of-the-art, especially since the introduction of sequence-to-sequence models and the attention mechanism (Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 192, "endOffset": 239}, {"referenceID": 33, "context": "Moreover, NMT systems have become competitive with, or better than, the previous state-of-the-art, especially since the introduction of sequence-to-sequence models and the attention mechanism (Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 192, "endOffset": 239}, {"referenceID": 20, "context": "The improved translation quality is often attributed to better handling of non-local dependencies and morphology generation (Luong and Manning, 2015; Bentivogli et al., 2016).", "startOffset": 124, "endOffset": 174}, {"referenceID": 4, "context": "The improved translation quality is often attributed to better handling of non-local dependencies and morphology generation (Luong and Manning, 2015; Bentivogli et al., 2016).", "startOffset": 124, "endOffset": 174}, {"referenceID": 32, "context": "Recent work has started exploring the role of the NMT encoder in learning source syntax (Shi et al., 2016), but research studies are yet to answer important questions such", "startOffset": 88, "endOffset": 106}, {"referenceID": 33, "context": "2) (Sutskever et al., 2014):", "startOffset": 3, "endOffset": 27}, {"referenceID": 10, "context": "In this work, we use long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) Figure 1: Illustration of our approach: (i) NMT system trained on parallel data; (ii) features extracted from pre-trained model; (iii) classifier trained using the extracted features.", "startOffset": 51, "endOffset": 85}, {"referenceID": 2, "context": "encoder-decoders with attention (Bahdanau et al., 2014), which we train on parallel data.", "startOffset": 32, "endOffset": 55}, {"referenceID": 26, "context": "We also experimented with a linear classifier and observed similar trends to the non-linear case, but overall lower results; Qian et al. (2016b) reported similar findings.", "startOffset": 125, "endOffset": 145}, {"referenceID": 15, "context": "model we adopt a convolutional neural network (CNN) over character embeddings that is also learned during training (Kim et al., 2015; Costajuss\u00e0 and Fonollosa, 2016); see appendix A.", "startOffset": 115, "endOffset": 165}, {"referenceID": 25, "context": "1 on Arabic POS/morphology (Pasha et al., 2014)), indicating that NMT models learn quite good representations.", "startOffset": 27, "endOffset": 47}, {"referenceID": 35, "context": "Modern NMT systems use very deep architectures with up to 8 or 16 layers (Wu et al., 2016; Zhou et al., 2016).", "startOffset": 73, "endOffset": 109}, {"referenceID": 9, "context": "A similar pattern was recently observed in a joint language-vision deep recurrent net (Gelderloos and Chrupa\u0142a, 2016).", "startOffset": 86, "endOffset": 117}, {"referenceID": 8, "context": "work visualizes hidden unit activations in recurrent neural networks that are trained for a given task (Elman, 1991; Karpathy et al., 2015; K\u00e1d\u00e1r et al., 2016; Qian et al., 2016a).", "startOffset": 103, "endOffset": 179}, {"referenceID": 13, "context": "work visualizes hidden unit activations in recurrent neural networks that are trained for a given task (Elman, 1991; Karpathy et al., 2015; K\u00e1d\u00e1r et al., 2016; Qian et al., 2016a).", "startOffset": 103, "endOffset": 179}, {"referenceID": 12, "context": "work visualizes hidden unit activations in recurrent neural networks that are trained for a given task (Elman, 1991; Karpathy et al., 2015; K\u00e1d\u00e1r et al., 2016; Qian et al., 2016a).", "startOffset": 103, "endOffset": 179}, {"referenceID": 26, "context": "work visualizes hidden unit activations in recurrent neural networks that are trained for a given task (Elman, 1991; Karpathy et al., 2015; K\u00e1d\u00e1r et al., 2016; Qian et al., 2016a).", "startOffset": 103, "endOffset": 179}, {"referenceID": 19, "context": "Different units have been used, from word embeddings (K\u00f6hn, 2015; Qian et al., 2016b), through LSTM gates or states (Qian et al.", "startOffset": 53, "endOffset": 85}, {"referenceID": 26, "context": ", 2016b), through LSTM gates or states (Qian et al., 2016a), to sentence embeddings (Adi et al.", "startOffset": 39, "endOffset": 59}, {"referenceID": 0, "context": ", 2016a), to sentence embeddings (Adi et al., 2016).", "startOffset": 33, "endOffset": 51}, {"referenceID": 0, "context": ", 2016a), to sentence embeddings (Adi et al., 2016). Our work is most similar to Shi et al. (2016), who use hidden vectors from a neural MT encoder to", "startOffset": 34, "endOffset": 99}, {"referenceID": 34, "context": "Vylomova et al. (2016) also analyze different representations for morphologically-rich languages in MT, but do not directly measure the quality of the learned representations.", "startOffset": 0, "endOffset": 23}, {"referenceID": 24, "context": "Word representations in MT Machine translation systems that deal with morphologically-rich languages resort to various techniques for representing morphological knowledge, such as word segmentation (Nieflen and Ney, 2000; Koehn and Knight, 2003; Badr et al., 2008) and factored translation models (Koehn and Hoang, 2007).", "startOffset": 198, "endOffset": 264}, {"referenceID": 18, "context": "Word representations in MT Machine translation systems that deal with morphologically-rich languages resort to various techniques for representing morphological knowledge, such as word segmentation (Nieflen and Ney, 2000; Koehn and Knight, 2003; Badr et al., 2008) and factored translation models (Koehn and Hoang, 2007).", "startOffset": 198, "endOffset": 264}, {"referenceID": 1, "context": "Word representations in MT Machine translation systems that deal with morphologically-rich languages resort to various techniques for representing morphological knowledge, such as word segmentation (Nieflen and Ney, 2000; Koehn and Knight, 2003; Badr et al., 2008) and factored translation models (Koehn and Hoang, 2007).", "startOffset": 198, "endOffset": 264}, {"referenceID": 17, "context": ", 2008) and factored translation models (Koehn and Hoang, 2007).", "startOffset": 40, "endOffset": 63}, {"referenceID": 22, "context": "Characters and other sub-word units have become increasingly popular in neural MT, although they had also been used in phrase-based MT (Luong et al., 2010).", "startOffset": 135, "endOffset": 155}, {"referenceID": 31, "context": "by byte-pair encoding (Sennrich et al., 2016) or the word-piece model (Wu et al.", "startOffset": 22, "endOffset": 45}, {"referenceID": 7, "context": "ing with a character-based convolutional/recurrent sub-network (Costa-juss\u00e0 and Fonollosa, 2016; Luong and Manning, 2016; Vylomova et al., 2016).", "startOffset": 63, "endOffset": 144}, {"referenceID": 21, "context": "ing with a character-based convolutional/recurrent sub-network (Costa-juss\u00e0 and Fonollosa, 2016; Luong and Manning, 2016; Vylomova et al., 2016).", "startOffset": 63, "endOffset": 144}, {"referenceID": 34, "context": "ing with a character-based convolutional/recurrent sub-network (Costa-juss\u00e0 and Fonollosa, 2016; Luong and Manning, 2016; Vylomova et al., 2016).", "startOffset": 63, "endOffset": 144}, {"referenceID": 15, "context": "Here we focus on a character CNN which has been used in language modeling and machine translation (Kim et al., 2015; Belinkov and Glass, 2016; Costa-juss\u00e0 and Fonollosa, 2016; Jozefowicz et al., 2016).", "startOffset": 98, "endOffset": 200}, {"referenceID": 3, "context": "Here we focus on a character CNN which has been used in language modeling and machine translation (Kim et al., 2015; Belinkov and Glass, 2016; Costa-juss\u00e0 and Fonollosa, 2016; Jozefowicz et al., 2016).", "startOffset": 98, "endOffset": 200}, {"referenceID": 7, "context": "Here we focus on a character CNN which has been used in language modeling and machine translation (Kim et al., 2015; Belinkov and Glass, 2016; Costa-juss\u00e0 and Fonollosa, 2016; Jozefowicz et al., 2016).", "startOffset": 98, "endOffset": 200}, {"referenceID": 11, "context": "Here we focus on a character CNN which has been used in language modeling and machine translation (Kim et al., 2015; Belinkov and Glass, 2016; Costa-juss\u00e0 and Fonollosa, 2016; Jozefowicz et al., 2016).", "startOffset": 98, "endOffset": 200}], "year": 2017, "abstractText": "Neural machine translation (MT) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture. However, little is known about what these models learn about source and target languages during the training process. In this work, we analyze the representations learned by neural MT models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks. We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects in the neural MT system and its ability to capture word structure.", "creator": "LaTeX with hyperref package"}}}