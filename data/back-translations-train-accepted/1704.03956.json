{"id": "1704.03956", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2017", "title": "Incremental Skip-gram Model with Negative Sampling", "abstract": "This paper explores an incremental training strategy for the skip-gram model with negative sampling (SGNS) from both empirical and theoretical perspectives. Existing methods of neural word embeddings, including SNGS, are multi-pass algorithms and thus cannot perform incremental model update. To address this problem, we present a simple incremental extension of SNGS and provide a thorough theoretical analysis to demonstrate its validity. Empirical experiments demonstrated the correctness of the theoretical analysis as well as the practical usefulness of the incremental algorithm.", "histories": [["v1", "Thu, 13 Apr 2017 00:36:33 GMT  (167kb,D)", "http://arxiv.org/abs/1704.03956v1", null], ["v2", "Sat, 15 Apr 2017 07:15:00 GMT  (167kb,D)", "http://arxiv.org/abs/1704.03956v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["nobuhiro kaji", "hayato kobayashi"], "accepted": true, "id": "1704.03956"}, "pdf": {"name": "1704.03956.pdf", "metadata": {"source": "CRF", "title": "Incremental Skip-gram Model with Negative Sampling", "authors": ["Nobuhiro Kaji", "Hayato Kobayashi"], "emails": ["nkaji@yahoo-corp.jp", "hakobaya@yahoo-corp.jp"], "sections": [{"heading": "1 Introduction", "text": "For example, negative sampling (Mikolov et al., 2013b) is necessary to pre-calculate the sound distribution from all training data before performing Stochastic Gradient Descent (SGD). Therefore, it must go through training data at least twice. The fact that these incremental soft-max (Mikolov et al., 2013b) must determine the tree structure and GloVe (Pennington et al., 2014) must count co-occurrences frequencies before performing SGD. The fact that these existing methods are multipass algorithms means that they cannot perform incremental model updates when additional training data is provided. Instead, they must fundamentally remodel the old and new training data. However, re-training is obviously inefficient as they process all training data."}, {"heading": "2 SGNS Overview", "text": "As a preliminary example, this section provides a brief overview of the SGNS.Given a word sequence, w1, w2,.., wn, for training, the Skip-gram model aims to minimize the following target to learn word embedding: LSG = \u2212 1n n \u00b2 i = 1 \u00b2 | j | \u2264 c 6 = 0 log p (wi + j | wi), where wi is a target word and wi \u2212 j \u2212 is a context word within a size window. \u2212 p (wi + j | wi) represents the probability that wi + j appears within the neighbor of wi, and is defined as asp (wi + j | wi) = exp (twi \u00b7 cwi + j)."}, {"heading": "3 Incremental SGNS", "text": "This section deals with the incremental training of SGNS. The incremental training algorithm (Section 3.1), its efficient implementation (Section 3.2) and the computational complexity (Section 3.3) are discussed alternately."}, {"heading": "3.1 Algorithm", "text": "Algorithm 1 represents incremental SGNS that traverse the training data in a single pass to gradually update word embedding. Unlike the original SGNS, it does not pre-compute the sound distribution. Instead, it reads the training data word for word to gradually update the word rate distribution and sound distribution while performing SGD. Below, the original SGNS (c.f., Section 2) is referred to as batch SGNS to emphasize that the sound distribution is calculated in batch fashion. SGD's learning rate is adjusted by using AdaGrad (Duchi et al., 2011). Although the linear decay function is widely used for training batch SGNS (Mikolov, 2013), adaptive methods such as AdaGrad are more suitable for incremental training as the amount of training data is unknown in advance or can be increased unhindered."}, {"heading": "3.2 Efficient implementation", "text": "Although the incremental SGNS is conceptually simple, implementation problems are involved. 1In practice, algorithm 1 buffers a sequence of words wi \u2212 c,.., wi + c in each step (instead of a single word wi), since it provides access to the context words wi + j in line 7. This is not a practical problem, since the window size c is usually small and independent of the size of the training data. Algorithm 1 incremental SGNS 1: f (w) \u2190 0 for all w \u00b2 W 2: for i = 1,..., n do 3: f (wi) \u2190 f (wi) + 1 4: q (w) \u2190 f (w) \u03b1 w \u00b2 W f (w \u00b2) \u03b1 for all w \u00b2 W (w \u00b2) \u03b1 for all w \u00b2 W 5: for j = \u2212 c,.., \u2212 1, \u2212 c, \u2212 1,..., c do 3: f (wi) \u2190 f (wi) \u00b2 f (w) \u03b1 for all w \u00b2 W W 5: for all w = \u2212 c,.., cf, \u2212 1, cc, pull out the sampling Gj + cd: 1, sample Sgorithm w: 7: D: 1, vD for each."}, {"heading": "3.2.1 Dynamic vocabulary", "text": "As new words appear endlessly in the training data, the vocabulary can grow indefinitely and deplete a memory. We solve this problem by dynamically changing the vocabulary set. Misra-Gries Algorithm (Misra and Gries, 1982) is used to roughly keep track of the most common words during the training, and these words are used as a dynamic vocabulary set. This method makes it possible to explicitly limit the maximum vocabulary size while dynamically changing the vocabulary set."}, {"heading": "3.2.2 Adaptive unigram table", "text": "Another problem is how to efficiently generate negative samples. Since k negative samples per target-context pair must be generated by the noise distribution, the sampling speed has a significant impact on overall training efficiency. Let's first consider how negative samples are generated in the batch SGNS. In a popular implementation (Mikolov, 2013), a word array (in the sense of a unigram table) is constructed so that the number of a word w in it is proportional to q (w). See Table 1 for an example. Negative samples can be efficiently generated by sampling the table elements uniformly at random. It only takes O (1) time to generate a negative pattern. The above method assumes that the noise distribution is fixed and therefore cannot be used directly for incremental training. A simple solution is to reconstruct the unigram table whenever new training data is provided."}, {"heading": "3.3 Computational complexity", "text": "Both incremental and batch SGNS have the same space complexity, which is independent of the training data size n. Both algorithms require O (| W |) space to store the word embedding and the word rate and O (| T |) space to store the Uniram table. Both algorithms also have the same time complexity and are linearly dependent on the training data size n, O (n). Although incremental SGNS require additional time to update the dynamic vocabulary and the adaptive Uniram table, these costs are practically negligible, as shown in Section 5.3."}, {"heading": "4 Theoretical Analysis", "text": "Although extending batch to incremental SGNS is simple and intuitive, it is not immediately clear whether incremental SGNS can learn both word embedding and its batch counterpart. To answer this question, we will examine incremental SGNS from a theoretical point of view in this section. The analysis begins by examining the difference between the goals optimized by batch and incremental SGNS (Section 4.1), and then examines the probable properties of their difference to demonstrate the relationship between batch and incremental SGNS (Section 4.2 and 4.3)."}, {"heading": "4.1 Objective difference", "text": "As discussed in Section 2, Batch SGNS optimizes the following target: LB (\u03b8) = \u2212 1n n \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p = p \u00b2 p = p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p = p \u00b2 p p \u00b2 p (.., t \u2212 W \u2212 p \u2212 p p p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 p \u2212 \u2212 p \u2212 \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212"}, {"heading": "4.2 Unsmoothed case", "text": "Let's start with the consideration of the objective difference \u2206 L (\u03b8) in the unsmoothed case \u03b1 = 1.0.The technical difficulty in the analysis \u2206 L (\u03b8) is that it depends on the word sequence in the training data. We assume that the words in the training data are generated from a stationary distribution and that the probable property of \u2206 L (\u03b8) is investigated. In the following, we introduce some definitions and notations as preparation for the analyses.Definition 1. Let Xi, w be a random variable representing \u03b4wi, w. It takes 1 if the i-th word in the training data is w-W and 0-others.Remember that we assume that the words in the training data are generated from a stationary distribution. This assumption means that the expectation and (co) variance of Xi, w are not of the index i. In the following, they are each given as E [Xi, w] = \u00b5V and [Xi, w =]."}, {"heading": "4.2.1 Convergence of the first and second order moments of \u2206L(\u03b8)", "text": "Theorem 1st Theorem 1st Theorem 2nd Theorem 1st Theorem 2nd Theorem 3rd Theorem 3rd Theorem 3rd Theorem 3rd Theorem 4th Theorem 4th Theorem 5th Theorem 5th Theorem 6th Theorem 6th Theorem 6th Theorem 6th Theorem 6th Theorem 6th Theorem 6th Theorem 7th Theorem 6th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 7th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 8th Theorem 10th 10th 10th 10th 10th Theorem 10th 10th 10th 10th 10th Theorem 10th 10th 10th 10th 10th 10th Theorem 10th 10th 10th 10th 10th 10th 10th Theorem 9th Theorem 9th Theorem 9th Theorem 9th Theorem 9th Theorem 9th Theorem 9th Theorem 9th Theorem 9th Theorem 9th Theorem 9th Theorem 9th Theorem 9th Theorem 9th Theorem 9"}, {"heading": "4.2.2 Main result", "text": "The above theorems reveal the relationship between the optimal solutions of the two goals as set out in the next lemma.Lemma 4. Let both theorems be the optimal solutions of LB (\u03b8) and LI (\u03b8)."}, {"heading": "4.3 Smoothed case", "text": "Next, we examine the smoothed case (0 < \u03b1 < 1). In this case, the sound distribution can be represented by using the values in the smoothed case: qi (w), fi (w), fi (w), fi (w), fi (w). Definition 3. Let Zi, w be a random variable representing qi (w). Then asZi, w = gw (Yi, 1, Yi, 2,.., Yi, | W |), where gw (x1, x2,.., x | W |), w = qi (w) is given. Da Zi, w = gw (Yi, Yi, 2,."}, {"heading": "4.4 Mini-batch SGNS", "text": "The same analysis result can also be obtained for the minimum batch SGNS. Theorems 2 and 3 can also be proved in the minimum batch case (see Appendix C for proof). The other part of the analysis remains the same."}, {"heading": "5 Experiments", "text": "Three experiments were performed to investigate the correctness of the theoretical analysis (Section 5.1) and the practical benefits of incremental SGNS (Section 5.2 and 5.3)."}, {"heading": "5.1 Validation of theorems", "text": "An empirical experiment was carried out to validate the result of the theoretical analysis. As it is difficult to directly assess the main result in Section 4.2.2, the theorems in Section 4.2.1, from which the main result can be easily derived, were examined. In particular, the first and second order moments of \u2206 L (\u03b8) were calculated on data sets of increasing size to empirically examine the convergence factory.Data sets of different sizes were constructed from the English Gigaword corpus (Napoles et al., 2012).Data sets composed of n words were constructed by random sampling of sentences from the Gigaword corpus.The value of n was varied over {103, 104, 105, 106, 107}. 10,000 different data sets were created for each size n to calculate the moments of first and second order x. Figure 1 (top left) shows first order protocol plots."}, {"heading": "5.2 Quality of word embeddings", "text": "The next experiment examines the quality of the word embedding learned by incremental SGNS by comparing it with the batch counterparts. The Gigaword corpus was used for training incrementally, using both our own implementation of the batch SGNS and WORD2VEC (Mikolov et al., 2013c) (incremental and w2v) for comparison. Training configurations of the three methods were set as far as possible, although it is impossible to do this perfectly. For example, incremental SGNS (referred to as incremental) uses the dynamic vocabulary (c.f., Section 3.2.1) and thus we set the maximum vocabulary sizem to control the word size. On the other hand, we set a frequency threshold to determine the vocabulary size of w2v. We set m = 240k for incremental, while the frequency threshold is set to 100 for w2v."}, {"heading": "5.3 Update time", "text": "The last experiment explores how much time incremental SGNS can save by avoiding retraining rates when the word is embedded. In this experiment, the incremental was first trained using the initial training data of size 5 n1 and then updated to the new training data of size n2 to measure the refresh time. For comparison, batch and w2v were retrained using the combination of the initial and new training data. We fixed n1 = 107 and varied n2 over {1 \u00d7 106, 2 \u00d7 106,..., 5 \u00d7 106}. Figure 2 (c) compares the refresh time of the three methods over different values of n2. We see that incremental significantly reduces the refresh time. It achieves a 10 and 7.3-fold acceleration compared to batch and w2v (when n2 = 106). This represents the advantage of the incremental algorithm, as well as the time efficiency of the dynamics of the number of the vocabulary being unpassable to the different dimensions, while we determine the number of the vocabulary is unpassable."}, {"heading": "6 Related Work", "text": "Distribution methods typically begin by constructing a word-context matrix and then applying dimension reduction techniques such as SVD to obtain high-quality word meaning representations. Although some investigated an incremental updating of the word-context matrix (Yin et al., 2015; Goyal and Daume III, 2011), they did not examine the reduced representations. On the other hand, neural word embedding has recently become popular as an alternative. However, most previous studies have not examined incremental strategies (Mikolov et al., 2013a, b; Pennington et al., 2014). More recently, Peng et al. (2017) proposed an incremental learning method of neural words as an alternative. As hierarchical soft-max and negative sampling have different benefits (Peng et al., 2017), the incremental SNS method is a soft-max learning method of NS."}, {"heading": "7 Conclusion and Future Work", "text": "This paper suggested incremental SGNS and provided in-depth theoretical analysis to prove their validity. We also conducted experiments to prove their effectiveness empirically. Although incremental model updating is often required in practical machine learning applications, little attention has been paid to the gradual embedding of words so far. We believe that incremental SGNS successfully addresses this situation and serves as a useful tool for practitioners. Success of this work suggests several research directions that need to be explored in the future, one possibility being to extend other embedding methods such as GloVe (Pennington et al., 2014) to incremental algorithms. Such studies would further expand the potential of word embedding methods."}, {"heading": "A Note on Adaptive Unigram Table", "text": "Algorithm 4 illustrates the efficient implementation of the adaptive uniform table (c.f., Section 3.2.2). In lines 8 and 10 F and \u03c4Fz are not always integers and are therefore likely to be converted to integers, as explained in the treatise. The time complexity of the algorithm 4 is O (1) per update in the case of \u03b1 = 1.0. If the update (line 8) takes O (1), since we always have F = 1. If we have F = 1, we have such an update. This means that the update (line 10-13) O (1) takes time. Even if \u03b1 6 = 1.0, the value of z becomes sufficiently large in practice, and thus the update becomes efficient, as shown in the experiment. Algorithm 4: Adaptive uniform table 1: f (w) \u2190 0 for all W 2: < z \u2190 0 for i = 1,."}, {"heading": "B Complete Proofs", "text": "This appendix returns complete proofs for theorems 1, 3, and 5.B.1 proof for theorems 1Proof. The moment of the first order of \"L\" can be rewritten: \"W\" (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (W \"(W) (\") (W \"(W) (W) (\" (W) (W) (\") (W\" (W) (\") (W\" (W \") (\") (W \"(\") (W \") (W\" (\") (W\") (\") (\" W \"(\") \"(W\") \"(\") \"(W\" (\")\" (W \"(\") \"(\") \"(W\" (\")\" (\")\" (W \"(\") \"(\") (\")\" (W \"(\") \"(\") \"(W\" (\")\" (\")\" (W \"(\" (\")\" (\")\" (W \")\" (W \"(\") \"(W\") \"(\" (W \"(\") \"(W\") \"(W\" (\")\" (W \"(\") (W \") (W\") (W \"(\") (W \"(\" (\") (W\") (\"(W\") (\"(W) (\" (W) (W) (\"(\") (\") (W) (\" (W) (\"(\" (W) (W) (\"(W) (\" (W) (\"(\") (\") (W) (\" (\") (W) (\" (\"(W) (\" (\"("}, {"heading": "C Theoretical Analysis of Mini-batch SGNS", "text": "This appendix shows that theorems 2 and 3 also apply to the mini-batch SGNS = 11 x W = 11 x W, i.e., the first and second order moments of L (\u03b8) are processed in the order O (log (n) n). Here, we are examining the mini-batch setting in which M words, as opposed to a single word, are processed at a specific time. Definition 4. Let Y (M) i \u2212 n, w be a random variable representing Qi (w) if \u03b1 = 1.0 and the mini-batch size M. Then asY (M) i, w = Yb (i, M), w = Yb (i), w = Yb (M), w = Yb (M), w \u2212 n \u2212 M e \u00b7 M. Note that we always have Y (M) n, w = Yn, w and i \u2264 b (i, M).We first examine the first moment of L (zi) by making a similar step as proof of theorem 1. The first moment of L \u00b7 M = M \u00b7 W n M, M always M (Xi) w i n n)."}, {"heading": "D Theoretical Analysis in Smoothed Case", "text": "In this appendix, the convergence of the moment of the first and second order of \"L\" (\"L\") (\"L\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (W \") (W\") (W \") (W\") (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (\"(W\") (W \") (W\" (W \") (W\") (W \") (W\" (W \") (W\") (W \") (W\" (W \") (W\") (W \") (W\" (W \") (W\") (W \") (W\" (W \") (W\") (W \"(W\") (W \") (W\") (W \"(W\") (W \") (W\" (W \") (\" (W \")\" (W \"(\") \"(W\") \"(W\" (\")\") (W \") (\" (\")\") (W \"(W\" (W \"(\") \"(\" (\")\" (\"W\" (\")\") \"(W\") \"(\" (\") (\") (W \") (\" W \"(\") (\"W\") \"(\" (\")\") (W \"(\") (W \"(\" (\") (\") (W \") (\" (\")\") \"(\") (W \"(W\") (\"(\" (\") (\") (W \"(\") (\") (\" (\") (\")"}, {"heading": "E Experimental Configurations", "text": "This appendix describes the experimental configurations not described in the paper.E.1 Verification of Theorems The vocabulary specified in the Gigaword corpus has been reduced to 1000 by converting rare words into the same special characters because it is expensive to evaluate the expectation terms for a large vocabulary set in.L (\u03b8).The parameter value has been set to 100-dimensional vectors, each of which is drawn equally from [\u2212 0.5, 0.5] randomly. In preliminary experiments we confirmed that the result is insensitive to the choice of the parameter value. Note that the same parameter value is used for all n. We set c and k as c = 5 and k = 5. The mean \u00b5w and covariances are obtained. w, v are needed to calculate the theoretical value of the moment of first order. They were used as the maximum probability estimate for all n."}], "references": [{"title": "Multimodal distributional semantics", "author": ["E. Bruni", "N.K. Tran", "M. Baroni."], "venue": "Journal of Artificial Intelligence Research 49:1\u201349.", "citeRegEx": "Bruni et al\\.,? 2013", "shortCiteRegEx": "Bruni et al\\.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Weighted random sampling over data streams", "author": ["Pavlos S. Efraimidis."], "venue": "ArXiv:1012.0256.", "citeRegEx": "Efraimidis.,? 2015", "shortCiteRegEx": "Efraimidis.", "year": 2015}, {"title": "Approximate scalable bounded space sketch for large data nlp", "author": ["Amit Goyal", "Hal Daume III."], "venue": "Proceedings of EMNLP. pages 250\u2013261. http://www.aclweb.org/anthology/D11-1023.", "citeRegEx": "Goyal and III.,? 2011", "shortCiteRegEx": "Goyal and III.", "year": 2011}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics 41:665\u2013695. http://aclweb.org/anthology/J/J15/J15-4004.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics 3:211\u2013225. https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Workshop at ICLR.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in NIPS. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-Tau Yih", "Geoffrey Zweig."], "venue": "Proceedings of NAACL. pages 746\u2013751. http://www.aclweb.org/anthology/N13-1090.", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Finding repeated elements", "author": ["Jayadev Misra", "David Gries."], "venue": "Science of Computer Programming 2(2):143\u2013152.", "citeRegEx": "Misra and Gries.,? 1982", "shortCiteRegEx": "Misra and Gries.", "year": 1982}, {"title": "Annotated english gigaword ldc2012t21", "author": ["Courtney Napoles", "Matthew Gormley", "Benjamin Van Durme"], "venue": null, "citeRegEx": "Napoles et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Napoles et al\\.", "year": 2012}, {"title": "Incrementally learning the hierarchical softmax function for neural language models", "author": ["Hao Peng", "Jianxin Li", "Yangqiu Song", "Yaopeng Liu."], "venue": "Proceedings of AAAI (to appear).", "citeRegEx": "Peng et al\\.,? 2017", "shortCiteRegEx": "Peng et al\\.", "year": 2017}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of EMNLP. pages 1532\u20131543. http://www.aclweb.org/anthology/D141162.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Software framework for topic modelling with large corpora", "author": ["Radim \u0158eh\u016f\u0159ek", "Petr Sojka."], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. pages 45\u201350.", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka.,? 2010", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka.", "year": 2010}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel."], "venue": "Journal of Artificial Intelligence Research 37:141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Random sampling with a reservoir", "author": ["Jeffrey S. Vitter."], "venue": "ACM Transactions on Mathematical Software 11:37\u201357.", "citeRegEx": "Vitter.,? 1985", "shortCiteRegEx": "Vitter.", "year": 1985}, {"title": "Online updating of word representations for part-of-speech tagging", "author": ["Wenpeng Yin", "Tobias Schnabel", "Hinrich Sch\u00fctze."], "venue": "Proceedings of EMNLP. pages 1329\u20131334. http://aclweb.org/anthology/D15-1155.", "citeRegEx": "Yin et al\\.,? 2015", "shortCiteRegEx": "Yin et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "For example, negative sampling (Mikolov et al., 2013b) needs to precompute the noise distribution from the entire training data before performing Stochastic Gradient Descent (SGD).", "startOffset": 31, "endOffset": 54}, {"referenceID": 7, "context": "Similarly, hierarchical soft-max (Mikolov et al., 2013b) has to determine the tree structure and GloVe (Pennington et al.", "startOffset": 33, "endOffset": 56}, {"referenceID": 12, "context": ", 2013b) has to determine the tree structure and GloVe (Pennington et al., 2014) has to count co-occurrence frequencies before performing SGD.", "startOffset": 55, "endOffset": 80}, {"referenceID": 11, "context": "Another situation is learning word embeddings from ever-evolving data such as news articles and microbologs (Peng et al., 2017).", "startOffset": 108, "endOffset": 127}, {"referenceID": 7, "context": "This paper investigates an incremental training method of word embeddings with a focus on the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) for its popularity.", "startOffset": 140, "endOffset": 163}, {"referenceID": 6, "context": "Since it is too expensive to optimize the above objective, Mikolov et al. (2013b) proposed negative sampling to speed up skip-gram training.", "startOffset": 59, "endOffset": 82}, {"referenceID": 1, "context": "The learning rate for SGD is adjusted by using AdaGrad (Duchi et al., 2011).", "startOffset": 55, "endOffset": 75}, {"referenceID": 9, "context": "The Misra-Gries algorithm (Misra and Gries, 1982) is used to approximately keep track of top-m frequent words during training, and those words are used as the dynamic vocabulary set.", "startOffset": 26, "endOffset": 49}, {"referenceID": 15, "context": "2 We propose a reservoir-based algorithm for efficiently updating the unigram table (Vitter, 1985; Efraimidis, 2015) (Algorithm 3).", "startOffset": 84, "endOffset": 116}, {"referenceID": 2, "context": "2 We propose a reservoir-based algorithm for efficiently updating the unigram table (Vitter, 1985; Efraimidis, 2015) (Algorithm 3).", "startOffset": 84, "endOffset": 116}, {"referenceID": 15, "context": "See (Vitter, 1985; Efraimidis, 2015) for reference.", "startOffset": 4, "endOffset": 36}, {"referenceID": 2, "context": "See (Vitter, 1985; Efraimidis, 2015) for reference.", "startOffset": 4, "endOffset": 36}, {"referenceID": 10, "context": "Datasets of various sizes were constructed from the English Gigaword corpus (Napoles et al., 2012).", "startOffset": 76, "endOffset": 98}, {"referenceID": 8, "context": "For the comparison, both our own implementation of batch SGNS as well as WORD2VEC (Mikolov et al., 2013c) were used (denoted as batch and w2v).", "startOffset": 82, "endOffset": 105}, {"referenceID": 5, "context": "The learned word embeddings were assessed on five benchmark datasets commonly used in the literature (Levy et al., 2015): WordSim353 (Agirre et al.", "startOffset": 101, "endOffset": 120}, {"referenceID": 0, "context": ", 2009), MEN (Bruni et al., 2013), SimLex-999 (Hill et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 4, "context": ", 2013), SimLex-999 (Hill et al., 2015), the MSR analogy dataset (Mikolov et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 8, "context": ", 2015), the MSR analogy dataset (Mikolov et al., 2013c), the Google analogy dataset (Mikolov et al.", "startOffset": 33, "endOffset": 56}, {"referenceID": 6, "context": ", 2013c), the Google analogy dataset (Mikolov et al., 2013a).", "startOffset": 37, "endOffset": 60}, {"referenceID": 14, "context": "Word representations based on distributional semantics have been common (Turney and Pantel, 2010; Baroni and Lenci, 2010).", "startOffset": 72, "endOffset": 121}, {"referenceID": 16, "context": "Although some investigated incremental updating of the word-context matrix (Yin et al., 2015; Goyal and Daume III, 2011), they did not explore the reduced representations.", "startOffset": 75, "endOffset": 120}, {"referenceID": 12, "context": "However, most previous studies have not investigated incremental strategies (Mikolov et al., 2013a,b; Pennington et al., 2014).", "startOffset": 76, "endOffset": 126}, {"referenceID": 11, "context": "Because hierarchical soft-max and negative sampling have different advantages (Peng et al., 2017), the incremental SGNS and their method are complementary to each other.", "startOffset": 78, "endOffset": 97}, {"referenceID": 13, "context": "GENSIM (\u0158eh\u016f\u0159ek and Sojka, 2010) also offers SGNS training.", "startOffset": 7, "endOffset": 32}, {"referenceID": 6, "context": "However, most previous studies have not investigated incremental strategies (Mikolov et al., 2013a,b; Pennington et al., 2014). Very recently, Peng et al. (2017) proposed an incremental learning method of hierarchical softmax.", "startOffset": 77, "endOffset": 162}], "year": 2017, "abstractText": "This paper explores an incremental training strategy for the skip-gram model with negative sampling (SGNS) from both empirical and theoretical perspectives. Existing methods of neural word embeddings, including SNGS, are multi-pass algorithms and thus cannot perform incremental model update. To address this problem, we present a simple incremental extension of SNGS and provide a thorough theoretical analysis to demonstrate its validity. Empirical experiments demonstrated the correctness of the theoretical analysis as well as the practical usefulness of the incremental algorithm.", "creator": "TeX"}}}