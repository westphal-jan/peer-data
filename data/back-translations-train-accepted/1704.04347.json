{"id": "1704.04347", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Exploiting Cross-Sentence Context for Neural Machine Translation", "abstract": "In translation, considering the document as a whole allows certain ambiguities and inconsistencies to be resolved. In this paper, we propose a cross-sentence context-aware approach and investigate the influence of historical contextual information on the performance of neural machine translation (NMT). First, this history is summarized in a hierarchical way. We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states. Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points.", "histories": [["v1", "Fri, 14 Apr 2017 04:56:36 GMT  (166kb,D)", "https://arxiv.org/abs/1704.04347v1", null], ["v2", "Mon, 17 Apr 2017 00:21:19 GMT  (166kb,D)", "http://arxiv.org/abs/1704.04347v2", null], ["v3", "Sun, 23 Jul 2017 05:19:42 GMT  (170kb,D)", "http://arxiv.org/abs/1704.04347v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["longyue wang", "zhaopeng tu", "andy way", "qun liu"], "accepted": true, "id": "1704.04347"}, "pdf": {"name": "1704.04347.pdf", "metadata": {"source": "CRF", "title": "Exploiting Cross-Sentence Context for Neural Machine Translation", "authors": ["Longyue Wang", "Zhaopeng Tu", "Andy Way", "Qun Liu"], "emails": ["qun.liu}@adaptcentre.ie", "tuzhaopeng@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, we will be able to go in search of a solution that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us to put ourselves in the position we are in."}, {"heading": "2 Approach", "text": "If a source sentence xm is to be translated, we consider its K sentences in the same document as a cross-sentence context C = {xm \u2212 K,..., xm \u2212 1}. In this section, we first model C, which is then integrated into the NMT."}, {"heading": "2.1 Summarizing Global Context", "text": "As shown in Figure 1, we summarize the representation of C in a hierarchical way: RNN sentence For a sentence xk in C, the sentence RNN reads the corresponding words {x1, k,..., xn, k,..., xN, k} sequentially and updates its hidden state: hn, k = f (hn \u2212 1, k, xn, k) (1), where f (\u00b7) is an activation function and hn, k is the hidden state at the time n. The last state hN, k stores job-sensitive information about all words in xk that are used to represent the summary of the whole sentence, i.e. Sk, hN, k. After editing a sentence in C, we can get all the sentence representations that are stored in the document RNN.Document RNN. It takes as input the sequence of the above sentence representations {S1,..., hk,..., SK} and represents the hidden state we consider in the state (the plugged state)."}, {"heading": "2.2 Integrating Global Context into NMT", "text": "We propose three strategies to integrate the history representation D in NMT: Initialization We used D to initialize either NMT encoder, NMT decoder, or both. For encoders, we use D as the initialization state and not as an all-zero state as in the standard NMT (Bahdanau et al., 2015). For decoders, we write the calculation of the initial hidden state s0 = tanh (WshN + WDD), where hN is the last hidden state in encoder and {Ws, WD} are the corresponding weight metrics."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Setup", "text": "We conducted experiments with Chinese-English translation tasks. As the document information is necessary to select the previous sentences, we collect all LDC corpora that contain document boundaries. The training corpus consists of 1M sentence pairs extracted from the LDC corpora3 with 25.4M Chinese words and 32.4M English words. As test sets, we chose the NIST05 (MT05) and the NIST06 (MT06) and NIST08 (MT08), using the case-insensitive BLEU score (Papineni et al., 2002) and the drawing test (Collins et al., 2005) to calculate statistical significance. We implemented our approach on an open source-based NMT model, Nematus4 (Sennrich et al., 2016; Sennrich et al., 2017)."}, {"heading": "3.2 Results", "text": "Table 1 shows the translation performance in terms of the BLEU score. In all cases, the proposed approaches significantly exceed the baseline by 2.3 BLEU points, indicating that NEMATUS is a strong NMT base system. It is in line with the results in (Tu et al., 2017b) (i.e. 26.93 vs. 29.41) on training corpses of similar size. The initialization strategy (lines 3-5) Initenc and Initdec improve translation performance by + 1.0 and + 1.3 BLEU points respectively, demonstrating the effectiveness of warm-up with a cross-sectional context. Combining these approaches further improves US context strategies (lines 6-7)."}, {"heading": "3.3 Analysis", "text": "First, we examine the extent to which the proposed system remedies the mistranslated errors. We randomly select 15 documents (about 60 sentences) from the test sets. As shown in Table 2, we count how many related errors are made: i) by NMT (Total) and ii) by our method (Fixed); and iii) newly generated (New). As for ambiguity, while we found that 38 words / sentences were translated into wrong equivalents, 76% of them are corrected by our model. Likewise, we solved 75% of inconsistency errors, including lexical, tense and indefinite cases (definite or indefinite articles). However, we also observe that our system brings relative 21% new errors. Case study Table 3 shows an example. The word... \"corrupt officials\" is mistranslated by the base system. Using the similar word... \""}, {"heading": "4 Related Work", "text": "While our approach is based on hierarchically recurring encoder decoders (HRED) (Sordoni et al., 2015), there are several key differences that reflect how we generalized the original model. Sordoni et al. (2015) use HRED to summarize a single representation of both the current and the previous sentence, which is limited to (1) the encoder decoder frame without an attention model. (2) The representation can only be used to initialize the decoder. In contrast, we use HRED to summarize the previous sentences alone, providing additional context for NMT. Our approach is more flexible on (1) it applies to all encoder decoder frames (e.g. with attention)."}, {"heading": "5 Conclusion and Future Work", "text": "We proposed two complementary approaches to integrating transverse contexts: 1) a warm-up of the encoder and decoder with global context representation and 2) the transverse context serves as an additional source of information for updating decoder states in which an established context gate plays an important role. We have shown quantitatively and qualitatively that the model presented clearly exceeds a strong attention-based NMT base system. We publish the code for these experiments at https: / / www.github.com / tuzhaopeng / LC-NMT. Our models benefit from larger contexts and may be further improved by other information at the document level, such as discourse relationships."}, {"heading": "Acknowledgments", "text": "This work is supported by the ADAPT project of the Science Foundation of Ireland (SFI) (grant no.: 13 / RC / 2106). The authors would also like to thank the anonymous reviewers for many helpful comments, especially Henry Elder for his generous help in proofreading this manuscript."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 3rd International Conference on Learning Representations. San Diego, USA, pages 1\u201315.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Incorporating global visual features into attentionbased neural machine translation", "author": ["Iacer Calixto", "Qun Liu", "Nick Campbell."], "venue": "arXiv preprint arXiv:1701.06521 .", "citeRegEx": "Calixto et al\\.,? 2017", "shortCiteRegEx": "Calixto et al\\.", "year": 2017}, {"title": "The trouble with smt consistency", "author": ["Marine Carpuat", "Michel Simard."], "venue": "Proceedings of the 7th Workshop on Statistical Machine Translation. Montreal, Quebec, Canada, pages 442\u2013449.", "citeRegEx": "Carpuat and Simard.,? 2012", "shortCiteRegEx": "Carpuat and Simard.", "year": 2012}, {"title": "Clause restructuring for statistical machine translation", "author": ["Michael Collins", "Philipp Koehn", "Ivona Kucerova."], "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics. Ann Arbor, Michigan, pages 531\u2013540.", "citeRegEx": "Collins et al\\.,? 2005", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "Multi-task learning for multiple language translation", "author": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."], "venue": "Proceedings of the 53rd Annual Meeting of the Assocaition for Computational Linguistics and the 7th International Joint", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Does neural machine translation benefit from larger context", "author": ["Sebastien Jean", "Stanislas Lauly", "Orhan Firat", "Kyunghyun Cho"], "venue": null, "citeRegEx": "Jean et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2017}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Seattle, Washington, USA, pages", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "D. Christopher Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal, pages", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Philadelphia, Pennsylvania,", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Nematus: a toolkit for neural machine", "author": ["Rico Sennrich", "Orhan Firat", "Kyunghyun Cho", "Alexandra Birch", "Barry Haddow", "Julian Hitschler", "Marcin Junczys-Dowmunt", "Samuel L\u00e4ubli", "Antonio Valerio Miceli Barone", "Jozef Mokry"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2017}, {"title": "Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers, Berlin, Germany, chapter Linguistic Input Features", "author": ["Rico Sennrich", "Barry Haddow"], "venue": "Improve Neural Machine Translation,", "citeRegEx": "Sennrich and Haddow.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2016}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V. Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelli-", "citeRegEx": "Serban et al\\.,? 2016", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A hierarchical recurrent encoderdecoder for generative context-aware query suggestion", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob Grue Simonsen", "JianYun Nie."], "venue": "Proceedings of the 24th ACM International", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Proceedings of the 2014 Neural Information Processing Systems. Montreal, Canada, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Context gates for neural machine translation", "author": ["Zhaopeng Tu", "Yang Liu", "Zhengdong Lu", "Xiaohua Liu", "Hang Li."], "venue": "Transactions of the Association for Computational Linguistics .", "citeRegEx": "Tu et al\\.,? 2017a", "shortCiteRegEx": "Tu et al\\.", "year": 2017}, {"title": "Neural machine translation with reconstruction", "author": ["Zhaopeng Tu", "Yang Liu", "Lifeng Shang", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of the 31th AAAI Conference on Artificial Intelligence (AAAI17). San Francisco, California, USA, pages 3097\u2013", "citeRegEx": "Tu et al\\.,? 2017b", "shortCiteRegEx": "Tu et al\\.", "year": 2017}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of the 54th annual meeting of the Association for Computational Linguistics. Berlin, Germany, pages 76\u201385.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "Proceedings of the International Conference on Machine Learning, Deep Learning Workshop. pages 1\u20138.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Document-level consistency verification in machine translation", "author": ["Tong Xiao", "Jingbo Zhu", "Shujie Yao", "Hao Zhang."], "venue": "Machine Translation Summit. Xiamen, China, volume 13, pages 131\u2013138.", "citeRegEx": "Xiao et al\\.,? 2011", "shortCiteRegEx": "Xiao et al\\.", "year": 2011}, {"title": "Multi-source neural translation", "author": ["Barret Zoph", "Kevin Knight."], "venue": "arXiv preprint arXiv:1601.00710 .", "citeRegEx": "Zoph and Knight.,? 2016", "shortCiteRegEx": "Zoph and Knight.", "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016).", "startOffset": 76, "endOffset": 172}, {"referenceID": 13, "context": "Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016).", "startOffset": 76, "endOffset": 172}, {"referenceID": 0, "context": "Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016).", "startOffset": 76, "endOffset": 172}, {"referenceID": 16, "context": "Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016).", "startOffset": 76, "endOffset": 172}, {"referenceID": 7, "context": "Using the encoder-decoder framework as well as gating and attention techniques, it has been shown that the performance of NMT has surpassed the performance of traditional statistical machine translation (SMT) on various language pairs (Luong et al., 2015).", "startOffset": 235, "endOffset": 255}, {"referenceID": 18, "context": "Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document (Xiao et al., 2011; Carpuat and Simard, 2012).", "startOffset": 153, "endOffset": 198}, {"referenceID": 2, "context": "Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document (Xiao et al., 2011; Carpuat and Simard, 2012).", "startOffset": 153, "endOffset": 198}, {"referenceID": 12, "context": "The cross-sentence context, or global context, has proven helpful to better capture the meaning or intention in sequential tasks such as query suggestion (Sordoni et al., 2015) and dialogue modeling (Vinyals and Le, 2015; Serban et al.", "startOffset": 154, "endOffset": 176}, {"referenceID": 17, "context": ", 2015) and dialogue modeling (Vinyals and Le, 2015; Serban et al., 2016).", "startOffset": 30, "endOffset": 73}, {"referenceID": 11, "context": ", 2015) and dialogue modeling (Vinyals and Le, 2015; Serban et al., 2016).", "startOffset": 30, "endOffset": 73}, {"referenceID": 12, "context": "Specifically, we employ a hierarchy of Recurrent Neural Networks (RNNs) to summarize the cross-sentence context from source-side previous sentences, which deploys an additional documentlevel RNN on top of the sentence-level RNN encoder (Sordoni et al., 2015).", "startOffset": 236, "endOffset": 258}, {"referenceID": 5, "context": "To the best of our knowledge, our work and Jean et al. (2017) are two independently early attempts to model crosssentence context for NMT.", "startOffset": 43, "endOffset": 62}, {"referenceID": 0, "context": "For encoder, we useD as the initialization state rather than all-zero states as in the standard NMT (Bahdanau et al., 2015).", "startOffset": 100, "endOffset": 123}, {"referenceID": 14, "context": "To this end, we extend auxiliary context strategy by introducing a context gate (Tu et al., 2017a) to dynamically control the amount of information flowing from the auxiliary global context at each decoding step, as shown in Figure 2 (c).", "startOffset": 80, "endOffset": 98}, {"referenceID": 8, "context": "We used case-insensitive BLEU score (Papineni et al., 2002) as our evaluation metric, and sign-test (Collins et al.", "startOffset": 36, "endOffset": 59}, {"referenceID": 3, "context": ", 2002) as our evaluation metric, and sign-test (Collins et al., 2005) for calculating statistical significance.", "startOffset": 48, "endOffset": 70}, {"referenceID": 10, "context": "We implemented our approach on top of an open source attention-based NMT model, Nematus4 (Sennrich and Haddow, 2016; Sennrich et al., 2017).", "startOffset": 89, "endOffset": 139}, {"referenceID": 9, "context": "We implemented our approach on top of an open source attention-based NMT model, Nematus4 (Sennrich and Haddow, 2016; Sennrich et al., 2017).", "startOffset": 89, "endOffset": 139}, {"referenceID": 15, "context": "It is consistent with the results in (Tu et al., 2017b) (i.", "startOffset": 37, "endOffset": 55}, {"referenceID": 12, "context": "While our approach is built on top of hierarchical recurrent encoder-decoder (HRED) (Sordoni et al., 2015), there are several key differences which reflect how we have generalized from the original model.", "startOffset": 84, "endOffset": 106}, {"referenceID": 12, "context": "While our approach is built on top of hierarchical recurrent encoder-decoder (HRED) (Sordoni et al., 2015), there are several key differences which reflect how we have generalized from the original model. Sordoni et al. (2015) use HRED to summarize a single representation from both the current and previous sentences, which limits itself to (1) it is only applicable to encoder-decoder framework without attention model, (2) the representation can only be used to initialize decoder.", "startOffset": 85, "endOffset": 227}, {"referenceID": 14, "context": "(2016) use Auxiliary Context mechanism for incorporating cross-sentence context, there are two main differences: 1) we have separate parameters to better control the effects of the cross- and intrasentence contexts, while they only have one parameter matrix to manage the single representation that encodes both contexts; 2) based on the intuition that not every target word generation requires equivalent cross-sentence context, we introduce a context gate (Tu et al., 2017a) to control the amount of information from it, while they don\u2019t.", "startOffset": 458, "endOffset": 476}, {"referenceID": 11, "context": "While both our approach and Serban et al. (2016) use Auxiliary Context mechanism for incorporating cross-sentence context, there are two main differences: 1) we have separate parameters to better control the effects of the cross- and intrasentence contexts, while they only have one parameter matrix to manage the single representation that encodes both contexts; 2) based on the intuition that not every target word generation requires equivalent cross-sentence context, we introduce a context gate (Tu et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 4, "context": "For example, Jean et al. (2017) use it to encode and select part of the previous source sentence for generating each target word.", "startOffset": 13, "endOffset": 32}, {"referenceID": 1, "context": "Calixto et al. (2017) utilize global image features extracted using a pre-trained convolutional neural network and incorporate them in NMT.", "startOffset": 0, "endOffset": 22}, {"referenceID": 1, "context": "Calixto et al. (2017) utilize global image features extracted using a pre-trained convolutional neural network and incorporate them in NMT. As additional attention leads to more computational cost, they can only incorporate limited information such as single preceding sentence in Jean et al. (2017). However, our architecture is free to this limitation, thus we use multiple preceding sentences (e.", "startOffset": 0, "endOffset": 300}, {"referenceID": 19, "context": "Our work is also related to multi-source (Zoph and Knight, 2016) and multi-target NMT (Dong et al.", "startOffset": 41, "endOffset": 64}, {"referenceID": 4, "context": "Our work is also related to multi-source (Zoph and Knight, 2016) and multi-target NMT (Dong et al., 2015), which incorporate additional source or target languages.", "startOffset": 86, "endOffset": 105}], "year": 2017, "abstractText": "In translation, considering the document as a whole can help to resolve ambiguities and inconsistencies. In this paper, we propose a cross-sentence context-aware approach and investigate the influence of historical contextual information on the performance of neural machine translation (NMT). First, this history is summarized in a hierarchical way. We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states. Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points.", "creator": "LaTeX with hyperref package"}}}