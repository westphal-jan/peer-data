{"id": "1704.04368", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Get To The Point: Summarization with Pointer-Generator Networks", "abstract": "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.", "histories": [["v1", "Fri, 14 Apr 2017 07:55:19 GMT  (124kb,D)", "http://arxiv.org/abs/1704.04368v1", "Accepted to ACL 2017"], ["v2", "Tue, 25 Apr 2017 05:47:50 GMT  (125kb,D)", "http://arxiv.org/abs/1704.04368v2", "Add METEOR evaluation results, add some citations, fix some equations (what are now equations 1, 8 and 11 were missing a bias term), fix url to pyrouge package, add acknowledgments"]], "COMMENTS": "Accepted to ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["abigail see", "peter j liu", "christopher d manning"], "accepted": true, "id": "1704.04368"}, "pdf": {"name": "1704.04368.pdf", "metadata": {"source": "CRF", "title": "Get To The Point: Summarization with Pointer-Generator Networks", "authors": ["Abigail See", "Peter J. Liu", "Christopher D. Manning"], "emails": ["abisee@stanford.edu", "peterjliu@google.com", "manning@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "There are two main approaches to abstraction: extractive and abstract. Extractive methods assemble abstracts exclusively from passages (usually whole sentences) taken directly from the source text, while abstract methods can generate new words and phrases that do not appear in the source text - as human-written abstractions usually do. The extractive approach is simpler because copying scraps of text from the source document ensures a basic level of grammaticality and accuracy. On the other hand, complex skills that are crucial for a high-quality summary, such as paraphrasing, generalizing, or incorporating knowledge from the real world, are only possible within an abstract framework (see Figure 5). Due to the difficulty of abstract summaries, which in 2016, the vast majority of previous work has been extractive."}, {"heading": "2 Our Models", "text": "In this section, we describe (1) our basic sequence-to-sequence model, (2) our pointer generator model, and (3) our coverage mechanism, which can be added to the first two models."}, {"heading": "2.1 Sequence-to-sequence attentional model", "text": "Our base model is similar to that of Nallapati et al. (2016) and is illustrated in Figure 2. The symbols of article wi are fed one by one into the encoder (a single-layer bidirectional LSTM), generating a sequence of encoder hidden states hi. At each step t, the decoder (a single-layer unidirectional LSTM) receives the word embedding the previous word (during training this is the previous word of the reference summary; at test date it is the previous word sent out by the decoder), and has the decoder state st. Attention distribution at is calculated as in Bahdanau et al. (2015): eti = v T tanh (Whhi + Wsst) (1) at = softmax (et) (2), where v, Wh and Ws are the final parameters. Attention distribution at is considered as prob-1Tensorlow code for the models."}, {"heading": "2.2 Pointer-generator network", "text": "Our pointer-generator network is a hybrid between our baseline and a pointer network (Vinyals et al., 2015), as it allows both the copying of words by pointing and the generation of words from a fixed vocabulary. In the pointer-generator model (shown in Figure 3), the attention distribution on and the context vector h \u0445 t are calculated as in Section 2.1. In addition, the probability of generating pgen-t is calculated from the context vector h-t, the decoder state st and the decoder input text xt: pgen = \u03c3 (W-h-h-h) h-t + W-sst + W-xxt) (6), where W-h-s and W-x are learnable parameters. Next, pgen is used as a soft switch to choose between generating a word from the vocabulary by sampling from the vocabulary or copying a word from the input sequence."}, {"heading": "2.3 Coverage mechanism", "text": "Repetition is a common problem for sequence models (Tu et al., 2016; Mi et al., 2016; Sankaran et al., 2016; Suzuki and Nagata, 2016) and is particularly pronounced in generating multi-sentence coverage (see Figure 1). We adapt the coverage model of Tu et al. (2016) to solve the problem. In our coverage model, we get a coverage vector ct that represents the sum of coverage losses over all previous coverage periods of the decoder: ct = loss of experience t \u2212 1t \u2032 = 0 at \u2032 (8) Intuitively, ct is a (unnormalised) distribution over the source text words that represents the degree of \"coverage\" these words have received so far through the attention mechanism. Note that c0 is a zero vector because in the first period none of the source document has been weighted."}, {"heading": "3 Related Work", "text": "In fact, we are able to be in a situation where we are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move. \""}, {"heading": "4 Dataset", "text": "We use the CNN / Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), which contains online news articles (an average of 781 tokens), paired with multi-sentence summaries (an average of 3.75 senses or 56 tokens). We used scripts from Nallapati et al. (2016) to obtain the same version of the data, which includes 287,226 training pairs, 13,368 validation pairs, and 11,490 test pairs. Both published dataset results (Nallapati et al., 2016, 2017) use the anonymized version of the data that has been pre-processed to replace each designated entity, such as the United Nations, with its own unique identifier for the sample pair, such as @ entity5. In contrast, we work directly with the original text (or non-anonymized version of the data), 2 which we believe is the most advantageous problem to resolve as it does not require pre-processing."}, {"heading": "5 Experiments", "text": "For all the experiments, our model has learned 256-dimensional hidden states and 128-dimensional word embedding. For the pointer-generator models, we use a vocabulary of 50k words for both source and target groups - so we have a larger vocabulary than Nallapati et al. \"s (2016) 150k source and 60k target vocabularies. For the base model, we also try to use a larger vocabulary size of 150k.S, with the addition of the pointer and coverage mechanisms introducing very few additional parameters into the network: for models with 50k vocabulary size, the base model has 21,499,600 parameters, the pointer generators add 1153 additional parameters, and the coverage adds 512 parameters. Unlike Nallapati et al parameters that introduce themselves."}, {"heading": "6 Results", "text": "This year, it has come to the point where there is only one person who is able to retaliate."}, {"heading": "7 Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Comparison with extractive systems", "text": "Table 1 shows that extractive systems tend to achieve higher ROUGE values than abstract ones, and that the lead-3 extractive baseline is extremely strong (even the best extractive system only beats it by a small margin). We offer two possible explanations for these observations. First, news articles tend to be structured with the most important information at the beginning; this partially explains the strength of the lead-3 baseline. In fact, we found that the use of only the first 400 tokens (about 20 sentences) of the article yielded significantly higher ROUGE values than the use of the first 800 tokens. Second, the nature of the task and the ROUGE metric make extractive approaches and the lead3 baseline hard to beat. \"The choice of content for the reference summaries is quite subjective - sometimes the sentences form a standalone summary; other times they simply represent a usable summary to further depict the UUleys.\" Given that the fact that on average, there are 39 selections of these summaries, there are equally valid or too many of these summaries in an average of 39 sets, Table 1 indicates that extractive systems tend to have higher ROUGE values than abstractive ones."}, {"heading": "7.2 How abstractive is our model?", "text": "We have shown that our pointer mechanism makes our abstract system more reliable by copying the actual details more often, but does the ease of copying make our system less abstract? Figure 6 shows that the sum marks of our final model contain a much lower rate of novel n-grams (i.e. those that do not appear in the article) than the reference sums. Although our final model copies whole articles, we observe that the other n-grams become more common - however, this statistic includes all miscopied words, UNK-tokens, and fabrications in addition to the good instances of abstraction."}, {"heading": "8 Conclusion", "text": "In this paper, we introduced a hybrid pointer generator architecture with coverage and showed that it reduces inaccuracies and repetitions. We applied our model to a new and challenging long-text dataset and significantly exceeded the abstract result of the current state of the art. Our model has many abstractive capabilities, but achieving a higher degree of abstraction remains an open research question."}, {"heading": "Supplementary Material", "text": "This appendix contains examples from the test set, with adjacent comparisons of the reference summaries and the summaries produced by our models. In each example: \u2022 Italic denotes words that fall out of the vocabulary \u2022 Red denotes factual errors in the summaries \u2022 Intensity of green shading denotes the value of the probability of generation pgen \u2022 Yellow shading intensity denotes the final value of the coverage vector at the end of the summary process of the final model."}, {"heading": "Pointer-Generator, With Coverage:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Pointer-Generator, No Coverage:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Baseline:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Reference Summary:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Pointer-Generator, With Coverage:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Pointer-Generator, No Coverage:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Baseline:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Reference Summary:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Pointer-Generator, With Coverage:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Pointer-Generator, No Coverage:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Baseline:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Reference Summary:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Pointer-Generator, With Coverage:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Pointer-Generator, No Coverage:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Baseline:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Reference Summary:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Pointer-Generator, With Coverage:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Pointer-Generator, No Coverage:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Baseline:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Reference Summary:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Pointer-Generator, With Coverage:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Pointer-Generator, No Coverage:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Baseline:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Reference Summary:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Pointer-Generator, With Coverage:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Pointer-Generator, No Coverage:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Baseline:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Reference Summary:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Pointer-Generator, With Coverage:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Pointer-Generator, No Coverage:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Baseline:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Reference Summary:", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "author": ["Sumit Chopra", "Michael Auli", "Alexander M Rush."], "venue": "North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Chopra et al\\.,? 2016", "shortCiteRegEx": "Chopra et al\\.", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research 12(Jul):2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor OK Li."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Pointing the unknown words", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Gulcehre et al\\.,? 2016", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Neural Information Processing Systems.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "North American Chapter of the Association for Computational Linguistics on Human Language Technology.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Looking for a few good metrics: Automatic summarization evaluation-how many samples are enough? In NACSIS/NII Test Collection for Information Retrieval (NTCIR) Workshop", "author": ["Chin-Yew Lin"], "venue": null, "citeRegEx": "Lin.,? \\Q2004\\E", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "From extractive to abstractive summarization: A journey", "author": ["Parth Mehta."], "venue": "ACL 2016 Student Research Workshop.", "citeRegEx": "Mehta.,? 2016", "shortCiteRegEx": "Mehta.", "year": 2016}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher."], "venue": "NIPS 2016 Workshop on Multi-class and Multi-label Learning in Extremely Large Label Spaces.", "citeRegEx": "Merity et al\\.,? 2016", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "Coverage embedding models for neural machine translation", "author": ["Haitao Mi", "Baskaran Sankaran", "Zhiguo Wang", "Abe Ittycheriah."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Mi et al\\.,? 2016", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "SummaRuNNer: A recurrent neural network based sequence model for extractive summarization of documents", "author": ["Ramesh Nallapati", "Feifei Zhai", "Bowen Zhou."], "venue": "Association for the Advancement of Artificial Intelligence.", "citeRegEx": "Nallapati et al\\.,? 2017", "shortCiteRegEx": "Nallapati et al\\.", "year": 2017}, {"title": "Abstractive text summarization using sequence-to-sequence RNNs and beyond", "author": ["Ramesh Nallapati", "Bowen Zhou", "Cicero dos Santos", "\u00c7aglar Gul\u00e7ehre", "Bing Xiang."], "venue": "Computational Natural Language Learning.", "citeRegEx": "Nallapati et al\\.,? 2016", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Automatic text summarization: Past, present and future", "author": ["Horacio Saggion", "Thierry Poibeau."], "venue": "Multi-source, Multilingual Information Extraction and Summarization, Springer, pages 3\u201321.", "citeRegEx": "Saggion and Poibeau.,? 2013", "shortCiteRegEx": "Saggion and Poibeau.", "year": 2013}, {"title": "Temporal attention model for neural machine translation", "author": ["Baskaran Sankaran", "Haitao Mi", "Yaser Al-Onaizan", "Abe Ittycheriah."], "venue": "arXiv preprint arXiv:1608.02927 .", "citeRegEx": "Sankaran et al\\.,? 2016", "shortCiteRegEx": "Sankaran et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Neural Information Processing Systems.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "RNN-based encoder-decoder approach with word frequency estimation", "author": ["Jun Suzuki", "Masaaki Nagata."], "venue": "arXiv preprint arXiv:1701.00138 .", "citeRegEx": "Suzuki and Nagata.,? 2016", "shortCiteRegEx": "Suzuki and Nagata.", "year": 2016}, {"title": "Neural headline generation on abstract meaning representation", "author": ["Sho Takase", "Jun Suzuki", "Naoaki Okazaki", "Tsutomu Hirao", "Masaaki Nagata."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Takase et al\\.,? 2016", "shortCiteRegEx": "Takase et al\\.", "year": 2016}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Neural Information Processing Systems.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio."], "venue": "International Conference on Machine", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Efficient summarization with read-again and copy mechanism", "author": ["Wenyuan Zeng", "Wenjie Luo", "Sanja Fidler", "Raquel Urtasun."], "venue": "arXiv preprint arXiv:1611.03382 .", "citeRegEx": "Zeng et al\\.,? 2016", "shortCiteRegEx": "Zeng et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "Due to the difficulty of abstractive summarization, the great majority of past work has been extractive (Saggion and Poibeau, 2013; Mehta, 2016).", "startOffset": 104, "endOffset": 144}, {"referenceID": 8, "context": "Due to the difficulty of abstractive summarization, the great majority of past work has been extractive (Saggion and Poibeau, 2013; Mehta, 2016).", "startOffset": 104, "endOffset": 144}, {"referenceID": 16, "context": "to-sequence models (Sutskever et al., 2014), in which recurrent neural networks (RNNs) both read and freely generate text, has made abstractive summarization viable (Chopra et al.", "startOffset": 19, "endOffset": 43}, {"referenceID": 1, "context": ", 2014), in which recurrent neural networks (RNNs) both read and freely generate text, has made abstractive summarization viable (Chopra et al., 2016; Nallapati et al., 2016; Rush et al., 2015; Zeng et al., 2016).", "startOffset": 129, "endOffset": 212}, {"referenceID": 12, "context": ", 2014), in which recurrent neural networks (RNNs) both read and freely generate text, has made abstractive summarization viable (Chopra et al., 2016; Nallapati et al., 2016; Rush et al., 2015; Zeng et al., 2016).", "startOffset": 129, "endOffset": 212}, {"referenceID": 13, "context": ", 2014), in which recurrent neural networks (RNNs) both read and freely generate text, has made abstractive summarization viable (Chopra et al., 2016; Nallapati et al., 2016; Rush et al., 2015; Zeng et al., 2016).", "startOffset": 129, "endOffset": 212}, {"referenceID": 22, "context": ", 2014), in which recurrent neural networks (RNNs) both read and freely generate text, has made abstractive summarization viable (Chopra et al., 2016; Nallapati et al., 2016; Rush et al., 2015; Zeng et al., 2016).", "startOffset": 129, "endOffset": 212}, {"referenceID": 5, "context": "Therefore we apply our model to the recently-introduced CNN/ Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), which contains news articles (39 sentences on average) paired with multi-sentence summaries, and show that we outperform the stateof-the-art abstractive system by at least 2 ROUGE points.", "startOffset": 80, "endOffset": 126}, {"referenceID": 12, "context": "Therefore we apply our model to the recently-introduced CNN/ Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), which contains news articles (39 sentences on average) paired with multi-sentence summaries, and show that we outperform the stateof-the-art abstractive system by at least 2 ROUGE points.", "startOffset": 80, "endOffset": 126}, {"referenceID": 20, "context": "Our hybrid pointer-generator network facilitates copying words from the source text via pointing (Vinyals et al., 2015), which improves accuracy and handling of OOV words, while retaining the ability to generate new words.", "startOffset": 97, "endOffset": 119}, {"referenceID": 19, "context": "We propose a novel variant of the coverage vector (Tu et al., 2016) from Neural Machine Translation, which we use to track and control coverage of the source document.", "startOffset": 50, "endOffset": 67}, {"referenceID": 3, "context": "The network, which can be viewed as a balance between extractive and abstractive approaches, is similar to Gu et al.\u2019s (2016) CopyNet that was applied to shorttext Chinese summarization.", "startOffset": 107, "endOffset": 126}, {"referenceID": 10, "context": "Our baseline model is similar to that of Nallapati et al. (2016), and is depicted in Figure 2.", "startOffset": 41, "endOffset": 65}, {"referenceID": 0, "context": "The attention distribution at is calculated as in Bahdanau et al. (2015):", "startOffset": 50, "endOffset": 73}, {"referenceID": 20, "context": "Our pointer-generator network is a hybrid between our baseline and a pointer network (Vinyals et al., 2015), as it allows both copying words via pointing, and generating words from a fixed vocabulary.", "startOffset": 85, "endOffset": 107}, {"referenceID": 19, "context": "Repetition is a common problem for sequenceto-sequence models (Tu et al., 2016; Mi et al., 2016; Sankaran et al., 2016; Suzuki and Nagata, 2016), and is especially pronounced when generating multi-sentence text (see Figure 1).", "startOffset": 62, "endOffset": 144}, {"referenceID": 10, "context": "Repetition is a common problem for sequenceto-sequence models (Tu et al., 2016; Mi et al., 2016; Sankaran et al., 2016; Suzuki and Nagata, 2016), and is especially pronounced when generating multi-sentence text (see Figure 1).", "startOffset": 62, "endOffset": 144}, {"referenceID": 15, "context": "Repetition is a common problem for sequenceto-sequence models (Tu et al., 2016; Mi et al., 2016; Sankaran et al., 2016; Suzuki and Nagata, 2016), and is especially pronounced when generating multi-sentence text (see Figure 1).", "startOffset": 62, "endOffset": 144}, {"referenceID": 17, "context": "Repetition is a common problem for sequenceto-sequence models (Tu et al., 2016; Mi et al., 2016; Sankaran et al., 2016; Suzuki and Nagata, 2016), and is especially pronounced when generating multi-sentence text (see Figure 1).", "startOffset": 62, "endOffset": 144}, {"referenceID": 10, "context": ", 2016; Mi et al., 2016; Sankaran et al., 2016; Suzuki and Nagata, 2016), and is especially pronounced when generating multi-sentence text (see Figure 1). We adapt the coverage model of Tu et al. (2016) to solve the problem.", "startOffset": 8, "endOffset": 203}, {"referenceID": 1, "context": "Their approach, which is centered on the attention mechanism, has been augmented with recurrent decoders (Chopra et al., 2016), Abstract Meaning Representations (Takase et al.", "startOffset": 105, "endOffset": 126}, {"referenceID": 18, "context": ", 2016), Abstract Meaning Representations (Takase et al., 2016), and hierarchical networks (Nallapati et al.", "startOffset": 42, "endOffset": 63}, {"referenceID": 12, "context": ", 2016), and hierarchical networks (Nallapati et al., 2016), further improving performance on those datasets.", "startOffset": 35, "endOffset": 59}, {"referenceID": 5, "context": "(2016) adapted the DeepMind question-answering dataset (Hermann et al., 2015) for summarization, resulting in the CNN/Daily Mail dataset, and provided the first abstractive baselines.", "startOffset": 55, "endOffset": 77}, {"referenceID": 11, "context": "The same authors then published a neural extractive approach (Nallapati et al., 2017), which uses hierarchical RNNs to select sentences, and found that it significantly outperformed their abstractive result with respect to the ROUGE metric.", "startOffset": 61, "endOffset": 85}, {"referenceID": 20, "context": "The pointer network (Vinyals et al., 2015) is a sequence-tosequence model that uses the soft attention distribution of Bahdanau et al.", "startOffset": 20, "endOffset": 42}, {"referenceID": 4, "context": "The pointer network has been used to create hybrid approaches for NMT (Gulcehre et al., 2016), language modeling (Merity et al.", "startOffset": 70, "endOffset": 93}, {"referenceID": 9, "context": ", 2016), language modeling (Merity et al., 2016), and summarization (Gulcehre et al.", "startOffset": 27, "endOffset": 48}, {"referenceID": 4, "context": ", 2016), and summarization (Gulcehre et al., 2016; Zeng et al., 2016; Gu et al., 2016; Nallapati et al., 2016).", "startOffset": 27, "endOffset": 110}, {"referenceID": 22, "context": ", 2016), and summarization (Gulcehre et al., 2016; Zeng et al., 2016; Gu et al., 2016; Nallapati et al., 2016).", "startOffset": 27, "endOffset": 110}, {"referenceID": 3, "context": ", 2016), and summarization (Gulcehre et al., 2016; Zeng et al., 2016; Gu et al., 2016; Nallapati et al., 2016).", "startOffset": 27, "endOffset": 110}, {"referenceID": 12, "context": ", 2016), and summarization (Gulcehre et al., 2016; Zeng et al., 2016; Gu et al., 2016; Nallapati et al., 2016).", "startOffset": 27, "endOffset": 110}, {"referenceID": 5, "context": "Rush et al. (2015) were the first to apply modern neural networks to abstractive text summarization, achieving state-of-the-art performance on DUC-2004 and Gigaword, two sentence-level summarization datasets.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "Their approach, which is centered on the attention mechanism, has been augmented with recurrent decoders (Chopra et al., 2016), Abstract Meaning Representations (Takase et al., 2016), and hierarchical networks (Nallapati et al., 2016), further improving performance on those datasets. However, large-scale datasets for summarization of longer text are rare. Nallapati et al. (2016) adapted the DeepMind question-answering dataset (Hermann et al.", "startOffset": 106, "endOffset": 382}, {"referenceID": 0, "context": ", 2015) is a sequence-tosequence model that uses the soft attention distribution of Bahdanau et al. (2015) to produce an output sequence consisting of elements from the input sequence.", "startOffset": 84, "endOffset": 107}, {"referenceID": 0, "context": ", 2015) is a sequence-tosequence model that uses the soft attention distribution of Bahdanau et al. (2015) to produce an output sequence consisting of elements from the input sequence. The pointer network has been used to create hybrid approaches for NMT (Gulcehre et al., 2016), language modeling (Merity et al., 2016), and summarization (Gulcehre et al., 2016; Zeng et al., 2016; Gu et al., 2016; Nallapati et al., 2016). Our approach is closest to Gu et al. (2016), with two differences: (i) We calculate an explicit switch probability pgen, whereas Gu et al.", "startOffset": 84, "endOffset": 468}, {"referenceID": 6, "context": "Originating from Statistical Machine Translation (Koehn et al., 2003), coverage was adapted for NMT by Tu et al.", "startOffset": 49, "endOffset": 69}, {"referenceID": 15, "context": "Temporal attention is a related technique that has been applied to NMT (Sankaran et al., 2016) and summarization (Nallapati et al.", "startOffset": 71, "endOffset": 94}, {"referenceID": 12, "context": ", 2016) and summarization (Nallapati et al., 2016).", "startOffset": 26, "endOffset": 50}, {"referenceID": 12, "context": "This theory is supported by the large boost that coverage gives our ROUGE scores (see Table 1), compared to the smaller boost given by temporal attention for the same task (Nallapati et al., 2016).", "startOffset": 172, "endOffset": 196}, {"referenceID": 4, "context": "Our approach is drastically different from that of Gulcehre et al. (2016) and Nallapati et al.", "startOffset": 51, "endOffset": 74}, {"referenceID": 4, "context": "Our approach is drastically different from that of Gulcehre et al. (2016) and Nallapati et al. (2016). Those works train their pointer components to activate only for out-of-vocabulary words or named entities (whereas we allow our model to freely learn when to use the pointer), and they do not mix the probabilities from the copy distribution and the vocabulary distribution.", "startOffset": 51, "endOffset": 102}, {"referenceID": 4, "context": "Our approach is drastically different from that of Gulcehre et al. (2016) and Nallapati et al. (2016). Those works train their pointer components to activate only for out-of-vocabulary words or named entities (whereas we allow our model to freely learn when to use the pointer), and they do not mix the probabilities from the copy distribution and the vocabulary distribution. We strongly believe our approach is better for abstractive summarization \u2013 in section 6 we show that the copy mechanism is vital for accurately reproducing rare but in-vocabulary words, and in section 7.2 we observe that the mixture model enables the language model and copy mechanism to work together to perform abstractive copying. Coverage. Originating from Statistical Machine Translation (Koehn et al., 2003), coverage was adapted for NMT by Tu et al. (2016) and Mi et al.", "startOffset": 51, "endOffset": 841}, {"referenceID": 4, "context": "Our approach is drastically different from that of Gulcehre et al. (2016) and Nallapati et al. (2016). Those works train their pointer components to activate only for out-of-vocabulary words or named entities (whereas we allow our model to freely learn when to use the pointer), and they do not mix the probabilities from the copy distribution and the vocabulary distribution. We strongly believe our approach is better for abstractive summarization \u2013 in section 6 we show that the copy mechanism is vital for accurately reproducing rare but in-vocabulary words, and in section 7.2 we observe that the mixture model enables the language model and copy mechanism to work together to perform abstractive copying. Coverage. Originating from Statistical Machine Translation (Koehn et al., 2003), coverage was adapted for NMT by Tu et al. (2016) and Mi et al. (2016), who both use a GRU to update the coverage vector each step.", "startOffset": 51, "endOffset": 862}, {"referenceID": 4, "context": "Our approach is drastically different from that of Gulcehre et al. (2016) and Nallapati et al. (2016). Those works train their pointer components to activate only for out-of-vocabulary words or named entities (whereas we allow our model to freely learn when to use the pointer), and they do not mix the probabilities from the copy distribution and the vocabulary distribution. We strongly believe our approach is better for abstractive summarization \u2013 in section 6 we show that the copy mechanism is vital for accurately reproducing rare but in-vocabulary words, and in section 7.2 we observe that the mixture model enables the language model and copy mechanism to work together to perform abstractive copying. Coverage. Originating from Statistical Machine Translation (Koehn et al., 2003), coverage was adapted for NMT by Tu et al. (2016) and Mi et al. (2016), who both use a GRU to update the coverage vector each step. We find that a simpler approach \u2013 summing the attention distributions to obtain the coverage vector \u2013 suffices. In this respect our approach is similar to Xu et al. (2015), who apply a similar method to image captioning.", "startOffset": 51, "endOffset": 1095}, {"referenceID": 5, "context": "We use the CNN/Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), which contains online news articles (781 tokens on average) paired with multi-sentence summaries (3.", "startOffset": 34, "endOffset": 80}, {"referenceID": 12, "context": "We use the CNN/Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), which contains online news articles (781 tokens on average) paired with multi-sentence summaries (3.", "startOffset": 34, "endOffset": 80}, {"referenceID": 5, "context": "We use the CNN/Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), which contains online news articles (781 tokens on average) paired with multi-sentence summaries (3.75 sentences or 56 tokens on average). We used scripts supplied by Nallapati et al. (2016) to obtain the same version of the the data, which has 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs.", "startOffset": 35, "endOffset": 273}, {"referenceID": 2, "context": "We train using Adagrad (Duchi et al., 2011) with learning rate 0.", "startOffset": 23, "endOffset": 43}, {"referenceID": 10, "context": "For the pointer-generator models, we use a vocabulary of 50k words for both source and target \u2013 note that due to the pointer network\u2019s ability to handle OOV words, we may use a smaller vocabulary size than Nallapati et al.\u2019s (2016) 150k source and 60k target vocabularies.", "startOffset": 206, "endOffset": 232}, {"referenceID": 10, "context": "For the pointer-generator models, we use a vocabulary of 50k words for both source and target \u2013 note that due to the pointer network\u2019s ability to handle OOV words, we may use a smaller vocabulary size than Nallapati et al.\u2019s (2016) 150k source and 60k target vocabularies. For the baseline model, we also try a larger vocabulary size of 150k. Note that the addition of the pointer and the coverage mechanism introduces very few extra parameters to the network: for the models with vocabulary size 50k, the baseline model has 21,499,600 parameters, the pointer-generator adds 1153 extra parameters, and coverage adds 512 extra parameters. Unlike Nallapati et al. (2016), we do not pretrain the word embeddings \u2013 they are learned from scratch during training.", "startOffset": 206, "endOffset": 669}, {"referenceID": 12, "context": "ROUGE-1 ROUGE-2 ROUGE-L abstractive model (Nallapati et al., 2016)* 35.", "startOffset": 42, "endOffset": 66}, {"referenceID": 11, "context": "57 lead-3 baseline (Nallapati et al., 2017)* 39.", "startOffset": 19, "endOffset": 43}, {"referenceID": 11, "context": "5 extractive model (Nallapati et al., 2017)* 39.", "startOffset": 19, "endOffset": 43}, {"referenceID": 11, "context": "We obtain our ROUGE scores using the same pyrouge package4 as Nallapati et al. (2016). We trained both our baseline models for about 600,000 iterations (33 epochs) \u2013 this is similar to the 35 epochs required by Nallapati et al.", "startOffset": 62, "endOffset": 86}, {"referenceID": 11, "context": "We obtain our ROUGE scores using the same pyrouge package4 as Nallapati et al. (2016). We trained both our baseline models for about 600,000 iterations (33 epochs) \u2013 this is similar to the 35 epochs required by Nallapati et al.\u2019s (2016) best model.", "startOffset": 62, "endOffset": 237}, {"referenceID": 12, "context": "We also report the lead3 baseline (which uses the first three sentences of the article as a summary), and compare to the best previous abstractive (Nallapati et al., 2016) and extractive (Nallapati et al.", "startOffset": 147, "endOffset": 171}, {"referenceID": 11, "context": ", 2016) and extractive (Nallapati et al., 2017) models.", "startOffset": 23, "endOffset": 47}, {"referenceID": 11, "context": "We also report the lead3 baseline (which uses the first three sentences of the article as a summary), and compare to the best previous abstractive (Nallapati et al., 2016) and extractive (Nallapati et al., 2017) models. Given that we generate plain-text summaries but Nallapati et al. (2016; 2017) generate \u2018anonymized\u2019 summaries (and do not re-substitute the original named entity text before running the ROUGE evaluation), our ROUGE scores are not strictly comparable. There is evidence to suggest that the original-text dataset may result in higher ROUGE scores in general than the anonymized dataset \u2013 the lead-3 baseline is higher on the former than the latter. One possible explanation is that multi-word named entities lead to a higher rate of n-gram overlap. As Nallapati et al. (2016) have not released their output on the test set, we have no means of comparison with their work beyond the ROUGE scores.", "startOffset": 148, "endOffset": 794}, {"referenceID": 11, "context": "We also report the lead3 baseline (which uses the first three sentences of the article as a summary), and compare to the best previous abstractive (Nallapati et al., 2016) and extractive (Nallapati et al., 2017) models. Given that we generate plain-text summaries but Nallapati et al. (2016; 2017) generate \u2018anonymized\u2019 summaries (and do not re-substitute the original named entity text before running the ROUGE evaluation), our ROUGE scores are not strictly comparable. There is evidence to suggest that the original-text dataset may result in higher ROUGE scores in general than the anonymized dataset \u2013 the lead-3 baseline is higher on the former than the latter. One possible explanation is that multi-word named entities lead to a higher rate of n-gram overlap. As Nallapati et al. (2016) have not released their output on the test set, we have no means of comparison with their work beyond the ROUGE scores. Nevertheless, given that the disparity in the lead-3 scores is (+1.1 ROUGE1, +2 ROUGE-2, +1.1 ROUGE-L) points respectively, and our best model scores exceed Nallapati et al. (2016) by (+4.", "startOffset": 148, "endOffset": 1095}, {"referenceID": 12, "context": "Our pointer-generator model with coverage improves the ROUGE scores further, convincingly surpassing the current best abstractive model (Nallapati et al., 2016) by several ROUGE points.", "startOffset": 136, "endOffset": 160}, {"referenceID": 11, "context": "However, our best model does not quite surpass the ROUGE scores of the lead-3 baseline, nor the current best extractive model (Nallapati et al., 2017).", "startOffset": 126, "endOffset": 150}, {"referenceID": 7, "context": "This inflexibility of ROUGE is exacerbated by only having one reference summary, which has been shown to lower ROUGE\u2019s reliability compared to multiple reference summaries (Lin, 2004).", "startOffset": 172, "endOffset": 183}], "year": 2017, "abstractText": "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.", "creator": "LaTeX with hyperref package"}}}