{"id": "1704.04452", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps", "abstract": "Concept maps can be used to concisely represent important information and bring structure into large document collections. Therefore, we study a variant of multi-document summarization that produces summaries in the form of concept maps. However, suitable evaluation datasets for this task are currently missing. To close this gap, we present a newly created corpus of concept maps that summarize heterogeneous collections of web documents on educational topics. It was created using a novel crowdsourcing approach that allows us to efficiently determine important elements in large document collections. We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization.", "histories": [["v1", "Fri, 14 Apr 2017 15:23:45 GMT  (33kb,D)", "http://arxiv.org/abs/1704.04452v1", null], ["v2", "Fri, 21 Jul 2017 13:44:37 GMT  (27kb,D)", "http://arxiv.org/abs/1704.04452v2", "Published at EMNLP 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tobias falke", "iryna gurevych"], "accepted": true, "id": "1704.04452"}, "pdf": {"name": "1704.04452.pdf", "metadata": {"source": "CRF", "title": "Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps", "authors": ["Tobias Falke", "Iryna Gurevych"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to identify themselves rarely write a fully formulated summary for themselves. Instead, user studies are created (Chin et al., 2009; Kang et al., 2011) that show that they note important keywords and phrases in order to identify and organize relationships between them. We believe that the study of the summary with similarly structured outputs is an important extension of traditional task.A representation that is more in line with observed user behavior."}, {"heading": "2 Task", "text": "Concept card-based MDS is defined as: Create a concept map that represents its most important content, meets a preset size limit, and is connected. We define a concept map as a labeled graphic that shows concepts as nodes, and relationships between them as edges. Labels are arbitrary sequences of tokens taken from documents, thereby extracting the summary task. A concept can be a unit, an abstract idea, an event, or an activity characterized by its unique label. Good cards should be propositively coherent, i.e. that each relationship, together with the two related concepts, forms a meaningful proposition.The task is complex and consists of several interdependent subtasks. You must extract suitable labels for concepts and relationships, and recognize various expressions that relate to the same concept across multiple documents. Furthermore, you must select the most important concepts and relationships for the summary, and then organize them into a composite that fulfills the graphics."}, {"heading": "3 Related Work", "text": "In fact, most of them will be able to feel as if they are able to survive on their own."}, {"heading": "4 Low-Context Importance Annotation", "text": "Lloret et al. (2013) describe several experiments on crowdsource reference summaries. Workers are asked to read 10 documents and then select 101duc.nist.gov, tac.nist.gov summary sentences from them for a $0.05 reward. They discovered several challenges, including poor work quality and the subjectivity of the annotation task, suggesting that crowdsourcing is not useful for this purpose.To solve these problems, we are introducing a new task design, low-context annotations, to identify summary parts of documents. Compared to Lloret et et al. \"s approach, it is more in line with best practices in crowdsourcing, as the tasks are simple, intuitive and small (Sabou et al., 2014) and workers receive adequate pay (Fort et al., 2011)."}, {"heading": "4.1 Task Design", "text": "The goal of our crowdsourcing scheme is to get a score for each proposal that indicates its meaning in a document cluster, so that a ranking by score would show what is most important and should be included in a summary. Unlike other work, we do not show the documents to the workers at all, but only provide a description of the document cluster on the topic, ensuring that the tasks can be done small, easily and quickly (see Figure 3). In preliminary tests, we found that despite the minimal context, this design works sensibly on our focused clusters on common educational topics. Consider, for example, Figure 3: It is easy to say that P1 is more important than P2 without reading the documents. We distinguish two task variants: Tasks on the Likert scale Instead of enforcing binary importance decisions, we use a 5-point liquor scale to provide a finer annotation."}, {"heading": "4.2 Pilot Study", "text": "To verify the proposed approach, we conducted a pilot study on Amazon Mechanical Turk using data from TAC2008 (Dang and Owczarzak, 2008). We collected estimates for 474 proposals from the first three clusters3 using both tasks. Each task on the Likert scale was assigned to 5 different workers and received $0.06. For comparison tasks, we also collected 5 labels per person, paid $0.05 and stitched about 7% of all possible pairs. We submitted them in groups of 100 pairs and selected pairs for subsequent batches based on the trust of TrueSkill modeling. Quality control Following the observations of Lloret al. (2013), we introduced several quality control measures. First, we limited our tasks to workers from the U.S. with an approval rate of at least 95%. Second, we identified low-quality workers by measuring the correlation of each worker with the average of the other four scores."}, {"heading": "5 Corpus Creation", "text": "This section presents the corpus construction process outlined in Figure 4, which combines automated pre-processing, scalable crowdsourcing, and high-quality expert comments to scale up the size of our document clusters. We spent about $150 on crowdsourcing and 1.5 hours of expert comments for each topic, while a single annotator would take over 8 hours (at 200 words per minute) to read all documents on a topic."}, {"heading": "5.1 Source Data", "text": "As a starting point, we used the DIP-Corpus (Habernal et al., 2016), a collection of 49 clusters with 100 websites on educational topics (e.g. bullying, homework supervision, drugs) with a brief description of each topic. It emerged from a large web crawl using state-of-the-art information search. We selected 30 of the topics for which we created the necessary concept map annotations."}, {"heading": "5.2 Proposition Extraction", "text": "Since concept maps consist of propositions that express the relationship between concepts (see Figure 1), we must impose such a structure on the plaintext in the document clusters. This could be done by manually annotating spans that represent concepts and relationships, but the size of our clusters makes this a huge effort: 2,288 sentences per topic (a total of 69,000) need to be processed. Therefore, we resort to an automatic approach. The Open Information Extraction Paradigm (Banko et al., 2007) provides a representation that is very similar to the one we want. For example, students with poor credit history should not lose hope and apply for federal loans with the FAFSA. Open IE systems extract tuples of two arguments and a relation phrase that represents propositions: (see with bad credit history, should not lose, hope) (see with bad credit history, apply for federal loans with the FAFSA) While the relationship of a concept theory, we should not lose hope in these (see with bad credit card arguments)."}, {"heading": "5.3 Proposition Filtering", "text": "Despite the similarity of the Open IE paradigm, not every tuple extracted is a suitable proposal for a concept card. Therefore, in order to reduce the effort in the following steps, we want to filter out unsuitable ones. A tuple is suitable if (1) it is a correct extraction, (2) is meaningful without any context, and (3) has arguments that represent correct concepts. We have created a policy that explains when a tuple is suitable for a concept card, and has conducted a small annotation study. Three annotators independently identified 500 randomly sampled tuples, and the match was 82% (= 0.60). We have found that tuples are predominantly unsuitable because they had insoluble pronouns that contradicted with (2) or arguments that were full clauses or propositions, while (1) the negative ratings were filtered by the trust."}, {"heading": "5.4 Importance Annotation", "text": "To handle the large number of suggestions, we combine the two task concepts: First, we collect 5 Likert scores for each proposal, purify the data and calculate averages. Then, we obtain 10% of the pairwise comparisons among them using the top 100 propositions7. Using TrueSkill, we obtain a fine-grained ranking of the 100 most important propositions. For Likert scores, the average agreement on all topics is 0.80, while the majority agreement for comparisons is 0.78. We repeated the data collection for three randomly selected topics and found that the Pearson correlation between the two runs is 0.73 (Spearman 0.73) for Likert scores and 0.72 (Spearman 0.71) for comparisons. These numbers show that the crowdsourcing approach works just as reliably on these data sets as on the TAC documents.In the overall evaluation of all 12k tasks, we added the most important 44k scores to the most important 5k positions."}, {"heading": "5.5 Proposition Revision", "text": "With a manageable number of theses, one commentator then applied a few simple transformations that correct common errors of the Open IE system. First, we divide theses with conjunctions in both arguments into separate theses per conjunction, which the Open IE system sometimes fails to do. Second, we correct stress errors that can occur in the argumentation or relation phases, especially when sentences have not been properly segmented. As a result, we have a set of high-quality theses for our concept map, which averages 56.1 theses per topic due to the initial transformation."}, {"heading": "5.6 Concept Map Construction", "text": "In this last step, we combine the set of important suggestions into a graph. For example, given the following two sentences (student, can borrow, Stafford Loan) (the student, does not have a credit history), it is easy to see, although the first arguments differ slightly, that both terms describe the concept students, which allows us to create a concept map with three concepts. Therefore, the annotation task involves deciding which of the available suggestions should be included in the map, which of their concepts should be combined and which of the available descriptions should be used. As these decisions depend heavily on each other and require context, we decided to use expert annotators instead of the subtask.Annotators received the topic description and the most important ranked propositions. With a simple annotation tool that provides a graph visualization, they were able to combine the propositions step by step."}, {"heading": "6 Corpus Analysis", "text": "In this section, we describe our newly created corpus, which, in addition to summaries in the form of concept maps, differs in several aspects from conventional summary corpus."}, {"heading": "6.1 Document Clusters", "text": "Size The corpus consists of document clusters covering 30 different topics. Each of these documents contains about 40 documents with an average of 2413 characters, resulting in an average cluster size of 97,880 characters. With these features, the document clusters are 15 times larger than typical DUC clusters with ten documents and five times larger than the 25 document clusters (Table 2). In addition, the documents are also more variable in length, as the (length-adjusted) standard deviation is twice as high as in the other company. Genres Because we used a large web crawl as the source for our corpus, it contains documents from a variety of genres. To further analyze this property, we categorized a sample of 50 documents from the corpus. Among them, we found professionally written articles and blog posts (28%), educational material for parents and children (26%), personal blog posts (16%), forum discussions and comments (12%), commented links (12%), and scientific articles (6%)."}, {"heading": "6.2 Concept Maps", "text": "As Table 3 shows, each of the 30 reference concept maps has exactly 25 concepts and between 24 and 28 relationships. Concepts and relationship names consist of an average of 3.2 characters, with the latter being slightly shorter in their characters. To get a better idea of what type of text spans were used as terms, we automatically marked them with their part of the language and determined their head with a dependency parser. Term names tend to be additionally quoted by nouns (82%) or verbs (15%), while they also contain adjectives, prepositions and determinators. However, terms for relations are almost always quoted by a verb (94%) and also contain prepositions, nouns and particles. These distributions are similar to those found by Villalon et al. (2010) for their (individual documents) term corpus.If we analyze the graphical structure of the maps, we find that all of them are related. On average, they have found 7.2 central concepts, those that have a higher significance than those that remain in the remaining position."}, {"heading": "7 Baseline Experiments", "text": "In this section, we briefly describe a baseline and evaluation scripts that we publish, with detailed documentation, along with the corpus.Baseline Method We have a simple approach that is inspired by previous work on the concept map and keyphrase extraction. 3. For each pair of concepts that appear in a sentence, select the tokens in a potential relationship. 4. If a pair of concepts has more than one relationship, select the one with the shortest label. 5. Rank all concepts that appear in a sentence, select the tokens in a possible relationship."}, {"heading": "8 Conclusion", "text": "In this paper, we introduced a low context annotation, a novel crowdsourcing scheme that created a new benchmark corpus for concept-based MDS. Corpus has large document clusters of heterogeneous web documents that provide a challenging summary task. Along with Corpus, we provide implementations of a basic methodology and evaluation scripts, and hope that our efforts will facilitate future research into this variant of the summary.8 For detailed definitions, see the published scripts and documents."}, {"heading": "Acknowledgments", "text": "We would like to thank Teresa Botschen, Andreas Hanselowski and Markus Zopf for their help with the explanatory work. This work was supported by the German Research Foundation in the Research Training Group \"Adaptive Processing of Information from Heterogeneous Sources\" (AIPHES) under funding number RTG 1994 / 1."}], "references": [{"title": "Open Information Extraction from the Web", "author": ["Michele Banko", "Michael J. Cafarella", "Stephen Soderland", "Matt Broadhead", "Oren Etzioni."], "venue": "Proceedings of the 20th International Joint Conference on Artifical Intelligence. Hyderabad, India, pages", "citeRegEx": "Banko et al\\.,? 2007", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "Comparing Rating Scales and Preference Judgements in Language Evaluation", "author": ["Anja Belz", "Eric Kow."], "venue": "Proceedings of the 6th International Natural Language Generation Conference. Trim, Co. Meath, Ireland, pages 7\u201316.", "citeRegEx": "Belz and Kow.,? 2010", "shortCiteRegEx": "Belz and Kow.", "year": 2010}, {"title": "Bridging the gap between extractive and abstractive summaries: Creation and evaluation of coherent extracts from heterogeneous sources", "author": ["Darina Benikova", "Margot Mieskes", "Christian M. Meyer", "Iryna Gurevych."], "venue": "Proceedings of the 26th In-", "citeRegEx": "Benikova et al\\.,? 2016", "shortCiteRegEx": "Benikova et al\\.", "year": 2016}, {"title": "Freebase", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Compilation Proceedings of the International Conference on Management Data & 27th Symposium on Principles of Database", "citeRegEx": "Bollacker et al\\.,? 2009", "shortCiteRegEx": "Bollacker et al\\.", "year": 2009}, {"title": "Concept Maps Applied to Mars Exploration Public Outreach", "author": ["Geoffrey Briggs", "David A. Shamma", "Alberto J. Ca\u00f1as", "Roger Carff", "Jeffrey Scargle", "Joseph D. Novak."], "venue": "Concept Maps: Theory, Methodology, Technology. Proceedings of the", "citeRegEx": "Briggs et al\\.,? 2004", "shortCiteRegEx": "Briggs et al\\.", "year": 2004}, {"title": "Pairwise Ranking Aggregation in a Crowdsourced Setting", "author": ["Xi Chen", "Paul N. Bennett", "Kevyn Collins-Thompson", "Eric Horvitz."], "venue": "Proceedings of the Sixth ACM International Conference on Web Search and Data Mining. Rome, Italy, pages 193\u2013", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Exploring the analytical processes of intelligence analysts", "author": ["George Chin", "Olga A. Kuchar", "Katherine E. Wolf."], "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. Boston, MA, USA, pages 11\u201320.", "citeRegEx": "Chin et al\\.,? 2009", "shortCiteRegEx": "Chin et al\\.", "year": 2009}, {"title": "2016. A Proposition", "author": ["Kuhnle", "Simone Teufel"], "venue": null, "citeRegEx": "Kuhnle and Teufel.,? \\Q2016\\E", "shortCiteRegEx": "Kuhnle and Teufel.", "year": 2016}, {"title": "Concept Maps Core Elements Candidates Recognition from Text", "author": ["Juliana H. Kowata", "Davidson Cury", "Maria Claudia Silva Boeres."], "venue": "Concept Maps: Making Learning Meaningful. Proceedings of the 4th International Conference on Concept", "citeRegEx": "Kowata et al\\.,? 2010", "shortCiteRegEx": "Kowata et al\\.", "year": 2010}, {"title": "The Measurement of Observer Agreement for Categorical Data", "author": ["J. Richard Landis", "Gary G. Koch."], "venue": "Biometrics 33(1):159\u2013174. https://doi.org/10.2307/2529310.", "citeRegEx": "Landis and Koch.,? 1977", "shortCiteRegEx": "Landis and Koch.", "year": 1977}, {"title": "Abstractive Multi-document Summarization with Semantic Information Extraction", "author": ["Wei Li."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal, pages 1908\u20131913.", "citeRegEx": "Li.,? 2015", "shortCiteRegEx": "Li.", "year": 2015}, {"title": "Abstractive News Summarization based on Event Semantic Link Network", "author": ["Wei Li", "Lei He", "Hai Zhuge."], "venue": "Proceedings of the 26th International Conference on Computational Linguistics (COLING). Osaka, Japan, pages 236\u2013246.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "ROUGE: A Package for Automatic Evaluation of Summaries", "author": ["Chin-Yew Lin."], "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop. pages 74\u201381. http://aclweb.org/anthology/W04-1013.", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Toward Abstractive Summarization Using Semantic Representations", "author": ["Fei Liu", "Jeffrey Flanigan", "Sam Thomson", "Norman Sadeh", "Noah A. Smith."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Analyzing the capabilities of crowdsourcing services for text summarization", "author": ["Elena Lloret", "Laura Plaza", "Ahmet Aker."], "venue": "Language Resources and Evaluation 47(2):337\u2013369. https://doi.org/10.1007/s10579-012-9198-8.", "citeRegEx": "Lloret et al\\.,? 2013", "shortCiteRegEx": "Lloret et al\\.", "year": 2013}, {"title": "Multidocument summarization: An added value to clustering in interactive retrieval", "author": ["Manuel J. Ma\u00f1a-L\u00f3pez", "Manuel de Buenaga", "Jos\u00e9 M. G\u00f3mez-Hidalgo."], "venue": "ACM Transactions on Information Systems 22(2):215\u2013241.", "citeRegEx": "Ma\u00f1a.L\u00f3pez et al\\.,? 2004", "shortCiteRegEx": "Ma\u00f1a.L\u00f3pez et al\\.", "year": 2004}, {"title": "Do summaries help? A Task-Based Evaluation of Multi-Document Summarization", "author": ["Kathleen McKeown", "Rebecca J. Passonneau", "David K. Elson", "Ani Nenkova", "Julia Hirschberg."], "venue": "Proceedings of the 28th Annual International ACM SIGIR Con-", "citeRegEx": "McKeown et al\\.,? 2005", "shortCiteRegEx": "McKeown et al\\.", "year": 2005}, {"title": "Construction of Text Summarization Corpus for the Credibility of Information on the Web", "author": ["Masahiro Nakano", "Hideyuki Shibuki", "Rintaro Miyazaki", "Madoka Ishioroshi", "Koichi Kaneko", "Tatsunori Mori."], "venue": "Proceedings of the Seventh", "citeRegEx": "Nakano et al\\.,? 2010", "shortCiteRegEx": "Nakano et al\\.", "year": 2010}, {"title": "Automatic Summarization", "author": ["Ani Nenkova", "Kathleen R. McKeown."], "venue": "Foundations and Trends in Information Retrieval 5(2):103\u2013233. https://doi.org/10.1561/1500000015.", "citeRegEx": "Nenkova and McKeown.,? 2011", "shortCiteRegEx": "Nenkova and McKeown.", "year": 2011}, {"title": "Theoretical Origins of Concept Maps, How to Construct Them, and Uses in Education", "author": ["Joseph D. Novak", "Alberto J. Ca\u00f1as."], "venue": "Reflecting Education 3(1):29\u201342.", "citeRegEx": "Novak and Ca\u00f1as.,? 2007", "shortCiteRegEx": "Novak and Ca\u00f1as.", "year": 2007}, {"title": "Learning How to Learn", "author": ["Joseph D. Novak", "D. Bob Gowin."], "venue": "Cambridge University Press, Cambridge. https://doi.org/10.1017/CBO9781139173469.", "citeRegEx": "Novak and Gowin.,? 1984", "shortCiteRegEx": "Novak and Gowin.", "year": 1984}, {"title": "Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity", "author": ["Mohammad Taher Pilehvar", "David Jurgens", "Roberto Navigli."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computa-", "citeRegEx": "Pilehvar et al\\.,? 2013", "shortCiteRegEx": "Pilehvar et al\\.", "year": 2013}, {"title": "Concept map construction from text documents using affinity propagation", "author": ["Iqbal Qasim", "Jin-Woo Jeong", "Jee-Uk Heu", "Dong-Ho Lee."], "venue": "Journal of Information Science 39(6):719\u2013 736. https://doi.org/10.1177/0165551513494645.", "citeRegEx": "Qasim et al\\.,? 2013", "shortCiteRegEx": "Qasim et al\\.", "year": 2013}, {"title": "Knowledge discovery from texts: A Concept Frame Graph Approach", "author": ["Kanagasabai Rajaraman", "Ah-Hwee Tan."], "venue": "Proceedings of the Eleventh International Conference on Information and Knowledge Management. McLean, VA, USA, pages 669\u2013", "citeRegEx": "Rajaraman and Tan.,? 2002", "shortCiteRegEx": "Rajaraman and Tan.", "year": 2002}, {"title": "Using concept maps as a cross-language resource discovery tool for large documents in digital libraries", "author": ["Ryan Richardson", "Edward A. Fox."], "venue": "Proceedings of the 5th ACM/IEEE-CS Joint Conference on Digital Libraries. Denver, CO, USA, page", "citeRegEx": "Richardson and Fox.,? 2005", "shortCiteRegEx": "Richardson and Fox.", "year": 2005}, {"title": "Information navigation on the web by clustering and summarizing query results", "author": ["Dmitri G. Roussinov", "Hsinchun Chen."], "venue": "Information Processing & Management 37(6):789\u2013816. https://doi.org/10.1016/S0306-4573(00)00062-5.", "citeRegEx": "Roussinov and Chen.,? 2001", "shortCiteRegEx": "Roussinov and Chen.", "year": 2001}, {"title": "Using Concept Maps for Information Conceptualization and Schematization in Technical Reading and Writing Courses: A Case Study for Computer Science Majors in Japan", "author": ["Debopriyo Roy."], "venue": "IEEE International Professional Communication", "citeRegEx": "Roy.,? 2008", "shortCiteRegEx": "Roy.", "year": 2008}, {"title": "Corpus Annotation through Crowdsourcing: Towards Best Practice Guidelines", "author": ["Marta Sabou", "Kalina Bontcheva", "Leon Derczynski", "Arno Scharl."], "venue": "Proceedings of the 9th International Conference on Language Resources and Evaluation. Reykjavik,", "citeRegEx": "Sabou et al\\.,? 2014", "shortCiteRegEx": "Sabou et al\\.", "year": 2014}, {"title": "Cheap and Fast \u2013 But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks", "author": ["Rion Snow", "Brendan O\u2019Connor", "Daniel Jurafsky", "Andrew Ng"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Snow et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2008}, {"title": "Creating a Large Benchmark for Open Information Extraction", "author": ["Gabriel Stanovsky", "Ido Dagan."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas, pages 2300\u20132305.", "citeRegEx": "Stanovsky and Dagan.,? 2016", "shortCiteRegEx": "Stanovsky and Dagan.", "year": 2016}, {"title": "JumpStarting Concept Map Construction with Knowledge Extracted from Documents", "author": ["Alejandro Valerio", "David B. Leake."], "venue": "Proceedings of the 2nd International Conference on Concept Mapping. San Jos\u00e9, Costa Rica, pages 296\u2013303.", "citeRegEx": "Valerio and Leake.,? 2006", "shortCiteRegEx": "Valerio and Leake.", "year": 2006}, {"title": "Automated Generation of Concept Maps to Support Writing", "author": ["Jorge J. Villalon."], "venue": "PhD Thesis, University of Sydney, Australia.", "citeRegEx": "Villalon.,? 2012", "shortCiteRegEx": "Villalon.", "year": 2012}, {"title": "Analysis of a Gold Standard for Concept Map Mining - How Humans Summarize Text Using Concept Maps", "author": ["Jorge J. Villalon", "Rafael A. Calvo", "Rodrigo Montenegro."], "venue": "Proceedings of the 4th International Conference on Concept Mapping. Vina", "citeRegEx": "Villalon et al\\.,? 2010", "shortCiteRegEx": "Villalon et al\\.", "year": 2010}, {"title": "Crowdsourced Top-k Algorithms: An Experimental Evaluation", "author": ["Xiaohang Zhang", "Guoliang Li", "Jianhua Feng."], "venue": "Proceedings of the Very Large Databases Endowment 9(8):612\u2013623.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "The Next Step for Multi-Document Summarization: A Heterogeneous Multi-Genre Corpus Built with a Novel Construction Approach", "author": ["Markus Zopf", "Maxime Peyrard", "Judith EckleKohler."], "venue": "Proceedings of the 26th Inter-", "citeRegEx": "Zopf et al\\.,? 2016", "shortCiteRegEx": "Zopf et al\\.", "year": 2016}, {"title": "Evaluating the Generation of Domain Ontologies in the Knowledge Puzzle Project", "author": ["Amal Zouaq", "Roger Nkambou."], "venue": "IEEE Transactions on Knowledge and Data Engineering 21(11):1559\u2013 1572. https://doi.org/10.1109/TKDE.2009.25.", "citeRegEx": "Zouaq and Nkambou.,? 2009", "shortCiteRegEx": "Zouaq and Nkambou.", "year": 2009}, {"title": "Implementation of method for generating concept map from unstructured text in the Croatian language", "author": ["Krunoslav Zubrinic", "Ines Obradovic", "Tomo Sjekavica."], "venue": "23rd International Conference on Software,", "citeRegEx": "Zubrinic et al\\.,? 2015", "shortCiteRegEx": "Zubrinic et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "Generated summaries have been shown to support humans dealing with large document collections in information seeking tasks (McKeown et al., 2005; Ma\u00f1a-L\u00f3pez et al., 2004; Roussinov and Chen, 2001).", "startOffset": 123, "endOffset": 196}, {"referenceID": 15, "context": "Generated summaries have been shown to support humans dealing with large document collections in information seeking tasks (McKeown et al., 2005; Ma\u00f1a-L\u00f3pez et al., 2004; Roussinov and Chen, 2001).", "startOffset": 123, "endOffset": 196}, {"referenceID": 25, "context": "Generated summaries have been shown to support humans dealing with large document collections in information seeking tasks (McKeown et al., 2005; Ma\u00f1a-L\u00f3pez et al., 2004; Roussinov and Chen, 2001).", "startOffset": 123, "endOffset": 196}, {"referenceID": 6, "context": "Instead, user studies (Chin et al., 2009; Kang et al., 2011) show that they note down important keywords and phrases, try to identify relationships between them and organize them accordingly.", "startOffset": 22, "endOffset": 60}, {"referenceID": 20, "context": "A representation that is more in line with observed user behavior is a concept map (Novak and Gowin, 1984), a labeled graph showing concepts as nodes and relationships between them as edges (Figure 1).", "startOffset": 83, "endOffset": 106}, {"referenceID": 19, "context": "Introduced in 1972 as a teaching tool (Novak and Ca\u00f1as, 2007), they have found many applications in education (Edwards and Fraser, 1983; Roy, 2008), for writing assistance (Villalon, 2012) or to structure information repositories (Briggs et al.", "startOffset": 38, "endOffset": 61}, {"referenceID": 26, "context": "Introduced in 1972 as a teaching tool (Novak and Ca\u00f1as, 2007), they have found many applications in education (Edwards and Fraser, 1983; Roy, 2008), for writing assistance (Villalon, 2012) or to structure information repositories (Briggs et al.", "startOffset": 110, "endOffset": 147}, {"referenceID": 31, "context": "Introduced in 1972 as a teaching tool (Novak and Ca\u00f1as, 2007), they have found many applications in education (Edwards and Fraser, 1983; Roy, 2008), for writing assistance (Villalon, 2012) or to structure information repositories (Briggs et al.", "startOffset": 172, "endOffset": 188}, {"referenceID": 4, "context": "Introduced in 1972 as a teaching tool (Novak and Ca\u00f1as, 2007), they have found many applications in education (Edwards and Fraser, 1983; Roy, 2008), for writing assistance (Villalon, 2012) or to structure information repositories (Briggs et al., 2004; Richardson and Fox, 2005).", "startOffset": 230, "endOffset": 277}, {"referenceID": 24, "context": "Introduced in 1972 as a teaching tool (Novak and Ca\u00f1as, 2007), they have found many applications in education (Edwards and Fraser, 1983; Roy, 2008), for writing assistance (Villalon, 2012) or to structure information repositories (Briggs et al., 2004; Richardson and Fox, 2005).", "startOffset": 230, "endOffset": 277}, {"referenceID": 14, "context": "In contrast to traditional approaches, it allows to determine important elements in a document cluster without reading all documents, making it feasible to crowdsource the work and overcome quality issues observed in previous work (Lloret et al., 2013).", "startOffset": 231, "endOffset": 252}, {"referenceID": 36, "context": "Some attempts have been made to automatically construct concept maps from text, working with either single documents (Zubrinic et al., 2015; Villalon, 2012; Valerio and Leake, 2006; Kowata et al., 2010) or document clusters (Qasim et al.", "startOffset": 117, "endOffset": 202}, {"referenceID": 31, "context": "Some attempts have been made to automatically construct concept maps from text, working with either single documents (Zubrinic et al., 2015; Villalon, 2012; Valerio and Leake, 2006; Kowata et al., 2010) or document clusters (Qasim et al.", "startOffset": 117, "endOffset": 202}, {"referenceID": 30, "context": "Some attempts have been made to automatically construct concept maps from text, working with either single documents (Zubrinic et al., 2015; Villalon, 2012; Valerio and Leake, 2006; Kowata et al., 2010) or document clusters (Qasim et al.", "startOffset": 117, "endOffset": 202}, {"referenceID": 8, "context": "Some attempts have been made to automatically construct concept maps from text, working with either single documents (Zubrinic et al., 2015; Villalon, 2012; Valerio and Leake, 2006; Kowata et al., 2010) or document clusters (Qasim et al.", "startOffset": 117, "endOffset": 202}, {"referenceID": 22, "context": ", 2010) or document clusters (Qasim et al., 2013; Zouaq and Nkambou, 2009; Rajaraman and Tan, 2002).", "startOffset": 29, "endOffset": 99}, {"referenceID": 35, "context": ", 2010) or document clusters (Qasim et al., 2013; Zouaq and Nkambou, 2009; Rajaraman and Tan, 2002).", "startOffset": 29, "endOffset": 99}, {"referenceID": 23, "context": ", 2010) or document clusters (Qasim et al., 2013; Zouaq and Nkambou, 2009; Rajaraman and Tan, 2002).", "startOffset": 29, "endOffset": 99}, {"referenceID": 8, "context": ", 2015; Villalon, 2012; Valerio and Leake, 2006; Kowata et al., 2010) or document clusters (Qasim et al., 2013; Zouaq and Nkambou, 2009; Rajaraman and Tan, 2002). These approaches extract concept and relation labels from syntactic structures and connect them to build a concept map. However, common task definitions and comparable evaluations are missing. In addition, only a few of them, namely Villalon (2012) and Valerio and Leake (2006), define summarization as their goal and try to compress the input to a substantially smaller size.", "startOffset": 49, "endOffset": 412}, {"referenceID": 8, "context": ", 2015; Villalon, 2012; Valerio and Leake, 2006; Kowata et al., 2010) or document clusters (Qasim et al., 2013; Zouaq and Nkambou, 2009; Rajaraman and Tan, 2002). These approaches extract concept and relation labels from syntactic structures and connect them to build a concept map. However, common task definitions and comparable evaluations are missing. In addition, only a few of them, namely Villalon (2012) and Valerio and Leake (2006), define summarization as their goal and try to compress the input to a substantially smaller size.", "startOffset": 49, "endOffset": 441}, {"referenceID": 18, "context": "For the subtask of selecting summary-worthy concepts and relations, techniques developed for traditional summarization (Nenkova and McKeown, 2011) and keyphrase extraction (Hasan and Ng, 2014) are related and applicable.", "startOffset": 119, "endOffset": 146}, {"referenceID": 11, "context": "Approaches that build graphs of propositions to create a summary (Fang et al., 2016; Li et al., 2016; Liu et al., 2015; Li, 2015) seem to be particularly related, however, there is one important difference: While they use graphs as an intermediate representation from which a textual summary is then generated,", "startOffset": 65, "endOffset": 129}, {"referenceID": 13, "context": "Approaches that build graphs of propositions to create a summary (Fang et al., 2016; Li et al., 2016; Liu et al., 2015; Li, 2015) seem to be particularly related, however, there is one important difference: While they use graphs as an intermediate representation from which a textual summary is then generated,", "startOffset": 65, "endOffset": 129}, {"referenceID": 10, "context": "Approaches that build graphs of propositions to create a summary (Fang et al., 2016; Li et al., 2016; Liu et al., 2015; Li, 2015) seem to be particularly related, however, there is one important difference: While they use graphs as an intermediate representation from which a textual summary is then generated,", "startOffset": 65, "endOffset": 129}, {"referenceID": 34, "context": "(Zopf et al., 2016) and (Benikova et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": ", 2016) and (Benikova et al., 2016).", "startOffset": 12, "endOffset": 35}, {"referenceID": 32, "context": "For concept map generation, one corpus with humancreated summary concept maps for student essays has been created (Villalon et al., 2010).", "startOffset": 114, "endOffset": 137}, {"referenceID": 9, "context": "Extending these efforts, several more specialized corpora have been created: With regard to size, Nakano et al. (2010) present a corpus of summaries for large-scale collections of web pages.", "startOffset": 44, "endOffset": 119}, {"referenceID": 3, "context": "Other types of information representation that also model concepts and their relationships are knowledge bases, such as Freebase (Bollacker et al., 2009), and ontologies.", "startOffset": 129, "endOffset": 153}, {"referenceID": 27, "context": "\u2019s approach, it is more in line with crowdsourcing best practices, as the tasks are simple, intuitive and small (Sabou et al., 2014) and workers receive reasonable payment (Fort et al.", "startOffset": 112, "endOffset": 132}, {"referenceID": 1, "context": "Comparisons are known to be easier to make and more consistent (Belz and Kow, 2010), but also more expensive, as the number of pairs grows quadratically with the number of objects.", "startOffset": 63, "endOffset": 83}, {"referenceID": 33, "context": ", 2007), a powerful Bayesian rank induction model (Zhang et al., 2016), to obtain importance estimates for each proposition.", "startOffset": 50, "endOffset": 70}, {"referenceID": 14, "context": "In addition, we included trap sentences, similar as in (Lloret et al., 2013), in around 80 of the tasks.", "startOffset": 55, "endOffset": 76}, {"referenceID": 10, "context": "Quality Control Following the observations of Lloret et al. (2013), we established several measures for quality control.", "startOffset": 3, "endOffset": 67}, {"referenceID": 5, "context": "Even with intelligent sampling strategies, such as the active learning in CrowdBT (Chen et al., 2013), the number of pairs is only reduced by a constant factor (Zhang et al.", "startOffset": 82, "endOffset": 101}, {"referenceID": 33, "context": ", 2013), the number of pairs is only reduced by a constant factor (Zhang et al., 2016).", "startOffset": 66, "endOffset": 86}, {"referenceID": 10, "context": "Agreement and Reliability For Likert-scale tasks, we follow Snow et al. (2008) and calculate agreement as the average Pearson correlation of a worker\u2019s Likert-score with the average score of the remaining workers.", "startOffset": 16, "endOffset": 79}, {"referenceID": 10, "context": "Agreement and Reliability For Likert-scale tasks, we follow Snow et al. (2008) and calculate agreement as the average Pearson correlation of a worker\u2019s Likert-score with the average score of the remaining workers.4 This measure is less strict than exact label agreement and can account for close labels and high- or low-scoring workers. We observe a correlation of 0.81, indicating substantial agreement. For comparisons, the majority agreement is 0.73. To further examine the reliability of the collected data, we followed the approach of Kiritchenko and Mohammed (2016) and simply repeated the crowdsourcing for one of the three topics.", "startOffset": 16, "endOffset": 572}, {"referenceID": 0, "context": "The Open Information Extraction paradigm (Banko et al., 2007) offers a representation very similar to the desired one.", "startOffset": 41, "endOffset": 61}, {"referenceID": 29, "context": "We used Open IE 46, a state-of-the-art system (Stanovsky and Dagan, 2016) to process all sentences.", "startOffset": 46, "endOffset": 73}, {"referenceID": 19, "context": "They were instructed to reach a size of 25 concepts, the recommended maximum size for a concept map (Novak and Ca\u00f1as, 2007).", "startOffset": 100, "endOffset": 123}, {"referenceID": 21, "context": "To support the annotators, the tool used ADW (Pilehvar et al., 2013), a state-of-the-art approach for semantic similarity, to suggest possible connections.", "startOffset": 45, "endOffset": 68}, {"referenceID": 9, "context": "Hence, the annotation shows substantial agreement (Landis and Koch, 1977).", "startOffset": 50, "endOffset": 73}, {"referenceID": 34, "context": "To capture this property, we follow Zopf et al. (2016) and compute, for every topic, the average Jensen-Shannon divergence between the word distribution of one document and the word distribution in the remaining documents.", "startOffset": 36, "endOffset": 55}, {"referenceID": 31, "context": "These distributions are very similar to those reported by Villalon et al. (2010) for their (single-document) concept map corpus.", "startOffset": 58, "endOffset": 81}, {"referenceID": 12, "context": "And finally, we compute ROUGE-2 (Lin, 2004) between the concatenation of all propositions from the maps.", "startOffset": 32, "endOffset": 43}], "year": 2017, "abstractText": "Concept maps can be used to concisely represent important information and bring structure into large document collections. Therefore, we study a variant of multidocument summarization that produces summaries in the form of concept maps. However, suitable evaluation datasets for this task are currently missing. To close this gap, we present a newly created corpus of concept maps that summarize heterogeneous collections of web documents on educational topics. It was created using a novel crowdsourcing approach that allows us to efficiently determine important elements in large document collections. We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization.", "creator": "LaTeX with hyperref package"}}}