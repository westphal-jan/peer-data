{"id": "1704.04601", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2017", "title": "MUSE: Modularizing Unsupervised Sense Embeddings", "abstract": "This paper proposes to address the word sense ambiguity issue in an unsupervised manner, where word sense representations are learned along a word sense selection mechanism given contexts. Prior work about learning multi-sense embeddings suffered from either ambiguity of different-level embeddings or inefficient sense selection. The proposed modular framework, MUSE, implements flexible modules to optimize distinct mechanisms, achieving the first purely sense-level representation learning system with linear-time sense selection. We leverage reinforcement learning to enable joint training on the proposed modules, and introduce various exploration techniques on sense selection for better robustness. The experiments on benchmark data show that the proposed approach achieves the state-of-the-art performance on synonym selection as well as on contextual word similarities in terms of MaxSimC.", "histories": [["v1", "Sat, 15 Apr 2017 07:36:49 GMT  (466kb,D)", "http://arxiv.org/abs/1704.04601v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["guang-he lee", "yun-nung chen"], "accepted": true, "id": "1704.04601"}, "pdf": {"name": "1704.04601.pdf", "metadata": {"source": "CRF", "title": "MUSE: Modularizing Unsupervised Sense Embeddings", "authors": ["Guang-He Lee", "Yun-Nung Chen"], "emails": ["yvchen}@csie.ntu.edu.tw"], "sections": [{"heading": "1 Introduction", "text": "Recently, we have dominated several areas of research in natural language processing (NLP), such as machine translation, language comprehension and dialog systems. However, most applications use the embedding of words to achieve semantics. Considering that natural language is very ambiguous, the standard selection of words may suffer from polysemy. (2014) It is pointed out that due to the inequality in vector space, when a word has two different meanings, the sum of the distances between the word and its synonyms is in any sense greater than the distance between the respective synonyms, which make each other irrelevant, in terms of the embedding of space."}, {"heading": "2 Related Work", "text": "There are three types of approaches to learning multiple naming in literature: 1) The brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated in the brainconsecrated brainconsecrated cnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnneeu.ndUn"}, {"heading": "3 Proposed Approach: MUSE", "text": "This paper proposes a framework for modularizing two key mechanisms for multi-sense word representation: a sensory selection module and a sensory representation module. The sensory selection module decides which sense to use in the context of a text, while the sensory representation module learns meaningful representations based on its statistical properties. In contrast to previous work, which has to compromise between efficient sensory selection and purely sensual representation learning, the proposed modularized framework module is able to perform efficient sensory selection and learning representations at the same time. To learn sensory selection models, a sensory selection model for the decoding of sensory identity should first be established. On the other hand, the sensory embeddings should guide the sensory selection model in the decoding of a sensory identity sequence. Therefore, these two modules should be confused. This indicates that a naive two-step algorithm, or two separate learning algorithms, are not optimal for the formulation proposed by previous work on the different sensory algorithms."}, {"heading": "3.1 Model Architecture", "text": "Our model architecture is shown in Figure 1, where there are two optimization modules."}, {"heading": "3.1.1 Sense Selection Module", "text": "In view of the fact that a sense of the word is determined by the local context, we can either formulate a probability policy (zik | C) or use a local context. (zik | C) To ensure efficiency, we can either formulate a probability policy (zik | C) by selecting meaning or apply the individual fitness q (zik | C) for each sense identity. To ensure efficiency, we use a linear neural architecture that occupies the word level. (zik | C) The architecture is similar to continuous terminology (CBOW) (Mikolov et al, 2013a)."}, {"heading": "3.1.2 Sense Representation Module", "text": "Semantic representation of learning is typically formulated as Maximum Probability Estimation (MLE), where only two sensory identities are required for stochastic training. (Other popular candidates, such as GloVe (Pennington et al., 2014) and CBOW (Mikolov et al., 2013a), require more sensory identities to be selected as input and are therefore not suitable for our scenario. (GloVe (Pennington et al., 2014) takes mathematically expensive collocation counter statistics for each token in a corpus as input, which requires a meaningful selection for each occurrence of the target throughout the corpus and is therefore not suitable for word selection. (Pennington et al al al al, 2014) Sensory expensive collocation counter statistics for each token in a corpus as input."}, {"heading": "3.2 Learning", "text": "Without a supervised signal for the proposed modules, it is desirable to combine two modules in such a way that they can mutually improve each other by their own estimates. On the one hand, it is trivial to forward the prediction of the sense selection module to the representation module. On the other hand, we cast the estimated collocation probability as a reward signal for the selected sense of effective learning. On the basis of various modeling methods ((((1) or (2) in the sense selection module), we connect the model with the respective learning algorithms. The learning algorithm can be regarded as an amplification learning algorithm that solves an onestep Markov decision process (MDP) (Sutton and Barto, 1998), in which the state, action and reward correspond to the context C-t, sense zik and the collocation protocol probability L-Log (\u00b7)."}, {"heading": "3.2.1 Policy Gradient Method", "text": "Since (1) fits a valid probability distribution, an intuitive way is to optimize the expectation to check the probability of collocation between the individual senses. Since the formulation in (4) is unidirectional (L) (zik | zjl) 6 = L (zjl | zik)), we perform a one-sided optimization for the goal sense zik in order to stabilize the model training3. That is, for the target word wi and the collocation word wj in view of the respective contexts C-t and C-t \"(0 < | t-t), we first draw a sense zjl for wj from the political procedure (\u00b7 C-t) and optimize the expected collocation probability for the goal sense zik as follows, maxEzik (C-t) [L] (zjl | zik). The goal is differentiable and supports the stochastic probability of estimation for the goal sense zik."}, {"heading": "3.2.2 Value-Based Method", "text": "To address the above problems, we apply the Qlearning algorithm (Mnih et al., 2013). Instead of maintaining a probability policy for sensory perception, Q-learning directly and independently estimates the Q value (resulting collocation protocol probability) for each sensory probability. Note: The reward corresponds to the Q value, so we will use reward and Q value interchangeably, based on the context. We continue to follow the convention of newer neural enhancement of learning by reducing the reward margin for the training phase (Mnih et al., 2013). Specifically, we replace the log probability protocols L (\u00b7 inf, 0] with the probability L (\u00b7) as a reward function."}, {"heading": "3.2.3 Joint Training", "text": "To train sensory selection and representation together, we first select a pair of colliding senses, zik and zjl, based on the sensory selection module with any selection strategy (e.g. algorithm 1: Learning Algorithm for wi = Ct-C dosample wj = Ct \"(0 < | t\" \u2212 t | \u2264 m); zik = select (Ct, wi); zjl = select (Ct, \"wj); optimize U, V by (4) for the senser presentation module; optimize P, Q by (5) or (7) for the sensory selection module; greedy); and then optimize the sensory selection module and the sensory selection module using the above derivatives. Algorithm 1 describes the proposed MUSE model training method. The main distinction between our modular framework and the two-stage cluster formation framework (Neelakal et; Selection 2014 and Vu) allows immediate selection of senses."}, {"heading": "3.3 Sense Selection Strategy", "text": "Given a fitness estimate for each sense, the exploitation of the greedy sense is the most popular strategy for clustering algorithms (Neelakantan et al., 2014; Ka-geba-ck et al., 2015) and hard EM algorithms (Qiu et al., 2016; Jauhar et al., 2015) in literature. However, there are two incentives for exploration: First, when fitness is not well estimated, it is desirable to explore underestimated senses in the early training phase. Second, we introduce exploration mechanisms for meaningful selection in natural language, sometimes multiple senses in a word would fit into the same context. The dilemma between exploring suboptimal options and exploiting optimal choices is referred to as the exploration-exploitation trade-off for enhanced learning (Sutton and Barto, 1998)."}, {"heading": "4 Experiments", "text": "We evaluate our proposed MUSE models in both quantitative and qualitative experiments."}, {"heading": "4.1 Experimental Setup", "text": "Our model is based on the Wikipedia dump of April 2010 (Shaoul and Westbury, 2010), which contains approximately 1 billion tokens. To be fair, we use the same vocabulary as Huang et al. (2012) and Neelakantan et al. (2014). For pre-processing, we convert all words into lowercase letters, apply the Stanford tokenizer and Stanford sentence tokenizer (Manning et al., 2014), and remove all sentences with less than 10 tokens. The number of senses per word in Q is set to 3 for fair comparison with previous work (Neelakantan et al., 2014)."}, {"heading": "4.2 Experiment 1: Contextual Word Similarity", "text": "In order to evaluate the quality of sensory perceptions learned, we calculate the similarity between the individual word pairs and their respective local contexts and compare them with the people who are able to move around the world. (Si, C, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S,"}, {"heading": "4.3 Experiment 2: Synonym Selection", "text": "We continue to evaluate our model of synonym selection with multi-meaning word representations (Jauhar et al., 2015). Three standard synonym selection datasets, ESL-50 (Turney, 2001), RD-300 (Jarmasz and Szpakowicz, 2004) and TOEFL-80 (Landauer and Dumais, 1997), are performed. In the datasets, each question consists of a question word wQ and four answer candidates {wA, wB, wC, wD}, and the goal is to choose the most semantically synonymous choice among the four candidates."}, {"heading": "4.4 Qualitative Analysis", "text": "Furthermore, we perform qualitative analyses to verify the semantic meanings of different senses learned through MUSE with k-closest neighbours (k-NN) by means of sensory representations. Furthermore, we provide contexts in the training corpus in which the sense is selected to validate the sensory selection module. Table 2 shows the results. The learned sensory embeddings of the words \"tie,\" \"blackberry\" and \"head\" correspond to clearly correct senses in different contexts."}, {"heading": "5 Conclusion", "text": "This paper proposes a novel modular framework for unsupervised sensory learning that supports not only the flexible design of modular tasks, but also joint optimization between modules. The proposed model is the first work to achieve purely sensory perceptions with linear time sensor selection and delivers the most advanced performance in selecting synonyms and benchmarking contextual word similarity in the MaxSimC sense. In the future, we plan to explore enhanced learning methods to include multi-meaning word representations for downstream NLP tasks."}, {"heading": "A Doubly Stochastic Gradient", "text": "To derive the double stochastic gradient for Equation (5), we first (5) designate the efficiency (5) as \"J\" with \"P,\" \"Q\" and solve the form of expectation as: \"J\" (3). The gradient in relation to the selection procedures should be: \"J\" (4) = \"D\" (4) = \"D\" (4) = \"D\" (4): \"D\" (4) = \"D\" (4) = \"D\" (4) = \"D\" (4) = \"D\" (4): \"D\" (4): \"D\" (5): \"D\" (5): \"D\" (5): \"D\" (5): \"D\" (5): \"D\" (5): \"D\" (5): \"D\" (5): \"D\" (5): \"D\" (5). \"D\" (D): \"D\" (5): \"D\" (5): \"D\" (5): \"D\" (5): \"D\" (5): \"D\" (5): \"D\" (D)."}], "references": [{"title": "Breaking sticks and ambi", "author": ["Sergey Bartunov", "Dmitry Kondrashkin", "Anton Osokin", "Dmitry Vetrov"], "venue": null, "citeRegEx": "Bartunov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bartunov et al\\.", "year": 2016}, {"title": "Improving distributed representation of word sense via wordnet gloss composition and context clustering", "author": ["Tao Chen", "Ruifeng Xu", "Yulan He", "Xuan Wang."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun."], "venue": "EMNLP. Citeseer, pages 1025\u2013 1035.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Retrofitting sense-specific word vectors using parallel text", "author": ["Allyson Ettinger", "Philip Resnik", "Marine Carpuat."], "venue": "Proceedings of NAACL-HLT . pages 1378\u20131383.", "citeRegEx": "Ettinger et al\\.,? 2016", "shortCiteRegEx": "Ettinger et al\\.", "year": 2016}, {"title": "Learning sense-specific word embeddings by exploiting bilingual resources", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu."], "venue": "COLING. pages 497\u2013507.", "citeRegEx": "Guo et al\\.,? 2014", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Improving Word Representations via Global Context and Multiple Word Prototypes", "author": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Annual Meeting of the Association for Computational Linguistics (ACL).", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Sensembed: Learning sense embeddings for word and relational similarity", "author": ["Ignacio Iacobacci", "Mohammad Taher Pilehvar", "Roberto Navigli."], "venue": "ACL (1). pages 95\u2013105.", "citeRegEx": "Iacobacci et al\\.,? 2015", "shortCiteRegEx": "Iacobacci et al\\.", "year": 2015}, {"title": "Roget\u2019s thesaurus and semantic similarity", "author": ["Mario Jarmasz", "Stan Szpakowicz."], "venue": "Recent Advances in Natural Language Processing III: Selected Papers from RANLP 2003:111.", "citeRegEx": "Jarmasz and Szpakowicz.,? 2004", "shortCiteRegEx": "Jarmasz and Szpakowicz.", "year": 2004}, {"title": "Ontologically grounded multi-sense representation learning for semantic vector space models", "author": ["Sujay Kumar Jauhar", "Chris Dyer", "Eduard H Hovy."], "venue": "HLT-NAACL. pages 683\u2013693.", "citeRegEx": "Jauhar et al\\.,? 2015", "shortCiteRegEx": "Jauhar et al\\.", "year": 2015}, {"title": "Neural context embeddings for automatic discovery of word senses", "author": ["Mikael K\u00e5geb\u00e4ck", "Fredrik Johansson", "Richard Johansson", "Devdatt Dubhashi."], "venue": "Proceedings of NAACL-HLT . pages 25\u201332.", "citeRegEx": "K\u00e5geb\u00e4ck et al\\.,? 2015", "shortCiteRegEx": "K\u00e5geb\u00e4ck et al\\.", "year": 2015}, {"title": "A solution to plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K Landauer", "Susan T Dumais."], "venue": "Psychological review 104(2):211.", "citeRegEx": "Landauer and Dumais.,? 1997", "shortCiteRegEx": "Landauer and Dumais.", "year": 1997}, {"title": "Molding cnns for text: non-linear, non-consecutive convolutions", "author": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola."], "venue": "EMNLP .", "citeRegEx": "Lei et al\\.,? 2015", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Rationalizing neural predictions", "author": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing .", "citeRegEx": "Lei et al\\.,? 2016", "shortCiteRegEx": "Lei et al\\.", "year": 2016}, {"title": "Do multi-sense embeddings improve natural language understanding", "author": ["Jiwei Li", "Dan Jurafsky"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Li and Jurafsky.,? \\Q2015\\E", "shortCiteRegEx": "Li and Jurafsky.", "year": 2015}, {"title": "Learning context-sensitive word embeddings with neural tensor skip-gram model", "author": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."], "venue": "IJCAI. pages 1284\u20131290.", "citeRegEx": "Liu et al\\.,? 2015a", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Topical word embeddings", "author": ["Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun."], "venue": "AAAI. pages 2418\u20132424.", "citeRegEx": "Liu et al\\.,? 2015b", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Association for Computational Linguistics", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of Workshop at ICLR .", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller."], "venue": "Communications of the ACM 38(11):39\u2013", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller."], "venue": "NIPS Deep Learning Workshop .", "citeRegEx": "Mnih et al\\.,? 2013", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Efficient nonparametric estimation of multiple embeddings per word in vector space", "author": ["Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language", "citeRegEx": "Neelakantan et al\\.,? 2014", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "volume 14, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "De-conflated semantic representations", "author": ["Mohammad Taher Pilehvar", "Nigel Collier."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing .", "citeRegEx": "Pilehvar and Collier.,? 2016", "shortCiteRegEx": "Pilehvar and Collier.", "year": 2016}, {"title": "Contextdependent sense embedding", "author": ["Lin Qiu", "Kewei Tu", "Yong Yu."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Qiu et al\\.,? 2016", "shortCiteRegEx": "Qiu et al\\.", "year": 2016}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Joseph Reisinger", "Raymond J Mooney."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Reisinger and Mooney.,? 2010", "shortCiteRegEx": "Reisinger and Mooney.", "year": 2010}, {"title": "Autoextend: Extending word embeddings to embeddings for synsets and lexemes", "author": ["Sascha Rothe", "Hinrich Sch\u00fctze."], "venue": "arXiv preprint arXiv:1507.01127 .", "citeRegEx": "Rothe and Sch\u00fctze.,? 2015", "shortCiteRegEx": "Rothe and Sch\u00fctze.", "year": 2015}, {"title": "The westbury lab wikipedia", "author": ["Cyrus Shaoul", "Chris Westbury"], "venue": null, "citeRegEx": "Shaoul and Westbury.,? \\Q2010\\E", "shortCiteRegEx": "Shaoul and Westbury.", "year": 2010}, {"title": "Bilingual learning of multi-sense embeddings with discrete autoencoders", "author": ["Simon \u0160uster", "Ivan Titov", "Gertjan van Noord."], "venue": "NAACL-HLT 2016 .", "citeRegEx": "\u0160uster et al\\.,? 2016", "shortCiteRegEx": "\u0160uster et al\\.", "year": 2016}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Richard S Sutton", "Andrew G Barto."], "venue": "MIT press Cambridge.", "citeRegEx": "Sutton and Barto.,? 1998", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "A probabilistic model for learning multi-prototype word embeddings", "author": ["Fei Tian", "Hanjun Dai", "Jiang Bian", "Bin Gao", "Rui Zhang", "Enhong Chen", "Tie-Yan Liu."], "venue": "COLING. pages 151\u2013160.", "citeRegEx": "Tian et al\\.,? 2014", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Mining the web for synonyms: Pmi-ir versus lsa on toefl", "author": ["Peter D Turney."], "venue": "European Conference on Machine Learning. Springer, pages 491\u2013502.", "citeRegEx": "Turney.,? 2001", "shortCiteRegEx": "Turney.", "year": 2001}, {"title": "K-embeddings: Learning conceptual embeddings for words using context", "author": ["Thuy Vu", "D Stott Parker."], "venue": "Proceedings of NAACL-HLT . pages 1262\u20131267.", "citeRegEx": "Vu and Parker.,? 2016", "shortCiteRegEx": "Vu and Parker.", "year": 2016}, {"title": "It makes sense: A wide-coverage word sense disambiguation system for free text", "author": ["Zhi Zhong", "Hwee Tou Ng."], "venue": "Proceedings of the ACL 2010 System Demonstrations. Association for Computational Linguistics, pages 78\u201383.", "citeRegEx": "Zhong and Ng.,? 2010", "shortCiteRegEx": "Zhong and Ng.", "year": 2010}], "referenceMentions": [{"referenceID": 25, "context": "Due to the theoretical inability to account for polysemy using a single embedding representation per word, multi-sense word representations are proposed to address the ambiguity issue using multiple embedding representations for different senses in a word (Reisinger and Mooney, 2010; Huang et al., 2012).", "startOffset": 256, "endOffset": 304}, {"referenceID": 5, "context": "Due to the theoretical inability to account for polysemy using a single embedding representation per word, multi-sense word representations are proposed to address the ambiguity issue using multiple embedding representations for different senses in a word (Reisinger and Mooney, 2010; Huang et al., 2012).", "startOffset": 256, "endOffset": 304}, {"referenceID": 20, "context": "Neelakantan et al. (2014) pointed out that, due to triangle inequality in vector space, if one word has two different senses but is restricted to one embedding, the sum of the distances between the word and its synonym in each sense would upper-bound the distance between the respective synonyms, which may be mutually irrelevant, in embedding space1.", "startOffset": 0, "endOffset": 26}, {"referenceID": 21, "context": "Under this framework, prior work focused on designing a single model to deliver both mechanisms (Neelakantan et al., 2014; Li and Jurafsky, 2015; Qiu et al., 2016).", "startOffset": 96, "endOffset": 163}, {"referenceID": 13, "context": "Under this framework, prior work focused on designing a single model to deliver both mechanisms (Neelakantan et al., 2014; Li and Jurafsky, 2015; Qiu et al., 2016).", "startOffset": 96, "endOffset": 163}, {"referenceID": 24, "context": "Under this framework, prior work focused on designing a single model to deliver both mechanisms (Neelakantan et al., 2014; Li and Jurafsky, 2015; Qiu et al., 2016).", "startOffset": 96, "endOffset": 163}, {"referenceID": 21, "context": "However, the previously proposed models introduce side-effects: 1) mixing word-level and sense-level tokens achieves efficient sense selection but introduces ambiguous word-level tokens during the representation learning process (Neelakantan et al., 2014; Li and Jurafsky, 2015), and 2) pure sense-level tokens prevent ambiguity from word-level tokens but require exponential time complexity when decoding a sense sequence (Qiu et al.", "startOffset": 229, "endOffset": 278}, {"referenceID": 13, "context": "However, the previously proposed models introduce side-effects: 1) mixing word-level and sense-level tokens achieves efficient sense selection but introduces ambiguous word-level tokens during the representation learning process (Neelakantan et al., 2014; Li and Jurafsky, 2015), and 2) pure sense-level tokens prevent ambiguity from word-level tokens but require exponential time complexity when decoding a sense sequence (Qiu et al.", "startOffset": 229, "endOffset": 278}, {"referenceID": 24, "context": ", 2014; Li and Jurafsky, 2015), and 2) pure sense-level tokens prevent ambiguity from word-level tokens but require exponential time complexity when decoding a sense sequence (Qiu et al., 2016).", "startOffset": 175, "endOffset": 193}, {"referenceID": 5, "context": "With the power of deep learning, some work exploited neural networks to learn embeddings with sense selection based on clustering (Huang et al., 2012; Neelakantan et al., 2014).", "startOffset": 130, "endOffset": 176}, {"referenceID": 21, "context": "With the power of deep learning, some work exploited neural networks to learn embeddings with sense selection based on clustering (Huang et al., 2012; Neelakantan et al., 2014).", "startOffset": 130, "endOffset": 176}, {"referenceID": 19, "context": "(2014) replaced the clustering procedure with a word sense disambiguation model using WordNet (Miller, 1995).", "startOffset": 94, "endOffset": 108}, {"referenceID": 18, "context": "(2016) developed a non-parametric Bayesian extension on the skip-gram model (Mikolov et al., 2013b).", "startOffset": 76, "endOffset": 99}, {"referenceID": 0, "context": "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model using WordNet (Miller, 1995).", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model using WordNet (Miller, 1995). K\u00e5geb\u00e4ck et al. (2015) and Vu and Parker (2016) further leveraged a weighting mechanism and interactive process in the clustering procedure.", "startOffset": 0, "endOffset": 145}, {"referenceID": 0, "context": "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model using WordNet (Miller, 1995). K\u00e5geb\u00e4ck et al. (2015) and Vu and Parker (2016) further leveraged a weighting mechanism and interactive process in the clustering procedure.", "startOffset": 0, "endOffset": 170}, {"referenceID": 0, "context": "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model using WordNet (Miller, 1995). K\u00e5geb\u00e4ck et al. (2015) and Vu and Parker (2016) further leveraged a weighting mechanism and interactive process in the clustering procedure. Moreover, Guo et al. (2014) leveraged bilingual resources for clustering.", "startOffset": 0, "endOffset": 291}, {"referenceID": 0, "context": "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model using WordNet (Miller, 1995). K\u00e5geb\u00e4ck et al. (2015) and Vu and Parker (2016) further leveraged a weighting mechanism and interactive process in the clustering procedure. Moreover, Guo et al. (2014) leveraged bilingual resources for clustering. However, most of the above approaches separated the clustering procedure and the representation learning procedure without a joint objective, which may suffer from the error propagation issue. Instead, the proposed approach, MUSE, enables joint training on sense selection and representation learning. Instead of clustering, probabilistic modeling methods have been applied for learning multisense embeddings in order to make the sense selection more flexible, where Tian et al. (2014) and Jauhar et al.", "startOffset": 0, "endOffset": 823}, {"referenceID": 0, "context": "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model using WordNet (Miller, 1995). K\u00e5geb\u00e4ck et al. (2015) and Vu and Parker (2016) further leveraged a weighting mechanism and interactive process in the clustering procedure. Moreover, Guo et al. (2014) leveraged bilingual resources for clustering. However, most of the above approaches separated the clustering procedure and the representation learning procedure without a joint objective, which may suffer from the error propagation issue. Instead, the proposed approach, MUSE, enables joint training on sense selection and representation learning. Instead of clustering, probabilistic modeling methods have been applied for learning multisense embeddings in order to make the sense selection more flexible, where Tian et al. (2014) and Jauhar et al. (2015) conducted probabilistic modeling with EM training.", "startOffset": 0, "endOffset": 848}, {"referenceID": 0, "context": "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model using WordNet (Miller, 1995). K\u00e5geb\u00e4ck et al. (2015) and Vu and Parker (2016) further leveraged a weighting mechanism and interactive process in the clustering procedure. Moreover, Guo et al. (2014) leveraged bilingual resources for clustering. However, most of the above approaches separated the clustering procedure and the representation learning procedure without a joint objective, which may suffer from the error propagation issue. Instead, the proposed approach, MUSE, enables joint training on sense selection and representation learning. Instead of clustering, probabilistic modeling methods have been applied for learning multisense embeddings in order to make the sense selection more flexible, where Tian et al. (2014) and Jauhar et al. (2015) conducted probabilistic modeling with EM training. Li and Jurafsky (2015) exploited Chinese Restaurant Process to infer the sense identity.", "startOffset": 0, "endOffset": 922}, {"referenceID": 0, "context": "Furthermore, Bartunov et al. (2016) developed a non-parametric Bayesian extension on the skip-gram model (Mikolov et al.", "startOffset": 13, "endOffset": 36}, {"referenceID": 24, "context": "Recently, Qiu et al. (2016) proposed an EM algorithm to learn purely sense-level representations, where the computational cost is high when decoding the sense identity sequence, because it takes exponential time to search all sense combination within a context window.", "startOffset": 10, "endOffset": 28}, {"referenceID": 23, "context": "Unlike a lot of relevant work that requires additional resources such as the lexical ontology (Pilehvar and Collier, 2016; Rothe and Sch\u00fctze, 2015; Jauhar et al., 2015; Chen et al., 2015; Iacobacci et al., 2015) or bilingual data (Guo et al.", "startOffset": 94, "endOffset": 211}, {"referenceID": 26, "context": "Unlike a lot of relevant work that requires additional resources such as the lexical ontology (Pilehvar and Collier, 2016; Rothe and Sch\u00fctze, 2015; Jauhar et al., 2015; Chen et al., 2015; Iacobacci et al., 2015) or bilingual data (Guo et al.", "startOffset": 94, "endOffset": 211}, {"referenceID": 8, "context": "Unlike a lot of relevant work that requires additional resources such as the lexical ontology (Pilehvar and Collier, 2016; Rothe and Sch\u00fctze, 2015; Jauhar et al., 2015; Chen et al., 2015; Iacobacci et al., 2015) or bilingual data (Guo et al.", "startOffset": 94, "endOffset": 211}, {"referenceID": 1, "context": "Unlike a lot of relevant work that requires additional resources such as the lexical ontology (Pilehvar and Collier, 2016; Rothe and Sch\u00fctze, 2015; Jauhar et al., 2015; Chen et al., 2015; Iacobacci et al., 2015) or bilingual data (Guo et al.", "startOffset": 94, "endOffset": 211}, {"referenceID": 6, "context": "Unlike a lot of relevant work that requires additional resources such as the lexical ontology (Pilehvar and Collier, 2016; Rothe and Sch\u00fctze, 2015; Jauhar et al., 2015; Chen et al., 2015; Iacobacci et al., 2015) or bilingual data (Guo et al.", "startOffset": 94, "endOffset": 211}, {"referenceID": 4, "context": ", 2015) or bilingual data (Guo et al., 2014; Ettinger et al., 2016; \u0160uster et al., 2016), which may be unavailable in some language, our model can be trained using only an unlabeled corpus.", "startOffset": 26, "endOffset": 88}, {"referenceID": 3, "context": ", 2015) or bilingual data (Guo et al., 2014; Ettinger et al., 2016; \u0160uster et al., 2016), which may be unavailable in some language, our model can be trained using only an unlabeled corpus.", "startOffset": 26, "endOffset": 88}, {"referenceID": 28, "context": ", 2015) or bilingual data (Guo et al., 2014; Ettinger et al., 2016; \u0160uster et al., 2016), which may be unavailable in some language, our model can be trained using only an unlabeled corpus.", "startOffset": 26, "endOffset": 88}, {"referenceID": 17, "context": "The architecture is similar to continuous bag-of-words (CBOW) (Mikolov et al., 2013a).", "startOffset": 62, "endOffset": 85}, {"referenceID": 24, "context": "The prior work using a single model with purely sense-level tokens (Qiu et al., 2016) requires exponential time to calculate the collocation energy for every possible combination of sense identities within a context window, O(n2m), for a single target sense.", "startOffset": 67, "endOffset": 85}, {"referenceID": 24, "context": "The prior work using a single model with purely sense-level tokens (Qiu et al., 2016) requires exponential time to calculate the collocation energy for every possible combination of sense identities within a context window, O(n2m), for a single target sense. Further, Qiu et al. (2016) took an additional sequence decoding step with quadratic time complexity O(n4mL), based on an exponential number n2m in the base unit.", "startOffset": 68, "endOffset": 286}, {"referenceID": 18, "context": "In this paper, we use the skip-gram formulation (Mikolov et al., 2013b) considering that it requires less training time, where only two sense identities are required for stochastic training.", "startOffset": 48, "endOffset": 71}, {"referenceID": 22, "context": "Other popular candidates, like GloVe (Pennington et al., 2014) and CBOW (Mikolov et al.", "startOffset": 37, "endOffset": 62}, {"referenceID": 17, "context": ", 2014) and CBOW (Mikolov et al., 2013a), require more sense identities to be selected as input and thus not suitable for our scenario.", "startOffset": 17, "endOffset": 40}, {"referenceID": 22, "context": "For example, GloVe (Pennington et al., 2014) takes computationally expensive collocation counting statistics for each token in a corpus as input, which requires sense selection for every occurrence of the target word across the whole corpus for a single optimization step.", "startOffset": 19, "endOffset": 44}, {"referenceID": 18, "context": "Instead of enumerating all possible collocated senses which is computationally expensive, we use the skip-gram objective (4) (Mikolov et al., 2013b) to approximate (3) as shown in the green block of Figure 1.", "startOffset": 125, "endOffset": 148}, {"referenceID": 30, "context": "In contrast, most related work using probabilistic modeling (Tian et al., 2014; Jauhar et al., 2015; Li and Jurafsky, 2015; Bartunov et al., 2016) binded sense representations with the sense selection mechanism, so efficient sense selection by leveraging wordlevel tokens can be achieved only at the cost of mixing word-level and sense-level tokens in their representation learning process.", "startOffset": 60, "endOffset": 146}, {"referenceID": 8, "context": "In contrast, most related work using probabilistic modeling (Tian et al., 2014; Jauhar et al., 2015; Li and Jurafsky, 2015; Bartunov et al., 2016) binded sense representations with the sense selection mechanism, so efficient sense selection by leveraging wordlevel tokens can be achieved only at the cost of mixing word-level and sense-level tokens in their representation learning process.", "startOffset": 60, "endOffset": 146}, {"referenceID": 13, "context": "In contrast, most related work using probabilistic modeling (Tian et al., 2014; Jauhar et al., 2015; Li and Jurafsky, 2015; Bartunov et al., 2016) binded sense representations with the sense selection mechanism, so efficient sense selection by leveraging wordlevel tokens can be achieved only at the cost of mixing word-level and sense-level tokens in their representation learning process.", "startOffset": 60, "endOffset": 146}, {"referenceID": 0, "context": "In contrast, most related work using probabilistic modeling (Tian et al., 2014; Jauhar et al., 2015; Li and Jurafsky, 2015; Bartunov et al., 2016) binded sense representations with the sense selection mechanism, so efficient sense selection by leveraging wordlevel tokens can be achieved only at the cost of mixing word-level and sense-level tokens in their representation learning process.", "startOffset": 60, "endOffset": 146}, {"referenceID": 14, "context": "In our experiment with |Zi| senses for word wi, we use (1/|Zi|) word-level unigram as sense-level unigram for efficiency and the 3/4-th power trick in Mikolov et al. (2013b). We note that our modular framework can easily maintain purely sense-level tokens with an arbitrary representation learning model.", "startOffset": 151, "endOffset": 174}, {"referenceID": 29, "context": "The learning algorithm can be viewed as a reinforcement learning algorithm solving a onestep Markov Decision Process (MDP) (Sutton and Barto, 1998), where the state, action, and reward correspond to context C\u0304t, sense zik, and collocation log likelihood log L\u0304(\u00b7) respectively.", "startOffset": 123, "endOffset": 147}, {"referenceID": 12, "context": "The objective is differentiable and supports stochastic optimization (Lei et al., 2016), which uses a stochastic sample zik for optimization.", "startOffset": 69, "endOffset": 87}, {"referenceID": 20, "context": "To address the above issues, we apply the Qlearning algorithm (Mnih et al., 2013).", "startOffset": 62, "endOffset": 81}, {"referenceID": 20, "context": "We further follow the convention of recent neural reinforcement learning by reducing the reward range to aid model training (Mnih et al., 2013).", "startOffset": 124, "endOffset": 143}, {"referenceID": 20, "context": "most literature adopted square loss to characterize the discrepancy between the target and estimated Q-values (Mnih et al., 2013).", "startOffset": 110, "endOffset": 129}, {"referenceID": 18, "context": "Furthermore, given that the collocation likelihood in (4) is an approximation to the original categorical distribution with a softmax function shown in (3) (Mikolov et al., 2013b), we revise the formulation by omitting the negative sampling term.", "startOffset": 156, "endOffset": 179}, {"referenceID": 21, "context": "The major distinction between our modular framework and two-stage clusteringrepresentation learning framework (Neelakantan et al., 2014; Vu and Parker, 2016) is that we establish a reward signal from the sense representation to the sense selection module to enable immediate and joint optimization.", "startOffset": 110, "endOffset": 157}, {"referenceID": 32, "context": "The major distinction between our modular framework and two-stage clusteringrepresentation learning framework (Neelakantan et al., 2014; Vu and Parker, 2016) is that we establish a reward signal from the sense representation to the sense selection module to enable immediate and joint optimization.", "startOffset": 110, "endOffset": 157}, {"referenceID": 21, "context": "Given a fitness estimation for each sense, exploiting the greedy sense is the most popular strategy for clustering algorithms (Neelakantan et al., 2014; K\u00e5geb\u00e4ck et al., 2015) and hard-EM algorithms (Qiu et al.", "startOffset": 126, "endOffset": 175}, {"referenceID": 9, "context": "Given a fitness estimation for each sense, exploiting the greedy sense is the most popular strategy for clustering algorithms (Neelakantan et al., 2014; K\u00e5geb\u00e4ck et al., 2015) and hard-EM algorithms (Qiu et al.", "startOffset": 126, "endOffset": 175}, {"referenceID": 24, "context": ", 2015) and hard-EM algorithms (Qiu et al., 2016; Jauhar et al., 2015) in literature.", "startOffset": 31, "endOffset": 70}, {"referenceID": 8, "context": ", 2015) and hard-EM algorithms (Qiu et al., 2016; Jauhar et al., 2015) in literature.", "startOffset": 31, "endOffset": 70}, {"referenceID": 29, "context": "The dilemma between exploring sub-optimal choices and exploiting the optimal choice is called exploration-exploitation trade-off in reinforcement learning (Sutton and Barto, 1998).", "startOffset": 155, "endOffset": 179}, {"referenceID": 20, "context": "\u2022 -Greedy: selects a random sense with probability, and adopts the greedy strategy otherwise (Mnih et al., 2013).", "startOffset": 93, "endOffset": 112}, {"referenceID": 27, "context": "Our model is trained on the April 2010 Wikipedia dump (Shaoul and Westbury, 2010), which contains approximately 1 billion tokens.", "startOffset": 54, "endOffset": 81}, {"referenceID": 16, "context": "For preprocessing, we convert all words to their lower cases, apply the Stanford tokenizer and the Stanford sentence tokenizer (Manning et al., 2014), and remove all sentences with less than 10 tokens.", "startOffset": 127, "endOffset": 149}, {"referenceID": 21, "context": "The number of senses per word in Q is set to 3 for fair comparison with prior work (Neelakantan et al., 2014).", "startOffset": 83, "endOffset": 109}, {"referenceID": 5, "context": "For fair comparison, we adopt the same vocabulary set as Huang et al. (2012) and Neelakantan et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 5, "context": "For fair comparison, we adopt the same vocabulary set as Huang et al. (2012) and Neelakantan et al. (2014). For preprocessing, we convert all words to their lower cases, apply the Stanford tokenizer and the Stanford sentence tokenizer (Manning et al.", "startOffset": 57, "endOffset": 107}, {"referenceID": 5, "context": "To evaluate the quality of the learned sense embeddings, we compute the similarity score between each word pair given their respective local contexts and compare with the human-judged score using Stanford\u2019s Contextual Word Similarities (SCWS) data (Huang et al., 2012).", "startOffset": 248, "endOffset": 268}, {"referenceID": 5, "context": "To evaluate the quality of the learned sense embeddings, we compute the similarity score between each word pair given their respective local contexts and compare with the human-judged score using Stanford\u2019s Contextual Word Similarities (SCWS) data (Huang et al., 2012). Specifically, given a list of word pairs with corresponding contexts, S = {(wi, C\u0304t, wj , C\u0304t\u2032)}, we calculate the Spearman\u2019s rank correlation \u03c1 between human-judged similarity and model similarity estimations. Two major contextual similarity estimations are introduced by Reisinger and Mooney (2010): AvgSimC and MaxSimC.", "startOffset": 249, "endOffset": 571}, {"referenceID": 0, "context": "8 Bartunov et al. (2016) 53.", "startOffset": 2, "endOffset": 25}, {"referenceID": 0, "context": "8 Bartunov et al. (2016) 53.8 61.2 Qiu et al. (2016) 64.", "startOffset": 2, "endOffset": 53}, {"referenceID": 5, "context": "The baselines for comparison include classic clustering methods (Huang et al., 2012; Neelakantan et al., 2014), EM algorithms (Tian et al.", "startOffset": 64, "endOffset": 110}, {"referenceID": 21, "context": "The baselines for comparison include classic clustering methods (Huang et al., 2012; Neelakantan et al., 2014), EM algorithms (Tian et al.", "startOffset": 64, "endOffset": 110}, {"referenceID": 30, "context": ", 2014), EM algorithms (Tian et al., 2014; Qiu et al., 2016; Bartunov et al., 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015)4, where all approaches are trained on the same corpus except Qiu et al.", "startOffset": 23, "endOffset": 83}, {"referenceID": 24, "context": ", 2014), EM algorithms (Tian et al., 2014; Qiu et al., 2016; Bartunov et al., 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015)4, where all approaches are trained on the same corpus except Qiu et al.", "startOffset": 23, "endOffset": 83}, {"referenceID": 0, "context": ", 2014), EM algorithms (Tian et al., 2014; Qiu et al., 2016; Bartunov et al., 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015)4, where all approaches are trained on the same corpus except Qiu et al.", "startOffset": 23, "endOffset": 83}, {"referenceID": 13, "context": ", 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015)4, where all approaches are trained on the same corpus except Qiu et al.", "startOffset": 40, "endOffset": 63}, {"referenceID": 0, "context": ", 2016; Bartunov et al., 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015)4, where all approaches are trained on the same corpus except Qiu et al. (2016) used more recent Wikipedia dumps.", "startOffset": 8, "endOffset": 166}, {"referenceID": 0, "context": ", 2016; Bartunov et al., 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015)4, where all approaches are trained on the same corpus except Qiu et al. (2016) used more recent Wikipedia dumps. The embedding sizes of all baselines are 300, except 50 in Huang et al. (2012). For every competitor with multiple settings, we report the best performance in each similarity measurement setting and show in Table 1.", "startOffset": 8, "endOffset": 279}, {"referenceID": 13, "context": "We run Li and Jurafsky (2015)\u2019s released code on our corpus for fair comparison.", "startOffset": 7, "endOffset": 30}, {"referenceID": 8, "context": "We further evaluate our model on synonym selection using multi-sense word representations (Jauhar et al., 2015).", "startOffset": 90, "endOffset": 111}, {"referenceID": 31, "context": "Three standard synonym selection datasets, ESL-50 (Turney, 2001), RD-300 (Jarmasz and Szpakowicz, 2004), and TOEFL-80 (Landauer and Dumais, 1997), are performed.", "startOffset": 50, "endOffset": 64}, {"referenceID": 7, "context": "Three standard synonym selection datasets, ESL-50 (Turney, 2001), RD-300 (Jarmasz and Szpakowicz, 2004), and TOEFL-80 (Landauer and Dumais, 1997), are performed.", "startOffset": 73, "endOffset": 103}, {"referenceID": 10, "context": "Three standard synonym selection datasets, ESL-50 (Turney, 2001), RD-300 (Jarmasz and Szpakowicz, 2004), and TOEFL-80 (Landauer and Dumais, 1997), are performed.", "startOffset": 118, "endOffset": 145}, {"referenceID": 8, "context": "For multi-sense representations system, it selects the synonym of the question word wQ using the maximum senselevel cosine similarity as a proxy of the semantic similarity (Jauhar et al., 2015).", "startOffset": 172, "endOffset": 193}, {"referenceID": 5, "context": "Our model is compared with the following baselines: 1) conventional word embeddings: global context vectors (Huang et al., 2012) and skipgram (Mikolov et al.", "startOffset": 108, "endOffset": 128}, {"referenceID": 18, "context": ", 2012) and skipgram (Mikolov et al., 2013b); 2) applying supervised word sense disambiguation using the", "startOffset": 21, "endOffset": 44}, {"referenceID": 33, "context": "IMS system and then applying skip-gram on disambiguated corpus (IMS+SG) (Zhong and Ng, 2010); 3) unsupervised sense embeddings: EM algorithm (Jauhar et al.", "startOffset": 72, "endOffset": 92}, {"referenceID": 8, "context": "IMS system and then applying skip-gram on disambiguated corpus (IMS+SG) (Zhong and Ng, 2010); 3) unsupervised sense embeddings: EM algorithm (Jauhar et al., 2015), multi-sense skipgram (MSSG) (Neelakantan et al.", "startOffset": 141, "endOffset": 162}, {"referenceID": 21, "context": ", 2015), multi-sense skipgram (MSSG) (Neelakantan et al., 2014), Chinese Restaurant Process (CPR) (Li and Jurafsky, 2015), and the MUSE models; 4) supervised sense embeddings with WordNet: retrofitting global context vectors (Retro-GC) and retrofitting skip-gram (Retro-SG) (Jauhar et al.", "startOffset": 37, "endOffset": 63}, {"referenceID": 13, "context": ", 2014), Chinese Restaurant Process (CPR) (Li and Jurafsky, 2015), and the MUSE models; 4) supervised sense embeddings with WordNet: retrofitting global context vectors (Retro-GC) and retrofitting skip-gram (Retro-SG) (Jauhar et al.", "startOffset": 42, "endOffset": 65}, {"referenceID": 8, "context": ", 2014), Chinese Restaurant Process (CPR) (Li and Jurafsky, 2015), and the MUSE models; 4) supervised sense embeddings with WordNet: retrofitting global context vectors (Retro-GC) and retrofitting skip-gram (Retro-SG) (Jauhar et al., 2015).", "startOffset": 218, "endOffset": 239}], "year": 2017, "abstractText": "This paper proposes to address the word sense ambiguity issue in an unsupervised manner, where word sense representations are learned along a word sense selection mechanism given contexts. Prior work about learning multi-sense embeddings suffered from either ambiguity of different-level embeddings or inefficient sense selection. The proposed modular framework, MUSE, implements flexible modules to optimize distinct mechanisms, achieving the first purely sense-level representation learning system with lineartime sense selection. We leverage reinforcement learning to enable joint training on the proposed modules, and introduce various exploration techniques on sense selection for better robustness. The experiments on benchmark data show that the proposed approach achieves the state-ofthe-art performance on synonym selection as well as on contextual word similarities in terms of MaxSimC.", "creator": "LaTeX with hyperref package"}}}