{"id": "1704.04675", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2017", "title": "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation", "abstract": "We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoder-decoder models for machine translation. We rely on graph-convolutional networks (GCNs), a recent class of neural networks developed for modeling graph-structured data. Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods. GCNs take word representations as input and produce word representations as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.", "histories": [["v1", "Sat, 15 Apr 2017 19:04:59 GMT  (52kb,D)", "http://arxiv.org/abs/1704.04675v1", null], ["v2", "Tue, 18 Apr 2017 15:02:17 GMT  (52kb,D)", "http://arxiv.org/abs/1704.04675v2", null], ["v3", "Fri, 28 Jul 2017 18:18:52 GMT  (56kb,D)", "http://arxiv.org/abs/1704.04675v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["joost bastings", "ivan titov", "wilker aziz", "diego marcheggiani", "khalil sima'an"], "accepted": true, "id": "1704.04675"}, "pdf": {"name": "1704.04675.pdf", "metadata": {"source": "CRF", "title": "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation", "authors": ["Joost Bastings", "Ivan Titov", "Wilker Aziz", "Diego Marcheggiani", "Khalil Sima\u2019an"], "emails": ["bastings@uva.nl", "titov@uva.nl", "w.aziz@uva.nl", "marcheggiani@uva.nl", "k.simaan@uva.nl"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who fight for the rights of women and men are fighting for equality between men and women."}, {"heading": "2 Background", "text": "Notation We use x for vectors, x1: t for a sequence of t vectors, and X for matrices. The i-th value of the vector x is denoted by xi."}, {"heading": "2.1 Neural Machine Translation", "text": "In NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b), a neural network is trained using sample translation pairs from a parallel corpus to estimate directly the conditional distribution p (y1: Ty | x1: Tx) of the translation of a source sentence x1: Tx (a sequence of Tx words) into a target sentence y1: Ty. NMT models typically consist of an encoder, a decoder, and a method for conditioning the decoder on the encoder, a mechanism of attention. We will now briefly describe the components we use in this paper."}, {"heading": "2.1.1 Encoders", "text": "An encoder is a function that takes the source set as input and generates a representation that encodes its semantic contents. We describe recurring, revolutionary, and bag-of-words encoders.Recurrent Recurrent Recurrent Neural Networks (RNNNs) (Elman, 1990) model of sequential data. They each receive an input vector and update their hidden state to summarize all input up to that point. Faced with an input sequence x1: Tx = x1, x2,.., xTx of the word embedding of an RNN is recursively defined as: RNN (x1: t) = f (xt, RNN \u2212 1), where f is a nonlinear function like an LSTM (Hochreiter and Schmidhuber, 1997) or a GRU (Cho et al, 2014b). We will use the RNN function as an abstract mapping from an input sequence x1: T to a hidden state."}, {"heading": "2.1.2 Decoder", "text": "In Bahdanau et al. (2015), the decoder is implemented as an RNN due to an additional input beci, the context vector, which is dynamically calculated at each step by means of an attention mechanism. The probability of a target word yi is now a function of the decoder-RNN state, the previous target word embedding and the context vector. The model is trained end-to-end to maximize the likelihood of logging the next target word due to its context."}, {"heading": "2.2 Graph Convolutional Networks", "text": "We will now describe the Graph Convolutional Networks (GCNs) by Kipf and Welling (2016). For a comprehensive overview of alternative GCN architectures, see Gilmer et al. (2017).A GCN is a multi-layer neural network that works directly on a graph and encodes information about the neighborhood of a node as a reality-rated vector. In each GCN layer, information flows along the edges of the graph; in other words, each node receives messages from all of its immediate neighbors. If multiple GCN layers are stacked, information about larger neighborhoods is integrated. In the second layer, for example, a node receives information from its immediate neighbors, but this information already contains information from their respective neighbors. By selecting the number of GCN layers, we regulate the distance of information travels: with k layers, a node will receive information from neighbors at the most k-hopps."}, {"heading": "2.3 Syntactic GCNs", "text": "This makes it possible to use linguistic structures such as dependency trees, where policies and edge markers play an important role. They also integrate edge-by-edge gates that let the model regulate the contributions of individual dependency borders. We follow the convention that heads point to their dependency borders in dependency trees, and therefore outgoing edges are used for head-to-head connections, and incoming edges are used for dependent and outgoing edges. We follow the convention that heads refer to their dependencies in dependency trees, and therefore outgoing edges are used for head-to-head connections, and incoming edges are used for dependency connections."}, {"heading": "3 Graph Convolutional Encoders", "text": "In this work, we focus on the use of structural information on the source side, i.e. in the encoder. We assume that the use of an encoder that contains syntax will lead to more informative representations of words, and that these representations, when used as context vectors, will lead to an improvement in translation quality. Consequently, in all our models we use the decoder from Bahdanau et al. (2015) and keep this part of the model constant. We do not use a maxout layer in the decoder, but apart from the fact that we do not deviate from the original definition."}, {"heading": "4 Experiments", "text": "Experiments are performed with the Neural Monkey Toolkit3 (Helcl and Libovicky, 2017), which is the model of Bahdanau et al. (2015) in TensorFlow.4 We use the Adam Optimizer (Kingma and Ba, 2015) with a learning rate of 0.001 (0.0002 for CNN models).5 The batch size is set to 80. Between the layers we use the dropout with a probability of 0.2, and in experiments with GCNs we use the same margin failure value. We train for 45 epochs and evaluate the BLEU performance of the model for each epoch on the validation set. For testing we select the model with the highest validation BLEU. The L2 regulation is set at a value of 10 \u2212 8. All model selection (including hyperparameter selection) was performed on the validation set. In all experiments we get translations with a greedy decoder, i.e. we choose the output kit with the highest probability of any experiment."}, {"heading": "4.1 Reordering artificial sequences", "text": "We encode the original order in the edges and test whether GCNs can successfully use this information.Data From a vocabulary of 26 types, we generate random sequences of 3-10 tokens. We then permutate them randomly and refer each token to its original predecessor with a label sampled from a set of 5 labels. Additionally, we point each token to any position in the sequence with a label of a unique set of 5 \"fake\" labels. We try 25,000 trainings and 1000 validation sequences. Model We use the BiRNN + GCN model, i.e. a bidirectional GRU with a 1-layer GCN at the top. We use 32, 64 and 128 units for embedding, GRU units and GCN layers, each representing only one real GCN model, i.e. a bidirectional GRU with a 1-layer GCN at the top."}, {"heading": "4.2 Machine Translation", "text": "Data for our experiments we use the English-German and English-Czech news commentaries v11 data sets from the WMT16 news translation shared task.6 For validation and test sets we use newstest2015 and newstest2016, respectively news.6http: / / www.statmt.org / wmt16 / translation-task.htmlPre-processing The English pages of the corpora are tokenized by SyntaxNet, 7 using the trained Parsey McParseface model. The Czech and German pages are tokenized using the Moses tokenizer.9 sentence pairs where each page is longer than 50 words are tokenization.Vocabularies For the English pages we construct vocabularies from all words except those with a training frequency of less than three."}, {"heading": "4.2.1 Results", "text": "Unsurprisingly, the bag-of-words baseline delivers the worst results, but it still gets BLEU of 9.5. We expect the BoW + GCN model to make slight gains over that baseline, which is what actually happens. BLEU1 and BLEU4 results are up + 4.3 and + 2.7, respectively. Next, we look at the revolutionary EUEUN encoders. The CNN baseline achieves a higher BLEU4 result than the BoW models with 12.6, but interestingly, its BLEU1 result is lower than the BLEU1-BLEU1 and BLCN models shows that BLEUs baseline data over the CNN baseline still achieves an improvement of + 1.9 and + 1.1 for BLEU4 and BLEU4, respectively. Finally, the BiRNN, the strongest baseline, achieves a BLEU4 of 14.9. Interestingly, the BLEU4 of BLEU4 and BLCN models still achieve an improvement in the result."}, {"heading": "5 Related Work", "text": "We verify various syntax accounts in NMT as well as other convolutional encoders. Syntactic features and / or constraints Sennrich and Haddow (2016) embed features such as POS tags, lemmas, and dependency tags and feed them into the network along with word embeddings. Eriguchi et al. (2016) analyze English sentences with an HPSG parser and use a tree LSTM to encode the internal nodes of the tree. In the decoder, word and node representations compete under the same attention mechanism. Stahlberg et al. (2016) use a truncated grid from a hierarchical phrase model (hiero) to constrain NMT trees, but instead they are constrained by symmetrical word alignments."}, {"heading": "6 Conclusions", "text": "We presented a simple and effective approach to integrating syntax into neural machine translation models and demonstrated consistent BLEU4 improvements for two demanding language pairs: English-German and English-Czech. Since GCNs are capable of encoding any type of graph-based structure, we intend to go beyond syntax in future work by using semantic annotations such as SRL and AMR and co-reference chains."}, {"heading": "Acknowledgments", "text": "This work was supported by the Dutch Organisation for Scientific Research (NWO) VICI Grant nr. 277-89-002."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR). http://arxiv.org/abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Abstract meaning representation (amr) 1.0 specification", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider"], "venue": "In Conference on Empiri-", "citeRegEx": "Banarescu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Banarescu et al\\.", "year": 2012}, {"title": "Learning to translate with source and target syntax", "author": ["David Chiang."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Uppsala, Sweden, pages 1443\u20131452.", "citeRegEx": "Chiang.,? 2010", "shortCiteRegEx": "Chiang.", "year": 2010}, {"title": "On the Properties of Neural Machine Translation: EncoderDecoder Approaches", "author": ["KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Better hypothesis testing for statistical machine translation: Controlling for optimizer instability", "author": ["Jonathan H. Clark", "Chris Dyer", "Alon Lavie", "Noah A. Smith."], "venue": "Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Clark et al\\.,? 2011", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "Convolutional neural networks on graphs with fast localized spectral filtering", "author": ["Micha\u00ebl Defferrard", "Xavier Bresson", "Pierre Vandergheynst."], "venue": "Advances in Neural Information Processing Systems 29: Annual Conference on Neural In-", "citeRegEx": "Defferrard et al\\.,? 2016", "shortCiteRegEx": "Defferrard et al\\.", "year": 2016}, {"title": "Convolutional networks on graphs for learning molecular fingerprints", "author": ["David K Duvenaud", "Dougal Maclaurin", "Jorge Iparraguirre", "Rafael Bombarell", "Timothy Hirzel", "Al\u00e1n Aspuru-Guzik", "Ryan P Adams."], "venue": "Advances in neural information pro-", "citeRegEx": "Duvenaud et al\\.,? 2015", "shortCiteRegEx": "Duvenaud et al\\.", "year": 2015}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman."], "venue": "Cognitive science 14(2):179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Tree-to-sequence attentional neural machine translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Eriguchi et al\\.,? 2016", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "Learning to Parse and Translate Improves Neural Machine Translation", "author": ["Akiko Eriguchi", "Yoshimasa Tsuruoka", "Kyunghyun Cho."], "venue": "ArXiv e-prints .", "citeRegEx": "Eriguchi et al\\.,? 2017", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2017}, {"title": "A convolutional encoder model for neural machine translation", "author": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin."], "venue": "CoRR abs/1611.02344. http://arxiv.org/abs/1611.02344.", "citeRegEx": "Gehring et al\\.,? 2016", "shortCiteRegEx": "Gehring et al\\.", "year": 2016}, {"title": "Neural Message Passing for Quantum Chemistry", "author": ["Justin Gilmer", "Samuel S. Schoenholz", "Patrick F. Riley", "Oriol Vinyals", "George E. Dahl."], "venue": "ArXiv eprints .", "citeRegEx": "Gilmer et al\\.,? 2017", "shortCiteRegEx": "Gilmer et al\\.", "year": 2017}, {"title": "Neural machine translation with source-side latent graph parsing", "author": ["Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "CoRR abs/1702.02265. http://arxiv.org/abs/1702.02265.", "citeRegEx": "Hashimoto and Tsuruoka.,? 2017", "shortCiteRegEx": "Hashimoto and Tsuruoka.", "year": 2017}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 770\u2013778.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Neural monkey: An open-source tool for sequence learning", "author": ["Jind\u0159ich Helcl", "Jind\u0159ich Libovick\u00fd."], "venue": "The Prague Bulletin of Mathematical Linguistics (107):5\u201317. https://doi.org/10.1515/pralin2017-0001.", "citeRegEx": "Helcl and Libovick\u00fd.,? 2017", "shortCiteRegEx": "Helcl and Libovick\u00fd.", "year": 2017}, {"title": "Opinion Mining with Deep Recurrent Neural Networks", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa-", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "Recurrent Continuous Translation Models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Seattle, Washington, USA, pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Neural machine translation in linear time", "author": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "A\u00e4ron van den Oord", "Alex Graves", "Koray Kavukcuoglu."], "venue": "CoRR abs/1610.10099. http://arxiv.org/abs/1610.10099.", "citeRegEx": "Kalchbrenner et al\\.,? 2016", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Molecular graph convolutions: moving beyond fingerprints", "author": ["Steven Kearnes", "Kevin McCloskey", "Marc Berndl", "Vijay Pande", "Patrick Riley."], "venue": "Journal of computer-aided molecular design 30(8):595\u2013608.", "citeRegEx": "Kearnes et al\\.,? 2016", "shortCiteRegEx": "Kearnes et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "ICLR. http://arxiv.org/abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Semisupervised classification with graph convolutional networks", "author": ["Thomas N. Kipf", "Max Welling."], "venue": "CoRR abs/1609.02907. http://arxiv.org/abs/1609.02907.", "citeRegEx": "Kipf and Welling.,? 2016", "shortCiteRegEx": "Kipf and Welling.", "year": 2016}, {"title": "Assessing the ability of lstms to learn syntaxsensitive dependencies", "author": ["Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg."], "venue": "Transactions of the Association for Computational Linguistics 4:521\u2013535. https://www.transacl.org/ojs/index.php/tacl/article/view/972.", "citeRegEx": "Linzen et al\\.,? 2016", "shortCiteRegEx": "Linzen et al\\.", "year": 2016}, {"title": "Multitask Sequence to Sequence Learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "CoRR abs/1511.06114. http://arxiv.org/abs/1511.06114.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Effective Approaches to Attentionbased Neural Machine Translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling", "author": ["Diego Marcheggiani", "Ivan Titov."], "venue": "ArXiv e-prints https://arxiv.org/abs/1703.04826.", "citeRegEx": "Marcheggiani and Titov.,? 2017", "shortCiteRegEx": "Marcheggiani and Titov.", "year": 2017}, {"title": "Syntax-aware Neural Machine Translation Using CCG", "author": ["Maria Nadejde", "Siva Reddy", "Rico Sennrich", "Tomasz Dwojak", "Marcin Junczys-Dowmunt", "Philipp Koehn", "Alexandra Birch."], "venue": "ArXiv e-prints .", "citeRegEx": "Nadejde et al\\.,? 2017", "shortCiteRegEx": "Nadejde et al\\.", "year": 2017}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei jing Zhu."], "venue": "pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Modeling Relational Data with Graph Convolutional Networks", "author": ["Michael Schlichtkrull", "Thomas N Kipf", "Peter Bloem", "Rianne van den Berg", "Ivan Titov", "Max Welling."], "venue": "ArXiv e-prints .", "citeRegEx": "Schlichtkrull et al\\.,? 2017", "shortCiteRegEx": "Schlichtkrull et al\\.", "year": 2017}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K. Paliwal."], "venue": "IEEE Transactions on Signal Processing 45(11):2673\u20132681. https://doi.org/10.1109/78.650093.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Linguistic Input Features Improve Neural Machine Translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "Proceedings of the First Conference on Machine Translation (WMT16). volume abs/1606.02892. http://arxiv.org/abs/1606.02892.", "citeRegEx": "Sennrich and Haddow.,? 2016", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2016}, {"title": "Edinburgh neural machine translation systems for wmt 16", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the First Conference on Machine Translation. Association for Computational Linguistics, Berlin, Germany, pages 371\u2013", "citeRegEx": "Sennrich et al\\.,? 2016a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Sennrich et al\\.,? 2016b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Does string-based neural mt learn source syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "author": ["Xing Shi", "Inkit Padhi", "Kevin Knight."], "venue": "Association for Computational Linguistics, Austin, Texas, pages", "citeRegEx": "Shi et al\\.,? 2016", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "Quasisynchronous grammars: Alignment by soft projection of syntactic dependencies", "author": ["David Smith", "Jason Eisner."], "venue": "Proceedings on the Workshop on Statistical Machine Translation. Association for Computa-", "citeRegEx": "Smith and Eisner.,? 2006", "shortCiteRegEx": "Smith and Eisner.", "year": 2006}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."], "venue": "In Proceedings of Association for Machine Translation in the Americas. pages 223\u2013231.", "citeRegEx": "Snover et al\\.,? 2006", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Syntactically guided neural machine translation", "author": ["Felix Stahlberg", "Eva Hasler", "Aurelien Waite", "Bill Byrne."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computa-", "citeRegEx": "Stahlberg et al\\.,? 2016", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2016}, {"title": "Fitting sentence level translation evaluation with many dense features", "author": ["Milo\u0161 Stanojevi\u0107", "Khalil Sima\u2019an"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Stanojevi\u0107 and Sima.an.,? \\Q2014\\E", "shortCiteRegEx": "Stanojevi\u0107 and Sima.an.", "year": 2014}, {"title": "The conll 2008 shared task on joint parsing of syntactic and semantic dependencies", "author": ["Mihai Surdeanu", "Richard Johansson", "Adam Meyers", "Llu\u0131\u0301s M\u00e0rquez", "Joakim Nivre"], "venue": "In Proceedings of CoNLL", "citeRegEx": "Surdeanu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2008}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Neural Information Processing Systems (NIPS). pages 3104\u20133112. http://papers.nips.cc/paper/5346-sequence-to-", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Learning to compose words into sentences with reinforcement learning", "author": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling."], "venue": "CoRR abs/1611.09100. http://arxiv.org/abs/1611.09100.", "citeRegEx": "Yogatama et al\\.,? 2016", "shortCiteRegEx": "Yogatama et al\\.", "year": 2016}, {"title": "Syntax augmented machine translation via chart parsing", "author": ["Andreas Zollmann", "Ashish Venugopal."], "venue": "Proceedings of the Workshop on Statistical Machine Translation. Association for Computational Linguistics, Strouds-", "citeRegEx": "Zollmann and Venugopal.,? 2006", "shortCiteRegEx": "Zollmann and Venugopal.", "year": 2006}], "referenceMentions": [{"referenceID": 32, "context": "Neural machine translation (NMT) is one of success stories of deep learning in natural language processing, with recent NMT systems outperforming traditional phrase-based approaches on many language pairs (Sennrich et al., 2016a).", "startOffset": 205, "endOffset": 229}, {"referenceID": 41, "context": "State-ofthe-art NMT systems rely on sequential encoderdecoders (Sutskever et al., 2014; Bahdanau et al., 2015) and lack any explicit modeling of syntax or any hierarchical structure of language.", "startOffset": 63, "endOffset": 110}, {"referenceID": 0, "context": "State-ofthe-art NMT systems rely on sequential encoderdecoders (Sutskever et al., 2014; Bahdanau et al., 2015) and lack any explicit modeling of syntax or any hierarchical structure of language.", "startOffset": 63, "endOffset": 110}, {"referenceID": 24, "context": ", multi-task learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.", "startOffset": 22, "endOffset": 118}, {"referenceID": 27, "context": ", multi-task learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.", "startOffset": 22, "endOffset": 118}, {"referenceID": 11, "context": ", multi-task learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.", "startOffset": 22, "endOffset": 118}, {"referenceID": 14, "context": ", multi-task learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.", "startOffset": 22, "endOffset": 118}, {"referenceID": 10, "context": ", learning representations of linguistic phrases (Eriguchi et al., 2016)).", "startOffset": 49, "endOffset": 72}, {"referenceID": 43, "context": "This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT.", "startOffset": 84, "endOffset": 152}, {"referenceID": 35, "context": "This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT.", "startOffset": 84, "endOffset": 152}, {"referenceID": 2, "context": "This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT.", "startOffset": 84, "endOffset": 152}, {"referenceID": 0, "context": "Attention-based NMT systems (Bahdanau et al., 2015; Luong et al., 2015b) represent source sentence words as latent-feature vectors in the encoder and use these vectors when generating a translation.", "startOffset": 28, "endOffset": 72}, {"referenceID": 25, "context": "Attention-based NMT systems (Bahdanau et al., 2015; Luong et al., 2015b) represent source sentence words as latent-feature vectors in the encoder and use these vectors when generating a translation.", "startOffset": 28, "endOffset": 72}, {"referenceID": 7, "context": "In order to produce syntax-aware feature representations of words, we exploit graphconvolutional networks (GCNs) (Duvenaud et al., 2015; Defferrard et al., 2016; Kearnes et al., 2016; Kipf and Welling, 2016).", "startOffset": 113, "endOffset": 207}, {"referenceID": 6, "context": "In order to produce syntax-aware feature representations of words, we exploit graphconvolutional networks (GCNs) (Duvenaud et al., 2015; Defferrard et al., 2016; Kearnes et al., 2016; Kipf and Welling, 2016).", "startOffset": 113, "endOffset": 207}, {"referenceID": 20, "context": "In order to produce syntax-aware feature representations of words, we exploit graphconvolutional networks (GCNs) (Duvenaud et al., 2015; Defferrard et al., 2016; Kearnes et al., 2016; Kipf and Welling, 2016).", "startOffset": 113, "endOffset": 207}, {"referenceID": 22, "context": "In order to produce syntax-aware feature representations of words, we exploit graphconvolutional networks (GCNs) (Duvenaud et al., 2015; Defferrard et al., 2016; Kearnes et al., 2016; Kipf and Welling, 2016).", "startOffset": 113, "endOffset": 207}, {"referenceID": 13, "context": "nodes at most k hops aways from the node) (Gilmer et al., 2017).", "startOffset": 42, "endOffset": 63}, {"referenceID": 26, "context": "We use Syntactic GCNs, a version of GCN operating on top of syntactic dependency trees, recently shown effective in the context of semantic role labeling (Marcheggiani and Titov, 2017).", "startOffset": 154, "endOffset": 184}, {"referenceID": 12, "context": "Though GCNs can take word embeddings as input, we will see that they are more effective when used as layers on top of recurrent neural network (RNN) or convolutional neural network (CNN) encoders (Gehring et al., 2016), enriching their states with syntactic information.", "startOffset": 196, "endOffset": 218}, {"referenceID": 23, "context": ", subject-verb agreement) reasonably well on their own, without explicit treebank supervision (Linzen et al., 2016; Shi et al., 2016).", "startOffset": 94, "endOffset": 133}, {"referenceID": 34, "context": ", subject-verb agreement) reasonably well on their own, without explicit treebank supervision (Linzen et al., 2016; Shi et al., 2016).", "startOffset": 94, "endOffset": 133}, {"referenceID": 40, "context": ", dependency-based semantic-role labeling representations (Surdeanu et al., 2008), AMR semantic graphs (Banarescu et al.", "startOffset": 58, "endOffset": 81}, {"referenceID": 1, "context": ", 2008), AMR semantic graphs (Banarescu et al., 2012) and co-reference chains).", "startOffset": 29, "endOffset": 53}, {"referenceID": 37, "context": "For example, unlike recursive neural networks (Socher et al., 2013), GCNs do not require the graphs to be trees.", "startOffset": 46, "endOffset": 67}, {"referenceID": 18, "context": "In NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b), given example translation pairs from a parallel corpus, a neural network is trained to directly estimate the conditional distribution p(y1:Ty |x1:Tx) of translating a source sentence x1:Tx (a sequence of Tx words) into a target sentence y1:Ty .", "startOffset": 7, "endOffset": 82}, {"referenceID": 41, "context": "In NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b), given example translation pairs from a parallel corpus, a neural network is trained to directly estimate the conditional distribution p(y1:Ty |x1:Tx) of translating a source sentence x1:Tx (a sequence of Tx words) into a target sentence y1:Ty .", "startOffset": 7, "endOffset": 82}, {"referenceID": 9, "context": "Recurrent Recurrent neural networks (RNNs) (Elman, 1990) model sequential data.", "startOffset": 43, "endOffset": 56}, {"referenceID": 0, "context": "For further details we refer to the encoder of Bahdanau et al. (2015).", "startOffset": 47, "endOffset": 70}, {"referenceID": 12, "context": "Convolutional Convolutional Neural Networks (CNNs) apply a fixed-size window over the input sequence to capture the local context of each word (Gehring et al., 2016).", "startOffset": 143, "endOffset": 165}, {"referenceID": 0, "context": "In Bahdanau et al. (2015) the decoder is implemented as an RNN conditioned on an additional input ci, the context vector, which is dynamically computed at each time step using an attention mechanism.", "startOffset": 3, "endOffset": 26}, {"referenceID": 21, "context": "We will now describe the Graph Convolutional Networks (GCNs) of Kipf and Welling (2016). For a comprehensive overview of alternative GCN architectures see Gilmer et al.", "startOffset": 64, "endOffset": 88}, {"referenceID": 13, "context": "For a comprehensive overview of alternative GCN architectures see Gilmer et al. (2017).", "startOffset": 66, "endOffset": 87}, {"referenceID": 22, "context": "We dropped the normalization factor used by Kipf and Welling (2016), as it is not used in syntactic GCNs of Marcheggiani and Titov (2017).", "startOffset": 44, "endOffset": 68}, {"referenceID": 22, "context": "We dropped the normalization factor used by Kipf and Welling (2016), as it is not used in syntactic GCNs of Marcheggiani and Titov (2017).", "startOffset": 44, "endOffset": 138}, {"referenceID": 29, "context": "For an alternative approach to integrating labels and directions, see applications of GCNs to statistical relation learning (Schlichtkrull et al., 2017).", "startOffset": 124, "endOffset": 152}, {"referenceID": 0, "context": "Consequently, in all our models, we use the decoder of Bahdanau et al. (2015) and keep this part of the model constant.", "startOffset": 55, "endOffset": 78}, {"referenceID": 12, "context": "Instead of the approach used by Gehring et al. (2016) (i.", "startOffset": 32, "endOffset": 54}, {"referenceID": 23, "context": "This is the most challenging setup for GCNs, as RNNs have been shown capable of capturing at least some degree of syntactic information without explicit supervision (Linzen et al., 2016), and hence they should be hard to improve on by incorporating treebank syntax.", "startOffset": 165, "endOffset": 186}, {"referenceID": 15, "context": "To ease optimization, we add a residual connection (He et al., 2016) between the GCN layers, when using more than one layer.", "startOffset": 51, "endOffset": 68}, {"referenceID": 16, "context": "Experiments are performed using the Neural Monkey toolkit3 (Helcl and Libovick\u00fd, 2017), which implements the model of Bahdanau et al.", "startOffset": 59, "endOffset": 86}, {"referenceID": 21, "context": "4 We use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.", "startOffset": 28, "endOffset": 49}, {"referenceID": 0, "context": "Experiments are performed using the Neural Monkey toolkit3 (Helcl and Libovick\u00fd, 2017), which implements the model of Bahdanau et al. (2015) in TensorFlow.", "startOffset": 118, "endOffset": 141}, {"referenceID": 12, "context": "Like Gehring et al. (2016) we note that Adam is too aggressive for CNN models, hence we use a lower learning rate.", "startOffset": 5, "endOffset": 27}, {"referenceID": 32, "context": "For Czech and German, to deal with rare words and phenomena such as inflection and compounding, we learn byte-pair encodings (BPE) as described by Sennrich et al. (2016b). Given the size of our data set, and following Wu et al.", "startOffset": 147, "endOffset": 171}, {"referenceID": 32, "context": "For Czech and German, to deal with rare words and phenomena such as inflection and compounding, we learn byte-pair encodings (BPE) as described by Sennrich et al. (2016b). Given the size of our data set, and following Wu et al. (2016), we use 8000 BPE merges to obtain robust frequencies for our subword units.", "startOffset": 147, "endOffset": 235}, {"referenceID": 28, "context": "Evaluation We report BLEU results (Papineni et al., 2002) using MultEval (Clark et al.", "startOffset": 34, "endOffset": 57}, {"referenceID": 5, "context": ", 2002) using MultEval (Clark et al., 2011).", "startOffset": 23, "endOffset": 43}, {"referenceID": 36, "context": "TER (Snover et al., 2006) and BEER (Stanojevi\u0107 and Sima\u2019an, 2014) metrics, even though omitted due to space considerations, are consistent with the reported results.", "startOffset": 4, "endOffset": 25}, {"referenceID": 39, "context": ", 2006) and BEER (Stanojevi\u0107 and Sima\u2019an, 2014) metrics, even though omitted due to space considerations, are consistent with the reported results.", "startOffset": 17, "endOffset": 47}, {"referenceID": 29, "context": "Syntactic features and/or constraints Sennrich and Haddow (2016) embed features such as POS-tags, lemmas and dependency labels and feed these into the network along with word embeddings.", "startOffset": 38, "endOffset": 65}, {"referenceID": 10, "context": "Eriguchi et al. (2016) parse English sentences with an HPSG parser and use a Tree-LSTM to encode the internal nodes of the tree.", "startOffset": 0, "endOffset": 23}, {"referenceID": 10, "context": "Eriguchi et al. (2016) parse English sentences with an HPSG parser and use a Tree-LSTM to encode the internal nodes of the tree. In the decoder, word and node representations compete under the same attention mechanism. Stahlberg et al. (2016) use a pruned lattice from a hierarchical phrase-based model (hiero) to constrain NMT.", "startOffset": 0, "endOffset": 243}, {"referenceID": 8, "context": "(2017) multi-task with a target-side RNNG parser (Dyer et al., 2016) and improve on various language pairs with English on the target side.", "startOffset": 49, "endOffset": 68}, {"referenceID": 21, "context": "Luong et al. (2015a) predict linearized constituency parses as an additional task.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "Eriguchi et al. (2017) multi-task with a target-side RNNG parser (Dyer et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 8, "context": "(2017) multi-task with a target-side RNNG parser (Dyer et al., 2016) and improve on various language pairs with English on the target side. Nadejde et al. (2017) multi-task with CCG tagging, and also integrate syntax on the target side by predicting a sequence of words interleaved with CCG supertags.", "startOffset": 50, "endOffset": 162}, {"referenceID": 14, "context": "Latent structure Hashimoto and Tsuruoka (2017) add a syntactically-inspired encoder on top of a BiLSTM layer.", "startOffset": 17, "endOffset": 47}, {"referenceID": 14, "context": "Latent structure Hashimoto and Tsuruoka (2017) add a syntactically-inspired encoder on top of a BiLSTM layer. They encode source words as a learned average of potential parents emulating a relaxed dependency tree. While their model is trained purely on translation data, they also experiment with pre-training the encoder using treebank annotation and report modest improvements on English-Japanese. Yogatama et al. (2016) introduce a model for language understanding and generation that composes words into sentences by inducing unlabeled binary bracketing trees without direct supervision.", "startOffset": 17, "endOffset": 423}, {"referenceID": 10, "context": "Convolutional encoders Gehring et al. (2016) show that CNNs can be competitive to BiRNNs when used as encoders.", "startOffset": 23, "endOffset": 45}, {"referenceID": 10, "context": "Convolutional encoders Gehring et al. (2016) show that CNNs can be competitive to BiRNNs when used as encoders. To increase the receptive field of a word\u2019s context they stack multiple CNN layers. Kalchbrenner et al. (2016) use convolution in both the encoder and the decoder; they make use of dilation to increase the receptive field.", "startOffset": 23, "endOffset": 223}, {"referenceID": 3, "context": "Finally, Cho et al. (2014a) propose a recursive convolutional neural network which builds a tree out of the word leaf nodes, but which ends up compressing the source sentence in a single vector.", "startOffset": 9, "endOffset": 28}], "year": 2017, "abstractText": "We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoderdecoder models for machine translation. We rely on graph-convolutional networks (GCNs), a recent class of neural networks developed for modeling graph-structured data. Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods. GCNs take word representations as input and produce word representations as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.", "creator": "LaTeX with hyperref package"}}}