{"id": "1704.04683", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2017", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations", "abstract": "We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of 28,000+ passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43%) and the ceiling human performance (95%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at", "histories": [["v1", "Sat, 15 Apr 2017 19:31:41 GMT  (47kb,D)", "https://arxiv.org/abs/1704.04683v1", null], ["v2", "Sun, 30 Apr 2017 15:47:40 GMT  (47kb,D)", "http://arxiv.org/abs/1704.04683v2", null], ["v3", "Sat, 10 Jun 2017 03:21:55 GMT  (47kb,D)", "http://arxiv.org/abs/1704.04683v3", null], ["v4", "Sat, 15 Jul 2017 18:48:57 GMT  (54kb,D)", "http://arxiv.org/abs/1704.04683v4", "EMNLP 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["guokun lai", "qizhe xie", "hanxiao liu", "yiming yang", "eduard h hovy"], "accepted": true, "id": "1704.04683"}, "pdf": {"name": "1704.04683.pdf", "metadata": {"source": "CRF", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations", "authors": ["Guokun Lai", "Qizhe Xie", "Hanxiao Liu", "Yiming Yang", "Eduard Hovy"], "emails": ["hovy}@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Building an intelligence agent capable of understanding text as humans is the biggest challenge in NLP research. With recent advances in deep learning techniques, it seems possible to achieve human performance in certain speech comprehension tasks, and an increase in effort has been devoted to the topic of machine comprehension, where humans aim to construct a system with the ability * shows that equivalent answers to questions related to a document to be understood (Chen et al., 2016; Kadlec et al., 2016; Yang et al., 2017) have been proposed towards this goal, allowing researchers to train deep learning systems and achieve results comparable to human performance."}, {"heading": "2 Related Work", "text": "In this section, we outline briefly existing data sets for machine reading comprehension, including their strengths and weaknesses."}, {"heading": "2.1 MCTest", "text": "MCTest (Richardson et al., 2013) is a popular dataset for answering questions in the same format as RACE, where each question with four candidate answers is linked to a single correct answer. Although questions in MCTest are of high quality through careful crowdsourcing, it contains only 500 shops and 2,000 questions, greatly limiting its use in training advanced machine understanding models. In addition, while MCTest is designed for seven-year-olds, RACE is designed for middle and high school students aged 12-18 and therefore more complicated and requires stronger thinking skills. In other words, RACE can be considered a larger and more difficult version of the MCTest dataset."}, {"heading": "2.2 Cloze-style datasets", "text": "In recent years, there have been several large-format data sets in the style of Cloze (Hermann et al., 2015; Hill et al., 2015; Bajgar et al., 2016; Onishi et al., 2016), whose questions are formulated by erasing a word or entity in a sentence. CNN / Daily Mail (Hermann et al., 2015) are the largest machine understanding data sets with 1.4 million questions, but both require limited thinking skills (Chen et al., 2016). In fact, the best machine data obtained by researchers (Chen et al., 2016; Dhingra et al., 2016) are constructed in a similar way to human performance on CNN / Daily Mail.Childrens Book Test (Hill et al., 2015) and Book Test (BT) (Bajgar et al., 2016). Each passage in CBT consists of 20 contiguous sentences drawn from children's books and the next (21st) sentence."}, {"heading": "2.3 Datasets with Span-based Answers", "text": "In datasets such as SQUAD (Rajpurkar et al., 2016), NEWSQA (Trischler et al., 2016) and MS MARCO (Nguyen et al., 2016), the answer to each question is in the form of a span of text in the article. The articles by SQUAD, NEWSQA and MS MARCO each come from Wikipedia, CNN News and the search engine Bing. The answer to a particular question may not be clear and could span several time periods. Instead of evaluating accuracy, researchers need to use F1-Score, BLEU (Papineni et al., 2002) or ROUGE (Lin and Hovy, 2003) as metrics that measure the overlap between the predictive and the basic truth responses, since the questions come without candidate spans. Datasets with span-based answers are a challenge, as the space of possible time periods is usually large. However, the limitation of the answers to Udips in the context of the NEA passage may be unrealistic and unrealistic once again for the intuitive ability to SUAD-4W3."}, {"heading": "2.4 Datasets from Examinations", "text": "For example, the AI2 Elementary School Science Questions (Khashabi et al., 2016) dataset contains 1080 questions for elementary school students; the NTCIR QA Lab (Shibuki et al., 2014) evaluates systems based on the task of solving questions about the entrance exam in the real world; the task of entrance exams at the CLEF QA Track (Pen und al., 2014; Rodrigo et al., 2015) evaluates the reading comprehension of the system. However, the data provided in these existing tasks is far from sufficient to learn advanced data-driven reading models, partly due to the costly process of data generation by human experts. To the best of knowledge, RACE is the first large-scale dataset of its kind."}, {"heading": "3 Data Analysis", "text": "In this section, we examine the nature of the questions dealt with in RACE at a detailed level. Specifically, we present the statistics of the data sets in Section 3.1, and then analyze the different argumentation / question types in RACE in the remaining subsections."}, {"heading": "3.1 Dataset Statistics", "text": "As mentioned in Section 1, RACE is collected from English exams for 12- to 15-year-old middle school students and 15- to 18-year-old high school students in China. To distinguish the two subgroups with a drastic difficulty gap, RACE-M refers to the middle school exams and RACE-H to the high school exams. We share 5% of the data as a development set and 5% as a test set for RACE-M and RACE-H. The number of samples in each set is shown in Table 2. Statistics for RACE-M and RACE-H are in Table 3. We can find that the length of passages and vocabulary size in RACE-H are much larger than those of RACE-M, evidence of the higher difficulty of high school exams. Note, however, that the articles and questions were selected to make Chinese students learning English as a foreign language easier to understand the vocabulary size and complexity of language constructs than other articles in Qaspedia and Wikipedia."}, {"heading": "3.2 Reasoning Types of the Questions", "text": "To get a comprehensive picture of the difficulty of the reasoning requirement of RACE, we perform human comments on question types. Following Chen et al. (2016); Trischler et al. (2016), we stratify the questions into five classes in ascending order of difficulty as follows: \u2022 Word comparison: The question matches exactly a span in the article. \u2022 Description: The question is associated with or paraphrased with exactly one sentence in the passage. \u2022 The answer can be extracted within the sentence. \u2022 One sentence of reasoning: The answer could be derived from a single sentence of the article by detecting incomplete information or conceptual overlaps. \u2022 Multi-sentence reasoning: The answer must be derived from the synthesis of information distributed across multiple sentences. \u2022 Insufficient / ambiguous: The question has no answer or the answer is not clearly related to the specified passage ACRACR. We refer readers to (Chen et 2016; Trischler-category, 2016) for each question."}, {"heading": "3.3 Subdividing Reasoning Types", "text": "In order to better understand our data set and facilitate future research, we list the subdivisions of the questions under the category of reasoning. We find the most common subdivisions of reasoning: Detailed reasoning, overall understanding, passage summary, postural analysis and world knowledge. A question can fall into several subdivisions. Definition of these subdivisions and related examples are as follows: 1. Detailed reasoning: To answer the question, the agent should be clear about the details of the passage. The answer appears in the passage, but it cannot be found by simply matching the question with the passage. For example, question 1 falls into this category in the sample passage. 2. Overall reasoning: The agent must understand the entire picture of the story in order to get the correct answer. For example, to answer question 2 in the sample passage, the agent is required to understand the entire story. 3. Passage summary: The question requires the agent to select the best summary of the four candidates."}, {"heading": "4 Collection Methodology", "text": "We have collected the raw data from three major free public sites 234 in China5, where reading comprehension problems are extracted from English exams designed by teachers in China. Pre-cleaning data includes a total of 137,918 passages and 519,878 questions, with 38,159 passages with 156,782 questions in the middle school group and 99,759 passages with 363,096 questions in the high school group. The following filter steps are performed to clean up the raw data. First, we remove any problems and questions that do not have the same format as our problem definition, for example, a question would be removed if the number of their options is not four. Second, we filter out any articles and questions that are not in the text information, i.e. we remove the articles and questions that contain images or tables. We also remove any questions that contain the keywords \"underlined\" or \"underlined paragraphs.\""}, {"heading": "5 Experiments", "text": "In this section, we compare the performance of several state-of-the-art reading comprehension models with human performance. We use accuracy as a metric to evaluate different models."}, {"heading": "5.1 Methods for Comparison", "text": "In fact, most of us will be able to retaliate, \"he said in an interview with the German Press Agency.\" We have to play by the rules, \"he said.\" We have to play by the rules that we have set ourselves. \""}, {"heading": "5.2 Human Evaluation", "text": "As described in Section 3.2, a randomly selected portion of the Amazon Turkers test set was labeled containing 500 questions, half of which came from RACE-H and the other half from RACE-M. Turks \"performance is 85% for RACE-M and 70% for RACE-H. However, it is hard to guarantee that every Turk will conduct the survey carefully, given the difficult and long passages of high school problems. So to get human performance on RACE, we manually labeled the percentage of valid questions. A question is valid if it is unique and has a correct answer. We found that 94.5% of the data that sets human performance is valid. Likewise, the peak performance on RACE-M and RACE-H is 95.4% and 94.2%, respectively."}, {"heading": "5.3 Main Results", "text": "We compare models and human ceiling performance on data sets that are not comparable to RACE. The compared data sets include RACE, MCTest, CNN / Daily Mail (CNN and DM), CBT, and WDW. Comparison shows that the missing tokens have either a common denominator (CBT-C) or a name (CBT-N), since the language models have already reached human performance on other levels (Hill et al., 2015). Comparison can be found in Table 5.Performance of the sliding window. First, we compare MCTest with the sliding window, where it is not possible to refer Stanford AR and Gated AR to MCTest."}, {"heading": "5.4 Reason Types Analysis", "text": "We evaluate people and models based on different questions presented in Figure 1. Turks perform best on text comparison problems, while they perform worst on thought problems. Sliding windows perform better on text comparison than problems that require reasoning or paraphrasing. Surprisingly, Stanford AR does not perform better than thought categories in the word comparison category. One possible reason for this is that the share of data in thought categories is greater than the share of data. Candidates \"answers to simple matching questions can also have similar word embeddings. For example, when it comes to color, it is difficult to distinguish candidate answers,\" green, \"\" red, \"\" blue, \"and\" yellow \"in the vector space. Similar performance in different categories also explains why the performance of neural models in middle and high school groups is close together in Table 5."}, {"heading": "6 Conclusion", "text": "We present a large, high-quality data set for reading comprehension, carefully designed to examine human capabilities in this task. RACE's desirable characteristics include broad domain / style coverage and the richness of the question format. Most importantly, it requires much more reasoning to succeed with RACE than with other data sets, as there is a significant gap between the performance of modern machine understanding models and that of humans. We hope that this data set will stimulate the development of more advanced machine understanding models."}, {"heading": "Acknowledgement", "text": "We would like to thank Graham Neubig for his design suggestions and Diyi Yang's help in sourcing the crowdsourced labels. This research was supported in part by the DARPA grant FA8750-12-2-0342, which was funded under the DEFT program."}, {"heading": "A Appendix", "text": "A.1 Example Question of passage Summary Section: Do you love holidays but hate to gain weight? You are not alone. Holidays are times to celebrate. Many people worry about their weight. However, if you plan properly, it is possible to maintain normal weight during the holidays. The idea is to enjoy the holidays but not to eat too much. You do not have to turn away from the foods you enjoy. Use a small plate that can encourage you to \"recharge.\" You should feel most comfortable eating a lot of food the size of your fist. Start with a small, low-fat meal or snack. This can help you not get upset about delicious foods. Top up on a soup and fruit or vegetables beforehand. Fill up soup and raw fruits or vegetables, or drink a large glass of water before you eat to feel full.Avoid fatty foods that are too fatty."}], "references": [{"title": "Embracing data abundance: Booktest dataset for reading comprehension", "author": ["Ondrej Bajgar", "Rudolf Kadlec", "Jan Kleindienst."], "venue": "arXiv preprint arXiv:1610.00956 .", "citeRegEx": "Bajgar et al\\.,? 2016", "shortCiteRegEx": "Bajgar et al\\.", "year": 2016}, {"title": "A thorough examination of the cnn/daily mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1606.02858 .", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Gated-attention readers for text comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "William W Cohen", "Ruslan Salakhutdinov."], "venue": "arXiv preprint arXiv:1606.01549 .", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1511.02301 .", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst."], "venue": "arXiv preprint arXiv:1603.01547 .", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Question answering via integer programming over semi-structured knowledge", "author": ["Daniel Khashabi", "Tushar Khot", "Ashish Sabharwal", "Peter Clark", "Oren Etzioni", "Dan Roth."], "venue": "arXiv preprint arXiv:1604.06076 .", "citeRegEx": "Khashabi et al\\.,? 2016", "shortCiteRegEx": "Khashabi et al\\.", "year": 2016}, {"title": "Automatic evaluation of summaries using n-gram cooccurrence statistics", "author": ["Chin-Yew Lin", "Eduard Hovy."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Hu-", "citeRegEx": "Lin and Hovy.,? 2003", "shortCiteRegEx": "Lin and Hovy.", "year": 2003}, {"title": "Ms marco: A human generated machine reading comprehension dataset", "author": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng."], "venue": "arXiv preprint arXiv:1611.09268 .", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Who did what: A large-scale person-centered cloze dataset", "author": ["Takeshi Onishi", "Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David McAllester."], "venue": "arXiv preprint arXiv:1608.05457 .", "citeRegEx": "Onishi et al\\.,? 2016", "shortCiteRegEx": "Onishi et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Overview of clef qa entrance exams task 2014", "author": ["Anselmo Pe\u00f1as", "Yusuke Miyao", "\u00c1lvaro Rodrigo", "Eduard H Hovy", "Noriko Kando."], "venue": "CLEF (Working Notes). pages 1194\u20131200.", "citeRegEx": "Pe\u00f1as et al\\.,? 2014", "shortCiteRegEx": "Pe\u00f1as et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP. volume 14, pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "arXiv preprint arXiv:1606.05250 .", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher JC Burges", "Erin Renshaw."], "venue": "EMNLP. volume 3, page 4.", "citeRegEx": "Richardson et al\\.,? 2013", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Overview of clef qa entrance exams task 2015", "author": ["\u00c1lvaro Rodrigo", "Anselmo Pe\u00f1as", "Yusuke Miyao", "Eduard H Hovy", "Noriko Kando."], "venue": "CLEF (Working Notes).", "citeRegEx": "Rodrigo et al\\.,? 2015", "shortCiteRegEx": "Rodrigo et al\\.", "year": 2015}, {"title": "Overview of the ntcir-11 qa-lab task", "author": ["Hideyuki Shibuki", "Kotaro Sakamoto", "Yoshinobu Kano", "Teruko Mitamura", "Madoka Ishioroshi", "Kelly Y Itakura", "Di Wang", "Tatsunori Mori", "Noriko Kando."], "venue": "NTCIR.", "citeRegEx": "Shibuki et al\\.,? 2014", "shortCiteRegEx": "Shibuki et al\\.", "year": 2014}, {"title": "Newsqa: A machine comprehension dataset", "author": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman."], "venue": "arXiv preprint arXiv:1611.09830 .", "citeRegEx": "Trischler et al\\.,? 2016", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Semi-supervised qa with generative domain-adaptive nets", "author": ["Zhilin Yang", "Junjie Hu", "Ruslan Salakhutdinov", "William W Cohen."], "venue": "arXiv preprint arXiv:1702.02206 .", "citeRegEx": "Yang et al\\.,? 2017", "shortCiteRegEx": "Yang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 1, "context": "\u2217* indicates equal contribution answer questions related to a document that it has to comprehend (Chen et al., 2016; Kadlec et al., 2016; Dhingra et al., 2016; Yang et al., 2017).", "startOffset": 97, "endOffset": 178}, {"referenceID": 4, "context": "\u2217* indicates equal contribution answer questions related to a document that it has to comprehend (Chen et al., 2016; Kadlec et al., 2016; Dhingra et al., 2016; Yang et al., 2017).", "startOffset": 97, "endOffset": 178}, {"referenceID": 2, "context": "\u2217* indicates equal contribution answer questions related to a document that it has to comprehend (Chen et al., 2016; Kadlec et al., 2016; Dhingra et al., 2016; Yang et al., 2017).", "startOffset": 97, "endOffset": 178}, {"referenceID": 17, "context": "\u2217* indicates equal contribution answer questions related to a document that it has to comprehend (Chen et al., 2016; Kadlec et al., 2016; Dhingra et al., 2016; Yang et al., 2017).", "startOffset": 97, "endOffset": 178}, {"referenceID": 13, "context": "Although efforts have been made with a similar motivation, including the MCTest dataset created by (Richardson et al., 2013) (containing 500 passages and 2000 questions) and several others (Pe\u00f1as et al.", "startOffset": 99, "endOffset": 124}, {"referenceID": 10, "context": ", 2013) (containing 500 passages and 2000 questions) and several others (Pe\u00f1as et al., 2014; Rodrigo et al., 2015; Khashabi et al., 2016; Shibuki et al., 2014), the usefulness of those datasets is significantly restricted due to their small sizes, especially not suitable for training powerful deep neural networks whose success relies on the availability of relatively large training sets.", "startOffset": 72, "endOffset": 159}, {"referenceID": 14, "context": ", 2013) (containing 500 passages and 2000 questions) and several others (Pe\u00f1as et al., 2014; Rodrigo et al., 2015; Khashabi et al., 2016; Shibuki et al., 2014), the usefulness of those datasets is significantly restricted due to their small sizes, especially not suitable for training powerful deep neural networks whose success relies on the availability of relatively large training sets.", "startOffset": 72, "endOffset": 159}, {"referenceID": 5, "context": ", 2013) (containing 500 passages and 2000 questions) and several others (Pe\u00f1as et al., 2014; Rodrigo et al., 2015; Khashabi et al., 2016; Shibuki et al., 2014), the usefulness of those datasets is significantly restricted due to their small sizes, especially not suitable for training powerful deep neural networks whose success relies on the availability of relatively large training sets.", "startOffset": 72, "endOffset": 159}, {"referenceID": 15, "context": ", 2013) (containing 500 passages and 2000 questions) and several others (Pe\u00f1as et al., 2014; Rodrigo et al., 2015; Khashabi et al., 2016; Shibuki et al., 2014), the usefulness of those datasets is significantly restricted due to their small sizes, especially not suitable for training powerful deep neural networks whose success relies on the availability of relatively large training sets.", "startOffset": 72, "endOffset": 159}, {"referenceID": 1, "context": "ing a large portion of questions in RACE requires the ability of reasoning, the most important feature as a machine comprehension dataset (Chen et al., 2016).", "startOffset": 138, "endOffset": 157}, {"referenceID": 13, "context": "MCTest (Richardson et al., 2013) is a popular dataset for question answering in the same format as RACE, where each question is associated with four candidate answers with a single correct answer.", "startOffset": 7, "endOffset": 32}, {"referenceID": 3, "context": "The past few years have witnessed several largescale cloze-style datasets (Hermann et al., 2015; Hill et al., 2015; Bajgar et al., 2016; Onishi et al., 2016), whose questions are formulated by obliterating a word or an entity in a sentence.", "startOffset": 74, "endOffset": 157}, {"referenceID": 0, "context": "The past few years have witnessed several largescale cloze-style datasets (Hermann et al., 2015; Hill et al., 2015; Bajgar et al., 2016; Onishi et al., 2016), whose questions are formulated by obliterating a word or an entity in a sentence.", "startOffset": 74, "endOffset": 157}, {"referenceID": 8, "context": "The past few years have witnessed several largescale cloze-style datasets (Hermann et al., 2015; Hill et al., 2015; Bajgar et al., 2016; Onishi et al., 2016), whose questions are formulated by obliterating a word or an entity in a sentence.", "startOffset": 74, "endOffset": 157}, {"referenceID": 1, "context": "However, both require limited reasoning ability (Chen et al., 2016).", "startOffset": 48, "endOffset": 67}, {"referenceID": 1, "context": "In fact, the best machine performance obtained by researchers (Chen et al., 2016; Dhingra et al., 2016) is close to human\u2019s performance on CNN/Daily Mail.", "startOffset": 62, "endOffset": 103}, {"referenceID": 2, "context": "In fact, the best machine performance obtained by researchers (Chen et al., 2016; Dhingra et al., 2016) is close to human\u2019s performance on CNN/Daily Mail.", "startOffset": 62, "endOffset": 103}, {"referenceID": 3, "context": "Childrens Book Test (CBT) (Hill et al., 2015) and Book Test (BT) (Bajgar et al.", "startOffset": 26, "endOffset": 45}, {"referenceID": 0, "context": ", 2015) and Book Test (BT) (Bajgar et al., 2016) are constructed in a similar manner.", "startOffset": 27, "endOffset": 48}, {"referenceID": 0, "context": "Machine comprehension models have also matched human performance on CBT (Bajgar et al., 2016).", "startOffset": 72, "endOffset": 93}, {"referenceID": 8, "context": "Who Did What (WDW) (Onishi et al., 2016) is yet another cloze-style dataset constructed from the LDC English Gigaword newswire corpus.", "startOffset": 19, "endOffset": 40}, {"referenceID": 12, "context": "In datasets such as SQUAD (Rajpurkar et al., 2016), NEWSQA (Trischler et al.", "startOffset": 26, "endOffset": 50}, {"referenceID": 16, "context": ", 2016), NEWSQA (Trischler et al., 2016) and MS MARCO (Nguyen et al.", "startOffset": 16, "endOffset": 40}, {"referenceID": 7, "context": ", 2016) and MS MARCO (Nguyen et al., 2016), the answer to each question is in the form of a text span in the article.", "startOffset": 21, "endOffset": 42}, {"referenceID": 9, "context": "Instead of evaluating the accuracy, researchers need to use F1 score, BLEU (Papineni et al., 2002) or ROUGE (Lin and Hovy, 2003) as metrics, which measure the overlap between the prediction and ground truth answers since the questions come without candidate spans.", "startOffset": 75, "endOffset": 98}, {"referenceID": 6, "context": ", 2002) or ROUGE (Lin and Hovy, 2003) as metrics, which measure the overlap between the prediction and ground truth answers since the questions come without candidate spans.", "startOffset": 17, "endOffset": 37}, {"referenceID": 16, "context": "3% on SQUAD (or 65% claimed by Trischler et al. (2016)) and 46.", "startOffset": 31, "endOffset": 55}, {"referenceID": 5, "context": ", the AI2 Elementary School Science Questions dataset (Khashabi et al., 2016)", "startOffset": 54, "endOffset": 77}, {"referenceID": 15, "context": "contains 1080 questions for students in elementary schools; NTCIR QA Lab (Shibuki et al., 2014) evaluates systems by the task of solving real-world university entrance exam questions; The Entrance Exams task at CLEF QA Track (Pe\u00f1as et al.", "startOffset": 73, "endOffset": 95}, {"referenceID": 10, "context": ", 2014) evaluates systems by the task of solving real-world university entrance exam questions; The Entrance Exams task at CLEF QA Track (Pe\u00f1as et al., 2014; Rodrigo et al., 2015) evaluates the system\u2019s read-", "startOffset": 137, "endOffset": 179}, {"referenceID": 14, "context": ", 2014) evaluates systems by the task of solving real-world university entrance exam questions; The Entrance Exams task at CLEF QA Track (Pe\u00f1as et al., 2014; Rodrigo et al., 2015) evaluates the system\u2019s read-", "startOffset": 137, "endOffset": 179}, {"referenceID": 1, "context": "Following Chen et al. (2016); Trischler et al.", "startOffset": 10, "endOffset": 29}, {"referenceID": 1, "context": "Following Chen et al. (2016); Trischler et al. (2016), we stratify the questions into five classes as follows with ascending order of difficulty:", "startOffset": 10, "endOffset": 54}, {"referenceID": 1, "context": "We refer readers to (Chen et al., 2016; Trischler et al., 2016) for examples of each category.", "startOffset": 20, "endOffset": 63}, {"referenceID": 16, "context": "We refer readers to (Chen et al., 2016; Trischler et al., 2016) for examples of each category.", "startOffset": 20, "endOffset": 63}, {"referenceID": 16, "context": "* denotes the numbers coming from (Trischler et al., 2016) based on 1000 samples per dataset, and numbers with \u2020 come from (Chen et al.", "startOffset": 34, "endOffset": 58}, {"referenceID": 1, "context": ", 2016) based on 1000 samples per dataset, and numbers with \u2020 come from (Chen et al., 2016).", "startOffset": 72, "endOffset": 91}, {"referenceID": 13, "context": "Sliding Window Algorithm Firstly, we build the rule-based baseline introduced by Richardson et al. (2013). It chooses the answer having the highest matching score.", "startOffset": 81, "endOffset": 106}, {"referenceID": 1, "context": "Stanford Attentive Reader Stanford Attentive Reader (Stanford AR) (Chen et al., 2016) is a", "startOffset": 66, "endOffset": 85}, {"referenceID": 1, "context": "Following Chen et al. (2016), we adopt a bilinear attention form.", "startOffset": 10, "endOffset": 29}, {"referenceID": 2, "context": "Gated-Attention Reader Gated AR (Dhingra et al., 2016) is the state-of-the-art model on multiple datasets.", "startOffset": 32, "endOffset": 54}, {"referenceID": 2, "context": "We refer readers to (Dhingra et al., 2016) for more details.", "startOffset": 20, "endOffset": 42}, {"referenceID": 11, "context": "We choose word embedding size d = 100 and use the 100-dimensional Glove word embedding (Pennington et al., 2014) as embedding initialization.", "startOffset": 87, "endOffset": 112}, {"referenceID": 1, "context": "Implementation Details We follow Chen et al. (2016) in our experiment settings.", "startOffset": 33, "endOffset": 52}, {"referenceID": 3, "context": "On CBT, we report performance on two subsets where the missing token is either a common noun (CBT-C) or name entity (CBT-N) since the language models have already reached human-level performance on other types (Hill et al., 2015).", "startOffset": 210, "endOffset": 229}], "year": 2017, "abstractText": "We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students\u2019 ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43%) and the ceiling human performance (95%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/ \u0303glai1/data/race/ and the code is available at https://github.com/ qizhex/RACE_AR_baselines", "creator": "LaTeX with hyperref package"}}}