{"id": "1704.04743", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Apr-2017", "title": "Towards String-To-Tree Neural Machine Translation", "abstract": "We present a simple method to incorporate syntactic information about the target language in a neural machine translation system by translating into linearized, lexicalized constituency trees. An experiment on the WMT16 German-English news translation task resulted in an improved BLEU score when compared to a syntax-agnostic NMT baseline trained on the same dataset. An analysis of the translations from the syntax-aware system shows that it performs more reordering during translation in comparison to the baseline. A small-scale human evaluation also showed an advantage to the syntax-aware system.", "histories": [["v1", "Sun, 16 Apr 2017 09:54:50 GMT  (637kb,D)", "https://arxiv.org/abs/1704.04743v1", "Accepted as a short paper in ACL 2017"], ["v2", "Thu, 20 Apr 2017 10:20:28 GMT  (1442kb,D)", "http://arxiv.org/abs/1704.04743v2", "Accepted as a short paper in ACL 2017"], ["v3", "Sat, 6 May 2017 07:25:19 GMT  (1443kb,D)", "http://arxiv.org/abs/1704.04743v3", "Accepted as a short paper in ACL 2017"]], "COMMENTS": "Accepted as a short paper in ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["roee aharoni", "yoav goldberg"], "accepted": true, "id": "1704.04743"}, "pdf": {"name": "1704.04743.pdf", "metadata": {"source": "CRF", "title": "Towards String-to-Tree Neural Machine Translation", "authors": ["Roee Aharoni"], "emails": ["roee.aharoni@gmail.com", "yoav.goldberg@gmail.com"], "sections": [{"heading": "1 Introduction and Model", "text": "In recent years, we have behaved in the manner and manner in which we have behaved in the world, in the manner and manner in which we have behaved in the world, in the manner in which we have behaved in the world. (We have behaved in the world of the word, of the language, of the language, of the language, of the language, of the language, of the language, of the word, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, of the language, the language, of the language, of the language, of the language,"}, {"heading": "2 Experiments & Results", "text": "For the green-egg-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-"}, {"heading": "3 Analysis", "text": "While we have not done a detailed error analysis, most of us seem to be sensible. We have gone through a significant amount of restructuring in the world to generate a fluent translation. A major advantage of S2T models in the world is that they allow a reordering of the world. First, we have drawn attention to hard alignments by selecting the source word for each target word. We compare the amount of restructuring in the world of bpe2bpe and bpe2tree models that aim to distort the world."}, {"heading": "4 Conclusions and Future Work", "text": "While this paper shows that syntactical information via the landing page can be beneficial to the NMT, this paper only scratches the surface of what can be done on the subject. First, better models can be proposed to alleviate the problem of long sequences in the linear approach or to allow for a more natural tree decoding scheme (Alvarez-Melis and Jaakkola, 2017). A comparison of our approach with other syntax conscious NMT models such as Eriguchi et al. (2017) and Nadejde et al. (2017) may also be of interest."}, {"heading": "Acknowledgments", "text": "This work was supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) and the Israeli Science Foundation (grant number 1555 / 15)."}, {"heading": "A Supplementary Material", "text": "This year it has come to the point that it has never come as far as this year."}], "references": [{"title": "Tree-structured decoding with doubly recurrent neural networks", "author": ["David Alvarez-Melis", "Tommi S. Jaakkola."], "venue": "International Conference on Learning Representations (ICLR) .", "citeRegEx": "Alvarez.Melis and Jaakkola.,? 2017", "shortCiteRegEx": "Alvarez.Melis and Jaakkola.", "year": 2017}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Graph convolutional encoders for syntax-aware neural machine translation", "author": ["Joost Bastings", "Ivan Titov", "Wilker Aziz", "Diego Marcheggiani", "Khalil Simaan."], "venue": "arXiv preprint arXiv:1704.04675 .", "citeRegEx": "Bastings et al\\.,? 2017", "shortCiteRegEx": "Bastings et al\\.", "year": 2017}, {"title": "Findings of the 2016 conference on machine translation", "author": ["Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri."], "venue": "Proceedings of the First Conference on Machine Translation. Associ-", "citeRegEx": "Post et al\\.,? 2016", "shortCiteRegEx": "Post et al\\.", "year": 2016}, {"title": "Coarseto-fine n-best parsing and maxent discriminative reranking", "author": ["Eugene Charniak", "Mark Johnson."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, pages", "citeRegEx": "Charniak and Johnson.,? 2005", "shortCiteRegEx": "Charniak and Johnson.", "year": 2005}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, pages 263\u2013270.", "citeRegEx": "Chiang.,? 2005", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "computational linguistics 33(2):201\u2013228.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "Parsing as language modeling", "author": ["Do Kook Choe", "Eugene Charniak."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas, pages 2331\u20132336.", "citeRegEx": "Choe and Charniak.,? 2016", "shortCiteRegEx": "Choe and Charniak.", "year": 2016}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Character-based decoding in treeto-sequence attention-based neural machine translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Proceedings of the 3rd Workshop on Asian Translation (WAT2016). pages 175\u2013183.", "citeRegEx": "Eriguchi et al\\.,? 2016a", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "Tree-to-sequence attentional neural machine translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Eriguchi et al\\.,? 2016b", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "Learning to parse and translate improves neural machine translation", "author": ["Akiko Eriguchi", "Yoshimasa Tsuruoka", "Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1702.03525 http://arxiv.org/abs/1702.03525.", "citeRegEx": "Eriguchi et al\\.,? 2017", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2017}, {"title": "Scalable inference and training of context-rich syntactic translation models", "author": ["Michel Galley", "Jonathan Graehl", "Kevin Knight", "Daniel Marcu", "Steve DeNeefe", "Wei Wang", "Ignacio Thayer."], "venue": "Proceedings of the 21st International Conference", "citeRegEx": "Galley et al\\.,? 2006", "shortCiteRegEx": "Galley et al\\.", "year": 2006}, {"title": "What\u2019s in a translation rule? In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings", "author": ["Michel Galley", "Mark Hopkins", "Kevin Knight", "Daniel Marcu."], "venue": "Association for Computational Linguistics, Boston,", "citeRegEx": "Galley et al\\.,? 2004", "shortCiteRegEx": "Galley et al\\.", "year": 2004}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Statistical Machine Translation", "author": ["Philipp Koehn."], "venue": "Cambridge University Press, New York, NY, USA, 1st edition.", "citeRegEx": "Koehn.,? 2010", "shortCiteRegEx": "Koehn.", "year": 2010}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "arXiv preprint arXiv:1511.06114 .", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Syntax-aware neural machine translation using CCG", "author": ["Maria Nadejde", "Siva Reddy", "Rico Sennrich", "Tomasz Dwojak", "Marcin Junczys-Dowmunt", "Philipp Koehn", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1702.01147 .", "citeRegEx": "Nadejde et al\\.,? 2017", "shortCiteRegEx": "Nadejde et al\\.", "year": 2017}, {"title": "How grammatical is characterlevel neural machine translation? assessing mt quality with contrastive translation pairs", "author": ["Rico Sennrich."], "venue": "arXiv preprint arXiv:1612.04629 .", "citeRegEx": "Sennrich.,? 2016", "shortCiteRegEx": "Sennrich.", "year": 2016}, {"title": "Nematus: a Toolkit for Neural Machine", "author": ["Rico Sennrich", "Orhan Firat", "Kyunghyun Cho", "Alexandra Birch", "Barry Haddow", "Julian Hitschler", "Marcin Junczys-Dowmunt", "Samuel L\u201daubli", "Antonio Valerio Miceli Barone", "Jozef Mokry", "Maria Nadejde"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2017}, {"title": "Linguistic input features improve neural machine translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "Proceedings of the First Conference on Machine Translation. Association for Computational Linguistics, Berlin, Germany, pages 83\u201391.", "citeRegEx": "Sennrich and Haddow.,? 2016", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2016}, {"title": "Edinburgh neural machine translation systems for wmt 16", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the First Conference on Machine Translation. Association for Computational Linguistics, Berlin, Germany, pages", "citeRegEx": "Sennrich et al\\.,? 2016a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Sennrich et al\\.,? 2016b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Does string-based neural mt learn source syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "author": ["Xing Shi", "Inkit Padhi", "Kevin Knight."], "venue": "Association for Computational", "citeRegEx": "Shi et al\\.,? 2016", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "Syntactically guided neural machine translation", "author": ["Felix Stahlberg", "Eva Hasler", "Aurelien Waite", "Bill Byrne."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for", "citeRegEx": "Stahlberg et al\\.,? 2016", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "arXiv preprint arXiv:1409.3215 .", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computa-", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems. pages 2773\u20132781.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "SyntaxBased Statistical Machine Translation", "author": ["P. Williams", "M. Gertz", "M. Post."], "venue": "Morgan & Claypool publishing.", "citeRegEx": "Williams et al\\.,? 2016", "shortCiteRegEx": "Williams et al\\.", "year": 2016}, {"title": "A syntaxbased statistical translation model", "author": ["Kenji Yamada", "Kevin Knight."], "venue": "Proceedings of the 39th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, pages 523\u2013530.", "citeRegEx": "Yamada and Knight.,? 2001", "shortCiteRegEx": "Yamada and Knight.", "year": 2001}, {"title": "A decoder for syntax-based statistical mt", "author": ["Kenji Yamada", "Kevin Knight."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, pages 303\u2013310.", "citeRegEx": "Yamada and Knight.,? 2002", "shortCiteRegEx": "Yamada and Knight.", "year": 2002}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701 .", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "quences with a maximum length of 50 tokens, and with minibatch size of 80. It was trained for one week on a single Nvidia TitanX GPU. Only in the low-resource experiments we applied dropout", "author": ["Sennrich"], "venue": null, "citeRegEx": "Sennrich,? \\Q2016\\E", "shortCiteRegEx": "Sennrich", "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has recently became the state-of-the-art approach to machine translation (Bojar et al.", "startOffset": 33, "endOffset": 112}, {"referenceID": 26, "context": "Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has recently became the state-of-the-art approach to machine translation (Bojar et al.", "startOffset": 33, "endOffset": 112}, {"referenceID": 1, "context": "Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has recently became the state-of-the-art approach to machine translation (Bojar et al.", "startOffset": 33, "endOffset": 112}, {"referenceID": 15, "context": ", 2016), while being much simpler than the previously dominant phrase-based statistical machine translation (SMT) approaches (Koehn, 2010).", "startOffset": 125, "endOffset": 138}, {"referenceID": 29, "context": "However, a large body of work was dedicated to syntax-based SMT (Williams et al., 2016).", "startOffset": 64, "endOffset": 87}, {"referenceID": 28, "context": "Our method is inspired by recent works in syntactic parsing, which model trees as sequences (Vinyals et al., 2015; Choe and Charniak, 2016).", "startOffset": 92, "endOffset": 139}, {"referenceID": 7, "context": "Our method is inspired by recent works in syntactic parsing, which model trees as sequences (Vinyals et al., 2015; Choe and Charniak, 2016).", "startOffset": 92, "endOffset": 139}, {"referenceID": 28, "context": "Note that the linearized trees we predict are different in their structure from those in Vinyals et al. (2015) as instead of having part of speech tags as terminals, they contain the words of the translated sentence.", "startOffset": 89, "endOffset": 111}, {"referenceID": 1, "context": "Since this modeling keeps the form of a sequence-to-sequence learning task, we can employ the conventional attention-based sequence to sequence paradigm (Bahdanau et al., 2014) as-is, while enriching the output with syntactic information.", "startOffset": 153, "endOffset": 176}, {"referenceID": 27, "context": "(2016a,b) replace the encoder in an attention-based model with a Tree-LSTM (Tai et al., 2015) over a constituency parse tree; Bastings et al.", "startOffset": 75, "endOffset": 93}, {"referenceID": 2, "context": ", 2015) over a constituency parse tree; Bastings et al. (2017) encoded sentences using graph-convolutional networks over dependency trees; Sennrich and Haddow (2016)", "startOffset": 40, "endOffset": 63}, {"referenceID": 2, "context": ", 2015) over a constituency parse tree; Bastings et al. (2017) encoded sentences using graph-convolutional networks over dependency trees; Sennrich and Haddow (2016)", "startOffset": 40, "endOffset": 166}, {"referenceID": 17, "context": "proposed a factored NMT approach, where each source word embedding is concatenated to embeddings of linguistic features of the word; Luong et al. (2015) incorporated syntactic knowledge via multi-task sequence to sequence learning: their", "startOffset": 133, "endOffset": 153}, {"referenceID": 25, "context": "system included a single encoder with multiple decoders, one of which attempts to predict the parse-tree of the source sentence; Stahlberg et al. (2016) proposed a hybrid approach in which translations are scored by combining scores from an", "startOffset": 129, "endOffset": 153}, {"referenceID": 5, "context": "NMT system with scores from a Hiero (Chiang, 2005, 2007) system. Shi et al. (2016) explored the syntactic knowledge encoded by an NMT encoder, showing the encoded vector can be used to predict syntactic information like constituency trees, voice and tense with high accuracy.", "startOffset": 37, "endOffset": 83}, {"referenceID": 8, "context": "(2017) proposed to model the target syntax in NMT in the form of dependency trees by using an RNNG-based decoder (Dyer et al., 2016), while Nadejde et al.", "startOffset": 113, "endOffset": 132}, {"referenceID": 8, "context": "In parallel and highly related to our work, Eriguchi et al. (2017) proposed to model the target syntax in NMT in the form of dependency trees by using an RNNG-based decoder (Dyer et al.", "startOffset": 44, "endOffset": 67}, {"referenceID": 8, "context": "(2017) proposed to model the target syntax in NMT in the form of dependency trees by using an RNNG-based decoder (Dyer et al., 2016), while Nadejde et al. (2017) incorporated target syntax by predicting CCG tags serialized into the target translation.", "startOffset": 114, "endOffset": 162}, {"referenceID": 4, "context": "In all cases we parse the English sentences into constituency trees using the BLLIP parser (Charniak and Johnson, 2005).", "startOffset": 91, "endOffset": 119}, {"referenceID": 23, "context": "1 To enable an open vocabulary translation we used sub-word units obtained via BPE (Sennrich et al., 2016b) on both source and target.", "startOffset": 83, "endOffset": 107}, {"referenceID": 8, "context": "We then experiment in a low-resource scenario using the German, Russian and Czech to English training data from the News Commentary v8 corpus, following Eriguchi et al. (2017). In all cases we parse the English sentences into constituency trees using the BLLIP parser (Charniak and Johnson, 2005).", "startOffset": 153, "endOffset": 176}, {"referenceID": 20, "context": "We use the NEMATUS (Sennrich et al., 2017)3 implementation of an attention-based NMT model.", "startOffset": 19, "endOffset": 42}, {"referenceID": 16, "context": "For all models we report results of the best performing single model on the dev-set (newstest2013+newstest2014 in the resource rich setting, newstest2015 in the rest, as measured by BLEU) when translating newstest2015 and newstest2016, similarly to Sennrich et al. (2016a); Eriguchi et al.", "startOffset": 249, "endOffset": 273}, {"referenceID": 9, "context": "(2016a); Eriguchi et al. (2017). To evaluate the stringto-tree translations we derive the surface form by removing the symbols that stand for nonterminals in the tree, followed by merging the subwords.", "startOffset": 9, "endOffset": 32}, {"referenceID": 16, "context": "pl script from the Moses toolkit (Koehn et al., 2007).", "startOffset": 33, "endOffset": 53}, {"referenceID": 13, "context": "We can use this information to extract GHKM rules (Galley et al., 2004).", "startOffset": 50, "endOffset": 71}, {"referenceID": 0, "context": "First, better models can be proposed to alleviate the long sequence problem in the linearized approach or allow a more natural tree decoding scheme (Alvarez-Melis and Jaakkola, 2017).", "startOffset": 148, "endOffset": 182}, {"referenceID": 19, "context": "A Contrastive evaluation (Sennrich, 2016) of a syntax-aware system vs.", "startOffset": 25, "endOffset": 41}, {"referenceID": 0, "context": "First, better models can be proposed to alleviate the long sequence problem in the linearized approach or allow a more natural tree decoding scheme (Alvarez-Melis and Jaakkola, 2017). Comparing our approach to other syntax aware NMT models like Eriguchi et al. (2017) and Nadejde et al.", "startOffset": 149, "endOffset": 268}, {"referenceID": 0, "context": "First, better models can be proposed to alleviate the long sequence problem in the linearized approach or allow a more natural tree decoding scheme (Alvarez-Melis and Jaakkola, 2017). Comparing our approach to other syntax aware NMT models like Eriguchi et al. (2017) and Nadejde et al. (2017) may also be of interest.", "startOffset": 149, "endOffset": 294}], "year": 2017, "abstractText": "We present a simple method to incorporate syntactic information about the target language in a neural machine translation system by translating into linearized, lexicalized constituency trees. Experiments on the WMT16 German-English news translation task shown improved BLEU scores when compared to a syntax-agnostic NMT baseline trained on the same dataset. An analysis of the translations from the syntax-aware system shows that it performs more reordering during translation in comparison to the baseline. A smallscale human evaluation also showed an advantage to the syntax-aware system.", "creator": "LaTeX with hyperref package"}}}