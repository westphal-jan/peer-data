{"id": "1704.04920", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Apr-2017", "title": "Deep Joint Entity Disambiguation with Local Neural Attention", "abstract": "We propose a novel deep learning model for joint document-level entity disambiguation, which leverages learned neural representations. Key components are entity embeddings, a neural attention mechanism over local context windows, and a differentiable joint inference stage for disambiguation. Our approach thereby combines benefits of deep learning with more traditional approaches such as graphical models and probabilistic mention-entity maps. Extensive experiments show that we are able to obtain competitive or state-of-the-art accuracy at moderate computational costs.", "histories": [["v1", "Mon, 17 Apr 2017 10:18:32 GMT  (409kb,D)", "http://arxiv.org/abs/1704.04920v1", null], ["v2", "Fri, 21 Jul 2017 23:47:24 GMT  (401kb,D)", "http://arxiv.org/abs/1704.04920v2", "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2017 long paper"], ["v3", "Mon, 31 Jul 2017 18:25:57 GMT  (389kb,D)", "http://arxiv.org/abs/1704.04920v3", "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2017 long paper"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["octavian-eugen ganea", "thomas hofmann"], "accepted": true, "id": "1704.04920"}, "pdf": {"name": "1704.04920.pdf", "metadata": {"source": "CRF", "title": "Deep Joint Entity Disambiguation with Local Neural Attention", "authors": ["Octavian-Eugen Ganea"], "emails": ["octavian.ganea@inf.ethz.ch", "thomas.hofmann@inf.ethz.ch"], "sections": [{"heading": null, "text": "We propose a novel deep learning model for the joint disambiguation of entities at the document level, using learned neural representations. Key components are entity embedding, a neuronal attention mechanism via local context windows, and a differentiated common deduction phase for disambiguation. Our approach combines the advantages of deep learning with more traditional approaches such as graphical models and probabilistic mention entity maps. Extensive experiments show that we are able to achieve competitive or standardized accuracy at moderate computing costs."}, {"heading": "1 Introduction", "text": "Entity Disambiguation (ED) is an important stage in understanding text that automatically resolves references to entities in a particular knowledge base (KBS), a task that is difficult due to the inherent ambiguity between mentions of surface shapes such as names and the entities they refer to. This ambiguity can often be grasped in part by the coexistence of names and entities extracted from entities-related entities.ED research largely focuses on two types of context information on disambiguity: local information based on words that occur in a context window around an entity, and global information, exploiting the coherence of referenced entities at the document level. Many methods of statecraft aim to combine the benefits of both, which is also the philosophy we follow in this paper."}, {"heading": "2 Contributions and Related Work", "text": "In fact, most of them are able to survive and survive on their own."}, {"heading": "3 Learning Entity Embeddings", "text": "In a first step, we propose to train entity vectors that can be used for the ED task (and potentially for other tasks), which compress the semantic meaning of entities and drastically reduce the need for manually crafted traits or co-occurrence statistics. Entity embeddings depend on word embeddings and are trained independently for each entity. A few arguments motivate this approach: (i) There is no need for entity co-occurrence statistics that would otherwise be prohibitive for large entity KBs and / or large memory points. (ii) Vectors of entities in a subset of interest can be trained separately to obtain potentially significant speed and memory savings that would otherwise be prohibitive for large entity KBs; 1 (iii) entities can easily be added in an increative way (the practice is selective in the few)."}, {"heading": "4 Local Model with Neural Attention", "text": "We are based on the realization that only a few contexts can be used to handle an ambiguous mention. (We) assume that we can only apply such a model if a context word can be relevant if it is related to at least one of the entity candidates of a given menu item. (We assume that we have a mention entity entity entity entity entity entity entity in front of us.) We assume that we have a mention entity entity entity entity entity entity entity entity entity entity entity entity in front of us, and that for each mention m, a pruned entity set of entities entities entities entities entities entities set (m), (m) -S have been identified."}, {"heading": "5 Document-Level Deep Model", "text": "Next, we address global ED forms of approximation between entities. Therefore, we present the idea of a document consisting of a sequence of mentions m = m1,.. mn, together with their context windows c = c1,.. cn. Our goal is to define a common probability distribution over the number of mentions. (m1) Our model is a fully connected pairwise conditional random field, defined on thelog scale asg (e, m, c) = n. \"(7) The invariant factors are the local values described in Eq. (4) The pair-wise factors are bilinear forms of the entity. (4)"}, {"heading": "6 Candidate Selection", "text": "We use a mentioning unit before p (e | m) both as a characteristic and for the selection of the candidates. It is calculated by calculating the probabilities from two indices, which are formed from the hyperlink statistics of the mentioned entities from Wikipedia and a large web corpus (Spitkovsky and Chang, 2012) as well as the YAGO index of (Hoffart et al., 2011) (with uniform prior mention). Candidate selection, i.e. the creation of \"e,\" takes place for each mention as follows: (i) the best 30 candidates are selected on the basis of p (e | m). In order to optimize memory and maturity (LBP has a square of complexity in S), we keep only 7 of these entities on the basis of the following heuristic mention: (i) the top 4 entities are selected on the basis of p (e | m), e.g. the persons are selected on the upper end of the list 3."}, {"heading": "7 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 ED Datasets", "text": "We validate our ED models on some of the most popular available datasets used by our predecessors 6. Statistics are provided in Table 2. \u2022 The AIDA CoNLL dataset (Hoffart et al., 2011) is one of the largest manually annotated ED datasets. It contains training (AIDA train), validation (AIDA-A) and test kits (AIDA-B). \u2022 MSNBC (MSB), AQUAINT (AQ) and ACE2004 (ACE) datasets cleaned and updated by (Guo and Barbosa, 2016) 7 \u2022 WNED-WIKI (WW) and WNED-CWEB (CWEB): larger but less reliable, automatically annotated datasets created by ClueWeb and Wikipedia (Guo and Barbosa, 2016)."}, {"heading": "7.2 Training Details and (Hyper)Parameters", "text": "Entity Vectors Training & Relatedness Evaluation for entity embeddings only, we use Wikipedia (Feb 2014) corpus for training. Entity Vectors are initialized multiple after a 0-mean normal distribution with standard deviation 1. We6TAC-KBP datasets used by (Yamada et al., 2016; Globerson et al., 2016) are not available at: bit.7Available at: bit.ly / 2gnSBLgfirst train each entity vector on the entity canonical description page (words included) for 400 iterations. Subsequently, Wikipedia hyperlinks of the respective entities are used for learning until validation score (description below) stops improved. In each iteration, 20 positive words, each with 5 negative words, are sampled and used for optimization as explained in Section 3. We use fixes for learning until validation (description below)."}, {"heading": "7.3 Baselines & Results", "text": "The best results for the AIDA data sets are reported by (Yamada et al., 2016) and (Globerson et al., 2016). We do not compare against (Pershina et al., 2015) because, as mentioned by (Globerson et al., 2016), their mention index artificially includes the gold unit (Guaranteed Gold Recall). For a fair comparison with previous work, we use in-35mm accuracy and micro F1 ratios (averaged per mention) to evaluate our approach. Results are shown in Tables 3 and 4. We run our system five times each time we select the best model on the validation set, and report the results on the test set for these models."}, {"heading": "7.4 Hyperparameter Studies", "text": "In Figure 5, we analyze the effect of two hyperparameters. First, we see that hard attention (i.e. R < K) helps to reduce the noise of uninformative context words. Second, we see that a small number of LBP iterations (firmly encoded in our network) is sufficient to achieve good accuracy, speeding up training and testing compared to traditional methods that perform LBP to convergence."}, {"heading": "7.5 Qualitative Analysis of Local Model", "text": "In Table 7, we show some examples of contextual words in which our local model participates for hard cases (where mention before the correct entity is low)."}, {"heading": "8 Conclusion", "text": "We have proposed a novel deep learning architecture for entity disambiguation that combines entity embedding, a contextual attention mechanism, an adaptive local score combination, and unrolled, differentiated messages for global inference9. Compared to many other methods, we do not rely on handmade features or an extensive corpus of entity cooperations or relationships. Our system is fully differentiated, although we have chosen to pre-train word and entity embedding. Extensive experiments demonstrate the competitiveness of our approach across a wide range of corpora. In the future, we would like to expand 9Code: github.com / dalab / deep-edthis system to perform Nile recognition, correlation resolution, and mention recognition."}, {"heading": "Acknowledgments", "text": "We thank Aurelien Lucchi, Marina Ganea, Jason Lee, Florian Schmidt and Hadi Daneshmand for their comments and suggestions."}], "references": [{"title": "Learning relatedness measures for entity linking", "author": ["Diego Ceccarelli", "Claudio Lucchese", "Salvatore Orlando", "Raffaele Perego", "Salvatore Trani."], "venue": "Proceedings of the 22nd ACM international conference on Information & Knowledge Management.", "citeRegEx": "Ceccarelli et al\\.,? 2013", "shortCiteRegEx": "Ceccarelli et al\\.", "year": 2013}, {"title": "Relational inference for wikification", "author": ["Xiao Cheng", "Dan Roth."], "venue": "Urbana 51(61801):16\u201358.", "citeRegEx": "Cheng and Roth.,? 2013", "shortCiteRegEx": "Cheng and Roth.", "year": 2013}, {"title": "Entity disambiguation with web links", "author": ["Andrew Chisholm", "Ben Hachey."], "venue": "Transactions of the Association for Computational Linguistics 3:145\u2013156.", "citeRegEx": "Chisholm and Hachey.,? 2015", "shortCiteRegEx": "Chisholm and Hachey.", "year": 2015}, {"title": "User conditional hashtag prediction for images", "author": ["Emily Denton", "Jason Weston", "Manohar Paluri", "Lubomir Bourdev", "Rob Fergus."], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.", "citeRegEx": "Denton et al\\.,? 2015", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Parameter learning with truncated message-passing", "author": ["Justin Domke."], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEE, pages 2937\u20132943.", "citeRegEx": "Domke.,? 2011", "shortCiteRegEx": "Domke.", "year": 2011}, {"title": "Learning graphical model parameters with approximate marginal inference", "author": ["Justin Domke."], "venue": "IEEE transactions on pattern analysis and machine intelligence 35(10):2454\u20132467.", "citeRegEx": "Domke.,? 2013", "shortCiteRegEx": "Domke.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research 12(Jul):2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Entity disambiguation by knowledge and text jointly embedding", "author": ["Wei Fang", "Jianwen Zhang", "Dilin Wang", "Zheng Chen", "Ming Li."], "venue": "CoNLL 2016 page 260.", "citeRegEx": "Fang et al\\.,? 2016", "shortCiteRegEx": "Fang et al\\.", "year": 2016}, {"title": "Tagme: on-the-fly annotation of short text fragments (by wikipedia entities)", "author": ["Paolo Ferragina", "Ugo Scaiella."], "venue": "Proceedings of the 19th ACM international conference on Information and knowledge management. ACM, pages 1625\u20131628.", "citeRegEx": "Ferragina and Scaiella.,? 2010", "shortCiteRegEx": "Ferragina and Scaiella.", "year": 2010}, {"title": "Capturing semantic similarity for entity linking with convolutional neural networks", "author": ["Matthew Francis-Landau", "Greg Durrett", "Dan Klein."], "venue": "arXiv preprint arXiv:1604.00734 .", "citeRegEx": "Francis.Landau et al\\.,? 2016", "shortCiteRegEx": "Francis.Landau et al\\.", "year": 2016}, {"title": "Probabilistic bag-of-hyperlinks model for entity linking", "author": ["Octavian-Eugen Ganea", "Marina Ganea", "Aurelien Lucchi", "Carsten Eickhoff", "Thomas Hofmann."], "venue": "Proceedings of the 25th International Conference on World Wide Web. International World", "citeRegEx": "Ganea et al\\.,? 2016", "shortCiteRegEx": "Ganea et al\\.", "year": 2016}, {"title": "Collective entity resolution with multi-focal attention", "author": ["Amir Globerson", "Nevena Lazic", "Soumen Chakrabarti", "Amarnag Subramanya", "Michael Ringgaard", "Fernando Pereira"], "venue": null, "citeRegEx": "Globerson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2016}, {"title": "Robust named entity disambiguation with random walks", "author": ["Zhaochen Guo", "Denilson Barbosa"], "venue": null, "citeRegEx": "Guo and Barbosa.,? \\Q2016\\E", "shortCiteRegEx": "Guo and Barbosa.", "year": 2016}, {"title": "Learning entity representation for entity disambiguation", "author": ["Zhengyan He", "Shujie Liu", "Mu Li", "Ming Zhou", "Longkai Zhang", "Houfeng Wang."], "venue": "ACL (2). pages 30\u201334.", "citeRegEx": "He et al\\.,? 2013", "shortCiteRegEx": "He et al\\.", "year": 2013}, {"title": "Robust disambiguation of named entities in text", "author": ["Johannes Hoffart", "Mohamed Amir Yosef", "Ilaria Bordino", "Hagen F\u00fcrstenau", "Manfred Pinkal", "Marc Spaniol", "Bilyana Taneva", "Stefan Thater", "Gerhard Weikum."], "venue": "Proceedings of the Conference", "citeRegEx": "Hoffart et al\\.,? 2011", "shortCiteRegEx": "Hoffart et al\\.", "year": 2011}, {"title": "Leveraging deep neural networks and knowledge graphs for entity disambiguation", "author": ["Hongzhao Huang", "Larry Heck", "Heng Ji."], "venue": "arXiv preprint arXiv:1504.07678 .", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Entity discovery and linking reading list http://nlp.cs.rpi.edu/kbp/2014/elreading.html", "author": ["Heng Ji"], "venue": null, "citeRegEx": "Ji.,? \\Q2016\\E", "shortCiteRegEx": "Ji.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando Pereira"], "venue": "In Proceedings of the eighteenth international conference on machine learning, ICML", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Plato: A selective context model for entity resolution", "author": ["Nevena Lazic", "Amarnag Subramanya", "Michael Ringgaard", "Fernando Pereira."], "venue": "Transactions of the Association for Computational Linguistics 3:503\u2013515.", "citeRegEx": "Lazic et al\\.,? 2015", "shortCiteRegEx": "Lazic et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning to link with wikipedia", "author": ["David Milne", "Ian H Witten."], "venue": "Proceedings of the 17th ACM conference on Information and knowledge management. ACM, pages 509\u2013518.", "citeRegEx": "Milne and Witten.,? 2008", "shortCiteRegEx": "Milne and Witten.", "year": 2008}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["Kevin P Murphy", "Yair Weiss", "Michael I Jordan."], "venue": "Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers Inc., pages", "citeRegEx": "Murphy et al\\.,? 1999", "shortCiteRegEx": "Murphy et al\\.", "year": 1999}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP. volume 14, pages 1532\u2013", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Personalized page rank for named entity disambiguation", "author": ["Maria Pershina", "Yifan He", "Ralph Grishman"], "venue": null, "citeRegEx": "Pershina et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pershina et al\\.", "year": 2015}, {"title": "Local and global algorithms for disambiguation to wikipedia", "author": ["Lev Ratinov", "Dan Roth", "Doug Downey", "Mike Anderson."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language", "citeRegEx": "Ratinov et al\\.,? 2011", "shortCiteRegEx": "Ratinov et al\\.", "year": 2011}, {"title": "A cross-lingual dictionary for english wikipedia", "author": ["Valentin I Spitkovsky", "Angel X Chang"], "venue": null, "citeRegEx": "Spitkovsky and Chang.,? \\Q2012\\E", "shortCiteRegEx": "Spitkovsky and Chang.", "year": 2012}, {"title": "End-to-end memory networks. In Advances in neural information processing systems", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": null, "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Modeling mention, context and entity with neural networks for entity disambiguation", "author": ["Yaming Sun", "Lei Lin", "Duyu Tang", "Nan Yang", "Zhenzhou Ji", "Xiaolong Wang."], "venue": "IJCAI. pages 1333\u20131339.", "citeRegEx": "Sun et al\\.,? 2015", "shortCiteRegEx": "Sun et al\\.", "year": 2015}, {"title": "Joint learning of the embedding of words and entities for named entity disambiguation", "author": ["Ikuya Yamada", "Hiroyuki Shindo", "Hideaki Takeda", "Yoshiyasu Takefuji."], "venue": "CoNLL 2016 page 250.", "citeRegEx": "Yamada et al\\.,? 2016", "shortCiteRegEx": "Yamada et al\\.", "year": 2016}, {"title": "Robust and collective entity disambiguation through semantic embeddings", "author": ["Stefan Zwicklbauer", "Christin Seifert", "Michael Granitzer."], "venue": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Informa-", "citeRegEx": "Zwicklbauer et al\\.,? 2016", "shortCiteRegEx": "Zwicklbauer et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "There is a vast prior research on entity disambiguation, highlighted by (Ji, 2016).", "startOffset": 72, "endOffset": 82}, {"referenceID": 20, "context": "(Mikolov et al., 2013; Pennington et al., 2014), which was recently extended to entities and ED by (Yamada et al.", "startOffset": 0, "endOffset": 47}, {"referenceID": 23, "context": "(Mikolov et al., 2013; Pennington et al., 2014), which was recently extended to entities and ED by (Yamada et al.", "startOffset": 0, "endOffset": 47}, {"referenceID": 29, "context": ", 2014), which was recently extended to entities and ED by (Yamada et al., 2016; Fang et al., 2016; Zwicklbauer et al., 2016; Huang et al., 2015).", "startOffset": 59, "endOffset": 145}, {"referenceID": 7, "context": ", 2014), which was recently extended to entities and ED by (Yamada et al., 2016; Fang et al., 2016; Zwicklbauer et al., 2016; Huang et al., 2015).", "startOffset": 59, "endOffset": 145}, {"referenceID": 30, "context": ", 2014), which was recently extended to entities and ED by (Yamada et al., 2016; Fang et al., 2016; Zwicklbauer et al., 2016; Huang et al., 2015).", "startOffset": 59, "endOffset": 145}, {"referenceID": 15, "context": ", 2014), which was recently extended to entities and ED by (Yamada et al., 2016; Fang et al., 2016; Zwicklbauer et al., 2016; Huang et al., 2015).", "startOffset": 59, "endOffset": 145}, {"referenceID": 27, "context": "ory networks (Sukhbaatar et al., 2015) and insights of (Lazic et al.", "startOffset": 13, "endOffset": 38}, {"referenceID": 19, "context": ", 2015) and insights of (Lazic et al., 2015), our model deploys attention to select words that are informative for the disambiguation decision.", "startOffset": 24, "endOffset": 44}, {"referenceID": 10, "context": "Our local model achieves better accuracy than the local probabilistic model of (Ganea et al., 2016), as well as the feature-engineered local model of (Globerson et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 11, "context": ", 2016), as well as the feature-engineered local model of (Globerson et al., 2016).", "startOffset": 58, "endOffset": 82}, {"referenceID": 9, "context": "For instance (Francis-Landau et al., 2016; He et al., 2013) use convolutional neural networks (CNNs) and stacked denoising auto-encoders, respectively, to learn representations of textual documents and canonical entity pages.", "startOffset": 13, "endOffset": 59}, {"referenceID": 13, "context": "For instance (Francis-Landau et al., 2016; He et al., 2013) use convolutional neural networks (CNNs) and stacked denoising auto-encoders, respectively, to learn representations of textual documents and canonical entity pages.", "startOffset": 13, "endOffset": 59}, {"referenceID": 28, "context": "In a similar local setting (Sun et al., 2015) embed mentions, their immediate contexts and their candidate entities using word embeddings and CNNs.", "startOffset": 27, "endOffset": 45}, {"referenceID": 18, "context": "Mentions in a document are resolved jointly, using a conditional random field (Lafferty et al., 2001) with parametrized potentials.", "startOffset": 78, "endOffset": 101}, {"referenceID": 22, "context": "We suggest to learn the latter by casting loopy belief propagation (LBP) (Murphy et al., 1999) as a rolled-out deep network.", "startOffset": 73, "endOffset": 94}, {"referenceID": 5, "context": "(Domke, 2013), and allows us to backpropagate through the (truncated) message passing, thereby optimizing the CRF potentials to work well in conjunction with the inference scheme.", "startOffset": 0, "endOffset": 13}, {"referenceID": 12, "context": "Previous work has investigated different approximation techniques, including: random graph walks (Guo and Barbosa, 2016), personalized PageRank (Pershina et al.", "startOffset": 97, "endOffset": 120}, {"referenceID": 24, "context": "Previous work has investigated different approximation techniques, including: random graph walks (Guo and Barbosa, 2016), personalized PageRank (Pershina et al., 2015), intermention voting (Ferragina and Scaiella, 2010), graph pruning (Hoffart et al.", "startOffset": 144, "endOffset": 167}, {"referenceID": 8, "context": ", 2015), intermention voting (Ferragina and Scaiella, 2010), graph pruning (Hoffart et al.", "startOffset": 29, "endOffset": 59}, {"referenceID": 14, "context": ", 2015), intermention voting (Ferragina and Scaiella, 2010), graph pruning (Hoffart et al., 2011), integer linear programming (Cheng and Roth, 2013), or ranking SVMs (Ratinov et al.", "startOffset": 75, "endOffset": 97}, {"referenceID": 1, "context": ", 2011), integer linear programming (Cheng and Roth, 2013), or ranking SVMs (Ratinov et al.", "startOffset": 36, "endOffset": 58}, {"referenceID": 25, "context": ", 2011), integer linear programming (Cheng and Roth, 2013), or ranking SVMs (Ratinov et al., 2011).", "startOffset": 76, "endOffset": 98}, {"referenceID": 10, "context": "Mostly connected to our approach is (Ganea et al., 2016) where LBP is used for inference (but not learning) in a probabilistic graphical model and (Globerson et al.", "startOffset": 36, "endOffset": 56}, {"referenceID": 11, "context": ", 2016) where LBP is used for inference (but not learning) in a probabilistic graphical model and (Globerson et al., 2016) where a single round of message passing with attention is performed.", "startOffset": 98, "endOffset": 122}, {"referenceID": 20, "context": "We start with a pre-trained word embedding map x :W \u2192 Rd that is known to encode semantic meaning of words w \u2208 W; specifically we use word2vec pretrained vectors (Mikolov et al., 2013).", "startOffset": 162, "endOffset": 184}, {"referenceID": 20, "context": "As in (Mikolov et al., 2013), we choose a modified unigram distribution", "startOffset": 6, "endOffset": 28}, {"referenceID": 19, "context": "We build on the insight that only a few context words are informative for resolving an ambiguous mention, something that has been exploited before in (Lazic et al., 2015).", "startOffset": 150, "endOffset": 170}, {"referenceID": 29, "context": "(Yamada et al., 2016) observe the same problem and adopt the restrictive strategy of removing all non-nouns.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "We therefore used a neural network with two fully connected layers of 100 hidden units and ReLU non-linearities, which we regularized as suggested in (Denton et al., 2015) by constraining the sum of squares of all weights in the linear layer.", "startOffset": 150, "endOffset": 171}, {"referenceID": 10, "context": "Similar to (Ganea et al., 2016), the above normalization helps balancing the unary and pairwise terms across documents with different numbers of mentions.", "startOffset": 11, "endOffset": 31}, {"referenceID": 5, "context": "As noted by (Domke, 2013), this method is robust to model mis-specification, avoids inherent difficulties of partition functions and is faster compared to double-loop likelihood training (where, for each stochastic update, inference is run until convergence is achieved).", "startOffset": 12, "endOffset": 25}, {"referenceID": 29, "context": "48 (Yamada et al., 2016) d = 500 0.", "startOffset": 3, "endOffset": 24}, {"referenceID": 0, "context": "Table 1: Entity relatedness results on the test set of (Ceccarelli et al., 2013).", "startOffset": 55, "endOffset": 80}, {"referenceID": 21, "context": "WLM is a well-known similarity method of (Milne and Witten, 2008).", "startOffset": 41, "endOffset": 65}, {"referenceID": 26, "context": "It is computed by averaging probabilities from two indexes build from mention entity hyperlink statistics from Wikipedia and a large Web corpus (Spitkovsky and Chang, 2012), plus the YAGO index of (Hoffart et al.", "startOffset": 144, "endOffset": 172}, {"referenceID": 14, "context": "It is computed by averaging probabilities from two indexes build from mention entity hyperlink statistics from Wikipedia and a large Web corpus (Spitkovsky and Chang, 2012), plus the YAGO index of (Hoffart et al., 2011) (with uniform prior).", "startOffset": 197, "endOffset": 219}, {"referenceID": 14, "context": "\u2022 AIDA-CoNLL dataset (Hoffart et al., 2011) is one of the biggest manually annotated ED datasets.", "startOffset": 21, "endOffset": 43}, {"referenceID": 12, "context": "\u2022 MSNBC (MSB), AQUAINT (AQ) and ACE2004 (ACE) datasets cleaned and updated by (Guo and Barbosa, 2016)7", "startOffset": 78, "endOffset": 101}, {"referenceID": 12, "context": "\u2022 WNED-WIKI (WW) and WNED-CWEB (CWEB): larger, but less reliable, automatically annotated datasets built from ClueWeb and Wikipedia by (Guo and Barbosa, 2016).", "startOffset": 135, "endOffset": 158}, {"referenceID": 29, "context": "TAC-KBP datasets used by (Yamada et al., 2016; Globerson et al., 2016; Sun et al., 2015) are no longer available.", "startOffset": 25, "endOffset": 88}, {"referenceID": 11, "context": "TAC-KBP datasets used by (Yamada et al., 2016; Globerson et al., 2016; Sun et al., 2015) are no longer available.", "startOffset": 25, "endOffset": 88}, {"referenceID": 28, "context": "TAC-KBP datasets used by (Yamada et al., 2016; Globerson et al., 2016; Sun et al., 2015) are no longer available.", "startOffset": 25, "endOffset": 88}, {"referenceID": 19, "context": "9 (Lazic et al., 2015) 86.", "startOffset": 2, "endOffset": 22}, {"referenceID": 11, "context": "4 (Globerson et al., 2016) 87.", "startOffset": 2, "endOffset": 26}, {"referenceID": 29, "context": "9 (Yamada et al., 2016) 87.", "startOffset": 2, "endOffset": 23}, {"referenceID": 15, "context": "Global models (Huang et al., 2015) 86.", "startOffset": 14, "endOffset": 34}, {"referenceID": 10, "context": "6 (Ganea et al., 2016) 87.", "startOffset": 2, "endOffset": 22}, {"referenceID": 2, "context": "6 (Chisholm and Hachey, 2015) 88.", "startOffset": 2, "endOffset": 29}, {"referenceID": 12, "context": "7 (Guo and Barbosa, 2016) 89.", "startOffset": 2, "endOffset": 25}, {"referenceID": 11, "context": "0 (Globerson et al., 2016) 91.", "startOffset": 2, "endOffset": 26}, {"referenceID": 29, "context": "0 (Yamada et al., 2016) 91.", "startOffset": 2, "endOffset": 23}, {"referenceID": 7, "context": "2 (Fang et al., 2016) 81.", "startOffset": 2, "endOffset": 21}, {"referenceID": 10, "context": "3 - (Ganea et al., 2016) 91 89.", "startOffset": 4, "endOffset": 24}, {"referenceID": 21, "context": "7 - (Milne and Witten, 2008) 78 85 81 64.", "startOffset": 4, "endOffset": 28}, {"referenceID": 14, "context": "7 (Hoffart et al., 2011) 79 56 80 58.", "startOffset": 2, "endOffset": 24}, {"referenceID": 25, "context": "6 63 (Ratinov et al., 2011) 75 83 82 56.", "startOffset": 5, "endOffset": 27}, {"referenceID": 1, "context": "2 (Cheng and Roth, 2013) 90 90 86 67.", "startOffset": 2, "endOffset": 24}, {"referenceID": 12, "context": "4 (Guo and Barbosa, 2016) 92 87 88 77 84.", "startOffset": 2, "endOffset": 25}, {"referenceID": 6, "context": "We use Adagrad (Duchi et al., 2011) with a learning rate of 0.", "startOffset": 15, "endOffset": 35}, {"referenceID": 0, "context": "We test and validate our entity embeddings on the respective parts of the entity relatedness dataset of (Ceccarelli et al., 2013).", "startOffset": 104, "endOffset": 129}, {"referenceID": 17, "context": "We use Adam (Kingma and Ba, 2014) with learning rate of 1e-4 until validation accuracy exceeds 90%, afterwards setting it to 1e5.", "startOffset": 12, "endOffset": 33}, {"referenceID": 12, "context": "Some baseline scores from Table 4 are taken from (Guo and Barbosa, 2016).", "startOffset": 49, "endOffset": 72}, {"referenceID": 29, "context": "The best results for the AIDA datasets are reported by (Yamada et al., 2016) and (Globerson et al.", "startOffset": 55, "endOffset": 76}, {"referenceID": 11, "context": ", 2016) and (Globerson et al., 2016).", "startOffset": 12, "endOffset": 36}, {"referenceID": 24, "context": "We do not compare against (Pershina et al., 2015) since, as noted also by (Globerson et al.", "startOffset": 26, "endOffset": 49}, {"referenceID": 11, "context": ", 2015) since, as noted also by (Globerson et al., 2016), their mention index artificially includes the gold entity (guaranteed gold recall).", "startOffset": 32, "endOffset": 56}], "year": 2017, "abstractText": "We propose a novel deep learning model for joint document-level entity disambiguation, which leverages learned neural representations. Key components are entity embeddings, a neural attention mechanism over local context windows, and a differentiable joint inference stage for disambiguation. Our approach thereby combines benefits of deep learning with more traditional approaches such as graphical models and probabilistic mention-entity maps. Extensive experiments show that we are able to obtain competitive or stateof-the-art accuracy at moderate computational costs.", "creator": "LaTeX with hyperref package"}}}