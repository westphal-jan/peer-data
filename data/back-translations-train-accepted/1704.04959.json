{"id": "1704.04959", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Apr-2017", "title": "Introspection: Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks. We use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "histories": [["v1", "Mon, 17 Apr 2017 13:23:36 GMT  (786kb,D)", "http://arxiv.org/abs/1704.04959v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["abhishek sinha", "mausoom sarkar", "aahitagni mukherjee", "balaji krishnamurthy"], "accepted": true, "id": "1704.04959"}, "pdf": {"name": "1704.04959.pdf", "metadata": {"source": "CRF", "title": "INTROSPECTION:ACCELERATING NEURAL NETWORK TRAINING BY LEARNING WEIGHT EVOLUTION", "authors": ["Abhishek Sinha", "Balaji Krishnamurthy"], "emails": [], "sections": [{"heading": null, "text": "Neural networks are functional approximation systems that have been state-of-the-art in numerous machine learning tasks. Despite their great success in terms of accuracy, their long training times make it difficult to use them for various tasks. In this paper, we explore the idea of learning weight development patterns from a simple network to accelerate the training of novel neural networks. We use a neural network to learn the training pattern from the MNIST classification and use it to accelerate the training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory consumption and is computationally efficient, and this method can also be used with other optimizers to achieve faster convergence. Results indicate a general trend in weight development during the training of neural networks."}, {"heading": "1 INTRODUCTION", "text": "However, the formation of a deep neural network for each AI task is a time-consuming process, because a large number of parameters need to be learned from training examples. Most deeper networks can take days to be trained on the GPU itself, making them a major bottleneck in the large-scale application of deep networks. Reducing training time by an efficient optimizer is essential for the fast design and testing of deep neural networks. In the context of neural networks, an optimization algorithm iteratively updates the parameters (weights) of a network based on a set of training examples to minimize objective function.The most common optimization algorithm is stochastic gradient descent. Even with the advent of newer and faster optimization algorithms such as Adagrad, Adadelta, RMSProp and Adam, there is a need to achieve faster convergence."}, {"heading": "2 RELATED WORK", "text": "Some of them are Momentum (Rumelhart et al., 1986), AdaGrad (Duchy et al., 2011), AdaDelta (Zeiler, 2012), RMSProp (Hinton et al., 2012) and Adam (Kingma & Ba, 2014), all of which reduce the convergence time by appropriately changing the learning rate during training, and our method can be used in conjunction with one of the above methods to further improve the convergence time. In the above approaches, the weight update is always a product of the gradient and the modified / unmodified learning rate."}, {"heading": "3 PATTERNS IN WEIGHT EVOLUTION", "text": "The development of weight shifts into neural networks trained on different classification tasks such as MNIST and CIFAR-10 datasets, and on different network architectures (weight shifts of different layers of fully connected as well as Convolutionary architectures), as well as on different optimization rules, was observed to follow a general trend depending on the task, which is a measure of how much a weight calendar deviates from its original value. In Figure 4, we show the frequency of weight changes: \u2022 The difference between the final and initial values of a scalar is a measure of how much a weight calendar deviates from its original value."}, {"heading": "3.1 WEIGHT PREDICTION", "text": "We collect the weight development trends of a network that is being trained, and use the collected data to train a neural network I to predict the future values of each weight based on its values in the previous time steps. Trained network I is then used to predict the weight values of an invisible network N during its training, which puts N in a state that allows faster convergence. The time required for the prognosis is comparatively smaller neural network, the inference time of which is negligible compared to the training time of the network to be trained (N). We call this predictor I introspection network, which is used for weight development during training, a comparatively smaller neural network whose inference time is negligible compared to the training time of the network to be trained (N). We call this predictor I introspection network, because it predicts the neural network during training, which is a neural network whose weight development is considered to be complete during the prognosis I, the prediction network I consists of four."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 TRAINING OF INTROSPECTION NETWORK", "text": "Introspection Network I is given on the training history of a network N0, which is gradually dependent on MNIST data. Network N0 consisted of 3 conventional layers and two fully connected layers. Forms of the conventional layer were used [5], 5, 8, 16."}, {"heading": "4.2 USING PRE-TRAINED INTROSPECTION NETWORK TO TRAIN UNSEEN NETWORKS", "text": "We illustrate our method by using it to accelerate the training of several deep neural networks with different architectures on 3 different datasets, namely MNIST, CIFAR-10 and ImageNet. We note that the same introspection network I, which was trained on the weight developments of the MNIST network N0, was used in all of these different cases. All networks were trained either with stochastic gradient descent or ADAM and Network I, which is used in some intermediate steps to bring the network into a state of higher accuracy. We refer to the time step in which the introspection network I is used to update all weights as a \"jumping point.\" The selection of steps in which I should be used depends on the distribution of the training step used for Training I."}, {"heading": "4.2.1 MNIST", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "4.2.2 CIFAR-10", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live, in which they live in which they live in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live in which they are able to live in which they live in which they"}, {"heading": "4.2.3 IMAGENET", "text": "To investigate the practicability and generalizability of our introspection network, we used it in the AlexNet training (Krizhevsky et al., 2012) (AlexNet1) on the ImageNet dataset (Russakovskyet al., 2015). It has 5 Conv layers and 3 fully connected layers. Max pooling and local reaction normalization were used after the two Starting Conv layers and the pooling layer is also present after the fifth Conv layer. We use SGD with a dynamic of 0.9 to train this network, starting with a learning rate of 0.01. The learning rate was reduced by one tenth per 100,000 iterations. The mini-battery size was 128. Approximately 300,000 steps are being taken for convergence. Weight updates were reduced at the training steps 120, 000, 130 000, 000, 144, 000 and 160, 000."}, {"heading": "4.3 COMPARISON WITH BASELINE TECHNIQUES", "text": "In this section, we offer a comparison with other optimizers and simple heuristics that can be used to update weights at different training steps, rather than updating them through an introspection network."}, {"heading": "4.4 COMPARISON WITH ADAM OPTIMIZER", "text": "We applied the introspection network to MNIST1 and MNIST3 networks trained with Adam Optimizer, with learning rates of 1e \u2212 4 and 1e \u2212 3. The results in Figure 22 and Figure 23 show that while Adam exceeded normal SGD and SGD with introspection, we were able to successfully apply and accelerate the introspection network to Adam Optimizer. In MNIST1, the maximum accuracy Adam achieved with introspection was 99.34%, in normal Adam 99.3%, in SGD with introspection 99.21%, and in normal SGD 99.08%. In introspection applied to Adam, the model reached the maximum accuracy that Adam achieved with introspection after only 7200 steps, while normal training required 10,000 steps. In MNIST3, Adam reached the maximum accuracy with Adam with introspection 95.9%, in normal Adam 96.9%, and in SNIST3, in normal Adam 96.9%."}, {"heading": "4.4.1 FITTING QUADRATIC CURVE", "text": "A separate square curve matched each of the weights of the model based on the 4 past weights selected from the history, and the weights selected from the history were in the same steps as those predicted by me.The new updated weight would be the value of the square curve at a later time. For other higher jump ratios, the updates would cause the model to deviate, and lower jump ratios did not show much improvement in performance.The graph showing the comparison in validation accuracy was shown below in Figure 24.Training Steps. The maximum accuracy achieved with the applied introspection was 99.21%, whereas with square adjustments Sratic Fit was. We note that although the best square fit was ultimately achieved almost the same maximum accuracy as that was achieved with the applied introspection."}, {"heading": "4.4.2 FITTING LINEAR CURVE", "text": "Instead of fitting a square curve to each of the weights, we tried to fit a linear curve. MNIST1 was used for experiments with jump ratios of 1.1 and 1.075, as the higher ratios would cause the model to drift apart after 2 or 3 jumps. Results are shown below in Figure 26 Training Steps. As no significant improvement in performance was observed, the experiment was not repeated."}, {"heading": "4.5 LINEAR INTROSPECTION NETWORK", "text": "We removed the ReLU nonlinearity from the introspection network and used the same training method as the normal introspection network to predict future values at 2t. Subsequently, we used this linear network in the MNIST1 network. We found that it offered some advantage over the normal SGD, but was not as good as the introspection network as shown in Figure 27."}, {"heading": "4.5.1 ADDING NOISE", "text": "The experiment was performed using MNIST1 for two different standard noise values, the results of which are shown in Figure 28 below. As no significant improvement in weight changes due to noise was observed for MNIST, the experiment was not performed using Cifar-10."}, {"heading": "5 LIMITATIONS AND OPEN QUESTIONS", "text": "Some of the open questions that need to be explored relate to determining the optimal jumping points and investigating the generalization capacity of the introspection network to accelerate training in RNNs and non-image tasks. Furthermore, we found that applying the jumps in very early training steps during training AlexNet1 tended to worsen the final results, which may be due to the fact that our introspection network is extremely simple and was only trained based on weight development data from the MNIST. A combination of a more powerful network and training data coming from a diverse set could mitigate this problem."}, {"heading": "6 CONCLUSION", "text": "We introduced a method to accelerate neural network training. To this end, we used a neural network I, which shows a general trend in the weight development of all neural networks. After learning the trend from neural network training, I updated the weight of many deep neural networks in three different tasks - MNIST, CIFAR-10 and ImageNet, with different network architectures, activations, optimizers and normalization strategies (batch standard, Lrn). Using the introspection network, I led in all cases to a faster convergence compared to existing methods. Our method has a small storage space, is computationally efficient and can be used in practical environments. Our method differs from other existing methods in that it uses the knowledge from weights of neural network training to accelerate the training of several invisible networks in new tasks. The results reported here on each basic pattern's existence."}, {"heading": "A APPENDIX", "text": "In this section we report on some initial results of the application of the introspection network I (trained after the weight development of the MNIST network N0) to accelerate the formation of the introspection network v1 (Szegedy et al., 2014).We trained the introspection network v1 on imagenet datasets with a mini batch size of 128 and an RMS optimizer (decay 0.9, impulse 0.9, epsilon 1.0) starting from a learning rate of 0.01 with a decay of 0.94 after two epochs. Network training is still ongoing, and we will eventually report on the final result. However, we thought it would be valuable to share the preliminary results evenly. We found that the application of the introspection network appears to significantly reduce the training time. In Figures 29 and 30 we see that the application of the introspection network leads to a gain of at least 730,000 steps for the normal training, after approximately 1.5 million steps have been achieved."}], "references": [{"title": "Learning to learn by gradient descent by gradient descent", "author": ["Marcin Andrychowicz", "Misha Denil", "Sergio Gomez", "Matthew W. Hoffman", "David Pfau", "Tom Schaul", "Nando de Frietas"], "venue": null, "citeRegEx": "Andrychowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz et al\\.", "year": 2016}, {"title": "Adaptive Subgradients Method For Online Learning and Stochastic Optimization", "author": ["John Duchy", "Elad Hazan", "Yoram Singer"], "venue": null, "citeRegEx": "Duchy et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchy et al\\.", "year": 2011}, {"title": "Deep Q-Networks for Accelerating the Training of Deep Neural Networks. 2016", "author": ["Zie Fu", "Zichuan Lin", "Danlu Chen", "Miau Liu", "Nicholas Leonard", "Jiashi Feng", "Tat-Seng Chua"], "venue": "URL https: //arxiv.org/pdf/1606.01467v3.pdf", "citeRegEx": "Fu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fu et al\\.", "year": 2016}, {"title": "Lecture 6a:Overview of mini-batch gradient descent", "author": ["Geoffrey Hinton", "Nitish Srivastava", "Kevin Swersky"], "venue": "URL https://class.coursera.org/neuralnets-2012-001/ lecture", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Decoupled Neural Interfaces using Synthetic Gradients", "author": ["Max Jaderberg", "Wojciech M. Czarnecki", "Simon Osindero", "Oriol Vinyals", "Alex Graves", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "Jaderberg et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2016}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["Alex Krizhevsky"], "venue": null, "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "URL https://papers.nips.cc/paper/ 4824-imagenet-classification-with-deep-convolutional-neural-networks. pdf", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Learning representations by backpropagating errors", "author": ["David Rumelhart", "Geoffrey Hinton", "Ronald Williams"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Adadelta:An adaptive learning method", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "Some of them are Momentum (Rumelhart et al., 1986), AdaGrad (Duchy et al.", "startOffset": 26, "endOffset": 50}, {"referenceID": 1, "context": ", 1986), AdaGrad (Duchy et al., 2011), AdaDelta (Zeiler, 2012), RMSProp (Hinton et al.", "startOffset": 17, "endOffset": 37}, {"referenceID": 8, "context": ", 2011), AdaDelta (Zeiler, 2012), RMSProp (Hinton et al.", "startOffset": 18, "endOffset": 32}, {"referenceID": 3, "context": ", 2011), AdaDelta (Zeiler, 2012), RMSProp (Hinton et al., 2012) and Adam (Kingma & Ba, 2014).", "startOffset": 42, "endOffset": 63}, {"referenceID": 0, "context": "More recent approaches (Andrychowicz et al., 2016) have tried to learn the function that takes as input the gradient and outputs the appropriate weight update.", "startOffset": 23, "endOffset": 50}, {"referenceID": 0, "context": "Further our approach has far lesser memory footprint as compared to (Andrychowicz et al., 2016).", "startOffset": 68, "endOffset": 95}, {"referenceID": 2, "context": "Another recent approach, called Q-gradient descent (Fu et al., 2016), uses a reinforcement learning framework to tune the hyperparameters of the optimization algorithm as the training progresses.", "startOffset": 51, "endOffset": 68}, {"referenceID": 4, "context": "Finally the recent approach by (Jaderberg et al., 2016) to predict synthetic gradients is similar to our work, in the sense that the weights are updates independently, but it still relies on an estimation of the gradient, while our update method does not.", "startOffset": 31, "endOffset": 55}, {"referenceID": 5, "context": "2 CIFAR-10 We applied our introspection network I on a CNN CIFAR1 for classifying images in the CIFAR10 (Krizhevsky, 2009) dataset.", "startOffset": 104, "endOffset": 122}, {"referenceID": 6, "context": "3 IMAGENET To investigate the practical feasibility and generalization ability of our introspection network, we applied it in training AlexNet(Krizhevsky et al., 2012) (AlexNet1) on the ImageNet (Russakovsky", "startOffset": 142, "endOffset": 167}], "year": 2017, "abstractText": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks. We use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "creator": "LaTeX with hyperref package"}}}