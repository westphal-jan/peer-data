{"id": "1704.05021", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Apr-2017", "title": "Sparse Communication for Distributed Gradient Descent", "abstract": "We make distributed stochastic gradient descent faster by exchanging 99% sparse updates instead of dense updates. In data-parallel training, nodes pull updated values of the parameters from a sharded server, compute gradients, push their gradients to the server, and repeat. These push and pull updates strain the network. However, most updates are near zero, so we map the 99% smallest updates (by absolute value) to zero then exchange sparse matrices. Even simple coordinate and value encoding achieves 50x reduction in bandwidth. Our experiment with a neural machine translation on 4 GPUs achieved a 22% speed boost without impacting BLEU score.", "histories": [["v1", "Mon, 17 Apr 2017 16:32:02 GMT  (208kb,D)", "https://arxiv.org/abs/1704.05021v1", "Submitted to EMNLP 2017"], ["v2", "Mon, 24 Jul 2017 21:47:51 GMT  (275kb,D)", "http://arxiv.org/abs/1704.05021v2", "EMNLP 2017"]], "COMMENTS": "Submitted to EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.DC cs.LG", "authors": ["alham fikri aji", "kenneth heafield"], "accepted": true, "id": "1704.05021"}, "pdf": {"name": "1704.05021.pdf", "metadata": {"source": "CRF", "title": "Sparse Communication for Distributed Gradient Descent", "authors": ["Alham Fikri", "Kenneth Heafield"], "emails": ["a.fikri@ed.ac.uk,", "kheafiel@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "By replacing sparse updates instead of dense updates, we accelerate the reduction of the distributed stochastic gradient. Grade updates are positively distorted, as most updates are close to zero, so we map the 99% smallest updates (by absolute value) to zero and then replace sparse matrices. This method can be combined with quantification to further improve compression. We study different configurations and apply them to neural machine translation and MNIST image classification tasks. Most configurations work with MNIST, while different configurations reduce the convergence rate for the more complex translation task. Our experiments show that we can achieve acceleration of up to 49% at MNIST and 22% at NMT without compromising final accuracy or BLEU."}, {"heading": "1 Introduction", "text": "We focus on data parallelism: nodes jointly optimize the same model on different parts of the training data by exchanging gradients and parameters over the network (Strom, 2015; Dryden et al., 2016). Since this network communication is costly, we developed and tested two methods to approximate compression of network traffic: 1-bit quantization (Silk et al., 2014) and sending sparse matrices by dropping small updates (Strom, 2015; Dryden et al., 2016), which have been developed and tested in the field of speech recognition and toy MNIST systems. When porting these approaches to neural machine translation (NMT) (N eco and Forcada, 1996; Bahdanau et al., 2014), we find that translation is less tolerant to quantization."}, {"heading": "2 Related Work", "text": "An orthogonal workline optimizes the SGD algorithm and communication pattern. Zinkevich et al. (2010) proposed an asynchronous architecture in which each node can independently move and pull the model to avoid waiting for the slower node. Chilimbi et al. (2014) and Recht et al. (2011) propose an update of the model without a lock that allows for racing conditions. In addition, Dean et al. (2012) execute several minibatches before replacing the updates, which reduces communication costs. Our work is a more continuous version in which the most important updates are sent between minibatches. Zhang et al. (2015) reduce weight gradients based on static parameters. Approximate weight compression is not a new idea. 1-bit SGD (Silk et al., 2014), and later quantization SGD (Alistarh et al. 2016), can be reduced by transforming the weight fluctuation of the BGD into 1."}, {"heading": "3 Distributed SGD", "text": "We used distributed SGD with parameter splitting (Dean et al., 2012), shown in Figure 1. Each of the N workers is both a client and a server. Servers are responsible for 1 / N of the parameters. Clients have a copy of all the parameters they use to calculate gradients. These gradients are divided into N pieces and forwarded to the corresponding servers. Likewise, each client draws parameters from all servers. Each node communicates with all N nodes regarding 1 / N of the parameters, so that the bandwidth per node is constant."}, {"heading": "4 Sparse Gradient Exchange", "text": "This approach differs slightly from Dryden et al. (2016) in that we have used a single threshold based on absolute value rather than deleting the positive and negative gradients separately, but this is easier to perform and works just as well. Small gradients can accumulate over time and we find that they nullify convergence. According to Silde et al. (2014) we remember residuals (in our case decreased values) locally and add them to the next gradient before dropping them again. Algorithm 1 Gradient-falling algorithms that exceed the gradient and the falling rate threshold."}, {"heading": "5 Experiment", "text": "We are experimenting with an image classification task based on the MNIST dataset (LeCun et al., 1998) and the Romanian \u2192 English Neural Machine Translation System. For our image classification experiment, we are building a fully networked neural network with three hidden 4069 neuron layers. We are using AdaGrad with an initial learning rate of 0.005. The mini-stack size of 40 is used. This setup is identical to Dryden et al. (2016).Our NMT experiment is based on Sennrich et al. (2016), which won first place in the 2016 Machine Translation Workshop. It is based on an attentive encoder decoder LSTM with 119M parameters. The default stack size is 80. We store and validate all 10,000 steps. We select 4 stored models with the highest validation BLEU and cut them into the final model."}, {"heading": "5.1 Drop Ratio", "text": "To find an appropriate R% waste ratio, we measured 90%, 99% and 99.9% and then measured the performance in terms of losses and classification accuracy or translation quality that BLEU (Papineni et al., 2002) approximated for image classification and NMT task. Figure 3 shows that the model is still learning after dropping 99.9% of the gradients, albeit with a worse BLEU value. However, the 99% drop in the gradient has little impact on convergence or BLEU, even though 50x less data has been replaced by offset value coding. The x-axis in both charts is stacked, showing that we do not rely on speed improvements to offset convergence. Dryden et al. (2016) used a fixed waste ratio of 98.4% without testing other options."}, {"heading": "5.2 Local vs Global Threshold", "text": "The results show that the normalization of the layers has no visible effect on the MNIST. On the other hand, our NMT system performs poorly because the parameters are located on different scales without the layer normalization and the global threshold is undermet. Furthermore, our NMT system has more parameter categories than the 3-layer network of the MNIST."}, {"heading": "5.3 Convergence Rate", "text": "In our NMT experiment with 4 Titan Xs, the communication time is only about 13% of the total training time. If 99% of the gradient falls, this results in an 11% speed improvement. In addition, we added an additional NMT experiment with a batch size of 32 to achieve a higher communication cost ratio. In this scenario, the communication time is 17% of the total training time and we see an average speed improvement of 22%. In MNIST, the communication is 41% of the total training time and we see an average speed improvement of 49%. In NMT, the calculation was made by reducing multitasking. We investigate the convergence rate: the combination of loss and speed. In MNIST, we train the model for 20 epochs, as mentioned in Dryden et al. (2016)."}, {"heading": "5.4 1-Bit Quantization", "text": "Dryden et al. (2016) took the mean values of the values to be quantified, as is more common, and also quantified at the column level, rather than selecting centers globally. We tested the 1-bit quantization with 3 different configurations: threshold, column-by-column average, and global average. Quantization is applied after the drop in the gradient, with a 99% drop rate, layer normalization, and a global threshold. Figure 8 shows that the 1-bit quantification slows the convergence rate for NMT. This differs from previous work (Seide et al., 2014; Dryden et al., 2016), which reported no effects from 1-bit quantification. Nevertheless, we agree with their experiments: All types of quantization work tested on MNIST. This underscores the need for a variety of tasks in experiments. NT et al., 2016), which does not have an impact from 1-bit quantification, so that quantification produces 1-bit more."}, {"heading": "6 Conclusion and Future Work", "text": "This can be exploited by keeping 99% of the gradient updates local and reducing the communication size to 50 times with a coordinate encoding. Previous work suggested that a 1-bit quantification could be used to further compress the communication. However, we found that this does not apply to NMT. We attribute this to an imbalance in the embedding levels. However, a 2-bit quantification is probably sufficient to separate large movers from small changes. In addition, our NMT system consists of many parameters with different scales, so normalization of the layers or the use of local thresholds is necessary. On the other hand, MNIST seems to work with all the configurations we have tried. Our experiment with 4 Titan X shows that on average only 17% of the time is spent on communication (at batch size 32) and we achieve 22% acceleration. Our future work is to test this approach on costly communication environments such as environments."}, {"heading": "Acknowledgments", "text": "Alham Fikri Aji is funded by the scholarship program of the Indonesia Endowment Fund for Education. Marcin Junczys-Dowmunt wrote the distributed base code and advised on this implementation. We thank the staff of the School of Informatics Computing and Finance for working around the NVIDIA limit of two Pascal Titan X per customer. Computing was funded by the Amazon Academic Research Awards program and Microsoft's donation of Azure Time to the Alan Turing Institute, which was supported by the Alan Turing Institute under the EPSRC grant EP / N510129 / 1."}], "references": [{"title": "Fast k-selection algorithms for graphics processing units", "author": ["Tolu Alabi", "Jeffrey D Blanchard", "Bradley Gordon", "Russel Steinbach."], "venue": "Journal of Experimental Algorithmics (JEA) 17:4\u20132.", "citeRegEx": "Alabi et al\\.,? 2012", "shortCiteRegEx": "Alabi et al\\.", "year": 2012}, {"title": "QSGD: randomized quantization for communication-optimal stochastic gradient descent", "author": ["Dan Alistarh", "Jerry Li", "Ryota Tomioka", "Milan Vojnovic."], "venue": "CoRR abs/1610.02132. http://arxiv.org/abs/1610.02132.", "citeRegEx": "Alistarh et al\\.,? 2016", "shortCiteRegEx": "Alistarh et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Project adam: Building an efficient and scalable deep learning training system", "author": ["Trishul M Chilimbi", "Yutaka Suzue", "Johnson Apacible", "Karthik Kalyanaraman."], "venue": "OSDI. volume 14, pages 571\u2013 582.", "citeRegEx": "Chilimbi et al\\.,? 2014", "shortCiteRegEx": "Chilimbi et al\\.", "year": 2014}, {"title": "Large scale distributed deep networks. In Advances in neural information processing systems", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": null, "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Communication quantization for data-parallel training of deep neural networks", "author": ["Nikoli Dryden", "Sam Ade Jacobs", "Tim Moon", "Brian Van Essen."], "venue": "Proceedings of the Workshop on Machine Learning in High Performance Computing Environments.", "citeRegEx": "Dryden et al\\.,? 2016", "shortCiteRegEx": "Dryden et al\\.", "year": 2016}, {"title": "Is neural machine translation ready for deployment? A case study on 30 translation directions", "author": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Hieu Hoang."], "venue": "Program of the 13th International Workshop on Spoken Language Translation (IWSLT", "citeRegEx": "Junczys.Dowmunt et al\\.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner."], "venue": "Proceedings of the IEEE 86(11):2278\u20132324.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Layer Normalization", "author": ["J. Lei Ba", "J.R. Kiros", "G.E. Hinton."], "venue": "ArXiv e-prints .", "citeRegEx": "Ba et al\\.,? 2016", "shortCiteRegEx": "Ba et al\\.", "year": 2016}, {"title": "Beyond mealy machines: Learning translators with recurrent neural networks", "author": ["Ram\u00f3n P \u00d1eco", "Mikel L Forcada."], "venue": "Proceedings of the 1996 International Neural Network Society Annual Meeting. San Diego, California, USA.", "citeRegEx": "\u00d1eco and Forcada.,? 1996", "shortCiteRegEx": "\u00d1eco and Forcada.", "year": 1996}, {"title": "BLEU: A method for automatic evalution of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Large-scale deep unsupervised learning using graphics processors", "author": ["Rajat Raina", "Anand Madhavan", "Andrew Y Ng."], "venue": "Proceedings of the 26th annual international conference on machine learning. ACM, pages 873\u2013880.", "citeRegEx": "Raina et al\\.,? 2009", "shortCiteRegEx": "Raina et al\\.", "year": 2009}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Benjamin Recht", "Christopher Re", "Stephen Wright", "Feng Niu."], "venue": "J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors,", "citeRegEx": "Recht et al\\.,? 2011", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "1-bit stochastic gradient descent and application to data-parallel distributed training of speech DNNs", "author": ["Frank Seide", "Hao Fu", "Jasha Droppo", "Gang Li", "Dong Yu."], "venue": "Interspeech. https://www.microsoft.com/en-", "citeRegEx": "Seide et al\\.,? 2014", "shortCiteRegEx": "Seide et al\\.", "year": 2014}, {"title": "Edinburgh neural machine translation systems for WMT 16", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the ACL 2016 First Conference on Machine Translation (WMT16).", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Scalable distributed dnn training using commodity gpu cloud computing", "author": ["Nikko Strom."], "venue": "INTERSPEECH. volume 7, page 10.", "citeRegEx": "Strom.,? 2015", "shortCiteRegEx": "Strom.", "year": 2015}, {"title": "Staleness-aware async-sgd for distributed deep learning", "author": ["Wei Zhang", "Suyog Gupta", "Xiangru Lian", "Ji Liu."], "venue": "CoRR abs/1511.05950. http://arxiv.org/abs/1511.05950.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Parallelized stochastic gradient descent", "author": ["Martin Zinkevich", "Markus Weimer", "Lihong Li", "Alex J Smola."], "venue": "Advances in neural information processing systems. pages 2595\u20132603.", "citeRegEx": "Zinkevich et al\\.,? 2010", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2010}, {"title": "CRC standard probability and statistics tables and formulae", "author": ["Daniel Zwillinger", "Stephen Kokoska."], "venue": "CRC Press.", "citeRegEx": "Zwillinger and Kokoska.,? 1999", "shortCiteRegEx": "Zwillinger and Kokoska.", "year": 1999}], "referenceMentions": [{"referenceID": 11, "context": "Distributed computing is essential to train large neural networks on large data sets (Raina et al., 2009).", "startOffset": 85, "endOffset": 105}, {"referenceID": 13, "context": "This network communication is costly, so prior work developed two ways to approximately compress network traffic: 1-bit quantization (Seide et al., 2014) and sending sparse matrices by dropping small updates (Strom, 2015; Dryden et al.", "startOffset": 133, "endOffset": 153}, {"referenceID": 15, "context": ", 2014) and sending sparse matrices by dropping small updates (Strom, 2015; Dryden et al., 2016).", "startOffset": 62, "endOffset": 96}, {"referenceID": 5, "context": ", 2014) and sending sparse matrices by dropping small updates (Strom, 2015; Dryden et al., 2016).", "startOffset": 62, "endOffset": 96}, {"referenceID": 9, "context": "In porting these approximations to neural machine translation (NMT) (\u00d1eco and Forcada, 1996; Bahdanau et al., 2014), we find that translation is less tolerant to quantization.", "startOffset": 68, "endOffset": 115}, {"referenceID": 2, "context": "In porting these approximations to neural machine translation (NMT) (\u00d1eco and Forcada, 1996; Bahdanau et al., 2014), we find that translation is less tolerant to quantization.", "startOffset": 68, "endOffset": 115}, {"referenceID": 5, "context": "Throughout this paper, we compare neural machine translation behavior with a toy MNIST system, chosen because prior work used a similar system (Dryden et al., 2016).", "startOffset": 143, "endOffset": 164}, {"referenceID": 18, "context": "More formally, gradient updates have positive skewness coefficient (Zwillinger and Kokoska, 1999): most are close to zero but a few are large.", "startOffset": 67, "endOffset": 97}, {"referenceID": 13, "context": "Zinkevich et al. (2010) proposed an asynchronous architecture where each node can push and pull the model independently to avoid waiting for the slower node.", "startOffset": 0, "endOffset": 24}, {"referenceID": 3, "context": "Chilimbi et al. (2014) and Recht et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 3, "context": "Chilimbi et al. (2014) and Recht et al. (2011) suggest updating the model without a lock, allowing race conditions.", "startOffset": 0, "endOffset": 47}, {"referenceID": 3, "context": "Chilimbi et al. (2014) and Recht et al. (2011) suggest updating the model without a lock, allowing race conditions. Additionally, Dean et al. (2012) run multiple minibatches before exchanging updates, reducing the communication cost.", "startOffset": 0, "endOffset": 149}, {"referenceID": 3, "context": "Chilimbi et al. (2014) and Recht et al. (2011) suggest updating the model without a lock, allowing race conditions. Additionally, Dean et al. (2012) run multiple minibatches before exchanging updates, reducing the communication cost. Our work is a more continuous version, in which the most important updates are sent between minibatches. Zhang et al. (2015) downweight gradients based on stale parameters.", "startOffset": 0, "endOffset": 359}, {"referenceID": 13, "context": "1-Bit SGD (Seide et al., 2014), and later Quantization SGD (Alistarh et al.", "startOffset": 10, "endOffset": 30}, {"referenceID": 1, "context": ", 2014), and later Quantization SGD (Alistarh et al., 2016), work by converting the gradient update into a 1-bit matrix, thus reducing data communication significantly.", "startOffset": 36, "endOffset": 59}, {"referenceID": 1, "context": ", 2014), and later Quantization SGD (Alistarh et al., 2016), work by converting the gradient update into a 1-bit matrix, thus reducing data communication significantly. Strom (2015) proposed threshold quantizaar X iv :1 70 4.", "startOffset": 37, "endOffset": 182}, {"referenceID": 5, "context": "Dryden et al. (2016) set the threshold so as to keep a constant number of gradients each iteration.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "We used distributed SGD with parameter sharding (Dean et al., 2012), shown in Figure 1.", "startOffset": 48, "endOffset": 67}, {"referenceID": 5, "context": "This approach is slightly different from Dryden et al. (2016) as we used a single threshold based on absolute value, instead of dropping the positive and negative gradients separately.", "startOffset": 41, "endOffset": 62}, {"referenceID": 13, "context": "Following Seide et al. (2014), we remember residuals (in our case dropped values) locally and add them to the next gradient, before dropping again.", "startOffset": 10, "endOffset": 30}, {"referenceID": 0, "context": "Selection to obtain the threshold is expensive (Alabi et al., 2012).", "startOffset": 47, "endOffset": 67}, {"referenceID": 7, "context": "We experiment with an image classification task based on MNIST dataset (LeCun et al., 1998) and Romanian\u2192English neural machine translation system.", "startOffset": 71, "endOffset": 91}, {"referenceID": 5, "context": "This setup is identical to Dryden et al. (2016). Our NMT experiment is based on Sennrich et al.", "startOffset": 27, "endOffset": 48}, {"referenceID": 5, "context": "This setup is identical to Dryden et al. (2016). Our NMT experiment is based on Sennrich et al. (2016), which won first place in the 2016 Workshop on Machine Translation.", "startOffset": 27, "endOffset": 103}, {"referenceID": 6, "context": "AmuNMT (Junczys-Dowmunt et al., 2016) is used for decoding with a beam size of 12.", "startOffset": 7, "endOffset": 37}, {"referenceID": 10, "context": "9% then measured performance in terms of loss and classification accuracy or translation quality approximated by BLEU (Papineni et al., 2002) for image classification and NMT task respectively.", "startOffset": 118, "endOffset": 141}, {"referenceID": 5, "context": "For MNIST, we train the model for 20 epochs as mentioned in Dryden et al. (2016). For NMT, we tested this with batch sizes of 80 and 32 and trained for 13.", "startOffset": 60, "endOffset": 81}, {"referenceID": 14, "context": "Strom (2015) quantized simply by mapping all surviving values to the dropping threshold, effectively the minimum surviving absolute value.", "startOffset": 0, "endOffset": 13}, {"referenceID": 5, "context": "Dryden et al. (2016) took the averages of values being quantized, as is more standard.", "startOffset": 0, "endOffset": 21}, {"referenceID": 13, "context": "This differs from prior work (Seide et al., 2014; Dryden et al., 2016) which reported no impact from 1-bit quantization.", "startOffset": 29, "endOffset": 70}, {"referenceID": 5, "context": "This differs from prior work (Seide et al., 2014; Dryden et al., 2016) which reported no impact from 1-bit quantization.", "startOffset": 29, "endOffset": 70}], "year": 2017, "abstractText": "We make distributed stochastic gradient descent faster by exchanging sparse updates instead of dense updates. Gradient updates are positively skewed as most updates are near zero, so we map the 99% smallest updates (by absolute value) to zero then exchange sparse matrices. This method can be combined with quantization to further improve the compression. We explore different configurations and apply them to neural machine translation and MNIST image classification tasks. Most configurations work on MNIST, whereas different configurations reduce convergence rate on the more complex translation task. Our experiments show that we can achieve up to 49% speed up on MNIST and 22% on NMT without damaging the final accuracy or BLEU.", "creator": "LaTeX with hyperref package"}}}