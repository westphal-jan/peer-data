{"id": "1704.05119", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Apr-2017", "title": "Exploring Sparsity in Recurrent Neural Networks", "abstract": "Recurrent Neural Networks (RNN) are widely used to solve a variety of problems and as the quantity of data and the amount of available compute have increased, so have model sizes. The number of parameters in recent state-of-the-art networks makes them hard to deploy, especially on mobile phones and embedded devices. The challenge is due to both the size of the model and the time it takes to evaluate it. In order to deploy these RNNs efficiently, we propose a technique to reduce the parameters of a network by pruning weights during the initial training of the network. At the end of training, the parameters of the network are sparse while accuracy is still close to the original dense neural network. The network size is reduced by 8x and the time required to train the model remains constant. Additionally, we can prune a larger dense network to achieve better than baseline performance while still reducing the total number of parameters significantly. Pruning RNNs reduces the size of the model and can also help achieve significant inference time speed-up using sparse matrix multiply. Benchmarks show that using our technique model size can be reduced by 90% and speed-up is around 2x to 7x.", "histories": [["v1", "Mon, 17 Apr 2017 20:42:05 GMT  (259kb,D)", "http://arxiv.org/abs/1704.05119v1", "Published as a conference paper at ICLR 2017"]], "COMMENTS": "Published as a conference paper at ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["sharan narang", "gregory diamos", "shubho sengupta", "erich elsen"], "accepted": true, "id": "1704.05119"}, "pdf": {"name": "1704.05119.pdf", "metadata": {"source": "CRF", "title": "RECURRENT NEURAL NETWORKS", "authors": ["Sharan Narang", "Greg Diamos", "Shubho Sengupta", "Erich Elsen"], "emails": ["sharan@baidu.com", "gdiamos@baidu.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, we have exploded in several areas such as speech recognition (Graves & Jaitly, 2014; Amodei et al., 2015), speech modeling (Jo \u0301 zefowicz et al., 2016) and machine translation (Wu et al., 2016), which at least partially relies on larger amounts of data, larger models and more computing power that allow larger volumes of data to be distributed over larger volumes. Thus, for example, the deep neural network used for acoustic modeling in Hannun et al. (2014) has 11 million parameters, which have grown to about 67 million for bi-directional RNNs, and further to 116 million for the newest GRU models. (2015) And in speech modeling of non-embedding parameters (mostly in the recurrent layers), we have even exploded as different kinds of manual thrift in embedding."}, {"heading": "2 RELATED WORK", "text": "rE \"s rf\u00fc ide rf\u00fc ide rf\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc"}, {"heading": "3 IMPLEMENTATION", "text": "After each optimization step, each weight is multiplied by its corresponding mask. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "4 EXPERIMENTS", "text": "We perform all our experiments with a training set of 2100 hours of English language data and a validation set of 3.5 hours of multi-speaker data. This is a small subset of the data sets we use to train our state-of-the-art automatic speech recognition models. We train the models with Nesterov SGD for 20 epochs. Besides the threshold hyperparameters, all other hyperparameters between the dense and sparse training runs remain unchanged."}, {"heading": "4.1 BIDIRECTIONAL RNNS", "text": "This year it has come to the point where we will be able to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \""}, {"heading": "4.2 GATED RECURRENT UNITS", "text": "We also experimented with GRU models in Table 5, which have 2,560 hidden units in the GRU layer and a total of 115 million parameters. For these experiments, we trim all layers except the folding layers, as they have relatively fewer parameters. Figure 1b compares the training and development curves of a sparse GRU model with a dense GRU model. The sparse GRU model shows a 13.8% decrease in accuracy relative to the dense model. As shown in Table 3, the sparse model has an overall spareness of 88.6% with 13 million parameters. Similar to the RNN models, we train a sparse GRU model with 3,568 hidden units. The dataset and hyperparameters are not changed by the previous GRU experiments. This model has an overall spareness of 91.82% with 17.8 million parameters. As shown in Table 3, the model with 3568 hidden units is only 2.2% worse than the base model with the GRU density."}, {"heading": "5 PERFORMANCE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 COMPUTE TIME", "text": "The success of deep learning in recent years has been driven by large models trained on large data sets, but this also increases the time after models are deployed. We can mitigate this effect by using sparse layers. A general matrix matrix multiply (GEMM) is the most computationally intensive operation when evaluating a neural network model. Table 6 compares the times for GEMM for recursive layers with different numbers of hidden units, which are 95% sparse. The performance benchmark was performed using NVIDIA's CUDNN and cuSPARSE libraries on a TitanX Maxwell GPU and compiled with CUDA 7.5. All experiments are performed with a minibatch of 1, and in this case the operation is known as a sparse matrix vector product (SpMV). We can achieve speed-ups ranging from 3x to 1.15x, depending on the size of the recursive layer."}, {"heading": "5.2 COMPRESSION", "text": "The Deep Speech 2 model can be compressed from 268 MB to about 32 MB (1760 hidden units) or 64 MB (3072 hidden units), and the GRU model can be compressed from 460 MB to 50 MB. These truncated models can be further quantified to float16 or other smaller data types to further reduce memory consumption without compromising accuracy."}, {"heading": "6 DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 PRUNING CHARACTERISTICS", "text": "The layers are arranged so that layer 1 comes closest to the input and layer 14 is the last recurring layer before the cost layer. We see that the initial layers are pruned more aggressively than the last layers. We also conducted experiments where the hyperparameters for the recurring layers are different, resulting in the same sparseness for all layers. However, we get a higher CER for these experiments. We conclude that for good accuracy it is important to prune the last layers slightly less than the original layers. In Figure 2b, we plot the intersection plan of a 95% sparsely recurring layer of the bidirectional model trained for 20 epochs (55,000 iterations). We start pruning the network at the beginning of the second epoch at 2700 iterations. We stop working one layer after 10 epochs (half of the total 25,000 iterations we complete at 15,000 iterations)."}, {"heading": "6.2 PERSISTENT KERNELS", "text": "Persistent Recurrent Neural Networks (Diamos et al., 2016) is a technique that increases the computational intensity of the evaluation of an RNN by caching the weights in on-chip memory such as caches, block RAM or register files across multiple time passes. A high degree of sparseness enables the storage of significantly large persistent RNNNs in on-chip memory. If all weights are stored in float16, an NVIDIA P100 GPU can support a vanilla RNN size of about 2600 hidden units. Using the same data type, at 90% sparseness and 99% sparseness, a P100 RNNU can support about 8000 or 24000 hidden units, respectively. We expect these cores to be limited from the memory used to store the parameters, which offers the potential of 146 times sparseness compared to the TitanX GPU, if the entire NN can be stored in the read-only RN instead of the RAM."}, {"heading": "7 CONCLUSION AND FUTURE WORK", "text": "We have shown that by trimming the weights of RNs during training, we can find sparser models that are more precise than dense models and significantly reduce the model size. These sparse models, due to their small size and increased computing power, are particularly suitable for use on mobile devices and on back-end server farms. Even with existing suboptimal sparse matrix vector libraries, we are implementing accelerations with these models. This technique is orthogonal to quantization techniques that would allow even further reductions in model size and corresponding performance increases. We want to investigate whether these techniques can be generalized to voice modeling tasks and whether they can effectively reduce the size of the embedding layers. We also want to compare the sparseness generated by our cutting technique with the L1 regulation. We are investigating training techniques that do not require dense matrices for a significant part of the calculation."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Bryan Catanzaro for helpful discussions in connection with this work."}], "references": [{"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Dario Amodei", "Rishita Anubhai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Jingdong Chen", "Mike Chrzanowski", "Adam Coates", "Greg Diamos"], "venue": "arXiv preprint arXiv:1512.02595,", "citeRegEx": "Amodei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Amodei et al\\.", "year": 2015}, {"title": "Moderngpu, 2016. URL https://nvlabs.github.io/moderngpu/ segreduce.html", "author": ["Sean Baxter"], "venue": null, "citeRegEx": "Baxter.,? \\Q2016\\E", "shortCiteRegEx": "Baxter.", "year": 2016}, {"title": "Strategies for training large vocabulary neural language models", "author": ["Welin Chen", "David Grangier", "Michael Auli"], "venue": "CoRR, abs/1512.04906,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Compressing neural networks with the hashing trick", "author": ["Wenlin Chen", "James T. Wilson", "Stephen Tyree", "Kilian Q. Weinberger", "Yixin Chen"], "venue": "CoRR, abs/1504.04788,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Marc\u2019Aurelio Ranzato", "Nando de Freitas"], "venue": "CoRR, abs/1306.0543,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Emily Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus"], "venue": "CoRR, abs/1404.0736,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Persistent rnns: Stashing recurrent weights onchip", "author": ["Greg Diamos", "Shubho Sengupta", "Bryan Catanzaro", "Mike Chrzanowski", "Adam Coates", "Erich Elsen", "Jesse Engel", "Awni Hannun", "Sanjeev Satheesh"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Diamos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Diamos et al\\.", "year": 2016}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir D. Bourdev"], "venue": "CoRR, abs/1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "In ICML,", "citeRegEx": "Graves and Jaitly.,? \\Q2014\\E", "shortCiteRegEx": "Graves and Jaitly.", "year": 2014}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "CoRR, abs/1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Advances in neural information processing systems 1. chapter Comparing Biases for Minimal Network Construction with Back-propagation, pp. 177\u2013185", "author": ["Stephen Jos\u00e9 Hanson", "Lorien Pratt"], "venue": "URL http://dl.acm.org/citation.cfm?id=89851.89872", "citeRegEx": "Hanson and Pratt.,? \\Q1989\\E", "shortCiteRegEx": "Hanson and Pratt.", "year": 1989}, {"title": "Optimal brain surgeon and general network pruning", "author": ["Babak Hassibi", "David G Stork", "Gregory J Wolff"], "venue": "In Neural Networks,", "citeRegEx": "Hassibi et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hassibi et al\\.", "year": 1993}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "CoRR, abs/1405.3866,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Exploring the limits of language modeling", "author": ["Rafal J\u00f3zefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "CoRR, abs/1602.02410,", "citeRegEx": "J\u00f3zefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2016}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S Denker", "Sara A Solla", "Richard E Howard", "Lawrence D Jackel"], "venue": "In NIPs,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Efficient sparse matrix-vector multiplication on x86-based many-core processors", "author": ["Xing Liu", "Mikhail Smelyanskiy", "Edmond Chow", "Pradeep Dubey"], "venue": "In Proceedings of the 27th International ACM Conference on International Conference on Supercomputing,", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Learning compact recurrent neural networks", "author": ["Zhiyun Lu", "Vikas Sindhwani", "Tara N. Sainath"], "venue": "CoRR, abs/1604.02594,", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Improving the speed of neural networks on cpus", "author": ["Vincent Vanhoucke", "Andrew Senior", "Mark Z. Mao"], "venue": "In Deep Learning and Unsupervised Feature Learning Workshop,", "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}, {"title": "Exploiting sparseness in deep neural networks for large vocabulary speech recognition", "author": ["Dong Yu", "Frank Seide", "Gang Li", "Li Deng"], "venue": "In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Recent advances in multiple fields such as speech recognition (Graves & Jaitly, 2014; Amodei et al., 2015), language modeling (J\u00f3zefowicz et al.", "startOffset": 62, "endOffset": 106}, {"referenceID": 15, "context": ", 2015), language modeling (J\u00f3zefowicz et al., 2016) and machine translation (Wu et al.", "startOffset": 27, "endOffset": 52}, {"referenceID": 10, "context": "Even in cases when the networks can be evaluated fast enough, it will still have a significant impact on battery life in mobile devices (Han et al., 2015).", "startOffset": 136, "endOffset": 154}, {"referenceID": 0, "context": "Recent advances in multiple fields such as speech recognition (Graves & Jaitly, 2014; Amodei et al., 2015), language modeling (J\u00f3zefowicz et al., 2016) and machine translation (Wu et al., 2016) can be at least partially attributed to larger training datasets, larger models and more compute that allows larger models to be trained on larger datasets. For example, the deep neural network used for acoustic modeling in Hannun et al. (2014) had 11 million parameters which grew to approximately 67 million for bidirectional RNNs and further to 116 million for the latest forward only GRU models in Amodei et al.", "startOffset": 86, "endOffset": 439}, {"referenceID": 0, "context": "Recent advances in multiple fields such as speech recognition (Graves & Jaitly, 2014; Amodei et al., 2015), language modeling (J\u00f3zefowicz et al., 2016) and machine translation (Wu et al., 2016) can be at least partially attributed to larger training datasets, larger models and more compute that allows larger models to be trained on larger datasets. For example, the deep neural network used for acoustic modeling in Hannun et al. (2014) had 11 million parameters which grew to approximately 67 million for bidirectional RNNs and further to 116 million for the latest forward only GRU models in Amodei et al. (2015). And in language modeling the size of the non-embedding parameters (mostly in the recurrent layers) have exploded even as various ways of hand engineering sparsity into the embeddings have been explored in J\u00f3zefowicz et al.", "startOffset": 86, "endOffset": 617}, {"referenceID": 0, "context": "Recent advances in multiple fields such as speech recognition (Graves & Jaitly, 2014; Amodei et al., 2015), language modeling (J\u00f3zefowicz et al., 2016) and machine translation (Wu et al., 2016) can be at least partially attributed to larger training datasets, larger models and more compute that allows larger models to be trained on larger datasets. For example, the deep neural network used for acoustic modeling in Hannun et al. (2014) had 11 million parameters which grew to approximately 67 million for bidirectional RNNs and further to 116 million for the latest forward only GRU models in Amodei et al. (2015). And in language modeling the size of the non-embedding parameters (mostly in the recurrent layers) have exploded even as various ways of hand engineering sparsity into the embeddings have been explored in J\u00f3zefowicz et al. (2016) and Chen et al.", "startOffset": 86, "endOffset": 848}, {"referenceID": 0, "context": "Recent advances in multiple fields such as speech recognition (Graves & Jaitly, 2014; Amodei et al., 2015), language modeling (J\u00f3zefowicz et al., 2016) and machine translation (Wu et al., 2016) can be at least partially attributed to larger training datasets, larger models and more compute that allows larger models to be trained on larger datasets. For example, the deep neural network used for acoustic modeling in Hannun et al. (2014) had 11 million parameters which grew to approximately 67 million for bidirectional RNNs and further to 116 million for the latest forward only GRU models in Amodei et al. (2015). And in language modeling the size of the non-embedding parameters (mostly in the recurrent layers) have exploded even as various ways of hand engineering sparsity into the embeddings have been explored in J\u00f3zefowicz et al. (2016) and Chen et al. (2015a). These large models face two significant challenges in deployment.", "startOffset": 86, "endOffset": 872}, {"referenceID": 4, "context": "We show this technique works with Gated Recurrent Units (GRU) (Cho et al., 2014) as well as vanilla RNNs.", "startOffset": 62, "endOffset": 80}, {"referenceID": 4, "context": "We show this technique works with Gated Recurrent Units (GRU) (Cho et al., 2014) as well as vanilla RNNs. In addition to the benefits of less storage and faster inference, this technique can also improve the accuracy over a dense baseline. By starting with a larger dense matrix than the baseline and then pruning it down, we can achieve equal or better accuracy compared to the baseline but with a much smaller number of parameters. This approach can be implemented easily in current training frameworks and is agnostic to the optimization algorithm. Furthermore, training time does not increase unlike previous approaches such as in Han et al. (2015). State of the art results in speech recognition generally require days to weeks of training time, so a further 3-4\u00d7 increase in training time is undesirable.", "startOffset": 63, "endOffset": 653}, {"referenceID": 19, "context": "One method is to use a fixed point representation to quantize weights to signed bytes and activations to unsigned bytes (Vanhoucke et al., 2011).", "startOffset": 120, "endOffset": 144}, {"referenceID": 5, "context": "One method is to carefully construct one of the factors and learn the other (Denil et al., 2013).", "startOffset": 76, "endOffset": 96}, {"referenceID": 6, "context": "Inspired by this technique, a low rank approximation for the convolution layers achieves twice the speed while staying within 1% of the original model in terms of accuracy (Denton et al., 2014).", "startOffset": 172, "endOffset": 193}, {"referenceID": 14, "context": "The convolution layer can also be approximated by a smaller set of basis filters (Jaderberg et al., 2014).", "startOffset": 81, "endOffset": 105}, {"referenceID": 8, "context": "Quantization techniques like k-means clustering of weights can also reduce the storage size of the models by focusing only on the fully connected layers (Gong et al., 2014).", "startOffset": 153, "endOffset": 172}, {"referenceID": 16, "context": "Yet another approach is to use the diagonal terms of a Hessian matrix to construct a saliency threshold and used this to drop weights that fall below a given saliency threshold (LeCun et al., 1989).", "startOffset": 177, "endOffset": 197}, {"referenceID": 13, "context": "Optimal Brain Surgeon is another work in the same vein that prunes weights using the inverse of a Hessian matrix with the additional advantage of no re-training after pruning (Hassibi et al., 1993).", "startOffset": 175, "endOffset": 197}, {"referenceID": 10, "context": "Both pruning and quantization techniques can be combined to get impressive gains on AlexNet trained on the ImageNet dataset (Han et al., 2015).", "startOffset": 124, "endOffset": 142}, {"referenceID": 18, "context": "There has also been some recent work to shrink model size for recurrent and LSTM networks used in automatic speech recognition (ASR) (Lu et al., 2016).", "startOffset": 133, "endOffset": 150}, {"referenceID": 16, "context": "Unlike the methods that need to approximate a Hessian (LeCun et al., 1989; Hassibi et al., 1993) our method uses a simple heuristic to choose the threshold used to drop weights.", "startOffset": 54, "endOffset": 96}, {"referenceID": 13, "context": "Unlike the methods that need to approximate a Hessian (LeCun et al., 1989; Hassibi et al., 1993) our method uses a simple heuristic to choose the threshold used to drop weights.", "startOffset": 54, "endOffset": 96}, {"referenceID": 10, "context": "Yet another advantage, when compared to methods that need re-training (Han et al., 2015), is that our pruning technique is part of training and needs", "startOffset": 70, "endOffset": 88}, {"referenceID": 18, "context": "Even though our technique requires judicious choice of pruning hyperparameters, we feel that it is easier than choosing the structure of matrices to guide the sparsification for recurrent networks (Lu et al., 2016).", "startOffset": 197, "endOffset": 214}, {"referenceID": 20, "context": "Another approach for pruning feed forward neural networks for speech recognition is using simple threshold to prune all weights (Yu et al., 2012) at a particular epoch.", "startOffset": 128, "endOffset": 145}, {"referenceID": 20, "context": "We also compare our gradual pruning approach to the hard pruning approach proposed in Yu et al. (2012). In their approach, all parameters below a certain threshold are pruned at particular epoch.", "startOffset": 86, "endOffset": 103}, {"referenceID": 1, "context": "State of the art SpMV routines can achieve close to device memory bandwidth for a wide array of matrix shapes and sparsity patterns (see Baxter (2016) and Liu et al.", "startOffset": 137, "endOffset": 151}, {"referenceID": 1, "context": "State of the art SpMV routines can achieve close to device memory bandwidth for a wide array of matrix shapes and sparsity patterns (see Baxter (2016) and Liu et al. (2013)).", "startOffset": 137, "endOffset": 173}, {"referenceID": 7, "context": "Persistent Recurrent Neural Networks (Diamos et al., 2016) is a technique that increases the computational intensity of evaluating an RNN by caching the weights in on-chip memory such as caches, block RAM, or register files across multiple timesteps.", "startOffset": 37, "endOffset": 58}], "year": 2017, "abstractText": "Recurrent Neural Networks (RNN) are widely used to solve a variety of problems and as the quantity of data and the amount of available compute have increased, so have model sizes. The number of parameters in recent state-of-the-art networks makes them hard to deploy, especially on mobile phones and embedded devices. The challenge is due to both the size of the model and the time it takes to evaluate it. In order to deploy these RNNs efficiently, we propose a technique to reduce the parameters of a network by pruning weights during the initial training of the network. At the end of training, the parameters of the network are sparse while accuracy is still close to the original dense neural network. The network size is reduced by 8\u00d7 and the time required to train the model remains constant. Additionally, we can prune a larger dense network to achieve better than baseline performance while still reducing the total number of parameters significantly. Pruning RNNs reduces the size of the model and can also help achieve significant inference time speed-up using sparse matrix multiply. Benchmarks show that using our technique model size can be reduced by 90% and speed-up is around 2\u00d7 to 7\u00d7.", "creator": "LaTeX with hyperref package"}}}