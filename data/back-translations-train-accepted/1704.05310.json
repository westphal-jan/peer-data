{"id": "1704.05310", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "Unsupervised Learning by Predicting Noise", "abstract": "Convolutional neural networks provide visual features that perform remarkably well in many computer vision applications. However, training these networks requires significant amounts of supervision. This paper introduces a generic framework to train deep networks, end-to-end, with no supervision. We propose to fix a set of target representations, called Noise As Targets (NAT), and to constrain the deep features to align to them. This domain agnostic approach avoids the standard unsupervised learning issues of trivial solutions and collapsing of features. Thanks to a stochastic batch reassignment strategy and a separable square loss function, it scales to millions of images. The proposed approach produces representations that perform on par with state-of-the-art unsupervised methods on ImageNet and Pascal VOC.", "histories": [["v1", "Tue, 18 Apr 2017 12:51:47 GMT  (2432kb,D)", "http://arxiv.org/abs/1704.05310v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.CV cs.LG", "authors": ["piotr bojanowski", "armand joulin"], "accepted": true, "id": "1704.05310"}, "pdf": {"name": "1704.05310.pdf", "metadata": {"source": "META", "title": "Unsupervised Learning by Predicting Noise", "authors": ["Piotr Bojanowski", "Armand Joulin"], "emails": ["<bojanowski@fb.com>."], "sections": [{"heading": "1. Introduction", "text": "In recent years, the number of people who are able to live in the USA, in Europe, in Europe, in Europe, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in Europe, in Europe, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the"}, {"heading": "2. Related work", "text": "In fact, most of them are able to survive on their own, without feeling able to flourish."}, {"heading": "3. Method", "text": "In this section, we present our model and discuss its relationships to various cluster approaches, including Kmean. Figure 1 gives an overview of our approach. We also show that it can be trained on massive data sets using an online process. Finally, we provide all the details for implementation."}, {"heading": "3.1. Unsupervised learning", "text": "We are interested in learning visual characteristics whose lines are the images. These characteristics are produced by applying a parameterized image to the images. (In the absence of a clear target, there are no clear targets, and we must therefore also learn them by setting a series of targets. (1) Where d is the dimensions of the target vectors that we use in the rest of the paper, we use matrix notations, i.e., we denote the matrix notations whose lines are the images, whose lines are the images, whose lines are the target representations yi, yi, yi, etc. we are the target vectors, i.e.) We denote the matrix notations whose lines are the target representations yi, and whose lines are the images, whose lines are the images. (1) Where d are the dimensions of the target vectors, we are in the rest of the paper, we use matrix notations, i.e., the lines of which are the images, whose lines are the representative and whose lines are the matrix."}, {"heading": "3.2. Optimization", "text": "In this section, we describe how we efficiently optimize the cost function described in Eq. (5) In view of the fact that we have the Eq. (5). (5).............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3.3. Implementation details", "text": "To ensure a fair empirical comparison with previous work, we follow Wang & Gupta (2015) and use an AlexNet architecture, we train them end-to-end with our unattended loss function, then test the quality of the learned visual trait by retraining a classifier at the top. During transfer learning, we consider the output of the last revolutionary layer as our characteristics as in Razavian et al. (2014) We use the same multi-layered perceptron (MLP) as in Krizhevsky et al. (2012) for classification. In practice, we observe that pre-processing of images greatly improves the quality of our learned traits. As in Ranzato et al. (2007), we use image gradients instead of images to avoid trivial solutions such as image processing. In practice, we observe that pre-processing of images greatly contributes to the quality of our learned traits."}, {"heading": "4. Experiments", "text": "We conduct several experiments to validate different design decisions in NAT. Then we evaluate the quality of our features by comparing them with state-of-the-art, uncontrolled approaches to several supported monitored tasks, namely object classification on ImageNet and object classification and detection of PASCAL VOC 2007 (Everingham et al., 2010).Transfer the features. To measure the quality of our features, we measure their performance in transfer learning. We freeze the parameters of all the convolutionary layers and overwrite the parameters of the MLP classifier with random Gaussian weights. We closely follow the training and testing procedure used for each of the data sets according to Donahue et al. (2016).Data sets and baselines. We use the ImageNet training set to learn our revolutionary network (Deng et al., 2009).This data set consists of 1, 281, 167 images belonging to 1 object categories."}, {"heading": "4.1. Detailed analysis", "text": "This year, we have reached the stage where we feel we are in a position to achieve the objectives I have mentioned, but we do not feel we are in a position to achieve them."}, {"heading": "4.2. Comparison with the state of the art", "text": "This year it will be so far that it will be able to do the aforementioned rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf"}, {"heading": "5. Conclusion", "text": "By aligning the output of a neural network with low-dimensional noise, we obtain state-of-the-art characteristics. Our approach explicitly aims to learn discriminatory features, while most uncontrolled approaches target surrogate problems, such as image denociation or image generation. In contrast to self-monitored approaches, we make very few assumptions about the input space, which makes our approach very easy and quick to learn. Interestingly, it also has some similarities with traditional cluster approaches and retrieval methods. While we demonstrate the potential of our approach to visual data, it will be interesting to try out other areas. Finally, this work only looks at simple sound distributions and alignment methods. One possible direction of research is to explore target distributions and alignments that are more informative, which would also strengthen the relationship between NAT and methods based on distribution such as earth movement."}], "references": [{"title": "Learning to see by moving", "author": ["P. Agrawal", "J. Carreira", "J. Malik"], "venue": "In ICCV,", "citeRegEx": "Agrawal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2015}, {"title": "Diffrac: a discriminative and flexible framework for clustering", "author": ["F. Bach", "Z. Harchaoui"], "venue": "In NIPS,", "citeRegEx": "Bach and Harchaoui,? \\Q2007\\E", "shortCiteRegEx": "Bach and Harchaoui", "year": 2007}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Finding actors and actions in movies", "author": ["P. Bojanowski", "F. Bach", "I. Laptev", "J. Ponce", "C. Schmid", "J. Sivic"], "venue": "In ICCV,", "citeRegEx": "Bojanowski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2013}, {"title": "Weakly supervised action labeling in videos under ordering constraints", "author": ["P. Bojanowski", "R. Lajugie", "F. Bach", "I. Laptev", "J. Ponce", "C. Schmid", "J. Sivic"], "venue": "In ECCV,", "citeRegEx": "Bojanowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2014}, {"title": "Learning feature representations with k-means", "author": ["A. Coates", "A. Ng"], "venue": "In Neural Networks: Tricks of the Trade. Springer,", "citeRegEx": "Coates and Ng,? \\Q2012\\E", "shortCiteRegEx": "Coates and Ng", "year": 2012}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "In CVPR,", "citeRegEx": "Dalal and Triggs,? \\Q2005\\E", "shortCiteRegEx": "Dalal and Triggs", "year": 2005}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.J. Li", "K. Li", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["E.L. Denton", "S. Chintala", "R. Fergus"], "venue": "In NIPS,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Unsupervised visual representation learning by context prediction", "author": ["C. Doersch", "A. Gupta", "A. Efros"], "venue": "In CVPR,", "citeRegEx": "Doersch et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Doersch et al\\.", "year": 2015}, {"title": "Adversarial feature learning", "author": ["J. Donahue", "P. Kr\u00e4henb\u00fchl", "T. Darrell"], "venue": "arXiv preprint arXiv:1605.09782,", "citeRegEx": "Donahue et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2016}, {"title": "Discriminative unsupervised feature learning with convolutional neural networks", "author": ["A. Dosovitskiy", "J. Springenberg", "M. Riedmiller", "T. Brox"], "venue": "In NIPS,", "citeRegEx": "Dosovitskiy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2014}, {"title": "The PASCAL visual object classes (VOC) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": null, "citeRegEx": "Everingham et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Everingham et al\\.", "year": 2010}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["K. Fukushima"], "venue": "Biological Cybernetics,", "citeRegEx": "Fukushima,? \\Q1980\\E", "shortCiteRegEx": "Fukushima", "year": 1980}, {"title": "Fast r-cnn", "author": ["R. Girshick"], "venue": "In CVPR,", "citeRegEx": "Girshick,? \\Q2015\\E", "shortCiteRegEx": "Girshick", "year": 2015}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Classes for fast maximum entropy training", "author": ["J. Goodman"], "venue": "In ICASSP,", "citeRegEx": "Goodman,? \\Q2001\\E", "shortCiteRegEx": "Goodman", "year": 2001}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Learning image representations tied to ego-motion", "author": ["D. Jayaraman", "K. Grauman"], "venue": "In ICCV,", "citeRegEx": "Jayaraman and Grauman,? \\Q2015\\E", "shortCiteRegEx": "Jayaraman and Grauman", "year": 2015}, {"title": "A convex relaxation for weakly supervised classifiers", "author": ["A. Joulin", "F. Bach"], "venue": "In ICML,", "citeRegEx": "Joulin and Bach,? \\Q2012\\E", "shortCiteRegEx": "Joulin and Bach", "year": 2012}, {"title": "Discriminative clustering for image co-segmentation", "author": ["A. Joulin", "F. Bach", "J. Ponce"], "venue": "In CVPR,", "citeRegEx": "Joulin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2010}, {"title": "Learning visual features from large weakly supervised data", "author": ["A. Joulin", "L. van der Maaten", "A. Jabri", "N. Vasilache"], "venue": null, "citeRegEx": "Joulin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2016}, {"title": "Auto-encoding variational bayes", "author": ["D. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma and Welling,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2013}, {"title": "Self-organized formation of topologically correct feature maps", "author": ["T. Kohonen"], "venue": "Biological cybernetics,", "citeRegEx": "Kohonen,? \\Q1982\\E", "shortCiteRegEx": "Kohonen", "year": 1982}, {"title": "Data-dependent initializations of convolutional neural networks", "author": ["Kr\u00e4henb\u00fchl", "Philipp", "Doersch", "Carl", "Donahue", "Jeff", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1511.06856,", "citeRegEx": "Kr\u00e4henb\u00fchl et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kr\u00e4henb\u00fchl et al\\.", "year": 2015}, {"title": "Discriminative clustering by regularized information maximization", "author": ["A. Krause", "P. Perona", "R.G. Gomes"], "venue": "In NIPS,", "citeRegEx": "Krause et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "The hungarian method for the assignment problem", "author": ["H.W. Kuhn"], "venue": "Naval research logistics quarterly,", "citeRegEx": "Kuhn,? \\Q1955\\E", "shortCiteRegEx": "Kuhn", "year": 1955}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "In NIPS,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Learning deep parsimonious representations", "author": ["R. Liao", "A. Schwing", "R. Zemel", "R. Urtasun"], "venue": "In NIPS,", "citeRegEx": "Liao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liao et al\\.", "year": 2016}, {"title": "Least squares quantization in pcm", "author": ["S. Lloyd"], "venue": "Transactions on information theory,", "citeRegEx": "Lloyd,? \\Q1982\\E", "shortCiteRegEx": "Lloyd", "year": 1982}, {"title": "Object recognition from local scale-invariant features", "author": ["D. Lowe"], "venue": "In ICCV,", "citeRegEx": "Lowe,? \\Q1999\\E", "shortCiteRegEx": "Lowe", "year": 1999}, {"title": "Convolutional kernel networks", "author": ["J. Mairal", "P. Koniusz", "Z. Harchaoui", "C. Schmid"], "venue": "In NIPS,", "citeRegEx": "Mairal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2014}, {"title": "neural-gas\u201d network learns topologies", "author": ["T. Martinetz", "Schulten", "K. A"], "venue": null, "citeRegEx": "Martinetz et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Martinetz et al\\.", "year": 1991}, {"title": "Stacked convolutional auto-encoders for hierarchical feature extraction", "author": ["J. Masci", "U. Meier", "D. Cire\u015fan", "J. Schmidhuber"], "venue": "In ICANN,", "citeRegEx": "Masci et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Masci et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Unsupervised learning of visual representations by solving jigsaw puzzles", "author": ["M. Noroozi", "P. Favaro"], "venue": "arXiv preprint arXiv:1603.09246,", "citeRegEx": "Noroozi and Favaro,? \\Q2016\\E", "shortCiteRegEx": "Noroozi and Favaro", "year": 2016}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "In CVPR,", "citeRegEx": "Oquab et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Oquab et al\\.", "year": 2014}, {"title": "Context encoders: Feature learning by inpainting", "author": ["D. Pathak", "P. Krahenbuhl", "J. Donahue", "T. Darrell", "A. Efros"], "venue": null, "citeRegEx": "Pathak et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pathak et al\\.", "year": 2016}, {"title": "Learning to segment object candidates", "author": ["P.O. Pinheiro", "R. Collobert", "P. Dollar"], "venue": "In NIPS,", "citeRegEx": "Pinheiro et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pinheiro et al\\.", "year": 2015}, {"title": "Linking people in videos with their names using coreference resolution", "author": ["V. Ramanathan", "A. Joulin", "P. Liang", "L. Fei-Fei"], "venue": "In ECCV,", "citeRegEx": "Ramanathan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ramanathan et al\\.", "year": 2014}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["M.A. Ranzato", "F.J. Huang", "Y.L. Boureau", "Y. LeCun"], "venue": "In CVPR,", "citeRegEx": "Ranzato et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2007}, {"title": "CNN features off-the-shelf: an astounding baseline for recognition", "author": ["Razavian", "A. Sharif", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "In arXiv 1403.6382,", "citeRegEx": "Razavian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Razavian et al\\.", "year": 2014}, {"title": "A metric for distributions with applications to image databases", "author": ["Y. Rubner", "C. Tomasi", "L.J. Guibas"], "venue": "In ICCV,", "citeRegEx": "Rubner et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Rubner et al\\.", "year": 1998}, {"title": "Image classification with the fisher vector: Theory and practice", "author": ["J. S\u00e1nchez", "F. Perronnin", "T. Mensink", "J. Verbeek"], "venue": null, "citeRegEx": "S\u00e1nchez et al\\.,? \\Q2013\\E", "shortCiteRegEx": "S\u00e1nchez et al\\.", "year": 2013}, {"title": "Colocalization in real-world images", "author": ["K. Tang", "A. Joulin", "Li", "L.-J", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Scale-invariant learning and convolutional networks", "author": ["M. Tygert", "S. Chintala", "A. Szlam", "Y. Tian", "W. Zaremba"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Tygert et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Tygert et al\\.", "year": 2017}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["A. van den Oord", "N. Kalchbrenner", "L. Espeholt", "O. Vinyals", "A. Graves"], "venue": null, "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Clustering of the selforganizing", "author": ["J. Vesanto", "E. Alhoniemi"], "venue": "map. Transactions on neural networks,", "citeRegEx": "Vesanto and Alhoniemi,? \\Q2000\\E", "shortCiteRegEx": "Vesanto and Alhoniemi", "year": 2000}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A"], "venue": "JMLR, 11(Dec):3371\u20133408,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Unsupervised learning of visual representations using videos", "author": ["X. Wang", "A. Gupta"], "venue": "In ICCV,", "citeRegEx": "Wang and Gupta,? \\Q2015\\E", "shortCiteRegEx": "Wang and Gupta", "year": 2015}, {"title": "Unsupervised deep embedding for clustering analysis", "author": ["J. Xie", "R. Girshick", "A. Farhadi"], "venue": "In ICML,", "citeRegEx": "Xie et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Maximum margin clustering", "author": ["L. Xu", "J. Neufeld", "B. Larson", "D. Schuurmans"], "venue": "In NIPS,", "citeRegEx": "Xu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2004}, {"title": "Joint unsupervised learning of deep representations and image clusters", "author": ["J. Yang", "D. Parikh", "D. Batra"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Colorful image colorization", "author": ["R. Zhang", "P. Isola", "A. Efros"], "venue": "In ECCV,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Stacked What-Where Auto-encoders", "author": ["J. Zhao", "M. Mathieu", "R. Goroshin", "Y. LeCun"], "venue": "In Workshop at ICLR,", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "In recent years, convolutional neural networks, or convnets (Fukushima, 1980; LeCun et al., 1989) have pushed the limits of computer vision (Krizhevsky et al.", "startOffset": 60, "endOffset": 97}, {"referenceID": 29, "context": "In recent years, convolutional neural networks, or convnets (Fukushima, 1980; LeCun et al., 1989) have pushed the limits of computer vision (Krizhevsky et al.", "startOffset": 60, "endOffset": 97}, {"referenceID": 27, "context": ", 1989) have pushed the limits of computer vision (Krizhevsky et al., 2012; He et al., 2016), leading to important progress in a variety of tasks, like object detection (Girshick, 2015) or image segmentation (Pinheiro et al.", "startOffset": 50, "endOffset": 92}, {"referenceID": 17, "context": ", 1989) have pushed the limits of computer vision (Krizhevsky et al., 2012; He et al., 2016), leading to important progress in a variety of tasks, like object detection (Girshick, 2015) or image segmentation (Pinheiro et al.", "startOffset": 50, "endOffset": 92}, {"referenceID": 14, "context": ", 2016), leading to important progress in a variety of tasks, like object detection (Girshick, 2015) or image segmentation (Pinheiro et al.", "startOffset": 84, "endOffset": 100}, {"referenceID": 40, "context": ", 2016), leading to important progress in a variety of tasks, like object detection (Girshick, 2015) or image segmentation (Pinheiro et al., 2015).", "startOffset": 123, "endOffset": 146}, {"referenceID": 43, "context": "Key to this success is their ability to produce features that easily transfer to new domains when trained on massive databases of labeled images (Razavian et al., 2014; Oquab et al., 2014) or weaklysupervised data (Joulin et al.", "startOffset": 145, "endOffset": 188}, {"referenceID": 38, "context": "Key to this success is their ability to produce features that easily transfer to new domains when trained on massive databases of labeled images (Razavian et al., 2014; Oquab et al., 2014) or weaklysupervised data (Joulin et al.", "startOffset": 145, "endOffset": 188}, {"referenceID": 22, "context": ", 2014) or weaklysupervised data (Joulin et al., 2016).", "startOffset": 33, "endOffset": 54}, {"referenceID": 10, "context": "Several strategies exist to learn deep convolutional features with no annotation (Donahue et al., 2016).", "startOffset": 81, "endOffset": 103}, {"referenceID": 9, "context": "They either try to capture a signal from the source as a form of selfsupervision (Doersch et al., 2015; Wang & Gupta, 2015) or learn the underlying distribution of images (Vincent et al.", "startOffset": 81, "endOffset": 123}, {"referenceID": 10, "context": "While some of these approaches obtain promising performance in transfer learning (Donahue et al., 2016; Wang & Gupta, 2015), they do not explicitly aim to learn discriminative features.", "startOffset": 81, "endOffset": 123}, {"referenceID": 11, "context": "Some attempts were made with retrieval based approaches (Dosovitskiy et al., 2014) and clustering (Yang et al.", "startOffset": 56, "endOffset": 82}, {"referenceID": 54, "context": ", 2014) and clustering (Yang et al., 2016; Liao et al., 2016), but they are hard to scale and have only been tested on small datasets.", "startOffset": 23, "endOffset": 61}, {"referenceID": 30, "context": ", 2014) and clustering (Yang et al., 2016; Liao et al., 2016), but they are hard to scale and have only been tested on small datasets.", "startOffset": 23, "endOffset": 61}, {"referenceID": 24, "context": "Similar to self-organizing maps (Kohonen, 1982; Martinetz & Schulten, 1991), we map deep features to a set of predefined representations in a low dimensional space.", "startOffset": 32, "endOffset": 75}, {"referenceID": 31, "context": "Our approach also shares some similarities with standard clustering approches like k-means (Lloyd, 1982) or discriminative clustering (Bach & Harchaoui, 2007).", "startOffset": 91, "endOffset": 104}, {"referenceID": 7, "context": "In addition, we propose an online algorithm able to scale to massive image databases like ImageNet (Deng et al., 2009).", "startOffset": 99, "endOffset": 118}, {"referenceID": 47, "context": "This is achieved by using a quadratic loss as in (Tygert et al., 2017) and a fast approximation of the Hungarian algorithm.", "startOffset": 49, "endOffset": 70}, {"referenceID": 27, "context": "We show the potential of our approach by training end-to-end on ImageNet a standard architecture, namely AlexNet (Krizhevsky et al., 2012) with no supervision.", "startOffset": 113, "endOffset": 138}, {"referenceID": 10, "context": "We test the quality of our features on several image classification problems, following the setting of Donahue et al. (2016). We are on par with state-of-the-art unsupervised and self-supervised learning approaches while being much simpler to train and to scale.", "startOffset": 103, "endOffset": 125}, {"referenceID": 33, "context": "Several approaches have been recently proposed to tackle the problem of deep unsupervised learning (Coates & Ng, 2012; Mairal et al., 2014; Dosovitskiy et al., 2014).", "startOffset": 99, "endOffset": 165}, {"referenceID": 11, "context": "Several approaches have been recently proposed to tackle the problem of deep unsupervised learning (Coates & Ng, 2012; Mairal et al., 2014; Dosovitskiy et al., 2014).", "startOffset": 99, "endOffset": 165}, {"referenceID": 52, "context": "Some of them are based on a clustering loss (Xie et al., 2016; Yang et al., 2016; Liao et al., 2016), but they are not tested at a scale comparable to that of supervised convnet training.", "startOffset": 44, "endOffset": 100}, {"referenceID": 54, "context": "Some of them are based on a clustering loss (Xie et al., 2016; Yang et al., 2016; Liao et al., 2016), but they are not tested at a scale comparable to that of supervised convnet training.", "startOffset": 44, "endOffset": 100}, {"referenceID": 30, "context": "Some of them are based on a clustering loss (Xie et al., 2016; Yang et al., 2016; Liao et al., 2016), but they are not tested at a scale comparable to that of supervised convnet training.", "startOffset": 44, "endOffset": 100}, {"referenceID": 11, "context": ", 2014; Dosovitskiy et al., 2014). Some of them are based on a clustering loss (Xie et al., 2016; Yang et al., 2016; Liao et al., 2016), but they are not tested at a scale comparable to that of supervised convnet training. Coates & Ng (2012) uses k-means to pre-train convnets, by learning each layer sequentially in a bottom-up fashion.", "startOffset": 8, "endOffset": 242}, {"referenceID": 11, "context": ", 2014; Dosovitskiy et al., 2014). Some of them are based on a clustering loss (Xie et al., 2016; Yang et al., 2016; Liao et al., 2016), but they are not tested at a scale comparable to that of supervised convnet training. Coates & Ng (2012) uses k-means to pre-train convnets, by learning each layer sequentially in a bottom-up fashion. In our work, we train the convnet end-to-end with a loss that shares similarities with k-means. Closer to our work, Dosovitskiy et al. (2014) proposes to train convnets by solving a retrieval problem.", "startOffset": 8, "endOffset": 480}, {"referenceID": 15, "context": "Traditional examples of this approach are variational autoencoders (Kingma & Welling, 2013), generative adversarial networks (Goodfellow et al., 2014), and to a lesser extent, noisy autoencoders (Vincent et al.", "startOffset": 125, "endOffset": 150}, {"referenceID": 50, "context": ", 2014), and to a lesser extent, noisy autoencoders (Vincent et al., 2010).", "startOffset": 52, "endOffset": 74}, {"referenceID": 15, "context": "Among those approaches, generative adversarial networks (GANs) (Goodfellow et al., 2014; Denton et al., 2015; Donahue et al., 2016) share another similarity with our approach, namely they are explicitly minimizing a discriminative loss to learn their features.", "startOffset": 63, "endOffset": 131}, {"referenceID": 8, "context": "Among those approaches, generative adversarial networks (GANs) (Goodfellow et al., 2014; Denton et al., 2015; Donahue et al., 2016) share another similarity with our approach, namely they are explicitly minimizing a discriminative loss to learn their features.", "startOffset": 63, "endOffset": 131}, {"referenceID": 10, "context": "Among those approaches, generative adversarial networks (GANs) (Goodfellow et al., 2014; Denton et al., 2015; Donahue et al., 2016) share another similarity with our approach, namely they are explicitly minimizing a discriminative loss to learn their features.", "startOffset": 63, "endOffset": 131}, {"referenceID": 8, "context": ", 2014; Denton et al., 2015; Donahue et al., 2016) share another similarity with our approach, namely they are explicitly minimizing a discriminative loss to learn their features. While these models cannot learn an inverse mapping, Donahue et al. (2016) recently proposed to add an encoder to extract visual features from GANs.", "startOffset": 8, "endOffset": 254}, {"referenceID": 9, "context": "Recently, a lot of work has explored self-supervison: leveraging supervision contained in the input signal (Doersch et al., 2015; Noroozi & Favaro, 2016; Pathak et al., 2016).", "startOffset": 107, "endOffset": 174}, {"referenceID": 39, "context": "Recently, a lot of work has explored self-supervison: leveraging supervision contained in the input signal (Doersch et al., 2015; Noroozi & Favaro, 2016; Pathak et al., 2016).", "startOffset": 107, "endOffset": 174}, {"referenceID": 36, "context": "In the same vein as word2vec (Mikolov et al., 2013), Doersch et al.", "startOffset": 29, "endOffset": 51}, {"referenceID": 0, "context": "Others have shown that temporal coherence in videos also provides a signal that can be used to learn powerful visual features (Agrawal et al., 2015; Jayaraman & Grauman, 2015; Wang & Gupta, 2015).", "startOffset": 126, "endOffset": 195}, {"referenceID": 8, "context": "Recently, a lot of work has explored self-supervison: leveraging supervision contained in the input signal (Doersch et al., 2015; Noroozi & Favaro, 2016; Pathak et al., 2016). In the same vein as word2vec (Mikolov et al., 2013), Doersch et al. (2015) show that spatial context is a strong signal to learn visual features.", "startOffset": 108, "endOffset": 251}, {"referenceID": 8, "context": "Recently, a lot of work has explored self-supervison: leveraging supervision contained in the input signal (Doersch et al., 2015; Noroozi & Favaro, 2016; Pathak et al., 2016). In the same vein as word2vec (Mikolov et al., 2013), Doersch et al. (2015) show that spatial context is a strong signal to learn visual features. Noroozi & Favaro (2016) have further extended this work.", "startOffset": 108, "endOffset": 346}, {"referenceID": 0, "context": "Others have shown that temporal coherence in videos also provides a signal that can be used to learn powerful visual features (Agrawal et al., 2015; Jayaraman & Grauman, 2015; Wang & Gupta, 2015). In particular, Wang & Gupta (2015) show that such features provide promising performance on ImageNet.", "startOffset": 127, "endOffset": 232}, {"referenceID": 2, "context": "Many have also used autoencoders with a reconstruction loss (Bengio et al., 2007; Ranzato et al., 2007; Masci et al., 2011).", "startOffset": 60, "endOffset": 123}, {"referenceID": 42, "context": "Many have also used autoencoders with a reconstruction loss (Bengio et al., 2007; Ranzato et al., 2007; Masci et al., 2011).", "startOffset": 60, "endOffset": 123}, {"referenceID": 35, "context": "Many have also used autoencoders with a reconstruction loss (Bengio et al., 2007; Ranzato et al., 2007; Masci et al., 2011).", "startOffset": 60, "endOffset": 123}, {"referenceID": 42, "context": "The decoder is often a fully connected network (Ranzato et al., 2007) or a deconvolutional network (Masci et al.", "startOffset": 47, "endOffset": 69}, {"referenceID": 35, "context": ", 2007) or a deconvolutional network (Masci et al., 2011; Zhao et al., 2016) but can be more sophisticated, like a PixelCNN network (van den Oord et al.", "startOffset": 37, "endOffset": 76}, {"referenceID": 56, "context": ", 2007) or a deconvolutional network (Masci et al., 2011; Zhao et al., 2016) but can be more sophisticated, like a PixelCNN network (van den Oord et al.", "startOffset": 37, "endOffset": 76}, {"referenceID": 24, "context": "This family of unsupervised methods aims at learning a low dimensional representation of the data that preserves certain topological properties (Kohonen, 1982; Vesanto & Alhoniemi, 2000).", "startOffset": 144, "endOffset": 186}, {"referenceID": 53, "context": "Many methods have been proposed to use discriminative losses for clustering (Xu et al., 2004; Bach & Harchaoui, 2007; Krause et al., 2010; Joulin & Bach, 2012).", "startOffset": 76, "endOffset": 159}, {"referenceID": 26, "context": "Many methods have been proposed to use discriminative losses for clustering (Xu et al., 2004; Bach & Harchaoui, 2007; Krause et al., 2010; Joulin & Bach, 2012).", "startOffset": 76, "endOffset": 159}, {"referenceID": 21, "context": "It has been successfully applied to several computer vision applications, like object discovery (Joulin et al., 2010; Tang et al., 2014) or video/text alignment (Bojanowski et al.", "startOffset": 96, "endOffset": 136}, {"referenceID": 46, "context": "It has been successfully applied to several computer vision applications, like object discovery (Joulin et al., 2010; Tang et al., 2014) or video/text alignment (Bojanowski et al.", "startOffset": 96, "endOffset": 136}, {"referenceID": 3, "context": ", 2014) or video/text alignment (Bojanowski et al., 2013; 2014; Ramanathan et al., 2014).", "startOffset": 32, "endOffset": 88}, {"referenceID": 41, "context": ", 2014) or video/text alignment (Bojanowski et al., 2013; 2014; Ramanathan et al., 2014).", "startOffset": 32, "endOffset": 88}, {"referenceID": 22, "context": ", 2004; Bach & Harchaoui, 2007; Krause et al., 2010; Joulin & Bach, 2012). In particular, Bach & Harchaoui (2007) shows that the ridge regression loss could be use to learn discriminative clusters.", "startOffset": 32, "endOffset": 114}, {"referenceID": 3, "context": ", 2014) or video/text alignment (Bojanowski et al., 2013; 2014; Ramanathan et al., 2014). In this work, we show that a similar framework can be designed for neural networks. As opposed to Xu et al. (2004), we address the empty assignment problems by restricting the set of possible reassignments to permutations rather than using global linear constrains the assignments.", "startOffset": 33, "endOffset": 205}, {"referenceID": 16, "context": "However, computing this loss is linear in the number of targets, making it impractical for large output spaces (Goodman, 2001).", "startOffset": 111, "endOffset": 126}, {"referenceID": 16, "context": "However, computing this loss is linear in the number of targets, making it impractical for large output spaces (Goodman, 2001). While there are workarounds to scale these losses to large output spaces, Tygert et al. (2017) has recently shown that using a squared `2 distance works well in many supervised settings, as long as the final activations are unit normalized.", "startOffset": 112, "endOffset": 223}, {"referenceID": 53, "context": "(2) would lead to a representation collapsing problem: all the images would be assigned to the same representation (Xu et al., 2004).", "startOffset": 115, "endOffset": 132}, {"referenceID": 11, "context": "If d is larger than n, this formulation would be similar to the framework of Dosovitskiy et al. (2014), and is impractical for large n.", "startOffset": 77, "endOffset": 103}, {"referenceID": 11, "context": "If d is larger than n, this formulation would be similar to the framework of Dosovitskiy et al. (2014), and is impractical for large n. On the other hand, if d is smaller than n, this formulation is equivalent to the discriminative clustering approach of Bach & Harchaoui (2007). Choosing such targets makes very strong assumptions on the nature of the underlying problem.", "startOffset": 77, "endOffset": 279}, {"referenceID": 44, "context": "In some sense, we are optimizing a crude approximation of the earth mover\u2019s distance between the distribution of deep features and a given target distribution (Rubner et al., 1998).", "startOffset": 159, "endOffset": 180}, {"referenceID": 28, "context": "(5) to this set, the linear assignment problem in P can be solved exactly with the Hungarian algorithm (Kuhn, 1955), but at the prohibitive cost of O(n).", "startOffset": 103, "endOffset": 115}, {"referenceID": 47, "context": "As noted by Tygert et al. (2017), batch normalization plays a crucial role when optimizing the l2 loss, as it avoids exploding gradients.", "startOffset": 12, "endOffset": 33}, {"referenceID": 47, "context": "The features are unit normalized for the square loss (Tygert et al., 2017).", "startOffset": 53, "endOffset": 74}, {"referenceID": 42, "context": "During transfer learning, we consider the output of the last convolutional layer as our features as in Razavian et al. (2014). We use the same multi-layer perceptron (MLP) as in Krizhevsky et al.", "startOffset": 103, "endOffset": 126}, {"referenceID": 27, "context": "We use the same multi-layer perceptron (MLP) as in Krizhevsky et al. (2012) for the classifier.", "startOffset": 51, "endOffset": 76}, {"referenceID": 32, "context": "Using this preprocessing is not surprising since most hand-made features like SIFT or HoG are based on image gradients (Lowe, 1999; Dalal & Triggs, 2005).", "startOffset": 119, "endOffset": 153}, {"referenceID": 27, "context": "In addition to this preprocessing, we also perform all the standard image transformations that are commonly applied in the supervised setting (Krizhevsky et al., 2012), such as random cropping and flipping of images.", "startOffset": 142, "endOffset": 167}, {"referenceID": 40, "context": "As in Ranzato et al. (2007), we use image gradients instead of the images to avoid trivial solutions like clustering according to colors.", "startOffset": 6, "endOffset": 28}, {"referenceID": 46, "context": "We project the output of the network on the `2 sphere as in Tygert et al. (2017). The network is trained with SGD with a batch size of 256.", "startOffset": 60, "endOffset": 81}, {"referenceID": 10, "context": "For the transfer learning experiments, we follow the guideline described in Donahue et al. (2016). 4.", "startOffset": 76, "endOffset": 98}, {"referenceID": 12, "context": "We then evaluate the quality of our features by comparing them to state-of-the-art unsupervised approaches on several auxiliary supervised tasks, namely object classification on ImageNet and object classification and detection of PASCAL VOC 2007 (Everingham et al., 2010).", "startOffset": 246, "endOffset": 271}, {"referenceID": 10, "context": "We precisely follow the training and testing procedure that is specific to each of the datasets following Donahue et al. (2016).", "startOffset": 106, "endOffset": 128}, {"referenceID": 7, "context": "We use the training set of ImageNet to learn our convolutional network (Deng et al., 2009).", "startOffset": 71, "endOffset": 90}, {"referenceID": 27, "context": "In addition to fully supervised approaches (Krizhevsky et al., 2012), we compare our method to several unsupervised approaches, i.", "startOffset": 43, "endOffset": 68}, {"referenceID": 45, "context": ", SIFT with Fisher Vectors (SIFT+FV) (S\u00e1nchez et al., 2013).", "startOffset": 37, "endOffset": 59}, {"referenceID": 6, "context": "We use the training set of ImageNet to learn our convolutional network (Deng et al., 2009). This dataset is composed of 1, 281, 167 images that belong to 1, 000 object categories. For the transfer learning experiments, we also consider PASCAL VOC 2007. In addition to fully supervised approaches (Krizhevsky et al., 2012), we compare our method to several unsupervised approaches, i.e., autoencoder, GAN and BiGAN as reported in Donahue et al. (2016). We also compare to selfsupervised approaches, i.", "startOffset": 72, "endOffset": 451}, {"referenceID": 0, "context": ", Agrawal et al. (2015); Doersch et al.", "startOffset": 2, "endOffset": 24}, {"referenceID": 0, "context": ", Agrawal et al. (2015); Doersch et al. (2015); Pathak et al.", "startOffset": 2, "endOffset": 47}, {"referenceID": 0, "context": ", Agrawal et al. (2015); Doersch et al. (2015); Pathak et al. (2016); Wang & Gupta (2015) and Zhang et al.", "startOffset": 2, "endOffset": 69}, {"referenceID": 0, "context": ", Agrawal et al. (2015); Doersch et al. (2015); Pathak et al. (2016); Wang & Gupta (2015) and Zhang et al.", "startOffset": 2, "endOffset": 90}, {"referenceID": 0, "context": ", Agrawal et al. (2015); Doersch et al. (2015); Pathak et al. (2016); Wang & Gupta (2015) and Zhang et al. (2016). Finally we compare to state-ofthe-art hand-made features, i.", "startOffset": 2, "endOffset": 114}, {"referenceID": 47, "context": "As previously observed by Tygert et al. (2017), the performances are similar, hence validating our choice of loss function.", "startOffset": 26, "endOffset": 47}, {"referenceID": 9, "context": "We compare our model with several self-supervised approaches (Wang & Gupta, 2015; Doersch et al., 2015; Zhang et al., 2016) and an unsupervised approach, i.", "startOffset": 61, "endOffset": 123}, {"referenceID": 55, "context": "We compare our model with several self-supervised approaches (Wang & Gupta, 2015; Doersch et al., 2015; Zhang et al., 2016) and an unsupervised approach, i.", "startOffset": 61, "endOffset": 123}, {"referenceID": 10, "context": "Like BiGANs (Donahue et al., 2016), NAT does not make any assumption about the domain but of the structure of its features.", "startOffset": 12, "endOffset": 34}, {"referenceID": 9, "context": "We compare our model with several self-supervised approaches (Wang & Gupta, 2015; Doersch et al., 2015; Zhang et al., 2016) and an unsupervised approach, i.e., Donahue et al. (2016). Note that self-supervised approaches use losses specifically designed for visual features.", "startOffset": 82, "endOffset": 182}, {"referenceID": 10, "context": "Among unsupervised approaches, NAT compares favorably to BiGAN (Donahue et al., 2016).", "startOffset": 63, "endOffset": 85}, {"referenceID": 45, "context": "SIFT+FV (S\u00e1nchez et al., 2013) 55.", "startOffset": 8, "endOffset": 30}, {"referenceID": 9, "context": "8 Doersch et al. (2015) 30.", "startOffset": 2, "endOffset": 24}, {"referenceID": 9, "context": "8 Doersch et al. (2015) 30.4 Zhang et al. (2016) 35.", "startOffset": 2, "endOffset": 49}, {"referenceID": 9, "context": "8 Doersch et al. (2015) 30.4 Zhang et al. (2016) 35.2 Noroozi & Favaro (2016) 38.", "startOffset": 2, "endOffset": 78}, {"referenceID": 10, "context": "BiGAN (Donahue et al., 2016) 32.", "startOffset": 6, "endOffset": 28}, {"referenceID": 10, "context": ", BiGAN (Donahue et al., 2016).", "startOffset": 8, "endOffset": 30}, {"referenceID": 10, "context": ", BiGAN (Donahue et al., 2016). Noroozi & Favaro (2016) uses a significantly larger amount of features than the original AlexNet.", "startOffset": 9, "endOffset": 56}, {"referenceID": 25, "context": "We get rid of batch normalization layers and use a data-dependent rescaling of the parameters (Kr\u00e4henb\u00fchl et al., 2015).", "startOffset": 94, "endOffset": 119}, {"referenceID": 10, "context": "Depending on the task, we finetune all layers in the network, or solely the classifier, following Donahue et al. (2016). In all experiments, the parameters of the convolutional layers are initialized with the ones obtained with our unsupervised approach.", "startOffset": 98, "endOffset": 120}, {"referenceID": 10, "context": "Depending on the task, we finetune all layers in the network, or solely the classifier, following Donahue et al. (2016). In all experiments, the parameters of the convolutional layers are initialized with the ones obtained with our unsupervised approach. The parameters of the classification layers are initialized with gaussian weights. We get rid of batch normalization layers and use a data-dependent rescaling of the parameters (Kr\u00e4henb\u00fchl et al., 2015). Table 4 shows the comparison between our model and other unsupervised approaches. The results for other methods are taken from Donahue et al. (2016) except for Zhang et al.", "startOffset": 98, "endOffset": 608}, {"referenceID": 10, "context": "Depending on the task, we finetune all layers in the network, or solely the classifier, following Donahue et al. (2016). In all experiments, the parameters of the convolutional layers are initialized with the ones obtained with our unsupervised approach. The parameters of the classification layers are initialized with gaussian weights. We get rid of batch normalization layers and use a data-dependent rescaling of the parameters (Kr\u00e4henb\u00fchl et al., 2015). Table 4 shows the comparison between our model and other unsupervised approaches. The results for other methods are taken from Donahue et al. (2016) except for Zhang et al. (2016).", "startOffset": 98, "endOffset": 639}, {"referenceID": 10, "context": "4 BiGAN (Donahue et al., 2016) 52.", "startOffset": 8, "endOffset": 30}, {"referenceID": 10, "context": "The GAN and autoencoder baselines are from Donahue et al. (2016). We report mean average prevision as customary on PASCAL VOC.", "startOffset": 43, "endOffset": 65}, {"referenceID": 10, "context": "performs slightly better than the best performing BiGAN model (Donahue et al., 2016).", "startOffset": 62, "endOffset": 84}], "year": 2017, "abstractText": "Convolutional neural networks provide visual features that perform remarkably well in many computer vision applications. However, training these networks requires significant amounts of supervision. This paper introduces a generic framework to train deep networks, end-to-end, with no supervision. We propose to fix a set of target representations, called Noise As Targets (NAT), and to constrain the deep features to align to them. This domain agnostic approach avoids the standard unsupervised learning issues of trivial solutions and collapsing of features. Thanks to a stochastic batch reassignment strategy and a separable square loss function, it scales to millions of images. The proposed approach produces representations that perform on par with state-of-the-art unsupervised methods on ImageNet and PASCAL VOC.", "creator": "LaTeX with hyperref package"}}}