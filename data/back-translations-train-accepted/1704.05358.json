{"id": "1704.05358", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "Representing Sentences as Low-Rank Subspaces", "abstract": "Sentences are important semantic units of natural language. A generic, distributional representation of sentences that can capture the latent semantics is beneficial to multiple downstream applications. We observe a simple geometry of sentences -- the word representations of a given sentence (on average 10.23 words in all SemEval datasets with a standard deviation 4.84) roughly lie in a low-rank subspace (roughly, rank 4). Motivated by this observation, we represent a sentence by the low-rank subspace spanned by its word vectors. Such an unsupervised representation is empirically validated via semantic textual similarity tasks on 19 different datasets, where it outperforms the sophisticated neural network models, including skip-thought vectors, by 15% on average.", "histories": [["v1", "Tue, 18 Apr 2017 14:30:32 GMT  (62kb,D)", "http://arxiv.org/abs/1704.05358v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiaqi mu", "suma bhat", "pramod viswanath"], "accepted": true, "id": "1704.05358"}, "pdf": {"name": "1704.05358.pdf", "metadata": {"source": "CRF", "title": "Representing Sentences as Low-Rank Subspaces", "authors": ["Jiaqi Mu", "Suma Bhat", "Pramod Viswanath"], "emails": ["pramodv}@illinois.edu"], "sections": [{"heading": "1 Introduction", "text": "There are two approaches to representations that contain rich syntactical information and can be modeled by sophisticated neural networks (e.g. evolutionary networks). In contrast to their ability to grasp linguistic regularities, this network contains 35 sentences: Similar words tend to have similar representations; similar pairs of words tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2014; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Arora et al., 2015; There are two approaches that capture much of the semantic information. Given the success of lexical representations, a question of great current interest is how to extend the power of distributed representations to sentences."}, {"heading": "2 Geometry of Sentences", "text": "Unlike previous studies (for example, doc2vec, Mikolov et al., 2013), our main contribution is the representation of sentences using non-vector space representations: a sentence can be well represented by the subspace of the word vectors - such a method is, of course, based on any word representation method. Due to the widespread use of Word2vec and GloVe, we use their publicly available word representations - word2vec et al., 2013) trained with Google News1 and GloVe et al., 2014) trained with Common Crawl2 - to test our observations."}, {"heading": "3 Experiments", "text": "In this section, we evaluate our sentence representations empirically on the STS tasks. The goal of this task is to test the degree to which the algorithm can capture the semantic equivalence between two sentences. For example, the similarity between \"a child who has a head start on a bicycle\" and \"a black and white cat playing with a blanket\" is 0 and the similarity between \"a cat standing on tree branches\" and \"a black and white cat standing high on tree branches\" is 3,6. The algorithm is then evaluated in terms of Pearson's correlation between the predicted score and human judgment. \"Datasets We test the performance of our algorithms on 19 different datasets, including Semeval STS tasks (2012, 2013, 2015, 2015), which come from multiple domains (for example, news, WordNet definitions, and Twitter)."}, {"heading": "4 Conclusion", "text": "This paper presents a novel unsupervised sentence representation using the Grassman geometry of word representations. While the current approach is based on pre-trained word representations, the joint learning of word and sentence representations and in conjunction with supervised data sets such as the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) is left to future research. Also interesting is the study of neural network architectures working on subspaces (as opposed to vectors), which allows a downstream evaluation of our novel representation."}], "references": [{"title": "Fine-grained analysis of sentence embeddings using auxiliary prediction tasks", "author": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg."], "venue": "arXiv preprint arXiv:1608.04207 .", "citeRegEx": "Adi et al\\.,? 2016", "shortCiteRegEx": "Adi et al\\.", "year": 2016}, {"title": "Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot", "author": ["Eneko Agirre", "Carmen Banea", "Claire Cardie", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo", "Inigo Lopez-Gazpio", "Montse Maritxalar", "Rada Mihalcea"], "venue": null, "citeRegEx": "Agirre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2015}, {"title": "Semeval-2014 task 10: Multilingual semantic textual similarity", "author": ["Eneko Agirre", "Carmen Banea", "Claire Cardie", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo", "Rada Mihalcea", "German Rigau", "Janyce Wiebe."], "venue": "Proceedings of the", "citeRegEx": "Agirre et al\\.,? 2014", "shortCiteRegEx": "Agirre et al\\.", "year": 2014}, {"title": "sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity", "author": ["Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor GonzalezAgirre", "Weiwei Guo."], "venue": "In* SEM 2013: The Second Joint Conference on Lexical and Computational Se-", "citeRegEx": "Agirre et al\\.,? 2013", "shortCiteRegEx": "Agirre et al\\.", "year": 2013}, {"title": "Semeval-2012 task 6: A pilot on semantic textual similarity", "author": ["Eneko Agirre", "Mona Diab", "Daniel Cer", "Aitor Gonzalez-Agirre."], "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the", "citeRegEx": "Agirre et al\\.,? 2012", "shortCiteRegEx": "Agirre et al\\.", "year": 2012}, {"title": "Rand-walk: A latent variable model approach to word embeddings", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski."], "venue": "arXiv preprint arXiv:1502.03520 .", "citeRegEx": "Arora et al\\.,? 2015", "shortCiteRegEx": "Arora et al\\.", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin."], "venue": "journal of machine learning research 3(Feb):1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12(Aug):2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Two step cca: A new spectral method for estimating vector models of words", "author": ["Paramveer Dhillon", "Jordan Rodu", "Dean Foster", "Lyle Ungar."], "venue": "arXiv preprint arXiv:1206.6403 .", "citeRegEx": "Dhillon et al\\.,? 2012", "shortCiteRegEx": "Dhillon et al\\.", "year": 2012}, {"title": "PPDB: The paraphrase database", "author": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch."], "venue": "Proceedings of NAACL-", "citeRegEx": "Ganitkevitch et al\\.,? 2013", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Geometry of compositionality", "author": ["Hongyu Gong", "Suma Bhat", "Pramod Viswanath."], "venue": "Association for Advancement of Artificial Intelligence (AAAI) .", "citeRegEx": "Gong et al\\.,? 2017", "shortCiteRegEx": "Gong et al\\.", "year": 2017}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["Felix Hill", "Kyunghyun Cho", "Anna Korhonen."], "venue": "arXiv preprint arXiv:1602.03483 .", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "arXiv preprint arXiv:1404.2188 .", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Siamese cbow: Optimizing word embeddings for sentence representations", "author": ["Tom Kenter", "Alexey Borisov", "Maarten de Rijke."], "venue": "arXiv preprint arXiv:1606.04640 .", "citeRegEx": "Kenter et al\\.,? 2016", "shortCiteRegEx": "Kenter et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882 .", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in neural information processing systems. pages 3294\u20133302.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "ICML. volume 14, pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Advances in neural information processing systems. pages 2177\u20132185.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "Interspeech. volume 2, page 3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton."], "venue": "Proceedings of the 24th international conference on Machine learning. ACM, pages 641\u2013648.", "citeRegEx": "Mnih and Hinton.,? 2007", "shortCiteRegEx": "Mnih and Hinton.", "year": 2007}, {"title": "Geometry of polysemy", "author": ["Jiaqi Mu", "Suma Bhat", "Pramod Viswanath."], "venue": "arXiv preprint arXiv:1610.07569 .", "citeRegEx": "Mu et al\\.,? 2016", "shortCiteRegEx": "Mu et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP. volume 14, pages 1532\u2013", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings of the conference on empirical", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Model-based word embeddings from decompositions of count matrices", "author": ["Karl Stratos", "Michael Collins", "Daniel Hsu."], "venue": "Proceedings of ACL. pages 1282\u20131291.", "citeRegEx": "Stratos et al\\.,? 2015", "shortCiteRegEx": "Stratos et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Towards universal paraphrastic sentence embeddings", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "arXiv preprint arXiv:1511.08198 .", "citeRegEx": "Wieting et al\\.,? 2015", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Yukun Zhu", "Ryan Kiros", "Richard Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "arXiv preprint", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 21, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 20, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 7, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 12, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 8, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 19, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 23, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 18, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 5, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 25, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 15, "context": ", convolutional neural networks (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (Sutskever et al.", "startOffset": 32, "endOffset": 70}, {"referenceID": 13, "context": ", convolutional neural networks (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (Sutskever et al.", "startOffset": 32, "endOffset": 70}, {"referenceID": 24, "context": ", 2016) and recursive neural networks (Socher et al., 2013)).", "startOffset": 38, "endOffset": 59}, {"referenceID": 27, "context": "words in this sentence (Wieting et al., 2015; Adi et al., 2016; Kenter et al., 2016).", "startOffset": 23, "endOffset": 84}, {"referenceID": 0, "context": "words in this sentence (Wieting et al., 2015; Adi et al., 2016; Kenter et al., 2016).", "startOffset": 23, "endOffset": 84}, {"referenceID": 14, "context": "words in this sentence (Wieting et al., 2015; Adi et al., 2016; Kenter et al., 2016).", "startOffset": 23, "endOffset": 84}, {"referenceID": 26, "context": "Recently, Wieting et al. (2015); Adi et al.", "startOffset": 10, "endOffset": 32}, {"referenceID": 0, "context": "(2015); Adi et al. (2016)", "startOffset": 8, "endOffset": 26}, {"referenceID": 10, "context": "The geometry is motivated by (Gong et al., 2017; Mu et al., 2016) where an interesting phenomenon is observed \u2013 the local context of a given word/phrase can be well represented by a low rank subspace.", "startOffset": 29, "endOffset": 65}, {"referenceID": 22, "context": "The geometry is motivated by (Gong et al., 2017; Mu et al., 2016) where an interesting phenomenon is observed \u2013 the local context of a given word/phrase can be well represented by a low rank subspace.", "startOffset": 29, "endOffset": 65}, {"referenceID": 23, "context": "Assembling successful distributional word representations (for example, GloVe (Pennington et al., 2014)) into sentence representations is an active research topic.", "startOffset": 78, "endOffset": 103}, {"referenceID": 19, "context": "Different from previous studies (for example, doc2vec (Mikolov et al., 2013), skip-thought vectors (Kiros et al.", "startOffset": 54, "endOffset": 76}, {"referenceID": 16, "context": ", 2013), skip-thought vectors (Kiros et al., 2015), Siamese CBOW (Kenter et al.", "startOffset": 30, "endOffset": 50}, {"referenceID": 14, "context": ", 2015), Siamese CBOW (Kenter et al., 2016)), our main contribution is to represent sentences using non-vector space representations: a sentence can be well represented by the subspace spanned by the context word vectors \u2013 such a method naturally builds on any word representation method.", "startOffset": 22, "endOffset": 43}, {"referenceID": 19, "context": "Due to the widespread use of word2vec and GloVe, we use their publicly available word representations \u2013 word2vec(Mikolov et al., 2013) trained using Google News1 and GloVe (Pennington et al.", "startOffset": 112, "endOffset": 134}, {"referenceID": 23, "context": ", 2013) trained using Google News1 and GloVe (Pennington et al., 2014) trained using Common Crawl2 \u2013 to test our observations.", "startOffset": 45, "endOffset": 70}, {"referenceID": 17, "context": "(of GloVe and skipgram) where we use the average of word vectors), doc2vec (D2V) (Le and Mikolov, 2014) and", "startOffset": 81, "endOffset": 103}, {"referenceID": 16, "context": ", skip-thought vectors (ST) (Kiros et al., 2015), Siamese CBOW", "startOffset": 28, "endOffset": 48}, {"referenceID": 14, "context": "(SC) (Kenter et al., 2016)).", "startOffset": 5, "endOffset": 26}, {"referenceID": 28, "context": "In order to enable a fair comparison, we use the Toronto Book Corpus (Zhu et al., 2015) to train word embeddings.", "startOffset": 69, "endOffset": 87}, {"referenceID": 14, "context": "In our experiment, we adapt the same setting as in (Kenter et al., 2016) where we use skip-gram", "startOffset": 51, "endOffset": 72}, {"referenceID": 19, "context": "(Mikolov et al., 2013) of and GloVe (Pennington et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 23, "context": ", 2013) of and GloVe (Pennington et al., 2014) to train a 300-dimensional word vectors for the words that occur 5 times or more in the training corpus.", "startOffset": 21, "endOffset": 46}, {"referenceID": 9, "context": "While the current approach relies on the pre-trained word representations, the joint learning of both word and sentence representations and in conjunction with supervised datasets such the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) is left to future research.", "startOffset": 216, "endOffset": 243}], "year": 2017, "abstractText": "Sentences are important semantic units of natural language. A generic, distributional representation of sentences that can capture the latent semantics is beneficial to multiple downstream applications. We observe a simple geometry of sentences \u2013 the word representations of a given sentence (on average 10.23 words in all SemEval datasets with a standard deviation 4.84) roughly lie in a low-rank subspace (roughly, rank 4). Motivated by this observation, we represent a sentence by the low-rank subspace spanned by its word vectors. Such an unsupervised representation is empirically validated via semantic textual similarity tasks on 19 different datasets, where it outperforms the sophisticated neural network models, including skip-thought vectors, by 15% on average.", "creator": "LaTeX with hyperref package"}}}