{"id": "1704.05572", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Answering Complex Questions Using Open Information Extraction", "abstract": "While there has been substantial progress in factoid question-answering (QA), answering complex questions remains challenging, typically requiring both a large body of knowledge and inference techniques. Open Information Extraction (Open IE) provides a way to generate semi-structured knowledge for QA, but to date such knowledge has only been used to answer simple questions with retrieval-based methods. We overcome this limitation by presenting a method for reasoning with Open IE knowledge, allowing more complex questions to be handled. Using a recently proposed support graph optimization framework for QA, we develop a new inference model for Open IE, in particular one that can work effectively with multiple short facts, noise, and the relational structure of tuples. Our model significantly outperforms a state-of-the-art structured solver on complex questions of varying difficulty, while also removing the reliance on manually curated knowledge.", "histories": [["v1", "Wed, 19 Apr 2017 01:07:56 GMT  (80kb,D)", "http://arxiv.org/abs/1704.05572v1", "Accepted as short paper at ACL 2017"]], "COMMENTS": "Accepted as short paper at ACL 2017", "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["tushar khot", "ashish sabharwal", "peter clark"], "accepted": true, "id": "1704.05572"}, "pdf": {"name": "1704.05572.pdf", "metadata": {"source": "CRF", "title": "Answering Complex Questions Using Open Information Extraction", "authors": ["Tushar Khot", "Ashish Sabharwal", "Peter Clark"], "emails": ["tushark@allenai.org", "ashishs@allenai.org", "peterc@allenai.org"], "sections": [{"heading": "1 Introduction", "text": "These KBs, however, are expensive to build and typically domain specific. Automatically constructed open vocabulary (subject; predicate; object) style tuples have broader coverage, but have only been used for simple questions where a single tupel is sufficient (Fader et al., 2014; Yin et al., 2015).Our goal in this paper is to develop a QA system that can argue with Open IE (Bankoet al., 2007) tuples for complex multiple-choice questions that require multiple sentences. Such a system can answer complex questions in resource-poor areas where curated knowledge is not available. Elementary science exam is one such domain that requires complex reasoning (Clark, 2015)."}, {"heading": "2 Related Work", "text": "We will discuss two classes of related work: answering web questions (simply arguing with large KB) and answering scientific questions (complex arguing with small KB).Web QA: There are several systems for on-demand web QA problems (Ferrucci et al., 2010; Brill et al., 2002). While structured KBs such as Freebase are used in many (Berant et al., 2013; Berant et al., 2014; Kwiatkowski et al., 2013), such approaches are limited by the coverage of the data. QA systems that use semi-structured Open IE tuples (Fader et al., 2013; Yin et al., 2015) or automatically extracted web tables (Sun et al., 2016; Pasupat and Liang, 2015) have broader coverage but are limited to simple questions with a single query. Science QA: Elementary scientific QA tasks require argumentation to deal with complex questions."}, {"heading": "3 Tuple Inference Solver", "text": "We define a tuple as (subject; predicate; objects) with zero or more objects. We designate the subject, predicate, and objects as fields of the tuple."}, {"heading": "3.1 Tuple KB", "text": "We use the text corpora (S) from Clark et al. (2016) to create our tuple KB. For each test set, we use the corresponding training questions Qtr to retrieve domain-relevant sentences from S. Specifically, for each multiple choice question (q, A) and choice, we use an \"A,\" all non-stop word tokens in q and a as an ElasticSearch1 query against S. We take the top 200 hits, run Open IE v4,2 and aggregate the resulting tuples across all a and all questions in Qtr to generate the tupel KB (T). 3"}, {"heading": "3.2 Tuple Selection", "text": "Faced with a multiple choice question qa with question text q and answers A = {ai}, we select the most relevant tuples from T and S. Selection from tuples KB: We use an inverted index to find the 1000 tuples that have the most overlapping sets of question marks tok (qa). 4. We also filter out all tuples that only overlap with tok (q) as they do not support an answer. We calculate the normalized TF-IDF score that treats question q as a query and each tuple, t as a document."}, {"heading": "3.3 Support Graph Search", "text": "This year it is more than ever before."}, {"heading": "4 Experiments", "text": "Comparing our method with two state-of-the-art systems for science tests in 4th and 8th grade, we show that (a) TUPLEINF with only automatically extracted tuples significantly exceeds TABLEILP with its originally curated knowledge and additional tuples, and (b) TUPLEINF's complementary approach to IR leads to an improved overall ensemble. Bold numbers point to statistical significance based on the binomical exact test (Howell, 2012) by p = 05.5. We consider two sets of questions. (1) The 4th grade set (1220 train, 1304 test) is a 10x larger superset of NY regents questions (Clark et al, 2016) and includes professionally written licensed questions. (2) The 8th grade set (293 train, 282 test) contains 8th grade questions (1220 train, 1304 test)."}, {"heading": "4.1 Results", "text": "Table 2 shows that TUPLEINF, without curated knowledge, outperforms TABLEILP in both questions by more than 11%. The bottom half of the table shows that even if both solvers receive the same knowledge (C + T), 11 the improved selection and simplified model of TUPLEINF12 leads to a statistically significant improvement. Our simple model, TUPLEINF (C + T), also achieves values comparable to TABLEILP in its target questions (61.4% compared to TABLEINF12), without specialized rules. Table 3 shows that while TUPLEINF achieves similar values to the IR solver, the approaches are complementary (structured lossy reasoning compared to lossless sentence calls). In fact, the two solvers differ in 47.3% of training questions."}, {"heading": "5 Error Analysis", "text": "We describe four classes of failures that we have observed, and the future work they propose. Missing important words: What material will spread to fill a larger container completely? (A) Air (B) Ice (C) Sand (D) Water In this question we have tuples that spread water and fill a larger container, but miss the critical word \"completely.\" An approach that is able to detect distinctive question words could help to avoid this. Lossy IE: What measure is the best method to separate a mixture of salt and water?... The IR solver answers this question correctly by using the phrase: Separate the salt and water mixture by evaporating the water. However, TUPLEINF is not able to answer this question, since Open IE is unable to extract tuples from this compelling sentence. While the additional structure of Open IE is useful for a more robust adjustment, Open IE converts important phrases into Open Tupels."}, {"heading": "6 Conclusion", "text": "Our results show that TUPLEINF is a new state-of-the-art structured elementary science solution that is not based on curated knowledge and generalizes to a higher degree. Errors due to loss-making IE and misalignments point to future work in incorporating context and distribution measures."}, {"heading": "A Appendix: ILP Model Details", "text": "To build the ILP model, we must first derive the question terms (qterm) from the question by dividing the question with an internal chunker based on the post tagger from FACTORY. 13Variables The ILP model has an active vertex variable for each qterm (xq), tuples (xt), tuples (xf), and question selection (xa). Table 4 describes the coefficients of these active variables. For example, the coefficient of each qterm is a constant value (0.8), scaled by three increments. idf boost, idfB for a qterm, x is calculated as a log (1 + (Tqa | T \u2032 qa |) / nx), where nx is the number of tuples in Tqa \u00b2 qa. The scientific term boost, scienceB increases the coefficients of qterms based on a list of 9K terms."}, {"heading": "B Experiment Details", "text": "We use the SCIP ILP optimization engine (Achterberg, 2009) to optimize our ILP model. To get the score for each solution selection ai, we force the active variable for this choice xai to one and use the objective functional value of the ILP model as the score. For the evaluation we use a 2-core Linux calculator with 2.5 GHz Amazon EC2 with 16 GB RAM. To evaluate TABLEILP and TUPLEINF on curated tables and tuples, we convert them as follows into the expected format of each solver. B.1 Using curated tables with TUPLEINF, we select for each question the 7 best matching tables using the tf-idf score of the table w.r.t. The question tokens and top 20 rows from each table we use the Jaccard similarity of the row with the question. (same as Khashabi et al. (2016)."}], "references": [{"title": "SCIP: solving constraint integer programs", "author": ["Tobias Achterberg."], "venue": "Math. Prog. Computation 1(1):1\u2013", "citeRegEx": "Achterberg.,? 2009", "shortCiteRegEx": "Achterberg.", "year": 2009}, {"title": "Open information extraction from the web", "author": ["Michele Banko", "Michael J. Cafarella", "Stephen Soderland", "Matthew Broadhead", "Oren Etzioni."], "venue": "IJCAI.", "citeRegEx": "Banko et al\\.,? 2007", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "Semantic parsing on Freebase from question-answer pairs", "author": ["J. Berant", "A. Chou", "R. Frostig", "P. Liang."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Semantic parsing via paraphrasing", "author": ["Jonathan Berant", "Percy Liang."], "venue": "ACL.", "citeRegEx": "Berant and Liang.,? 2014", "shortCiteRegEx": "Berant and Liang.", "year": 2014}, {"title": "An analysis of the AskMSR question-answering system", "author": ["Eric Brill", "Susan Dumais", "Michele Banko."], "venue": "Proceedings of EMNLP. pages 257\u2013264.", "citeRegEx": "Brill et al\\.,? 2002", "shortCiteRegEx": "Brill et al\\.", "year": 2002}, {"title": "Taking up the gaokao challenge: An information retrieval approach", "author": ["Gong Cheng", "Weixi Zhu", "Ziwei Wang", "Jianghui Chen", "Yuzhong Qu."], "venue": "IJCAI.", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Elementary school science and math tests as a driver for AI: take the Aristo challenge! In 29th AAAI/IAAI", "author": ["Peter Clark."], "venue": "Austin, TX, pages 4019\u20134021.", "citeRegEx": "Clark.,? 2015", "shortCiteRegEx": "Clark.", "year": 2015}, {"title": "Combining retrieval, statistics, and inference to answer elementary science questions", "author": ["Peter Clark", "Oren Etzioni", "Tushar Khot", "Ashish Sabharwal", "Oyvind Tafjord", "Peter Turney", "Daniel Khashabi."], "venue": "30th AAAI.", "citeRegEx": "Clark et al\\.,? 2016", "shortCiteRegEx": "Clark et al\\.", "year": 2016}, {"title": "Recognizing textual entailment: Rational, evaluation and approaches\u2013erratum", "author": ["Ido Dagan", "Bill Dolan", "Bernardo Magnini", "Dan Roth."], "venue": "Natural Language Engineering 16(01):105\u2013105.", "citeRegEx": "Dagan et al\\.,? 2010", "shortCiteRegEx": "Dagan et al\\.", "year": 2010}, {"title": "Paraphrase-driven learning for open question answering", "author": ["Anthony Fader", "Luke S. Zettlemoyer", "Oren Etzioni."], "venue": "ACL.", "citeRegEx": "Fader et al\\.,? 2013", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["Anthony Fader", "Luke S. Zettlemoyer", "Oren Etzioni."], "venue": "KDD.", "citeRegEx": "Fader et al\\.,? 2014", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "Building Watson: An overview of the DeepQA project. AI Magazine 31(3):59\u201379", "author": ["David Ferrucci", "Eric Brown", "Jennifer Chu-Carroll", "James Fan", "David Gondek", "Aditya A Kalyanpur", "Adam Lally", "J William Murdock", "Eric Nyberg", "John Prager"], "venue": null, "citeRegEx": "Ferrucci et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ferrucci et al\\.", "year": 2010}, {"title": "Statistical methods for psychology", "author": ["David Howell."], "venue": "Cengage Learning.", "citeRegEx": "Howell.,? 2012", "shortCiteRegEx": "Howell.", "year": 2012}, {"title": "Question answering via integer programming over semistructured knowledge", "author": ["Daniel Khashabi", "Tushar Khot", "Ashish Sabharwal", "Peter Clark", "Oren Etzioni", "Dan Roth."], "venue": "IJCAI.", "citeRegEx": "Khashabi et al\\.,? 2016", "shortCiteRegEx": "Khashabi et al\\.", "year": 2016}, {"title": "Exploring Markov logic networks for question answering", "author": ["Tushar Khot", "Niranjan Balasubramanian", "Eric Gribkoff", "Ashish Sabharwal", "Peter Clark", "Oren Etzioni."], "venue": "EMNLP.", "citeRegEx": "Khot et al\\.,? 2015", "shortCiteRegEx": "Khot et al\\.", "year": 2015}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["Tom Kwiatkowski", "Eunsol Choi", "Yoav Artzi", "Luke S. Zettlemoyer."], "venue": "EMNLP.", "citeRegEx": "Kwiatkowski et al\\.,? 2013", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George Miller."], "venue": "Communications of the ACM 38(11):39\u2013", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Compositional semantic parsing on semi-structured tables", "author": ["Panupong Pasupat", "Percy Liang."], "venue": "ACL.", "citeRegEx": "Pasupat and Liang.,? 2015", "shortCiteRegEx": "Pasupat and Liang.", "year": 2015}, {"title": "Markov logic networks", "author": ["Matthew Richardson", "Pedro Domingos."], "venue": "Machine learning 62(1\u2013 2):107\u2013136.", "citeRegEx": "Richardson and Domingos.,? 2006", "shortCiteRegEx": "Richardson and Domingos.", "year": 2006}, {"title": "Towards addressing the winograd schema challenge - building and using a semantic parser and a knowledge hunting module", "author": ["Arpit Sharma", "Nguyen Ha Vo", "Somak Aditya", "Chitta Baral."], "venue": "IJCAI.", "citeRegEx": "Sharma et al\\.,? 2015", "shortCiteRegEx": "Sharma et al\\.", "year": 2015}, {"title": "Table cell search for question answering", "author": ["Huan Sun", "Hao Ma", "Xiaodong He", "Wen tau Yih", "Yu Su", "Xifeng Yan."], "venue": "WWW.", "citeRegEx": "Sun et al\\.,? 2016", "shortCiteRegEx": "Sun et al\\.", "year": 2016}, {"title": "Answering questions with complex semantic constraints on open knowledge bases", "author": ["Pengcheng Yin", "Nan Duan", "Ben Kao", "Jun-Wei Bao", "Ming Zhou."], "venue": "CIKM.", "citeRegEx": "Yin et al\\.,? 2015", "shortCiteRegEx": "Yin et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Structured curated KBs have been used successfully for this task (Berant et al., 2013; Berant and Liang, 2014).", "startOffset": 65, "endOffset": 110}, {"referenceID": 3, "context": "Structured curated KBs have been used successfully for this task (Berant et al., 2013; Berant and Liang, 2014).", "startOffset": 65, "endOffset": 110}, {"referenceID": 10, "context": "Automatically constructed open vocabulary (subject; predicate; object) style tuples have broader coverage, but have only been used for simple questions where a single tuple suffices (Fader et al., 2014; Yin et al., 2015).", "startOffset": 182, "endOffset": 220}, {"referenceID": 21, "context": "Automatically constructed open vocabulary (subject; predicate; object) style tuples have broader coverage, but have only been used for simple questions where a single tuple suffices (Fader et al., 2014; Yin et al., 2015).", "startOffset": 182, "endOffset": 220}, {"referenceID": 1, "context": "Our goal in this work is to develop a QA system that can perform reasoning with Open IE (Banko et al., 2007) tuples for complex multiple-choice questions that require tuples from multiple sentences.", "startOffset": 88, "endOffset": 108}, {"referenceID": 6, "context": "Elementary-level science exams is one such domain, requiring complex reasoning (Clark, 2015).", "startOffset": 79, "endOffset": 92}, {"referenceID": 7, "context": "Due to the lack of a large-scale structured KB, state-of-the-art systems for this task either rely on shallow reasoning with large text corpora (Clark et al., 2016; Cheng et al., 2016) or deeper, structured reasoning with a small amount of automatically acquired (Khot et al.", "startOffset": 144, "endOffset": 184}, {"referenceID": 5, "context": "Due to the lack of a large-scale structured KB, state-of-the-art systems for this task either rely on shallow reasoning with large text corpora (Clark et al., 2016; Cheng et al., 2016) or deeper, structured reasoning with a small amount of automatically acquired (Khot et al.", "startOffset": 144, "endOffset": 184}, {"referenceID": 14, "context": ", 2016) or deeper, structured reasoning with a small amount of automatically acquired (Khot et al., 2015) or manually curated (Khashabi et al.", "startOffset": 86, "endOffset": 105}, {"referenceID": 13, "context": ", 2015) or manually curated (Khashabi et al., 2016) knowledge.", "startOffset": 28, "endOffset": 51}, {"referenceID": 13, "context": "A candidate system for such reasoning, and which we draw inspiration from, is the TABLEILP system of Khashabi et al. (2016). TABLEILP treats QA as a search for an optimal subgraph that connects terms in the question and answer via rows in a set of curated tables, and solves the optimization problem using Integer Linear Programming (ILP).", "startOffset": 101, "endOffset": 124}, {"referenceID": 11, "context": "Web QA: There exist several systems for retrieval-based Web QA problems (Ferrucci et al., 2010; Brill et al., 2002).", "startOffset": 72, "endOffset": 115}, {"referenceID": 4, "context": "Web QA: There exist several systems for retrieval-based Web QA problems (Ferrucci et al., 2010; Brill et al., 2002).", "startOffset": 72, "endOffset": 115}, {"referenceID": 2, "context": "While structured KBs such as Freebase have been used in many (Berant et al., 2013; Berant and Liang, 2014; Kwiatkowski et al., 2013), such approaches are limited by the coverage of the data.", "startOffset": 61, "endOffset": 132}, {"referenceID": 3, "context": "While structured KBs such as Freebase have been used in many (Berant et al., 2013; Berant and Liang, 2014; Kwiatkowski et al., 2013), such approaches are limited by the coverage of the data.", "startOffset": 61, "endOffset": 132}, {"referenceID": 15, "context": "While structured KBs such as Freebase have been used in many (Berant et al., 2013; Berant and Liang, 2014; Kwiatkowski et al., 2013), such approaches are limited by the coverage of the data.", "startOffset": 61, "endOffset": 132}, {"referenceID": 21, "context": "QA systems using semistructured Open IE tuples (Fader et al., 2013, 2014; Yin et al., 2015) or automatically extracted web tables (Sun et al.", "startOffset": 47, "endOffset": 91}, {"referenceID": 20, "context": ", 2015) or automatically extracted web tables (Sun et al., 2016; Pasupat and Liang, 2015) have broader coverage but are limited to simple questions with a single query.", "startOffset": 46, "endOffset": 89}, {"referenceID": 17, "context": ", 2015) or automatically extracted web tables (Sun et al., 2016; Pasupat and Liang, 2015) have broader coverage but are limited to simple questions with a single query.", "startOffset": 46, "endOffset": 89}, {"referenceID": 18, "context": "Markov Logic Networks (Richardson and Domingos, 2006) have been used to perform probabilistic reasoning over a small set of logical rules (Khot et al.", "startOffset": 22, "endOffset": 53}, {"referenceID": 14, "context": "Markov Logic Networks (Richardson and Domingos, 2006) have been used to perform probabilistic reasoning over a small set of logical rules (Khot et al., 2015).", "startOffset": 138, "endOffset": 157}, {"referenceID": 7, "context": "Simple IR techniques have also been proposed for science tests (Clark et al., 2016) and Gaokao tests (equivalent to the SAT exam in China) (Cheng et al.", "startOffset": 63, "endOffset": 83}, {"referenceID": 5, "context": ", 2016) and Gaokao tests (equivalent to the SAT exam in China) (Cheng et al., 2016).", "startOffset": 63, "endOffset": 83}, {"referenceID": 6, "context": "We use the text corpora (S) from Clark et al. (2016) to build our tuple KB.", "startOffset": 33, "endOffset": 53}, {"referenceID": 19, "context": "On-the-fly tuples from text: To handle questions from new domains not covered by the training set, we extract additional tuples on the fly from S (similar to Sharma et al. (2015)).", "startOffset": 158, "endOffset": 179}, {"referenceID": 8, "context": "Unlike standard alignment models used for tasks such as Recognizing Textual Entailment (RTE) (Dagan et al., 2010), however, we must score alignments between a set Tqa \u222a T \u2032 qa of structured tuples and a (potentially multi-sentence) multiple-choice question qa.", "startOffset": 93, "endOffset": 113}, {"referenceID": 16, "context": "9 While TABLEILP used WordNet (Miller, 1995) paths to compute the weight, this measure results in unreliable scores when faced with longer phrases found in Open IE tuples.", "startOffset": 30, "endOffset": 44}, {"referenceID": 12, "context": "Numbers in bold indicate statistical significance based on the Binomial exact test (Howell, 2012) at p = 0.", "startOffset": 83, "endOffset": 97}, {"referenceID": 7, "context": "(1) 4th Grade set (1220 train, 1304 test) is a 10x larger superset of the NY Regents questions (Clark et al., 2016), and includes professionally written licensed questions.", "startOffset": 95, "endOffset": 115}, {"referenceID": 7, "context": "IR is a simple yet powerful information-retrieval baseline (Clark et al., 2016) that selects the answer option with the best matching sentence in a corpus.", "startOffset": 59, "endOffset": 79}, {"referenceID": 13, "context": "TABLEILP is the stateof-the-art structured inference baseline (Khashabi et al., 2016) developed for science questions.", "startOffset": 62, "endOffset": 85}, {"referenceID": 6, "context": "web pages used by Clark et al. (2016). This corpus is used by the IR solver and also used to create the tuple KB T and on-the-fly tuples T \u2032 qa.", "startOffset": 18, "endOffset": 38}, {"referenceID": 7, "context": "we train an ensemble system (Clark et al., 2016) which, as shown in the table, provides a substantial boost over the individual solvers.", "startOffset": 28, "endOffset": 48}, {"referenceID": 6, "context": "we train an ensemble system (Clark et al., 2016) which, as shown in the table, provides a substantial boost over the individual solvers. Further, IR + TUPLEINF is consistently better than IR + TABLEILP. Finally, in combination with IR and the statistical association based PMI solver (that scores 54.1% by itself) of Clark et al. (2016), TUPLEINF achieves a score of 58.", "startOffset": 29, "endOffset": 337}], "year": 2017, "abstractText": "While there has been substantial progress in factoid question-answering (QA), answering complex questions remains challenging, typically requiring both a large body of knowledge and inference techniques. Open Information Extraction (Open IE) provides a way to generate semi-structured knowledge for QA, but to date such knowledge has only been used to answer simple questions with retrievalbased methods. We overcome this limitation by presenting a method for reasoning with Open IE knowledge, allowing more complex questions to be handled. Using a recently proposed support graph optimization framework for QA, we develop a new inference model for Open IE, in particular one that can work effectively with multiple short facts, noise, and the relational structure of tuples. Our model significantly outperforms a state-of-the-art structured solver on complex questions of varying difficulty, while also removing the reliance on manually curated knowledge.", "creator": "LaTeX with hyperref package"}}}