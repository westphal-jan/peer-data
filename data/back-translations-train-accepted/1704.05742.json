{"id": "1704.05742", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Adversarial Multi-task Learning for Text Classification", "abstract": "Neural network models have shown their promising opportunities for multi-task learning, which focus on learning the shared layers to extract the common and task-invariant features. However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks. In this paper, we propose an adversarial multi-task learning framework, alleviating the shared and private latent feature spaces from interfering with each other. We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach. Besides, we show that the shared knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks. The datasets of all 16 tasks are publicly available at \\url{", "histories": [["v1", "Wed, 19 Apr 2017 14:17:25 GMT  (402kb,D)", "http://arxiv.org/abs/1704.05742v1", "Accepted by ACL2017"]], "COMMENTS": "Accepted by ACL2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["pengfei liu", "xipeng qiu", "xuanjing huang"], "accepted": true, "id": "1704.05742"}, "pdf": {"name": "1704.05742.pdf", "metadata": {"source": "CRF", "title": "Adversarial Multi-task Learning for Text Classification", "authors": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang"], "emails": ["pfliu14@fudan.edu.cn", "xpqiu@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move and to move, to move, to move, to move and to move, to move, to move and to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "2 Recurrent Models for Text Classification", "text": "There are many neural set models that can be used for the LSTM to define the recurrent neural networks (Sutskever et al., 2014), and recursive neural networks (LSTM), where the recurrent neural networks (LSTM) are due to their superior performance in various NLP tasks (Liu et al., 2016a), and recursive neural networks (Lin et al., 2017), where the recurrent neural networks (LSTM) and the recurrent neural networks (LLLP et al., 2016a; Lin et al., 2017). Long-term memory networks (LSTM) is a type of recurrent neural networks (RNN) (Elman, 1990), and specifically addresses the issue of long-term dependencies."}, {"heading": "3 Multi-task Learning for Text Classification", "text": "The aim of multi-task learning is to use the correlation between these related tasks to improve classification by learning tasks in parallel. To facilitate this, we give some explanations for the notations used in this essay. Formally, we refer to Dk as a dataset with Nk examples for task k. Specifically, Dk = {(xki, yki)} Nk i = 1 (7), where xki and y k i denote a sentence and the corresponding designation for task k."}, {"heading": "3.1 Two Sharing Schemes for Sentence Modeling", "text": "The key factor of multi-task learning is the sharing scheme in the latent attribute space. In the neural network-based model, the latent features can be considered as the states of hidden neurons. Specific to text classification are the hidden states of the LSTM at the end of a sentence. Therefore, the sharing schemes differ in how the common features are grouped. At this point, we will first introduce two sharing schemes with multi-task learning: Full-shared schemes and Shared-Private schemes. Fully-Shared models (FS-MTL) In the Full-Shared model, we use a single Shared-LSTM layer to extract features for all tasks. For example, it takes the view that the features of the task m can be fully divided by task n and vice versa. This model ignores the fact that some features are task-dependent."}, {"heading": "3.2 Task-Specific Output Layer", "text": "For a set in task k, its feature h (k), which is emitted by the deep muti task architectures, is ultimately fed into the corresponding task-specific Softmax layer for classification or other tasks. Network parameters are trained to minimize the cross entropy of the predicted and actual distributions of all tasks. LTask loss task can be calculated as follows: LTask = K \u2211 k = 1 \u03b1kL (y (k), y (k)))) (10), with \u03b1k being the weights for each task k. L (y, y) is defined as equation 6."}, {"heading": "4 Incorporating Adversarial Training", "text": "Although the shared-private model separates the feature space into the shared-private space, there is no guarantee that shared-feature space cannot exist in the private feature space, or vice versa. Therefore, some useful shared feature space in the shared-private model could be ignored, and the shared feature space is also vulnerable to contamination by some task-specific information. Therefore, a simple principle can be applied in multi-task learning that a good shared feature space should contain more common information and not task-specific information. To solve this problem, we are introducing training into the multi-task framework, as shown in Figure 3 (ASPMTL)."}, {"heading": "4.1 Adversarial Network", "text": "The goal is to learn a generative distribution pG (x) that matches the real data distribution Pdata (x). Specifically, GAN learns a generative network G and a discriminatory model D in which G generates samples from the generator distribution pG (x). And D learns to determine whether a sample originates from pG (x) or Pdata (x). This min-max game can be optimized by the following risk: \u03c6 = min G max D (Ex \u0445 Pdata [logD (x)] + Ez \u0445 p (z) [log (1 \u2212 D (G (z)))))) (11) Although originally proposed for generating random samples, adversarial network can be used as a general tool for measuring equivalence between distributions (Taigman et al., 2016). Formally (Ajakan et al., 2014), the adverse loss to divergence H is successfully combined with the method of distribution H cannot be achieved by a method H."}, {"heading": "4.2 Task Adversarial Loss for MTL", "text": "Inspired by adversarial networks (Goodfellow et al., 2014), we proposed an adversarial sharedprivate model for multitask learning, in which a common recurring neural layer works counter-productively towards a learnable multilayer perceptron and prevents it from making an accurate prediction of the types of tasks. This adversarial training encourages the common space to be purer and ensures that the common representation is not contaminated by task-specific characteristics. Task Discriminator Discriminator is used to convert the split representation of sentences into a probability distribution, estimating which types of tasks the coded sentence originates from the coded sentence. D (skT, successD) = softmax (b + Us k T) (12), where U-Rd \u00d7 d is a learnable parameter and b-Rd is a biological adapversarial. Adversarial Loss task, with most of the existing learning algorithms we add a multiversarial loss v."}, {"heading": "4.3 Orthogonality Constraints", "text": "Motivated by the recent work (Jia et al., 2010; Salzmann et al., 2010; Bousmalis et al., 2016) on the analysis of common and private latent space, we are introducing orthogonal constraints that punish redundant latent representations and encourage shared and private extractors to encode various aspects of the input factors. After studying many optional methods, we find that, at a loss, what is used by Bousmalis et al. (2016) is optimal and achieves better performance: Ldiff = K, k = 1, 2, (14) where the square extractor 2F is the Frobenius standard. S k and Hk are two matrices whose series are the output of the shared extractor Es (,; eigs) and the task-specific extrator Ek (eigent)."}, {"heading": "4.4 Put It All Together", "text": "The final loss function of our model can be described as follows: L = LTask + \u03bbLAdv + \u03b3LDiff (15), where \u03bb and \u03b3 are hyperparameters, the networks are trained with reverse propagation and this minimax optimization is possible by using a gradient reverse layer (Ganin and Lempitsky, 2015)."}, {"heading": "5 Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Dataset", "text": "The first 14 sets of data are product reviews containing Amazon product reviews from different areas, such as books, DVDs, electronics, etc. The goal is to classify a product review as either positive or negative, and these data sets are collected based on the raw data 1 provided by (Blitzer et al., 2007). Specifically, we extract the sets and the corresponding labels from the unprocessed original data 2. The only pre-processing of these sets is done using the Stanford token izer 3. The remaining two sets of data relate to movie reviews. IMDB data set 4 consists of binary class movie reviews (Maas et al., 2011).A key aspect of this data set is that each film review has several sets. The MR data set also consists of movie reviews from rotten tomato websites with two classes 5 (Pang et al., 2005).All data sets in each task are broken down in detail on development and broken down into statistics into 1% of each set."}, {"heading": "5.2 Competitor Methods for Multi-task Learning", "text": "The multi-task frameworks proposed by previous work are different, although not all can be applied to the tasks we have focused on. Nevertheless, we have selected two most closely related neural models for multi-task learning and implemented them as competing methods.1https: / / www.cs.jhu.edu / \u02dc mdredze / datasets / sentiment / 2Blitzer et al. (2007) also provides two additional processed data sets in Bag-of-Words format that are not suitable for neural-based models.3http: / / nlp.stanford.edu / software / tokenizer.shtml4https: / / www.cs.jhu.edu / the-layer-of-people-edu / 201.edu"}, {"heading": "5.3 Hyperparameters", "text": "The word embeddings for all models are initialized with the 200d GloVe vectors (Pennington et al., 2014), the other parameters are initialized by random samples from an even distribution in [\u2212 0,1, 0,1], the size of the mini-batch is set to 16, and for each task we take the hyperparameters that achieve the best performance on the development set by a small grid search using combinations of the initial learning rate [0,1, 0,01], \u03bb [0,01, 0,1] and \u03b3 [0,01, 0,1]."}, {"heading": "5.4 Performance Evaluation", "text": "Table 2 shows the error rates for 16 text classification tasks. The \"Single Task\" column shows the results of vanilla LSTM, bidirectional LSTM (BiLSTM), stacked LSTM (sLSTM), and the average error rates of previous three models. The \"Multiple Tasks\" column shows the results of corresponding multi-task models. This table shows that the performance of most tasks can be greatly improved with the help of multi-task learning, where our model has the lowest error rates. Specifically, ASP-MTL achieves an average improvement of 4.1% compared to SP-MTL, while SP-MTL surpasses with 1.0%, indicating the importance of mutual learning."}, {"heading": "5.5 Shared Knowledge Transfer", "text": "To test the transferability of our learned Shared Extractor, we also design an experiment in which we alternately select 15 tasks to train our model MS with multi-task learning, then transfer the learned Shared Layer to a second Network MT, which is used for the remaining task. Parameters of the transferred Layer are frozen, and the rest of the parameters of the Network MT are randomly initialized. Formally, we examine two mechanisms in the direction of the transferred Shared Extractor. As shown in Figure 4, the first Single Channel (SC) model consists of a Shared Feature Extractor It is from MS, then the extracted representation is sent to an output layer. In contrast, the BiChannel (BC) model introduces an additional LSTM layer to encode task-specific, more specific information."}, {"heading": "5.6 Visualization", "text": "In order to gain an intuitive understanding of how the established orthogonal constraints work in comparison to vanilla shared-private models, we are designing an experiment to study the behavior of neurons from private layers and shared layers. Specifically, we are referring to the activation of the jneuron in the time step t where this phenomenon does not exist."}, {"heading": "6 Related Work", "text": "There are two threads of related work. One thread is multi-task learning with neural networks. In addition, neural multi-task learning uses different LSTM layers to construct multi-task learning frameworks for text classification (Collobert and Weston, 2008; Glorot et al., 2011). Liu et al. (2016c) proposes a generic multi-task framework in which different tasks can exchange information via an external memory and communicate via a read / write mechanism. This work has potential limitations on learning only a common space solely on the basis of sharing parameters, while our model proposes two strategies for learning the clear and non-redundant shared-private space.Another thread of work is the Adversarial network. Adversarial networks have recently emerged as a general tool that measures the equivalence between distributions, and has proven effective in a variety of tasks."}, {"heading": "7 Conclusion", "text": "In this paper, we have proposed a contradictory multi-task learning framework in which task-specific and task-invariant characteristics are not learnt redundantly, thus capturing the common private separation of different tasks. We have demonstrated the effectiveness of our approach by applying our model to 16 different text classification tasks, conducting comprehensive qualitative analysis, deriving insights, and indirectly explaining the quantitative improvements in overall performance."}, {"heading": "Acknowledgments", "text": "We would like to thank the anonymous critics for their valuable comments and Kaiyu Qian, Gang Niu for useful discussions. This work was partly funded by the National Natural Science Foundation of China (No. 61532011 and 61672162), the National High Technology Research and Development Program of China (No. 2015AA015408), the Shanghai Municipal Science and Technology Commission (No. 16JC1420401)."}], "references": [{"title": "Domain-adversarial neural networks", "author": ["Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "Fran\u00e7ois Laviolette", "Mario Marchand."], "venue": "arXiv preprint arXiv:1412.4446 .", "citeRegEx": "Ajakan et al\\.,? 2014", "shortCiteRegEx": "Ajakan et al\\.", "year": 2014}, {"title": "A theory of learning from different domains", "author": ["Shai Ben-David", "John Blitzer", "Koby Crammer", "Alex Kulesza", "Fernando Pereira", "Jennifer Wortman Vaughan."], "venue": "Machine learning 79(1-2):151\u2013175.", "citeRegEx": "Ben.David et al\\.,? 2010", "shortCiteRegEx": "Ben.David et al\\.", "year": 2010}, {"title": "Analysis of representations for domain adaptation. Advances in neural information processing systems 19:137", "author": ["Shai Ben-David", "John Blitzer", "Koby Crammer", "Fernando Pereira"], "venue": null, "citeRegEx": "Ben.David et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2007}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["John Blitzer", "Mark Dredze", "Fernando Pereira"], "venue": "In ACL", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Domain separation networks", "author": ["Konstantinos Bousmalis", "George Trigeorgis", "Nathan Silberman", "Dilip Krishnan", "Dumitru Erhan."], "venue": "Advances in Neural Information Processing Systems. pages 343\u2013 351.", "citeRegEx": "Bousmalis et al\\.,? 2016", "shortCiteRegEx": "Bousmalis et al\\.", "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.3555 .", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of ICML.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The JMLR 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman."], "venue": "Cognitive science 14(2):179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Yaroslav Ganin", "Victor Lempitsky."], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15). pages 1180\u20131189.", "citeRegEx": "Ganin and Lempitsky.,? 2015", "shortCiteRegEx": "Ganin and Lempitsky.", "year": 2015}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11). pages 513\u2013520.", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."], "venue": "Advances in Neural Information Processing Systems. pages 2672\u20132680.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv preprint arXiv:1308.0850 .", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Factorized latent spaces with structured sparsity", "author": ["Yangqing Jia", "Mathieu Salzmann", "Trevor Darrell."], "venue": "Advances in Neural Information Processing Systems. pages 982\u2013990.", "citeRegEx": "Jia et al\\.,? 2010", "shortCiteRegEx": "Jia et al\\.", "year": 2010}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings of The 32nd International Conference on Machine Learning.", "citeRegEx": "Jozefowicz et al\\.,? 2015", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of ACL.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "A structured self-attentive sentence embedding", "author": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1703.03130 .", "citeRegEx": "Lin et al\\.,? 2017", "shortCiteRegEx": "Lin et al\\.", "year": 2017}, {"title": "Deep fusion LSTMs for text semantic matching", "author": ["Pengfe Liu", "Xipeng Qiu", "Jifan Chen", "Xuanjing Huang."], "venue": "Proceedings of ACL.", "citeRegEx": "Liu et al\\.,? 2016a", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Multi-timescale long short-term memory neural network for modelling sentences and documents", "author": ["PengFei Liu", "Xipeng Qiu", "Xinchi Chen", "Shiyu Wu", "Xuanjing Huang."], "venue": "Proceedings of the Conference on EMNLP.", "citeRegEx": "Liu et al\\.,? 2015a", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Deep multi-task learning with shared memory", "author": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Liu et al\\.,? 2016b", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Recurrent neural network for text classification with multi-task learning", "author": ["PengFei Liu", "Xipeng Qiu", "Xuanjing Huang."], "venue": "Proceedings of International Joint Conference on Artificial Intelligence.", "citeRegEx": "Liu et al\\.,? 2016c", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Representation learning using multi-task deep neural networks for semantic classification and information retrieval", "author": ["Xiaodong Liu", "Jianfeng Gao", "Xiaodong He", "Li Deng", "Kevin Duh", "Ye-Yi Wang."], "venue": "NAACL.", "citeRegEx": "Liu et al\\.,? 2015b", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "arXiv preprint arXiv:1511.06114 .", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Learning word vectors for sentiment analysis", "author": ["Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings of the ACL. pages 142\u2013150.", "citeRegEx": "Maas et al\\.,? 2011", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Cross-stitch networks for multi-task learning", "author": ["Ishan Misra", "Abhinav Shrivastava", "Abhinav Gupta", "Martial Hebert."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 3994\u20134003.", "citeRegEx": "Misra et al\\.,? 2016", "shortCiteRegEx": "Misra et al\\.", "year": 2016}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Bo Pang", "Lillian Lee."], "venue": "Proceedings of the 43rd annual meeting on association for computational linguistics. Association for Computational", "citeRegEx": "Pang and Lee.,? 2005", "shortCiteRegEx": "Pang and Lee.", "year": 2005}, {"title": "Image-text multi-modal representation learning by adversarial backpropagation", "author": ["Gwangbeen Park", "Woobin Im."], "venue": "arXiv preprint arXiv:1612.08354 .", "citeRegEx": "Park and Im.,? 2016", "shortCiteRegEx": "Park and Im.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the EMNLP 12:1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Factorized orthogonal latent spaces", "author": ["Mathieu Salzmann", "Carl Henrik Ek", "Raquel Urtasun", "Trevor Darrell."], "venue": "AISTATS. pages 701\u2013708.", "citeRegEx": "Salzmann et al\\.,? 2010", "shortCiteRegEx": "Salzmann et al\\.", "year": 2010}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in NIPS. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Unsupervised cross-domain image generation", "author": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf."], "venue": "arXiv preprint arXiv:1611.02200 .", "citeRegEx": "Taigman et al\\.,? 2016", "shortCiteRegEx": "Taigman et al\\.", "year": 2016}, {"title": "Facial landmark detection by deep multi-task learning", "author": ["Zhanpeng Zhang", "Ping Luo", "Chen Change Loy", "Xiaoou Tang."], "venue": "European Conference on Computer Vision. Springer, pages 94\u2013108.", "citeRegEx": "Zhang et al\\.,? 2014", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 25, "context": "Recently, neuralbased models for multi-task learning have become very popular, ranging from computer vision (Misra et al., 2016; Zhang et al., 2014) to natural language processing (Collobert and Weston, 2008; Luong et al.", "startOffset": 108, "endOffset": 148}, {"referenceID": 33, "context": "Recently, neuralbased models for multi-task learning have become very popular, ranging from computer vision (Misra et al., 2016; Zhang et al., 2014) to natural language processing (Collobert and Weston, 2008; Luong et al.", "startOffset": 108, "endOffset": 148}, {"referenceID": 6, "context": ", 2014) to natural language processing (Collobert and Weston, 2008; Luong et al., 2015), since they provide a convenient way of combining information from multiple tasks.", "startOffset": 39, "endOffset": 87}, {"referenceID": 23, "context": ", 2014) to natural language processing (Collobert and Weston, 2008; Luong et al., 2015), since they provide a convenient way of combining information from multiple tasks.", "startOffset": 39, "endOffset": 87}, {"referenceID": 31, "context": "There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014; Liu et al., 2015a), convolutional neural networks (Collobert et al.", "startOffset": 113, "endOffset": 176}, {"referenceID": 5, "context": "There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014; Liu et al., 2015a), convolutional neural networks (Collobert et al.", "startOffset": 113, "endOffset": 176}, {"referenceID": 19, "context": "There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014; Liu et al., 2015a), convolutional neural networks (Collobert et al.", "startOffset": 113, "endOffset": 176}, {"referenceID": 7, "context": ", 2015a), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al.", "startOffset": 40, "endOffset": 91}, {"referenceID": 16, "context": ", 2015a), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al.", "startOffset": 40, "endOffset": 91}, {"referenceID": 30, "context": ", 2014), and recursive neural networks (Socher et al., 2013).", "startOffset": 39, "endOffset": 60}, {"referenceID": 18, "context": "Here we adopt recurrent neural network with long short-term memory (LSTM) due to their superior performance in various NLP tasks (Liu et al., 2016a; Lin et al., 2017).", "startOffset": 129, "endOffset": 166}, {"referenceID": 17, "context": "Here we adopt recurrent neural network with long short-term memory (LSTM) due to their superior performance in various NLP tasks (Liu et al., 2016a; Lin et al., 2017).", "startOffset": 129, "endOffset": 166}, {"referenceID": 13, "context": "Long Short-term Memory Long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) is a type of recurrent neural network (RNN) (Elman, 1990), and specifically addresses the issue of learning long-term dependencies.", "startOffset": 61, "endOffset": 95}, {"referenceID": 8, "context": "Long Short-term Memory Long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) is a type of recurrent neural network (RNN) (Elman, 1990), and specifically addresses the issue of learning long-term dependencies.", "startOffset": 140, "endOffset": 153}, {"referenceID": 15, "context": "While there are numerous LSTM variants, here we use the LSTM architecture used by (Jozefowicz et al., 2015), which is similar to the architecture of (Graves, 2013) but without peep-hole connections.", "startOffset": 82, "endOffset": 107}, {"referenceID": 12, "context": ", 2015), which is similar to the architecture of (Graves, 2013) but without peep-hole connections.", "startOffset": 49, "endOffset": 63}, {"referenceID": 11, "context": "Adversarial networks have recently surfaced and are first used for generative model (Goodfellow et al., 2014).", "startOffset": 84, "endOffset": 109}, {"referenceID": 32, "context": "While originally proposed for generating random samples, adversarial network can be used as a general tool to measure equivalence between distributions (Taigman et al., 2016).", "startOffset": 152, "endOffset": 174}, {"referenceID": 0, "context": "Formally, (Ajakan et al., 2014) linked the adversarial loss to the H-divergence between two distributions and successfully achieve unsupervised domain adaptation with adversarial network.", "startOffset": 10, "endOffset": 31}, {"referenceID": 4, "context": "Motivated by theory on domain adaptation (Ben-David et al., 2010, 2007; Bousmalis et al., 2016) that a transferable feature is one for which an algorithm cannot learn to identify the domain of origin of the input observation.", "startOffset": 41, "endOffset": 95}, {"referenceID": 11, "context": "Inspired by adversarial networks (Goodfellow et al., 2014), we proposed an adversarial sharedprivate model for multi-task learning, in which a shared recurrent neural layer is working adversarially towards a learnable multi-layer perceptron, preventing it from making an accurate prediction about the types of tasks.", "startOffset": 33, "endOffset": 58}, {"referenceID": 14, "context": "Motivated by recently work(Jia et al., 2010; Salzmann et al., 2010; Bousmalis et al., 2016)", "startOffset": 26, "endOffset": 91}, {"referenceID": 29, "context": "Motivated by recently work(Jia et al., 2010; Salzmann et al., 2010; Bousmalis et al., 2016)", "startOffset": 26, "endOffset": 91}, {"referenceID": 4, "context": "Motivated by recently work(Jia et al., 2010; Salzmann et al., 2010; Bousmalis et al., 2016)", "startOffset": 26, "endOffset": 91}, {"referenceID": 4, "context": "After exploring many optional methods, we find below loss is optimal, which is used by Bousmalis et al. (2016) and achieve a better performance:", "startOffset": 87, "endOffset": 111}, {"referenceID": 9, "context": "The networks are trained with backpropagation and this minimax optimization becomes possible via the use of a gradient reversal layer (Ganin and Lempitsky, 2015).", "startOffset": 134, "endOffset": 161}, {"referenceID": 3, "context": "These datasets are collected based on the raw data 1 provided by (Blitzer et al., 2007).", "startOffset": 65, "endOffset": 87}, {"referenceID": 24, "context": "The IMDB dataset4 consists of movie reviews with binary classes (Maas et al., 2011).", "startOffset": 64, "endOffset": 83}, {"referenceID": 26, "context": "The MR dataset also consists of movie reviews from rotten tomato website with two classes 5(Pang and Lee, 2005).", "startOffset": 91, "endOffset": 111}, {"referenceID": 6, "context": "\u2022 MT-CNN: This model is proposed by Collobert and Weston (2008) with convolutional layer, in which lookup-tables are shared partially while other layers are task-specific.", "startOffset": 36, "endOffset": 64}, {"referenceID": 3, "context": "edu/ \u0303mdredze/ datasets/sentiment/ Blitzer et al. (2007) also provides two extra processed datasets with the format of Bag-of-Words, which are not proper for neural-based models.", "startOffset": 35, "endOffset": 57}, {"referenceID": 18, "context": "\u2022 MT-DNN: The model is proposed by Liu et al. (2015b) with bag-of-words input and multi-layer perceptrons, in which a hidden layer is shared.", "startOffset": 35, "endOffset": 54}, {"referenceID": 28, "context": "The word embeddings for all of the models are initialized with the 200d GloVe vectors ((Pennington et al., 2014)).", "startOffset": 87, "endOffset": 112}, {"referenceID": 6, "context": "Neural networks based multi-task learning has been proven effective in many NLP problems (Collobert and Weston, 2008; Glorot et al., 2011).", "startOffset": 89, "endOffset": 138}, {"referenceID": 10, "context": "Neural networks based multi-task learning has been proven effective in many NLP problems (Collobert and Weston, 2008; Glorot et al., 2011).", "startOffset": 89, "endOffset": 138}, {"referenceID": 0, "context": "Ajakan et al. (2014); Bousmalis et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Ajakan et al. (2014); Bousmalis et al. (2016) applied adverarial training to domain adaptation, aiming at transferring the knowledge of one source domain to target domain.", "startOffset": 0, "endOffset": 46}, {"referenceID": 0, "context": "Ajakan et al. (2014); Bousmalis et al. (2016) applied adverarial training to domain adaptation, aiming at transferring the knowledge of one source domain to target domain. Park and Im (2016) proposed a novel approach for multimodal representation learning which uses adversarial back-propagation concept.", "startOffset": 0, "endOffset": 191}], "year": 2017, "abstractText": "Neural network models have shown their promising opportunities for multi-task learning, which focus on learning the shared layers to extract the common and task-invariant features. However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks. In this paper, we propose an adversarial multi-task learning framework, alleviating the shared and private latent feature spaces from interfering with each other. We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach. Besides, we show that the shared knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks. The datasets of all 16 tasks are publicly available at http://nlp.fudan.", "creator": "LaTeX with hyperref package"}}}