{"id": "1704.05753", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection", "abstract": "Linguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures.", "histories": [["v1", "Wed, 19 Apr 2017 14:41:21 GMT  (91kb,A)", "http://arxiv.org/abs/1704.05753v1", "Accepted for publication at ACL 2017"], ["v2", "Thu, 19 Oct 2017 23:04:39 GMT  (91kb,A)", "http://arxiv.org/abs/1704.05753v2", "Published at ACL 2017"]], "COMMENTS": "Accepted for publication at ACL 2017", "reviews": [], "SUBJECTS": "cs.CL cs.HC", "authors": ["youxuan jiang", "jonathan k kummerfeld", "walter s lasecki"], "accepted": true, "id": "1704.05753"}, "pdf": {"name": "1704.05753.pdf", "metadata": {"source": "META", "title": "Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection", "authors": ["Youxuan Jiang", "Jonathan K. Kummerfeld"], "emails": ["lyjiang@umich.edu", "jkummerf@umich.edu", "wlasecki@umich.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.05 753v 1 [cs.C L] 19 Apr 201 7Linguistically diverse datasets are critical for the training and evaluation of robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing in capturing paraphrases. We look at variations in instructions, incentives, data areas, and workflows. Colloquial paraphrases are comprehensively analyzed for accuracy, grammaticality, and linguistic diversity. Our observations provide new insights into the trade-offs between accuracy and diversity of responses in the set that arise as a result of the task, and provide guidance for future methods of generating paraphrases."}, {"heading": "1 Introduction", "text": "Paraphrases are useful for a number of tasks, including the evaluation of machine translation (Kauchak and Barzilay, 2006), semantic paraphrases (Wang et al., 2015) and answering questions (Fader et al., 2013). Crowdsourcing has been widely used as a scalable and cost-effective means of generating paraphrases (Negri et al., 2012; Wang et al., 2012; Tschirsich and Hintz, 2013), but there has been limited analysis of the factors influencing diversity and correctness of workers \"paraphrases. In this paper, we are conducting a systematic examination of crowdsourcing paraphrase design decisions, including initial research into worker incentives to rewrite. For worker incentives, we offer bonuses either if a paraphrase is novel (encouraging diversity) or if it coincides with another worker's paraphrase (promoting overcorrectness)."}, {"heading": "2 Related Work", "text": "Previous work on generating crowdsourcing paraphrases falls into two categories: work on modifying the creation process or workflow, and studies on the effects of prompting or priming on the output of crowdsourcing employees. Beyond crowd-sourcing, other work has explored the use of experts or automated systems to generate paraphrases."}, {"heading": "2.1 Workflows for Crowd-Paraphrasing", "text": "The most common approach to crowdsourcing paraphrase generation is to provide a sentence as a prompt and request a single paraphrase of an employee. A common addition is to ask another group of workers to evaluate whether a paraphrase generated is correct (Buzek et al., 2010; Burrows et al., 2013). Negri et al. (2012) also researched an alternative workflow in which each worker writes two paraphrases, which are then given to other workers as a prompt sentence and form a binary tree of paraphrases. They found that paraphrases were more diverse deeper in the tree, but understanding how correctness and grammar vary in such a tree remains an open question. Near real-time crowdsourcing (Bigham et al., 2010) allowed Lasecki et al. (2013a) to elicit deviations from whole conversations by providing a setting and a target for pairs of crowdsourcing workers."}, {"heading": "2.2 The Effects of Priming", "text": "When crowdworkers perform a task, they are prepared (influenced) by the examples, instructions, and context they see. This preparatory work can lead to systematic variations in the resulting paraphrases. Mitchell et al. (2014) showed that providing context in the form of earlier statements from a dialogue only offers benefits when four or more are included. Kumaran et al. (2014) provided drawings as instructions, receiving different paraphrases, but without exact semantic equivalence. When each sentence expresses a small number of predicates, Wang et al. (2012) found that providing the predicates resulted in a slightly faster paraphrase than providing a full sentence or short sentence for each predicate. We expand on this work by examining how the nature of the examples shown affects paraphrasing."}, {"heading": "2.3 Expert and Automated Generation", "text": "Finally, there are two general lines of research on paraphrasing that do not focus on the use of crowds: the first is the automatic collection of paraphrases from parallel data sources, such as translations of the same text or captions for the same image (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Bouamor et al., 2012; Pavlick et al., 2015). These resources are extremely extensive, but usually do not offer (1) the strong semantic equivalence we are interested in, and (2) focus on phrases rather than complete sentences. The second line of work examines the generation of grids that encode hundreds of thousands of paraphrases compactly (Dreyer and Marcu, 2012; Bojar et al., 2013). Unfortunately, these grids tend to be expensive to produce, with experts taking one to three hours per sentence."}, {"heading": "3 Experimental Design", "text": "We conducted a series of experiments to examine factors in the creation of crowdsourced paraphrases. To do this in a controlled manner, we investigated a single variation per condition."}, {"heading": "3.1 Definition of Valid Paraphrases", "text": "For example: Prompt: Which higher classes are four credits? Are there four higher credits? We considered the two questions above to be paraphrases, since they are both explicit and implicit requirements for a list of classes, although the second is a polar question and the first is not. However: Prompt: Which is simpler from EECS 378 and EECS 280? Is EECS 378 simpler than EECS 280? We did not consider the two above questions to be paraphrases, since the first asks for one of two class options and the second a yes or no."}, {"heading": "3.2 Baseline", "text": "In order to avoid confusion or training effects between the different conditions, we allowed the workers to participate in all conditions only once. Initial instructions shown to the workers were the same under all conditions (deviations occurred only after one worker accepted the task), the workers were paid 5 cents per copy they had written, plus, when all the workers were ready, a 5 cent bonus for copies that matched the copy of another worker in the same condition. Although we do not actually want duplicate copies, this incentive can encourage the workers to follow the instructions more closely and produce grammatical and correct sentences."}, {"heading": "3.3 Conditions", "text": "We showed either: no examples (no examples), two examples with lexical changes only (lexical examples), one example with lexical changes and one with syntactic changes (mixed examples) 1993, or two examples each containing both lexical and syntactical changes (baseline).The discrepancies between these conditions can be interpreted differently by workers, leading them to generate different paraphrases.Incentive The 5 cent bonus per paraphrase was either not included (no bonus), awarded for each sentence that was a duplicate at the end of the task (baseline), or awarded for each sentence that does not match the paraphrase of another worker (Novelty Bonus). Bonuses that depend on other workers, we can either promote creativity or conformity."}, {"heading": "3.4 Metrics", "text": "Semantic equivalence In order for a paraphrase to be valid, its meaning must match the original sentence. To evaluate this paraphrase, two of the authors - a native speaker and a non-native but fluent speaker - evaluated each sentence independently of each other and then discussed each case of disagreement in order to establish a consensus judgment. Prior to the consensus step, we used a kappa test to measure significance, as it is a binary classification process. Grammar We also assessed whether the sentences were grammatical, again with two commentators evaluating each sentence and resolving discrepancies. As it was a binary classification, we used a kappa-2 test for signification, as it is a test of signification. The time it takes to write paraphrases is important to estimate the time to completion, and to ensure that workers receive a fair payment."}, {"heading": "4 Results", "text": "We collected 2600 paraphrases: 10 paraphrases per set, for 20 sets, for each of the 13 conditions. The cost, including initial tests, was $196.30, including $20.30 for bonuses. Table 1 shows the results for all metrics."}, {"heading": "4.1 Discussion: Task Variation", "text": "rE \"s tis rf\u00fc ide rf\u00fc rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rtef\u00fc the rtef\u00fc the rf\u00fc the rtef\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rtef\u00fc the rtef\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rtef\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rtef\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc"}, {"heading": "4.2 Discussion: Domains", "text": "The only significant change in correctness at UBUNTU is probably due to the widespread use of jargon in the dataset, for example: prompt: ok, what does Journalctl show? As far as grammar is concerned, GEOQUERY is particularly low; frequent errors have been confusion between singular / plural and has / have. WSJ is the domain with the most fluctuations, it has on average significantly longer sentences, which explains the higher amount of time spent. This could also explain the lower distinctness and the PINC value, as workers would often retain large portions of the sentence, sometimes rearranged, but otherwise unchanged."}, {"heading": "5 Conclusion", "text": "While previous work has used crowdsourcing to generate paraphrases, we are conducting the first systematic study of factors influencing the process. We find that the biggest discrepancies are caused by priming effects: the use of simpler examples leads to less diversity but more frequent semantic equivalence. Meanwhile, encouraging workers to use paraphrases from other workers (rather than reusing the original prompt) increases diversity. Our results provide clear guidelines for future paraphrase generation and support the creation of larger, more diverse future datasets."}, {"heading": "6 Acknowledgements", "text": "We thank the members of the UMich / IBM Sapphire Project, as well as all study participants and anonymous reviewers, for their helpful suggestions on this work. This material is partly based on work supported by IBM under contract 4915012629. Any opinions, findings, conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of IBM."}], "references": [{"title": "Practical statistics for medical research", "author": ["Douglas G Altman"], "venue": "CRC press. https://www.crcpress.com/Practical-Statistics-for-Medical-Research/Altm", "citeRegEx": "Altman.,? \\Q1990\\E", "shortCiteRegEx": "Altman.", "year": 1990}, {"title": "Vizwiz: nearly real-time answers to visual questions", "author": ["Jeffrey P Bigham", "Chandrika Jayant", "Hanjie Ji", "Greg Little", "Andrew Miller", "Robert C Miller", "Robin Miller", "Aubrey Tatarowicz", "Brandyn White", "Samual White"], "venue": null, "citeRegEx": "Bigham et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bigham et al\\.", "year": 2010}, {"title": "Scratching the surface of possible translations", "author": ["Ond\u0159ej Bojar", "Matou\u0161 Mach\u00e1\u010dek", "Ale\u0161 Tamchyna", "Daniel Zeman"], "venue": null, "citeRegEx": "Bojar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2013}, {"title": "A contrastive review of paraphrase acquisition techniques", "author": ["Houda Bouamor", "Aurlien Max", "Gabriel Illouz", "Anne Vilnat"], "venue": null, "citeRegEx": "Bouamor et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bouamor et al\\.", "year": 2012}, {"title": "Paraphrase acquisition via crowdsourcing", "author": ["Steven Burrows", "Martin Potthast", "Benno Stein"], "venue": null, "citeRegEx": "Burrows et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Burrows et al\\.", "year": 2013}, {"title": "Error driven paraphrase annotation using mechanical turk", "author": ["Olivia Buzek", "Philip Resnik", "Ben Bederson"], "venue": "In Proc. of the NAACL", "citeRegEx": "Buzek et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Buzek et al\\.", "year": 2010}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["David Chen", "William Dolan"], "venue": "In Proc. of the 49th Annual Meet-", "citeRegEx": "Chen and Dolan.,? \\Q2011\\E", "shortCiteRegEx": "Chen and Dolan.", "year": 2011}, {"title": "Collecting paraphrase corpora from volunteer contributors", "author": ["Timothy Chklovski"], "venue": null, "citeRegEx": "Chklovski.,? \\Q2005\\E", "shortCiteRegEx": "Chklovski.", "year": 2005}, {"title": "Tree adjoining grammar and the reluctant paraphrasing of text", "author": ["Mark Dras"], "venue": "Ph.D. thesis, Macquarie University NSW 2109 Australia. http://web.science.mq.edu.au/madras/papers/thesis.pdf", "citeRegEx": "Dras.,? \\Q1999\\E", "shortCiteRegEx": "Dras.", "year": 1999}, {"title": "HyTER: Meaning-equivalent semantics for translation evaluation", "author": ["Markus Dreyer", "Daniel Marcu"], "venue": "In Proc. of the 2012 Conference of the North", "citeRegEx": "Dreyer and Marcu.,? \\Q2012\\E", "shortCiteRegEx": "Dreyer and Marcu.", "year": 2012}, {"title": "Paraphrase-driven learning for open question answering", "author": ["Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni"], "venue": "In Proc. of the 51st", "citeRegEx": "Fader et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "PPDB: The paraphrase database", "author": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch"], "venue": "In Proc. of the", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "A simple sequentially rejective multiple test procedure", "author": ["Sture Holm"], "venue": "Scandinavian Journal of Statistics", "citeRegEx": "Holm.,? \\Q1979\\E", "shortCiteRegEx": "Holm.", "year": 1979}, {"title": "Paraphrasing for automatic evaluation", "author": ["David Kauchak", "Regina Barzilay"], "venue": null, "citeRegEx": "Kauchak and Barzilay.,? \\Q2006\\E", "shortCiteRegEx": "Kauchak and Barzilay.", "year": 2006}, {"title": "Online gaming for crowd-sourcing phrase-equivalents", "author": ["A Kumaran", "Melissa Densmore", "Shaishav Kumar"], "venue": "In Proc. of COL-", "citeRegEx": "Kumaran et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kumaran et al\\.", "year": 2014}, {"title": "Conversations in the crowd: Collecting data for task-oriented dialog learning. In Scaling Speech, Language Understanding and Dialogue through Crowdsourcing Workshop at the First AAAI Con", "author": ["Walter S. Lasecki", "Ece Kamar", "Dan Bohus"], "venue": null, "citeRegEx": "Lasecki et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lasecki et al\\.", "year": 2013}, {"title": "Real\u2013 time crowd control of existing interfaces", "author": ["Walter S Lasecki", "Kyle I Murray", "Samuel White", "Robert C Miller", "Jeffrey P Bigham"], "venue": "In Proc. of the 24th annual ACM symposium on User interface software and technology", "citeRegEx": "Lasecki et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lasecki et al\\.", "year": 2011}, {"title": "Chorus: a crowd-powered conversational assistant", "author": ["Walter S Lasecki", "Rachel Wesley", "Jeffrey Nichols", "Anand Kulkarni", "James F Allen", "Jeffrey P Bigham"], "venue": "In Proc. of the 26th annual ACM symposium on User interface soft-", "citeRegEx": "Lasecki et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lasecki et al\\.", "year": 2013}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz"], "venue": "Computational Linguistics", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Financial incentives and the performance of crowds", "author": ["Winter Mason", "Duncan J Watts"], "venue": "ACM SigKDD Explorations Newsletter", "citeRegEx": "Mason and Watts.,? \\Q2010\\E", "shortCiteRegEx": "Mason and Watts.", "year": 2010}, {"title": "Crowdsourcing language generation templates for dialogue systems", "author": ["Margaret Mitchell", "Dan Bohus", "Ece Kamar"], "venue": "In Proc. of the INLG and SIGDIAL 2014 Joint Session. http://www.aclweb.org/anthology/W14-5003", "citeRegEx": "Mitchell et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2014}, {"title": "Chinese whispers: Cooperative paraphrase acquisition", "author": ["Matteo Negri", "Yashar Mehdad", "Alessandro Marchetti", "Danilo Giampiccolo", "Luisa Bentivogli"], "venue": "In Proc. of the Eighth International Conference on Lan-", "citeRegEx": "Negri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Negri et al\\.", "year": 2012}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification", "author": ["Ellie Pavlick", "Pushpendre Rastogi", "Juri Ganitkevich"], "venue": null, "citeRegEx": "Pavlick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pavlick et al\\.", "year": 2015}, {"title": "Using multiple clause constructors in inductive logic programming for semantic parsing", "author": ["Lappoon R. Tang", "Raymond J. Mooney"], "venue": "In Proc. of the 12th European Conference on Machine Learning", "citeRegEx": "Tang and Mooney.,? \\Q2001\\E", "shortCiteRegEx": "Tang and Mooney.", "year": 2001}, {"title": "Leveraging crowdsourcing for paraphrase recognition", "author": ["Martin Tschirsich", "Gerold Hintz"], "venue": "In Proc. of the 7th Linguistic Annotation Workshop and Interoperability with Discourse", "citeRegEx": "Tschirsich and Hintz.,? \\Q2013\\E", "shortCiteRegEx": "Tschirsich and Hintz.", "year": 2013}, {"title": "Crowdsourcing the acquisition of natural language corpora: Methods and observations", "author": ["W.Y.Wang", "D. Bohus", "E. Kamar", "E. Horvitz"], "venue": "IEEE Spoken Language Technology Workshop (SLT). http://ieeexplore.ieee.org/document/6424200/", "citeRegEx": "Y.Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Y.Wang et al\\.", "year": 2012}, {"title": "Building a semantic parser overnight", "author": ["Yushi Wang", "Jonathan Berant", "Percy Liang"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": "Paraphrases are useful for a range of tasks, including machine translation evaluation (Kauchak and Barzilay, 2006), semantic parsing (Wang et al.", "startOffset": 86, "endOffset": 114}, {"referenceID": 27, "context": "Paraphrases are useful for a range of tasks, including machine translation evaluation (Kauchak and Barzilay, 2006), semantic parsing (Wang et al., 2015), and question answering (Fader et al.", "startOffset": 133, "endOffset": 152}, {"referenceID": 10, "context": ", 2015), and question answering (Fader et al., 2013).", "startOffset": 32, "endOffset": 52}, {"referenceID": 21, "context": "Crowdsourcing has been widely used as a scalable and cost-effective means of generating paraphrases (Negri et al., 2012; Wang et al., 2012; Tschirsich and Hintz, 2013), but there has been", "startOffset": 100, "endOffset": 167}, {"referenceID": 25, "context": "Crowdsourcing has been widely used as a scalable and cost-effective means of generating paraphrases (Negri et al., 2012; Wang et al., 2012; Tschirsich and Hintz, 2013), but there has been", "startOffset": 100, "endOffset": 167}, {"referenceID": 5, "context": "One frequent addition is to ask a different set of workers to evaluate whether a generated paraphrase is correct (Buzek et al., 2010; Burrows et al., 2013).", "startOffset": 113, "endOffset": 155}, {"referenceID": 4, "context": "One frequent addition is to ask a different set of workers to evaluate whether a generated paraphrase is correct (Buzek et al., 2010; Burrows et al., 2013).", "startOffset": 113, "endOffset": 155}, {"referenceID": 4, "context": ", 2010; Burrows et al., 2013). Negri et al. (2012) also explored an alternate workflow in which each worker writes", "startOffset": 8, "endOffset": 51}, {"referenceID": 1, "context": "Near real-time crowdsourcing (Bigham et al., 2010) allowed Lasecki et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 16, "context": "Continuous real-time crowdsourcing (Lasecki et al., 2011) allows Chorus Lasecki et al.", "startOffset": 35, "endOffset": 57}, {"referenceID": 1, "context": "Near real-time crowdsourcing (Bigham et al., 2010) allowed Lasecki et al. (2013a) to elicit variations on entire conversations by providing a setting and goal to pairs of crowd workers.", "startOffset": 30, "endOffset": 82}, {"referenceID": 1, "context": "Near real-time crowdsourcing (Bigham et al., 2010) allowed Lasecki et al. (2013a) to elicit variations on entire conversations by providing a setting and goal to pairs of crowd workers. Continuous real-time crowdsourcing (Lasecki et al., 2011) allows Chorus Lasecki et al. (2013b) users to hold conversations with groups of crowd workers as if the crowd was a single individual, allowing for the collection of example conversations in more realistic settings.", "startOffset": 30, "endOffset": 281}, {"referenceID": 1, "context": "Near real-time crowdsourcing (Bigham et al., 2010) allowed Lasecki et al. (2013a) to elicit variations on entire conversations by providing a setting and goal to pairs of crowd workers. Continuous real-time crowdsourcing (Lasecki et al., 2011) allows Chorus Lasecki et al. (2013b) users to hold conversations with groups of crowd workers as if the crowd was a single individual, allowing for the collection of example conversations in more realistic settings. The only prior work regarding incentives we are aware of is by Chklovski (2005), who collected paraphrases in a game where the goal was to match an existing paraphrase, with extra points awarded for doing so with fewer hints.", "startOffset": 30, "endOffset": 540}, {"referenceID": 19, "context": "Mitchell et al. (2014) showed that providing context, in the form of previous utterances from a dialogue, only provides benefits once four or more are included.", "startOffset": 0, "endOffset": 23}, {"referenceID": 14, "context": "Kumaran et al. (2014) provided drawings as prompts, obtaining diverse paraphrases, but without exact semantic equivalence.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": "Kumaran et al. (2014) provided drawings as prompts, obtaining diverse paraphrases, but without exact semantic equivalence. When each sentence expresses a small set of slot-filler predicates, Wang et al. (2012) found that providing the list of predicates led to slightly faster paraphrasing than giving either a complete sentence or a short sentence for each predicate.", "startOffset": 0, "endOffset": 210}, {"referenceID": 9, "context": "The second line of work explores the creation of lattices that compactly encode hundreds of thousands of paraphrases (Dreyer and Marcu, 2012; Bojar et al., 2013).", "startOffset": 117, "endOffset": 161}, {"referenceID": 2, "context": "The second line of work explores the creation of lattices that compactly encode hundreds of thousands of paraphrases (Dreyer and Marcu, 2012; Bojar et al., 2013).", "startOffset": 117, "endOffset": 161}, {"referenceID": 8, "context": "\u201da pair of units of text deemed to be interchangeable\u201d (Dras, 1999).", "startOffset": 55, "endOffset": 67}, {"referenceID": 19, "context": "level of payment because prior work has found that workers work quality is not increased by increased financial incentives due to an anchoring effect relative to the base rate we define (Mason and Watts, 2010).", "startOffset": 186, "endOffset": 209}, {"referenceID": 0, "context": "36 for grammaticality (fair agreement) (Altman, 1990).", "startOffset": 39, "endOffset": 53}, {"referenceID": 22, "context": "1 We also considered BLEU (Papineni et al., 2002), which measures n-gram overlap and is used as a proxy for correctness in MT.", "startOffset": 26, "endOffset": 49}, {"referenceID": 12, "context": "01 level, both after applying the HolmBonferroni method across each row (Holm, 1979).", "startOffset": 72, "endOffset": 84}, {"referenceID": 27, "context": "Wang et al. (2015) report \u223c28 sec/paraphrase.", "startOffset": 0, "endOffset": 19}], "year": 2017, "abstractText": "Linguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. Wemanually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures.", "creator": "LaTeX with hyperref package"}}}