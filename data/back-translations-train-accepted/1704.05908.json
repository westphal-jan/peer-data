{"id": "1704.05908", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "An Interpretable Knowledge Transfer Model for Knowledge Base Completion", "abstract": "Knowledge bases are important resources for a variety of natural language processing tasks but suffer from incompleteness. We propose a novel embedding model, \\emph{ITransF}, to perform knowledge base completion. Equipped with a sparse attention mechanism, ITransF discovers hidden concepts of relations and transfer statistical strength through the sharing of concepts. Moreover, the learned associations between relations and concepts, which are represented by sparse attention vectors, can be interpreted easily. We evaluate ITransF on two benchmark datasets---WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information.", "histories": [["v1", "Wed, 19 Apr 2017 19:35:54 GMT  (869kb,D)", "https://arxiv.org/abs/1704.05908v1", "ACL 2017"], ["v2", "Wed, 3 May 2017 05:20:09 GMT  (1737kb,D)", "http://arxiv.org/abs/1704.05908v2", "Accepted by ACL 2017. Minor update"]], "COMMENTS": "ACL 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["qizhe xie", "xuezhe ma", "zihang dai", "eduard h hovy"], "accepted": true, "id": "1704.05908"}, "pdf": {"name": "1704.05908.pdf", "metadata": {"source": "CRF", "title": "An Interpretable Knowledge Transfer Model for Knowledge Base Completion", "authors": ["Qizhe Xie", "Xuezhe Ma", "Zihang Dai", "Eduard Hovy"], "emails": ["hovy}@cs.cmu.edu", "Hits@10"], "sections": [{"heading": "1 Introduction", "text": "The basic motivation behind these studies is that there are some statistical regulations among the intertwined facts in the multirelational knowledge base, through the discovery of generatable regulabilities in known facts that can be faithfully recovered for excellent representation."}, {"heading": "2 Notation and Previous Models", "text": "Let E denote the set of entities, and R the set of relationships. In completing the knowledge base, given a training set P of triples (h, r, t) where h, t, E are the head and tail entities that have a relationship r, R, e.g. (Steve Jobs, FounderOf, Apple), we want to predict missing facts such as (Steve Jobs, Profession, Businessman). Most of the embedding models for completing the knowledge base define an energy function fr (h, t) according to the plausibility of the fact (Bordes et al., 2011, 2014, 2013; Socher et al., 2013; Wang et al., 2014; Yang et al., 2015; Guu et al., 2015; Nguyen et al., 2016b). The models are learned to generate energy fr (h, t) from a plausible tripel (h, r, t) of a triple (h, mize, fr) and maxit."}, {"heading": "3 Interpretable Knowledge Transfer", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Model", "text": "As discussed above, a fundamental weakness in TransR and STransE is then reduced to a smaller concept."}, {"heading": "3.2 Block Iterative Optimization", "text": "Although frugality is advantageous in practice, it is generally difficult to find the optimal solution under \"0 constraints.\" Therefore, we resort to an approximate algorithm in this way of working. For convenience, we refer to the parameters with and without the sparse constraints, such as essentially avoiding the sparse partition and the dense partition. Based on this idea, we will consider the high-level idea of the approximate algorithm to optimize one of the two partitions iteratively, while we can keep the other fixed. Since all parameters in the dense partition, including the embedding, the projection matrices and the pre-softmax results are fully differentiated with the sparse partition, we can simply use the SGD to optimize the dense partition. Then, the core difficulty lies in the step of optimizing the sparse partition (i.e."}, {"heading": "3.3 Corrupted Sample Generating Method", "text": "Remember that we have to scan a negative triple (h, r, t) to calculate the hinged loss shown in Eq.2, since we have a positive triple (h, r, t). Negative triple distribution is denoted by N (h, r, t). Previous work (Bordes et al., 2013; Lin et al., 2015b; Yang et al., 2015; Nguyen et al., 2016b) generally construct a series of corrupted triples by replacing the head or tail entity with a random entity uniformly sampled from the KB. However, the uniform sampling of corrupted entities may not be optimal. Often, head and tail entities that have a relationship assigned can only belong to one particular domain. If the corrupted entity comes from other domains, it is very easy for the model to give a large energy gap between true tripled 1. Since the energy gap is not higher for this corrupted one, the energy gap is higher for this triple."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Setup", "text": "To evaluate the link prediction, we conduct experiments with the WN18 (WordNet) and FB15k (Freebase) introduced by Bordes et al. (2013) and use the same training / validation / test split as in (Bordes et al., 2013). Information on the two data sets is in Table 1. In the task of completing the knowledge base, we evaluate the performance of the model in predicting the head or tail entity, taking into account the relationship and the other entity. For example, to predict the head relation r and tail t in triple (h, r, t), we calculate the energy function for (h, t) for each entity h \u00b2 in the knowledge base and classify all entities according to the energy. We follow Bordes et al. (2013) to report the filter results, i.e. remove all other correct candidates h \u00b2 in the order of precedence. The rank of the correct entity is then determined by the Entity 10 and we report the mean @ (10) and the mean (10)."}, {"heading": "4.2 Implementation Details", "text": "We initialize the projection matrices with identity matrices added with a small scan from the normal distribution N (0, 0.0052); the unit and relation vectors of ITransF are initialized by TransE (Bordes et al., 2013), following Lin et al. (2015b); Ji et al. (2015); Garc\u0131 \"a-Dura\" n et al. (2016, 2015); Lin et al. (2015a). We performed minibatch SGD until convergence. We use the \"Bernoulli\" sampling method to generate faulty triples as in Wang et al. (2014), Lin et al. (2015b), He et al. (2015), Ji et al. (2015) and Lin et al. (2015a). STransE (Nguyen et al., 2016b) is the most similar knowledge embedding for our models."}, {"heading": "4.3 Results & Analysis", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "5 Analysis on Sparseness", "text": "It is desirable because it contributes to the interpretation and computational power of our model. We examine whether enforcing thrift would impair model performance and compare our method with other sparse coding methods in this sector. We show that our model can achieve similar results with scant attention and a significantly lower computational burden."}, {"heading": "6 Related Work", "text": "In KBC, CTransR (Lin et al., 2015b) enables the embedding of relationships between similar relationships, but they bundle relationships before training, rather than learning them in principle. Furthermore, they do not solve the problem of data sparseness because there is no sharing of projection matrices, which have many more parameters. Learning the association between semantic relationships has been used in related problems such as measuring relational similarity (Turney, 2012) and adapting relationships (Bollegala et al., 2015). Data sparseness is a common problem in many areas. Transfer learning (Pan and Yang, 2010) has proven to be promising, Knowl-4We use the toolkit of (Faruqui et al., 2015).edge and statistical strengths in similar models or languages. For example, Bharadwaj et al. (2016) is the transfer of models to resource-rich languages by parameter division through common phological features."}, {"heading": "7 Conclusion and Future Work", "text": "In summary, we propose a knowledge embedding model that can detect common hidden concepts and design a learning algorithm to induce the interpretable sparse representation. Empirically, we show that our model enables the performance of two benchmark datasets without external resources compared to all previous models of the same kind. In the future, we plan to enable ITransF to perform multi-level inference, and to extend the sharing mechanism to entity and relationship embedding, thereby further improving statistical binding between parameters. In addition, our framework can also be applied to multi-task learning, thereby fostering a better allocation between different tasks."}, {"heading": "Acknowledgments", "text": "We thank anonymous reviewers and Graham Neubig for valuable comments. We thank Yulun Du, Paul Mitchell, Abhilasha Ravichander, Pengcheng Yin and Chunting Zhou for suggestions on the draft. We also appreciate the great work environment provided by the staff at LTI. This research was partially supported by DARPA grant FA8750-12-2-0342, which was funded under the DEFT program."}, {"heading": "A Appendix", "text": "A.1 Domain Sampling Probability In this section, we define the probability pr to generate a negative sample from the same domain mentioned in Section 3.3. The probability cannot be too high to avoid generating negative samples that are actually correct, since in the sample generally a lot of facts are missing. Nr = [h, r, t) is the induced number of edges with the probability r. We define the probability pr aspr = min (MTr | Nr | 0.5) (4) Our motivation for such a formulation is as follows: Assumption Or is the amount that contains all truthful facts with the probability r, i.e. all approximate relationships and all other missing ones."}], "references": [{"title": "Semantic parsing on Freebase from question-answer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio."], "venue": "Proceedings of the Python", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Phonologically aware neural model for named entity recognition in low resource transfer settings", "author": ["Akash Bharadwaj", "David Mortensen", "Chris Dyer", "Jaime Carbonell."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Bharadwaj et al\\.,? 2016", "shortCiteRegEx": "Bharadwaj et al\\.", "year": 2016}, {"title": "Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Proceedings of the 2008 ACM SIGMOD International Conference on Man-", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Embedding semantic relations into word representations", "author": ["Danushka Bollegala", "Takanori Maehara", "Ken-ichi Kawarabayashi."], "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence.", "citeRegEx": "Bollegala et al\\.,? 2015", "shortCiteRegEx": "Bollegala et al\\.", "year": 2015}, {"title": "A Semantic Matching Energy Function for Learning with Multi-relational Data", "author": ["Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio."], "venue": "Machine Learning 94(2):233\u2013259.", "citeRegEx": "Bordes et al\\.,? 2014", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Translating Embeddings for Modeling Multirelational Data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto GarciaDuran", "Jason Weston", "Oksana Yakhnenko."], "venue": "Advances in Neural Information Processing Systems 26, pages 2787\u20132795.", "citeRegEx": "Bordes et al\\.,? 2013", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Learning Structured Embeddings of Knowledge Bases", "author": ["Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio."], "venue": "Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence. pages 301\u2013306.", "citeRegEx": "Bordes et al\\.,? 2011", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Cfo: Conditional focused neural question answering with largescale knowledge bases", "author": ["Zihang Dai", "Lei Li", "Wei Xu."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Asso-", "citeRegEx": "Dai et al\\.,? 2016", "shortCiteRegEx": "Dai et al\\.", "year": 2016}, {"title": "Sparse overcomplete word vector representations", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Dani Yogatama", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "WordNet: An Electronic Lexical Database", "author": ["Christiane D. Fellbaum."], "venue": "MIT Press.", "citeRegEx": "Fellbaum.,? 1998", "shortCiteRegEx": "Fellbaum.", "year": 1998}, {"title": "Composing Relationships with Translations", "author": ["Alberto Garc\u0131\u0301a-Dur\u00e1n", "Antoine Bordes", "Nicolas Usunier"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.", "year": 2015}, {"title": "Combining Two and Three-Way Embedding Models for Link Prediction in Knowledge Bases", "author": ["Alberto Garc\u0131\u0301a-Dur\u00e1n", "Antoine Bordes", "Nicolas Usunier", "Yves Grandvalet"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.", "year": 2016}, {"title": "Traversing Knowledge Graphs in Vector Space", "author": ["Kelvin Guu", "John Miller", "Percy Liang."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 318\u2013327.", "citeRegEx": "Guu et al\\.,? 2015", "shortCiteRegEx": "Guu et al\\.", "year": 2015}, {"title": "Learning to Represent Knowledge Graphs with Gaussian Embedding", "author": ["Shizhu He", "Kang Liu", "Guoliang Ji", "Jun Zhao."], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. pages 623\u2013632.", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Knowledge Graph Embedding via Dynamic Mapping Matrix", "author": ["Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Ji et al\\.,? 2015", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "DBpedia - A Large-scale, Multilingual Knowledge Base", "author": ["Jens Lehmann", "Robert Isele", "Max Jakob", "Anja Jentzsch", "Dimitris Kontokostas", "Pablo N. Mendes", "Sebastian Hellmann", "Mohamed Morsey", "Patrick van Kleef", "S\u00f6ren Auer", "Christian Bizer"], "venue": null, "citeRegEx": "Lehmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lehmann et al\\.", "year": 2015}, {"title": "LightRNN: Memory and Computation-Efficient Recurrent Neural Networks", "author": ["Xiang Li", "Tao Qin", "Jian Yang", "Tieyan Liu."], "venue": "Advances in Neural Information Processing Systems 29.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Modeling Relation Paths for Representation Learning of Knowledge Bases", "author": ["Yankai Lin", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun", "Siwei Rao", "Song Liu."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language", "citeRegEx": "Lin et al\\.,? 2015a", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Learning Entity and Relation Embeddings for Knowledge Graph Completion", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu."], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence Learning, pages", "citeRegEx": "Lin et al\\.,? 2015b", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "K-sparse autoencoders", "author": ["Alireza Makhzani", "Brendan Frey."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Makhzani and Frey.,? 2014", "shortCiteRegEx": "Makhzani and Frey.", "year": 2014}, {"title": "From softmax to sparsemax: A sparse model of attention and multi-label classification", "author": ["Andr\u00e9 FT Martins", "Ram\u00f3n Fernandez Astudillo."], "venue": "Proceedings of the 33th International Conference on Machine Learning.", "citeRegEx": "Martins and Astudillo.,? 2016", "shortCiteRegEx": "Martins and Astudillo.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Daniel Jurafsky."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Neighborhood mixture model for knowledge base completion", "author": ["Dat Quoc Nguyen", "Kairit Sirts", "Lizhen Qu", "Mark Johnson."], "venue": "Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL). Association for Com-", "citeRegEx": "Nguyen et al\\.,? 2016a", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "STransE: a novel embedding model of entities and relationships in knowledge bases", "author": ["Dat Quoc Nguyen", "Kairit Sirts", "Lizhen Qu", "Mark Johnson."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for", "citeRegEx": "Nguyen et al\\.,? 2016b", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "A Review of Relational Machine Learning for Knowledge Graphs", "author": ["Maximilian Nickel", "Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich."], "venue": "Proceedings of the IEEE, to appear .", "citeRegEx": "Nickel et al\\.,? 2015", "shortCiteRegEx": "Nickel et al\\.", "year": 2015}, {"title": "A Three-Way Model for Collective Learning on Multi-Relational Data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel."], "venue": "Proceedings of the 28th International Conference on Machine Learning. pages 809\u2013816.", "citeRegEx": "Nickel et al\\.,? 2011", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang."], "venue": "IEEE Transactions on knowledge and data engineering 22(10):1345\u20131359.", "citeRegEx": "Pan and Yang.,? 2010", "shortCiteRegEx": "Pan and Yang.", "year": 2010}, {"title": "Outrageously large neural networks", "author": ["Noam Shazeer", "Azalia Mirhoseini", "Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "venue": null, "citeRegEx": "Shazeer et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Shazeer et al\\.", "year": 2017}, {"title": "Implicit reasonet: Modeling large-scale structured relationships with shared memory", "author": ["Yelong Shen", "Po-Sen Huang", "Ming-Wei Chang", "Jianfeng Gao."], "venue": "arXiv preprint arXiv:1611.04642 .", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng."], "venue": "Advances in Neural Information Processing Systems 26, pages 926\u2013934.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "YAGO: A Core of Semantic Knowledge", "author": ["Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum."], "venue": "Proceedings of the 16th International Conference on World Wide Web. pages 697\u2013706.", "citeRegEx": "Suchanek et al\\.,? 2007", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani."], "venue": "Journal of the Royal Statistical Society. Series B (Methodological) pages 267\u2013288.", "citeRegEx": "Tibshirani.,? 1996", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Observed Versus Latent Features for Knowledge Base and Text Inference", "author": ["Kristina Toutanova", "Danqi Chen."], "venue": "Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality. pages 57\u201366.", "citeRegEx": "Toutanova and Chen.,? 2015", "shortCiteRegEx": "Toutanova and Chen.", "year": 2015}, {"title": "Representing Text for Joint Embedding of Text and Knowledge Bases", "author": ["Kristina Toutanova", "Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural", "citeRegEx": "Toutanova et al\\.,? 2015", "shortCiteRegEx": "Toutanova et al\\.", "year": 2015}, {"title": "Domain and function: A dualspace model of semantic relations and compositions", "author": ["Peter D Turney."], "venue": "Journal of Artificial Intelligence Research 44:533\u2013 585.", "citeRegEx": "Turney.,? 2012", "shortCiteRegEx": "Turney.", "year": 2012}, {"title": "Knowledge Graph Embedding by Translating on Hyperplanes", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen."], "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, pages 1112\u20131119.", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Mining inference formulas by goal-directed random walks", "author": ["Zhuoyu Wei", "Jun Zhao", "Kang Liu."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas,", "citeRegEx": "Wei et al\\.,? 2016", "shortCiteRegEx": "Wei et al\\.", "year": 2016}, {"title": "Knowledge Base Completion via Searchbased Question Answering", "author": ["Robert West", "Evgeniy Gabrilovich", "Kevin Murphy", "Shaohua Sun", "Rahul Gupta", "Dekang Lin."], "venue": "Proceedings of the 23rd International Conference on World Wide Web.", "citeRegEx": "West et al\\.,? 2014", "shortCiteRegEx": "West et al\\.", "year": 2014}, {"title": "Embedding Entities and Relations for Learning and Inference in Knowledge Bases", "author": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "author": ["Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Yih et al\\.,? 2015", "shortCiteRegEx": "Yih et al\\.", "year": 2015}, {"title": "Transfer learning for low-resource neural machine translation", "author": ["Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computa-", "citeRegEx": "Zoph et al\\.,? 2016", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": "Knowledge bases (KB), such as WordNet (Fellbaum, 1998), Freebase (Bollacker et al.", "startOffset": 38, "endOffset": 54}, {"referenceID": 3, "context": "Knowledge bases (KB), such as WordNet (Fellbaum, 1998), Freebase (Bollacker et al., 2008), YAGO (Suchanek et al.", "startOffset": 65, "endOffset": 89}, {"referenceID": 32, "context": ", 2008), YAGO (Suchanek et al., 2007) and DBpedia (Lehmann et al.", "startOffset": 14, "endOffset": 37}, {"referenceID": 16, "context": ", 2007) and DBpedia (Lehmann et al., 2015), are useful resources", "startOffset": 20, "endOffset": 42}, {"referenceID": 0, "context": "for many applications such as question answering (Berant et al., 2013; Yih et al., 2015; Dai et al., 2016) and information extraction (Mintz et al.", "startOffset": 49, "endOffset": 106}, {"referenceID": 41, "context": "for many applications such as question answering (Berant et al., 2013; Yih et al., 2015; Dai et al., 2016) and information extraction (Mintz et al.", "startOffset": 49, "endOffset": 106}, {"referenceID": 8, "context": "for many applications such as question answering (Berant et al., 2013; Yih et al., 2015; Dai et al., 2016) and information extraction (Mintz et al.", "startOffset": 49, "endOffset": 106}, {"referenceID": 23, "context": ", 2016) and information extraction (Mintz et al., 2009).", "startOffset": 35, "endOffset": 55}, {"referenceID": 31, "context": "sizes (Socher et al., 2013; West et al., 2014), leading to a number of studies on automatic knowledge base completion (KBC) (Nickel et al.", "startOffset": 6, "endOffset": 46}, {"referenceID": 39, "context": "sizes (Socher et al., 2013; West et al., 2014), leading to a number of studies on automatic knowledge base completion (KBC) (Nickel et al.", "startOffset": 6, "endOffset": 46}, {"referenceID": 26, "context": ", 2014), leading to a number of studies on automatic knowledge base completion (KBC) (Nickel et al., 2015) or link prediction.", "startOffset": 85, "endOffset": 106}, {"referenceID": 5, "context": "As a seminal work, Bordes et al. (2013) proposes the TransE, which models the statistical regularities with linear translations between entity embeddings operated by a relation embed-", "startOffset": 19, "endOffset": 40}, {"referenceID": 5, "context": "dependent space (Bordes et al., 2014; Ji et al., 2015; Lin et al., 2015b; Nguyen et al., 2016b), and then model the translation property in the projected space.", "startOffset": 16, "endOffset": 95}, {"referenceID": 15, "context": "dependent space (Bordes et al., 2014; Ji et al., 2015; Lin et al., 2015b; Nguyen et al., 2016b), and then model the translation property in the projected space.", "startOffset": 16, "endOffset": 95}, {"referenceID": 19, "context": "dependent space (Bordes et al., 2014; Ji et al., 2015; Lin et al., 2015b; Nguyen et al., 2016b), and then model the translation property in the projected space.", "startOffset": 16, "endOffset": 95}, {"referenceID": 25, "context": "dependent space (Bordes et al., 2014; Ji et al., 2015; Lin et al., 2015b; Nguyen et al., 2016b), and then model the translation property in the projected space.", "startOffset": 16, "endOffset": 95}, {"referenceID": 25, "context": "For instance, STransE (Nguyen et al., 2016b) utilizes two projection matrices per relation, one for the head entity and the other for the tail entity.", "startOffset": 22, "endOffset": 44}, {"referenceID": 35, "context": "Previously, a line of research makes use of external information such as textual relations from web-scale corpus or node features (Toutanova et al., 2015; Toutanova and Chen, 2015; Nguyen et al., 2016a), alleviating the sparsity problem.", "startOffset": 130, "endOffset": 202}, {"referenceID": 34, "context": "Previously, a line of research makes use of external information such as textual relations from web-scale corpus or node features (Toutanova et al., 2015; Toutanova and Chen, 2015; Nguyen et al., 2016a), alleviating the sparsity problem.", "startOffset": 130, "endOffset": 202}, {"referenceID": 24, "context": "Previously, a line of research makes use of external information such as textual relations from web-scale corpus or node features (Toutanova et al., 2015; Toutanova and Chen, 2015; Nguyen et al., 2016a), alleviating the sparsity problem.", "startOffset": 130, "endOffset": 202}, {"referenceID": 11, "context": "In parallel, recent work has proposed to model regularities beyond local facts by considering multirelation paths (Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Lin et al., 2015a; Shen et al., 2016).", "startOffset": 114, "endOffset": 180}, {"referenceID": 18, "context": "In parallel, recent work has proposed to model regularities beyond local facts by considering multirelation paths (Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Lin et al., 2015a; Shen et al., 2016).", "startOffset": 114, "endOffset": 180}, {"referenceID": 30, "context": "In parallel, recent work has proposed to model regularities beyond local facts by considering multirelation paths (Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Lin et al., 2015a; Shen et al., 2016).", "startOffset": 114, "endOffset": 180}, {"referenceID": 31, "context": "Most of the embedding models for knowledge base completion define an energy function fr(h, t) according to the fact\u2019s plausibility (Bordes et al., 2011, 2014, 2013; Socher et al., 2013; Wang et al., 2014; Yang et al., 2015; Guu et al., 2015; Nguyen et al., 2016b).", "startOffset": 131, "endOffset": 263}, {"referenceID": 37, "context": "Most of the embedding models for knowledge base completion define an energy function fr(h, t) according to the fact\u2019s plausibility (Bordes et al., 2011, 2014, 2013; Socher et al., 2013; Wang et al., 2014; Yang et al., 2015; Guu et al., 2015; Nguyen et al., 2016b).", "startOffset": 131, "endOffset": 263}, {"referenceID": 40, "context": "Most of the embedding models for knowledge base completion define an energy function fr(h, t) according to the fact\u2019s plausibility (Bordes et al., 2011, 2014, 2013; Socher et al., 2013; Wang et al., 2014; Yang et al., 2015; Guu et al., 2015; Nguyen et al., 2016b).", "startOffset": 131, "endOffset": 263}, {"referenceID": 13, "context": "Most of the embedding models for knowledge base completion define an energy function fr(h, t) according to the fact\u2019s plausibility (Bordes et al., 2011, 2014, 2013; Socher et al., 2013; Wang et al., 2014; Yang et al., 2015; Guu et al., 2015; Nguyen et al., 2016b).", "startOffset": 131, "endOffset": 263}, {"referenceID": 25, "context": "Most of the embedding models for knowledge base completion define an energy function fr(h, t) according to the fact\u2019s plausibility (Bordes et al., 2011, 2014, 2013; Socher et al., 2013; Wang et al., 2014; Yang et al., 2015; Guu et al., 2015; Nguyen et al., 2016b).", "startOffset": 131, "endOffset": 263}, {"referenceID": 22, "context": "Motivated by the linear translation phenomenon observed in well trained word embeddings (Mikolov et al., 2013), TransE (Bordes et al.", "startOffset": 88, "endOffset": 110}, {"referenceID": 6, "context": ", 2013), TransE (Bordes et al., 2013) represents the head entity h, the relation r and the tail entity t with vectors h, r and t \u2208 Rn respectively, which were trained so that h+r \u2248 t.", "startOffset": 16, "endOffset": 37}, {"referenceID": 19, "context": "To better model relation-specific aspects of the same entity, TransR (Lin et al., 2015b) uses projection matrices and projects the head entity and the tail entity to a relation-dependent space.", "startOffset": 69, "endOffset": 88}, {"referenceID": 25, "context": "STransE (Nguyen et al., 2016b) extends TransR", "startOffset": 8, "endOffset": 30}, {"referenceID": 33, "context": "However, directly posing `1 regularization (Tibshirani, 1996) on the attention vectors fails to produce sparse representations in our preliminary experiment, which motivates us to en-", "startOffset": 43, "endOffset": 61}, {"referenceID": 6, "context": "Previous work (Bordes et al., 2013; Lin et al., 2015b; Yang et al., 2015; Nguyen et al., 2016b) generally constructs a set of corrupted triples by replacing the head entity or tail entity with a random entity uniformly sampled from the KB.", "startOffset": 14, "endOffset": 95}, {"referenceID": 19, "context": "Previous work (Bordes et al., 2013; Lin et al., 2015b; Yang et al., 2015; Nguyen et al., 2016b) generally constructs a set of corrupted triples by replacing the head entity or tail entity with a random entity uniformly sampled from the KB.", "startOffset": 14, "endOffset": 95}, {"referenceID": 40, "context": "Previous work (Bordes et al., 2013; Lin et al., 2015b; Yang et al., 2015; Nguyen et al., 2016b) generally constructs a set of corrupted triples by replacing the head entity or tail entity with a random entity uniformly sampled from the KB.", "startOffset": 14, "endOffset": 95}, {"referenceID": 25, "context": "Previous work (Bordes et al., 2013; Lin et al., 2015b; Yang et al., 2015; Nguyen et al., 2016b) generally constructs a set of corrupted triples by replacing the head entity or tail entity with a random entity uniformly sampled from the KB.", "startOffset": 14, "endOffset": 95}, {"referenceID": 6, "context": "(2013) and use the same training/validation/test split as in (Bordes et al., 2013).", "startOffset": 61, "endOffset": 82}, {"referenceID": 5, "context": "To evaluate link prediction, we conduct experiments on the WN18 (WordNet) and FB15k (Freebase) introduced by Bordes et al. (2013) and use the same training/validation/test split as in (Bordes et al.", "startOffset": 109, "endOffset": 130}, {"referenceID": 5, "context": "We follow Bordes et al. (2013) to report the filter results, i.", "startOffset": 10, "endOffset": 31}, {"referenceID": 6, "context": "by TransE (Bordes et al., 2013), following Lin et al.", "startOffset": 10, "endOffset": 31}, {"referenceID": 5, "context": "by TransE (Bordes et al., 2013), following Lin et al. (2015b); Ji et al.", "startOffset": 11, "endOffset": 62}, {"referenceID": 5, "context": "by TransE (Bordes et al., 2013), following Lin et al. (2015b); Ji et al. (2015); Garc\u0131\u0301a-Dur\u00e1n et al.", "startOffset": 11, "endOffset": 80}, {"referenceID": 5, "context": "by TransE (Bordes et al., 2013), following Lin et al. (2015b); Ji et al. (2015); Garc\u0131\u0301a-Dur\u00e1n et al. (2016, 2015); Lin et al. (2015a). We ran minibatch SGD until convergence.", "startOffset": 11, "endOffset": 135}, {"referenceID": 33, "context": "rect triples as used in Wang et al. (2014), Lin et al.", "startOffset": 24, "endOffset": 43}, {"referenceID": 16, "context": "(2014), Lin et al. (2015b), He et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 14, "context": "(2015b), He et al. (2015), Ji et al.", "startOffset": 9, "endOffset": 26}, {"referenceID": 14, "context": "(2015b), He et al. (2015), Ji et al. (2015) and Lin et al.", "startOffset": 9, "endOffset": 44}, {"referenceID": 14, "context": "(2015b), He et al. (2015), Ji et al. (2015) and Lin et al. (2015a).", "startOffset": 9, "endOffset": 67}, {"referenceID": 25, "context": "STransE (Nguyen et al., 2016b) is the most similar knowledge embedding model to ours except that they use distinct projection matrices for each relation.", "startOffset": 8, "endOffset": 30}, {"referenceID": 1, "context": "All the models are implemented with Theano (Bergstra et al., 2010).", "startOffset": 43, "endOffset": 66}, {"referenceID": 30, "context": "Note that although IRN (Shen et al., 2016) does not explicitly exploit path information, it performs multi-step inference through the multiple usages of external memory.", "startOffset": 23, "endOffset": 42}, {"referenceID": 7, "context": "Model Additional Information WN18 FB15k Mean Rank Hits@10 Mean Rank Hits@10 SE (Bordes et al., 2011) No 985 80.", "startOffset": 79, "endOffset": 100}, {"referenceID": 5, "context": "8 Unstructured (Bordes et al., 2014) No 304 38.", "startOffset": 15, "endOffset": 36}, {"referenceID": 6, "context": "3 TransE (Bordes et al., 2013) No 251 89.", "startOffset": 9, "endOffset": 30}, {"referenceID": 37, "context": "1 TransH (Wang et al., 2014) No 303 86.", "startOffset": 9, "endOffset": 28}, {"referenceID": 19, "context": "4 TransR (Lin et al., 2015b) No 225 92.", "startOffset": 9, "endOffset": 28}, {"referenceID": 19, "context": "7 CTransR (Lin et al., 2015b) No 218 92.", "startOffset": 10, "endOffset": 29}, {"referenceID": 14, "context": "2 KG2E (He et al., 2015) No 348 93.", "startOffset": 7, "endOffset": 24}, {"referenceID": 15, "context": "0 TransD (Ji et al., 2015) No 212 92.", "startOffset": 9, "endOffset": 26}, {"referenceID": 12, "context": "3 TATEC (Garc\u0131\u0301a-Dur\u00e1n et al., 2016) No 58 76.", "startOffset": 8, "endOffset": 36}, {"referenceID": 31, "context": "7 NTN (Socher et al., 2013) No 66.", "startOffset": 6, "endOffset": 27}, {"referenceID": 40, "context": "4 DISTMULT (Yang et al., 2015) No 94.", "startOffset": 11, "endOffset": 30}, {"referenceID": 25, "context": "7 STransE (Nguyen et al., 2016b) No 206 (244) 93.", "startOffset": 10, "endOffset": 32}, {"referenceID": 11, "context": "RTransE (Garc\u0131\u0301a-Dur\u00e1n et al., 2015) Path 50 76.", "startOffset": 8, "endOffset": 36}, {"referenceID": 18, "context": "2 PTransE (Lin et al., 2015a) Path 58 84.", "startOffset": 10, "endOffset": 29}, {"referenceID": 34, "context": "6 NLFeat (Toutanova and Chen, 2015) Node + Link Features 94.", "startOffset": 9, "endOffset": 35}, {"referenceID": 38, "context": "0 Random Walk (Wei et al., 2016) Path 94.", "startOffset": 14, "endOffset": 32}, {"referenceID": 30, "context": "7 IRN (Shen et al., 2016) External Memory 249 95.", "startOffset": 6, "endOffset": 25}, {"referenceID": 24, "context": "Following Nguyen et al. (2016b) and Shen et al.", "startOffset": 10, "endOffset": 32}, {"referenceID": 24, "context": "Following Nguyen et al. (2016b) and Shen et al. (2016), we divide the models into two groups.", "startOffset": 10, "endOffset": 55}, {"referenceID": 9, "context": "(Faruqui et al., 2015).", "startOffset": 0, "endOffset": 22}, {"referenceID": 19, "context": "In KBC, CTransR (Lin et al., 2015b) enables relation embedding sharing across similar relations, but they cluster relations before training rather than learning it in a principled way.", "startOffset": 16, "endOffset": 35}, {"referenceID": 36, "context": "Learning the association between semantic relations has been used in related problems such as relational similarity measurement (Turney, 2012) and relation adaptation (Bollegala et al.", "startOffset": 128, "endOffset": 142}, {"referenceID": 4, "context": "Learning the association between semantic relations has been used in related problems such as relational similarity measurement (Turney, 2012) and relation adaptation (Bollegala et al., 2015).", "startOffset": 167, "endOffset": 191}, {"referenceID": 28, "context": "Transfer learning (Pan and Yang, 2010) has been shown to be promising to transfer knowl-", "startOffset": 18, "endOffset": 38}, {"referenceID": 9, "context": "We use the toolkit provided by (Faruqui et al., 2015).", "startOffset": 31, "endOffset": 53}, {"referenceID": 2, "context": "For example, Bharadwaj et al. (2016) transfers models on resource-rich languages to low resource languages by parameter sharing through common phonological features in name entity recognition.", "startOffset": 13, "endOffset": 37}, {"referenceID": 2, "context": "For example, Bharadwaj et al. (2016) transfers models on resource-rich languages to low resource languages by parameter sharing through common phonological features in name entity recognition. Zoph et al. (2016) initialize from models trained by resource-rich languages to translate low-resource languages.", "startOffset": 13, "endOffset": 212}, {"referenceID": 21, "context": "Several works on obtaining a sparse attention (Martins and Astudillo, 2016; Makhzani and Frey, 2014; Shazeer et al., 2017) share a similar idea of sorting the values before softmax and only keeping theK largest values.", "startOffset": 46, "endOffset": 122}, {"referenceID": 20, "context": "Several works on obtaining a sparse attention (Martins and Astudillo, 2016; Makhzani and Frey, 2014; Shazeer et al., 2017) share a similar idea of sorting the values before softmax and only keeping theK largest values.", "startOffset": 46, "endOffset": 122}, {"referenceID": 29, "context": "Several works on obtaining a sparse attention (Martins and Astudillo, 2016; Makhzani and Frey, 2014; Shazeer et al., 2017) share a similar idea of sorting the values before softmax and only keeping theK largest values.", "startOffset": 46, "endOffset": 122}, {"referenceID": 17, "context": "The block iterative optimization algorithm in our work is inspired by LightRNN (Li et al., 2016).", "startOffset": 79, "endOffset": 96}], "year": 2017, "abstractText": "Knowledge bases are important resources for a variety of natural language processing tasks but suffer from incompleteness. We propose a novel embedding model, ITransF, to perform knowledge base completion. Equipped with a sparse attention mechanism, ITransF discovers hidden concepts of relations and transfer statistical strength through the sharing of concepts. Moreover, the learned associations between relations and concepts, which are represented by sparse attention vectors, can be interpreted easily. We evaluate ITransF on two benchmark datasets\u2014 WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information.", "creator": "LaTeX with hyperref package"}}}