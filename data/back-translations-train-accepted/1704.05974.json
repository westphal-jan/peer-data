{"id": "1704.05974", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2017", "title": "Cross-domain Semantic Parsing via Paraphrasing", "abstract": "Existing studies on semantic parsing mainly focus on the in-domain setting. We formulate cross-domain semantic parsing as a domain adaptation problem: train a semantic parser on some source domains and then adapt it to the target domain. Due to the diversity of logical forms in different domains, this problem presents unique and intriguing challenges. By converting logical forms into canonical utterances in natural language, we reduce semantic parsing to paraphrasing, and develop an attentive sequence-to-sequence paraphrase model that is general and flexible to adapt to different domains. We discover two problems, small micro variance and large macro variance, of pre-trained word embeddings that hurdle their direct use in neural networks, and propose standardization techniques as a remedy. On the popular Overnight dataset, which contains eight domains, we show that both cross-domain training and standardized pre-trained word embeddings can bring significant improvement.", "histories": [["v1", "Thu, 20 Apr 2017 01:26:23 GMT  (749kb,D)", "https://arxiv.org/abs/1704.05974v1", null], ["v2", "Mon, 24 Jul 2017 19:35:17 GMT  (756kb,D)", "http://arxiv.org/abs/1704.05974v2", "12 pages, 2 figures, accepted by EMNLP2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yu su", "xifeng yan"], "accepted": true, "id": "1704.05974"}, "pdf": {"name": "1704.05974.pdf", "metadata": {"source": "CRF", "title": "Cross-domain Semantic Parsing via Paraphrasing", "authors": ["Yu Su", "Xifeng Yan"], "emails": ["ysu@cs.ucsb.edu", "xyan@cs.ucsb.edu"], "sections": [{"heading": "1 Introduction", "text": "Semantic parsing has been used in many areas, including retrieving data / knowledge bases (Woods, 1973; Cell and Mooney, 1996; Berant et al., 2013), controlling IoT devices (Campagna et al., 2017), and communicating with robots (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Bisk et al., 2016). Despite the broad scope, studies on semantic parsing have mainly focused on the indomain setting, where both training and test data are drawn from the same domain. How to build semantic parsers that can learn across domains remains an underaddressed problem. In this paper, we examine cross-domain setic parsing."}, {"heading": "2 Cross-domain Semantic Parsing", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Problem Definition", "text": "Unless otherwise specified, we use u to designate the input expression, c for the canonical expression, and z for the logical form. We call U a set of all possible expressions. Suppose Z is the set of logical forms, a semantic parser is a figure f: U \u2192 Z that maps each input expression to a logical form (a zero logical form can be included in Z to reject utterances outside the domain). In cross-domain semantic parsing, we assume that there is a set of K source domains {Zi} Ki = 1, each with a set of training examples {(uij, zij)} Ni j = 1. In principle, it is advantageous to model the source domains separately (thumb \u0301 III and Marcu, 2006), which distinguishes the possibility of separating domain date-specific data from domain-specific data."}, {"heading": "2.2 Framework", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2.3 Prior Work", "text": "Most studies on semantic relationships are described using carefully selected symbols (e.g.), there are a number of studies that have particular relevance to this work. In recent efforts to use semantic parsing to large knowledge bases such as Freebase (Bollacker et al., 2008), researchers have explored several ways to tap into the semantics of knowledge base relationships, which are often based on at least one (often both) of the following assumptions: (1) The structure of freebase entities can be associated with external text corporations, and serve as anchors for searching for semantic relationships of freebase from text. For example, Cai and Alexander (2013), among others. They use phrases from Wikipedia that contain each entity of a freebase relationship as a support set of the relationship. (2) Self-explanatory predicate symbols are described with a carefully selected symbol."}, {"heading": "3 Paraphrase Model", "text": "In this section, we propose a paraphrase based on the Seq2Seq model (SDG-1). We have used similar models in semantic analysis (Jia and Liang, 2016; Dong and Lapata, 2016), but for the direct representation of statements on logical forms. We note that it can also be used as a paraphrase for semantic analysis. Several other neural models have been proposed for imaging (Socher et al., 2011; Yin and Guesses, 2015), but it is not the focus of this work to compare all alternative models."}, {"heading": "4 Pre-trained Word Embedding for Domain Adaptation", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "5 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Data Analysis", "text": "The OVERNIGHT dataset (Wang et al., 2015) contains 8 different domains. Each domain is based on a separate knowledge base with logical forms written in \u03bb-DCS (Liang et al., 2013). Logical forms are converted into canonical expressions via a simple grammar, and the input expressions are collected by asking crowd-workers to paraphrase the canonical expressions. Different domains are designed to emphasize different types of linguistic phenomena. For example, the CALENDAR domain requires a semantic parser to deal with temporal language such as \"meetings that begin after 10 o'clock,\" while the BLOCKS domain has spatial languages such as \"the block above block 1.\" Vocabularies vary remarkably across domains (Table 2). For each domain, only 45% to 70% of the words are covered by any of the other 7 domains."}, {"heading": "5.2 Experiment Setup", "text": "We compare our model with all previous methods evaluated on the OVERNIGHT dataset. Wang et al. (2015) use a log-linear model with a rich set of features, including the paraphrase features derived from the PPDB (Ganitkevitch et al.), to evaluate logical forms. (2016) we use a multi-layered perceptron to encode the uniformities and bigrams of the input utterance, and then use the derivative sequence of a logical form under a grammar. (2016) we also use a multi-layered perceptron to encode the uniformities and bigrams of the input utterance, using an RNN to predict the derivative sequence of a logical form under the grammar."}, {"heading": "5.3 Experiment Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.3.1 Comparison with Previous Methods", "text": "Our basic model (Random + I) achieves an accuracy comparable to the best in-domain model to date (Jia and Liang, 2016). With our key innovations, cross-domain training and word embedding standardization, our complete model is able to surpass the best model to date and achieve the best accuracy in 6 of the 8 domains. Next, we examine the novelties separately."}, {"heading": "5.3.2 Word Embedding Initialization", "text": "The results in the domain clearly show the sensitivity of the model performance to text embed initialization. However, with the raw WORD2VEC vectors or with normalization by example, the performance is significantly worse than with random initialization (6.2% or 7.3%). However, based on the previous analysis, one should not be too surprised. The problem of small microvariance harms the optimization. In contrast, both of the proposed standardization techniques result in better performance in the domain than random initialization (1.4% or 2.5%), thereby achieving a new accuracy in the domain (78.2%) with OVERNIGHT. Results show that the trained WORD2VEC vectors can actually provide useful information, but only if they are properly standardized."}, {"heading": "5.3.3 Cross-domain Training", "text": "Even with raw WORD2VEC embedding or normalization by individual examples, cross-domain training helps the model escape bad initialization, even though it is still inferior to alternative initializations. Again, the best results are achieved through standardization, with standardization by individual examples yielding slightly greater improvement than standardization by individual characteristics. We observe that the improvement through cross-domain training is correlated with the abundance of domain-internal training data of the target domain. To further investigate this observation, we use the ratio between the number of examples (N) and the vocabulary size (| V |) to indicate the data abundance of a domain (the higher, the more frequent), and calculate the Pearson correlation coefficient between data abundance and improved accuracy from cross-domain training (X \u2212 I)."}, {"heading": "5.3.4 Using Downsampled Training Data", "text": "Compared to the size of the vocabulary and the number of logical forms, cross-domain training data is indeed abundant in the OVERNIGHT dataset. In cross-domain semantic parsing, we are more interested in the scenario in which there is insufficient training data for the target area. To emulate this scenario, we take down the cross-domain training data for each target domain, but still use all training data from the source domain (i.e. Nt Ns) The results are shown in Figure 2. Cross-domain training gains are most significant when cross-domain training data is scarce. The more cross-domain training data is collected, the lower the expected gain. These results reinforce those from Table 4. It is noteworthy that the effect of cross-domain training is almost as good as when using all data. For domains with abundant training data such as SOCIAL, where only 30% of cross-domain training data is used, the training model is as good as can be achieved with almost all use."}, {"heading": "6 Discussion", "text": "Scalability, including vertical scalability, i.e. scalability to handle more complex inputs and logical constructs, and horizontal scalability, i.e. how to scale more ranges, is one of the most critical challenges facing semantic parsing today. In this work, we took an early step toward horizontal scalability and proposed a cross-domain framework for semantic parsing. Using a cross-domain paraphrase model, we demonstrated that cross-domain training of semantic consensus under domain adaptation can be quite effective. We also explored how to properly standardize pre-trained word embedding in neural networks, especially for domain adaptations. This work opens up a number of future directions. As discussed in Section 2.3, many conventional domain adaptations and representative ideas require a full domain adaptation when we can only pre-fit this framework."}, {"heading": "Acknowledgments", "text": "The authors thank the anonymous reviewers for their thoughtful comments. This research is partly sponsored by the Army Research Laboratory under the cooperation agreements W911NF09-2-0053 and NSF IIS 1528175. The views and conclusions contained herein are those of the authors and should not be interpreted to represent, express or imply, the official guidelines of the Army Research Laboratory or the U.S. Government. The U.S. Government has the authority to reproduce and distribute reproductions for government purposes, notwithstanding the copyright notices contained herein."}], "references": [{"title": "Tensorflow: Large-scale machine learning", "author": ["Mart\u0131\u0301n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions", "author": ["Yoav Artzi", "Luke Zettlemoyer."], "venue": "Transactions of the Association for Computational Linguistics, 1:49\u201362.", "citeRegEx": "Artzi and Zettlemoyer.,? 2013", "shortCiteRegEx": "Artzi and Zettlemoyer.", "year": 2013}, {"title": "Semantic parsing on Freebase from question-answer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Semantic parsing via paraphrasing", "author": ["Jonathan Berant", "Percy Liang."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Berant and Liang.,? 2014", "shortCiteRegEx": "Berant and Liang.", "year": 2014}, {"title": "Natural language communication with robots", "author": ["Yonatan Bisk", "Deniz Yuret", "Daniel Marcu."], "venue": "Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Bisk et al\\.,? 2016", "shortCiteRegEx": "Bisk et al\\.", "year": 2016}, {"title": "Domain adaptation with structural correspondence learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Blitzer et al\\.,? 2006", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Proceedings of the ACM SIGMOD International conference on Management of", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Semantic parsing freebase: Towards open-domain semantic parsing", "author": ["Qingqing Cai", "Alexander Yates."], "venue": "Second Joint Conference on Lexical and Computational Semantics (* SEM).", "citeRegEx": "Cai and Yates.,? 2013", "shortCiteRegEx": "Cai and Yates.", "year": 2013}, {"title": "Almond: The architecture of an open, crowdsourced, privacy-preserving, programmable virtual assistant", "author": ["Giovanni Campagna", "Rakesh Ramesh", "Silei Xu", "Michael Fischer", "Monica S Lam."], "venue": "Proceedings of the International Conference", "citeRegEx": "Campagna et al\\.,? 2017", "shortCiteRegEx": "Campagna et al\\.", "year": 2017}, {"title": "Adaptation of maximum entropy capitalizer: Little data can help a lot", "author": ["Ciprian Chelba", "Alex Acero."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Chelba and Acero.,? 2004", "shortCiteRegEx": "Chelba and Acero.", "year": 2004}, {"title": "Learning to interpret natural language navigation instructions from observations", "author": ["David L Chen", "Raymond J Mooney."], "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.", "citeRegEx": "Chen and Mooney.,? 2011", "shortCiteRegEx": "Chen and Mooney.", "year": 2011}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["Minmin Chen", "Zhixiang Xu", "Kilian Weinberger", "Fei Sha."], "venue": "Proceedings of the International Conference on Machine Learning.", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv:1406.1078", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Frustratingly easy domain adaptation", "author": ["Hal Daum\u00e9 III."], "venue": "arXiv:0907.1815 [cs.LG].", "citeRegEx": "III.,? 2009", "shortCiteRegEx": "III.", "year": 2009}, {"title": "Domain adaptation for machine translation by mining unseen words", "author": ["Hal Daum\u00e9 III", "Jagadeesh Jagarlamudi."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "III and Jagarlamudi.,? 2011", "shortCiteRegEx": "III and Jagarlamudi.", "year": 2011}, {"title": "Domain adaptation for statistical classifiers", "author": ["Hal Daum\u00e9 III", "Daniel Marcu."], "venue": "Journal of Artificial Intelligence Research, 26:101\u2013126.", "citeRegEx": "III and Marcu.,? 2006", "shortCiteRegEx": "III and Marcu.", "year": 2006}, {"title": "Language to logical form with neural attention", "author": ["Li Dong", "Mirella Lapata."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Dong and Lapata.,? 2016", "shortCiteRegEx": "Dong and Lapata.", "year": 2016}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol", "Pascal Vincent", "Samy Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Paraphrase-driven learning for open question answering", "author": ["Anthony Fader", "Luke S Zettlemoyer", "Oren Etzioni."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Fader et al\\.,? 2013", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "PPDB: The paraphrase database", "author": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch."], "venue": "Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Ganitkevitch et al\\.,? 2013", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Machine Learning.", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Leveraging domain-independent information in semantic parsing", "author": ["Dan Goldwasser", "Dan Roth."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Goldwasser and Roth.,? 2013", "shortCiteRegEx": "Goldwasser and Roth.", "year": 2013}, {"title": "The symbol grounding problem", "author": ["Stevan Harnad."], "venue": "Physica D: Nonlinear Phenomena, 42(1-3):335\u2013 346.", "citeRegEx": "Harnad.,? 1990", "shortCiteRegEx": "Harnad.", "year": 1990}, {"title": "Neural semantic parsing over multiple knowledge-bases", "author": ["Jonathan Herzig", "Jonathan Berant."], "venue": "arXiv:1702.01569 [cs.CL].", "citeRegEx": "Herzig and Berant.,? 2017", "shortCiteRegEx": "Herzig and Berant.", "year": 2017}, {"title": "Improving neural networks by preventing coadaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov."], "venue": "arXiv:1207.0580", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen."], "venue": "Proceedings of the Annual Conference on Neural Information Processing Systems.", "citeRegEx": "Hu et al\\.,? 2014", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy."], "venue": "Proceedings of the International Conference on Machine Learning, pages 448\u2013456.", "citeRegEx": "Ioffe and Szegedy.,? 2015", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Data recombination for neural semantic parsing", "author": ["Robin Jia", "Percy Liang."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Jia and Liang.,? 2016", "shortCiteRegEx": "Jia and Liang.", "year": 2016}, {"title": "Smart reply: Automated response suggestion for email", "author": ["Anjuli Kannan", "Karol Kurach", "Sujith Ravi", "Tobias Kaufmann", "Andrew Tomkins", "Balint Miklos", "Greg Corrado", "L\u00e1szl\u00f3 Luk\u00e1cs", "Marina Ganea", "Peter Young"], "venue": null, "citeRegEx": "Kannan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2016}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["Tom Kwiatkowski", "Eunsol Choi", "Yoav Artzi", "Luke Zettlemoyer."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Kwiatkowski et al\\.,? 2013", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics, 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Lambda dependency-based compositional semantics", "author": ["Percy Liang."], "venue": "arXiv:1309.4408 [cs.AI].", "citeRegEx": "Liang.,? 2013", "shortCiteRegEx": "Liang.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Proceedings of the Annual Conference on Neural Information Processing Systems.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Paraphrase generation from latent-variable PCFGs for semantic parsing", "author": ["Shashi Narayan", "Siva Reddy", "Shay B Cohen."], "venue": "arXiv:1601.06068 [cs.CL].", "citeRegEx": "Narayan et al\\.,? 2016", "shortCiteRegEx": "Narayan et al\\.", "year": 2016}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang."], "venue": "IEEE Transactions on knowledge and data engineering, 22(10):1345\u20131359.", "citeRegEx": "Pan and Yang.,? 2010", "shortCiteRegEx": "Pan and Yang.", "year": 2010}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of Conference on", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Large-scale semantic parsing without questionanswer pairs", "author": ["Siva Reddy", "Mirella Lapata", "Mark Steedman."], "venue": "Transactions of the Association for Computational Linguistics, 2:377\u2013392.", "citeRegEx": "Reddy et al\\.,? 2014", "shortCiteRegEx": "Reddy et al\\.", "year": 2014}, {"title": "Transforming dependency structures to logical forms for semantic parsing", "author": ["Siva Reddy", "Oscar T\u00e4ckstr\u00f6m", "Michael Collins", "Tom Kwiatkowski", "Dipanjan Das", "Mark Steedman", "Mirella Lapata."], "venue": "Transactions of the Association for Computational", "citeRegEx": "Reddy et al\\.,? 2016", "shortCiteRegEx": "Reddy et al\\.", "year": 2016}, {"title": "Universal semantic parsing", "author": ["Siva Reddy", "Oscar T\u00e4ckstr\u00f6m", "Slav Petrov", "Mark Steedman", "Mirella Lapata."], "venue": "arXiv:1702.03196 [cs.CL].", "citeRegEx": "Reddy et al\\.,? 2017", "shortCiteRegEx": "Reddy et al\\.", "year": 2017}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H Huang", "Jeffrey Pennington", "Andrew Y Ng", "Christopher D Manning."], "venue": "Proceedings of the Annual Conference on Neural Information Pro-", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "On generating characteristic-rich question sets for QA evaluation", "author": ["Yu Su", "Huan Sun", "Brian Sadler", "Mudhakar Srivatsa", "Izzeddin G\u00fcr", "Zenghui Yan", "Xifeng Yan."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Su et al\\.,? 2016", "shortCiteRegEx": "Su et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Proceedings of the Annual Conference on Neural Information Processing Systems.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Understanding natural language commands for robotic navigation and mobile manipulation", "author": ["Stefanie A Tellex", "Thomas Fleming Kollar", "Steven R Dickerson", "Matthew R Walter", "Ashis Banerjee", "Seth Teller", "Nicholas Roy."], "venue": "Proceedings of the AAAI", "citeRegEx": "Tellex et al\\.,? 2011", "shortCiteRegEx": "Tellex et al\\.", "year": 2011}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Proceedings of the Annual Conference on Neural Information Processing Systems.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Building a semantic parser overnight", "author": ["Yushi Wang", "Jonathan Berant", "Percy Liang."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Progress in natural language understanding: an application to lunar geology", "author": ["William A Woods."], "venue": "Proceedings of the American Federation of Information Processing Societies Conference.", "citeRegEx": "Woods.,? 1973", "shortCiteRegEx": "Woods.", "year": 1973}, {"title": "Sequence-based structured prediction for semantic parsing", "author": ["Chunyang Xiao", "Marc Dymetman", "Claire Gardent."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Xiao et al\\.,? 2016", "shortCiteRegEx": "Xiao et al\\.", "year": 2016}, {"title": "Question answering on freebase via relation extraction and textual evidence", "author": ["Kun Xu", "Siva Reddy", "Yansong Feng", "Songfang Huang", "Dongyan Zhao."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "author": ["Scott Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Yih et al\\.,? 2015", "shortCiteRegEx": "Yih et al\\.", "year": 2015}, {"title": "MultiGranCNN: An architecture for general matching of text chunks on multiple levels of granularity", "author": ["Wenpeng Yin", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Yin and Sch\u00fctze.,? 2015", "shortCiteRegEx": "Yin and Sch\u00fctze.", "year": 2015}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["John M Zelle", "Raymon J Mooney."], "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.", "citeRegEx": "Zelle and Mooney.,? 1996", "shortCiteRegEx": "Zelle and Mooney.", "year": 1996}], "referenceMentions": [{"referenceID": 46, "context": "Semantic parsing has been applied in many domains, including querying data/knowledge bases (Woods, 1973; Zelle and Mooney, 1996; Berant et al., 2013), controlling IoT devices (Campagna et al.", "startOffset": 91, "endOffset": 149}, {"referenceID": 51, "context": "Semantic parsing has been applied in many domains, including querying data/knowledge bases (Woods, 1973; Zelle and Mooney, 1996; Berant et al., 2013), controlling IoT devices (Campagna et al.", "startOffset": 91, "endOffset": 149}, {"referenceID": 2, "context": "Semantic parsing has been applied in many domains, including querying data/knowledge bases (Woods, 1973; Zelle and Mooney, 1996; Berant et al., 2013), controlling IoT devices (Campagna et al.", "startOffset": 91, "endOffset": 149}, {"referenceID": 8, "context": ", 2013), controlling IoT devices (Campagna et al., 2017), and", "startOffset": 33, "endOffset": 56}, {"referenceID": 10, "context": "communicating with robots (Chen and Mooney, 2011; Tellex et al., 2011; Artzi and Zettlemoyer, 2013; Bisk et al., 2016).", "startOffset": 26, "endOffset": 118}, {"referenceID": 43, "context": "communicating with robots (Chen and Mooney, 2011; Tellex et al., 2011; Artzi and Zettlemoyer, 2013; Bisk et al., 2016).", "startOffset": 26, "endOffset": 118}, {"referenceID": 1, "context": "communicating with robots (Chen and Mooney, 2011; Tellex et al., 2011; Artzi and Zettlemoyer, 2013; Bisk et al., 2016).", "startOffset": 26, "endOffset": 118}, {"referenceID": 4, "context": "communicating with robots (Chen and Mooney, 2011; Tellex et al., 2011; Artzi and Zettlemoyer, 2013; Bisk et al., 2016).", "startOffset": 26, "endOffset": 118}, {"referenceID": 5, "context": "Traditional domain adaptation (Daum\u00e9 III and Marcu, 2006; Blitzer et al., 2006) only concerns natural languages, while semantic parsing concerns both natural and formal languages.", "startOffset": 30, "endOffset": 79}, {"referenceID": 23, "context": ", other symbols like hired by or even predicate1 can also be used for the employer predicate, reminiscent of the symbol grounding problem (Harnad, 1990).", "startOffset": 138, "endOffset": 152}, {"referenceID": 3, "context": "Inspired by the recent success of paraphrasing based semantic parsing (Berant and Liang, 2014; Wang et al., 2015), we propose to use natural language as an intermediate representation for crossdomain semantic parsing.", "startOffset": 70, "endOffset": 113}, {"referenceID": 45, "context": "Inspired by the recent success of paraphrasing based semantic parsing (Berant and Liang, 2014; Wang et al., 2015), we propose to use natural language as an intermediate representation for crossdomain semantic parsing.", "startOffset": 70, "endOffset": 113}, {"referenceID": 45, "context": "To give some idea of the difficulty, for each of the eight domains in the popular OVERNIGHT (Wang et al., 2015) dataset, 30% to 55% of the words never occur in any of the other domains, a similar problem observed in domain adaptation for", "startOffset": 92, "endOffset": 111}, {"referenceID": 33, "context": "WORD2VEC (Mikolov et al., 2013) to combat the vocabulary variety across domains.", "startOffset": 9, "endOffset": 31}, {"referenceID": 32, "context": ", \u03bb-DCS (Liang, 2013) as in the OVERNIGHT dataset.", "startOffset": 8, "endOffset": 21}, {"referenceID": 3, "context": "Our framework follows the research line of semantic parsing via paraphrasing (Berant and Liang, 2014; Wang et al., 2015).", "startOffset": 77, "endOffset": 120}, {"referenceID": 45, "context": "Our framework follows the research line of semantic parsing via paraphrasing (Berant and Liang, 2014; Wang et al., 2015).", "startOffset": 77, "endOffset": 120}, {"referenceID": 45, "context": "ous work (Wang et al., 2015) has demonstrated how to design such a mapping, where a domaingeneral grammar and a domain-specific lexicon are constructed to automatically convert every logical form to a canonical utterance.", "startOffset": 9, "endOffset": 28}, {"referenceID": 23, "context": "This indeed brings an additional cost, but we believe it is reasonable and even necessary for three reasons: (1) Only domain administrators know the predicate semantics the best, so it has to be them to reveal that by grounding the predicates to natural language (the symbol grounding problem (Harnad, 1990)).", "startOffset": 293, "endOffset": 307}, {"referenceID": 45, "context": "(3) Canonical utterances are understandable by average users, and thus can also be used for training data collection via crowdsourcing (Wang et al., 2015; Su et al., 2016), which can amortize the cost.", "startOffset": 135, "endOffset": 171}, {"referenceID": 41, "context": "(3) Canonical utterances are understandable by average users, and thus can also be used for training data collection via crowdsourcing (Wang et al., 2015; Su et al., 2016), which can amortize the cost.", "startOffset": 135, "endOffset": 171}, {"referenceID": 3, "context": "In the previous work based on paraphrasing (Berant and Liang, 2014; Wang et al., 2015), semantic parsers are implemented as log-linear models with hand-engineered domainspecific features (including paraphrase features).", "startOffset": 43, "endOffset": 86}, {"referenceID": 45, "context": "In the previous work based on paraphrasing (Berant and Liang, 2014; Wang et al., 2015), semantic parsers are implemented as log-linear models with hand-engineered domainspecific features (including paraphrase features).", "startOffset": 43, "endOffset": 86}, {"referenceID": 21, "context": "learning for domain adaptation (Glorot et al., 2011; Chen et al., 2012), we propose a paraphrase model based on the sequence-to-sequence (Seq2Seq) model (Sutskever et al.", "startOffset": 31, "endOffset": 71}, {"referenceID": 11, "context": "learning for domain adaptation (Glorot et al., 2011; Chen et al., 2012), we propose a paraphrase model based on the sequence-to-sequence (Seq2Seq) model (Sutskever et al.", "startOffset": 31, "endOffset": 71}, {"referenceID": 42, "context": ", 2012), we propose a paraphrase model based on the sequence-to-sequence (Seq2Seq) model (Sutskever et al., 2014), which can be trained end to end without feature engineering.", "startOffset": 89, "endOffset": 113}, {"referenceID": 6, "context": "In the recent efforts of scaling semantic parsing to large knowledge bases like Freebase (Bollacker et al., 2008), researchers have explored several ways to infer the semantics of knowledge base relations unseen in training, which are often based on at least one (often both) of the following assumptions: (1) Distant supervision.", "startOffset": 89, "endOffset": 113}, {"referenceID": 2, "context": "For example, Cai and Alexander (2013), among others (Berant et al., 2013; Xu et al., 2016), use sentences from Wikipedia that contain any entity pair of a Freebase relation as the support set of the relation.", "startOffset": 52, "endOffset": 90}, {"referenceID": 48, "context": "For example, Cai and Alexander (2013), among others (Berant et al., 2013; Xu et al., 2016), use sentences from Wikipedia that contain any entity pair of a Freebase relation as the support set of the relation.", "startOffset": 52, "endOffset": 90}, {"referenceID": 5, "context": "In the recent efforts of scaling semantic parsing to large knowledge bases like Freebase (Bollacker et al., 2008), researchers have explored several ways to infer the semantics of knowledge base relations unseen in training, which are often based on at least one (often both) of the following assumptions: (1) Distant supervision. Freebase entities can be linked to external text corpora, and serve as anchors for seeking semantics of Freebase relations from text. For example, Cai and Alexander (2013), among others (Berant et al.", "startOffset": 90, "endOffset": 503}, {"referenceID": 49, "context": "For example, Yih et al. (2015) directly compute", "startOffset": 13, "endOffset": 31}, {"referenceID": 30, "context": "Kwiatkowski et al. (2013) also extract lexical features from input utterance and the surface form of entities and relations.", "startOffset": 0, "endOffset": 26}, {"referenceID": 24, "context": "In parallel of this work, Herzig and Berant (2017) have explored another direction of semantic parsing with multiple domains, where they use all the domains to train a single semantic parser, and attach a domain-specific encoding to the training data of each domain to help the semantic parser differentiate between domains.", "startOffset": 26, "endOffset": 51}, {"referenceID": 2, "context": "Berant et al. (2013) propose to learn from utterance-denotation pairs instead of utterance-logical form pairs, while Wang et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Berant et al. (2013) propose to learn from utterance-denotation pairs instead of utterance-logical form pairs, while Wang et al. (2015) and Su et al.", "startOffset": 0, "endOffset": 136}, {"referenceID": 2, "context": "Berant et al. (2013) propose to learn from utterance-denotation pairs instead of utterance-logical form pairs, while Wang et al. (2015) and Su et al. (2016) manage to employ crowd workers with no linguistic expertise for data collection.", "startOffset": 0, "endOffset": 157}, {"referenceID": 2, "context": "Berant et al. (2013) propose to learn from utterance-denotation pairs instead of utterance-logical form pairs, while Wang et al. (2015) and Su et al. (2016) manage to employ crowd workers with no linguistic expertise for data collection. Jia and Liang (2016) propose an interesting form of data augmentation.", "startOffset": 0, "endOffset": 259}, {"referenceID": 30, "context": "been used, such as combinatory categorial grammar (Kwiatkowski et al., 2013; Reddy et al., 2014), dependency tree (Reddy et al.", "startOffset": 50, "endOffset": 96}, {"referenceID": 37, "context": "been used, such as combinatory categorial grammar (Kwiatkowski et al., 2013; Reddy et al., 2014), dependency tree (Reddy et al.", "startOffset": 50, "endOffset": 96}, {"referenceID": 22, "context": ", 2016, 2017), and semantic role structure (Goldwasser and Roth, 2013).", "startOffset": 43, "endOffset": 70}, {"referenceID": 18, "context": "For example, Fader et al. (2013) leverage question paraphrases to for question answering, while Narayan et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 18, "context": "For example, Fader et al. (2013) leverage question paraphrases to for question answering, while Narayan et al. (2016) generate paraphrases as a way of data augmentation.", "startOffset": 13, "endOffset": 118}, {"referenceID": 5, "context": "Cross-domain semantic parsing can greatly benefit from the rich literature of domain adaptation and transfer learning (Daum\u00e9 III and Marcu, 2006; Blitzer et al., 2006; Pan and Yang, 2010; Glorot et al., 2011).", "startOffset": 118, "endOffset": 208}, {"referenceID": 35, "context": "Cross-domain semantic parsing can greatly benefit from the rich literature of domain adaptation and transfer learning (Daum\u00e9 III and Marcu, 2006; Blitzer et al., 2006; Pan and Yang, 2010; Glorot et al., 2011).", "startOffset": 118, "endOffset": 208}, {"referenceID": 21, "context": "Cross-domain semantic parsing can greatly benefit from the rich literature of domain adaptation and transfer learning (Daum\u00e9 III and Marcu, 2006; Blitzer et al., 2006; Pan and Yang, 2010; Glorot et al., 2011).", "startOffset": 118, "endOffset": 208}, {"referenceID": 5, "context": "Cross-domain semantic parsing can greatly benefit from the rich literature of domain adaptation and transfer learning (Daum\u00e9 III and Marcu, 2006; Blitzer et al., 2006; Pan and Yang, 2010; Glorot et al., 2011). For example, Chelba and Acero (2004) use parameters trained in the source domain as prior to regularize parameters in the target domain.", "startOffset": 146, "endOffset": 247}, {"referenceID": 5, "context": "Cross-domain semantic parsing can greatly benefit from the rich literature of domain adaptation and transfer learning (Daum\u00e9 III and Marcu, 2006; Blitzer et al., 2006; Pan and Yang, 2010; Glorot et al., 2011). For example, Chelba and Acero (2004) use parameters trained in the source domain as prior to regularize parameters in the target domain. The feature augmentation technique from Daum\u00e9 III (2009) can be very helpful when there are multiple source domains.", "startOffset": 146, "endOffset": 404}, {"referenceID": 42, "context": "In this section we propose a paraphrase model based on the Seq2Seq model (Sutskever et al., 2014) with soft attention.", "startOffset": 73, "endOffset": 97}, {"referenceID": 28, "context": "Similar models have been used in semantic parsing (Jia and Liang, 2016; Dong and Lapata, 2016) but for directly mapping utterances to logical forms.", "startOffset": 50, "endOffset": 94}, {"referenceID": 16, "context": "Similar models have been used in semantic parsing (Jia and Liang, 2016; Dong and Lapata, 2016) but for directly mapping utterances to logical forms.", "startOffset": 50, "endOffset": 94}, {"referenceID": 40, "context": "Several other neural models have been proposed for paraphrasing (Socher et al., 2011; Hu et al., 2014; Yin and Sch\u00fctze, 2015), but it is not the focus of this work to compare all the alternatives.", "startOffset": 64, "endOffset": 125}, {"referenceID": 26, "context": "Several other neural models have been proposed for paraphrasing (Socher et al., 2011; Hu et al., 2014; Yin and Sch\u00fctze, 2015), but it is not the focus of this work to compare all the alternatives.", "startOffset": 64, "endOffset": 125}, {"referenceID": 50, "context": "Several other neural models have been proposed for paraphrasing (Socher et al., 2011; Hu et al., 2014; Yin and Sch\u00fctze, 2015), but it is not the focus of this work to compare all the alternatives.", "startOffset": 64, "endOffset": 125}, {"referenceID": 12, "context": "where gated recurrent unit (GRU) as defined in (Cho et al., 2014) is used as the recurrence.", "startOffset": 47, "endOffset": 65}, {"referenceID": 44, "context": "The attention takes a form similar to (Vinyals et al., 2015).", "startOffset": 38, "endOffset": 60}, {"referenceID": 25, "context": "We apply dropout (Hinton et al., 2012) on both input and output of the GRU cells to prevent overfitting.", "startOffset": 17, "endOffset": 38}, {"referenceID": 42, "context": "One is to use it to generate the most likely output utterance u\u2032 given an input utterance u (Sutskever et al., 2014),", "startOffset": 92, "endOffset": 116}, {"referenceID": 29, "context": "An alternative way is to use the model to rank the legitimate canonical utterances (Kannan et al., 2016):", "startOffset": 83, "endOffset": 104}, {"referenceID": 17, "context": "However, deep neural networks are very sensitive to initialization (Erhan et al., 2010), and a statistical analysis of the pre-trained WORD2VEC vectors reveals some characteristics that may not be desired for initializing deep neural networks.", "startOffset": 67, "endOffset": 87}, {"referenceID": 20, "context": "word2vec/ Under some conditions, including using Xavier initialization (also introduced in that paper and now widely used) for weights, Glorot and Bengio (2010) have shown that the activation variances in a feedforward neural network will be roughly the same as the input variances (word embedding here) at the beginning of training.", "startOffset": 136, "endOffset": 161}, {"referenceID": 31, "context": "Levy et al. (2015) have suggested per-example normalization4 of pre-trained word embeddings for lexical tasks like word similarity and analogy, which do no involve deep neu-", "startOffset": 0, "endOffset": 19}, {"referenceID": 44, "context": "For example, Vinyals et al. (2015) have found that directly using the WORD2VEC vectors for initialization can bring a", "startOffset": 13, "endOffset": 35}, {"referenceID": 36, "context": "It can also be found in the implementation of Glove (Pennington et al., 2014): https://github.", "startOffset": 52, "endOffset": 77}, {"referenceID": 27, "context": "The proposed standardization technique appears in a similar spirit to batch normalization (Ioffe and Szegedy, 2015).", "startOffset": 90, "endOffset": 115}, {"referenceID": 45, "context": "The OVERNIGHT dataset (Wang et al., 2015) contains 8 different domains.", "startOffset": 22, "endOffset": 41}, {"referenceID": 32, "context": "written in \u03bb-DCS (Liang, 2013).", "startOffset": 17, "endOffset": 30}, {"referenceID": 19, "context": "(2015) use a log-linear model with a rich set of features, including paraphrase features derived from PPDB (Ganitkevitch et al., 2013), to rank logical forms.", "startOffset": 107, "endOffset": 134}, {"referenceID": 42, "context": "Wang et al. (2015) use a log-linear model with a rich set of features, including paraphrase features derived from PPDB (Ganitkevitch et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 19, "context": "(2015) use a log-linear model with a rich set of features, including paraphrase features derived from PPDB (Ganitkevitch et al., 2013), to rank logical forms. Xiao et al. (2016) use a multi-layer perceptron to encode the unigrams and bigrams of the input utterance, and then use a RNN to predict the derivation sequence of a logical form under a grammar.", "startOffset": 108, "endOffset": 178}, {"referenceID": 19, "context": "(2015) use a log-linear model with a rich set of features, including paraphrase features derived from PPDB (Ganitkevitch et al., 2013), to rank logical forms. Xiao et al. (2016) use a multi-layer perceptron to encode the unigrams and bigrams of the input utterance, and then use a RNN to predict the derivation sequence of a logical form under a grammar. Similar to ours, Jia and Liang (2016) also use a Seq2Seq model with bi-directional RNN encoder and attentive decoder, but it is used to predict linearized logical forms.", "startOffset": 108, "endOffset": 393}, {"referenceID": 24, "context": "In parallel of this work, Herzig and Berant (2017) have explored another direction of cross-domain training: they use all of the do-", "startOffset": 26, "endOffset": 51}, {"referenceID": 28, "context": "the same as Jia and Liang (2016). It is the current best-performing method on the OVERNIGHT dataset.", "startOffset": 12, "endOffset": 33}, {"referenceID": 42, "context": "Previous Methods Wang et al. (2015) 74.", "startOffset": 17, "endOffset": 36}, {"referenceID": 42, "context": "Previous Methods Wang et al. (2015) 74.4 41.9 54.0 75.9 59.0 70.8 48.2 46.3 58.8 Xiao et al. (2016) 75.", "startOffset": 17, "endOffset": 100}, {"referenceID": 27, "context": "7 Jia and Liang (2016) 78.", "startOffset": 2, "endOffset": 23}, {"referenceID": 24, "context": "8 Herzig and Berant (2017) 82.", "startOffset": 2, "endOffset": 27}, {"referenceID": 0, "context": "Our model is implemented in Tensorflow (Abadi et al., 2016), and the code can be found at https://github.", "startOffset": 39, "endOffset": 59}, {"referenceID": 28, "context": "Our base model (Random + I) achieves an accuracy comparable to the previous best in-domain model (Jia and Liang, 2016).", "startOffset": 97, "endOffset": 118}, {"referenceID": 19, "context": "In addition to pre-trained word embeddings, other language resources like paraphrase corpora (Ganitkevitch et al., 2013) can be incorporated into the paraphrase model to further facilitate domain adaptation.", "startOffset": 93, "endOffset": 120}], "year": 2017, "abstractText": "Existing studies on semantic parsing mainly focus on the in-domain setting. We formulate cross-domain semantic parsing as a domain adaptation problem: train a semantic parser on some source domains and then adapt it to the target domain. Due to the diversity of logical forms in different domains, this problem presents unique and intriguing challenges. By converting logical forms into canonical utterances in natural language, we reduce semantic parsing to paraphrasing, and develop an attentive sequence-to-sequence paraphrase model that is general and flexible to adapt to different domains. We discover two problems, small micro variance and large macro variance, of pretrained word embeddings that hinder their direct use in neural networks, and propose standardization techniques as a remedy. On the popular OVERNIGHT dataset, which contains eight domains, we show that both cross-domain training and standardized pre-trained word embedding can bring significant improvement.", "creator": "LaTeX with hyperref package"}}}