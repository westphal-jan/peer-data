{"id": "1704.06104", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2017", "title": "Neural End-to-End Learning for Computational Argumentation Mining", "abstract": "We investigate neural techniques for end-to-end computational argumentation mining. We frame the problem as a token-based dependency parsing as well as a token-based sequence tagging model, including a multi-task learning setup. Contrary to models that operate on the argument component level, we find that framing the problem as dependency parsing leads to subpar performance results. In contrast, less complex (local) tagging models based on BiLSTMs perform robustly across classification scenarios, being able to catch long-range dependencies inherent to the argumentation mining problem. Moreover, we find that jointly learning 'natural' subtasks, in a multi-task learning setup, improves performance.", "histories": [["v1", "Thu, 20 Apr 2017 12:20:43 GMT  (62kb)", "https://arxiv.org/abs/1704.06104v1", "To be published at ACL 2017"], ["v2", "Sat, 22 Apr 2017 12:20:45 GMT  (59kb)", "http://arxiv.org/abs/1704.06104v2", "To be published at ACL 2017"]], "COMMENTS": "To be published at ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["steffen eger", "johannes daxenberger", "iryna gurevych"], "accepted": true, "id": "1704.06104"}, "pdf": {"name": "1704.06104.pdf", "metadata": {"source": "CRF", "title": "Neural End-to-End Learning for Computational Argumentation Mining", "authors": ["Steffen Eger", "Johannes Daxenberger", "Iryna Gurevych"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 4.06 104v 2 [cs.C L] 22 Apr 201 7We study neural techniques for endless computational argumentation mining (AM). We design AM both as a token-based dependency sparsing and as a token-based sequence tagging problem, including multi-task learning setups. In contrast to models that operate at the level of argument components, we find that framing AM as dependency sparing results in below-average performance results. In contrast, less complex (local) tagging models based on BiLSTMs function robustly across classification scenarios and can absorb far-reaching dependencies inherent in the AM problem. Furthermore, we find that learning \"natural\" subtasks together in a multi-task learning setup improves performance."}, {"heading": "1 Introduction", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to achieve our goals, and that we are able to achieve them."}, {"heading": "2 Related Work", "text": "AM has applications in the field of legal decision-making (Palau and Moens, 2009; Moens et al., 2007), document summary and analysis of scientific papers (Kirschner et al., 2015), and its importance for education has been highlighted by recent work on writing aids (Zhang et al., 2016) and essay evaluation (Persing and Ng, 2015; Somasundaran et al., 2016). Most work on AM addresses sub-tasks of AM, such as localizing / classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015). Relatively few papers address the complete AM problem of component and relationship identification. Peldszus and Stede (2016) present a corpus of microtexts containing only argumentatively relevant text of controlled complexity."}, {"heading": "3 Data", "text": "We use the data set of persuasive essays (PE) by Staff and Gurevych (2017), which contains student essays written in response to controversial topics such as \"Competition or Cooperation - which is better?\" As Table 1 Details, the corpus consists of 402 essays, 80 of which are reserved for examination. The an-1Scripts that document how we conducted our experiments are from https: / / github.com / UKPLab / acl2017-neural _ end2end _ AM.notation distinguishes between important claims (the central position of an author in relation to the topic of the essay), claims (controversial statements that are either for or against the large claims), and premises that give grounds for claims or other premises and either support or challenge them. In total, there are 751 large claims, 1506 claims, and 3832 premises. There are 5338 relationships, most of which support relationships."}, {"heading": "4 Models", "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "5 Experiments", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "Stability Analysis", "text": "Table 4 shows averages and standard deviations of two selected models, namely the STagBLCC tagging framework and the LSTM parser, over several different runs (different random initializations and different hyperparameters, as described in the supplementary material).These results show that the taggers exhibit lower standard deviations than the parsers. Particularly striking is the difference at the essay level, where the parsers often fail to learn, i.e. their performance values are close to 0%. As discussed above, we attribute this to the increased model capacity of the parsers compared to the taggers, making them more susceptible to overfitting. Data shortages are another very likely source of error in this context, as the parsers observe only 322 (albeit very rich) trees in the training data, while the taggers are always roughly trained for 120K tokens. At paragraph level, they observe more trees, namely 1786."}, {"heading": "Error analysis", "text": "For example, the ILP system predicts the following premise: \"As a practical embodiment, students should be prepared to present themselves to society after graduation,\" while the gold premise omits the preceding discourse marker and thus states: \"Students should be prepared to present themselves to society after graduation.\" On the other hand, our results in Table 2 show that the neural taggers BLCC and BLC (in the LSTMER model) are much better at accurately identifying such entity boundaries than either the ILP model or the neural parsers. While the problems of the parsers are most likely due to the complexity of the model, we assume that the increased error rates of the ILP model are due to a weaker underlying tagging (CRF vs. Biural Parser, 2016; BLM 4 and STIB) and the fact that both (macroSTIB-STIB and STIR-C) are higher in comparison to the macroeconomic model."}, {"heading": "6 Conclusion", "text": "We present the first study on neural end-to-end AM. We experimented with different framings, 4The BIO tagging task is independent and therefore not affected by the ILP constraints in the model of Stab and Gurevych (2017). Same goes for the model of Persing and Ng (2016).5Denoted FscoreM in Sokolova and Lapalme (2009).such coding AM as a dependency parsing problem, as a sequence tagging problem with particular label set, as multi-task tagging problem, and as a problem with both sequential and tree structure information. We show that (1) neural computational AM is as good or (substantial) better than a competitive feature-based ILP formulation, while eliminating the need for manual feature engineering and expensive ILP constraint design and as optimal."}, {"heading": "Acknowledgments", "text": "We would like to thank Lucie Flekova, Judith Eckle-Kohler, Nils Reimers and Christian Stab for their valuable feedback and discussions, as well as the anonymous reviewers for their suggestions. The second author was supported by the Federal Ministry of Education and Research (BMBF) under the funding code 01UG1416B (CEDIFOR)."}, {"heading": "Supplementary Material", "text": "In fact, it is the case that one sees oneself in a position to go to another world, in which one can go to another world, in which one must go to another world, in which one can go to another world, in which one must go to another world, in which one must go to another world, in which one must go to another world, in which one goes to another world, in which one goes to another world, in which one feels oneself transported back to another world."}, {"heading": "Links to code used", "text": "We used the following code for our experiments: BLCC (https: / / github.com / XuezheMax / LasagneNLP); MTL BL (https: / / bitbucket.org / soegaard / mtlcnn / src); LSTM-ER (https: / / github.com / tticoin / LSTM-ER); LSTM parser (https: / / github.com / clab / lstmparser); Kiperwasser parser (https: / / github.com / elikip / bist-parser); Mate parser (https: / / code.google.com / archive / p / matetools / wikis / ParserAndModels.wiki); MST parser (http: / / www.seas.upenn.edu / strctlrn / MSTParser / MSTParser.html)."}, {"heading": "Error Analysis", "text": "We perform some other error analyses, focusing on the three best models ILP, LSTM-ER and STagBLCC.Which component types are particularly difficult to detect? Table 6 examines the F1 values for component segmentation + classification. In this case, there are seven classes: {B, I} \u00d7 {C, MC, P}.We note that the O class is particularly simple, as is I-P. These two are the most common names in the data and are therefore most robustly estimated. While all systems have more difficulty predicting the beginning of a claim than in continuing it (often due to the difficulty of including or omitting discourse markers as shown above), important claims follow a reverse trend. Further analyses show that claims are often mistaken for premises and vice versa, and important claims for claims or - to a lesser extent - for premises are wrong."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyunghyunCho", "Yoshua Bengio."], "venue": "Proc. ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer."], "venue": "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in", "citeRegEx": "Bengio et al\\.,? 2015", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Random search for hyper-parameter optimization", "author": ["James Bergstra", "Yoshua Bengio."], "venue": "J. Mach. Learn. Res. 13:281\u2013305.", "citeRegEx": "Bergstra and Bengio.,? 2012", "shortCiteRegEx": "Bergstra and Bengio.", "year": 2012}, {"title": "Identifying justifications in written dialogs", "author": ["Or Biran", "Owen Rambow."], "venue": "Fifth IEEE International Conference on Semantic Computing (ICSC). pages 162\u2013168.", "citeRegEx": "Biran and Rambow.,? 2011", "shortCiteRegEx": "Biran and Rambow.", "year": 2011}, {"title": "A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing", "author": ["Bernd Bohnet", "Joakim Nivre."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and", "citeRegEx": "Bohnet and Nivre.,? 2012", "shortCiteRegEx": "Bohnet and Nivre.", "year": 2012}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th International Conference on Machine Learning. ACM, New", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Argument extraction for supporting public policy formulation", "author": ["Eirini Florou", "Stasinos Konstantopoulos", "Antonis Koukourikos", "Pythagoras Karampiperis."], "venue": "Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social", "citeRegEx": "Florou et al\\.,? 2013", "shortCiteRegEx": "Florou et al\\.", "year": 2013}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "End-to-end sequence labeling via bi-directional lstm-cnns-crf", "author": ["Xuezhe Ma", "Eduard Hovy."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational", "citeRegEx": "Ma and Hovy.,? 2016", "shortCiteRegEx": "Ma and Hovy.", "year": 2016}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["Ryan McDonald", "Fernando Pereira", "Kiril Ribarov", "Jan Haji\u010d."], "venue": "Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Pro-", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "End-to-end relation extraction using lstms on sequences and tree structures", "author": ["Makoto Miwa", "Mohit Bansal."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Compu-", "citeRegEx": "Miwa and Bansal.,? 2016", "shortCiteRegEx": "Miwa and Bansal.", "year": 2016}, {"title": "Automatic detection of arguments in legal texts", "author": ["Marie-Francine Moens", "Erik Boiy", "Raquel Mochales Palau", "Chris Reed."], "venue": "Proceedings of the 11th International Conference on Artificial Intelligence and Law. ACM, New", "citeRegEx": "Moens et al\\.,? 2007", "shortCiteRegEx": "Moens et al\\.", "year": 2007}, {"title": "Constrained decoding for text-level discourse parsing", "author": ["Philippe Muller", "Stergos D. Afantenos", "Pascal Denis", "Nicholas Asher."], "venue": "COLING 2012, 24th International Conference on Computational Linguistics, Proceedings of the Conference:", "citeRegEx": "Muller et al\\.,? 2012", "shortCiteRegEx": "Muller et al\\.", "year": 2012}, {"title": "Efficient higher-order CRFs for morphological tagging", "author": ["Thomas M\u00fcller", "Helmut Schmid", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguis-", "citeRegEx": "M\u00fcller et al\\.,? 2013", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2013}, {"title": "Argumentation mining: The detection, classification and structure of arguments in text", "author": ["Raquel Mochales Palau", "Marie-Francine Moens."], "venue": "Proceedings of the 12th International Conference on Artificial Intelligence and Law. ACM,", "citeRegEx": "Palau and Moens.,? 2009", "shortCiteRegEx": "Palau and Moens.", "year": 2009}, {"title": "Joint prediction in mst-style discourse parsing for argumentation mining", "author": ["Andreas Peldszus", "Manfred Stede."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computa-", "citeRegEx": "Peldszus and Stede.,? 2015", "shortCiteRegEx": "Peldszus and Stede.", "year": 2015}, {"title": "An annotated corpus of argumentative microtexts", "author": ["Andreas Peldszus", "Manfred Stede."], "venue": "Argumentation and Reasoned Action: Proceedings of the 1st European Conference on Argumentation. Lisabon, pages 801\u2013815.", "citeRegEx": "Peldszus and Stede.,? 2016", "shortCiteRegEx": "Peldszus and Stede.", "year": 2016}, {"title": "Multi-task multi-domain representation learning for sequence tagging", "author": ["Nanyun Peng", "Mark Dredze."], "venue": "CoRR abs/1608.02689. http://arxiv.org/abs/1608.02689.", "citeRegEx": "Peng and Dredze.,? 2016", "shortCiteRegEx": "Peng and Dredze.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa-", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Modeling argument strength in student essays", "author": ["Isaac Persing", "Vincent Ng."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing", "citeRegEx": "Persing and Ng.,? 2015", "shortCiteRegEx": "Persing and Ng.", "year": 2015}, {"title": "End-to-end argumentation mining in student essays", "author": ["Isaac Persing", "Vincent Ng."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Persing and Ng.,? 2016", "shortCiteRegEx": "Persing and Ng.", "year": 2016}, {"title": "Here\u2019s my point: Argumentation Mining with Pointer Networks", "author": ["Peter Potash", "Alexey Romanov", "Anna Rumshisky."], "venue": "Arxiv preprint https://arxiv.org/abs/1612.08994 .", "citeRegEx": "Potash et al\\.,? 2016", "shortCiteRegEx": "Potash et al\\.", "year": 2016}, {"title": "Language resources for studying argument", "author": ["Chris Reed", "Raquel Mochales-Palau", "Glenn Rowe", "Marie-Francine Moens."], "venue": "Proceedings of the Sixth International Conference on Language Resources and Evaluation. Marrakech, Morocco, LREC \u201908,", "citeRegEx": "Reed et al\\.,? 2008", "shortCiteRegEx": "Reed et al\\.", "year": 2008}, {"title": "Show me your evidence - an automatic method for context dependent evidence detection", "author": ["Ruty Rinott", "Lena Dankin", "Carlos Alzate Perez", "Mitesh M. Khapra", "Ehud Aharoni", "Noam Slonim."], "venue": "Proceedings of the 2015 Conference on Em-", "citeRegEx": "Rinott et al\\.,? 2015", "shortCiteRegEx": "Rinott et al\\.", "year": 2015}, {"title": "Applying kernel methods to argumentationmining", "author": ["N. Rooney", "H. Wang", "F. Browne."], "venue": "TwentyFifth International FLAIRS Conference.", "citeRegEx": "Rooney et al\\.,? 2012", "shortCiteRegEx": "Rooney et al\\.", "year": 2012}, {"title": "Progressive neural networks", "author": ["Andrei A. Rusu", "Neil C. Rabinowitz", "Guillaume Desjardins", "Hubert Soyer", "James Kirkpatrick", "Koray Kavukcuoglu", "Razvan Pascanu", "Raia Hadsell."], "venue": "arXiv preprint arXiv:1606.04671 .", "citeRegEx": "Rusu et al\\.,? 2016", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Deep multi-task learning with low level tasks supervised at lower layers", "author": ["Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for", "citeRegEx": "S\u00f8gaard and Goldberg.,? 2016", "shortCiteRegEx": "S\u00f8gaard and Goldberg.", "year": 2016}, {"title": "A systematic analysis of performance measures for classification tasks", "author": ["Marina Sokolova", "Guy Lapalme."], "venue": "Information Processing & Management 45(4):427\u2013437. https://doi.org/10.1016/j.ipm.2009.03.002.", "citeRegEx": "Sokolova and Lapalme.,? 2009", "shortCiteRegEx": "Sokolova and Lapalme.", "year": 2009}, {"title": "Evaluating argumentative and narrative essays using graphs", "author": ["Swapna Somasundaran", "Brian Riordan", "Binod Gyawali", "Su-Youn Yoon."], "venue": "COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the", "citeRegEx": "Somasundaran et al\\.,? 2016", "shortCiteRegEx": "Somasundaran et al\\.", "year": 2016}, {"title": "Parsing argumentation structures in persuasive essays", "author": ["Christian Stab", "Iryna Gurevych."], "venue": "Computational Linguistics (in press), preprint: http://arxiv.org/abs/1604.07370).", "citeRegEx": "Stab and Gurevych.,? 2017", "shortCiteRegEx": "Stab and Gurevych.", "year": 2017}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, Curran Associates, Inc., pages", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Efficient hyper-parameter optimization for NLP applications", "author": ["Lidan Wang", "Minwei Feng", "Bowen Zhou", "Bing Xiang", "Sridhar Mahadevan."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Joint inference for fine-grained opinion extraction", "author": ["Bishan Yang", "Claire Cardie."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational", "citeRegEx": "Yang and Cardie.,? 2013", "shortCiteRegEx": "Yang and Cardie.", "year": 2013}, {"title": "Multi-task cross-lingual sequence tagging from scratch", "author": ["Zhilin Yang", "Ruslan Salakhutdinov", "William W. Cohen."], "venue": "CoRR abs/1603.06270.", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Using context to predict the purpose of argumentative writing revisions", "author": ["Fan Zhang", "Diane J. Litman."], "venue": "The Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pages", "citeRegEx": "Zhang and Litman.,? 2016", "shortCiteRegEx": "Zhang and Litman.", "year": 2016}, {"title": "Dependency parsing as head selection", "author": ["Xingxing Zhang", "Jianpeng Cheng", "Mirella Lapata."], "venue": "Proceedings of EACL 2017 (long papers). Association for Computational Linguistics.", "citeRegEx": "Zhang et al\\.,? 2017", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}, {"title": "2016), and the \u201cstruc", "author": ["Komninos", "Manandhar"], "venue": null, "citeRegEx": "Komninos and Manandhar,? \\Q2016\\E", "shortCiteRegEx": "Komninos and Manandhar", "year": 2016}], "referenceMentions": [{"referenceID": 22, "context": "tween argument components; (d) classifying relations into classes such as \u201cSupport\u201d or \u201cAttack\u201d (Persing and Ng, 2016; Stab and Gurevych, 2017).", "startOffset": 96, "endOffset": 143}, {"referenceID": 31, "context": "tween argument components; (d) classifying relations into classes such as \u201cSupport\u201d or \u201cAttack\u201d (Persing and Ng, 2016; Stab and Gurevych, 2017).", "startOffset": 96, "endOffset": 143}, {"referenceID": 21, "context": "Two recent approaches to this end-to-end learning scenario are Persing and Ng (2016) and Stab and Gurevych (2017).", "startOffset": 63, "endOffset": 85}, {"referenceID": 21, "context": "Two recent approaches to this end-to-end learning scenario are Persing and Ng (2016) and Stab and Gurevych (2017). Both solve the end-to-end task by first training independent models for each subtask and then defining an integer linear programming (ILP) model that encodes global constraints such as that each premise has a parent, etc.", "startOffset": 63, "endOffset": 114}, {"referenceID": 24, "context": "Hand-crafted features pose a problem because AM is to some degree an \u201carbitrary\u201d problem in that the notion of \u201cargument\u201d critically relies on the underlying argumentation theory (Reed et al., 2008; Biran and Rambow, 2011; Habernal and Gurevych, 2015; Stab and Gurevych, 2017).", "startOffset": 179, "endOffset": 276}, {"referenceID": 3, "context": "Hand-crafted features pose a problem because AM is to some degree an \u201carbitrary\u201d problem in that the notion of \u201cargument\u201d critically relies on the underlying argumentation theory (Reed et al., 2008; Biran and Rambow, 2011; Habernal and Gurevych, 2015; Stab and Gurevych, 2017).", "startOffset": 179, "endOffset": 276}, {"referenceID": 31, "context": "Hand-crafted features pose a problem because AM is to some degree an \u201carbitrary\u201d problem in that the notion of \u201cargument\u201d critically relies on the underlying argumentation theory (Reed et al., 2008; Biran and Rambow, 2011; Habernal and Gurevych, 2015; Stab and Gurevych, 2017).", "startOffset": 179, "endOffset": 276}, {"referenceID": 14, "context": "Hence, it is not surprising that \u2018discourse parsing\u2019 (Muller et al., 2012) has been suggested for AM (Peldszus and Stede, 2015).", "startOffset": 53, "endOffset": 74}, {"referenceID": 17, "context": ", 2012) has been suggested for AM (Peldszus and Stede, 2015).", "startOffset": 34, "endOffset": 60}, {"referenceID": 6, "context": "Third, we frame AM as a multi-task (tagging) problem (Caruana, 1997; Collobert and Weston, 2008).", "startOffset": 53, "endOffset": 96}, {"referenceID": 12, "context": "Fourth, we evaluate the model of Miwa and Bansal (2016) that combines sequential (entity) and tree structure (relation) information and is in principle applicable to any problem where the aim is to extract entities and their relations.", "startOffset": 33, "endOffset": 56}, {"referenceID": 16, "context": "AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), doc-", "startOffset": 45, "endOffset": 88}, {"referenceID": 13, "context": "AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), doc-", "startOffset": 45, "endOffset": 88}, {"referenceID": 36, "context": "Its importance for the educational domain has been highlighted by recent work on writing assistance (Zhang and Litman, 2016) and essay scoring (Persing and Ng, 2015; Somasundaran et al.", "startOffset": 100, "endOffset": 124}, {"referenceID": 21, "context": "Its importance for the educational domain has been highlighted by recent work on writing assistance (Zhang and Litman, 2016) and essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016).", "startOffset": 143, "endOffset": 192}, {"referenceID": 30, "context": "Its importance for the educational domain has been highlighted by recent work on writing assistance (Zhang and Litman, 2016) and essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016).", "startOffset": 143, "endOffset": 192}, {"referenceID": 8, "context": "Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015).", "startOffset": 80, "endOffset": 203}, {"referenceID": 13, "context": "Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015).", "startOffset": 80, "endOffset": 203}, {"referenceID": 26, "context": "Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015).", "startOffset": 80, "endOffset": 203}, {"referenceID": 25, "context": "Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015).", "startOffset": 80, "endOffset": 203}, {"referenceID": 8, "context": "Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015). Relatively few works address the full AM problem of component and relation identification. Peldszus and Stede (2016) present a corpus of microtexts containing only argumentatively relevant text of controlled complexity.", "startOffset": 81, "endOffset": 322}, {"referenceID": 8, "context": "Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015). Relatively few works address the full AM problem of component and relation identification. Peldszus and Stede (2016) present a corpus of microtexts containing only argumentatively relevant text of controlled complexity. To our best knowledge, Stab and Gurevych (2017) created the only corpus of attested high quality which annotates the AM problem in its entire complexity: it contains token-level annotations of components, their types, as well as relations and their types.", "startOffset": 81, "endOffset": 473}, {"referenceID": 10, "context": "The model of Ma and Hovy (2016) adds convolutional neural nets (CNNs) on the character-level to BiLSTMCRFs, leading to BiLSTM-CRF-CNN (BLCC) models.", "startOffset": 13, "endOffset": 32}, {"referenceID": 28, "context": "jointly (S\u00f8gaard and Goldberg, 2016; Peng and Dredze, 2016; Yang et al., 2016; Rusu et al., 2016; H\u00e9ctor and Plank, 2017).", "startOffset": 8, "endOffset": 121}, {"referenceID": 19, "context": "jointly (S\u00f8gaard and Goldberg, 2016; Peng and Dredze, 2016; Yang et al., 2016; Rusu et al., 2016; H\u00e9ctor and Plank, 2017).", "startOffset": 8, "endOffset": 121}, {"referenceID": 35, "context": "jointly (S\u00f8gaard and Goldberg, 2016; Peng and Dredze, 2016; Yang et al., 2016; Rusu et al., 2016; H\u00e9ctor and Plank, 2017).", "startOffset": 8, "endOffset": 121}, {"referenceID": 27, "context": "jointly (S\u00f8gaard and Goldberg, 2016; Peng and Dredze, 2016; Yang et al., 2016; Rusu et al., 2016; H\u00e9ctor and Plank, 2017).", "startOffset": 8, "endOffset": 121}, {"referenceID": 28, "context": "In the MTL framework of S\u00f8gaard and Goldberg (2016) the underlying model is a BiLSTM with several hidden layers.", "startOffset": 24, "endOffset": 52}, {"referenceID": 5, "context": "Instead, they rely, for example, on encoding the core features of parsers as low-dimensional embedding vectors (Chen and Manning, 2014) but ignore feature combinations.", "startOffset": 111, "endOffset": 135}, {"referenceID": 9, "context": "of word on top of the stack\u201d, and conjunctions of core features such as \u201cword is X and POS is Y\u201d (see McDonald et al. (2005)).", "startOffset": 102, "endOffset": 125}, {"referenceID": 5, "context": "Instead, they rely, for example, on encoding the core features of parsers as low-dimensional embedding vectors (Chen and Manning, 2014) but ignore feature combinations. Kiperwasser and Goldberg (2016) design a neural parser that uses only four features: the BiLSTM vector representations of the top 3 items on the stack and the first item on the buffer.", "startOffset": 112, "endOffset": 201}, {"referenceID": 5, "context": "Instead, they rely, for example, on encoding the core features of parsers as low-dimensional embedding vectors (Chen and Manning, 2014) but ignore feature combinations. Kiperwasser and Goldberg (2016) design a neural parser that uses only four features: the BiLSTM vector representations of the top 3 items on the stack and the first item on the buffer. In contrast, Dyer et al. (2015)\u2019s neural parser associates each stack with a \u201cstack LSTM\u201d that encodes their contents.", "startOffset": 112, "endOffset": 386}, {"referenceID": 12, "context": "LSTM-ER Miwa and Bansal (2016) present a", "startOffset": 8, "endOffset": 31}, {"referenceID": 1, "context": "In addition to de-coupling entity and relation detection but jointly modeling them, pretraining on entities and scheduled sampling (Bengio et al., 2015) is applied to prevent low performance at early training stages of entity detection and relation classification.", "startOffset": 131, "endOffset": 152}, {"referenceID": 31, "context": "We use the feature-based ILP model from Stab and Gurevych (2017) as a comparison system.", "startOffset": 40, "endOffset": 65}, {"referenceID": 21, "context": "Stab and Gurevych (2017) and Persing and Ng (2016).", "startOffset": 29, "endOffset": 51}, {"referenceID": 12, "context": "Still, a joint model like that of Miwa and Bansal (2016) de-couples the two tasks in such a way that many model parameters are shared across the tasks, similarly as in MTL.", "startOffset": 34, "endOffset": 57}, {"referenceID": 37, "context": "It has frequently been observed that models tend to produce output consistent with constraints in their training data in such situations (Zhang et al., 2017; H\u00e9ctor and Plank, 2017); thus, they have learned the constraints.", "startOffset": 137, "endOffset": 181}, {"referenceID": 21, "context": "Evaluation Metric We adopt the evaluation metric suggested in Persing and Ng (2016). This computes true positives TP, false positives FP, and", "startOffset": 62, "endOffset": 84}, {"referenceID": 21, "context": "For space reasons, we refer to Persing and Ng (2016) for specifics, but to illustrate, for components, true positives are defined as the set of components in the gold standard for which there exists a predicted component with the same type that \u2018matches\u2019.", "startOffset": 31, "endOffset": 53}, {"referenceID": 21, "context": "For space reasons, we refer to Persing and Ng (2016) for specifics, but to illustrate, for components, true positives are defined as the set of components in the gold standard for which there exists a predicted component with the same type that \u2018matches\u2019. Persing and Ng (2016) define a notion of what we may term \u2018level \u03b1 matching\u2019: for example, at the 100% level (exact match) predicted and gold components must have exactly the same spans, whereas at the 50% level they must only share at least 50% of their tokens (approximate match).", "startOffset": 31, "endOffset": 278}, {"referenceID": 11, "context": "(a) Dependency Parsing We show results for the two feature-based parsers MST (McDonald et al., 2005), Mate (Bohnet and Nivre, 2012) as well as the neural parsers by Dyer et al.", "startOffset": 77, "endOffset": 100}, {"referenceID": 4, "context": ", 2005), Mate (Bohnet and Nivre, 2012) as well as the neural parsers by Dyer et al.", "startOffset": 14, "endOffset": 38}, {"referenceID": 4, "context": ", 2005), Mate (Bohnet and Nivre, 2012) as well as the neural parsers by Dyer et al. (2015) (LSTM-Parser) and Kiperwasser and Goldberg (2016) (Kiperwasser).", "startOffset": 15, "endOffset": 91}, {"referenceID": 4, "context": ", 2005), Mate (Bohnet and Nivre, 2012) as well as the neural parsers by Dyer et al. (2015) (LSTM-Parser) and Kiperwasser and Goldberg (2016) (Kiperwasser).", "startOffset": 15, "endOffset": 141}, {"referenceID": 10, "context": "we use the BLCC tagger from Ma and Hovy (2016) and refer to the resulting system as", "startOffset": 28, "endOffset": 47}, {"referenceID": 28, "context": "(c) MTL As indicated, we use the MTL tagging framework from S\u00f8gaard and Goldberg (2016) for multi-task experiments.", "startOffset": 60, "endOffset": 88}, {"referenceID": 15, "context": "This indicates that complex sequence tagging may benefit when we train a \u201csublabeler\u201d in the same neural architecture, a finding that may be particularly relevant for morphological POS tagging (M\u00fcller et al., 2013).", "startOffset": 193, "endOffset": 214}, {"referenceID": 15, "context": "This indicates that complex sequence tagging may benefit when we train a \u201csublabeler\u201d in the same neural architecture, a finding that may be particularly relevant for morphological POS tagging (M\u00fcller et al., 2013). Unlike S\u00f8gaard and Goldberg (2016), we do not find that the optimal architecture is the one in which \u201clower\u201d tasks (such as C or R) feed from lower layers.", "startOffset": 194, "endOffset": 251}, {"referenceID": 22, "context": "On the one hand, it has been observed that even humans have problems exactly identifying such entity boundaries (Persing and Ng, 2016; Yang and Cardie, 2013).", "startOffset": 112, "endOffset": 157}, {"referenceID": 34, "context": "On the one hand, it has been observed that even humans have problems exactly identifying such entity boundaries (Persing and Ng, 2016; Yang and Cardie, 2013).", "startOffset": 112, "endOffset": 157}, {"referenceID": 31, "context": "6% in Stab and Gurevych (2017).", "startOffset": 6, "endOffset": 31}, {"referenceID": 28, "context": "The BIO tagging task is independent and thus not affected by the ILP constraints in the model of Stab and Gurevych (2017). The same holds true for the model of Persing and Ng (2016).", "startOffset": 97, "endOffset": 122}, {"referenceID": 21, "context": "The same holds true for the model of Persing and Ng (2016). Denoted FscoreM in Sokolova and Lapalme (2009).", "startOffset": 37, "endOffset": 59}, {"referenceID": 21, "context": "The same holds true for the model of Persing and Ng (2016). Denoted FscoreM in Sokolova and Lapalme (2009). STagBLCC LSTM-ER ILP HUB", "startOffset": 37, "endOffset": 107}, {"referenceID": 31, "context": "Our work yields new state-of-the-art results in end-to-end AMon the PE dataset from Stab and Gurevych (2017).", "startOffset": 84, "endOffset": 109}, {"referenceID": 0, "context": "Another possible framing, not considered here, is to frame AM as an encoder-decoder problem (Bahdanau et al., 2015; Vinyals et al., 2015).", "startOffset": 92, "endOffset": 137}, {"referenceID": 32, "context": "Another possible framing, not considered here, is to frame AM as an encoder-decoder problem (Bahdanau et al., 2015; Vinyals et al., 2015).", "startOffset": 92, "endOffset": 137}, {"referenceID": 23, "context": "ponent classification and relation identification has been investigated in Potash et al. (2016).", "startOffset": 75, "endOffset": 96}], "year": 2017, "abstractText": "We investigate neural techniques for endto-end computational argumentation mining (AM). We frame AM both as a tokenbased dependency parsing and as a tokenbased sequence tagging problem, including a multi-task learning setup. Contrary to models that operate on the argument component level, we find that framing AM as dependency parsing leads to subpar performance results. In contrast, less complex (local) tagging models based on BiLSTMs perform robustly across classification scenarios, being able to catch longrange dependencies inherent to the AM problem. Moreover, we find that jointly learning \u2018natural\u2019 subtasks, in a multi-task learning setup, improves performance.", "creator": "gnuplot 5.0 patchlevel 3"}}}