{"id": "1704.06393", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2017", "title": "Neural System Combination for Machine Translation", "abstract": "Neural machine translation (NMT) becomes a new approach to machine translation and generates much more fluent results compared to statistical machine translation (SMT).", "histories": [["v1", "Fri, 21 Apr 2017 04:36:55 GMT  (211kb,D)", "http://arxiv.org/abs/1704.06393v1", "Accepted as a short paper by ACL-2017"]], "COMMENTS": "Accepted as a short paper by ACL-2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["long zhou", "wenpeng hu", "jiajun zhang", "chengqing zong"], "accepted": true, "id": "1704.06393"}, "pdf": {"name": "1704.06393.pdf", "metadata": {"source": "CRF", "title": "Neural System Combination for Machine Translation", "authors": ["Long Zhou", "Wenpeng Hu", "Jiajun Zhang", "Chengqing Zong"], "emails": ["long.zhou@nlpr.ia.ac.cn", "wenpeng.hu@nlpr.ia.ac.cn", "jjzhang@nlpr.ia.ac.cn", "cqzong@nlpr.ia.ac.cn"], "sections": [{"heading": "1 Introduction", "text": "In recent years, the quality of machine translation has improved significantly (Kalchburner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Junczys-Dowmunt et al., 2016a). Although most sentences are more fluent than statistical machine translation (SMT) (Koehn et al., 2016; Chiang, 2005), NMT has a problem with the adequacy of translation, especially for the rare and unknown words. In addition, it suffers from translation and undertranslation to some extent (Tu et al., 2016). Compared to NMT, SMT, such as phrase law-based machine translation (Koehn et al., 2003) and hierarchical systems, we are unable to translate the SMT system (HPMT)."}, {"heading": "2 Neural Machine Translation", "text": "The NMT encoder with an attention mechanism (Bahdanau et al., 2015) has been proposed to gently align each decoder state to the encoder states and calculate the conditional probability of translation taking into account the source sentence. The encoder is a bidirectional neural network with gated recurrent units (GRU) (Cho et al., 2014) that reads an input sequence X = (x1, x2,..., xm) and converts it into a sequence of hidden states H = h1, h2,..., hm.The decoder is a recursive neural network that predicts a target sequence Y = (y1, y2,..., yn). Each word yj is predicted on the basis of a recursive hidden state sj, the previously predicted word yj \u2212 1 and a context vector cj. cj from the weighted sum of annotations hi."}, {"heading": "3 Neural System Combination for Machine Translation", "text": "Macherey and Och (2007) provided empirical evidence that these systems to be combined must be almost uncorrelated in order to be advantageous for the system combination. Since NMT and SMT are two types of translation models with large differences, we are trying to develop a combination model of neural systems that can take advantage of the different systems. Model: Figure 1 illustrates the combination framework of neural systems that can use the source set and the results of the MT systems as input. Here, we use the MT results as inputs to present the model in detail. Formally, the result sequences Z (Zn, Zp and Zh) of three MT systems for the same source set and the previously generated target sequences Y < j = (y2, yj \u2212 1), the probability of the next target is yj isp (yj | Y < j, Z) = softcf (j, sj \u2212 y1)."}, {"heading": "4 Experiments", "text": "We carry out our experiments on the Chinese-English translation task. MT systems involved in the system combination are PBMT, HPMT and NMT. The evaluation metric is casein-sensitive BLEU (Papineni et al., 2002)."}, {"heading": "4.1 Data preparation", "text": "Our training data consists of 2.08M pairs of sentences extracted from the LDC corpus. For validation purposes, we use the Chinese-English data set NIST 2003, NIST 2004-2006 records as test records. We list all translation methods as follows: \u2022 PBMT: It is the phrase-based SMT system of the first hour. We use its default setting and train a 4 gram language model on the target part of the bilingual training data. \u2022 HPMT: It is a hierarchical phrase-based SMT system that uses its default configuration as a PBMT in Moses. \u2022 NMT: It is an attention-based NMT system, with the same setting as in Section 2."}, {"heading": "4.2 Training Details", "text": "The hyperparameters used in our neural combination system are described as follows: We limit both the Chinese and English vocabulary in our experiments to 30k; the number of hidden units is 1000 and the word embedding dimension is 500 for all source and target words; the network parameters are updated using the Adadelta algorithm; we use the bar size b = 10 at test time; as for the Confusion Network-based Jane system, we use its default configuration and train a 4 gram language model on target data and 10 million Xinhua portions of the gigaword corpus."}, {"heading": "4.3 Main Results", "text": "We compare our neural combination system with the best single motors and the modern traditional combination system Jane (Freitag et al., 2014). Table 1 shows the BLEU of different models for development data and test data. The BLEU value of the neural multi-source combination model is 2.53 higher than that of the best single model HPMT. The source language input results in a further improvement of + 1.12 BLEU points. As shown in Table 1, Jane exceeds the best single-source combination system by 1.92 BLEU points. However, our neural combination system with source language receives an improvement of 1.67 BLEU points over Jane. In addition, the extension of our neural combination system by ensemble decoding 2 leads to a further significant increase of + 1.69 BLEU points."}, {"heading": "4.4 Word Order of Translation", "text": "We evaluate the word order using the automatic evaluation metrics RIBES (Isozaki et al., 2010), whose value is a metric based on rank correlation coefficients with word accuracy. RIBES is known to correlate more strongly with human judgments than BLEU for English, as discussed in Isozaki et al. (2010).2We use four neural combination models in the ensemble model. Figure 2 illustrates experimental results of RIBES values, showing that our neural combination model outperforms the best result of a single MT system and Jane. Although BLEU of Jane is higher than a single NMT system, the word sequence Jane is worse in terms of RIBES."}, {"heading": "4.5 Rare and Unknown Words Translation", "text": "It is difficult for NMT systems to deal with rare words, as low-frequency words in training data cannot capture latent translation mappings in the neural network model. However, we do not need to restrict vocabulary in SMT, as they are often able to translate rare words into training data. As shown in Table 2, the number of unknown words in our proposed model 137 is lower than the original NMT model. Table 4 shows an example of system combinations. The Chinese word zuzhiwang is an obsolete vocabulary (OOV) for NMT, and the baseline NMT cannot translate this word correctly. Although PBMT and HPMT translate this word well, they do not match the grammar. By combining the merits of NMT and SMT, our model gets the correct translation."}, {"heading": "4.6 Effect of Ensemble Decoding", "text": "The performance of the candidate systems is very important for the outcome of the system combination, and we use ensemble strategy with four NMT models to improve the performance of the original NMT system. As shown in Table 3, the E-NMT with ensemble strategy outperforms the original NMT system by + 1.40 BLEU points, and it has become the best system in all MT systems, which is + 0.68 BLEU points higher than HPMT. After replacing the original NMT with a strong ENMT, Jane outperforms the original result by + 0.45 BLEU points, and our model achieves an improvement of + 3.08 BLEU points over Jane. Experiments also show that our proposed is effective and robust for the system combination."}, {"heading": "5 Related Work", "text": "Most of the existing approaches and models focus mainly on developing better attention models (Luong et al., 2015a; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016; Meng et al., 2016), better strategies for dealing with rare and unknown words (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and the integration of SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and the integration of SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., SMi-System, 2016). Our focus in this work is on exploiting the benefits of NMT and SMT by combining systems."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we propose a novel combination system for machine translation, the central idea being to exploit the advantages of NMT and SMT by adapting the multi-source NMT model; the combination method for neural systems can not only take into account the fluctuation of NMT and the adequacy of SMT, but also include source sentences as input; furthermore, our approach can continue to use ensemble deciphering to improve performance over traditional system combination methods; experiments with Sino-English data sets show that our approaches achieve significant improvements over the best single system and the most modern traditional system combination methods; and in future work, we plan to encode the best translation results to further improve system combination quality; and it is interesting to extend this approach to other tasks such as sentence compression and text abstraction."}, {"heading": "Acknowledgments", "text": "The research was funded by the Natural Science Foundation of China under grant numbers 61333018 and 61673380 and is also supported by the CAS Strategic Priority Research Program under grant number XDB02070007."}], "references": [{"title": "Improving alignments for better confusion networks for combining machine translation systems", "author": ["Necip Fazil Ayan", "Jing Zheng", "Wen Wang."], "venue": "Proceedings of COLING 2008.", "citeRegEx": "Ayan et al\\.,? 2008", "shortCiteRegEx": "Ayan et al\\.", "year": 2008}, {"title": "Nuural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of ICLR 2015.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Computing consensus translation from multiple machine translation systems", "author": ["Srinivas Bangalore", "German Bordel", "Giuseppe Richardi."], "venue": "Proceedings of IEEE ASRU.", "citeRegEx": "Bangalore et al\\.,? 2001", "shortCiteRegEx": "Bangalore et al\\.", "year": 2001}, {"title": "A comparative study of hypothesis alignment and its improvement for machine translation system combination", "author": ["Boxing Chen", "Min Zhang", "Haizhou Li", "Aiti Aw."], "venue": "Proceedings of ACL 2009.", "citeRegEx": "Chen et al\\.,? 2009", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "Semi-supervised learning for neural machine translation", "author": ["Yong Cheng", "Wei Xu", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."], "venue": "Proceedings of ACL 2016.", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang."], "venue": "Proceedings of ACL 2005.", "citeRegEx": "Chiang.,? 2005", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "Learning phrase representations using RNN encoderCdecoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Pro-", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Lattice-based system combination for statistical machine translation", "author": ["Yang Feng", "Yang Liu", "Haitao Mi", "Qun Liu", "Yajuan Lu."], "venue": "Proceedings of ACL 2009.", "citeRegEx": "Feng et al\\.,? 2009", "shortCiteRegEx": "Feng et al\\.", "year": 2009}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of NAACL-HLT 2016.", "citeRegEx": "Firat et al\\.,? 2016a", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Zero-resource translation with multi-lingual neural machine translation", "author": ["Orhan Firat", "Baskaran Sankaran", "Yaser Al-Onaizan", "Fatos T. Yarman Vural", "Kyunghyun Cho."], "venue": "Proceedings of EMNLP 2016.", "citeRegEx": "Firat et al\\.,? 2016b", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Jane: open source machine translation system combiantion", "author": ["Markus Freitag", "Matthias Huck", "Hermann Ney."], "venue": "Proceedings of EACL 2014.", "citeRegEx": "Freitag et al\\.,? 2014", "shortCiteRegEx": "Freitag et al\\.", "year": 2014}, {"title": "Ensemble learning for multi-source neural machine translation", "author": ["Ekaterina Garmash", "Christof Monz."], "venue": "Proceedings of COLING 2016.", "citeRegEx": "Garmash and Monz.,? 2016", "shortCiteRegEx": "Garmash and Monz.", "year": 2016}, {"title": "Improved neural machine translation with SMT features", "author": ["Wei He", "Zhongjun He", "Hua Wu", "Haifeng Wang."], "venue": "Proceedings of AAAI 2016.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Combining machine translation output with open source", "author": ["Kenneth Heafield", "Alon Lavie."], "venue": "The Prague Bulletin of Machematical Linguistics.", "citeRegEx": "Heafield and Lavie.,? 2010", "shortCiteRegEx": "Heafield and Lavie.", "year": 2010}, {"title": "Automatic evaluation of translation quality for distant language pairs", "author": ["Hideki Isozaki", "Tsutomu Hirao", "Kevin Duh", "Katsuhito Sudoh", "Hajime Tsukada."], "venue": "Proceedings of EMNLP 2010.", "citeRegEx": "Isozaki et al\\.,? 2010", "shortCiteRegEx": "Isozaki et al\\.", "year": 2010}, {"title": "Is neural machine translation ready for deployment? A case study on 30 translation directions", "author": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Hieu Hoang."], "venue": "Proceedings of IWSLT 2016.", "citeRegEx": "Junczys.Dowmunt et al\\.,? 2016a", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "The AMU-UEDIN submission to the WMT16 news translation task: attention-based NMT models as feature functions in phrase-based SMT", "author": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Rico Sennrich."], "venue": "Proceedings of WMT 2016.", "citeRegEx": "Junczys.Dowmunt et al\\.,? 2016b", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of EMNLP 2013.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz J. Och", "Daniel Marcu."], "venue": "Proceedings of ACL NAACL 2013.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Minimum bayes-risk decoding for statistical machine translation", "author": ["Shankar Kumar", "William Byrne."], "venue": "Proceedings of HLT-NAACL 2004.", "citeRegEx": "Kumar and Byrne.,? 2004", "shortCiteRegEx": "Kumar and Byrne.", "year": 2004}, {"title": "The CASIA statistical machine translation system for IWSLT 2009", "author": ["Maoxi Li", "Jiajun Zhang", "Yu Zhou", "Chengqing Zong."], "venue": "Proceedings of IWSLT2009.", "citeRegEx": "Li et al\\.,? 2009", "shortCiteRegEx": "Li et al\\.", "year": 2009}, {"title": "Word reordering alignment for combination of statistical machine translation systems", "author": ["Maoxi Li", "Chengqing Zong."], "venue": "Proceedings of the International Symposium on Chinese Spoken Language Processing.", "citeRegEx": "Li and Zong.,? 2008", "shortCiteRegEx": "Li and Zong.", "year": 2008}, {"title": "Towards zero unknown word in neural machine translation", "author": ["Xiaoqing Li", "Jiajun Zhang", "Chengqing Zong."], "venue": "Proceedings of IJCAI 2016.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of EMNLP 2015.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of ACL 2015.", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "System combination for machine translation through paraphrasing", "author": ["Wei-Yun Ma", "Kathleen Mckeown."], "venue": "Proceedings of EMNLP 2015.", "citeRegEx": "Ma and Mckeown.,? 2015", "shortCiteRegEx": "Ma and Mckeown.", "year": 2015}, {"title": "An empirical study on computing consensus translations from multiple machine translation systems", "author": ["Wolfgang Macherey", "Franz Josef Och."], "venue": "Proceedings of EMNLP 2007.", "citeRegEx": "Macherey and Och.,? 2007", "shortCiteRegEx": "Macherey and Och.", "year": 2007}, {"title": "Interactive attention for neural machine translation", "author": ["Fandong Meng", "Zhengdong Lu", "Hang Li", "Qun Liu."], "venue": "Proceedings of COLING 2016.", "citeRegEx": "Meng et al\\.,? 2016", "shortCiteRegEx": "Meng et al\\.", "year": 2016}, {"title": "A coverage embedding model for neural machine translation", "author": ["Haitao Mi", "Baskaran Sankaran", "Zhiguo Wang", "Niyu Ge", "Abe Ittycheriah."], "venue": "Proceedings of EMNLP 2016.", "citeRegEx": "Mi et al\\.,? 2016a", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Supervised attentions for neural machine translation", "author": ["Haitao Mi", "Zhiguo Wang", "Niyu Ge", "Abe Ittycheriah."], "venue": "Proceedings of EMNLP 2016.", "citeRegEx": "Mi et al\\.,? 2016b", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Pre-translation for neural machine translation", "author": ["Jan Niehues", "Eunah Cho", "Thanh-Le Ha", "Alex Waibel."], "venue": "Proceedings of COLING 2016.", "citeRegEx": "Niehues et al\\.,? 2016", "shortCiteRegEx": "Niehues et al\\.", "year": 2016}, {"title": "Statistical multi-source translation", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Proceedings of MT Summit.", "citeRegEx": "Och and Ney.,? 2001", "shortCiteRegEx": "Och and Ney.", "year": 2001}, {"title": "Bleu: a methof for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of ACL 2002.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Improved word-level system combination for machine translation", "author": ["Antti-Veikko I. Rosti", "Spyros Matsoukas", "Richard Schwartz."], "venue": "Proceedings of ACL 2007.", "citeRegEx": "Rosti et al\\.,? 2007", "shortCiteRegEx": "Rosti et al\\.", "year": 2007}, {"title": "Incremental hypothesis alignment for building confusion networks with appplication to machine translation systems combination", "author": ["Antti-Veikko I. Rosti", "Bing Zhang", "Spyros Matsoukas", "Richard Schwartz."], "venue": "Proceedings of the Third ACL Workshop", "citeRegEx": "Rosti et al\\.,? 2008", "shortCiteRegEx": "Rosti et al\\.", "year": 2008}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of ACL 2016.", "citeRegEx": "Sennrich et al\\.,? 2016a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of ACL 2016.", "citeRegEx": "Sennrich et al\\.,? 2016b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Minimum risk training for neural machine translation", "author": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."], "venue": "Proceedings of ACL 2016.", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Proceedings of NIPS 2014.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of ACL 2016.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Bridging neural machine translation and bilingual dictionaries", "author": ["Jiajun Zhang", "Chengqing Zong."], "venue": "arXiv preprint arXiv:1610.07272.", "citeRegEx": "Zhang and Zong.,? 2016a", "shortCiteRegEx": "Zhang and Zong.", "year": 2016}, {"title": "Exploiting source-side monolingual data in neural machine translation", "author": ["Jiajun Zhang", "Chengqing Zong"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Zhang and Zong.,? \\Q2016\\E", "shortCiteRegEx": "Zhang and Zong.", "year": 2016}, {"title": "Sentence-level paraphrasing for machine translation system combination", "author": ["Junguo Zhu", "Muyun Yang", "Sheng Li", "Tiejun Zhao."], "venue": "Proceedings of ICYCSEE 2016.", "citeRegEx": "Zhu et al\\.,? 2016", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}, {"title": "Multi-source neural translation", "author": ["Barret Zoph", "Kevin Knight."], "venue": "Proceedings of NAACL-HLT 2016.", "citeRegEx": "Zoph and Knight.,? 2016", "shortCiteRegEx": "Zoph and Knight.", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "Neural machine translation has significantly improved the quality of machine translation in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Junczys-Dowmunt et al., 2016a).", "startOffset": 113, "endOffset": 223}, {"referenceID": 38, "context": "Neural machine translation has significantly improved the quality of machine translation in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Junczys-Dowmunt et al., 2016a).", "startOffset": 113, "endOffset": 223}, {"referenceID": 1, "context": "Neural machine translation has significantly improved the quality of machine translation in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Junczys-Dowmunt et al., 2016a).", "startOffset": 113, "endOffset": 223}, {"referenceID": 15, "context": "Neural machine translation has significantly improved the quality of machine translation in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Junczys-Dowmunt et al., 2016a).", "startOffset": 113, "endOffset": 223}, {"referenceID": 18, "context": "Although most sentences are more fluent than translations by statistical machine translation (SMT) (Koehn et al., 2003; Chiang, 2005), NMT has a problem to address translation adequacy especially for the rare and unknown words.", "startOffset": 99, "endOffset": 133}, {"referenceID": 5, "context": "Although most sentences are more fluent than translations by statistical machine translation (SMT) (Koehn et al., 2003; Chiang, 2005), NMT has a problem to address translation adequacy especially for the rare and unknown words.", "startOffset": 99, "endOffset": 133}, {"referenceID": 39, "context": "Additionally, it suffers from over-translation and under-translation to some extent (Tu et al., 2016).", "startOffset": 84, "endOffset": 101}, {"referenceID": 18, "context": "Compared to NMT, SMT, such as phrase-based machine translation (PBMT, (Koehn et al., 2003)) and hierarchical phrase-based machine translation (HPMT,", "startOffset": 70, "endOffset": 90}, {"referenceID": 5, "context": "(Chiang, 2005)), does not need to limit the vocabulary and can guarantee translation coverage of source sentences.", "startOffset": 0, "endOffset": 14}, {"referenceID": 19, "context": "Traditionally, system combination has been explored respectively in sentence-level, phrase-level, and word-level (Kumar and Byrne, 2004; Feng et al., 2009; Chen et al., 2009).", "startOffset": 113, "endOffset": 174}, {"referenceID": 7, "context": "Traditionally, system combination has been explored respectively in sentence-level, phrase-level, and word-level (Kumar and Byrne, 2004; Feng et al., 2009; Chen et al., 2009).", "startOffset": 113, "endOffset": 174}, {"referenceID": 3, "context": "Traditionally, system combination has been explored respectively in sentence-level, phrase-level, and word-level (Kumar and Byrne, 2004; Feng et al., 2009; Chen et al., 2009).", "startOffset": 113, "endOffset": 174}, {"referenceID": 33, "context": "Among them, word-level combination approaches that adopt confusion network for decoding have been quite successful (Rosti et al., 2007; Ayan et al., 2008; Freitag et al., 2014).", "startOffset": 115, "endOffset": 176}, {"referenceID": 0, "context": "Among them, word-level combination approaches that adopt confusion network for decoding have been quite successful (Rosti et al., 2007; Ayan et al., 2008; Freitag et al., 2014).", "startOffset": 115, "endOffset": 176}, {"referenceID": 10, "context": "Among them, word-level combination approaches that adopt confusion network for decoding have been quite successful (Rosti et al., 2007; Ayan et al., 2008; Freitag et al., 2014).", "startOffset": 115, "endOffset": 176}, {"referenceID": 43, "context": "In this paper, we propose a neural system combination framework, which is adapted from the multi-source NMT model (Zoph and Knight, 2016).", "startOffset": 114, "endOffset": 137}, {"referenceID": 1, "context": "The encoder-decoder NMT with an attention mechanism (Bahdanau et al., 2015) has been proposed to softly align each decoder state with the encoder states, and computes the conditional probability of the translation given the source sentence.", "startOffset": 52, "endOffset": 75}, {"referenceID": 6, "context": "The encoder is a bidirectional neural network with gated recurrent units (GRU) (Cho et al., 2014) which reads an input sequence X = (x1, x2, .", "startOffset": 79, "endOffset": 97}, {"referenceID": 10, "context": "38 Jane (Freitag et al., 2014) 39.", "startOffset": 8, "endOffset": 30}, {"referenceID": 32, "context": "The evaluation metric is caseinsensitive BLEU (Papineni et al., 2002).", "startOffset": 46, "endOffset": 69}, {"referenceID": 10, "context": "We compare our neural combination system with the best individual engines, and the state-of-theart traditional combination system Jane (Freitag et al., 2014).", "startOffset": 135, "endOffset": 157}, {"referenceID": 14, "context": "We evaluate word order by the automatic evaluation metrics RIBES (Isozaki et al., 2010), whose score is a metric based on rank correlation coefficients with word precision.", "startOffset": 65, "endOffset": 87}, {"referenceID": 14, "context": "We evaluate word order by the automatic evaluation metrics RIBES (Isozaki et al., 2010), whose score is a metric based on rank correlation coefficients with word precision. RIBES is known to have stronger correlation with human judgements than BLEU for English as discussed in Isozaki et al. (2010).", "startOffset": 66, "endOffset": 299}, {"referenceID": 23, "context": "Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al.", "startOffset": 93, "endOffset": 170}, {"referenceID": 39, "context": "Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al.", "startOffset": 93, "endOffset": 170}, {"referenceID": 27, "context": "Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al.", "startOffset": 93, "endOffset": 170}, {"referenceID": 24, "context": ", 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al.", "startOffset": 63, "endOffset": 148}, {"referenceID": 22, "context": ", 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al.", "startOffset": 63, "endOffset": 148}, {"referenceID": 40, "context": ", 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al.", "startOffset": 63, "endOffset": 148}, {"referenceID": 36, "context": ", 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al.", "startOffset": 63, "endOffset": 148}, {"referenceID": 4, "context": ", 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al.", "startOffset": 51, "endOffset": 118}, {"referenceID": 35, "context": ", 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al.", "startOffset": 51, "endOffset": 118}, {"referenceID": 37, "context": ", 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016).", "startOffset": 64, "endOffset": 131}, {"referenceID": 16, "context": ", 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016).", "startOffset": 64, "endOffset": 131}, {"referenceID": 12, "context": ", 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016).", "startOffset": 64, "endOffset": 131}, {"referenceID": 2, "context": "In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et al., 2016), and reported stateof-the-art performances in benchmarks for SMT.", "startOffset": 112, "endOffset": 280}, {"referenceID": 34, "context": "In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et al., 2016), and reported stateof-the-art performances in benchmarks for SMT.", "startOffset": 112, "endOffset": 280}, {"referenceID": 21, "context": "In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et al., 2016), and reported stateof-the-art performances in benchmarks for SMT.", "startOffset": 112, "endOffset": 280}, {"referenceID": 20, "context": "In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et al., 2016), and reported stateof-the-art performances in benchmarks for SMT.", "startOffset": 112, "endOffset": 280}, {"referenceID": 13, "context": "In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et al., 2016), and reported stateof-the-art performances in benchmarks for SMT.", "startOffset": 112, "endOffset": 280}, {"referenceID": 10, "context": "In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et al., 2016), and reported stateof-the-art performances in benchmarks for SMT.", "startOffset": 112, "endOffset": 280}, {"referenceID": 25, "context": "In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et al., 2016), and reported stateof-the-art performances in benchmarks for SMT.", "startOffset": 112, "endOffset": 280}, {"referenceID": 42, "context": "In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et al., 2016), and reported stateof-the-art performances in benchmarks for SMT.", "startOffset": 112, "endOffset": 280}, {"referenceID": 31, "context": "Moreover, multi-source MT has been proved to be very effective to combine multiple source languages (Och and Ney, 2001; Zoph and Knight, 2016; Firat et al., 2016a,b; Garmash and Monz, 2016).", "startOffset": 100, "endOffset": 189}, {"referenceID": 43, "context": "Moreover, multi-source MT has been proved to be very effective to combine multiple source languages (Och and Ney, 2001; Zoph and Knight, 2016; Firat et al., 2016a,b; Garmash and Monz, 2016).", "startOffset": 100, "endOffset": 189}, {"referenceID": 11, "context": "Moreover, multi-source MT has been proved to be very effective to combine multiple source languages (Och and Ney, 2001; Zoph and Knight, 2016; Firat et al., 2016a,b; Garmash and Monz, 2016).", "startOffset": 100, "endOffset": 189}, {"referenceID": 2, "context": "In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et al., 2016), and reported stateof-the-art performances in benchmarks for SMT. Here, we propose a neural system combination model which combines the advantages of NMT and SMT efficiently. Recently, Niehues et al. (2016) use phrasebased SMT to pre-translate the inputs into target translations.", "startOffset": 113, "endOffset": 488}], "year": 2017, "abstractText": "Neural machine translation (NMT) becomes a new approach to machine translation and generates much more fluent results compared to statistical machine translation (SMT). However, SMT is usually better than NMT in translation adequacy. It is therefore a promising direction to combine the advantages of both NMT and SMT. In this paper, we propose a neural system combination framework leveraging multi-source NMT, which takes as input the outputs of NMT and SMT systems and produces the final translation. Extensive experiments on the Chineseto-English translation task show that our model archives significant improvement by 5.3 BLEU points over the best single system output and 3.4 BLEU points over the state-of-the-art traditional system combination methods.", "creator": "LaTeX with hyperref package"}}}