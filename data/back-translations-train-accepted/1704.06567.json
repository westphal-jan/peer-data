{"id": "1704.06567", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2017", "title": "Attention Strategies for Multi-Source Sequence-to-Sequence Learning", "abstract": "Modeling attention in neural multi-source sequence-to-sequence learning remains a relatively unexplored area, despite its usefulness in tasks that incorporate multiple source languages or modalities. We propose two novel approaches to combine the outputs of attention mechanisms over each source sequence, flat and hierarchical. We compare the proposed methods with existing techniques and present results of systematic evaluation of those methods on the WMT16 Multimodal Translation and Automatic Post-editing tasks. We show that the proposed methods achieve competitive results on both tasks.", "histories": [["v1", "Fri, 21 Apr 2017 14:39:27 GMT  (186kb,D)", "http://arxiv.org/abs/1704.06567v1", "7 pages; Accepted to ACL 2017"]], "COMMENTS": "7 pages; Accepted to ACL 2017", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["jindrich libovick\u00fd", "jindrich helcl"], "accepted": true, "id": "1704.06567"}, "pdf": {"name": "1704.06567.pdf", "metadata": {"source": "CRF", "title": "Attention Strategies for Multi-Source Sequence-to-Sequence Learning", "authors": ["Jind\u0159ich Libovick\u00fd"], "emails": ["helcl}@ufal.mff.cuni.cz"], "sections": [{"heading": "1 Introduction", "text": "Sequence-to-sequence (S2S) learning with attention mechanism has recently become the most successful paradigm with state-of-the-art results in machine translation (MT) (Bahdanau et al., 2014; Sennrich et al., 2016a), caption (Xu et al., 2015; Lu et al., 2016), text summary (Rush et al., 2015) and other NLP tasks. In this paper, we focus on a specific case of S2S learning with a single encoder. Depending on the modality, it may be either a recursive neural network (RNN) for textual input data or a revolutionary network for image files. In this work, we focus on a specific case of S2S learning with multiple input sequences of possibly different modalities and a single output decoder."}, {"heading": "2 Attentive S2S Learning", "text": "The attention mechanisms in S2S-Learning made it possible for an RNN decoder to access the information directly before sending out a symbol. () This distribution is used for the calculation of the context vector - the weight-dependent averages of the encoder-hidden states - as additional input to the encoder-hidden states. (2014) Dei Sorge-Model of the attention-energies eij, the attention-distribution-attention-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution-distribution"}, {"heading": "3 Attention Combination", "text": "In S2S models with multiple encoders, the decoder must be able to combine the attention information collected by the encoders. A widely used technique for combining multiple attention models into one decoder is the concatenation of the context vectors c (1) i,..., c (N) i (Zoph and Knight, 2016; Firat et al., 2016). As mentioned in Section 1, this setting forces the model to treat each encoder independently, and implicitly resolves the attention combination in subsequent network layers. In this section, we propose two alternative strategies for combining the attention of multiple encoders. Either we let the decoder learn how to distribute the affi jointly across all the hidden states of the encoder (flat attention combination), or we factorize the distribution to individual encoders (hierarchical combination). Both alternatives allow us to interpret the distribution across the encoder, as each encoder will interpret the amount of attention in each encoding step."}, {"heading": "3.1 Flat Attention Combination", "text": "The difference between the concatenation of the context vectors and the flat attention combination is that the \u03b1i coefficients are calculated jointly for all encoders: \u03b1 (k) ij = exp (e (k) ij) \u2211 Nn = 1 \u2211 T (n) x m = 1 exp (n) im (6), where T (n) x is the length of the input sequence of the n-th encoder and e (k) ij is the attention energy of the j-th state of the k-th encoder in the i-th decoding step. These attention energies are calculated as in Equation 1. The parameters va and Wa are divided among the encoders, and Ua is different for each encoder and serves as the encoder specific hidden projection."}, {"heading": "3.2 Hierarchical Attention Combination", "text": "The hierarchical attention combination model calculates each context vector independently of each other, similar to the concatenation approach. Instead of concatenating, a second attention mechanism is constructed using the context vectors. We divide the calculation of attention distribution into two steps: First, we calculate the context vector for each encoder independently of each other using Equation 3. Second, we project the context vectors (and optionally the guard) into a common space (equation 8), we calculate another distribution using the projected context vectors (equation 9) and their respective weighted average (equation 10): e (k) i = v b tanh (Wbsi + U (k) b (k) b (k) i) i) i), (8) \u03b2 (k) exp (k) (k) x x x x x x x x x x x x x x x x x (k) x x x x x x x x x x x x x x (x x x) x x x (b) x c c c c c (c) x (x (x) x (k) x (x x x) x (k) x (k) x (k) x x x x x (k) x (k) x x x x (k) x x x x x x x x x x x x x x x x x (x x x x x x x x x x x x x) x x x x x x x x x x x x x x x x x x x x x (k) x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "4 Experiments", "text": "We evaluate the strategies of attention combination presented in Section 3 on the tasks of multimodal translation (Section 4.1) and automatic post-processing (Section 4.2). The models were implemented with the Neural Monkey Sequence-to-Sequence Learning Toolkit (Helcl and Libovicky, \"2017) 1. In both setups, we process text input with bidirectional GRU network (Cho et al., 2014) with 300 units hidden in each direction and 300 units embedded. For the projection space of attention, we use 500 hidden units. We optimize the network to minimize output cross-entropy using the Adam algorithm (Kingma and Ba, 2014) with a learning rate of 10-4."}, {"heading": "4.1 Multimodal Translation", "text": "The multimodal translation goal (Specia et al., 2016) is to generate multilingual captions that affect both the image and the caption in the source language. We train and evaluate the model based on the Multi30k dataset (Elliott et al., 2016), which consists of 29,000 training techniques (images together with English captions and their German translations), 1,014 validation instances and 1,000 test instances. Results are evaluated using the BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2011). In our model, the visual input is processed using a pre-formed VGG 16 network (Simonyan and Zisserman, 2014) without further fine-tuning. Attention distribution over the visual input is calculated from the last revolutionary layer of the network."}, {"heading": "4.2 Automatic MT Post-editing", "text": "We used data from the APE task WMT16 (Bojar et al., 2016; Turchi et al., 2016), which consists of 12,000 trainings, 2,000 validations and 1,000 test set triplets from the IT domain. Each triplet contains an English source set, an automatically generated German translation of the source set and a manually edited German sentence for reference. In the case of this data set, the MT results are almost perfect and there was little effort required to edit the sentences afterwards. The results are evaluated on the basis of the humantargeted error rate (HTER) (Snover et al., 2006) and the BLEU score (Papineni et al., 2002). Following Libovicky \u0301 et al. (2016), we code the target sentence as a sequence of editing."}, {"heading": "5 Related Work", "text": "Attempts to use S2S models for APE are relatively rare (Bojar et al., 2016). Niehues et al. (2016) combine both inputs in a long sequence, forcing the encoder to work with both the source and target languages. Their attention is then similar to our flat combination strategy; however, it can only be used for sequential data. The best system from the WMT '16 competition (Junczys-Dowmunt and Grundkiewicz, 2016) consists of two separate S2S models, one translated from the MT edition to post-processed targets and the second from source sets to post-processed targets. The decoder average production distributions are similar to decoder ensembles. The biggest source of improvement in this state of the art was a post editor generated from additional training data rather than from changes in the network architecture. Caglayan et al al al al al multimodal al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al (2016) used an architecture."}, {"heading": "6 Conclusions", "text": "Both methods are based on the calculation of a common distribution across the hidden states of all encoders. We conducted experiments with the proposed strategies for multimodal translation and automatic post-processing tasks and showed that the flat and hierarchical attention combination can be applied to these tasks while maintaining the competitiveness of the previously used techniques. In contrast to the simple context vector concatenation, the presented combination strategies can be used with the conditional GRU units in the decoder. Furthermore, the hierarchical combination strategy shows faster learning than the other strategies."}, {"heading": "Acknowledgments", "text": "We thank you for the fruitful discussions and comments on the draft paper. This work was financed by the grant of the Czech Science Foundation no. P103 / 12 / G084, the EU grant no. H2020-ICT-2014-1-645452 (QT21) and the grant from Charles University no. 52315 / 2014 and the SIA project no. 260 453. Language resources developed and / or stored and / or distributed by the LINDAT-Clarin project of the Ministry of Education of the Czech Republic (project LM2010013) were used for this work."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Findings of the 2016 conference on machine translation (WMT16)", "author": ["bino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri"], "venue": "In Proceedings of the First Conference on Machine Translation (WMT). Volume", "citeRegEx": "bino et al\\.,? \\Q2016\\E", "shortCiteRegEx": "bino et al\\.", "year": 2016}, {"title": "Log-linear combinations of monolingual and bilingual neural machine translation models for automatic post-editing", "author": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz."], "venue": "Proceedings of the First Conference on Ma-", "citeRegEx": "Junczys.Dowmunt and Grundkiewicz.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt and Grundkiewicz.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6980. http://arxiv.org/abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "CUNI system for WMT16 automatic post-editing and multimodal translation tasks", "author": ["Jind\u0159ich Libovick\u00fd", "Jind\u0159ich Helcl", "Marek Tlust\u00fd", "Ond\u0159ej Bojar", "Pavel Pecina."], "venue": "Proceedings of the First Conference on Machine", "citeRegEx": "Libovick\u00fd et al\\.,? 2016", "shortCiteRegEx": "Libovick\u00fd et al\\.", "year": 2016}, {"title": "Knowing when to look: Adaptive attention via a visual sentinel for image captioning", "author": ["Jiasen Lu", "Caiming Xiong", "Devi Parikh", "Richard Socher."], "venue": "CoRR abs/1612.01887. http://arxiv.org/abs/1612.01887.", "citeRegEx": "Lu et al\\.,? 2016", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Pre-translation for neural machine translation", "author": ["Jan Niehues", "Eunah Cho", "Thanh-Le Ha", "Alex Waibel."], "venue": "CoRR abs/1610.05243. http://arxiv.org/abs/1610.05243.", "citeRegEx": "Niehues et al\\.,? 2016", "shortCiteRegEx": "Niehues et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computa-", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Edinburgh neural machine translation systems for WMT 16", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the First Conference on Machine Translation. Association for Computational", "citeRegEx": "Sennrich et al\\.,? 2016a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Sennrich et al\\.,? 2016b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for largescale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman."], "venue": "CoRR abs/1409.1556. http://arxiv.org/abs/1409.1556.", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."], "venue": "Proceedings of association for machine translation in the Americas. volume 200.", "citeRegEx": "Snover et al\\.,? 2006", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "A shared task on multimodal machine translation and crosslingual image description", "author": ["Lucia Specia", "Stella Frank", "Khalil Sima\u2019an", "Desmond Elliott"], "venue": "In Proceedings of the First Conference on Machine Translation", "citeRegEx": "Specia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2016}, {"title": "WMT16 APE shared task data", "author": ["Marco Turchi", "Rajen Chatterjee", "Matteo Negri."], "venue": "LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics, Charles University in Prague. http://hdl.handle.net/11372/LRT-1632.", "citeRegEx": "Turchi et al\\.,? 2016", "shortCiteRegEx": "Turchi et al\\.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."], "venue": "David Blei", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Multi-source neural translation", "author": ["Barret Zoph", "Kevin Knight."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational", "citeRegEx": "Zoph and Knight.,? 2016", "shortCiteRegEx": "Zoph and Knight.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Sequence-to-sequence (S2S) learning with attention mechanism recently became the most successful paradigm with state-of-the-art results in machine translation (MT) (Bahdanau et al., 2014; Sennrich et al., 2016a), image captioning (Xu et al.", "startOffset": 164, "endOffset": 211}, {"referenceID": 9, "context": "Sequence-to-sequence (S2S) learning with attention mechanism recently became the most successful paradigm with state-of-the-art results in machine translation (MT) (Bahdanau et al., 2014; Sennrich et al., 2016a), image captioning (Xu et al.", "startOffset": 164, "endOffset": 211}, {"referenceID": 15, "context": ", 2016a), image captioning (Xu et al., 2015; Lu et al., 2016), text summarization (Rush et al.", "startOffset": 27, "endOffset": 61}, {"referenceID": 5, "context": ", 2016a), image captioning (Xu et al., 2015; Lu et al., 2016), text summarization (Rush et al.", "startOffset": 27, "endOffset": 61}, {"referenceID": 8, "context": ", 2016), text summarization (Rush et al., 2015) and other NLP tasks.", "startOffset": 28, "endOffset": 47}, {"referenceID": 16, "context": "The existing approaches to this problem do not explicitly model different importance of the inputs to the decoder (Firat et al., 2016; Zoph and Knight, 2016).", "startOffset": 114, "endOffset": 157}, {"referenceID": 0, "context": "The standard attention model as described by Bahdanau et al. (2014) defines the attention energies eij , attention distribution \u03b1ij , and the conar X iv :1 70 4.", "startOffset": 45, "endOffset": 68}, {"referenceID": 5, "context": "Recently, Lu et al. (2016) introduced sentinel gate, an extension of the attentive RNN decoder with LSTM units (Hochreiter and Schmidhuber, 1997).", "startOffset": 10, "endOffset": 27}, {"referenceID": 16, "context": ", c (N) i (Zoph and Knight, 2016; Firat et al., 2016).", "startOffset": 10, "endOffset": 53}, {"referenceID": 3, "context": "We optimize the network to minimize the output cross-entropy using the Adam algorithm (Kingma and Ba, 2014) with learning rate 10\u22124.", "startOffset": 86, "endOffset": 107}, {"referenceID": 13, "context": "The goal of multimodal translation (Specia et al., 2016) is to generate target-language image captions given both the image and its caption in the source language.", "startOffset": 35, "endOffset": 56}, {"referenceID": 7, "context": "The results are evaluated using the BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2011).", "startOffset": 41, "endOffset": 64}, {"referenceID": 11, "context": "In our model, the visual input is processed with a pre-trained VGG 16 network (Simonyan and Zisserman, 2014) without further fine-tuning.", "startOffset": 78, "endOffset": 108}, {"referenceID": 10, "context": "We use byte-pair encoding (Sennrich et al., 2016b) with a vocabulary of 20,000 subword units shared between the textual encoder and the decoder.", "startOffset": 26, "endOffset": 50}, {"referenceID": 14, "context": "We used the data from the WMT16 APE Task (Bojar et al., 2016; Turchi et al., 2016), which consists of 12,000 training, 2,000 validation, and 1,000 test sentence triplets from the IT domain.", "startOffset": 41, "endOffset": 82}, {"referenceID": 12, "context": "The results are evaluated using the humantargeted error rate (HTER) (Snover et al., 2006) and BLEU score (Papineni et al.", "startOffset": 68, "endOffset": 89}, {"referenceID": 7, "context": ", 2006) and BLEU score (Papineni et al., 2002).", "startOffset": 23, "endOffset": 46}, {"referenceID": 4, "context": "Following Libovick\u00fd et al. (2016), we encode the target sentence as a sequence of edit operations sh ar e se nt .", "startOffset": 10, "endOffset": 34}, {"referenceID": 6, "context": "Niehues et al. (2016) concatenate both inputs into one long sequence, which forces the encoder to be able to work with both source and target language.", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "The best system from the WMT\u201916 competition (Junczys-Dowmunt and Grundkiewicz, 2016) trains two separate S2S models, one translating from MT output to post-edited targets and the second one from source sentences to post-edited targets.", "startOffset": 44, "endOffset": 84}, {"referenceID": 11, "context": "The best-performing architecture uses the last fully-connected layer of VGG-19 network (Simonyan and Zisserman, 2014) as decoder initialization and only attends to the text encoder hidden states.", "startOffset": 87, "endOffset": 117}, {"referenceID": 6, "context": "Similarly to Niehues et al. (2016) in the APE task, even further improvement was achieved by synthetically extending the dataset.", "startOffset": 13, "endOffset": 35}], "year": 2017, "abstractText": "Modeling attention in neural multi-source sequence-to-sequence learning remains a relatively unexplored area, despite its usefulness in tasks that incorporate multiple source languages or modalities. We propose two novel approaches to combine the outputs of attention mechanisms over each source sequence, flat and hierarchical. We compare the proposed methods with existing techniques and present results of systematic evaluation of those methods on the WMT16 Multimodal Translation and Automatic Post-editing tasks. We show that the proposed methods achieve competitive results on both tasks.", "creator": "LaTeX with hyperref package"}}}