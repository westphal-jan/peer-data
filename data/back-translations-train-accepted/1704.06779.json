{"id": "1704.06779", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Apr-2017", "title": "Lexical Features in Coreference Resolution: To be Used With Caution", "abstract": "Lexical features are a major source of information in state-of-the-art coreference resolvers. Lexical features implicitly model some of the linguistic phenomena at a fine granularity level. They are especially useful for representing the context of mentions. In this paper we investigate a drawback of using many lexical features in state-of-the-art coreference resolvers. We show that if coreference resolvers mainly rely on lexical features, they can hardly generalize to unseen domains. Furthermore, we show that the current coreference resolution evaluation is clearly flawed by only evaluating on a specific split of a specific dataset in which there is a notable overlap between the training, development and test sets.", "histories": [["v1", "Sat, 22 Apr 2017 09:59:42 GMT  (124kb,D)", "http://arxiv.org/abs/1704.06779v1", "6 pages, ACL 2017"]], "COMMENTS": "6 pages, ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["nafise sadat moosavi", "michael strube 0001"], "accepted": true, "id": "1704.06779"}, "pdf": {"name": "1704.06779.pdf", "metadata": {"source": "CRF", "title": "Lexical Features in Coreference Resolution: To be Used With Caution", "authors": ["Nafise Sadat Moosavi", "Michael Strube"], "emails": ["nafise.moosavi@h-its.org", "michael.strube@h-its.org"], "sections": [{"heading": "1 Introduction", "text": "As Durrett and Klein (2013) have shown, lexical characteristics implicitly model some linguistic phenomena previously modelled by heuristic characteristics, but on a finer level of granularity. However, we question whether the knowledge acquired mainly by lexical characteristics can be generalized to other areas. However, the introduction of the CoNLL dataset allowed a significant boost in the performance of coreference resolvers, i.e. about 10 percent difference between the CoNLL score of the currently best coreference resolver, Deep-Coref by Clark and Manning (2016b), and the winner of the 2011 CoNLL dataset, which was created on the basis of Stanford rule correction, is unable to evaluate a coreference corridor."}, {"heading": "2 Lexical Features", "text": "The large difference in performance between core reference resolvers that use lexical features and those that do not imply the importance of lexical features. Durrett and Klein (2013) show that lexical features implicitly capture some phenomena, such as definition and syntactic roles previously modelled by heuristic features. Durrett and Klein (2013) use exact surface shapes as lexical features. However, when word embedding is used instead of surface shapes, the use of lexical features is even more advantageous. Word embedding is an efficient method of capturing semantic relationships, and in particular, it provides an efficient way of describing the context of mentions. Durrett and Klein (2013) show that the addition of some heuristic features, such as sex, num-ar Xiv: 170 4.06 779v 1 [cs.C 22] Apr 7201 on the surface of the person and their animation page, agreements on their animation side."}, {"heading": "3 Out-of-Domain Evaluation", "text": "Apart from the obvious success of lexical characteristics, it is questionable how well the knowledge, which is captured primarily by the lexical data, can be generalized to cover other areas. Just as it was the case in the past, in the future, in the future, in the future, in the future, in the future, in the future, in the future, in the future, in the future, in the future, in the future, in the future, in the future, in the future, in the future, in the future, in the future, in the future, in the future, in the future, in the future, in the future, in the future, in the future, the future, the future, the future, the future, the future, the future, the future, the future, the, the, the future, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the"}, {"heading": "4 Why do Improvements Fade Away?", "text": "This year, we have reached a point where it can only take one year to reach an agreement."}, {"heading": "5 Discussion", "text": "This bias prevents us from developing more robust and generalizable coreference resolvers. Although coreference resolution is an important step in understanding text, it is not an end goal. Coreference resolvers are used in tasks and domains for which coreference resolutions may not be available. Therefore, generalizability should be emphasized when developing coreference resolutions.Furthermore, we show that there is a significant overlap between the training and validation sets in the CoNLL dataset. LEA metrics (Moosavi and Strube, 2016) are introduced as an attempt to make coreference evaluations more reliable. However, to ensure valid developments in coreference resolution, it is not sufficient to have reliable evaluation metrics on which the evaluations are performed."}, {"heading": "Acknowledgments", "text": "The authors thank Kevin Clark for answering all our questions about deepcoref and the three anonymous reviewers for their thoughtful comments. This work was funded by the Klaus Tschira Foundation, Heidelberg. The first author was supported by a doctoral scholarship from the Heidelberg Institute for Theoretical Studies."}], "references": [{"title": "Algorithms for scoring coreference chains", "author": ["Amit Bagga", "Breck Baldwin."], "venue": "Proceedings of the 1st International Conference on Language Resources and Evaluation, Granada, Spain, 28\u201330 May 1998. pages 563\u2013566.", "citeRegEx": "Bagga and Baldwin.,? 1998", "shortCiteRegEx": "Bagga and Baldwin.", "year": 1998}, {"title": "Deep reinforcement learning for mentionranking coreference models", "author": ["Kevin Clark", "Christopher D. Manning."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Austin,", "citeRegEx": "Clark and Manning.,? 2016a", "shortCiteRegEx": "Clark and Manning.", "year": 2016}, {"title": "Improving coreference resolution by learning entitylevel distributed representations", "author": ["Kevin Clark", "Christopher D. Manning."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long", "citeRegEx": "Clark and Manning.,? 2016b", "shortCiteRegEx": "Clark and Manning.", "year": 2016}, {"title": "Easy victories and uphill battles in coreference resolution", "author": ["Greg Durrett", "Dan Klein."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, Wash., 18\u201321 October 2013. pages 1971\u20131982.", "citeRegEx": "Durrett and Klein.,? 2013", "shortCiteRegEx": "Durrett and Klein.", "year": 2013}, {"title": "Coreference in Wikipedia: Main concept resolution", "author": ["Abbas Ghaddar", "Philippe Langlais."], "venue": "Proceedings of the 20th Conference on Computational Natural Language Learning, Berlin, Germany, 11\u201312 August 2016. pages 229\u2013238.", "citeRegEx": "Ghaddar and Langlais.,? 2016a", "shortCiteRegEx": "Ghaddar and Langlais.", "year": 2016}, {"title": "WikiCoref: An English coreference-annotated corpus of Wikipedia articles", "author": ["Abbas Ghaddar", "Philippe Langlais."], "venue": "Proceedings of the 10th International Conference on Language Resources and Evaluation, Portoro\u017e,", "citeRegEx": "Ghaddar and Langlais.,? 2016b", "shortCiteRegEx": "Ghaddar and Langlais.", "year": 2016}, {"title": "Deterministic coreference resolution based on entity-centric, precision-ranked rules", "author": ["Heeyoung Lee", "Angel Chang", "Yves Peirsman", "Nathanael Chambers", "Mihai Surdeanu", "Dan Jurafsky."], "venue": "Computational Linguistics 39(4):885\u2013916.", "citeRegEx": "Lee et al\\.,? 2013", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "Do supervised distributional methods really learn lexical inference relations", "author": ["Omer Levy", "Steffen Remus", "Chris Biemann", "Ido Dagan"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "On coreference resolution performance metrics", "author": ["Xiaoqiang Luo."], "venue": "Proceedings of the Human Language Technology Conference and the 2005", "citeRegEx": "Luo.,? 2005", "shortCiteRegEx": "Luo.", "year": 2005}, {"title": "Recall error analysis for coreference resolution", "author": ["Sebastian Martschat", "Michael Strube."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, Doha, Qatar, 25\u201329 October 2014. pages 2070\u20132081.", "citeRegEx": "Martschat and Strube.,? 2014", "shortCiteRegEx": "Martschat and Strube.", "year": 2014}, {"title": "Latent structures for coreference resolution", "author": ["Sebastian Martschat", "Michael Strube."], "venue": "Transactions of the Association for Computational Linguistics 3:405\u2013418. http://www.aclweb.org/anthology/Q151029.pdf.", "citeRegEx": "Martschat and Strube.,? 2015", "shortCiteRegEx": "Martschat and Strube.", "year": 2015}, {"title": "Which coreference evaluation metric do you trust? A proposal for a link-based entity aware metric", "author": ["Nafise Sadat Moosavi", "Michael Strube."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational", "citeRegEx": "Moosavi and Strube.,? 2016", "shortCiteRegEx": "Moosavi and Strube.", "year": 2016}, {"title": "A modeltheoretic coreference scoring scheme", "author": ["Marc Vilain", "John Burger", "John Aberdeen", "Dennis Connolly", "Lynette Hirschman."], "venue": "Proceedings of the 6th Message Understanding Conference (MUC-6). Morgan Kaufmann, San Mateo, Cal.,", "citeRegEx": "Vilain et al\\.,? 1995", "shortCiteRegEx": "Vilain et al\\.", "year": 1995}], "referenceMentions": [{"referenceID": 3, "context": "As shown by Durrett and Klein (2013), lexical features implicitly model some linguistic phenomena, which were previously modeled by heuristic features, but at a finer level of granularity.", "startOffset": 12, "endOffset": 37}, {"referenceID": 1, "context": "about 10 percent difference between the CoNLL score of the currently best coreference resolver, deep-coref by Clark and Manning (2016b), and the winner of the CoNLL 2011 shared task, the Stanford rule-based system by Lee et al.", "startOffset": 110, "endOffset": 136}, {"referenceID": 1, "context": "about 10 percent difference between the CoNLL score of the currently best coreference resolver, deep-coref by Clark and Manning (2016b), and the winner of the CoNLL 2011 shared task, the Stanford rule-based system by Lee et al. (2013). However, this substantial improvement does not seem to be visible in downstream tasks.", "startOffset": 110, "endOffset": 235}, {"referenceID": 4, "context": "on a new dataset, even with consistent definitions of mentions and coreference relations (Ghaddar and Langlais, 2016a).", "startOffset": 89, "endOffset": 118}, {"referenceID": 3, "context": "Durrett and Klein (2013) show that lexical features implicitly capture some phenomena, e.", "startOffset": 0, "endOffset": 25}, {"referenceID": 3, "context": "Durrett and Klein (2013) use exact surface forms as lexical features.", "startOffset": 0, "endOffset": 25}, {"referenceID": 1, "context": "Clark and Manning (2016b) capture the required information", "startOffset": 0, "endOffset": 26}, {"referenceID": 1, "context": "The main difference is that Clark and Manning (2016b)", "startOffset": 28, "endOffset": 54}, {"referenceID": 3, "context": "use word embeddings instead of the exact surface forms that are used by Durrett and Klein (2013).", "startOffset": 72, "endOffset": 97}, {"referenceID": 9, "context": "Based on the error analysis by cort (Martschat and Strube, 2014), in comparison to systems that do not use word embeddings, deep-coref has fewer recall and precision errors especially for pronouns.", "startOffset": 36, "endOffset": 64}, {"referenceID": 4, "context": "As reported by Ghaddar and Langlais (2016b),", "startOffset": 15, "endOffset": 44}, {"referenceID": 6, "context": "worse than the rule-based system (Lee et al., 2013), on the new dataset, WikiCoref (Ghaddar and Langlais, 2016b), even though WikiCoref is annotated with the same annotation guidelines as the CoNLL dataset.", "startOffset": 33, "endOffset": 51}, {"referenceID": 5, "context": ", 2013), on the new dataset, WikiCoref (Ghaddar and Langlais, 2016b), even though WikiCoref is annotated with the same annotation guidelines as the CoNLL dataset.", "startOffset": 39, "endOffset": 68}, {"referenceID": 12, "context": "The results are reported using MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), the average F1 score of these three metrics, i.", "startOffset": 35, "endOffset": 56}, {"referenceID": 0, "context": ", 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), the average F1 score of these three metrics, i.", "startOffset": 12, "endOffset": 37}, {"referenceID": 8, "context": ", 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), the average F1 score of these three metrics, i.", "startOffset": 45, "endOffset": 56}, {"referenceID": 11, "context": "CoNLL score, and LEA (Moosavi and Strube, 2016).", "startOffset": 21, "endOffset": 47}, {"referenceID": 3, "context": "berkeley is the mention-ranking model of Durrett and Klein (2013) with the FINAL feature set including the head, first, last, preceding and following words of a mention, the ancestry, length,", "startOffset": 41, "endOffset": 66}, {"referenceID": 9, "context": "cort is the mention-ranking model of Martschat and Strube (2015). cort uses the following set of features: the head, first, last, preceding and following words of a mention, the ancestry, length,", "startOffset": 37, "endOffset": 65}, {"referenceID": 4, "context": "berkeley and cort scores in Table 1 are taken from Ghaddar and Langlais (2016a).", "startOffset": 51, "endOffset": 80}, {"referenceID": 1, "context": "deep-coref is the mention-ranking model of Clark and Manning (2016b). deep-coref incorporates a large set of embeddings, i.", "startOffset": 43, "endOffset": 69}, {"referenceID": 11, "context": "metric (Moosavi and Strube, 2016) for choosing the best model.", "startOffset": 7, "endOffset": 33}, {"referenceID": 3, "context": "berkeley-final is the coreference resolver of Durrett and Klein (2013) with the FINAL feature", "startOffset": 46, "endOffset": 71}, {"referenceID": 1, "context": "used by Clark and Manning (2016a). However, based on the performance on the development set, we only train the model for 50 iterations in out-ofdomain evaluations.", "startOffset": 8, "endOffset": 34}, {"referenceID": 9, "context": "We compute the recall errors by cort error analysis tool (Martschat and Strube, 2014).", "startOffset": 57, "endOffset": 85}, {"referenceID": 2, "context": "The resolution of such pairs has been referred to as capturing semantic similarity (Clark and Manning, 2016b).", "startOffset": 83, "endOffset": 109}, {"referenceID": 7, "context": "The effect of lexical features is also analyzed by Levy et al. (2015) for tasks like hypernymy and entailment.", "startOffset": 51, "endOffset": 70}, {"referenceID": 11, "context": "The LEA metric (Moosavi and Strube, 2016) is introduced as an attempt to make coreference evaluations more reliable.", "startOffset": 15, "endOffset": 41}], "year": 2017, "abstractText": "Lexical features are a major source of information in state-of-the-art coreference resolvers. Lexical features implicitly model some of the linguistic phenomena at a fine granularity level. They are especially useful for representing the context of mentions. In this paper we investigate a drawback of using many lexical features in state-of-the-art coreference resolvers. We show that if coreference resolvers mainly rely on lexical features, they can hardly generalize to unseen domains. Furthermore, we show that the current coreference resolution evaluation is clearly flawed by only evaluating on a specific split of a specific dataset in which there is a notable overlap between the training, development and test sets.", "creator": "LaTeX with hyperref package"}}}