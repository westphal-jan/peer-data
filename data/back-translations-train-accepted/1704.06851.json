{"id": "1704.06851", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Apr-2017", "title": "Affect-LM: A Neural Language Model for Customizable Affective Text Generation", "abstract": "Human verbal communication includes affective messages which are conveyed through use of emotionally colored words. There has been a lot of research in this direction but the problem of integrating state-of-the-art neural language models with affective information remains an area ripe for exploration. In this paper, we propose an extension to an LSTM (Long Short-Term Memory) language model for generating conversational text, conditioned on affect categories. Our proposed model, Affect-LM enables us to customize the degree of emotional content in generated sentences through an additional design parameter. Perception studies conducted using Amazon Mechanical Turk show that Affect-LM generates naturally looking emotional sentences without sacrificing grammatical correctness. Affect-LM also learns affect-discriminative word representations, and perplexity experiments show that additional affective information in conversational text can improve language model prediction.", "histories": [["v1", "Sat, 22 Apr 2017 21:10:10 GMT  (346kb,D)", "http://arxiv.org/abs/1704.06851v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sayan ghosh", "mathieu chollet", "eugene laksana", "louis-philippe morency", "stefan scherer"], "accepted": true, "id": "1704.06851"}, "pdf": {"name": "1704.06851.pdf", "metadata": {"source": "CRF", "title": "Affect-LM: A Neural Language Model for Customizable Affective Text Generation", "authors": ["Sayan Ghosh", "Mathieu Chollet", "Eugene Laksana", "Louis-Philippe Morency", "Stefan Scherer"], "emails": ["1sghosh@ict.usc.edu", "chollet@ict.usc.edu", "elaksana@ict.usc.edu", "scherer@ict.usc.edu", "2morency@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In this context, the analysis of human influence from the text is an important topic in the field of natural language understanding. Examples of this are the sensory analysis of Twitter (Nakov et al., 2016), the analysis of affects from poetry (Kao and Jurafsky, 2012) and the investigation of correlations between functional words and psychological processes (Pennebaker, 2011). People exchange verbal messages that contain not only syntactic information, but also information about their mental and emotional states. Examples are the use of emotionally tinged words (such as anger and joy) and swear words, which we have in human verbal communication of great importance for the understanding of spoken language systems."}, {"heading": "2 Related Work", "text": "Language modeling is an integral component of spoken language systems, and traditionally ngram approaches have been used (Stolcke et al., 2002), with the shortcoming that they are unable to generalize on word sequences that are not included in the training set but occur in invisible data. Bengio et al. (2003) proposed neural language models that address this shortcoming by generalizing through word representations. Mikolov et al. (2010) and Sundermeyer et al. (2012) extend neural language models to a recurring architecture in which a target word wt is predicted from a context of all previous words w1, w2,..., wt \u2212 1 with an LSTM (Long-Term Memory) neural network. There have also been recent efforts to build language models based on other modalities or attributes of neural systems. For example, Vinyet al were developed as neural image modeling models."}, {"heading": "3 Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 LSTM Language Model", "text": "Before providing a formulation for our proposed model, we briefly describe an LSTM language model. We chose this model as a starting point because it was reported to achieve a state of confusion compared to other approaches, such as n-gram models with Kneser-Ney smoothing (Jozefowicz et al., 2016). Unlike an ordinary recurrent neural network, an LSTM network does not suffer from the vanishing gradient problem, which is more pronounced for very long sequences (Hochreiter and Schmidhuber, 1997). Formally, by the chain rule of probability, for a sequence of M words w1, w2, wM, the common probability of all words is given by. P (w1, wM) = t = M-t = 1 P (wt | w2, wt) the equation of M words w1, wt \u2212 f, which represents a sequence of M words, w2 of all words common to m \u2212."}, {"heading": "3.3 Descriptors for Affect Category Information", "text": "Our proposed model learns a generative model of the next word wt conditionally not only on the previous words \u03b2 \u03b2 \u03b2, w2,..., wt \u2212 1, but also on the affect category et \u2212 1, which are additional information about emotional contents. During the model training, the affect category is derived from the context data itself. Thus, we define a suitable trait that an affective lexicon can use to derive emotions in context. (2001), LIWC is based on a dictionary in which each word is assigned to a predefined LIWC category. Categories are selected based on their association with social, affective and cognitive processes."}, {"heading": "4 Experimental Setup", "text": "In Section 1, we introduced three primary research questions related to the proposed Affect-LM model's ability to generate emotionally tinged conversation texts without sacrificing grammatical correctness, and to achieve less perplexity than an LSTM language model when evaluated against emotionally tinged corpora. In this section, we discuss our experimental setup to address these questions, with a description of the architecture of Affect-LM and the corpora used for training and evaluating the language models."}, {"heading": "4.1 Speech Corpora", "text": "The Fisher English Training Speech Corpus consists of 70 + hours of dyadic interviews between a human subject and virtual questions, in which we selected three emotionally tinged conversation corpus. A brief description of each corpus is given below, and in Table 1, we report on relevant statistics, such as the total number of words, along with the fraction of emotionally tinged words (which belong to the affective word categories of the LIWC) in each corpus. Fisher English Training Speech Parts 1 & 2: The Fisher dataset al., 2004), the speech consists of 10-minute telephone conversations, along with their associated transcripts. Each conversation is between two strangers who are invited to speak on a randomly selected topic from a series. Examples of conversation topics are Minimum Weight, Time Travel, and Comedy."}, {"heading": "4.3 Language Modeling Experiments", "text": "Affect-LM can also be used as a language model, where the next predicted word is estimated from the words in context, along with an affect category extracted from the context words themselves (rather than being externally encoded as in the generation). To assess whether additional emotional information could improve predictive performance, we train the corpora detailed in Section 4.1 in two steps, as described below: (1) Training and validation of the language models on Fisher records - The Fisher corpus is divided into a 75: 15: 10 ratio corresponding to the training, validation, and evaluation of the subsets, and after implementation in Zaremba et al. (2014) we train the language models (both the baseline and Affect-LM) on the training part for 13 epochs, with the learning rate of 1.0 for the first four epochs and the rate of validation for each of the following epochs, and the rate of validation for each of the following epochs, and the first four epochs respectively."}, {"heading": "4.4 Sentence Generation Perception Study", "text": "We evaluate Affect-LM's ability to produce emotionally tinged text of varying degrees without seriously degrading grammatical correctness by conducting a comprehensive perception study on Amazon's Mechanical Turk (MTurk) platform. MTurk has been successfully used in the past for a wide range of perceptual experiments and has proven to be an excellent resource for collecting human ratings for large studies (Buhrmester et al., 2011). Specifically, we generated more than 200 sentences for four sentence beginnings (namely the three sentence beginnings listed in Table 2, and one sentence end indicating that the model should generate a new sentence) in five affectivity categories (positive emotions), anger, sadness, anxiety, and negative emotions. The Affect-LM model trainedon the Fisher corpus was used for sentence generation, with each sentence rated by two human raters having a minimum approval rating of 98% in the United States and the United States."}, {"heading": "5 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Generation of Emotional Text", "text": "In Section 3.4 we described the process of the text sample from the model, which is based on affective input information (research question Q1). Table 2 shows three sentences generated by the model for input sentence beginnings, which I find so..., Why did you... and I tell him... for each of the five affect categories - happy (positive emotion), angry, sad and neutral (no emotion). They were selected from a pool of 20 sentences generated for each category and each sentence beginning."}, {"heading": "5.2 MTurk Perception Experiments", "text": "In the aftermath, it has shown itself that it is a very complex situation, in which a very complex, complex, complex and complex world is at stake, in which human beings do not only care about themselves, but also about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves, about themselves."}, {"heading": "5.3 Language Modeling Results", "text": "In Table 3, we deal with the research question Q3 by presenting the perplexity values obtained by the base model and Affect-LM when trained on the Fisher corpus and then adjusted to three emotional corpus (each adapted model is individually trained on CMU-MOSI, DAIC and SEMAINE). Fisher-trained models are evaluated on all corpus, while each adapted model is evaluated only on its respective corpus. For all corpus, we find that Affect-LM achieves a lower perplexity on average than the base model, which means that the category information derived from the context words improves the prediction of the language model. The average perplexity improvement is 1.44 (relative improvement 1.94%) for the Fisher-trained model, while it is 0.79 (1.31%) for the adapted models. We note that greater improvements in perplexity in body modes with higher MOM content than in Fisher-based words are observed in Table 75%."}, {"heading": "5.4 Word Representations", "text": "In Eq.3, Affect-LM learns a weight matrix V, which determines the correlation between the predicted word wt and the affect category et \u2212 1. Thus, each line of the matrix Vi is an emotionally significant embedding of the i-word in the vocabulary. In Figure 4, we present a visualization of these embedding, with each data point being a separate word and words appearing in the LIWC dictionary colored by the affect category to which they belong (we have only labeled words in the categories positive emotion, negative emotion, anger, sadness, and fear, as these categories contain the most common words). Words colored gray are those that do not appear in the LIWC dictionary. In Figure 4, we observe that the embedding contains affective information, where the positive emotions are strongly separated from the negative emotions (sad, angry, fear) that are summarized."}, {"heading": "6 Conclusions and Future Work", "text": "In this work, we introduced a novel language model Affect-LM for generating affective conversation text based on context words, an affective category, and an affective strength parameter. MTurk perceptual studies show that the model can generate expressive text with varying degrees of emotional strength without compromising grammatical correctness. We also evaluate Affect-LM as a language model and show that it achieves lower perplexity than an LSTM base model when the affect category is extracted from the words in context. In future work, we would like to expand this model by investigating language generation that depends on other modalities such as facial images and language, as well as applications such as dialogue generation for virtual agents."}, {"heading": "Acknowledgments", "text": "This material is based on work supported by the US Army Research Laboratory under Contract No. W911NF-14-D-0005. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author (s) and do not necessarily reflect the views of the government, and no official endorsement should be derived from them. Sayan Ghosh also recognizes the Viterbi Graduate School Fellowship to fund his graduate studies."}], "references": [{"title": "A neural knowledge language model", "author": ["Sungjin Ahn", "Heeyoul Choi", "Tanel P\u00e4rnamaa", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1608.00318 .", "citeRegEx": "Ahn et al\\.,? 2016", "shortCiteRegEx": "Ahn et al\\.", "year": 2016}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin."], "venue": "Journal of machine learning research 3(Feb):1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Amazon\u2019s mechanical turk a new source of inexpensive, yet high-quality, data? Perspectives on psychological science 6(1):3\u20135", "author": ["Michael Buhrmester", "Tracy Kwang", "Samuel D Gosling"], "venue": null, "citeRegEx": "Buhrmester et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Buhrmester et al\\.", "year": 2011}, {"title": "Web resources for language modeling in conversational speech recognition", "author": ["Ivan Bulyko", "Mari Ostendorf", "Manhung Siu", "Tim Ng", "Andreas Stolcke", "\u00d6zg\u00fcr \u00c7etin."], "venue": "ACM Transactions on Speech and Language Processing (TSLP) 5(1):1.", "citeRegEx": "Bulyko et al\\.,? 2007", "shortCiteRegEx": "Bulyko et al\\.", "year": 2007}, {"title": "Describing multimedia content using attention-based encoder-decoder networks", "author": ["Kyunghyun Cho", "Aaron Courville", "Yoshua Bengio."], "venue": "IEEE Transactions on Multimedia 17(11):1875\u20131886.", "citeRegEx": "Cho et al\\.,? 2015", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "The fisher corpus: a resource for the next generations of speech-to-text", "author": ["Christopher Cieri", "David Miller", "Kevin Walker."], "venue": "LREC. volume 4, pages 69\u201371.", "citeRegEx": "Cieri et al\\.,? 2004", "shortCiteRegEx": "Cieri et al\\.", "year": 2004}, {"title": "Tensorflow: A system for large-scale machine learning", "author": ["Mart\u0131\u0301n Abadi"], "venue": "In Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)", "citeRegEx": "Abadi,? \\Q2016\\E", "shortCiteRegEx": "Abadi", "year": 2016}, {"title": "The distress analysis interview corpus of human and computer interviews", "author": ["Jonathan et al. Gratch."], "venue": "LREC. Citeseer, pages 3123\u20133128.", "citeRegEx": "Gratch.,? 2014", "shortCiteRegEx": "Gratch.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."], "venue": "arXiv preprint arXiv:1602.02410 .", "citeRegEx": "Jozefowicz et al\\.,? 2016", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "A computational analysis of style, affect, and imagery in contemporary poetry", "author": ["Justine Kao", "Dan Jurafsky"], "venue": null, "citeRegEx": "Kao and Jurafsky.,? \\Q2012\\E", "shortCiteRegEx": "Kao and Jurafsky.", "year": 2012}, {"title": "A patternbased model for generating text to express emotion", "author": ["Fazel Keshtkar", "Diana Inkpen."], "venue": "Affective Computing and Intelligent Interaction, Springer, pages 11\u201321.", "citeRegEx": "Keshtkar and Inkpen.,? 2011", "shortCiteRegEx": "Keshtkar and Inkpen.", "year": 2011}, {"title": "Multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel"], "venue": null, "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Generating affective natural language for parents of neonatal infants", "author": ["Saad Mahamood", "Ehud Reiter."], "venue": "Proceedings of the 13th European Workshop on Natural Language Generation. Association for Computational Linguistics, pages 12\u201321.", "citeRegEx": "Mahamood and Reiter.,? 2011", "shortCiteRegEx": "Mahamood and Reiter.", "year": 2011}, {"title": "Personage: Personality generation for dialogue", "author": ["Fran\u00e7ois Mairesse", "Marilyn Walker"], "venue": null, "citeRegEx": "Mairesse and Walker.,? \\Q2007\\E", "shortCiteRegEx": "Mairesse and Walker.", "year": 2007}, {"title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent", "author": ["Gary McKeown", "Michel Valstar", "Roddy Cowie", "Maja Pantic", "Marc Schroder."], "venue": "IEEE Transactions on Affective", "citeRegEx": "McKeown et al\\.,? 2012", "shortCiteRegEx": "McKeown et al\\.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "Interspeech. volume 2, page 3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Semeval2016 task 4: Sentiment analysis in twitter", "author": ["Preslav Nakov", "Alan Ritter", "Sara Rosenthal", "Fabrizio Sebastiani", "Veselin Stoyanov."], "venue": "Proceedings of SemEval pages 1\u201318.", "citeRegEx": "Nakov et al\\.,? 2016", "shortCiteRegEx": "Nakov et al\\.", "year": 2016}, {"title": "The secret life of pronouns", "author": ["James W Pennebaker."], "venue": "New Scientist 211(2828):42\u201345.", "citeRegEx": "Pennebaker.,? 2011", "shortCiteRegEx": "Pennebaker.", "year": 2011}, {"title": "Linguistic inquiry and word count: Liwc 2001", "author": ["James W Pennebaker", "Martha E Francis", "Roger J Booth."], "venue": "Mahway: Lawrence Erlbaum Associates 71(2001):2001.", "citeRegEx": "Pennebaker et al\\.,? 2001", "shortCiteRegEx": "Pennebaker et al\\.", "year": 2001}, {"title": "Affective computing, volume 252", "author": ["Rosalind Picard."], "venue": "MIT press Cambridge.", "citeRegEx": "Picard.,? 1997", "shortCiteRegEx": "Picard.", "year": 1997}, {"title": "A Blueprint for Affective Computing: A sourcebook and manual", "author": ["Klaus R Scherer", "Tanja B\u00e4nziger", "Etienne Roesch."], "venue": "Oxford University Press.", "citeRegEx": "Scherer et al\\.,? 2010", "shortCiteRegEx": "Scherer et al\\.", "year": 2010}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "Interspeech. volume 2002, page 2002.", "citeRegEx": "Stolcke,? 2002", "shortCiteRegEx": "Stolcke", "year": 2002}, {"title": "Lstm neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "Interspeech. pages 194\u2013197.", "citeRegEx": "Sundermeyer et al\\.,? 2012", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages", "author": ["Amir Zadeh", "Rowan Zellers", "Eli Pincus", "LouisPhilippe Morency."], "venue": "IEEE Intelligent Systems 31(6):82\u201388.", "citeRegEx": "Zadeh et al\\.,? 2016", "shortCiteRegEx": "Zadeh et al\\.", "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329 .", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 22, "context": "Affect is a term that subsumes emotion and longer term constructs such as mood and personality and refers to the experience of feeling or emotion (Scherer et al., 2010).", "startOffset": 146, "endOffset": 168}, {"referenceID": 21, "context": "Picard (1997) provides", "startOffset": 0, "endOffset": 14}, {"referenceID": 18, "context": "Within this context the analysis of human affect from text is an important topic in natural language understanding, examples of which include sentiment analysis from Twitter (Nakov et al., 2016), affect analysis from poetry (Kao and Jurafsky, Affect-LM \u201cI feel so .", "startOffset": 174, "endOffset": 194}, {"referenceID": 19, "context": "2012) and studies of correlation between function words and social/psychological processes (Pennebaker, 2011).", "startOffset": 91, "endOffset": 109}, {"referenceID": 16, "context": "There has been a resurgence of research effort in recurrent neural networks for language modeling (Mikolov et al., 2010), which have yielded performances far superior to baseline language models based on n-gram approaches.", "startOffset": 98, "endOffset": 120}, {"referenceID": 17, "context": "word semantics (Mikolov et al., 2013), encoder-", "startOffset": 15, "endOffset": 37}, {"referenceID": 4, "context": "decoder models for sentence representations (Cho et al., 2015), language modeling integrated with symbolic knowledge (Ahn et al.", "startOffset": 44, "endOffset": 62}, {"referenceID": 0, "context": ", 2015), language modeling integrated with symbolic knowledge (Ahn et al., 2016) and neural caption generation (Vinyals et al.", "startOffset": 62, "endOffset": 80}, {"referenceID": 25, "context": ", 2016) and neural caption generation (Vinyals et al., 2015), but to the best of our knowledge there has been no work on augmenting neural language modeling with affective information, or on data-driven approaches to generate emotional text.", "startOffset": 38, "endOffset": 60}, {"referenceID": 3, "context": "for speech recognition applications (Bulyko et al., 2007).", "startOffset": 36, "endOffset": 57}, {"referenceID": 20, "context": "keyword spotting from a dictionary of emotional words, such as the LIWC (Linguistic Inquiry and Word Count) tool (Pennebaker et al., 2001).", "startOffset": 113, "endOffset": 138}, {"referenceID": 1, "context": "Bengio et al. (2003) proposed neural language models, which address this shortcoming by generalizing through word representations.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Bengio et al. (2003) proposed neural language models, which address this shortcoming by generalizing through word representations. Mikolov et al. (2010) and Sundermeyer et al.", "startOffset": 0, "endOffset": 153}, {"referenceID": 1, "context": "Bengio et al. (2003) proposed neural language models, which address this shortcoming by generalizing through word representations. Mikolov et al. (2010) and Sundermeyer et al. (2012) extend neural language models to a recurrent architecture, where a target word wt is predicted from a context of all preceding words w1, w2, .", "startOffset": 0, "endOffset": 183}, {"referenceID": 1, "context": "Bengio et al. (2003) proposed neural language models, which address this shortcoming by generalizing through word representations. Mikolov et al. (2010) and Sundermeyer et al. (2012) extend neural language models to a recurrent architecture, where a target word wt is predicted from a context of all preceding words w1, w2, ..., wt\u22121 with an LSTM (Long Short-Term Memory) neural network. There also has been recent effort on building language models conditioned on other modalities or attributes of the data. For example, Vinyals et al. (2015) introduced the neural image caption", "startOffset": 0, "endOffset": 544}, {"referenceID": 12, "context": "Kiros et al. (2014) used an LBL model (Log-Bilinear language model) for", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "Mahamood and Reiter (2011) use several NLG (natural language generation) strategies for producing affective medical reports for parents of neonatal infants undergoing healthcare.", "startOffset": 0, "endOffset": 27}, {"referenceID": 12, "context": "Mahamood and Reiter (2011) use several NLG (natural language generation) strategies for producing affective medical reports for parents of neonatal infants undergoing healthcare. While they study the difference between affective and non-affective reports, their work is limited only to heuristic based systems and do not include conversational text. Mairesse and Walker (2007) developed PERSONAGE, a system for dialogue generation conditioned on extraversion dimensions.", "startOffset": 0, "endOffset": 377}, {"referenceID": 11, "context": "In Keshtkar and Inkpen (2011), the authors use heuristics and rule-based approaches", "startOffset": 3, "endOffset": 30}, {"referenceID": 9, "context": "a baseline since it has been reported to achieve state-of-the-art perplexities compared to other approaches, such as n-gram models with Kneser-Ney smoothing (Jozefowicz et al., 2016).", "startOffset": 157, "endOffset": 182}, {"referenceID": 8, "context": "work does not suffer from the vanishing gradient problem which is more pronounced for very long sequences (Hochreiter and Schmidhuber, 1997).", "startOffset": 106, "endOffset": 140}, {"referenceID": 19, "context": "Introduced by Pennebaker et al. (2001), LIWC is based on a dictionary, where each word is assigned to a predefined LIWC category.", "startOffset": 14, "endOffset": 39}, {"referenceID": 5, "context": "Fisher English Training Speech Parts 1 & 2: The Fisher dataset (Cieri et al., 2004) consists of speech from telephonic conversations of 10 minutes each, along with their associated transcripts.", "startOffset": 63, "endOffset": 83}, {"referenceID": 7, "context": "Distress Assessment Interview Corpus (DAIC): The DAIC corpus introduced by Gratch (2014) consists of 70+ hours of dyadic interviews between a human subject and a virtual human, where the virtual human asks questions designed to diagnose symptoms of psychological distress in the subject such as depression or PTSD (Post Trau-", "startOffset": 75, "endOffset": 89}, {"referenceID": 15, "context": "SEMAINE dataset: SEMAINE (McKeown et al., 2012) is a large audiovisual corpus consisting of interactions between subjects and an operator simulating a SAL (Sensitive Artificial Listener).", "startOffset": 25, "endOffset": 47}, {"referenceID": 26, "context": "Multimodal Opinion-level Sentiment Intensity Dataset (CMU-MOSI): (Zadeh et al., 2016) This is a multimodal annotated corpus of opinion", "startOffset": 65, "endOffset": 85}, {"referenceID": 27, "context": "mentation as described in Zaremba et al. (2014) and to which we have added a separate energy term for the affect category in implementing Affect-LM.", "startOffset": 26, "endOffset": 48}, {"referenceID": 27, "context": "in two stages as described below: (1) Training and validation of the language models on Fisher dataset- The Fisher corpus is split in a 75:15:10 ratio corresponding to the training, validation and evaluation subsets respectively, and following the implementation in Zaremba et al. (2014), we train the language models (both the baseline and Affect-LM) on the training split for 13 epochs, with a learning rate of 1.", "startOffset": 266, "endOffset": 288}, {"referenceID": 2, "context": "the past for a wide range of perception experiments and has been shown to be an excellent resource to collect human ratings for large studies (Buhrmester et al., 2011).", "startOffset": 142, "endOffset": 167}, {"referenceID": 20, "context": "specific emotions, such as angry, sad, and anxious (Pennebaker et al., 2001).", "startOffset": 51, "endOffset": 76}], "year": 2017, "abstractText": "Human verbal communication includes affective messages which are conveyed through use of emotionally colored words. There has been a lot of research in this direction but the problem of integrating state-of-the-art neural language models with affective information remains an area ripe for exploration. In this paper, we propose an extension to an LSTM (Long Short-Term Memory) language model for generating conversational text, conditioned on affect categories. Our proposed model, Affect-LM enables us to customize the degree of emotional content in generated sentences through an additional design parameter. Perception studies conducted using Amazon Mechanical Turk show that AffectLM generates naturally looking emotional sentences without sacrificing grammatical correctness. Affect-LM also learns affectdiscriminative word representations, and perplexity experiments show that additional affective information in conversational text can improve language model prediction.", "creator": "LaTeX with hyperref package"}}}