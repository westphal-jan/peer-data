{"id": "1704.06855", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Apr-2017", "title": "Deep Multitask Learning for Semantic Dependency Parsing", "abstract": "We present a deep neural architecture that parses sentences into three semantic dependency graph formalisms. By using efficient, nearly arc-factored inference and a bidirectional-LSTM composed with a multi-layer perceptron, our base system is able to significantly improve the state of the art for semantic dependency parsing, without using hand-engineered features or syntax. We then explore two multitask learning approaches---one that shares parameters across formalisms, and one that uses higher-order structures to predict the graphs jointly. We find that both approaches improve performance across formalisms on average, achieving a new state of the art. Our code is open-source and available at", "histories": [["v1", "Sat, 22 Apr 2017 22:56:04 GMT  (1244kb,D)", "https://arxiv.org/abs/1704.06855v1", "Proceedings of ACL 2017"], ["v2", "Tue, 25 Apr 2017 19:15:03 GMT  (857kb,D)", "http://arxiv.org/abs/1704.06855v2", "Proceedings of ACL 2017"]], "COMMENTS": "Proceedings of ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hao peng", "sam thomson", "noah a smith"], "accepted": true, "id": "1704.06855"}, "pdf": {"name": "1704.06855.pdf", "metadata": {"source": "CRF", "title": "Deep Multitask Learning for Semantic Dependency Parsing", "authors": ["Hao Peng", "Sam Thomson", "Noah A. Smith"], "emails": ["hapeng@cs.washington.edu,", "nasmith@cs.washington.edu,", "sthomson@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the measures mentioned in order to reactivate them."}, {"heading": "2 Broad-Coverage Semantic Dependency Parsing (SDP)", "text": "In fact, it is not the case that you go in search of another country than you go in search of another country. (...) It is not the case that you go in search of another country. (...) It is not the case that you go in search of another country. (...) It is the case that you go in search of another country. (...) It is not the case that you go in search of another country. (...) It is the case that you go in search of another country. (...) It is the case that you go in search of another country. (...) It is the case that you go in search of another country. \"(...) It is not the case that you go in search of another country. (...) It is the case that you intend to go in search of another country. (...) It is the case that you intend to go in search of another country. (...) It is the case.\" (...) It is the case. \"(...) It is so.\" (...) It is the case. \"(...) It is the case.\" (...) It is not the case. \"() It is not the case.\" (... it is. \"It is the case.\""}, {"heading": "3 Single-Task SDP", "text": "At this point, we present our basic model, in which training and prediction for each formalism are completely separate. We also specify a basic notation that will be reused for our multitask extensions."}, {"heading": "3.1 Problem Formulation", "text": "The output of the semantic dependency analysis is a steered graph (see Figure 1). Each arc has a label from a predefined group L that indicates the semantic relationship of the child to the head. In view of the input sentence x, we leave Y (x) the set of possible semantic graphs above x. the graph we are looking for maximizes a score function S: y = arg max y Y (x) S (x, y), (1) We split S into a sum of local scores for local structures (or \"parts\") p in the graph: S (x, y) = \u2211 p (p). (2) For the sake of notation simplification, we omit the dependence on s on x. See Figure 2a for examples of local structures. s is a parameterized function whose parameters (which are denoted and suppressed here for clarity) are learned from the training data (\u00a7 3.3). Since we can search across all possible structures (for each pair), each of which is referred to as a label."}, {"heading": "3.2 Basic Model", "text": "Our basic model is inspired by the recent successes in neural arc-factored graph-based dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Kuncoro et al., 2016). It draws heavily on the neural bow scoring architectures in this work, but decodes with a different algorithm under slightly different constraints."}, {"heading": "3.2.1 Basic Structures", "text": "Our basic model factors over three types of structures (p in Eq.2): \u2022 predicate indicating a predicate word denotes i \u2192 \u00b7; \u2022 unlabeled arc representing the existence of an arc from a predicate to an argument, labeled i \u2192 j; \u2022 labeled arc, a arc labeled with a semantic scroll, labeled i '\u2192 j. Here are i and j word indexes in a given sentence, and \"displays the arc name. This list corresponds to the most basic structures used by Martins and Almeida (2014). Selecting an output corresponds exactly to the selection that instantiations of these structures contain. To ensure internal consistency of predictions, the following constraints are imposed during decoding: \u2022 i \u2192 \u00b7 if and only if there is at least one that has i \u2192 j constellation, then the constellation is indefinite."}, {"heading": "3.2.2 Basic Scoring", "text": "Similar to Kiperwasser and Goldberg (2016), our models are designed to solve the problem of the disappearance of 1997. In one sentence, they are captured with a bidirectional LSTM (BiLSTM). Any other type of structure (predicate, unlabeled arc, labeled arc) then shares the same BiLSTM representations, which feed them into a multi-layer perception (MLP) that is specific to the structure type. We present the architecture somewhat differently from previous work to facilitate the transition to the multitask scenario (\u00a7 4). In our presentation, we separate the model into a function that represents the input (according to the BiLSTM) and the starting levels of the MLPs, and a function that corresponds to the last layers of the MLPs."}, {"heading": "3.3 Learning", "text": "Informally, the goal is to learn parameters for the score function, so that the gold parser is evaluated for each faulty parser with a margin proportional to the cost of the wrong parse. Formally, D = {(xi, yi)} N i = 1 should be the training set consisting of N sentence pairs xi and its gold parser yi. Training is then the following \"2-regulated empirical risk mitigation problem: min. 2 + 1 NN \u2211 i = 1L (xi, yi), (6) in which all parameters are included in the model, and L is the structured hinge loss: L (xi, yi) = max. y Y (xi, y) {S (xi, y) + c (y, yi) \u2212 S (xi, yi)."}, {"heading": "3.4 Experiments", "text": "We evaluate our basic model based on the English data set of SemEval 2015 Task 18 closed track.3 As in previous work (Almeida and Martins, 2015; Du et al., 2015), we share, resulting in 33,964 training sets from Sections 00-19 of the WSJ corpus, 1692 development sets from Sections 20, 1410 sets from Sections 21 as in-domain test data, and 1,849 sets sampled from the Brown Corpus as out-of-domain test data. The closed track differs from the open and gold tracks in that it does not allow access to syntactical analysis. In the open track, additional machine-generated syntactical parses are provided, while the gold track provides access to various gold standard syntactic analyses. Our model is evaluated using closed-track data; it does not have access to any syntactical analyses during training or testing."}, {"heading": "4 Multitask SDP", "text": "We present two enhancements to our one-task model that use both training data for all three formalisms to improve performance in analyzing each formalism. We describe a firstorder model in which the representation functions are improved through parameter exchange, while the conclusions for each task remain separate (\u00a7 4.2). Then we introduce a model with cross-task parent structures in which common conclusions are used across different tasks (\u00a7 4.3). Both multi-task models use AD3 for decoding and are trained with the same marginal goal as in our one-task model."}, {"heading": "4.1 Problem Formulation", "text": "To distinguish the three tasks (e.g. y (t), \u03c6 (t), where T = {DM, PAS, PSD}. Our task now is to predict three diagrams {y (t)} t \u0442T for a given input set. Multitask SDP can also be understood as splitting x into a single uniform multigraph y = t-T y (t). Similar to Equations 1-2, we split the y-value S (x, y) into a sum of local values for local structures in y, and we seek a multigraph y that maximizes S (x, y)."}, {"heading": "4.2 Multitask SDP with Parameter Sharing", "text": "In this sense, we let each task keep its own specialized MLPs and examine two variants of our model that share parameters at the BiLSTM level. The first variant consists of a set of task-specific BiLSTM encoders and a common task that is shared across all tasks. We call it FREDA. FREDA uses a neural generalization of \"frustratingly simple\" domain adaptation (thumb \u0301 III, 2007; Kim et al., 2016), which augments domain-specific features with a common set of features to capture global patterns. Formally, the \"h (t)\" generalization of the three task-specific encoders (thumb \u0301 III, 2007; Kim et al., 2016), which uses a new set of features to capture global patterns, cannot be formally described as the three task-specific coders."}, {"heading": "4.3 Multitask SDP with Cross-Task Structures", "text": "In the second half of the last decade we have it in the second half of the last decade in the second half of the last decade in the second half of the last decade in the second half of the last decade in the second half of the last decade in the second half of the last decade in the second half of the last decade in the second half of the last decade in the second half of the last decade in the second half of the last decade in the second half of the last decade in the second half of the last decade in the second half of the last decade in the second half of the last decade in the second half of the last century in the second half of the last decade in the second half of the last decade in the second half of the last decade in the second half of the last decade in the second half of the last decade in the second half of the last decade in the second half of the last decade in the second half of the last decade in the second half of the last decade in the second half of the second half of the last decade."}, {"heading": "4.4 Implementation Details", "text": "Each input token is assigned to a concatenation of three real vectors: a pre-learned word vector = weighted; a randomly initialized word vector; and a randomly initialized POS tag vector.8 All three are updated7 Common conclusions come at a price; our third model of order is capable of producing approximately 5.2 sentences (i.e. 15.5 task-specific diagrams) per second on a single Xeon E5-2690 2.60GHz CPU.8There are slight differences in the language component data provided with the three formalisms.For the basic models we use during the training, we use 100-dimensional GloVe \u2212 and \u2212 m vectors, which are trained via Wikipedia and Gigaword as pre-learned word embeddings. To deal with foreign words, we use word dropouts (Iyyer et al., 2015) and randomly replace a word with a special improbability symbol."}, {"heading": "4.5 Experiments", "text": "We compare four multi-task variants with the basic model, as well as the two base systems introduced in \u00a7 3,4. \u2022 SHARED1 is a first-order model, which is either a first-order model. It uses a common encoder as well as task-specific. \u2022 SHARED3 is a third-order model, which is based on \"frugally simple\" parameter parts. It uses a common encoder as well as task-specific structures. \u2022 SHARED3 is separate for each task. \u2022 SHARED3 is a third-order model. It follows SHARED1 and uses a single common BiLSTM encoder, but additionally employs cross-task structures and inference. \u2022 FREDA3 is also a third-order model. It is FREDA1 and SHARED3, dividing both \"frustratingly simple\" parameters and overlapping tasks."}, {"heading": "5 Related Work", "text": "We point to two important strands of related work.Graph-based parsing. Graph-based parsing was originally invented to handle non-projective syntax (McDonald et al., 2005; Koo et al., 2010; Martins et al., 2013, among others), but has been adapted to semantic parsing (Flanigan et al., 2014; Martins and Almeida, 2014; Thomson et al., 2014; Kuhlmann et al., 2015; Pei et al., 2015, among others).Local structure assessments have traditionally been performed using linear models of hand-generated features, but more recently various forms of representation learning have been explored to learn feature combinations (Lei et al., 2014; Taub-Tabib et al., 2015, among others).Our work is perhaps closest to those that used BiLSTMs to code inputs (Kiperwasser and Goldberg, 2016; Docoro et al., 2016; Wang et al; and zbert."}, {"heading": "6 Conclusion", "text": "We have identified two orthogonal ways to apply deep multi-task learning to graph-based parsing: the first method divides parameters when coding tokens in input with recurrent neural networks, and the second introduces interactions between output structures across formalisms. Without syntactic parsing, these approaches even surpass state-of-the-art semantic dependency analysis systems that use syntax. As our techniques are generally applicable to directed graphs, they can easily be expanded to include other formalisms, semantic or otherwise. In future work, we hope to explore cross-task scoring and consequences for tasks where no parallel annotations are available. Our code is opensource and available at https: / / github. com / Noahs-ARK / NeurboParser."}, {"heading": "Acknowledgements", "text": "We thank Ark, Maxwell Forbes, Luheng He, Kenton Lee, Julian Michael and Jin-ge Yao for their helpful comments on an earlier version of this draft, and the anonymous critics for their valuable feedback. This work was supported by NSF funding IIS-1562364 and DARPA funding FA8750-122-0342 under the DEFT program."}], "references": [{"title": "Lisbon: Evaluating TurboSemanticParser on multiple languages and out-of-domain data", "author": ["Mariana S.C. Almeida", "Andr\u00e9 F.T. Martins."], "venue": "Proc. of SemEval.", "citeRegEx": "Almeida and Martins.,? 2015", "shortCiteRegEx": "Almeida and Martins.", "year": 2015}, {"title": "Many languages, one parser", "author": ["Waleed Ammar", "George Mulcaire", "Miguel Ballesteros", "Chris Dyer", "Noah Smith."], "venue": "TACL 4:431\u2013444.", "citeRegEx": "Ammar et al\\.,? 2016", "shortCiteRegEx": "Ammar et al\\.", "year": 2016}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Rie Kubota Ando", "Tong Zhang."], "venue": "JMLR 6:1817\u20131853.", "citeRegEx": "Ando and Zhang.,? 2005", "shortCiteRegEx": "Ando and Zhang.", "year": 2005}, {"title": "SemEval\u201907 task 19: Frame semantic structure extraction", "author": ["Collin Baker", "Michael Ellsworth", "Katrin Erk."], "venue": "Proc. of SemEval.", "citeRegEx": "Baker et al\\.,? 2007", "shortCiteRegEx": "Baker et al\\.", "year": 2007}, {"title": "Abstract meaning representation for sembanking", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider."], "venue": "Proc. of LAW VII & ID.", "citeRegEx": "Banarescu et al\\.,? 2013", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e8jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "JMLR 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Domain adaptation with structural correspondence learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira."], "venue": "Proc. of EMNLP.", "citeRegEx": "Blitzer et al\\.,? 2006", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Teoria statistica delle classi e calcolo delle probabilit\u00e0", "author": ["Carlo E. Bonferroni."], "venue": "Pubblicazioni del R. Istituto Superiore di Scienze Economiche e Commerciali di Firenze 8:3\u201362.", "citeRegEx": "Bonferroni.,? 1936", "shortCiteRegEx": "Bonferroni.", "year": 1936}, {"title": "Experiments with a higherorder projective dependency parser", "author": ["Xavier Carreras."], "venue": "Proc. of CoNLL.", "citeRegEx": "Carreras.,? 2007", "shortCiteRegEx": "Carreras.", "year": 2007}, {"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Machine Learning 28(1):41\u201375.", "citeRegEx": "Caruana.,? 1997", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "Natural language understanding with distributed representation", "author": ["Kyunghyun Cho."], "venue": "ArXiv:1511.07916.", "citeRegEx": "Cho.,? 2015", "shortCiteRegEx": "Cho.", "year": 2015}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proc. of ICML.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "An open source grammar development environment and broad-coverage English grammar using HPSG", "author": ["Ann Copestake", "Dan Flickinger."], "venue": "Proc. of LREC.", "citeRegEx": "Copestake and Flickinger.,? 2000", "shortCiteRegEx": "Copestake and Flickinger.", "year": 2000}, {"title": "Minimal recursion semantics: An introduction", "author": ["Ann Copestake", "Dan Flickinger", "Ivan A. Sag", "Carl Pollard."], "venue": "Research on Language & Computation 3(4):281\u2013332.", "citeRegEx": "Copestake et al\\.,? 2005", "shortCiteRegEx": "Copestake et al\\.", "year": 2005}, {"title": "Frustratingly easy domain adaptation", "author": ["Hal Daum\u00e9 III."], "venue": "Proc. of ACL.", "citeRegEx": "III.,? 2007", "shortCiteRegEx": "III.", "year": 2007}, {"title": "Universal Stanford dependencies: A cross-linguistic typology", "author": ["Marie-Catherine de Marneffe", "Timothy Dozat", "Natalia Silveira", "Katri Haverinen", "Filip Ginter", "Joakim Nivre", "Christopher D. Manning."], "venue": "Proc. of LREC.", "citeRegEx": "Marneffe et al\\.,? 2014", "shortCiteRegEx": "Marneffe et al\\.", "year": 2014}, {"title": "Deep biaffine attention for neural dependency parsing", "author": ["Timothy Dozat", "Christopher D. Manning."], "venue": "Proc. of ICLR.", "citeRegEx": "Dozat and Manning.,? 2017", "shortCiteRegEx": "Dozat and Manning.", "year": 2017}, {"title": "Peking: Profiling syntactic tree parsing techniques for semantic graph parsing", "author": ["Yantao Du", "Fan Zhang", "Weiwei Sun", "Xiaojun Wan."], "venue": "Proc. of SemEval.", "citeRegEx": "Du et al\\.,? 2014", "shortCiteRegEx": "Du et al\\.", "year": 2014}, {"title": "Peking: Building semantic dependency graphs with a hybrid parser", "author": ["Yantao Du", "Fan Zhang", "Xun Zhang", "Weiwei Sun", "Xiaojun Wan."], "venue": "Proc. of SemEval.", "citeRegEx": "Du et al\\.,? 2015", "shortCiteRegEx": "Du et al\\.", "year": 2015}, {"title": "Semantic role labeling with neural network factors", "author": ["Nicholas FitzGerald", "Oscar T\u00e4ckstr\u00f6m", "Kuzman Ganchev", "Dipanjan Das."], "venue": "Proc. of EMNLP.", "citeRegEx": "FitzGerald et al\\.,? 2015", "shortCiteRegEx": "FitzGerald et al\\.", "year": 2015}, {"title": "A discriminative graph-based parser for the abstract meaning representation", "author": ["Jeffrey Flanigan", "Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith."], "venue": "Proc. of ACL.", "citeRegEx": "Flanigan et al\\.,? 2014", "shortCiteRegEx": "Flanigan et al\\.", "year": 2014}, {"title": "DeepBank: A dynamically annotated treebank of the Wall Street Journal", "author": ["Daniel Flickinger", "Yi Zhang", "Valia Kordoni."], "venue": "Proc. of TLT .", "citeRegEx": "Flickinger et al\\.,? 2012", "shortCiteRegEx": "Flickinger et al\\.", "year": 2012}, {"title": "Automatic labeling of semantic roles", "author": ["Daniel Gildea", "Daniel Jurafsky."], "venue": "Computational Linguistics 28(3):245\u2013288.", "citeRegEx": "Gildea and Jurafsky.,? 2002", "shortCiteRegEx": "Gildea and Jurafsky.", "year": 2002}, {"title": "Supervised Sequence Labelling with Recurrent Neural Networks, volume 385 of Studies in Computational Intelligence", "author": ["Alex Graves."], "venue": "Springer.", "citeRegEx": "Graves.,? 2012", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "ArXiv 1308.0850.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "A universal framework for inductive transfer parsing across multi-typed treebanks", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu."], "venue": "Proc. of COLING.", "citeRegEx": "Guo et al\\.,? 2016", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Multi-lingual joint parsing of syntactic and semantic dependencies with a latent variable model", "author": ["James Henderson", "Paola Merlo", "Ivan Titov", "Gabriele Musillo."], "venue": "Computational Linguistics 39(4):949\u2013998.", "citeRegEx": "Henderson et al\\.,? 2013", "shortCiteRegEx": "Henderson et al\\.", "year": 2013}, {"title": "Semantic frame identification with distributed word representations", "author": ["Karl Moritz Hermann", "Dipanjan Das", "Jason Weston", "Kuzman Ganchev."], "venue": "Proc. of ACL.", "citeRegEx": "Hermann et al\\.,? 2014", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "The expression of a tensor or a polyadic as a sum of products", "author": ["Frank L. Hitchcock."], "venue": "Journal of Mathematical Physics 6(1):164\u2013189.", "citeRegEx": "Hitchcock.,? 1927", "shortCiteRegEx": "Hitchcock.", "year": 1927}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III."], "venue": "Proc. of ACL.", "citeRegEx": "Iyyer et al\\.,? 2015", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Training parsers on incompatible treebanks", "author": ["Richard Johansson."], "venue": "Proc. of NAACL.", "citeRegEx": "Johansson.,? 2013", "shortCiteRegEx": "Johansson.", "year": 2013}, {"title": "Turku: Semantic dependency parsing as a sequence classification", "author": ["Jenna Kanerva", "Juhani Luotolahti", "Filip Ginter."], "venue": "Proc. of SemEval.", "citeRegEx": "Kanerva et al\\.,? 2015", "shortCiteRegEx": "Kanerva et al\\.", "year": 2015}, {"title": "Frustratingly easy neural domain adaptation", "author": ["Young-Bum Kim", "Karl Stratos", "Ruhi Sarikaya."], "venue": "Proc. of COLING.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "Proc. of ICLR.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "TACL 4:313\u2013 327.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Tensor decompositions and applications", "author": ["Tamara G. Kolda", "Brett W. Bader."], "venue": "SIAM Review 51(3):455\u2013500.", "citeRegEx": "Kolda and Bader.,? 2009", "shortCiteRegEx": "Kolda and Bader.", "year": 2009}, {"title": "Dual decomposition for parsing with non-projective head automata", "author": ["Terry Koo", "Alexander M. Rush", "Michael Collins", "Tommi Jaakkola", "David Sontag."], "venue": "Proc. of EMNLP.", "citeRegEx": "Koo et al\\.,? 2010", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "Frame-semantic role labeling with heterogeneous annotations", "author": ["Meghana Kshirsagar", "Sam Thomson", "Nathan Schneider", "Jaime Carbonell", "Noah A. Smith", "Chris Dyer."], "venue": "Proc. of ACL.", "citeRegEx": "Kshirsagar et al\\.,? 2015", "shortCiteRegEx": "Kshirsagar et al\\.", "year": 2015}, {"title": "Link\u00f6ping: Cubic-time graph parsing with a simple scoring scheme", "author": ["Marco Kuhlmann."], "venue": "Proc. of SemEval.", "citeRegEx": "Kuhlmann.,? 2014", "shortCiteRegEx": "Kuhlmann.", "year": 2014}, {"title": "Distilling an ensemble of greedy dependency parsers into one MST parser", "author": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Noah A. Smith."], "venue": "Proc. of EMNLP.", "citeRegEx": "Kuncoro et al\\.,? 2016", "shortCiteRegEx": "Kuncoro et al\\.", "year": 2016}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["Tom Kwiatkowski", "Eunsol Choi", "Yoav Artzi", "Luke S. Zettlemoyer."], "venue": "Proc. of EMNLP.", "citeRegEx": "Kwiatkowski et al\\.,? 2013", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "Low-rank tensors for scoring dependency structures", "author": ["Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proc. of ACL.", "citeRegEx": "Lei et al\\.,? 2014", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "Joint arc-factored parsing of syntactic and semantic dependencies", "author": ["Xavier Llu\u0131\u0301s", "Xavier Carreras", "Llu\u0131\u0301s M\u00e0rquez"], "venue": "TACL", "citeRegEx": "Llu\u0131\u0301s et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Llu\u0131\u0301s et al\\.", "year": 2013}, {"title": "Neural probabilistic model for non-projective MST parsing", "author": ["Xuezhe Ma", "Eduard Hovy."], "venue": "ArXiv 1701.00874.", "citeRegEx": "Ma and Hovy.,? 2016", "shortCiteRegEx": "Ma and Hovy.", "year": 2016}, {"title": "Priberam: A turbo semantic parser with second order features", "author": ["Andr\u00e9 F.T. Martins", "Mariana S.C. Almeida."], "venue": "Proc. of SemEval.", "citeRegEx": "Martins and Almeida.,? 2014", "shortCiteRegEx": "Martins and Almeida.", "year": 2014}, {"title": "Turning on the turbo: Fast third-order non-projective turbo parsers", "author": ["Andr\u00e9 F.T. Martins", "Miguel B. Almeida", "Noah A. Smith."], "venue": "Proc. of ACL.", "citeRegEx": "Martins et al\\.,? 2013", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Concise integer linear programming formulations for dependency parsing", "author": ["Andr\u00e9 F.T. Martins", "Noah Smith", "Eric Xing."], "venue": "Proc. of ACL.", "citeRegEx": "Martins et al\\.,? 2009", "shortCiteRegEx": "Martins et al\\.", "year": 2009}, {"title": "Dual decomposition with many overlapping components", "author": ["Andr\u00e9 F.T. Martins", "Noah A. Smith", "Pedro M.Q. Aguiar", "M\u00e1rio A.T. Figueiredo."], "venue": "Proc. of EMNLP.", "citeRegEx": "Martins et al\\.,? 2011", "shortCiteRegEx": "Martins et al\\.", "year": 2011}, {"title": "Online large-margin training of dependency parsers", "author": ["Ryan McDonald", "Koby Crammer", "Fernando Pereira."], "venue": "Proc. of ACL.", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "From linguistic theory to syntactic analysis: Corpus-oriented grammar development and feature forest model", "author": ["Yusuke Miyao"], "venue": null, "citeRegEx": "Miyao.,? \\Q2006\\E", "shortCiteRegEx": "Miyao.", "year": 2006}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton."], "venue": "Proc. of ICML.", "citeRegEx": "Mnih and Hinton.,? 2007", "shortCiteRegEx": "Mnih and Hinton.", "year": 2007}, {"title": "SemEval 2015 task 18: Broad-coverage semantic dependency parsing", "author": ["Stephan Oepen", "Marco Kuhlmann", "Yusuke Miyao", "Daniel Zeman", "Silvie Cinkov\u00e1", "Dan Flickinger", "Jan Haji\u010d", "Zde\u0148ka Ure\u0161ov\u00e1."], "venue": "Proc. of SemEval.", "citeRegEx": "Oepen et al\\.,? 2015", "shortCiteRegEx": "Oepen et al\\.", "year": 2015}, {"title": "SemEval 2014 task 8: Broad-coverage semantic dependency parsing", "author": ["Stephan Oepen", "Marco Kuhlmann", "Yusuke Miyao", "Daniel Zeman", "Dan Flickinger", "Jan Haji\u010d", "Angelina Ivanova", "Yi Zhang."], "venue": "Proc. of SemEval.", "citeRegEx": "Oepen et al\\.,? 2014", "shortCiteRegEx": "Oepen et al\\.", "year": 2014}, {"title": "The proposition bank: An annotated corpus of semantic roles", "author": ["Martha Palmer", "Daniel Gildea", "Paul Kingsbury."], "venue": "Computational Linguistics 31(1):71\u2013106.", "citeRegEx": "Palmer et al\\.,? 2005", "shortCiteRegEx": "Palmer et al\\.", "year": 2005}, {"title": "An effective neural network model for graph-based dependency parsing", "author": ["Wenzhe Pei", "Tao Ge", "Baobao Chang."], "venue": "Proc. of ACL.", "citeRegEx": "Pei et al\\.,? 2015", "shortCiteRegEx": "Pei et al\\.", "year": 2015}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proc. of EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Head-Driven Phrase Structure Grammar", "author": ["Carl Pollard", "Ivan A. Sag."], "venue": "The University of Chicago Press.", "citeRegEx": "Pollard and Sag.,? 1994", "shortCiteRegEx": "Pollard and Sag.", "year": 1994}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K. Paliwal."], "venue": "IEEE Transactions on Signal Processing 45(11):2673\u20132681.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Dependency parsing by belief propagation", "author": ["David Smith", "Jason Eisner."], "venue": "Proc. of EMNLP.", "citeRegEx": "Smith and Eisner.,? 2008", "shortCiteRegEx": "Smith and Eisner.", "year": 2008}, {"title": "Deep multi-task learning with low level tasks supervised at lower layers", "author": ["Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proc. of ACL.", "citeRegEx": "S\u00f8gaard and Goldberg.,? 2016", "shortCiteRegEx": "S\u00f8gaard and Goldberg.", "year": 2016}, {"title": "Learning distributed representations for structured output prediction", "author": ["Vivek Srikumar", "Christopher D Manning."], "venue": "Proc. of NIPS.", "citeRegEx": "Srikumar and Manning.,? 2014", "shortCiteRegEx": "Srikumar and Manning.", "year": 2014}, {"title": "The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies", "author": ["Mihai Surdeanu", "Richard Johansson", "Adam Meyers", "Llu\u0131\u0301s M\u00e0rquez", "Joakim Nivre"], "venue": "In Proc. of CoNLL", "citeRegEx": "Surdeanu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2008}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proc. of NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Greedy, joint syntacticsemantic parsing with stack LSTMs", "author": ["Swabha Swayamdipta", "Miguel Ballesteros", "Chris Dyer", "Noah A. Smith."], "venue": "Proc. of CoNLL.", "citeRegEx": "Swayamdipta et al\\.,? 2016", "shortCiteRegEx": "Swayamdipta et al\\.", "year": 2016}, {"title": "Max-margin Markov networks", "author": ["Ben Taskar", "Carlos Guestrin", "Daphne Koller."], "venue": "Advances in Neural Information Processing Systems 16.", "citeRegEx": "Taskar et al\\.,? 2004", "shortCiteRegEx": "Taskar et al\\.", "year": 2004}, {"title": "Template kernels for dependency parsing", "author": ["Hillel Taub-Tabib", "Yoav Goldberg", "Amir Globerson."], "venue": "Proc. of NAACL.", "citeRegEx": "Taub.Tabib et al\\.,? 2015", "shortCiteRegEx": "Taub.Tabib et al\\.", "year": 2015}, {"title": "CMU: Arc-factored, discriminative semantic dependency parsing", "author": ["Sam Thomson", "Brendan O\u2019Connor", "Jeffrey Flanigan", "David Bamman", "Jesse Dodge", "Swabha Swayamdipta", "Nathan Schneider", "Chris Dyer", "Noah A. Smith"], "venue": "In Proc. of Se-", "citeRegEx": "Thomson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Thomson et al\\.", "year": 2014}, {"title": "Graph-based dependency parsing with bidirectional LSTM", "author": ["Wenhui Wang", "Baobao Chang."], "venue": "Proc. of ACL.", "citeRegEx": "Wang and Chang.,? 2016", "shortCiteRegEx": "Wang and Chang.", "year": 2016}, {"title": "The Penn Chinese TreeBank: Phrase structure annotation of a large corpus", "author": ["Naiwen Xue", "Fei Xia", "Fu-dong Chiou", "Martha Palmer."], "venue": "Natural Language Engineering 11(2):207\u2013238.", "citeRegEx": "Xue et al\\.,? 2005", "shortCiteRegEx": "Xue et al\\.", "year": 2005}, {"title": "Transition-based parsing for deep dependency structures", "author": ["Xun Zhang", "Yantao Du", "Weiwei Sun", "Xiaojun Wan."], "venue": "Computational Linguistics 42(3):353\u2013389.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Greed is good if randomized: New inference for dependency parsing", "author": ["Yuan Zhang", "Tao Lei", "Regina Barzilay", "Tommi S. Jaakkola."], "venue": "Proc. of EMNLP.", "citeRegEx": "Zhang et al\\.,? 2014", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Stackpropagation: Improved representation learning for syntax", "author": ["Yuan Zhang", "David Weiss."], "venue": "Proc. of ACL.", "citeRegEx": "Zhang and Weiss.,? 2016", "shortCiteRegEx": "Zhang and Weiss.", "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "tations can be exploited using multitask learning (Caruana, 1997), allowing us to learn from more data.", "startOffset": 50, "endOffset": 65}, {"referenceID": 52, "context": "We use the 2015 SemEval shared task on Broad-Coverage Semantic Dependency Parsing (SDP; Oepen et al., 2015) as our testbed.", "startOffset": 82, "endOffset": 107}, {"referenceID": 52, "context": "Numbers taken from Oepen et al. (2015).", "startOffset": 19, "endOffset": 39}, {"referenceID": 53, "context": "First defined in a SemEval 2014 shared task (Oepen et al., 2014), and then extended by Oepen et al.", "startOffset": 44, "endOffset": 64}, {"referenceID": 52, "context": "First defined in a SemEval 2014 shared task (Oepen et al., 2014), and then extended by Oepen et al. (2015), the broad-coverage semantic depency parsing (SDP) task is centered around three semantic formalisms whose annotations have been converted into bilexical dependencies.", "startOffset": 45, "endOffset": 107}, {"referenceID": 22, "context": "While at first glance similar to syntactic dependencies, semantic dependencies have distinct goals and characteristics, more akin to semantic role labeling (SRL; Gildea and Jurafsky, 2002) or the abstract meaning representation (AMR; Banarescu et al.", "startOffset": 156, "endOffset": 188}, {"referenceID": 4, "context": "While at first glance similar to syntactic dependencies, semantic dependencies have distinct goals and characteristics, more akin to semantic role labeling (SRL; Gildea and Jurafsky, 2002) or the abstract meaning representation (AMR; Banarescu et al., 2013).", "startOffset": 228, "endOffset": 257}, {"referenceID": 20, "context": "This is in contrast to AMR whose vertices are abstract concepts, with no explicit alignment to tokens, which makes parsing more difficult (Flanigan et al., 2014).", "startOffset": 138, "endOffset": 161}, {"referenceID": 54, "context": "This is in contrast to PropBank-style SRL, which gives an analysis of only verbal and nominal predicates (Palmer et al., 2005).", "startOffset": 105, "endOffset": 126}, {"referenceID": 53, "context": "Semantic dependency graphs also tend to have higher levels of nonprojectivity than syntactic trees (Oepen et al., 2014).", "startOffset": 99, "endOffset": 119}, {"referenceID": 21, "context": "The DM (DELPH-IN MRS) representation comes from DeepBank (Flickinger et al., 2012), which are manually-corrected parses from the LinGO English Resource Grammar (Copestake and Flickinger, 2000).", "startOffset": 57, "endOffset": 82}, {"referenceID": 12, "context": ", 2012), which are manually-corrected parses from the LinGO English Resource Grammar (Copestake and Flickinger, 2000).", "startOffset": 85, "endOffset": 117}, {"referenceID": 57, "context": "LinGO is a head-driven phrase structure grammar (HPSG; Pollard and Sag, 1994) with minimal recursion semantics (Copestake et al.", "startOffset": 48, "endOffset": 77}, {"referenceID": 13, "context": "LinGO is a head-driven phrase structure grammar (HPSG; Pollard and Sag, 1994) with minimal recursion semantics (Copestake et al., 2005).", "startOffset": 111, "endOffset": 135}, {"referenceID": 50, "context": "The PAS (Predicate-Argument Structures) representation is extracted from the Enju Treebank, which consists of automatic parses from the Enju HPSG parser (Miyao, 2006).", "startOffset": 153, "endOffset": 166}, {"referenceID": 69, "context": "PAS annotations are also available for the Penn Chinese Treebank (Xue et al., 2005).", "startOffset": 65, "endOffset": 83}, {"referenceID": 41, "context": "This may make another disambiguation step necessary to use these representations in a downstream task, but there is evidence that modeling semantic composition separately from grounding in any ontology is an effective way to achieve broad coverage (Kwiatkowski et al., 2013).", "startOffset": 248, "endOffset": 274}, {"referenceID": 52, "context": "have significantly lower performance on it (Oepen et al., 2015).", "startOffset": 43, "endOffset": 63}, {"referenceID": 45, "context": "This approach also underlies state-of-the-art approaches to SDP (Martins and Almeida, 2014).", "startOffset": 64, "endOffset": 91}, {"referenceID": 35, "context": "Our basic model is inspired by recent successes in neural arc-factored graph-based dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Kuncoro et al., 2016).", "startOffset": 102, "endOffset": 181}, {"referenceID": 16, "context": "Our basic model is inspired by recent successes in neural arc-factored graph-based dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Kuncoro et al., 2016).", "startOffset": 102, "endOffset": 181}, {"referenceID": 40, "context": "Our basic model is inspired by recent successes in neural arc-factored graph-based dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Kuncoro et al., 2016).", "startOffset": 102, "endOffset": 181}, {"referenceID": 45, "context": "This list corresponds to the most basic structures used by Martins and Almeida (2014). Selecting an output y corresponds precisely to selecting which instantiations of these structures are included.", "startOffset": 59, "endOffset": 86}, {"referenceID": 20, "context": "We also enforce a determinism constraint (Flanigan et al., 2014): certain labels must not appear on more than one arc emanating from the same token.", "startOffset": 41, "endOffset": 64}, {"referenceID": 48, "context": "Our structures do overlap though, and we employ AD (Martins et al., 2011) to find the highest-scoring internally consistent semantic graph.", "startOffset": 51, "endOffset": 73}, {"referenceID": 35, "context": "Similarly to Kiperwasser and Goldberg (2016), our model learns representations of tokens in a sentence using a bi-directional LSTM (BiLSTM).", "startOffset": 13, "endOffset": 45}, {"referenceID": 29, "context": "Long shortterm memory networks (LSTMs) are a variant of recurrent neural networks (RNNs) designed to alleviate the vanishing gradient problem in RNNs (Hochreiter and Schmidhuber, 1997).", "startOffset": 150, "endOffset": 184}, {"referenceID": 58, "context": "A bi-directional LSTM (BiLSTM) runs over the sequence in both directions (Schuster and Paliwal, 1997; Graves, 2012).", "startOffset": 73, "endOffset": 115}, {"referenceID": 23, "context": "A bi-directional LSTM (BiLSTM) runs over the sequence in both directions (Schuster and Paliwal, 1997; Graves, 2012).", "startOffset": 73, "endOffset": 115}, {"referenceID": 10, "context": "3); we refer the readers to Cho (2015) for technical details.", "startOffset": 28, "endOffset": 39}, {"referenceID": 65, "context": "c is a weighted Hamming distance that trades off between precision and recall (Taskar et al., 2004).", "startOffset": 78, "endOffset": 99}, {"referenceID": 45, "context": "Following Martins and Almeida (2014), we encourage recall over precision by using the costs 0.", "startOffset": 10, "endOffset": 37}, {"referenceID": 0, "context": "3 We split as in previous work (Almeida and Martins, 2015; Du et al., 2015), resulting in 33,964 training sentences from \u00a700\u201319 of the WSJ corpus, 1,692 development sentences from \u00a720, 1,410 sentences from \u00a721 as in-domain test data, and 1,849 sentences sampled from the Brown Corpus as out-of-domain test data.", "startOffset": 31, "endOffset": 75}, {"referenceID": 18, "context": "3 We split as in previous work (Almeida and Martins, 2015; Du et al., 2015), resulting in 33,964 training sentences from \u00a700\u201319 of the WSJ corpus, 1,692 development sentences from \u00a720, 1,410 sentences from \u00a721 as in-domain test data, and 1,849 sentences sampled from the Brown Corpus as out-of-domain test data.", "startOffset": 31, "endOffset": 75}, {"referenceID": 7, "context": "Underlines indicate statistical significance with Bonferroni (1936) correction compared to the best baseline system.", "startOffset": 50, "endOffset": 68}, {"referenceID": 16, "context": "As our model uses no explicit syntactic information, the most comparable models to ours are two state-of-the-art closed track systems due to Du et al. (2015) and Almeida and Martins (2015).", "startOffset": 141, "endOffset": 158}, {"referenceID": 0, "context": "(2015) and Almeida and Martins (2015). Du et al.", "startOffset": 11, "endOffset": 38}, {"referenceID": 0, "context": "(2015) and Almeida and Martins (2015). Du et al. (2015) rely on graphtree transformation techniques proposed by Du et al.", "startOffset": 11, "endOffset": 56}, {"referenceID": 0, "context": "(2015) and Almeida and Martins (2015). Du et al. (2015) rely on graphtree transformation techniques proposed by Du et al. (2014), and apply a voting ensemble to wellstudied tree-oriented parsers.", "startOffset": 11, "endOffset": 129}, {"referenceID": 0, "context": "(2015) and Almeida and Martins (2015). Du et al. (2015) rely on graphtree transformation techniques proposed by Du et al. (2014), and apply a voting ensemble to wellstudied tree-oriented parsers. Closely related to ours is Almeida and Martins (2015), who used rich, hand-engineered second-order features and AD for inference.", "startOffset": 11, "endOffset": 250}, {"referenceID": 60, "context": "A common approach when using BiLSTMs for multitask learning is to share the BiLSTM part of the model across tasks, while training specialized classifiers for each task (S\u00f8gaard and Goldberg, 2016).", "startOffset": 168, "endOffset": 196}, {"referenceID": 33, "context": "FREDA uses a neural generalization of \u201cfrustratingly easy\u201d domain adaptation (Daum\u00e9 III, 2007; Kim et al., 2016), where one augments domainspecific features with a shared set of features to capture global patterns.", "startOffset": 77, "endOffset": 112}, {"referenceID": 8, "context": "In syntactic parsing, higher-order structures have commonly been used to model interactions between multiple adjacent arcs in the same dependency tree (Carreras, 2007; Smith and Eisner, 2008; Martins et al., 2009; Zhang et al., 2014, inter alia). Llu\u0131\u0301s et al. (2013), in contrast, used second-order structures to jointly model syntactic dependencies and semantic roles.", "startOffset": 152, "endOffset": 268}, {"referenceID": 42, "context": "Borrowing from Lei et al. (2014), we introduce a low-rank tensor scoring strategy that, given a higher-order structure p, models interactions between the firstorder structures (i.", "startOffset": 15, "endOffset": 33}, {"referenceID": 28, "context": "Using the fact that a tensor of rank at most r can be decomposed into a sum of r rank-1 tensors (Hitchcock, 1927), we reparameterize W to enforce the low-rank constraint by construction:", "startOffset": 96, "endOffset": 113}, {"referenceID": 41, "context": "Following Lei et al. (2014), we upperbound the rank of W by r to limit the number of parameters (r is a hyperparameter, decided empirically).", "startOffset": 10, "endOffset": 28}, {"referenceID": 36, "context": "(11) We refer readers to Kolda and Bader (2009) for mathematical details.", "startOffset": 25, "endOffset": 48}, {"referenceID": 56, "context": "We use 100-dimensional GloVe (Pennington et al., 2014) vectors trained over Wikipedia and Gigaword as pre-trained word embeddings.", "startOffset": 29, "endOffset": 54}, {"referenceID": 30, "context": "To deal with out-of-vocabulary words, we apply word dropout (Iyyer et al., 2015) and randomly replace a word w with a special unksymbol with probability \u03b1 1+#(w) , where #(w) is the count of w in the training set.", "startOffset": 60, "endOffset": 80}, {"referenceID": 34, "context": "Models are trained for up to 30 epochs with Adam (Kingma and Ba, 2015), with \u03b21 = \u03b22 = 0.", "startOffset": 49, "endOffset": 70}, {"referenceID": 16, "context": "5 every 10 epochs (Dozat and Manning, 2017).", "startOffset": 18, "endOffset": 43}, {"referenceID": 24, "context": "We clip the `2 norm of gradients to 1 (Graves, 2013; Sutskever et al., 2014), and we do not use mini-batches.", "startOffset": 38, "endOffset": 76}, {"referenceID": 63, "context": "We clip the `2 norm of gradients to 1 (Graves, 2013; Sutskever et al., 2014), and we do not use mini-batches.", "startOffset": 38, "endOffset": 76}, {"referenceID": 45, "context": "We use the same pruner as Martins and Almeida (2014), where a first-order feature-rich unlabeled pruning model is trained for each task, and arcs with posterior probability below 10\u22124 are discarded.", "startOffset": 26, "endOffset": 53}, {"referenceID": 0, "context": "In addition, we also examine the effects of syntax by comparing our models to the state-of-the-art open track system (Almeida and Martins, 2015).", "startOffset": 117, "endOffset": 144}, {"referenceID": 0, "context": "We observe similar trends on the out-of-domain test set (Table 4b), with the exception that, on PSD, our best-performing model\u2019s improvement over the open-track system of Almeida and Martins (2015) is not statistically significant.", "startOffset": 171, "endOffset": 198}, {"referenceID": 0, "context": "With automatically generated syntactic parses, Almeida and Martins (2015) manage to obtain more than 1% absolute improvements over their closed track en-", "startOffset": 47, "endOffset": 74}, {"referenceID": 67, "context": "try, which is consistent with the extensive evaluation by Zhang et al. (2016), but we leave the incorporation of syntactic trees to future work.", "startOffset": 58, "endOffset": 78}, {"referenceID": 42, "context": "Syntactic parsing could be treated as yet another output task, as explored in Llu\u0131\u0301s et al. (2013) and in the transition-based frameworks of Henderson et al.", "startOffset": 78, "endOffset": 99}, {"referenceID": 26, "context": "(2013) and in the transition-based frameworks of Henderson et al. (2013) and Swayamdipta et al.", "startOffset": 49, "endOffset": 73}, {"referenceID": 26, "context": "(2013) and in the transition-based frameworks of Henderson et al. (2013) and Swayamdipta et al. (2016).", "startOffset": 49, "endOffset": 103}, {"referenceID": 52, "context": "Scores from Oepen et al. (2015).", "startOffset": 12, "endOffset": 32}, {"referenceID": 35, "context": "Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016).", "startOffset": 71, "endOffset": 191}, {"referenceID": 40, "context": "Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016).", "startOffset": 71, "endOffset": 191}, {"referenceID": 68, "context": "Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016).", "startOffset": 71, "endOffset": 191}, {"referenceID": 16, "context": "Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016).", "startOffset": 71, "endOffset": 191}, {"referenceID": 44, "context": "Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016).", "startOffset": 71, "endOffset": 191}, {"referenceID": 2, "context": "Successes in multitask learning have been enabled by advances in representation learning as well as earlier explorations of parameter sharing (Ando and Zhang, 2005; Blitzer et al., 2006; Daum\u00e9 III, 2007).", "startOffset": 142, "endOffset": 203}, {"referenceID": 6, "context": "Successes in multitask learning have been enabled by advances in representation learning as well as earlier explorations of parameter sharing (Ando and Zhang, 2005; Blitzer et al., 2006; Daum\u00e9 III, 2007).", "startOffset": 142, "endOffset": 203}, {"referenceID": 8, "context": "Collobert and Weston (2008) proposed sharing the same word representation while solving multiple NLP tasks.", "startOffset": 0, "endOffset": 28}, {"referenceID": 8, "context": "Collobert and Weston (2008) proposed sharing the same word representation while solving multiple NLP tasks. Zhang and Weiss (2016) use a continuous stacking model for POS tagging and parsing.", "startOffset": 0, "endOffset": 131}, {"referenceID": 1, "context": "Ammar et al. (2016) and Guo et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Ammar et al. (2016) and Guo et al. (2016) explored parameter sharing for multilingual parsing.", "startOffset": 0, "endOffset": 42}, {"referenceID": 1, "context": "Ammar et al. (2016) and Guo et al. (2016) explored parameter sharing for multilingual parsing. Johansson (2013) and Kshirsagar et al.", "startOffset": 0, "endOffset": 112}, {"referenceID": 1, "context": "Ammar et al. (2016) and Guo et al. (2016) explored parameter sharing for multilingual parsing. Johansson (2013) and Kshirsagar et al. (2015) applied ideas from domain adaptation to multitask learning.", "startOffset": 0, "endOffset": 141}], "year": 2017, "abstractText": "We present a deep neural architecture that parses sentences into three semantic dependency graph formalisms. By using efficient, nearly arc-factored inference and a bidirectional-LSTM composed with a multi-layer perceptron, our base system is able to significantly improve the state of the art for semantic dependency parsing, without using hand-engineered features or syntax. We then explore two multitask learning approaches\u2014one that shares parameters across formalisms, and one that uses higher-order structures to predict the graphs jointly. We find that both approaches improve performance across formalisms on average, achieving a new state of the art. Our code is open-source and available at https://github.com/ Noahs-ARK/NeurboParser.", "creator": "LaTeX with hyperref package"}}}