{"id": "1704.06869", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2017", "title": "Argument Mining with Structured SVMs and RNNs", "abstract": "We propose a novel factor graph model for argument mining, designed for settings in which the argumentative relations in a document do not necessarily form a tree structure. (This is the case in over 20% of the web comments dataset we release.) Our model jointly learns elementary unit type classification and argumentative relation prediction. Moreover, our model supports SVM and RNN parametrizations, can enforce structure constraints (e.g., transitivity), and can express dependencies between adjacent relations and propositions. Our approaches outperform unstructured baselines in both web comments and argumentative essay datasets.", "histories": [["v1", "Sun, 23 Apr 2017 01:14:55 GMT  (52kb,D)", "http://arxiv.org/abs/1704.06869v1", "Accepted for publication at ACL 2017. 11 pages, 5 figures. Code atthis https URLand data atthis http URL"]], "COMMENTS": "Accepted for publication at ACL 2017. 11 pages, 5 figures. Code atthis https URLand data atthis http URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vlad niculae", "joonsuk park", "claire cardie"], "accepted": true, "id": "1704.06869"}, "pdf": {"name": "1704.06869.pdf", "metadata": {"source": "CRF", "title": "Argument Mining with Structured SVMs and RNNs", "authors": ["Vlad Niculae", "Joonsuk Park", "Claire Cardie"], "emails": ["vlad@cs.cornell.edu", "jpark@cs.williams.edu", "cardie@cs.cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, \"he said,\" we've never lost so much time, \"he said.\" We've never lost so much time, \"he said.\" We've never lost so much time, \"he said.\" We've never lost so much time as this year, \"he said.\" We've never lost so much time, \"he said,\" but we've never lost so much time. \""}, {"heading": "2 Related work", "text": "In this context, it should be noted that this is not a single case, but a case in which the presumption implies that it is a presumption that it is a presumption, a presumption of the presumption, a presumption of the presumption, a presumption of the presumption, a presumption of the presumption, a presumption of the presumption, a presumption of the presumption, a presumption of the presumption, a presumption of the presumption, a presumption of the presumption, a presumption of the presupposition, a presumption of the presupposition, a presupposition of the presupposition, a presupposition of the presupposition, a presupposition of the presupposition, a presupposition, a presupposition, a presupposition, a presupposition, a presupposition, a presupposition, a presupposition, a presupposition, a presupposition, a presupposition, a presupposition, a presupposition, a presupposition, a presupposition, a presupposition, a presupposition, a presupposition, a presupposition, a presupposition, a presupposition, a presupposition, a presupposition, or a presupposition, a presupposition, a presupposition, a presupposition, a presupposition, or a presupposition, a presupposition, a presupposition, a presupposition, or a presupposition, a presupposition, a presupposition, a presupposition, or a presupposition, a presupposition, a presupposition, a presupposition, or a presupposition, a presupposition, or a presupposition, a presupposition, or a presupposition, a presupposition, or a presupposition, a presupposition, or a presupposition, a presupposition, or a presupposition, a presupposition, a presup"}, {"heading": "3 Data", "text": "We publish a new argument Mining Dataset consisting of user comments on proposed rules relating to Consumer Debt Collection Practices (CDCP) collected by the Consumer Financial Protection Bureau from an eRulemaking website, http: / / regulationroom.org.Argumentation structures found in web discussion forums, such as the eRulemaking One we use, may be freer than those encountered in controlled, provoked letters such as (Peldszus and Stede, 2015).For this reason, we adopt the model proposed by Park et al. (2015a), which does not include links to tree structure formation, but fully directed graphics. In fact, over 20% of the comments in our dataset contain argumentative structures that would not be allowed in a tree. Possible link types are reason and evidence, and propagation types are divided into five fine-grained categories: POLICY and VALUE contain subjective judgments / interpretations that are not differentiated from those in IMIL."}, {"heading": "3.1 Annotation results", "text": "Each user comment was commented on by two commentators, who independently commented on the boundaries and types of suggestions, as well as the links to each other.4 To create the final corpus, a third commentator manually resolved the conflicts using 5 and two automatic pre-processing steps: We remove the transitive closure of the link and remove a small number of nested suggestions. 6 The resulting data set contains 731 comments, consisting of about 3,800 sentences (about 4,700 suggestions) and 88k words. Of the 43k possible proposition pairs, there are links between only 1,300 (about 3%), whereas UKP has fewer documents (402), but they are longer, with a total of 7,100 sentences (6,100 suggestions) and 147k4The commentators used the GATE annotation tool (Cunningham et al., 2011)."}, {"heading": "4 Structured learning", "text": "for argumentation mining"}, {"heading": "4.1 Preliminaries", "text": "Binary and multi-class classification have been used with some success to formulate and link predictions separately, but we are looking for a way to jointly learn the problem of argument mining at the document level in order to better model contextual dependencies and constraints. We are therefore turning to structured learning, a framework that provides the desired level of expressivity. In general, learning from a dataset of documents xi-X and its associated designations yi-Y involves searching for model parameters w that can \"select\" the best designation under a scoring function f: y: = argmaxy-Y f (x, y; w). (1) Unlike classification or regression, where X is usually a feature space Rd and Y-R (e.g. we predict a holistic class index or probability), structured learning allows more complex inputs and outputs. This makes it impossible to evaluate the structuring factor suggested by 1, so that it is worth having them expressed by equal numeration in 1."}, {"heading": "4.2 Model description", "text": "The question we need to ask ourselves is whether this is a type of term that we can actually use in terms of the way in which these terms are used, even if they may be of different types, and this can be addressed in our model by including the possible proposition types for the two datasets. Possible proposition types P differ in the way they are documented in Table 1. How we describe the variables and factors that represent a factor diagram, we will refer to Figure 2 for illustration.Unary potentials we refer to. Each proposition a and each linkage has a random variable in the factor diagram that we refer to Figure 2 for."}, {"heading": "4.3 Argument structure SVM", "text": "In fact, most of them are able to move to another world in which they are able to integrate, and in which they are able to integrate."}, {"heading": "4.4 Argument structure RNN", "text": "We have proven effective for natural language problems, even with minimal-to-no feature engineering. Inspired by the use of LSTMs (Hochreiter and Schmidhuber, 1997) for the MST dependence on Kiperwasser and Goldberg (2016), we parameterize the potentials in our factor diagram with an LSTM-based neural network, 9 replace the MST inference with the more general AD3 algorithm, and use relaxed solutions for training when the inference is inexact.We extract the potentials of all words with a corpus frequency > 1, initialized with GloVe word vectors. We use a deep bi-directional LSTM to encode contextual information that represents as an average of the LSTM outputs of their words, from which we continuously identify the potentials of the proposition. We apply a multi-layer perceptron (MLP) with profiles of final rectified levels to each output, not exceeded at the final activation of the final level."}, {"heading": "4.5 Baseline models", "text": "We compare our proposed models with equivalent independent unit classifiers. The standard version of a structured SVM is an l2-regulated linear SVM.10 For the RNN, we calculate unary potentials in the same way as in the structured model, but apply independent hinged losses to each variable instead of the global structured hinged loss. Since the RNN weights are divided, this is a form of multi-task learning. Basic forecasts can be 10 We train our SVM with the help of SAGA (Defazio et al., 2014) in Lightning (Blondel and Pedregosa, 2016). They can be interpreted as unary potentials, so we can simply round their output to the highest scoring labels, or we can alternatively perform test time references and impose the desired structure."}, {"heading": "5 Results", "text": "For model selection and development, we used kfold cross-validation at the document level: On CDCP, we use k = 3 to avoid small validation folds, while following UKP staff and Gurevych (2016) by using k = 5. We compare our proposed structured learning systems (the linearly structured SVM and the structured RNN) with the corresponding baseline versions. We organize our experiments into three incremental variants of our factor graph: simple, complete and strict, each with the following components: 11 components basic complete strict (baseline) inconsistencies X X X X-X linkages. The features X X X higher X-X linkages structure X X X-X strict constraints XFollowing staff and Gurevych (2016), we calculate F1 values at the proposition and link levels and report their average linkages as a summary of the overall performance."}, {"heading": "5.1 Discussion and analysis", "text": "There are only four possible configurations of compatibility characteristics, and we can show all cases in Figure 3, next to the basic model. The basic model can only learn whether certain configurations are more likely than others. The complete model with compatibility characteristics is more subtle, but it is unlikely that there is a connection between other countries. It is very likely that the links between individual countries do not work."}, {"heading": "6 Conclusions and future work", "text": "We introduce an argumentation analysis model based on AD3-loose conclusions in expressive factor graphs and experiment with both linear structured SVMs and structured RNNs, parameterized with higher order factors and link structure constraints. We demonstrate our model using a new argumentation mining data set with more liberal argumentation structure annotation. Our model also achieves state-of-the-art link prediction performance on UKP paper records. Future work. Stab and Gurevych (2016) found polynomial nuclei useful for modeling functional interactions, but nucleus SVMs scale poorly, we intend to investigate alternative methods for recording functional interactions. While we focus on monological reasoning, our model could be extended to dialogues that do not motivate tree-like structures for argumentation theory (Afantenos and Asher, 2014)."}, {"heading": "Acknowledgements", "text": "We would like to thank Andre \u0301 Martins, Andreas Mueller, Arzoo Katyiar, Chenhao Tan, Felix Wu, Jack Hessel, Justine Zhang, Mathieu Blondel, Tianze Shi, Tobias Schnabel and the rest of the Cornell NLP Seminar for the extremely helpful discussions. We would like to thank the anonymous critics for their thorough and well-founded feedback."}], "references": [{"title": "Counterargumentation and discourse: A case study", "author": ["References Stergos Afantenos", "Nicholas Asher."], "venue": "Proceedings of ArgNLP.", "citeRegEx": "Afantenos and Asher.,? 2014", "shortCiteRegEx": "Afantenos and Asher.", "year": 2014}, {"title": "Polynomial networks and factorization machines: New insights and efficient training algorithms", "author": ["Mathieu Blondel", "Masakazu Ishihata", "Akinori Fujino", "Naonori Ueda."], "venue": "Proceedings of ICML.", "citeRegEx": "Blondel et al\\.,? 2016", "shortCiteRegEx": "Blondel et al\\.", "year": 2016}, {"title": "Lightning: large-scale linear classification, regression and ranking in Python", "author": ["Mathieu Blondel", "Fabian Pedregosa."], "venue": "https://doi.org/10.5281/zenodo.200504.", "citeRegEx": "Blondel and Pedregosa.,? 2016", "shortCiteRegEx": "Blondel and Pedregosa.", "year": 2016}, {"title": "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["Aaron Defazio", "Francis Bach", "Simon LacosteJulien."], "venue": "Proceedings of NIPS.", "citeRegEx": "Defazio et al\\.,? 2014", "shortCiteRegEx": "Defazio et al\\.", "year": 2014}, {"title": "Argumentation mining in user-generated web discourse", "author": ["Ivan Habernal", "Iryna Gurevych."], "venue": "Computational Linguistics .", "citeRegEx": "Habernal and Gurevych.,? 2016", "shortCiteRegEx": "Habernal and Gurevych.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Investigating LSTMs for joint extraction of opinion entities and relations", "author": ["Arzoo Katiyar", "Claire Cardie."], "venue": "Proceedings of ACL.", "citeRegEx": "Katiyar and Cardie.,? 2016", "shortCiteRegEx": "Katiyar and Cardie.", "year": 2016}, {"title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "arXiv:1603.04351 preprint.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Content Analysis: An Introduction to Its Methodology", "author": ["Klaus Krippendorff."], "venue": "Commtext. Sage.", "citeRegEx": "Krippendorff.,? 1980", "shortCiteRegEx": "Krippendorff.", "year": 1980}, {"title": "Factor graphs and the sum-product algorithm", "author": ["Frank R Kschischang", "Brendan J Frey", "H-A Loeliger."], "venue": "IEEE Transactions on Information Theory 47(2):498\u2013519.", "citeRegEx": "Kschischang et al\\.,? 2001", "shortCiteRegEx": "Kschischang et al\\.", "year": 2001}, {"title": "Block-coordinate Frank-Wolfe optimization for structural SVMs", "author": ["Simon Lacoste-Julien", "Martin Jaggi", "Mark Schmidt", "Patrick Pletscher."], "venue": "Proceedings of ICML.", "citeRegEx": "Lacoste.Julien et al\\.,? 2013", "shortCiteRegEx": "Lacoste.Julien et al\\.", "year": 2013}, {"title": "A PDTB-styled end-to-end discourse parser", "author": ["Ziheng Lin", "Hwee Tou Ng", "Min-Yen Kan."], "venue": "Natural Language Engineering 20(02):151\u2013184.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Priberam: A Turbo Semantic Parser with second order features", "author": ["Andr\u00e9 FT Martins", "Mariana SC Almeida."], "venue": "Proceedings of SemEval.", "citeRegEx": "Martins and Almeida.,? 2014", "shortCiteRegEx": "Martins and Almeida.", "year": 2014}, {"title": "Turning on the Turbo: Fast thirdorder non-projective Turbo Parsers", "author": ["Andr\u00e9 FT Martins", "Miguel B Almeida", "Noah A Smith."], "venue": "Proceedings of ACL.", "citeRegEx": "Martins et al\\.,? 2013", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "AD3: Alternating directions dual decomposition for MAP inference in graphical models", "author": ["Andr\u00e9 FT Martins", "M\u00e1rio AT Figueiredo", "Pedro MQ Aguiar", "Noah A Smith", "Eric P Xing."], "venue": "Journal of Machine Learning Research 16:495\u2013545.", "citeRegEx": "Martins et al\\.,? 2015", "shortCiteRegEx": "Martins et al\\.", "year": 2015}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["Ryan McDonald", "Fernando Pereira", "Kiril Ribarov", "Jan Haji\u010d."], "venue": "Proceedings of EMNLP.", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Train and test tightness of LP relaxations in structured prediction", "author": ["Ofer Meshi", "Mehrdad Mahdavi", "Adrian Weller", "David Sontag."], "venue": "Proceedings of ICML.", "citeRegEx": "Meshi et al\\.,? 2016", "shortCiteRegEx": "Meshi et al\\.", "year": 2016}, {"title": "PyStruct: learning structured prediction in Python", "author": ["Andreas C M\u00fcller", "Sven Behnke."], "venue": "Journal of Machine Learning Research 15(1):2055\u20132060.", "citeRegEx": "M\u00fcller and Behnke.,? 2014", "shortCiteRegEx": "M\u00fcller and Behnke.", "year": 2014}, {"title": "DyNet: The dynamic neural network toolkit", "author": ["Ji", "Lingpeng Kong", "Adhiguna Kuncoro", "Gaurav Kumar", "Chaitanya Malaviya", "Paul Michel", "Yusuke Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin"], "venue": null, "citeRegEx": "Ji et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2017}, {"title": "Toward machine-assisted participation in eRulemaking: An argumentation model of evaluability", "author": ["Joonsuk Park", "Cheryl Blake", "Claire Cardie."], "venue": "Proceedings of ICAIL.", "citeRegEx": "Park et al\\.,? 2015a", "shortCiteRegEx": "Park et al\\.", "year": 2015}, {"title": "Conditional random fields for identifying appropriate types of support for propositions in online user comments", "author": ["Joonsuk Park", "Arzoo Katiyar", "Bishan Yang."], "venue": "Proceedings of the 2nd Workshop on Argumentation Mining. Association for Compu-", "citeRegEx": "Park et al\\.,? 2015b", "shortCiteRegEx": "Park et al\\.", "year": 2015}, {"title": "Joint prediction in MST-style discourse parsing for argumentation mining", "author": ["Andreas Peldszus", "Manfred Stede."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Peldszus and Stede.,? 2015", "shortCiteRegEx": "Peldszus and Stede.", "year": 2015}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Here\u2019s my point: Argumentation mining with pointer networks", "author": ["Peter Potash", "Alexey Romanov", "Anna Rumshisky."], "venue": "arXiv:1612.08994 preprint.", "citeRegEx": "Potash et al\\.,? 2016", "shortCiteRegEx": "Potash et al\\.", "year": 2016}, {"title": "Parsing argumentation structures in persuasive essays", "author": ["Christian Stab", "Iryna Gurevych."], "venue": "arXiv:1604.07370 preprint.", "citeRegEx": "Stab and Gurevych.,? 2016", "shortCiteRegEx": "Stab and Gurevych.", "year": 2016}, {"title": "Max-margin Markov networks", "author": ["Ben Taskar", "Carlos Guestrin", "Daphne Koller."], "venue": "Proceedings of NIPS.", "citeRegEx": "Taskar et al\\.,? 2004", "shortCiteRegEx": "Taskar et al\\.", "year": 2004}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["Ioannis Tsochantaridis", "Thorsten Joachims", "Thomas Hofmann", "Yasemin Altun."], "venue": "Journal of Machine Learning Research 6(Sep):1453\u20131484.", "citeRegEx": "Tsochantaridis et al\\.,? 2005", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 21, "context": "Most annotation and prediction efforts in argument mining have focused on tree or forest structures (Peldszus and Stede, 2015; Stab and Gurevych, 2016), constraining argument structures to form one or more trees.", "startOffset": 100, "endOffset": 151}, {"referenceID": 24, "context": "Most annotation and prediction efforts in argument mining have focused on tree or forest structures (Peldszus and Stede, 2015; Stab and Gurevych, 2016), constraining argument structures to form one or more trees.", "startOffset": 100, "endOffset": 151}, {"referenceID": 9, "context": "By formulating argument mining as inference in a factor graph (Kschischang et al., 2001), our model (described in Section 4) can account for correlations between the two tasks, can consider second order link structures (e.", "startOffset": 62, "endOffset": 88}, {"referenceID": 26, "context": "(Tsochantaridis et al., 2005), and recurrent neural networks with structured loss, extending (Kiperwasser and Goldberg, 2016).", "startOffset": 0, "endOffset": 29}, {"referenceID": 7, "context": ", 2005), and recurrent neural networks with structured loss, extending (Kiperwasser and Goldberg, 2016).", "startOffset": 71, "endOffset": 103}, {"referenceID": 24, "context": "Secondly, on the UKP argumentative essays v2 (henceforth UKP), where argument graphs are annotated strictly as multiple trees (Stab and Gurevych, 2016).", "startOffset": 126, "endOffset": 151}, {"referenceID": 24, "context": "On UKP, we improve link prediction over the best reported result in (Stab and Gurevych, 2016), which is based on integer linear programming postprocessing.", "startOffset": 68, "endOffset": 93}, {"referenceID": 14, "context": "In particular, maximum spanning tree (MST) methods for arc-factored dependency parsing have been successfully used by McDonald et al. (2005) and applied to argument mining with mixed results by Peldszus and Stede (2015).", "startOffset": 118, "endOffset": 141}, {"referenceID": 14, "context": "In particular, maximum spanning tree (MST) methods for arc-factored dependency parsing have been successfully used by McDonald et al. (2005) and applied to argument mining with mixed results by Peldszus and Stede (2015). As they are not designed for the task, MST parsers cannot directly handle proposition classification or model the correlation between proposition and link prediction\u2014a limitation our model addresses.", "startOffset": 118, "endOffset": 220}, {"referenceID": 7, "context": "Using RNN features in an MST parser with a structured loss was proposed by Kiperwasser and Goldberg (2016); their model can be seen as a particular case of our factor graph approach, limited to link prediction with a tree structure constraint.", "startOffset": 75, "endOffset": 107}, {"referenceID": 14, "context": "Furthermore, our model can enforce the tree structure constraint, required on the UKP dataset, using MST cycle constraints used by Stab and Gurevych (2016), thanks to the AD3 inference algorithm (Martins et al., 2015).", "startOffset": 195, "endOffset": 217}, {"referenceID": 12, "context": "Such higher-order structures and logic constraints have been successfully used for dependency and semantic parsing by Martins et al. (2013) and Martins and Almeida (2014); to our knowledge we are the first to apply them to argument mining, as well as the first to parametrize them with neural networks.", "startOffset": 118, "endOffset": 140}, {"referenceID": 12, "context": "(2013) and Martins and Almeida (2014); to our knowledge we are the first to apply them to argument mining, as well as the first to parametrize them with neural networks.", "startOffset": 11, "endOffset": 38}, {"referenceID": 12, "context": "(2013) and Martins and Almeida (2014); to our knowledge we are the first to apply them to argument mining, as well as the first to parametrize them with neural networks. Stab and Gurevych (2016) used an integer linear program to combine the output of independent proposition and link classifiers using a hand-crafted scoring formula, an approach similar to our baseline.", "startOffset": 11, "endOffset": 195}, {"referenceID": 12, "context": "(2013) and Martins and Almeida (2014); to our knowledge we are the first to apply them to argument mining, as well as the first to parametrize them with neural networks. Stab and Gurevych (2016) used an integer linear program to combine the output of independent proposition and link classifiers using a hand-crafted scoring formula, an approach similar to our baseline. Our factor graph method can combine the two tasks in a more principled way, as it fully learns the correlation between the two tasks without relying on hand-crafted scoring, and therefore can readily be applied to other argumentation datasets. Furthermore, our model can enforce the tree structure constraint, required on the UKP dataset, using MST cycle constraints used by Stab and Gurevych (2016), thanks to the AD3 inference algorithm (Martins et al.", "startOffset": 11, "endOffset": 771}, {"referenceID": 24, "context": "Sequence tagging has been applied to the related structured tasks of proposition identification and classification (Stab and Gurevych, 2016; Habernal and Gurevych, 2016; Park et al., 2015b); integrating such models is an important next step.", "startOffset": 115, "endOffset": 189}, {"referenceID": 4, "context": "Sequence tagging has been applied to the related structured tasks of proposition identification and classification (Stab and Gurevych, 2016; Habernal and Gurevych, 2016; Park et al., 2015b); integrating such models is an important next step.", "startOffset": 115, "endOffset": 189}, {"referenceID": 20, "context": "Sequence tagging has been applied to the related structured tasks of proposition identification and classification (Stab and Gurevych, 2016; Habernal and Gurevych, 2016; Park et al., 2015b); integrating such models is an important next step.", "startOffset": 115, "endOffset": 189}, {"referenceID": 23, "context": "Meanwhile, a new direction in argument mining explores pointer networks (Potash et al., 2016); a promising method, currently lacking support for tree structures and domain-specific constraints.", "startOffset": 72, "endOffset": 93}, {"referenceID": 21, "context": "Argumentation structures found in web discussion forums, such as the eRulemaking one we use, can be more free-form than the ones encountered in controlled, elicited writing such as (Peldszus and Stede, 2015).", "startOffset": 181, "endOffset": 207}, {"referenceID": 19, "context": "For this reason, we adopt the model proposed by Park et al. (2015a), which does not constrain links to form tree structures, but unrestricted directed graphs.", "startOffset": 48, "endOffset": 68}, {"referenceID": 24, "context": "In comparison, the UKP dataset (Stab and Gurevych, 2016) only makes the syntactic distinction between CLAIM, MAJOR CLAIM, and PREMISE types, but it also includes attack links.", "startOffset": 31, "endOffset": 56}, {"referenceID": 8, "context": "Inter-annotator agreement is measured with Krippendorf\u2019s \u03b1 (Krippendorff, 1980) with respect to elementary unit type (\u03b1=64.", "startOffset": 59, "endOffset": 79}, {"referenceID": 24, "context": "For simplicity and comparability, we follow Stab and Gurevych (2016) in using binary link labels even if links could be of different types.", "startOffset": 44, "endOffset": 69}, {"referenceID": 13, "context": "As in higher-order structured models for semantic and dependency parsing (Martins et al., 2013; Martins and Almeida, 2014), we implement three types of second order factors: grandparent (a \u2192 b \u2192 c), sibling (a \u2190 b \u2192 c), and co-parent (a \u2192 b \u2190 c).", "startOffset": 73, "endOffset": 122}, {"referenceID": 12, "context": "As in higher-order structured models for semantic and dependency parsing (Martins et al., 2013; Martins and Almeida, 2014), we implement three types of second order factors: grandparent (a \u2192 b \u2192 c), sibling (a \u2190 b \u2192 c), and co-parent (a \u2192 b \u2190 c).", "startOffset": 73, "endOffset": 122}, {"referenceID": 19, "context": "For instance, proposition types that appear in CDCP data can be ordered by the level of objectivity (Park et al., 2015a), as shown in Table 1.", "startOffset": 100, "endOffset": 120}, {"referenceID": 14, "context": ", directed tree factors) defined through maximization oracles (Martins et al., 2015).", "startOffset": 62, "endOffset": 84}, {"referenceID": 16, "context": "At training time, however, fractional solutions can be used as-is; this makes better use of each iteration and actually increases the ratio of integral solutions in future iterations, as well as at test time, as proven by Meshi et al. (2016). We also find that after around 15 training iterations with fractional solutions, over 99% of inference calls are integral.", "startOffset": 222, "endOffset": 242}, {"referenceID": 25, "context": "We train the models by minimizing the structured hinge loss (Taskar et al., 2004): \u2211", "startOffset": 60, "endOffset": 81}, {"referenceID": 26, "context": "This gives birth to a linear structured SVM (Tsochantaridis et al., 2005), which, when using l2 regularization, can be trained efficiently in the dual using the online block-coordinate FrankWolfe algorithm of Lacoste-Julien et al.", "startOffset": 44, "endOffset": 73}, {"referenceID": 17, "context": "(2013), as implemented in the pystruct library (M\u00fcller and Behnke, 2014).", "startOffset": 47, "endOffset": 72}, {"referenceID": 10, "context": ", 2005), which, when using l2 regularization, can be trained efficiently in the dual using the online block-coordinate FrankWolfe algorithm of Lacoste-Julien et al. (2013), as implemented in the pystruct library (M\u00fcller and Behnke, 2014).", "startOffset": 143, "endOffset": 172}, {"referenceID": 11, "context": "lexical (unigrams and dependency tuples), structural (token statistics and proposition location), indicators (from hand-crafted lexicons), contextual, syntactic (subclauses, depth, tense, modal, and POS), probability, discourse (Lin et al., 2014), and average GloVe embeddings (Pennington et al.", "startOffset": 228, "endOffset": 246}, {"referenceID": 22, "context": ", 2014), and average GloVe embeddings (Pennington et al., 2014).", "startOffset": 38, "endOffset": 63}, {"referenceID": 24, "context": "In our case, since propositions consist of many words, BOW features may be too noisy and too dense; so for simplicity we again take a cue from the link-specific features used by Stab and Gurevych (2016). Our higher-order factor features are: same sentence indicators (for all 3 and for each pair), proposition order (one for each of the 6 possible orderings), Jaccard similarity (between all 3 and between each pair), presence of any shared nouns (between all 3 and between each pair), and shared noun ratios: nouns shared by all 3 divided by total nouns in each proposition and each pair, and shared nouns between each pair with respect to each proposition.", "startOffset": 178, "endOffset": 203}, {"referenceID": 5, "context": "Inspired by the use of LSTMs (Hochreiter and Schmidhuber, 1997) for MST dependency parsing by Kiperwasser and Goldberg (2016), we parametrize the potentials in our factor graph with an LSTM-based neural network,9 replacing MST inference with the more general AD3 algorithm, and using relaxed solutions for training when inference is inexact.", "startOffset": 29, "endOffset": 63}, {"referenceID": 5, "context": "Inspired by the use of LSTMs (Hochreiter and Schmidhuber, 1997) for MST dependency parsing by Kiperwasser and Goldberg (2016), we parametrize the potentials in our factor graph with an LSTM-based neural network,9 replacing MST inference with the more general AD3 algorithm, and using relaxed solutions for training when inference is inexact.", "startOffset": 30, "endOffset": 126}, {"referenceID": 7, "context": "To score a dependency a \u2192 b, Kiperwasser and Goldberg (2016) pass the con-", "startOffset": 29, "endOffset": 61}, {"referenceID": 1, "context": "Instead of explicit lower-order terms, we propose augmenting a, b, and c with a constant feature of 1, which has approximately the same effect, while benefiting from the parameter sharing in the low-rank factorization; an effect described by Blondel et al. (2016). Siblings and co-parents factors are similarly parametrized with their own tensors.", "startOffset": 242, "endOffset": 264}, {"referenceID": 3, "context": "We train our SVM using SAGA (Defazio et al., 2014) in lightning (Blondel and Pedregosa, 2016).", "startOffset": 28, "endOffset": 50}, {"referenceID": 2, "context": ", 2014) in lightning (Blondel and Pedregosa, 2016).", "startOffset": 21, "endOffset": 50}, {"referenceID": 24, "context": "For model selection and development we used kfold cross-validation at document level: on CDCP we set k = 3 to avoid small validation folds, while on UKP we follow Stab and Gurevych (2016) setting k = 5.", "startOffset": 163, "endOffset": 188}, {"referenceID": 6, "context": "While feature engineering still outperforms our RNN model, we find that RNNs shine on proposition classification, especially on UKP, and that structured training can make them competitive, reducing their observed lag on link prediction (Katiyar and Cardie, 2016), possibly through mitigating class imbalance.", "startOffset": 236, "endOffset": 262}, {"referenceID": 23, "context": "Following Stab and Gurevych (2016), we compute F1 scores at proposition and link level, and also report their average as a summary of overall performance.", "startOffset": 10, "endOffset": 35}, {"referenceID": 23, "context": "Following Stab and Gurevych (2016), we compute F1 scores at proposition and link level, and also report their average as a summary of overall performance.12 The results of a single prediction run on the test set are displayed in Table 2. The overall trend is that training using a structured objective is better than the baseline models, even when structured inference is applied on the baseline predictions. On UKP, for link prediction, the linear baseline can reach good performance when using inference, similar to the approach of Stab and Gurevych (2016), but the improvement in proposition prediction leads to higher overall F1 for the structured models.", "startOffset": 10, "endOffset": 559}, {"referenceID": 0, "context": "While we focus on monological argumentation, our model could be extended to dialogs, for which argumentation theory thoroughly motivates non-tree structures (Afantenos and Asher, 2014).", "startOffset": 157, "endOffset": 184}, {"referenceID": 23, "context": "Stab and Gurevych (2016) found polynomial kernels useful for modeling feature interactions, but kernel structured SVMs scale poorly, we intend to investigate alternate ways to capture feature interactions.", "startOffset": 0, "endOffset": 25}], "year": 2017, "abstractText": "We propose a novel factor graph model for argument mining, designed for settings in which the argumentative relations in a document do not necessarily form a tree structure. (This is the case in over 20% of the web comments dataset we release.) Our model jointly learns elementary unit type classification and argumentative relation prediction. Moreover, our model supports SVM and RNN parametrizations, can enforce structure constraints (e.g., transitivity), and can express dependencies between adjacent relations and propositions. Our approaches outperform unstructured baselines in both web comments and argumentative essay datasets.", "creator": "LaTeX with hyperref package"}}}