{"id": "1704.06877", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2017", "title": "Learning to Skim Text", "abstract": "Recurrent Neural Networks are showing much promise in many sub-areas of natural language processing, ranging from document classification to machine translation to automatic question answering. Despite their promise, many recurrent models have to read the whole text word by word, making it slow to handle long documents. For example, it is difficult to use a recurrent network to read a book and answer questions about it. In this paper, we present an approach of reading text while skipping irrelevant information if needed. The underlying model is a recurrent network that learns how far to jump after reading a few words of the input text. We employ a standard policy gradient method to train the model to make discrete jumping decisions. In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q\\&amp;A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.", "histories": [["v1", "Sun, 23 Apr 2017 03:54:22 GMT  (250kb,D)", "http://arxiv.org/abs/1704.06877v1", null], ["v2", "Sat, 29 Apr 2017 19:58:31 GMT  (250kb,D)", "http://arxiv.org/abs/1704.06877v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["adams wei yu", "hongrae lee", "quoc v le"], "accepted": true, "id": "1704.06877"}, "pdf": {"name": "1704.06877.pdf", "metadata": {"source": "CRF", "title": "Learning to Skim Text", "authors": ["Adams Wei Yu", "Hongrae Lee", "Quoc V. Le"], "emails": ["weiyu@cs.cmu.edu", "hrlee@google.com", "qvl@google.com"], "sections": [{"heading": "1 Introduction", "text": "The last few years have seen great success in applying neural networks to many important applications in natural language processing, e.g., partial language tagging, chunking, named entity recognition (Collobert et al., 2011), sensation analysis (Socher et al., 2011, 2013), document classification (Kim, 2014; Le and Mikolov, 2014; Dai and Le, 2015), machine translation (Kalchburner and Blunsom, 2013; Sutskever \u0445 However, most of the work has been done with Google.et al., 2014; Bahdanau et al., 2014; Sennrich et al., 2015; Wu et al., 2016), dialogical / dialogical modeling (Sordatoes et al., 2015; Vinyals and Le, 2015; Shang et al., 2015), document summarization (Rush et al., 2015; Nallapati et al., 2016), we read."}, {"heading": "2 Methodology", "text": "In this section, we present the proposed model called LSTM-Jump. First, we describe its main structure, followed by the difficulty of estimating some of the model parameters due to their non-differentiability. To solve this problem, we resort to a formulation to strengthen learning and apply a policy gradient method."}, {"heading": "2.1 Model Overview", "text": "The main architecture of the proposed model is illustrated in Figure 1, which is based on an LSTM recursive neural network. While K is a fixed parameter of the model, N and R are hyperparameters that can vary between training and test. Also, throughout the paper, we would use d1: p to read a sequence d1, d2,..., dp.In the following, we describe in detail how the model works when processing text. Using a training example x1: T, the recursive network reads the embedding of the first R tokens x1: R and outputs the hidden state. Then, this state is used to calculate the jumping softmax that determines a distribution over the jump steps between 1 and K. The model then provides samples from this distribution with a jump step that is used to decide which problem to read into the model."}, {"heading": "2.2 Training with REINFORCE", "text": "Our goal for training is to estimate the parameters of LSTM, and possibly the word embedding, which is then called \u03b8m, along with the erratic action parameters. (This is essentially what our experiments cover), how transverse entropy is viewed objectively. (This is essentially what our experiments cover), how transverse entropy is objectively J1 (which is no longer differentiable via transverse entropy entropy.) Therefore, we formulate it as an affirmation problem and apply the political gradient method to train the model. Specifically, we need to estimate a reward function via transverse entropy that can be constructed as follows. (It is not possible that we apply the transverse entropy sequence sequence during training with an example x1: T."}, {"heading": "2.3 Inference", "text": "During the conclusion, we can either use sampling or greedy evaluation by selecting the most likely jump step suggested by the Jump Softmax and follow this path. In our experiments, we will adopt the sampling scheme."}, {"heading": "3 Experimental Results", "text": "In this section, we present our empirical studies to understand the efficiency of the proposed model in reading text. The experimental tasks are: synthetic number prediction, mood analysis, classification of news topics and automatic answering of questions. These, with the exception of the first, are representative tasks when reading texts that include different sizes of data sets and different levels of word processing, from the character to the word to the sentence. Table 1 summarizes the statistics of the data set in our experiments. To exclude the potential effects of advanced models, we limit our comparison between the vanilla LSTM instance (Hochreiter and Schmidhuber, 1997) and our model, which is referred to as the LSTM jump. In short, we show that achieving the same or even better test accuracy, our model is up to 6 times and 66 times faster than the base model LSTM model in real and synthetic datasets, respectively, as the experiments that we are able to skip selectively in text, a large one."}, {"heading": "3.1 Number Prediction with a Synthetic Dataset", "text": "In fact, most of them are able to play by the rules, and they are able to play by the rules."}, {"heading": "3.2 Word Level Sentiment Analysis with Rotten Tomatoes and IMDB datasets", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "3.3 Character Level News Article Classification with AG dataset", "text": "The data set contains four classes of topics (World, Sports, Business, Sci / Tech) from the AG News Corpus, 7 a collection of more than 1 million news articles. The data we use is the subset constructed by Zhang et al. (2015) for the classification with Convolutionary Networks at the character level. There are 30,000 training examples and 1900 test examples for each class, in which 15% of the training data is set aside for validation. Using the nonspace alphabet are: abcdefghijklmnopqrstuvwxyz0123456 789-,.??: / @ $% & * < > () {} Since the vocabulary size is small, we choose 16 as the embedding size of the jump."}, {"heading": "3.4 Sentence Level Automatic Question Answering with Children\u2019s Book Test dataset", "text": "The final task is to answer all the questions in which we go in search of a new concept. \"We have gone in search of a new concept,\" he said. \"We have gone in search of a new concept,\" he said. \"We have gone in search of a new concept.\" \"We have gone in search of a new concept,\" he said. \"We will go in search of a new concept.\" \"We will go in search of a new concept.\" \"We will go in search of a new concept.\" \"We will go in search of a new concept.\" \"We will go in search of a new concept.\" \"We will go in search of a new concept.\" \"We will go in search of a new concept.\" \"We will go in search of a new concept.\" \"We will go in search of a new concept.\""}, {"heading": "4 Related Work", "text": "In fact, there is a fundamental difference between this work and our work, that we can assume a discrete distribution of resources, while they assume a continuous distribution of resources; the difference lies mainly in the inherent characteristics of the text and the image. It is difficult to learn political strategies that include more than 25 possible discrete places."}, {"heading": "5 Conclusions", "text": "In this thesis, we focus on learning how to skim text in order to read it quickly. In particular, we propose a \"jumping\" model that decides how many tokens to skip by scanning from a Softmax after reading every few characters. Such jumping behavior is modeled as a discrete decision-making process that can be trained by reinforcement learning algorithms like REINFORCE. In four different tasks with six data sets (one synthetic and five real), we test the efficiency of the proposed method at different stages of text skipping, from character to word and then to sentence. Results indicate that our model is many times faster, while accuracy is at the level of the LSTM base model."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the Google Brain team, especially Zhifeng Chen and Yuan Yu, for the helpful discussion about the implementation of this model on Tensorflow. The first author would also like to thank Chen Liang, Hanxiao Liu, Yingtao Tian, Fish Tung, Chiyuan Zhang and Yu Zhang for their help during the project. Finally, the authors would like to thank them for the invaluable feedback from anonymous reviewers."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "arXiv preprint arXiv:1603.06042 .", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Multiple object recognition with visual attention", "author": ["Jimmy Ba", "Volodymyr Mnih", "Koray Kavukcuoglu."], "venue": "arXiv preprint arXiv:1412.7755 .", "citeRegEx": "Ba et al\\.,? 2014", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Listen, attend and spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1508.01211 .", "citeRegEx": "Chan et al\\.,? 2015", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "A thorough examination of the cnn/daily mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, Au-", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Hierarchical question answering for long documents", "author": ["Eunsol Choi", "Daniel Hewlett", "Alexandre Lacoste", "Illia Polosukhin", "Jakob Uszkoreit", "Jonathan Berant."], "venue": "arXiv preprint arXiv:1611.01839 .", "citeRegEx": "Choi et al\\.,? 2016", "shortCiteRegEx": "Choi et al\\.", "year": 2016}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1609.01704 .", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12(Aug):2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Semisupervised sequence learning", "author": ["Andrew M. Dai", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems. pages 3079\u2013 3087.", "citeRegEx": "Dai and Le.,? 2015", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Adaptive computation time for recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv preprint arXiv:1603.08983 .", "citeRegEx": "Graves.,? 2016", "shortCiteRegEx": "Graves.", "year": 2016}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka."], "venue": "arXiv preprint arXiv:1410.5401 .", "citeRegEx": "Graves et al\\.,? 2014", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems. pages 1693\u2013", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "arXiv:1511.02301 .", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi", "J\u00fcrgen Schmidhuber."], "venue": "S. C. Kremer and J. F. Kolen, editors, A Field Guide to Dynamical Recurrent Neural Net-", "citeRegEx": "Hochreiter et al\\.,? 2001", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Variable computation in recurrent neural networks", "author": ["Yacine Jernite", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1611.06188 .", "citeRegEx": "Jernite et al\\.,? 2016", "shortCiteRegEx": "Jernite et al\\.", "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882 .", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "A clockwork rnn", "author": ["Jan Koutnik", "Klaus Greff", "Faustino Gomez", "Juergen Schmidhuber."], "venue": "International Conference on Machine Learning.", "citeRegEx": "Koutnik et al\\.,? 2014", "shortCiteRegEx": "Koutnik et al\\.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V. Le", "Tomas Mikolov."], "venue": "International Conference on Machine Learning (ICML).", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Learning recurrent span representations for extractive question answering", "author": ["Kenton Lee", "Tom Kwiatkowski", "Ankur Parikh", "Dipanjan Das."], "venue": "arXiv preprint arXiv:1611.01436 .", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Rationalizing neural predictions", "author": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola."], "venue": "arXiv preprint arXiv:1606.04155 .", "citeRegEx": "Lei et al\\.,? 2016", "shortCiteRegEx": "Lei et al\\.", "year": 2016}, {"title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision", "author": ["Chen Liang", "Jonathan Berant", "Quoc Le", "Kenneth D. Forbus", "Ni Lao."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational", "citeRegEx": "Liang et al\\.,? 2017", "shortCiteRegEx": "Liang et al\\.", "year": 2017}, {"title": "Learning word vectors for sentiment analysis", "author": ["Andrew L Maas", "Raymond E Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Maas et al\\.,? 2011", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "In Advances in neural information processing systems", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Abstractive text summarization using sequence-to-sequence RNNs and beyond", "author": ["Ramesh Nallapati", "Bowen Zhou", "Caglar Gulcehre", "Bing Xiang"], "venue": "In Conference on Computational Natural Language Learning (CoNLL)", "citeRegEx": "Nallapati et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Bo Pang", "Lillian Lee."], "venue": "Proceedings of the 43rd annual meeting on association for computational linguistics. Association for Computational", "citeRegEx": "Pang and Lee.,? 2005", "shortCiteRegEx": "Pang and Lee.", "year": 2005}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "CoRR abs/1511.06732", "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Annual Meeting of the Association for Computational Linguistics (ACL).", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."], "venue": "arXiv preprint arXiv:1611.01603 .", "citeRegEx": "Seo et al\\.,? 2016", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Attention for fine-grained categorization", "author": ["Pierre Sermanet", "Andrea Frome", "Esteban Real."], "venue": "arXiv preprint arXiv:1412.7054 .", "citeRegEx": "Sermanet et al\\.,? 2014", "shortCiteRegEx": "Sermanet et al\\.", "year": 2014}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "Annual Meeting of the Association for Computational Linguistics (ACL).", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Reasonet: Learning to stop reading in machine comprehension", "author": ["Yelong Shen", "Po-Sen Huang", "Jianfeng Gao", "Weizhu Chen."], "venue": "arXiv preprint arXiv:1609.05284 .", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proceedings of the conference on empirical methods in natural lan-", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y. Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A parallel-hierarchical model for machine comprehension on sparse data", "author": ["Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Jing He", "Phillip Bachman", "Kaheer Suleman."], "venue": "arXiv preprint arXiv:1603.08884 .", "citeRegEx": "Trischler et al\\.,? 2016", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "arXiv preprint arXiv:1506.05869 .", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "arXiv preprint arXiv:1608.07905 .", "citeRegEx": "Wang and Jiang.,? 2016", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Multi-perspective context matching for machine comprehension", "author": ["Zhiguo Wang", "Haitao Mi", "Wael Hamza", "Radu Florian."], "venue": "arXiv preprint arXiv:1612.04211 .", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merri\u00ebnboer", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1502.05698 .", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams."], "venue": "Machine Learning 8:229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Dynamic coattention networks for question answering", "author": ["Caiming Xiong", "Victor Zhong", "Richard Socher."], "venue": "arXiv preprint arXiv:1611.01604 .", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Reinforcement learning neural turing machines-revised", "author": ["Wojciech Zaremba", "Ilya Sutskever."], "venue": "arXiv preprint arXiv:1505.00521 .", "citeRegEx": "Zaremba and Sutskever.,? 2015", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2015}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "Advances in neural information processing systems. pages 649\u2013657.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": ", partof-speech tagging, chunking, named entity recognition (Collobert et al., 2011), sentiment analysis (Socher et al.", "startOffset": 60, "endOffset": 84}, {"referenceID": 17, "context": ", 2011, 2013), document classification (Kim, 2014; Le and Mikolov, 2014; Zhang et al., 2015; Dai and Le, 2015), machine translation (Kalchbrenner and Blunsom, 2013; Sutskever", "startOffset": 39, "endOffset": 110}, {"referenceID": 20, "context": ", 2011, 2013), document classification (Kim, 2014; Le and Mikolov, 2014; Zhang et al., 2015; Dai and Le, 2015), machine translation (Kalchbrenner and Blunsom, 2013; Sutskever", "startOffset": 39, "endOffset": 110}, {"referenceID": 48, "context": ", 2011, 2013), document classification (Kim, 2014; Le and Mikolov, 2014; Zhang et al., 2015; Dai and Le, 2015), machine translation (Kalchbrenner and Blunsom, 2013; Sutskever", "startOffset": 39, "endOffset": 110}, {"referenceID": 8, "context": ", 2011, 2013), document classification (Kim, 2014; Le and Mikolov, 2014; Zhang et al., 2015; Dai and Le, 2015), machine translation (Kalchbrenner and Blunsom, 2013; Sutskever", "startOffset": 39, "endOffset": 110}, {"referenceID": 38, "context": ", 2016), conversational/dialogue modeling (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015), document summarization (Rush et al.", "startOffset": 42, "endOffset": 106}, {"referenceID": 41, "context": ", 2016), conversational/dialogue modeling (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015), document summarization (Rush et al.", "startOffset": 42, "endOffset": 106}, {"referenceID": 34, "context": ", 2016), conversational/dialogue modeling (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015), document summarization (Rush et al.", "startOffset": 42, "endOffset": 106}, {"referenceID": 30, "context": ", 2015), document summarization (Rush et al., 2015; Nallapati et al., 2016),", "startOffset": 32, "endOffset": 75}, {"referenceID": 27, "context": ", 2015), document summarization (Rush et al., 2015; Nallapati et al., 2016),", "startOffset": 32, "endOffset": 75}, {"referenceID": 0, "context": "parsing (Andor et al., 2016) and automatic question answering (Q&A) (Weston et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 44, "context": ", 2016) and automatic question answering (Q&A) (Weston et al., 2015; Hermann et al., 2015; Wang and Jiang, 2016; Wang et al., 2016; Trischler et al., 2016; Lee et al., 2016; Seo et al., 2016; Xiong et al., 2016).", "startOffset": 47, "endOffset": 211}, {"referenceID": 11, "context": ", 2016) and automatic question answering (Q&A) (Weston et al., 2015; Hermann et al., 2015; Wang and Jiang, 2016; Wang et al., 2016; Trischler et al., 2016; Lee et al., 2016; Seo et al., 2016; Xiong et al., 2016).", "startOffset": 47, "endOffset": 211}, {"referenceID": 42, "context": ", 2016) and automatic question answering (Q&A) (Weston et al., 2015; Hermann et al., 2015; Wang and Jiang, 2016; Wang et al., 2016; Trischler et al., 2016; Lee et al., 2016; Seo et al., 2016; Xiong et al., 2016).", "startOffset": 47, "endOffset": 211}, {"referenceID": 43, "context": ", 2016) and automatic question answering (Q&A) (Weston et al., 2015; Hermann et al., 2015; Wang and Jiang, 2016; Wang et al., 2016; Trischler et al., 2016; Lee et al., 2016; Seo et al., 2016; Xiong et al., 2016).", "startOffset": 47, "endOffset": 211}, {"referenceID": 40, "context": ", 2016) and automatic question answering (Q&A) (Weston et al., 2015; Hermann et al., 2015; Wang and Jiang, 2016; Wang et al., 2016; Trischler et al., 2016; Lee et al., 2016; Seo et al., 2016; Xiong et al., 2016).", "startOffset": 47, "endOffset": 211}, {"referenceID": 21, "context": ", 2016) and automatic question answering (Q&A) (Weston et al., 2015; Hermann et al., 2015; Wang and Jiang, 2016; Wang et al., 2016; Trischler et al., 2016; Lee et al., 2016; Seo et al., 2016; Xiong et al., 2016).", "startOffset": 47, "endOffset": 211}, {"referenceID": 32, "context": ", 2016) and automatic question answering (Q&A) (Weston et al., 2015; Hermann et al., 2015; Wang and Jiang, 2016; Wang et al., 2016; Trischler et al., 2016; Lee et al., 2016; Seo et al., 2016; Xiong et al., 2016).", "startOffset": 47, "endOffset": 211}, {"referenceID": 46, "context": ", 2016) and automatic question answering (Q&A) (Weston et al., 2015; Hermann et al., 2015; Wang and Jiang, 2016; Wang et al., 2016; Trischler et al., 2016; Lee et al., 2016; Seo et al., 2016; Xiong et al., 2016).", "startOffset": 47, "endOffset": 211}, {"referenceID": 14, "context": "In our experiments, we use the basic LSTM recurrent networks (Hochreiter and Schmidhuber, 1997) as the base model and benchmark the proposed algorithm on a range of document classification or reading comprehension tasks, using various datasets such as Rotten Tomatoes (Pang ar X iv :1 70 4.", "startOffset": 61, "endOffset": 95}, {"referenceID": 24, "context": "and Lee, 2005), IMDB (Maas et al., 2011), AG News (Zhang et al.", "startOffset": 21, "endOffset": 40}, {"referenceID": 48, "context": ", 2011), AG News (Zhang et al., 2015) and Children\u2019s Book", "startOffset": 17, "endOffset": 37}, {"referenceID": 12, "context": "Test (Hill et al., 2015).", "startOffset": 5, "endOffset": 24}, {"referenceID": 45, "context": "rithm (Williams, 1992):", "startOffset": 6, "endOffset": 22}, {"referenceID": 45, "context": "It is shown (Williams, 1992; Zaremba and Sutskever, 2015) that any number bi will yield an unbiased estimation.", "startOffset": 12, "endOffset": 57}, {"referenceID": 47, "context": "It is shown (Williams, 1992; Zaremba and Sutskever, 2015) that any number bi will yield an unbiased estimation.", "startOffset": 12, "endOffset": 57}, {"referenceID": 26, "context": "of Mnih et al. (2014) that bi = wbh s i + cb and the parameter \u03b8b = {wb, cb} is learned by minimizing (Rs\u2212 bi )2.", "startOffset": 3, "endOffset": 22}, {"referenceID": 14, "context": "To exclude the potential impact of advanced models, we restrict our comparison between the vanilla LSTM (Hochreiter and Schmidhuber, 1997) and our model, which is referred to as LSTM-Jump.", "startOffset": 104, "endOffset": 138}, {"referenceID": 18, "context": "General Experiment Settings We use the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.", "startOffset": 54, "endOffset": 75}, {"referenceID": 25, "context": "We choose the pre-trained word2vec embeddings5 (Mikolov et al., 2013) as", "startOffset": 47, "endOffset": 69}, {"referenceID": 24, "context": "The second dataset is IMDB (Maas et al., 2011),6 which contains 25,000 training and 25,000 testing movie reviews, where the average length of text is 240 words, much longer than that of Rotten Tomatoes.", "startOffset": 27, "endOffset": 46}, {"referenceID": 48, "context": "The data we use is the subset constructed by Zhang et al. (2015) for classification with character-level convolutional networks.", "startOffset": 45, "endOffset": 65}, {"referenceID": 12, "context": "We benchmark on the data set Children\u2019s Book Test (CBT) (Hill et al., 2015).", "startOffset": 56, "endOffset": 75}, {"referenceID": 4, "context": "Using such bilinear form to select answer basically follows the idea of Chen et al. (2016), as it is shown to have good performance.", "startOffset": 72, "endOffset": 91}, {"referenceID": 45, "context": "Similar to our approach, the model is trained end-to-end using the REINFORCE algorithm (Williams, 1992).", "startOffset": 87, "endOffset": 103}, {"referenceID": 1, "context": ", 2014; Ba et al., 2014; Sermanet et al., 2014), where a recurrent model is used to combine visual evidence at multiple fixations processed by a convolutional neural network. Similar to our approach, the model is trained end-to-end using the REINFORCE algorithm (Williams, 1992). However, a major difference between those work and ours is that we have to sample from discrete jumping distribution, while they can sample from continuous distribution such as Gaussian. The difference is mainly due to the inborn characteristics of text and image. In fact, as pointed out by Mnih et al. (2014), it was difficult to learn policies over more than 25 possible discrete locations.", "startOffset": 8, "endOffset": 591}, {"referenceID": 5, "context": "This idea has recently been explored in the context of natural language processing applications, where the main goal is to filter irrelevant content using a small network (Choi et al., 2016).", "startOffset": 171, "endOffset": 190}, {"referenceID": 35, "context": "learning (Shen et al., 2016).", "startOffset": 9, "endOffset": 28}, {"referenceID": 35, "context": "learning (Shen et al., 2016). The key difference between our work and Shen et al. (2016) is that they focus on early stopping after multiple pass of data to ensure accuracy whereas our method focuses on selective reading with single pass to enable fast processing.", "startOffset": 10, "endOffset": 89}, {"referenceID": 22, "context": "The concept of \u201chard\u201d attention has also been used successfully in the context of making neural network predictions more interpretable (Lei et al., 2016).", "startOffset": 135, "endOffset": 153}, {"referenceID": 22, "context": "The concept of \u201chard\u201d attention has also been used successfully in the context of making neural network predictions more interpretable (Lei et al., 2016). The key difference between our work and Lei et al. (2016)\u2019s method is that our method optimizes for faster inference, and is more dynamic in its jumping.", "startOffset": 136, "endOffset": 213}, {"referenceID": 2, "context": "proach by (Bahdanau et al., 2014).", "startOffset": 10, "endOffset": 33}, {"referenceID": 9, "context": "Our method belongs to adaptive computation of neural networks, whose idea is recently explored by (Graves, 2016; Jernite et al., 2016), where different amount of computations are allocated dynamically per time step.", "startOffset": 98, "endOffset": 134}, {"referenceID": 15, "context": "Our method belongs to adaptive computation of neural networks, whose idea is recently explored by (Graves, 2016; Jernite et al., 2016), where different amount of computations are allocated dynamically per time step.", "startOffset": 98, "endOffset": 134}, {"referenceID": 9, "context": "to (Graves, 2016; Jernite et al., 2016), we do not find training with policy gradient methods problematic in our experiments.", "startOffset": 3, "endOffset": 39}, {"referenceID": 15, "context": "to (Graves, 2016; Jernite et al., 2016), we do not find training with policy gradient methods problematic in our experiments.", "startOffset": 3, "endOffset": 39}, {"referenceID": 10, "context": "It is therefore related to the prior work on Neural Turing Machines (Graves et al., 2014) and especially its RL version (Zaremba and Sutskever, 2015).", "startOffset": 68, "endOffset": 89}, {"referenceID": 47, "context": ", 2014) and especially its RL version (Zaremba and Sutskever, 2015).", "startOffset": 38, "endOffset": 67}, {"referenceID": 47, "context": "Compared to (Zaremba and Sutskever, 2015), the output tape in our method is more simple and reward signals in our problems are less sparse, which explains why our model is easy to train.", "startOffset": 12, "endOffset": 41}, {"referenceID": 13, "context": "Our method, by skipping irrelevant content, shortens the length of recurrent networks, thereby addressing the vanishing or exploding gradients in them (Hochreiter et al., 2001).", "startOffset": 151, "endOffset": 176}, {"referenceID": 14, "context": "The baseline method itself, Long Short Term Memory (Hochreiter and Schmidhuber, 1997), belongs to the same category of methods.", "startOffset": 51, "endOffset": 85}, {"referenceID": 19, "context": "In this category, there are several recent methods that try to achieve the same goal, such as having recurrent networks that operate in different frequency (Koutnik et al., 2014) or is organized in a hierarchical fashion (Chan et al.", "startOffset": 156, "endOffset": 178}, {"referenceID": 3, "context": ", 2014) or is organized in a hierarchical fashion (Chan et al., 2015; Chung et al., 2016).", "startOffset": 50, "endOffset": 89}, {"referenceID": 6, "context": ", 2014) or is organized in a hierarchical fashion (Chan et al., 2015; Chung et al., 2016).", "startOffset": 50, "endOffset": 89}, {"referenceID": 23, "context": "Lastly, we should point out that we are among the recent efforts that deploy reinforcement learning to the field of natural language processing, some of which have achieved encouraging results in the realm of such as neural symbolic machine (Liang et al., 2017), machine reasoning (Shen et al.", "startOffset": 241, "endOffset": 261}, {"referenceID": 35, "context": ", 2017), machine reasoning (Shen et al., 2016) and sequence generation (Ranzato et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 29, "context": ", 2016) and sequence generation (Ranzato et al., 2015).", "startOffset": 32, "endOffset": 54}], "year": 2017, "abstractText": "Recurrent Neural Networks are showing much promise in many sub-areas of natural language processing, ranging from document classification to machine translation to automatic question answering. Despite their promise, many recurrent models have to read the whole text word by word, making it slow to handle long documents. For example, it is difficult to use a recurrent network to read a book and answer questions about it. In this paper, we present an approach of reading text while skipping irrelevant information if needed. The underlying model is a recurrent network that learns how far to jump after reading a few words of the input text. We employ a standard policy gradient method to train the model to make discrete jumping decisions. In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q&A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.", "creator": "LaTeX with hyperref package"}}}