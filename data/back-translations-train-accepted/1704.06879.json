{"id": "1704.06879", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2017", "title": "Deep Keyphrase Generation", "abstract": "Keyphrase provides highly-summative information that can be effectively used for understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divided the to-be-summarized content into multiple text chunks, then ranked and selected the most meaningful ones. These approaches could neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text. We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks. We name it as deep keyphrase generation since it attempts to capture the deep semantic meaning of the content with a deep learning method. Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also can generate absent keyphrases based on the semantic meaning of the text. Code and dataset are available at", "histories": [["v1", "Sun, 23 Apr 2017 04:34:26 GMT  (2267kb,D)", "http://arxiv.org/abs/1704.06879v1", "11 pages. Accepted by ACL2017"]], "COMMENTS": "11 pages. Accepted by ACL2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rui meng", "sanqiang zhao", "shuguang han", "daqing he", "peter brusilovsky", "yu chi"], "accepted": true, "id": "1704.06879"}, "pdf": {"name": "1704.06879.pdf", "metadata": {"source": "CRF", "title": "Deep Keyphrase Generation", "authors": ["Rui Meng", "Sanqiang Zhao", "Shuguang Han", "Daqing He", "Peter Brusilovsky", "Yu Chi"], "emails": ["yuc73}@pitt.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move to another world, in which they will be able to move to another world, in which they will be able to move to another world, in which they will be able to move to another world, in which they will be able to live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live in which they will live, in which they will live in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, that they will live, in which they will live, in which they will live, in which they will live, that they will live, that they will live, that they will live, that they will live, that they will live, that they will live, that they will live, that they will live, that they will live, that they will live, that they will live, that they will live, that they will live, that they will live, that they will live, that they"}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Automatic Keyphrase Extraction", "text": "A key phrase provides a concise and precise way to describe a topic or subtopic in a document. A number of extraction algorithms have been proposed, and the process of extracting key phrases can typically be broken down into two steps. The first step is to create a list of phrase candidates using heuristic methods. However, as these candidates are prepared for further filtering, a significant number of candidates are produced in this step to increase the possibility that most of the correct key phrases will be retained. Primary methods for extracting sequences of words that match certain part-of-speech tag patterns (e.g. nouns, adjectives) (Liu et al., 2011; Wang et al., 2016; Le et al., 2016), and extracting key n-grams or noun-phrases (Hulth, 2003; Medelyan et al., 2008).The second step consists of defining each candidate phrase by its probability to be a key phrase in the document."}, {"heading": "2.2 Encoder-Decoder Model", "text": "The RNN encoder decoder model (also known as sequence-to-sequence learning) is an end-to-end approach. It was first introduced by Cho et al. (2014) and Sutskever et al. (2014) to solve translation problems. As it provides a powerful tool for modelling variable sequences in an end-to-end manner, it is suitable for many tasks of natural language processing and can quickly achieve great successes (Rush et al., 2015; Vinyals et al., 2015; Serban et al., 2016). Various strategies have been explored to improve the performance of the encoder decoder model. Attention mechanism (Bahdanau et al., 2014) is a soft approach that allows the model to automatically locate the relevant input components. In order to use the important information in the source text, some studies in 2016 attempted to insert certain parts of the content from the source algorithet (during the copying of the target text from the source and in the allotment)."}, {"heading": "3 Methodology", "text": "In this section, we will present our proposed method of deep keyphrase generation in detail. First, the task of keyphrase generation is defined, followed by an overview of how we apply the RNN encoder decoder model. Details of the framework and the copy mechanism are presented in sections 3.3 and 3.4."}, {"heading": "3.1 Problem Definition", "text": "Faced with a keyphrase dataset consisting of N data sequences, the i-th data sample (x (i), p (i)) contains a source code x (i) and the Mi target keyphrases p (i) = (p (i, 1), p (i, 2),..., p (i) L xip (i, j) = y (i, j) 1, y (i, j) 2,.., y (i, j) L p (i, j) Lx (i) 1, Lp (i, j) 2,.., x (i) L xip (i, j) = y (i, j) 2,., y (i, j) L p (i, j) Lx (i) and Lp (i, j) then denotes the length of the word sequence of x (i), the word sequence of x (i), the word sequence of Mi (i), the data pairs of Mi (i), the data sequence of (x)."}, {"heading": "3.2 Encoder-Decoder Model", "text": "The basic idea of our keyphrase generation model is to compress the content of the source code into a hidden representation with an encoder and generate corresponding keyphrases with the decoder, based on the representation. Both the encoder and the decoder are implemented with recursive neural networks (RNN). The RNN encoder converts the variable length input sequence x = (x1, x2,..., xT) into a set of hidden representations h = (h1, h2,., hT) by iterating the following equations over time t: ht = f (xt, ht \u2212 1) (1), where f is a non-linear function. We get the context vector c, which represents the entire input x by a non-linear function q.c = q (h1, h2,..., hT) (2), where the decoder sequence ycoder is the yst (1), yc the input sequence yst (1), the word sequence yst (1), and the qst (1)."}, {"heading": "3.3 Details of the Encoder and Decoder", "text": "Previous studies (Bahdanau et al., 2014; Cho et al., 2014) suggest that it can generally provide better performance in speech modelling than a simple relapsing neural network and a simpler structure than other long-term memory networks (Hochreiter et al., 1997). Consequently, the above-mentioned non-linear function f is replaced by the GRU function (see in (Cho et al., 2014). Another upstream GRU is used as a decoder. Furthermore, an attention mechanism is assumed to improve performance. \u2212 hT: ci = T = 1 ij = an equilibrium between (1) and (1) exact position (1) (p)."}, {"heading": "3.4 Copying Mechanism", "text": "To ensure the quality of the learned representation and reduce the size of the vocabulary, the RNN model would typically take into account a certain number of common words (e.g. 30,000 words in (Cho et al., 2014), but a large number of long-tail words are simply ignored. Therefore, the RNN model is unable to remember keywords that contain words outside the vocabulary. In fact, important sentences can also be identified by selecting suitable words from the source text, although their exact meaning is not known. The copy mechanism (Gu et al., 2016) is a workable solution that allows RNN to predict words outside the vocabulary by selecting suitable words from the source text, even if their exact meaning is not known. The probability of predicting each new word yt consists of two parts. The first term is the probability of generating the term c x (see Equation 3) and the second is the probability of copying the source text: each yt consists of two parts."}, {"heading": "4 Experiment Settings", "text": "This section begins with a discussion of how we designed our evaluation experiments, followed by a description of training and test data sets. We then present our evaluation metrics and baselines."}, {"heading": "4.1 Training Dataset", "text": "The largest dataset comes from Krapivin et al. (2008), which contains 2,304 scientific publications, but this set of data is unable to train a robust, recurring neural network model. In fact, millions of scientific papers are available online, each containing the key sentences assigned by their authors. Therefore, we have collected a large amount of high-quality scientific computer science metadata from various online digital libraries, including ACM Digital Library, ScienceDirect, Wiley and Web of Science, etc. (Han et al., 2013; Rui et al., 2016). In total, we have obtained a dataset of 567,830 articles after removing duplicates and overlaps with test datasets that are 200 times larger than Krapivin et al. (2008)."}, {"heading": "4.2 Testing Datasets", "text": "For the more comprehensive evaluation of the proposed model, four widely used scientific publication datasets were used. As these datasets contain only a few hundred or a few thousand publications, we are contributing a new test dataset KP20k with a much larger number of scientific articles. We take the title and abstraction as the source text, and each dataset is described in detail below. - Inspec (Hulth, 2003): This dataset provides 2,304 abstracts. We take the 500 test papers and their corresponding uncontrolled keyphrases for evaluation, and the remaining 1,500 papers are used to train the monitored baseline models. - Krapivin (Krapivin et al., 2008): This dataset provides 2,304 full-text papers and keyphrases assigned by the author. However, the author did not mention how the test data is divided, so we selected the first 400 papers in alphabetical order as test data, and the remaining papers are used to monitor the bases."}, {"heading": "4.3 Implementation Details", "text": "In total, there are 2,780,316 < text, keyphrase > pairs for training, in which text refers to the concatenation of the title and the summary of a publication and keyphrase indicates a keyword assigned by the author. Steps of word preprocessing including tokenization, lowercase and replacement of all digits with symbol < digit > are used. Two encoder decoder models are trained, one with only one attention mechanism (RNN) and one with activated attention and copy mechanism (CopyRNN). For both models, we select the 50,000 most common words as our vocabulary, the dimension of embedding is set to 150, the dimension of hidden layers is set to 300, and the word embedding is randomly initialized with a uniform distribution in [-0,0,0,1]. Models are optimized with Adam (Kingma and Ba, 2014), setting the initial formulas 10, maximum scale = clipping to 1%, and the search problem to 0.5%."}, {"heading": "4.4 Baseline Models", "text": "Four unattended algorithms are used (Tf-Idf, TextRank (Mihalcea and Tarau, 2004), SingleRank (Wan and Xiao, 2008) and ExpandRank (Wan and Xiao, 2008) as well as two monitored algorithms (KEA (Witten et al., 1999) and Maui (Medelyan et al., 2009a)."}, {"heading": "4.5 Evaluation Metric", "text": "To measure the performance of the algorithm, three evaluation metrics are used: macro averaged precision, retrieval, and F measurement (F1). According to the standard definition, precision is defined as the number of correctly predicted keyphrases over the number of all predicted keyphrases, and retrieval is calculated from the number of correctly predicted keyphrases over the total number of records. Note that when determining the match of two keyphrases, we use Porter Stemmers for preprocessing."}, {"heading": "5 Results and Analysis", "text": "We carry out an empirical study on three different tasks to evaluate our model."}, {"heading": "5.1 Predicting Present Keyphrases", "text": "This is the same as the keyphrase extraction in previous studies, where we analyze how well our superior model targets a broadly defined task. To make a fair comparison, we need to consider the current keyphrases for the evaluation in this task."}, {"heading": "5.2 Predicting Absent Keyphrases", "text": "As I said, an important motivation for this work is that we are interested in the proposed model's ability to predict missing keyphrases based on the \"understanding\" of content. It should be noted that such a prediction is a very demanding task, and to the best of our knowledge no existing methods can accomplish this task. Therefore, when discussing the results of this task, we only provide the RNN and CopyRNN services. Here, we evaluate the performance within the retrieval of the top 10 and top 50 results to see how many missing keyphrases can be correctly predicted. We use the missing keyphrases in the test datasets for evaluation. Table 3 presents the retrieval results of the top 10 / 50 keyphrases for our RNN and CopyRNN models, in which we observe that CopyRNN, on average, contains about 8% (15%) of the keyphrases in the top 10 (50) keyphrases as text that we can include in the text that indicates both of the source phrases, although both of these models indicate that this is complete."}, {"heading": "5.3 Transferring the Model to the News Domain", "text": "RNN and CopyRNN are monitored models, and they are trained on data in a particular area and writing style. However, with sufficient training in a large data set, we expect the models to be able to learn universal language characteristics that are also effective in other corporations. In this task, we will test our model on a different type of text to see if the model would work if it were transferred to another environment. We will use the popular data set of the news article DUC2001 (Wan and Xiao, 2008) for analysis. The data set consists of 308 news articles and 2,488 manually commented keyphrases. The result of this analysis is presented in Table 4, from which we could see that the CopyRNN can extract a portion of the correct keyphrases from an unknown text. Compared to the results reported in (Hasan and Ng, 2010), the performance of CopyRNN is better than Textkeytank MikeRau (year 2004 and three complete cluster names)."}, {"heading": "6 Discussion", "text": "Our experimental results show that the CopyRNN model not only delivers good results in predicting current keyphrases, but is also capable of generating topic-relevant keyphrases that are missing from the text. In a broader sense, this model attempts to map a long text (i.e. a paper summary) with representative short pieces of text (i.e. keyphrases) that can potentially be used to improve information retrieval by generating high-quality index terms, as well as to support surfing by grouping long documents into short, readable sentences. So far, we have tested our model with scientific publications and news articles, and demonstrated that our model captures universal speech patterns and extracts key information from unknown texts. We believe that our model has greater potential to be generalized to other domains and types such as books, online reviews, etc., when trained on a larger data corpus."}, {"heading": "7 Conclusions and Future Work", "text": "In this paper, we proposed an RNN-based generative model for predicting keyphrases in scientific texts. To our knowledge, this is the first application of the encoder decoder model to a keyphrase prediction task. Our model summarizes phrases based on the deep semantic meaning of the text, and is able to handle rare phrases by incorporating a copy mechanism. Comprehensive empirical studies show the effectiveness of our proposed model for generating both current and missing keyphrases for different text types. Our future work could include the following two directions: - In this work, we merely evaluated the performance of the proposed model by conducting offline experiments. In the future, we are interested in comparing the model with human annotators and using human judges to evaluate the quality of predicted phrases."}, {"heading": "Acknowledgments", "text": "We thank Jiatao Gu and Miltiadis Allamanis for passing on the source code and the helpful advice. We also thank Wei Lu, Yong Huang, Qikai Cheng and other IRLAB members at Wuhan University for their support in developing data sets. This work is partially supported by the National Science Foundation under grant number 1525186."}], "references": [{"title": "A Convolutional Attention Network for Extreme Summarization of Source Code", "author": ["M. Allamanis", "H. Peng", "C. Sutton."], "venue": "ArXiv e-prints .", "citeRegEx": "Allamanis et al\\.,? 2016", "shortCiteRegEx": "Allamanis et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Opinion expression mining by exploiting keyphrase extraction", "author": ["G\u00e1bor Berend."], "venue": "IJCNLP. Citeseer, pages 1162\u20131170.", "citeRegEx": "Berend.,? 2011", "shortCiteRegEx": "Berend.", "year": 2011}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Domain-specific keyphrase extraction", "author": ["Eibe Frank", "Gordon W Paynter", "Ian H Witten", "Carl Gutwin", "Craig G Nevill-Manning"], "venue": null, "citeRegEx": "Frank et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Frank et al\\.", "year": 1999}, {"title": "Lstm recurrent networks learn simple context-free and contextsensitive languages", "author": ["Felix A Gers", "E Schmidhuber."], "venue": "IEEE Transactions on Neural Networks 12(6):1333\u20131340.", "citeRegEx": "Gers and Schmidhuber.,? 2001", "shortCiteRegEx": "Gers and Schmidhuber.", "year": 2001}, {"title": "Extracting keyphrases from research papers using citation networks", "author": ["Sujatha Das Gollapalli", "Cornelia Caragea."], "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence. AAAI Press, AAAI\u201914, pages 1629\u20131635.", "citeRegEx": "Gollapalli and Caragea.,? 2014", "shortCiteRegEx": "Gollapalli and Caragea.", "year": 2014}, {"title": "Extracting key terms from noisy and multitheme documents", "author": ["Maria Grineva", "Maxim Grinev", "Dmitry Lizorkin."], "venue": "Proceedings of the 18th International Conference on World Wide Web. ACM, New York, NY, USA, WWW \u201909, pages 661\u2013670.", "citeRegEx": "Grineva et al\\.,? 2009", "shortCiteRegEx": "Grineva et al\\.", "year": 2009}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor OK Li."], "venue": "arXiv preprint arXiv:1603.06393 .", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Supporting exploratory people search: a study of factor transparency and user control", "author": ["Shuguang Han", "Daqing He", "Jiepu Jiang", "Zhen Yue."], "venue": "Proceedings of the 22nd ACM international conference on Information & Knowledge Management. ACM,", "citeRegEx": "Han et al\\.,? 2013", "shortCiteRegEx": "Han et al\\.", "year": 2013}, {"title": "Conundrums in unsupervised keyphrase extraction: making sense of the state-of-the-art", "author": ["Kazi Saidul Hasan", "Vincent Ng."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics: Posters. Association for Computational Lin-", "citeRegEx": "Hasan and Ng.,? 2010", "shortCiteRegEx": "Hasan and Ng.", "year": 2010}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Improved automatic keyword extraction given more linguistic knowledge", "author": ["Anette Hulth."], "venue": "Proceedings of the 2003 conference on Empirical methods in natural language processing. Association for Computational Linguistics, pages 216\u2013223.", "citeRegEx": "Hulth.,? 2003", "shortCiteRegEx": "Hulth.", "year": 2003}, {"title": "A study on automatically extracted keywords in text categorization", "author": ["Anette Hulth", "Be\u00e1ta B Megyesi."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Compu-", "citeRegEx": "Hulth and Megyesi.,? 2006", "shortCiteRegEx": "Hulth and Megyesi.", "year": 2006}, {"title": "Phrasier: a system for interactive document retrieval using keyphrases", "author": ["Steve Jones", "Mark S Staveley."], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. ACM, pages", "citeRegEx": "Jones and Staveley.,? 1999", "shortCiteRegEx": "Jones and Staveley.", "year": 1999}, {"title": "Automatic hypertext keyphrase detection", "author": ["Daniel Kelleher", "Saturnino Luz."], "venue": "Proceedings of the 19th International Joint Conference on Artificial Intelligence. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, IJCAI\u201905, pages 1608\u20131609.", "citeRegEx": "Kelleher and Luz.,? 2005", "shortCiteRegEx": "Kelleher and Luz.", "year": 2005}, {"title": "Semeval-2010 task 5: Automatic keyphrase extraction from scientific articles", "author": ["Su Nam Kim", "Olena Medelyan", "Min-Yen Kan", "Timothy Baldwin."], "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation. Association for Computa-", "citeRegEx": "Kim et al\\.,? 2010", "shortCiteRegEx": "Kim et al\\.", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Large dataset for keyphrases extraction", "author": ["Mikalai Krapivin", "Aliaksandr Autayeu", "Maurizio Marchese."], "venue": "Technical Report DISI-09-055, DISI, Trento, Italy.", "citeRegEx": "Krapivin et al\\.,? 2008", "shortCiteRegEx": "Krapivin et al\\.", "year": 2008}, {"title": "Unsupervised Keyphrase Extraction: Introducing New Kinds of Words to Keyphrases", "author": ["Tho Thi Ngoc Le", "Minh Le Nguyen", "Akira Shimazu"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Le et al\\.", "year": 2016}, {"title": "Automatic keyphrase extraction by bridging vocabulary gap", "author": ["Zhiyuan Liu", "Xinxiong Chen", "Yabin Zheng", "Maosong Sun."], "venue": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning. Association for Computational", "citeRegEx": "Liu et al\\.,? 2011", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Automatic keyphrase extraction via topic decomposition", "author": ["Zhiyuan Liu", "Wenyi Huang", "Yabin Zheng", "Maosong Sun."], "venue": "Proceedings of the 2010 conference on empirical methods in natural language processing. Association for Compu-", "citeRegEx": "Liu et al\\.,? 2010", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Clustering to find exemplar terms for keyphrase extraction", "author": ["Zhiyuan Liu", "Peng Li", "Yabin Zheng", "Maosong Sun."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1. Association", "citeRegEx": "Liu et al\\.,? 2009", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Humb: Automatic key term extraction from scientific articles in grobid", "author": ["Patrice Lopez", "Laurent Romary."], "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics, Strouds-", "citeRegEx": "Lopez and Romary.,? 2010", "shortCiteRegEx": "Lopez and Romary.", "year": 2010}, {"title": "Sequence level training with recurrent neural networks", "author": ["Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "Keyword extraction from a single document using word co-occurrence statistical information", "author": ["Yutaka Matsuo", "Mitsuru Ishizuka."], "venue": "International Journal on Artificial Intelligence Tools 13(01):157\u2013 169.", "citeRegEx": "Matsuo and Ishizuka.,? 2004", "shortCiteRegEx": "Matsuo and Ishizuka.", "year": 2004}, {"title": "Human-competitive tagging using automatic keyphrase extraction", "author": ["Olena Medelyan", "Eibe Frank", "Ian H Witten."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3. Association", "citeRegEx": "Medelyan et al\\.,? 2009a", "shortCiteRegEx": "Medelyan et al\\.", "year": 2009}, {"title": "Human-competitive tagging using automatic keyphrase extraction", "author": ["Olena Medelyan", "Eibe Frank", "Ian H. Witten."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3.", "citeRegEx": "Medelyan et al\\.,? 2009b", "shortCiteRegEx": "Medelyan et al\\.", "year": 2009}, {"title": "Topic indexing with wikipedia", "author": ["Olena Medelyan", "Ian H Witten", "David Milne."], "venue": "Proceedings of the AAAI WikiAI workshop. volume 1, pages 19\u201324.", "citeRegEx": "Medelyan et al\\.,? 2008", "shortCiteRegEx": "Medelyan et al\\.", "year": 2008}, {"title": "Textrank: Bringing order into texts", "author": ["Rada Mihalcea", "Paul Tarau."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Mihalcea and Tarau.,? 2004", "shortCiteRegEx": "Mihalcea and Tarau.", "year": 2004}, {"title": "PTR: Phrase-Based Topical Ranking for Automatic Keyphrase Extraction in Scientific Publications", "author": ["Minmei Wang", "Bo Zhao", "Yihua Huang"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Kea: Practical automatic keyphrase extraction", "author": ["Ian H Witten", "Gordon W Paynter", "Eibe Frank", "Carl Gutwin", "Craig G Nevill-Manning."], "venue": "Proceedings of the fourth ACM conference on Digital libraries. ACM, pages 254\u2013255.", "citeRegEx": "Witten et al\\.,? 1999", "shortCiteRegEx": "Witten et al\\.", "year": 1999}, {"title": "Efficient summarization with read-again and copy mechanism", "author": ["Wenyuan Zeng", "Wenjie Luo", "Sanja Fidler", "Raquel Urtasun."], "venue": "arXiv preprint arXiv:1611.03382 .", "citeRegEx": "Zeng et al\\.,? 2016", "shortCiteRegEx": "Zeng et al\\.", "year": 2016}, {"title": "Keyphrase extraction using deep recurrent neural networks on twitter", "author": ["Qi Zhang", "Yang Wang", "Yeyun Gong", "Xuanjing Huang."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "World wide web site summarization", "author": ["Yongzheng Zhang", "Nur Zincir-Heywood", "Evangelos Milios."], "venue": "Web Intelligence and Agent Systems: An International Journal 2(1):39\u201353.", "citeRegEx": "Zhang et al\\.,? 2004", "shortCiteRegEx": "Zhang et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 22, "context": "As a result, many studies have focused on ways of automatically extracting keyphrases from textual content (Liu et al., 2009; Medelyan et al., 2009a; Witten et al., 1999).", "startOffset": 107, "endOffset": 170}, {"referenceID": 26, "context": "As a result, many studies have focused on ways of automatically extracting keyphrases from textual content (Liu et al., 2009; Medelyan et al., 2009a; Witten et al., 1999).", "startOffset": 107, "endOffset": 170}, {"referenceID": 31, "context": "As a result, many studies have focused on ways of automatically extracting keyphrases from textual content (Liu et al., 2009; Medelyan et al., 2009a; Witten et al., 1999).", "startOffset": 107, "endOffset": 170}, {"referenceID": 14, "context": "document is called keypharase extraction, and it has been widely used in many applications, such as information retrieval (Jones and Staveley, 1999), text summarization (Zhang et al.", "startOffset": 122, "endOffset": 148}, {"referenceID": 34, "context": "document is called keypharase extraction, and it has been widely used in many applications, such as information retrieval (Jones and Staveley, 1999), text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006),", "startOffset": 169, "endOffset": 189}, {"referenceID": 13, "context": ", 2004), text categorization (Hulth and Megyesi, 2006),", "startOffset": 29, "endOffset": 54}, {"referenceID": 2, "context": "and opinion mining (Berend, 2011).", "startOffset": 19, "endOffset": 33}, {"referenceID": 22, "context": "Most of the existing keyphrase extraction algorithms have addressed this problem through two steps (Liu et al., 2009; Tomokiyo and Hurst, 2003).", "startOffset": 99, "endOffset": 143}, {"referenceID": 12, "context": "Researchers have tried to use n-grams or noun phrases with certain part-of-speech patterns for identifying potential candidates (Hulth, 2003; Le et al., 2016; Liu et al., 2010; Wang et al., 2016).", "startOffset": 128, "endOffset": 195}, {"referenceID": 19, "context": "Researchers have tried to use n-grams or noun phrases with certain part-of-speech patterns for identifying potential candidates (Hulth, 2003; Le et al., 2016; Liu et al., 2010; Wang et al., 2016).", "startOffset": 128, "endOffset": 195}, {"referenceID": 21, "context": "Researchers have tried to use n-grams or noun phrases with certain part-of-speech patterns for identifying potential candidates (Hulth, 2003; Le et al., 2016; Liu et al., 2010; Wang et al., 2016).", "startOffset": 128, "endOffset": 195}, {"referenceID": 30, "context": "Researchers have tried to use n-grams or noun phrases with certain part-of-speech patterns for identifying potential candidates (Hulth, 2003; Le et al., 2016; Liu et al., 2010; Wang et al., 2016).", "startOffset": 128, "endOffset": 195}, {"referenceID": 3, "context": "To effectively capture both the semantic and syntactic features, we use recurrent neural networks (RNN) (Cho et al., 2014; Gers and Schmidhuber, 2001) to compress the semantic information in the given text into a dense vector (i.", "startOffset": 104, "endOffset": 150}, {"referenceID": 5, "context": "To effectively capture both the semantic and syntactic features, we use recurrent neural networks (RNN) (Cho et al., 2014; Gers and Schmidhuber, 2001) to compress the semantic information in the given text into a dense vector (i.", "startOffset": 104, "endOffset": 150}, {"referenceID": 8, "context": "Furthermore, we incorporate a copying mechanism (Gu et al., 2016) to allow our model to find important parts based on positional information.", "startOffset": 48, "endOffset": 65}, {"referenceID": 20, "context": ", nouns, adjectives) (Liu et al., 2011; Wang et al., 2016; Le et al., 2016), and extracting important n-grams or noun phrases (Hulth, 2003; Medelyan et al.", "startOffset": 21, "endOffset": 75}, {"referenceID": 30, "context": ", nouns, adjectives) (Liu et al., 2011; Wang et al., 2016; Le et al., 2016), and extracting important n-grams or noun phrases (Hulth, 2003; Medelyan et al.", "startOffset": 21, "endOffset": 75}, {"referenceID": 19, "context": ", nouns, adjectives) (Liu et al., 2011; Wang et al., 2016; Le et al., 2016), and extracting important n-grams or noun phrases (Hulth, 2003; Medelyan et al.", "startOffset": 21, "endOffset": 75}, {"referenceID": 12, "context": ", 2016), and extracting important n-grams or noun phrases (Hulth, 2003; Medelyan et al., 2008).", "startOffset": 58, "endOffset": 94}, {"referenceID": 28, "context": ", 2016), and extracting important n-grams or noun phrases (Hulth, 2003; Medelyan et al., 2008).", "startOffset": 58, "endOffset": 94}, {"referenceID": 29, "context": "As for unsupervised approaches, primary ideas include finding the central nodes in text graph (Mihalcea and Tarau, 2004; Grineva et al., 2009), detecting representative phrases from topical clusters (Liu et al.", "startOffset": 94, "endOffset": 142}, {"referenceID": 7, "context": "As for unsupervised approaches, primary ideas include finding the central nodes in text graph (Mihalcea and Tarau, 2004; Grineva et al., 2009), detecting representative phrases from topical clusters (Liu et al.", "startOffset": 94, "endOffset": 142}, {"referenceID": 7, "context": "As for unsupervised approaches, primary ideas include finding the central nodes in text graph (Mihalcea and Tarau, 2004; Grineva et al., 2009), detecting representative phrases from topical clusters (Liu et al., 2009, 2010), and so on. Aside from the commonly adopted two-step process, another two previous studies realized the keyphrase extraction in entirely different ways. Tomokiyo and Hurst (2003) applied two language models to measure the phraseness and informa-", "startOffset": 121, "endOffset": 403}, {"referenceID": 20, "context": "Liu et al. (2011) share the most similar ideas to our work.", "startOffset": 0, "endOffset": 18}, {"referenceID": 20, "context": "Liu et al. (2011) share the most similar ideas to our work. They used a word alignment model, which learns a translation from the documents to the keyphrases. This approach alleviates the problem of vocabulary gaps between source and target to a certain degree. However, this translation model is unable to handle semantic meaning. Additionally, this model was trained with the target of title/summary to enlarge the number of training samples, which may diverge from the real objective of generating keyphrases. Zhang et al. (2016) proposed a joint-layer recurrent neural network model to extract keyphrases from tweets, which is another application of deep neural networks in the context of keyphrase extraction.", "startOffset": 0, "endOffset": 533}, {"referenceID": 1, "context": "The attention mechanism (Bahdanau et al., 2014) is a soft alignment approach that allows the model to automatically locate the relevant input", "startOffset": 24, "endOffset": 47}, {"referenceID": 2, "context": "It was first introduced by Cho et al. (2014) and Sutskever et al.", "startOffset": 27, "endOffset": 45}, {"referenceID": 2, "context": "It was first introduced by Cho et al. (2014) and Sutskever et al. (2014) to solve translation problems.", "startOffset": 27, "endOffset": 73}, {"referenceID": 1, "context": "Previous studies (Bahdanau et al., 2014; Cho et al., 2014) indicate that it can generally provide better performance of language modeling than a simple RNN and a simpler structure than other Long Short-Term Memory networks (Hochreiter and Schmidhuber, 1997).", "startOffset": 17, "endOffset": 58}, {"referenceID": 3, "context": "Previous studies (Bahdanau et al., 2014; Cho et al., 2014) indicate that it can generally provide better performance of language modeling than a simple RNN and a simpler structure than other Long Short-Term Memory networks (Hochreiter and Schmidhuber, 1997).", "startOffset": 17, "endOffset": 58}, {"referenceID": 11, "context": ", 2014) indicate that it can generally provide better performance of language modeling than a simple RNN and a simpler structure than other Long Short-Term Memory networks (Hochreiter and Schmidhuber, 1997).", "startOffset": 172, "endOffset": 206}, {"referenceID": 3, "context": "result, the above non-linear function f is replaced by the GRU function (see in (Cho et al., 2014)).", "startOffset": 80, "endOffset": 98}, {"referenceID": 1, "context": "was firstly introduced by Bahdanau et al. (2014) to make the model dynamically focus on the important parts in input.", "startOffset": 26, "endOffset": 49}, {"referenceID": 3, "context": "30,000 words in (Cho et al., 2014)), but a large amount of long-tail words are simply ignored.", "startOffset": 16, "endOffset": 34}, {"referenceID": 8, "context": "The copying mechanism (Gu et al., 2016) is one feasible solution that enables RNN to predict out-of-vocabulary words by selecting appropriate words from the source text.", "startOffset": 22, "endOffset": 39}, {"referenceID": 8, "context": "Please see (Gu et al., 2016) for more details.", "startOffset": 11, "endOffset": 28}, {"referenceID": 18, "context": "The largest one came from Krapivin et al. (2008), which contains 2,304 scientific publications.", "startOffset": 26, "endOffset": 49}, {"referenceID": 9, "context": "(Han et al., 2013; Rui et al., 2016).", "startOffset": 0, "endOffset": 36}, {"referenceID": 9, "context": "(Han et al., 2013; Rui et al., 2016). In total, we obtained a dataset of 567,830 articles, after removing duplicates and overlaps with testing datasets, which is 200 times larger than the one of Krapivin et al. (2008). Note that our model is only trained on 527,830 articles, since 40,000 publications are randomly held out, among which 20,000 articles were used for building a new test dataset KP20k.", "startOffset": 1, "endOffset": 218}, {"referenceID": 12, "context": "\u2013 Inspec (Hulth, 2003): This dataset provides 2,000 paper abstracts.", "startOffset": 9, "endOffset": 22}, {"referenceID": 18, "context": "\u2013 Krapivin (Krapivin et al., 2008): This", "startOffset": 11, "endOffset": 34}, {"referenceID": 16, "context": "\u2013 SemEval-2010 (Kim et al., 2010): 288 ar-", "startOffset": 15, "endOffset": 33}, {"referenceID": 17, "context": "Models are optimized using Adam (Kingma and Ba, 2014) with initial learning rate = 10\u22124, gradient clipping = 0.", "startOffset": 32, "endOffset": 53}, {"referenceID": 29, "context": "Four unsupervised algorithms (Tf-Idf, TextRank (Mihalcea and Tarau, 2004), SingleRank (Wan and Xiao, 2008), and ExpandRank (Wan and Xiao, 2008)) and two supervised algorithms (KEA (Witten et al.", "startOffset": 47, "endOffset": 73}, {"referenceID": 31, "context": "Four unsupervised algorithms (Tf-Idf, TextRank (Mihalcea and Tarau, 2004), SingleRank (Wan and Xiao, 2008), and ExpandRank (Wan and Xiao, 2008)) and two supervised algorithms (KEA (Witten et al., 1999) and Maui (Medelyan et al.", "startOffset": 180, "endOffset": 201}, {"referenceID": 26, "context": ", 1999) and Maui (Medelyan et al., 2009a)) are adopted as baselines.", "startOffset": 17, "endOffset": 41}, {"referenceID": 10, "context": "We set up the four unsupervised methods following the optimal settings in (Hasan and Ng, 2010), and the two supervised methods following the default setting as specified in their papers.", "startOffset": 74, "endOffset": 94}, {"referenceID": 10, "context": "The measures on NUS and SemEval here are higher than the ones reported in (Hasan and Ng, 2010) and (Kim et al.", "startOffset": 74, "endOffset": 94}, {"referenceID": 16, "context": "The measures on NUS and SemEval here are higher than the ones reported in (Hasan and Ng, 2010) and (Kim et al., 2010), probably because we utilized the paper abstract instead of the full text for training, which may", "startOffset": 99, "endOffset": 117}, {"referenceID": 10, "context": "Compared to the results reported in (Hasan and Ng, 2010), the performance of CopyRNN is better than TextRank (Mihalcea and Tarau, 2004) and KeyCluster (Liu et al.", "startOffset": 36, "endOffset": 56}, {"referenceID": 29, "context": "Compared to the results reported in (Hasan and Ng, 2010), the performance of CopyRNN is better than TextRank (Mihalcea and Tarau, 2004) and KeyCluster (Liu et al.", "startOffset": 109, "endOffset": 135}, {"referenceID": 22, "context": "Compared to the results reported in (Hasan and Ng, 2010), the performance of CopyRNN is better than TextRank (Mihalcea and Tarau, 2004) and KeyCluster (Liu et al., 2009), but lags behind the other three baselines.", "startOffset": 151, "endOffset": 169}], "year": 2017, "abstractText": "Keyphrase provides highly-summative information that can be effectively used for understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divided the to-be-summarized content into multiple text chunks, then ranked and selected the most meaningful ones. These approaches could neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text. We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks. We name it as deep keyphrase generation since it attempts to capture the deep semantic meaning of the content with a deep learning method. Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also can generate absent keyphrases based on the semantic meaning of the text. Code and dataset are available at https://github.com/memray/seq2seq-", "creator": "LaTeX with hyperref package"}}}