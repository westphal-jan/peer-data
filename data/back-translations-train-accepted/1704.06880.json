{"id": "1704.06880", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2017", "title": "Misspecified Linear Bandits", "abstract": "We consider the problem of online learning in misspecified linear stochastic multi-armed bandit problems. Regret guarantees for state-of-the-art linear bandit algorithms such as Optimism in the Face of Uncertainty Linear bandit (OFUL) hold under the assumption that the arms expected rewards are perfectly linear in their features. It is, however, of interest to investigate the impact of potential misspecification in linear bandit models, where the expected rewards are perturbed away from the linear subspace determined by the arms features. Although OFUL has recently been shown to be robust to relatively small deviations from linearity, we show that any linear bandit algorithm that enjoys optimal regret performance in the perfectly linear setting (e.g., OFUL) must suffer linear regret under a sparse additive perturbation of the linear model. In an attempt to overcome this negative result, we define a natural class of bandit models characterized by a non-sparse deviation from linearity. We argue that the OFUL algorithm can fail to achieve sublinear regret even under models that have non-sparse deviation.We finally develop a novel bandit algorithm, comprising a hypothesis test for linearity followed by a decision to use either the OFUL or Upper Confidence Bound (UCB) algorithm. For perfectly linear bandit models, the algorithm provably exhibits OFULs favorable regret performance, while for misspecified models satisfying the non-sparse deviation property, the algorithm avoids the linear regret phenomenon and falls back on UCBs sublinear regret scaling. Numerical experiments on synthetic data, and on recommendation data from the public Yahoo! Learning to Rank Challenge dataset, empirically support our findings.", "histories": [["v1", "Sun, 23 Apr 2017 04:37:57 GMT  (599kb,D)", "http://arxiv.org/abs/1704.06880v1", "Thirty-First AAAI Conference on Artificial Intelligence, 2017"]], "COMMENTS": "Thirty-First AAAI Conference on Artificial Intelligence, 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["avishek ghosh", "sayak ray chowdhury", "aditya gopalan"], "accepted": true, "id": "1704.06880"}, "pdf": {"name": "1704.06880.pdf", "metadata": {"source": "CRF", "title": "Misspecified Linear Bandits", "authors": ["Avishek Ghosh", "Sayak Ray Chowdhury", "Aditya Gopalan"], "emails": ["ghosh@berkeley.edu", "srchowdhury@ece.iisc.ernet.in", "aditya@ece.iisc.ernet.in"], "sections": [{"heading": "1 Introduction", "text": "It is one of the most widely studied performance measures of bandit problems, and it is well known that the optimum regret that can be achieved in an iid stochastic bandit instance can be achieved in a different way that allows the development of artificial intelligence (www.aaaai.org), without additional information on the distribution of rewards, is1 O-O), for example, through the cele copyright c \u00a9 2017, the Association for the Development of Artificial Intelligence (www.aaaai.org). All rights reserved. 1The notation of polylogarithmic factors.brated Upper Confidence Bound (UCB) of the (Auer, Cesa-Bianchi, and Fischer 2002).The (polynomial) dependence of regret in a standardized factors.brated Upper Confidence Bound (UCB) of the Upper Confidence of Bound (Cesa-Bianchi, and Fischer 2002)."}, {"heading": "2 Setup & Preliminaries", "text": "Consider a multi-armed bandit problem with N arms, and a d-dimensional (d > N) context or feature vector xi-Rd connected to each arm, i = 1,.., N. An arm i yields a stochastic and independent reward with expectation \u00b5i when playing. Let's allow the best expected reward, and let X be the matrix that has feature vectors for each arm as columns: X = [x1 | x2 |.. | xN]. Rd \u00b7 N, where X T is assumed to have the full number of columns. Define \u00b5 = [\u00b51 \u00b52..... \u00b5N] T-RN is the expected reward vector. At any time Instant t = 1, 2,... the learner selects any of the N arms and observes the reward collected from that arm. The action set for the player isA = {1, 2, N} Regret after T is perfect."}, {"heading": "3 Lower Bound for Linear Bandit Algorithms under Large Sparse Deviation", "text": "In this section, we present our first key result - a general lower limit for regretting each \"optimal\" linear bandit algorithm on wrongly stated problem cases. Specifically, we show that for linear parameterized models of dimension d, any linear bandit algorithm that enjoys optimal O (d) repentance scaling must indeed suffer linear regret on a wrongly indicated model in which only one arm has a wrongly adjusted expected reward. Theory 1. Let A be an algorithm for the linear bandit problem whose expected regret O (d) is disrupted on each linear problem instance with characteristic dimension d, time horizon T, and expected rewards limited in absolute value by 1. There exists an instance of a sparsely disturbed linear bandit problem, whereby the expected reward of an arm is affected, causing A to suffer the linear argument, i.e., the optimal condition of the bandit problem is shifted to the theory 1, but the expected reward of an arm has been disturbed."}, {"heading": "4 Performance of OFUL Under Deviation", "text": "A state-of-the-art algorithm for the linear bandit problem is OFUL. We study the performance of OFUL for various cases of deviations (suitable for \"small\" and \"large\"). In particular, we argue that OFUL is robust against small deviations, but for large deviations the performance of OFUL is very poor, resulting in linear repentance scaling. The results motivate us to propose a more robust algorithm to address linear bandit problems with significantly large deviations. At a time t \u2265 1, OFUL solves a regularized linear minimum squares problem based on previous actions and observations up to t \u2212 1 to estimate the unknown parameter."}, {"heading": "4.1 OFUL with Small Deviation", "text": "If the deviation from linearity is considerably low, it can be shown that OFUL is similar to the perfect linear model in terms of repentance scaling (see (Gopalan, Maillard and Zaki 2016, Theorem 3). Assuming that the \"small\" deviation is quantified in detail, the cumulative regret up to the time of OFUL by ROFUL (T) \u2264 8\u03c1 \u2032 Td log (1 + TL2\u03bbd) (\u03bb1 / 2S + R \u221a 2 log 1\u03b4 + d log (1 + TL2\u03bbd) results, which is a geometric constant that measures the \"distortion\" in the actual reward of the arms in terms of (linear) approximation and is a slight paramarization of the repentance presence."}, {"heading": "4.2 OFUL with Large Sparse Deviation", "text": "The regret of OFUL under pure linear bandit instance is O (d \u221a T). Therefore, the cumulative expected regret becomes under large, sparse deviation from this theorem 1, primarily because the OFUL algorithm is known to be the most competitive in terms of repentance scaling. It is conceivable that similar results can be shown for other related bandit strategies such as ConfidenceBall (Dani, Hayes and Kakade 2008), Uncertainty Ellipsoid (Rusmevichientong and Tsitsiklis 2010), etc."}, {"heading": "4.3 OFUL with Large Non-sparse Deviation", "text": "\"We need to identify a natural class of structured large deviations that we call non-sparse dub = > Sparse. We impose the following structure with respect to spareness on the expected rewards \u00b5. Recall from Section 2 that X denotes the context matrix, \u00b5 the mean reward vector, \u03b8 a choice of weights, and the deviation from the mean \u00b5; thus, \u00b5 = X T devise 1 (non-sparse deviation). Given a feature set Xf = {x1, xN} the mean reward vector l > 0, \u03b2 [0, 1], an expected reward vector. [RN] foreign exchange is said to be the (l, \u03b2) deviation property if, P (xTid + 1,... f i1]. \u2212 1 [\u00b5i1, id]."}, {"heading": "5 A Linear Bandit Algorithm Robust to Large, Non-sparse Deviations", "text": "In this section, the goal is achieved to develop a new algorithm that maintains the sublinear repentance property in a model with not sparse, large deviations. Non-sparse deviations can occur naturally in the presence of stochastic measurement or estimation sounds; for example, xi and x-i are the measured and original context vector for arm i with xi = x-i +. Ti-II-II can act as a Gaussian random variable with the mean E (Hi-V) = i. Let's replace \u00b5 = X-T-V +. It is possible to find a suitable pair (l, \u03b2) (definition 1) for this model and thus \u00b5 is not sparse. The associated feature vectors corresponding to the mean reward vector of definition 1 are called \"uniformly disturbed features.\" We now define 2 hypotheses -H0 and H1, which are intuitively \"non-linear\" and \"1.\""}, {"heading": "5.1 A Robust Linear Bandit (RLB) Algorithm", "text": "The sequence of actions for the proposed novel bandit algorithm, namely Robust Linear Bandit (RLB), is summarized in algorithm 1, which mainly consists of three steps. First, RLB performs a first sampling phase in which d + 1 arms of N play for the rest of the horizon either OFUL or UCB for the rest of the horizon. We will illustrate the necessity of the non-economical deviation as follows: Consider a problem instance with = (0,.), c, 0., 0."}, {"heading": "6 Regret Analysis", "text": "The goal of the RLB is to learn the gap from linearity and play accordingly to obtain the regret of Table 1. For zero deviation, the RLB exploits the linear reward structure and suffers a regret6Due to space constraints, Lemas 2, 3 and 4 with their evidence are converted to supplementary material. For large, not sparse definitions, the RLB discards the contexts and avoids linear regret. During the initial sampling phase up to this point, the regret is scaled linearly, as each step is either forced exploration or exploitation, i.e. Rs (\u03c4) = O. After that, based on the player's decision, either OFUL or UCB is played. For H0, we use the regret of OFUL as indicated in (Abbasi-Yadkori, Pa'l and Szepesva \u0301 ri 2011)."}, {"heading": "6.1 Regret of Algorithm 1", "text": "Lemma (3) shows that if H0 is true, LB-k > LB and UCB are played with a probability of 1 \u2212 \u03b41 (k, \u03bb) and \u03b41 (k, \u03bb) and accumulated remorse accordingly. By an appropriate choice of k and \u03bb, \u03b41 (k, \u03bb) can be arbitrarily placed close to 0. Similarly, under H1, the corresponding probabilities are pressed to 2 (k, \u03bb) + \u03b2 and 1 \u2212 \u03b42 (k, \u03bb) \u2212 \u03b2 or (Lemma 4), respectively. \u03b2 results from the definition of non-sparse deviation. Therefore, under non-sparse deviation, the probability of OFUL and linear remorse occurring is pressed to 2 (k, \u03bb) + \u03b2 each to an arbitrarily small value, which by correct choice of k and \u03bb as typical, \u03b2 is very small and close to 0. We can choose as Log (T), a sublinear function of T."}, {"heading": "7 Simulation Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Synthetic Data", "text": "In this setup, we assume that N = 1000, d = 20 and k = 50. \u03bb and R are assumed to be 0.001 and 0.1, respectively. Context vectors and mean rewards are generated randomly (in the range [0, 1]). All high probability events are simulated with an error probability of 0.001. The simulation is run for 1000 cases and cumulative regret is correctly predicted in Figure 1.Under H0 with a probability of false alarm 0.0001. Figure 1 shows the regret performance of the RLB. In the sampling phase, the regret is linear and thus larger than the disturbed OFUL and UCB algorithms. After the sampling phase, regret of the RLB closely follows regret of the OFUL with a probability of 0.9999. The false alarm probability can be further increased if the value of k is increased. If we allow the time horizon T to be very large, the deviation in terms of regret between UCB phase and diesel RLB will be significantly larger with a false alarm of 0.001."}, {"heading": "7.2 Yahoo! Learning to Rank Data", "text": "The performance of the RLB is evaluated on the Yahoo! data set \"Learning to Rank Challenge\" (Chapelle and Chang 2011). Specifically, we use the file set2.test.txt. The data set consists of query documents with 103174 rows and 702 columns. The first column lists the regret of the user (which we take as a reward) with entries {0, 1, 3, 4} and the second column captures the user ID. We treat the remaining 700 columns as a context vector corresponding to each user. We randomly select 20,000 rows and 50 columns."}, {"heading": "8 Conclusion and Future work", "text": "We have shown that a state-of-the-art linear bandit algorithm such as OFUL is not always robust against deviations from linearity. To overcome this, we have proposed a robust bandit algorithm and provided a formal upper limit on regret. Experiments with both synthetic and real data sets support our argument that (a) feature reward cards can often be anything but linear in practice, and (b) the use of a strategy that is aware of the potential deviation from linearity and tests for it appropriately lead to performance gains. Furthermore, it would be interesting to investigate non-linearity structures other than sparse deviations, as investigated here, and to deduce lower limits for the class of general bandit problems with predetermined characteristics. It is also interesting to examine the performance of Bayesian-inspired algorithms such as Thompson Sampling in linear bandits in the presence of deviations."}, {"heading": "Acknowledgements", "text": "This work was partially funded by the DST INSPIRE Faculty Scholarship IFA13-ENG-69. The authors are grateful to anonymous reviewers for useful comments."}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["P\u00e1l Abbasi-Yadkori", "D. P\u00e1l", "C. Szepesv\u00e1ri"], "venue": "In Proc. NIPS", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2011}, {"title": "Analysis of Thompson sampling for the multi-armed bandit problem", "author": ["Agrawal", "S. Goyal 2012] Agrawal", "N. Goyal"], "venue": "Journal of Machine Learning Research - Proceedings Track", "citeRegEx": "Agrawal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2012}, {"title": "Thompson sampling for contextual bandits with linear payoffs", "author": ["Agrawal", "S. Goyal 2013] Agrawal", "N. Goyal"], "venue": null, "citeRegEx": "Agrawal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2013}, {"title": "Minimax policies for adversarial and stochastic bandits", "author": ["Audibert", "Bubeck 2009] Audibert", "J.-Y", "S. Bubeck"], "venue": "COLT", "citeRegEx": "Audibert et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2009}, {"title": "Finite-time analysis of the multiarmed bandit problem. Machine Learning 47(2):235\u2013256", "author": ["Cesa-Bianchi Auer", "P. Fischer 2002] Auer", "N. CesaBianchi", "P. Fischer"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "On the (surprising) sufficiency of linear models for dynamic pricing with demand learning. Management Science 61(4):723\u2013739", "author": ["Besbes", "O. Zeevi 2015] Besbes", "A. Zeevi"], "venue": null, "citeRegEx": "Besbes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Besbes et al\\.", "year": 2015}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["Bubeck", "S. Cesa-Bianchi 2012] Bubeck", "N. CesaBianchi"], "venue": "arXiv preprint arXiv:1204.5721", "citeRegEx": "Bubeck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2012}, {"title": "Finite-time regret bounds for the multiarmed bandit problem", "author": ["Cesa-Bianchi", "N. Fischer 1998] Cesa-Bianchi", "P. Fischer"], "venue": "In In 5th International Conference on Machine Learning,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 1998}, {"title": "Yahoo! learning to rank challenge overview", "author": ["Chapelle", "O. Chang 2011] Chapelle", "Y. Chang"], "venue": "In Yahoo! Learning to Rank Challenge,", "citeRegEx": "Chapelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2011}, {"title": "Contextual bandits with linear payoff functions", "author": ["Chu"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Chu,? \\Q2011\\E", "shortCiteRegEx": "Chu", "year": 2011}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Hayes Dani", "V. Kakade 2008] Dani", "T.P. Hayes", "S.M. Kakade"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "A near optimal policy for channel allocation in cognitive radio", "author": ["Filippi"], "venue": "Recent Advances in Reinforcement Learning,", "citeRegEx": "Filippi,? \\Q2008\\E", "shortCiteRegEx": "Filippi", "year": 2008}, {"title": "Low-rank bandits with latent mixtures", "author": ["Maillard Gopalan", "A. Zaki 2016] Gopalan", "O.-A. Maillard", "M. Zaki"], "venue": "arXiv preprint arXiv:1609.01508", "citeRegEx": "Gopalan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2016}, {"title": "Kernel regularized least squares: Reducing misspecification bias with a flexible and interpretable machine learning approach. Political Analysis 22(2):143\u2013168", "author": ["Hainmueller", "J. Hazlett 2014] Hainmueller", "C. Hazlett"], "venue": null, "citeRegEx": "Hainmueller et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hainmueller et al\\.", "year": 2014}, {"title": "On bayesian upper confidence bounds for bandit problems", "author": ["Garivier Kaufmann", "E. Cappe 2012] Kaufmann", "A. Garivier", "O. Cappe"], "venue": "In AISTATS", "citeRegEx": "Kaufmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "Thompson sampling: an asymptotically optimal finite-time analysis", "author": ["Korda Kaufmann", "E. Munos 2012] Kaufmann", "N. Korda", "R. Munos"], "venue": "In ALT", "citeRegEx": "Kaufmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["Li"], "venue": null, "citeRegEx": "Li,? \\Q2010\\E", "shortCiteRegEx": "Li", "year": 2010}, {"title": "Linearly parameterized bandits. Mathematics of Operations Research 35(2):395\u2013411", "author": ["Rusmevichientong", "J.N. Tsitsiklis"], "venue": null, "citeRegEx": "Rusmevichientong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rusmevichientong et al\\.", "year": 2010}, {"title": "Reinforcement learning: An introduction, volume 1. MIT press Cambridge", "author": ["Sutton", "R.S. Barto 1998] Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson 1933] Thompson"], "venue": null, "citeRegEx": "Thompson,? \\Q1933\\E", "shortCiteRegEx": "Thompson", "year": 1933}], "referenceMentions": [{"referenceID": 19, "context": "2010), Optimism in the Face of Uncertainty Linear bandit (OFUL) (Abbasi-Yadkori, P\u00e1l, and Szepesv\u00e1ri 2011) and Thompson sampling (Thompson 1933) give regret \u00d5(d \u221a T ) where d is the feature dimension.", "startOffset": 129, "endOffset": 144}, {"referenceID": 19, "context": "php?datatype=c Boltzmann exploration (Sutton and Barto 1998), BayesUCB (Kaufmann, Garivier, and Cappe 2012), MOSS (Audibert and Bubeck 2009) and Thompson sampling (Thompson 1933; Agrawal and Goyal 2012; Kaufmann, Korda, and Munos 2012), to name a few.", "startOffset": 163, "endOffset": 235}], "year": 2017, "abstractText": "We consider the problem of online learning in misspecified linear stochastic multi-armed bandit problems. Regret guarantees for state-of-the-art linear bandit algorithms such as Optimism in the Face of Uncertainty Linear bandit (OFUL) hold under the assumption that the arms expected rewards are perfectly linear in their features. It is, however, of interest to investigate the impact of potential misspecification in linear bandit models, where the expected rewards are perturbed away from the linear subspace determined by the arms features. Although OFUL has recently been shown to be robust to relatively small deviations from linearity, we show that any linear bandit algorithm that enjoys optimal regret performance in the perfectly linear setting (e.g., OFUL) must suffer linear regret under a sparse additive perturbation of the linear model. In an attempt to overcome this negative result, we define a natural class of bandit models characterized by a non-sparse deviation from linearity. We argue that the OFUL algorithm can fail to achieve sublinear regret even under models that have non-sparse deviation. We finally develop a novel bandit algorithm, comprising a hypothesis test for linearity followed by a decision to use either the OFUL or Upper Confidence Bound (UCB) algorithm. For perfectly linear bandit models, the algorithm provably exhibits OFULs favorable regret performance, while for misspecified models satisfying the non-sparse deviation property, the algorithm avoids the linear regret phenomenon and falls back on UCBs sublinear regret scaling. Numerical experiments on synthetic data, and on recommendation data from the public Yahoo! Learning to Rank Challenge dataset, empirically support our findings.", "creator": "LaTeX with hyperref package"}}}