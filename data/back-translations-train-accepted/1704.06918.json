{"id": "1704.06918", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2017", "title": "Neural Machine Translation via Binary Code Prediction", "abstract": "In this paper, we propose a new method for calculating the output layer in neural machine translation systems. The method is based on predicting a binary code for each word and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case. In addition, we also introduce two advanced approaches to improve the robustness of the proposed model: using error-correcting codes and combining softmax and binary codes. Experiments on two English-Japanese bidirectional translation tasks show proposed models achieve BLEU scores that approach the softmax, while reducing memory usage to the order of less than 1/10 and improving decoding speed on CPUs by x5 to x10.", "histories": [["v1", "Sun, 23 Apr 2017 12:38:13 GMT  (345kb)", "http://arxiv.org/abs/1704.06918v1", "Accepted as a long paper at ACL2017"]], "COMMENTS": "Accepted as a long paper at ACL2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yusuke oda", "philip arthur", "graham neubig", "koichiro yoshino", "satoshi nakamura 0001"], "accepted": true, "id": "1704.06918"}, "pdf": {"name": "1704.06918.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Translation via Binary Code Prediction", "authors": ["Yusuke Oda", "Philip Arthur", "Graham Neubig", "Koichiro Yoshino", "Satoshi Nakamura"], "emails": ["philip.arthur.om0}@is.naist.jp,", "gneubig@cs.cmu.edu,", "s-nakamura}@is.naist.jp"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.06 918v 1 [cs.C L] 23 Apr 201 7In this paper, we propose a new method for calculating the output layer in neural machine translation systems, based on predicting a binary code for each word and can reduce the calculation time / memory requirements of the output layer, which should be logarithmic at best. Furthermore, we present two advanced approaches to improve the robustness of the proposed model: the use of error correction codes and the combination of softmax- and binary codes. Experiments on two English \u2194 Japanese bidirectional translation tasks show that proposed models achieve BLEU values that approach the Softmax, while memory usage is reduced to less than 1 / 10 and the decoding speed on CPUs is improved by x5 to x10."}, {"heading": "1 Introduction", "text": "In dealing with the problem, it is as if it is a real problem."}, {"heading": "2 Problem Description and Prior Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Formulation and Standard Softmax", "text": "Most current NMT models use unit representations to represent the words in the output vocabulary - each word w is represented by a unique, sparse vector eid (w) \u0445R V, where only one element at the position corresponding to the word ID id (w) is equal to 1, while others are 0. V represents the vocabulary size of the target language. NMT models optimize network parameters by treating the most uniform representation eid (w) as the true probability distribution and minimizing the intersection entropy between it and the softmax probability v: LH (v, id (w): = H (eid (w), v), (1) = log sumexpu \u2212 uid (w), (2) v: = expu / sum expu, (3) u: = Whuh + \u03b2u, (4) where sumx represents the sum of all elements in x."}, {"heading": "2.2 Prior Work on Suppressing Complexity of NMTModels", "text": "The hierarchical Softmax method (Morin and Bengio, 2005) predicts each word based on binary decisions and reduces the computing time to O (H log V). However, this method still requires O (HV) space for the parameters and requires a much more complicated calculation than the standard Softmax, especially at test time.The differentiated Softmax method (Chen et al., 2016) splits words into clusters and predicts words using a separate part of the hidden layer for each word cluster. This method makes the output layer conversion matrix more sparse than a fully connected Softmax layer and can reduce the time-space calculation by ignoring the zero part of the matrix. However, this method limits the use of hidden layer and the size of the matrix relative to V.Sampling-based approximations (Mnih and Teh, 2012; Mikolov et, 2013 to reduce the space directly) to the approaches of 2016."}, {"heading": "3 Binary Code Prediction Models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Representing Words using Bit Arrays", "text": "Figure 2 (a) shows the conventional Softmax prediction (5), and Figure 2 (b) shows the binary code prediction model proposed in this study (q). Unlike the conventional Softmax method, the proposed method predicts each output word indirectly using dense bit arrays corresponding to each word. Let b (w): = [b1 (w), \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 bit (w), \u00b7 B is the target bit arrays reached for each word w, with each bi (w) being an independent binary function given w, and B being the number of bits in the entire array. For convenience, we introduce some constraints on b. A word w is mapped to only one bit array b (w). Second, all unique words can be discriminated against by b, i.e. all bit arrays fulfill this satisfaction: 1id (w) 6 = id (w)."}, {"heading": "3.2 Loss Functions", "text": "To learn correct binary representations, we can use arbitrary loss functions that are (sub-) differentiable and meet a constraint that includes: LB (q, b) (Brown et al., 1992) with zero padding to adjust code lengths, and some original assignment methods based on the word2vec embeddings (Mikolov et al., 2013). Algorithm 1Mapping words to bit fields. Required: w \u2212 V Ensure: b \u00b2 {0, 1} B = bit field that represents wx: = 0 if w = UNK 1, if w = BOS 2, if w = EOS 2 + rank (w), otherwisebi: = = x / 2 i \u2212 1 = bit field that also represents loss."}, {"heading": "3.3 Efficiency of the Binary Code Prediction", "text": "The calculation complexity for the Whq and \u03b2q parameters is O (HB), which is equivalent to O (H log V) when using a minimal mapping method such as the one in Algorithm 1, and is significantly smaller than O (HV) when using standard Softmax predictions. For example, if we choose V = 65536 = 216 and apply the mapping method of Algorithm 1, B = 16 and the total amount of calculation in the output layer could be suppressed to 1 / 4096 of its original size. On another note, the binary code prediction model proposed in this study shares some ideas with the hierarchical Softmax method (Morin and Bengio, 2005). In fact, if we use a binary tree-based mapping function for b, our model can be tested as the hierarchical Softmax with two."}, {"heading": "3.4 Hybrid Softmax/Binary Model", "text": "According to the Zipf Law (Zipf, 1949), the distribution of word phenomena in an actual q corpus is limited to a small subset of vocabulary. As a result, the proposed model mostly learns properties for common words and cannot get enough opportunities to learn for rare words. However, to alleviate this problem, we are introducing a hybrid model that uses both softmax prediction and binary code prediction, as shown in Figure 2 (c). In this model, the output layer calculates a standard softmax for the N \u2212 1 most common words and an OTHER marker that specifies all rare words. If the softmax layer predicts OTHER, then the binary code layer is used to predict the representation of rare words. In this case, the actual probability of generating a particular word can be divided into two equations according to the frequency of words: Pr (w | h) and quantity (w)."}, {"heading": "3.5 Applying Error-correcting Codes", "text": "The 2 methods proposed in previous sections impose constraints on all bits in q, and the actual value of the q bit must be correctly estimated in order to select the right word."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experimental Settings", "text": "We examined the performance of the proposed methods on two English-Japanese bi-directional translation tasks that have different translation difficulties: ASPEC (Nakazawa et al., 2016) and BTEC (Takezawa et al., 1999). Table 1 describes details of two corpora. To prepare the input for the training, we used Tokenizer.perl in Moses (Koehn et al., 2007) and KyTea (Neubig et al., 2011) for English-Japanese tokenizations, lowercase letters used. Perl from Moses and replaced words outside the vocabulary such as this rank (w) > V \u2212 3 in the UNK markers. We implemented each NMT model using C + + in the DyNet framework (Neubig et al., 2017) and trained / tested on 1 GPU (GeForce GTX TITAN X). Each test is also performed on CPUs to compare its processing time."}, {"heading": "4.2 Results and Discussion", "text": "The aforementioned persons have been taken into custody by the police and taken into custody. (red) The police are now looking for witnesses who can give clues to the incident. (red) The police are looking for witnesses who can give clues to the incident. (red) The police are looking for witnesses who can give clues to the incident. (red) The police are looking for witnesses who can give clues to the perpetrators. (red) The police are looking for witnesses who can give clues to the perpetrators. (red) The police are looking for witnesses who can give clues to the perpetrators. (red) The police are asking for sufficient clues. (red)"}, {"heading": "5 Conclusion", "text": "In this study, we proposed neural machine translation models that indirectly predict output words via binary codes, and two model improvements: a hybrid prediction model that uses both softmax and binary codes, and the introduction of error correction codes to introduce the robustness of binary code predictions. Experiments show that the proposed model can achieve comparative translation qualities to standard Softmax prediction, while significantly suppressing the number of parameters in the output layer and increasing computing speed during training, and especially during testing. An interesting possibility for future work is to automatically learn coding and error correction codes that are well suited to the type of binary code prediction we are performing here. In algorithms 2 and 3, we use heuristically determined windings, and it is likely that learning them together with the model could lead to improved accuracy or better compression capability."}, {"heading": "Acknowledgments", "text": "Part of this work was supported by JSPS KAKENHI Grant Numbers JP16H05873 and JP17H00747 as well as Grant-in-Aid for JSPS Fellows Grant Number 15J10649."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyunghyunCho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Class-based n-gram models of natural language", "author": ["Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai."], "venue": "Computational linguistics 18(4):467\u2013479.", "citeRegEx": "Brown et al\\.,? 1992", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Strategies for training large vocabulary neural language models. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Wenlin Chen", "David Grangier", "andMichael Auli"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Variablelength word encodings for neural translation models", "author": ["Rohan Chitnis", "John DeNero."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Lisbon, Portu-", "citeRegEx": "Chitnis and DeNero.,? 2015", "shortCiteRegEx": "Chitnis and DeNero.", "year": 2015}, {"title": "Solving multiclass learning problems via errorcorrecting output codes", "author": ["Thomas G. Dietterich", "Ghulum Bakiri."], "venue": "Journal of Artificial Intelligence Research 2:263\u2013286.", "citeRegEx": "Dietterich and Bakiri.,? 1995", "shortCiteRegEx": "Dietterich and Bakiri.", "year": 1995}, {"title": "Multilabel classification with error-correcting codes", "author": ["Chun-Sung Ferng", "Hsuan-Tien Lin."], "venue": "Journal of Machine Learning Research 20:281\u2013295.", "citeRegEx": "Ferng and Lin.,? 2011", "shortCiteRegEx": "Ferng and Lin.", "year": 2011}, {"title": "Multilabel classification using error-correcting codes of hard or soft bits", "author": ["Chun-Sung Ferng", "Hsuan-Tien Lin."], "venue": "IEEE transactions on neural networks and learning systems 24(11):1888\u20131900.", "citeRegEx": "Ferng and Lin.,? 2013", "shortCiteRegEx": "Ferng and Lin.", "year": 2013}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins."], "venue": "Neural computation 12(10):2451\u20132471.", "citeRegEx": "Gers et al\\.,? 2000", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Notes on digital coding", "author": ["Marcel J.E. Golay."], "venue": "Proceedings of the Institute of Radio Engineers 37:657.", "citeRegEx": "Golay.,? 1949", "shortCiteRegEx": "Golay.", "year": 1949}, {"title": "A method for the construction of minimum-redundancy codes", "author": ["David A. Huffman."], "venue": "Proceedings of the Institute of Radio Engineers 40(9):1098\u20131101.", "citeRegEx": "Huffman.,? 1952", "shortCiteRegEx": "Huffman.", "year": 1952}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "On nearest-neighbor error-correcting output codes with application to all-pairs multiclass support vector machines", "author": ["Aldebaro Klautau", "Nikola Jevti\u0107", "Alon Orlitsky."], "venue": "Journal of Machine Learning Research 4(April):1\u201315.", "citeRegEx": "Klautau et al\\.,? 2003", "shortCiteRegEx": "Klautau et al\\.", "year": 2003}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Richard Zens", "Chris Dyer", "Ondrej Bojar", "Alexandra Constantin", "Evan Herbst."], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion", "citeRegEx": "Zens et al\\.,? 2007", "shortCiteRegEx": "Zens et al\\.", "year": 2007}, {"title": "Multilabel classification using error correction codes", "author": ["Abbas Z Kouzani."], "venue": "International Symposium on Intelligence Computation and Applications. Springer, pages 444\u2013454.", "citeRegEx": "Kouzani.,? 2010", "shortCiteRegEx": "Kouzani.", "year": 2010}, {"title": "Multilabel classification by bch code and random forests", "author": ["Abbas Z Kouzani", "Gulisong Nasireding."], "venue": "International journal of recent trends in engineering 2(1):113\u2013116.", "citeRegEx": "Kouzani and Nasireding.,? 2009", "shortCiteRegEx": "Kouzani and Nasireding.", "year": 2009}, {"title": "Vocabulary selection strategies for neural machine translation", "author": ["Gurvan L\u2019Hostis", "David Grangier", "Michael Auli"], "venue": null, "citeRegEx": "L.Hostis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "L.Hostis et al\\.", "year": 2016}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black."], "venue": "arXiv preprint arXiv:1511.04586 .", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Using svm and error-correcting codes for multiclass dialog act classification in meeting corpus", "author": ["Yang Liu."], "venue": "INTERSPEECH.", "citeRegEx": "Liu.,? 2006", "shortCiteRegEx": "Liu.", "year": 2006}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Compu-", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Vocabulary manipulation for neural machine translation", "author": ["Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computa-", "citeRegEx": "Mi et al\\.,? 2016", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["AndriyMnih", "YeeWhye Teh."], "venue": "Proceedings of the 29th International Conference on Machine Learning.", "citeRegEx": "AndriyMnih and Teh.,? 2012", "shortCiteRegEx": "AndriyMnih and Teh.", "year": 2012}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio."], "venue": "Proceedings of Tenth International Workshop on Artificial Intelligence and Statistics. volume 5, pages 246\u2013252.", "citeRegEx": "Morin and Bengio.,? 2005", "shortCiteRegEx": "Morin and Bengio.", "year": 2005}, {"title": "Aspec: Asian scientific paper excerpt corpus", "author": ["Toshiaki Nakazawa", "Manabu Yaguchi", "Kiyotaka Uchimoto", "Masao Utiyama", "Eiichiro Sumita", "Sadao Kurohashi", "Hitoshi Isahara."], "venue": "Proceedings of the Ninth International Conference on Language", "citeRegEx": "Nakazawa et al\\.,? 2016", "shortCiteRegEx": "Nakazawa et al\\.", "year": 2016}, {"title": "Dynet: The dynamic neural network toolkit", "author": ["Lingpeng Kong", "Adhiguna Kuncoro", "Gaurav Kumar", "Chaitanya Malaviya", "Paul Michel", "Yusuke Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin."], "venue": "arXiv preprint", "citeRegEx": "Kong et al\\.,? 2017", "shortCiteRegEx": "Kong et al\\.", "year": 2017}, {"title": "Pointwise prediction for robust, adaptable japanese morphological analysis", "author": ["Graham Neubig", "Yosuke Nakata", "Shinsuke Mori."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Neubig et al\\.,? 2011", "shortCiteRegEx": "Neubig et al\\.", "year": 2011}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguistics. Association for", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "A mathematical theory of communication", "author": ["Claude E. Shannon."], "venue": "Bell System Technical Journal 27(3):379\u2013423.", "citeRegEx": "Shannon.,? 1948", "shortCiteRegEx": "Shannon.", "year": 1948}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Building a bilingual travel conversation database for speech translation research", "author": ["Toshiyuki Takezawa."], "venue": "Proc. of the 2nd international workshop on East-Asian resources and evaluation conference on language resources and evaluation. pages 17\u201320.", "citeRegEx": "Takezawa.,? 1999", "shortCiteRegEx": "Takezawa.", "year": 1999}, {"title": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm", "author": ["Andrew Viterbi."], "venue": "IEEE transactions on Information Theory 13(2):260\u2013269.", "citeRegEx": "Viterbi.,? 1967", "shortCiteRegEx": "Viterbi.", "year": 1967}, {"title": "Human behavior and the principle of least effort", "author": ["George. K. Zipf"], "venue": null, "citeRegEx": "Zipf.,? \\Q1949\\E", "shortCiteRegEx": "Zipf.", "year": 1949}], "referenceMentions": [{"referenceID": 30, "context": "This is particularly a problem in neural machine translation (NMT) models (Sutskever et al., 2014), such as the attention-based models (Bahdanau et al.", "startOffset": 74, "endOffset": 98}, {"referenceID": 0, "context": ", 2014), such as the attention-based models (Bahdanau et al., 2014; Luong et al., 2015) shown in Figure 1.", "startOffset": 44, "endOffset": 87}, {"referenceID": 18, "context": ", 2014), such as the attention-based models (Bahdanau et al., 2014; Luong et al., 2015) shown in Figure 1.", "startOffset": 44, "endOffset": 87}, {"referenceID": 32, "context": "Second, we propose the use of convolutional error correcting codes with Viterbi decoding (Viterbi, 1967), which add redundancy to the binary representation, and even in the face of localized mistakes in the calculation of the representation, are able to recover the correct word.", "startOffset": 89, "endOffset": 104}, {"referenceID": 30, "context": "According to Equation (4), this model clearly requires time/space computation in proportion to O(HV ), and the actual load of the computation of the output layer is directly affected by the size of vocabulary V , which is typically set around tens of thousands (Sutskever et al., 2014).", "startOffset": 261, "endOffset": 285}, {"referenceID": 22, "context": "The hierarchical softmax (Morin and Bengio, 2005) predicts each word based on binary decision and reduces computation time to O(H log V ).", "startOffset": 25, "endOffset": 49}, {"referenceID": 2, "context": "The differentiated softmax (Chen et al., 2016) divides words into clusters, and predicts words using separate part of the hidden layer for each word clusters.", "startOffset": 27, "endOffset": 46}, {"referenceID": 20, "context": "Sampling-based approximations (Mnih and Teh, 2012; Mikolov et al., 2013) to the denominator of the softmax have also", "startOffset": 30, "endOffset": 72}, {"referenceID": 19, "context": "Vocabulary selection approaches (Mi et al., 2016; L\u2019Hostis et al., 2016) can also reduce the vocabulary size at testing, but these methods abandon full search over the target space and the quality of picked vocabularies directly affects the translation quality.", "startOffset": 32, "endOffset": 72}, {"referenceID": 15, "context": "Vocabulary selection approaches (Mi et al., 2016; L\u2019Hostis et al., 2016) can also reduce the vocabulary size at testing, but these methods abandon full search over the target space and the quality of picked vocabularies directly affects the translation quality.", "startOffset": 32, "endOffset": 72}, {"referenceID": 16, "context": "Other methods using characters (Ling et al., 2015) or subwords (Sennrich et al.", "startOffset": 31, "endOffset": 50}, {"referenceID": 27, "context": ", 2015) or subwords (Sennrich et al., 2016; Chitnis and DeNero, 2015) can be applied to suppress the vocabulary size, but these methods also make for longer sequences, and thus are not a direct solution to problems of computational efficiency.", "startOffset": 20, "endOffset": 69}, {"referenceID": 3, "context": ", 2015) or subwords (Sennrich et al., 2016; Chitnis and DeNero, 2015) can be applied to suppress the vocabulary size, but these methods also make for longer sequences, and thus are not a direct solution to problems of computational efficiency.", "startOffset": 20, "endOffset": 69}, {"referenceID": 9, "context": "Other methods examined included random codes, Huffman codes (Huffman, 1952) and Brown clustering (Brown et al.", "startOffset": 60, "endOffset": 75}, {"referenceID": 1, "context": "Other methods examined included random codes, Huffman codes (Huffman, 1952) and Brown clustering (Brown et al., 1992) with zero-padding to adjust code lengths, and some original allocation methods based on the word2vec embeddings (Mikolov et al.", "startOffset": 97, "endOffset": 117}, {"referenceID": 20, "context": ", 1992) with zero-padding to adjust code lengths, and some original allocation methods based on the word2vec embeddings (Mikolov et al., 2013).", "startOffset": 120, "endOffset": 142}, {"referenceID": 22, "context": "On a different note, the binary code prediction model proposed in this study shares some ideas with the hierarchical softmax (Morin and Bengio, 2005) approach.", "startOffset": 125, "endOffset": 149}, {"referenceID": 33, "context": "According to the Zipf\u2019s law (Zipf, 1949), the distribution of word appearances in an actual corpus is biased to a small subset of the vocabulary.", "startOffset": 28, "endOffset": 40}, {"referenceID": 2, "context": "As a result, we can control the actual computation for the hybrid model to be much smaller than the standard softmax complexity O(HV ), The idea of separated prediction of frequent words and rare words comes from the differentiated softmax (Chen et al., 2016) approach.", "startOffset": 240, "endOffset": 259}, {"referenceID": 28, "context": "This ability to be robust to errors is a central idea behind error-correcting codes (Shannon, 1948).", "startOffset": 84, "endOffset": 99}, {"referenceID": 8, "context": "In general, an error-correcting code has the ability to correct up to \u230a(d\u22121)/2\u230b bit errors when all centroids differ d bits from each other (Golay, 1949).", "startOffset": 140, "endOffset": 153}, {"referenceID": 4, "context": "Errorcorrecting codes have been examined in some previous work on multi-class classification tasks, and have reported advantages from the raw classification (Dietterich and Bakiri, 1995; Klautau et al., 2003; Liu, 2006; Kouzani and Nasireding, 2009; Kouzani, 2010; Ferng and Lin, 2011, 2013).", "startOffset": 157, "endOffset": 291}, {"referenceID": 11, "context": "Errorcorrecting codes have been examined in some previous work on multi-class classification tasks, and have reported advantages from the raw classification (Dietterich and Bakiri, 1995; Klautau et al., 2003; Liu, 2006; Kouzani and Nasireding, 2009; Kouzani, 2010; Ferng and Lin, 2011, 2013).", "startOffset": 157, "endOffset": 291}, {"referenceID": 17, "context": "Errorcorrecting codes have been examined in some previous work on multi-class classification tasks, and have reported advantages from the raw classification (Dietterich and Bakiri, 1995; Klautau et al., 2003; Liu, 2006; Kouzani and Nasireding, 2009; Kouzani, 2010; Ferng and Lin, 2011, 2013).", "startOffset": 157, "endOffset": 291}, {"referenceID": 14, "context": "Errorcorrecting codes have been examined in some previous work on multi-class classification tasks, and have reported advantages from the raw classification (Dietterich and Bakiri, 1995; Klautau et al., 2003; Liu, 2006; Kouzani and Nasireding, 2009; Kouzani, 2010; Ferng and Lin, 2011, 2013).", "startOffset": 157, "endOffset": 291}, {"referenceID": 13, "context": "Errorcorrecting codes have been examined in some previous work on multi-class classification tasks, and have reported advantages from the raw classification (Dietterich and Bakiri, 1995; Klautau et al., 2003; Liu, 2006; Kouzani and Nasireding, 2009; Kouzani, 2010; Ferng and Lin, 2011, 2013).", "startOffset": 157, "endOffset": 291}, {"referenceID": 32, "context": "In this study, we applied convolutional codes (Viterbi, 1967) to convert between original and redundant bits.", "startOffset": 46, "endOffset": 61}, {"referenceID": 32, "context": "This method is based on the Viterbi algorithm (Viterbi, 1967) and estimates original bits", "startOffset": 46, "endOffset": 61}, {"referenceID": 23, "context": "We examined the performance of the proposed methods on two English-Japanese bidirectional translation tasks which have different translation difficulties: ASPEC (Nakazawa et al., 2016) and BTEC (Takezawa, 1999).", "startOffset": 161, "endOffset": 184}, {"referenceID": 31, "context": ", 2016) and BTEC (Takezawa, 1999).", "startOffset": 17, "endOffset": 33}, {"referenceID": 25, "context": ", 2007) and KyTea (Neubig et al., 2011) for English/Japanese tokenizations respectively, applied lowercase.", "startOffset": 18, "endOffset": 39}, {"referenceID": 18, "context": "(2014), unidirectional decoder with the same style of (Luong et al., 2015), and the concat global attention model also proposed in Luong et al.", "startOffset": 54, "endOffset": 74}, {"referenceID": 0, "context": "We used a bidirectional RNN-based encoder applied in Bahdanau et al. (2014), unidirectional decoder with the same style of (Luong et al.", "startOffset": 53, "endOffset": 76}, {"referenceID": 0, "context": "We used a bidirectional RNN-based encoder applied in Bahdanau et al. (2014), unidirectional decoder with the same style of (Luong et al., 2015), and the concat global attention model also proposed in Luong et al. (2015). Each recurrent unit is constructed using a 1-layer LSTM (input/forget/output gates and non-", "startOffset": 53, "endOffset": 220}, {"referenceID": 7, "context": "peepholes) (Gers et al., 2000) with 30% dropout (Srivastava et al.", "startOffset": 11, "endOffset": 30}, {"referenceID": 29, "context": ", 2000) with 30% dropout (Srivastava et al., 2014) for the input/output vec-", "startOffset": 25, "endOffset": 50}, {"referenceID": 10, "context": "We used the Adam optimizer (Kingma and Ba, 2014) with fixed hyperparameters \u03b1 = 0.", "startOffset": 27, "endOffset": 48}, {"referenceID": 26, "context": "For evaluating the quality of each model, we calculated case-insensitive BLEU (Papineni et al., 2002) every 1000 mini-batches.", "startOffset": 78, "endOffset": 101}], "year": 2017, "abstractText": "In this paper, we propose a new method for calculating the output layer in neural machine translation systems. The method is based on predicting a binary code for each word and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case. In addition, we also introduce two advanced approaches to improve the robustness of the proposed model: using error-correcting codes and combining softmax and binary codes. Experiments on two English \u2194 Japanese bidirectional translation tasks show proposed models achieve BLEU scores that approach the softmax, while reducing memory usage to the order of less than 1/10 and improving decoding speed on CPUs by x5 to x10.", "creator": "LaTeX with hyperref package"}}}