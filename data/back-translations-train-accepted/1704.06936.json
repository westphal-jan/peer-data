{"id": "1704.06936", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2017", "title": "A* CCG Parsing with a Supertag and Dependency Factored Model", "abstract": "We propose a new A* CCG parsing model in which the probability of a tree is decomposed into factors of CCG categories and its syntactic dependencies both defined on bi-directional LSTMs. Our factored model allows the precomputation of all probabilities and runs very efficiently, while modeling sentence structures explicitly via dependencies. Our model achieves the state-of-the-art results on English and Japanese CCG parsing.", "histories": [["v1", "Sun, 23 Apr 2017 15:16:53 GMT  (833kb)", "http://arxiv.org/abs/1704.06936v1", "long paper (11 pages) accepted to ACL 2017"]], "COMMENTS": "long paper (11 pages) accepted to ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["masashi yoshikawa", "hiroshi noji", "yuji matsumoto"], "accepted": true, "id": "1704.06936"}, "pdf": {"name": "1704.06936.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["}@is.naist.jp"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.06 936v 1 [cs.C L] 23 Apr 201 7in which the probability of a tree is broken down into factors of CCG categories and its syntactic dependencies, both of which are defined on bidirectional LSTMs. Our factor model enables the prediction of all probabilities and runs very efficiently, while sentence structures are explicitly modelled on dependencies. Our model achieves the state-of-the-art results using English and Japanese CCG parsing.1"}, {"heading": "1 Introduction", "text": "In fact, it is so that it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, and in which it is about a way, and in which it is about a way, in which it is about a way, in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about which it is about a way and in which it is about a way and in which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about which it is about which it is about which it is about a way and in which it is about which it is about which it is about which it is about which it is about which it is about which it is about a way and in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which"}, {"heading": "2 Background", "text": "Our work is based on A * CCG analyses (Section 2.1), which we extend in Section 3 with a head prediction model for bi-LSTMs (Section 2.2)."}, {"heading": "2.1 Supertag-factored A* CCG Parsing", "text": "This year it has come to the point where it will be able to retaliate, \"he said in an interview with the\" Welt am Sonntag. \""}, {"heading": "2.2 Bi-LSTM Dependency Parsing", "text": "For dependency modeling, we borrow the idea from the most recent graph-based analysis of neural dependencies (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016), where each dependency arch is evaluated directly on the results of Bi-LSTMs. Although the model is top notch, Bi-LSTMs enable conditioning of the entire set and result in state-of-the-art performance. Note that this mechanism is similar to modeling the supertag distribution discussed above, since for each word the head selection distribution is unigrammable and can be pre-calculated. As we will see, our common model remains locally factored and A * searchable. To calculate the score, we use an advanced bilinear transformation of Dozat and Manning (2016), which also models the prior nature of each token they call biaffins."}, {"heading": "3 Proposed Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 A* parsing with Supertag and Dependency Factored Model", "text": "We define a CCG tree y for a set x = < xi,., xN > as a triplet of a list of CCG categories c = < c1,., cN >, dependencies h = < h1,.., hN >, and the derivative where hi is the header index of xi. (2) The added term pdep is an unigram distribution of the head choice.A *, search is still tractable under this model.The search problem is changed as: y = arg max y y y (x) (1, N] logPtag (ci | 3) logPdep is an unigrammed distribution of the header choice.A *, search is still tractable under this model.A The search word is changed as: y = arg max y y y y y (x) (i) (i) (i).)"}, {"heading": "3.2 Network Architecture", "text": "After Lewis et al. (2016) and Dozat and Manning (2016), we model Ptag and Pdep with biLSTMs to use the entire set to capture the long-range phenomena. See Figure 3 for the entire network architecture in which Ptag and Pdep share the common bi-LSTM hidden vectors. First, we match each word xi to its hidden vector ri with bi-LSTMs. Input into the LSTMs are word embeddings, which we describe in Section 6. We add special start and end tags to each set with the transferable parameters according to Lewis et al. (2016). For Pdep, we use the biaffine transformation in Dozat and Manning (2016): g dep i = MLP dep Kind (ri), g dep hi = MLP Dephead (rhi), g dep hi = MLP Dephead (rhi), Pdep (x) Dephi, Dep (depi), and Wexp (depi) depi depp (depi) where most of these can be used."}, {"heading": "4 CCG to Dependency Conversion", "text": "In fact, the fact is that most of us are able to play by the rules that they have given themselves, and that they are able to play by the rules that they have given themselves."}, {"heading": "5 Tri-training", "text": "We extend the existing tri-training method to our models and apply it to our English partners. Tri-training is one of the semi-monitored methods in which the outputs of two parsers are intersected with unmarked data to generate (silver) new training data. This method is successfully applied to dependency analysis (Wei\u00df et al., 2015) and CCG supertagging (Lewis et al., 2016). We simply combine the two previous approaches. Lewis et al. (2016) obtain their silver data with the high-quality supertags. As they make these data publicly available 5, we obtain our silver data by assigning dependency5 https: / / github.com / uwnlp / taggerflowstructures on top of them.6We train two very different dependency savers from the training data provided by CCGbank Section 02-21. These training data differ from our dependency conversion strategies (section 4)."}, {"heading": "6 Experiments", "text": "We conduct experiments at English and Japanese CCGbanks."}, {"heading": "6.1 English Experimental Settings", "text": "We follow the standard data splitters and use Sections 02-21 for training, Section 00 for development, and Section 23 for final evaluation; we specify labeled and unlabeled F1 of the extracted semantic dependencies of the CCG obtained using a C & C parser generator; for our models, we adopt the trimming strategies of Lewis and Steedman (2014) and allow a maximum of 50 categories per word; use a variable width bar of \u03b2 = 0.00001; and use a tag dictionary that maps common words to possibility.6 We comment on POS tags on this data using Stanford POS taggers (Toutanova et al., 2003).superday7. Unless otherwise specified, we only allow normal form parses (Eisner, 1996; Hockenmaier and Bisk, 2010), using the same subset of constants as Lewis and Steedman."}, {"heading": "6.2 Japanese Experimental Settings", "text": "We follow the standard move / dev / test splits of the Japanese CCGbank (Uematsu et al., 2013), for the baselines we use an existing Shift-reduce CCG parser implemented in an NLP tool Jigg9 (Noji and Miyao, 2016), and our implementation of the supertag factored model with bi-LSTMs.For the Japanese, we use the concatenation of word vectors computed in the Japanese Wikipedia Entity Vector10 and 100-dimensional vectors computed from randomly initialized 50-dimensional character embedding by convolution (dos Santos and Zadrozny, 2014).We do not use affix vectors as affixes in the Japanese Entity Vector10, and 100-dimensional vectors initialized from randomly initialized."}, {"heading": "6.3 English Parsing Results", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "6.4 Japanese Parsing Result", "text": "We show the results of the Japanese parsing experiment in Table 5. The simple application of Lewis12This experiment is carried out on a laptop with a 4-thread 2.0 GHz CPU.et al. (2016) (Supertag model) and shows the lowest binding value of 81.5%. We observe a performance boost with our method, especially with HEADFINAL dependencies, which exceed the baseline shift-reduce parser by 1.1 points in category assignment and 4.0 points in Bunsetsu dependencies. The deteriorated results of the simple application of the supertag model can be attributed to the fact that the structure of a Japanese sentence is still highly ambiguous in view of the supertag (Figure 5), especially for constructions in which phratic adverbial / adnominal modifiers (with the supertag S / S) are involved."}, {"heading": "7 Related Work", "text": "For Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag (1994)), there are studies that use the predicted dependency structure to improve the accuracy of HPSG analysis. Sagae et al. (2007) use dependencies to constrain the shape of the output tree. As in our method, they define for each rule (scheme) application which child becomes the head and place a soft constraint that these dependencies coincide with the output of the dependency saver. Our method differs from using the unique dependency structure alone, but rather looking for a CCG tree that is optimal in terms of dependencies and CCG supertags. Zhang et al al. (2010) we use syntactic dependencies in a different way and show that dependency-based features of Lewis are sectar."}, {"heading": "8 Conclusion", "text": "We have introduced a new A * CCG parsing method, in which the probability of a CCG tree is broken down into local factors of the CCG categories and its dependency structure. By explicitly modelling the dependency structure, we do not need deterministic heuristics to resolve attachment ambiguities, and keep the model factored locally so that all probabilities can be pre-calculated before the search begins. Our parser efficiently finds the optimal parse and achieves state-of-the-art performance in both English and Japanese parsing."}, {"heading": "Acknowledgments", "text": "We thank Mike Lewis for answering our questions and your Github repository, from which we learned many things. We also thank Yuichiro Sawai for the faster LSTM implementation. This work was partially supported by JSPSKAKENHI Grant Number 16H06981 and also by JST CREST Grant Number JPMJCR1301."}], "references": [{"title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorflow.org", "author": ["sudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "sudevan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "sudevan et al\\.", "year": 2015}, {"title": "Supertagging: An Approach to Almost Parsing", "author": ["Srinivas Bangalore", "Aravind K Joshi."], "venue": "Computational linguistics 25(2):237\u2013265.", "citeRegEx": "Bangalore and Joshi.,? 1999", "shortCiteRegEx": "Bangalore and Joshi.", "year": 1999}, {"title": "Wide\u2013 Coverage Efficient Statistical Parsing with CCG and Log-Linear Models", "author": ["Stephen Clark", "James R. Curran."], "venue": "Computational Linguistics, Volume 33, Number 4, December 2007 http://aclweb.org/anthology/J07-4004.", "citeRegEx": "Clark and Curran.,? 2007", "shortCiteRegEx": "Clark and Curran.", "year": 2007}, {"title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)", "author": ["Djork-Arn\u00e9 Clevert", "Thomas Unterthiner", "Sepp Hochreiter."], "venue": "CoRR abs/1511.07289. http://arxiv.org/abs/1511.07289.", "citeRegEx": "Clevert et al\\.,? 2015", "shortCiteRegEx": "Clevert et al\\.", "year": 2015}, {"title": "Learning Character-level Representations for Part-of-Speech Tagging", "author": ["C\u0131\u0301cero Nogueira dos Santos", "Bianca Zadrozny"], "venue": null, "citeRegEx": "Santos and Zadrozny.,? \\Q2014\\E", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "Deep Biaffine Attention for Neural Dependency Parsing", "author": ["Timothy Dozat", "Christopher D. Manning."], "venue": "CoRR abs/1611.01734. http://arxiv.org/abs/1611.01734.", "citeRegEx": "Dozat and Manning.,? 2016", "shortCiteRegEx": "Dozat and Manning.", "year": 2016}, {"title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "A. Noah Smith."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Efficient Normal-Form Parsing for Combinatory Categorial Grammar", "author": ["Jason Eisner."], "venue": "34th Annual Meeting of the Association for Computational Linguistics. http://aclweb.org/anthology/P96-1011.", "citeRegEx": "Eisner.,? 1996", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "Normal\u2013 form parsing for Combinatory Categorial Grammars with generalized composition and type-raising", "author": ["Julia Hockenmaier", "Yonatan Bisk."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010). Col-", "citeRegEx": "Hockenmaier and Bisk.,? 2010", "shortCiteRegEx": "Hockenmaier and Bisk.", "year": 2010}, {"title": "CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank", "author": ["Julia Hockenmaier", "Mark Steedman."], "venue": "Computational Linguistics 33(3):355\u2013396. http://www.aclweb.org/anthology/J07-3004.", "citeRegEx": "Hockenmaier and Steedman.,? 2007", "shortCiteRegEx": "Hockenmaier and Steedman.", "year": 2007}, {"title": "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "Transactions of the Association for Computational Linguistics 4:313\u2013327.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "A* Parsing: Fast Exact Viterbi Parse Selection", "author": ["Dan Klein", "Christopher D. Manning."], "venue": "Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Japanese Dependency Analysis using Cascaded Chunking", "author": ["Taku Kudo", "Yuji Matsumoto."], "venue": "Proceedings of the 6th Conference on Natural Language Learning, CoNLL 2002, Held in cooperation with COLING 2002, Taipei, Taiwan, 2002.", "citeRegEx": "Kudo and Matsumoto.,? 2002", "shortCiteRegEx": "Kudo and Matsumoto.", "year": 2002}, {"title": "Global Neural CCG Parsing with Optimality Guarantees", "author": ["Kenton Lee", "Mike Lewis", "Luke Zettlemoyer."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Low-Rank Tensors for Scoring Dependency Structures", "author": ["Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Lei et al\\.,? 2014", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "Joint A* CCG Parsing and Semantic Role Labelling", "author": ["Mike Lewis", "Luheng He", "Luke Zettlemoyer."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 1444\u2013", "citeRegEx": "Lewis et al\\.,? 2015", "shortCiteRegEx": "Lewis et al\\.", "year": 2015}, {"title": "LSTM CCG Parsing", "author": ["Mike Lewis", "Kenton Lee", "Luke Zettlemoyer."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Associa-", "citeRegEx": "Lewis et al\\.,? 2016", "shortCiteRegEx": "Lewis et al\\.", "year": 2016}, {"title": "A* CCG Parsing with a Supertag-factored Model", "author": ["Mike Lewis", "Mark Steedman."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, pages 990\u2013", "citeRegEx": "Lewis and Steedman.,? 2014", "shortCiteRegEx": "Lewis and Steedman.", "year": 2014}, {"title": "DyNet: The Dynamic Neural Network Toolkit", "author": ["Lingpeng Kong", "Adhiguna Kuncoro", "Gaurav Kumar", "Chaitanya Malaviya", "Paul Michel", "Yusuke Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin."], "venue": "arXiv", "citeRegEx": "Kong et al\\.,? 2017", "shortCiteRegEx": "Kong et al\\.", "year": 2017}, {"title": "Jigg: A Framework for an Easy Natural Language Processing Pipeline", "author": ["Hiroshi Noji", "Yusuke Miyao."], "venue": "Proceedings of ACL2016 System Demonstrations. Association for Computational Linguistics, pages 103\u2013108.", "citeRegEx": "Noji and Miyao.,? 2016", "shortCiteRegEx": "Noji and Miyao.", "year": 2016}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u2013 1543. http://www.aclweb.org/anthology/D14-1162.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Head-driven phrase structure grammar", "author": ["Carl Pollard", "Ivan A Sag."], "venue": "University of Chicago Press.", "citeRegEx": "Pollard and Sag.,? 1994", "shortCiteRegEx": "Pollard and Sag.", "year": 1994}, {"title": "HPSG Parsing with Shallow Dependency Constraints", "author": ["Kenji Sagae", "Yusuke Miyao", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Linguistics,", "citeRegEx": "Sagae et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sagae et al\\.", "year": 2007}, {"title": "The Syntactic Process", "author": ["Mark Steedman."], "venue": "The MIT Press.", "citeRegEx": "Steedman.,? 2000", "shortCiteRegEx": "Steedman.", "year": 2000}, {"title": "Chainer: a Next-Generation Open Source Framework for Deep Learning", "author": ["Seiya Tokui", "Kenta Oono", "Shohei Hido", "Justin Clayton."], "venue": "Proceedings of Workshop on Machine Learning Systems (LearningSys) in", "citeRegEx": "Tokui et al\\.,? 2015", "shortCiteRegEx": "Tokui et al\\.", "year": 2015}, {"title": "Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer."], "venue": "Proceedings of the 2003 Human Language Technology Confer-", "citeRegEx": "Toutanova et al\\.,? 2003", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Word Representations: A Simple and General Method for Semi-Supervised Learning", "author": ["Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Associa-", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Japanese Dependency Structure Analysis Based on Maximum Entropy Models", "author": ["Kiyotaka Uchimoto", "Satoshi Sekine", "Hitoshi Isahara."], "venue": "Ninth Conference of the European Chapter of the Association for Computational Linguistics.", "citeRegEx": "Uchimoto et al\\.,? 1999", "shortCiteRegEx": "Uchimoto et al\\.", "year": 1999}, {"title": "Integrating Multiple Dependency Corpora for Inducing Wide-coverage Japanese CCG Resources", "author": ["Sumire Uematsu", "Takuya Matsuzaki", "Hiroki Hanaoka", "Yusuke Miyao", "Hideki Mima."], "venue": "Proceedings of the 51st Annual Meeting of the", "citeRegEx": "Uematsu et al\\.,? 2013", "shortCiteRegEx": "Uematsu et al\\.", "year": 2013}, {"title": "Supertagging With LSTMs", "author": ["Ashish Vaswani", "Yonatan Bisk", "Kenji Sagae", "Ryan Musa."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Asso-", "citeRegEx": "Vaswani et al\\.,? 2016", "shortCiteRegEx": "Vaswani et al\\.", "year": 2016}, {"title": "Structured Training for Neural Network Transition-Based Parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th In-", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Shift-Reduce CCG Parsing with a Dependency Model", "author": ["Wenduan Xu", "Stephen Clark", "Yue Zhang."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "A Simple Approach for HPSG Supertagging Using Dependency Information", "author": ["Yao-zhong Zhang", "Takuya Matsuzaki", "Jun\u2019ichi Tsujii"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "Supertagging in lexicalized grammar parsing is known as almost parsing (Bangalore and Joshi, 1999), in that each supertag is syntactically informative and most ambiguities are resolved once a correct supertag is assigned to every word.", "startOffset": 71, "endOffset": 98}, {"referenceID": 17, "context": "Recently this property is effectively exploited in A* Combinatory Categorial Grammar (CCG; Steedman (2000)) parsing (Lewis and Steedman, 2014; Lewis et al., 2016), in which the probability of a CCG tree y on a sentence x of length N is the product of the probabilities of supertags (categories) ci (locally factored model):", "startOffset": 116, "endOffset": 162}, {"referenceID": 16, "context": "Recently this property is effectively exploited in A* Combinatory Categorial Grammar (CCG; Steedman (2000)) parsing (Lewis and Steedman, 2014; Lewis et al., 2016), in which the probability of a CCG tree y on a sentence x of length N is the product of the probabilities of supertags (categories) ci (locally factored model):", "startOffset": 116, "endOffset": 162}, {"referenceID": 1, "context": "Supertagging in lexicalized grammar parsing is known as almost parsing (Bangalore and Joshi, 1999), in that each supertag is syntactically informative and most ambiguities are resolved once a correct supertag is assigned to every word. Recently this property is effectively exploited in A* Combinatory Categorial Grammar (CCG; Steedman (2000)) parsing (Lewis and Steedman, 2014; Lewis et al.", "startOffset": 72, "endOffset": 343}, {"referenceID": 15, "context": "Lewis et al.\u2019s approach to this problem is resorting to some deterministic rule. For example, Lewis et al. (2016) employ the attach low heuristics, which is motivated by the right-branching tendency of English, and always prioritizes (b) for this type of ambiguity.", "startOffset": 0, "endOffset": 114}, {"referenceID": 10, "context": "1 with a strong unigram model, for which we utilize the scoring model in the recent successful graph-based dependency parsing on LSTMs (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016).", "startOffset": 135, "endOffset": 192}, {"referenceID": 5, "context": "1 with a strong unigram model, for which we utilize the scoring model in the recent successful graph-based dependency parsing on LSTMs (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016).", "startOffset": 135, "endOffset": 192}, {"referenceID": 14, "context": "LSTM) architecture of Lewis et al. (2016) predicting the supertag of a word to predict the head of it at the same time with a bilinear transformation.", "startOffset": 22, "endOffset": 42}, {"referenceID": 13, "context": "The importance of modeling structures beyond supertags is demonstrated in the performance gain in Lee et al. (2016), which adds a recursive component to the model of Eq.", "startOffset": 98, "endOffset": 116}, {"referenceID": 13, "context": "The importance of modeling structures beyond supertags is demonstrated in the performance gain in Lee et al. (2016), which adds a recursive component to the model of Eq. 1. Unfortunately, this formulation loses the efficiency of the original one since it needs to compute a recursive neural network every time it searches for a new node. Our model does not resort to the recursive networks while modeling tree structures via dependencies. We also extend the tri-training method of Lewis et al. (2016) to learn our model with dependencies from unlabeled data.", "startOffset": 98, "endOffset": 501}, {"referenceID": 13, "context": "The importance of modeling structures beyond supertags is demonstrated in the performance gain in Lee et al. (2016), which adds a recursive component to the model of Eq. 1. Unfortunately, this formulation loses the efficiency of the original one since it needs to compute a recursive neural network every time it searches for a new node. Our model does not resort to the recursive networks while modeling tree structures via dependencies. We also extend the tri-training method of Lewis et al. (2016) to learn our model with dependencies from unlabeled data. On English CCGbank test data, our model with this technique achieves 88.8% and 94.0% in terms of labeled and unlabeled F1, which mark the best scores so far. Besides English, we provide experiments on Japanese CCG parsing. Japanese employs freer word order dominated by the case markers and a deterministic rule such as the attach low method may not work well. We show that this is actually the case; our method outperforms the simple application of Lewis et al. (2016) in a large margin, 10.", "startOffset": 98, "endOffset": 1029}, {"referenceID": 17, "context": "Lewis and Steedman (2014) utilize this characteristics of the grammar.", "startOffset": 0, "endOffset": 26}, {"referenceID": 15, "context": "Scoring model For modeling Ptag , Lewis and Steedman (2014) use a log-linear model with features from a fixed window context.", "startOffset": 34, "endOffset": 60}, {"referenceID": 15, "context": "Lewis et al. (2016) extend this with bi-LSTMs, which encode the complete sentence and capture the long range syntactic information.", "startOffset": 0, "endOffset": 20}, {"referenceID": 17, "context": "In Lewis and Steedman (2014), they prioritize the parse with longer dependencies, which they judge with a conversion rule from a CCG", "startOffset": 3, "endOffset": 29}, {"referenceID": 15, "context": "Lewis et al. (2016) employ another heuristics prioritizing low attachments of constituencies, but inevitably these heuristics cannot be flawless in any situations.", "startOffset": 0, "endOffset": 20}, {"referenceID": 10, "context": "For modeling dependencies, we borrow the idea from the recent graph-based neural dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) in which each dependency arc is scored directly on the outputs of bi-LSTMs.", "startOffset": 100, "endOffset": 157}, {"referenceID": 5, "context": "For modeling dependencies, we borrow the idea from the recent graph-based neural dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) in which each dependency arc is scored directly on the outputs of bi-LSTMs.", "startOffset": 100, "endOffset": 157}, {"referenceID": 5, "context": "For modeling dependencies, we borrow the idea from the recent graph-based neural dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) in which each dependency arc is scored directly on the outputs of bi-LSTMs. Though the model is first-order, bi-LSTMs enable conditioning on the entire sentence and lead to the state-of-the-art performance. Note that this mechanism is similar to modeling of the supertag distribution discussed above, in that for each word the distribution of the head choice is unigram and can be precomputed. As we will see this keeps our joint model still locally factored and A* search tractable. For score calculation, we use an extended bilinear transformation by Dozat and Manning (2016) that models the prior headness of each token as well, which they call biaffine.", "startOffset": 133, "endOffset": 736}, {"referenceID": 14, "context": "Following Lewis et al. (2016) and Dozat and Manning (2016), we model Ptag and Pdep using biLSTMs for exploiting the entire sentence to capture the long range phenomena.", "startOffset": 10, "endOffset": 30}, {"referenceID": 5, "context": "(2016) and Dozat and Manning (2016), we model Ptag and Pdep using biLSTMs for exploiting the entire sentence to capture the long range phenomena.", "startOffset": 11, "endOffset": 36}, {"referenceID": 5, "context": "(2016) and Dozat and Manning (2016), we model Ptag and Pdep using biLSTMs for exploiting the entire sentence to capture the long range phenomena. See Figure 3 for the overall network architecture, where Ptag and Pdep share the common bi-LSTM hidden vectors. First we map every word xi to their hidden vector ri with bi-LSTMs. The input to the LSTMs is word embeddings, which we describe in Section 6. We add special start and end tokens to each sentence with the trainable parameters following Lewis et al. (2016). For Pdep, we use the biaffine transformation in Dozat and Manning (2016):", "startOffset": 11, "endOffset": 514}, {"referenceID": 5, "context": "(2016) and Dozat and Manning (2016), we model Ptag and Pdep using biLSTMs for exploiting the entire sentence to capture the long range phenomena. See Figure 3 for the overall network architecture, where Ptag and Pdep share the common bi-LSTM hidden vectors. First we map every word xi to their hidden vector ri with bi-LSTMs. The input to the LSTMs is word embeddings, which we describe in Section 6. We add special start and end tokens to each sentence with the trainable parameters following Lewis et al. (2016). For Pdep, we use the biaffine transformation in Dozat and Manning (2016):", "startOffset": 11, "endOffset": 588}, {"referenceID": 15, "context": "Though Lewis et al. (2016) simply use an MLP for mapping ri to Ptag , we additionally utilize the hidden vector of the most probable head hi = argmaxhi Pdep(h \u2032 i|x), and apply ri and rhi to a bilinear function:", "startOffset": 7, "endOffset": 27}, {"referenceID": 5, "context": "2 This is inspired by the formulation of label prediction in Dozat and Manning (2016), which performs the best among other settings that remove or reverse the dependence between the head model and the supertag model.", "startOffset": 61, "endOffset": 86}, {"referenceID": 17, "context": "Lewis and Steedman (2014) describe one way to extract dependencies from a CCG tree (LEWISRULE).", "startOffset": 0, "endOffset": 26}, {"referenceID": 17, "context": "LEWISRULE This is the same as the conversion rule in Lewis and Steedman (2014). As shown in Figure 4c the output looks a familiar English dependency tree.", "startOffset": 53, "endOffset": 79}, {"referenceID": 9, "context": "One issue when applying this method for obtaining the training data is that due to the mismatch between the rule set of our CCG parser, for which we follow Lewis and Steedman (2014), and the grammar in English CCGbank (Hockenmaier and Steedman, 2007) we cannot extract dependencies from some of annotated CCG trees.", "startOffset": 218, "endOffset": 250}, {"referenceID": 16, "context": "One issue when applying this method for obtaining the training data is that due to the mismatch between the rule set of our CCG parser, for which we follow Lewis and Steedman (2014), and the grammar in English CCGbank (Hockenmaier and Steedman, 2007) we cannot extract dependencies from some of annotated CCG trees.", "startOffset": 156, "endOffset": 182}, {"referenceID": 27, "context": "Japanese dependency parsing (Uchimoto et al., 1999; Kudo and Matsumoto, 2002) has exploited this property explicitly by only allowing left-toright dependency arcs.", "startOffset": 28, "endOffset": 77}, {"referenceID": 12, "context": "Japanese dependency parsing (Uchimoto et al., 1999; Kudo and Matsumoto, 2002) has exploited this property explicitly by only allowing left-toright dependency arcs.", "startOffset": 28, "endOffset": 77}, {"referenceID": 17, "context": "4 For example, the combinatory rules in Lewis and Steedman (2014) do not contain Nconj \u2192 N N in CCGbank.", "startOffset": 40, "endOffset": 66}, {"referenceID": 30, "context": "This method is successfully applied to dependency parsing (Weiss et al., 2015) and CCG supertagging (Lewis et al.", "startOffset": 58, "endOffset": 78}, {"referenceID": 16, "context": ", 2015) and CCG supertagging (Lewis et al., 2016).", "startOffset": 29, "endOffset": 49}, {"referenceID": 15, "context": "Lewis et al. (2016) obtain their silver data annotated with the high quality supertags.", "startOffset": 0, "endOffset": 20}, {"referenceID": 14, "context": "The first parser is graph-based RBGParser (Lei et al., 2014) with the default settings except that we train an unlabeled parser and use word embeddings of Turian et al.", "startOffset": 42, "endOffset": 60}, {"referenceID": 6, "context": "The second parser is transition-based lstm-parser (Dyer et al., 2015) with the default parameters.", "startOffset": 50, "endOffset": 69}, {"referenceID": 13, "context": "The first parser is graph-based RBGParser (Lei et al., 2014) with the default settings except that we train an unlabeled parser and use word embeddings of Turian et al. (2010). The second parser is transition-based lstm-parser (Dyer et al.", "startOffset": 43, "endOffset": 176}, {"referenceID": 15, "context": "Following Lewis et al. (2016), we include 15 copies of CCGbank training set when using these silver data.", "startOffset": 10, "endOffset": 30}, {"referenceID": 17, "context": "For our models, we adopt the pruning strategies in Lewis and Steedman (2014) and allow at most 50 categories per word, use a variable-width beam with \u03b2 = 0.", "startOffset": 51, "endOffset": 77}, {"referenceID": 25, "context": "We annotate POS tags on this data using Stanford POS tagger (Toutanova et al., 2003).", "startOffset": 60, "endOffset": 84}, {"referenceID": 7, "context": "Unless otherwise stated, we only allow normal form parses (Eisner, 1996; Hockenmaier and Bisk, 2010), choosing the same subset of the constraints as Lewis and Steedman (2014).", "startOffset": 58, "endOffset": 100}, {"referenceID": 8, "context": "Unless otherwise stated, we only allow normal form parses (Eisner, 1996; Hockenmaier and Bisk, 2010), choosing the same subset of the constraints as Lewis and Steedman (2014).", "startOffset": 58, "endOffset": 100}, {"referenceID": 7, "context": "Unless otherwise stated, we only allow normal form parses (Eisner, 1996; Hockenmaier and Bisk, 2010), choosing the same subset of the constraints as Lewis and Steedman (2014).", "startOffset": 59, "endOffset": 175}, {"referenceID": 20, "context": "We use as word representation the concatenation of word vectors initialized to GloVe (Pennington et al., 2014), and randomly initialized prefix and suffix vectors of the length 1 to 4, which is inspired by Lewis et al.", "startOffset": 85, "endOffset": 110}, {"referenceID": 15, "context": ", 2014), and randomly initialized prefix and suffix vectors of the length 1 to 4, which is inspired by Lewis et al. (2016). All affixes appearing less than two times in the training data are mapped to \u201cUNK\u201d.", "startOffset": 103, "endOffset": 123}, {"referenceID": 3, "context": "Other model configurations are: 4-layer biLSTMs with left and right 300-dimensional LSTMs, 1-layer 100-dimensional MLPs with ELU non-linearity (Clevert et al., 2015) for all MLP dep child, MLP dep head, MLP tag child and MLP tag head, and the Adam optimizer with \u03b21 = 0.", "startOffset": 143, "endOffset": 165}, {"referenceID": 5, "context": "75 for every 2,500 iteration starting from 2e\u22123, which is shown to be effective for training the biaffine parser (Dozat and Manning, 2016).", "startOffset": 113, "endOffset": 138}, {"referenceID": 28, "context": "We follow the default train/dev/test splits of Japanese CCGbank (Uematsu et al., 2013).", "startOffset": 64, "endOffset": 86}, {"referenceID": 19, "context": "For the baselines, we use an existing shift-reduce CCG parser implemented in an NLP tool Jigg (Noji and Miyao, 2016), and our implementation of the supertag-factored model using bi-LSTMs.", "startOffset": 94, "endOffset": 116}, {"referenceID": 2, "context": "C&C (Clark and Curran, 2007) 85.", "startOffset": 4, "endOffset": 28}, {"referenceID": 29, "context": "7 w/ LSTMs (Vaswani et al., 2016) 88.", "startOffset": 11, "endOffset": 33}, {"referenceID": 16, "context": "EasySRL (Lewis et al., 2016) 87.", "startOffset": 8, "endOffset": 28}, {"referenceID": 16, "context": "EasySRL (Lewis et al., 2016) 88.", "startOffset": 8, "endOffset": 28}, {"referenceID": 13, "context": "neuralccg (Lee et al., 2016) 88.", "startOffset": 10, "endOffset": 28}, {"referenceID": 29, "context": "ment, our parser shows the better result than all the baseline parsers except C&C with an LSTM supertagger (Vaswani et al., 2016).", "startOffset": 107, "endOffset": 129}, {"referenceID": 13, "context": "0% unlabeled F1, outperforming the current state-of-theart neuralccg (Lee et al., 2016) that uses recursive neural networks by 0.", "startOffset": 69, "endOffset": 87}, {"referenceID": 24, "context": "There are also many implementation differences in our parser (C++A* parser with neural network model implemented with Chainer (Tokui et al., 2015)) and neuralccg (Java parser with C++ TensorFlow (Abadi et al.", "startOffset": 126, "endOffset": 146}, {"referenceID": 19, "context": "Noji and Miyao (2016) 93.", "startOffset": 0, "endOffset": 22}, {"referenceID": 21, "context": "For Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag (1994)), there are studies to use the predicted dependency structure to improve HPSG parsing accuracy.", "startOffset": 48, "endOffset": 71}, {"referenceID": 21, "context": "For Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag (1994)), there are studies to use the predicted dependency structure to improve HPSG parsing accuracy. Sagae et al. (2007) use dependencies to constrain the form of the output tree.", "startOffset": 48, "endOffset": 187}, {"referenceID": 32, "context": "Zhang et al. (2010) use the syntactic dependencies in a different way, and show that dependency-based features are useful for predicting HPSG supertags.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "In the CCG parsing literature, some work optimizes a dependency model, instead of supertags or a derivation (Clark and Curran, 2007; Xu et al., 2014).", "startOffset": 108, "endOffset": 149}, {"referenceID": 31, "context": "In the CCG parsing literature, some work optimizes a dependency model, instead of supertags or a derivation (Clark and Curran, 2007; Xu et al., 2014).", "startOffset": 108, "endOffset": 149}, {"referenceID": 2, "context": "In the CCG parsing literature, some work optimizes a dependency model, instead of supertags or a derivation (Clark and Curran, 2007; Xu et al., 2014). This approach is reasonable given that the objective matches the evaluation metric. Instead of modeling dependencies alone, our method finds a CCG derivation that has a higher dependency score. Lewis et al. (2015) present a joint model of CCG parsing and semantic role labeling (SRL), which is closely related to our approach.", "startOffset": 109, "endOffset": 365}], "year": 2017, "abstractText": "We propose a new A* CCG parsing model in which the probability of a tree is decomposed into factors of CCG categories and its syntactic dependencies both defined on bi-directional LSTMs. Our factored model allows the precomputation of all probabilities and runs very efficiently, while modeling sentence structures explicitly via dependencies. Our model achieves the stateof-the-art results on English and Japanese CCG parsing.", "creator": "LaTeX with hyperref package"}}}