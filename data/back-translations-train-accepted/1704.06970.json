{"id": "1704.06970", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2017", "title": "Differentiable Scheduled Sampling for Credit Assignment", "abstract": "We demonstrate that a continuous relaxation of the argmax operation can be used to create a differentiable approximation to greedy decoding for sequence-to-sequence (seq2seq) models. By incorporating this approximation into the scheduled sampling training procedure (Bengio et al., 2015)--a well-known technique for correcting exposure bias--we introduce a new training objective that is continuous and differentiable everywhere and that can provide informative gradients near points where previous decoding decisions change their value. In addition, by using a related approximation, we demonstrate a similar approach to sampled-based training. Finally, we show that our approach outperforms cross-entropy training and scheduled sampling procedures in two sequence prediction tasks: named entity recognition and machine translation.", "histories": [["v1", "Sun, 23 Apr 2017 20:05:36 GMT  (533kb,D)", "http://arxiv.org/abs/1704.06970v1", "Accepted at ACL2017 (this http URL)"]], "COMMENTS": "Accepted at ACL2017 (this http URL)", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["kartik goyal", "chris dyer", "taylor berg-kirkpatrick"], "accepted": true, "id": "1704.06970"}, "pdf": {"name": "1704.06970.pdf", "metadata": {"source": "CRF", "title": "Differentiable Scheduled Sampling for Credit Assignment", "authors": ["Kartik Goyal", "Chris Dyer"], "emails": ["kartikgo@cs.cmu.edu", "cdyer@google.com", "tberg@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to move, to move, to move, to fight, to fight, to move, to move, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "2 Discontinuity in Scheduled Sampling", "text": "While the step-by-step model (Bengio et al., 2015) is an effective way to correct exposure problems, it cannot distinguish between cascading errors that can lead to a sequence of wrong decisions and local errors that have rather harmless effects. Specifically, the planned sampling focuses on learning optimal behavior in the current step given the fixed decoding decision of the previous step. If a previous bad decision is largely responsible for the current error, the training process has difficulty adjusting the parameters accordingly. The following machine translation example underscores this lending issue: Ref: The cat purrs. Pred: The dog barks, the model prefers the word \"bellts\" after incorrectly predicting \"dog\" at step 2. \"To correct this error, the planned sampling process would lower the score of the\" purrs \"in step 3 due to the fact that the model is predicted (wrong)\" step dog in step 2, which is not the ideal learning behavior. \""}, {"heading": "3 Credit Assignment via Relaxation", "text": "In this section we explain in detail the continuous relaxation of greedy decoding with which we build a fully continuous training target, as well as a related approach to sample-based training.1 For simplicity, the \"always sample\" variant of the planned sampling is described (Bengio et al., 2015)."}, {"heading": "3.1 Soft Argmax", "text": "In the scheduled sampling, the embedding of the best scoring word in the previous step is passed on as input in the current step. This operation2 can be expressed as an ASIC value, where y is a word in the vocabulary, si-1 (y) is the output value of that word in the previous step, and e-i-1 is the embedding that is passed on to the next step. This process can be loosened by replacing the indicator function with a peak Softmax function with hyperparameter \u03b1 to define a soft Argmax procedure: e-i-1 = complete ye (y) \u00b7 exp (\u03b1 si-1 (y)) \u0432y-exp (\u03b1 si-1 (y)). Since the above equation approximates the true Argmax embedding, we obtain with a finite and large sum a linear combination of all words (and therefore a word strongly dominated by the continuous function)."}, {"heading": "3.2 Soft Reparametrized Sampling", "text": "Another variant of scheduled sampling is to transfer a sample embedding from the Softmax distribution to the previous step to the current step instead of the Argmax. This should allow better exploration of the search space during optimization due to the additional randomness and therefore lead to a more robust model. However, in this section we discuss and review an approach to the Gumbel repair trick that we use as a module in our sample-based decoder. This approach was proposed by Maddison et al. (2017) and Jang et al. (2016), who showed that the same soft Argmax operation introduced above can be used to reduce the variance of stochastic gradients when derived from Softmax distributions. Unlike soft Argmax, this approach is not a fully continuous approach to sampling operation, but it leads to much more informative gradients compared to naive planned sampling."}, {"heading": "3.3 Differentiable Relaxed Decoders", "text": "With the argmax relaxation described above, we have a recipe for a fully differentiable greedy decoder based on informative gradients close to change points. Our final training network for scheduled sampling with relaxed greedy decoding is shown in Figure 2. Instead of conditioning the current hidden state, on the argmax embedding from the previous step, we use the e-soft Argmax embedding, e-soft Argmax embedding, e-soft embedding, e-i-1, defined in Section 3.1. This removes the discontinuity in the original greedy sampling goal by passing on a linear combination of embedding dominated by argmax embedding to the next step. Figure 13This differs from the use of the expected softmax embedding because our approach annexes the actual sampling process, the sampling approach of the embedding process."}, {"heading": "4 Related Work", "text": "Gormley et al. (2015) \"s approach awareness training is conceptually related, but focuses on variable decoding methods. Hoang et al. (2017) also propose a continuous relaxation of decoders, but focus on the development of better inference methods. Grefenstette et al. (2015) successfully use a soft approximation of Argmax in neural stack mechanisms. Finally, Ranzato et al. (2016) experiment with a similarly motivated goal that was not fully continuous, but performed worse than standard training."}, {"heading": "5 Experimental Setup", "text": "We conduct experiments with machine translation (1 layer, 256 units) and named entity recognition (NER).Data: For MT, we use the same data set (the German-English part of the IWSLT 2014 campaign for machine translation evaluation), pre-processing and data splits as Ranzato et al. (2016).For named entity recognition, we use the CONLL 2003 shared task data (TjongKim Sang and De Meulder, 2003) for the German language and use the provided data splits. We do not pre-process the data.The output vocabulary length for MT is 32000 and 10 for NER.Implementation Details: For MT, we use a seq2seq model with a simple attention mechanism (Bahdanau et al., 2015), a bidirectional LSTM encoder (1 layer, 256 units) and a LSTM decoder (1 layer, 256 units)."}, {"heading": "6 Results", "text": "All three approaches improve over the standard seq2seq-based training. In addition, both approaches, which use continuous loosening (greedy and sample-based), show higher results than standard scheduled samples (Bengio et al., 2015). However, the best results for NER were achieved with the relaxed, greedy annealed \u03b1 decoder, which yielded a gain of + 3.1 compared to the standard seq2seq sample and a gain of + 1.5 F1 over scheduled standard sample. For MT, we achieve the best results with the relaxed sample-based decoder, which yields a gain of + 1.5 BLEU over the standard seq2seq sample and a gain of + 0.75 BLEU over scheduled sampling. We observe that the repaired sample-based method, although not fully exhaustive, unlike the soft greedy approach, may be results in good training, especially in the search for the MT during the search horizons."}, {"heading": "7 Conclusion", "text": "Our positive results suggest that lending mechanisms can be useful when added to models aimed at reducing exposure bias. Furthermore, our results suggest that continuous loosening of the Argmax operation can be used as an effective approach to hard decoding during training."}, {"heading": "Acknowledgements", "text": "We thank Graham Neubig for helpful discussions and the three anonymous reviewers for their valuable feedback."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "An actor-critic algorithm for sequence prediction", "author": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2017", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2017}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer."], "venue": "Advances in Neural Information Processing Systems. pages 1171\u20131179.", "citeRegEx": "Bengio et al\\.,? 2015", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Report on the 11th iwslt evaluation campaign, iwslt 2014", "author": ["Mauro Cettolo", "Jan Niehues", "Sebastian St\u00fcker", "Luisa Bentivogli", "Marcello Federico."], "venue": "Proceedings of the International Workshop on Spoken Language Translation, Hanoi, Vietnam.", "citeRegEx": "Cettolo et al\\.,? 2014", "shortCiteRegEx": "Cettolo et al\\.", "year": 2014}, {"title": "Search-based structured prediction", "author": ["Hal Daum\u00e9", "John Langford", "Daniel Marcu."], "venue": "Machine learning 75(3):297\u2013325.", "citeRegEx": "Daum\u00e9 et al\\.,? 2009", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2009}, {"title": "Learning as search optimization: Approximate large margin methods for structured prediction", "author": ["Hal Daum\u00e9 III", "Daniel Marcu."], "venue": "Proceedings of the 22nd international conference on Machine learning. ACM, pages 169\u2013176.", "citeRegEx": "III and Marcu.,? 2005", "shortCiteRegEx": "III and Marcu.", "year": 2005}, {"title": "Approximation-aware dependency parsing by belief propagation", "author": ["Matthew R. Gormley", "Mark Dredze", "Jason Eisner."], "venue": "Transactions of the Association for Computational Linguistics (TACL) .", "citeRegEx": "Gormley et al\\.,? 2015", "shortCiteRegEx": "Gormley et al\\.", "year": 2015}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems. pages 1828\u20131836.", "citeRegEx": "Grefenstette et al\\.,? 2015", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Decoding as continuous optimization in neural machine translation", "author": ["Cong Duy Vu Hoang", "Gholamreza Haffari", "Trevor Cohn."], "venue": "arXiv preprint arXiv:1701.02854 .", "citeRegEx": "Hoang et al\\.,? 2017", "shortCiteRegEx": "Hoang et al\\.", "year": 2017}, {"title": "Categorical reparameterization with gumbel-softmax", "author": ["Eric Jang", "Shixiang Gu", "Ben Poole."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Jang et al\\.,? 2016", "shortCiteRegEx": "Jang et al\\.", "year": 2016}, {"title": "The concrete distribution: A continuous relaxation of discrete random variables", "author": ["Chris J Maddison", "Andriy Mnih", "Yee Whye Teh."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Maddison et al\\.,? 2017", "shortCiteRegEx": "Maddison et al\\.", "year": 2017}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["St\u00e9phane Ross", "Geoffrey J Gordon", "Drew Bagnell."], "venue": "AISTATS. volume 1, page 6.", "citeRegEx": "Ross et al\\.,? 2011", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "AAAI\u201916 Proceedings of the Thirtieth AAAI Conference on Artifi-", "citeRegEx": "Serban et al\\.,? 2015", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "author": ["Erik F Tjong Kim Sang", "Fien De Meulder."], "venue": "Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4.", "citeRegEx": "Sang and Meulder.,? 2003", "shortCiteRegEx": "Sang and Meulder.", "year": 2003}, {"title": "Sequence-to-sequence learning as beam-search optimization", "author": ["Sam Wiseman", "Alexander M Rush."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Wiseman and Rush.,? 2016", "shortCiteRegEx": "Wiseman and Rush.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio."], "venue": "ICML. volume 14, pages 77\u201381.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "By incorporating this approximation into the scheduled sampling training procedure (Bengio et al., 2015)\u2013a well-known technique for correcting exposure bias\u2013we introduce a new training objective that is continuous and differentiable everywhere and that can provide informative gradients near points where previous decoding decisions change their value.", "startOffset": 83, "endOffset": 104}, {"referenceID": 16, "context": "Sequence-to-Sequence (seq2seq) models have demonstrated excellent performance in several tasks including machine translation (Sutskever et al., 2014), summarization (Rush et al.", "startOffset": 125, "endOffset": 149}, {"referenceID": 14, "context": ", 2014), summarization (Rush et al., 2015), dialogue generation (Serban et al.", "startOffset": 23, "endOffset": 42}, {"referenceID": 15, "context": ", 2015), dialogue generation (Serban et al., 2015), and image captioning (Xu et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 19, "context": ", 2015), and image captioning (Xu et al., 2015).", "startOffset": 30, "endOffset": 47}, {"referenceID": 12, "context": "These include reinforcement learning (Ranzato et al., 2016; Bahdanau et al., 2017), imitation learning (Daum\u00e9 et al.", "startOffset": 37, "endOffset": 82}, {"referenceID": 1, "context": "These include reinforcement learning (Ranzato et al., 2016; Bahdanau et al., 2017), imitation learning (Daum\u00e9 et al.", "startOffset": 37, "endOffset": 82}, {"referenceID": 5, "context": ", 2017), imitation learning (Daum\u00e9 et al., 2009; Ross et al., 2011; Bengio et al., 2015), and beam-based approaches (Wiseman and Rush, 2016; Andor et al.", "startOffset": 28, "endOffset": 88}, {"referenceID": 13, "context": ", 2017), imitation learning (Daum\u00e9 et al., 2009; Ross et al., 2011; Bengio et al., 2015), and beam-based approaches (Wiseman and Rush, 2016; Andor et al.", "startOffset": 28, "endOffset": 88}, {"referenceID": 3, "context": ", 2017), imitation learning (Daum\u00e9 et al., 2009; Ross et al., 2011; Bengio et al., 2015), and beam-based approaches (Wiseman and Rush, 2016; Andor et al.", "startOffset": 28, "endOffset": 88}, {"referenceID": 18, "context": ", 2015), and beam-based approaches (Wiseman and Rush, 2016; Andor et al., 2016; Daum\u00e9 III and Marcu, 2005).", "startOffset": 35, "endOffset": 106}, {"referenceID": 0, "context": ", 2015), and beam-based approaches (Wiseman and Rush, 2016; Andor et al., 2016; Daum\u00e9 III and Marcu, 2005).", "startOffset": 35, "endOffset": 106}, {"referenceID": 3, "context": "In this paper, we focus on one the simplest to implement and least computationally expensive approaches, scheduled sampling (Bengio et al., 2015), which stochastically incorporates contexts from previous decoding decisions into training.", "startOffset": 124, "endOffset": 145}, {"referenceID": 3, "context": "In addition, we demonstrate a related approximation and reparametrization for sample-based training (another training scenario considered by scheduled sampling (Bengio et al., 2015)) that can yield stochastic gradients with lower variance than in standard scheduled sampling.", "startOffset": 160, "endOffset": 181}, {"referenceID": 3, "context": "While scheduled sampling (Bengio et al., 2015) is an effective way to rectify exposure bias, it cannot differentiate between cascading errors, which can lead to a sequence of bad decisions, and local errors, which have more benign effects.", "startOffset": 25, "endOffset": 46}, {"referenceID": 3, "context": "For the sake of simplicity, the \u2018always sample\u2019 variant of scheduled sampling is described (Bengio et al., 2015).", "startOffset": 91, "endOffset": 112}, {"referenceID": 10, "context": "This approximation was proposed by Maddison et al. (2017) and Jang et al.", "startOffset": 35, "endOffset": 58}, {"referenceID": 10, "context": "(2017) and Jang et al. (2016), who showed that the same soft argmax operation introduced above can be used for reducing variance of stochastic gradients when sampling from softmax distributions.", "startOffset": 11, "endOffset": 30}, {"referenceID": 3, "context": "As we discuss in Section 5, mixing model predictions randomly with ground truth symbols during training (Bengio et al., 2015; Daum\u00e9 et al., 2009; Ross et al., 2011), while annealing the probability of using the ground truth with each epoch, results in better models and more stable training.", "startOffset": 104, "endOffset": 164}, {"referenceID": 5, "context": "As we discuss in Section 5, mixing model predictions randomly with ground truth symbols during training (Bengio et al., 2015; Daum\u00e9 et al., 2009; Ross et al., 2011), while annealing the probability of using the ground truth with each epoch, results in better models and more stable training.", "startOffset": 104, "endOffset": 164}, {"referenceID": 13, "context": "As we discuss in Section 5, mixing model predictions randomly with ground truth symbols during training (Bengio et al., 2015; Daum\u00e9 et al., 2009; Ross et al., 2011), while annealing the probability of using the ground truth with each epoch, results in better models and more stable training.", "startOffset": 104, "endOffset": 164}, {"referenceID": 3, "context": "For the case of scheduled sampling with sample-based training\u2013where decisions are sampled rather than chosen greedily (Bengio et al., 2015)\u2013we conduct experiments using a related training procedure.", "startOffset": 118, "endOffset": 139}, {"referenceID": 4, "context": "Data: For MT, we use the same dataset (the German-English portion of the IWSLT 2014 machine translation evaluation campaign (Cettolo et al., 2014)), preprocessing and data splits as Ranzato et al.", "startOffset": 124, "endOffset": 146}, {"referenceID": 4, "context": "Data: For MT, we use the same dataset (the German-English portion of the IWSLT 2014 machine translation evaluation campaign (Cettolo et al., 2014)), preprocessing and data splits as Ranzato et al. (2016). For named entity recognition, we use the CONLL 2003 shared task data (Tjong Kim Sang and De Meulder, 2003) for German language and use the provided data splits.", "startOffset": 125, "endOffset": 204}, {"referenceID": 2, "context": "Implementation details: For MT, we use a seq2seq model with a simple attention mechanism (Bahdanau et al., 2015), a bidirectional LSTM encoder (1 layer, 256 units), and an LSTM decoder (1 layer, 256 units).", "startOffset": 89, "endOffset": 112}, {"referenceID": 3, "context": "Hyperparameter tuning: We start by training with actual ground truth sequences for the first epoch and decay the probability of selecting the ground truth token as an inverse sigmoid (Bengio et al., 2015) of epochs with a decay strength parameter k.", "startOffset": 183, "endOffset": 204}, {"referenceID": 3, "context": "We compare our approach with two baselines: standard cross-entropy loss minimization for seq2seq models (\u2018Baseline CE\u2019) and the standard scheduled sampling procedure (Bengio et al. (2015)).", "startOffset": 167, "endOffset": 188}, {"referenceID": 3, "context": "We compare our approach (\u03b1-soft argmax with fixed and annealed temperature) with standard cross entropy training (Baseline CE) and discontinuous scheduled sampling (Bengio et al. (2015)).", "startOffset": 165, "endOffset": 186}, {"referenceID": 3, "context": "Moreover, both approaches using continuous relaxations (greedy and sample-based) outperform standard scheduled sampling (Bengio et al., 2015).", "startOffset": 120, "endOffset": 141}, {"referenceID": 3, "context": "We also observed that the \u2018always sample\u2019 case of relaxed greedy decoding, in which we never mix in ground truth inputs (see Bengio et al. (2015)), worked well for NER but resulted in unstable training for MT.", "startOffset": 125, "endOffset": 146}], "year": 2017, "abstractText": "We demonstrate that a continuous relaxation of the argmax operation can be used to create a differentiable approximation to greedy decoding for sequence-tosequence (seq2seq) models. By incorporating this approximation into the scheduled sampling training procedure (Bengio et al., 2015)\u2013a well-known technique for correcting exposure bias\u2013we introduce a new training objective that is continuous and differentiable everywhere and that can provide informative gradients near points where previous decoding decisions change their value. In addition, by using a related approximation, we demonstrate a similar approach to sampled-based training. Finally, we show that our approach outperforms cross-entropy training and scheduled sampling procedures in two sequence prediction tasks: named entity recognition and machine translation.", "creator": "LaTeX with hyperref package"}}}