{"id": "1704.06986", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2017", "title": "Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling", "abstract": "Fixed-vocabulary language models fail to account for one of the most characteristic statistical facts of natural language: the frequent creation and reuse of new word types. Although character-level language models offer a partial solution in that they can create word types not attested in the training corpus, they do not capture the \"bursty\" distribution of such words. In this paper, we augment a hierarchical LSTM language model that generates sequences of word tokens character by character with a caching mechanism that learns to reuse previously generated words. To validate our model we construct a new open-vocabulary language modeling corpus (the Multilingual Wikipedia Corpus, MWC) from comparable Wikipedia articles in 7 typologically diverse languages and demonstrate the effectiveness of our model across this range of languages.", "histories": [["v1", "Sun, 23 Apr 2017 21:31:22 GMT  (668kb)", "http://arxiv.org/abs/1704.06986v1", "ACL 2017"]], "COMMENTS": "ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kazuya kawakami", "chris dyer", "phil blunsom"], "accepted": true, "id": "1704.06986"}, "pdf": {"name": "1704.06986.pdf", "metadata": {"source": "CRF", "title": "Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling", "authors": ["Kazuya Kawakami", "Chris Dyer", "Phil Blunsom"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 4.06 986v 1 [cs.C L] 23 Apr 201 7takes into account one of the most characteristic statistical facts of natural language: the frequent generation and reuse of new word types. Although character-level language models offer a partial solution by generating word types that are not used in the training corpus, they do not capture the \"crisp\" distribution of such words. In this essay, we supplement a hierarchical LSTM language model that generates sequences of word marks character by character with a caching mechanism that learns to reuse already generated words. To validate our model, we construct a new open language modeling corpus (the multilingual Wikipedia corpus; MWC) from comparable Wikipedia articles in 7 typologically different languages, demonstrating the effectiveness of our model in this area of languages."}, {"heading": "1 Introduction", "text": "In fact, most language models are based on a closed vocabulary, which consists of the most common words in a corpus. Rare tokens are typically replaced by the unknown word token. < < < < / p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p p > p p p p p p p p > p p p p p p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p"}, {"heading": "2 Model", "text": "In this section, we describe our hierarchical sign language model with a word cache. As is typical of RNN language models, our model uses the chain rule to break down the problem into incremental predictions of the next word based on the story: p (w) = | w | p = 1p (wt | w < t). We make two modifications to the traditional RNN language model, which we then describe. First, we start with a cache-less model, which we call the Hierarchical Sign Language Model (HCLM; \u00a7 2.1), which generates and constructs words as a string by encoding a character sequence with an LSTM (Ling et al., 2015)."}, {"heading": "2.1 Hierarchical Character-level Language", "text": "The HCLM consists of four components, three LSTMs (Hochreiter and Schmidhuber, 1997): a character encoder, a word-level context encoder, and a character decoder (referred to as LSTMenc, LSTMctx, and LSTMdec, respectively), and a softmax output layer over the character vocabulary. Fig. 1 illustrates an unrolled HCLM. Suppose the model reads the word wt \u2212 1 and predicts the next word wt \u2212 dectx. First, the model reads the string containing the word wt \u2212 1,1,. Fig. \u2212 1, | ct \u2212 1 | is the length of the word generated at times t \u2212 1."}, {"heading": "2.2 Continuous cache component", "text": "The cache component is an external memory structure that stores K elements of recent history. Similar to the memory structure used in Grave et al. (2017), after each generation of wt a word is added to a key-value memory. Otherwise, if the memory is not full, an empty space is chosen or the least used slot is overwritten. When writing a new word into memory, the key is the RNN representation used to generate the word (ht) and the value is the word itself (wt). In the case if the word already exists in the cache at any position i, the ki is updated to be the arithmetic average of ht and the existing mah value. To define the copying probability from the cache, a tandem value is copied."}, {"heading": "2.3 Character-level Neural Cache Language Model", "text": "The word probability p (wt | w < t) is defined as a mixture of the following two probabilities: The first is a probability of the language model plm (wt | w < t) and the second is the pointer probability pptr (wt | w < t). The final probability p (wt | w < t) is derived from p (wt | w < t) + (1 \u2212 \u03bbt) pptr (wt | w < t), where \"t\" is calculated by a multilayered perceptron with two nonlinear transformations using ht as input, followed by a transformation by the logistic sigmoid function: \u03b3t = MLP (ht), \u0441t = 11 \u2212 e \u2212 \u03b3t."}, {"heading": "2.4 Training objective", "text": "The model parameters and character projection parameters are trained together by maximizing the following log probability of observed characters in the training corpus, L = \u2212 \u2211 log p (wt | w < t)."}, {"heading": "3 Datasets", "text": "We evaluate our model against a set of data sets and use existing benchmarks to compare with previous published results, as well as a new multilingual corpus that specifically tests the performance of our model in a number of typological settings."}, {"heading": "3.1 Penn Tree Bank (PTB)", "text": "To allow a fair comparison with previous work, we applied the standard pre-processing method used by Mikolov et al. (2010). In standard pre-processing, tokenization is applied, words are reduced and punctuation is removed. In addition, rarer words are replaced by unknown tokens (UNK), 2 which limit the vocabulary size to 10k. Due to this pre-processing, we do not expect this data set to benefit from the modelling innovations we introduced in the thesis. Fig.1 summarizes the body statistics."}, {"heading": "3.2 WikiText-2", "text": "Merity et al. (2017) proposed the WikiText 2 corpus as a new benchmark dataset. 3 They pointed out that the pre-processed PTB corpus is unrealistic for real-world usage in terms of word distribution. Since the vocabulary size is set at 10k, word frequency does not have a long tail. The wikiText 2 corpus consists of 720 articles. They provided two versions. The version for word-level linguistic modeling was pre-processed by discarding rare words. But for character models, they provided raw documents without any removal of word or character types or lowercase writing, but with tokenization. We will make one change to this corpus: Since Wikipedia articles use characters from other languages extensively, character types that occur less than 25 times have been replaced by a dummy character (this plays the role of < UNK > token in the character vocabulary)."}, {"heading": "3.3 Multilingual Wikipedia Corpus (MWC)", "text": "In fact, most of them are able to survive on their own if they are not able to play by the rules."}, {"heading": "4 Experiments", "text": "We now turn to a series of experiments to show the value of our hierarchical cache language model at the character level. We trained the model with LSTM units for each dataset. To compare our results with a strong baseline, we also train a model without the cache. Model Configuration For HCLM and HCLM with cache models, we used 600 dimensions for embedding characters and the LSTMs have 600 hidden units for all experiments. Thus, the complexity of the model remains roughly the same as previous work that used a LSTM with 1000 dimensions. Our base model LSTM has 1000 dimensions for embedding and reccurence weights. For the cache model, we used the cache size 100 in each experiment. All parameters, including the character projection parameters, are randomly scanned from a uniform distribution of \u2212 0.08 to 0.08. The initial hidden and memory state of Lc and LctMx are zero."}, {"heading": "4.1 Results", "text": "PTB Tab. 4 summarizes the results on the PTB dataset.87 Our baseline HCLM model reached 1.276 bPC, which is better performance than the LSTM with zoneout regularization (Krueger et al., 2017). And HCLM with cache surpassed the baseline model with 1.247 bPC and achieved competitive results with state-of-the-art models with regularization on repeat weights that were not used in our experiments.Expressed in terms of perplexity per word (i.e., instead of normalizing ourselves by the length of the corpus in characters, we normalize ourselves by words and exhibits), the test perplexity on HCLM is at 94.79. The performance of the unregulated 2-layer LSTM with 1000 hidden units on word level PTB dataset is 114.5 and the same model with aborters reaches 87.0."}, {"heading": "5 Analysis", "text": "In this section, we will qualitatively analyze the behavior of the proposed model Church. To analyze the model, we will calculate the following trailing probabilities that show whether the model used the cache that gave a word and its preceding context. Let's be a random variable that tells whether the cache or the LM should be used to generate the word at the time t. We would like to know if the cache was used at the time t. This can be calculated as follows: p (zt | w) = p (zt, wt | ht, cachet) p (wt | ht, cachet) p (wt, cachet) p (wt | ht, cachet) p (wt, cachet), where cachet is the state of the cache at the time t. We report on the average trailing probability of the cache generation that the first occurrence of w, p (z | w).Tab.7 shows the words in the text 2 test that are set more cache than cache at the time."}, {"heading": "6 Discussion", "text": "Our results show that the HCLM exceeds a basic LSTM. However, with the addition of the caching mechanism, the HCLM consistently becomes more powerful than both the baseline HCLM and the LSTM. This is true even for the PTB, which does not have rare or OOV words in its test kit (due to pre-processing) by caching repetitive common words such as the one. In true open vocabulary settings (i.e. WikiText-2 and MWC), the improvements are much more pronounced than expected. Computational complexity. Compared to word-level models, our model needs to read and generate each word by letter, and it requires a soft-max across the entire memory. However, the calculation is still linear in terms of the length of the sequence, and the softmax in terms of the memory cells."}, {"heading": "7 Related Work", "text": "Caching language models were proposed by Kuhn and De Mori (1990) to explain rupture, and recently this idea was taken up to extend neural language models by a caching mechanism (Merity et al., 2017; Grave et al., 2017). Open vocabulary models of neural language have also been extensively researched (Sutskever et al., 2011; Mikolov et al., 2012; Graves, 2013 et al.). Attempts to sensitize them to word-level dynamics have also been made (Chung et al., 2017). The only models in which open vocabulary language is modeled together with a caching mechanism are the non-parametric Bajian language models based on hierarchical Pitman-Yor processes that generate a lexicon of word types using a character model and then generate a text (Teh, 2006; Goldwater et al, 2009; Chaunhal, 2013)."}, {"heading": "8 Conclusion", "text": "In this paper, we have proposed a character-level language model with an adaptive cache that selectively assigns word probabilities from the past or decoding at the character level, and we show empirically that our model efficiently models word strings and achieves better confusion in each standard dataset. To further validate the performance of our model in different languages, we have collected multilingual Wikipedia corpus for 7 typologically different languages. We also show that our model performs better than character models by modelling the fragility of words in a local context. Therefore, the model proposed in this paper is not directly applicable to languages such as Chinese and Japanese where word segments are not explicitly observable. We will examine a model that may marginalize word segmentation as a latent variable in future work."}, {"heading": "Acknowledgements", "text": "We thank the three anonymous reviewers for their valuable feedback, and the third author thanks the EPSRC and nvidia Corporation for their support."}, {"heading": "A Corpus Statistics", "text": "Fig. 4 shows the frequency distribution of OOV word types in 6 languages."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Knowledge-rich morphological priors for bayesian language models", "author": ["Victor Chahuneau", "Noah A. Smith", "Chris Dyer."], "venue": "Proc. NAACL.", "citeRegEx": "Chahuneau et al\\.,? 2013", "shortCiteRegEx": "Chahuneau et al\\.", "year": 2013}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio."], "venue": "Proc. ICLR.", "citeRegEx": "Chung et al\\.,? 2017", "shortCiteRegEx": "Chung et al\\.", "year": 2017}, {"title": "Empirical estimates of adaptation: the chance of two Noriegas is closer to p/2 than p", "author": ["KennethWChurch."], "venue": "Proc. COLING.", "citeRegEx": "KennethWChurch.,? 2000", "shortCiteRegEx": "KennethWChurch.", "year": 2000}, {"title": "Poisson mixtures", "author": ["KennethWChurch andWilliam AGale."], "venue": "Natural Language Engineering 1(2):163\u2013 190.", "citeRegEx": "AGale.,? 1995", "shortCiteRegEx": "AGale.", "year": 1995}, {"title": "Recurrent batch normalization", "author": ["Tim Cooijmans", "Nicolas Ballas", "C\u00e9sar Laurent", "\u00c7a\u011flar G\u00fcl\u00e7ehre", "Aaron Courville."], "venue": "Proc. ICLR.", "citeRegEx": "Cooijmans et al\\.,? 2017", "shortCiteRegEx": "Cooijmans et al\\.", "year": 2017}, {"title": "A Bayesian framework for word segmentation: Exploring the effects of context", "author": ["Sharon Goldwater", "Thomas L Griffiths", "Mark Johnson."], "venue": "Cognition 112(1):21\u201354.", "citeRegEx": "Goldwater et al\\.,? 2009", "shortCiteRegEx": "Goldwater et al\\.", "year": 2009}, {"title": "Improving neural languagemodels with a continuous cache", "author": ["Edouard Grave", "Armand Joulin", "Nicolas Usunier."], "venue": "Proc. ICLR.", "citeRegEx": "Grave et al\\.,? 2017", "shortCiteRegEx": "Grave et al\\.", "year": 2017}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv preprint arXiv:1308.0850 .", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Hypernetworks", "author": ["David Ha", "Andrew Dai", "Quoc V Le."], "venue": "Proc. ICLR.", "citeRegEx": "Ha et al\\.,? 2017", "shortCiteRegEx": "Ha et al\\.", "year": 2017}, {"title": "Information retrieval: Computational and theoretical aspects", "author": ["Harold Stanley Heaps."], "venue": "Academic Press, Inc.", "citeRegEx": "Heaps.,? 1978", "shortCiteRegEx": "Heaps.", "year": 1978}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proc. ICLR.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "A clockwork RNN", "author": ["Jan Koutnik", "Klaus Greff", "Faustino Gomez", "Juergen Schmidhuber."], "venue": "Proc. ICML.", "citeRegEx": "Koutnik et al\\.,? 2014", "shortCiteRegEx": "Koutnik et al\\.", "year": 2014}, {"title": "Zoneout: Regularizing rnns by randomly preserving hidden activations", "author": ["David Krueger", "TeganMaharaj", "J\u00e1nos Kram\u00e1r", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh Goyal", "Yoshua Bengio", "Hugo Larochelle", "Aaron Courville"], "venue": null, "citeRegEx": "Krueger et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Krueger et al\\.", "year": 2017}, {"title": "A cachebased natural language model for speech recognition", "author": ["Roland Kuhn", "Renato De Mori."], "venue": "IEEE transactions on pattern analysis and machine intelligence 12(6):570\u2013583.", "citeRegEx": "Kuhn and Mori.,? 1990", "shortCiteRegEx": "Kuhn and Mori.", "year": 1990}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Tiago Lu\u00eds", "Lu\u00eds Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso."], "venue": "Proc. EMNLP.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Pointer sentinel mixturemodels", "author": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher."], "venue": "Proc. ICLR.", "citeRegEx": "Merity et al\\.,? 2017", "shortCiteRegEx": "Merity et al\\.", "year": 2017}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "Proc. Interspeech.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Subword language modeling with neural networks", "author": ["Tom\u00e1\u0161 Mikolov", "Ilya Sutskever", "Anoop Deoras", "HaiSon Le", "Stefan Kombrink", "Jan Cernocky."], "venue": "preprint (http://www. fit. vutbr. cz/imikolov/rnnlm/char. pdf) .", "citeRegEx": "Mikolov et al\\.,? 2012", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Recurrent dropout without memory loss", "author": ["Stanislau Semeniuta", "Aliaksei Severyn", "Erhardt Barth."], "venue": "Proc. COLING.", "citeRegEx": "Semeniuta et al\\.,? 2016", "shortCiteRegEx": "Semeniuta et al\\.", "year": 2016}, {"title": "A hierarchical recurrent encoderdecoder for generative context-aware query suggestion", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob Grue Simonsen", "JianYun Nie."], "venue": "Proc. CIKM.", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton."], "venue": "Proc. ICML.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "A hierarchical Bayesian language model based on Pitman-Yor processes", "author": ["Yee Whye Teh."], "venue": "Proc. ACL.", "citeRegEx": "Teh.,? 2006", "shortCiteRegEx": "Teh.", "year": 2006}, {"title": "On multiplicative integration with recurrent neural networks", "author": ["YuhuaiWu", "Saizheng Zhang", "Ying Zhang", "Yoshua Bengio", "Ruslan R Salakhutdinov."], "venue": "Proc. NIPS.", "citeRegEx": "YuhuaiWu et al\\.,? 2016", "shortCiteRegEx": "YuhuaiWu et al\\.", "year": 2016}, {"title": "Neural architecture search with reinforcement learning", "author": ["Barret Zoph", "Quoc V Le."], "venue": "arXiv preprint arXiv:1611.01578 .", "citeRegEx": "Zoph and Le.,? 2016", "shortCiteRegEx": "Zoph and Le.", "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": "First, vocabularies keep growing as the number of documents in a corpus grows: new words are constantly being created (Heaps, 1978).", "startOffset": 118, "endOffset": 131}, {"referenceID": 22, "context": "The open-vocabulary problem can be solved by dispensing with word-level models in favor of models that predict sentences as sequences of characters (Sutskever et al., 2011; Chung et al., 2017).", "startOffset": 148, "endOffset": 192}, {"referenceID": 2, "context": "The open-vocabulary problem can be solved by dispensing with word-level models in favor of models that predict sentences as sequences of characters (Sutskever et al., 2011; Chung et al., 2017).", "startOffset": 148, "endOffset": 192}, {"referenceID": 21, "context": "Our starting point is a hierarchical LSTM that has been previously used for modeling sentences (word by word) in a conversation (Sordoni et al., 2015), except here we model words (character by character) in a sentence.", "startOffset": 128, "endOffset": 150}, {"referenceID": 17, "context": "To this model, we add a caching mechanism similar to recent proposals for caching that have been advocated for closed-vocabulary models (Merity et al., 2017; Grave et al., 2017).", "startOffset": 136, "endOffset": 177}, {"referenceID": 7, "context": "To this model, we add a caching mechanism similar to recent proposals for caching that have been advocated for closed-vocabulary models (Merity et al., 2017; Grave et al., 2017).", "startOffset": 136, "endOffset": 177}, {"referenceID": 16, "context": "1) which generates words as a sequence of characters and constructs a \u201cword embedding\u201d by encoding a character sequence with an LSTM (Ling et al., 2015).", "startOffset": 133, "endOffset": 152}, {"referenceID": 20, "context": "TheHCLM is an adaptation of the hierarchical recurrent encoder-decoder of Sordoni et al. (2015) which was used to model dialog as a sequence of actions sentences which are themselves sequences of words.", "startOffset": 74, "endOffset": 96}, {"referenceID": 17, "context": "This cache mechanism is similar to the model proposed by Merity et al. (2017).", "startOffset": 57, "endOffset": 78}, {"referenceID": 11, "context": "The HCLM consists of four components, three LSTMs (Hochreiter and Schmidhuber, 1997): a character encoder, a word-level context encoder, and a character decoder (denoted LSTMenc, LSTMctx, and LSTMdec, respectively), and a softmax output layer over the character vocabulary.", "startOffset": 50, "endOffset": 84}, {"referenceID": 6, "context": "Similarly to the memory structure used in Grave et al. (2017), a word is added to a key-value memory after each generation of wt.", "startOffset": 42, "endOffset": 62}, {"referenceID": 0, "context": "To define the copy probability from the cache at time t, a distribution over copy sites is defined using the attention mechanism of Bahdanau et al. (2015). To do so, we construct a query vector (rt) from the RNN\u2019s current hidden state ht,", "startOffset": 132, "endOffset": 155}, {"referenceID": 7, "context": "We remark that Grave et al. (2017) use a clever trick to estimate the probability, \u03bbt of drawing from the LM by augmenting their (closed) vocabulary with a special symbol indicating that a copy should be used.", "startOffset": 15, "endOffset": 35}, {"referenceID": 18, "context": "For fair comparison with previous works, we followed the standard preprocessing method used by Mikolov et al. (2010). In the standard preprocessing, tokenization is applied, words are lowercased, and punctuation is removed.", "startOffset": 95, "endOffset": 117}, {"referenceID": 2, "context": "This is somewhat surprising modeling choice, but it has become conventional (Chung et al., 2017).", "startOffset": 76, "endOffset": 96}, {"referenceID": 12, "context": "Learning The models were trained with the Adam update rule (Kingma and Ba, 2015) with a learning rate of 0.", "startOffset": 59, "endOffset": 80}, {"referenceID": 8, "context": "Following the definition in Graves (2013), bits-per-character is the average value of \u2212 log2 p(wt | w<t) over the whole test set,", "startOffset": 28, "endOffset": 42}, {"referenceID": 14, "context": "276 bpc which is better performance than the LSTM with Zoneout regularization (Krueger et al., 2017).", "startOffset": 78, "endOffset": 100}, {"referenceID": 13, "context": "CW-RNN (Koutnik et al., 2014) - 1.", "startOffset": 7, "endOffset": 29}, {"referenceID": 19, "context": "46 HF-MRNN (Mikolov et al., 2012) - 1.", "startOffset": 11, "endOffset": 33}, {"referenceID": 19, "context": "39 ME n-gram (Mikolov et al., 2012) - 1.", "startOffset": 13, "endOffset": 35}, {"referenceID": 5, "context": "37 RBN (Cooijmans et al., 2017) 1.", "startOffset": 7, "endOffset": 31}, {"referenceID": 20, "context": "32 Recurrent Dropout (Semeniuta et al., 2016) 1.", "startOffset": 21, "endOffset": 45}, {"referenceID": 14, "context": "301 Zoneout (Krueger et al., 2017) 1.", "startOffset": 12, "endOffset": 34}, {"referenceID": 2, "context": "297 HM-LSTM (Chung et al., 2017) - 1.", "startOffset": 12, "endOffset": 32}, {"referenceID": 9, "context": "27 HyperNetwork (Ha et al., 2017) 1.", "startOffset": 16, "endOffset": 33}, {"referenceID": 9, "context": "265 LayerNorm HyperNetwork (Ha et al., 2017) 1.", "startOffset": 27, "endOffset": 44}, {"referenceID": 9, "context": "250 2-LayerNorm HyperLSTM (Ha et al., 2017)* - 1.", "startOffset": 26, "endOffset": 43}, {"referenceID": 25, "context": "219 2-Layer with New Cell (Zoph and Le, 2016)* - 1.", "startOffset": 26, "endOffset": 45}, {"referenceID": 17, "context": "9 with LSTM with ZoneOut and Variational Dropout regularization (Merity et al., 2017).", "startOffset": 64, "endOffset": 85}, {"referenceID": 17, "context": "Caching language models were proposed to account for burstiness by Kuhn and De Mori (1990), and recently, this idea has been incorporated to augment neural language models with a caching mechanism (Merity et al., 2017; Grave et al., 2017).", "startOffset": 197, "endOffset": 238}, {"referenceID": 7, "context": "Caching language models were proposed to account for burstiness by Kuhn and De Mori (1990), and recently, this idea has been incorporated to augment neural language models with a caching mechanism (Merity et al., 2017; Grave et al., 2017).", "startOffset": 197, "endOffset": 238}, {"referenceID": 2, "context": "Attempts to make them more aware of wordlevel dynamics, using models similar to our hierarchical formulation, have also been proposed (Chung et al., 2017).", "startOffset": 134, "endOffset": 154}, {"referenceID": 23, "context": "The only models that are open vocabulary language modeling together with a caching mechanism are the nonparametric Bayesian language models based on hierarchical Pitman\u2013Yor processes which generate a lexicon of word types using a character model, and then generate a text using these (Teh, 2006; Goldwater et al., 2009; Chahuneau et al., 2013).", "startOffset": 284, "endOffset": 343}, {"referenceID": 6, "context": "The only models that are open vocabulary language modeling together with a caching mechanism are the nonparametric Bayesian language models based on hierarchical Pitman\u2013Yor processes which generate a lexicon of word types using a character model, and then generate a text using these (Teh, 2006; Goldwater et al., 2009; Chahuneau et al., 2013).", "startOffset": 284, "endOffset": 343}, {"referenceID": 1, "context": "The only models that are open vocabulary language modeling together with a caching mechanism are the nonparametric Bayesian language models based on hierarchical Pitman\u2013Yor processes which generate a lexicon of word types using a character model, and then generate a text using these (Teh, 2006; Goldwater et al., 2009; Chahuneau et al., 2013).", "startOffset": 284, "endOffset": 343}], "year": 2017, "abstractText": "Fixed-vocabulary language models fail to account for one of the most characteristic statistical facts of natural language: the frequent creation and reuse of new word types. Although character-level language models offer a partial solution in that they can create word types not attested in the training corpus, they do not capture the \u201cbursty\u201d distribution of such words. In this paper, we augment a hierarchical LSTM language model that generates sequences of word tokens character by character with a caching mechanism that learns to reuse previously generated words. To validate our model we construct a new open-vocabulary language modeling corpus (the Multilingual Wikipedia Corpus; MWC) from comparable Wikipedia articles in 7 typologically diverse languages and demonstrate the effectiveness of our model across this range of languages.", "creator": "LaTeX with hyperref package"}}}