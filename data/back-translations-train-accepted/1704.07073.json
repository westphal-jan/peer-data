{"id": "1704.07073", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Selective Encoding for Abstractive Sentence Summarization", "abstract": "We propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of a sentence encoder, a selective gate network, and an attention equipped decoder. The sentence encoder and decoder are built with recurrent neural networks. The selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder. The second level representation is tailored for sentence summarization task, which leads to better performance. We evaluate our model on the English Gigaword, DUC 2004 and MSR abstractive sentence summarization datasets. The experimental results show that the proposed selective encoding model outperforms the state-of-the-art baseline models.", "histories": [["v1", "Mon, 24 Apr 2017 07:57:37 GMT  (584kb,D)", "http://arxiv.org/abs/1704.07073v1", "10 pages; To appear in ACL 2017"]], "COMMENTS": "10 pages; To appear in ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["qingyu zhou", "nan yang", "furu wei", "ming zhou 0001"], "accepted": true, "id": "1704.07073"}, "pdf": {"name": "1704.07073.pdf", "metadata": {"source": "CRF", "title": "Selective Encoding for Abstractive Sentence Summarization", "authors": ["Qingyu Zhou", "Nan Yang", "Furu Wei", "Ming Zhou"], "emails": ["qyzhou@hit.edu.cn", "nanya@microsoft.com", "fuwei@microsoft.com", "mingzhou@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it will be able to fix and fix the mentioned bugs."}, {"heading": "2 Related Work", "text": "In fact, most of them will be able to abide by the rules that they apply in practice, and they will have to abide by the rules that they apply in practice."}, {"heading": "3 Problem Formulation", "text": "When summarizing a sentence based on an input sentence x = (x1, x2,.., xn), where n is the sentence length, xi-Vs and Vs is the source vocabulary, the system summarizes x by producing y = (y1, y2,..., yl), where l \u2264 n is the sentence length, yi-Vt and Vt is the target vocabulary. If | y | | x |, which means that all the summarizing words y must appear in the input, we call this an extractive sentence summary. If | y | * | x |, which means that not all the summarizing words come from the input sentence, we call this an abstract sentence summary. Table 1 provides an example. In this work, we focus on the task of abstract sentence summarization."}, {"heading": "4 Model", "text": "As shown in Figure 2, our model consists of a sentence encoder using the Gated Recurrent Unit (GRU) (Cho et al., 2014), a selective gate network, and an attention-grabbing GRU decoder. First, the bidirectional GRU encoder reads the input words x = (x1, x2,.., xn) and builds its representation (h1, h2,.., hn). Then, the selective gate selects and filters the word representations according to the sentence meaning to generate a customized sentence representation for abstract sentence summary tasks. Finally, the GRU decoder generates the output summary taking into account the customized representation. In the following sections, we present the sentence encoder, the selective mechanism, or summary decoder."}, {"heading": "4.1 Sentence Encoder", "text": "The role of the sentence encoder is to read the input sentence and construct the basic sentence representation, using a bidirectional GRU (BiGRU) as a recursive unit, where GRU is defined as follows: zi = \u03c3 (Wz [xi, hi \u2212 1]) ri = \u03c3 (Wr [xi, hi \u2212 1]) h-i = tanh (Wh [xi, ri hi \u2212 1]) hi = (1 \u2212 zi) hi \u2212 1 + zi h-i (1) (2) (3) (4), where Wz, Wr and Wh are weight matrices. The BiGRU consists of a forward GRU and a backward GRU. The forward GRU reads the input sentence word embeddings from left to right and receives a sequence of hidden states (~ h1, ~ h2,.., ~ h2). The backward GRU reads the input quadrants from left to right (~ h2), ~ h2, and ~ h2)."}, {"heading": "4.2 Selective Mechanism", "text": "In the Sequence-to-Sequence Machine Translation (MT) model, the encoder and decoder are responsible for mapping input-sentence information to a list of vectors and decoding the sentence-representation vectors to generate an output sentence (Bahdanau et al., 2015). Some previous work applied this framework to summary tasks (Nallapati et al., 2016; Gu et al., 2016; Gulcehre et al., 2016). However, the abstract summary of sentences differs in two ways. First, there is no explicit alignment between the input sentence and the output summary except for the usual words. Second, the summary task must retain the highlights and remove the unnecessary information, while MT must literally retain all the information."}, {"heading": "4.3 Summary Decoder", "text": "Above the record encoder and the selective gate network, we then use GRU with attention as decoder to generate the output summary. At each decoding time step t, the GRU reads the previous word, the wt \u2212 1, and the previous context vector ct \u2212 1 as inputs to calculate the new hidden state st. To initialize the hidden state of the GRU, we use a linear plane with the last hidden state of the encoder ~ h1 as input: st = GRU (wt \u2212 1, ct \u2212 1, st \u2212 1) s0 = tanh (Wd ~ h1 + b) (10) (11), where Wd is the weight matrix and b is the bias vector. The context vector ct for the current time step t t is generated by the concatenated attention mechanism (Luong et al., 2015) s0 = tanh (Wd ~ h1 + Ub), the current state of the decoder is \u2212 d."}, {"heading": "4.4 Objective Function", "text": "Our goal is to maximize the summary output probability in view of the input set. Therefore, we optimize the negative log likelihood loss function: J (\u03b8) = \u2212 1 | D | \u2211 (x, y) \u0445 D log p (y | x) (18), where D denotes a series of parallel sentence pairs and \u03b8 is the model parameter. To learn the model parameter, we use stochastic gradient descent (SGD) with minibatch."}, {"heading": "5 Experiments", "text": "In this section, we present the data set we use, the evaluation metric, the implementation details, the baselines we compare with, and the performance of our system."}, {"heading": "5.1 Dataset", "text": "The parallel corpus is produced by pairing the first sentence and the headline in the news article with some heuristic rules. We use the script1 released by Rush et al. (2015) to prepare the process and extract the development datasets. It performs various basic text normalizations, including PTB tokenization, replacing all digit characters with #, and replaces words seen less than 5 times with < unk >. The extracted corpus contains about 3.8M sentence summaries for PTB tokenization, lower case sums, and replaces words labeled with < unk > less than 5 times."}, {"heading": "5.2 Evaluation Metric", "text": "We use ROUGE (Lin, 2004) as a yardstick. ROUGE measures the quality of the summary by calculating overlapping lexical units, such as unigrams, bigrams, trigrams, and longest common subsequence (LCS). It becomes the default yardstick for common DUC tasks and is popular for summary evaluations. After previous work, we use ROUGE-1 (unicram), ROUGE-2 (bi-2Dank Rush et al. (2015), we have acquired the test set they use. After Chopra et al. (2016), we remove pairs with blank titles, resulting in slightly different accuracy compared to Rush et al. (2015) for their systems. The purified test set contains 1951 sets of summary pairs.3Our development and test sets can be found at https: / / res.qyzhou.megram) and ROUGE-L (LCS) as yardsticks reported in the evaluation results."}, {"heading": "5.3 Implementation Details", "text": "Model Parameters The input and output vocabulary is collected from training data containing 119,504 and 68,883 word types, respectively. We set the word embedding size to 300 and all hidden GRU state variables to 512. We use Dropout (Srivastava et al., 2014) with the probability of p = 0.5. Model training We randomly initialize model parameters using a Gaussian distribution using the Xavier scheme (Glorot and Bengio, 2010). As an optimization algorithm, we use Adam (Kingma and Ba, 2015). For the hyperparameters of the Adam optimizer, we set the learning rate \u03b1 = 0.001, two impulse parameters \u03b21 = 0.9 and \u03b22 = 0.999 and = 10 \u2212 8. During the training, we test the model performance (ROUGE-2 F1) for the development set for all 2,000 lots. We halve the Adam learning rate when the ROUGE F1-2 tests are repeated for twelve consecutive values."}, {"heading": "5.4 Baseline", "text": "We compare the SEASS model with the following state-of-the-art base lines: ABS Rush et al. (2015) use an attentive CNN encoder and NNLM decoder to complete the sentence summary task. We trained this base model with published Code1 and evaluated it with our internal English Gigaword test set and MSR ATC test set. ABS + Based on the ABS model, Rush et al. (2015) further align their model with the DUC 2003 data set, which results in improvements over the DUC 2004 test set. CAs2s As an extension of the ABS model, Chopra et al. (2016) use a conversion-based encoder and RNN decoder based on the ABS model to enhance an RNN decoder that exceeds the ABS model. Feats2s Nallapati et al. (2016) use as an extension of the ABS model a full 2016 RNN sequence decoder and add an RNN decoder to the RNN encoder or some of the encoder or 2M to enhance the models."}, {"heading": "5.5 Results", "text": "In fact, we will be able to reform the whole system in the way that we have experienced in recent years."}, {"heading": "6 Discussion", "text": "In this section, we first compare the performance of SEASS with the base model s2s + att to illustrate that the proposed method is successful in selecting information and in building customized representation for abstract sentence summary. We then analyze selective coding by visualizing the heat map. The length of the sentences in the test sets ranges from 10 to 80. We group the sentences with an interval of 4 and get 18 different groups and we draw the first 14 groups. We find that the performance curve of our SEASS model is always at the top of the s2s + att with a certain margin. For the groups of 16, 20, 24, 32, 56 and 60, the SEASS model is associated with large improvements compared to the s2s + att model. Overall, these improvements point to a significant effect of the Gate method."}, {"heading": "7 Conclusion", "text": "This article proposes a selective encoding model that extends the sequence-to-sequence model to abstract summaries of sentences. It mimics the behavior of a human summariser and selects important information before writing the summary. With the proposed selective mechanism, we build an end-to-end neural network summary model consisting of three phases: encoding, selection, and decoding. Experimental results show that the selective encoding model significantly improves performance compared to the most advanced methods on English Gigaword, DUC 2004, and MSR-ATC test sets."}, {"heading": "Acknowledgments", "text": "We thank Chuanqi Tan, Junwei Bao, Shuangzhi Wu and the anonymous reviewers for their helpful comments and Alexander M. Rush for providing the dataset for comparison and for helpful discussions."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of a sentence encoder, a selective gate network, and an attention equipped decoder. The sentence encoder and decoder are built with recurrent neural networks. The selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder. The second level representation is tailored for sentence summarization task, which leads to better performance. We evaluate our model on the English Gigaword, DUC 2004 and MSR abstractive sentence summarization datasets. The experimental results show that the proposed selective encoding model outperforms the state-ofthe-art baseline models.", "creator": "LaTeX with hyperref package"}}}