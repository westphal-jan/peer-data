{"id": "1704.07092", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Robust Incremental Neural Semantic Graph Parsing", "abstract": "Parsing sentences to linguistically-expressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focused almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69% Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.", "histories": [["v1", "Mon, 24 Apr 2017 08:50:15 GMT  (57kb,D)", "https://arxiv.org/abs/1704.07092v1", "12 pages; Accepted to ACL 2017"], ["v2", "Thu, 27 Jul 2017 15:39:41 GMT  (57kb,D)", "http://arxiv.org/abs/1704.07092v2", "12 pages; ACL 2017"]], "COMMENTS": "12 pages; Accepted to ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jan buys", "phil blunsom"], "accepted": true, "id": "1704.07092"}, "pdf": {"name": "1704.07092.pdf", "metadata": {"source": "CRF", "title": "Robust Incremental Neural Semantic Graph Parsing", "authors": ["Jan Buys", "Phil Blunsom"], "emails": ["jan.buys@cs.ox.ac.uk", "phil.blunsom@cs.ox.ac.uk"], "sections": [{"heading": null, "text": "Robust Incremental Neural Semantic Graph ParsingJan Buys1 and Phil Blunsom1,2 1Department of Computer Science, University of Oxford 2DeepMind {jan.buys, phil.blunsom} @ cs.ox.ac.ukAbstract Parsing sentences on linguistically expressive semantic representations is a central goal of Natural Language Processing. However, statistical parsing focuses almost exclusively on bilexical dependencies or domain-specific logical shapes. We propose a neural transition-based parser that represents the first complete semantic graph parser for Minimal Recursion Semantics (MRS).The model architecture uses stack-based embedding features and predicts graphs along with unexicalized predictors and their token alignments. Our parser is more precise than attention-based baselines on MRS, and on an additional abstract measuring is faster than our benchmark AMS, which is the precedent of a magnetic representation."}, {"heading": "1 Introduction", "text": "An important goal of Natural Language Understanding (NLU) is to structure sentences, analyze interpretable meanings that can be used to query, infer, and argue. Recently, we have surpassed traditional pipeline approaches that predict syntactic or semantic structures as intermediate steps, to NLU tasks such as sentiment analysis and semantic relativization (Le and Mikolov, 2014; Kiros et al., 1Code, models, and data preparation scripts are available at https: / / github.com / janmbuys, on MRR-Parser.2015), to answer questions (Hermann et al., 2015), and textual entailment (Rockta). The linguistic structure used in applications is predominantly flat, limited to bilexic dependencies or trees.In this paper, we focus on robust parsing in linguistically deep representations."}, {"heading": "2 Meaning Representations", "text": "In fact, it is so that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is"}, {"heading": "3 Incremental Graph Parsing", "text": "Let e = e1, e2,..., eI be a symbolized English sentence, t = t1, t2,..., tJ be a sequential representation of its graph derivative, and a = a1, a2,..., aJ be an alignment sequence consisting of integers in the range 1,..., I. We model the conditional distribution p (t, a | e) that decomposes asJ-j = 1 p (aj | (a, t) 1: j \u2212 1, e) p (tj | a1: j, t1: j \u2212 1, e)."}, {"heading": "3.1 Top-down linearization", "text": "We will now consider how to linearize the semantic graphs before defining the neural models to parameterize the parser in Section 4. The first approach is to linearize a graph as the preceding traverse of its encompassing tree, starting with a particular root node (see Figure 2). Variants of this approach have been proposed for the analysis of neural constituencies (Vinyals et al., 2015b), logical shape prediction (Dong and Lapata, 2016; Jia and Liang, 2016), and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). Linearization involves marking edges whose direction is reversed in the encompassing tree by adding -of. Edges that are not included in the encompassing tree are referred to as re-entry, with special edges whose dependents refer to the original nodes."}, {"heading": "3.2 Transition-based parsing", "text": "In fact, it is that we see ourselves in a position to hold our own, that we are in a position to put ourselves on top, \"he said.\" We have to put ourselves on top, \"he said.\" We have to put ourselves on top, \"he said.\" We have to put ourselves on top, \"he said."}, {"heading": "3.3 Delexicalization and lemma prediction", "text": "We dissect the predicate predicates of the surfaces by predicting the lemmas of the candidates for input marks, and delicalized predicates consisting only of meaning markers. The complete predicates of the surface are then recovered by the predicted alignments. We extract a dictionary that maps words on lemmas from the ERG lexicon. The lemmas of the candidates are predicted using this dictionary, and if no lexicon entry with a lemmatizer is available. The same approach is used to predict constants, along with additional normalizations such as mapping numbers to digits. We use the toolkit of the Stanford CoreNLP symbol (Manning et al., 2014) to tokenize and lemmatize sentences and mark the tokens with the Stanford Named Entity Recognizer (Finkel et al., 2005)."}, {"heading": "4 Encoder-Decoder Models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Sentence encoder", "text": "The proposition e is coded with a bidirectional RNN. We use a standard LSTM architecture without peephole connections (Jozefowicz et al., 2015). For each character e we embed its word, POS tag and named entity (NE) tag as vectors xw, xt and xn, respectively. The embedding is concatenated and undergoes a linear transformation g (e) = W (x) [xw; xt; xn] + b x, so that g (e) has the same dimension as the LSTM. Each input position i is represented by a hidden state hi, which is the concatenation of its forward and backward LSTM state vectors."}, {"heading": "4.2 Hard attention decoder", "text": "We model the alignment of graph nodes to record tokens, a, as a random variable. For the Arceager model, aj corresponds to the alignment of the node of the buffer after action tj has been executed. The distribution of tj covers all transitions and predicates (corresponding to shifts), predicted with a single softmax.The parser output is predicted by an RNN decoder. Leave sj the decoder-hidden state at output position j. We initialize s0 with the final state of the reverse encoder. The alignment is predicted with a pointer network (Vinyals et al., 2015a).The logs are calculated with an MLP that predicts the decoder hidden state against any of the encoder hidden states (for i = 1,.., I), uij = w T tanh (W (1)."}, {"heading": "4.3 Stack-based model", "text": "These features are embedded from the bidirectional RNN encoder, according to the orientation of the nodes on the buffer and on the stack. This approach is similar to the dependency analysis features proposed by Kiperwasser and Goldberg (2016) and Cross and Huang (2016a), although they do not use RNN decoders. To implement these features, the layer that computes the output vector is extended tooj = W (3) sj + W (4) haj + W (7) hst0, where st0 is the sentence alignment index of the element on the stack. The input layer to the next RNN time step is similarly extended todj + 1 = W (5) e (tj) e (4) hst0, where st0 is the sentence alignment index of the element on the stack."}, {"heading": "5 Related Work", "text": "Prior to the work on MRS analysis, predominantly structures are predicted in the context of grammar-based analysis, where sentences are analyzed on HPSG derivatives consistent with grammar, in this case the ERG (Flickinger, 2000). However, the nodes in the derivative trees are characteristic structures from which MRS is extracted by standardization, and this approach fails to analyze sentences for which no valid derivative is found. Some efforts have also been made to develop robust MRS parsers. A proposed approach learns a PCFG grammar to find the most likely structure (Toutanova et al., 2005). This approach is implemented in the PET (Callmeier, 2000) and ACE2 parsers."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Data", "text": "DeepBank (Flickinger et al., 2012) is an HPG and MRS annotation of the Penn Treebank Wall Street Journal (WSJ) corpus. It was developed using an approach known as dynamic treebanking (Oepen et al., 2004), which combines treebank annotations with grammatical development, in this case2http: / / sweaglesw.org / linguistics / ace / of the ERG. This approach has been shown to result in a high interannotator match: 0.94 vs. 0.71 for AMR (Bender et al., 2015). Parses are provided only for sentences for which the ERG has an analysis acceptable to the annotator - meaning that we cannot evaluate the accuracy of sentences that the ERG cannot evaluate (about 15% of the original corpus)."}, {"heading": "6.2 Evaluation", "text": "Dridan and Oepen (2011) proposed an evaluation metric called Elementary Dependency Matching (EDM) for MRS-based graphs. EDM calculates the F1 score of tuples of predicates and arguments. A predicate tuple consists of the label and the character range of a predicate, while an argument tupel consists of the character ranges of the head and dependent nodes of the relationship together with the argument label. To tolerate subtle tokenization differences in punctuation, we allow assignment of spin pairs whose ends differ by one character. Instead, the match metric (Cai and Knight, 2013) proposed for the evaluation of AMR graphs also measures graph overlaps, but does not rely on sentence alignments to determine the correspondences between graph nodes. Smatch is calculated by inference over graph alignments to predict the maximum gold graph between the F1 and the predicted score."}, {"heading": "6.3 Model setup", "text": "Our parser is implemented in TensorFlow (Abadi et al., 2015). For training, we use Adam (Kingma and Ba, 2015) with a learning rate of 0.01 and lot size 64. Gradient standards are truncated to 5.0 (Pascanu et al., 2013). We use single-layer LSTMs with a drop-out of 0.3 (tailored to the development set) on input and output connections. We use size 256 encoder and decoder embedding and size 32 POS and NE tag embedding, for DMRS and EDS diagrams the hidden unit size is set to 256, and for AMR to 128. This configuration, found through grid search and heuristic search within the range of models that fit a single GPU, delivered the best performance on the development set among multiple graph linearizations. Encoder embedding (in the first 100 dimensions each) is pre-initialized with 0.5."}, {"heading": "6.4 MRS parsing results", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "6.5 AMR parsing", "text": "The results of the development group are listed in Table 5. Arc-eager models once again perform better, mainly due to improved precision of concept prediction. However, concept prediction remains the most important weakness of the model; Damonte et al. (2017) reports that state-of-the-art AMR parsers yield 83% of the results of concept prediction. We report on test results in Table 6. Our best neural model outperforms the base parser JAMR (Flanigan et al., 2014), but still lags behind the performance of modern AMR parsers such as CAMR (Wang et al., 2016) and AMR Eager (Damonte et al., 2017). These models make extensive use of external resources, including syntactic parsers and semantic role markers. Our attention-based encoder decoder model already outperforms previous sequence-to-AMR parsers (Barzdins et al., 2017)."}, {"heading": "7 Conclusion", "text": "We have introduced a robust, far-reaching parser for MRS that is faster than existing parsers and suitable for batch processing. We believe there are many future ways to further increase the accuracy of such parsers, including different training objectives, more structured architectures, and semi-supervised learning."}, {"heading": "Acknowledgments", "text": "The first author thanks the financial support of the Clarendon Fund and the Skye Foundation. We thank Stephan Oepen for feedback and help with data preparation and members of the Oxford NLP group for valuable discussions."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng."], "venue": "Software available from tensorflow.org. http://tensorflow.org/.", "citeRegEx": "Warden et al\\.,? 2015", "shortCiteRegEx": "Warden et al\\.", "year": 2015}, {"title": "Broad-coverage CCG semantic parsing with AMR", "author": ["Yoav Artzi", "Kenton Lee", "Luke Zettlemoyer."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational", "citeRegEx": "Artzi et al\\.,? 2015", "shortCiteRegEx": "Artzi et al\\.", "year": 2015}, {"title": "Abstract meaning representation for sembanking", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider."], "venue": "Proceedings of the", "citeRegEx": "Banarescu et al\\.,? 2013", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Riga at semeval-2016 task 8: Impact of smatch extensions and character-level neural translation on AMR parsing accuracy", "author": ["Guntis Barzdins", "Didzis Gosko."], "venue": "Proceedings of SemEval.", "citeRegEx": "Barzdins and Gosko.,? 2016", "shortCiteRegEx": "Barzdins and Gosko.", "year": 2016}, {"title": "Layers of interpretation: On grammar and compositionality", "author": ["Emily M Bender", "Dan Flickinger", "Stephan Oepen", "Woodley Packard", "Ann Copestake."], "venue": "Proceedings of the 11th International Conference on Computational Semantics. pages 239\u2013", "citeRegEx": "Bender et al\\.,? 2015", "shortCiteRegEx": "Bender et al\\.", "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel R. Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts."], "venue": "Proceedings of ACL. pages 1466\u20131477.", "citeRegEx": "Bowman et al\\.,? 2016", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Oxford at SemEval2017 Task 9: Neural AMR parsing with pointeraugmented attention", "author": ["Jan Buys", "Phil Blunsom."], "venue": "Proceedings of SemEval.", "citeRegEx": "Buys and Blunsom.,? 2017", "shortCiteRegEx": "Buys and Blunsom.", "year": 2017}, {"title": "Smatch: An evaluation metric for semantic feature structures", "author": ["Shu Cai", "Kevin Knight."], "venue": "Proceedings of ACL (short papers).", "citeRegEx": "Cai and Knight.,? 2013", "shortCiteRegEx": "Cai and Knight.", "year": 2013}, {"title": "PET - a platform for experimentation with efficient HPSG processing techniques", "author": ["Ulrich Callmeier."], "venue": "Natural Language Engineering 6(1):99\u2013 107.", "citeRegEx": "Callmeier.,? 2000", "shortCiteRegEx": "Callmeier.", "year": 2000}, {"title": "Invited talk: Slacker semantics: Why superficiality, dependency and avoidance of commitment can be the right way to go", "author": ["Ann Copestake."], "venue": "Proceedings of EACL. pages 1\u20139. http://www.aclweb.org/anthology/E09-1001.", "citeRegEx": "Copestake.,? 2009", "shortCiteRegEx": "Copestake.", "year": 2009}, {"title": "Resources for building applications with dependency minimal recursion semantics", "author": ["Ann Copestake", "Guy Emerson", "Michael Wayne Goodman", "Matic Horvat", "Alexander Kuhnle", "Ewa Muszyska"], "venue": null, "citeRegEx": "Copestake et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Copestake et al\\.", "year": 2016}, {"title": "Translation using minimal recursion semantics", "author": ["Ann Copestake", "Dan Flickinger", "Rob Malouf", "Susanne Riehemann", "Ivan Sag."], "venue": "In Proceedings of the Sixth International Conference on Theoretical and Methodological Issues in Machine Translation.", "citeRegEx": "Copestake et al\\.,? 1995", "shortCiteRegEx": "Copestake et al\\.", "year": 1995}, {"title": "Minimal recursion semantics: An introduction", "author": ["Ann Copestake", "Dan Flickinger", "Carl Pollard", "Ivan A Sag."], "venue": "Research on Language and Computation 3(2-3):281\u2013332.", "citeRegEx": "Copestake et al\\.,? 2005", "shortCiteRegEx": "Copestake et al\\.", "year": 2005}, {"title": "Incremental parsing with minimal features using bi-directional lstm", "author": ["James Cross", "Liang Huang."], "venue": "Proceedings of ACL. page 32.", "citeRegEx": "Cross and Huang.,? 2016a", "shortCiteRegEx": "Cross and Huang.", "year": 2016}, {"title": "Spanbased constituency parsing with a structurelabel system and provably optimal dynamic oracles", "author": ["James Cross", "Liang Huang."], "venue": "Proceedings of EMNLP. pages 1\u201311. https://aclweb.org/anthology/D16-1001.", "citeRegEx": "Cross and Huang.,? 2016b", "shortCiteRegEx": "Cross and Huang.", "year": 2016}, {"title": "An incremental parser for abstract meaning representation", "author": ["Marco Damonte", "Shay B. Cohen", "Giorgio Satta."], "venue": "Proceedings of EACL. pages 536\u2013 546. http://www.aclweb.org/anthology/E17-1051.", "citeRegEx": "Damonte et al\\.,? 2017", "shortCiteRegEx": "Damonte et al\\.", "year": 2017}, {"title": "Language to logical form with neural attention", "author": ["Li Dong", "Mirella Lapata."], "venue": "Proceedings of ACL. pages 33\u201343. http://www.aclweb.org/anthology/P16-1004.", "citeRegEx": "Dong and Lapata.,? 2016", "shortCiteRegEx": "Dong and Lapata.", "year": 2016}, {"title": "Parser evaluation using elementary dependency matching", "author": ["Rebecca Dridan", "Stephan Oepen."], "venue": "Proceedings of the 12th International Conference on Parsing Technologies. Association for Computational Linguistics, pages 225\u2013230.", "citeRegEx": "Dridan and Oepen.,? 2011", "shortCiteRegEx": "Dridan and Oepen.", "year": 2011}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proceedings of ACL. pages 334\u2013 343. http://www.aclweb.org/anthology/P15-1033.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith."], "venue": "Proceedings of NAACL.", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Incorporating non-local information into information extraction systems by Gibbs sampling", "author": ["Jenny Rose Finkel", "Trond Grenager", "Christopher Manning."], "venue": "Proceedings of ACL. pages 363\u2013370. http://dx.doi.org/10.3115/1219840.1219885.", "citeRegEx": "Finkel et al\\.,? 2005", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "A discriminative graph-based parser for the abstract meaning representation", "author": ["Jeffrey Flanigan", "Sam Thomson", "Jaime G. Carbonell", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of ACL. pages 1426\u2013 1436. http://aclweb.org/anthology/P/P14/P14-", "citeRegEx": "Flanigan et al\\.,? 2014", "shortCiteRegEx": "Flanigan et al\\.", "year": 2014}, {"title": "On building a more effcient grammar by exploiting types", "author": ["Dan Flickinger."], "venue": "Natural Language Engineering 6(01):15\u201328.", "citeRegEx": "Flickinger.,? 2000", "shortCiteRegEx": "Flickinger.", "year": 2000}, {"title": "Deepbank", "author": ["Dan Flickinger", "Yi Zhang", "Valia Kordoni."], "venue": "a dynamically annotated treebank of the wall street journal. In Proceedings of the 11th International Workshop on Treebanks and Linguistic Theories. pages 85\u201396.", "citeRegEx": "Flickinger et al\\.,? 2012", "shortCiteRegEx": "Flickinger et al\\.", "year": 2012}, {"title": "A transition-based parser for 2-planar dependency structures", "author": ["Carlos G\u00f3mez-Rodr\u0131\u0301guez", "Joakim Nivre"], "venue": "In Proceedings of ACL", "citeRegEx": "G\u00f3mez.Rodr\u0131\u0301guez and Nivre.,? \\Q2010\\E", "shortCiteRegEx": "G\u00f3mez.Rodr\u0131\u0301guez and Nivre.", "year": 2010}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems. pages 1693\u2013", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Data recombination for neural semantic parsing", "author": ["Robin Jia", "Percy Liang."], "venue": "Proceedings of ACL. pages 12\u201322. http://www.aclweb.org/anthology/P16-1002.", "citeRegEx": "Jia and Liang.,? 2016", "shortCiteRegEx": "Jia and Liang.", "year": 2016}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings of ICML. pages 2342\u20132350.", "citeRegEx": "Jozefowicz et al\\.,? 2015", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Lexicalfunctional grammar: A formal system for grammatical representation", "author": ["Ronald M Kaplan", "Joan Bresnan."], "venue": "Formal Issues in LexicalFunctional Grammar pages 29\u2013130.", "citeRegEx": "Kaplan and Bresnan.,? 1982", "shortCiteRegEx": "Kaplan and Bresnan.", "year": 1982}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "Proceedings of ICLR. http://arxiv.org/abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Simple and accurate dependency parsing using bidirectional lstm feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "Transactions of the Association for Computational Linguistics 4:313\u2013327.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in neural information processing systems. pages 3294\u20133302.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Towards a catalogue of linguistic graph banks", "author": ["Marco Kuhlmann", "Stephan Oepen."], "venue": "Computational Linguistics 42(4):819\u2013827.", "citeRegEx": "Kuhlmann and Oepen.,? 2016", "shortCiteRegEx": "Kuhlmann and Oepen.", "year": 2016}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "ICML. volume 14, pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso."], "venue": "Proceedings of NAACL-HLT . pages 1299\u20131304. http://www.aclweb.org/anthology/N15-1142.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Semeval-2016 task 8: Meaning representation parsing", "author": ["Jonathan May."], "venue": "Proceedings of SemEval. pages 1063\u20131073. http://www.aclweb.org/anthology/S16-1166.", "citeRegEx": "May.,? 2016", "shortCiteRegEx": "May.", "year": 2016}, {"title": "Neural shift-reduce ccg semantic parsing", "author": ["Dipendra Kumar Misra", "Yoav Artzi."], "venue": "Proceedings of EMNLP. Austin, Texas, pages 1775\u20131786. https://aclweb.org/anthology/D16-1183.", "citeRegEx": "Misra and Artzi.,? 2016", "shortCiteRegEx": "Misra and Artzi.", "year": 2016}, {"title": "Algorithms for deterministic incremental dependency parsing", "author": ["Joakim Nivre."], "venue": "Computational Linguistics 34(4):513\u2013553.", "citeRegEx": "Nivre.,? 2008", "shortCiteRegEx": "Nivre.", "year": 2008}, {"title": "Lingo redwoods", "author": ["Stephan Oepen", "Dan Flickinger", "Kristina Toutanova", "Christopher D. Manning."], "venue": "Research on Language and Computation 2(4):575\u2013596. https://doi.org/10.1007/s11168-0047430-4.", "citeRegEx": "Oepen et al\\.,? 2004", "shortCiteRegEx": "Oepen et al\\.", "year": 2004}, {"title": "Semeval 2015 task 18: Broad-coverage semantic dependency parsing", "author": ["Stephan Oepen", "Marco Kuhlmann", "Yusuke Miyao", "Daniel Zeman", "Silvie Cinkova", "Dan Flickinger", "Jan Hajic", "Zdenka Uresova."], "venue": "Proceedings of SemEval. pages 915\u2013926.", "citeRegEx": "Oepen et al\\.,? 2015", "shortCiteRegEx": "Oepen et al\\.", "year": 2015}, {"title": "Semeval 2014 task 8: Broad-coverage semantic dependency parsing", "author": ["Stephan Oepen", "Marco Kuhlmann", "Yusuke Miyao", "Daniel Zeman", "Dan Flickinger", "Jan Hajic", "Angelina Ivanova", "Yi Zhang."], "venue": "Proceedings of SemEval. pages 63\u201372.", "citeRegEx": "Oepen et al\\.,? 2014", "shortCiteRegEx": "Oepen et al\\.", "year": 2014}, {"title": "Discriminant-based MRS banking", "author": ["Stephan Oepen", "Jan Tore L\u00f8nning."], "venue": "Proceedings of the 5th International Conference on Language Resources and Evaluation. pages 1250\u20131255.", "citeRegEx": "Oepen and L\u00f8nning.,? 2006", "shortCiteRegEx": "Oepen and L\u00f8nning.", "year": 2006}, {"title": "The proposition bank: An annotated corpus of semantic roles", "author": ["Martha Palmer", "Daniel Gildea", "Paul Kingsbury."], "venue": "Computational linguistics 31(1):71\u2013 106.", "citeRegEx": "Palmer et al\\.,? 2005", "shortCiteRegEx": "Palmer et al\\.", "year": 2005}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "ICML (3) 28:1310\u20131318.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Uofr at semeval-2016 task 8: Learning synchronous hyperedge replacement grammar for amr parsing", "author": ["Xiaochang Peng", "Daniel Gildea."], "venue": "Proceedings of SemEval-2016. pages 1185\u20131189. http://www.aclweb.org/anthology/S16-1183.", "citeRegEx": "Peng and Gildea.,? 2016", "shortCiteRegEx": "Peng and Gildea.", "year": 2016}, {"title": "Addressing the data sparsity issue in neural amr parsing", "author": ["Xiaochang Peng", "Chuan Wang", "Daniel Gildea", "Nianwen Xue."], "venue": "Proceedings of EACL. Preprint. http://www.cs.brandeis.edu/ cwang24/files/eacl17.pdf.", "citeRegEx": "Peng et al\\.,? 2017", "shortCiteRegEx": "Peng et al\\.", "year": 2017}, {"title": "Head-driven phrase structure grammar", "author": ["Carl Pollard", "Ivan A Sag."], "venue": "University of Chicago Press.", "citeRegEx": "Pollard and Sag.,? 1994", "shortCiteRegEx": "Pollard and Sag.", "year": 1994}, {"title": "Parsing English into abstract meaning representation using syntax-based machine translation", "author": ["Michael Pust", "Ulf Hermjakob", "Kevin Knight", "Daniel Marcu", "Jonathan May."], "venue": "Proceedings of EMNLP. Association for Computational", "citeRegEx": "Pust et al\\.,? 2015", "shortCiteRegEx": "Pust et al\\.", "year": 2015}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom."], "venue": "arXiv preprint arXiv:1509.06664 .", "citeRegEx": "Rockt\u00e4schel et al\\.,? 2015", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Shiftreduce dependency DAG parsing", "author": ["Kenji Sagae", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of Coling", "citeRegEx": "Sagae and Tsujii.,? \\Q2008\\E", "shortCiteRegEx": "Sagae and Tsujii.", "year": 2008}, {"title": "Online graph planarisation for synchronous parsing of semantic and syntactic dependencies", "author": ["Ivan Titov", "James Henderson", "Paola Merlo", "Gabriele Musillo."], "venue": "IJCAI. pages 1562\u20131567.", "citeRegEx": "Titov et al\\.,? 2009", "shortCiteRegEx": "Titov et al\\.", "year": 2009}, {"title": "Stochastic HPSG parse disambiguation using the redwoods corpus", "author": ["Kristina Toutanova", "Christopher D. Manning", "Dan Flickinger", "Stephan Oepen."], "venue": "Research on Language and Computation 3(1):83\u2013105.", "citeRegEx": "Toutanova et al\\.,? 2005", "shortCiteRegEx": "Toutanova et al\\.", "year": 2005}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Advances in Neural Information Processing Systems 28. pages 2692\u2013 2700. http://papers.nips.cc/paper/5866-pointernetworks.pdf.", "citeRegEx": "Vinyals et al\\.,? 2015a", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems. pages 2755\u20132763.", "citeRegEx": "Vinyals et al\\.,? 2015b", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Camr at semeval2016 task 8: An extended transition-based amr parser", "author": ["Chuan Wang", "Sameer Pradhan", "Xiaoman Pan", "Heng Ji", "Nianwen Xue."], "venue": "Proceedings of SemEval. pages 1173\u2013 1178. http://www.aclweb.org/anthology/S16-1181.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Boosting transition-based AMR parsing with refined actions and auxiliary analyzers", "author": ["Chuan Wang", "Nianwen Xue", "Sameer Pradhan."], "venue": "Proceedings of ACL (2). pages 857\u2013862. http://www.aclweb.org/anthology/P15-2141.pdf.", "citeRegEx": "Wang et al\\.,? 2015a", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "A transition-based algorithm for AMR parsing", "author": ["Chuan Wang", "Nianwen Xue", "Sameer Pradhan."], "venue": "Proceedings of NAACL 2015. pages 366\u2013375. http://aclweb.org/anthology/N/N15/N151040.pdf.", "citeRegEx": "Wang et al\\.,? 2015b", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Transition-Based Parsing for Large-Scale Head-Driven Phrase Structure Grammars", "author": ["Gisle Ytrest\u00f8l."], "venue": "Ph.D. thesis, University of Oslo.", "citeRegEx": "Ytrest\u00f8l.,? 2012", "shortCiteRegEx": "Ytrest\u00f8l.", "year": 2012}, {"title": "Large-scale corpus-driven PCFG approximation of an HPSG", "author": ["Yi Zhang", "Hans-Ulrich Krieger."], "venue": "Proceedings of the 12th international conference on parsing technologies. Association for Computational Linguistics, pages 198\u2013208.", "citeRegEx": "Zhang and Krieger.,? 2011", "shortCiteRegEx": "Zhang and Krieger.", "year": 2011}, {"title": "Efficiency in unification-based n-best parsing", "author": ["Yi Zhang", "Stephan Oepen", "John Carroll."], "venue": "Proceedings of IWPT . pages 48\u201359. http://www.aclweb.org/anthology/W/W07/W072207.", "citeRegEx": "Zhang et al\\.,? 2007", "shortCiteRegEx": "Zhang et al\\.", "year": 2007}, {"title": "Robust parsing, meaning composition, and evaluation: Integrating grammar approximation, default unification, and elementary semantic dependencies", "author": ["Yi Zhang", "Stephan Oepen", "Rebecca Dridan", "Dan Flickinger", "Hans-Ulrich Krieger."], "venue": "Un-", "citeRegEx": "Zhang et al\\.,? 2014", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 25, "context": "2015), question answering (Hermann et al., 2015) and textual entailment (Rockt\u00e4schel et al.", "startOffset": 26, "endOffset": 48}, {"referenceID": 48, "context": ", 2015) and textual entailment (Rockt\u00e4schel et al., 2015).", "startOffset": 31, "endOffset": 57}, {"referenceID": 22, "context": "which serves as the semantic representation of the English Resource Grammar (ERG) (Flickinger, 2000).", "startOffset": 82, "endOffset": 100}, {"referenceID": 51, "context": "forming disambiguation with a maximum entropy model (Toutanova et al., 2005; Zhang et al., 2007); this approach has high precision but incomplete coverage.", "startOffset": 52, "endOffset": 96}, {"referenceID": 59, "context": "forming disambiguation with a maximum entropy model (Toutanova et al., 2005; Zhang et al., 2007); this approach has high precision but incomplete coverage.", "startOffset": 52, "endOffset": 96}, {"referenceID": 41, "context": "We develop parsers for two graph-based conversions of MRS, Elementary Dependency Structure (EDS) (Oepen and L\u00f8nning, 2006) and Dependency MRS (DMRS) (Copestake, 2009), of which the latter is inter-convertible with MRS.", "startOffset": 97, "endOffset": 122}, {"referenceID": 9, "context": "We develop parsers for two graph-based conversions of MRS, Elementary Dependency Structure (EDS) (Oepen and L\u00f8nning, 2006) and Dependency MRS (DMRS) (Copestake, 2009), of which the latter is inter-convertible with MRS.", "startOffset": 149, "endOffset": 166}, {"referenceID": 2, "context": "Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a graph-based semantic representation that shares the goals of MRS.", "startOffset": 38, "endOffset": 62}, {"referenceID": 21, "context": "of AMR parsers have been developed (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Damonte et al., 2017), but corpora are still under active development and low inter-annotator agreement places on upper bound of 83% F1 on expected parser performance (Banarescu et al.", "startOffset": 35, "endOffset": 120}, {"referenceID": 56, "context": "of AMR parsers have been developed (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Damonte et al., 2017), but corpora are still under active development and low inter-annotator agreement places on upper bound of 83% F1 on expected parser performance (Banarescu et al.", "startOffset": 35, "endOffset": 120}, {"referenceID": 1, "context": "of AMR parsers have been developed (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Damonte et al., 2017), but corpora are still under active development and low inter-annotator agreement places on upper bound of 83% F1 on expected parser performance (Banarescu et al.", "startOffset": 35, "endOffset": 120}, {"referenceID": 15, "context": "of AMR parsers have been developed (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Damonte et al., 2017), but corpora are still under active development and low inter-annotator agreement places on upper bound of 83% F1 on expected parser performance (Banarescu et al.", "startOffset": 35, "endOffset": 120}, {"referenceID": 2, "context": ", 2017), but corpora are still under active development and low inter-annotator agreement places on upper bound of 83% F1 on expected parser performance (Banarescu et al., 2013).", "startOffset": 153, "endOffset": 177}, {"referenceID": 6, "context": "We apply our model to AMR parsing by introducing structure that is present explicitly in MRS but not in AMR (Buys and Blunsom, 2017).", "startOffset": 108, "endOffset": 132}, {"referenceID": 18, "context": "Parsers based on RNNs have achieved state-ofthe-art performance for dependency parsing (Dyer et al., 2015; Kiperwasser and Goldberg, 2016) and constituency parsing (Vinyals et al.", "startOffset": 87, "endOffset": 138}, {"referenceID": 30, "context": "Parsers based on RNNs have achieved state-ofthe-art performance for dependency parsing (Dyer et al., 2015; Kiperwasser and Goldberg, 2016) and constituency parsing (Vinyals et al.", "startOffset": 87, "endOffset": 138}, {"referenceID": 53, "context": ", 2015; Kiperwasser and Goldberg, 2016) and constituency parsing (Vinyals et al., 2015b; Dyer et al., 2016; Cross and Huang, 2016b).", "startOffset": 65, "endOffset": 131}, {"referenceID": 19, "context": ", 2015; Kiperwasser and Goldberg, 2016) and constituency parsing (Vinyals et al., 2015b; Dyer et al., 2016; Cross and Huang, 2016b).", "startOffset": 65, "endOffset": 131}, {"referenceID": 14, "context": ", 2015; Kiperwasser and Goldberg, 2016) and constituency parsing (Vinyals et al., 2015b; Dyer et al., 2016; Cross and Huang, 2016b).", "startOffset": 65, "endOffset": 131}, {"referenceID": 32, "context": "Sentence meaning is represented with rooted, labelled, connected, directed graphs (Kuhlmann and Oepen, 2016).", "startOffset": 82, "endOffset": 108}, {"referenceID": 12, "context": "Minimal Recursion Semantics (MRS) is a framework for computational semantics that can be used for parsing or generation (Copestake et al., 2005).", "startOffset": 120, "endOffset": 144}, {"referenceID": 46, "context": "MRS was designed to be integrated with feature-based grammars such as Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) or Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982).", "startOffset": 114, "endOffset": 137}, {"referenceID": 28, "context": "MRS was designed to be integrated with feature-based grammars such as Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) or Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982).", "startOffset": 174, "endOffset": 200}, {"referenceID": 22, "context": "MRS has been implement the English Resource Grammar (ERG) (Flickinger, 2000), a broad-coverage high-precision HPSG grammar.", "startOffset": 58, "endOffset": 76}, {"referenceID": 9, "context": "Copestake (2009) extended this conversion to avoid information loss,", "startOffset": 0, "endOffset": 17}, {"referenceID": 10, "context": "The resulting representation, Dependency MRS (DMRS), can be converted back to the original MRS, or used directly in MRS-based applications (Copestake et al., 2016).", "startOffset": 139, "endOffset": 163}, {"referenceID": 2, "context": "AMR (Banarescu et al., 2013) graphs can be represented in the same framework, despite a number of linguistic differences with MRS.", "startOffset": 4, "endOffset": 28}, {"referenceID": 42, "context": "AMR predicates are based on PropBank (Palmer et al., 2005), annotated as lemmas plus sense labels, but they form only a subset of concepts.", "startOffset": 37, "endOffset": 58}, {"referenceID": 53, "context": "Variants of this approach have been proposed for neural constituency parsing (Vinyals et al., 2015b), logical form pre-", "startOffset": 77, "endOffset": 100}, {"referenceID": 16, "context": "diction (Dong and Lapata, 2016; Jia and Liang, 2016) and AMR parsing (Barzdins and Gosko, 2016; Peng et al.", "startOffset": 8, "endOffset": 52}, {"referenceID": 26, "context": "diction (Dong and Lapata, 2016; Jia and Liang, 2016) and AMR parsing (Barzdins and Gosko, 2016; Peng et al.", "startOffset": 8, "endOffset": 52}, {"referenceID": 3, "context": "diction (Dong and Lapata, 2016; Jia and Liang, 2016) and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017).", "startOffset": 69, "endOffset": 114}, {"referenceID": 45, "context": "diction (Dong and Lapata, 2016; Jia and Liang, 2016) and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017).", "startOffset": 69, "endOffset": 114}, {"referenceID": 37, "context": "Transition-based parsing (Nivre, 2008) has been used extensively to predict dependency graphs in-", "startOffset": 25, "endOffset": 38}, {"referenceID": 49, "context": "We apply a variant of the arc-eager transition system that has been proposed for graph (as opposed to tree) parsing (Sagae and Tsujii, 2008; Titov et al., 2009; G\u00f3mez-Rodr\u0131\u0301guez and Nivre, 2010) to derive a transition-based parser for", "startOffset": 116, "endOffset": 194}, {"referenceID": 50, "context": "We apply a variant of the arc-eager transition system that has been proposed for graph (as opposed to tree) parsing (Sagae and Tsujii, 2008; Titov et al., 2009; G\u00f3mez-Rodr\u0131\u0301guez and Nivre, 2010) to derive a transition-based parser for", "startOffset": 116, "endOffset": 194}, {"referenceID": 24, "context": "We apply a variant of the arc-eager transition system that has been proposed for graph (as opposed to tree) parsing (Sagae and Tsujii, 2008; Titov et al., 2009; G\u00f3mez-Rodr\u0131\u0301guez and Nivre, 2010) to derive a transition-based parser for", "startOffset": 116, "endOffset": 194}, {"referenceID": 15, "context": "Damonte et al. (2017) proposed an arc-eager AMR parser, but", "startOffset": 0, "endOffset": 22}, {"referenceID": 20, "context": ", 2014) to tokenize and lemmatize sentences, and tag tokens with the Stanford Named Entity Recognizer (Finkel et al., 2005).", "startOffset": 102, "endOffset": 123}, {"referenceID": 42, "context": "The lexicon is restricted to Propbank (Palmer et al., 2005) predicates; for other concepts we extract a lexicon from the training data.", "startOffset": 38, "endOffset": 59}, {"referenceID": 27, "context": "We use a standard LSTM architecture without peephole connections (Jozefowicz et al., 2015).", "startOffset": 65, "endOffset": 90}, {"referenceID": 52, "context": "The alignment is predicted with a pointer network (Vinyals et al., 2015a).", "startOffset": 50, "endOffset": 73}, {"referenceID": 28, "context": "This approach is similar to the features proposed by Kiperwasser and Goldberg (2016) and Cross and Huang (2016a) for dependency parsing, although they do not use RNN decoders.", "startOffset": 53, "endOffset": 85}, {"referenceID": 13, "context": "This approach is similar to the features proposed by Kiperwasser and Goldberg (2016) and Cross and Huang (2016a) for dependency parsing, although they do not use RNN decoders.", "startOffset": 89, "endOffset": 113}, {"referenceID": 5, "context": "Our implementation of the stack-based model enables batch processing in static computation graphs, similar to Bowman et al. (2016). We maintain a stack of alignment indexes for each element in the batch, which is updated inside the computa-", "startOffset": 110, "endOffset": 131}, {"referenceID": 22, "context": "case the ERG (Flickinger, 2000).", "startOffset": 13, "endOffset": 31}, {"referenceID": 51, "context": "Maximum entropy models are used to score the derivations in order to find the most likely parse (Toutanova et al., 2005).", "startOffset": 96, "endOffset": 120}, {"referenceID": 8, "context": "This approach is implemented in the PET (Callmeier, 2000) and ACE2 parsers.", "startOffset": 40, "endOffset": 57}, {"referenceID": 58, "context": "One proposed approach learns a PCFG grammar to approximate the HPSG derivations (Zhang and Krieger, 2011; Zhang et al., 2014).", "startOffset": 80, "endOffset": 125}, {"referenceID": 60, "context": "One proposed approach learns a PCFG grammar to approximate the HPSG derivations (Zhang and Krieger, 2011; Zhang et al., 2014).", "startOffset": 80, "endOffset": 125}, {"referenceID": 8, "context": "This approach is implemented in the PET (Callmeier, 2000) and ACE2 parsers. There have also been some efforts to develop robust MRS parsers. One proposed approach learns a PCFG grammar to approximate the HPSG derivations (Zhang and Krieger, 2011; Zhang et al., 2014). MRS is then extracted with robust unification to compose potentially incompatible feature structures, although that still fails for a small proportion of sentences. The model is trained on a large corpus of Wikipedia text parsed with the grammar-based parser. Ytrest\u00f8l (2012) proposed a transition-based approach to HPSG parsing that produces derivations from which both syntactic and semantic (MRS) parses can be extracted.", "startOffset": 41, "endOffset": 544}, {"referenceID": 21, "context": "Flanigan et al. (2014) proposed a twostage parser that first predicts concepts or subgraphs corresponding to sentence segments, and then parses these concepts into a graph structure.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "(2015) proposed a parser based on syntaxbased machine translation (MT), while AMR has also been integrated into CCG Semantic Parsing (Artzi et al., 2015; Misra and Artzi, 2016).", "startOffset": 133, "endOffset": 176}, {"referenceID": 36, "context": "(2015) proposed a parser based on syntaxbased machine translation (MT), while AMR has also been integrated into CCG Semantic Parsing (Artzi et al., 2015; Misra and Artzi, 2016).", "startOffset": 133, "endOffset": 176}, {"referenceID": 43, "context": "Pust et al. (2015) proposed a parser based on syntaxbased machine translation (MT), while AMR has also been integrated into CCG Semantic Parsing (Artzi et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "(2015) proposed a parser based on syntaxbased machine translation (MT), while AMR has also been integrated into CCG Semantic Parsing (Artzi et al., 2015; Misra and Artzi, 2016). Recently Damonte et al. (2017) and Peng et al.", "startOffset": 134, "endOffset": 209}, {"referenceID": 1, "context": "(2015) proposed a parser based on syntaxbased machine translation (MT), while AMR has also been integrated into CCG Semantic Parsing (Artzi et al., 2015; Misra and Artzi, 2016). Recently Damonte et al. (2017) and Peng et al. (2017) proposed AMR parsers based on neural networks.", "startOffset": 134, "endOffset": 232}, {"referenceID": 23, "context": "DeepBank (Flickinger et al., 2012) is an HPSG and MRS annotation of the Penn Treebank Wall Street Journal (WSJ) corpus.", "startOffset": 9, "endOffset": 34}, {"referenceID": 38, "context": "It was developed following an approach known as dynamic treebanking (Oepen et al., 2004) that couples treebank annotation with grammar development, in this case", "startOffset": 68, "endOffset": 88}, {"referenceID": 4, "context": "71 for AMR (Bender et al., 2015).", "startOffset": 11, "endOffset": 32}, {"referenceID": 35, "context": "ing Shared Task (May, 2016).", "startOffset": 16, "endOffset": 27}, {"referenceID": 21, "context": "We obtain alignments using the rule-based JAMR aligner (Flanigan et al., 2014).", "startOffset": 55, "endOffset": 78}, {"referenceID": 7, "context": "The Smatch metric (Cai and Knight, 2013), proposed for evaluating AMR graphs, also measures graph overlap, but does not rely on sentence alignments to determine the correspondences between graph nodes.", "startOffset": 18, "endOffset": 40}, {"referenceID": 29, "context": "For training we use Adam (Kingma and Ba, 2015) with learning rate 0.", "startOffset": 25, "endOffset": 46}, {"referenceID": 43, "context": "0 (Pascanu et al., 2013).", "startOffset": 2, "endOffset": 24}, {"referenceID": 34, "context": "with pre-trained order-sensitive embeddings (Ling et al., 2015).", "startOffset": 44, "endOffset": 63}, {"referenceID": 32, "context": "An EDS corpus which consists of about 95% of the DeepBank data has also been released6, with the goal of enabling comparison with other semantic graph parsing formalisms, including CCG dependencies and Prague Semantic Dependencies, on the same data set (Kuhlmann and Oepen, 2016).", "startOffset": 253, "endOffset": 279}, {"referenceID": 15, "context": "However, concept prediction remains the most important weakness of the model; Damonte et al. (2017) reports that state-of-the-art AMR parsers score 83% on concept prediction.", "startOffset": 78, "endOffset": 100}, {"referenceID": 21, "context": "best neural model outperforms the baseline JAMR parser (Flanigan et al., 2014), but still lags behind the performance of state-of-the-art AMR parsers such as CAMR (Wang et al.", "startOffset": 55, "endOffset": 78}, {"referenceID": 54, "context": ", 2014), but still lags behind the performance of state-of-the-art AMR parsers such as CAMR (Wang et al., 2016) and AMR Eager (Damonte et al.", "startOffset": 92, "endOffset": 111}, {"referenceID": 15, "context": ", 2016) and AMR Eager (Damonte et al., 2017).", "startOffset": 22, "endOffset": 44}, {"referenceID": 3, "context": "Our attention-based encoder-decoder model already outperforms previous sequence-to-sequence AMR parsers (Barzdins and Gosko, 2016; Peng et al., 2017), and the arc-eager model boosts accuracy further.", "startOffset": 104, "endOffset": 149}, {"referenceID": 45, "context": "Our attention-based encoder-decoder model already outperforms previous sequence-to-sequence AMR parsers (Barzdins and Gosko, 2016; Peng et al., 2017), and the arc-eager model boosts accuracy further.", "startOffset": 104, "endOffset": 149}, {"referenceID": 15, "context": "54 Damonte et al. (2017) 64", "startOffset": 3, "endOffset": 25}, {"referenceID": 3, "context": "(2017) 52 Barzdins and Gosko (2016) 43.", "startOffset": 10, "endOffset": 36}, {"referenceID": 44, "context": "model (Peng and Gildea, 2016) which is comparable as it does not make extensive use of external resources.", "startOffset": 6, "endOffset": 29}], "year": 2017, "abstractText": "Parsing sentences to linguisticallyexpressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focussed almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69% Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.1", "creator": "LaTeX with hyperref package"}}}