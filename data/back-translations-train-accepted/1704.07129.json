{"id": "1704.07129", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "An Analysis of Action Recognition Datasets for Language and Vision Tasks", "abstract": "A large amount of recent research has focused on tasks that combine language and vision, resulting in a proliferation of datasets and methods. One such task is action recognition, whose applications include image annotation, scene under- standing and image retrieval. In this survey, we categorize the existing ap- proaches based on how they conceptualize this problem and provide a detailed review of existing datasets, highlighting their di- versity as well as advantages and disad- vantages. We focus on recently devel- oped datasets which link visual informa- tion with linguistic resources and provide a fine-grained syntactic and semantic anal- ysis of actions in images.", "histories": [["v1", "Mon, 24 Apr 2017 10:38:23 GMT  (332kb,D)", "http://arxiv.org/abs/1704.07129v1", "To appear in Proceedings of ACL 2017, 8 pages"]], "COMMENTS": "To appear in Proceedings of ACL 2017, 8 pages", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["spandana gella", "frank keller"], "accepted": true, "id": "1704.07129"}, "pdf": {"name": "1704.07129.pdf", "metadata": {"source": "CRF", "title": "An Analysis of Action Recognition Datasets for Language and Vision Tasks", "authors": ["Spandana Gella", "Frank Keller"], "emails": ["S.Gella@sms.ed.ac.uk,", "keller@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is a matter of a way in which people put themselves and themselves at the centre, in which they put themselves at the centre, in which they put themselves at the centre, in which they put themselves at the centre, in which they put themselves at the centre, in which they put themselves at the centre, in which they put themselves at the centre, in which they put themselves at the centre, in which they put themselves at the centre, in which they put themselves at the centre, in which they put themselves at the centre."}, {"heading": "2 Datasets for Action Recognition", "text": "We note that the number of verbs included in these datasets is often smaller than the number of reported action names (see Table 1, Columns # V and # L), and in many cases the action title includes an object reference. Some of the first action title records, such as the Ikizler and Willow datasets (Ikizler et al., 2008; Delaitre et al., 2010), had action names such as throwing and running; they were taken from the sports field and showed diversity in camera position, background, and resolution. Then, data sets were created to capture variations in human poses in the sports field for actions such as tennis and cricket bowling; typically, features based on poses and body parts were used to build models (Gupta et al., 2009). Further data sets were created based on the intuition that object information was selected to help with the modeling of the Yain Akin 2007, Yao and Fayao (2010)."}, {"heading": "2.1 Identifying Visual Verbs and Verb Senses", "text": "The limitations of early data sets (small size, domain specificity, and the use of ad hoc labels combining verb and object) have recently been addressed in a series of broad datasets that provide linguistically motivated labels. Frequently, these datasets use existing linguistic resources such as VerbNet (Schuler, 2005), OntoNotes (Hovy et al., 2006), and FrameNet (Baker et al., 1998) to classify verbs and their senses, allowing for a more general, semantically motivated treatment of verbs and verb phrases, and also taking into account that not all verbs can be represented. For example, abstract verbs such as acceptance and appropriation are not representable at all, while other verbs have both representable and non-representable senses: the game is not representable in the game of emotions, but representable in the game of senses and playing with sports."}, {"heading": "2.2 Datasets Beyond Action Classification", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2.3 Diversity in Datasets", "text": "With the exception of a few datasets such as COCO-a, VerSe, imSitu, all datasets have been manually selected for action detection or focus on actions in specific areas such as sports. Alternatively, many datasets only cover actions relevant to specific object categories such as musical instruments, animals, and vehicles. In the real world, people interact with many more objects and perform actions that are relevant to a wide range of areas such as personal care, household activities, or socializing, limiting the variety and reach of existing datasets for action detection. Recently proposed datasets address this problem in part by using generic language resources to broaden the vocabulary of verbs in action labels. The diversity problem has also been highlighted and addressed in the most recent datasets on video action detection (Caba Heilbron et al., 2015; Sigurdsson et al., 2016), which show different word categories and the specificity of Ferraro datasets by their distribution."}, {"heading": "3 Relevant Language and Vision Tasks", "text": "Template based description generation systems for both videos and images based on identification subject-verb-object triples and use language modeling to generate or ranking descriptions (Yang et al., 2011; Thomason et al., 2014; Bernardi et al., 2016). Understanding actions also plays an important role in answering questions, especially when the question relates to an action depicted in the image. There are some specifically curated question-answer systems that target human activities or relationships between objects (Yu et al., 2015). Mallya and Lazebnik (2016) have shown that systems trained on action recognition data sets could be used to improve the accuracy of systems for visual response to questions related to human activity and human-object relationships. Action recognition data sets could be used to learn visually similar actions to those that panda interact with and feed a panda to a baby's text, which can be learned from Ramathan or from a baby alone (2015)."}, {"heading": "4 Action Recognition Models", "text": "Most of the models proposed for action classification and human-object interaction are based on the identification of additional higher-level visual cues present in the image, including human bodies or body parts (Ikizler et al., 2008; Gupta et al., 2009; Yao et al., 2011; Andriluka et al., 2014), objects (Gupta et al., 2009), and scenes (Li and Fei-Fei, 2007). Higher-level visual cues are derived from features taken from the image, such as Scale Invariant Feature Transforms (SIFT), Histogram of Oriented Gradients (HOG), and Spatial Envelopes (Gist) features (Lowe, 1999; Dalal and Triggs, 2005), which are useful for identifying key points, recognizing people, and recognizing scenes or background information in images."}, {"heading": "5 Discussion", "text": "Linguistic resources such as WordNet, OntoNotes and FrameNet play a key role in the sense of ambiguity and semantic role designation. Visual plot disambiguation and visual semantic role designation tasks are extensions of their textual counterparts, in which context is provided as image rather than text. Linguistic resources must therefore play a key role if we are to make rapid progress in these language and vision tasks, despite the fact that the WordNet-Noun hierarchy (for example) relies on linguistic resources (Chao et al., 2015; Gella et al., 2016; Yatskar et al., 2016) that the WordNet-Noun hierarchy (for example) has played an important role in recent advances in object recognition."}, {"heading": "6 Conclusions", "text": "In this paper, we have outlined the evolution of action detection data sets and tasks from simple ad hoc labels to fine-grained annotation of verb semantics. It is encouraging to see the recent increase in data sets that deal with meaning ambiguity and comment on semantic roles while using standard linguistic resources. A major problem with existing data sets is their limited coverage and distorted distribution of verbs or verb senses. Another challenge is the inconsistency in annotation schemes and task definitions between data sets. Thus, Chao et al. (2015) used WordNet senses as interaction labels, while Gella et al. (2016) used the more coarse-grained OntoNotes senses. Yatskar et al. (2016) used FrameNet frames for semantic role annotations, while Gupta and Malik (2015) used manually-grained roles, when we should develop diesel-grained models."}], "references": [{"title": "2d human pose estimation: New benchmark and state of the art analysis", "author": ["Mykhaylo Andriluka", "Leonid Pishchulin", "Peter Gehler", "Bernt Schiele."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 3686\u20133693.", "citeRegEx": "Andriluka et al\\.,? 2014", "shortCiteRegEx": "Andriluka et al\\.", "year": 2014}, {"title": "VQA: visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh."], "venue": "2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, De-", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "The berkeley framenet project", "author": ["Collin F Baker", "Charles J Fillmore", "John B Lowe."], "venue": "Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics-", "citeRegEx": "Baker et al\\.,? 1998", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Automatic description generation from images: A survey of models, datasets, and evaluation", "author": ["Raffaella Bernardi", "Ruket Cakici", "Desmond Elliott", "Aykut Erdem", "Erkut Erdem", "Nazli Ikizler-Cinbis", "Frank Keller", "Adrian Muscat", "Barbara Plank"], "venue": null, "citeRegEx": "Bernardi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bernardi et al\\.", "year": 2016}, {"title": "Activitynet: A large-scale video benchmark for human activity understanding", "author": ["Fabian Caba Heilbron", "Victor Escorcia", "Bernard Ghanem", "Juan Carlos Niebles."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-", "citeRegEx": "Heilbron et al\\.,? 2015", "shortCiteRegEx": "Heilbron et al\\.", "year": 2015}, {"title": "HICO: A benchmark for recognizing human-object interactions in images", "author": ["Yu-Wei Chao", "Zhan Wang", "Yugeng He", "Jiaxuan Wang", "Jia Deng."], "venue": "2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015.", "citeRegEx": "Chao et al\\.,? 2015", "shortCiteRegEx": "Chao et al\\.", "year": 2015}, {"title": "Microsoft COCO captions: Data collection and evaluation server", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C. Lawrence Zitnick."], "venue": "CoRR abs/1504.00325.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Histograms of oriented gradients for human detection", "author": ["Navneet Dalal", "Bill Triggs."], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on. IEEE, volume 1, pages 886\u2013893.", "citeRegEx": "Dalal and Triggs.,? 2005", "shortCiteRegEx": "Dalal and Triggs.", "year": 2005}, {"title": "Recognizing human actions in still images: a study of bag-of-features and part-based representations", "author": ["Vincent Delaitre", "Ivan Laptev", "Josef Sivic."], "venue": "BMVC 2010-21st British Machine Vision Conference.", "citeRegEx": "Delaitre et al\\.,? 2010", "shortCiteRegEx": "Delaitre et al\\.", "year": 2010}, {"title": "The Pascal visual object classes (VOC) challenge", "author": ["Mark Everingham", "Luc J. Van Gool", "Christopher K.I. Williams", "John M. Winn", "Andrew Zisserman."], "venue": "International Journal of Computer Vision 88(2):303\u2013338.", "citeRegEx": "Everingham et al\\.,? 2010", "shortCiteRegEx": "Everingham et al\\.", "year": 2010}, {"title": "A survey of current datasets for vision and language research", "author": ["Francis Ferraro", "Nasrin Mostafazadeh", "Ting-Hao (Kenneth) Huang", "Lucy Vanderwende", "Jacob Devlin", "Michel Galley", "Margaret Mitchell"], "venue": "In Proceedings of the 2015 Conference", "citeRegEx": "Ferraro et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ferraro et al\\.", "year": 2015}, {"title": "Computational studies of human motion: Part 1, tracking and motion synthesis. Foundations and Trends in Computer Graphics and Vision 1(2/3)", "author": ["David A. Forsyth", "Okan Arikan", "Leslie Ikemoto", "James F. O\u2019Brien", "Deva Ramanan"], "venue": null, "citeRegEx": "Forsyth et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Forsyth et al\\.", "year": 2005}, {"title": "Unsupervised visual sense disambiguation for verbs using multimodal embeddings", "author": ["Spandana Gella", "Mirella Lapata", "Frank Keller."], "venue": "Proceedings of the 2016 Conference on North American Chapter of the Association for Computational Lin-", "citeRegEx": "Gella et al\\.,? 2016", "shortCiteRegEx": "Gella et al\\.", "year": 2016}, {"title": "Observing human-object interactions: Using spatial and functional compatibility for recognition", "author": ["Abhinav Gupta", "Aniruddha Kembhavi", "Larry S. Davis."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 31(10):1775\u20131789.", "citeRegEx": "Gupta et al\\.,? 2009", "shortCiteRegEx": "Gupta et al\\.", "year": 2009}, {"title": "Visual semantic role labeling", "author": ["Saurabh Gupta", "Jitendra Malik."], "venue": "CoRR abs/1505.04474.", "citeRegEx": "Gupta and Malik.,? 2015", "shortCiteRegEx": "Gupta and Malik.", "year": 2015}, {"title": "Ontonotes: The 90% solution", "author": ["Eduard H. Hovy", "Mitchell P. Marcus", "Martha Palmer", "Lance A. Ramshaw", "Ralph M. Weischedel."], "venue": "Human Language Technology Conference of the North American Chapter of the Association of Compu-", "citeRegEx": "Hovy et al\\.,? 2006", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "Recognizing actions from still images", "author": ["Nazli Ikizler", "Ramazan Gokberk Cinbis", "Selen Pehlivan", "Pinar Duygulu."], "venue": "19th International Conference on Pattern Recognition (ICPR 2008), December 811, 2008, Tampa, Florida, USA. pages 1\u20134.", "citeRegEx": "Ikizler et al\\.,? 2008", "shortCiteRegEx": "Ikizler et al\\.", "year": 2008}, {"title": "Object, scene and actions: Combining multiple features for human action recognition", "author": ["Nazli Ikizler-Cinbis", "Stan Sclaroff."], "venue": "European conference on computer vision. Springer, pages 494\u2013507.", "citeRegEx": "Ikizler.Cinbis and Sclaroff.,? 2010", "shortCiteRegEx": "Ikizler.Cinbis and Sclaroff.", "year": 2010}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma"], "venue": null, "citeRegEx": "Krishna et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krishna et al\\.", "year": 2016}, {"title": "Exploiting language models to recognize unseen actions", "author": ["Dieu Thu Le", "Raffaella Bernardi", "Jasper Uijlings."], "venue": "Proceedings of the 3rd ACM conference on International conference on multimedia retrieval. ACM, pages 231\u2013238.", "citeRegEx": "Le et al\\.,? 2013", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "Proceedings of the Third Workshop on Vision and Language, Dublin City University and the Association for Computational Linguistics, chapter TUHOI: Trento Universal Human Object Interaction", "author": ["Dieu-Thu Le", "Jasper Uijlings", "Raffaella Bernardi"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "English verb classes and alternations: A preliminary investigation", "author": ["Beth Levin."], "venue": "University of Chicago Press.", "citeRegEx": "Levin.,? 1993", "shortCiteRegEx": "Levin.", "year": 1993}, {"title": "What, where and who? classifying events by scene and object recognition", "author": ["Li-Jia Li", "Li Fei-Fei."], "venue": "Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on. IEEE, pages 1\u20138.", "citeRegEx": "Li and Fei.Fei.,? 2007", "shortCiteRegEx": "Li and Fei.Fei.", "year": 2007}, {"title": "Object recognition from local scale-invariant features", "author": ["David G Lowe."], "venue": "Computer vision, 1999. The proceedings of the seventh IEEE international conference on. Ieee, volume 2, pages 1150\u20131157.", "citeRegEx": "Lowe.,? 1999", "shortCiteRegEx": "Lowe.", "year": 1999}, {"title": "Visual relationship detection with language priors", "author": ["Cewu Lu", "Ranjay Krishna", "Michael Bernstein", "Li Fei-Fei."], "venue": "European Conference on Computer Vision. Springer, pages 852\u2013869.", "citeRegEx": "Lu et al\\.,? 2016", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Learning models for actions and person-object interactions with transfer to question answering", "author": ["Arun Mallya", "Svetlana Lazebnik."], "venue": "European Conference on Computer Vision. Springer, pages 414\u2013428.", "citeRegEx": "Mallya and Lazebnik.,? 2016", "shortCiteRegEx": "Mallya and Lazebnik.", "year": 2016}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller."], "venue": "Communications of the ACM 38(11):39\u2013", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "A vision of \u201dvision and language\u201d comprises action: An example from road traffic", "author": ["Hans-Hellmut Nagel."], "venue": "Artif. Intell. Rev. 8(2-3):189\u2013214.", "citeRegEx": "Nagel.,? 1994", "shortCiteRegEx": "Nagel.", "year": 1994}, {"title": "Babelnet: Building a very large multilingual semantic network", "author": ["Roberto Navigli", "Simone Paolo Ponzetto."], "venue": "ACL 2010, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, July 11-16, 2010, Uppsala, Swe-", "citeRegEx": "Navigli and Ponzetto.,? 2010", "shortCiteRegEx": "Navigli and Ponzetto.", "year": 2010}, {"title": "Learning semantic relationships for better action retrieval in images", "author": ["Vignesh Ramanathan", "Congcong Li", "Jia Deng", "Wei Han", "Zhen Li", "Kunlong Gu", "Yang Song", "Samy Bengio", "Chuck Rossenberg", "Li Fei-Fei."], "venue": "Proceedings of the IEEE Con-", "citeRegEx": "Ramanathan et al\\.,? 2015", "shortCiteRegEx": "Ramanathan et al\\.", "year": 2015}, {"title": "Describing common human visual actions in images", "author": ["Matteo Ruggero Ronchi", "Pietro Perona."], "venue": "Proceedings of the British Machine Vision Conference (BMVC 2015). BMVA Press, pages 52.1\u2013", "citeRegEx": "Ronchi and Perona.,? 2015", "shortCiteRegEx": "Ronchi and Perona.", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael S. Bernstein", "Alexander C. Berg", "Fei-Fei Li."], "venue": "Interna-", "citeRegEx": "Russakovsky et al\\.,? 2015", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "VerbNet: A broadcoverage, comprehensive verb lexicon", "author": ["Karin Kipper Schuler."], "venue": "Ph.D. thesis, University of Pennsylvania.", "citeRegEx": "Schuler.,? 2005", "shortCiteRegEx": "Schuler.", "year": 2005}, {"title": "Hollywood in homes: Crowdsourcing data collection for activity understanding", "author": ["Gunnar A Sigurdsson", "G\u00fcl Varol", "Xiaolong Wang", "Ali Farhadi", "Ivan Laptev", "Abhinav Gupta."], "venue": "European Conference on Computer Vision. Springer, pages 510\u2013526.", "citeRegEx": "Sigurdsson et al\\.,? 2016", "shortCiteRegEx": "Sigurdsson et al\\.", "year": 2016}, {"title": "Convolutional learning of spatio-temporal features", "author": ["Graham W Taylor", "Rob Fergus", "Yann LeCun", "Christoph Bregler."], "venue": "European conference on computer vision. Springer, pages 140\u2013153.", "citeRegEx": "Taylor et al\\.,? 2010", "shortCiteRegEx": "Taylor et al\\.", "year": 2010}, {"title": "Integrating language and vision to generate natural language descriptions of videos in the wild", "author": ["Jesse Thomason", "Subhashini Venugopalan", "Sergio Guadarrama", "Kate Saenko", "Raymond J. Mooney."], "venue": "COLING 2014, 25th International Con-", "citeRegEx": "Thomason et al\\.,? 2014", "shortCiteRegEx": "Thomason et al\\.", "year": 2014}, {"title": "Multi-modal representations for improved bilingual lexicon learning", "author": ["Ivan Vuli\u0107", "Douwe Kiela", "Stephen Clark", "MarieFrancine Moens."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. ACL, pages", "citeRegEx": "Vuli\u0107 et al\\.,? 2016", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2016}, {"title": "Grounded semantic role labeling", "author": ["Shaohua Yang", "Qiaozi Gao", "Changsong Liu", "Caiming Xiong", "Song-Chun Zhu", "Joyce Y. Chai."], "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Yezhou Yang", "Ching Lik Teo", "Hal Daum\u00e9 III", "Yiannis Aloimonos."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational", "citeRegEx": "Yang et al\\.,? 2011", "shortCiteRegEx": "Yang et al\\.", "year": 2011}, {"title": "Grouplet: A structured image representation for recognizing human and object interactions", "author": ["Bangpeng Yao", "Li Fei-Fei."], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, pages 9\u201316.", "citeRegEx": "Yao and Fei.Fei.,? 2010", "shortCiteRegEx": "Yao and Fei.Fei.", "year": 2010}, {"title": "Human action recognition by learning bases of action attributes and parts", "author": ["Bangpeng Yao", "Xiaoye Jiang", "Aditya Khosla", "Andy Lai Lin", "Leonidas Guibas", "Li Fei-Fei."], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on. IEEE, pages", "citeRegEx": "Yao et al\\.,? 2011", "shortCiteRegEx": "Yao et al\\.", "year": 2011}, {"title": "Situation recognition: Visual semantic role labeling for image understanding", "author": ["Mark Yatskar", "Luke Zettlemoyer", "Ali Farhadi."], "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 26-", "citeRegEx": "Yatskar et al\\.,? 2016", "shortCiteRegEx": "Yatskar et al\\.", "year": 2016}, {"title": "Visual madlibs: Fill in the blank description generation and question answering", "author": ["Licheng Yu", "Eunbyung Park", "Alexander C. Berg", "Tamara L. Berg."], "venue": "2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, De-", "citeRegEx": "Yu et al\\.,? 2015", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Learning deep features for discriminative localization", "author": ["Bolei Zhou", "Aditya Khosla", "Agata Lapedriza", "Aude Oliva", "Antonio Torralba."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 2921\u20132929.", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 27, "context": "It has been widely studied in computer vision, often on videos (Nagel, 1994; Forsyth et al., 2005),", "startOffset": 63, "endOffset": 98}, {"referenceID": 11, "context": "It has been widely studied in computer vision, often on videos (Nagel, 1994; Forsyth et al., 2005),", "startOffset": 63, "endOffset": 98}, {"referenceID": 34, "context": "where motion and temporal information provide cues for recognizing actions (Taylor et al., 2010).", "startOffset": 75, "endOffset": 96}, {"referenceID": 16, "context": "Due to the absence of motion cues and temporal features (Ikizler et al., 2008) action recognition from stills is more challenging.", "startOffset": 56, "endOffset": 78}, {"referenceID": 29, "context": "portant for classifying actions, but most methods are not scalable to larger numbers of actions (Ramanathan et al., 2015).", "startOffset": 96, "endOffset": 121}, {"referenceID": 20, "context": "To address these issues and also to understand the range of possible interactions between humans and objects, the human\u2013object interaction (HOI) detection task has been proposed, in which all possible interactions between a human and a given object have to be identified (Le et al., 2014; Chao et al., 2015; Lu et al., 2016).", "startOffset": 271, "endOffset": 324}, {"referenceID": 5, "context": "To address these issues and also to understand the range of possible interactions between humans and objects, the human\u2013object interaction (HOI) detection task has been proposed, in which all possible interactions between a human and a given object have to be identified (Le et al., 2014; Chao et al., 2015; Lu et al., 2016).", "startOffset": 271, "endOffset": 324}, {"referenceID": 24, "context": "To address these issues and also to understand the range of possible interactions between humans and objects, the human\u2013object interaction (HOI) detection task has been proposed, in which all possible interactions between a human and a given object have to be identified (Le et al., 2014; Chao et al., 2015; Lu et al., 2016).", "startOffset": 271, "endOffset": 324}, {"referenceID": 14, "context": "Recent work (Gupta and Malik, 2015; Yatskar et al., 2016) has filled this gap by proposing the task of visual semantic role la-", "startOffset": 12, "endOffset": 57}, {"referenceID": 41, "context": "Recent work (Gupta and Malik, 2015; Yatskar et al., 2016) has filled this gap by proposing the task of visual semantic role la-", "startOffset": 12, "endOffset": 57}, {"referenceID": 16, "context": "A few of the first action recognition datasets such as the Ikizler and Willow datasets (Ikizler et al., 2008; Delaitre et al., 2010) had action labels such as throwing and running; they were taken from", "startOffset": 87, "endOffset": 132}, {"referenceID": 8, "context": "A few of the first action recognition datasets such as the Ikizler and Willow datasets (Ikizler et al., 2008; Delaitre et al., 2010) had action labels such as throwing and running; they were taken from", "startOffset": 87, "endOffset": 132}, {"referenceID": 13, "context": "tures based on poses and body parts were used to build models (Gupta et al., 2009).", "startOffset": 62, "endOffset": 82}, {"referenceID": 9, "context": "2010), which resulted in the use of action labels such as riding horse or riding bike (Everingham et al., 2010; Yao et al., 2011).", "startOffset": 86, "endOffset": 129}, {"referenceID": 40, "context": "2010), which resulted in the use of action labels such as riding horse or riding bike (Everingham et al., 2010; Yao et al., 2011).", "startOffset": 86, "endOffset": 129}, {"referenceID": 32, "context": "Often these datasets use existing linguistic resources such as VerbNet (Schuler, 2005), OntoNotes (Hovy et al.", "startOffset": 71, "endOffset": 86}, {"referenceID": 15, "context": "Often these datasets use existing linguistic resources such as VerbNet (Schuler, 2005), OntoNotes (Hovy et al., 2006) and FrameNet (Baker et al.", "startOffset": 98, "endOffset": 117}, {"referenceID": 29, "context": "The process of identifying depictable verbs or verb senses is used by Ronchi and Perona (2015), Gella et al.", "startOffset": 70, "endOffset": 95}, {"referenceID": 12, "context": "The process of identifying depictable verbs or verb senses is used by Ronchi and Perona (2015), Gella et al. (2016) and Yatskar et al.", "startOffset": 96, "endOffset": 116}, {"referenceID": 12, "context": "The process of identifying depictable verbs or verb senses is used by Ronchi and Perona (2015), Gella et al. (2016) and Yatskar et al. (2016) to identify visual verbs, visual verb senses, and the semantic roles of the participating objects respectively.", "startOffset": 96, "endOffset": 142}, {"referenceID": 12, "context": "The process of identifying depictable verbs or verb senses is used by Ronchi and Perona (2015), Gella et al. (2016) and Yatskar et al. (2016) to identify visual verbs, visual verb senses, and the semantic roles of the participating objects respectively. In all the cases the process of identifying visual verbs or senses is carried out by human annotators via crowd-sourcing platforms. Visualness labels for 935 OntoNotes verb senses corresponding to 154 verbs is provided by Gella et al. (2016), while Yatskar et al.", "startOffset": 96, "endOffset": 496}, {"referenceID": 12, "context": "The process of identifying depictable verbs or verb senses is used by Ronchi and Perona (2015), Gella et al. (2016) and Yatskar et al. (2016) to identify visual verbs, visual verb senses, and the semantic roles of the participating objects respectively. In all the cases the process of identifying visual verbs or senses is carried out by human annotators via crowd-sourcing platforms. Visualness labels for 935 OntoNotes verb senses corresponding to 154 verbs is provided by Gella et al. (2016), while Yatskar et al. (2016) provides visualness la-", "startOffset": 96, "endOffset": 525}, {"referenceID": 16, "context": "Dataset Task #L #V Obj Imgs Sen Des Cln ML Resource Example Labels Ikizler (Ikizler et al., 2008) AC 6 6 0 467 N N Y N \u2212 running, walking Sports Dataset (Gupta et al.", "startOffset": 75, "endOffset": 97}, {"referenceID": 13, "context": ", 2008) AC 6 6 0 467 N N Y N \u2212 running, walking Sports Dataset (Gupta et al., 2009) AC 6 6 4 300 N N Y N \u2212 tennis serve, cricket bowling Willow (Delaitre et al.", "startOffset": 63, "endOffset": 83}, {"referenceID": 8, "context": ", 2009) AC 6 6 4 300 N N Y N \u2212 tennis serve, cricket bowling Willow (Delaitre et al., 2010) AC 7 6 5 986 N N Y Y \u2212 riding bike, photographing PPMI (Yao and Fei-Fei, 2010) AC 24 2 12 4.", "startOffset": 68, "endOffset": 91}, {"referenceID": 39, "context": ", 2010) AC 7 6 5 986 N N Y Y \u2212 riding bike, photographing PPMI (Yao and Fei-Fei, 2010) AC 24 2 12 4.", "startOffset": 63, "endOffset": 86}, {"referenceID": 40, "context": "8k N N Y N \u2212 play guitar, hold violin Stanford 40 Actions (Yao et al., 2011) AC 40 33 31 9.", "startOffset": 58, "endOffset": 76}, {"referenceID": 19, "context": "5k N N Y Y \u2212 riding bike, riding horse 89 Actions (Le et al., 2013) AC 89 36 19 2k N N Y N \u2212 ride bike, fix bike MPII Human Pose (Andriluka et al.", "startOffset": 50, "endOffset": 67}, {"referenceID": 0, "context": ", 2013) AC 89 36 19 2k N N Y N \u2212 ride bike, fix bike MPII Human Pose (Andriluka et al., 2014) AC 410 \u2212 66 40.", "startOffset": 69, "endOffset": 93}, {"referenceID": 20, "context": "5k N N Y N \u2212 riding car, hair styling TUHOI (Le et al., 2014) HOI 2974 \u2212 189 10.", "startOffset": 44, "endOffset": 61}, {"referenceID": 30, "context": "8k N N Y Y \u2212 sit on chair, play with dog COCO-a (Ronchi and Perona, 2015) HOI \u2212 140 80 10k N Y Y Y VerbNet walk bike, hold bike Google Images (Ramanathan et al.", "startOffset": 48, "endOffset": 73}, {"referenceID": 29, "context": "8k N N Y Y \u2212 sit on chair, play with dog COCO-a (Ronchi and Perona, 2015) HOI \u2212 140 80 10k N Y Y Y VerbNet walk bike, hold bike Google Images (Ramanathan et al., 2015) AC 2880 \u2212 \u2212 102k N N N N \u2212 riding horse, riding camel HICO (Chao et al.", "startOffset": 142, "endOffset": 167}, {"referenceID": 5, "context": ", 2015) AC 2880 \u2212 \u2212 102k N N N N \u2212 riding horse, riding camel HICO (Chao et al., 2015) HOI 600 111 80 47k Y N Y Y WordNet ride#v#1 bike; hold#v#2 bike VCOCO-SRL (Gupta and Malik, 2015) VSRL \u2212 26 48 10k N Y Y Y \u2212 verb: hit; instr: bat; obj: ball imSitu (Yatskar et al.", "startOffset": 67, "endOffset": 86}, {"referenceID": 14, "context": ", 2015) HOI 600 111 80 47k Y N Y Y WordNet ride#v#1 bike; hold#v#2 bike VCOCO-SRL (Gupta and Malik, 2015) VSRL \u2212 26 48 10k N Y Y Y \u2212 verb: hit; instr: bat; obj: ball imSitu (Yatskar et al.", "startOffset": 82, "endOffset": 105}, {"referenceID": 41, "context": ", 2015) HOI 600 111 80 47k Y N Y Y WordNet ride#v#1 bike; hold#v#2 bike VCOCO-SRL (Gupta and Malik, 2015) VSRL \u2212 26 48 10k N Y Y Y \u2212 verb: hit; instr: bat; obj: ball imSitu (Yatskar et al., 2016) VSRL \u2212 504 11k 126k Y N Y N FrameNet WordNet verb: ride; agent: girl#n#2 vehicle: bike#n#1; place: road#n#2 VerSe (Gella et al.", "startOffset": 173, "endOffset": 195}, {"referenceID": 12, "context": ", 2016) VSRL \u2212 504 11k 126k Y N Y N FrameNet WordNet verb: ride; agent: girl#n#2 vehicle: bike#n#1; place: road#n#2 VerSe (Gella et al., 2016) VSD 163 90 \u2212 3.", "startOffset": 122, "endOffset": 142}, {"referenceID": 18, "context": "02 Visual Genome (Krishna et al., 2016) VRD 42.", "startOffset": 17, "endOffset": 39}, {"referenceID": 6, "context": "datasets such as MSCOCO (Chen et al., 2015) and the VQA dataset (Antol et al.", "startOffset": 24, "endOffset": 43}, {"referenceID": 1, "context": ", 2015) and the VQA dataset (Antol et al., 2015).", "startOffset": 28, "endOffset": 48}, {"referenceID": 26, "context": "Unlike other HOI datasets such as TUHOI which label interactions as verbs and ignore senses, the HOI categories of HICO are based on WordNet (Miller, 1995) verb senses.", "startOffset": 141, "endOffset": 155}, {"referenceID": 18, "context": "Visual Genome: The dataset created by Krishna et al. (2016) has dense annotations of objects, attributes, and relationships between objects.", "startOffset": 38, "endOffset": 60}, {"referenceID": 18, "context": "Krishna et al. (2016) combine all the annotations of objects, relationships, and attributes into directed", "startOffset": 0, "endOffset": 22}, {"referenceID": 30, "context": "COCO-a: Ronchi and Perona (2015) present", "startOffset": 8, "endOffset": 33}, {"referenceID": 32, "context": "Visual VerbNet (VVN), a list of 140 common visual verbs manually mined from English VerbNet (Schuler, 2005).", "startOffset": 92, "endOffset": 107}, {"referenceID": 12, "context": "VerSe: Gella et al. (2016) created a dataset of 3.", "startOffset": 7, "endOffset": 27}, {"referenceID": 21, "context": "The authors further divided their 90 verbs into motion and non-motion verbs according to Levin (1993) verb classes and analyzed visual ambiguity in the task of visual", "startOffset": 89, "endOffset": 102}, {"referenceID": 14, "context": "VCOCO-SRL: Gupta and Malik (2015) annotated a dataset of 16k person instances in 10k im-", "startOffset": 11, "endOffset": 34}, {"referenceID": 41, "context": "imSitu: Yatskar et al. (2016) annotated a large dataset of 125k images with 504 verbs, 1.", "startOffset": 8, "endOffset": 30}, {"referenceID": 10, "context": "An analysis of various image description and question answering datasets by Ferraro et al. (2015) shows the bias in the distribution of word categories.", "startOffset": 76, "endOffset": 98}, {"referenceID": 38, "context": "eling to generate or rank descriptions (Yang et al., 2011; Thomason et al., 2014; Bernardi et al., 2016).", "startOffset": 39, "endOffset": 104}, {"referenceID": 35, "context": "eling to generate or rank descriptions (Yang et al., 2011; Thomason et al., 2014; Bernardi et al., 2016).", "startOffset": 39, "endOffset": 104}, {"referenceID": 3, "context": "eling to generate or rank descriptions (Yang et al., 2011; Thomason et al., 2014; Bernardi et al., 2016).", "startOffset": 39, "endOffset": 104}, {"referenceID": 42, "context": "There are some specifically curated question answering datasets which target human activities or relationships between a pair of objects (Yu et al., 2015).", "startOffset": 137, "endOffset": 154}, {"referenceID": 25, "context": "Mallya and Lazebnik (2016) have shown that systems trained on action recog-", "startOffset": 0, "endOffset": 27}, {"referenceID": 29, "context": "feeding a panda or tickling a baby and calming a baby, which cannot be learned from text alone (Ramanathan et al., 2015).", "startOffset": 95, "endOffset": 120}, {"referenceID": 37, "context": "Visual semantic role labeling is a crucial step for grounding actions in the physical world (Yang et al., 2016).", "startOffset": 92, "endOffset": 111}, {"referenceID": 16, "context": "cation and human\u2013object interaction tasks rely on identifying higher-level visual cues present in the image, including human bodies or body parts (Ikizler et al., 2008; Gupta et al., 2009; Yao et al., 2011; Andriluka et al., 2014), objects (Gupta et al.", "startOffset": 146, "endOffset": 230}, {"referenceID": 13, "context": "cation and human\u2013object interaction tasks rely on identifying higher-level visual cues present in the image, including human bodies or body parts (Ikizler et al., 2008; Gupta et al., 2009; Yao et al., 2011; Andriluka et al., 2014), objects (Gupta et al.", "startOffset": 146, "endOffset": 230}, {"referenceID": 40, "context": "cation and human\u2013object interaction tasks rely on identifying higher-level visual cues present in the image, including human bodies or body parts (Ikizler et al., 2008; Gupta et al., 2009; Yao et al., 2011; Andriluka et al., 2014), objects (Gupta et al.", "startOffset": 146, "endOffset": 230}, {"referenceID": 0, "context": "cation and human\u2013object interaction tasks rely on identifying higher-level visual cues present in the image, including human bodies or body parts (Ikizler et al., 2008; Gupta et al., 2009; Yao et al., 2011; Andriluka et al., 2014), objects (Gupta et al.", "startOffset": 146, "endOffset": 230}, {"referenceID": 13, "context": ", 2014), objects (Gupta et al., 2009), and scenes (Li and Fei-Fei, 2007).", "startOffset": 17, "endOffset": 37}, {"referenceID": 22, "context": ", 2009), and scenes (Li and Fei-Fei, 2007).", "startOffset": 20, "endOffset": 42}, {"referenceID": 20, "context": "In addition to identifying humans and objects, the relative position or angle between a human and an object is useful in learning human\u2013object interactions (Le et al., 2014).", "startOffset": 156, "endOffset": 173}, {"referenceID": 5, "context": "end convolutional neural network architectures which learn visual cues such as objects and image features for action recognition (Chao et al., 2015; Zhou et al., 2016; Mallya and Lazebnik, 2016).", "startOffset": 129, "endOffset": 194}, {"referenceID": 43, "context": "end convolutional neural network architectures which learn visual cues such as objects and image features for action recognition (Chao et al., 2015; Zhou et al., 2016; Mallya and Lazebnik, 2016).", "startOffset": 129, "endOffset": 194}, {"referenceID": 25, "context": "end convolutional neural network architectures which learn visual cues such as objects and image features for action recognition (Chao et al., 2015; Zhou et al., 2016; Mallya and Lazebnik, 2016).", "startOffset": 129, "endOffset": 194}, {"referenceID": 20, "context": "models rely solely on visual information, models proposed for human\u2013object interaction or visual relationship detection sometimes combine human and object identification (using visual features) with linguistic knowledge (Le et al., 2014; Krishna et al., 2016; Lu et al., 2016).", "startOffset": 220, "endOffset": 276}, {"referenceID": 18, "context": "models rely solely on visual information, models proposed for human\u2013object interaction or visual relationship detection sometimes combine human and object identification (using visual features) with linguistic knowledge (Le et al., 2014; Krishna et al., 2016; Lu et al., 2016).", "startOffset": 220, "endOffset": 276}, {"referenceID": 24, "context": "models rely solely on visual information, models proposed for human\u2013object interaction or visual relationship detection sometimes combine human and object identification (using visual features) with linguistic knowledge (Le et al., 2014; Krishna et al., 2016; Lu et al., 2016).", "startOffset": 220, "endOffset": 276}, {"referenceID": 24, "context": "on identifying actions, especially methods that focus on relationships that are infrequent or unseen, utilize word vectors learned on large text corpora as an additional source of information (Lu et al., 2016).", "startOffset": 192, "endOffset": 209}, {"referenceID": 12, "context": "Similarly, Gella et al. (2016) show that em-", "startOffset": 11, "endOffset": 31}, {"referenceID": 31, "context": "This is despite the fact that the WordNet noun hierarchy (for example) has played an important role in recent progress in object recognition, by virtue of underlying the ImageNet database, the de-facto standard for this task (Russakovsky et al., 2015).", "startOffset": 225, "endOffset": 251}, {"referenceID": 36, "context": "The success of ImageNet for objects has in turn helped NLP tasks such as bilingual lexicon induction (Vuli\u0107 et al., 2016).", "startOffset": 101, "endOffset": 121}, {"referenceID": 5, "context": "For example Chao et al. (2015) used WordNet senses as interaction labels, while Gella et al.", "startOffset": 12, "endOffset": 31}, {"referenceID": 5, "context": "For example Chao et al. (2015) used WordNet senses as interaction labels, while Gella et al. (2016) used the more coarsegrained OntoNotes senses.", "startOffset": 12, "endOffset": 100}, {"referenceID": 5, "context": "For example Chao et al. (2015) used WordNet senses as interaction labels, while Gella et al. (2016) used the more coarsegrained OntoNotes senses. Yatskar et al. (2016) used FrameNet frames for semantic role annotation, while Gupta and Malik (2015) used manually curated roles.", "startOffset": 12, "endOffset": 168}, {"referenceID": 5, "context": "For example Chao et al. (2015) used WordNet senses as interaction labels, while Gella et al. (2016) used the more coarsegrained OntoNotes senses. Yatskar et al. (2016) used FrameNet frames for semantic role annotation, while Gupta and Malik (2015) used manually curated roles.", "startOffset": 12, "endOffset": 248}], "year": 2017, "abstractText": "A large amount of recent research has focused on tasks that combine language and vision, resulting in a proliferation of datasets and methods. One such task is action recognition, whose applications include image annotation, scene understanding and image retrieval. In this survey, we categorize the existing approaches based on how they conceptualize this problem and provide a detailed review of existing datasets, highlighting their diversity as well as advantages and disadvantages. We focus on recently developed datasets which link visual information with linguistic resources and provide a fine-grained syntactic and semantic analysis of actions in images.", "creator": "LaTeX with hyperref package"}}}