{"id": "1704.07138", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search", "abstract": "We present Grid Beam Search (GBS), an algorithm which extends beam search to allow the inclusion of pre-specified lexical constraints. The algorithm can be used with any model that generates a sequence $ \\mathbf{\\hat{y}} = \\{y_{0}\\ldots y_{T}\\} $, by maximizing $ p(\\mathbf{y} | \\mathbf{x}) = \\prod\\limits_{t}p(y_{t} | \\mathbf{x}; \\{y_{0} \\ldots y_{t-1}\\}) $. Lexical constraints take the form of phrases or words that must be present in the output sequence. This is a very general way to incorporate additional knowledge into a model's output without requiring any modification of the model parameters or training data. We demonstrate the feasibility and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation. Experiments show that GBS can provide large improvements in translation quality in interactive scenarios, and that, even without any user input, GBS can be used to achieve significant gains in performance in domain adaptation scenarios.", "histories": [["v1", "Mon, 24 Apr 2017 10:55:20 GMT  (719kb,D)", "http://arxiv.org/abs/1704.07138v1", "Accepted as a long paper at ACL 2017"], ["v2", "Tue, 2 May 2017 13:52:08 GMT  (710kb,D)", "http://arxiv.org/abs/1704.07138v2", "Accepted as a long paper at ACL 2017"]], "COMMENTS": "Accepted as a long paper at ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chris hokamp", "qun liu"], "accepted": true, "id": "1704.07138"}, "pdf": {"name": "1704.07138.pdf", "metadata": {"source": "CRF", "title": "Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search", "authors": ["Chris Hokamp", "Qun Liu"], "emails": ["chris.hokamp@computing.dcu.ie", "qun.liu@dcu.ie"], "sections": [{"heading": null, "text": "We introduce Grid Beam Search (GBS), an algorithm that extends the beam search to allow the inclusion of predefined lexical constraints. The algorithm can be used with any model that generates a sequence y = {y0.... yT} by maximizing p (y | x) = q t p (yt | x; {y0... yt \u2212 1}). Lexical constraints take the form of phrases or words that must be present in the output sequence. This is a very general method of integrating additional knowledge into the output of a model without requiring any changes in model parameters or training data. We demonstrate the feasibility and flexibility of Lexically Confined Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as domain adaptation for Neural Machine Translation. Experiments show that GBS can provide great improvements in translation quality in user scenarios without even having significant gains in interactive scenarios."}, {"heading": "1 Introduction", "text": "The consequence is that most people are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "2 Background: Beam Search for Sequence Generation", "text": "Indeed, the number of possible sequences for such a sequence is very high. (1) The number of possible sequences for such a sequence is very high. (2) The search for such a sequence can be prolonged by factoring the individual sequences. (2) The number of sequences for such a sequence is very high. (3) The number of sequences for such a sequence is very high. (3) The number of sequences for such a sequence is very high. (4) The number of sequences for such a sequence is very high. (4) The sequence for such a sequence is very high. (5) The sequence for such a sequence is very high. (5) The sequence for such a sequence is very high. (6) The sequence for such a sequence is very high. (6) The sequence for such a sequence is very high. \"(6) The sequence for such a sequence is very high.\""}, {"heading": "3 Grid Beam Search", "text": "The goal is to organize the decoding in such a way that we can restrict the search space in a way that contains one or more pre-made sub-sequences. (1) The decoding of the individual sub-areas of the search process is not covered by the individual sub-areas. (1) The decoding of the individual sub-areas of the search, while the c-variable type of search indicates how many constraints are covered in the hypotheses. (1) Each step of c extends over a single compulsion token. (2) In other words, the arrangement of the sequences in which individual tokens can be indexed. (2)"}, {"heading": "3.1 Multi-token Constraints", "text": "By distinguishing between open and closed hypotheses, we can allow arbitrary multiple-symbol phrases in the search. Therefore, the set of constraints on a given output can include both single tokens and phrases. Each hypothesis maintains a coverage vector to ensure that constraints in a search path cannot be repeated - hypotheses that have already covered constraints can only create or begin constraints that have not yet been covered. Also, note that discontinuous lexical constraints, such as English or German phrases, can simply be included in GBS by adding filters to the search that require one or more conditions to be met before a constraint can be used."}, {"heading": "3.2 Subword Units", "text": "Both the calculation of the score for a hypothesis and the granularity of the tokens (character, subword, word, etc.) are left to the underlying model. Since our decoder can handle arbitrary constraints, there is a risk that constraints contain tokens that were never observed in the training data and are therefore unknown from the model. Particularly in domain adaptation scenarios, some user-specified constraints most likely contain invisible tokens. Subword representations provide an elegant way around this problem by splitting unknown or rare tokens into character tokens that are part of the model vocabulary (Sennrich et al., 2016; Wu et al., 2016). In the experiments in Section 4, we use this technique to ensure that no input tokens are unknown, even if a constraint contains words that never appeared in the training data."}, {"heading": "3.3 Efficiency", "text": "Since the number of beams is multiplied by the number of constraints, the runtime complexity of a na\u00efve implementation of GBS O (ktc) is. Standard time-based beam search is O (kt); therefore, if a character not observed in training data is observed at the prediction time, it will be unknown. However, we have not observed this in any of our experiments. Consider the efficiency of this algorithm. Note that the beams in each column c are independent of Figure 3, which means that GBS can be paralleled so that all beams can be filled simultaneously in any timeframe. Also, we note that most of the time is spent calculating the states for the hypotheses candidates, so that by keeping the beam size small we can make GBS significantly faster."}, {"heading": "3.4 Models", "text": "The models used in our experiments are state-of-the-art Neural Machine Translation (NMT) systems using our own implementation of NMT with attention to the source sequence (Bahdanau et al., 2014).We used Blocks and Fuel to implement our NMT models (van Merrinboer et al., 2015).In order to perform the experiments in the following section, we trained basic translation models for English-German (EN-DE), English-French (EN-FR) and English-Portuguese (ENPT).We created a common subword representation for each language pair by extracting a vocabulary of 80,000 symbols from the concatenated source and target data. See the appendix for more details on our training data and hyperparameter configuration for each language pair. The beamSize parameter is set to 10 (for all experiments).Because our experiments can now use more unique implemetations over each language pair, we can now use the NMT for each impleter pair."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Pick-Revise for Interactive Post Editing", "text": "Pick-Revise is an interaction cycle for MT PostEditing proposed by Cheng et al. (2016). Starting3we uses the notation for the g-function of Bahdanau et al. (2014) with the original translation hypothesis, a (simulated) user first selects a part of the hypothesis that is incorrect, and then delivers the correct translation for that part of the output. The user-provided correction is then used as a limitation for the next decryption cycle. The pick-revise process can be repeated as many times as necessary, adding a new constraint to each cycle.We slightly modify the experiments of Cheng et al. (2016) and assume that the user only provides sequences of up to three words that are missing from the hypothesis. 4 To simulate the user interaction, we select a phrase from the current character to the MT in each iteration cycle, not coming from the current translation in the reference hypothesis."}, {"heading": "4.2 Domain Adaptation via Terminology", "text": "The requirement to use domain-specific terminologies is common in the real world (Crego et al., 2016). Existing approaches include placeholder tokens in NMT systems that require modification of the pre- and post-processing of the data, and the formation of the system with its explicit orientations between source and target, so that we cannot use the alignment information to capture the phrase threshing that contains the same placeholders that occur in the test data (Crego et al., 2016). The MT system also loses the ability to model the tools in the terminology as they are represented by abstract tools such as \"< TERM 1 >. An attractive alternative is to consider the terms restrictive, so that any existing system can adapt itself."}, {"heading": "4.3 Analysis", "text": "Subjective analysis of decoder output shows that phrases added as constraints are not only correctly placed within the output sequence, but also have a global impact on translation quality. This is a desirable effect for user interaction, as it implies that users can first boost quality by adding the most critical constraints (i.e. those that are most important for output). Table 3 shows some examples from the experiments in Table 1, where the addition of lexical constraints has led our NMT systems away from initially quite low-rated hypotheses to results that perfectly match the reference translations."}, {"heading": "5 Related Work", "text": "Most of the time it is the case that you are able to play by the rules without having to play by them."}, {"heading": "6 Conclusion", "text": "Licensed decoding is a flexible way to incorporate any subsequence into the output of any model that generates token-by-token output sequences. A wide range of popular text generation models have this feature, and GBS should be easy to use with any model that already uses a beam search. In translation interfaces, where translators can make corrections to an existing hypothesis, these user input can be used as constraints, generating a new output every time a user fixes an error. By simulating this scenario, we have shown that such a workflow can greatly improve translation quality in any process. By using domain-specific terminology to generate target-side constraints, we have shown that a generic domain model can be adapted to a new domain without retraining. Surprisingly, this simple method can lead to significant performance gains even when terminology is created automatically."}, {"heading": "Acknowledgments", "text": "This project was funded by the Science Foundation Ireland at the ADAPT Centre for Digital Content Technology (www.adaptcentre.ie) at Dublin City University under the SFI Research Centres Programme (Grant 13 / RC / 2106), which is co-funded by the European Regional Development Fund and the Horizon 2020 research and innovation programme under grant agreement 645452 (QT21). We thank the anonymous reviewers as well as Iacer Calixto, Peyman Passban and Henry Elder for their helpful feedback on early versions of this work."}, {"heading": "A NMT System Configurations", "text": "We train all systems for 500000 iterations, with validation every 5000 steps. The best single model of validation is used in all experiments for a language pair. We use \"2 regularization for all parameters with \u03b1 = 1e \u2212 5. Dropout is used at the output levels with p (Drop) = 0.5. We sort minibatches by record length of the source and remix the training data according to each epoch. All systems use bidirectional GRUs (Cho et al., 2014) to create the source representation and GRUs for the decoder transition. We use AdaDelta (Zeiler, 2012) to update gradations and large gradations to 1.0.Training configurations EN-DE Embedding 300 Recurrent Layers Size 1000 Source Vocab Size 80000 Target Vocab Size portuguese Vocab Size 90000 Batch Size Size (Vocab Size 50 EN-Emrent FR from Size Size Size Size 700000 Size Size Size Size Size English from Size 600000 Size Size Size Size Size English Size 600000 Size Vocab Size 4.0000"}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "PRIMT: A pickrevise framework for interactive machine translation", "author": ["Shanbo Cheng", "Shujian Huang", "Huadong Chen", "Xinyu Dai", "Jiajun Chen."], "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "Comput. Linguist. 33(2):201\u2013228. https://doi.org/10.1162/coli.2007.33.2.201.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "\u00c7alar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Word association norms, mutual information, and lexicography", "author": ["Kenneth Ward Church", "Patrick Hanks."], "venue": "Comput. Linguist. 16(1):22\u201329. http://dl.acm.org/citation.cfm?id=89086.89095.", "citeRegEx": "Church and Hanks.,? 1990", "shortCiteRegEx": "Church and Hanks.", "year": 1990}, {"title": "Interactive-predictive translation based on multiple word-segments", "author": ["Miguel Domingo", "Alvaro Peris", "Francisco Casacuberta."], "venue": "Baltic J. Modern Computing 4(2):282\u2013291.", "citeRegEx": "Domingo et al\\.,? 2016", "shortCiteRegEx": "Domingo et al\\.", "year": 2016}, {"title": "Text Prediction for Translators", "author": ["George F. Foster."], "venue": "Ph.D. thesis, Montreal, P.Q., Canada, Canada. AAINQ72434.", "citeRegEx": "Foster.,? 2002", "shortCiteRegEx": "Foster.", "year": 2002}, {"title": "Mixed-Initiative Natural Language Translation", "author": ["Spence Green."], "venue": "Ph.D. thesis, Stanford, CA, United States.", "citeRegEx": "Green.,? 2014", "shortCiteRegEx": "Green.", "year": 2014}, {"title": "Globally coherent text generation with neural checklist models", "author": ["Chlo\u00e9 Kiddon", "Luke Zettlemoyer", "Yejin Choi."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin,", "citeRegEx": "Kiddon et al\\.,? 2016", "shortCiteRegEx": "Kiddon et al\\.", "year": 2016}, {"title": "Neural interactive translation prediction", "author": ["Rebecca Knowles", "Philipp Koehn."], "venue": "AMTA 2016, Vol. page 107.", "citeRegEx": "Knowles and Koehn.,? 2016", "shortCiteRegEx": "Knowles and Koehn.", "year": 2016}, {"title": "A process study of computeraided translation", "author": ["Philipp Koehn."], "venue": "Machine Translation 23(4):241\u2013 263. https://doi.org/10.1007/s10590-010-9076-3.", "citeRegEx": "Koehn.,? 2009", "shortCiteRegEx": "Koehn.", "year": 2009}, {"title": "Statistical Machine Translation", "author": ["Philipp Koehn."], "venue": "Cambridge University Press, New York, NY, USA, 1st edition.", "citeRegEx": "Koehn.,? 2010", "shortCiteRegEx": "Koehn.", "year": 2010}, {"title": "The alignment template approach to statistical machine translation", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Comput. Linguist. 30(4):417\u2013449. https://doi.org/10.1162/0891201042544884.", "citeRegEx": "Och and Ney.,? 2004", "shortCiteRegEx": "Och and Ney.", "year": 2004}, {"title": "Heuristics: Intelligent Search Strategies for Computer Problem Solving", "author": ["Judea Pearl."], "venue": "AddisonWesley Longman Publishing Co., Inc., Boston, MA, USA.", "citeRegEx": "Pearl.,? 1984", "shortCiteRegEx": "Pearl.", "year": 1984}, {"title": "Optimal beam search for machine translation", "author": ["Alexander Rush", "Yin-Wen Chang", "Michael Collins."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguis-", "citeRegEx": "Rush et al\\.,? 2013", "shortCiteRegEx": "Rush et al\\.", "year": 2013}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston."], "venue": "Llus Mrquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton, editors, EMNLP. The Association for Com-", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V. Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intel-", "citeRegEx": "Serban et al\\.,? 2016", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Dirt cheap web-scale parallel text from the common crawl", "author": ["Jason R. Smith", "Herve Saint-amand", "Chris Callisonburch", "Magdalena Plamada", "Adam Lopez."], "venue": "In Proceedings of the Conference of the Association for Computational Linguistics (ACL.", "citeRegEx": "Smith et al\\.,? 2013", "shortCiteRegEx": "Smith et al\\.", "year": 2013}, {"title": "Exploiting objective annotations for measuring translation post-editing effort", "author": ["Lucia Specia."], "venue": "Proceedings of the European Association for Machine Translation. May.", "citeRegEx": "Specia.,? 2011", "shortCiteRegEx": "Specia.", "year": 2011}, {"title": "The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages", "author": ["Ralf Steinberger", "Bruno Pouliquen", "Anna Widiger", "Camelia Ignat", "Toma Erjavec", "Dan Tufi."], "venue": "In Proceedings of the 5th International Conference on Language Resources", "citeRegEx": "Steinberger et al\\.,? 2006", "shortCiteRegEx": "Steinberger et al\\.", "year": 2006}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proceedings of the 27th", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Neural machine translation with reconstruction", "author": ["Zhaopeng Tu", "Yang Liu", "Lifeng Shang", "Xiaohua Liu", "Hang Li."], "venue": "arXiv preprint arXiv:1611.01874 .", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["Bart van Merrinboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley", "Jan Chorowski", "Yoshua Bengio."], "venue": "CoRR abs/1506.00619.", "citeRegEx": "Merrinboer et al\\.,? 2015", "shortCiteRegEx": "Merrinboer et al\\.", "year": 2015}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "PeiHao Su", "David Vandyke", "Steve Young."], "venue": "Proceedings of the 2015 Conference on Em-", "citeRegEx": "Wen et al\\.,? 2015", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Models and inference for prefix-constrained machine translation", "author": ["Joern Wuebker", "Spence Green", "John DeNero", "Sasa Hasan", "Minh-Thang Luong."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Wuebker et al\\.,? 2016", "shortCiteRegEx": "Wuebker et al\\.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."], "venue": "David Blei and Francis Bach, editors,", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "CoRR abs/1212.5701. http://arxiv.org/abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 15, "context": "Examples include automatic summarization (Rush et al., 2015), machine translation (Koehn, 2010; Bahdanau et al.", "startOffset": 41, "endOffset": 60}, {"referenceID": 11, "context": ", 2015), machine translation (Koehn, 2010; Bahdanau et al., 2014), caption generation (Xu et al.", "startOffset": 29, "endOffset": 65}, {"referenceID": 0, "context": ", 2015), machine translation (Koehn, 2010; Bahdanau et al., 2014), caption generation (Xu et al.", "startOffset": 29, "endOffset": 65}, {"referenceID": 26, "context": ", 2014), caption generation (Xu et al., 2015), and dialog generation (Serban et al.", "startOffset": 28, "endOffset": 45}, {"referenceID": 17, "context": ", 2015), and dialog generation (Serban et al., 2016), among others.", "startOffset": 31, "endOffset": 52}, {"referenceID": 10, "context": "Examples include Post-Editing (PE) (Koehn, 2009; Specia, 2011) and InteractivePredictive MT (Foster, 2002; Barrachina et al.", "startOffset": 35, "endOffset": 62}, {"referenceID": 19, "context": "Examples include Post-Editing (PE) (Koehn, 2009; Specia, 2011) and InteractivePredictive MT (Foster, 2002; Barrachina et al.", "startOffset": 35, "endOffset": 62}, {"referenceID": 6, "context": "Examples include Post-Editing (PE) (Koehn, 2009; Specia, 2011) and InteractivePredictive MT (Foster, 2002; Barrachina et al., 2009; Green, 2014).", "startOffset": 92, "endOffset": 144}, {"referenceID": 7, "context": "Examples include Post-Editing (PE) (Koehn, 2009; Specia, 2011) and InteractivePredictive MT (Foster, 2002; Barrachina et al., 2009; Green, 2014).", "startOffset": 92, "endOffset": 144}, {"referenceID": 13, "context": "A common solution is to use a heuristic search to attempt to find the best output efficiently (Pearl, 1984; Koehn, 2010; Rush et al., 2013).", "startOffset": 94, "endOffset": 139}, {"referenceID": 11, "context": "A common solution is to use a heuristic search to attempt to find the best output efficiently (Pearl, 1984; Koehn, 2010; Rush et al., 2013).", "startOffset": 94, "endOffset": 139}, {"referenceID": 14, "context": "A common solution is to use a heuristic search to attempt to find the best output efficiently (Pearl, 1984; Koehn, 2010; Rush et al., 2013).", "startOffset": 94, "endOffset": 139}, {"referenceID": 12, "context": "Beam search (Och and Ney, 2004) is probably the most popular search algorithm for decoding sequences.", "startOffset": 12, "endOffset": 31}, {"referenceID": 11, "context": "For example, in Phrase-Based Statistical MT (PB-SMT) (Koehn, 2010), beams are organized by the number of source words that are covered by the hypotheses in the beam \u2013 a hypothesis is \u201cfinished\u201d when it has covered all source words.", "startOffset": 53, "endOffset": 66}, {"referenceID": 2, "context": "In chart-based decoding algorithms such as CYK, beams are also tied to coverage of the input, but are organized as cells in a chart, which facilitates search for the optimal latent structure of the output (Chiang, 2007).", "startOffset": 205, "endOffset": 219}, {"referenceID": 21, "context": "With the recent success of neural models for text generation, beam search has become the de-facto choice for decoding optimal output sequences (Sutskever et al., 2014).", "startOffset": 143, "endOffset": 167}, {"referenceID": 21, "context": "A simpler alternative is to organize beams by output timesteps from t0 \u00b7 \u00b7 \u00b7 tN , where N is a hyperparameter that can be set heuristically, for example by multiplying a factor with the length of the input to make an educated guess about the maximum length of the output (Sutskever et al., 2014).", "startOffset": 271, "endOffset": 295}, {"referenceID": 22, "context": "that the performance of some architectures can actually degrade with larger beam size (Tu et al., 2016).", "startOffset": 86, "endOffset": 103}, {"referenceID": 16, "context": "Subword representations provide an elegant way to circumvent this problem, by breaking unknown or rare tokens into character n-grams which are part of the model\u2019s vocabulary (Sennrich et al., 2016; Wu et al., 2016).", "startOffset": 174, "endOffset": 214}, {"referenceID": 0, "context": "The models used for our experiments are stateof-the-art Neural Machine Translation (NMT) systems using our own implementation of NMT with attention over the source sequence (Bahdanau et al., 2014).", "startOffset": 173, "endOffset": 196}, {"referenceID": 1, "context": "Pick-Revise is an interaction cycle for MT PostEditing proposed by Cheng et al. (2016). Starting", "startOffset": 67, "endOffset": 87}, {"referenceID": 0, "context": "we use the notation for the g function from Bahdanau et al. (2014)", "startOffset": 44, "endOffset": 67}, {"referenceID": 1, "context": "We modify the experiments of Cheng et al. (2016) slightly, and assume that the user only provides sequences of up to three words which are missing from the hypothesis.", "startOffset": 29, "endOffset": 49}, {"referenceID": 4, "context": "We divide the corpus into approximately 100,000 training sentences, and 1000 test segments, and automatically generate a terminology by computing the Pointwise Mutual Information (PMI) (Church and Hanks, 1990) between source and target n-grams in the training set.", "startOffset": 185, "endOffset": 209}, {"referenceID": 6, "context": "The largest body of work considers Interactive Machine Translation (IMT): an MT system searches for the optimal target-language suffix given a complete source sentence and a desired prefix for the target output (Foster, 2002; Barrachina et al., 2009; Green, 2014).", "startOffset": 211, "endOffset": 263}, {"referenceID": 7, "context": "The largest body of work considers Interactive Machine Translation (IMT): an MT system searches for the optimal target-language suffix given a complete source sentence and a desired prefix for the target output (Foster, 2002; Barrachina et al., 2009; Green, 2014).", "startOffset": 211, "endOffset": 263}, {"referenceID": 6, "context": "The largest body of work considers Interactive Machine Translation (IMT): an MT system searches for the optimal target-language suffix given a complete source sentence and a desired prefix for the target output (Foster, 2002; Barrachina et al., 2009; Green, 2014). IMT can be viewed as subcase of constrained decoding, where there is only one constraint which is guaranteed to be placed at the beginning of the output sequence. Wuebker et al. (2016) introduce prefix-decoding, which modifies the SMT beam search to first ensure that the target prefix is covered, and only then continues to build hypotheses for the suffix using beams organized by coverage of the remaining phrases in the source segment.", "startOffset": 212, "endOffset": 450}, {"referenceID": 6, "context": "The largest body of work considers Interactive Machine Translation (IMT): an MT system searches for the optimal target-language suffix given a complete source sentence and a desired prefix for the target output (Foster, 2002; Barrachina et al., 2009; Green, 2014). IMT can be viewed as subcase of constrained decoding, where there is only one constraint which is guaranteed to be placed at the beginning of the output sequence. Wuebker et al. (2016) introduce prefix-decoding, which modifies the SMT beam search to first ensure that the target prefix is covered, and only then continues to build hypotheses for the suffix using beams organized by coverage of the remaining phrases in the source segment. Wuebker et al. (2016) and Knowles and Koehn (2016) also present a simple modification of NMT models for IMT, enabling models to predict suffixes for user-supplied prefixes.", "startOffset": 212, "endOffset": 726}, {"referenceID": 6, "context": "The largest body of work considers Interactive Machine Translation (IMT): an MT system searches for the optimal target-language suffix given a complete source sentence and a desired prefix for the target output (Foster, 2002; Barrachina et al., 2009; Green, 2014). IMT can be viewed as subcase of constrained decoding, where there is only one constraint which is guaranteed to be placed at the beginning of the output sequence. Wuebker et al. (2016) introduce prefix-decoding, which modifies the SMT beam search to first ensure that the target prefix is covered, and only then continues to build hypotheses for the suffix using beams organized by coverage of the remaining phrases in the source segment. Wuebker et al. (2016) and Knowles and Koehn (2016) also present a simple modification of NMT models for IMT, enabling models to predict suffixes for user-supplied prefixes.", "startOffset": 212, "endOffset": 755}, {"referenceID": 1, "context": "The Pick-Revise (PRIMT) (Cheng et al., 2016) framework for Interactive Post Editing introduces the concept of edit cycles.", "startOffset": 24, "endOffset": 44}, {"referenceID": 24, "context": "Some recent work considers the inclusion of soft lexical constraints directly into deep models for dialog generation, and special cases, such as recipe generation from a list of ingredients (Wen et al., 2015; Kiddon et al., 2016).", "startOffset": 190, "endOffset": 229}, {"referenceID": 8, "context": "Some recent work considers the inclusion of soft lexical constraints directly into deep models for dialog generation, and special cases, such as recipe generation from a list of ingredients (Wen et al., 2015; Kiddon et al., 2016).", "startOffset": 190, "endOffset": 229}], "year": 2017, "abstractText": "We present Grid Beam Search (GBS), an algorithm which extends beam search to allow the inclusion of pre-specified lexical constraints. The algorithm can be used with any model that generates a sequence \u0177 = {y0 . . . yT }, by maximizing p(y|x) = \u220f t p(yt|x; {y0 . . . yt\u22121}). Lexical constraints take the form of phrases or words that must be present in the output sequence. This is a very general way to incorporate additional knowledge into a model\u2019s output without requiring any modification of the model parameters or training data. We demonstrate the feasibility and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation. Experiments show that GBS can provide large improvements in translation quality in interactive scenarios, and that, even without any user input, GBS can be used to achieve significant gains in performance in domain adaptation scenarios.", "creator": "LaTeX with hyperref package"}}}