{"id": "1704.07156", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Semi-supervised Multitask Learning for Sequence Labeling", "abstract": "We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unannotated data.", "histories": [["v1", "Mon, 24 Apr 2017 11:47:06 GMT  (195kb,D)", "http://arxiv.org/abs/1704.07156v1", "ACL 2017"]], "COMMENTS": "ACL 2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["marek rei"], "accepted": true, "id": "1704.07156"}, "pdf": {"name": "1704.07156.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised Multitask Learning for Sequence Labeling", "authors": ["Marek Rei"], "emails": ["marek.rei@cl.cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "This year, it has never been as good as it has been this year."}, {"heading": "2 Neural Sequence Labeling", "text": "In fact, the two models that have emerged in the past few years in the United States are a mixture composed of the different concepts and concepts. (...) It is not that the individual concepts are interwoven with each other. (...) It is that the individual concepts and concepts of the individual concepts are interwoven with each other. (...) It is that both concepts are interwoven with each other. (...) It is as if both concepts are interwoven with each other. (...) It is as if both concepts are interwoven with each other. (...) It is as if both concepts are interwoven with each other. (...) It is as if both concepts are interwoven with each other. (...) It is as if both concepts are interwoven. (...) It is as if both concepts are interwoven."}, {"heading": "3 Language Modeling Objective", "text": "The sequence labeling model in Section 2 is optimized only on the basis of the correct labels. While each character in the input has a desired label, many of these contribute very little to the training process. For example, in the CoNLL 2003 NER database (Tjong Kim Sang and De Meulder, 2003) there are only 8 possible labels and 83% of the labels have the label O, indicating that no named entity is recognized. This ratio is even higher for error detection, with 86% of all tokens not containing any errors in the FCE dataset (Yannakoudakis et al., 2011). Sequence labeling models are able to learn this bias in label distribution without obtaining much additional information from the majority labels. Therefore, we propose an additional goal that would allow the models to fully utilize the training data."}, {"heading": "4 Evaluation Setup", "text": "The proposed architecture was evaluated on 10 different sequence tagging components that covered the tasks of error detection that were only replaced by any arbitrary procedure, such as error detection, chunking, and POStagging. Word embedding in the model was subsequently done with publicly available lecturers that started with word embedding (Mikolov et al., 2013). For general domain datasets, we implemented with 300-dimensional embedding on Google News.2 for biomedical datasets, the word embedding with 200-dimensional vectors that was implemented on PubMed and PMC.3The neural network framework was implemented with Theano (Al-Rfou et al., 2016) and we make the code publicly available online.4 For most of the hyperparameters, we follow the settings of Rei et al. (2016) to facilitate direct comparison with 200 previous work on the STM layered layers and STM for both directions."}, {"heading": "5 Error Detection", "text": "This year it is more than ever before."}, {"heading": "6 NER and Chunking", "text": "Over the next few years, we will have to consider the extent to which we will be able to solve the problems mentioned."}, {"heading": "7 POS tagging", "text": "Penn Treebank POS-Tag Corpus (Marcus et al., 1993) contains texts from the Wall Street Journal and has been provided with 48 different Part-of-Speech tags. In addition, we use the POS-annotated subset of the GENIA-Corpus (Ohta et al., 2002) with 2,000 biomedical PubMed abstracts. Following Tsuruoka et al. (2005), we use the same 210 document test set. Finally, we also evaluate the Finnish and Spanish sections of the Universal Dependencies v1.2 data set (UD, Nivre et al. (2015)) to examine the performance of morphologically complex and Romance languages. These data sets are somewhat more balanced in terms of label distributions, compared with error detection and NER data sets, as no single label covers more than 50% of the tokens. POS tagging language therefore provides a large variability of these training results with regard to labels, where additional information can be seen on ENIA labels."}, {"heading": "8 Related Work", "text": "Our work builds on previous research that examined multi-task learning in the context of different sequence marking tasks. Caruana (1998) described the idea of multi-task learning and has since extended it to many language processing tasks using neural networks. Collobert and Weston (2008) proposed a multi-task framework that uses the weight distribution between networks optimized for different monitored tasks. Cheng et al. (2015) described a system for detecting vocabulary names outside the vocabulary by predicting the next word in the sequence. While using a recurring architecture, we propose a language modeling goal that can be integrated with a bi-directional network, making it applicable to existing sequence sequences that label frame words. Plank et al. (2016) described a related architecture for POS tagging, where each word is related to the frequency of a particular word, and each word is adjacent to the common one."}, {"heading": "9 Conclusion", "text": "One half of a bi-directional LSTM is trained as an advanced language model, while the other half is trained as a backward-facing language model. At the same time, the two are also combined to predict the most likely name for each word. This modification can be applied to several common sequence markup architectures and does not require additional annotated or unannotated data. The goal of learning to predict surrounding words provides an additional source of information during the training. The model is encouraged to discover useful features to learn the language distribution and composition patterns in the training data. While language modeling is not the main goal of the system, this additional training goal leads to more precise sequence markup models on several different tasks. The architecture has been evaluated against a set of data sets covering the task of error detection, called entity detection, decryption, and POS identification of models for better identification of models for data detection."}], "references": [{"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Bisson", "Josh Bleecher Snyder", "Nicolas Bouchard", "Nicolas Boulanger-Lewandowski", "Others."], "venue": "arXiv e-prints abs/1605.0:19. http://arxiv.org/abs/1605.02688.", "citeRegEx": "Bisson et al\\.,? 2016", "shortCiteRegEx": "Bisson et al\\.", "year": 2016}, {"title": "MultiTask Learning of Keyphrase Boundary Classification", "author": ["Isabelle Augenstein", "Anders S\u00f8gaard."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. http://arxiv.org/abs/1704.00514.", "citeRegEx": "Augenstein and S\u00f8gaard.,? 2017", "shortCiteRegEx": "Augenstein and S\u00f8gaard.", "year": 2017}, {"title": "Identifying beneficial task relations for multi-task learning in deep neural networks", "author": ["Joachim Bingel", "Anders S\u00f8gaard."], "venue": "arXiv preprint. http://arxiv.org/abs/1702.08303.", "citeRegEx": "Bingel and S\u00f8gaard.,? 2017", "shortCiteRegEx": "Bingel and S\u00f8gaard.", "year": 2017}, {"title": "Multitask Learning", "author": ["Rich Caruana."], "venue": "Ph.D. thesis.", "citeRegEx": "Caruana.,? 1998", "shortCiteRegEx": "Caruana.", "year": 1998}, {"title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling", "author": ["Ciprian Chelba", "Tom\u00e1\u0161 Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson."], "venue": "arXiv preprint. http://arxiv.org/abs/1312.3005.", "citeRegEx": "Chelba et al\\.,? 2013", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "Open-Domain Name Error Detection using a MultiTask RNN", "author": ["Hao Cheng", "Hao Fang", "Mari Ostendorf."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Cheng et al\\.,? 2015", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning (ICML \u201908)", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12. https://doi.org/10.1145/2347736.2347755.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton."], "venue": "International Conference on Acoustics, Speech and Signal Processing (ICASSP) https://doi.org/10.1109/ICASSP.2013.6638947.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long Short-term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9. https://doi.org/10.1.1.56.7752.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Bidirectional LSTM-CRF Models for Sequence Tagging", "author": ["Zhiheng Huang", "Wei Xu", "Kai Yu."], "venue": "arXiv:1508.01991 http://arxiv.org/pdf/1508.01991v1.pdf.", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Opinion Mining with Deep Recurrent Neural Networks", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "Introduction to the Bio-entity Recognition Task at JNLPBA", "author": ["Jin-Dong Kim", "Tomoko Ohta", "Yoshimasa Tsuruoka", "Yuka Tateisi", "Nigel Collier."], "venue": "Proceedings of the International Joint Workshop on Natural Language", "citeRegEx": "Kim et al\\.,? 2004", "shortCiteRegEx": "Kim et al\\.", "year": 2004}, {"title": "CHEMDNER: The drugs and chemical names extraction challenge", "author": ["Martin Krallinger", "Florian Leitner", "Obdulia Rabal", "Miguel Vazquez", "Julen Oyarzabal", "Alfonso Valencia."], "venue": "Journal of Cheminformatics 7(Suppl 1). https://doi.org/10.1186/1758-", "citeRegEx": "Krallinger et al\\.,? 2015", "shortCiteRegEx": "Krallinger et al\\.", "year": 2015}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando Pereira."], "venue": "Proceedings of the 18th International Conference on Machine Learning.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Neural Architectures for Named Entity Recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."], "venue": "Proceedings of NAACL-HLT 2016.", "citeRegEx": "Lample et al\\.,? 2016", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics 19. https://doi.org/10.1162/coli.2010.36.1.36100.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tom\u00e1\u0161 Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR 2013).", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Recurrent Neural Network based Language Model", "author": ["Tom\u00e1\u0161 Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1\u0161 Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur."], "venue": "Interspeech (September):1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Andriy Mnih", "Yee Whye Teh."], "venue": "Neural Information Processing Systems (NIPS).", "citeRegEx": "Mnih and Teh.,? 2012", "shortCiteRegEx": "Mnih and Teh.", "year": 2012}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio."], "venue": "Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics https://doi.org/10.1109/JCDL.2003.1204852.", "citeRegEx": "Morin and Bengio.,? 2005", "shortCiteRegEx": "Morin and Bengio.", "year": 2005}, {"title": "The CoNLL-2014 Shared Task on Grammatical Error Correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant."], "venue": "Proceedings of the Eighteenth Conference on Computa-", "citeRegEx": "Ng et al\\.,? 2014", "shortCiteRegEx": "Ng et al\\.", "year": 2014}, {"title": "The GENIA corpus: An annotated research abstract corpus in molecular biology domain", "author": ["Tomoko Ohta", "Yuka Tateisi", "Jin-Dong Kim."], "venue": "Proceedings of the second international conference on Human Language Technology Research", "citeRegEx": "Ohta et al\\.,? 2002", "shortCiteRegEx": "Ohta et al\\.", "year": 2002}, {"title": "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss", "author": ["Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Com-", "citeRegEx": "Plank et al\\.,? 2016", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Attending to Characters in Neural Sequence", "author": ["Marek Rei", "Gamal K.O. Crichton", "Sampo Pyysalo"], "venue": null, "citeRegEx": "Rei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rei et al\\.", "year": 2016}, {"title": "Compositional Sequence Labeling Models for Error Detection in Learner Writing", "author": ["Marek Rei", "Helen Yannakoudakis."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Rei and Yannakoudakis.,? 2016", "shortCiteRegEx": "Rei and Yannakoudakis.", "year": 2016}, {"title": "Dropout : A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research (JMLR) 15.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Introduction to the CoNLL-2000 shared task: Chunking", "author": ["Erik F. Tjong Kim Sang", "Sabine Buchholz."], "venue": "Proceedings of the 2nd Workshop on Learning Language in Logic and the 4th Conference on Computational Natural Language Learning", "citeRegEx": "Sang and Buchholz.,? 2000", "shortCiteRegEx": "Sang and Buchholz.", "year": 2000}, {"title": "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition", "author": ["Erik F. Tjong Kim Sang", "Fien De Meulder."], "venue": "Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003.", "citeRegEx": "Sang and Meulder.,? 2003", "shortCiteRegEx": "Sang and Meulder.", "year": 2003}, {"title": "Developing a robust partof-speech tagger for biomedical text", "author": ["Yoshimasa Tsuruoka", "Yuka Tateishi", "Jin Dong Kim", "Tomoko Ohta", "John McNaught", "Sophia Ananiadou", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of Panhellenic Conference on Informatics", "citeRegEx": "Tsuruoka et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsuruoka et al\\.", "year": 2005}, {"title": "A New Dataset and Method for Automatically Grading ESOL Texts", "author": ["Helen Yannakoudakis", "Ted Briscoe", "Ben Medlock."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technolo-", "citeRegEx": "Yannakoudakis et al\\.,? 2011", "shortCiteRegEx": "Yannakoudakis et al\\.", "year": 2011}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D. Zeiler."], "venue": "arXiv preprint arXiv:1212.5701 http://arxiv.org/abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Exploring Deep Knowledge Resources in Biomedical Name Recognition", "author": ["GuoDong Zhou", "Jian Su."], "venue": "Workshop on Natural Language Processing in Biomedicine and Its Applications at COLING .", "citeRegEx": "Zhou and Su.,? 2004", "shortCiteRegEx": "Zhou and Su.", "year": 2004}], "referenceMentions": [{"referenceID": 7, "context": "However, recent work has shown that neural network architectures are able to achieve comparable or improved performance, while automatically discovering useful features for a specific task and only requiring a sequence of tokens as input (Collobert et al., 2011; Irsoy and Cardie, 2014; Lample et al., 2016).", "startOffset": 238, "endOffset": 307}, {"referenceID": 11, "context": "However, recent work has shown that neural network architectures are able to achieve comparable or improved performance, while automatically discovering useful features for a specific task and only requiring a sequence of tokens as input (Collobert et al., 2011; Irsoy and Cardie, 2014; Lample et al., 2016).", "startOffset": 238, "endOffset": 307}, {"referenceID": 15, "context": "However, recent work has shown that neural network architectures are able to achieve comparable or improved performance, while automatically discovering useful features for a specific task and only requiring a sequence of tokens as input (Collobert et al., 2011; Irsoy and Cardie, 2014; Lample et al., 2016).", "startOffset": 238, "endOffset": 307}, {"referenceID": 30, "context": "This ratio is even lower for error detection, with only 14% of all tokens being annotated as an error in the FCE dataset (Yannakoudakis et al., 2011).", "startOffset": 121, "endOffset": 149}, {"referenceID": 18, "context": "Neural language modeling architectures also have many similarities to common sequence labeling frameworks: words are first mapped to distributed embeddings, followed by a recurrent neural network (RNN) module for composing word sequences into an informative context representation (Mikolov et al., 2010; Graves et al., 2013; Chelba et al., 2013).", "startOffset": 281, "endOffset": 345}, {"referenceID": 8, "context": "Neural language modeling architectures also have many similarities to common sequence labeling frameworks: words are first mapped to distributed embeddings, followed by a recurrent neural network (RNN) module for composing word sequences into an informative context representation (Mikolov et al., 2010; Graves et al., 2013; Chelba et al., 2013).", "startOffset": 281, "endOffset": 345}, {"referenceID": 4, "context": "Neural language modeling architectures also have many similarities to common sequence labeling frameworks: words are first mapped to distributed embeddings, followed by a recurrent neural network (RNN) module for composing word sequences into an informative context representation (Mikolov et al., 2010; Graves et al., 2013; Chelba et al., 2013).", "startOffset": 281, "endOffset": 345}, {"referenceID": 24, "context": "We use the neural network model of Rei et al. (2016) as the baseline architecture for our sequence labeling experiments.", "startOffset": 35, "endOffset": 53}, {"referenceID": 9, "context": "Two LSTM (Hochreiter and Schmidhuber, 1997) components, moving in opposite directions through the sentence, are", "startOffset": 9, "endOffset": 43}, {"referenceID": 10, "context": "(2001)), which allows the network to look for the most optimal path through all possible label sequences (Huang et al., 2015; Lample et al., 2016).", "startOffset": 105, "endOffset": 146}, {"referenceID": 15, "context": "(2001)), which allows the network to look for the most optimal path through all possible label sequences (Huang et al., 2015; Lample et al., 2016).", "startOffset": 105, "endOffset": 146}, {"referenceID": 13, "context": "dom Field (CRF, Lafferty et al. (2001)), which allows the network to look for the most optimal path through all possible label sequences (Huang et al.", "startOffset": 16, "endOffset": 39}, {"referenceID": 24, "context": "We also make use of the character-level component described by Rei et al. (2016), which builds an alternative representation for each word.", "startOffset": 63, "endOffset": 81}, {"referenceID": 30, "context": "This ratio is even higher for error detection, with 86% of all tokens containing no errors in the FCE dataset (Yannakoudakis et al., 2011).", "startOffset": 110, "endOffset": 138}, {"referenceID": 20, "context": "While our implementation uses a basic softmax as the output layer for the language modeling components, the efficiency during training could be further improved by integrating noise-contrastive estimation (NCE, Mnih and Teh (2012)) or hierarchical softmax (Morin and Bengio, 2005).", "startOffset": 256, "endOffset": 280}, {"referenceID": 19, "context": "While our implementation uses a basic softmax as the output layer for the language modeling components, the efficiency during training could be further improved by integrating noise-contrastive estimation (NCE, Mnih and Teh (2012)) or hierarchical softmax (Morin and Bengio, 2005).", "startOffset": 211, "endOffset": 231}, {"referenceID": 17, "context": "The word embeddings in the model were initialised with publicly available pretrained vectors, created using word2vec (Mikolov et al., 2013).", "startOffset": 117, "endOffset": 139}, {"referenceID": 24, "context": "4 For most of the hyperparameters, we follow the settings by Rei et al. (2016) in order to facilitate direct comparison with previous work.", "startOffset": 61, "endOffset": 79}, {"referenceID": 31, "context": "Sentences were grouped into batches of size 64 and parameters were optimised using AdaDelta (Zeiler, 2012) with default learning rate 1.", "startOffset": 92, "endOffset": 106}, {"referenceID": 26, "context": "During development we found that applying dropout (Srivastava et al., 2014) on word embeddings improves performance on nearly all datasets, compared to this baseline.", "startOffset": 50, "endOffset": 75}, {"referenceID": 24, "context": "tem by Rei et al. (2016) is used as the baseline.", "startOffset": 7, "endOffset": 25}, {"referenceID": 30, "context": "the first benchmark, we use the publicly released First Certificate in English (FCE, Yannakoudakis et al. (2011)) dataset, containing 33,673 manually annotated sentences.", "startOffset": 85, "endOffset": 113}, {"referenceID": 21, "context": "In addition, we evaluate on the CoNLL 2014 shared task dataset (Ng et al., 2014), which has been converted to an error detection task.", "startOffset": 63, "endOffset": 80}, {"referenceID": 21, "context": "In addition, we evaluate on the CoNLL 2014 shared task dataset (Ng et al., 2014), which has been converted to an error detection task. This contains 1,312 sentences, written by higher-proficiency learners on more technical topics. They have been manually corrected by two separate annotators, and we report results on each of these annotations. For both datasets we use the FCE training set for model optimisation and results on the CoNLL-14 dataset indicate outof-domain performance. Rei and Yannakoudakis (2016) present results on these datasets using the same setup, along with evaluating the top shared task submissions on the task of error detection.", "startOffset": 64, "endOffset": 514}, {"referenceID": 24, "context": "These results also improve over the previous best results by Rei and Yannakoudakis (2016) and Rei et al.", "startOffset": 61, "endOffset": 90}, {"referenceID": 24, "context": "These results also improve over the previous best results by Rei and Yannakoudakis (2016) and Rei et al. (2016), when all systems are trained on the same FCE dataset.", "startOffset": 94, "endOffset": 112}, {"referenceID": 13, "context": "We also report results on two biomedical NER datasets: The BioCreative IV Chemical and Drug corpus (CHEMDNER, Krallinger et al. (2015)) of 10,000", "startOffset": 110, "endOffset": 135}, {"referenceID": 12, "context": "abstracts, annotated for mentions of chemical and drug names, and the JNLPBA corpus (Kim et al., 2004) of 2,404 abstracts annotated for mentions of different cells and proteins.", "startOffset": 84, "endOffset": 102}, {"referenceID": 28, "context": "55% by Zhou and Su (2004) and 72.", "startOffset": 7, "endOffset": 26}, {"referenceID": 21, "context": "70% by Rei et al. (2016). On CoNLL-03, Lample et al.", "startOffset": 7, "endOffset": 25}, {"referenceID": 14, "context": "On CoNLL-03, Lample et al. (2016) achieve a considerably higher result of 90.", "startOffset": 13, "endOffset": 34}, {"referenceID": 10, "context": "However, our system does outperform a similar architecture by Huang et al. (2015), achieving 86.", "startOffset": 62, "endOffset": 82}, {"referenceID": 16, "context": "The Penn Treebank POS-tag corpus (Marcus et al., 1993) contains texts from the Wall Street Journal and has been annotated with 48 different part-of-speech tags.", "startOffset": 33, "endOffset": 54}, {"referenceID": 22, "context": "In addition, we use the POS-annotated subset of the GENIA corpus (Ohta et al., 2002) containing 2,000 biomedical PubMed abstracts.", "startOffset": 65, "endOffset": 84}, {"referenceID": 16, "context": "The Penn Treebank POS-tag corpus (Marcus et al., 1993) contains texts from the Wall Street Journal and has been annotated with 48 different part-of-speech tags. In addition, we use the POS-annotated subset of the GENIA corpus (Ohta et al., 2002) containing 2,000 biomedical PubMed abstracts. Following Tsuruoka et al. (2005), we use the same 210document test set.", "startOffset": 34, "endOffset": 325}, {"referenceID": 16, "context": "The Penn Treebank POS-tag corpus (Marcus et al., 1993) contains texts from the Wall Street Journal and has been annotated with 48 different part-of-speech tags. In addition, we use the POS-annotated subset of the GENIA corpus (Ohta et al., 2002) containing 2,000 biomedical PubMed abstracts. Following Tsuruoka et al. (2005), we use the same 210document test set. Finally, we also evaluate on the Finnish and Spanish sections of the Universal Dependencies v1.2 dataset (UD, Nivre et al. (2015)), in order to investigate performance on morphologically complex and Romance languages.", "startOffset": 34, "endOffset": 494}, {"referenceID": 3, "context": "learning was described by Caruana (1998) and has since been extended to many language processing tasks using neural networks.", "startOffset": 26, "endOffset": 41}, {"referenceID": 3, "context": "learning was described by Caruana (1998) and has since been extended to many language processing tasks using neural networks. For example, Collobert and Weston (2008) proposed a multitask framework using weight-sharing between networks that are optimised for different supervised tasks.", "startOffset": 26, "endOffset": 167}, {"referenceID": 1, "context": "Recently, Augenstein and S\u00f8gaard (2017) explored multi-task learning for classifying keyphrase boundaries, by incorporating tasks such as semantic super-sense tagging and identification of multi-word expressions.", "startOffset": 10, "endOffset": 40}], "year": 2017, "abstractText": "We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unannotated data.", "creator": "LaTeX with hyperref package"}}}