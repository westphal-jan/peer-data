{"id": "1704.07431", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "A Challenge Set Approach to Evaluating Machine Translation", "abstract": "Neural machine translation represents an exciting leap forward in translation quality. But what longstanding weaknesses does it resolve, and which remain? We address these questions with a challenge set approach to translation evaluation and error analysis. A challenge set consists of a small set of sentences, each hand-designed to probe a system's capacity to bridge a particular structural divergence between languages. To exemplify this approach, we present an English-French challenge set, and use it to analyze phrase-based and neural systems. The resulting analysis provides not only a more fine-grained picture of the strengths of neural systems, but also insight into which linguistic phenomena remain out of reach.", "histories": [["v1", "Mon, 24 Apr 2017 19:34:38 GMT  (43kb)", "http://arxiv.org/abs/1704.07431v1", "26 pages, including appendix"], ["v2", "Fri, 5 May 2017 16:28:52 GMT  (55kb)", "http://arxiv.org/abs/1704.07431v2", "27 pages, including appendix. Machine readable data included in a separate file"], ["v3", "Wed, 10 May 2017 00:03:19 GMT  (55kb,A)", "http://arxiv.org/abs/1704.07431v3", "27 pages, including appendix. Machine readable data included in a separate file"], ["v4", "Sun, 6 Aug 2017 17:13:11 GMT  (56kb,A)", "http://arxiv.org/abs/1704.07431v4", "EMNLP 2017. 28 pages, including appendix. Machine readable data included in a separate file"], ["v5", "Tue, 29 Aug 2017 02:08:33 GMT  (56kb,A)", "http://arxiv.org/abs/1704.07431v5", "EMNLP 2017. 28 pages, including appendix. Machine readable data included in a separate file. This version corrects typos in the challenge set"]], "COMMENTS": "26 pages, including appendix", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["pierre isabelle", "colin cherry", "george f foster"], "accepted": true, "id": "1704.07431"}, "pdf": {"name": "1704.07431.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["first.last@nrc.gc.ca"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.07 431v 1 [cs.C L] 24 Apr 201 7Exciting leap forward in translation quality. But what long-standing weaknesses does it resolve and which remain? We address these questions with a challenging approach to translation evaluation and error analysis. A task consists of a small group of sentences, each designed by hand to examine a system's ability to bridge a certain structural divergence between languages. To illustrate this approach, we present an Anglo-French task and use it to analyze phrase-based and neural systems. The resulting analysis provides not only a finer grained picture of the strengths of neural systems, but also insights into which linguistic phenomena remain out of reach."}, {"heading": "1 Introduction", "text": "The advent of neural techniques in machine translation (MT) (Kalchburner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) has led to profound improvements in MT quality. In particular, for \"simple\" language pairs such as English / French and English / Spanish, neural (NMT) systems are much closer to human performance than previous statistical techniques (Wu et al., 2016), putting pressure on automatic assessment metrics such as BLEU (Papineni et al., 2002) that exploit surface matrices that are relatively insensitive to subtle differences."}, {"heading": "2 Related Work", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves. (...) Most of them are able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. \"(...) Most of them are not able to abide by the rules.\""}, {"heading": "3 Challenge Set Evaluation", "text": "This particular language pair happened to be the most practical for us, but similar sentences can be built for any language pair. One aspect of MT performance that we wanted to exclude from our evaluation is the robustness of sparse data. To control this, when creating source and reference sentences, we chose words that appeared at least 100 times in the training corpus described in Section 4.1.11, with three principal exceptions: boeuf (87 occurrences) and buried (58 occurrences) - both components of idiomatic phrases - and lateralized (0 occurrences)."}, {"heading": "3.1 Building the Challenge Set", "text": "The challenging aspect of the test theorem presented here stems from the fact that the English theorems were chosen in such a way that their nearest French equivalent differs significantly structurally from the source. Translational divergences have been extensively studied in the past - see, for example, Vinay and Darbelnet, 1958; Dorr, 1994). We expect the degree of difficulty of an MT test theorem to correlate well with its density of divergence phenomena. We classify divergence phenomena into three main types: morphosyntactic, lexicosyntactic, and purely syntactic divergences."}, {"heading": "Morpho-syntactic divergences", "text": "When translating a word into the richer language, there is a need to recover additional grammatically relevant information from the context of the target language word. Note that in our defined cases, we only include those where the relevant information is available in the linguistic context. 2We lack the space to describe all the subtypes of morphosyntactic divergences that occur in our catalogue of demands but are illustrated by representative examples. A particularly important case is that of the Subject-Verb Convention. French verbs typically have more than 30 different flexed forms, while English verbs typically have 4 or 5. Consequently, English verb forms greatly underestimate their French counterparts. Much of the missing information has to be filled in by forced agreement in person, number and gender with the grammatical subject of the verb. However, extracting these parameters can prove difficult."}, {"heading": "Lexico-syntactic divergences", "text": "Syntactically governing words such as verbs tend to make certain demands on their complements: they subcategorize according to additions of a certain syntactic type. However, a source-language governor and his target-language counterpart may differ in terms of their respective requirements, and the translation of such words must then trigger adjustments in the target-language complement pattern. We can only examine some of the subtypes instantiated in our catalogue of requirements.A good example is the change of reasoning, referring to the situation in which the translation of a source verbs Vs like Vt is correct, but only on the condition that the arguments (usually subject and object) are reversed. \"The translation of\" miss \"as\" maneuvering a verb \"is one such case: John misses Mary \u2192 Mary manque a'John.If the switch is not executed, this leads to a severe case of slavery (usually subject and object).A second example of such a synxico-crossover of Terry is the translation of movement."}, {"heading": "Syntactic divergences", "text": "Some syntactical divergences do not relate to the presence of a particular lexical object, but rather to differences in the basic set of available syntactical patterns. Source-language instances of structures that do not exist in the target language must be mapped to equivalent structures. Here are some of the subtypes that appear in our catalogue of demands. French pronouns are an important case of divergence from English. French is basically an SVO language, just like English, but it differs from this canonical order when postverbal additions are pronounced: the pronouns must then be clicked, i.e. phonetically tied to the verb, in this case to the left side of the verb. He gave Mary a book."}, {"heading": "3.2 Evaluation Methodology", "text": "Given the very small size of our task, it is easy to make a human assessment of the respective results of a handful of different systems, and the obvious advantage is that the evaluation is absolute, not in relation to one or a few reference translations. The purpose of each set of tasks is to test one and only one system capability, namely the ability to handle the respective divergence subtype correctly. As shown in Figure 1, we provide commentators with a question specifying the divergence phenomenon that is currently being tested, together with a reference translation of the highlighted divergence ranges. As a result, the assessments are made simple: Has the desired divergence been correctly bridged, yes or no? 3 There is no need to mentally intersect across a number of different aspects of the test sentence, as one does when evaluating the global translation quality of a sentence, e.g. on a 5-point scale. However, we recognize that measuring the complexity of the translation maneuvers remains critical."}, {"heading": "4 Machine Translation Systems", "text": "We trained state-of-the-art neural and phrase-based systems for the English-French translation of data from the WMT 2014 evaluation."}, {"heading": "4.1 Data", "text": "We used the LIUM shared task subset of the WMT 2014 corpora, 4 which maintains the provided tokenization and corpus organization but maps characters in lowercase letters. Table 1 provides corpus statistics. 3 Sometimes, the system produces a translation that circumvents the divergence problem. For example, it can avoid divergence in adverbs by rephrasing the translation to use an adjective instead. In these rare cases, we instruct our commentators to refrain from judging whether or not the translation is correct. 4http: / / www.statmt.org / wmt14 / translation-task.html http: / / www-lium.univ-lemans.fr / \u0445 schwenk / nnmt-sharedtask"}, {"heading": "4.2 Phrase-based systems", "text": "To ensure a competitive PBMT baseline, we performed phrase extraction, performing both IBM4 and HMM alignments with a phrase length limitation of 7; after the frequency cut, the resulting phrase table contained 516M entries. For each extracted phrase pair, we collected statistics for the Galley and Manning hierarchical reordering model (2008).We trained an NNJM model (Devlin et al., 2014) on the HMM-aligned training corpus, with input and output vocabulary sizes of 64K and 32K. Words not included in the vocabulary were assigned to one of 100 mkcls classes. We trained for 60 epochs of 20K x 128 minibatches, yielding a final dev-determined perplexity of 6.88.Our set of protocol modules were assigned to one of 100 mkcls classes. We trained for 60 epochs of 20K x 128 minibatches and yielded a final dev-determined perplexity of 64K and 32K."}, {"heading": "4.3 Neural systems", "text": "To build our NMT system, we used the Nematus toolkit, 5, which implements a single-layer neural sequence-to-sequence architecture with attention (Bahdanau et al., 2015) and gated recurrent5https: / / github.com / rsennrich / nematusunits (Cho et al., 2014). We used 512-dimensional word embedding with source and target vocabulary sizes of 90K and 1024-dimensional state vectors. The model contains 172M parameters. We processed the data using a BPE model that we learned from the source and target corpora (Sennrich et al., 2016). Sentries longer than 50 words and 1024-dimensional state vectors. The training used the Adadelta algorithm (pointer, 2012), with a minibatch size of 100 and gradients running to 1.0 color points, with an Epoch of 30."}, {"heading": "5 Experiments", "text": "The 108-sentence Anglo-French challenge set out in Appendix A was submitted to the four MT systems described in Section 4: PBMT-1, PBMT-2, NMT and GNMT. Three bilingual native speakers of French rated each translated sentence as either a success or a failure in accordance with the protocol described in Section 3.2. For example, the 26 sentences in the subcategories S1-S5 of Appendix A. The corresponding translations were judged successful only if the translated verb correctly matches the translated subject."}, {"heading": "5.1 Quantitative comparison", "text": "Table 2 summarizes our results in terms of the percentage of successful translations, global, and across any type of divergence. For comparison with traditional metrics, we also include the BLEU values measured on the 2014 WMT testset. As we can see, the two PBMT systems perform very poorly in terms of our challenges, especially in 6https: / / translate.google.com the morphosyntactic and purely syntactic types. Their relatively better handling of lexicosyntactic cases probably reflects the fact that PBMT systems are inherently more attuned to lexical cues than to morphology or syntax. The two NMT systems are clear winners in all three categories. Overall, the GNMT system is best with a 68% success rate, probably due to the data and architectural factors mentioned in Section 4.3.7WMT."}, {"heading": "5.2 Qualitative assessment of NMT", "text": "We now turn to analyzing the strengths and weaknesses of neural MT through the microscope of our divergence categorization system, hoping that this could help focus future research on key issues. In this discussion, we ignore the results of PBMT-2 and compare: a) the results of PBMT-1 with those of NMT, since both systems were trained on the same dataset; and b) the results of these two systems with those of Google NMT, which was trained on a much larger dataset. For the rest of this section, we refer to the sets of our challenge, which are based on the subcategory-based numbering scheme S1-S267We cannot provide a full comparison with the Google system before NMT. However, in October 2016, we introduced a smaller 35-set version of our challenge for both the Google system and our PBMT-1 system. Google received only 4 of these examples correctly (11.4%), while our PT-1% (17.1%) got 17.1%."}, {"heading": "Strengths of neural MT", "text": "Overall, both neural MT systems perform much better than PBMT-1 in bridging divergences. \u2022 Their dramatic advantage over morphosyntactic divergences (a jump from 16% to 72% in the case of our two local systems) results from performances such as: \u2022 The subject's head nomenclature match characteristics appear to be correctly distributed to the verb phrase via intervening noun phrase additions (sentences S1a-c). \u2022 Subject agreement characteristics appear to be correctly distributed to each element of a coordinated verb phrase (S3a-c). \u2022 A large portion of the calculus at stake in the de-termination of the match characteristics of a subject noun phrase (see our relevant description in Section 3.1) appears to be correctly captured in the 12 translations of S4. \u2022 Most instances of the difficult case of participatory auxiliary SVP systems are also treated correctly according to the S5T structure."}, {"heading": "Weaknesses of neural MT", "text": "In fact, it is in such a way that one will be able to go to a place where one is able to go to another world, in which one is able to discover another world, in which one is able to explore another world, in which one is able to explore another world, in which one is able to create a new world, in which one is able to create a new world, in which one is able to create a new world, in which one is able to create a new world, in which one is able to invent another world, in which one is able to create a new world, in which one is able to create a new world, in which one is able to create a new world, in which one is able to create a new world, in which one is able to create a new world, in which one is able to create a new world, a new world, in which one is able to invent a new world, a new world, a new world, a new world, a new, a new world, a new, a new world, a new, a new, a new world, a new, a new, a new, a new, a new world, a new, a new, a new, a new, a new, a new world, a new, a new, a new, a new, a new, a new world, a new, a new, a new, a new, a new, a new, a new world, a new, a new, a new, a new, a new, a new, a new, a new, a new, a new world, a new, a new, a new, a new, a new, a new, a new, a new, a new, a new, a new world, a new, a new, a new, a new, a new, a new, a new, a new, a new, a new, a new world, a new, a new, a new, a new, a new, a new, a new, a new, a new, a world, a new, a new, a new, a new, a new, a new, a new, a new, a, a new, a new, a, a new, a new, a new, a, a, a new, a, a, a new, a new, a, a new, a, a, a new, a"}, {"heading": "6 Conclusions", "text": "We have presented a radically different way of assessing machine translation systems: the use of challenge sets designed to test MT systems on \"hard\" linguistic material, while offering a fine-grained linguistic classification of their successes and failures, an approach designed to complement rather than replace the traditional assessment tools of our community. Our proposed error categorization scheme allows different strengths and weaknesses of PBMT and neuronal MT to be brought to light. Except for the processing of idioms, it has been shown to be in favor of neuronal MT in all cases where a clear difference has been observed. A key factor in the superiority of NMT appears to be its ability to overcome many limitations of N-gram language modeling, which clearly plays a role in dealing with subject-verb agreements, double-object verbs, the overlapping sub-category frameworks and, not least, the Idiomatic Movement in this case, is a linguistics movement in French."}, {"heading": "7 Future Work", "text": "It is our hope that the findings derived from the evaluation of a new language pair will help to inspire future MT research and draw attention to the fact that even \"simple\" language pairs such as English-French still have many linguistic problems to solve. However, there are several ways to improve and expand our approach to the Challenge Set itself. First, it would be interesting to see if similar values can be achieved by automatic means. Human judgments for this set provide a gold standard against which proposed automatic judgments can be meta-evaluated. Second, constructing such a challenge set is as much an art as a science and requires an imprecise knowledge of the structural differences between the two languages of interest. A method to automatically create such a challenge for a new language pair would be extremely useful."}, {"heading": "Acknowledgments", "text": "We would like to thank Cyril Goutte, Eric Joanis and Michel Simard, who have graciously spent the time needed to evaluate the performance of four different MT systems on our challenge sets. We also thank Roland Kuhn for valuable discussions and comments on an earlier version of the paper."}, {"heading": "A Supplemental Material", "text": "The sentences are grouped according to linguistic categories and sub-categories, and for the sake of simplicity we also add a reference translation, which is a manual translation, which is supposed to be the simplest solution to the present divergence problem. Of course, this reference translation is rarely the only acceptable solution to the problem of divergence that is being sought. Our judges were given these references, but were instructed to use their knowledge of French to assess whether the divergence has been correctly bridged, regardless of the similarity of the translation to the reference. In all translations, the place of the desired divergence is highlighted in bold, and it is precisely in this part that our commentators are asked to give a verdict. For each system edition, we provide a summary of our commentator's judgments on how to deal with the phenomenon of interest. We mark the translation with a translation machine when it is provided with two more interesting dates, in order to also assess the deviation of the comments."}, {"heading": "Morpho-Syntactic", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "S-V agreement, across distractors", "text": "\"It is a question of reason.\" \"It is a question of reason.\" \"It is a question of reason.\" \"It is a question of reason.\" \"It is a question of reason.\" \"It is a question of reason.\" \"It is a question of reason.\" \"It is a question of reason.\" \"It is a question of reason.\" \"It is a question of reason.\" \"It is a question of reason.\" \"It is a question of reason.\" \"It is a question of reason.\" \"It is a question of reason.\" \"It is a question of reason.\" \"It is a question of reason.\" \"It is a question of reason.\" \"It is a question of reason.\""}, {"heading": "S-V agreement, coordinated targets", "text": "\"I am very happy that their politicians were more ignorant than stupid.Ref Leurs politiciens e,\" he says, \"but they do not know what to do.\" PBMT-1 Les-1 Les politiciens e. \"taient plus ignorant pides e.\" taient plus ignorant. \"NMT Leurs politiciens e.\" taient plus ignorant taient plus ignorant. \"taient plus ignorant and ignorant.\""}, {"heading": "S-V agreement, past participles", "text": "Are the markings of the marked parties the correct ones? (Past participle placed after aid AVOIR coincides with the verb object iff \"object iff\" precedes auxiliary. Otherwise, participle is in male singular form.).S5a Source The woman who saw a mouse in the corridor is charming. Ref La femme qui a vu une souris dans le couloir est charmante. PBMT-1 La femme qui a vu une souris dans le couloir est charmante. NMT La femme qui a vu une souris dans le couloir est charmante. \u2022 Google La femme qui a vu une souris dans le couloir est charmante. \u2022 S5b Source dans le couloir est charmante."}, {"heading": "Subjunctive mood", "text": "Is the marked verb in the right mood? (Certain triggering verbs, adjectives or subordinate conjunctions cause the subjunctive mood in the subordinate clause governed by them.).S6a Source He will come, provided that you come. Ref Il viendra a \"condition que vous veniez aussi. PBMT-1 Il viendra a\" condition que vous venez aussi. \"Ref Il est malheureux qu'il ne vienne pas non plus. PBMT-1 Il est regrettlich qu'il n'est pas non plus a\" venez aussi. \"S6b Source It is regrettable that he does not come. Ref Il est malheureux qu'il\" il \"il\" il regrettlich qu'il n'est pas non plus a \"venir.\""}, {"heading": "Lexico-Syntactic", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Argument switch", "text": "Are the experienced and the object of the \"missing\" situation correctly preserved in the French translation? (Argument switch). S7a Source Mary misses Jim.Ref Jim manque cruel cruelty a \"Mary. PBMT-1 Marie manque cruel cruelty de Jim. \u2717 NMT Mary a lamentabment manque\" de Jim. \"Google Mary manque cruel cruelty a\" Jim. \"S7b Source My sister really misses New York.Ref New York manque beaucoup a\" ma s\u0153ur. \"PBMT-1 Ma s\u0153ur est vraiment absent de New York.\" Ref Ma s\u0153ur est vraiment manquante a \"New York.\" Google Ma s\u0153ur manque vraiment New York. \""}, {"heading": "Double-object verbs", "text": "Are \"gift\" and \"recipient\" arguments reproduced correctly in French? (English double object constructions) S8a source John gave his wonderful wife a nice gift. Ref John a donne \"un beau pre\" sent a \"sa merveilleuse e\" pouse. PBMT-1 John a donne \"sa merveilleuse femme un beau cadeau.\" NMT John a donne \"a\" sa merveilleuse femme un beau cadeau. \"Google John a donne\" a \"son e\" pouse merveilleuse un pre \"sent gentil.\" S8b source John told the children a nice story. Ref John a raconte \"une belle histoire aux enfants. PBMT-1 John a dit aux enfants une belle histoire.\" \"NMT John a dit aux enfants une histoire.\""}, {"heading": "Fail to", "text": "S9a Source John has not recognized the relevance of this point. Ref John n'a pas vu la pertinence de ce point. PBMT-1 John a omis de voir la pertinence de ce point. Ref Il n'a pas re \"pondu. PBMT-1 Il n'a pas re\" ussi a \"re\" pondre. \"NMT Il n'a pas re\" pondu. \"Google Il n'a pas re\" pondu. \"S9c Source Those who do not comply with this requirement will be punished. Ref Ceux qui ne se conformise pas a\" cette exigence seront pe \"nalise.\" PBMT-1 Source Those who do not comply with this requirement will be punished."}, {"heading": "Manner-of-movement verbs", "text": "Is the movement action in the English source reproduced correctly in French? (Mannerof movement verbs with path argument may need to be rephrased in French.) S10a source John would like to swim across the river. Ref John aimerait traverser la rivie. PBMT-1 John aimerait nager dans la rivie."}, {"heading": "Overlapping subcat frames", "text": "Is the French verb for \"knowing\" correctly chosen? (Choice between \"savoir\" / \"conna\u0131\" bill \"depends on the syntactic nature of its object) S11a Source Paul knows this is a fact.Ref Paul sait que c'est un fait. PBMT-1 Paul sait que un fait. PBMT-1 Paul sait que que un fait."}, {"heading": "Factitives", "text": "Is the English verb correctly rendered in the French translation? (Agentive use of some French verbs requires embedding it under \"fair.\") S13a Source John cooked a large chicken. Ref John a fait cuire un gros poulet. PBMT-1 John cuit un gros poulet. \u2022 NMT John cuit un gros poulet. \u2022 Google John a fait cuire un gros poulet. \u2713 S13b Source John melted a lot of ice. Ref John a fait fondre beaucoup de glace. PBMT-1 John fondu a lot of ice. \u2022 NMT John a fondu beaucoup de glace. \u2022 Google John a fondu beaucoup de glace. \u2022 S13c Source She loves to grow flowers. Ref Elle aime faire pousser des fleurs."}, {"heading": "Noun Compounds", "text": "Is the English nominal composition with the correct preposition in the French translation? S14a Source Use the meat knife. Ref Utilisez le couteau a viande. PBMT-1 Source Use the butter knife.Ref Utilisez le couteau a viande. PBMT-1 Utilisez le couteau a viande."}, {"heading": "Common idioms", "text": "S15a Source Stop to the bush.Ref Cessez de tourner autour du pot. \u2713 S15b Source You put the cart in front of the horse. Ref Vous mettez la charrue devant les b\u0153ufs. PBMT-1 Vous pouvez mettre la charrue avant les b\u0153ufs. \u2713 NMT Vous mettez la charrue avant le cheval. Ref Vous mettez le chariot devant les b\u0153ufs. PBMT-1 Vous pouvez mettre la charrue avant les b\u0153ufs."}, {"heading": "Syntactically flexible idioms", "text": "S16a Source The cart was pulled in front of the horse.Ref La charrue a e'te'mise devant les b\u0153ufs. PBMT-1 On met la charrue devant le cheval. \u2717 NMT Le chariot a e \"te\" mis avant le cheval. \u2717 Google Le chariot a e \"te\" mis devant le cheval. \"S16b Source This argument hit the nail on the head. Ref Avec cet argument, la cause est entendue. PBMT-1 Avec cette argument, l'ongle a e\" te \"frappe\" e a \"la te-te-te-te-te.\""}, {"heading": "Syntactic", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Yes-no question syntax", "text": "Is the English question correctly reproduced as a French question? S17a Source Have the children ever seen this film? Ref Les enfants ont-ils de \"ja\" vu ce film? PBMT-1 Les enfants jamais regarde \"ce film? \u2717 NMT Les enfants ont-ils de\" ja \"regard\" ce film? \u2713 Google Les enfants ont-ils de \"ja\" regard \"ce film? \u2022 S17b Source Hasn your boss refuses you promotion? Ref Votre Patron ne vous a-t-il pas refuse\" une promotion? PBMT-1 N'a pas nie \"votre patron vous un promotion? \u2022 NMT Est-ce que votre patron vous a refuse\" une promotion? \u2022 Google Votre patron ne vous a-t-il pas refuse \"une promotion? \u2022 S17c Source Should I not attend this meeting?"}, {"heading": "Tag questions", "text": "S18a Source Mary looked really happy tonight, didn't she? Ref Mary avait l'air vraiment heureuse ce soir, n'est-ce pas? PBMT-1 Marie a regarde \"vraiment heureux de ce soir, n'est-ce pas elle?\" NMT Mary s'est montre \"e vraiment heureuse ce soir, ne l'a pas fait?\" Google Mary avait l'air vraiment heureuse ce soir, n'est-ce pas? \"S18b Source We shouldn't do this again, should we? Ref Nous ne devrions pas refaire cela, n'est-ce pas pas? PBMT-1 Nous ne devrions pas'est\" Elfaire-e faire?"}, {"heading": "WH-MVT and stranded preps", "text": "Is the dangling preposition of the English sentence correctly placed in the French translation? S19a Source The guy she goes out with is handsome.Ref Le type avec qui elle sort est beau. PBMT-1 Le mec qu'elle va sortir avec est beau. \u2717 NMT Le mec qu \"elle sort avec est beau.\" Google Le mec avec qui elle sort est beau. \"S19b Source Who is she running out of these days? Ref Avec qui sort-elle ces jours-ci? PBMT-1 Qu'est-ce ville qu'elle allait sortir avec ces jours.\""}, {"heading": "Adverb-triggered inversion", "text": "Is the inversion of the subject verb caused by adverb in the English sentence correctly reproduced in the French translation? S20a Source Rarely did the dog run out.Ref Rarement le chien courait-il. PBMT-1 Rarement le chien courir. \u2022 NMT Il est rare que le chien marche. \u2022 Google Rarement le chien courir. \u2022 S20b Source Never before has it been so unhappy. Ref Jamais encore n'avait-elle e \"te\" aussi malheureuse. PBMT-1 Jamais auparavant, si elle avait e \"te\" si malheureux."}, {"heading": "Middle voice", "text": "Is the generic statement made in the English sentence reproduced correctly and naturally in the French translation? S21a Quelle soup is eaten with a large spoon. Ref La soupe se mange avec une grande cuille: re PBMT-1 La soupe est mange: avec une grande cuille: re. \u2022 NMT La soupe est consomme: e avec une grosse cuille: re Google La soupe est consomme: e avec une grande cuille: re. \u2022 S21b Quelle Masonry is cut with a diamond blade."}, {"heading": "Fronted \u201cshould\u201d", "text": "S22a Quelle Should Paul go, I would be sad. Ref Si Paul devait s'en aller, je serais triste. S22b Quelle Should he become president, she would be promoted immediately. Ref S'il devait devenir pre-sident, elle recevrait imme \"diatement une promotion. PBMT-1 S'il devait devenir triste. S22b Quelle Should he become president, she would be promoted immediately. Ref S'il devait devenir pre-sident, elle recevrait imme\" diatement une promotion. PBMT-1 S'il devait devenir pre-president, elle serait prevenir devenir pre-sident, elle serait diil diil \"il diatement.\""}, {"heading": "Clitic pronouns", "text": "S23a Source She had a lot of money but he had none. Ref Elle avait beaucoup d'argent pas. \u2713 NMT Elle avait beaucoup d'argent, mais il n'a pas eu d'argent. \u2713 Google Elle avait beaucoup d'argent mais il n'en avait pas. \u2713 S23b Source He did not talk to them very often. Ref Il ne leur parlait pas tre s souvent. PBMT-1 Il n'a pas leur parler tre s souvent."}, {"heading": "Ordinal placement", "text": "Is the relative order of ordinals and digits in the French translation correct? S24a Source The first four men were exhausted. Ref Les quatre premiers hommes e \"taient tous e\" puise \"s. PBMT-1 Les quatre premiers hommes e\" taient e \"puise\" s. \"NMT Les quatre premiers hommes e\" te \"e\" puise \"s.\" Google Les quatre premiers hommes e \"taient e\" puise \"s.\" S24b Source The last three candidates were eliminated. Ref Les trois derniers candidats ont e \"e\" limine \"s.\" PBMT-1 Les trois derniers candidats ont e \"es\" e \"limine\" s \"s\" s. \"\" \"NMT Les trois derniers\" s. \""}, {"heading": "Inalienable possession", "text": "Is the French translation correct and of course both in: a) the use of a certain determinator on the body part noun; and b) the presence or absence of a reflective pronoun before the verb? S25a source He washed his hands. Ref Ils s'est lave \u0301 les mains. PBMT-1 Ils se lavait les mains."}, {"heading": "Zero REL PRO", "text": "S26a Source The strangers that the woman saw were working. Ref Les inconnus que la femme vit travaillaient. PBMT-1 Les e'trangers la femme vit travaillaient. \u2713 S26b Source The man that your sister hates is vicious. Ref L'homme que votre s\u0153ur de \"teste est me\" chant. PBMT-1 L'homme ta'trangers que la femme vit travaillaient. \u2713 NMT L'homme que ta soeur hait est le mal. \u2713 S26c Source The girl that my friend spoke of is gone.f La fille dont parmon parami le la'homme que votre s\u0153ur hait est me \"sang."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the Third International Conference on Learning Representations (ICLR). San Diego, USA.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Neural versus phrase-based machine translation quality: a case study", "author": ["Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo", "Marcello Federico."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural", "citeRegEx": "Bentivogli et al\\.,? 2016", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2016}, {"title": "Findings of the 2016 conference on machine translation", "author": ["Popel", "Matt Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri."], "venue": "Proceedings of the First Conference on Ma-", "citeRegEx": "Popel et al\\.,? 2016", "shortCiteRegEx": "Popel et al\\.", "year": 2016}, {"title": "Improved reordering for phrase-based translation using sparse features", "author": ["Colin Cherry."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Cherry.,? 2013", "shortCiteRegEx": "Cherry.", "year": 2013}, {"title": "Batch tuning strategies for statistical machine translation", "author": ["Colin Cherry", "George Foster."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Cherry and Foster.,? 2012", "shortCiteRegEx": "Cherry and Foster.", "year": 2012}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Jacob Devlin", "Rabih Zbib", "ZhongqiangHuang", "Thomas Lamar", "Richard Schwartz", "John Makhoul."], "venue": "Proceedings of the 52nd Annual Meeting of the Asso-", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Machine translation divergences: a formal description and proposed solution", "author": ["Bonnie J. Dorr."], "venue": "Computational Linguistics 20:4.", "citeRegEx": "Dorr.,? 1994", "shortCiteRegEx": "Dorr.", "year": 1994}, {"title": "QCRI machine translation systems for IWSLT 16", "author": ["Nadir Durrani", "Fahim Dalvi", "Hassan Sajjad", "Stephan Vogel."], "venue": "Proceedings of the 13th InternationalWorkshop on Spoken Language Translation (IWSLT). Seattle, Washington.", "citeRegEx": "Durrani et al\\.,? 2016", "shortCiteRegEx": "Durrani et al\\.", "year": 2016}, {"title": "A simple and effective hierarchical phrase reordering model", "author": ["Michel Galley", "Christopher D. Manning."], "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing. Association for Computational", "citeRegEx": "Galley and Manning.,? 2008", "shortCiteRegEx": "Galley and Manning.", "year": 2008}, {"title": "Forest rescoring: Faster decoding with integrated language models", "author": ["Liang Huang", "David Chiang."], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Lin-", "citeRegEx": "Huang and Chiang.,? 2007", "shortCiteRegEx": "Huang and Chiang.", "year": 2007}, {"title": "Is neural machine translation ready for deployment? a case study on 30 translation directions", "author": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Hieu Hoang."], "venue": "Proceedings of the 13th International Workshop on Spoken Language Translation", "citeRegEx": "Junczys.Dowmunt et al\\.,? 2016a", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "The amu-uedin submission to the wmt16 news translation task: Attention-based nmt models as feature functions in phrase-based smt", "author": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Rico Sennrich."], "venue": "Proceedings of the First Conference", "citeRegEx": "Junczys.Dowmunt et al\\.,? 2016b", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "How grammatical is characterlevel neural machine translation? assessing MT quality with contrastive translation pairs", "author": ["Rico Sennrich."], "venue": "CoRR abs/1612.04629. http://arxiv.org/abs/1612.04629.", "citeRegEx": "Sennrich.,? 2016", "shortCiteRegEx": "Sennrich.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems 27. Curran Associates, Inc., pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A multifaceted evaluation of neural versus statistical machine translation for 9 language directions", "author": ["Antonio Toral", "V\u0131\u0301ctorM. S\u00e1nchez-Cartagena"], "venue": "In Proceedings of the The 15th Conference of the European Chapter of the Association", "citeRegEx": "Toral and S\u00e1nchez.Cartagena.,? \\Q2017\\E", "shortCiteRegEx": "Toral and S\u00e1nchez.Cartagena.", "year": 2017}, {"title": "Stylistique compar\u00e9e du fran\u00e7ais et de l\u2019anglais, volume 1", "author": ["Jean-Paul Vinay", "Jean Darbelnet."], "venue": "Didier, Paris.", "citeRegEx": "Vinay and Darbelnet.,? 1958", "shortCiteRegEx": "Vinay and Darbelnet.", "year": 1958}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "CoRR abs/1212.5701. http://arxiv.org/abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 12, "context": "The advent of neural techniques in machine translation (MT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) has led to profound improvements in MT quality.", "startOffset": 60, "endOffset": 134}, {"referenceID": 16, "context": "The advent of neural techniques in machine translation (MT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) has led to profound improvements in MT quality.", "startOffset": 60, "endOffset": 134}, {"referenceID": 13, "context": "This puts pressure on automatic evaluation metrics such as BLEU (Papineni et al., 2002), which exploit surface-matching heuristics that are relatively insensitive to subtle differences.", "startOffset": 64, "endOffset": 87}, {"referenceID": 10, "context": "Since then, controlled comparisons have used BLEU to show that NMT outperforms strong PBMT systems on 30 translation directions from the United Nations Parallel Corpus (Junczys-Dowmunt et al., 2016a), and on the IWSLT English-Arabic tasks (Durrani et al.", "startOffset": 168, "endOffset": 199}, {"referenceID": 7, "context": ", 2016a), and on the IWSLT English-Arabic tasks (Durrani et al., 2016).", "startOffset": 48, "endOffset": 70}, {"referenceID": 1, "context": "Bentivogli et al. (2016) carried out a number of experiments on IWSLT 2015 EnglishGerman evaluation data, where they compare machine outputs to professional post-edits in order to automatically detect a number of error categories.", "startOffset": 0, "endOffset": 25}, {"referenceID": 1, "context": "Bentivogli et al. (2016) carried out a number of experiments on IWSLT 2015 EnglishGerman evaluation data, where they compare machine outputs to professional post-edits in order to automatically detect a number of error categories. Compared to PBMT, NMT required less postediting effort over-all, with substantial improvements in lexical, morphological and word order errors. NMT consistently out-performed PBMT, but its performance degraded faster as sentence length increased. Later, Toral and S\u00e1nchez-Cartagena (2017) conducted a similar study, examining the outputs of competition-grade systems for the 9 WMT 2016 directions that included NMT competitors.", "startOffset": 0, "endOffset": 520}, {"referenceID": 14, "context": "Most recently, Sennrich (2016) proposed an approach to perform targeted evaluations of NMT through the use of contrastive translation pairs.", "startOffset": 15, "endOffset": 31}, {"referenceID": 14, "context": "Manual evaluation side-steps some of the pitfalls that can come with Sennrich (2016)\u2019s contrastive pairs, as a ranking of two contrastive sentences may not necessarily reflect whether the error in question will occur in the system\u2019s actual output.", "startOffset": 69, "endOffset": 85}, {"referenceID": 18, "context": "Translational divergences have been extensively studied in the past \u2013 see for example (Vinay and Darbelnet, 1958; Dorr, 1994).", "startOffset": 86, "endOffset": 125}, {"referenceID": 6, "context": "Translational divergences have been extensively studied in the past \u2013 see for example (Vinay and Darbelnet, 1958; Dorr, 1994).", "startOffset": 86, "endOffset": 125}, {"referenceID": 5, "context": "We trained an NNJM model (Devlin et al., 2014) on the HMM-aligned training corpus, with input and output vocabulary sizes of 64K and 32K.", "startOffset": 25, "endOffset": 46}, {"referenceID": 4, "context": "Tuning was carried out using batch lattice MIRA (Cherry and Foster, 2012).", "startOffset": 48, "endOffset": 73}, {"referenceID": 5, "context": "For each extracted phrase pair, we collected statistics for the hierarchical reordering model of Galley and Manning (2008). We trained an NNJM model (Devlin et al.", "startOffset": 97, "endOffset": 123}, {"referenceID": 3, "context": "Our set of log-linear features consisted of forward and backward Kneser-Ney smoothed phrase probabilities and HMM lexical probabilities (4 features); hierarchical reordering probabilities (6); the NNJM probability (1); a set of sparse features as described by Cherry (2013) (10,386); wordcount and distortion penalties (2); and 5-gram language models trained on the French half of the training corpus and the French monolingual corpus (2).", "startOffset": 260, "endOffset": 274}, {"referenceID": 3, "context": "Our set of log-linear features consisted of forward and backward Kneser-Ney smoothed phrase probabilities and HMM lexical probabilities (4 features); hierarchical reordering probabilities (6); the NNJM probability (1); a set of sparse features as described by Cherry (2013) (10,386); wordcount and distortion penalties (2); and 5-gram language models trained on the French half of the training corpus and the French monolingual corpus (2). Tuning was carried out using batch lattice MIRA (Cherry and Foster, 2012). Decoding used the cube-pruning algorithm of Huang and Chiang (2007), with a distortion limit of 7.", "startOffset": 260, "endOffset": 583}, {"referenceID": 0, "context": "To build our NMT system, we used the Nematus toolkit, which implements a single-layer neural sequence-to-sequence architecture with attention (Bahdanau et al., 2015) and gated recurrent", "startOffset": 142, "endOffset": 165}, {"referenceID": 15, "context": "We preprocessed the data using a BPE model learned from source and target corpora (Sennrich et al., 2016).", "startOffset": 82, "endOffset": 105}, {"referenceID": 19, "context": "Training used the Adadelta algorithm (Zeiler, 2012), with a minibatch size of 100 and gradients clipped to 1.", "startOffset": 37, "endOffset": 51}, {"referenceID": 10, "context": "To decode, we used the AmuNMT decoder (Junczys-Dowmunt et al., 2016a) with a beam size of 4.", "startOffset": 38, "endOffset": 69}, {"referenceID": 10, "context": "Following Junczys-Dowmunt et al. (2016b), we averaged the parameters from the last 8 checkpoints.", "startOffset": 10, "endOffset": 41}], "year": 2017, "abstractText": "Neural machine translation represents an exciting leap forward in translation quality. But what longstanding weaknesses does it resolve, and which remain? We address these questions with a challenge set approach to translation evaluation and error analysis. A challenge set consists of a small set of sentences, each hand-designed to probe a system\u2019s capacity to bridge a particular structural divergence between languages. To exemplify this approach, we present an English-French challenge set, and use it to analyze phrase-based and neural systems. The resulting analysis provides not only a more fine-grained picture of the strengths of neural systems, but also insight into which linguistic phenomena remain out of reach.", "creator": "LaTeX with hyperref package"}}}