{"id": "1704.07489", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Multi-Task Video Captioning with Video and Entailment Generation", "abstract": "Video captioning, the task of describing the content of a video, has seen some promising improvements in recent years with sequence-to-sequence models, but accurately learning the temporal and logical dynamics involved in the task still remains a challenge, especially given the lack of sufficient annotated data. We improve video captioning by sharing knowledge with two related directed-generation tasks: a temporally-directed unsupervised video prediction task to learn richer context-aware video encoder representations, and a logically-directed language entailment generation task to learn better video-entailing caption decoder representations. For this, we present a many-to-many multi-task learning model that shares parameters across the encoders and decoders of the three tasks. We achieve significant improvements and the new state-of-the-art on several standard video captioning datasets using diverse automatic and human evaluations. We also show mutual multi-task improvements on the entailment generation task.", "histories": [["v1", "Mon, 24 Apr 2017 23:07:32 GMT  (4376kb)", "https://arxiv.org/abs/1704.07489v1", "Accepted at ACL 2017 (13 pages w/ supplementary)"], ["v2", "Tue, 8 Aug 2017 17:08:58 GMT  (5518kb)", "http://arxiv.org/abs/1704.07489v2", "ACL 2017 (14 pages w/ supplementary)"]], "COMMENTS": "Accepted at ACL 2017 (13 pages w/ supplementary)", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["ramakanth pasunuru", "mohit bansal"], "accepted": true, "id": "1704.07489"}, "pdf": {"name": "1704.07489.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["mbansal}@cs.unc.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.07 489v 2 [cs.C L] 8A ug2 017Video subtitling, the task of describing the content of a video, has seen some promising improvements with sequence-to-sequence models in recent years, but learning the temporal and logical dynamics associated with the task remains a challenge, especially given the lack of sufficient annotated data. We improve video subtitling by sharing knowledge with two related tasks of the directed generation: a time-driven, unattended video prediction task to learn richer context-sensitive video encoder representations, and a logically controlled voice generation task to learn better video conditional capture decoder representations. To this end, we present a multi-task learning model that splits parameters between the encoders and decoders of the three tasks."}, {"heading": "1 Introduction", "text": "It has various applications such as assisting a visually impaired person and improving the quality of online video search or retrieval. This task has recently gained momentum in natural language processing and in computer vision communities, especially with the advent of powerful image editing functions and sequence-to-sequence LSTMmodels. It is also a step forward from static caption, because in addition to modeling spatial visual features, the model also needs to learn cross-temporal action dynamics and the logical action of logical storyline language dynamics. Previous work in video sequencing (Venugopalan et al al.) has shown that recurrent neural networks (RNails) are a good choice for modeling the temporal information in the video."}, {"heading": "2 Related Work", "text": "In 2016, we used a two-step pipeline to first triple-extract a subject, a verb, and an object (S, V, O) and then generate a sentence based on it. However, in order to time the important image sequence, Venugopalan et al., Venugopalan et al., 2015a) proposed a visual frame-to-sequence model of visual functions (of conventional neural networks prepared for image recognition) to take advantage of the temporal task, Venugopalan et al., 2015a) proposed a sequence-to-sequence model with video encoder and speech decoder RNs. Recently, Venugopalan et al al al al al al al al al al al al al al al al al al al al. (2016) explored linguistic improvements in the image sequence through the use of external language models. In addition, an attention or an alignment mechanism was added to learn the relationships between the encoder and the decoder or an object."}, {"heading": "3 Models", "text": "We will first discuss a simple encoder decoder model as a basic reference for video captioning. Next, we will improve this with an attention mechanism. Finally, we will present similar models for the unattended prediction and generation tasks of videos, and then combine them with captioning videos using the multi-task approach."}, {"heading": "3.1 Baseline Sequence-to-Sequence Model", "text": "Our base model is similar to the standard RNNmodel encoder decoder (Sutskever et al., 2014), in which the final state of the RNN encoder is entered as the initial state for the RNN decoder, as shown in Fig. 2. The RNN is based on Long Short Term Memory (LSTM) units that can remember long sequences due to oblivion gates (Hochreiter and Schmidhuber, 1997). In the video caption, our input to the encoder is the video image characteristics 1 {f1, f2,..., fn} of the length n and the caption word sequence {w1, w2,..., wm} of the length m is generated during the decryption phase.The distribution of the output sequence w.r.t. The input sequence is: p (w1 \u2212 f1,..., fn) = 1p (wt | hd) of the output sequence r.w.The output state is distributed over the 1 and 1 sequence."}, {"heading": "3.2 Attention-based Model", "text": "Our attention model architecture is similar to Bahdanau et al. (2015), with a bidirectional LSTMRNN as encoder and a unidirectional LSTMRNN as decoder, see Fig. 3. At each step t, the LSTM hidden state hdt decoder is a non-linear, recurring function of the previous decoder hidden state hdt \u2212 1, of the word generated in the previous time step wt \u2212 1 and of the context vector ct: hdt = S (h d \u2212 1, wt \u2212 1, ct) (2) 1We use several popular image functions such as VGGNet, GoogLeNet and Inception-v4. Details in Sec. 4.1.where ct is a weighted sum of hidden states of the encoder {hei}: ct = n \u2211 i = 1\u03b1t, ih e i (3) These attention weights act as a sequence mechanism by assigning higher weights to specific states, which are better coordinated with each other (W = p)."}, {"heading": "3.3 Unsupervised Video Prediction", "text": "We model the unattended video representation by predicting the sequence of future video images taking into account the current image sequence. Similar to paragraph 3.2, a bi-directional LSTM RNN encoder and an LSTM RNN decoder are used together with attention. If the image plane characteristics of a video are n {f1, f2,..., fn}, they are split into two groups, so that the model must predict (decode) the rest of the frames in view of the current frames {f1, f2,.., fk} (in its encoder), so that the video encoder learns rich temporal representations that are aware of their action-based context and also robust to missing frames and varying image lengths or motion speeds. The optimization function is defined as: minimizing erroneous step \u2212 k \u2212 fk = \u2212 2 \u2212 where \u2212 t = 1 \u2212 dt \u2212 1 is predicted."}, {"heading": "3.4 Entailment Generation", "text": "Considering a sentence (premise), the task of the sequence generation is to generate a sentence (hypothesis) that is a logical deduction or implication of the premise. Our sequence generation model, in turn, uses a bidirectional LSTM RNN encoder and an LSTM RNN decoder with an attention mechanism (similar to paragraph 3.2). If the premise sp is a word sequence {wp1, w p 2,..., w p n} and the hypothesis sh {wh1, w h 2,..., w h m}, the distribution of the associated hypothesis is w.r.t. The premise is: p (wh1,..., w h m | w p 1,..., w p p n) = m, t = 1p (wht | h d t) (8), where the distribution p (wht | h d t) is reached over all words in the vocabulary and the decoder state hqn is similar."}, {"heading": "3.5 Multi-Task Learning", "text": "Our primary goal is to improve the video captioning model, in which visual content translates into a textual form to a directed (and related) generation, giving an interesting opportunity to share temporally and logically directed knowledge with both visual and linguistic generation tasks. Figure 4 shows our general multi-task model for learning video captioning, unattended video prediction, and text sequence generation together. Here, the video captioning task shares its video encoder (parameter) with the video preview task (one-too-many settings) encoder to learn context-aware and time-driven visual representations (see Section 3.3). In addition, the video captioning task is shared with the video captioning task (parameter) decoder (video preview task) to bring the subtitles to life. \""}, {"heading": "4 Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "4.2 Evaluation (Automatic and Human)", "text": "For our video captioning and the results of subsequent generation, we use four different automatic rating metrics that are popular for image / video captioning and voice captioning in general: METEOR (Denkowski and Lavie, 2014), BLEU-4 (Papineni et al., 2002), CIDEr-D (Vedantam et al., 2015) and ROUGE-L (Lin, 2004). METEOR and CIDEr-D in particular were justified for generation tasks, as CIDEr-D relies on consensus among the (large) number of references and METEOR is based on soft matching based on stems, paraphrasing and WordNet synonyms. We use the standard evaluation code of the Microsoft COCO server (Chen et al., 2015) to obtain these results and also to compare the results with previous papers."}, {"heading": "4.3 Training Details", "text": "We adjust all hyperparameters to the dev splits: hidden state variable LSTM-RNN, learning rate, weight initializations and mixing ratios in the minibatch (additional tuning ranges).We use the following settings in all our models (unless otherwise stated): We unroll video encoders / decoder RNNNs in 50 time steps and language encoders / decoder RNNNs in 30 time steps. We use a 1024-dimensional RNN hidden state variable and 512-dim vectors to embed visual features and word vectors. We use Adam optimizers (Kingma and Ba, 2015).We apply a drop-out of 0.5."}, {"heading": "5 Results and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Video Captioning on YouTube2Text", "text": "In fact, it is such that most of us will be able to move into another world, in which they can move into another world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "5.2 Video Captioning on MSR-VTT, M-VAD", "text": "In Table 2, we train and evaluate our definitive multiple-to-multiple multi-task model on two additional video subtitle datasets (using their standard splits; details supplemented).First, we evaluate the new MSR VTT dataset (Xu et al., 2016).Since it is a current dataset, we list the results of previous work reported by the MSR VTT dataset.8 We significantly improve on all of these datasets. In addition, they maintain a leader board 9 on this dataset and we also report on the top 3 systems from it. Based on their ranking methodology, our multi-task model reaches the new 1st place in this leader.In Table 3, we continue to evaluate our model on the challenging film-based M-VAD dataset and achieve further improvements over all previous work (Venugopalan et al., 2015a; 7multiple-to-multiple-to-metals-one)."}, {"heading": "5.3 Entailment Generation Results", "text": "Next, we show that the task of creating subtitles conversely supports the task of creating subtitles. Starting from a premise, the task of creating subtitles is to generate a related hypothesis using only the subset of the subtitle pairs of the SNLI corpus, but with a multi-reference split setup to enable automatic metric evaluation and overlapping of premises (see paragraph 4.1). All hyperparameter details (also matched to the validation set) are presented in the supplement. Table 4 shows the results of creating subtitles for the baseline (sequence-to-sequence with attention, 3 ensemble, beam search) and the multi-task model that uses video subtitling (split decoder) above the baseline. A mixing ratio of 100: 20 miniature batches alternating with attention, 3 ensemble, beam search) and the multi-task model that uses video subtitling (split decoder) above the baseline."}, {"heading": "5.4 Human Evaluation", "text": "In addition to automated evaluation metrics, we present pilot-scale human evaluations based on the results of YouTube2Text (Table 1) and genesis history (Table 4). In any case, we compare our strongest baseline with our definitive multi-task model (M-to-M in the case of video labels and M-to-1 in the case of matchups). We evaluate a random sample of 300 generated captions (or associated hypotheses) from the test set of three human evaluators. We remove the model identity to anonymize the two models, and ask human evaluators to select the better model based on relevance and coherence (described in Sec. 4.2). As shown in Table 5 and Table 6.10After previous work, we only use METEOR because M-VAD has a single reference label per video."}, {"heading": "5.5 Analysis", "text": "Fig. 5 shows the results of video caption generation on the YouTube2Text dataset, which compares our final M-to-M multi-task model with our strongest attention-based base model for three categories of videos: (a) complex examples in which the multi-task model performs better than the baseline; (b) ambiguous examples (i.e., confusing the basic truth itself) in which the multi-task model still correctly predicts one of the possible categories (c) complex examples in which both models perform poorly. Overall, we find that the multi-task model generates labels that are better in both time-to-video-based multi-task prediction and logical entanglement (i.e. the correct subset of the full video premise) w.r.The supplementary document also provides examples of improvements made by the single-to-video multi-task model alone are better than the single-to-M task model."}, {"heading": "6 Conclusion", "text": "We presented a multimodal, multi-task-based learning approach to enhance video captioning by incorporating temporally and logically directed knowledge through video prediction and engagement-generating tasks. We achieve the best reported results (and rank) on three sets of data, based on multiple automatic and human assessments. We also show mutual improvements of tasks to the new engagement-generating task. In future work, we will apply our entailment-based multi-task paradigm to other language-generating tasks such as captions and document summaries."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their helpful comments. This work was partially supported by a Google Faculty Research Award, an IBM Faculty Award, a Bloomberg Data Science Research Grant and NVidia GPU Awards."}, {"heading": "A Experimental Setup", "text": "This year, it will be able to take the lead, \"he said in an interview with the Taiwanese daily Le."}, {"heading": "B Training Details", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "C Analysis", "text": "In paragraph 5.5 of the main paper, we discussed examples in which we compared the generated captions of the final multiple-to-multiple-task model with those of the baseline. As shown in Table 8, our one-to-multiple-task model better identifies actions and objects compared to the baseline, because the video prediction task helps to better learn context-conscious visual representations, e.g. \"a man eats something\" vs. \"a man drinks something\" and \"a woman cuts a vegetable\" vs. \"a woman cuts an onion.\" On the other hand, the multiple-to-multiple task (with withdrawal generation) seems to be more involved in generating a caption that represents a logistically implied withdrawal situation, e.g. \"a cat plays with a cat\" vs. \"a cat plays a woman\" and \"speaks a woman.\""}], "references": [{"title": "Multi-task feature learning", "author": ["Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil."], "venue": "NIPS.", "citeRegEx": "Argyriou et al\\.,? 2007", "shortCiteRegEx": "Argyriou et al\\.", "year": 2007}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyunghyunCho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning."], "venue": "EMNLP.", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Learning to learn, Springer, pages 95\u2013133.", "citeRegEx": "Caruana.,? 1998", "shortCiteRegEx": "Caruana.", "year": 1998}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["David L Chen", "William B Dolan."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association for Com-", "citeRegEx": "Chen and Dolan.,? 2011", "shortCiteRegEx": "Chen and Dolan.", "year": 2011}, {"title": "Microsoft COCO captions: Data collection and evaluation server", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C Lawrence Zitnick."], "venue": "arXiv preprint arXiv:1504.00325 .", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Michael Denkowski", "Alon Lavie."], "venue": "EACL.", "citeRegEx": "Denkowski and Lavie.,? 2014", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "An introduction to the bootstrap", "author": ["Bradley Efron", "Robert J Tibshirani."], "venue": "CRC press.", "citeRegEx": "Efron and Tibshirani.,? 1994", "shortCiteRegEx": "Efron and Tibshirani.", "year": 1994}, {"title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot", "author": ["Sergio Guadarrama", "Niveda Krishnamoorthy", "Girish Malkarnenkar", "Subhashini Venugopalan", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": null, "citeRegEx": "Guadarrama et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Guadarrama et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "A multi-modal clustering method for web videos", "author": ["Haiqi Huang", "Yueming Lu", "Fangwei Zhang", "Songlin Sun."], "venue": "International Conference on Trustworthy Computing and Services. pages 163\u2013 169.", "citeRegEx": "Huang et al\\.,? 2013", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy."], "venue": "ICML.", "citeRegEx": "Ioffe and Szegedy.,? 2015", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "UNAL-NLP: Combining soft cardinality features for semantic textual similarity, relatedness and entailment", "author": ["Sergio Jimenez", "George Duenas", "Julia Baquero", "Alexander Gelbukh", "Av Juan Dios B\u00e1tiz", "AvMendiz\u00e1bal."], "venue": "In SemEval. pages", "citeRegEx": "Jimenez et al\\.,? 2014", "shortCiteRegEx": "Jimenez et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "ICLR.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Generating natural language inference chains", "author": ["Vladyslav Kolesnyk", "Tim Rockt\u00e4schel", "Sebastian Riedel."], "venue": "arXiv preprint arXiv:1606.01404 .", "citeRegEx": "Kolesnyk et al\\.,? 2016", "shortCiteRegEx": "Kolesnyk et al\\.", "year": 2016}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["Abhishek Kumar", "Hal Daum\u00e9 III."], "venue": "ICML.", "citeRegEx": "Kumar and III.,? 2012", "shortCiteRegEx": "Kumar and III.", "year": 2012}, {"title": "Illinois-LH: A denotational and distributional approach to semantics", "author": ["Alice Lai", "Julia Hockenmaier."], "venue": "Proc. SemEval 2:5.", "citeRegEx": "Lai and Hockenmaier.,? 2014", "shortCiteRegEx": "Lai and Hockenmaier.", "year": 2014}, {"title": "ROUGE: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 workshop. volume 8.", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "ICLR.", "citeRegEx": "Luong et al\\.,? 2016", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Computer-intensive methods for testing hypotheses", "author": ["Eric W Noreen."], "venue": "Wiley New York.", "citeRegEx": "Noreen.,? 1989", "shortCiteRegEx": "Noreen.", "year": 1989}, {"title": "Hierarchical recurrent neural encoder for video representation with application to captioning", "author": ["Pingbo Pan", "ZhongwenXu", "Yi Yang", "FeiWu", "Yueting Zhuang."], "venue": "CVPR. pages 1029\u20131038.", "citeRegEx": "Pan et al\\.,? 2016a", "shortCiteRegEx": "Pan et al\\.", "year": 2016}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Yingwei Pan", "Tao Mei", "Ting Yao", "Houqiang Li", "Yong Rui."], "venue": "CVPR. pages 4594\u20134602.", "citeRegEx": "Pan et al\\.,? 2016b", "shortCiteRegEx": "Pan et al\\.", "year": 2016}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "ToddWard", "andWeiJing Zhu"], "venue": "In ACL", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman."], "venue": "ICLR.", "citeRegEx": "Simonyan and Zisserman.,? 2015", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2015}, {"title": "UCF101: A dataset of 101 human actions classes from videos in the wild", "author": ["Khurram Soomro", "Amir Roshan Zamir", "Mubarak Shah."], "venue": "arXiv preprint arXiv:1212.0402 .", "citeRegEx": "Soomro et al\\.,? 2012", "shortCiteRegEx": "Soomro et al\\.", "year": 2012}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Nitish Srivastava", "Elman Mansimov", "Ruslan Salakhutdinov."], "venue": "ICML. pages 843\u2013852.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "NIPS. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning", "author": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke."], "venue": "CoRR.", "citeRegEx": "Szegedy et al\\.,? 2016", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich."], "venue": "CVPR. pages 1\u20139.", "citeRegEx": "Szegedy et al\\.,? 2015", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Integrating language and vision to generate natural language descriptions of videos in the wild", "author": ["Jesse Thomason", "Subhashini Venugopalan", "Sergio Guadarrama", "Kate Saenko", "Raymond J Mooney."], "venue": "COLING.", "citeRegEx": "Thomason et al\\.,? 2014", "shortCiteRegEx": "Thomason et al\\.", "year": 2014}, {"title": "Using descriptive video services to create a large data source for video annotation research", "author": ["Atousa Torabi", "Christopher Pal", "Hugo Larochelle", "Aaron Courville."], "venue": "arXiv preprint arXiv:1503.01070 .", "citeRegEx": "Torabi et al\\.,? 2015", "shortCiteRegEx": "Torabi et al\\.", "year": 2015}, {"title": "CIDEr: Consensus-based image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh."], "venue": "CVPR. pages 4566\u20134575.", "citeRegEx": "Vedantam et al\\.,? 2015", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Improving lstm-based video description with linguistic knowledge mined from text", "author": ["Subhashini Venugopalan", "Lisa Anne Hendricks", "Raymond Mooney", "Kate Saenko."], "venue": "EMNLP.", "citeRegEx": "Venugopalan et al\\.,? 2016", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2016}, {"title": "Sequence to sequence-video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeffrey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko."], "venue": "CVPR. pages 4534\u20134542.", "citeRegEx": "Venugopalan et al\\.,? 2015a", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["Subhashini Venugopalan", "Huijuan Xu", "Jeff Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko."], "venue": "NAACL HLT.", "citeRegEx": "Venugopalan et al\\.,? 2015b", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Msrvtt: A large video description dataset for bridging video and language", "author": ["Jun Xu", "Tao Mei", "Ting Yao", "Yong Rui."], "venue": "CVPR. pages 5288\u20135296.", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville."], "venue": "CVPR. pages 4507\u20134515.", "citeRegEx": "Yao et al\\.,? 2015", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["Haonan Yu", "Jiang Wang", "Zhiheng Huang", "Yi Yang", "Wei Xu."], "venue": "CVPR.", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329 .", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 33, "context": "Previous work in video captioning (Venugopalan et al., 2015a; Pan et al., 2016b) has shown that recurrent neural networks (RNNs) are a good choice for modeling the temporal information in the video.", "startOffset": 34, "endOffset": 80}, {"referenceID": 21, "context": "Previous work in video captioning (Venugopalan et al., 2015a; Pan et al., 2016b) has shown that recurrent neural networks (RNNs) are a good choice for modeling the temporal information in the video.", "startOffset": 34, "endOffset": 80}, {"referenceID": 36, "context": "Furthermore, an attention mechanism between the video frames and the caption words captures some of the temporal matching relations better (Yao et al., 2015; Pan et al., 2016a).", "startOffset": 139, "endOffset": 176}, {"referenceID": 20, "context": "Furthermore, an attention mechanism between the video frames and the caption words captures some of the temporal matching relations better (Yao et al., 2015; Pan et al., 2016a).", "startOffset": 139, "endOffset": 176}, {"referenceID": 20, "context": "More recently, hierarchical two-level RNNs were proposed to allow for longer inputs and to model the full paragraph caption dynamics of long video clips (Pan et al., 2016a; Yu et al., 2016).", "startOffset": 153, "endOffset": 189}, {"referenceID": 37, "context": "More recently, hierarchical two-level RNNs were proposed to allow for longer inputs and to model the full paragraph caption dynamics of long video clips (Pan et al., 2016a; Yu et al., 2016).", "startOffset": 153, "endOffset": 189}, {"referenceID": 17, "context": ", 2016b) has shown that recurrent neural networks (RNNs) are a good choice for modeling the temporal information in the video. A sequence-to-sequence model is then used to \u2018translate\u2019 the video to a caption. Venugopalan et al. (2016) showed linguistic improvements over this by fusing the decoder with external language models.", "startOffset": 83, "endOffset": 234}, {"referenceID": 18, "context": "We model this via many-to-many multi-task learning based sequence-to-sequence models (Luong et al., 2016) that allow the sharing of parameters among the encoders and decoders across the three different tasks, with additional shareable attention mechanisms.", "startOffset": 85, "endOffset": 105}, {"referenceID": 2, "context": ", premise-to-entailment generation (based on the image caption domain SNLI corpus (Bowman et al., 2015)), shares its decoder with the video captioning decoder, and helps it learn better video-entailed caption representations, since the caption is essentially an entailment of the video, i.", "startOffset": 82, "endOffset": 103}, {"referenceID": 24, "context": ", video-to-video generation (adapted from Srivastava et al. (2015)), shares its encoder with the video captioning task\u2019s encoder, and helps it learn richer video representations that can predict their temporal context and action sequence.", "startOffset": 42, "endOffset": 67}, {"referenceID": 8, "context": "Early video captioning work (Guadarrama et al., 2013; Thomason et al., 2014; Huang et al., 2013) used a two-stage pipeline to first extract a subject, verb, and object (S,V,O) triple and then generate a sentence based on it.", "startOffset": 28, "endOffset": 96}, {"referenceID": 29, "context": "Early video captioning work (Guadarrama et al., 2013; Thomason et al., 2014; Huang et al., 2013) used a two-stage pipeline to first extract a subject, verb, and object (S,V,O) triple and then generate a sentence based on it.", "startOffset": 28, "endOffset": 96}, {"referenceID": 10, "context": "Early video captioning work (Guadarrama et al., 2013; Thomason et al., 2014; Huang et al., 2013) used a two-stage pipeline to first extract a subject, verb, and object (S,V,O) triple and then generate a sentence based on it.", "startOffset": 28, "endOffset": 96}, {"referenceID": 8, "context": "Early video captioning work (Guadarrama et al., 2013; Thomason et al., 2014; Huang et al., 2013) used a two-stage pipeline to first extract a subject, verb, and object (S,V,O) triple and then generate a sentence based on it. Venugopalan et al. (2015b) fed mean-pooled static frame-level visual features (from convolution neural networks pre-trained on image recognition) of the video as input to the language decoder.", "startOffset": 29, "endOffset": 252}, {"referenceID": 8, "context": "Early video captioning work (Guadarrama et al., 2013; Thomason et al., 2014; Huang et al., 2013) used a two-stage pipeline to first extract a subject, verb, and object (S,V,O) triple and then generate a sentence based on it. Venugopalan et al. (2015b) fed mean-pooled static frame-level visual features (from convolution neural networks pre-trained on image recognition) of the video as input to the language decoder. To harness the important frame sequence temporal ordering, Venugopalan et al. (2015a) proposed a sequence-to-sequence model with video encoder and language decoder RNNs.", "startOffset": 29, "endOffset": 504}, {"referenceID": 36, "context": "Moreover, an attention or alignment mechanism was added between the encoder and the decoder to learn the temporal relations (matching) between the video frames and the caption words (Yao et al., 2015; Pan et al., 2016a).", "startOffset": 182, "endOffset": 219}, {"referenceID": 20, "context": "Moreover, an attention or alignment mechanism was added between the encoder and the decoder to learn the temporal relations (matching) between the video frames and the caption words (Yao et al., 2015; Pan et al., 2016a).", "startOffset": 182, "endOffset": 219}, {"referenceID": 29, "context": "More recently, Venugopalan et al. (2016) explored linguistic improvements to the caption decoder by fusing it with external language models.", "startOffset": 15, "endOffset": 41}, {"referenceID": 17, "context": "(2016) explored linguistic improvements to the caption decoder by fusing it with external language models. Moreover, an attention or alignment mechanism was added between the encoder and the decoder to learn the temporal relations (matching) between the video frames and the caption words (Yao et al., 2015; Pan et al., 2016a). In contrast to static visual features, Yao et al. (2015) also considered temporal video features from a 3D-CNN model pretrained on an action recognition task.", "startOffset": 16, "endOffset": 385}, {"referenceID": 20, "context": "To explore long range temporal relations, Pan et al. (2016a) proposed a two-level hierarchical RNN encoder which limits the length of input information and allows temporal transitions between segments.", "startOffset": 42, "endOffset": 61}, {"referenceID": 20, "context": "To explore long range temporal relations, Pan et al. (2016a) proposed a two-level hierarchical RNN encoder which limits the length of input information and allows temporal transitions between segments. Yu et al. (2016)\u2019s hierarchical RNN generates sentences at the first level and the second level captures inter-sentence dependencies in a paragraph.", "startOffset": 42, "endOffset": 219}, {"referenceID": 20, "context": "To explore long range temporal relations, Pan et al. (2016a) proposed a two-level hierarchical RNN encoder which limits the length of input information and allows temporal transitions between segments. Yu et al. (2016)\u2019s hierarchical RNN generates sentences at the first level and the second level captures inter-sentence dependencies in a paragraph. Pan et al. (2016b) proposed to simultaneously learn the RNN word probabilities and a visual-semantic joint embedding space that enforces the relationship between the semantics of the entire sentence and the visual content.", "startOffset": 42, "endOffset": 370}, {"referenceID": 3, "context": "Multi-task learning is a useful learning paradigm to improve the supervision and the generalization performance of a task by jointly training it with related tasks (Caruana, 1998; Argyriou et al., 2007; Kumar and Daum\u00e9 III, 2012).", "startOffset": 164, "endOffset": 229}, {"referenceID": 0, "context": "Multi-task learning is a useful learning paradigm to improve the supervision and the generalization performance of a task by jointly training it with related tasks (Caruana, 1998; Argyriou et al., 2007; Kumar and Daum\u00e9 III, 2012).", "startOffset": 164, "endOffset": 229}, {"referenceID": 0, "context": "Multi-task learning is a useful learning paradigm to improve the supervision and the generalization performance of a task by jointly training it with related tasks (Caruana, 1998; Argyriou et al., 2007; Kumar and Daum\u00e9 III, 2012). Recently, Luong et al. (2016) combined", "startOffset": 180, "endOffset": 261}, {"referenceID": 25, "context": "Srivastava et al. (2015) address this by propos-", "startOffset": 0, "endOffset": 25}, {"referenceID": 16, "context": "(2015) allowed training end-to-end neural networks that outperform earlier feature-based RTE models (Lai and Hockenmaier, 2014; Jimenez et al., 2014).", "startOffset": 100, "endOffset": 149}, {"referenceID": 12, "context": "(2015) allowed training end-to-end neural networks that outperform earlier feature-based RTE models (Lai and Hockenmaier, 2014; Jimenez et al., 2014).", "startOffset": 100, "endOffset": 149}, {"referenceID": 2, "context": "The recent Stanford Natural Language Inference (SNLI) corpus by Bowman et al. (2015) allowed training end-to-end neural networks that outperform earlier feature-based RTE models (Lai and Hockenmaier, 2014; Jimenez et al.", "startOffset": 64, "endOffset": 85}, {"referenceID": 2, "context": "The recent Stanford Natural Language Inference (SNLI) corpus by Bowman et al. (2015) allowed training end-to-end neural networks that outperform earlier feature-based RTE models (Lai and Hockenmaier, 2014; Jimenez et al., 2014). However, directly generating the entailed hypothesis sentences given a premise sentence would be even more beneficial than retrieving or reranking sentence pairs, because most downstream generation tasks only come with the source sentence and not pairs. Recently, Kolesnyk et al. (2016) tried a sequenceto-sequence model for this on the original SNLI dataset, which is a single-reference setting and hence restricts automatic evaluation.", "startOffset": 64, "endOffset": 516}, {"referenceID": 26, "context": "model (Sutskever et al., 2014) where the final state of the encoder RNN is input as an initial state to the decoder RNN, as shown in Fig.", "startOffset": 6, "endOffset": 30}, {"referenceID": 9, "context": "The RNN is based on Long Short Term Memory (LSTM) units, which are good at memorizing long sequences due to forget-style gates (Hochreiter and Schmidhuber, 1997).", "startOffset": 127, "endOffset": 161}, {"referenceID": 1, "context": "Our attention model architecture is similar to Bahdanau et al. (2015), with a bidirectional LSTMRNN as the encoder and a unidirectional LSTMRNN as the decoder, see Fig.", "startOffset": 47, "endOffset": 70}, {"referenceID": 4, "context": "First, we use the YouTube2Text or MSVD (Chen and Dolan, 2011) for our primary results, which con-", "startOffset": 39, "endOffset": 61}, {"referenceID": 35, "context": "We also use MSR-VTT (Xu et al., 2016) with 10, 000 diverse video clips (from a video search engine) \u2013 it has 200, 000 video clipsentence pairs and around 20 captions per video; and M-VAD (Torabi et al.", "startOffset": 20, "endOffset": 37}, {"referenceID": 30, "context": ", 2016) with 10, 000 diverse video clips (from a video search engine) \u2013 it has 200, 000 video clipsentence pairs and around 20 captions per video; and M-VAD (Torabi et al., 2015) with 49, 000 movie-based video clips but only 1 or 2 captions per video, making most evaluation metrics (except paraphrase-based METEOR) infeasible.", "startOffset": 157, "endOffset": 178}, {"referenceID": 24, "context": "Video Prediction Dataset For our unsupervised video representation learning task, we use the UCF-101 action videos dataset (Soomro et al., 2012), which contains 13, 320 video clips of 101 action categories, and suits our video captioning task well because it also contains short video clips of a single action or few actions.", "startOffset": 123, "endOffset": 144}, {"referenceID": 2, "context": "Entailment Generation Dataset For the entailment generation encoder-decoder model, we use the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), which contains human-annotated English sentence pairs with classification labels of entailment, contradiction and neutral.", "startOffset": 144, "endOffset": 165}, {"referenceID": 2, "context": "Entailment Generation Dataset For the entailment generation encoder-decoder model, we use the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), which contains human-annotated English sentence pairs with classification labels of entailment, contradiction and neutral. It has a total of 570, 152 sentence pairs out of which 190, 113 correspond to true entailment pairs, and we use this subset in our multi-task video captioning model. For improving video captioning, we use the same training/validation/test splits as provided by Bowman et al. (2015), which is 183, 416 training, 3, 329 validation, and 3, 368 testing pairs (for the entailment subset).", "startOffset": 145, "endOffset": 572}, {"referenceID": 23, "context": ", 2009) \u2013 VGGNet (Simonyan and Zisserman, 2015), GoogLeNet (Szegedy et al.", "startOffset": 17, "endOffset": 47}, {"referenceID": 28, "context": ", 2009) \u2013 VGGNet (Simonyan and Zisserman, 2015), GoogLeNet (Szegedy et al., 2015; Ioffe and Szegedy, 2015), and Inception-v4 (Szegedy et al.", "startOffset": 59, "endOffset": 106}, {"referenceID": 11, "context": ", 2009) \u2013 VGGNet (Simonyan and Zisserman, 2015), GoogLeNet (Szegedy et al., 2015; Ioffe and Szegedy, 2015), and Inception-v4 (Szegedy et al.", "startOffset": 59, "endOffset": 106}, {"referenceID": 27, "context": ", 2015; Ioffe and Szegedy, 2015), and Inception-v4 (Szegedy et al., 2016).", "startOffset": 51, "endOffset": 73}, {"referenceID": 6, "context": "matic evaluation metrics that are popular for image/video captioning and language generation in general: METEOR (Denkowski and Lavie, 2014), BLEU-4 (Papineni et al.", "startOffset": 112, "endOffset": 139}, {"referenceID": 22, "context": "matic evaluation metrics that are popular for image/video captioning and language generation in general: METEOR (Denkowski and Lavie, 2014), BLEU-4 (Papineni et al., 2002), CIDEr-D (Vedantam et al.", "startOffset": 148, "endOffset": 171}, {"referenceID": 31, "context": ", 2002), CIDEr-D (Vedantam et al., 2015), and ROUGE-L (Lin, 2004).", "startOffset": 17, "endOffset": 40}, {"referenceID": 17, "context": ", 2015), and ROUGE-L (Lin, 2004).", "startOffset": 21, "endOffset": 32}, {"referenceID": 5, "context": "We use the standard evaluation code from the Microsoft COCO server (Chen et al., 2015) to obtain these results and also to compare the results with previous papers.", "startOffset": 67, "endOffset": 86}, {"referenceID": 13, "context": "We use Adam optimizer (Kingma and Ba, 2015).", "startOffset": 22, "endOffset": 43}, {"referenceID": 26, "context": "The final baseline model \u2297 instead uses an ensemble (E), which is a standard denoising method (Sutskever et al., 2014) that performs inference over ten randomly initialized models, i.", "startOffset": 94, "endOffset": 118}, {"referenceID": 34, "context": "Models METEOR CIDEr-D ROUGE-L BLEU-4 PREVIOUS WORK LSTM-YT (V) (Venugopalan et al., 2015b) 26.", "startOffset": 63, "endOffset": 90}, {"referenceID": 33, "context": "2 S2VT (V + A) (Venugopalan et al., 2015a) 29.", "startOffset": 15, "endOffset": 42}, {"referenceID": 36, "context": "8 Temporal Attention (G + C) (Yao et al., 2015) 29.", "startOffset": 29, "endOffset": 47}, {"referenceID": 21, "context": "9 LSTM-E (V + C) (Pan et al., 2016b) 31.", "startOffset": 17, "endOffset": 36}, {"referenceID": 32, "context": "3 Glove + DeepFusion (V) (E) (Venugopalan et al., 2016) 31.", "startOffset": 29, "endOffset": 55}, {"referenceID": 37, "context": "1 p-RNN (V + C) (Yu et al., 2016) 32.", "startOffset": 16, "endOffset": 33}, {"referenceID": 20, "context": "9 HNRE + Attention (G + C) (Pan et al., 2016a) 33.", "startOffset": 27, "endOffset": 46}, {"referenceID": 19, "context": "03 for METEOR, based on the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1994) with 100K samples.", "startOffset": 43, "endOffset": 85}, {"referenceID": 7, "context": "03 for METEOR, based on the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1994) with 100K samples.", "startOffset": 43, "endOffset": 85}, {"referenceID": 7, "context": "03 for METEOR, based on the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1994) with 100K samples. that a mixing ratio of 100 : 50 alternating minibatches (between the captioning and entailment tasks) works well here. Again, Table 1 shows statistically significant improvements in all the metrics in comparison to the best baseline model (and all previous works) under this multi-task setting. Note that in our initial experiments, our entailment generation model helped the video captioning task significantly more than the alternative approach of simply improving fluency by adding (or deep-fusing) an external language model (or pretrained word embeddings) to the decoder (using both in-domain and out-of-domain language models), again because a caption is also \u2018entailed\u2019 by a video in a logically-directed sense and hence this matches our captioning task better (also see results of Venugopalan et al. (2016) in Table 1).", "startOffset": 58, "endOffset": 920}, {"referenceID": 35, "context": "3 Yao et al. (2015) 25.", "startOffset": 2, "endOffset": 20}, {"referenceID": 35, "context": "2 Xu et al. (2016) 25.", "startOffset": 2, "endOffset": 19}, {"referenceID": 35, "context": "Results are reimplementations as per Xu et al. (2016). We also report the top 3 leaderboard systems \u2013 our model achieves the new rank 1 based on their ranking method.", "startOffset": 37, "endOffset": 54}, {"referenceID": 31, "context": "Models METEOR Yao et al. (2015) 5.", "startOffset": 14, "endOffset": 32}, {"referenceID": 30, "context": "7 Venugopalan et al. (2015a) 6.", "startOffset": 2, "endOffset": 29}, {"referenceID": 20, "context": "7 Pan et al. (2016a) 6.", "startOffset": 2, "endOffset": 21}, {"referenceID": 18, "context": "1), because these hyperparameters depend on the primary task being improved, as also discussed in previous work (Luong et al., 2016).", "startOffset": 112, "endOffset": 132}, {"referenceID": 4, "context": "YouTube2Text or MSVD The Microsoft Research Video Description Corpus (MSVD) or YouTube2Text (Chen and Dolan, 2011) is used for our primary video captioning experiments.", "startOffset": 92, "endOffset": 114}, {"referenceID": 4, "context": "YouTube2Text or MSVD The Microsoft Research Video Description Corpus (MSVD) or YouTube2Text (Chen and Dolan, 2011) is used for our primary video captioning experiments. It has 1970 YouTube videos in the wild with many diverse captions in multiple languages for each video. Caption annotations to these videos are collected using Amazon Mechanical Turk (AMT). All our experiments use only English captions. On average, each video has 40 captions, and the overall dataset has about 80, 000 unique video-caption pairs. The average clip duration is roughly 10 seconds. We used the standard split as stated in Venugopalan et al. (2015a), i.", "startOffset": 93, "endOffset": 632}, {"referenceID": 35, "context": "We used the standard split as provided in (Xu et al., 2016), i.", "startOffset": 42, "endOffset": 59}, {"referenceID": 30, "context": "Again, we used the standard train/val/test split as provided in Torabi et al. (2015).", "startOffset": 64, "endOffset": 85}, {"referenceID": 24, "context": "For our unsupervised video representation learning task, we use the UCF-101 action videos dataset (Soomro et al., 2012), which contains 13, 320 video clips of 101 action categories and with an average clip length of 7.", "startOffset": 98, "endOffset": 119}, {"referenceID": 23, "context": ", 2009) \u2013 VGGNet (Simonyan and Zisserman, 2015), GoogLeNet (Szegedy et al.", "startOffset": 17, "endOffset": 47}, {"referenceID": 28, "context": ", 2009) \u2013 VGGNet (Simonyan and Zisserman, 2015), GoogLeNet (Szegedy et al., 2015; Ioffe and Szegedy, 2015), and Inception-v4 (Szegedy et al.", "startOffset": 59, "endOffset": 106}, {"referenceID": 11, "context": ", 2009) \u2013 VGGNet (Simonyan and Zisserman, 2015), GoogLeNet (Szegedy et al., 2015; Ioffe and Szegedy, 2015), and Inception-v4 (Szegedy et al.", "startOffset": 59, "endOffset": 106}, {"referenceID": 27, "context": ", 2015; Ioffe and Szegedy, 2015), and Inception-v4 (Szegedy et al., 2016).", "startOffset": 51, "endOffset": 73}, {"referenceID": 13, "context": "We use the Adam optimizer (Kingma and Ba, 2015) with default coefficients and a batch size of 32.", "startOffset": 26, "endOffset": 47}, {"referenceID": 38, "context": "5 to the vertical connections of LSTM (Zaremba et al., 2014) to reduce overfitting.", "startOffset": 38, "endOffset": 60}], "year": 2017, "abstractText": "Video captioning, the task of describing the content of a video, has seen some promising improvements in recent years with sequence-to-sequence models, but accurately learning the temporal and logical dynamics involved in the task still remains a challenge, especially given the lack of sufficient annotated data. We improve video captioning by sharing knowledge with two related directed-generation tasks: a temporally-directed unsupervised video prediction task to learn richer context-aware video encoder representations, and a logically-directed language entailment generation task to learn better video-entailed caption decoder representations. For this, we present a many-to-many multi-task learning model that shares parameters across the encoders and decoders of the three tasks. We achieve significant improvements and the new state-ofthe-art on several standard video captioning datasets using diverse automatic and human evaluations. We also show mutual multi-task improvements on the entailment generation task.", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}