{"id": "1704.07511", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "Scalable Planning with Tensorflow for Hybrid Nonlinear Domains", "abstract": "Given recent deep learning results that demonstrate the ability to effectively optimize high-dimensional non-convex functions with gradient descent optimization on GPUs, we ask in this paper whether symbolic gradient optimization tools such as Tensorflow can be effective for planning in hybrid (mixed discrete and continuous) nonlinear domains with high dimensional state and action spaces? To this end, we demonstrate that hybrid planning with Tensorflow and RMSProp gradient descent is competitive with mixed integer linear program (MILP) based optimization on piecewise linear planning domains (where we can compute optimal solutions) and substantially outperforms state-of-the-art interior point methods for nonlinear planning domains. Furthermore, we remark that Tensorflow is highly scalable, converging to a strong policy on a large-scale concurrent domain with a total of 576,000 continuous actions over a horizon of 96 time steps in only 4 minutes. We provide a number of insights that clarify such strong performance including observations that despite long horizons, RMSProp avoids both the vanishing and exploding gradients problem. Together these results suggest a new frontier for highly scalable planning in nonlinear hybrid domains by leveraging GPUs and the power of recent advances in gradient descent with highly optmized toolkits like Tensorflow.", "histories": [["v1", "Tue, 25 Apr 2017 01:52:45 GMT  (138kb)", "https://arxiv.org/abs/1704.07511v1", "8 pages"], ["v2", "Sat, 29 Apr 2017 17:58:42 GMT  (215kb)", "http://arxiv.org/abs/1704.07511v2", "8 pages"]], "COMMENTS": "8 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ga wu", "buser say", "scott sanner"], "accepted": true, "id": "1704.07511"}, "pdf": {"name": "1704.07511.pdf", "metadata": {"source": "CRF", "title": "Scalable Planning with Tensorflow for Hybrid Nonlinear Domains", "authors": [], "emails": ["wuga@mie.utoronto.ca", "bsay@mie.utoronto.ca", "ssanner@mie.utoronto.ca"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.07 511v 2 [cs.L G] 29 Apr 201 7"}, {"heading": "Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "Hybrid Nonlinear Planning via Tensorflow", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Hybrid Planning", "text": "A hybrid planning problem is a tuple < S, A, T, R, C >, where S denotes the amount of states, A the amount of actions limited by action restraints C, R: S \u00b7 A \u2192 R the reward function, and T: S \u00b7 A \u2192 S the transition function. There is also a starting state s0 and the planning goal is to maximize the cumulative reward over a decision horizon of H-time steps. Before proceeding, we outline the necessary notation: \u2022 st: mixed discrete, continuous state vectors at the time. \u2022 at: mixed discrete, continuous action vectors at the time. \u2022 sitj: the j dimension of the state vectors of the problem i at the time. \u2022 aitj: the j dimension of the action vectors at the time. \u2022 R (st, at): a non-positive reward function - higher absolute values indicate higher costs. \u2022 T (st, at): a linear (non-cumulative) function of the action vector at the time."}, {"heading": "Planning through Backpropagation", "text": "It is a standard method for optimizing parameters of large multi-layered neural networks via gradient descent. Through the chain rule of derivatives, back propagation leads the derivation of the output error of a neural network back to each of its precursor parameters in a single linear time lapse in the size of the network, which is now simply used as reverse mode of automatic differentiation. Despite its relative efficiency, back propagation in large (deep) neural networks is still computationally expensive, and it is only with the advance of recent GPU-based symbolic tools such as Tensorflow [Abadi et al] that recent advances in education have made very large deep neural networks possible. In this paper we return to the idea of the training parameters of the network that fixed input factors for optimizing input factors (i.e., actions) are subject to fixed parameters (effectively transition and reward is assumed)."}, {"heading": "Long Horizon Planning", "text": "In view of the fact that the transition dynamics of the planning problems are typically not stationary, the tensor flow composition of a nonlinear planning problem reflects the same structure as recurring neural networks (RNN) of deep networks, which are usually used in deep learning. Here, the connection is not superficial, since a long-standing difficulty in the formation of RNNNs lies in the problem of disappearing gradients, i.e., the multiplication of long gradients in the chain rule usually makes them extremely small and irrelevant to weight updates, especially when using nonlinear transfer functions such as sigmoid. In hybrid planning problems, however, continuous state actualizations often take the form si (t + 1) j = root gradients in the chain rule, i.e. we find that the transfer function here is linear in sitj, which is the largest determinator of (+ 1), which is different from the largest of grades + 1, and which can explode by the largest of grades (1)."}, {"heading": "Experiments", "text": "In this section, we present our three benchmark domains and then validate the performance of tensor flow planning in the following steps. (1) We evaluate the optimum performance of tensor flow backpropagation planning on linear and bilinear domains by comparing it to the optimal solution of Mixture Integer Linear Programming (MILP). (2) We evaluate the performance of tensor flow backpropagation planning on nonlinear domains (which MILPs cannot handle) by comparing it to the Matlab-based nonlinear Interior Point Solver FMINCON. (4) We examine the impact of several popular gradient descend optimizers on planning performance. (5) We evaluate the optimization of learning rate."}, {"heading": "Domain Descriptions", "text": "The following equation shows the transition function: dt = 99 percent of environments supporting different complexity transitions. < dt = 1 percent of environments supporting different complexity transitions. < dt = 1 percent of environments supporting different complexity transitions. < dt = 1 percent of environments moving to the target state is as fast as possible (see Figure 1). Therefore, we calculate the reward based on the distance from the target state to the target state in each time step asR (st, at). \u2212 dt \u2212 s The goal of the problem is for an agent to move to the target state as quickly as possible. We have designed three different transition functions: nonlinear, bilinear and linear. The nonlinear transition has a radius deceleration zone at the center of the field. The moving distance of the agent is reduced based on its Euclidean distance to the center of the deceleration zone."}, {"heading": "Planning Performance", "text": "In this section we will examine the performance of Tensorflow optimization by comparing it with the MILP on linear domains and with Matlab's non-linear internal point-solver fmincon on nonlinear domains. We conducted our experiments on the Ubuntu Linux system with an E5-1620 v4 CPU, 16 GB RAM and a GTX1080 GPU. Tensorflow version is beta 0.12.1, Matlab version is R2016b and the MILP version is IBM ILOG CPLEX 12.6.3."}, {"heading": "Performance in Linear Domains", "text": "In Figure 2, we show that planning tensor flow back propagation leads to lower cost plans than domain specific heuristic guidelines, and the total cost is relatively close to the optimal reward that MILP provides. While planning tensor flow back propagation always performs well, when comparing the performance of tensorflow in bilinear and linear areas of navigation with the MILP solution, we find that tensorflow performs much better than the discredited linear domain compared to the MILP in the bilinear domain. The reason for this is simple: gradient optimization of smooth bilinear functions is actually much easier for tensorflow than the piecemeal linear discredited version, which features large, piecemeal steps that make it difficult for RMSProp to obtain a consistent and smooth gradient signal."}, {"heading": "Performance in Nonlinear Domains", "text": "In Figure 3, we show that Tensorflow backpropagation planning always delivers the best performance of all three methods. In relatively simple areas such as navigation, the non-linear fmincon solver is a very competitive solution, while for the complex area of HVAC with a large simultaneous action space, the fmincon solver shows complete failure to solve the problem within the specified timeframe. In Figure 4 (a), Tensorflow backpropagation planning shows 16 times faster optimization in the first 15 seconds, which is close to the result given by fmincon at 4min. In Figure 4 (b), the optimization speed of this solver shows one hundred times faster than the non-linear fmincon solver to achieve the same value (if fmincon ever reaches it). These remarkable results demonstrate the performance of the fast parallel GPU calculation of the Tensorflow framework."}, {"heading": "Scalability", "text": "In Table 1, we demonstrate the scalability of tensor flow backpropagation planning based on the runtimes required for convergence in different domains, and the extreme efficiency with which tensor flow can converge into exceptionally large non-linear hybrid planning domains."}, {"heading": "Optimization Methods", "text": "In this experiment, we are investigating the effects of various backpropagation optimizers. In Figure 5, we show that the RMSProp optimizer provides an exceptionally fast convergence between the five standard optimizers of Tensorflow. This observation reflects the earlier analysis and discussion regarding Equation (7) that RMSProp manages to avoid exploding gradients. Although Adagrad and Adadelta have similar mechanisms, their normalization methods can cause disappearing gradients after several epochs, which corresponds to our observation of near-flat curves in these methods. This is a strong indicator that exploding gradients are a major concern for hybrid planning with gradient drop, and that RMSProp performs well despite this well-known potential problem for long horizon gradients."}, {"heading": "Optimization Rate", "text": "In Figure 6, we show that the best learning optimization rate for the HVAC domain is 0.01, as this rate converges extremely quickly to near-optimal values. Overall, the trend is that smaller optimization rates have a better chance of achieving a better final optimization solution, but can be extremely slow, as shown in Optimization Rate 0.001. So, while larger optimization rates can lead to overruns, too small rates for practical use simply converge too slowly, suggesting a critical need to adjust the optimization rate per planning area."}, {"heading": "Conclusion", "text": "We investigated the practicability of using the Tensorflow toolbox for fast, large-scale planning in nonlinear hybrid domains. We worked with a direct symbolic (nonlinear) planning domain set-up for Tensorflow, for which we optimized planning measures directly through gradient-based back propagation. We then investigated long-horizon planning and suggested that RMSProp avoids both the vanishing and dexploding gradient problems and showed experiments to confirm these results. Our key empirical results showed that Tensorflow with RMSProp is competitive with MILPs on linear domains (where the optimal solution is known - indicating approximate optimality of Tensorflow and RMSProb for these nonconvex functions) and Matlab's state-of-the-art interior point optimizer on nonlinear domains (where the optimal solution is known - which clearly indicates optimum Tensorflow and Texorflow for use)."}, {"heading": "Network Structure Clarification", "text": "An intermediate state of the planning problem is calculated as hidden states by the RNN-like network structure. Cumulative reward function isL (r1, r2, r3 \u00b7 \u00b7 \u00b7) = H \u2211 i\u03b3iriBack spread of negative cumulative reward in relation to the action sequence means updating actions to optimize the cumulative reward formally defined in Eq.1 and 2. Additional visualization of the HVAC problem Figure 8 shows that although our model and our heuristic method show the same temperature control performance, our model saves more energy. 1Github Repository: https: / / github.com / wuga214 / TOOLBOX-Learning-and-Planning-through-Backpropagation"}], "references": [{"title": "TensorFlow: Large-scale machine learning", "author": ["Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": "on heterogeneous systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Occupancy-driven energy management for smart building automation", "author": ["Agarwal et al", "2010] Yuvraj Agarwal", "Bharathan Balaji", "Rajesh Gupta", "Jacob Lyles", "Michael Wei", "Thomas Weng"], "venue": "In Proceedings of the 2nd ACM Workshop on Embedded Sensing Systems", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "JAIR)", "author": ["Amanda Jane Coles", "Andrew Coles", "Maria Fox", "Derek Long. A hybrid LP-RPG heuristic for modelling numeric resource flows in planning. J. Artif. Intell. Res"], "venue": "46:343\u2013412,", "citeRegEx": "Coles et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "pages 72\u201383", "author": ["R\u00e9mi Coulom. Efficient selectivity", "backup operators in monte-carlo tree search. In International Conference on Computers", "Games"], "venue": "Springer Berlin Heidelberg,", "citeRegEx": "Coulom. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Energy efficient building environment control strategies using realtime occupancy measurements", "author": ["Erickson et al", "2009] Varick L. Erickson", "Yiqing Lin", "Ankur Kamthe", "Rohini Brahme", "Alberto E. Cerpa", "Michael D. Sohn", "Satish Narayanan"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Nonlinear Model Predictive PathFollowing Control. In Nonlinear Model Predictive Control - Towards New Challenging Applications, Lecture Notes in Control and Information", "author": ["Faulwasser", "Findeisen", "2009] Timm Faulwasser", "Rolf Findeisen"], "venue": null, "citeRegEx": "Faulwasser et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Faulwasser et al\\.", "year": 2009}, {"title": "Optimal planning with global numerical state constraints", "author": ["Ivankovic et al", "2014] Franc Ivankovic", "Patrik Haslum", "Sylvie Thiebaux", "Vikas Shivashankar", "Dana Nau"], "venue": "In International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "In Proceedings of the 23rd International Conference on Automated Planning and Scheduling", "author": ["Thomas Keller", "Malte Helmert. Trial-based heuristic tree search for finite horizon mdps"], "venue": "ICAPS 2013, Rome, Italy, June 10-14, 2013,", "citeRegEx": "Keller and Helmert. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of the 17th European Conference on Machine Learning (ECML-06)", "author": ["Levente Kocsis", "Csaba Szepesv\u00e1ri. Bandit based Monte-Carlo planning"], "venue": "pages 282\u2013293,", "citeRegEx": "Kocsis and Szepesv\u00e1ri. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors", "author": ["S. Linnainmaa"], "venue": "Master\u2019s thesis, Univ. Helsinki", "citeRegEx": "Linnainmaa. 1970", "shortCiteRegEx": null, "year": 1970}, {"title": "A planning based framework for controlling hybrid systems", "author": ["L\u00f6hr et al", "2012] Johannes L\u00f6hr", "Patrick Eyerich", "Thomas Keller", "Bernhard Nebel"], "venue": "In Proceedings of the Twenty-Second International Conference on Automated Planning and Scheduling,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "NIPS Deep Learning Workshop.", "citeRegEx": "Mnih et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Heuristic planning for hybrid systems", "author": ["Piotrowski et al", "2016] Wiktor Mateusz Piotrowski", "Maria Fox", "Derek Long", "Daniele Magazzeni", "Fabio Mercorio"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17,", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis"], "venue": "search. Nature,", "citeRegEx": "Sutskever et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2016}, {"title": "USA", "author": ["Richard S. Sutton", "Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press", "MA Cambridge"], "venue": "1st edition,", "citeRegEx": "Sutton and Barto. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Algorithms for Reinforcement Learning", "author": ["Csaba Szepesv\u00e1ri"], "venue": "Morgan & Claypool,", "citeRegEx": "Szepesv\u00e1ri. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running", "author": ["Tieleman", "Hinton", "2012] T. Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Bandit-based planning and learning in continuous-action markov decision processes", "author": ["Weinstein", "Littman", "2012] Ari Weinstein", "Michael L. Littman"], "venue": "In Proceedings of the Twenty-Second International Conference on Automated Planning and Scheduling,", "citeRegEx": "Weinstein et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Weinstein et al\\.", "year": 2012}, {"title": "Reservoir management and operations models: A state-of-the-art review", "author": ["William G Yeh"], "venue": "Water Resources research, 21,12:17971818,", "citeRegEx": "Yeh. 1985", "shortCiteRegEx": null, "year": 1985}], "referenceMentions": [{"referenceID": 18, "context": "Introduction Many real-world hybrid (mixed discrete continuous) planning problems such as Reservoir Control [Yeh, 1985], Heating, Ventilation and Air Conditioning (HVAC) [Erickson et al.", "startOffset": 108, "endOffset": 119}, {"referenceID": 2, "context": "Unfortunately, existing state-of-the-art hybrid planners [Ivankovic et al., 2014; L\u00f6hr et al., 2012; Coles et al., 2013; Piotrowski et al., 2016] are not compatible with arbitrary nonlinear transition and reward models.", "startOffset": 57, "endOffset": 145}, {"referenceID": 3, "context": "Monte Carlo Tree Search (MCTS) methods [Coulom, 2006; Kocsis and Szepesv\u00e1ri, 2006; Keller and Helmert, 2013] 0.", "startOffset": 39, "endOffset": 108}, {"referenceID": 8, "context": "Monte Carlo Tree Search (MCTS) methods [Coulom, 2006; Kocsis and Szepesv\u00e1ri, 2006; Keller and Helmert, 2013] 0.", "startOffset": 39, "endOffset": 108}, {"referenceID": 7, "context": "Monte Carlo Tree Search (MCTS) methods [Coulom, 2006; Kocsis and Szepesv\u00e1ri, 2006; Keller and Helmert, 2013] 0.", "startOffset": 39, "endOffset": 108}, {"referenceID": 14, "context": ", Q-learning) with function approximation [Sutton and Barto, 1998; Szepesv\u00e1ri, 2010] and deep extensions [Mnih et al.", "startOffset": 42, "endOffset": 84}, {"referenceID": 15, "context": ", Q-learning) with function approximation [Sutton and Barto, 1998; Szepesv\u00e1ri, 2010] and deep extensions [Mnih et al.", "startOffset": 42, "endOffset": 84}, {"referenceID": 11, "context": ", Q-learning) with function approximation [Sutton and Barto, 1998; Szepesv\u00e1ri, 2010] and deep extensions [Mnih et al., 2013] do not require any knowledge of the (nonlinear) transition model or reward, but they also do not directly apply to domains with high-dimensional continuous action spaces.", "startOffset": 105, "endOffset": 124}, {"referenceID": 9, "context": "learning due to its compilation of complex layered symbolic functions into a representation amenable to fast GPU-based reverse-mode automatic differentiation [Linnainmaa, 1970] for gradient-based optimization.", "startOffset": 158, "endOffset": 176}, {"referenceID": 9, "context": "Via the chain rule of derivatives, backpropagation passes the derivative of the output error of a neural network back to each of its parameters in a single linear time pass in the size of the network using what is now simply known as reverse-mode automatic differentiation [Linnainmaa, 1970].", "startOffset": 273, "endOffset": 291}, {"referenceID": 18, "context": "Reservoir Control: Reservoir Control [Yeh, 1985] is a system to control multiple connected reservoirs.", "startOffset": 37, "endOffset": 48}], "year": 2017, "abstractText": "Given recent deep learning results that demonstrate the ability to effectively optimize high-dimensional non-convex functions with gradient descent optimization on GPUs, we ask in this paper whether symbolic gradient optimization tools such as Tensorflow can be effective for planning in hybrid (mixed discrete and continuous) nonlinear domains with high dimensional state and action spaces? To this end, we demonstrate that hybrid planning with Tensorflow and RMSProp gradient descent is competitive with mixed integer linear program (MILP) based optimization on piecewise linear planning domains (where we can compute optimal solutions) and substantially outperforms state-of-the-art interior point methods for nonlinear planning domains. Furthermore, we remark that Tensorflow is highly scalable, converging to a strong policy on a largescale concurrent domain with a total of 576,000 continuous actions over a horizon of 96 time steps in only 4 minutes. We provide a number of insights that clarify such strong performance including observations that despite long horizons, RMSProp avoids both the vanishing and exploding gradients problem. Together these results suggest a new frontier for highly scalable planning in nonlinear hybrid domains by leveraging GPUs and the power of recent advances in gradient descent with highly optmized toolkits like Tensorflow. Introduction Many real-world hybrid (mixed discrete continuous) planning problems such as Reservoir Control [Yeh, 1985], Heating, Ventilation and Air Conditioning (HVAC) [Erickson et al., 2009; Agarwal et al., 2010], and Navigation [Faulwasser and Findeisen, 2009] have highly nonlinear transition and (possibly nonlinear) reward functions to optimize. Unfortunately, existing state-of-the-art hybrid planners [Ivankovic et al., 2014; L\u00f6hr et al., 2012; Coles et al., 2013; Piotrowski et al., 2016] are not compatible with arbitrary nonlinear transition and reward models. Monte Carlo Tree Search (MCTS) methods [Coulom, 2006; Kocsis and Szepesv\u00e1ri, 2006; Keller and Helmert, 2013] 0.600 0.750 .900 0.600 0.750 .900 0.600 0.750 0.9 00 0.600 0.750 .900 0.600 0.750 .900 0.600 0.750 .900 0.600 0.750 0.9 00 Epochs:10 Epochs:20 Epochs:40 Epochs:80 Epochs:160 Epochs:320 Figure 1: The evolution of RMSProp gradient descent based Tensorflow planning in a 2D Navigation domain with nested central rectangles indicating nonlinearly increasing resistance to robot movement. (top) In initial RMSProp epochs, the plan evolves directly towards the goal shown as a star. (bottom) As later epochs of RMSProp descend the objective cost surface, the fastest path evolves to avoid the central obstacle entirely. including AlphaGo [Silver et al., 2016] that can use any (nonlinear) black box model of transition dynamics do not inherently work with continuous action spaces due to the infinite branching factor. While MCTS with continuous action extensions such as HOOT [Weinstein and Littman, 2012] have been proposed, their continuous partitioningmethods do not scale to high-dimensional continuous action spaces (e.g., 100\u2019s or 1,000\u2019s of dimensions as used in this paper). Finally, offline model-free reinforcement learning (e.g., Q-learning) with function approximation [Sutton and Barto, 1998; Szepesv\u00e1ri, 2010] and deep extensions [Mnih et al., 2013] do not require any knowledge of the (nonlinear) transition model or reward, but they also do not directly apply to domains with high-dimensional continuous action spaces. I.e., offline learning methods like Q-learning require action maximization for every update, but in high-dimensional continuous action spaces such nonlinear function maximization is non-convex and computationally intractable at the scale of millions or billions of updates. To address the above scalability and expressivity limitations of existing methods, we turn to Tensorflow [Abadi et al., 2015], which is a symbolic computation platform used in the machine learning community for deep learning due to its compilation of complex layered symbolic functions into a representation amenable to fast GPU-based reverse-mode automatic differentiation [Linnainmaa, 1970] for gradient-based optimization. Given recent results in gradient descent optimization with deep learning that demonstrate the ability to effectively optimize high-dimensional non-convex functions, we ask whether Tensorflow can be effective for planning in hybrid (mixed discrete and continuous) nonlinear domains with high dimensional state and action spaces? Our results answer this question affirmatively, where we demonstrate that hybrid planning with Tensorflow and RMSProp gradient descent [Tieleman and Hinton, 2012] is surprisingly effective at planning in complex hybrid nonlinear domains. As evidence, we reference figure 1, where we show Tensorflow with RMSProp optimizing a path in a 2d nonlinear Navigation domain. In general, Tensorflow with RMSProp planning results are competitive with optimal MILPbased optimization on piecewise linear planning domains and directly extend to nonlinear domains, where they substantially outperform interior point methods for nonlinear function optimization. Furthermore, we remark that Tensorflow converges to a strong policy on a large-scale concurrent domain with 576,000 continuous actions spread over a horizon of 96 time steps in 4 minutes. To explain such excellent results, we note that gradient descent algorithms such as RMSProp are highly effective for the non-convex function optimization that occurs in deep learning. Further, we provide an analysis of many transition functions in planning domains that suggest gradient descent on these domains will not suffer from either the vanishing or exploding gradient problems and hence provide a strong signal for optimization over long horizons. Together these results suggest a new frontier for highly scalable planning in nonlinear hybrid domains by leveraging GPUs and the power of recent advances in gradient descent with the Tensorflow toolkit. Hybrid Nonlinear Planning via Tensorflow", "creator": "LaTeX with hyperref package"}}}