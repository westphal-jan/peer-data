{"id": "1704.07535", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "Abstract Syntax Networks for Code Generation and Semantic Parsing", "abstract": "Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering.", "histories": [["v1", "Tue, 25 Apr 2017 04:37:35 GMT  (952kb,D)", "http://arxiv.org/abs/1704.07535v1", "ACL 2017. MR and MS contributed equally"]], "COMMENTS": "ACL 2017. MR and MS contributed equally", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG stat.ML", "authors": ["maxim rabinovich", "mitchell stern", "dan klein"], "accepted": true, "id": "1704.07535"}, "pdf": {"name": "1704.07535.pdf", "metadata": {"source": "CRF", "title": "Abstract Syntax Networks for Code Generation and Semantic Parsing", "authors": ["Maxim Rabinovich", "Mitchell Stern", "Dan Klein"], "emails": ["rabinovich@cs.berkeley.edu", "mitchell@cs.berkeley.edu", "klein@cs.berkeley.edu"], "sections": [{"heading": null, "text": "Abstract Syntax Networks for Code Generation and Semantic Parsing Maxim Rabinovich, Mitchell Stern and Dan Klein Computer Science DivisionUniversity of California, Berkeley {rabinovich, mitchell, klein} @ cs.berkeley.eduAbstract Tasks such as code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-shaped, executable outputs. We present abstract syntax networks, a modeling framework for these problems. The outputs are presented as abstract syntax trees (ASTs) and built by a decoder with a dynamically determined modular structure comparable to the structure of the output tree. On the benchmark HEARTHSTONE dataset for code generation, our model achieves 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1%, and we also execute specific parameters on the JATGEO, without any specific technical parameters."}, {"heading": "1 Introduction", "text": "In this context, it should be noted that most of them are able to decide whether or not they are able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...)"}, {"heading": "1.1 Related work", "text": "Encoder decoder architectures, with and without attention, have been successfully applied to both sequence prediction tasks such as machine translation and tree prediction tasks such as constituency parsing (Cross and Huang, 2016; Dyer et al., 2016; Vinyals et al., 2015). In the latter case, the work has focused on making the task look like sequence-tosequence prediction, either by flattening the output tree (Vinyals et al., 2015) or by presenting it as a sequence-sequence approach to construction decisions (Cross and Huang, 2016; Dyer et al., 2016). Our work differs both from the use of a recursive top-down generation model, and from the use of a recursive top-down generation method. Dong and Lapata have introduced a sequence-sequence approach approach to semantic parsing, including a limited form of top-down recursion, but without the coupling between the output or the grammatics."}, {"heading": "2 Data Representation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Abstract Syntax Trees", "text": "Our model uses the Fragment Abstract Syntax Description Language (ASDL) Framework (Wang et al., 1997), which represents code fragments as trees with typed nodes. Primitive types correspond to atomic values, such as integers or identifiers. Accordingly, primitive nodes are commented on with a primitive type and a value of this type - for example, in Figure 3a, the identification node that \"create minion\" represents a function of the same name.Composite types correspond to language constructs, such as expressions or statements. Each type has a collection of constructors, each of which represents a node of this type. Figure 4 shows constructors for the statement (stmt) and expression types (expr).The associated language constructs include function and class definitions, return statements, binary operations, and function calls. Composite types comment syntax trees over composite types with a selection of constructs and a type."}, {"heading": "2.2 Input Representation", "text": "We present inputs as collections of named components, each consisting of a sequence of tokens. In the case of semantic parsing, inputs have a single component that contains the query set. In the case of HEARTHSTONE, the name and description of the card are presented as sequences of characters or tokens, while categorical attributes are presented as single-token sequences. In the case of HEARTHSTONE, we limit our input and output vocabularies to values that occur more than once in the training set."}, {"heading": "3 Model Architecture", "text": "Our model uses an encoder decoder architecture with hierarchical attention. The key idea behind our approach is to structure the decoder as a collection of mutually recursive modules. Modules correspond to elements of AST grammar and are composed in such a way that they reflect the structure of the tree to be created. A vertical LSTM state is passed from module to module to disseminate information during the decoding process. The encoder uses bidirectional LSTMs to embed each component and a feed network to combine them."}, {"heading": "3.1 Encoder", "text": "Each component c of the input is encoded with a component-specific bidirectional LSTM, resulting in forward and backward token encodings (\u2212 \u2192 hc, \u2190 \u2212 hc) that are later used by the attention mechanism. To obtain an encoding of the input as a whole for decoder initialization, we concatenate the final forward and backward encodings of each component into a single vector and apply a linear projection."}, {"heading": "3.2 Decoder Modules", "text": "eiD rf\u00fc the green green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the for the green for the green for the green for the for the green for the green for the for the green for the green for the for the green for the green for the for the for the for the green for the for the green for the green for the green for the"}, {"heading": "3.3 Decoding Process", "text": "The decoding process proceeds through mutual recursion between the constituent modules, with the syntactical structure of the output tree reflecting the call graph of the generation method. At each step, the active decoding module makes either a generational decision, propagates the state along the tree, or both. To construct a composite node of a particular type, the decoder calls the corresponding composite type module to obtain a constructor and its associated module, which is then called to obtain updated vertical LSTM states for each of the constructor fields, and the corresponding constructor field modules are called to advance the process to those children. This process continues downward, stopping at each primitive node where a value is generated but no further recursion is performed."}, {"heading": "3.4 Attention", "text": "Following the sequence tosequence models, we compute a raw bilinear attention result qrawt for each token in the input by looking at the current state of the decoder x and the encoding of the token et: qrawt = e > t Wx.The current state x can be either the vertical LSTM state in isolation or a concentration of the vertical LSTM state and either a horizontal LSTM state or a character LSTM state (for the string generation).Each submodule that computes attention uses a separate attention score qcompcomph for each component of the input, regardless of its content: qcompcompc = w > c x.The final token attentions are the sums of the raw token level scores and the corresponding component scoring scores scores."}, {"heading": "4 Experimental evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Semantic parsing", "text": "Data We use three semantic parsing datasets: JOBS, GEO, and ATIS. All three consist of natural language queries paired with a logical representation of their names. JOBS consists of 640 such pairs with prologue-style logical representations, while GEO and ATIS consist of 880 and 5,410 such pairs with logical forms of \u03bb calculation, respectively. We use the same training test split as Zettlemoyer and Collins (2005) for JOBS and GEO, and the standard training development test split for ATIS. We use the pre-processed versions of these datasets provided by Dong and Lapata (2016), with text in the input reduced and inserted using NLTK (Bird et al., 2009). Matching units appearing in the same input output pair have been replaced by numbered abstract identifiers of the same type. Rating We calculate accuracies between the tree-level accuracies for the validation of the 2016 alias well as for the tree-level comparisons published."}, {"heading": "4.2 Code generation", "text": "We use the HEARTHSTONE dataset introduced by Ling et al. (2016), which consists of 665 maps paired with their implementations in open source hearthbreaker engineering.3 Our training test split is identical to that of Ling et al. (2016), with split sizes of 533, 66 and 66, respectively. Maps contain two types of components: text components that contain the name of the map and a description of its function, and categorical components that contain numerical attributes (attack, health, cost and durability) or enumerated attributes (rarity, type, race, and class).The name of the map is displayed as a sequence of characters, while 3Online at https: / / github.com / danielyule / Hearthbreaker.its description consists of a sequence of tokens split between whitespace and punctuation."}, {"heading": "4.3 Settings", "text": "We select the dimension from {30, 40, 50, 60, 70} for the smaller JOBS and GEO datasets or from {50, 75, 100, 125, 150} for the larger ATIS and HEARTHSTONE datasets. The dimensionality used for input to the encoder is always set to 100. We apply the dropout to the non-recurring connections of the vertical and horizontal LSTMs and select the noise ratio from {0.2, 0.3, 0.4, 0.5}. All parameters are randomly initialized based on the glorot initialization (Glorot and Bengio, 2010). We perform 200 passes over the data for the JOBS and GEO experiments, or 400 passes for the ATIS and HEARTHSTONE experiments. Early interruption based on exact match is used for the sectoral parameters."}, {"heading": "4.4 Results", "text": "Our results on the semantic parsing datasets are presented in Table 1. Our basic system achieves a new state-of-the-art accuracy of 91.4% in the JOBS dataset, and this figure improves to 92.9% when the monitored attention is added. In the ATIS and GEO datasets, we exceed and match the results of Dong and Lapata (2016) respectively, however, these values fall short of the previous best results of Wang et al. (2014) of 91.3% and 90.4%, respectively. This difference may be partially due to the use of typed information or rich lexicals in most previous semantic parsing approaches (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2013; Wang et al., 2014; Zhao and Huang, 2015). In the HEARTHSTONE dataset, we improve significantly over the initial results of Ling et al. (2016) in all evaluation metrics, as we improve several additional results in 1 and 7% of the BLala."}, {"heading": "4.5 Error Analysis and Discussion", "text": "As the examples in Figures 6-8 show, the generation of the code is only a matter of consistency with the overall structure and the inclusion of the correct values in the initiator and some other places. In such cases, our system predicts the correct code, except for instances in which the strings are transferred incorrectly. Introducing a dedicated copying mechanism, such as that of Ling et al. (2016) or specialized machines for string transmission, could alleviate this latter problem. The next simplest category of card code pairs consists of those in which the logic of the card is mostly implemented via nested function calls."}, {"heading": "5 Conclusion", "text": "ASNs offer a modular encoder decoder architecture that can easily handle a variety of tasks with structured output spaces. They are particularly useful for recursive decomposition processes, where they can provide a simple decoding process that is closely intertwined with the inherent structure of the output data. Our results show that they hold great promise for tree prediction tasks, and we believe that applying them to more general output structures is an interesting path for future work."}, {"heading": "Acknowledgments", "text": "MR is supported by an NSF Graduate Research Fellowship and a Google Fellowship from the Fannie and John Hertz Foundation. MS is supported by an NSF Graduate Research Fellowship."}], "references": [{"title": "Compilers: Principles, Techniques, and Tools (2Nd Edition)", "author": ["Alfred V. Aho", "Monica S. Lam", "Ravi Sethi", "Jeffrey D. Ullman."], "venue": "Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA.", "citeRegEx": "Aho et al\\.,? 2006", "shortCiteRegEx": "Aho et al\\.", "year": 2006}, {"title": "Bimodal modelling of source code and natural language", "author": ["Miltiadis Allamanis", "Daniel Tarlow", "Andrew D. Gordon", "Yi Wei."], "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015.", "citeRegEx": "Allamanis et al\\.,? 2015", "shortCiteRegEx": "Allamanis et al\\.", "year": 2015}, {"title": "Tree-structured decoding with doubly-recurrent neural networks", "author": ["David Alvarez-Melis", "Tommi S. Jaakkola."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR) 2017.", "citeRegEx": "Alvarez.Melis and Jaakkola.,? 2017", "shortCiteRegEx": "Alvarez.Melis and Jaakkola.", "year": 2017}, {"title": "Neural module networks", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Oral.", "citeRegEx": "Andreas et al\\.,? 2016", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Deepcoder: Learning to write programs", "author": ["Matej Balog", "Alexander L. Gaunt", "Marc Brockschmidt", "Sebastian Nowozin", "Daniel Tarlow."], "venue": "CoRR abs/1611.01989.", "citeRegEx": "Balog et al\\.,? 2016", "shortCiteRegEx": "Balog et al\\.", "year": 2016}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "J. Mach. Learn. Res. 3:1137\u20131155. http://dl.acm.org/citation.cfm?id=944919.944966.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Natural Language Processing with Python", "author": ["Steven Bird", "Ewan Klein", "Edward Loper."], "venue": "O\u2019Reilly Media, Inc., 1st edition.", "citeRegEx": "Bird et al\\.,? 2009", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D. Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29,", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL", "author": ["Doha"], "venue": null, "citeRegEx": "2014 and Doha,? \\Q2014\\E", "shortCiteRegEx": "2014 and Doha", "year": 2014}, {"title": "Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles", "author": ["James Cross", "Liang Huang."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016,", "citeRegEx": "Cross and Huang.,? 2016", "shortCiteRegEx": "Cross and Huang.", "year": 2016}, {"title": "Language to logical form with neural attention", "author": ["Li Dong", "Mirella Lapata."], "venue": "CoRR abs/1601.01280.", "citeRegEx": "Dong and Lapata.,? 2016", "shortCiteRegEx": "Dong and Lapata.", "year": 2016}, {"title": "Recurrent neural network", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6980. http://arxiv.org/abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["Tom Kwiatkowski", "Eunsol Choi", "Yoav Artzi", "Luke S. Zettlemoyer."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013,", "citeRegEx": "Kwiatkowski et al\\.,? 2013", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "Learning programs: A hierarchical bayesian approach", "author": ["Percy Liang", "Michael I. Jordan", "Dan Klein."], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel. pages 639\u2013646.", "citeRegEx": "Liang et al\\.,? 2010", "shortCiteRegEx": "Liang et al\\.", "year": 2010}, {"title": "Learning dependency-based compositional semantics", "author": ["Percy Liang", "Michael I. Jordan", "Dan Klein."], "venue": "Comput. Linguist. 39(2):389\u2013446. https://doi.org/10.1162/COLI a 00127.", "citeRegEx": "Liang et al\\.,? 2013", "shortCiteRegEx": "Liang et al\\.", "year": 2013}, {"title": "Latent predictor networks for code generation", "author": ["Wang Ling", "Phil Blunsom", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Fumin Wang", "Andrew Senior."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Compu-", "citeRegEx": "Ling et al\\.,? 2016", "shortCiteRegEx": "Ling et al\\.", "year": 2016}, {"title": "Structured generative models of natural source code", "author": ["Chris J. Maddison", "Daniel Tarlow."], "venue": "Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 2126 June 2014. pages 649\u2013657.", "citeRegEx": "Maddison and Tarlow.,? 2014", "shortCiteRegEx": "Maddison and Tarlow.", "year": 2014}, {"title": "A machine learning framework for programming by example", "author": ["Aditya Krishna Menon", "Omer Tamuz", "Sumit Gulwani", "Butler W. Lampson", "Adam Kalai."], "venue": "Proceedings of the 30th International Conference on Machine Learning, ICML 2013, At-", "citeRegEx": "Menon et al\\.,? 2013", "shortCiteRegEx": "Menon et al\\.", "year": 2013}, {"title": "Dynet: The dynamic neural network toolkit", "author": ["Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin."], "venue": "arXiv preprint arXiv:1701.03980 .", "citeRegEx": "Oda et al\\.,? 2017", "shortCiteRegEx": "Oda et al\\.", "year": 2017}, {"title": "Towards a theory of natural language interfaces to databases", "author": ["Ana-Maria Popescu", "Oren Etzioni", "Henry Kautz."], "venue": "Proceedings of the 8th international conference on Intelligent user interfaces. ACM, pages 149\u2013157.", "citeRegEx": "Popescu et al\\.,? 2003", "shortCiteRegEx": "Popescu et al\\.", "year": 2003}, {"title": "Tree-structured variational autoencoder", "author": ["Richard Shin", "Alexander A. Alemi", "Geoffrey Irving", "Oriol Vinyals."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR) 2017.", "citeRegEx": "Shin et al\\.,? 2017", "shortCiteRegEx": "Shin et al\\.", "year": 2017}, {"title": "Introduction to the Theory of Computation", "author": ["Michael Sipser."], "venue": "Course Technology, second edition.", "citeRegEx": "Sipser.,? 2006", "shortCiteRegEx": "Sipser.", "year": 2006}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E. Hinton."], "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Sys-", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Morpho-syntactic lexical generalization for ccg semantic parsing", "author": ["Adrienne Wang", "Tom Kwiatkowski", "Luke S Zettlemoyer."], "venue": "EMNLP. pages 1284\u20131295.", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "The zephyr abstract syntax description language", "author": ["Daniel C. Wang", "Andrew W. Appel", "Jeff L. Korn", "Christopher S. Serra."], "venue": "Proceedings of the Conference on Domain-Specific Languages on Conference on Domain-Specific Languages (DSL),", "citeRegEx": "Wang et al\\.,? 1997", "shortCiteRegEx": "Wang et al\\.", "year": 1997}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["Luke S. Zettlemoyer", "Michael Collins."], "venue": "UAI \u201905, Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence, Ed-", "citeRegEx": "Zettlemoyer and Collins.,? 2005", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2005}, {"title": "Online learning of relaxed ccg grammars for parsing to logical form", "author": ["Luke S. Zettlemoyer", "Michael Collins."], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-", "citeRegEx": "Zettlemoyer and Collins.,? 2007", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2007}, {"title": "Type-driven incremental semantic parsing with polymorphism", "author": ["Kai Zhao", "Liang Huang."], "venue": "NAACL HLT 2015, The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Zhao and Huang.,? 2015", "shortCiteRegEx": "Zhao and Huang.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Sequence-to-sequence models have proven effective for both tasks (Dong and Lapata, 2016; Ling et al., 2016), using encoder-decoder frameworks to exploit the sequential structure on both the input and output side.", "startOffset": 65, "endOffset": 107}, {"referenceID": 18, "context": "Sequence-to-sequence models have proven effective for both tasks (Dong and Lapata, 2016; Ling et al., 2016), using encoder-decoder frameworks to exploit the sequential structure on both the input and output side.", "startOffset": 65, "endOffset": 107}, {"referenceID": 0, "context": "The wellformedness case is of particular interest, since it can readily be enforced by representing outputs as abstract syntax trees (ASTs) (Aho et al., 2006), an approach that can be seen as a much lighter weight", "startOffset": 140, "endOffset": 158}, {"referenceID": 11, "context": "The ci0 and ci1 tokens are entity abstractions introduced in preprocessing (Dong and Lapata, 2016).", "startOffset": 75, "endOffset": 98}, {"referenceID": 28, "context": "version of CCG-based semantic parsing (Zettlemoyer and Collins, 2005).", "startOffset": 38, "endOffset": 69}, {"referenceID": 8, "context": "As is common with neural approaches to structured prediction (Chen and Manning, 2014; Vinyals et al., 2015), our decoder proceeds greedily and accesses not only a fixed encoding but also an attention-based representation of the input (Bahdanau et al.", "startOffset": 61, "endOffset": 107}, {"referenceID": 25, "context": "As is common with neural approaches to structured prediction (Chen and Manning, 2014; Vinyals et al., 2015), our decoder proceeds greedily and accesses not only a fixed encoding but also an attention-based representation of the input (Bahdanau et al.", "startOffset": 61, "endOffset": 107}, {"referenceID": 4, "context": ", 2015), our decoder proceeds greedily and accesses not only a fixed encoding but also an attention-based representation of the input (Bahdanau et al., 2014).", "startOffset": 134, "endOffset": 157}, {"referenceID": 18, "context": "1% exact match (Ling et al., 2016).", "startOffset": 15, "endOffset": 34}, {"referenceID": 26, "context": "Likewise, we perform competitively on the ATIS and GEO datasets, matching or exceeding the exact match reported by Dong and Lapata (2016), though not quite reaching the records held by the best previous semantic parsing approaches (Wang et al., 2014).", "startOffset": 231, "endOffset": 250}, {"referenceID": 11, "context": "Likewise, we perform competitively on the ATIS and GEO datasets, matching or exceeding the exact match reported by Dong and Lapata (2016), though not quite reaching the records held by the best previous semantic parsing approaches (Wang et al.", "startOffset": 115, "endOffset": 138}, {"referenceID": 10, "context": "Encoder-decoder architectures, with and without attention, have been applied successfully both to sequence prediction tasks like machine translation and to tree prediction tasks like constituency parsing (Cross and Huang, 2016; Dyer et al., 2016; Vinyals et al., 2015).", "startOffset": 204, "endOffset": 268}, {"referenceID": 12, "context": "Encoder-decoder architectures, with and without attention, have been applied successfully both to sequence prediction tasks like machine translation and to tree prediction tasks like constituency parsing (Cross and Huang, 2016; Dyer et al., 2016; Vinyals et al., 2015).", "startOffset": 204, "endOffset": 268}, {"referenceID": 25, "context": "Encoder-decoder architectures, with and without attention, have been applied successfully both to sequence prediction tasks like machine translation and to tree prediction tasks like constituency parsing (Cross and Huang, 2016; Dyer et al., 2016; Vinyals et al., 2015).", "startOffset": 204, "endOffset": 268}, {"referenceID": 25, "context": "In the latter case, work has focused on making the task look like sequence-tosequence prediction, either by flattening the output tree (Vinyals et al., 2015) or by representing it as a sequence of construction decisions (Cross and Huang, 2016; Dyer et al.", "startOffset": 135, "endOffset": 157}, {"referenceID": 10, "context": ", 2015) or by representing it as a sequence of construction decisions (Cross and Huang, 2016; Dyer et al., 2016).", "startOffset": 70, "endOffset": 112}, {"referenceID": 12, "context": ", 2015) or by representing it as a sequence of construction decisions (Cross and Huang, 2016; Dyer et al., 2016).", "startOffset": 70, "endOffset": 112}, {"referenceID": 5, "context": "Meanwhile, a separate line of work has focused on the problem of program induction from input-output pairs (Balog et al., 2016; Liang et al., 2010; Menon et al., 2013).", "startOffset": 107, "endOffset": 167}, {"referenceID": 16, "context": "Meanwhile, a separate line of work has focused on the problem of program induction from input-output pairs (Balog et al., 2016; Liang et al., 2010; Menon et al., 2013).", "startOffset": 107, "endOffset": 167}, {"referenceID": 20, "context": "Meanwhile, a separate line of work has focused on the problem of program induction from input-output pairs (Balog et al., 2016; Liang et al., 2010; Menon et al., 2013).", "startOffset": 107, "endOffset": 167}, {"referenceID": 1, "context": "Allamanis et al. (2015) and Maddison and Tarlow (2014) proposed modeling code with a neural language model, generating concrete syntax trees in left-first depth-first order, focusing on metrics like perplexity and applications like code snippet retrieval.", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "Allamanis et al. (2015) and Maddison and Tarlow (2014) proposed modeling code with a neural language model, generating concrete syntax trees in left-first depth-first order, focusing on metrics like perplexity and applications like code snippet retrieval.", "startOffset": 0, "endOffset": 55}, {"referenceID": 1, "context": "Allamanis et al. (2015) and Maddison and Tarlow (2014) proposed modeling code with a neural language model, generating concrete syntax trees in left-first depth-first order, focusing on metrics like perplexity and applications like code snippet retrieval. More recently, Shin et al. (2017) attacked the same problem using a grammar-based variational autoencoder with top-down generation similar to ours instead.", "startOffset": 0, "endOffset": 290}, {"referenceID": 2, "context": "The prediction framework most similar in spirit to ours is the doubly-recurrent decoder network introduced by Alvarez-Melis and Jaakkola (2017), which propagates information down the tree using a vertical LSTM and between siblings using a horizontal LSTM.", "startOffset": 110, "endOffset": 144}, {"referenceID": 18, "context": "Apart from ours, the best results on the codegeneration task associated with the HEARTHSTONE dataset are based on a sequence-tosequence approach to the problem (Ling et al., 2016).", "startOffset": 160, "endOffset": 179}, {"referenceID": 3, "context": "Previously, Andreas et al. (2016) introduced neural module networks (NMNs) for visual question answering, with modules corresponding to linguistic substructures within the input query.", "startOffset": 12, "endOffset": 34}, {"referenceID": 27, "context": "Our model makes use of the Abstract Syntax Description Language (ASDL) framework (Wang et al., 1997), which represents code fragments as trees with typed nodes.", "startOffset": 81, "endOffset": 100}, {"referenceID": 6, "context": "Synthesis is delegated to a character-level LSTM language model (Bengio et al., 2003), and part of the role of the primitive module for open class types is to choose whether to synthesize a new value or not.", "startOffset": 64, "endOffset": 85}, {"referenceID": 18, "context": "Rather than providing an explicit copying mechanism (Ling et al., 2016), we instead generate alignments where possible to define a set of tokens on which the attention at a given primitive node should be concentrated.", "startOffset": 52, "endOffset": 71}, {"referenceID": 7, "context": "We use the preprocessed versions of these datasets made available by Dong and Lapata (2016), where text in the input has been lowercased and stemmed using NLTK (Bird et al., 2009), and matching entities appearing in the same input-output pair have been replaced by numbered abstract identifiers of the same type.", "startOffset": 160, "endOffset": 179}, {"referenceID": 26, "context": "We use the same training-test split as Zettlemoyer and Collins (2005) for JOBS and GEO, and the standard training-development-test split for ATIS.", "startOffset": 39, "endOffset": 70}, {"referenceID": 10, "context": "We use the preprocessed versions of these datasets made available by Dong and Lapata (2016), where text in the input has been lowercased and stemmed using NLTK (Bird et al.", "startOffset": 69, "endOffset": 92}, {"referenceID": 11, "context": "Following the publicly released code of Dong and Lapata (2016), we canonicalize the order of the children within conjunction and disjunction nodes to avoid spurious errors, but otherwise perform no transformations before comparison.", "startOffset": 40, "endOffset": 63}, {"referenceID": 18, "context": "Data We use the HEARTHSTONE dataset introduced by Ling et al. (2016), which consists of 665 cards paired with their implementations in the open-source Hearthbreaker engine.", "startOffset": 50, "endOffset": 69}, {"referenceID": 18, "context": "Data We use the HEARTHSTONE dataset introduced by Ling et al. (2016), which consists of 665 cards paired with their implementations in the open-source Hearthbreaker engine.3 Our trainingdevelopment-test split is identical to that of Ling et al. (2016), with split sizes of 533, 66, and 66, respectively.", "startOffset": 50, "endOffset": 252}, {"referenceID": 18, "context": "Evaluation For direct comparison to the results of Ling et al. (2016), we evaluate our predicted code based on exact match and token-level BLEU relative to the reference implementations from the library.", "startOffset": 51, "endOffset": 70}, {"referenceID": 13, "context": "All parameters are randomly initialized using Glorot initialization (Glorot and Bengio, 2010).", "startOffset": 68, "endOffset": 93}, {"referenceID": 14, "context": "We use the Adam optimizer (Kingma and Ba, 2014) with its default settings for optimization, with a batch size of 20 for the semantic parsing experiments, or a batch size of 10 for the HEARTHSTONE experiments.", "startOffset": 26, "endOffset": 47}, {"referenceID": 18, "context": "LPN refers to the system of Ling et al. (2016). Our nearest neighbor baseline NEAREST follows that of Ling et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 18, "context": "LPN refers to the system of Ling et al. (2016). Our nearest neighbor baseline NEAREST follows that of Ling et al. (2016), though it performs somewhat better; its nonzero exact match number stems from spurious repetition in the data.", "startOffset": 28, "endOffset": 121}, {"referenceID": 29, "context": "This difference may be partially attributable to the use of typing information or rich lexicons in most previous semantic parsing approaches (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2013; Wang et al., 2014; Zhao and Huang, 2015).", "startOffset": 141, "endOffset": 239}, {"referenceID": 15, "context": "This difference may be partially attributable to the use of typing information or rich lexicons in most previous semantic parsing approaches (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2013; Wang et al., 2014; Zhao and Huang, 2015).", "startOffset": 141, "endOffset": 239}, {"referenceID": 26, "context": "This difference may be partially attributable to the use of typing information or rich lexicons in most previous semantic parsing approaches (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2013; Wang et al., 2014; Zhao and Huang, 2015).", "startOffset": 141, "endOffset": 239}, {"referenceID": 30, "context": "This difference may be partially attributable to the use of typing information or rich lexicons in most previous semantic parsing approaches (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2013; Wang et al., 2014; Zhao and Huang, 2015).", "startOffset": 141, "endOffset": 239}, {"referenceID": 11, "context": "On the ATIS and GEO datasets, we respectively exceed and match the results of Dong and Lapata (2016). However, these fall short of the previous best results of 91.", "startOffset": 78, "endOffset": 101}, {"referenceID": 11, "context": "On the ATIS and GEO datasets, we respectively exceed and match the results of Dong and Lapata (2016). However, these fall short of the previous best results of 91.3% and 90.4%, respectively, obtained by Wang et al. (2014). This difference may be partially attributable to the use of typing information or rich lexicons in most previous semantic parsing approaches (Zettlemoyer and Collins, 2007; Kwiatkowski et al.", "startOffset": 78, "endOffset": 222}, {"referenceID": 18, "context": "On the HEARTHSTONE dataset, we improve significantly over the initial results of Ling et al. (2016) across all evaluation metrics, as shown in Table 2.", "startOffset": 81, "endOffset": 100}, {"referenceID": 18, "context": "Introducing a dedicated copying mechanism like the one used by Ling et al. (2016) or more specialized machinery for string transduction may alleviate this latter problem.", "startOffset": 63, "endOffset": 82}, {"referenceID": 24, "context": "Direct evaluation of functional equivalence is of course impossible in general (Sipser, 2006), and practically challenging even for the HEARTHSTONE dataset because it requires integrating with the game engine.", "startOffset": 79, "endOffset": 93}], "year": 2017, "abstractText": "Syntax Networks for Code Generation and Semantic Parsing Maxim Rabinovich\u2217 Mitchell Stern\u2217 Dan Klein Computer Science Division University of California, Berkeley {rabinovich,mitchell,klein}@cs.berkeley.edu", "creator": "LaTeX with hyperref package"}}}