{"id": "1704.07556", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "Adversarial Multi-Criteria Learning for Chinese Word Segmentation", "abstract": "Different linguistic perspectives causes many diverse segmentation criteria for Chinese word segmentation (CWS). Most existing methods focus on improve the performance for each single criterion. However, it is interesting to exploit these different criteria and mining their common underlying knowledge. In this paper, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge from multiple heterogeneous segmentation criteria. Experiments on eight corpora with heterogeneous segmentation criteria show that the performance of each corpus obtains a significant improvement, compared to single-criterion learning. Source codes of this paper are available on Github.", "histories": [["v1", "Tue, 25 Apr 2017 06:42:24 GMT  (2894kb,D)", "http://arxiv.org/abs/1704.07556v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xinchi chen", "zhan shi", "xipeng qiu", "xuanjing huang"], "accepted": true, "id": "1704.07556"}, "pdf": {"name": "1704.07556.pdf", "metadata": {"source": "CRF", "title": "Adversarial Multi-Criteria Learning for Chinese Word Segmentation", "authors": ["Xinchi Chen", "Zhan Shi", "Xipeng Qiu", "Xuanjing Huang"], "emails": ["xinchichen13@fudan.edu.cn", "zshi16@fudan.edu.cn", "xpqiu@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "It is a preliminary and important task for Chinese Natural Language Processing (NLP). Currently, state-of-the-art methods are based on statistically monitored learning algorithms and rely on a large-scale annotated corpus at an extremely high cost. Although there has been great success in building up CWS corpora, they are somewhat incompatible due to different segmentation criteria. (2001) Yu, Lu, Zhu, Duan, Sun, Wang, Zhao, and Penn Chinese Treebank (CTB) (Fei (2000), we use different segmentation criteria. it is a waste of resources if we fully exploit this corpora."}, {"heading": "2 General Neural Model for Chinese Word Segmentation", "text": "Chinese word segmentation task is usually considered a character-based sequence markup problem. Specifically, each character in a sentence is referred to as one of L = {B, M, E, S}, indicating the beginning, middle, end of a word, or a word with a single character. There are many widely used methods for solving sequence markup problems such as the Maximum Entropy Markov Model (MEMM), Conditional Random Fields (CRF), etc. Recently, neural networks have been widely applied to Chinese word segmentation tasks because they can minimize the effort involved in feature engineering (Zheng et al. (2013). Zheng, Chen, and Xu; Pei et al. (2014). Pei, Ge, and Baobao; Chen et al. (2015a) Chen, Qiu, Zhu, and Huang neural Architecture: The neural structure can be defined by the layer Y-Zu, Zu, Zhu, and Huhang (2015a)."}, {"heading": "2.1 Embedding layer", "text": "In neural models, the first step is usually to map discrete language symbols to distributed embedding vectors. Formally, we look for embedding vectors from the embedding matrix for each character xi as exi-Rde, where de is a hyperparameter indicating the size of the embedding of characters."}, {"heading": "2.2 Feature layers", "text": "Although there are numerous LSTM variants, we are using the LSTM architecture used by (Jozefowicz et al. (2015) Jozefowicz, Zaremba and Sutskever, which is similar to the architecture of (Graves (2013), but without peephole connections. LSTM LSTM introduces a gate mechanism and a memory cell to maintain long dependency information and prevent the gradient from disappearing. Formally, LSTM with entrance gate i, exit gate o, gate f and memory cell c could be expressed as follows: ii oi fi c. (Wg [exihi \u2212 1] + bg), (2) ci = ci \u2212 1 fi + c i ii, (3) hi = oi."}, {"heading": "2.3 Inference Layer", "text": "After character extraction, we use conditional random fields (CRF) (Lafferty et al. (2001) Lafferty, McCallum, and Pereira) as the layer for inference tags. In CRF layer, p (Y | X) in Eq (1) could be formalized as p (Y | X) = 1 (Y | X) = 1 (Y \u00b2 | X)) = 1 (Y \u00b2 | X). (7) This is the potential function, and we only consider interactions between two consecutive labels (first-order linear chain CRFs): 1 (Y | X) = 2 (X, i, yi \u2212 1, yi), 8 (x, i \u2032, y \u2032, y) = Exp (s (X, i) y + by \u2032 y), (9), where \u2032 y \u00b2 R are feasible parameters for label pair (y \u2032, y \u00b2, y). Score function s (X, i \u2032 R | L = 1 (S, i \u2032, y +), y = each L)."}, {"heading": "3 Multi-Criteria Learning for Chinese Word Segmentation", "text": "Although neural models on CWS are widespread, most of them cannot handle heterogeneous segmentation criteria at the same time with incompatible criteria. Inspired by the success of multi-task learning (Caruana (1997); Ben-David and Schuller (2003); Liu et al. (2016a) Liu, Qiu and Huang; Liu et al. (2016b) Liu, Qiu and Huang), we consider the heterogeneous criteria to be several \"related\" tasks that can improve each other's performance at the same time with shared information.Formally, we assume that there are M-corpora with heterogeneous segmentation criteria. We refer to Dm as a corpus m with Nm samples: Dm = {(X (m) i, Y (m) i)} Nm i = 1, (11) where Xmi and Y m i denote the i-th sentence and the corresponding label in the corpus."}, {"heading": "3.1 Model-I: Parallel Shared-Private Model", "text": "In the characteristic level of Model-I, we consider the private layer and the split layer as two parallel layers. For corpusm, the hidden states of the split layer and the private layer are: h (s) i = Bi-LSTM (exi, \u2212 \u2192 h (s) i \u2212 1, \u2190 \u2212 h (s) i + 1, \u03b8s), (12) h (m) i = Bi-LSTM (exi, \u2212 \u2192 h (m) i \u2212 1, \u2190 \u2212 h (m) i + 1, \u03b8m), (13) and the score function in the CRF layer is calculated as follows: s (m) (X, i) = W (m) s > [h (s) ih (m) i] + b (m) s, (14) where W (m) s, R2dh \u00d7 | L | and b (m) s, R | L | are criterion-specific parameters for corpus."}, {"heading": "3.2 Model-II: Stacked Shared-Private Model", "text": "The private layer takes the output of the split layer as input. For corpus m, the hidden states of the split layer and the private layer are: h (s) i = Bi-LSTM (exi, \u2212 \u2192 h (s) i \u2212 1, \u2190 \u2212 h (s) i + 1, \u03b8s), (15) h (m) i = Bi-LSTM ([exi h (s) i], \u2212 \u2192 h (s) i + 1, \u03b8m) (16) and the score function in the CRF layer is calculated as follows: s (m) (X, i) = W (m) s > h (m) i + b (m) s, (17) where W (m) s (R2dh \u00d7 | L | and b (m) s (R | L | criterion parameters for corpus are stacked."}, {"heading": "3.3 Model-III: Skip-Layer Shared-Private Model", "text": "In the characteristic level of the model III, the common level and the private level are stacked as model II. Additionally, we send the results of the common level directly to the CRF level. Model III can be considered as a combination of model I and model II. For corpus m, the hidden states of the common level and the private level are identical to Eq (15) and (16), and the score function in the CRF level is calculated as the same as Eq (14)."}, {"heading": "3.4 Objective function", "text": "The objective function Jseg can be calculated as follows: (1) Jseg (1) = M \u00b2 m = 1 Nm \u00b2 i = 1 log p (Y (m) i | X (m) i; (1) m \u00b2, (18) where 0) and 0) denote all parameters in private or shared layers respectively."}, {"heading": "4 Incorporating Adversarial Training for Shared Layer", "text": "Although the Shared-Private model separates the Feature Space into Shared Space and Private Space, there is no guarantee that Sharable Features do not exist in the Private Feature Space or vice versa. Inspired by the work on the Domain Adaptation (Ajakan et al. (2014) Ajakan, Germain, Larochelle, Laviolette and Marchand; Ganin et al. (2016) Ganin, Ustinova, Ajakan, Germain, Larochelle, Laviolette, Marchand, and Lempitsky; Bousmalis et al. (2016) Bousmalis, Trigeorgis, Silberman, Krishnan and Erhan, we hope that the features extracted through Shared Layer are invariant across the heterogonal segmentation criteria. Therefore, we are jointly optimizing the Shared Layer through Adversarial Training (Goodfellow et al. (2014) Goodfellow, Pouget-Abadie, Mirza, Xu de, Section, Airfield, Sharley Section = Farzley."}, {"heading": "4.1 Adversarial loss function", "text": "The criterion discriminator maximizes the cross entropy of the predicted criterion distribution p (\u00b7 | X) and the true criterion. (20) An opposing loss aims to generate common characteristics, so that a criterion criterion criterion criterion cannot reliably predict the criterion by using these common characteristics. (21) Therefore, we maximize the entropy of the predicted criterion distribution when we train common parameters pi log pi (s): s J 2adv (v) = M = 1 Nm \u00b2 i = 1 H (p (m | X (m) i) i))), (21) where H (p) = \u2212 p) = \u2212 p = \u2212 p, i log pi log pi (s): an entropy of distribution pi (s): s J 2adv (v) = 1 Nm \u00b2 i = 1 H (p (2016) & kropa, Germain, Larodo Y = \u2212 p = \u2212 p = \u2212 p, i log pi log pi log pi log pi log pi log pi (s) = \u2212) = \u2212 p) = \u2212 p, \u2212 p, pi log pi log pi log pi log pi log pi log pi log pi log pi log pi log pi (s), u (s): an, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, p, p, u, p, p, \u2212 u, u, p, u, p, p, p, p, p, p, p \u2212 u, p, p \u2212 u, u, p, p \u2212 u, \u2212 u, u, u, u, u, p, p, p, p, p, p \u2212 u, p, p, p, p, p, p, p, p, p, p \u2212 u, p \u2212 u, p, p, p, p \u2212 u, p, p, p, p, p, p, p, p, p, p \u2212 u, p, p, p, p, p, p,"}, {"heading": "5 Training", "text": "Finally, we combine tasks and enemy target functions, where \u03bb is the weight that controls the interaction of loss conditions, and D. The training method consists in alternately optimizing two discriminatory classifiers, as shown in Algorithm 1. We use Adam (Kingma and Ba (2014)) with minibatches to maximize the objectives. In particular, when applying an adversary strategy, we first train 2400 epochs (each epoch trains only on eight batches of different corpora), then we optimize only Jseg (\u0442m, Es) with set goals until convergence (early-stop strategy)."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Datasets", "text": "To evaluate our proposed architecture, we are experimenting with eight dominant CWS datasets from SIGHAN2005 (Emerson (2005)) and SIGHAN2008 (Jin and Chen (2008)). Table 1 details the eight datasets. Among these datasets, AS, CITYU and CKIP are traditional Chinese datasets, while the remains, MSRA, PKU, CTB, NCC and SXU, are simplified Chinese."}, {"heading": "6.2 Experimental Configurations", "text": "For hyperparameter configurations, we set both the character embed size de and the dimensionality of the LSTM hidden states i.e. 100. The initial learning rate \u03b1 is set to 0.01. The loss weight coefficient \u03bb is set to 0.05. Since the scale of each data set varies, we use different training batch sizes for data sets. Specifically, we set batch sizes for AS- and MSR data sets as 512, 256, and 128 for remnants, respectively. We use exposure strategies when embedding layers, maintaining 80% input (20% failure rate). For initialization, we randomize all parameters according to a uniform distribution (\u2212 0.05, 0.05). We simply model traditional Chinese characters in simplified Chinese and optimize the embedding of the same character matrix across data sets pre-trained on the Chinese Wikipedia corpus."}, {"heading": "6.3 Overall Results", "text": "In the first block, we can see that the performance is increased by the use of Bi-LSTM, and the performance of Bi-LSTM cannot be improved by simply increasing the depth of networks. In addition, although the F-value of the LSTM model in (Chen et al. (2015b) Chen, Qiu, Zhu, Liu, and Huang) it is additionally contained an external idiom dictionary. (2) In the second block, we have proposed three models that are based on multi-criteria learning performance. Model-I gains 0.75% improvement in averaging F readings compared to BiLSTM results (94.14%). Only the performance on MSRA decreases slightly."}, {"heading": "6.4 Speed", "text": "As shown in Figure 5, the proposed model progresses step by step on all data sets. After about 1000 epochs, the performance becomes stable and convergent. We also test the decoding speed, and our mods process an average of 441.38 sets per second. As the proposed models and the base models (Bi-LSTM and Stacked Bi-LSTM) have almost the same complexity, all models are almost equally efficient. However, the time consumption of the training process varies from model to model. For models without opposing training, it costs about 10 hours to train (the same for stacked Bi-LSTM and Stacked Bi-LSTM, eight sets to train), while for models with opposing training it takes about 16 hours. All experiments are carried out on the hardware with Intel (R) Xeon (Xeon) Xeon (Bi-LSTM) PVIVE43-V3 @ DIZ 40Z."}, {"heading": "6.5 Error Analysis", "text": "Furthermore, we examine the advantages of the proposed models by comparing the error distributions between one-criterion learning (base model Bi-LSTM) and multi-criterion learning (model I and model I with opposing training), as shown in Figure 6. According to the results, we have found that a large proportion of the points in Figure 6a and Figure 6b are above diagonal lines, implying that performance benefits from the integration of knowledge and supplementary information from other companies. As shown in Table 2 of the CITYU test theorem, the performance of Model I and its opposite version (model I + ADV) increases from 92.17% to 95.59% and 95.42%, respectively. Furthermore, we find that a contrary strategy is effective to prevent criterion-specific features from arising from creeping into the common space. Thus, for example, the segmentation granularity of the personal name often differs according to heterogeneous criteria that could be corrected by the 33 percent of our personal error models."}, {"heading": "7 Knowledge Transfer", "text": "In this section, we examine the ability of knowledge transfer through two experiments: (1) simplified Chinese to traditional Chinese and (2) formal texts to informal texts."}, {"heading": "7.1 Simplified Chinese to Traditional Chinese", "text": "Traditional Chinese and Simplified Chinese are two similar languages with slight differences in character shapes (e.g., several traditional characters could be mapped to a simplified character). We are investigating whether data sets in Traditional Chinese and Simplified Chinese could help each other. Table 3 shows the results of Model-I using 3 traditional Chinese data sets using 5 simplified Chinese data sets. Specifically, we first train the model using simplified Chinese data sets, then we train traditional Chinese data sets independently with set common parameters. As we can see, the average performance of the F score is increased by 0.41% (from 93.78% to 94.19%), suggesting that common features learned from simplified Chinese segmentation criteria can help improve the performance of traditional Chinese data sets."}, {"heading": "7.2 Formal Texts to Informal Texts", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.2.1 Dataset", "text": "We use the NLPCC 2016 dataset 2 (Qiu et al. (2016) Qiu, Qian and Shi) to evaluate our model on microblog texts. The NLPCC 2016 data is provided by the joint task of the 5th CCF Conference on Natural Language Processing & Chinese Computing (NLPCC 2016): Chinese word segmentation and POS tagging for microblog text. Unlike the popular Newswire dataset, the NLPCC 2016 dataset is collected by Sina Weibo3, which consists of informal texts from the microblog covering various topics such as finance, sports, entertainment, etc. The information of the dataset is shown in Table 4."}, {"heading": "7.2.2 Results", "text": "Formal documents (such as the eight datasets in Table 1) and microblog texts differ in many aspects, so we are continuing to investigate whether the formal texts could help improve the performance of microblog texts. Table 5 gives the results of Model-I on the NLPCC 2016 dataset using the eight datasets in Table 1. Specifically: 2https: / / github.com / FudanNLP / NLPCC-WordSeg-Weibo3http: / / www.weibo.com / we first train the model on the eight datasets, then on the NLPCC 2016 dataset alone with set common parameters. The base model is Bi-LSTM, which is trained on the NLPCC 2016 dataset alone. As we can see, the performance is increased by 0.30% to F readings (from 93.94% to 94.24%), and we also found that the OOV memory rate can be improved by 3.7% to the learned micro-text."}, {"heading": "8 Related Works", "text": "There is much work on using heterogeneous annotation data to improve various NLP tasks. Jiang et al. (2009) Jiang, Huang, and Liu) proposed a structure-based stack model that could train a model for a specific desired annotation criterion by using knowledge from corpora with other heterogeneous annotations. Sun and Wan (2012) proposed a structure-based stack model to reduce the approximation error using structured features such as subwords. These models are unidirectional tools and also suffer from error propagation problems. (2013) Qiu et al. Qiu et al. (2013) Qiu, Zhao, and Huang) used multi-aspect learning frames to improve the performance of POS tagging on two heterogeneous datasets. Li et al al. (2015) Li, Chao, Zhang, Zhang, and Chen proposed an annex based on two notation models."}, {"heading": "9 Conclusions & Future Works", "text": "In this paper, we propose contradictory multi-criterion learning for CWS by making full use of the underlying common knowledge across multiple heterogeneous criteria. Experiments show that our proposed three common-private models are effective in extracting the common information and achieving significant improvements over the single-criterion methods."}, {"heading": "Acknowledgments", "text": "We appreciate the contribution of Jingjing Gong and Jiacheng Xu. We would also like to thank the anonymous critics for their valuable comments, which is partly funded by the National Natural Science Foundation of China (No. 61532011 and 61672162), Shanghai Municipal Science and Technology Commission on (No. 16JC1420401)."}], "references": [{"title": "Domain-adversarial neural networks", "author": ["Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "Fran\u00e7ois Laviolette", "Mario Marchand."], "venue": "arXiv preprint arXiv:1412.4446 .", "citeRegEx": "Ajakan et al\\.,? 2014", "shortCiteRegEx": "Ajakan et al\\.", "year": 2014}, {"title": "Exploiting task relatedness for multiple task learning", "author": ["S. Ben-David", "R. Schuller."], "venue": "Learning Theory and Kernel Machines pages 567\u2013580.", "citeRegEx": "Ben.David and Schuller.,? 2003", "shortCiteRegEx": "Ben.David and Schuller.", "year": 2003}, {"title": "Domain separation networks", "author": ["Konstantinos Bousmalis", "George Trigeorgis", "Nathan Silberman", "Dilip Krishnan", "Dumitru Erhan."], "venue": "Advances in Neural Information Processing Systems. pages 343\u2013 351.", "citeRegEx": "Bousmalis et al\\.,? 2016", "shortCiteRegEx": "Bousmalis et al\\.", "year": 2016}, {"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Machine learning 28(1):41\u201375.", "citeRegEx": "Caruana.,? 1997", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "Exploiting heterogeneous annotations for weibo word segmentation and pos tagging", "author": ["Jiayuan Chao", "Zhenghua Li", "Wenliang Chen", "Min Zhang."], "venue": "National CCF Conference on Natural Language Processing and Chinese Computing. Springer, pages", "citeRegEx": "Chao et al\\.,? 2015", "shortCiteRegEx": "Chao et al\\.", "year": 2015}, {"title": "Neural network for heterogeneous annotations", "author": ["Hongshen Chen", "Yue Zhang", "Qun Liu."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing .", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Gated recursive neural network for chinese word segmentation", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Xuanjing Huang."], "venue": "Proceedings of Annual Meeting of the Association for Computational Linguistics..", "citeRegEx": "Chen et al\\.,? 2015a", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Long short-term memory neural networks for chinese word segmentation", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang."], "venue": "EMNLP. pages 1197\u20131206.", "citeRegEx": "Chen et al\\.,? 2015b", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of ICML.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "The second international chinese word segmentation bakeoff", "author": ["Thomas Emerson."], "venue": "Proceedings of the fourth SIGHAN workshop on Chinese language Processing. volume 133.", "citeRegEx": "Emerson.,? 2005", "shortCiteRegEx": "Emerson.", "year": 2005}, {"title": "The part-of-speech tagging guidelines for the penn chinese treebank (3.0)", "author": ["XIA Fei"], "venue": "URL: http://www. cis. upenn. edu/ \u0303 chinese/segguide", "citeRegEx": "Fei.,? \\Q2000\\E", "shortCiteRegEx": "Fei.", "year": 2000}, {"title": "Domain-adversarial training of neural networks", "author": ["Yaroslav Ganin", "Evgeniya Ustinova", "Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "Fran\u00e7ois Laviolette", "Mario Marchand", "Victor Lempitsky."], "venue": "Journal of Machine Learning Research", "citeRegEx": "Ganin et al\\.,? 2016", "shortCiteRegEx": "Ganin et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."], "venue": "Advances in Neural Information Processing Systems. pages 2672\u20132680.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv preprint arXiv:1308.0850 .", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Automatic adaptation of annotation standards: Chinese word segmentation and POS tagging: a case study", "author": ["W. Jiang", "L. Huang", "Q. Liu."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint", "citeRegEx": "Jiang et al\\.,? 2009", "shortCiteRegEx": "Jiang et al\\.", "year": 2009}, {"title": "The fourth international chinese language processing bakeoff: Chinese word segmentation, named entity recognition and chinese pos tagging", "author": ["G. Jin", "X. Chen."], "venue": "Sixth SIGHAN Workshop on Chinese Language Processing. page 69.", "citeRegEx": "Jin and Chen.,? 2008", "shortCiteRegEx": "Jin and Chen.", "year": 2008}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings of The 32nd International Conference on Machine Learning.", "citeRegEx": "Jozefowicz et al\\.,? 2015", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Coupled sequence labeling on heterogeneous annotations: Pos tagging as a case study", "author": ["Zhenghua Li", "Jiayuan Chao", "Min Zhang", "Wenliang Chen."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Fast coupled sequence labeling on heterogeneous annotations via context-aware pruning", "author": ["Zhenghua Li", "Jiayuan Chao", "Min Zhang", "Jiwen Yang."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Deep multi-task learning with shared memory", "author": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Liu et al\\.,? 2016a", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Recurrent neural network for text classification with multi-task learning", "author": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."], "venue": "Proceedings of International Joint Conference on Artificial Intelligence.", "citeRegEx": "Liu et al\\.,? 2016b", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "arXiv preprint arXiv:1511.06114 .", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Maxmargin tensor neural network for chinese word segmentation", "author": ["Wenzhe Pei", "Tao Ge", "Chang Baobao."], "venue": "Proceedings of ACL.", "citeRegEx": "Pei et al\\.,? 2014", "shortCiteRegEx": "Pei et al\\.", "year": 2014}, {"title": "Overview of the NLPCC-ICCPOL 2016 shared task: Chinese word segmentation for micro-blog texts", "author": ["Xipeng Qiu", "Peng Qian", "Zhan Shi."], "venue": "International Conference on Computer Processing of Oriental Languages. Springer, pages 901\u2013906.", "citeRegEx": "Qiu et al\\.,? 2016", "shortCiteRegEx": "Qiu et al\\.", "year": 2016}, {"title": "Joint chinese word segmentation and pos tagging on heterogeneous annotated corpora with multiple task learning", "author": ["Xipeng Qiu", "Jiayi Zhao", "Xuanjing Huang."], "venue": "EMNLP. pages 658\u2013668.", "citeRegEx": "Qiu et al\\.,? 2013", "shortCiteRegEx": "Qiu et al\\.", "year": 2013}, {"title": "Reducing approximation and estimation errors for chinese lexical processing with heterogeneous annotations", "author": ["Weiwei Sun", "Xiaojun Wan."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-", "citeRegEx": "Sun and Wan.,? 2012", "shortCiteRegEx": "Sun and Wan.", "year": 2012}, {"title": "Processing norms of modern Chinese corpus", "author": ["S. Yu", "J. Lu", "X. Zhu", "H. Duan", "S. Kang", "H. Sun", "H. Wang", "Q. Zhao", "W. Zhan."], "venue": "Technical report, Technical report.", "citeRegEx": "Yu et al\\.,? 2001", "shortCiteRegEx": "Yu et al\\.", "year": 2001}, {"title": "Deep learning for chinese word segmentation and pos tagging", "author": ["Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu."], "venue": "EMNLP. pages 647\u2013657.", "citeRegEx": "Zheng et al\\.,? 2013", "shortCiteRegEx": "Zheng et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 28, "context": "As shown in Table 1, given a sentence \u201cYaoMing reaches the final\u201d, the two commonlyused corpora, PKU\u2019s People\u2019s Daily (PKU) (Yu et al.(2001)Yu, Lu, Zhu, Duan, Kang, Sun, Wang, Zhao, and Zhan) and Penn Chinese Treebank (CTB) (Fei(2000)), use different segmentation criteria.", "startOffset": 125, "endOffset": 141}, {"referenceID": 10, "context": "(2001)Yu, Lu, Zhu, Duan, Kang, Sun, Wang, Zhao, and Zhan) and Penn Chinese Treebank (CTB) (Fei(2000)), use different segmentation criteria.", "startOffset": 91, "endOffset": 101}, {"referenceID": 10, "context": "Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging (Jiang et al.(2009)Jiang, Huang, and Liu; Sun and Wan(2012); Qiu et al.", "startOffset": 136, "endOffset": 155}, {"referenceID": 10, "context": "Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging (Jiang et al.(2009)Jiang, Huang, and Liu; Sun and Wan(2012); Qiu et al.", "startOffset": 136, "endOffset": 195}, {"referenceID": 10, "context": "Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging (Jiang et al.(2009)Jiang, Huang, and Liu; Sun and Wan(2012); Qiu et al.(2013)Qiu, Zhao, and Huang; Li et al.", "startOffset": 136, "endOffset": 213}, {"referenceID": 10, "context": "Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging (Jiang et al.(2009)Jiang, Huang, and Liu; Sun and Wan(2012); Qiu et al.(2013)Qiu, Zhao, and Huang; Li et al.(2015)Li, Chao, Zhang, and Chen; Li et al.", "startOffset": 136, "endOffset": 250}, {"referenceID": 10, "context": "Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging (Jiang et al.(2009)Jiang, Huang, and Liu; Sun and Wan(2012); Qiu et al.(2013)Qiu, Zhao, and Huang; Li et al.(2015)Li, Chao, Zhang, and Chen; Li et al.(2016)Li, Chao, Zhang, and Yang).", "startOffset": 136, "endOffset": 292}, {"referenceID": 5, "context": "Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks (Collobert and Weston(2008); Luong et al.", "startOffset": 107, "endOffset": 134}, {"referenceID": 5, "context": "Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks (Collobert and Weston(2008); Luong et al.(2015)Luong, Le, Sutskever, Vinyals, and Kaiser; Chen et al.", "startOffset": 107, "endOffset": 154}, {"referenceID": 5, "context": "(2015)Luong, Le, Sutskever, Vinyals, and Kaiser; Chen et al.(2016)Chen, Zhang, and Liu).", "startOffset": 49, "endOffset": 67}, {"referenceID": 2, "context": "Specifically, we regard each segmentation criterion as a single task and propose three different shared-private models under the framework of multi-task learning (Caruana(1997); BenDavid and Schuller(2003)), where a shared layer is used to extract the criteria-invariant features, and a private layer is used to extract the criteriaspecific features.", "startOffset": 163, "endOffset": 177}, {"referenceID": 2, "context": "Specifically, we regard each segmentation criterion as a single task and propose three different shared-private models under the framework of multi-task learning (Caruana(1997); BenDavid and Schuller(2003)), where a shared layer is used to extract the criteria-invariant features, and a private layer is used to extract the criteriaspecific features.", "startOffset": 163, "endOffset": 206}, {"referenceID": 0, "context": "Inspired by the success of adversarial strategy on domain adaption (Ajakan et al.(2014)Ajakan, Germain, Larochelle, Laviolette, and Marchand; Ganin et al.", "startOffset": 68, "endOffset": 88}, {"referenceID": 0, "context": "Inspired by the success of adversarial strategy on domain adaption (Ajakan et al.(2014)Ajakan, Germain, Larochelle, Laviolette, and Marchand; Ganin et al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle, Laviar X iv :1 70 4.", "startOffset": 68, "endOffset": 161}, {"referenceID": 2, "context": "olette, Marchand, and Lempitsky; Bousmalis et al.(2016)Bousmalis, Trigeorgis, Silberman, Krishnan, and Erhan), we further utilize adversarial strategy to make sure the shared layer can extract the common underlying and criteria-invariant features, which are suitable for all the criteria.", "startOffset": 33, "endOffset": 56}, {"referenceID": 26, "context": "Recently, neural networks are widely applied to Chinese word segmentation task for their ability to minimize the effort in feature engineering (Zheng et al.(2013)Zheng, Chen, and Xu; Pei et al.", "startOffset": 144, "endOffset": 163}, {"referenceID": 22, "context": "(2013)Zheng, Chen, and Xu; Pei et al.(2014)Pei, Ge, and Baobao; Chen et al.", "startOffset": 27, "endOffset": 44}, {"referenceID": 5, "context": "(2014)Pei, Ge, and Baobao; Chen et al.(2015a)Chen, Qiu, Zhu, and Huang; Chen et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 5, "context": "(2014)Pei, Ge, and Baobao; Chen et al.(2015a)Chen, Qiu, Zhu, and Huang; Chen et al.(2015b)Chen, Qiu, Zhu, Liu, and Huang).", "startOffset": 27, "endOffset": 91}, {"referenceID": 15, "context": "While there are numerous LSTM variants, here we use the LSTM architecture used by (Jozefowicz et al.(2015)Jozefowicz, Zaremba, and Sutskever), which is similar to the architecture of (Graves(2013)) but without peep-hole connections.", "startOffset": 83, "endOffset": 107}, {"referenceID": 13, "context": "(2015)Jozefowicz, Zaremba, and Sutskever), which is similar to the architecture of (Graves(2013)) but without peep-hole connections.", "startOffset": 84, "endOffset": 97}, {"referenceID": 18, "context": "After extracting features, we employ conditional random fields (CRF) (Lafferty et al.(2001)Lafferty, McCallum, and Pereira) layer to inference tags.", "startOffset": 70, "endOffset": 92}, {"referenceID": 2, "context": "Inspired by the success of multi-task learning (Caruana(1997); Ben-David and Schuller(2003); Liu et al.", "startOffset": 48, "endOffset": 62}, {"referenceID": 1, "context": "Inspired by the success of multi-task learning (Caruana(1997); Ben-David and Schuller(2003); Liu et al.", "startOffset": 63, "endOffset": 92}, {"referenceID": 1, "context": "Inspired by the success of multi-task learning (Caruana(1997); Ben-David and Schuller(2003); Liu et al.(2016a)Liu, Qiu, and Huang; Liu et al.", "startOffset": 63, "endOffset": 111}, {"referenceID": 1, "context": "Inspired by the success of multi-task learning (Caruana(1997); Ben-David and Schuller(2003); Liu et al.(2016a)Liu, Qiu, and Huang; Liu et al.(2016b)Liu, Qiu, and Huang), we regard the heterogenous criteria as multiple \u201crelated\u201d tasks, which could improve the performance of each other simultaneously with shared information.", "startOffset": 63, "endOffset": 149}, {"referenceID": 0, "context": "Inspired by the work on domain adaptation (Ajakan et al.(2014)Ajakan, Germain, Larochelle, Laviolette, and Marchand; Ganin et al.", "startOffset": 43, "endOffset": 63}, {"referenceID": 0, "context": "Inspired by the work on domain adaptation (Ajakan et al.(2014)Ajakan, Germain, Larochelle, Laviolette, and Marchand; Ganin et al.(2016)Ganin,", "startOffset": 43, "endOffset": 136}, {"referenceID": 2, "context": "Ustinova, Ajakan, Germain, Larochelle, Laviolette, Marchand, and Lempitsky; Bousmalis et al.(2016)Bousmalis, Trigeorgis, Silberman, Krishnan, and Erhan), we hope that the features extracted by shared layer is invariant across the heterogonous segmentation criteria.", "startOffset": 76, "endOffset": 99}, {"referenceID": 2, "context": "Ustinova, Ajakan, Germain, Larochelle, Laviolette, Marchand, and Lempitsky; Bousmalis et al.(2016)Bousmalis, Trigeorgis, Silberman, Krishnan, and Erhan), we hope that the features extracted by shared layer is invariant across the heterogonous segmentation criteria. Therefore, we jointly optimize the shared layer via adversarial training (Goodfellow et al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio).", "startOffset": 76, "endOffset": 364}, {"referenceID": 11, "context": "Unlike (Ganin et al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle, Laviolette, Marchand, and Lempitsky), we use entropy term instead of negative cross-entropy.", "startOffset": 8, "endOffset": 27}, {"referenceID": 17, "context": "We use Adam (Kingma and Ba(2014)) with minibatchs to maximize the objectives.", "startOffset": 13, "endOffset": 33}, {"referenceID": 9, "context": "To evaluate our proposed architecture, we experiment on eight prevalent CWS datasets from SIGHAN2005 (Emerson(2005)) and SIGHAN2008 (Jin and Chen(2008)).", "startOffset": 102, "endOffset": 116}, {"referenceID": 9, "context": "To evaluate our proposed architecture, we experiment on eight prevalent CWS datasets from SIGHAN2005 (Emerson(2005)) and SIGHAN2008 (Jin and Chen(2008)).", "startOffset": 102, "endOffset": 152}, {"referenceID": 21, "context": "We simply map traditional Chinese characters to simplified Chinese, and optimize on the same character embedding matrix across datasets, which is pre-trained on Chinese Wikipedia corpus, using word2vec toolkit (Mikolov et al.(2013)Mikolov, Chen, Corrado, and Dean).", "startOffset": 211, "endOffset": 232}, {"referenceID": 5, "context": "Following previous work (Chen et al.(2015b)Chen, Qiu, Zhu, Liu, and Huang; Pei et al.", "startOffset": 25, "endOffset": 44}, {"referenceID": 5, "context": "Following previous work (Chen et al.(2015b)Chen, Qiu, Zhu, Liu, and Huang; Pei et al.(2014)Pei, Ge, and Baobao), all experiments including baseline results are using pre-trained character embedding with bigram feature.", "startOffset": 25, "endOffset": 92}, {"referenceID": 5, "context": "In addition, although the F value of LSTM model in (Chen et al.(2015b)Chen, Qiu, Zhu, Liu, and Huang) is 97.", "startOffset": 52, "endOffset": 71}, {"referenceID": 26, "context": "1 Dataset We use the NLPCC 2016 dataset2 (Qiu et al.(2016)Qiu, Qian, and Shi) to evaluate our model on micro-blog texts.", "startOffset": 42, "endOffset": 59}, {"referenceID": 10, "context": "Jiang et al.(2009)Jiang, Huang, and Liu) proposed a stacking-based model which could train a model for one specific desired annotation criterion by utilizing knowledge from corpora with other heterogeneous annotations.", "startOffset": 0, "endOffset": 19}, {"referenceID": 10, "context": "Jiang et al.(2009)Jiang, Huang, and Liu) proposed a stacking-based model which could train a model for one specific desired annotation criterion by utilizing knowledge from corpora with other heterogeneous annotations. Sun and Wan(2012)) proposed a structure-based stacking model to reduce the approximation error, which makes use of structured features such as subwords.", "startOffset": 0, "endOffset": 237}, {"referenceID": 10, "context": "Jiang et al.(2009)Jiang, Huang, and Liu) proposed a stacking-based model which could train a model for one specific desired annotation criterion by utilizing knowledge from corpora with other heterogeneous annotations. Sun and Wan(2012)) proposed a structure-based stacking model to reduce the approximation error, which makes use of structured features such as subwords. These models are unidirectional aid and also suffer from error propagation problem. Qiu et al.(2013)Qiu, Zhao, and Huang) used multi-tasks learning framework to improve the performance of POS tagging on two heterogeneous datasets.", "startOffset": 0, "endOffset": 473}, {"referenceID": 10, "context": "Jiang et al.(2009)Jiang, Huang, and Liu) proposed a stacking-based model which could train a model for one specific desired annotation criterion by utilizing knowledge from corpora with other heterogeneous annotations. Sun and Wan(2012)) proposed a structure-based stacking model to reduce the approximation error, which makes use of structured features such as subwords. These models are unidirectional aid and also suffer from error propagation problem. Qiu et al.(2013)Qiu, Zhao, and Huang) used multi-tasks learning framework to improve the performance of POS tagging on two heterogeneous datasets. Li et al.(2015)Li, Chao, Zhang, and Chen) proposed a coupled sequence labeling model which could directly learn and infer two heterogeneous annotations.", "startOffset": 0, "endOffset": 619}, {"referenceID": 4, "context": "Chao et al.(2015)Chao, Li, Chen, and Zhang) also utilize multiple corpora using coupled sequence labeling model.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "Chao et al.(2015)Chao, Li, Chen, and Zhang) also utilize multiple corpora using coupled sequence labeling model. These methods adopt the shallow classifiers, therefore suffering from the problem of defining shared features. Our proposed models use deep neural networks, which can easily share information with hidden shared layers. Chen et al.(2016)Chen, Zhang, and Liu) also adopted neural network models for exploiting heterogeneous annotations based on neural multi-view model, which can be regarded as a simplified version of our proposed models by re-", "startOffset": 0, "endOffset": 350}], "year": 2017, "abstractText": "Different linguistic perspectives causes many diverse segmentation criteria for Chinese word segmentation (CWS). Most existing methods focus on improve the performance for each single criterion. However, it is interesting to exploit these different criteria and mining their common underlying knowledge. In this paper, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge from multiple heterogeneous segmentation criteria. Experiments on eight corpora with heterogeneous segmentation criteria show that the performance of each corpus obtains a significant improvement, compared to single-criterion learning. Source codes of this paper are available on Github1.", "creator": "TeX"}}}