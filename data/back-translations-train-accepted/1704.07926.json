{"id": "1704.07926", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood", "abstract": "Our goal is to learn a semantic parser that maps natural language utterances into executable programs when only indirect supervision is available: examples are labeled with the correct execution result, but not the program itself. Consequently, we must search the space of programs for those that output the correct result, while not being misled by spurious programs: incorrect programs that coincidentally output the correct result. We connect two common learning paradigms, reinforcement learning (RL) and maximum marginal likelihood (MML), and then present a new learning algorithm that combines the strengths of both. The new algorithm guards against spurious programs by combining the systematic search traditionally employed in MML with the randomized exploration of RL, and by updating parameters such that probability is spread more evenly across consistent programs. We apply our learning algorithm to a new neural semantic parser and show significant gains over existing state-of-the-art results on a recent context-dependent semantic parsing task.", "histories": [["v1", "Tue, 25 Apr 2017 22:51:12 GMT  (121kb,D)", "http://arxiv.org/abs/1704.07926v1", "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (2017)"]], "COMMENTS": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (2017)", "reviews": [], "SUBJECTS": "cs.AI cs.LG stat.ML", "authors": ["kelvin guu", "panupong pasupat", "evan zheran liu", "percy liang"], "accepted": true, "id": "1704.07926"}, "pdf": {"name": "1704.07926.pdf", "metadata": {"source": "CRF", "title": "From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood", "authors": ["Kelvin Guu", "Panupong Pasupat", "Evan Zheran Liu", "Percy Liang"], "emails": ["kguu@stanford.edu", "ppasupat@stanford.edu", "evanliu@stanford.edu", "pliang@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live."}, {"heading": "2 Task", "text": "The semantic particle reaction in SCONE dataset1 (Long et al, 2016). As shown in Figure 1, it consists of a world that contains several objects (e.g. humans), each of which has specific properties (e.g. shirt color and color)."}, {"heading": "3 Model", "text": "We present a program as a sequence prediction model with attention, which represents a sequence of program tokens in postfix notation; for example, we can execute the programs incrementally from left to right by moving the constants (e.g. yellow) to the stack, while the functions (e.g. hasHat) push back corresponding arguments from the stack and push-back of the calculated result (e.g. the list of people with yellow hats). Appendix B lists the full number of program tokens, and how they are executed."}, {"heading": "4 Reinforcement learning versus maximum marginal likelihood", "text": "After formulating our task as a sequence prediction problem, we still have to opt for a learning algorithm. First, we compare two standard paradigms: reinforcement learning (RL) and maximum marginal probability (MML). In the next section, we propose a better alternative."}, {"heading": "4.1 Comparing objective functions", "text": "From an RL perspective, in a training example (x, y), a sequence of decisions is made (z = (z1,.., zT) and a reward is given at the end of the episode: R (z) = 1 if z performs to y and 0 otherwise (dependence on x and y is omitted in the notation). We focus on policy gradient methods where a stochastic policy function is trained to maximize the expected reward. In our setup, p\u03b8 (z | x) is the policy (with parameters \u03b8) and its expected reward in a given example (x, y) isG (x, y) = x (z) p\u03b8 (z | x), (1) where the sum is above all possible programs. The general RL goal, JRL, is the expected reward in examples: JRL = x, y)."}, {"heading": "4.2 Comparing gradients", "text": "The gradients of JRL and JMML are closely related to each other. They are both in the form that q (z) q (z) q (z) x (x) x (x) x (z) x (z) x (z) x (z) equal q (z) = pTB (z) for JRL, (7) qMML (z) = R (z) x (z) x (z) x (z) x (z) x (z) x (z), where q (z) equal qRL (z) = pTB (z | x) for JRL, (7) qMML (z) = R (z) pTB (z | x) pTB (z) (8) = pTB (z | x, R (z) 6 = 0) for JMML. 3 Note that the protocol of the product in (5) is not equal to thesum in (2)."}, {"heading": "4.3 Comparing gradient approximation strategies", "text": "In the policy-gradient literature, Monte Carlo integration (MC) is the typical approximation strategy. For example, the popular REINFORCE algorithm (Williams, 1992) uses Monte Carlo sampling to calculate an unbiased estimate of the gradient. (9) Where S is a collection of B samples, z (b) q (z), and c is a baseline (Williams, 1992) that is used to reduce the variance of the estimate without changing its expectation. (In the MML literature for latent sequences, expectation is typically approached via numerical integration (NUM)."}, {"heading": "5 Tackling spurious programs", "text": "In this section we will illustrate why wrong programs are problematic for the most commonly used methods in RL (REINFORCE) and MML (Beam Search MML). We will describe two key problems and propose a solution for each, based on insights gained from our comparison of RL and MML in Section 4."}, {"heading": "5.1 Spurious programs bias exploration", "text": "In both cases, it is likely that the current policy will have a low probability of the correct distribution of z-shaped programs, and consequently will not be able to increase the probability of z-shaped programs. (That is, they will increase the probability of z-shaped programs.) That is, they will not get a grip on the probability of z-shaped programs. (That is, they will get a grip on the probability of z-shaped programs.) We will not get a grip on the probability of z-shaped programs. (That means that they will not get a grip on the probability of z-shaped programs.)"}, {"heading": "5.2 Spurious programs dominate gradients", "text": "In both RL and MML programs, even if the exploration is perfect and the gradient is calculated exactly \u03b2 \u03b2, counterfeit programs can still be problematic. Even if a faulty exploration visits each program, we see from the gradient weights q (z) in (7) and (8) that programs are weighted proportionally to their current policy probability. If a faulty program z \"has a 100 times higher probability than z\" as in Figure 2, the gradient will be about 99% of its size appreciation toward z \"and only 1% toward z,\" even though the two programs receive the same reward. This implies that it would require many updates for z \"to catch up with the gradient. In fact, z\" gradient \"could never catch up, depending on gradient upgrades for other training examples. Simply increasing the learning rate is insufficient as it would result in the model qqz being\" weighted toward an L, \"which could potentially lead to an optimization of ML."}, {"heading": "5.3 Summary of the proposed approach", "text": "We have described two problems5 and their solutions: We reduce the exploration bias by greedy, randomized beam search and perform a more balanced optimization using \u03b2-meritocratic parameter update rule. We call our resulting approach RANDOMER. Table 1 summarizes how RANDOMER combines desirable properties of REINFORCE and BS-MML."}, {"heading": "6 Experiments", "text": "We evaluate our proposed methods in all three areas of the SCONE dataset. Accuracy is defined as the percentage of test examples in which the model produces the correct final state wM. All test examples have M = 5 (5utts), but we also report accuracy after processing the first 3 expressions (3utts). To control the effects of randomness, we train 5 instances of each model with different random seeds. We report the mean accuracy of the instances unless otherwise noted. Following Long et al. (2016), we split each training example into smaller examples. In the face of an example with 5 expressions, u = [u1,., u5] we consider all length-1 and length-2 substrings of u. [u1], [u2], [u3, u4], u4], u5, u5. \""}, {"heading": "6.1 Main results", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "7 Related work and discussion", "text": "In fact, it is so that in the initial stages of formation, in which we go in search, it is difficult for us to go in search. It is also the problem of the false choice of place, the reward that we cannot afford. (...) We are very well able to go in search. (...) It is not as if we go in search. (...) It is as if we go in search. (...) It is as if we go in search of the truth. (...) It is as if we go in search of the truth. (...) It is as if we go in search of the truth. \"(...) It is as if we are in search of the truth.\" (...) It is as if we are in search of the truth. (...) It is as if we are not in search of the truth. (...) It is as if we are in search of the truth. (...) It is not as if we are in search of the truth."}, {"heading": "A Hyperparameters in Table 2", "text": "In fact, it is not as if it were a reactionary act, but one that sees itself in a position to put itself in its place."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Tucker", "V. Vanhoucke", "V. Vasudevan", "F.B. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng."], "venue": "arXiv preprint arXiv:1603.04467 .", "citeRegEx": "Tucker et al\\.,? 2015", "shortCiteRegEx": "Tucker et al\\.", "year": 2015}, {"title": "Bootstrapping semantic parsers from conversations", "author": ["Y. Artzi", "L. Zettlemoyer."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 421\u2013432.", "citeRegEx": "Artzi and Zettlemoyer.,? 2011", "shortCiteRegEx": "Artzi and Zettlemoyer.", "year": 2011}, {"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions", "author": ["Y. Artzi", "L. Zettlemoyer."], "venue": "Transactions of the Association for Computational Linguistics (TACL) 1:49\u201362.", "citeRegEx": "Artzi and Zettlemoyer.,? 2013", "shortCiteRegEx": "Artzi and Zettlemoyer.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Unifying countbased exploration and intrinsic motivation", "author": ["M. Bellemare", "S. Srinivasan", "G. Ostrovski", "T. Schaul", "D. Saxton", "R. Munos."], "venue": "Advances in Neural Information Processing Systems (NIPS). pages 1471\u20131479.", "citeRegEx": "Bellemare et al\\.,? 2016", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["S. Bengio", "O. Vinyals", "N. Jaitly", "N. Shazeer."], "venue": "Advances in Neural Information Processing Systems (NIPS). pages 1171\u2013 1179.", "citeRegEx": "Bengio et al\\.,? 2015", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Reinforcement learning for mapping instructions to actions", "author": ["S. Branavan", "H. Chen", "L.S. Zettlemoyer", "R. Barzilay."], "venue": "Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-", "citeRegEx": "Branavan et al\\.,? 2009", "shortCiteRegEx": "Branavan et al\\.", "year": 2009}, {"title": "Deep reinforcement learning for mention-ranking coreference models", "author": ["K. Clark", "C.D. Manning."], "venue": "arXiv preprint arXiv:1609.08667 .", "citeRegEx": "Clark and Manning.,? 2016", "shortCiteRegEx": "Clark and Manning.", "year": 2016}, {"title": "Driving semantic parsing from the world\u2019s response", "author": ["J. Clarke", "D. Goldwasser", "M. Chang", "D. Roth."], "venue": "Computational Natural Language Learning (CoNLL). pages 18\u201327.", "citeRegEx": "Clarke et al\\.,? 2010", "shortCiteRegEx": "Clarke et al\\.", "year": 2010}, {"title": "Efficient selectivity and backup operators in Monte-Carlo tree search", "author": ["R. Coulom."], "venue": "International Conference on Computers and Games. pages 72\u201383.", "citeRegEx": "Coulom.,? 2006", "shortCiteRegEx": "Coulom.", "year": 2006}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["L.N.M.A.P. Dempster", "R.D.B."], "venue": "Journal of the Royal Statistical Society: Series B 39(1):1\u201338.", "citeRegEx": "Dempster and ,? 1977", "shortCiteRegEx": "Dempster and ", "year": 1977}, {"title": "Language to logical form with neural attention", "author": ["L. Dong", "M. Lapata."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Dong and Lapata.,? 2016", "shortCiteRegEx": "Dong and Lapata.", "year": 2016}, {"title": "Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes", "author": ["M.O. Duff."], "venue": "Ph.D. thesis, University of Massachusetts Amherst.", "citeRegEx": "Duff.,? 2002", "shortCiteRegEx": "Duff.", "year": 2002}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio."], "venue": "International Conference on Artificial Intelligence and Statistics.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Data recombination for neural semantic parsing", "author": ["R. Jia", "P. Liang."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Jia and Liang.,? 2016", "shortCiteRegEx": "Jia and Liang.", "year": 2016}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["M. Kearns", "S. Singh."], "venue": "Machine Learning 49(2):209\u2013232.", "citeRegEx": "Kearns and Singh.,? 2002", "shortCiteRegEx": "Kearns and Singh.", "year": 2002}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Weakly supervised training of semantic parsers", "author": ["J. Krishnamurthy", "T. Mitchell."], "venue": "Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL). pages 754\u2013765.", "citeRegEx": "Krishnamurthy and Mitchell.,? 2012", "shortCiteRegEx": "Krishnamurthy and Mitchell.", "year": 2012}, {"title": "Motor Skill Learning with Local Trajectory Methods", "author": ["S. Levine."], "venue": "Ph.D. thesis, Stanford University.", "citeRegEx": "Levine.,? 2014", "shortCiteRegEx": "Levine.", "year": 2014}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["J. Li", "W. Monroe", "A. Ritter", "D. Jurafsky", "M. Galley", "J. Gao."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Neural symbolic machines: Learning semantic parsers on Freebase with weak supervision", "author": ["C. Liang", "J. Berant", "Q. Le", "K.D.F.N. Lao."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Liang et al\\.,? 2017", "shortCiteRegEx": "Liang et al\\.", "year": 2017}, {"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein."], "venue": "Association for Computational Linguistics (ACL). pages 590\u2013599.", "citeRegEx": "Liang et al\\.,? 2011", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "Simpler context-dependent logical forms via model projections", "author": ["R. Long", "P. Pasupat", "P. Liang."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Long et al\\.,? 2016", "shortCiteRegEx": "Long et al\\.", "year": 2016}, {"title": "Improving policy gradient by exploring under-appreciated rewards", "author": ["O. Nachum", "M. Norouzi", "D. Schuurmans."], "venue": "arXiv preprint arXiv:1611.09321 .", "citeRegEx": "Nachum et al\\.,? 2016", "shortCiteRegEx": "Nachum et al\\.", "year": 2016}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["K. Narasimhan", "T. Kulkarni", "R. Barzilay."], "venue": "arXiv preprint arXiv:1506.08941 .", "citeRegEx": "Narasimhan et al\\.,? 2015", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["A. Neelakantan", "Q.V. Le", "I. Sutskever."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Neelakantan et al\\.,? 2016", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "Reward augmented maximum likelihood for neural structured prediction", "author": ["M. Norouzi", "S. Bengio", "N. Jaitly", "M. Schuster", "Y. Wu", "D. Schuurmans"], "venue": "In Advances In Neural Information Processing Systems", "citeRegEx": "Norouzi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2016}, {"title": "Minimum error rate training in statistical machine translation", "author": ["F.J. Och."], "venue": "Association for Computational Linguistics (ACL). pages 160\u2013167.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Deep exploration via bootstrapped DQN", "author": ["I. Osband", "C. Blundell", "A. Pritzel", "B.V. Roy."], "venue": "Advances In Neural Information Processing Systems. pages 4026\u20134034.", "citeRegEx": "Osband et al\\.,? 2016", "shortCiteRegEx": "Osband et al\\.", "year": 2016}, {"title": "Generalization and exploration via randomized value functions", "author": ["I. Osband", "B.V. Roy", "Z. Wen."], "venue": "arXiv preprint arXiv:1402.0635 .", "citeRegEx": "Osband et al\\.,? 2014", "shortCiteRegEx": "Osband et al\\.", "year": 2014}, {"title": "Compositional semantic parsing on semi-structured tables", "author": ["P. Pasupat", "P. Liang."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Pasupat and Liang.,? 2015", "shortCiteRegEx": "Pasupat and Liang.", "year": 2015}, {"title": "Inferring logical forms from denotations", "author": ["P. Pasupat", "P. Liang."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Pasupat and Liang.,? 2016", "shortCiteRegEx": "Pasupat and Liang.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Sequence level training with recurrent neural networks", "author": ["M. Ranzato", "S. Chopra", "M. Auli", "W. Zaremba."], "venue": "arXiv preprint arXiv:1511.06732 .", "citeRegEx": "Ranzato et al\\.,? 2015", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Largescale semantic parsing without question-answer pairs", "author": ["S. Reddy", "M. Lapata", "M. Steedman."], "venue": "Transactions of the Association for Computational Linguistics (TACL) 2(10):377\u2013392.", "citeRegEx": "Reddy et al\\.,? 2014", "shortCiteRegEx": "Reddy et al\\.", "year": 2014}, {"title": "Programming with a differentiable forth interpreter", "author": ["S. Riedel", "M. Bosnjak", "T. Rockt\u00e4schel."], "venue": "CoRR, abs/1605.06640 .", "citeRegEx": "Riedel et al\\.,? 2016", "shortCiteRegEx": "Riedel et al\\.", "year": 2016}, {"title": "A reduction of imitation learning and structured prediction to noregret online learning", "author": ["S. Ross", "G. Gordon", "A. Bagnell."], "venue": "Artificial Intelligence and Statistics (AISTATS).", "citeRegEx": "Ross et al\\.,? 2011", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Minimum risk training for neural machine translation", "author": ["S. Shen", "Y. Cheng", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu."], "venue": "arXiv preprint arXiv:1512.02433 .", "citeRegEx": "Shen et al\\.,? 2015", "shortCiteRegEx": "Shen et al\\.", "year": 2015}, {"title": "Minimum risk annealing for training log-linear models", "author": ["D.A. Smith", "J. Eisner."], "venue": "International Conference on Computational Linguistics and Association for Computational Linguistics (COLING/ACL). pages 787\u2013794.", "citeRegEx": "Smith and Eisner.,? 2006", "shortCiteRegEx": "Smith and Eisner.", "year": 2006}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R. Sutton", "D. McAllester", "S. Singh", "Y. Mansour."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Sutton et al\\.,? 1999", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Improving multi-step prediction of learned time series models", "author": ["A. Venkatraman", "M. Hebert", "J.A. Bagnell."], "venue": "Association for the Advancement of Artificial Intelligence (AAAI). pages 3024\u20133030. R. J. Williams. 1992. Simple statistical gradient-", "citeRegEx": "Venkatraman et al\\.,? 2015", "shortCiteRegEx": "Venkatraman et al\\.", "year": 2015}, {"title": "Neural enquirer: Learning to query tables", "author": ["P. Yin", "Z. Lu", "H. Li", "B. Kao."], "venue": "arXiv preprint arXiv:1512.00965 .", "citeRegEx": "Yin et al\\.,? 2015", "shortCiteRegEx": "Yin et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "put (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Liang et al., 2017).", "startOffset": 4, "endOffset": 128}, {"referenceID": 21, "context": "put (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Liang et al., 2017).", "startOffset": 4, "endOffset": 128}, {"referenceID": 17, "context": "put (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Liang et al., 2017).", "startOffset": 4, "endOffset": 128}, {"referenceID": 2, "context": "put (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Liang et al., 2017).", "startOffset": 4, "endOffset": 128}, {"referenceID": 20, "context": "put (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Liang et al., 2017).", "startOffset": 4, "endOffset": 128}, {"referenceID": 39, "context": "In the natural language processing literature, there are two common approaches for handling this situation: 1) reinforcement learning (RL), particularly the REINFORCE algorithm (Williams, 1992; Sutton et al., 1999), which maximizes the expected reward of a sequence of actions; and 2) maximum marginal likelihood (MML), which treats the sequence of actions as a latent variable, and then maximizes the marginal likelihood of observing the correct program output (Dempster et al.", "startOffset": 177, "endOffset": 214}, {"referenceID": 31, "context": "This is because in addition to the sparsity of correct programs, our task also requires weeding out spurious programs (Pasupat and Liang, 2016): incorrect interpretations ar X iv :1 70 4.", "startOffset": 118, "endOffset": 143}, {"referenceID": 22, "context": "We evaluate our resulting system on SCONE, the context-dependent semantic parsing dataset of Long et al. (2016). Our approach outperforms standard RL and MML methods in a direct comparison, and achieves new state-of-the-art results, improving over Long et al.", "startOffset": 93, "endOffset": 112}, {"referenceID": 22, "context": "We evaluate our resulting system on SCONE, the context-dependent semantic parsing dataset of Long et al. (2016). Our approach outperforms standard RL and MML methods in a direct comparison, and achieves new state-of-the-art results, improving over Long et al. (2016) in all three domains of SCONE, and by over 30% accuracy on the most challenging one.", "startOffset": 93, "endOffset": 267}, {"referenceID": 22, "context": "We consider the semantic parsing task in the SCONE dataset1 (Long et al., 2016).", "startOffset": 60, "endOffset": 79}, {"referenceID": 3, "context": "from left to right using a neural encoder-decoder model with attention (Bahdanau et al., 2015).", "startOffset": 71, "endOffset": 94}, {"referenceID": 32, "context": "where \u03a6u(um,i) is the fixed GloVe word embedding (Pennington et al., 2014) of the ith word in um.", "startOffset": 49, "endOffset": 74}, {"referenceID": 3, "context": "Unlike Bahdanau et al. (2015), which used a recurrent network for the decoder, we opt for a feed-forward network for simplicity.", "startOffset": 7, "endOffset": 30}, {"referenceID": 22, "context": "Following Long et al. (2016), we decompose each training example into smaller examples.", "startOffset": 10, "endOffset": 29}, {"referenceID": 13, "context": "Model parameters are randomly initialized (Glorot and Bengio, 2010), with no pre-training.", "startOffset": 42, "endOffset": 67}, {"referenceID": 16, "context": "We use the Adam optimizer (Kingma and Ba, 2014) (which is applied to the gradient in (6)), a learning rate of 0.", "startOffset": 26, "endOffset": 47}, {"referenceID": 32, "context": "use fixed GloVe vectors (Pennington et al., 2014) to embed the words in each utterance.", "startOffset": 24, "endOffset": 49}, {"referenceID": 33, "context": "For REINFORCE, we also experimented with a regression-estimated baseline (Ranzato et al., 2015), but found it to perform worse than a constant baseline.", "startOffset": 73, "endOffset": 95}, {"referenceID": 22, "context": "Table 2 compares RANDOMER to results from Long et al. (2016) as well as two baselines, REINFORCE and BSMML (using the same neural model but different learning algorithms).", "startOffset": 42, "endOffset": 61}, {"referenceID": 22, "context": "LONG+16 results are directly from Long et al. (2016). Hyperparameters are chosen by best performance on validation set (see Appendix A).", "startOffset": 34, "endOffset": 53}, {"referenceID": 6, "context": "Concurrently, there has been a recent surge of interest in reinforcement learning, along with the wide application of the classic REINFORCE algorithm (Williams, 1992)\u2014to troubleshooting (Branavan et al., 2009), dialog generation (Li et al.", "startOffset": 186, "endOffset": 209}, {"referenceID": 19, "context": ", 2009), dialog generation (Li et al., 2016), game playing (Narasimhan et al.", "startOffset": 27, "endOffset": 44}, {"referenceID": 24, "context": ", 2016), game playing (Narasimhan et al., 2015), coreference resolution (Clark and Manning, 2016), machine translation (Norouzi et al.", "startOffset": 22, "endOffset": 47}, {"referenceID": 7, "context": ", 2015), coreference resolution (Clark and Manning, 2016), machine translation (Norouzi et al.", "startOffset": 32, "endOffset": 57}, {"referenceID": 26, "context": ", 2015), coreference resolution (Clark and Manning, 2016), machine translation (Norouzi et al., 2016), and even semantic parsing (Liang et al.", "startOffset": 79, "endOffset": 101}, {"referenceID": 20, "context": ", 2016), and even semantic parsing (Liang et al., 2017).", "startOffset": 35, "endOffset": 55}, {"referenceID": 9, "context": "The RL answer would be better exploration, which can take many forms including simple action-dithering such as -greedy, entropy regularization (Williams and Peng, 1991), Monte Carlo tree search (Coulom, 2006), randomized value functions (Osband et al.", "startOffset": 194, "endOffset": 208}, {"referenceID": 12, "context": ", 2014, 2016), and methods which prioritize learning environment dynamics (Duff, 2002) or under-explored states (Kearns and Singh, 2002; Bellemare et al.", "startOffset": 74, "endOffset": 86}, {"referenceID": 15, "context": ", 2014, 2016), and methods which prioritize learning environment dynamics (Duff, 2002) or under-explored states (Kearns and Singh, 2002; Bellemare et al., 2016; Nachum et al., 2016).", "startOffset": 112, "endOffset": 181}, {"referenceID": 4, "context": ", 2014, 2016), and methods which prioritize learning environment dynamics (Duff, 2002) or under-explored states (Kearns and Singh, 2002; Bellemare et al., 2016; Nachum et al., 2016).", "startOffset": 112, "endOffset": 181}, {"referenceID": 23, "context": ", 2014, 2016), and methods which prioritize learning environment dynamics (Duff, 2002) or under-explored states (Kearns and Singh, 2002; Bellemare et al., 2016; Nachum et al., 2016).", "startOffset": 112, "endOffset": 181}, {"referenceID": 33, "context": "It is tempting to group our approach with sequence learning methods which interpolate between supervised learning and reinforcement learning (Ranzato et al., 2015; Venkatraman et al., 2015; Ross et al., 2011; Norouzi et al., 2016; Bengio et al., 2015; Levine, 2014).", "startOffset": 141, "endOffset": 265}, {"referenceID": 40, "context": "It is tempting to group our approach with sequence learning methods which interpolate between supervised learning and reinforcement learning (Ranzato et al., 2015; Venkatraman et al., 2015; Ross et al., 2011; Norouzi et al., 2016; Bengio et al., 2015; Levine, 2014).", "startOffset": 141, "endOffset": 265}, {"referenceID": 36, "context": "It is tempting to group our approach with sequence learning methods which interpolate between supervised learning and reinforcement learning (Ranzato et al., 2015; Venkatraman et al., 2015; Ross et al., 2011; Norouzi et al., 2016; Bengio et al., 2015; Levine, 2014).", "startOffset": 141, "endOffset": 265}, {"referenceID": 26, "context": "It is tempting to group our approach with sequence learning methods which interpolate between supervised learning and reinforcement learning (Ranzato et al., 2015; Venkatraman et al., 2015; Ross et al., 2011; Norouzi et al., 2016; Bengio et al., 2015; Levine, 2014).", "startOffset": 141, "endOffset": 265}, {"referenceID": 5, "context": "It is tempting to group our approach with sequence learning methods which interpolate between supervised learning and reinforcement learning (Ranzato et al., 2015; Venkatraman et al., 2015; Ross et al., 2011; Norouzi et al., 2016; Bengio et al., 2015; Levine, 2014).", "startOffset": 141, "endOffset": 265}, {"referenceID": 18, "context": "It is tempting to group our approach with sequence learning methods which interpolate between supervised learning and reinforcement learning (Ranzato et al., 2015; Venkatraman et al., 2015; Ross et al., 2011; Norouzi et al., 2016; Bengio et al., 2015; Levine, 2014).", "startOffset": 141, "endOffset": 265}, {"referenceID": 11, "context": "There has been recent interest in using recurrent neural networks for semantic parsing, both for modeling logical forms (Dong and Lapata, 2016; Jia and Liang, 2016; Liang et al., 2017) and for end-to-end execution (Yin et al.", "startOffset": 120, "endOffset": 184}, {"referenceID": 14, "context": "There has been recent interest in using recurrent neural networks for semantic parsing, both for modeling logical forms (Dong and Lapata, 2016; Jia and Liang, 2016; Liang et al., 2017) and for end-to-end execution (Yin et al.", "startOffset": 120, "endOffset": 184}, {"referenceID": 20, "context": "There has been recent interest in using recurrent neural networks for semantic parsing, both for modeling logical forms (Dong and Lapata, 2016; Jia and Liang, 2016; Liang et al., 2017) and for end-to-end execution (Yin et al.", "startOffset": 120, "endOffset": 184}, {"referenceID": 41, "context": ", 2017) and for end-to-end execution (Yin et al., 2015; Neelakantan et al., 2016).", "startOffset": 37, "endOffset": 81}, {"referenceID": 25, "context": ", 2017) and for end-to-end execution (Yin et al., 2015; Neelakantan et al., 2016).", "startOffset": 37, "endOffset": 81}, {"referenceID": 5, "context": ", 2016; Bengio et al., 2015; Levine, 2014). These methods generally seek to make RL training easier by pre-training or \u201cwarm-starting\u201d with fully supervised learning. This requires each training example to be labeled with a reasonably correct output sequence. In our setting, this would amount to labeling each example with the correct program, which is not known. Hence, these methods cannot be directly applied. Without access to correct output sequences, we cannot directly maximize likelihood, and instead resort to maximizing the marginal likelihood (MML). Rather than proposing MML as a form of pre-training, we argue that MML is a superior substitute for the standard RL objective, and that the \u03b2-meritocratic update is even better. Simulated annealing. Our \u03b2-meritocratic update employs exponential smoothing, which bears resemblance to the simulated annealing strategy of Och (2003); Smith and Eisner (2006); Shen et al.", "startOffset": 8, "endOffset": 892}, {"referenceID": 5, "context": ", 2016; Bengio et al., 2015; Levine, 2014). These methods generally seek to make RL training easier by pre-training or \u201cwarm-starting\u201d with fully supervised learning. This requires each training example to be labeled with a reasonably correct output sequence. In our setting, this would amount to labeling each example with the correct program, which is not known. Hence, these methods cannot be directly applied. Without access to correct output sequences, we cannot directly maximize likelihood, and instead resort to maximizing the marginal likelihood (MML). Rather than proposing MML as a form of pre-training, we argue that MML is a superior substitute for the standard RL objective, and that the \u03b2-meritocratic update is even better. Simulated annealing. Our \u03b2-meritocratic update employs exponential smoothing, which bears resemblance to the simulated annealing strategy of Och (2003); Smith and Eisner (2006); Shen et al.", "startOffset": 8, "endOffset": 917}, {"referenceID": 5, "context": ", 2016; Bengio et al., 2015; Levine, 2014). These methods generally seek to make RL training easier by pre-training or \u201cwarm-starting\u201d with fully supervised learning. This requires each training example to be labeled with a reasonably correct output sequence. In our setting, this would amount to labeling each example with the correct program, which is not known. Hence, these methods cannot be directly applied. Without access to correct output sequences, we cannot directly maximize likelihood, and instead resort to maximizing the marginal likelihood (MML). Rather than proposing MML as a form of pre-training, we argue that MML is a superior substitute for the standard RL objective, and that the \u03b2-meritocratic update is even better. Simulated annealing. Our \u03b2-meritocratic update employs exponential smoothing, which bears resemblance to the simulated annealing strategy of Och (2003); Smith and Eisner (2006); Shen et al. (2015). However, a key difference is that these methods smooth the objective function whereas we smooth an expectation in the gradient.", "startOffset": 8, "endOffset": 937}, {"referenceID": 5, "context": ", 2016; Bengio et al., 2015; Levine, 2014). These methods generally seek to make RL training easier by pre-training or \u201cwarm-starting\u201d with fully supervised learning. This requires each training example to be labeled with a reasonably correct output sequence. In our setting, this would amount to labeling each example with the correct program, which is not known. Hence, these methods cannot be directly applied. Without access to correct output sequences, we cannot directly maximize likelihood, and instead resort to maximizing the marginal likelihood (MML). Rather than proposing MML as a form of pre-training, we argue that MML is a superior substitute for the standard RL objective, and that the \u03b2-meritocratic update is even better. Simulated annealing. Our \u03b2-meritocratic update employs exponential smoothing, which bears resemblance to the simulated annealing strategy of Och (2003); Smith and Eisner (2006); Shen et al. (2015). However, a key difference is that these methods smooth the objective function whereas we smooth an expectation in the gradient. To underscore the difference, we note that fixing \u03b2 = 0 in our method (total smoothing) is quite effective, whereas total smoothing in the simulated annealing methods would correspond to a completely flat objective function, and an uninformative gradient of zero everywhere. Neural semantic parsing. There has been recent interest in using recurrent neural networks for semantic parsing, both for modeling logical forms (Dong and Lapata, 2016; Jia and Liang, 2016; Liang et al., 2017) and for end-to-end execution (Yin et al., 2015; Neelakantan et al., 2016). We develop a neural model for the context-dependent setting, which is made possible by a new stackbased language similar to Riedel et al. (2016).", "startOffset": 8, "endOffset": 1771}], "year": 2017, "abstractText": "Our goal is to learn a semantic parser that maps natural language utterances into executable programs when only indirect supervision is available: examples are labeled with the correct execution result, but not the program itself. Consequently, we must search the space of programs for those that output the correct result, while not being misled by spurious programs: incorrect programs that coincidentally output the correct result. We connect two common learning paradigms, reinforcement learning (RL) and maximum marginal likelihood (MML), and then present a new learning algorithm that combines the strengths of both. The new algorithm guards against spurious programs by combining the systematic search traditionally employed in MML with the randomized exploration of RL, and by updating parameters such that probability is spread more evenly across consistent programs. We apply our learning algorithm to a new neural semantic parser and show significant gains over existing state-of-theart results on a recent context-dependent semantic parsing task.", "creator": "LaTeX with hyperref package"}}}