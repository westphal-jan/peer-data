{"id": "1704.08012", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "Topically Driven Neural Language Model", "abstract": "Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics.", "histories": [["v1", "Wed, 26 Apr 2017 08:33:14 GMT  (237kb,D)", "http://arxiv.org/abs/1704.08012v1", "11 pages, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) (to appear)"], ["v2", "Tue, 2 May 2017 07:05:44 GMT  (237kb,D)", "http://arxiv.org/abs/1704.08012v2", "11 pages, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) (to appear)"]], "COMMENTS": "11 pages, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) (to appear)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jey han lau", "timothy baldwin", "trevor cohn"], "accepted": true, "id": "1704.08012"}, "pdf": {"name": "1704.08012.pdf", "metadata": {"source": "CRF", "title": "Topically Driven Neural Language Model", "authors": ["Jey Han Lau", "Timothy Baldwin", "Trevor Cohn"], "emails": ["jeyhan.lau@gmail.com,", "tb@ldwin.net,", "t.cohn@unimelb.edu.au"], "sections": [{"heading": "1 Introduction", "text": "Thematic models provide a powerful tool for extracting the macroeconomic content structure of a document collection in the form of latent themes (usually multinomial distributions across terms), with a wealth of applications in NLP (Hall et al., 2008; Newman et al., 2010a; Wang and McCallum, 2006). A variety of variants of the classic LDA method (Lead et al., 2003) have been proposed, including recent work on neural theme models (Cao et al., 2015; Wan et al., 2012; Larochelle and Lauly, 2012; Hinton and Salakhutdinov, 2009). Separately, language models have long been a fundamental component of any NLP task involving the generation or textual normalization of a loud input (including language, OCR, and processing of social media text)."}, {"heading": "2 Related Work", "text": "Griffiths et al. (2004) propose a model that learns topics and word dependencies from a Bayesian framework. Word generation is driven either by LDA or an HMM. For LDA, a word is generated based on a sampled topic in the document. For thear Xiv: 170 4.08 012v 1 [cs.C L] 26 Apr 201 7HMM, a word is bound to previous words. A major difference from our model is that their language model is driven by an HMM that uses a fixed window and is therefore unable to track far-reaching dependencies. Cao et al. (2015) refer to the topic model view of documents and words - documents with a multinomic distribution over topics and topics that exhibit a multinomic distribution over words - from a network perspective, embedding these relationships in differentiable functions. Thus, the model lost the stochasticity and Bayesian consequence of LDA."}, {"heading": "3 Architecture", "text": "The architecture of the proposed topically controlled language model (henceforth \"tdlm\") is illustrated in Figure 1. There are two components in tdlm: a language model and a theme model. The language model is designed to capture word relationships in sentences, while the theme model learns topical information in documents. The theme model works like an auto-encoder, where it receives the document words as input and optimizes them to predict them. The theme model takes word embedding in a document and creates a document vector using a Convolutionary Network. Given the document vector, we associate it with the themes via an attention scheme to calculate a weighted mean of theme vectors, which is then used to predict a word in the documentation. The language model is a standard LSTM language model (Hochreiter and Schmidhuber, 1997; Mikolov et al., 2010), but it integrates the weighted theme vector generated by the theme model to predict the following words."}, {"heading": "3.1 Topic Model Component", "text": "To get the most attention for the i-th word in the document, we must present a document of n words as a concatenation of its word vectors: x1: n = x1 x2... xnwhere we apply the theme of concatenation to process the word vectors, but for clarity we will explain the network with a filter.Let wv, deer is a revolutionary filter that we apply to a window of hwords to generate a characteristic. A characteristic for a window of words xi: i + h \u2212 1 is given as follows: ci = I (w) vxi: i + h \u2212 1 + bv) where bv is a revolutionary filter, and I am the identity.1 A characteristic card c is a collection of characteristics derived from all windows of words xi: c = [c1, cn \u2212 h]"}, {"heading": "3.2 Language Model Component", "text": "The language model is implemented with the help of LSTM units (Hochreiter and Schmidhuber, 1997): it = \u03c3 (Wivt + Uiht \u2212 1 + bi) ft = \u03c3 (Wfvt + Ufht \u2212 1 + bf) ot = \u03c3 (Wovt + Uoht \u2212 1 + bi) c-t = tanh (Wcvt + Ucht \u2212 1 + bc) ct = ft ct \u2212 1 + it c-t = ot-tanh (ct), where elementary product is designated; it, ft, ot are the input, forgetfulness and output activation steps t in due course; and vt, ht and ct are the input words embedding, LSTM hidden state, or cell state. In the following, W, U, and b are used to refer to the model parameters. Traditionally, a language model works at the sentence level, with the next word, ht, and ct, the prediction of the next word embedded in the sentence."}, {"heading": "3.3 Training and Regularisation", "text": "tdlm is trained using minibatches and SGD.3. For the language model, a minibatch consists of a set of sentences, while the theme model is a set of documents (each predicting a sequence of m1 words).We treat the language and theme models as subtasks in a multi-task learning environment and train them together using categorical cross-entropy losses. Most parameters in the theme model are shared by the language model, such as their scopes (dashed lines) in Figure 1.Hyperparameters of tdlm in Table 1. Text embedding for the theme model and language model components are not shared, although their dimensions are the same (e).4 For m1, m2 and m3, sequences / documents shorter than these thresholds are padded. Sentries longer than m2 are split into multiple sequences, and documents longer than m3 are truncated."}, {"heading": "4 Language Model Evaluation", "text": "With respect to Dataset, we use doc-3We as an optimizer (Kingma and Ba, 2014).4Word embedding is updated during the training (2011).BNC is the written part of the British National Corpus (BNC Consortium, 2007), which contains excerpts from journals, books, essays, news, and other text types. (BNC is the written part of the British National Corpus (BNC Consortium, 2007), which contains excerpts from journals, books, letters, memoranda, and other types of text. (For APNEWS and BNC, we happen to be a set of documents for our experiments. For preprocessing, we tokenise words and phrases with Stanford CoreNLP (Klein and Manning, 2003)."}, {"heading": "5 Topic Model Evaluation", "text": "In fact, most of them are able to establish themselves in the US as if they were able to establish themselves, \"he said in an interview with the\" New York Times \":\" It is very important that they are able to establish themselves. \""}, {"heading": "6 Extensions", "text": "One of tdlm's strengths is its flexibility, since it takes the form of a neural network. To demonstrate this flexibility, we will examine two simple extensions of tdlm, in which we: (1) create a monitored model using document names (Section 6.1) and (2) include additional document metadata (Section 6.2)."}, {"heading": "6.1 Supervised Model", "text": "In the datasets in which the document labels are known, the supervised subject model extensions are designed to use the additional information to improve the modeling quality. The supervised setting also has an additional advantage in this model evaluation, as the models can be evaluated quantitatively via the classification accuracy. To include the supervised document labels, we treat document classification as another sub-task in tdlm. Faced with a document and its label, we feed the document through the theme model network to generate the document thematic representation s, and connect it with another dense layer to generate the probability distribution in the classrooms. During the training, we have additional minibatches for the documents. We start with the document classification that completes the topic and language models. We use 20NEWS in this experiment, which is a popular dataset for text classification."}, {"heading": "6.2 Incorporating Document Metadata", "text": "In APNEWS, each news article contains additional document metadata, including subject classification labels, such as \"General News,\" \"Accidents and Disasters,\" and \"Military and Defense.\" We present an extension to integrate document metadata into tdlm to demonstrate its flexibility in integrating this additional information. As some of the documents in our original APNEWS sample were missing tags, we again sampled a number of APNEWS articles of the same size as our original, which all have tags. In total, about 1500 unique tags can be found among the training articles. To integrate these tags, we present each of them as a learnable vector and associate them with the document vector before calculating the attention distribution. Let zi-Rf denotes the f dimension vector for the i-th day. For the j-th document that we consider as a sample, we present all tags as learnable vectors that precede the calculation of attention distribution."}, {"heading": "7 Discussion", "text": "Topics generated by topic models are typically interpreted based on their most likely top N-words. In tdlm, we can also generate topic-related sentences, which provides another way to understand the topics. To do this, we can limit the topic vector for the language model to being the topic output vector of a specific topic (Equation (3)). We present 4 topics from an APNEWS model (k = 100; LSTM size = \"large\") and 3 randomly generated sentences, each of which is trained to 16 Because the vanilla tdlm is trained on the new APNEWS dataset, the numbers differ slightly from those in Tables 3 and 4.17The 5-dimensional vectors are compressed using PCA.topic in Table 8.18. The generated sentences emphasize the content of the topics and provide another interpretable aspect for the topics. These results also confirm that the language model is driven by topics."}, {"heading": "8 Conclusion", "text": "We propose tdlm, a topically controlled neural language model. tdlm consists of two components: a language model and a theme model, which are jointly trained using a neural network. tdlm demonstrates that it surpasses a modern language model that incorporates a wider context, and that its themes are potentially more coherent than LDA themes. We also propose simple extensions of tdlm to include information such as document labels and metadata, and achieve encouraging results."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their insightful comments and valuable suggestions. This work was partly funded by the Australian Research Council."}], "references": [{"title": "Evaluating topic coherence using distributional semantics", "author": ["Nikos Aletras", "Mark Stevenson."], "venue": "Proceedings of the Tenth International Workshop on Computational Semantics (IWCS-10). Potsdam, Germany, pages 13\u201322.", "citeRegEx": "Aletras and Stevenson.,? 2013", "shortCiteRegEx": "Aletras and Stevenson.", "year": 2013}, {"title": "Latent Dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan."], "venue": "Journal of Machine Learning Research 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "The British National Corpus, version 3 (BNC XML Edition)", "author": ["BNC Consortium."], "venue": "Distributed by Oxford University Computing Services on behalf of the BNC Consortium. http://www.natcorp.ox.ac.uk/.", "citeRegEx": "Consortium.,? 2007", "shortCiteRegEx": "Consortium.", "year": 2007}, {"title": "A novel neural topic model and its supervised extension", "author": ["Ziqiang Cao", "Sujian Li", "Yang Liu", "Wenjie Li", "Heng Ji."], "venue": "Proceedings of the 29th Annual Conference on Artificial Intelligence (AAAI15). Austin, Texas, pages 2210\u20132216.", "citeRegEx": "Cao et al\\.,? 2015", "shortCiteRegEx": "Cao et al\\.", "year": 2015}, {"title": "Reading tea leaves: How humans interpret topic models", "author": ["Jonathan Chang", "Sean Gerrish", "Chong Wang", "Jordan L. Boyd-Graber", "David M. Blei."], "venue": "Advances in Neural Information Processing Systems 21 (NIPS-09). Vancouver, Canada, pages 288\u2013296.", "citeRegEx": "Chang et al\\.,? 2009", "shortCiteRegEx": "Chang et al\\.", "year": 2009}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statisti-", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "NIPS Deep Learning and Representation Learning Workshop. Montreal, Canada, pages 103\u2013", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Recurrent nets that time and count", "author": ["Felix A. Gers", "J\u00fcrgen Schmidhuber."], "venue": "Proceedings of the International Joint Conference on Neural Networks (IJCNN\u20192000). Como, Italy, pages 198\u2013194.", "citeRegEx": "Gers and Schmidhuber.,? 2000", "shortCiteRegEx": "Gers and Schmidhuber.", "year": 2000}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka."], "venue": "CoRR abs/1410.5401.", "citeRegEx": "Graves et al\\.,? 2014", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Finding scientific topics", "author": ["Thomas L. Griffiths", "Mark Steyvers."], "venue": "Proceedings of the National Academy of Sciences 101:5228\u20135235.", "citeRegEx": "Griffiths and Steyvers.,? 2004", "shortCiteRegEx": "Griffiths and Steyvers.", "year": 2004}, {"title": "Integrating topics and syntax", "author": ["Thomas L. Griffiths", "Mark Steyvers", "David M. Blei", "Joshua B. Tenenbaum."], "venue": "Advances in Neural Information Processing Systems 17 (NIPS-05). Vancouver, Canada, pages 537\u2013544.", "citeRegEx": "Griffiths et al\\.,? 2004", "shortCiteRegEx": "Griffiths et al\\.", "year": 2004}, {"title": "Studying the history of ideas using topic models", "author": ["David Hall", "Daniel Jurafsky", "Christopher D. Manning."], "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP 2008). Honolulu, USA, pages 363\u2013371.", "citeRegEx": "Hall et al\\.,? 2008", "shortCiteRegEx": "Hall et al\\.", "year": 2008}, {"title": "Replicated softmax: an undirected topic model", "author": ["Geoffrey E. Hinton", "Ruslan R. Salakhutdinov."], "venue": "Advances in Neural Information Processing Systems 21 (NIPS-09). Vancouver, Canada, pages 1607\u2013 1614.", "citeRegEx": "Hinton and Salakhutdinov.,? 2009", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2009}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9:1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Document context language models", "author": ["Yangfeng Ji", "Trevor Cohn", "Lingpeng Kong", "Chris Dyer", "Jacob Eisenstein."], "venue": "Proceedings of ICLR-16 Workshop, 2016. Toulon, France.", "citeRegEx": "Ji et al\\.,? 2016", "shortCiteRegEx": "Ji et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D. Manning."], "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL 2003). Sapporo, Japan, pages 423\u2013430.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "A neural autoregressive topic model", "author": ["Hugo Larochelle", "Stanislas Lauly."], "venue": "Advances in Neural Information Processing Systems 25. pages 2708\u2013 2716.", "citeRegEx": "Larochelle and Lauly.,? 2012", "shortCiteRegEx": "Larochelle and Lauly.", "year": 2012}, {"title": "The sensitivity of topic coherence evaluation to topic cardinality", "author": ["Jey Han Lau", "Timothy Baldwin."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics \u2014 Human Language Technologies", "citeRegEx": "Lau and Baldwin.,? 2016", "shortCiteRegEx": "Lau and Baldwin.", "year": 2016}, {"title": "Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality", "author": ["Jey Han Lau", "David Newman", "Timothy Baldwin."], "venue": "Proceedings of the 14th Conference of the EACL (EACL 2014). Gothenburg, Sweden, pages 530\u2013539.", "citeRegEx": "Lau et al\\.,? 2014", "shortCiteRegEx": "Lau et al\\.", "year": 2014}, {"title": "Learning word vectors for sentiment analysis", "author": ["Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human", "citeRegEx": "Maas et al\\.,? 2011", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Supervised topic models", "author": ["Jon D. McAuliffe", "David M. Blei."], "venue": "Advances in Neural Information Processing Systems 20 (NIPS-08). Vancouver, Canada, pages 121\u2013128.", "citeRegEx": "McAuliffe and Blei.,? 2008", "shortCiteRegEx": "McAuliffe and Blei.", "year": 2008}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "Proceedings of the 11th Annual Conference of the International Speech Communication Association (IN-", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Optimizing semantic coherence in topic models", "author": ["David Mimno", "Hanna Wallach", "Edmund Talley", "Miriam Leenders", "Andrew McCallum."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Mimno et al\\.,? 2011", "shortCiteRegEx": "Mimno et al\\.", "year": 2011}, {"title": "Visualizing document collections and search results using topic mapping", "author": ["David Newman", "Timothy Baldwin", "Lawrence Cavedon", "Sarvnaz Karimi", "David Martinez", "Justin Zobel."], "venue": "Journal of Web Semantics 8(2\u20133):169\u2013175.", "citeRegEx": "Newman et al\\.,? 2010a", "shortCiteRegEx": "Newman et al\\.", "year": 2010}, {"title": "Automatic evaluation of topic coherence", "author": ["David Newman", "Jey Han Lau", "Karl Grieser", "Timothy Baldwin."], "venue": "Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association", "citeRegEx": "Newman et al\\.,? 2010b", "shortCiteRegEx": "Newman et al\\.", "year": 2010}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Vu Pham", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour."], "venue": "CoRR abs/1312.4569.", "citeRegEx": "Pham et al\\.,? 2013", "shortCiteRegEx": "Pham et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15:1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus."], "venue": "Advances in Neural Information Processing Systems 28 (NIPS-15). Montreal, Canada, pages 2440\u20132448.", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Recurrent memory networks for language modeling", "author": ["Ke Tran", "Arianna Bisazza", "Christof Monz."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics \u2014 Human Language Technologies", "citeRegEx": "Tran et al\\.,? 2016", "shortCiteRegEx": "Tran et al\\.", "year": 2016}, {"title": "Evaluation methods for topic models", "author": ["Hanna M. Wallach", "Iain Murray", "Ruslan Salakhutdinov", "David Mimno."], "venue": "Proceedings of the 26th International Conference on Machine Learning (ICML-09). Montreal, Canada, pages 1105\u20131112.", "citeRegEx": "Wallach et al\\.,? 2009", "shortCiteRegEx": "Wallach et al\\.", "year": 2009}, {"title": "A hybrid neural network-latent topic model", "author": ["Li Wan", "Leo Zhu", "Rob Fergus."], "venue": "Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS-12). La Palma, Canary Islands, pages 1287\u20131294.", "citeRegEx": "Wan et al\\.,? 2012", "shortCiteRegEx": "Wan et al\\.", "year": 2012}, {"title": "Largercontext language modelling with recurrent neural network", "author": ["Tian Wang", "Kyunghyun Cho."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Berlin, Germany, pages 1319\u20131329.", "citeRegEx": "Wang and Cho.,? 2016", "shortCiteRegEx": "Wang and Cho.", "year": 2016}, {"title": "Topics over time: a non-Markov continuous-time model of topical trends", "author": ["Xuerui Wang", "Andrew McCallum."], "venue": "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Philadelphia, USA,", "citeRegEx": "Wang and McCallum.,? 2006", "shortCiteRegEx": "Wang and McCallum.", "year": 2006}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."], "venue": "CoRR abs/1410.3916.", "citeRegEx": "Weston et al\\.,? 2014", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "Depth-gated LSTM", "author": ["Kaisheng Yao", "Trevor Cohn", "Katerina Vylomova", "Kevin Duh", "Chris Dyer."], "venue": "CoRR abs/1508.03790.", "citeRegEx": "Yao et al\\.,? 2015", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "CoRR abs/1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "Topic models provide a powerful tool for extracting the macro-level content structure of a document collection in the form of the latent topics (usually in the form of multinomial distributions over terms), with a plethora of applications in NLP (Hall et al., 2008; Newman et al., 2010a; Wang and McCallum, 2006).", "startOffset": 246, "endOffset": 312}, {"referenceID": 25, "context": "Topic models provide a powerful tool for extracting the macro-level content structure of a document collection in the form of the latent topics (usually in the form of multinomial distributions over terms), with a plethora of applications in NLP (Hall et al., 2008; Newman et al., 2010a; Wang and McCallum, 2006).", "startOffset": 246, "endOffset": 312}, {"referenceID": 34, "context": "Topic models provide a powerful tool for extracting the macro-level content structure of a document collection in the form of the latent topics (usually in the form of multinomial distributions over terms), with a plethora of applications in NLP (Hall et al., 2008; Newman et al., 2010a; Wang and McCallum, 2006).", "startOffset": 246, "endOffset": 312}, {"referenceID": 1, "context": "A myriad of variants of the classical LDA method (Blei et al., 2003) have been proposed, including recent work on neural topic models (Cao et al.", "startOffset": 49, "endOffset": 68}, {"referenceID": 3, "context": ", 2003) have been proposed, including recent work on neural topic models (Cao et al., 2015; Wan et al., 2012; Larochelle and Lauly, 2012; Hinton and Salakhutdinov, 2009).", "startOffset": 73, "endOffset": 169}, {"referenceID": 32, "context": ", 2003) have been proposed, including recent work on neural topic models (Cao et al., 2015; Wan et al., 2012; Larochelle and Lauly, 2012; Hinton and Salakhutdinov, 2009).", "startOffset": 73, "endOffset": 169}, {"referenceID": 18, "context": ", 2003) have been proposed, including recent work on neural topic models (Cao et al., 2015; Wan et al., 2012; Larochelle and Lauly, 2012; Hinton and Salakhutdinov, 2009).", "startOffset": 73, "endOffset": 169}, {"referenceID": 13, "context": ", 2003) have been proposed, including recent work on neural topic models (Cao et al., 2015; Wan et al., 2012; Larochelle and Lauly, 2012; Hinton and Salakhutdinov, 2009).", "startOffset": 73, "endOffset": 169}, {"referenceID": 33, "context": "The primary purpose of a language model is to predict the probability of a span of text, traditionally at the sentence level, under the assumption that sentences are independent of one another, although recent work has started using broader local context such as the preceding sentences (Wang and Cho, 2016; Ji et al., 2016).", "startOffset": 287, "endOffset": 324}, {"referenceID": 15, "context": "The primary purpose of a language model is to predict the probability of a span of text, traditionally at the sentence level, under the assumption that sentences are independent of one another, although recent work has started using broader local context such as the preceding sentences (Wang and Cho, 2016; Ji et al., 2016).", "startOffset": 287, "endOffset": 324}, {"referenceID": 15, "context": "Wang and Cho (2016) and Ji et al. (2016) relax the sentence independence assumption in language modelling, and use preceeding sentences as additional context.", "startOffset": 24, "endOffset": 41}, {"referenceID": 15, "context": "Wang and Cho (2016) and Ji et al. (2016) relax the sentence independence assumption in language modelling, and use preceeding sentences as additional context. By treating words in preceeding sentences as a bag of words, Wang and Cho (2016) use an attentional mechanism to focus on these words when predicting the next word.", "startOffset": 24, "endOffset": 240}, {"referenceID": 14, "context": "The language model is a standard LSTM language model (Hochreiter and Schmidhuber, 1997; Mikolov et al., 2010), but it incorporates the weighted topic vector generated by the topic model to predict succeeding words.", "startOffset": 53, "endOffset": 109}, {"referenceID": 23, "context": "The language model is a standard LSTM language model (Hochreiter and Schmidhuber, 1997; Mikolov et al., 2010), but it incorporates the weighted topic vector generated by the topic model to predict succeeding words.", "startOffset": 53, "endOffset": 109}, {"referenceID": 7, "context": "To capture the most salient features in c, we apply a max-over-time pooling operation (Collobert et al., 2011), yielding a scalar:", "startOffset": 86, "endOffset": 110}, {"referenceID": 14, "context": "The language model is implemented using LSTM units (Hochreiter and Schmidhuber, 1997):", "startOffset": 51, "endOffset": 85}, {"referenceID": 9, "context": "The attention mechanism was inspired by memory networks (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Tran et al., 2016).", "startOffset": 56, "endOffset": 142}, {"referenceID": 35, "context": "The attention mechanism was inspired by memory networks (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Tran et al., 2016).", "startOffset": 56, "endOffset": 142}, {"referenceID": 29, "context": "The attention mechanism was inspired by memory networks (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Tran et al., 2016).", "startOffset": 56, "endOffset": 142}, {"referenceID": 30, "context": "The attention mechanism was inspired by memory networks (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Tran et al., 2016).", "startOffset": 56, "endOffset": 142}, {"referenceID": 5, "context": "We use a gating unit similar to a GRU (Cho et al., 2014; Chung et al., 2014) to allow tdlm to learn the degree of influence of topical information on the language model:", "startOffset": 38, "endOffset": 76}, {"referenceID": 6, "context": "We use a gating unit similar to a GRU (Cho et al., 2014; Chung et al., 2014) to allow tdlm to learn the degree of influence of topical information on the language model:", "startOffset": 38, "endOffset": 76}, {"referenceID": 28, "context": "To regularise tdlm, we use dropout regularisation (Srivastava et al., 2014).", "startOffset": 50, "endOffset": 75}, {"referenceID": 27, "context": "We apply dropout to d and s in the topic model, and to the input word embedding and hidden output of the LSTM in the language model (Pham et al., 2013; Zaremba et al., 2014).", "startOffset": 132, "endOffset": 173}, {"referenceID": 37, "context": "We apply dropout to d and s in the topic model, and to the input word embedding and hidden output of the LSTM in the language model (Pham et al., 2013; Zaremba et al., 2014).", "startOffset": 132, "endOffset": 173}, {"referenceID": 16, "context": "We use Adam as the optimiser (Kingma and Ba, 2014).", "startOffset": 29, "endOffset": 50}, {"referenceID": 15, "context": "We use Adam as the optimiser (Kingma and Ba, 2014). Word embeddings are updated during training. ument collections from 3 sources: APNEWS, IMDB and BNC. APNEWS is a collection of Associated Press5 news articles from 2009 to 2016. IMDB is a set of movie reviews collected by Maas et al. (2011). BNC is the written portion of the British National Corpus (BNC Consortium, 2007), which contains excerpts from journals, books, letters, essays, memoranda, news and other types of text.", "startOffset": 30, "endOffset": 293}, {"referenceID": 17, "context": "For preprocessing, we tokenise words and sentences using Stanford CoreNLP (Klein and Manning, 2003).", "startOffset": 74, "endOffset": 99}, {"referenceID": 33, "context": "lclm: A larger context language model that incorporates context from preceding sentences (Wang and Cho, 2016), by treating the preceding sentence as a bag of words, and using an", "startOffset": 89, "endOffset": 109}, {"referenceID": 8, "context": "Multi-layer LSTMs are vanilla stacked LSTMs without skip connections (Gers and Schmidhuber, 2000) or depthgating (Yao et al.", "startOffset": 69, "endOffset": 97}, {"referenceID": 36, "context": "Multi-layer LSTMs are vanilla stacked LSTMs without skip connections (Gers and Schmidhuber, 2000) or depthgating (Yao et al., 2015).", "startOffset": 113, "endOffset": 131}, {"referenceID": 1, "context": "We first train an LDA model (Blei et al., 2003; Griffiths and Steyvers, 2004) to learn 50/100/150 topics for APNEWS, IMDB and BNC.", "startOffset": 28, "endOffset": 77}, {"referenceID": 10, "context": "We first train an LDA model (Blei et al., 2003; Griffiths and Steyvers, 2004) to learn 50/100/150 topics for APNEWS, IMDB and BNC.", "startOffset": 28, "endOffset": 77}, {"referenceID": 31, "context": "There are various ways to estimate test perplexity (Wallach et al., 2009), but Chang et al.", "startOffset": 51, "endOffset": 73}, {"referenceID": 3, "context": ", 2009), but Chang et al. (2009) show that perplexity does not correlate with the coherence of the generated topics.", "startOffset": 13, "endOffset": 33}, {"referenceID": 3, "context": ", 2009), but Chang et al. (2009) show that perplexity does not correlate with the coherence of the generated topics. Newman et al. (2010b); Mimno et al.", "startOffset": 13, "endOffset": 139}, {"referenceID": 3, "context": ", 2009), but Chang et al. (2009) show that perplexity does not correlate with the coherence of the generated topics. Newman et al. (2010b); Mimno et al. (2011); Aletras and Stevenson (2013) propose automatic approaches to computing topic coherence, and Lau et al.", "startOffset": 13, "endOffset": 160}, {"referenceID": 0, "context": "(2011); Aletras and Stevenson (2013) propose automatic approaches to computing topic coherence, and Lau et al.", "startOffset": 8, "endOffset": 37}, {"referenceID": 0, "context": "(2011); Aletras and Stevenson (2013) propose automatic approaches to computing topic coherence, and Lau et al. (2014) summarises these methods to understand their differences.", "startOffset": 8, "endOffset": 118}, {"referenceID": 26, "context": "Given the top-n words of a topic, coherence is computed based on the sum of pairwise NPMI scores between topic words, where the word probabilities used in the NPMI calculation are based on co-occurrence statistics mined from English Wikipedia with a sliding window (Newman et al., 2010b; Lau et al., 2014).", "startOffset": 265, "endOffset": 305}, {"referenceID": 20, "context": "Given the top-n words of a topic, coherence is computed based on the sum of pairwise NPMI scores between topic words, where the word probabilities used in the NPMI calculation are based on co-occurrence statistics mined from English Wikipedia with a sliding window (Newman et al., 2010b; Lau et al., 2014).", "startOffset": 265, "endOffset": 305}, {"referenceID": 20, "context": "Following Lau et al. (2014), we compute topic coherence using normalised PMI (\u201cNPMI\u201d) scores.", "startOffset": 10, "endOffset": 28}, {"referenceID": 19, "context": "Based on the findings of Lau and Baldwin (2016), we average topic coherence over the top5/10/15/20 topic words.", "startOffset": 25, "endOffset": 48}, {"referenceID": 3, "context": "ntm: ntm is a neural topic model proposed by Cao et al. (2015). The document-topic and topicword multinomials are expressed from a neural network perspective using differentiable functions.", "startOffset": 45, "endOffset": 63}, {"referenceID": 17, "context": "For preprocessing we tokenise words and sentence using Stanford CoreNLP (Klein and Manning, 2003), and lowercase all words.", "startOffset": 72, "endOffset": 97}, {"referenceID": 3, "context": "Both ntm and lda have natural supervised extensions (Cao et al., 2015; McAuliffe and Blei, 2008) for incorporating document labels.", "startOffset": 52, "endOffset": 96}, {"referenceID": 22, "context": "Both ntm and lda have natural supervised extensions (Cao et al., 2015; McAuliffe and Blei, 2008) for incorporating document labels.", "startOffset": 52, "endOffset": 96}], "year": 2017, "abstractText": "Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics.", "creator": "LaTeX with hyperref package"}}}