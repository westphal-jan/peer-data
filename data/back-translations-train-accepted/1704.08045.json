{"id": "1704.08045", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "The Loss Surface of Deep and Wide Neural Networks", "abstract": "While the optimization problem behind deep neural networks is highly non-convex, it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points. It has been argued that this is the case as all local minima are close to being globally optimal. We show that this is (almost) true, in fact almost all local minima are globally optimal, for a fully connected network with squared loss and analytic activation function given that the number of hidden units of one layer of the network is larger than the number of training points and the network structure from this layer on is pyramidal.", "histories": [["v1", "Wed, 26 Apr 2017 10:24:54 GMT  (68kb)", "https://arxiv.org/abs/1704.08045v1", "14 pages"], ["v2", "Mon, 12 Jun 2017 19:43:39 GMT  (82kb)", "http://arxiv.org/abs/1704.08045v2", "ICML 2017. Main results now hold for larger classes of loss functions"]], "COMMENTS": "14 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.NE stat.ML", "authors": ["quynh nguyen", "matthias hein"], "accepted": true, "id": "1704.08045"}, "pdf": {"name": "1704.08045.pdf", "metadata": {"source": "CRF", "title": "The Loss Surface of Deep and Wide Neural Networks", "authors": ["Quynh Nguyen", "Matthias Hein"], "emails": ["<quynh@cs.uni-saarland.de>."], "sections": [{"heading": null, "text": "ar Xiv: 170 4.08 045v 2 [cs.L G] 12. Jun 2017Neural networks are highly non-convex, it is often observed in practice that training deep networks seems possible without getting stuck in sub-optimal points. It has been argued that this is the case because all local minima are nearly globally optimal. We show that this is (almost) true, in fact almost all local minima are globally optimal, for a fully connected network with square loss and analytical activation function, as the number of hidden units of a network layer is greater than the number of training points and the network structure is pyramidal from that layer onwards."}, {"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to leave the country in which it is located, unless it is able to, it is able to reactivate it."}, {"heading": "2. Feedforward Neural Networks and Backpropagation", "text": "s define the number of training samples and the number of training problems by X = [x1]. (xN] T-RN \u00b7 d, Y = [y1,.., yN] T \u00b7 m the input / output matrix for the training data (xi, yi) N = 1, where d is the input dimension and m is the number of classes. We look at fully connected networks with L layers ranging from 0, 1, 2,.,. L corresponding to the input layer, 1st hidden layer, etc. and output layer. The network structure is determined by the weight matrices (Wk) L k = 1, W: = Rd \u00b7 n1,."}, {"heading": "3. Main Result", "text": "We first discuss some preliminary work and then present our main result together with a detailed discussion. For better readability, we postpone the proof of the main result to the next section, which contains several interim results of independent interest."}, {"heading": "3.1. Previous Work", "text": "Our work can be considered a generalization of the work of (Gori & Tesi, 1992; Yu & Chen, 1995) While (Yu & Chen, 1995) has shown that for a single-layer network, if n1 = N \u2212 1, then any local minimum is a global minimum, the work of (Gori & Tesi, 1992) also requires multi-layer networks. For the convenience of the reader, we must first reformulate Theorem 1 of (Gori & Tesi, 1992) using our previously introduced notation. The critical points of a continuously differentiable function f: Rd \u2192 R are the points at which the gradient disappears, i.e. f (x) = 0. \u2212 This is a necessary condition for a local minimum. Theorem 3.1 (Gori & Tesi, 1992) Let it be: P \u2192 R is defined as in (1) with minimal square loss l (a) = 2."}, {"heading": "3.2. First Main Result and Discussion", "text": "A function f: Rd \u2192 R is real analytical when the corresponding Taylor multivariate series converges to f (x) on an open subset of Rd (Krantz & Parks, 2002). \u2212 All results in this section are verifiable under the following assumptions about loss / activation function and training data. \u2212 All results in this section are analyzed under the following assumptions: \u2212 Target and (a) assumptions are limited or (b) there are positive assumptions 1, 2, 3, 4, s.t. \u2212 Target and setpoints for all i 6 = j, 2. Target is analytical on R, strictly monotonically increasing and (a) there are positive assumptions. \u2212 Targets 1, 2, \u03c12, \u03c14, sc."}, {"heading": "4. Proof of Main Result", "text": "For better readability, then there is still a point at which the terms of Lemma 3.5 are true and the loss is minimal."}, {"heading": "If \u03c0(j) = j thenE(\u03b1)\u03c0(j)j = \u03c3(\u03b2). In cases where \u03c0(j) <", "text": "(j 6 = N) it applies that [zj \u2212 z\u03c0 (j)) T a > 0 (and thus for sufficiently large \u03b1 (n). (b) It applies (n) n (n). (b) It applies (n). (b) It applies (n). (c) It applies (n). (c) It applies (n). (c) It applies (n). (c). (c) It applies (n). (c). (c) It applies (n). (c). (c). (c) It applies (n). (n). (c). (c). (n). (n). (n). (n). (n). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c. (c). (c). (c). (c). (c. (c). (c. (c). (c). (c). (c). (c. (c). (c). (c). (c. (c). (c. (c). (c). (c). (c. (c). c. c. c. (c. c. c. c. c. c. c. c. c. c. c. c. (c. c. c c. c. c. c. c. c c c c. c c. c c. c c c c c c c. (c c c c c. c c c c c c c c c. c c c c c c c. c c c c c c c c c. c c c c c c c. c c c c c c c. c c c c c c c c c c c c. c c c c c c c c c c c c c c c c c c"}, {"heading": "5. Relaxing the Condition on the Number of Hidden Units", "text": "We have seen that nk-1 is a sufficient condition that leads to a fairly simple structure of critical points, in the sense that all local minima that have a complete ranking in levels k + 2 to L are automatically optimal, which indicates that suboptimal local optimal points are either completely absent or relatively rare. The following result will give some intuitions about the case nk < N \u2212 1, but will not be as strong as our main result 3.8, which makes statements about a large class of critical points. The main idea is that with condition nk-1 the data are more linear separable."}, {"heading": "6. Discussion", "text": "Our results show that the loss area behaves well when there is a broad layer in the network. Implicitly, such a broad layer is often present in Convolutionary Neural Networks used in computer vision. Therefore, it is an interesting future research question how and if our results can be generalized to low connectivity neural networks. We think that the results presented in this paper represent an important addition to the current understanding of why deep learning works so efficiently, especially since in this paper we work directly with the neural networks used in practice, without any modifications or simplifications."}, {"heading": "Acknowledgment", "text": "The authors acknowledge the support of the ERC Start-Up Fund NOLEPRO 307793."}], "references": [{"title": "Exponentially many local minima for single neurons", "author": ["P. Auer", "M. Herbster", "M.K. Warmuth"], "venue": "In NIPS,", "citeRegEx": "Auer et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Auer et al\\.", "year": 1996}, {"title": "Do deep nets really need to be deep", "author": ["J. Ba", "R. Caruana"], "venue": "In NIPS,", "citeRegEx": "Ba and Caruana,? \\Q2014\\E", "shortCiteRegEx": "Ba and Caruana", "year": 2014}, {"title": "Neural networks and principle component analysis: Learning from examples without local minima", "author": ["P. Baldi", "K. Hornik"], "venue": "Neural Networks,", "citeRegEx": "Baldi and Hornik,? \\Q1988\\E", "shortCiteRegEx": "Baldi and Hornik", "year": 1988}, {"title": "Bayesian Reasoning and Machine Learning", "author": ["D. Barber"], "venue": null, "citeRegEx": "Barber,? \\Q2012\\E", "shortCiteRegEx": "Barber", "year": 2012}, {"title": "Globally optimal gradient descent for a convnet with gaussian inputs, 2017", "author": ["A. Brutzkus", "A. Globerson"], "venue": null, "citeRegEx": "Brutzkus and Globerson,? \\Q2017\\E", "shortCiteRegEx": "Brutzkus and Globerson", "year": 2017}, {"title": "Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping", "author": ["R. Caruana", "S. Lawrence", "L. Giles"], "venue": "In NIPS,", "citeRegEx": "Caruana et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Caruana et al\\.", "year": 2001}, {"title": "The loss surfaces of multilayer networks", "author": ["A. Choromanska", "M. Hena", "M. Mathieu", "G.B. Arous", "Y. LeCun"], "venue": "In AISTATS,", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Open problem: The landscape of the loss surfaces of multilayer networks. JMLR, 2015b", "author": ["A. Choromanska", "Y. LeCun", "G.B. Arous"], "venue": null, "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Deep, big, simple neural nets for handwritten digit recognition", "author": ["D.C. Ciresan", "U. Meier", "L.M. Gambardella", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Ciresan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2010}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Successes and failures of backpropagation: A theoretical investigation", "author": ["P. Frasconi", "M. Gori", "A. Tesi"], "venue": "Progress in Neural Networks: Architecture,", "citeRegEx": "Frasconi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Frasconi et al\\.", "year": 1997}, {"title": "Globally optimal training of generalized polynomial neural networks with nonlinear spectral methods", "author": ["A. Gautier", "Q. Nguyen", "M. Hein"], "venue": null, "citeRegEx": "Gautier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gautier et al\\.", "year": 2016}, {"title": "Qualitatively characterizing neural network optimization problems", "author": ["I.J. Goodfellow", "O. Vinyals", "A.M. Saxe"], "venue": "In ICLR,", "citeRegEx": "Goodfellow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "On the problem of local minima in backpropagation", "author": ["M. Gori", "A. Tesi"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Gori and Tesi,? \\Q1992\\E", "shortCiteRegEx": "Gori and Tesi", "year": 1992}, {"title": "Global optimality in tensor factorization, deep learning, and beyond", "author": ["B.D. Haeffele", "R. Vidal"], "venue": null, "citeRegEx": "Haeffele and Vidal,? \\Q2015\\E", "shortCiteRegEx": "Haeffele and Vidal", "year": 2015}, {"title": "Multiple view geometry in computer vision", "author": ["R. Hartley", "A. Zisserman"], "venue": null, "citeRegEx": "Hartley and Zisserman,? \\Q2004\\E", "shortCiteRegEx": "Hartley and Zisserman", "year": 2004}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "author": ["M. Janzamin", "H. Sedghi", "A. Anandkumar"], "venue": null, "citeRegEx": "Janzamin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Janzamin et al\\.", "year": 2016}, {"title": "Deep learning without poor local minima", "author": ["K. Kawaguchi"], "venue": "In NIPS,", "citeRegEx": "Kawaguchi,? \\Q2016\\E", "shortCiteRegEx": "Kawaguchi", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "In NIPS,", "citeRegEx": "LeCun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1990}, {"title": "Gradient descent only converges to minimizers", "author": ["J.D. Lee", "M. Simchowitz", "M.I. Jordan", "B. Recht"], "venue": "In COLT,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "How far can we go without convolution", "author": ["Z. Lin", "R. Memisevic", "K. Konda"], "venue": "Improving fully-connected networks. preprint,", "citeRegEx": "Lin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Elementary classical analysis", "author": ["J.E. Marsden"], "venue": "W.H.Freeman and Company,", "citeRegEx": "Marsden,? \\Q1974\\E", "shortCiteRegEx": "Marsden", "year": 1974}, {"title": "Lectures on H-Cobordism Theorem", "author": ["J. Milnor"], "venue": null, "citeRegEx": "Milnor,? \\Q1965\\E", "shortCiteRegEx": "Milnor", "year": 1965}, {"title": "The zero set of a real analytic function", "author": ["B. Mityagin"], "venue": null, "citeRegEx": "Mityagin,? \\Q2015\\E", "shortCiteRegEx": "Mityagin", "year": 2015}, {"title": "Pathsgd: Path-normalized optimization in deep neural networks", "author": ["B. Neyshabur", "R.R. Salakhutdinov", "N. Srebro"], "venue": "In NIPS,", "citeRegEx": "Neyshabur et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neyshabur et al\\.", "year": 2015}, {"title": "Complex powers of analytic functions and meromorphic renormalization in qft", "author": ["V.D. Nguyen"], "venue": null, "citeRegEx": "Nguyen,? \\Q2015\\E", "shortCiteRegEx": "Nguyen", "year": 2015}, {"title": "Theory ii: Landscape of the empirical risk in deep learning, 2017", "author": ["T. Poggio", "Q. Liao"], "venue": null, "citeRegEx": "Poggio and Liao,? \\Q2017\\E", "shortCiteRegEx": "Poggio and Liao", "year": 2017}, {"title": "Piecewise convexity of artificial neural networks, 2017", "author": ["B. Rister", "D.L. Rubin"], "venue": null, "citeRegEx": "Rister and Rubin,? \\Q2017\\E", "shortCiteRegEx": "Rister and Rubin", "year": 2017}, {"title": "On the quality of the initial basin in overspecified networks", "author": ["I. Safran", "O. Shamir"], "venue": "In ICML,", "citeRegEx": "Safran and Shamir,? \\Q2016\\E", "shortCiteRegEx": "Safran and Shamir", "year": 2016}, {"title": "Singularity of the hessian in deep learning, 2016", "author": ["L. Sagun", "L. Bottou", "Y. LeCun"], "venue": null, "citeRegEx": "Sagun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sagun et al\\.", "year": 2016}, {"title": "Training a single sigmoidal neuron is hard", "author": ["J. Sima"], "venue": "Neural Computation,", "citeRegEx": "Sima,? \\Q2002\\E", "shortCiteRegEx": "Sima", "year": 2002}, {"title": "Learning relus via gradient descent, 2017", "author": ["M. Soltanolkotabi"], "venue": null, "citeRegEx": "Soltanolkotabi,? \\Q2017\\E", "shortCiteRegEx": "Soltanolkotabi", "year": 2017}, {"title": "Exponentially vanishing suboptimal local minima in multilayer neural networks, 2017", "author": ["D. Soudry", "E. Hoffer"], "venue": null, "citeRegEx": "Soudry and Hoffer,? \\Q2017\\E", "shortCiteRegEx": "Soudry and Hoffer", "year": 2017}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P. andManzagol"], "venue": "JLMR, 11:3371\u20133408,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "On the local minima free condition of backpropagation learning", "author": ["X. Yu", "G. Chen"], "venue": "IEEE Transaction on Neural Networks,", "citeRegEx": "Yu and Chen,? \\Q1995\\E", "shortCiteRegEx": "Yu and Chen", "year": 1995}, {"title": "Understanding deep learning requires re-thinking generalization", "author": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "Vinyals", "Oriol"], "venue": "In ICLR,", "citeRegEx": "Zhang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}, {"title": "The landscape of deep learning algorithms, 2017", "author": ["P. Zhou", "J. Feng"], "venue": null, "citeRegEx": "Zhou and Feng,? \\Q2017\\E", "shortCiteRegEx": "Zhou and Feng", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "This problem turns out to be very difficult as there can be exponentially many distinct local minima (Auer et al., 1996; Safran & Shamir, 2016).", "startOffset": 101, "endOffset": 143}, {"referenceID": 31, "context": "It has been shown that the training of a network with a single neuron with a variety of activation functions turns out to be NP-hard (Sima, 2002).", "startOffset": 133, "endOffset": 145}, {"referenceID": 9, "context": "Surprisingly, it has been observed (Dauphin et al., 2014; Goodfellow et al., 2015) that in the training of stateof-the-art feedforward neural networks with sparse connectivity like convolutional neural networks (LeCun et al.", "startOffset": 35, "endOffset": 82}, {"referenceID": 12, "context": "Surprisingly, it has been observed (Dauphin et al., 2014; Goodfellow et al., 2015) that in the training of stateof-the-art feedforward neural networks with sparse connectivity like convolutional neural networks (LeCun et al.", "startOffset": 35, "endOffset": 82}, {"referenceID": 19, "context": ", 2015) that in the training of stateof-the-art feedforward neural networks with sparse connectivity like convolutional neural networks (LeCun et al., 1990; Krizhevsky et al., 2012) or fully connected ones Department of Mathematics and Computer Science, Saarland University, Germany.", "startOffset": 136, "endOffset": 181}, {"referenceID": 18, "context": ", 2015) that in the training of stateof-the-art feedforward neural networks with sparse connectivity like convolutional neural networks (LeCun et al., 1990; Krizhevsky et al., 2012) or fully connected ones Department of Mathematics and Computer Science, Saarland University, Germany.", "startOffset": 136, "endOffset": 181}, {"referenceID": 12, "context": "However, as the authors admit themselves in (Goodfellow et al., 2015), the reason for this might be that there is a connection between the fact that these networks have good performance and that they are easy to train.", "startOffset": 44, "endOffset": 69}, {"referenceID": 20, "context": "(Brutzkus & Globerson, 2017; Lee et al., 2016; Poggio & Liao, 2017; Rister & Rubin, 2017; Soudry & Hoffer, 2017; Zhou & Feng, 2017).", "startOffset": 0, "endOffset": 131}, {"referenceID": 16, "context": "However, it turns out that these approaches are either not practical (Janzamin et al., 2016; Haeffele & Vidal, 2015; Soltanolkotabi, 2017) as they require e.", "startOffset": 69, "endOffset": 138}, {"referenceID": 32, "context": "However, it turns out that these approaches are either not practical (Janzamin et al., 2016; Haeffele & Vidal, 2015; Soltanolkotabi, 2017) as they require e.", "startOffset": 69, "endOffset": 138}, {"referenceID": 11, "context": "knowledge about the data generating measure, or they modify the neural network structure and objective (Gautier et al., 2016).", "startOffset": 103, "endOffset": 125}, {"referenceID": 17, "context": "One class of networks which are simpler to analyze are deep linear networks for which it has been shown that every local minimum is a global minimum (Baldi & Hornik, 1988; Kawaguchi, 2016).", "startOffset": 149, "endOffset": 188}, {"referenceID": 17, "context": "It has recently been shown (Kawaguchi, 2016) that if some of these assumptions are dropped one basically recovers the result of the linear case, but the model is still unrealistic.", "startOffset": 27, "endOffset": 44}, {"referenceID": 10, "context": "Moreover, it extends results of (Gori & Tesi, 1992; Frasconi et al., 1997) who have shown that for certain deep feedforward neural networks almost all local minima are globally optimal whenever the training data is linearly independent.", "startOffset": 32, "endOffset": 74}, {"referenceID": 21, "context": "in (Lin et al., 2016) they have 50,000 training samples and the network has one hidden layer with 10,000 hidden units and (Ba & Caruana, 2014) have 1.", "startOffset": 3, "endOffset": 21}, {"referenceID": 8, "context": "We refer to (Ciresan et al., 2010; Neyshabur et al., 2015; Vincent et al., 2010; Caruana et al., 2001) for other examples where the number of hidden units of one layer is on the order of the number of training samples.", "startOffset": 12, "endOffset": 102}, {"referenceID": 25, "context": "We refer to (Ciresan et al., 2010; Neyshabur et al., 2015; Vincent et al., 2010; Caruana et al., 2001) for other examples where the number of hidden units of one layer is on the order of the number of training samples.", "startOffset": 12, "endOffset": 102}, {"referenceID": 34, "context": "We refer to (Ciresan et al., 2010; Neyshabur et al., 2015; Vincent et al., 2010; Caruana et al., 2001) for other examples where the number of hidden units of one layer is on the order of the number of training samples.", "startOffset": 12, "endOffset": 102}, {"referenceID": 5, "context": "We refer to (Ciresan et al., 2010; Neyshabur et al., 2015; Vincent et al., 2010; Caruana et al., 2001) for other examples where the number of hidden units of one layer is on the order of the number of training samples.", "startOffset": 12, "endOffset": 102}, {"referenceID": 23, "context": "This is answered by an application of Sard\u2019s/Morse theorem in (Milnor, 1965).", "startOffset": 62, "endOffset": 76}, {"referenceID": 30, "context": "Note however that in practice the Hessian at critical points can be close to singular (at least up to numerical precision), which might affect the training of neural networks negatively (Sagun et al., 2016).", "startOffset": 186, "endOffset": 206}, {"referenceID": 26, "context": "2 (Nguyen, 2015; Mityagin, 2015) If f : R \u2192 R is a real analytic function which is not identically zero then the set {x \u2208 R | f(x) = 0} has Lebesgue measure zero.", "startOffset": 2, "endOffset": 32}, {"referenceID": 24, "context": "2 (Nguyen, 2015; Mityagin, 2015) If f : R \u2192 R is a real analytic function which is not identically zero then the set {x \u2208 R | f(x) = 0} has Lebesgue measure zero.", "startOffset": 2, "endOffset": 32}, {"referenceID": 22, "context": "(Marsden, 1974).", "startOffset": 0, "endOffset": 15}, {"referenceID": 36, "context": "As modern neural networks are expressive enough to represent any function, see (Zhang et al., 2017) for an interesting discussion on this, one can expect that in some layer the training data becomes linearly separable.", "startOffset": 79, "endOffset": 99}, {"referenceID": 10, "context": "3 recovers the similar result of (Gori & Tesi, 1992; Frasconi et al., 1997) for onehidden layer networks.", "startOffset": 33, "endOffset": 75}, {"referenceID": 3, "context": "340 (Barber, 2012).", "startOffset": 4, "endOffset": 18}], "year": 2017, "abstractText": "While the optimization problem behind deep neural networks is highly non-convex, it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points. It has been argued that this is the case as all local minima are close to being globally optimal. We show that this is (almost) true, in fact almost all local minima are globally optimal, for a fully connected network with squared loss and analytic activation function given that the number of hidden units of one layer of the network is larger than the number of training points and the network structure from this layer on is pyramidal.", "creator": "LaTeX with hyperref package"}}}