{"id": "1704.08059", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "Riemannian Optimization for Skip-Gram Negative Sampling", "abstract": "Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in \"word2vec\" software, is usually optimized by stochastic gradient descent. However, the optimization of SGNS objective can be viewed as a problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.", "histories": [["v1", "Wed, 26 Apr 2017 11:17:51 GMT  (620kb,D)", "http://arxiv.org/abs/1704.08059v1", "9 pages, 4 figures, ACL 2017"]], "COMMENTS": "9 pages, 4 figures, ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexander fonarev", "oleksii grinchuk", "gleb gusev", "pavel serdyukov", "ivan v oseledets"], "accepted": true, "id": "1704.08059"}, "pdf": {"name": "1704.08059.pdf", "metadata": {"source": "CRF", "title": "Riemannian Optimization for Skip-Gram Negative Sampling", "authors": ["Alexander Fonarev", "Oleksii Hrinchuk", "Gleb Gusev", "Pavel Serdyukov", "Ivan Oseledets"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "2 Problem Setting", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Skip-Gram Negative Sampling", "text": "In this thesis we look at the Skip-Gram context pair observed in the corpus. < < r > Word pairs = > Word pairs (Mikolov et al., 2013), which is a probabilistic discrimination model. < r > Word pairs used as a word sequence w1,.., wn, where n can be larger than 1012 and wi \u00b2 VW, belongs to a vocabulary of words VW. # A context c \u00b2 VC of the word wi is a word from sentence {wi \u2212 L,..., wi \u2212 1, wi + L} for some fixed window size L. Let's say c \u00b2 Rd is the word embedding model of word w and context c, respectively. Suppose they are specified by the following mappings: W: VW \u2192 Rd, C: VC \u2192 R. The ultimate goal of SGNS word embedding is that we have a good mappingsW and C.D are a multiset of all the word contexts that are observed in the corpus."}, {"heading": "2.2 Optimization over Low-Rank Matrices", "text": "Based on the perspective proposed in (Levy and Goldberg, 2014), let us show that the optimization problem indicated in (3) can be considered a problem in the search for a matrix that maximizes a certain objective function and has a ranking constraint (step 1 of the scheme described in section 1)."}, {"heading": "2.2.1 SGNS Loss Function", "text": "As shown in (Levy and Goldberg, 2014), the logarithmic probability (3) can be represented as the sum of lw, c (w, c) over all pairs (w, c), where lw, c (w, c) has the following form: lw, c (w, c) = # (w, c) log \u03c3 (< w, c >) + + k # (w) # (c) | D | log \u03c3 (\u2212 < w, c >). (4) A crucial observation is that this loss function depends only on the scalar product < w, c > but not on embedding w and c: lw, c (w, c) = fw, c (xw, c), where c, c #, c, c (c) are the scalable product."}, {"heading": "2.2.2 Matrix Notation", "text": "Describe | VW | as n and | VC | as m. Let us call W-Rn \u00b7 d and C-Rm \u00b7 d matrices, where each line w-Rd of matrix W is the word embedding the corresponding word w and each line c-Rd of matrix C is the context in which the corresponding context c is embedded. Then, the elements of the product of these matrices X = WC > are the scalar products xw, c of all pairs (w, c): X = (xw, c), w-VW, c-VC. Note that this matrix has the rank d because X is equal to the product of two matrices with the sizes (n-d) and (d-m). Now we can write the SGNS target given by (3) as a function of X-VW, c-VC and VC in the form of optimization."}, {"heading": "2.3 Computing Embeddings from a Low-Rank Solution", "text": "Once X is found, we must restore W and C so that X = WC > (step 2 in the scheme described in section 1 = > similarities) does not lead to a unique solution, because if (W, C) fulfils this equation, then WS \u2212 1 and CS > also fulfil it for each non-singular matrix S. In addition, different solutions can achieve different values of linguistic metrics (see section 4.2 for details).While our paper focuses on step 1, for step 2 we use a heuristic approach proposed in (Levy et al., 2015), and it shows good results in practice."}, {"heading": "3 Proposed Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Riemannian Optimization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 General Scheme", "text": "The main idea of the belt optimization (Udriste, 1994) is to consider (6) as a limited optimization problem. Suppose we have an approximate solution Xi at a current stage of the optimization process, where i is the number of steps. To improve Xi, the next step of the standard gradient increase outputs the pointsXi + F (Xi), where Q (Xi) is the gradient of object F at point Xi. Note that the gradient F (Xi) can, of course, be considered as a matrix in Rn \u00b7 m. The point Xi + F (Xi) leaves the manifold Md because its rank is generally greater than that of Xi. Therefore, Riemannic optimization methods point Xi + F (Xi) back to multiple Md. The standard belt-mannic gradient method projects the gradient step first onto the tangential space at the current point Xi and then pulls it back: Xi + 1 (TM + PF problem) Xi."}, {"heading": "3.1.2 Projector-Splitting Algorithm", "text": "In our paper we use a simplified version of such an approach, which puts the splitting method (Xi) into practice (Xi), directly into the multiplicity and does not require a projection onto the tangent space PTM as shown in Figure 1: Xi + XX xx xx xx xx xx xx x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3. RELATED WORK", "text": "[?] Levi main [?] rF (Xi) Xi + rF (Xi) Xi + rF (Xi) Xi = UiSiV T iXiXi + 1Xi + 1 = Ui + 1Si + 1Si xx xx xx. [?] rF (Xi) Xi + xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"}, {"heading": "3.2 Algorithm", "text": "In the case of the SGNS object specified by (5), an element of the gradient F has the form: (HQ F (X) w, c = \u2202 fw, c (xw, c) \u2202 xw, c = = # (w, c) \u00b7 \u03c3 (\u2212 xw, c) \u2212 k # (w) # (c) | D | \u00b7 \u03c3 (xw, c).To make the method more flexible with respect to the convergence properties, we additionally use the same for the step size parameters. In this case, Retracto R returns Xi + \u03bb F (Xi) instead of Xi + \u0445F (Xi) to the m niche. The entire optimization procedure is summarized in algorithm 1."}, {"heading": "4 Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Training Models", "text": "We compare our method (\"RO-SGNS\" in the tables) performance with two > basic functions: SGNS embedding optimized via Stochastic Gradient Descent (\"SVD-SPPMI\" in the tables) (Mikolov et al., 2013) and embedding obtained by SVD via SPPMI matrix (\"SVD-SPPMI\" in the tables) (Levy and Goldberg, 2014). We have also experimented with block-by-block alternating optimization via the factors W and C, but the results are almost identical to the SGD results, so we do not include them in our work. \u2212 The source code of our experiments is online1.The models were trained on English Wikipedia \"enwik9\" corpus2, which has previously been used in most work on this topic."}, {"heading": "4.2 Evaluation", "text": "We evaluate word embedding on the word similarity task using the following popular datasets: \"wordsim-353\" (Finkelstein et al., 2001); 3 datasets), \"simlex-999\" (Hill et al., 2016) and \"men\" (Bruni et al., 2014). The original dataset \"wordsim-353\" is a mixture of the word pairs for both word similarity and word kinship tasks. This dataset was divided (Agirre et al., 2009) into two overlapping parts: \"wordsim-sim\" (\"ws-sim\" in the tables) and \"wordsim-rel\" (\"wsrel\" in the tables) to separate the words from different tasks. In our experiments, we use both at eye level with the full version of \"wordsimsim353\" (\"ws-\" in the tables)."}, {"heading": "5 Results of Experiments", "text": "We also see that SGD-SGNS and SVD-SPPMI methods provide quite similar results, but the proposed method achieves significantly better results than the SGNS method, which, however, implies the feasibility of using SGNS optimization frameworks in SGNS optimization problems. It is interesting to note that SVD-SPPMI method, which does not directly optimize optimization targets, achieves better results than the SGNS method, which aims at optimizing SGNS. This fact also confirms the idea described in Section 2.2.2 that independent optimization through parameters W and C can reduce performance. However, the target performance measurement of embedding models is the correlation between semantic similarity and human assessment (Section 4.2). Table 2 presents the comparison of the methods related to this."}, {"heading": "6 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Word Embeddings", "text": "There are several open source implementations of the neural SGNS network, widely known as \"word2vec.\" As shown in Section 2.2, Skip-Gram Negative Sampling Optimization, can be reformulated as a problem when searching for a low-rank matrix. To be able to use SVD for this task out-of-the-box, the authors of (Levy and Goldberg, 2014) used the surrogate version of SGNS as an objective function. There are two general assumptions made in their algorithm that distinguish it from SGNS optimization: 1. SVD optimizes as opposed to SGNS loss functions."}, {"heading": "6.2 Riemannian Optimization", "text": "An introduction to the optimization of Rieman manifolds can be found in (Udriste, 1994).The overview of the withdrawal of high-level matrices to low-level manifolds is given in (Absil and Oseledets, 2015).The projector splitting algorithm was introduced in (Lubich and Oseledets, 2014) and also mentioned in (Absil and Oseledets, 2015) as \"Lie-Trotter retreat.\" Rieman optimization is successfully applied to various data science problems: for example matrix completion (Vandereycken, 2013), large-scale recommendation systems (Tan et al., 2014) and tensor completion (Kressner et al., 2014)."}, {"heading": "7 Conclusions", "text": "In our thesis, we proposed a general two-stage scheme for training the SGNS Word Embedding Model and introduced the algorithm that performs the search for a low-level solution using the belt optimization framework. We also demonstrated the superiority of our method by providing an experimental comparison with existing state-of-the-art approaches. A possible direction for future work is to apply more advanced optimization techniques to step 1 of the scheme proposed in section 1 and explore step 2 - to obtain embedding with a given low-level matrix."}, {"heading": "Acknowledgments", "text": "This research was supported by the Ministry of Education and Science of the Russian Federation (funding 14.756.31.0001)."}], "references": [{"title": "Low-rank retractions: a survey and new results", "author": ["P-A Absil", "Ivan V Oseledets."], "venue": "Computational Optimization and Applications 62(1):5\u201329.", "citeRegEx": "Absil and Oseledets.,? 2015", "shortCiteRegEx": "Absil and Oseledets.", "year": 2015}, {"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa."], "venue": "NAACL. pages 19\u201327.", "citeRegEx": "Agirre et al\\.,? 2009", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Block power method for svd decomposition", "author": ["AH Bentbib", "A Kanber."], "venue": "Analele Stiintifice Ale Unversitatii Ovidius Constanta-Seria Matematica 23(2):45\u201358.", "citeRegEx": "Bentbib and Kanber.,? 2015", "shortCiteRegEx": "Bentbib and Kanber.", "year": 2015}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam-Khanh Tran", "Marco Baroni."], "venue": "J. Artif. Intell. Res.(JAIR) 49(1-47).", "citeRegEx": "Bruni et al\\.,? 2014", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "WWW. pages 406\u2013414.", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "word2vec explained: deriving mikolov et al.\u2019s negativesampling word-embedding method", "author": ["Yoav Goldberg", "Omer Levy"], "venue": null, "citeRegEx": "Goldberg and Levy.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg and Levy.", "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics .", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Towards a better understanding of predict and count models", "author": ["S Sathiya Keerthi", "Tobias Schnabel", "Rajiv Khanna."], "venue": "arXiv preprint arXiv:1511.02024 .", "citeRegEx": "Keerthi et al\\.,? 2015", "shortCiteRegEx": "Keerthi et al\\.", "year": 2015}, {"title": "Dynamical low-rank approximation", "author": ["Othmar Koch", "Christian Lubich."], "venue": "SIAM J. Matrix Anal. Appl. 29(2):434\u2013454.", "citeRegEx": "Koch and Lubich.,? 2007", "shortCiteRegEx": "Koch and Lubich.", "year": 2007}, {"title": "Low-rank tensor completion by riemannian optimization", "author": ["Daniel Kressner", "Michael Steinlechner", "Bart Vandereycken."], "venue": "BIT Numerical Mathematics 54(2):447\u2013468.", "citeRegEx": "Kressner et al\\.,? 2014", "shortCiteRegEx": "Kressner et al\\.", "year": 2014}, {"title": "How to generate a good word embedding", "author": ["Siwei Lai", "Kang Liu", "Shi He", "Jun Zhao"], "venue": null, "citeRegEx": "Lai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2015}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "NIPS. pages 2177\u20132185.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "ACL 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "A projector-splitting integrator for dynamical lowrank approximation", "author": ["Christian Lubich", "Ivan V Oseledets."], "venue": "BIT Numerical Mathematics 54(1):171\u2013188.", "citeRegEx": "Lubich and Oseledets.,? 2014", "shortCiteRegEx": "Lubich and Oseledets.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "NIPS. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Fixed-rank matrix factorizations and riemannian low-rank optimization", "author": ["Bamdev Mishra", "Gilles Meyer", "Silv\u00e8re Bonnabel", "Rodolphe Sepulchre."], "venue": "Computational Statistics 29(3-4):591\u2013621.", "citeRegEx": "Mishra et al\\.,? 2014", "shortCiteRegEx": "Mishra et al\\.", "year": 2014}, {"title": "On the degrees of freedom of reduced-rank estimators in multivariate regression", "author": ["A Mukherjee", "K Chen", "N Wang", "J Zhu."], "venue": "Biometrika 102(2):457\u2013 477.", "citeRegEx": "Mukherjee et al\\.,? 2015", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2015}, {"title": "word2vec parameter learning explained", "author": ["Xin Rong."], "venue": "arXiv preprint arXiv:1411.2738 .", "citeRegEx": "Rong.,? 2014", "shortCiteRegEx": "Rong.", "year": 2014}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["Tobias Schnabel", "Igor Labutov", "David Mimno", "Thorsten Joachims."], "venue": "EMNLP.", "citeRegEx": "Schnabel et al\\.,? 2015", "shortCiteRegEx": "Schnabel et al\\.", "year": 2015}, {"title": "Riemannian pursuit for big matrix recovery", "author": ["Mingkui Tan", "Ivor W Tsang", "Li Wang", "Bart Vandereycken", "Sinno Jialin Pan."], "venue": "ICML. volume 32, pages 1539\u20131547.", "citeRegEx": "Tan et al\\.,? 2014", "shortCiteRegEx": "Tan et al\\.", "year": 2014}, {"title": "Convex functions and optimization methods on Riemannian manifolds, volume 297", "author": ["Constantin Udriste."], "venue": "Springer Science & Business Media.", "citeRegEx": "Udriste.,? 1994", "shortCiteRegEx": "Udriste.", "year": 1994}, {"title": "Low-rank matrix completion by riemannian optimization", "author": ["Bart Vandereycken."], "venue": "SIAM Journal on Optimization 23(2):1214\u20131236.", "citeRegEx": "Vandereycken.,? 2013", "shortCiteRegEx": "Vandereycken.", "year": 2013}, {"title": "Guarantees of riemannian optimization for low rank matrix recovery", "author": ["Ke Wei", "Jian-Feng Cai", "Tony F Chan", "Shingyu Leung."], "venue": "SIAM Journal on Matrix Analysis and Applications 37(3):1198\u20131222.", "citeRegEx": "Wei et al\\.,? 2016", "shortCiteRegEx": "Wei et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "One of the most popular word embedding models (Mikolov et al., 2013) is a discriminative neural network that optimizes Skip-Gram Negative Sampling (SGNS) objective (see Equation 3).", "startOffset": 46, "endOffset": 68}, {"referenceID": 11, "context": "A successful attempt to follow the above described steps, which outperforms the original SGNS optimization approach in terms of various linguistic tasks, was proposed in (Levy and Goldberg, 2014).", "startOffset": 170, "endOffset": 195}, {"referenceID": 11, "context": "2 in (Levy and Goldberg, 2014) for details).", "startOffset": 5, "endOffset": 30}, {"referenceID": 15, "context": "This leads to an optimization problem over matrix X with the lowrank constraint, which is often (Mishra et al., 2014) solved by applying Riemannian optimization framework (Udriste, 1994).", "startOffset": 96, "endOffset": 117}, {"referenceID": 20, "context": ", 2014) solved by applying Riemannian optimization framework (Udriste, 1994).", "startOffset": 61, "endOffset": 76}, {"referenceID": 13, "context": "In our paper, we use the projector-splitting algorithm (Lubich and Oseledets, 2014), which is easy to implement and has low computational complexity.", "startOffset": 55, "endOffset": 83}, {"referenceID": 11, "context": "\u2022 Our algorithm outperforms state-of-the-art competitors in terms of SGNS objective and the semantic similarity linguistic metric (Levy and Goldberg, 2014; Mikolov et al., 2013; Schnabel et al., 2015).", "startOffset": 130, "endOffset": 200}, {"referenceID": 14, "context": "\u2022 Our algorithm outperforms state-of-the-art competitors in terms of SGNS objective and the semantic similarity linguistic metric (Levy and Goldberg, 2014; Mikolov et al., 2013; Schnabel et al., 2015).", "startOffset": 130, "endOffset": 200}, {"referenceID": 18, "context": "\u2022 Our algorithm outperforms state-of-the-art competitors in terms of SGNS objective and the semantic similarity linguistic metric (Levy and Goldberg, 2014; Mikolov et al., 2013; Schnabel et al., 2015).", "startOffset": 130, "endOffset": 200}, {"referenceID": 14, "context": "1 Skip-Gram Negative Sampling In this paper, we consider the Skip-Gram Negative Sampling (SGNS) word embedding model (Mikolov et al., 2013), which is a probabilistic discriminative model.", "startOffset": 117, "endOffset": 139}, {"referenceID": 14, "context": "Usually, this optimization is done via the stochastic gradient descent procedure that is performed during passing through the corpus (Mikolov et al., 2013; Rong, 2014).", "startOffset": 133, "endOffset": 167}, {"referenceID": 17, "context": "Usually, this optimization is done via the stochastic gradient descent procedure that is performed during passing through the corpus (Mikolov et al., 2013; Rong, 2014).", "startOffset": 133, "endOffset": 167}, {"referenceID": 11, "context": "2 Optimization over Low-Rank Matrices Relying on the prospect proposed in (Levy and Goldberg, 2014), let us show that the optimization problem given by (3) can be considered as a problem of searching for a matrix that maximizes a certain objective function and has the rank-d constraint (Step 1 in the scheme described in Section 1).", "startOffset": 74, "endOffset": 99}, {"referenceID": 11, "context": "1 SGNS Loss Function As shown in (Levy and Goldberg, 2014), the logarithmic likelihood (3) can be represented as the sum of lw,c(w, c) over all pairs (w, c), where lw,c(w, c) has the following form: lw,c(w, c) = #(w, c) log \u03c3(\u3008w, c\u3009)+", "startOffset": 33, "endOffset": 58}, {"referenceID": 20, "context": "where Md is the manifold (Udriste, 1994) of all matrices in Rn\u00d7m with rank d: Md = {X \u2208 Rn\u00d7m : rank(X) = d}.", "startOffset": 25, "endOffset": 40}, {"referenceID": 16, "context": "This entails the optimization in the space with ((n + m \u2212 d) \u00b7 d) degrees of freedom (Mukherjee et al., 2015) instead of ((n+m) \u00b7 d), what simplifies the optimization process (see Section 5 for the experimental results).", "startOffset": 85, "endOffset": 109}, {"referenceID": 12, "context": "While our paper focuses on Step 1, we use, for Step 2, a heuristic approach that was proposed in (Levy et al., 2015) and it shows good results in practice.", "startOffset": 97, "endOffset": 116}, {"referenceID": 12, "context": "However, scaling by \u221a \u03a3 instead of \u03a3 was shown in (Levy et al., 2015) to be a better solution in experiments.", "startOffset": 50, "endOffset": 69}, {"referenceID": 20, "context": "1 General Scheme The main idea of Riemannian optimization (Udriste, 1994) is to consider (6) as a constrained optimization problem.", "startOffset": 58, "endOffset": 73}, {"referenceID": 22, "context": "Theoretical properties and convergence guarantees of such methods are discussed in (Wei et al., 2016) more thoroughly.", "startOffset": 83, "endOffset": 101}, {"referenceID": 13, "context": "Instead of this approach, we use the projectorsplitting method (Lubich and Oseledets, 2014), which is a second-order retraction onto the manifold (for details, see the review (Absil and Oseledets, 2015)).", "startOffset": 63, "endOffset": 91}, {"referenceID": 0, "context": "Instead of this approach, we use the projectorsplitting method (Lubich and Oseledets, 2014), which is a second-order retraction onto the manifold (for details, see the review (Absil and Oseledets, 2015)).", "startOffset": 175, "endOffset": 202}, {"referenceID": 2, "context": "also qui e in u tiv : instead of compu ing the full SVD of Xi + \u2207F (Xi) according to the gradient projection method, we use just one step of the block power numerical method (Bentbib and Kanber, 2015) which computes the SVD, what reduces the computational complexity.", "startOffset": 174, "endOffset": 200}, {"referenceID": 8, "context": "Indeed, the gradient with respect to U (while keeping the orthogonality constraints) can be written (Koch and Lubich, 2007) as:", "startOffset": 100, "endOffset": 123}, {"referenceID": 14, "context": "1 Training Models We compare our method (\u201cRO-SGNS\u201d in the tables) performance to two baselines: SGNS embeddings optimized via Stochastic Gradient Descent, implemented in the original \u201cword2vec\u201d, (\u201cSGDSGNS\u201d in the tables) (Mikolov et al., 2013) and embeddings obtained by SVD over SPPMI matrix (\u201cSVD-SPPMI\u201d in the tables) (Levy and Goldberg, 2014).", "startOffset": 221, "endOffset": 243}, {"referenceID": 11, "context": ", 2013) and embeddings obtained by SVD over SPPMI matrix (\u201cSVD-SPPMI\u201d in the tables) (Levy and Goldberg, 2014).", "startOffset": 85, "endOffset": 110}, {"referenceID": 11, "context": "Like in previous studies, we counted only the words which occur more than 200 times in the training corpus (Levy and Goldberg, 2014; Mikolov et al., 2013).", "startOffset": 107, "endOffset": 154}, {"referenceID": 14, "context": "Like in previous studies, we counted only the words which occur more than 200 times in the training corpus (Levy and Goldberg, 2014; Mikolov et al., 2013).", "startOffset": 107, "endOffset": 154}, {"referenceID": 11, "context": "The size of the context window was set to 5 for all experiments, as it was done in (Levy and Goldberg, 2014; Mikolov et al., 2013).", "startOffset": 83, "endOffset": 130}, {"referenceID": 14, "context": "The size of the context window was set to 5 for all experiments, as it was done in (Levy and Goldberg, 2014; Mikolov et al., 2013).", "startOffset": 83, "endOffset": 130}, {"referenceID": 4, "context": "We use the following popular datasets for this purpose: \u201cwordsim-353\u201d ((Finkelstein et al., 2001); 3 datasets), \u201csimlex-999\u201d (Hill et al.", "startOffset": 71, "endOffset": 97}, {"referenceID": 6, "context": ", 2001); 3 datasets), \u201csimlex-999\u201d (Hill et al., 2016) and \u201cmen\u201d (Bruni et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 3, "context": ", 2016) and \u201cmen\u201d (Bruni et al., 2014).", "startOffset": 18, "endOffset": 38}, {"referenceID": 1, "context": "This dataset was split (Agirre et al., 2009) into two intersecting parts: \u201cwordsim-sim\u201d (\u201cws-sim\u201d in the tables) and \u201cwordsim-rel\u201d (\u201cwsrel\u201d in the tables) to separate the words from different tasks.", "startOffset": 23, "endOffset": 44}, {"referenceID": 10, "context": "The idea to stop optimization procedure on some iteration is also discussed in (Lai et al., 2015).", "startOffset": 79, "endOffset": 97}, {"referenceID": 14, "context": "1 Word Embeddings Skip-Gram Negative Sampling was introduced in (Mikolov et al., 2013).", "startOffset": 64, "endOffset": 86}, {"referenceID": 5, "context": "The \u201cnegative sampling\u201d approach is thoroughly described in (Goldberg and Levy, 2014), and the learning method is explained in (Rong, 2014).", "startOffset": 60, "endOffset": 85}, {"referenceID": 17, "context": "The \u201cnegative sampling\u201d approach is thoroughly described in (Goldberg and Levy, 2014), and the learning method is explained in (Rong, 2014).", "startOffset": 127, "endOffset": 139}, {"referenceID": 11, "context": "In order to be able to use out-of-the-box SVD for this task, the authors of (Levy and Goldberg, 2014) used the surrogate version of SGNS as the objective function.", "startOffset": 76, "endOffset": 101}, {"referenceID": 11, "context": "As mentioned in (Levy and Goldberg, 2014), SGNS objective weighs different (w, c) pairs differently, unlike the SVD, which works with the same weight for all pairs and may entail the performance fall.", "startOffset": 16, "endOffset": 41}, {"referenceID": 7, "context": "The comprehensive explanation of the relation between SGNS and SVD-SPPMI methods is provided in (Keerthi et al., 2015).", "startOffset": 96, "endOffset": 118}, {"referenceID": 10, "context": "(Lai et al., 2015; Levy et al., 2015) Original Google word2vec: https://code.", "startOffset": 0, "endOffset": 37}, {"referenceID": 12, "context": "(Lai et al., 2015; Levy et al., 2015) Original Google word2vec: https://code.", "startOffset": 0, "endOffset": 37}, {"referenceID": 20, "context": "2 Riemannian Optimization An introduction to optimization over Riemannian manifolds can be found in (Udriste, 1994).", "startOffset": 100, "endOffset": 115}, {"referenceID": 0, "context": "The overview of retractions of high rank matrices to low-rank manifolds is provided in (Absil and Oseledets, 2015).", "startOffset": 87, "endOffset": 114}, {"referenceID": 13, "context": "The projector-splitting algorithm was introduced in (Lubich and Oseledets, 2014), and also was mentioned in (Absil and Oseledets, 2015) as \u201cLie-Trotter retraction\u201d.", "startOffset": 52, "endOffset": 80}, {"referenceID": 0, "context": "The projector-splitting algorithm was introduced in (Lubich and Oseledets, 2014), and also was mentioned in (Absil and Oseledets, 2015) as \u201cLie-Trotter retraction\u201d.", "startOffset": 108, "endOffset": 135}, {"referenceID": 21, "context": "Riemannian optimization is succesfully applied to various data science problems: for example, matrix completion (Vandereycken, 2013), largescale recommender systems (Tan et al.", "startOffset": 112, "endOffset": 132}, {"referenceID": 19, "context": "Riemannian optimization is succesfully applied to various data science problems: for example, matrix completion (Vandereycken, 2013), largescale recommender systems (Tan et al., 2014), and tensor completion (Kressner et al.", "startOffset": 165, "endOffset": 183}, {"referenceID": 9, "context": ", 2014), and tensor completion (Kressner et al., 2014).", "startOffset": 31, "endOffset": 54}], "year": 2017, "abstractText": "Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in \u201cword2vec\u201d software, is usually optimized by stochastic gradient descent. However, the optimization of SGNS objective can be viewed as a problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.", "creator": "LaTeX with hyperref package"}}}