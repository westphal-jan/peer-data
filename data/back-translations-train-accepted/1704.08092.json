{"id": "1704.08092", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "A Recurrent Neural Model with Attention for the Recognition of Chinese Implicit Discourse Relations", "abstract": "We introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order-agnostic approaches. Our model benefits from a partial sampling scheme and is conceptually simple, yet achieves state-of-the-art performance on the Chinese Discourse Treebank. We also visualize its attention activity to illustrate the model's ability to selectively focus on the relevant parts of an input sequence.", "histories": [["v1", "Wed, 26 Apr 2017 13:10:12 GMT  (198kb,D)", "http://arxiv.org/abs/1704.08092v1", "To appear at ACL2017, code available atthis https URL"]], "COMMENTS": "To appear at ACL2017, code available atthis https URL", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["samuel r\u00f6nnqvist", "niko schenk", "christian chiarcos"], "accepted": true, "id": "1704.08092"}, "pdf": {"name": "1704.08092.pdf", "metadata": {"source": "CRF", "title": "A Recurrent Neural Model with Attention for the Recognition of Chinese Implicit Discourse Relations", "authors": ["Samuel R\u00f6nnqvist", "Niko Schenk", "Christian Chiarcos"], "emails": ["sronnqvi@abo.fi", "schenk@informatik.uni-frankfurt.de", "chiarcos@informatik.uni-frankfurt.de"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move in a certain direction, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in fact, in which they, in a certain directions, in which they, in which they, in which they, in which they, in which they, in a certain directions, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they,"}, {"heading": "2 Approach", "text": "We propose the use of a two-directional long-term research, comprising several layers illustrated by attention. (Hochreiter and Schmidhuber, 1997, LSTM) The model builds on previous work on LSTM, in particular its bidirectional functioning (Graves and Schmidhuber, 2005), attention mechanisms for recurring models (Bahdanau et al., 2014), and the combined use of these techniques for recognizing entity relationships in annotated sequences (Zhou et al, 2016). Specifically, our model is a flexible, recurrent network with sequential inspection capabilities that highlight portions of the input sequence provided by the use of attention mechanisms."}, {"heading": "3 Evaluation", "text": "We evaluate our recursive relationships with the CoNLL 2016, which include the official training, development and testing sets of the CDTB; Table 2 provides an overview of the implicit distribution.6 In accordance with previous distribution mechanisms (Rutherford et al., 2016), we treat entity relations as implicit and exclusionary. In the evaluation, we focus on the pure path for which the gold arguments are provided, and on a particular pair of arguments with the correct sense. The results are shown in our proposed architecture, it is possible to label 257 / 352."}, {"heading": "4 Summary & Outlook", "text": "In this paper, we have introduced the first attention-based recurrent neural sentient labeler specifically designed for implicit discourse relationships in China. Its ability to model discourse units sequentially and collectively has been shown to be extremely beneficial both in terms of the state-of-the-art performance of CDTB (which exceeds word order agnostic feedback-forward approaches) and in terms of revealing observations into the inner workings of the model through its attention mechanism. Its architecture is structurally simple, benefits from partial argument sampling, and can be adapted just as well to similar relationship recognition tasks. In future work, we intend to extend our approach to different languages and domains, e.g. to recent data sets for understanding narrative stories or answering questions (Mostafazadeh et al., 2016; Feng et al., 2015). We believe that recurring modeling implicit tasks can be a successful driver of such discourse tasks."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Ayah Zirikly, Philip Schulz and Wei Ding for their very helpful suggestions on an early version of the paper and also the anonymous reviewers for their valuable feedback and insightful comments. We thank Farrokh Mehryary for their technical support in implementing the attention layer. Computational resources were provided by the CSC - IT Centre for Science, Finland, and the Arcada University of Applied Sciences, Helsinki, Finland. Our research at Goethe University Frankfurt was supported by the project \"Linked Open Dictionaries (LiODi, 2015-2020),\" funded by the German Federal Ministry of Education and Research (BMBF). 7The code involved in this study is available to the public at http: / / www.acoli.informatik. uni-frankfurt.de / resources /."}], "references": [{"title": "Uniform Information Density at the Level of Discourse Relations: Negation Markers and Discourse Connective Omission", "author": ["Fatemeh Torabi Asr", "Vera Demberg."], "venue": "11th International Conference on Computational Semantics (IWCS).", "citeRegEx": "Asr and Demberg.,? 2015", "shortCiteRegEx": "Asr and Demberg.", "year": 2015}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Implicit Discourse Relation Detection via a Deep Architecture with Gated Relevance Network", "author": ["Jifan Chen", "Qi Zhang", "Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Com-", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Applying deep learning to answer selection: A study and an open task", "author": ["Minwei Feng", "Bing Xiang", "Michael R. Glass", "Lidan Wang", "Bowen Zhou."], "venue": "2015 IEEE Workshop on Automatic Speech Recognition and", "citeRegEx": "Feng et al\\.,? 2015", "shortCiteRegEx": "Feng et al\\.", "year": 2015}, {"title": "Text-level Discourse Parsing with Rich Linguistic Features", "author": ["Vanessa Wei Feng", "Graeme Hirst."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1. Association for Computational Linguis-", "citeRegEx": "Feng and Hirst.,? 2012", "shortCiteRegEx": "Feng and Hirst.", "year": 2012}, {"title": "Chinese Gigaword", "author": ["David Graff", "Ke Chen."], "venue": "LDC Catalog No.: LDC2003T09, ISBN, 1:58563\u2013 58230.", "citeRegEx": "Graff and Chen.,? 2005", "shortCiteRegEx": "Graff and Chen.", "year": 2005}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks 18(5-6):602\u2013610. https://doi.org/10.1016/j.neunet.2005.06.042.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems. pages 1693\u2013", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Using Discourse Commitments to Recognize Textual Entailment", "author": ["Andrew Hickl."], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1. Association for Computational Linguistics, Strouds-", "citeRegEx": "Hickl.,? 2008", "shortCiteRegEx": "Hickl.", "year": 2008}, {"title": "Single-Document Summarization as a Tree Knapsack Problem", "author": ["Tsutomu Hirao", "Yasuhisa Yoshida", "Masaaki Nishino", "Norihito Yasuda", "Masaaki Nagata."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Hirao et al\\.,? 2013", "shortCiteRegEx": "Hirao et al\\.", "year": 2013}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Comput. 9(8):1735\u2013 1780. https://doi.org/10.1162/neco.1997.9.8.1735.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Chinese Discourse Relation Recognition", "author": ["Hen-Hsen Huang", "Hsin-Hsi Chen."], "venue": "Proceedings of 5th International Joint Conference on Natural Language Processing. Asian Federation of Natural Language Processing, Chiang Mai, Thailand, pages", "citeRegEx": "Huang and Chen.,? 2011", "shortCiteRegEx": "Huang and Chen.", "year": 2011}, {"title": "A Latent Variable Recurrent Neural Network for Discourse-Driven Language Models", "author": ["Yangfeng Ji", "Gholamreza Haffari", "Jacob Eisenstein."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association", "citeRegEx": "Ji et al\\.,? 2016", "shortCiteRegEx": "Ji et al\\.", "year": 2016}, {"title": "Discourse Relation Sense Classification Systems for CoNLL-2016 Shared Task", "author": ["Ping Jian", "Xiaohan She", "Chenwei Zhang", "Pengcheng Zhang", "Jian Feng."], "venue": "Proceedings of the CoNLL-16 shared task. Association for", "citeRegEx": "Jian et al\\.,? 2016", "shortCiteRegEx": "Jian et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6980. http://arxiv.org/abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Temporal Interpretation, Discourse Relations and Commonsense entailment", "author": ["Alex Lascarides", "Nicholas Asher."], "venue": "Linguistics and Philosophy 16(5):437\u2013493.", "citeRegEx": "Lascarides and Asher.,? 1993", "shortCiteRegEx": "Lascarides and Asher.", "year": 1993}, {"title": "Deep learning", "author": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton."], "venue": "Nature 521(7553):436\u2013444.", "citeRegEx": "LeCun et al\\.,? 2015", "shortCiteRegEx": "LeCun et al\\.", "year": 2015}, {"title": "Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention", "author": ["Yang Liu", "Sujian Li."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Liu and Li.,? 2016", "shortCiteRegEx": "Liu and Li.", "year": 2016}, {"title": "Rhetorical structure theory: Toward a functional theory of text organization", "author": ["William C. Mann", "Sandra A. Thompson."], "venue": "Text 8(3):243\u2013281.", "citeRegEx": "Mann and Thompson.,? 1988", "shortCiteRegEx": "Mann and Thompson.", "year": 1988}, {"title": "The Penn Discourse TreeBank", "author": ["nie Webber"], "venue": null, "citeRegEx": "Webber.,? \\Q2008\\E", "shortCiteRegEx": "Webber.", "year": 2008}, {"title": "Neural Network Models for Im", "author": ["wen Xue"], "venue": null, "citeRegEx": "Xue.,? \\Q2016\\E", "shortCiteRegEx": "Xue.", "year": 2016}, {"title": "Discourse processing for context question answering based on linguistic knowledge", "author": ["Mingyu Sun", "Joyce Y Chai."], "venue": "Knowledge-Based Systems 20(6):511\u2013526.", "citeRegEx": "Sun and Chai.,? 2007", "shortCiteRegEx": "Sun and Chai.", "year": 2007}, {"title": "Discourse connectors for latent subjectivity in sentiment analysis", "author": ["Rakshit S. Trivedi", "Jacob Eisenstein."], "venue": "Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguis-", "citeRegEx": "Trivedi and Eisenstein.,? 2013", "shortCiteRegEx": "Trivedi and Eisenstein.", "year": 2013}, {"title": "Two Endto-end Shallow Discourse Parsers for English and Chinese in CoNLL-2016 Shared Task", "author": ["Jianxiang Wang", "Man Lan."], "venue": "Proceedings of the CoNLL-16 shared task. Association for Computational Linguistics, pages 33\u201340.", "citeRegEx": "Wang and Lan.,? 2016", "shortCiteRegEx": "Wang and Lan.", "year": 2016}, {"title": "D-LTAG: extending lexicalized TAG to discourse", "author": ["Bonnie L. Webber."], "venue": "Cognitive Science 28(5):751\u2013779. http://dblp.unitrier.de/db/journals/cogsci/cogsci28.html.", "citeRegEx": "Webber.,? 2004", "shortCiteRegEx": "Webber.", "year": 2004}, {"title": "Discourse Sense Classification from Scratch using Focused RNNs", "author": ["Gregor Weiss", "Marko Bajec."], "venue": "Proceedings of the CoNLL-16 shared task. Association for Computational Linguistics, pages 50\u201354. https://doi.org/10.18653/v1/K16-2006.", "citeRegEx": "Weiss and Bajec.,? 2016", "shortCiteRegEx": "Weiss and Bajec.", "year": 2016}, {"title": "The CoNLL-2016 Shared Task on Shallow Discourse Parsing", "author": ["Nianwen Xue", "Hwee Tou Ng", "Sameer Pradhan", "Bonnie Webber", "Attapol Rutherford", "Chuan Wang", "Hongmin Wang."], "venue": "Proceedings of the Twentieth Conference on Computational Nat-", "citeRegEx": "Xue et al\\.,? 2016", "shortCiteRegEx": "Xue et al\\.", "year": 2016}, {"title": "Shallow Convolutional Neural Network for Implicit Discourse Relation Recognition", "author": ["Biao Zhang", "Jinsong Su", "Deyi Xiong", "Yaojie Lu", "Hong Duan", "Junfeng Yao."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "AttentionBased Bidirectional Long Short-Term Memory Networks for Relation Classification", "author": ["Peng Zhou", "Wei Shi", "Jun Tian", "Zhenyu Qi", "Bingchen Li", "Hongwei Hao", "Bo Xu."], "venue": "Proceedings of the 54th Annual Meeting of the Association for", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "PDTBstyle Discourse Annotation of Chinese Text", "author": ["Yuping Zhou", "Nianwen Xue."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for", "citeRegEx": "Zhou and Xue.,? 2012", "shortCiteRegEx": "Zhou and Xue.", "year": 2012}], "referenceMentions": [{"referenceID": 21, "context": "Automatically detecting how meaning units are organized benefits practical downstream applications, such as question answering (Sun and Chai, 2007), recognizing textual entailment (Hickl, 2008), sentiment analysis (Trivedi and Eisenstein, 2013), or text summarization (Hirao et al.", "startOffset": 127, "endOffset": 147}, {"referenceID": 8, "context": "Automatically detecting how meaning units are organized benefits practical downstream applications, such as question answering (Sun and Chai, 2007), recognizing textual entailment (Hickl, 2008), sentiment analysis (Trivedi and Eisenstein, 2013), or text summarization (Hirao et al.", "startOffset": 180, "endOffset": 193}, {"referenceID": 22, "context": "Automatically detecting how meaning units are organized benefits practical downstream applications, such as question answering (Sun and Chai, 2007), recognizing textual entailment (Hickl, 2008), sentiment analysis (Trivedi and Eisenstein, 2013), or text summarization (Hirao et al.", "startOffset": 214, "endOffset": 244}, {"referenceID": 9, "context": "Automatically detecting how meaning units are organized benefits practical downstream applications, such as question answering (Sun and Chai, 2007), recognizing textual entailment (Hickl, 2008), sentiment analysis (Trivedi and Eisenstein, 2013), or text summarization (Hirao et al., 2013).", "startOffset": 268, "endOffset": 288}, {"referenceID": 18, "context": "Various formalisms in terms of semantic coherence frameworks have been proposed to account for these contextual assumptions (Mann and Thompson, 1988; Lascarides and Asher, 1993; Webber, 2004).", "startOffset": 124, "endOffset": 191}, {"referenceID": 15, "context": "Various formalisms in terms of semantic coherence frameworks have been proposed to account for these contextual assumptions (Mann and Thompson, 1988; Lascarides and Asher, 1993; Webber, 2004).", "startOffset": 124, "endOffset": 191}, {"referenceID": 24, "context": "Various formalisms in terms of semantic coherence frameworks have been proposed to account for these contextual assumptions (Mann and Thompson, 1988; Lascarides and Asher, 1993; Webber, 2004).", "startOffset": 124, "endOffset": 191}, {"referenceID": 4, "context": "Motivation: Previous work on implicit sense labeling is heavily feature-rich and requires domainspecific, semantic lexicons (Pitler et al., 2009; Feng and Hirst, 2012; Huang and Chen, 2011).", "startOffset": 124, "endOffset": 189}, {"referenceID": 11, "context": "Motivation: Previous work on implicit sense labeling is heavily feature-rich and requires domainspecific, semantic lexicons (Pitler et al., 2009; Feng and Hirst, 2012; Huang and Chen, 2011).", "startOffset": 124, "endOffset": 189}, {"referenceID": 27, "context": "These promising neural methods attempt to infer latent representations appropriate for implicit relation classification (Zhang et al., 2015; Ji et al., 2016; Chen et al., 2016).", "startOffset": 120, "endOffset": 176}, {"referenceID": 12, "context": "These promising neural methods attempt to infer latent representations appropriate for implicit relation classification (Zhang et al., 2015; Ji et al., 2016; Chen et al., 2016).", "startOffset": 120, "endOffset": 176}, {"referenceID": 2, "context": "These promising neural methods attempt to infer latent representations appropriate for implicit relation classification (Zhang et al., 2015; Ji et al., 2016; Chen et al., 2016).", "startOffset": 120, "endOffset": 176}, {"referenceID": 17, "context": "specific architectures (Liu and Li, 2016), while discourse modeling techniques for Chinese have received very little attention in the literature and are still seriously underrepresented in terms of publicly available systems.", "startOffset": 23, "endOffset": 41}, {"referenceID": 29, "context": "What is more, over 80% of all words in Chinese discourse relations are implicit\u2014compared to only 52% in English (Zhou and Xue, 2012).", "startOffset": 112, "endOffset": 132}, {"referenceID": 26, "context": "Recently, in the context of the CoNLL 2016 shared task (Xue et al., 2016), a first independent evaluation platform beyond class level has been established.", "startOffset": 55, "endOffset": 73}, {"referenceID": 20, "context": "Recently, in the context of the CoNLL 2016 shared task (Xue et al., 2016), a first independent evaluation platform beyond class level has been established. Surprisingly, the best performing neural architectures to date are standard feedforward networks, cf. Wang and Lan (2016); Schenk et al.", "startOffset": 56, "endOffset": 278}, {"referenceID": 20, "context": "Recently, in the context of the CoNLL 2016 shared task (Xue et al., 2016), a first independent evaluation platform beyond class level has been established. Surprisingly, the best performing neural architectures to date are standard feedforward networks, cf. Wang and Lan (2016); Schenk et al. (2016); Qin et al.", "startOffset": 56, "endOffset": 300}, {"referenceID": 20, "context": "Recently, in the context of the CoNLL 2016 shared task (Xue et al., 2016), a first independent evaluation platform beyond class level has been established. Surprisingly, the best performing neural architectures to date are standard feedforward networks, cf. Wang and Lan (2016); Schenk et al. (2016); Qin et al. (2016). Even though these specific models completely ignore word order within", "startOffset": 56, "endOffset": 319}, {"referenceID": 28, "context": "Inspired by Zhou et al. (2016), our system is a practical adaptation of the recent", "startOffset": 12, "endOffset": 31}, {"referenceID": 6, "context": "lar its bidirectional mode of operation (Graves and Schmidhuber, 2005), attention mechanisms for recurrent models (Bahdanau et al.", "startOffset": 40, "endOffset": 70}, {"referenceID": 1, "context": "lar its bidirectional mode of operation (Graves and Schmidhuber, 2005), attention mechanisms for recurrent models (Bahdanau et al., 2014; Hermann et al., 2015), and the combined use of these techniques for entity relation recognition in annotated sequences (Zhou et al.", "startOffset": 114, "endOffset": 159}, {"referenceID": 7, "context": "lar its bidirectional mode of operation (Graves and Schmidhuber, 2005), attention mechanisms for recurrent models (Bahdanau et al., 2014; Hermann et al., 2015), and the combined use of these techniques for entity relation recognition in annotated sequences (Zhou et al.", "startOffset": 114, "endOffset": 159}, {"referenceID": 28, "context": ", 2015), and the combined use of these techniques for entity relation recognition in annotated sequences (Zhou et al., 2016).", "startOffset": 105, "endOffset": 124}, {"referenceID": 5, "context": "The embedding layer is initialized using pre-trained word vectors, in our case 300-dimensional Chinese Gigaword vectors (Graff and Chen, 2005).", "startOffset": 120, "endOffset": 142}, {"referenceID": 0, "context": "Asr and Demberg (2015) and, in particular, Rohde and Horton (2010) in their psycholinguistic study on implicit causality verbs.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "Asr and Demberg (2015) and, in particular, Rohde and Horton (2010) in their psycholinguistic study on implicit causality verbs.", "startOffset": 0, "endOffset": 67}, {"referenceID": 16, "context": "LeCun et al. (2015). Due to these aspects, we believe this data augmentation technique to be effective in reinforcing the overall robustness of our model.", "startOffset": 0, "endOffset": 20}, {"referenceID": 14, "context": "We employ Adam optimization (Kingma and Ba, 2014) using the cross-entropy loss function with mini batch size of 80.", "startOffset": 28, "endOffset": 49}, {"referenceID": 21, "context": "1 Wang and Lan (2016) 73.", "startOffset": 2, "endOffset": 22}, {"referenceID": 21, "context": "1 Wang and Lan (2016) 73.53 1 Wang and Lan (2016) 72.", "startOffset": 2, "endOffset": 50}, {"referenceID": 21, "context": "1 Wang and Lan (2016) 73.53 1 Wang and Lan (2016) 72.42 2 Qin et al. (2016) 71.", "startOffset": 2, "endOffset": 76}, {"referenceID": 21, "context": "1 Wang and Lan (2016) 73.53 1 Wang and Lan (2016) 72.42 2 Qin et al. (2016) 71.57 2 Schenk et al. (2016) 71.", "startOffset": 2, "endOffset": 105}, {"referenceID": 21, "context": "1 Wang and Lan (2016) 73.53 1 Wang and Lan (2016) 72.42 2 Qin et al. (2016) 71.57 2 Schenk et al. (2016) 71.87 3 Schenk et al. (2016) 70.", "startOffset": 2, "endOffset": 134}, {"referenceID": 19, "context": "59 3 Rutherford and Xue (2016) 70.", "startOffset": 20, "endOffset": 31}, {"referenceID": 19, "context": "59 3 Rutherford and Xue (2016) 70.47 4 Rutherford and Xue (2016) 68.", "startOffset": 20, "endOffset": 65}, {"referenceID": 19, "context": "59 3 Rutherford and Xue (2016) 70.47 4 Rutherford and Xue (2016) 68.30 4 Qin et al. (2016) 67.", "startOffset": 20, "endOffset": 91}, {"referenceID": 19, "context": "59 3 Rutherford and Xue (2016) 70.47 4 Rutherford and Xue (2016) 68.30 4 Qin et al. (2016) 67.41 5 Weiss and Bajec (2016) 66.", "startOffset": 20, "endOffset": 122}, {"referenceID": 19, "context": "59 3 Rutherford and Xue (2016) 70.47 4 Rutherford and Xue (2016) 68.30 4 Qin et al. (2016) 67.41 5 Weiss and Bajec (2016) 66.67 5 Weiss and Bajec (2016) 64.", "startOffset": 20, "endOffset": 153}, {"referenceID": 19, "context": "59 3 Rutherford and Xue (2016) 70.47 4 Rutherford and Xue (2016) 68.30 4 Qin et al. (2016) 67.41 5 Weiss and Bajec (2016) 66.67 5 Weiss and Bajec (2016) 64.07 6 Weiss and Bajec (2016) 61.", "startOffset": 20, "endOffset": 184}, {"referenceID": 19, "context": "59 3 Rutherford and Xue (2016) 70.47 4 Rutherford and Xue (2016) 68.30 4 Qin et al. (2016) 67.41 5 Weiss and Bajec (2016) 66.67 5 Weiss and Bajec (2016) 64.07 6 Weiss and Bajec (2016) 61.44 6 Weiss and Bajec (2016) 63.", "startOffset": 20, "endOffset": 215}, {"referenceID": 13, "context": "51 7 Jian et al. (2016) 21.", "startOffset": 5, "endOffset": 24}, {"referenceID": 13, "context": "51 7 Jian et al. (2016) 21.90 7 Jian et al. (2016) 21.", "startOffset": 5, "endOffset": 51}, {"referenceID": 23, "context": "forward system of Wang and Lan (2016) and all other word order-agnostic approaches.", "startOffset": 18, "endOffset": 38}, {"referenceID": 3, "context": "tion answering (Mostafazadeh et al., 2016; Feng et al., 2015).", "startOffset": 15, "endOffset": 61}], "year": 2017, "abstractText": "We introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order-agnostic approaches. Our model benefits from a partial sampling scheme and is conceptually simple, yet achieves state-of-the-art performance on the Chinese Discourse Treebank. We also visualize its attention activity to illustrate the model\u2019s ability to selectively focus on the relevant parts of an input sequence.", "creator": "LaTeX with hyperref package"}}}