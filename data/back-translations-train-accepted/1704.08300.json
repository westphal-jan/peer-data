{"id": "1704.08300", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "Diversity driven attention model for query-based abstractive summarization", "abstract": "Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encode-attend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28\\% (absolute) in ROUGE-L scores.", "histories": [["v1", "Wed, 26 Apr 2017 19:06:37 GMT  (500kb,D)", "http://arxiv.org/abs/1704.08300v1", "Accepted at ACL 2017"]], "COMMENTS": "Accepted at ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["preksha nema", "mitesh m khapra", "anirban laha", "balaraman ravindran"], "accepted": true, "id": "1704.08300"}, "pdf": {"name": "1704.08300.pdf", "metadata": {"source": "CRF", "title": "Diversity driven Attention Model for Query-based Abstractive Summarization", "authors": ["Preksha Nema", "Mitesh M. Khapra", "Anirban Laha", "Balaraman Ravindran"], "emails": ["preksha@cse.iitm.ac.in", "miteshk@cse.iitm.ac.in", "anirlaha@in.ibm.com", "ravi@cse.iitm.ac.in"], "sections": [{"heading": null, "text": "Abstractive Summary aims to generate a shorter version of the document that covers all the important points in a compact and coherent manner. On the other hand, Query-based Summary highlights those points that are relevant in the context of a particular query. In this paper, we propose a query-based summary model based on the encode-attend-decode paradigm with two important additions (i) a query-based attention model (in addition to the query model) that learns to focus on different parts of the query in different time steps (rather than using a static representation for the query) and (ii) a new diversity-based attention model that aims to alleviate the problem of repeated phrases in the summary."}, {"heading": "1 Introduction", "text": "This year, as never before in the history of a country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in"}, {"heading": "2 Related Work", "text": "In fact, the vast majority of this work has focused on an extractive summary, where the idea is to construct a summary by selecting the most relevant sentences from the document (Neto et al., 2002), (Erkan and Radev, 2013), (Colmenares et al., 2015), (Riedhammer et al., 2010), (Neto et al., 2013), (Erkan and Radev, 2013), (Filippova and altun, 2013), (Colmenares et al., 2015) \"We refer the reader (Das and Martins, 2007) and (Nenkova and McKeown, 2012).\""}, {"heading": "3 Dataset", "text": "As mentioned above, there are no existing datasets for query-based abstract summaries. We are creating such a dataset from Debatepedia, an encyclopedia of pros and cons arguments and quotes on critical debate topics. There are 663 debates in the corpus (we have only considered those debates that have at least one query with a document); these 663 debates belong to 53 overlapping categories such as politics, law, crime, environment, health, morality, religion, etc. A given topic may belong to more than one category. For example, the topic \"eye for a glance\" belongs to both \"law\" and \"morality.\" The average number of queries per debate is 5 and the average number of documents per query is 4. Please refer to the url1 datasheet for more details on the number of debates per category. Thus, Figure 1 shows the queries related to the topic \"algae biofuel.\" It also lists the set of documents and a summary for all documents that are directly linked to the abstractions."}, {"heading": "4 Proposed model", "text": "Given the way in which we now describe the modeling in which we use the proposed attention, the question of how we should behave is very complex. (...) The question of how we behave is how we behave (...), how we behave (...), how we behave (...), how we behave (...), how we behave (...), how we behave (...), how we act (...), how we act (...), how we act (...), how we act (...), how we act (...), how we act (...), how we act (...), how we act (...), how we act (...), how we act (...), how we act (...), how we act (...), how we act (...), how we act (...)."}, {"heading": "4.1 Diversity based attention model", "text": "It is possible that the context vectors supplied to the decoder are very similar in successive time steps. We propose four models (D1, D2, SD1, SD2) to address this problem directly. (D1: In this model, after calculating dt as in Equation (8), we make it orthogonal to the context vector at present t \u2212 1: d \u00b2 t = dt \u2212 dTt \u00b2 t \u2212 t \u00b2 t \u00b2 t \u00b2 t \u2212 t \u00b2 t \u00b2 t \u2212 t \u00b2 t \u2212 t \u00b2 t \u00b2 t (11) SD1: The above model implies a hard orthogonal context condition on the context vector (d \u00b2 t).We also propose a relaxed version of the above model that uses a gating parameter, which determines which fraction of the previous context vector vector should be subtracted."}, {"heading": "5 Baseline Methods", "text": "We compare with two recently proposed methods of base diversity (Chen et al., 2016) as described below. Note that these methods were proposed in the context of abstract summary (non-query-based abstract summary), and we adapt them to the task of the query-based abstract summary. Below, we highlight only the most important differences to our model in calculating the context vector d \u00b2 t passed to the decoder. M1: This model collects all previous context vectors as \"t \u2212 1 j = 1 d \u00b2 j\" and takes this story into account, while calculating a multiple context vector: d \u00b2 t = tanh (Wcdt \u2212 Uc t \u2212 1 x j = 1 d \u00b2 j) (15), where Wc, Uc \u00b2 Rl4 \u00b7 l4 are diagonal matrices. We then use this diversity-driven context d \u00b2 t in Equation (9) and (10 \u2212 1) M2 are in addition to a G4 context (in this model)."}, {"heading": "6 Experimental Setup", "text": "We evaluate our models based on the data set described in Section 3. Note that there is no previous baseline based on query abstract summary \u2022 64 described, so we can only compare with different variations of encoder decoder models as described above. Further, we compare our diversity based attention models with existing models for diversity by adapting them appropriately to this problem, as described above. In particular, we compare the performance of the following models: \u2022 Vanilla e-a-d: This is the Vanilla Encoderattention decoder decoder model adapted to the problem of abstract summary. It contains the following components (i) Document encoder (ii) Document attention model (iii) Decoder decoder decoder it does not contain an encoder attention model for the query. This helps us understand the meaning of the query. \u2022 Queryenc: This model contains the query encoder query 1 in addition to the three components used in the vanilla model above."}, {"heading": "7 Discussions", "text": "In this section we discuss the results of the experiments mentioned in Table 3. 1. Effect of the query: When comparing rows 1 and 2, we find that adding an encoder to the query and influencing the results of the decoder associated with it actually improves performance, which is expected because the query contains some keywords that could help sharpen the focus of the summary. 2. Effect of the query attention model: When comparing rows 2 and 3, we find that using an attention model to dynamically calculate the query presentation improves the results at each time step, suggesting that the attention model actually learns to focus on relevant parts of the query at different time steps. 3. Effect of the diversity models: All diversity models presented in the treatise (lines 7, 8, 9, 10) give significant improvements over the non-diversity models. Specifically, the modified LM-based diversity model leads to the best results."}, {"heading": "8 Conclusion", "text": "The unique feature of the model is a novel diversification mechanism based on successive orthogonalization, which gives us the flexibility to (i) provide different context vectors in successive time steps, and (ii) pay repeated attention to words later in the summary if necessary (as opposed to existing models that aggressively erase the history); and we have introduced a new dataset and empirically verified that we perform significantly better (gain of 28% (absolute) in the ROUGE L score) than when applying a pure encode-attend decoding mechanism to this problem; we note that adding an attention mechanism brings significant improvements to the query chain; we also compare it with a state-of-the-art diversity model and are currently working to extend it to dialog systems and general summaries (gain of 7% (absolute) in the ROUGE score), which is generally diversifiable enough with other suggested tasks."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Distraction-based neural networks for modeling documents", "author": ["Qian Chen", "Xiaodan Zhu", "Zhenhua Ling", "Si Wei", "Hui Jiang."], "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI-16). pages 2754\u20132760.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "author": ["Sumit Chopra", "Michael Auli", "Alexander M Rush", "SEAS Harvard."], "venue": "Proceedings of NAACL-HLT16 pages 93\u201398.", "citeRegEx": "Chopra et al\\.,? 2016", "shortCiteRegEx": "Chopra et al\\.", "year": 2016}, {"title": "Heads: Headline generation as sequence prediction using an abstract feature-rich space", "author": ["Carlos A Colmenares", "Marina Litvak", "Amin Mantrach", "Fabrizio Silvestri."], "venue": "HLT-NAACL. pages 133\u2013142.", "citeRegEx": "Colmenares et al\\.,? 2015", "shortCiteRegEx": "Colmenares et al\\.", "year": 2015}, {"title": "A survey on automatic text summarization", "author": ["Dipanjan Das", "Andr\u00e9 FT Martins."], "venue": "Literature Survey for the Language and Statistics II course at CMU 4:192\u2013195.", "citeRegEx": "Das and Martins.,? 2007", "shortCiteRegEx": "Das and Martins.", "year": 2007}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G\u00fcnes Erkan", "Dragomir R Radev."], "venue": "Journal of Artificial Intelligence Research 22:457\u2013479.", "citeRegEx": "Erkan and Radev.,? 2004", "shortCiteRegEx": "Erkan and Radev.", "year": 2004}, {"title": "Overcoming the lack of parallel data in sentence compression", "author": ["Katja Filippova", "Yasemin Altun."], "venue": "EMNLP. Citeseer, pages 1481\u20131491.", "citeRegEx": "Filippova and Altun.,? 2013", "shortCiteRegEx": "Filippova and Altun.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Lcsts: A large scale chinese short text summarization dataset", "author": ["Baotian Hu", "Qingcai Chen", "Fangze Zhu."], "venue": "arXiv preprint arXiv:1506.05865 .", "citeRegEx": "Hu et al\\.,? 2015", "shortCiteRegEx": "Hu et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "A persona-based neural conversation model", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Georgios P Spithourakis", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint arXiv:1603.06155 .", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Generating news headlines with recurrent neural networks", "author": ["Konstantin Lopyrev."], "venue": "arXiv preprint arXiv:1512.01712 .", "citeRegEx": "Lopyrev.,? 2015", "shortCiteRegEx": "Lopyrev.", "year": 2015}, {"title": "Automatic summarization, volume 3", "author": ["Inderjeet Mani."], "venue": "John Benjamins Publishing.", "citeRegEx": "Mani.,? 2001", "shortCiteRegEx": "Mani.", "year": 2001}, {"title": "Abstractive text summarization using sequence-to-sequence rnns and beyond", "author": ["Ramesh Nallapati", "Bowen Zhou", "Caglar Gulcehre", "Bing Xiang"], "venue": null, "citeRegEx": "Nallapati et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "A survey of text summarization techniques", "author": ["Ani Nenkova", "Kathleen McKeown."], "venue": "Mining text data, Springer, pages 43\u201376.", "citeRegEx": "Nenkova and McKeown.,? 2012", "shortCiteRegEx": "Nenkova and McKeown.", "year": 2012}, {"title": "Automatic text summarization using a machine learning approach", "author": ["Joel Larocca Neto", "Alex A Freitas", "Celso AA Kaestner."], "venue": "Brazilian Symposium on Artificial Intelligence. Springer, pages 205\u2013215.", "citeRegEx": "Neto et al\\.,? 2002", "shortCiteRegEx": "Neto et al\\.", "year": 2002}, {"title": "Self reinforcement for important passage retrieval", "author": ["Ricardo Ribeiro", "Lu\u0131\u0301s Marujo", "David Martins de Matos", "Joao P Neto", "Anatole Gershman", "Jaime Carbonell"], "venue": "In Proceedings of the 36th international ACM SIGIR conference on Research and", "citeRegEx": "Ribeiro et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ribeiro et al\\.", "year": 2013}, {"title": "Long story short\u2013global unsupervised models for keyphrase based meeting summarization", "author": ["Korbinian Riedhammer", "Benoit Favre", "Dilek Hakkani-T\u00fcr."], "venue": "Speech Communication 52(10):801\u2013 815.", "citeRegEx": "Riedhammer et al\\.,? 2010", "shortCiteRegEx": "Riedhammer et al\\.", "year": 2010}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1509.00685 .", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Temporal attention model for neural machine translation", "author": ["Baskaran Sankaran", "Haitao Mi", "Yaser Al-Onaizan", "Abe Ittycheriah."], "venue": "arXiv preprint arXiv:1608.02927 .", "citeRegEx": "Sankaran et al\\.,? 2016", "shortCiteRegEx": "Sankaran et al\\.", "year": 2016}, {"title": "Bbn/umd at duc-2004: Topiary", "author": ["David Zajic", "Bonnie Dorr", "Richard Schwartz"], "venue": null, "citeRegEx": "Zajic et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zajic et al\\.", "year": 2004}, {"title": "Comparing the roles of textual, acoustic and spoken-language features on spontaneous-conversation summarization", "author": ["Xiaodan Zhu", "Gerald Penn."], "venue": "Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume:", "citeRegEx": "Zhu and Penn.,? 2006", "shortCiteRegEx": "Zhu and Penn.", "year": 2006}, {"title": "Summarizing multiple spoken documents: finding evidence from untranscribed audio", "author": ["Xiaodan Zhu", "Gerald Penn", "Frank Rudzicz."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint", "citeRegEx": "Zhu et al\\.,? 2009", "shortCiteRegEx": "Zhu et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Over the past few years neural models based on the encode-attend-decode (Bahdanau et al., 2014) paradigm have shown great success in various natural language generation (NLG) tasks such as machine translation (Bahdanau et al.", "startOffset": 72, "endOffset": 95}, {"referenceID": 0, "context": ", 2014) paradigm have shown great success in various natural language generation (NLG) tasks such as machine translation (Bahdanau et al., 2014), abstractive summarization ((Rush et al.", "startOffset": 121, "endOffset": 144}, {"referenceID": 18, "context": ", 2014), abstractive summarization ((Rush et al., 2015),(Nallapati et al.", "startOffset": 36, "endOffset": 55}, {"referenceID": 13, "context": ", 2015),(Nallapati et al., 2016)) dialog (Li et al.", "startOffset": 8, "endOffset": 32}, {"referenceID": 10, "context": ", 2016)) dialog (Li et al., 2016), etc.", "startOffset": 16, "endOffset": 33}, {"referenceID": 1, "context": "This problem has also been reported by (Chen et al., 2016) in the context of summarization and by (Sankaran et al.", "startOffset": 39, "endOffset": 58}, {"referenceID": 19, "context": ", 2016) in the context of summarization and by (Sankaran et al., 2016) in the context of machine translation.", "startOffset": 47, "endOffset": 70}, {"referenceID": 7, "context": "To account for the complete history (or all previous context vectors) we also propose an extension of this idea where we pass the sequence of context vectors through a LSTM (Hochreiter and Schmidhuber, 1997) and ensure that the current state produced by the LSTM is orthogonal to the history.", "startOffset": 173, "endOffset": 207}, {"referenceID": 12, "context": "Summarization has been studied in the context of text ((Mani, 2001), (Das and Martins, 2007), (Nenkova and McKeown, 2012)) as well as speech ((Zhu and Penn, 2006), (Zhu et al.", "startOffset": 55, "endOffset": 67}, {"referenceID": 4, "context": "Summarization has been studied in the context of text ((Mani, 2001), (Das and Martins, 2007), (Nenkova and McKeown, 2012)) as well as speech ((Zhu and Penn, 2006), (Zhu et al.", "startOffset": 69, "endOffset": 92}, {"referenceID": 14, "context": "Summarization has been studied in the context of text ((Mani, 2001), (Das and Martins, 2007), (Nenkova and McKeown, 2012)) as well as speech ((Zhu and Penn, 2006), (Zhu et al.", "startOffset": 94, "endOffset": 121}, {"referenceID": 21, "context": "Summarization has been studied in the context of text ((Mani, 2001), (Das and Martins, 2007), (Nenkova and McKeown, 2012)) as well as speech ((Zhu and Penn, 2006), (Zhu et al.", "startOffset": 142, "endOffset": 162}, {"referenceID": 22, "context": "Summarization has been studied in the context of text ((Mani, 2001), (Das and Martins, 2007), (Nenkova and McKeown, 2012)) as well as speech ((Zhu and Penn, 2006), (Zhu et al., 2009)).", "startOffset": 164, "endOffset": 182}, {"referenceID": 15, "context": "A vast majority of this work has focused on extractive summarization where the idea is to construct a summary by selecting the most relevant sentences from the document ((Neto et al., 2002), (Erkan and Radev, 2004), (Filippova and Altun, 2013), (Colmenares et al.", "startOffset": 170, "endOffset": 189}, {"referenceID": 5, "context": ", 2002), (Erkan and Radev, 2004), (Filippova and Altun, 2013), (Colmenares et al.", "startOffset": 9, "endOffset": 32}, {"referenceID": 6, "context": ", 2002), (Erkan and Radev, 2004), (Filippova and Altun, 2013), (Colmenares et al.", "startOffset": 34, "endOffset": 61}, {"referenceID": 3, "context": ", 2002), (Erkan and Radev, 2004), (Filippova and Altun, 2013), (Colmenares et al., 2015), (Riedhammer et al.", "startOffset": 63, "endOffset": 88}, {"referenceID": 17, "context": ", 2015), (Riedhammer et al., 2010), (Ribeiro et al.", "startOffset": 9, "endOffset": 34}, {"referenceID": 16, "context": ", 2010), (Ribeiro et al., 2013)).", "startOffset": 9, "endOffset": 31}, {"referenceID": 4, "context": "We refer the reader to (Das and Martins, 2007) and (Nenkova and McKeown, 2012) for an excellent survey of", "startOffset": 23, "endOffset": 46}, {"referenceID": 14, "context": "We refer the reader to (Das and Martins, 2007) and (Nenkova and McKeown, 2012) for an excellent survey of", "startOffset": 51, "endOffset": 78}, {"referenceID": 0, "context": "Recent research in abstractive summarization has focused on data driven neural models based on the encode-attend-decode paradigm (Bahdanau et al., 2014).", "startOffset": 129, "endOffset": 152}, {"referenceID": 18, "context": "For example, (Rush et al., 2015), report state of the art results on the GigaWord and DUC corpus using such a model.", "startOffset": 13, "endOffset": 32}, {"referenceID": 0, "context": "Recent research in abstractive summarization has focused on data driven neural models based on the encode-attend-decode paradigm (Bahdanau et al., 2014). For example, (Rush et al., 2015), report state of the art results on the GigaWord and DUC corpus using such a model. Similarly, the work of Lopyrev (2015) uses neural networks to generate news headline from short news stories.", "startOffset": 130, "endOffset": 309}, {"referenceID": 0, "context": "Recent research in abstractive summarization has focused on data driven neural models based on the encode-attend-decode paradigm (Bahdanau et al., 2014). For example, (Rush et al., 2015), report state of the art results on the GigaWord and DUC corpus using such a model. Similarly, the work of Lopyrev (2015) uses neural networks to generate news headline from short news stories. Chopra et al. (2016) extend the work of Rush et al.", "startOffset": 130, "endOffset": 402}, {"referenceID": 0, "context": "Recent research in abstractive summarization has focused on data driven neural models based on the encode-attend-decode paradigm (Bahdanau et al., 2014). For example, (Rush et al., 2015), report state of the art results on the GigaWord and DUC corpus using such a model. Similarly, the work of Lopyrev (2015) uses neural networks to generate news headline from short news stories. Chopra et al. (2016) extend the work of Rush et al. (2015) and report further improvements on the two datasets.", "startOffset": 130, "endOffset": 440}, {"referenceID": 0, "context": "Recent research in abstractive summarization has focused on data driven neural models based on the encode-attend-decode paradigm (Bahdanau et al., 2014). For example, (Rush et al., 2015), report state of the art results on the GigaWord and DUC corpus using such a model. Similarly, the work of Lopyrev (2015) uses neural networks to generate news headline from short news stories. Chopra et al. (2016) extend the work of Rush et al. (2015) and report further improvements on the two datasets. Hu et al. (2015) introduced a dataset for Chinese short text summarization and evaluated a similar RNN encoder-decoder model on it.", "startOffset": 130, "endOffset": 510}, {"referenceID": 0, "context": "Recent research in abstractive summarization has focused on data driven neural models based on the encode-attend-decode paradigm (Bahdanau et al., 2014). For example, (Rush et al., 2015), report state of the art results on the GigaWord and DUC corpus using such a model. Similarly, the work of Lopyrev (2015) uses neural networks to generate news headline from short news stories. Chopra et al. (2016) extend the work of Rush et al. (2015) and report further improvements on the two datasets. Hu et al. (2015) introduced a dataset for Chinese short text summarization and evaluated a similar RNN encoder-decoder model on it. One recurring problem in encoder-decoder models for NLG is that they often repeat the same phrase/word multiple times in the summary (at the cost of both coherency and fluency). Sankaran et al. (2016) study this problem in the context of MT and propose a temporal attention model which enforces the attention weights for successive time steps to be different from each other.", "startOffset": 130, "endOffset": 826}, {"referenceID": 0, "context": "Recent research in abstractive summarization has focused on data driven neural models based on the encode-attend-decode paradigm (Bahdanau et al., 2014). For example, (Rush et al., 2015), report state of the art results on the GigaWord and DUC corpus using such a model. Similarly, the work of Lopyrev (2015) uses neural networks to generate news headline from short news stories. Chopra et al. (2016) extend the work of Rush et al. (2015) and report further improvements on the two datasets. Hu et al. (2015) introduced a dataset for Chinese short text summarization and evaluated a similar RNN encoder-decoder model on it. One recurring problem in encoder-decoder models for NLG is that they often repeat the same phrase/word multiple times in the summary (at the cost of both coherency and fluency). Sankaran et al. (2016) study this problem in the context of MT and propose a temporal attention model which enforces the attention weights for successive time steps to be different from each other. Similarly, and more relevant to this work, Chen et al. (2016) propose a distraction based attention model which maintains a history of attention vectors and context vectors.", "startOffset": 130, "endOffset": 1063}, {"referenceID": 1, "context": "We compare with two recently proposed baseline diversity methods (Chen et al., 2016) as described below.", "startOffset": 65, "endOffset": 84}, {"referenceID": 9, "context": "We used Adam (Kingma and Ba, 2014) as the optimization algorithm with the initial learning rate set to 0.", "startOffset": 13, "endOffset": 34}], "year": 2017, "abstractText": "Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encodeattend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores.ive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encodeattend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores.", "creator": "LaTeX with hyperref package"}}}