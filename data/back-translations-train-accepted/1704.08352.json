{"id": "1704.08352", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "From Characters to Words to in Between: Do We Capture Morphology?", "abstract": "Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams. While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies. On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled. Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses, even when learned from an order of magnitude more data.", "histories": [["v1", "Wed, 26 Apr 2017 21:10:53 GMT  (90kb,D)", "http://arxiv.org/abs/1704.08352v1", "Accepted at ACL 2017"]], "COMMENTS": "Accepted at ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["clara vania", "adam lopez"], "accepted": true, "id": "1704.08352"}, "pdf": {"name": "1704.08352.pdf", "metadata": {"source": "CRF", "title": "From Characters to Words to in Between: Do We Capture Morphology?", "authors": ["Clara Vania", "Adam Lopez"], "emails": ["c.vania@ed.ac.uk,", "alopez@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "2 Morphological Typology", "text": "A morpheme is the smallest unit of meaning in a word. Some morphemes express the core meaning (roots), while others express one or more dependent features of the core meaning, such as person, gender, or aspect. Morphological analysis identifies the lemmas and characteristics of a word. A morpheme is the surface realization of a morpheme (Morley, 2000) that can vary from word to word. These distinctions are listed in Table 1. Morphological Typology Languages based on the processes by which morphemes are composed to form words. While most languages have a variety of such processes, some processes are much more common than others, and we will largely identify our experimental languages with these processes. When morphemes are combined sequentially, morphology is related. However, morphemes can also be composed by non-concentrating processes."}, {"heading": "3 Representation Models", "text": "We compare ten different models, different subword units and composition functions commonly used in recent works but evaluated on the basis of different tasks (Table 2). With the word w, we calculate its representation w as follows: w = f (Ws, \u03c3 (w)) (1), where \u03c3 is a deterministic function that returns a sequence of subword units; Ws is a parameter matrix of representations for the vocabulary of subsubject units; and f is a composition function that takes \u03c3 (w) and Ws as input and w. All representations we consider take this form and vary only in f and \u03c3."}, {"heading": "3.1 Subword Units", "text": "We look at four variants of \u03c3 in Equation 1, each of which returns a different type of subword unit: characters, cartograms, or one of two types of morphs. We obtain morphs from Morfessor (Smit et al., 2014) or word segmentation based on Byte Pair Encoding (BPE; Gage (1994)), which has proven effective for handling rare words in neural machine translation (Sennrich et al., 2016). BPE works by iteratively replacing frequent pairs of characters with a single unused character. For Morfessor, we use default parameters, while for BPE we set the number of merge operations to 10,000.1. When we segment into cartograms, we look at all trigrams in the word, including those covering the ideal beginning and end of word characters, as in Sperr et al. (2013)."}, {"heading": "3.2 Composition Functions", "text": "We use three variants of f in Eq. 1. The first constructs the representation w of the word w of adding1BPE taking a single parameter: the number of merge operations. We have tried different parameter values (1k, 10k, 100k) and examined the resulting segmentation on the English dataset manually. Qualitatively, 10k has given the most plausible segmentation and we have used this setting across all languages. the representations of its subwords s1,.., sn = \u03c3 (w), where the representation of the si-character vector si.w = n-character i = 1si (2) The only subword unit we do not compose by characters, as they produce the same representation for many different words.Our second composition function is a bi-LSTM, which we adapt at the character level of the model of Ling et al."}, {"heading": "3.3 Language Model", "text": "We use language models (LM) because they are simple and fundamental for many NLP applications. Considering a text sequence s = w1,..., wT, our LM calculates the probability of s as follows: P (w1,..., wT) = T-t = 1 P (yt | w1,..., wt-1) (8), where yt = wt if wt is in the output vocabulary and yt = UNK otherwise. Our language model is an LSTM variant of the recurring neural network language (RNN) LM (Mikolov et al., 2010). At step t, it receives input wt and predicts yt + 1. With equation 1, it first calculates the representation wt of wt. In view of this representation and the previous state, ht \u2212 1, it results in a new state ht \u2212 1 and forecasts yt = LSTM (wt \u2212 1)."}, {"heading": "4 Experiments", "text": "We conduct experiments in ten languages (Table 4). We use data sets from Ling et al. (2015) for English and Turkish. For Czech and Russian, we use Universal Dependencies (UD) v1.3 (Nivre et al., 2015). For other languages, we use pre-edited Wikipedia data (Al-Rfou et al., 2013).2 For each data set, we use about 1.2 million tokens to train, and about 150,000 tokens for each development and test. Pre-processing involves reducing (except for character models) and removing hyperlinks. To ensure that we compare models and not implementations, we implement all models in a single framework using tensorflow (Abadi et al., 2015).3 We use a common setup for all experiments based on Ling et al. (2015), Kim et al. (2016) and Miyamoto and Cho."}, {"heading": "4.1 Training and Evaluation", "text": "Our LSTM-LM uses two hidden layers with 200 hidden units and display vectors for words, characters and morphs, all of which have dimensions of 200. All parameters are randomly initialized from -0.1 to 0.1, trained by stochastic gradient descent with a minibatch size of 32, time steps of 20, for 50 epochs. To avoid overmatching, we apply the failure rate with a 0.5 probability to the input layer and all LSTM cells (including those in the Bi-LSTM, if used). For models that do not use Bi-LSTM composition, we start with a learning rate of 1.0 and reduce it by half if the validation perplexity does not decrease by 0.1 after 3 epochs. For models with Bi-LSTM composition, we use a constant learning rate of 0.2 and stop training if the validation perplexity does not improve after 3 epochs."}, {"heading": "5 Results and Analysis", "text": "In six out of ten languages, the character trigrams composed with Bi-LSTMs achieve the least confusion. As far as we know, this particular model has not yet been tested, although it is similar (but more general than) to the Sperr et al. (2013) model. We also observe that BPE always performs better than morfessor, even for agglutinative languages. We are now turning to a more detailed analysis by morphological typology. Fusion languages composed with Bi-LSTMs outperform all other models, especially for Czech and Russian (up to 20%), which is not surprising as both are morphologically richer than English.Agglutinative languages. We observe different results for each language."}, {"heading": "5.1 Effects of Morphological Analysis", "text": "In the experiments above, we used unattended morphological segmentation as a proxy for morphological analysis (Table 3). However, as discussed in Section 2, this is fairly approximate, so it is natural to wonder what would happen if we had true morphological analysis. If character-level models are strong enough to capture the effects of morphology, then they should have the predictive accuracy of a model with access to that analysis. To find out, we conducted an oracle experiment in which the human-annotated morphological analyses are used in the UD datasets for Czech and Russian, the only languages in our set for which these analyses were available. In these experiments, we treat the lemma and each morphological characteristic as a subword unit.The results (Table 6) show that the bi-LSTM composition of these representations exceeds the allothier models for both languages."}, {"heading": "5.2 Automatic Morphological Analysis", "text": "The oracle experiments show promising results when we annotated data. However, because these annotations are expensive, we also investigated the use of automatic morphological analysis. We obtained analyses for Arabic with the MADAMIRA (Pasha et al., 2014).4 As in the annotation experiment, we treated each morphological feature as a subordinate unit. The resulting confusion of 71.94 and 42.85 for addition and bi-LSTMs, respectively, is worse than that with cartoons (39.87), although they approach the best confusions. 4 We experimented only with Arabic, since MADAMIRA refutes words in contexts; most of the other analyzers we found did not and would require additional work to clarify them."}, {"heading": "5.3 Targeted Perplexity Results", "text": "One difficulty in interpreting the results of Table 5 with respect to specific morphological processes is that helplessness is measured for all words. But these processes do not apply to all words, so it may be that the effects of certain morphological processes are washed out. To get a clearer picture, we have measured helplessness for only certain subsets of words in our test data: specifically, since target word wi, we measure helplessness of the word wi + 1. In other words, we analyze the confusion when the curved words are of interest in recent history, taking advantage of the recursion tendency of our LSTM-LM. This is the perplexity that is most likely to be strongly influenced by different representations, as we do not vary representations of the predicted word itself. We consider several cases: nouns and verbs in the Czech Republic and Russia, where word classes can be identified from annotations, and reductions in Indonesia."}, {"heading": "5.4 Qualitative Analysis", "text": "Table 11 represents the closest neighbors with cosinal similarity for words that occur in the vocabulary, rarely and outside of the 5We use Indonesian appendices listed in Larasati et al. (2011). [6] For common words, standard word embeddings are clearly superior in terms of lexical meaning. Character and morph representations tend to find words that are orthographically similar, suggesting that they are more modelable than root morphems. The same pattern applies to rare words and OOV words. We suspect that the subword models outperform words in language modeling because they use affixes to signal the word class. Similar patterns are found in Japan.We analyze reduplication by asking for reduced words to find their closest neighbors using the BPE-bi-LSTM model. If the model were sensitive to reductions, we would expect to see the next morphological neighbor among its morphological variants."}, {"heading": "6 Conclusion", "text": "We presented a systematic comparison of word representation models with different morphological awareness between languages with different morphological typologies. Our results confirm previous findings that character-level models are effective for many languages, but these models do not match the predictive accuracy of the model with explicit knowledge of morphology, even after we have increased the size of training data tenfold. Furthermore, our qualitative analyses suggest that they learn orthographic similarities from appendices and lose the meaning of root morphology. Although morphological analyses 6https: / / radimrehurek.com / gensim / are available in limited quantities, our results suggest that there may be benefits in semi-supervised learning from partially commented data. Across languages with different typologies, our experiments show that the subordination unit models are most effective for agglutinative languages. However, these results do not generalize to all languages, as factors such as morphology and orthography influence the usefulness of these representations."}, {"heading": "Acknowledgments", "text": "Clara Vania is supported by the Indonesian Endowment Fund for Education (LPDP), the Centre for Doctoral Training in Data Science, funded by the UK EPSRC (grant EP / L016427 / 1), and the University of Edinburgh. We thank Sameer Bansal, Toms Bergmanis, Marco Damonte, Federico Fancellu, Sorcha Gilroy, Sharon Goldwater, Frank Keller, Mirella Lapata, Felicia Liu, Jonathan Mallinson, Joana Ribeiro, Naomi Saphra, Ida Szubert and the anonymous reviewers for the helpful discussion of this work and comments on earlier drafts of the paper."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org", "author": ["sudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "sudevan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "sudevan et al\\.", "year": 2015}, {"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning. Association for Computa-", "citeRegEx": "Al.Rfou et al\\.,? 2013", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax", "author": ["Emily M. Bender."], "venue": "Morgan & Claypool Publishers.", "citeRegEx": "Bender.,? 2013", "shortCiteRegEx": "Bender.", "year": 2013}, {"title": "Enriching word vectors with subword information", "author": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."], "venue": "CoRR abs/1607.04606. http://arxiv.org/abs/1607.04606.", "citeRegEx": "Bojanowski et al\\.,? 2016", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2016}, {"title": "Compositional Morphology for Word Representations and Language Modeling", "author": ["Jan A. Botha", "Phil Blunsom."], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML). Beijing, China.", "citeRegEx": "Botha and Blunsom.,? 2014", "shortCiteRegEx": "Botha and Blunsom.", "year": 2014}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa-", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Morphological word-embeddings", "author": ["Ryan Cotterell", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Com-", "citeRegEx": "Cotterell and Sch\u00fctze.,? 2015", "shortCiteRegEx": "Cotterell and Sch\u00fctze.", "year": 2015}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computa-", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "A new algorithm for data compression", "author": ["Philip Gage."], "venue": "C Users J. 12(2):23\u201338. http://dl.acm.org/citation.cfm?id=177910.177914.", "citeRegEx": "Gage.,? 1994", "shortCiteRegEx": "Gage.", "year": 1994}, {"title": "Multilingual language processing from bytes", "author": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Gillick et al\\.,? 2016", "shortCiteRegEx": "Gillick et al\\.", "year": 2016}, {"title": "Bidirectional lstm networks for improved phoneme classification and recognition", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "J\u00fcrgen Schmidhuber."], "venue": "Proceedings of the 15th International Conference on Artificial Neu-", "citeRegEx": "Graves et al\\.,? 2005", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Understanding Morphology", "author": ["Martin Haspelmath."], "venue": "Understanding Language Series. Arnold, London, second edition.", "citeRegEx": "Haspelmath.,? 2010", "shortCiteRegEx": "Haspelmath.", "year": 2010}, {"title": "An extensive empirical evaluation of character-based morphological tagging for 14 languages", "author": ["Georg Heigold", "Guenter Neumann", "Josef van Genabith."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Com-", "citeRegEx": "Heigold et al\\.,? 2017", "shortCiteRegEx": "Heigold et al\\.", "year": 2017}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Comput. 9(8):1735\u2013 1780. https://doi.org/10.1162/neco.1997.9.8.1735.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, Association for Computational Linguistics, chapter MED: The LMU", "author": ["Katharina Kann", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Kann and Sch\u00fctze.,? \\Q2016\\E", "shortCiteRegEx": "Kann and Sch\u00fctze.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander Rush."], "venue": "Proceedings of the 2016 Conference on Artificial Intelligence (AAAI).", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Indonesian Morphology Tool (MorphInd): Towards an Indonesian Corpus, Springer Berlin Heidelberg, Berlin, Heidelberg, pages 119\u2013 129", "author": ["Septina Dian Larasati", "Vladislav Kubo\u0148", "Daniel Zeman."], "venue": "https://doi.org/10.1007/978-3-642-23138-4 8.", "citeRegEx": "Larasati et al\\.,? 2011", "shortCiteRegEx": "Larasati et al\\.", "year": 2011}, {"title": "Fully character-level neural machine translation without explicit segmentation", "author": ["Jason Lee", "Kyunghyun Cho", "Thomas Hofmann."], "venue": "CoRR abs/1610.03017. http://arxiv.org/abs/1610.03017.", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis."], "venue": "Proceedings of the", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Thang Luong", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning. Association for", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Comput. Linguist. 19(2):313\u2013330. http://dl.acm.org/citation.cfm?id=972470.972475.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["Tom\u00e1\u0161 Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur."], "venue": "Proceedings of the 11th Annual Conference of the International Speech Communication", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Gated word-character recurrent language model", "author": ["Yasumasa Miyamoto", "Kyunghyun Cho."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Lin-", "citeRegEx": "Miyamoto and Cho.,? 2016", "shortCiteRegEx": "Miyamoto and Cho.", "year": 2016}, {"title": "Syntax in Functional Grammar: An Introduction to Lexicogrammar in Systemic Linguistics", "author": ["G. David Morley."], "venue": "Continuum.", "citeRegEx": "Morley.,? 2000", "shortCiteRegEx": "Morley.", "year": 2000}, {"title": "Madamira: A fast, comprehensive tool for morphological analysis and disambiguation of ara", "author": ["Arfath Pasha", "Mohamed Al-Badrashiny", "Mona Diab", "Ahmed El Kholy", "Ramy Eskander", "Nizar Habash", "Manoj Pooleery", "Owen Rambow", "Ryan Roth"], "venue": null, "citeRegEx": "Pasha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pasha et al\\.", "year": 2014}, {"title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "author": ["Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Plank et al\\.,? 2016", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Co-learning of word representations and morpheme representations", "author": ["Siyu Qiu", "Qing Cui", "Jiang Bian", "Bin Gao", "TieYan Liu."], "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers.", "citeRegEx": "Qiu et al\\.,? 2014", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "Attending to characters in neural sequence labeling models", "author": ["Marek Rei", "Gamal Crichton", "Sampo Pyysalo."], "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016", "citeRegEx": "Rei et al\\.,? 2016", "shortCiteRegEx": "Rei et al\\.", "year": 2016}, {"title": "Computational Approach to Morphology and Syntax", "author": ["Brian Roark", "Richard Sproat."], "venue": "Oxford University Press.", "citeRegEx": "Roark and Sproat.,? 2007", "shortCiteRegEx": "Roark and Sproat.", "year": 2007}, {"title": "Learning character-level representations for partof-speech tagging", "author": ["Cicero Dos Santos", "Bianca Zadrozny."], "venue": "Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine Learning. PMLR,", "citeRegEx": "Santos and Zadrozny.,? 2014", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Morfessor 2.0: Toolkit", "author": ["Mikko Kurimo"], "venue": null, "citeRegEx": "Kurimo.,? \\Q2014\\E", "shortCiteRegEx": "Kurimo.", "year": 2014}], "referenceMentions": [{"referenceID": 20, "context": "word units obtained from morphological segmentation or analysis (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Sch\u00fctze, 2015).", "startOffset": 64, "endOffset": 138}, {"referenceID": 4, "context": "word units obtained from morphological segmentation or analysis (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Sch\u00fctze, 2015).", "startOffset": 64, "endOffset": 138}, {"referenceID": 7, "context": "word units obtained from morphological segmentation or analysis (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Sch\u00fctze, 2015).", "startOffset": 64, "endOffset": 138}, {"referenceID": 19, "context": "Since this variation is limited, the general relationship between morphology and orthography can be exploited by composing the representations of characters (Ling et al., 2015; Kim et al., 2016), character n-grams (Sperr et al.", "startOffset": 157, "endOffset": 194}, {"referenceID": 16, "context": "Since this variation is limited, the general relationship between morphology and orthography can be exploited by composing the representations of characters (Ling et al., 2015; Kim et al., 2016), character n-grams (Sperr et al.", "startOffset": 157, "endOffset": 194}, {"referenceID": 3, "context": ", 2016), character n-grams (Sperr et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016; Botha and Blunsom, 2014), bytes (Plank et al.", "startOffset": 27, "endOffset": 119}, {"referenceID": 4, "context": ", 2016), character n-grams (Sperr et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016; Botha and Blunsom, 2014), bytes (Plank et al.", "startOffset": 27, "endOffset": 119}, {"referenceID": 26, "context": ", 2016; Botha and Blunsom, 2014), bytes (Plank et al., 2016; Gillick et al., 2016), or combinations thereof (Santos and Zadrozny, 2014; Qiu et al.", "startOffset": 40, "endOffset": 82}, {"referenceID": 10, "context": ", 2016; Botha and Blunsom, 2014), bytes (Plank et al., 2016; Gillick et al., 2016), or combinations thereof (Santos and Zadrozny, 2014; Qiu et al.", "startOffset": 40, "endOffset": 82}, {"referenceID": 30, "context": ", 2016), or combinations thereof (Santos and Zadrozny, 2014; Qiu et al., 2014).", "startOffset": 33, "endOffset": 78}, {"referenceID": 27, "context": ", 2016), or combinations thereof (Santos and Zadrozny, 2014; Qiu et al., 2014).", "startOffset": 33, "endOffset": 78}, {"referenceID": 2, "context": "The last question is raised by Bender (2013): languages are typologically diverse, and the behavior of a model on one language may not generalize to others.", "startOffset": 31, "endOffset": 45}, {"referenceID": 24, "context": "A morph is the surface realization of a morpheme (Morley, 2000), which may vary from word to word.", "startOffset": 49, "endOffset": 63}, {"referenceID": 12, "context": "For example (Haspelmath, 2010):", "startOffset": 12, "endOffset": 30}, {"referenceID": 29, "context": "For example, the Arabic root ktb (\u201cwrite\u201d) produces (Roark and Sproat, 2007): katab \u201cwrote\u201d (Arabic)", "startOffset": 52, "endOffset": 76}, {"referenceID": 31, "context": ", 2014) or a word segmentation based on Byte Pair Encoding (BPE; Gage (1994)), which has been shown to be effective for handling rare words in neural machine translation (Sennrich et al., 2016).", "startOffset": 170, "endOffset": 193}, {"referenceID": 9, "context": ", 2014) or a word segmentation based on Byte Pair Encoding (BPE; Gage (1994)), which has been shown to be effective for handling rare words in neural machine translation (Sennrich et al.", "startOffset": 65, "endOffset": 77}, {"referenceID": 14, "context": "Given si and the previous LSTM hidden state hi\u22121, an LSTM (Hochreiter and Schmidhuber, 1997) computes the following outputs for the subword at position i:", "startOffset": 58, "endOffset": 92}, {"referenceID": 18, "context": "Our second composition function is a bidirectional long-short-term memory (bi-LSTM), which we adapt based on its use in the characterlevel model of Ling et al. (2015) and its widespread use in NLP generally.", "startOffset": 148, "endOffset": 167}, {"referenceID": 11, "context": "A bi-LSTM (Graves et al., 2005) combines the", "startOffset": 10, "endOffset": 31}, {"referenceID": 16, "context": "The third composition function is a convolutional neural network (CNN) with highway layers, as in Kim et al. (2016). Let c1, .", "startOffset": 98, "endOffset": 116}, {"referenceID": 12, "context": "(2013) words, character n-grams addition Luong et al. (2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al.", "startOffset": 41, "endOffset": 61}, {"referenceID": 3, "context": "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al.", "startOffset": 39, "endOffset": 64}, {"referenceID": 3, "context": "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition Santos and Zadrozny (2014) words, characters CNN Cotterell and Sch\u00fctze (2015) words, morphological analyses addition Sennrich et al.", "startOffset": 39, "endOffset": 117}, {"referenceID": 3, "context": "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition Santos and Zadrozny (2014) words, characters CNN Cotterell and Sch\u00fctze (2015) words, morphological analyses addition Sennrich et al.", "startOffset": 39, "endOffset": 179}, {"referenceID": 3, "context": "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition Santos and Zadrozny (2014) words, characters CNN Cotterell and Sch\u00fctze (2015) words, morphological analyses addition Sennrich et al.", "startOffset": 39, "endOffset": 230}, {"referenceID": 3, "context": "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition Santos and Zadrozny (2014) words, characters CNN Cotterell and Sch\u00fctze (2015) words, morphological analyses addition Sennrich et al. (2016) morphs (BPE) none Kim et al.", "startOffset": 39, "endOffset": 292}, {"referenceID": 3, "context": "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition Santos and Zadrozny (2014) words, characters CNN Cotterell and Sch\u00fctze (2015) words, morphological analyses addition Sennrich et al. (2016) morphs (BPE) none Kim et al. (2016) characters CNN Ling et al.", "startOffset": 39, "endOffset": 328}, {"referenceID": 3, "context": "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition Santos and Zadrozny (2014) words, characters CNN Cotterell and Sch\u00fctze (2015) words, morphological analyses addition Sennrich et al. (2016) morphs (BPE) none Kim et al. (2016) characters CNN Ling et al. (2015) characters bi-LSTM Wieting et al.", "startOffset": 39, "endOffset": 362}, {"referenceID": 3, "context": "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition Santos and Zadrozny (2014) words, characters CNN Cotterell and Sch\u00fctze (2015) words, morphological analyses addition Sennrich et al. (2016) morphs (BPE) none Kim et al. (2016) characters CNN Ling et al. (2015) characters bi-LSTM Wieting et al. (2016) character n-grams addition Bojanowski et al.", "startOffset": 39, "endOffset": 403}, {"referenceID": 3, "context": "(2016) character n-grams addition Bojanowski et al. (2016) character n-grams addition Vylomova et al.", "startOffset": 34, "endOffset": 59}, {"referenceID": 3, "context": "(2016) character n-grams addition Bojanowski et al. (2016) character n-grams addition Vylomova et al. (2016) characters, morphs (Morfessor) bi-LSTM, CNN Miyamoto and Cho (2016) words, characters bi-LSTM Rei et al.", "startOffset": 34, "endOffset": 109}, {"referenceID": 3, "context": "(2016) character n-grams addition Bojanowski et al. (2016) character n-grams addition Vylomova et al. (2016) characters, morphs (Morfessor) bi-LSTM, CNN Miyamoto and Cho (2016) words, characters bi-LSTM Rei et al.", "startOffset": 34, "endOffset": 177}, {"referenceID": 3, "context": "(2016) character n-grams addition Bojanowski et al. (2016) character n-grams addition Vylomova et al. (2016) characters, morphs (Morfessor) bi-LSTM, CNN Miyamoto and Cho (2016) words, characters bi-LSTM Rei et al. (2016) words, characters bi-LSTM Lee et al.", "startOffset": 34, "endOffset": 221}, {"referenceID": 3, "context": "(2016) character n-grams addition Bojanowski et al. (2016) character n-grams addition Vylomova et al. (2016) characters, morphs (Morfessor) bi-LSTM, CNN Miyamoto and Cho (2016) words, characters bi-LSTM Rei et al. (2016) words, characters bi-LSTM Lee et al. (2016) characters CNN Kann and Sch\u00fctze (2016) characters, morphological analyses none Heigold et al.", "startOffset": 34, "endOffset": 265}, {"referenceID": 3, "context": "(2016) character n-grams addition Bojanowski et al. (2016) character n-grams addition Vylomova et al. (2016) characters, morphs (Morfessor) bi-LSTM, CNN Miyamoto and Cho (2016) words, characters bi-LSTM Rei et al. (2016) words, characters bi-LSTM Lee et al. (2016) characters CNN Kann and Sch\u00fctze (2016) characters, morphological analyses none Heigold et al.", "startOffset": 34, "endOffset": 304}, {"referenceID": 3, "context": "(2016) character n-grams addition Bojanowski et al. (2016) character n-grams addition Vylomova et al. (2016) characters, morphs (Morfessor) bi-LSTM, CNN Miyamoto and Cho (2016) words, characters bi-LSTM Rei et al. (2016) words, characters bi-LSTM Lee et al. (2016) characters CNN Kann and Sch\u00fctze (2016) characters, morphological analyses none Heigold et al. (2017) words, characters bi-LSTM, CNN", "startOffset": 34, "endOffset": 366}, {"referenceID": 22, "context": "Our language model is an LSTM variant of recurrent neural network language (RNN) LM (Mikolov et al., 2010).", "startOffset": 84, "endOffset": 106}, {"referenceID": 1, "context": "For other languages, we use preprocessed Wikipedia data (Al-Rfou et al., 2013).", "startOffset": 56, "endOffset": 78}, {"referenceID": 18, "context": "We use datasets from Ling et al. (2015) for English and Turkish.", "startOffset": 21, "endOffset": 40}, {"referenceID": 18, "context": "3 We use a common setup for all experiments based on that of Ling et al. (2015), Kim et al.", "startOffset": 61, "endOffset": 80}, {"referenceID": 16, "context": "(2015), Kim et al. (2016), and Miyamoto and Cho (2016).", "startOffset": 8, "endOffset": 26}, {"referenceID": 16, "context": "(2015), Kim et al. (2016), and Miyamoto and Cho (2016). In preliminary experiments, we confirmed that our models produced similar patterns of perplexities for the reimplemented word and character LSTM", "startOffset": 8, "endOffset": 55}, {"referenceID": 19, "context": "com/claravania/subword-lstm-lm models of Ling et al. (2015). Even following de-", "startOffset": 41, "endOffset": 60}, {"referenceID": 21, "context": "Our final model with biLSTMs composition follows Miyamoto and Cho (2016) as it gives us the same perplexity results for our preliminary experiments on the Penn Treebank dataset (Marcus et al., 1993), preprocessed by Mikolov et al.", "startOffset": 177, "endOffset": 198}, {"referenceID": 21, "context": "Our final model with biLSTMs composition follows Miyamoto and Cho (2016) as it gives us the same perplexity results for our preliminary experiments on the Penn Treebank dataset (Marcus et al.", "startOffset": 49, "endOffset": 73}, {"referenceID": 21, "context": "Our final model with biLSTMs composition follows Miyamoto and Cho (2016) as it gives us the same perplexity results for our preliminary experiments on the Penn Treebank dataset (Marcus et al., 1993), preprocessed by Mikolov et al. (2010).", "startOffset": 178, "endOffset": 238}, {"referenceID": 16, "context": "For the character CNN model, we use the same settings as the small model of Kim et al. (2016).", "startOffset": 76, "endOffset": 94}, {"referenceID": 19, "context": "To make our results comparable to Ling et al. (2015), for each language we limit the output vocabulary to the most frequent 5,000 training words plus an unknown word token.", "startOffset": 34, "endOffset": 53}, {"referenceID": 19, "context": "To make our results comparable to Ling et al. (2015), for each language we limit the output vocabulary to the most frequent 5,000 training words plus an unknown word token. To learn to predict unknown words, we follow Ling et al. (2015): in training, words that occur only once are stochastically replaced with the unknown token with probability 0.", "startOffset": 34, "endOffset": 237}, {"referenceID": 25, "context": "We obtained analyses for Arabic with the MADAMIRA (Pasha et al., 2014).", "startOffset": 50, "endOffset": 70}, {"referenceID": 17, "context": "We use Indonesian affixes listed in Larasati et al. (2011) Language type-level (%) token-level (%)", "startOffset": 36, "endOffset": 59}], "year": 2017, "abstractText": "Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams. While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies. On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled. Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses, even when learned from an order of magnitude more data.", "creator": "LaTeX with hyperref package"}}}