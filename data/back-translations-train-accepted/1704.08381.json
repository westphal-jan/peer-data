{"id": "1704.08381", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation", "abstract": "Sequence-to-sequence models have shown strong performance across a broad range of applications. However, their application to parsing and generating text usingAbstract Meaning Representation (AMR)has been limited, due to the relatively limited amount of labeled data and the non-sequential nature of the AMR graphs. We present a novel training procedure that can lift this limitation using millions of unlabeled sentences and careful preprocessing of the AMR graphs. For AMR parsing, our model achieves competitive results of 62.1SMATCH, the current best score reported without significant use of external semantic resources. For AMR generation, our model establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequence-based AMR models are robust against ordering variations of graph-to-sequence conversions.", "histories": [["v1", "Wed, 26 Apr 2017 23:53:34 GMT  (170kb,D)", "https://arxiv.org/abs/1704.08381v1", "Accepted in ACL 2017"], ["v2", "Fri, 9 Jun 2017 19:17:32 GMT  (509kb,D)", "http://arxiv.org/abs/1704.08381v2", "Accepted in ACL 2017"], ["v3", "Fri, 18 Aug 2017 11:28:05 GMT  (1332kb,D)", "http://arxiv.org/abs/1704.08381v3", "Accepted in ACL 2017"]], "COMMENTS": "Accepted in ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ioannis konstas", "srinivasan iyer", "mark yatskar", "yejin choi", "luke zettlemoyer"], "accepted": true, "id": "1704.08381"}, "pdf": {"name": "1704.08381.pdf", "metadata": {"source": "CRF", "title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation", "authors": ["Ioannis Konstas", "Srinivasan Iyer", "Mark Yatskar", "Yejin Choi", "Luke Zettlemoyer"], "emails": ["ikonstas@cs.washington.edu", "sviyer@cs.washington.edu", "my89@cs.washington.edu", "yejin@cs.washington.edu", "lsz@cs.washington.edu", "lukez@allenai.org"], "sections": [{"heading": null, "text": "Neural AMR: Sequence-to-Sequence Models for Parsing and GenerationIoannis Konstas \u2020 Srinivasan Iyer \u2020 Mark Yatskar \u2020 Yejin Choi \u2020 Luke Zettlemoyer \u2020 \u2021 \u2020 Paul G. Allen School of Computer Science & Engineering, Univ. of Washington, Seattle, WA {ikonstas, sviyer, my89, yejin, lsz} @ cs.washington.edu \u2021 Allen Institute for Artificial Intelligence, Seattle, WA lukez @ allenai.orgAbstractSequence-to-sequence models have shown strong performance in a wide range of applications, but their application to parsing and generating text using Abstract Meaning Representation (AMR) is limited due to the relatively limited amount of marked data and the non-sequential nature of AMR diagrams. We present a novel training method that can remove this limitation by pre-processing millions of unmarked sentences and pre-marked AMR diagrams."}, {"heading": "1 Introduction", "text": "In fact, it is such that most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to move, to fight, to move, to fight, to move, to fight, to fight, to move, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "2 Related Work", "text": "There are a number of approaches based on a series of alignments based on bilexical terms and handwritten rules. In contrast, our models are built directly on parallel corpora and make minimal use of alignments to anonymize named entities. Grammar-based Parsing Wang et al. (2016) (CAMR) perform a series of shift-reducing transformations on the output of an externally trained language. (2017), Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammatical approach with categorical categories."}, {"heading": "3 Methods", "text": "In this section, we will first explain the formal definition of AMR analysis and generation (Section 3.1), then the sequence-to-sequence models we use (Section 3.2), graph-to-sequence conversion (Section 3.3), and our paired training method (Section 3.4)."}, {"heading": "3.1 Tasks", "text": "The AMR is a rooted directed acylic graph. It contains nodes whose names correspond to sensually identified verbs, nouns, or AMR-specific concepts, e.g. elect.01, Obama, and Person in Figure 1. One of these nodes is a distinguishable root, e.g. the node, and in Figure 1. In addition, the graph contains labeled edges that correspond to the style of the PropBank (Palmer et al., 2005), semantic roles for verbs or other relationships introduced for AMR, e.g. arg0 or op1 in Figure 1. The set of node and edge names in an AMR graph is drawn from a series of symbols C, and each word in a sentence is drawn from a vocabulary W. We examine the task of forming an AMR parser, i.e. finding a set of sequential parameters for a model f that predicts a sentence that AMR provides a specific sentence (max."}, {"heading": "3.2 Sequence-to-sequence Model", "text": "For both tasks, we use a stacked LSTM sequence of neural architecture used in neural machine translation (Bahdanau et al., 2015; Wuet al., 2016).1 Our model uses a global attention decoder and an unknown phrase with small modifications (Luong et al., 2015).The model uses a stacked bi-directional LSTM encoder to encode an input sequence and a stacked LSTM to decode the hidden states produced by the encoder. We perform two modifications on the encoder: (1) we concatenate the hidden states at each level of the stack instead of the top of the stack, and (2) we introduce dropouts at the first level of the encoder."}, {"heading": "3.3 Linearization", "text": "Our seq2seq models require both input and target to be represented as a linear sequence of tokens. We define a linearization order for an AMR graph as any sequence of its nodes and edges. Linearization is defined as (1) a linearization order and (2) a rendering function that generates an arbitrary number of tokens when applied to an element in the linearization order (see Section 4.2 for implementation details)."}, {"heading": "3.4 Paired Training", "text": "Obtaining a corpus of jointly annotated sentence pairs and AMR diagrams is expensive, and current data sets cover only thousands of examples. Neural sequence-to-sequence models suffer from rarity with so few training pairs. To reduce the effect of rarity, we use an external, unannotated corpus of sentences Se, and a method that pairs the training of the parser and generator. Our method is described in Algorithm 1 and first forms a parser on the datasetD of sentence pairs and AMR diagrams. Then, it uses self-training1We extended the Harvard NLP seq2seq framework from http: / / nlp.seas.harvard.edu / code.Algorithm 1 Paired Training Procedure Input: Training set of sentences and AMR graphs (a)."}, {"heading": "4 AMR Preprocessing", "text": "We use a number of pre-processing steps, including AMR linerization, anonymization, and other changes we make to sentence-graph pairs. Our methods have two goals: (1) to reduce the complexity of linearized sequences to facilitate learning while maintaining enough original information, and (2) to address the scarcity of certain vocabulary entries from the open class vocabulary, such as named entities (NEs) and quantities. Figure 2 (d) contains sample inputs and outputs with all of our pre-processing techniques. Graph simplification To reduce the total length of the linearized graph, we first remove variable names and the instance of the relationship (/) before each concept. In the case of reoccurring nodes, we replace variable mentions with their co-referencing concept. Even if this exchange causes information loss, the round context often helps to restore the resemblance of the surface form, for example, the surface form corrected with 1."}, {"heading": "4.1 Anonymization of Named Entities", "text": "To reduce the number of entries that contain a: name role, structures that refer to entities such as person, country, miscellaneous entities labeled with * -enitity, and numerical values that refer to one of the over 140 fine-grained entity types are collected. We exclude data types that contain a: name role. This includes structures that refer to entities such as person, country, miscellaneous entities labeled with * -enitity, and typed numerical values, * -quantity. We exclude data types that contain a: name role. We then replace these subgraphs with a token that indicates a fine-grained type and index."}, {"heading": "4.2 Linearization", "text": "Our order of linearization is defined by the order of the nodes visited by the first depth search, including the steps of backward straddling. For example, in Figure 2, starting with meet the order, meet,: ARG0, person,: ARG1-of, expert,: ARG2-of, group,: ARG2-of,: ARG1-of,: ARG0.4 The order traverses children in the order in which they are represented in the AMR. We consider alternative orders of children in Section 7, but always follow the pattern described above. Rendering function Our rendering function marks perimeter and generates tags after the pre-arranged traversing of the graph: (1) if the element is a node, it prints the type of the node. (2) If the element is an edge, it prints the type of the edge and then recursively creates an enclosed string for the (concept node) immediately after it."}, {"heading": "5 Experimental Setup", "text": "We run all the experiments on the AMR corpus in SemEval-2016 Task 8 (LDC2015E86), which contains 16,833 / 1,368 / 1,371 pull / dev / test examples. For the paired training procedure of Algorithm 1, we use Gigaword as our external corpus and sample sentences, which only contain words from the AMR corpus vocabulary W. We sampled the original sentence to ensure that there is no overlap with the AMR training or the test sets. Table 24Sense, instance and variable information have been removed at the point of linearisation."}, {"heading": "6 Results", "text": "In fact, we will be able to use a wide range of possibilities to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to see the world, to see the world, to travel around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world"}, {"heading": "7 Linearization Evaluation", "text": "In this section, we examine three strategies for converting AMR diagrams into sequences in the context of AMR generation and show that our models are largely agnostic to linearization orders. In contrast to SMT-based AMR generation methods (Pourdamghani et al., 2016), our results argue that seq2seq models can learn to ignore artifacts of converting diagrams into linear sequences."}, {"heading": "7.1 Linearization Orders", "text": "All linearizations we consider possible use the pattern described in Section 4.2, but differ in the order in which children are visited. Each linearization generates anonymized, domain-marked output (see Section 4) of the form shown in Figure 2 (d). The proposal traverses children in the order represented by human-made AMR annotations, just as in Figure 2 (d).Global Random. We construct a random global order of all edge types that appear in AMR charts, and reuse them for each example in the dataset. We traverse children based on the position in the global order of the edge that leads to a child. Randomly For each example in the dataset, we traverse children according to a different random order of edge types."}, {"heading": "7.2 Results", "text": "We present AMR generation results for the three proposed linearization orders in Table 6. Random linearization orders yield slightly worse results than traversing the graph according to the human linearization order. Surprisingly, a random linearization order per example performs almost identically to a global random order and argues that seq2seq models can learn to ignore artifacts of converting graphs into linear sequences.On the other hand, the model that follows the human order behaves better, leading us to suspect that it contains additional information that is used in the graphical structure of the AMR. To further investigate, we compare the relative order of the edge pairs in the linearization order, which is derived from these parent pairs of the 57.6, as we found in a majority of the 57.6 children."}, {"heading": "8 Qualitative Results", "text": "Figure 3 shows sample editions of our entire system. The text generated for the first graph is almost perfect with only a small grammatical error due to anonymization. The second example is more difficult, with a deep structure branching to the right and coordination of verbs stabilizing and pushing into the subordinate clause under the direction of the state. The model omitted some information from the graph, namely the terms terrorism and virus. In the third example, larger parts of the graph are missing, such as the entire subgraph under the guidance of an expert. Also, the model makes incorrect investment decisions in the last two sub-graphs (it is the evidence that is incontestable and irrefutable, not the equipment), largely due to insufficient annotation (thing), hardening their generation. 9Consider the sentences \"She went to school in New York two years ago\" and \"Two years ago she went to school in New York,\" where \"two years ago\" the time modifying epization was \"most common\" and we found the 7 in the table."}, {"heading": "9 Conclusions", "text": "We applied sequence-to-sequence models to the tasks of AMR parsing and AMR generation by carefully pre-processing the graph representation and scaling our models to millions of unlabeled sentences from the gigaword corpus by pre-training. Crucially, we do not rely on resources such as knowledge bases and externally trained parsers. We achieve competitive results in the parsing task (SMATCH 62.1) and in state-of-the-art generation (BLEU 33.8). For future work, we would like to extend our work to other representations of meaning such as Minimal Recursion Semantics (MRS; Copestake et al. (2005)). This formalism treats certain linguistic phenomena differently from AMR (e.g. negation and co-reference), contains explicit annotations on concepts of number, tension and case, and finally treats multiple languages (Bender, 2014)."}, {"heading": "Acknowledgments", "text": "The research was partially supported by DARPA under the DEFT program by AFRL (FA8750-13-2-0019) and the CwC program by ARO (W911NF-15-1-0543), ARO (W911NF-16-1-0121), NSF (IIS-1252835, IIS-1562364, IIS-1524371), an Allen Distinguished Investigator Award, Samsung GRO and gifts from Google and Facebook. Authors would like to thank Rik Koncel-Kedziorski, the UW NLP group and anonymous critics for their thorough and helpful comments. 10A list of actively cultivated languages can be found here: http: / / moin.delph-in.net / GrammarCataloguelimit: arg0 (Contract: arg0-of (Control: arg1 Weapons): arg1 (Weapon: mod: conventionally arg1-.net / Grammarguelimite cataloguelimite), which can be used in conventional weapons with conventional limits."}], "references": [{"title": "Broadcoverage CCG semantic parsing with AMR", "author": ["Yoav Artzi", "Kenton Lee", "Luke Zettlemoyer."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Lisbon, Portugal, pages 1699\u20131710.", "citeRegEx": "Artzi et al\\.,? 2015", "shortCiteRegEx": "Artzi et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 2015 International Conference on Learning Representations. CBLS, San Diego, California. http://arxiv.org/abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "RIGA at SemEval-2016 Task 8: Impact of Smatch extensions and character-level neural translation on AMR parsing accuracy", "author": ["Guntis Barzdins", "Didzis Gosko."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation. Association for Computa-", "citeRegEx": "Barzdins and Gosko.,? 2016", "shortCiteRegEx": "Barzdins and Gosko.", "year": 2016}, {"title": "Language CoLLAGE: Grammatical description with the LinGO grammar matrix", "author": ["Emily M. Bender."], "venue": "Proceedings of the 9th International Conference on Language Resources and Evaluation. Reykjavik, Iceland, pages 2447\u2013 2451.", "citeRegEx": "Bender.,? 2014", "shortCiteRegEx": "Bender.", "year": 2014}, {"title": "The Meaning Factory at SemEval-2016 Task 8: Producing AMRs with Boxer", "author": ["Johannes Bjerva", "Johan Bos", "Hessel Haagsma."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation. Association for Computational Linguistics, San Diego, California, pages", "citeRegEx": "Bjerva et al\\.,? 2016", "shortCiteRegEx": "Bjerva et al\\.", "year": 2016}, {"title": "ICL-HD at SemEval-2016 Task 8: Meaning representation parsing - augmenting AMR parsing with a preposition semantic role labeling neural network", "author": ["Lauritz Brandt", "David Grimm", "Mengfei Zhou", "Yannick Versley."], "venue": "Proceedings of the 10th International Work-", "citeRegEx": "Brandt et al\\.,? 2016", "shortCiteRegEx": "Brandt et al\\.", "year": 2016}, {"title": "Smatch: an evaluation metric for semantic feature structures", "author": ["Shu Cai", "Kevin Knight."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Sofia, Bulgaria, pages 748\u2013752.", "citeRegEx": "Cai and Knight.,? 2013", "shortCiteRegEx": "Cai and Knight.", "year": 2013}, {"title": "Minimal Recursion Semantics: An introduction", "author": ["Ann Copestake", "Dan Flickinger", "Carl Pollard", "Ivan A. Sag."], "venue": "Research on Language and Computation 3(2):281\u2013 332. https://doi.org/10.1007/s11168-006-6327-9.", "citeRegEx": "Copestake et al\\.,? 2005", "shortCiteRegEx": "Copestake et al\\.", "year": 2005}, {"title": "An incremental parser for abstract meaning representation", "author": ["Marco Damonte", "Shay B. Cohen", "Giorgio Satta."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computa-", "citeRegEx": "Damonte et al\\.,? 2017", "shortCiteRegEx": "Damonte et al\\.", "year": 2017}, {"title": "Incorporating non-local information into information extraction systems by Gibbs sampling", "author": ["Jenny Rose Finkel", "Trond Grenager", "Christopher Manning."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computa-", "citeRegEx": "Finkel et al\\.,? 2005", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Cmu at semeval-2016 task 8: Graph-based amr parsing with infinite ramp loss", "author": ["Jeffrey Flanigan", "Chris Dyer", "Noah A. Smith", "Jaime Carbonell."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). Association for Computational Linguis-", "citeRegEx": "Flanigan et al\\.,? 2016a", "shortCiteRegEx": "Flanigan et al\\.", "year": 2016}, {"title": "Generation from abstract meaning representation using tree transducers", "author": ["Jeffrey Flanigan", "Chris Dyer", "Noah A. Smith", "Jaime Carbonell."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics. Association for", "citeRegEx": "Flanigan et al\\.,? 2016b", "shortCiteRegEx": "Flanigan et al\\.", "year": 2016}, {"title": "A discriminative graphbased parser for the abstract meaning representation", "author": ["Jeffrey Flanigan", "Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Association for Com-", "citeRegEx": "Flanigan et al\\.,? 2014", "shortCiteRegEx": "Flanigan et al\\.", "year": 2014}, {"title": "UCL+Sheffield at SemEval-2016 Task 8: Imitation learning for AMR parsing with an alphabound", "author": ["James Goodman", "Andreas Vlachos", "Jason Naradowsky."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation. Association for Computa-", "citeRegEx": "Goodman et al\\.,? 2016", "shortCiteRegEx": "Goodman et al\\.", "year": 2016}, {"title": "Liberal event extraction and event schema induction", "author": ["Lifu Huang", "Taylor Cassidy", "Xiaocheng Feng", "Heng Ji", "Clare R. Voss", "Jiawei Han", "Avirup Sil."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Association for Com-", "citeRegEx": "Huang et al\\.,? 2016", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Semantics-Based Machine Translation with Hyperedge Replacement Grammars", "author": ["Bevan Jones", "Jacob Andreas", "Daniel Bauer", "Karl Moritz Hermann", "Kevin Knight."], "venue": "Proceedings of the 2012 International Conference on Computational Linguistics. Bombay, India, pages", "citeRegEx": "Jones et al\\.,? 2012", "shortCiteRegEx": "Jones et al\\.", "year": 2012}, {"title": "Open source toolkit", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ond\u0159ej Bojar", "Alexandra Constantin", "Evan Herbst"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Toward abstractive summarization using semantic representations", "author": ["Fei Liu", "Jeffrey Flanigan", "Sam Thomson", "Norman Sadeh", "Noah A. Smith."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics. Association", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Neural shift-reduce CCG semantic parsing", "author": ["Dipendra Kumar Misra", "Yoav Artzi."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas, pages 1775\u20131786.", "citeRegEx": "Misra and Artzi.,? 2016", "shortCiteRegEx": "Misra and Artzi.", "year": 2016}, {"title": "Annotated Gigaword", "author": ["Courtney Napoles", "Matthew Gormley", "Benjamin Van Durme."], "venue": "Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction. Association for Computational Linguistics, Montr\u00e9al, Canada,", "citeRegEx": "Napoles et al\\.,? 2012", "shortCiteRegEx": "Napoles et al\\.", "year": 2012}, {"title": "The Proposition Bank: An annotated corpus of semantic roles", "author": ["Martha Palmer", "Daniel Gildea", "Paul Kingsbury."], "venue": "Computational Linguistics 31(1):71\u2013106. http://www.cs.rochester.edu/ gildea/palmer-propbankcl.pdf.", "citeRegEx": "Palmer et al\\.,? 2005", "shortCiteRegEx": "Palmer et al\\.", "year": 2005}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguistics. Association for Computational Lin-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Addressing the data sparsity issue in neural AMR parsing", "author": ["Xiaochang Peng", "Chuan Wang", "Daniel Gildea", "Nianwen Xue."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics. Association for Com-", "citeRegEx": "Peng et al\\.,? 2017", "shortCiteRegEx": "Peng et al\\.", "year": 2017}, {"title": "Aligning English strings with abstract meaning representation graphs", "author": ["Nima Pourdamghani", "Yang Gao", "Ulf Hermjakob", "Kevin Knight."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Association for Com-", "citeRegEx": "Pourdamghani et al\\.,? 2014", "shortCiteRegEx": "Pourdamghani et al\\.", "year": 2014}, {"title": "Generating English from abstract meaning representations", "author": ["Nima Pourdamghani", "Kevin Knight", "Ulf Hermjakob."], "venue": "Proceedings of the 9th International Natural Language Generation conference. Association for Computational Linguistics, Edinburgh, UK, pages 21\u201325.", "citeRegEx": "Pourdamghani et al\\.,? 2016", "shortCiteRegEx": "Pourdamghani et al\\.", "year": 2016}, {"title": "Parsing english into abstract meaning representation using syntax-based machine translation", "author": ["Michael Pust", "Ulf Hermjakob", "Kevin Knight", "Daniel Marcu", "Jonathan May."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association", "citeRegEx": "Pust et al\\.,? 2015", "shortCiteRegEx": "Pust et al\\.", "year": 2015}, {"title": "M2L at SemEval-2016 Task 8: AMR parsing with neural networks", "author": ["Yevgeniy Puzikov", "Daisuke Kawahara", "Sadao Kurohashi."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation. Association for Computational Linguistics, San Diego, California, pages", "citeRegEx": "Puzikov et al\\.,? 2016", "shortCiteRegEx": "Puzikov et al\\.", "year": 2016}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Berlin, Germany,", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Jacy: An Implemented Grammar of Japanese", "author": ["Melanie Siegel", "Emily M. Bender", "Francis Bond."], "venue": "CSLI Studies in Computational Linguistics. CSLI Publications, Stanford. http://web.stanford.edu/group/cslipublications/cslipublications/site/9781684000180.shtml.", "citeRegEx": "Siegel et al\\.,? 2016", "shortCiteRegEx": "Siegel et al\\.", "year": 2016}, {"title": "AMR-to-text generation as a traveling salesman problem", "author": ["Linfeng Song", "Yue Zhang", "Xiaochang Peng", "Zhiguo Wang", "Daniel Gildea."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computa-", "citeRegEx": "Song et al\\.,? 2016", "shortCiteRegEx": "Song et al\\.", "year": 2016}, {"title": "Neural headline generation on abstract meaning representation", "author": ["Sho Takase", "Jun Suzuki", "Naoaki Okazaki", "Tsutomu Hirao", "Masaaki Nagata."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Compu-", "citeRegEx": "Takase et al\\.,? 2016", "shortCiteRegEx": "Takase et al\\.", "year": 2016}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141 ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Proceedings of the 28th International Conference on Neural Information Processing Systems, MIT Press, pages 2773\u2013", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "CAMR at SemEval-2016 Task 8: An extended transition-based AMR parser", "author": ["Chuan Wang", "Sameer Pradhan", "Xiaoman Pan", "Heng Ji", "Nianwen Xue."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation. Association for Computational", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "AMR parsing with an incremental joint model", "author": ["Junsheng Zhou", "Feiyu Xu", "Hans Uszkoreit", "Weiguang QU", "Ran Li", "Yanhui Gu."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Austin,", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "AMR has been used as an intermediate meaning representation for several applications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al.", "startOffset": 120, "endOffset": 140}, {"referenceID": 17, "context": ", 2012), summarization (Liu et al., 2015), sentence compression (Takase et al.", "startOffset": 23, "endOffset": 41}, {"referenceID": 31, "context": ", 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 14, "context": ", 2016), and event extraction (Huang et al., 2016).", "startOffset": 30, "endOffset": 50}, {"referenceID": 19, "context": "of neural network models (Misra and Artzi, 2016; Peng et al., 2017; Barzdins and Gosko, 2016).", "startOffset": 25, "endOffset": 93}, {"referenceID": 23, "context": "of neural network models (Misra and Artzi, 2016; Peng et al., 2017; Barzdins and Gosko, 2016).", "startOffset": 25, "endOffset": 93}, {"referenceID": 2, "context": "of neural network models (Misra and Artzi, 2016; Peng et al., 2017; Barzdins and Gosko, 2016).", "startOffset": 25, "endOffset": 93}, {"referenceID": 1, "context": "cations (Wu et al., 2016; Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015).", "startOffset": 8, "endOffset": 90}, {"referenceID": 18, "context": "cations (Wu et al., 2016; Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015).", "startOffset": 8, "endOffset": 90}, {"referenceID": 32, "context": "cations (Wu et al., 2016; Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015).", "startOffset": 8, "endOffset": 90}, {"referenceID": 20, "context": "bootstrap a high quality AMR parser from millions of unlabeled Gigaword sentences (Napoles et al., 2012) and then use the automatically parsed AMR graphs to pre-train an AMR generator.", "startOffset": 82, "endOffset": 104}, {"referenceID": 10, "context": "Alignment-based Parsing Flanigan et al. (2014) (JAMR) pipeline concept and relation identification with a graph-based algorithm.", "startOffset": 24, "endOffset": 47}, {"referenceID": 10, "context": "Alignment-based Parsing Flanigan et al. (2014) (JAMR) pipeline concept and relation identification with a graph-based algorithm. Zhou et al. (2016) extend JAMR by performing the", "startOffset": 24, "endOffset": 148}, {"referenceID": 24, "context": "(2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al., 2014), and employing several external semantic resources.", "startOffset": 101, "endOffset": 128}, {"referenceID": 24, "context": "Grammar-based Parsing Wang et al. (2016) (CAMR) perform a series of shift-reduce transformations on the output of an externally-trained dependency parser, similar to Damonte et al.", "startOffset": 22, "endOffset": 41}, {"referenceID": 5, "context": "(2016) (CAMR) perform a series of shift-reduce transformations on the output of an externally-trained dependency parser, similar to Damonte et al. (2017), Brandt et al.", "startOffset": 132, "endOffset": 154}, {"referenceID": 3, "context": "(2017), Brandt et al. (2016), Puzikov et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 3, "context": "(2017), Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al.", "startOffset": 8, "endOffset": 52}, {"referenceID": 3, "context": "(2017), Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al.", "startOffset": 8, "endOffset": 79}, {"referenceID": 0, "context": "Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al. (2016). Pust et al.", "startOffset": 0, "endOffset": 170}, {"referenceID": 0, "context": "Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al. (2016). Pust et al. (2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al.", "startOffset": 0, "endOffset": 190}, {"referenceID": 2, "context": "Neural Parsing Recently there have been a few seq2seq systems for AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017).", "startOffset": 78, "endOffset": 123}, {"referenceID": 23, "context": "Neural Parsing Recently there have been a few seq2seq systems for AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017).", "startOffset": 78, "endOffset": 123}, {"referenceID": 2, "context": "Neural Parsing Recently there have been a few seq2seq systems for AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). Similar to our approach, Peng et al. (2017) deal with sparsity by", "startOffset": 79, "endOffset": 169}, {"referenceID": 10, "context": "AMR Generation Flanigan et al. (2016b) specify a number of tree-to-string transduction rules", "startOffset": 15, "endOffset": 39}, {"referenceID": 24, "context": "Pourdamghani et al. (2016) also use an MT decoder; they learn a classifier that linearizes the input AMR graph in an order that follows the output", "startOffset": 0, "endOffset": 27}, {"referenceID": 30, "context": "Song et al. (2016) recast generation as a traveling salesman problem, after partitioning the graph into fragments and finding the best linearization order.", "startOffset": 0, "endOffset": 19}, {"referenceID": 28, "context": "Data Augmentation Our paired training procedure is largely inspired by Sennrich et al. (2016). They improve neural MT performance for low resource language pairs by using a back-translation MT system for a large monolingual corpus of the target language in order to create synthetic output,", "startOffset": 71, "endOffset": 94}, {"referenceID": 21, "context": "Furthermore, the graph contains labeled edges, which correspond to PropBank-style (Palmer et al., 2005) seman-", "startOffset": 82, "endOffset": 103}, {"referenceID": 1, "context": "ral machine translation (Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 24, "endOffset": 64}, {"referenceID": 18, "context": "1 Our model uses a global attention decoder and unknown word replacement with small modifications (Luong et al., 2015).", "startOffset": 98, "endOffset": 118}, {"referenceID": 24, "context": "Following Pourdamghani et al. (2016) we also remove senses from all concepts for AMR generation only.", "startOffset": 10, "endOffset": 37}, {"referenceID": 12, "context": "On the training set, we use alignments obtained using the JAMR aligner (Flanigan et al., 2014) and the unsupervised aligner of Pourdamghani et al.", "startOffset": 71, "endOffset": 94}, {"referenceID": 10, "context": "On the training set, we use alignments obtained using the JAMR aligner (Flanigan et al., 2014) and the unsupervised aligner of Pourdamghani et al. (2014) in order to find mappings of anonymized subgraphs to spans of text and replace mapped text", "startOffset": 72, "endOffset": 154}, {"referenceID": 9, "context": "four coarse entity types used in the Stanford NER system (Finkel et al., 2005): person, location, organization and misc.", "startOffset": 57, "endOffset": 78}, {"referenceID": 26, "context": "SBMT (Pust et al., 2015) - - 69.", "startOffset": 5, "endOffset": 24}, {"referenceID": 10, "context": "1 JAMR (Flanigan et al., 2016a) - - - 69.", "startOffset": 7, "endOffset": 31}, {"referenceID": 33, "context": "0 CAMR (Wang et al., 2016) 72.", "startOffset": 7, "endOffset": 26}, {"referenceID": 0, "context": "5 CCG* (Artzi et al., 2015) 67.", "startOffset": 7, "endOffset": 27}, {"referenceID": 12, "context": "3 JAMR (Flanigan et al., 2014) - - - 64.", "startOffset": 7, "endOffset": 30}, {"referenceID": 23, "context": "SEQ2SEQ (Peng et al., 2017) - - - 55.", "startOffset": 8, "endOffset": 27}, {"referenceID": 2, "context": "0 CHAR-LSTM (Barzdins and Gosko, 2016) - - - - - 43.", "startOffset": 12, "endOffset": 38}, {"referenceID": 6, "context": "We evaluate AMR parsing with SMATCH (Cai and Knight, 2013), and AMR generation using BLEU (Papineni et al.", "startOffset": 36, "endOffset": 58}, {"referenceID": 22, "context": "We evaluate AMR parsing with SMATCH (Cai and Knight, 2013), and AMR generation using BLEU (Papineni et al., 2002)5.", "startOffset": 90, "endOffset": 113}, {"referenceID": 16, "context": "We use the multi-BLEU script from the MOSES decoder suite (Koehn et al., 2007).", "startOffset": 58, "endOffset": 78}, {"referenceID": 25, "context": "PBMT* (Pourdamghani et al., 2016) 27.", "startOffset": 6, "endOffset": 33}, {"referenceID": 30, "context": "9 TSP (Song et al., 2016) 21.", "startOffset": 6, "endOffset": 25}, {"referenceID": 11, "context": "4 TREETOSTR (Flanigan et al., 2016b) 23.", "startOffset": 12, "endOffset": 36}, {"referenceID": 12, "context": "Our full models outperform the original version of JAMR (Flanigan et al., 2014), a graph-based model but still lags", "startOffset": 56, "endOffset": 79}, {"referenceID": 23, "context": "Following previous work, we find that seq2seq-based AMR parsing is largely ineffective without anonymization (Peng et al., 2017).", "startOffset": 109, "endOffset": 128}, {"referenceID": 25, "context": "Our results argue, unlike SMT-based AMR generation methods (Pourdamghani et al., 2016), that seq2seq models can learn to ignore artifacts of the conversion of graphs to linear sequences.", "startOffset": 59, "endOffset": 86}, {"referenceID": 3, "context": ", negation, and co-reference), contains explicit annotation on concepts for number, tense and case, and finally handles multiple languages10 (Bender, 2014).", "startOffset": 141, "endOffset": 155}, {"referenceID": 29, "context": "Taking a step further, we would like to apply our models on Semantics-Based Machine Translation using MRS as an intermediate representation between pairs of languages, and investigate the added benefit compared to directly translating the surface strings, especially in the case of distant language pairs such as English and Japanese (Siegel et al., 2016).", "startOffset": 334, "endOffset": 355}, {"referenceID": 6, "context": "the Minimal Recursion Semantics (MRS; Copestake et al. (2005)).", "startOffset": 38, "endOffset": 62}], "year": 2017, "abstractText": "Sequence-to-sequence models have shown strong performance across a broad range of applications. However, their application to parsing and generating text using Abstract Meaning Representation (AMR) has been limited, due to the relatively limited amount of labeled data and the nonsequential nature of the AMR graphs. We present a novel training procedure that can lift this limitation using millions of unlabeled sentences and careful preprocessing of the AMR graphs. For AMR parsing, our model achieves competitive results of 62.1 SMATCH, the current best score reported without significant use of external semantic resources. For AMR generation, our model establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequencebased AMR models are robust against ordering variations of graph-to-sequence conversions.", "creator": "LaTeX with hyperref package"}}}