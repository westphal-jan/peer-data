{"id": "1704.08760", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2017", "title": "Learning a Neural Semantic Parser from User Feedback", "abstract": "We present an approach to rapidly and easily build natural language interfaces to databases for new domains, whose performance improves over time based on user feedback, and requires minimal intervention. To achieve this, we adapt neural sequence models to map utterances directly to SQL with its full expressivity, bypassing any intermediate meaning representations. These models are immediately deployed online to solicit feedback from real users to flag incorrect queries. Finally, the popularity of SQL facilitates gathering annotations for incorrect predictions using the crowd, which is directly used to improve our models. This complete feedback loop, without intermediate representations or database specific engineering, opens up new ways of building high quality semantic parsers. Experiments suggest that this approach can be deployed quickly for any new target domain, as we show by learning a semantic parser for an online academic database from scratch.", "histories": [["v1", "Thu, 27 Apr 2017 22:05:06 GMT  (477kb,D)", "http://arxiv.org/abs/1704.08760v1", "Accepted at ACL 2017"]], "COMMENTS": "Accepted at ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["srinivasan iyer", "ioannis konstas", "alvin cheung", "jayant krishnamurthy", "luke zettlemoyer"], "accepted": true, "id": "1704.08760"}, "pdf": {"name": "1704.08760.pdf", "metadata": {"source": "CRF", "title": "Learning a Neural Semantic Parser from User Feedback", "authors": ["Srinivasan Iyer", "Ioannis Konstas", "Alvin Cheung", "Jayant Krishnamurthy", "Luke Zettlemoyer"], "emails": ["sviyer@cs.washington.edu", "ikonstas@cs.washington.edu", "akcheung@cs.washington.edu", "lsz@cs.washington.edu", "jayantk@allenai.org", "lukez@allenai.org"], "sections": [{"heading": null, "text": "To achieve this, we adapt neural sequence models to map expressions directly to SQL with its full expressivity, bypassing all intermediate representations of meaning. These models are immediately deployed online to gather feedback from real users to flag incorrect queries. Finally, the popularity of SQL makes it easier to collect annotations for incorrect predictions using the crowd, which is directly used to improve our models. This full feedback loop, without intermediate representations or database-specific engineering, opens up new ways to build high-quality semantic parsers. Experiments suggest that this approach can be quickly applied to any new target domain, as we show by learning a semantic parser for an online academic database from the ground up."}, {"heading": "1 Introduction", "text": "In fact, it is that we are able to hide, and that we are able to hide, \"he said."}, {"heading": "2 Related Work", "text": "Although different meaning languages have been used with semantic parsers - such as regular expressions that support a broad database (Kushman and Barzilay, 2013; Locascio et al., 2016), Abstract Meaning Representations (AMR) (Artzi et al., 2015; Misra and Artzi, 2016), and systems of equations (Kushman et al., 2014; Roy et al., 2016) - parsers for database queries typically use either logic programs (Zelle and Mooney, 1996), Lambda calculus (Zettlemoyer and Collins, 2005), or \"Liang et al\" (Liang et al., 2013), such as the meaning of repetition of language. All three of these languages are modeled by natural language to facilitate parsing, but none of them are used to query databases outside the semantic parsing literature, so they are understood by few people and not supported by standard database implementations."}, {"heading": "3 Feedback-based Learning", "text": "Our feedback-based learning approach can be used to quickly deploy semantic parsers to generate q q q feedback for each new domain. It is a simple interactive learning algorithm that uses a preliminary semantic parser to then iteratively improve that parser using user feedback and selective annotation of queries. A key requirement of this algorithm is the ability to comment queries for selected user utterances cost-effectively and efficiently. We address this requirement by developing a model that directly issues SQL queries (Section 4), which can also be produced by crowdworkers.Our algorithm alternates between phases of training the model and making predictions in order to collect user feedback with the aim of improving performance at each successive stage. The process is in Algorithm 1. Our neural Model N is initially trained on synthetic data T that is domain-independent bias-based."}, {"heading": "4 Semantic Parsing to SQL", "text": "We use a neural sequence-to-sequence model to map natural language questions directly to SQL queries, and this allows us to scale our feedback-based learning approach by simply using crowd-sourced labels when needed. We also present two data enhancement techniques that use content from the database schema and external paraphrase resources."}, {"heading": "4.1 Model", "text": "We use an encoder decoder model with global attention, similar to Luong et al. (2015), in which the anonymized statement (see Section 4.2) is encoded using a bidirectional LSTM network and then decrypted to directly predict SQL query tokens. The decoder predicts a conditional probability distribution of possible values for the next SQL token using a combination of the previous SQL token embedding, attention to the hidden states of the encoder network, and an attention signal from the previous time step. If Qi represents an embedding for the next SQL token state, the previous token embedding is used using a combination of SQL token embedding, attention to the hidden states of the encoder network, and an attention signal from the previous time step."}, {"heading": "4.2 Entity Anonymization", "text": "During the training, when the SQL is available, we derive the type from the corresponding column name; for example, Boston is a city in the city name = \"Boston.\" To recognize the units in the statements at the test date, we build a search engine for all units from the target database. For each span of words (starting with a large span and gradually shrinking), we query the search engine using a TF-IDF scheme to find the unit closest to the span, and then replace it with the type of the unit. We store these mappings and apply them to the generated SQL to fill in the names of the entities. TF-IDF matching provides some flexibility in matching the entity names in the statements, for example, a user could say Donald Knuth instead of E."}, {"heading": "4.3 Data Augmentation", "text": "We present two augmentation strategies that either (1) provide the initial training data to begin interactive learning before more highlighted examples become available, or (2) use external paraphrase resources to improve generalization; these templates contain slots whose values populate a database schema; an example template is shown in Figure 2a. < ENT > types represent tables in the database, < ENT > tables represent tables. < ENT > types represent tables in the database. < ENT > types represent tables in the database. < ENT > types represent tables in the database. < ENT > ENT > are displayed in a column. < COL > represents a column in the specific table and < ENT >."}, {"heading": "5 Benchmark Experiments", "text": "Our first experiments show that our semantic parsing model has a comparable accuracy to previous work, despite the increased difficulty of directly generating SQL. We demonstrate this result by running our model on two benchmark semantic parsing datasets, GEO880 and ATIS."}, {"heading": "5.1 Data sets", "text": "GEO880 is a collection of 880 statements sent to a database of geographical facts (geobase) in the United States, originally in prologue format. Popescu et al. (2003) created a relational geobase database schema along with SQL queries for a subset of 700 statements. To compare with previous work on the complete corpus, we commented on the remaining statements and used the standard 600 / 280 training / test split (Zettlemoyer and Collins, 2005).ATIS is a collection of 5,418 statements about a flight booking system, accompanied by a relational database and SQL queries to answer the questions. We use 4,473 statements for training, 497 for development, and 448 for the post-Kwiatkowski et al. (2011).The original SQL queries were very inefficient to execute due to the use of IN clauses and SQL queries, so in total we changed the queries during Ramsay 867, and during Q."}, {"heading": "5.2 Experimental Methodology", "text": "We use a standard methodology of traction, development and testing for our experiments. The training set is augmented by schematics and 3 paraphrases per training example, as in Section 4. Utterances have been anonymized by replacing them with their corresponding types and all words that occur only once have been replaced by UNK symbols. The development set is used for hyperparameter tuning and early stopping. For GEO880, we use cross-validation on the training set to tune hyperparameters. We used a mini-batch size of 100 and Adam (Kingma and Ba, 2015) with a learning rate of 0.001 for 70 epochs for all our experiments. We used a bar size of 5 for decoding. We report on the accuracy of our SQL query forecasts by running them on the target database and comparing the result with the true result."}, {"heading": "5.3 Results", "text": "Tables 2 and 3 show test accuracies based on the terms of our model on GEO880 and ATIS, respectively, compared to previous work. 2 To our knowledge, this is the first result of a direct analysis of SQL to achieve a comparable performance to previous work, without any database-specific feature engineering. Popescu et al. (2003) and Giordani and Moschitti (2012) also directly produce SQL queries, but on a subset of 700 examples from GEO880. The former works only on semantically tractable statements, where words un-2Note that 2.8% of GEO880 and 5% of ATIS gold test sets produce SQL queries (before each processing), while the latter use a reranking approach that also limits the complexity of SQL queries that can be handled."}, {"heading": "6 Interactive Learning Experiments", "text": "In this section, we will learn a semantic parser for an academic field from scratch using an online system using our interactive learning algorithm (Section 3). After three train deployment cycles, the system answered 63.51% of user questions correctly. To our knowledge, this is the first attempt to learn a semantic parser with a live system, and is enabled by our models that can parse language directly to SQL without manual intervention."}, {"heading": "6.1 User Interface", "text": "We developed a web interface for accepting natural language questions to an academic database of users by using our model to generate an SQL query and display the results after execution. Several sample comments are also displayed to help users understand the domain. Along with the results of the generated SQL query, users are asked to provide feedback that is used for interactive learning. Screenshots of our user interface are available in our Supplementary Materials.Collecting accurate user feedback on predicted queries is a key challenge in the interactive learning environment for two reasons. First, the results of the system may be flawed due to poor entity identification or incompleteness in the database, none of which is under the control of the semantic parser. Second, it can be difficult for users to determine whether the presented results are actually correct. This determination is particularly difficult if the system responds to the user's usage type incorrectly, for example, when the result is requested by the user."}, {"heading": "6.2 Three-Stage Online Experiment", "text": "In this experiment, which uses our developed user interface, we used algorithm 1 to learn semantic feedback from scratch; the experiment had three stages; at each stage, we recruited 10 new users (computer science students) and asked them to send at least 10 comments to the system and give feedback on the results. We considered the results to be either correct or incomplete after completing a short SQL test; the remaining erroneous comments were sent to a crowd worker for comment and were used to reinvent the system for the next stage."}, {"heading": "6.3 SCHOLAR dataset", "text": "We are publishing a new semantic parsing dataset for academic database searching using the statements collected in the user study. We are adding additional expressions labeled by crowdworkers to these highlighted expressions. (Note that these additional expressions were not used in the online experiment.) The final dataset includes 816 natural language expressions labeled with SQL, divided into a 600 / 216 turn / test split. We are also providing a database on which to run these queries, which contain scientific papers with their authors, quotations, journals, keywords and datasets. Table 1 shows statistics of this dataset. Our parser achieves an accuracy of 67% for this turn / test split in the fully monitored environment. In comparison, an adjacent strategy using cosmic similarity metrics using a TF-IDF representation for the expressions leads to an accuracy of 52.75% for this turn / test split."}, {"heading": "6.4 Simulated Interactive Experiments", "text": "We conducted additional simulated interactive learning experiments with GEO880 and ATIS to better understand the behavior of our feedback loop for train delivery, the effects of our approaches to data amplification, and the required commentary effort. We randomly split each training session into K-lots and present these lots one by one to our interactive learning algorithm. Correctness feedback is provided by comparing the result of the predicted query with the gold query, i.e. we assume that users are able to distinguish correct results perfectly from false ones. Figure 3 shows accuracy on GEO880 and ATIS, respectively, on each lot when the model is trained on all previous lots. As in the live experiment, accuracy improves with successive lots. Data augmentation using templates helps in the early stages of GEO880, but its benefits diminish as more marked lots are obtained."}, {"heading": "7 Conclusion", "text": "Our approach uses an attention-based neural sequence-to-sequence model with data augmentation from the target database and paraphrasing to analyze SQL utterances. This model is used in an online system where user feedback on its predictions is used to select utterances for crowdworker annotations. We note that the performance of the semantic analysis model is comparable to previous systems, which either map from utterances to logical forms or generate SQL to two benchmark datasets, GEO880 and ATIS. We also demonstrate the effectiveness of our online system by learning a semantic parser from the ground up for an academic domain. A key advantage of our approach is that it is non-language specific and can be easily ported to other commonly used query languages, such as SPARQL or ElasticSearch."}, {"heading": "Acknowledgments", "text": "The research was supported in part by DARPA under the DEFT program by AFRL (FA8750-13-2-0019), ARO (W911NF-16-10121), NSF (IIS-1252835, IIS-1562364, IIS1546083, IIS-1651489, CNS-1563788), DOE (DE-SC0016260), an Allen Distinguished Investigator Award and gifts from NVIDIA, Adobe and Google. Authors thank Rik Koncel-Kedziorski and the anonymous critics for their helpful comments."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We present an approach to rapidly and easily build natural language interfaces to databases for new domains, whose performance improves over time based on user feedback, and requires minimal intervention. To achieve this, we adapt neural sequence models to map utterances directly to SQL with its full expressivity, bypassing any intermediate meaning representations. These models are immediately deployed online to solicit feedback from real users to flag incorrect queries. Finally, the popularity of SQL facilitates gathering annotations for incorrect predictions using the crowd, which is directly used to improve our models. This complete feedback loop, without intermediate representations or database specific engineering, opens up new ways of building high quality semantic parsers. Experiments suggest that this approach can be deployed quickly for any new target domain, as we show by learning a semantic parser for an online academic database from scratch.", "creator": "LaTeX with hyperref package"}}}