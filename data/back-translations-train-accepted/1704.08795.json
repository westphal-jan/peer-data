{"id": "1704.08795", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2017", "title": "Mapping Instructions and Visual Observations to Actions with Reinforcement Learning", "abstract": "We propose to directly map raw visual observations and text input to actions for instruction execution. While existing approaches assume access to structured environment representations or use a pipeline of separately trained models, we learn a single model to jointly reason about linguistic and visual input. We use reinforcement learning in a contextual bandit setting to train a neural network agent. To guide the agent's exploration, we use reward shaping with different forms of supervision. Our approach does not require intermediate representations, planning procedures, or training different models. We evaluate in a simulated environment, and show significant improvements over supervised learning and common reinforcement learning variants.", "histories": [["v1", "Fri, 28 Apr 2017 03:12:57 GMT  (2383kb,D)", "http://arxiv.org/abs/1704.08795v1", null], ["v2", "Sat, 22 Jul 2017 15:10:11 GMT  (4607kb,D)", "http://arxiv.org/abs/1704.08795v2", "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dipendra kumar misra", "john langford", "yoav artzi"], "accepted": true, "id": "1704.08795"}, "pdf": {"name": "1704.08795.pdf", "metadata": {"source": "CRF", "title": "Mapping Instructions and Visual Observations to Actions with Reinforcement Learning", "authors": ["Dipendra K. Misra", "John Langford", "Yoav Artzi"], "emails": ["yoav}@cs.cornell.edu", "jcl@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is in such a way that most of us are able to survive ourselves, if they do not put themselves in a position to survive themselves, \"he said.\" We must put ourselves in a position to survive, \"he said.\" We must put ourselves in a position to put ourselves in the position in which we find ourselves. \"He added:\" We must put ourselves in a position to put ourselves in the position in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live in which we live, in which we live, in which we live, we live in which we live, in which we live, in which we live in which we live, in which we live, in which we live, in which we live, we live in which we live, in which we live, in which we live, we live in which we live, in which we live, we live in which, we live in which, we live in which, we live in which, we live in which, we live in which, we live in which, we live, we live in which, we live in which, we live in which, we live in which, we live in which, we live in which, we live in which, we, we live in which, we live in which, we live in which, we live in which, we live in which, we, we live in which, we live in which, we live in which, we live in which, we live in which, we live in which, we, we live in which, we live in which, we live in which, we live in which, we live in which, we, we live in which, we live in which, we"}, {"heading": "2 Technical Overview", "text": "(...) The conspiracy theory of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories of the conspiracy theories."}, {"heading": "3 Related Work", "text": "Learning to follow instructions has been extensively studied with structured environmental representations, including semantic problem solving (Chen and Mooney, 2011; Kim and Mooney, 2012, 2013; Artzi and Zettlemoyer, 2013; Artzi et al., 2014a, b; Misra et al., 2015, 2016), alignment models (Andreas and Klein, 2015), enhancement of learning (Branavan et al., 2009, 2010; Vogel and Jurafsky, 2010), and neural network models (Mei et al., 2016), we are investigating the problem of an agent used as input and raw visual input. Instruction after visual input has been used with separately learned models of visual reasoning (Matuszek et al., 2010, 2012; Tellex et al., 2011; Paul et al., 2016). Instead of dismantling the problem, we take and learn from instructions that are learning with demonstrations or target states."}, {"heading": "4 Model", "text": "We model the agent-policy with a neural network. The agent observes the instruction and a RGB picture of the world. The agent-commissioner-commissioner-commissioner-commissioner-commissioner-commissioner-commissioner-commissioner-commissioner-commissioner-commissioner-commissioner-commissioner-commissioners-commissioners-commissioners-commissioners-commissioners-commissioners-commissioners-commissioners-commissioners-commissioners-commissioners-commissioners-commissioners-commissioners-commissioners-and-commissioners-commissioners-supervisory-commissioners-commissioners-and-commissioning-commissioners-commissioners-and-commissioners"}, {"heading": "5 Learning", "text": "We assume that access to a training series of N examples (x), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (s), s (i), s (i), s (s), s (i), s (s), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s, s (i), s (i), s, s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i), s (i, s (i), s (i), s (i), s (i, s (i), s (i), s (i), s (i), s (i, s (i), s (i, s (i), s (i), s (i), s (i), s (i, s (i), s"}, {"heading": "6 Reward Shaping", "text": "We use a method for converting a reward function by adding a term to the problem reward function = 1. We use a method for converting the reward function = 1. We use a method for converting the reward function = 1. We use a method for converting the reward function = 1. We use a method for converting the reward function = 1. We use a method for converting the additional algorithms = 1. We use a method for converting the reward function = 1. We use a method for converting the reward function (1). We use a method for converting the reward function (1). We use a method for converting the reward function (1). We use a method for converting the reward function (1). We use a method for converting the reward function (1). We use a method for converting the reward function (1). We use a method for converting the reward function (1)."}, {"heading": "7 Experimental Setup", "text": "In fact, most of them are capable of coming out on top."}, {"heading": "8 Results", "text": "Our approach, which encompasses both terms in a contextual bandit environment, clearly outperforms the other methods.The low performance of REINFORCE and DQN illustrates the challenge of general learning through relatively high sample complexes (Kakade and Langford, 2002).The low performance of REINFORCE and DQN illustrates the challenge of general learning through relatively high sample complexity (Kearns and long-term)."}, {"heading": "9 Conclusions", "text": "Our solution offers an effective combination of both approaches: reward design to create a relatively stable optimization in a contextual bandit environment that uses a signal similar to that of supervised learning, with an amplification base that allows for significant exploration opportunities and easy ways to intelligently initialize. This combination is designed for a regime of a few samples as we address them. If the number of samples is unlimited, the drawbacks observed in this scenario do not hold up to optimizing longer-term reward."}, {"heading": "Acknowledgments", "text": "This research was supported by a Google Faculty Award and an Amazon Web Services Research Grant. We thank Cornell NLP Group and Microsoft Research Machine Learning NYC Group for their support and insightful comments."}, {"heading": "A Reward Shaping Theorems", "text": "In Section 6, we present two terms of reward shaping. We follow the safe shaping theorems of Ng et al. (1999) and Wiewiora et al. (2003). The theorems outline potentially \u2212 \u2212 \u2212 based terms that realize sufficient conditions for safe shaping \u2212 aj. The use of safe terms guarantees the sequence of strategies according to the original problem reward. (While the theory only applies to optimization of overall reward, we empirically show the effectiveness of safe shaping terms in a contextual bandit setting. For convenience, we offer the definitions of potential-based shaping terms and theorems defined by Ng et al. (1999) and Wiewiora et al. (2003) using our notation. We refer the reader to the original papers for full details and forecasts. The distance-based shaping term F1 is defined on the basis of the 1999-based function of safe shaping (the F)."}, {"heading": "B Baseline Systems", "text": "We implement several systems to evaluate our approach. The demonstration system provides a ceiling, while the rest are baselines and existing methods. STOP The agent performs the STOP action right at the start of execution. RANDOM The agent performs random actions until STOP is sampled or J actions have been sampled, with J being the execution horizon. SUPERVISED Given the training data with N-statement-state-execution triplets, we generate training data from state action packages and optimize the log probability of the data. Formally, we optimize the goal: J = 1N N N = 1 m (i)."}, {"heading": "C Parameters and Initialization", "text": "C.1 Architecture ParametersWe use an RGB image of 120x120 pixels = each, the third apply 32 4 \u00d7 4 filters with a step of 2. The last layer performs an affine transformation to create a 200-dimensional vector. We scale all images to have zero mean and standard. We use a single-layer RNN with 150-dimensional word embedding and 250 LSTM units. The dimension of the action, the embedding of a 200-dimensional vector is 56, including 32 for the embedding of the block and 24 for the embedding of the instructions. W (1) is a 506 x 120 matrix and b (1) is a 120-dimensional vector. W (D) is 120 x 20 for 20 blocks, and W (B) is 120 x 5 for the four directions (North, South, East, West) and the STOP action J 04 we are as a 4-dimensional unit."}, {"heading": "D Dataset Comparisons", "text": "The following datasets in Table 4 are briefly reviewed: Blocks (Bisk et al., 2016), SAIL (MacMahon et al., 2006; Chen and Mooney, 2011), Matuszek (Matuszek et al., 2012) and Misra (Misra et al., 2015). Overall, Blocks offers the largest training set and a relatively complex environment with well over 2,4318 possible states. [4] The most similar dataset is SAIL, which provides only partial observation of the environment (i.e. the agent only observes what surrounds him). However, SAIL is less complex in other dimensions related to instructions, trajectories and action space. Furthermore, while there are a large number of possible states, SAIL comprises only 400 states. The limited number of different states in SAIL makes it difficult to learn vision models that generalize well. Misra (Misra et al., 2015) provides a parameterized action space (e.g. a relatively small number of actions that can be taken by us)."}, {"heading": "E Common Questions", "text": "It is indeed the case that we are able to manoeuvre ourselves into a situation where we have to put ourselves at the centre."}], "references": [{"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Alekh Agarwal", "Daniel J. Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert E. Schapire."], "venue": "Proceedings of the International Conference on Machine Learning.", "citeRegEx": "Agarwal et al\\.,? 2014", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Alignmentbased compositional semantics for instruction following", "author": ["Jacob Andreas", "Dan Klein."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. https://doi.org/10.18653/v1/D15-1138.", "citeRegEx": "Andreas and Klein.,? 2015", "shortCiteRegEx": "Andreas and Klein.", "year": 2015}, {"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Andreas et al\\.,? 2016a", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Neural module networks", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."], "venue": "Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "Andreas et al\\.,? 2016b", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "VQA: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh."], "venue": "International Journal of Computer Vision.", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Learning compact lexicons for CCG semantic parsing", "author": ["Yoav Artzi", "Dipanjan Das", "Slav Petrov."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. https://doi.org/10.3115/v1/D14-1134.", "citeRegEx": "Artzi et al\\.,? 2014a", "shortCiteRegEx": "Artzi et al\\.", "year": 2014}, {"title": "Programming by demonstration with situated semantic parsing", "author": ["Yoav Artzi", "Maxwell Forbes", "Kenton Lee", "Maya Cakmak."], "venue": "AAAI Fall Symposium Series.", "citeRegEx": "Artzi et al\\.,? 2014b", "shortCiteRegEx": "Artzi et al\\.", "year": 2014}, {"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions", "author": ["Yoav Artzi", "Luke Zettlemoyer."], "venue": "Transactions of the Association of Computational Linguistics 1:49\u201362. http://aclweb.org/anthology/Q13-1005.", "citeRegEx": "Artzi and Zettlemoyer.,? 2013", "shortCiteRegEx": "Artzi and Zettlemoyer.", "year": 2013}, {"title": "Natural language communication with robots", "author": ["Yonatan Bisk", "Deniz Yuret", "Daniel Marcu."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.", "citeRegEx": "Bisk et al\\.,? 2016", "shortCiteRegEx": "Bisk et al\\.", "year": 2016}, {"title": "Reinforcement learning for mapping instructions to actions", "author": ["S.R.K. Branavan", "Harr Chen", "Luke Zettlemoyer", "Regina Barzilay."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference", "citeRegEx": "Branavan et al\\.,? 2009", "shortCiteRegEx": "Branavan et al\\.", "year": 2009}, {"title": "Reading between the lines: Learning to map high-level instructions to commands", "author": ["S.R.K. Branavan", "Luke Zettlemoyer", "Regina Barzilay."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Branavan et al\\.,? 2010", "shortCiteRegEx": "Branavan et al\\.", "year": 2010}, {"title": "Reinforcement learning from demonstration through shaping", "author": ["Tim Brys", "Anna Harutyunyan", "Halit Bener Suay", "Sonia Chernova", "Matthew E. Taylor", "Ann Now\u00e9."], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence.", "citeRegEx": "Brys et al\\.,? 2015", "shortCiteRegEx": "Brys et al\\.", "year": 2015}, {"title": "Learning to interpret natural language navigation instructions from observations", "author": ["David L. Chen", "Raymond J. Mooney."], "venue": "Proceedings of the National Conference on Artificial Intelligence.", "citeRegEx": "Chen and Mooney.,? 2011", "shortCiteRegEx": "Chen and Mooney.", "year": 2011}, {"title": "Bootstrap, review, decode: Using out-ofdomain textual data to improve image captioning", "author": ["Wenhu Chen", "Aur\u00e9lien Lucchi", "Thomas Hofmann."], "venue": "CoRR abs/1611.05321.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Microsoft COCO captions: Data collection and evaluation server", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C. Lawrence Zitnick."], "venue": "CoRR abs/1504.00325.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Deep reinforcement learning for mention-ranking coreference models", "author": ["Kevin Clark", "D. Christopher Manning."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. http://aclweb.org/anthology/D16-1245.", "citeRegEx": "Clark and Manning.,? 2016", "shortCiteRegEx": "Clark and Manning.", "year": 2016}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman."], "venue": "Cognitive Science 14:179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Deep reinforcement learning with a natural language action space", "author": ["Ji He", "Jianshu Chen", "Xiaodong He", "Jianfeng Gao", "Lihong Li", "Li Deng", "Mari Ostendorf."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Compu-", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning", "author": ["Justin Johnson", "Bharath Hariharan", "Laurens van der Maaten", "Li Fei-Fei", "C. Lawrence Zitnick", "Ross B. Girshick."], "venue": "CoRR abs/1612.06890.", "citeRegEx": "Johnson et al\\.,? 2016", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["Sham Kakade", "John Langford."], "venue": "Machine Learning, Proceedings of the Nineteenth", "citeRegEx": "Kakade and Langford.,? 2002", "shortCiteRegEx": "Kakade and Langford.", "year": 2002}, {"title": "A sparse sampling algorithm for near-optimal planning in large markov decision processes", "author": ["Michael Kearns", "Yishay Mansour", "Andrew Y. Ng."], "venue": "Proeceediings of the International Joint Conference on Artificial Intelligence.", "citeRegEx": "Kearns et al\\.,? 1999", "shortCiteRegEx": "Kearns et al\\.", "year": 1999}, {"title": "Unsupervised PCFG induction for grounded language learning with highly ambiguous supervision", "author": ["Joohyun Kim", "Raymond Mooney."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Kim and Mooney.,? 2012", "shortCiteRegEx": "Kim and Mooney.", "year": 2012}, {"title": "Adapting discriminative reranking to grounded language learning", "author": ["Joohyun Kim", "Raymond Mooney."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).", "citeRegEx": "Kim and Mooney.,? 2013", "shortCiteRegEx": "Kim and Mooney.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Reinforcement learning in robotics: A survey", "author": ["Jens Kober", "J. Andrew Bagnell", "Jan Peters."], "venue": "International Journal of Robotics Research 32:1238\u2013 1274.", "citeRegEx": "Kober et al\\.,? 2013", "shortCiteRegEx": "Kober et al\\.", "year": 2013}, {"title": "PAC reinforcement learning with rich observations", "author": ["Akshay Krishnamurthy", "Alekh Agarwal", "John Langford."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Krishnamurthy et al\\.,? 2016", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2016}, {"title": "The epochgreedy algorithm for multi-armed bandits with side information", "author": ["John Langford", "Tong Zhang."], "venue": "Advances in Neural Information Processing Systems 20, Proceedings of the TwentyFirst Annual Conference on Neural Information", "citeRegEx": "Langford and Zhang.,? 2007", "shortCiteRegEx": "Langford and Zhang.", "year": 2007}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel."], "venue": "Journal of Machine Learning Research 17.", "citeRegEx": "Levine et al\\.,? 2016", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Dan Jurafsky", "Michel Galley", "Jianfeng Gao."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Walk the talk: Connecting language, knowledge, action in route instructions", "author": ["Matthew MacMahon", "Brian Stankiewics", "Benjamin Kuipers."], "venue": "Proceedings of the National Conference on Artificial Intelligence.", "citeRegEx": "MacMahon et al\\.,? 2006", "shortCiteRegEx": "MacMahon et al\\.", "year": 2006}, {"title": "Following directions using statistical machine translation", "author": ["C. Matuszek", "D. Fox", "K. Koscher."], "venue": "Proceedings of the international conference on Human-robot interaction.", "citeRegEx": "Matuszek et al\\.,? 2010", "shortCiteRegEx": "Matuszek et al\\.", "year": 2010}, {"title": "Learning to parse natural language commands to a robot control system", "author": ["C. Matuszek", "E. Herbst", "Luke S. Zettlemoyer", "D. Fox."], "venue": "Proceedings of the International Symposium on Experimental Robotics.", "citeRegEx": "Matuszek et al\\.,? 2012", "shortCiteRegEx": "Matuszek et al\\.", "year": 2012}, {"title": "What to talk about and how? selective generation using lstms with coarse-to-fine alignment", "author": ["Hongyuan Mei", "Mohit Bansal", "R. Matthew Walter."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Mei et al\\.,? 2016", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Tell me dave: Contextsensitive grounding of natural language to manipulation instructions", "author": ["Dipendra K. Misra", "Jaeyong Sung", "Kevin Lee", "Ashutosh Saxena."], "venue": "The International Journal of Robotics Research 35(1-3):281\u2013300.", "citeRegEx": "Misra et al\\.,? 2016", "shortCiteRegEx": "Misra et al\\.", "year": 2016}, {"title": "Environment-driven lexicon induction for high-level instructions", "author": ["Kumar Dipendra Misra", "Kejia Tao", "Percy Liang", "Ashutosh Saxena."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Misra et al\\.,? 2015", "shortCiteRegEx": "Misra et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adri\u00e0 Puigdom\u00e8nech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P. Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu."], "venue": "Proceedings of the International", "citeRegEx": "Mnih et al\\.,? 2016", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin A. Riedmiller."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Mnih et al\\.,? 2013", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski."], "venue": "Nature", "citeRegEx": "Mnih et al\\.,? 2015", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Language understanding for textbased games using deep reinforcement learning", "author": ["Karthik Narasimhan", "Tejas Kulkarni", "Regina Barzilay."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Narasimhan et al\\.,? 2015", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Improving information extraction by acquiring external evidence with reinforcement learning", "author": ["Karthik Narasimhan", "Adam Yala", "Regina Barzilay."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Narasimhan et al\\.,? 2016", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2016}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["Andrew Y. Ng", "Daishi Harada", "Stuart J. Russell."], "venue": "Proceedings of the International Conference on Machine Learning.", "citeRegEx": "Ng et al\\.,? 1999", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "Control of memory, active perception, and action in minecraft", "author": ["Junhyuk Oh", "Valliappa Chockalingam", "Satinder P. Singh", "Honglak Lee."], "venue": "Proceedings of the International Conference on Machine Learning.", "citeRegEx": "Oh et al\\.,? 2016", "shortCiteRegEx": "Oh et al\\.", "year": 2016}, {"title": "Efficient grounding of abstract spatial concepts for natural language interaction with robot manipulators", "author": ["Rohan Paul", "Jacob Arkin", "Nicholas Roy", "Thomas M. Howard."], "venue": "Robotics: Science and Systems.", "citeRegEx": "Paul et al\\.,? 2016", "shortCiteRegEx": "Paul et al\\.", "year": 2016}, {"title": "Sim-to-real robot learning from pixels with progressive nets", "author": ["Andrei A. Rusu", "Matej Vecerik", "Thomas Roth\u00f6rl", "Nicolas Heess", "Razvan Pascanu", "Raia Hadsell."], "venue": "CoRR .", "citeRegEx": "Rusu et al\\.,? 2016", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I. Jordan", "Pieter Abbeel"], "venue": null, "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis."], "venue": "Nature 529 7587:484\u2013", "citeRegEx": "Sutskever et al\\.,? 2016", "shortCiteRegEx": "Sutskever et al\\.", "year": 2016}, {"title": "A corpus of compositional language for visual reasoning", "author": ["Alane Suhr", "Mike Lewis", "James Yeh", "Yoav Artzi."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.", "citeRegEx": "Suhr et al\\.,? 2017", "shortCiteRegEx": "Suhr et al\\.", "year": 2017}, {"title": "Robobarista: Object part based transfer of manipulation trajectories from crowd-sourcing in 3d pointclouds", "author": ["Jaeyong Sung", "Seok Hyun Jin", "Ashutosh Saxena."], "venue": "International Symposium on Robotics Research.", "citeRegEx": "Sung et al\\.,? 2015", "shortCiteRegEx": "Sung et al\\.", "year": 2015}, {"title": "Reinforcement learning: An introduction", "author": ["Richard S. Sutton", "Andrew G. Barto."], "venue": "IEEE Trans. Neural Networks 9:1054\u20131054.", "citeRegEx": "Sutton and Barto.,? 1998", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S. Sutton", "David A. McAllester", "Satinder P. Singh", "Yishay Mansour."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Sutton et al\\.,? 1999", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Understanding natural language commands for robotic navigation and mobile manipulation", "author": ["S. Tellex", "T. Kollar", "S. Dickerson", "M. Walter", "A.G. Banerjee", "S. Teller", "N. Roy."], "venue": "Proceedings of the National Conference on Artificial Intelligence.", "citeRegEx": "Tellex et al\\.,? 2011", "shortCiteRegEx": "Tellex et al\\.", "year": 2011}, {"title": "Learning to follow navigational directions", "author": ["Adam Vogel", "Daniel Jurafsky."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. http://aclweb.org/anthology/P10-1083.", "citeRegEx": "Vogel and Jurafsky.,? 2010", "shortCiteRegEx": "Vogel and Jurafsky.", "year": 2010}, {"title": "Instructions, intentions and expectations", "author": ["B. Webber", "N. Badler", "B. Di Eugenio", "C. Geib", "L. Levison", "M. Moore."], "venue": "Artificial Intelligence 73(1):253\u2013 269.", "citeRegEx": "Webber et al\\.,? 1995", "shortCiteRegEx": "Webber et al\\.", "year": 1995}, {"title": "Principled methods for advising reinforcement learning agents", "author": ["Eric Wiewiora", "Garrison W. Cottrell", "Charles Elkan."], "venue": "Proceedings of the International Conference on Machine Learning.", "citeRegEx": "Wiewiora et al\\.,? 2003", "shortCiteRegEx": "Wiewiora et al\\.", "year": 2003}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams."], "venue": "Machine Learning 8.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Function optimization using connectionist reinforcement learning algorithms", "author": ["Ronald J Williams", "Jing Peng."], "venue": "Connection Science 3(3):241\u2013268.", "citeRegEx": "Williams and Peng.,? 1991", "shortCiteRegEx": "Williams and Peng.", "year": 1991}, {"title": "Understanding natural language", "author": ["Terry Winograd."], "venue": "Cognitive Psychology 3(1):1\u2013191.", "citeRegEx": "Winograd.,? 1972", "shortCiteRegEx": "Winograd.", "year": 1972}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Jamie Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio."], "venue": "Proceedings of the Inter-", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning", "author": ["Yuke Zhu", "Roozbeh Mottaghi", "Eric Kolve", "Joseph J. Lim", "Abhinav Gupta", "Li Fei-Fei", "Ali Farhadi"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2017}, {"title": "Understanding in which scenarios one technique is better than the other is an important question for future", "author": ["Narasimhan"], "venue": null, "citeRegEx": "Narasimhan,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan", "year": 2015}, {"title": "2016) and the maps domain of Vogel and Jurafsky (2010), provide segmented sequences of instructions. Existing approaches take advantage of this segmentation during training", "author": ["Artzi", "Zettlemoyer", "Mei"], "venue": null, "citeRegEx": "Artzi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Artzi et al\\.", "year": 2010}, {"title": "2016) all train on segmented data and test on sequences of instructions by doing inference on one sentence at a time", "author": ["ple", "Chen", "Mooney", "Artzi", "Zettlemoyer", "Mei"], "venue": null, "citeRegEx": "ple et al\\.,? \\Q2016\\E", "shortCiteRegEx": "ple et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 33, "context": "Existing approaches addressing this problem assume structured environment representations (e.g.,. Chen and Mooney, 2011; Mei et al., 2016), or combine separately trained models (e.", "startOffset": 90, "endOffset": 138}, {"referenceID": 49, "context": "We use reinforcement learning (Sutton and Barto, 1998) to observe a broader set of states through exploration.", "startOffset": 30, "endOffset": 54}, {"referenceID": 44, "context": "Following recent work in robotics (e.g., Levine et al., 2016; Rusu et al., 2016), we assume the training environment, in contrast to the test environment, is instrumented and provides access to the state.", "startOffset": 34, "endOffset": 80}, {"referenceID": 41, "context": "We use reward shaping (Ng et al., 1999) to exploit the training data to add to the reward additional information.", "startOffset": 22, "endOffset": 39}, {"referenceID": 27, "context": "setting (Langford and Zhang, 2007), where optimizing the immediate reward is sufficient and has better sample complexity than unconstrained reinforcement learning (Agarwal et al.", "startOffset": 8, "endOffset": 34}, {"referenceID": 0, "context": "setting (Langford and Zhang, 2007), where optimizing the immediate reward is sufficient and has better sample complexity than unconstrained reinforcement learning (Agarwal et al., 2014).", "startOffset": 163, "endOffset": 185}, {"referenceID": 8, "context": "We evaluate with the block world environment and data of Bisk et al. (2016), where each instruction moves one block (Figure 1).", "startOffset": 57, "endOffset": 76}, {"referenceID": 12, "context": "Learning to follow instructions was studied extensively with structured environment representations, including with semantic parsing (Chen and Mooney, 2011; Kim and Mooney, 2012, 2013; Artzi and Zettlemoyer, 2013; Artzi et al., 2014a,b; Misra et al., 2015, 2016), alignment models (Andreas and Klein, 2015), reinforcement learning (Branavan et al.", "startOffset": 133, "endOffset": 262}, {"referenceID": 7, "context": "Learning to follow instructions was studied extensively with structured environment representations, including with semantic parsing (Chen and Mooney, 2011; Kim and Mooney, 2012, 2013; Artzi and Zettlemoyer, 2013; Artzi et al., 2014a,b; Misra et al., 2015, 2016), alignment models (Andreas and Klein, 2015), reinforcement learning (Branavan et al.", "startOffset": 133, "endOffset": 262}, {"referenceID": 1, "context": ", 2015, 2016), alignment models (Andreas and Klein, 2015), reinforcement learning (Branavan et al.", "startOffset": 32, "endOffset": 57}, {"referenceID": 52, "context": ", 2015, 2016), alignment models (Andreas and Klein, 2015), reinforcement learning (Branavan et al., 2009, 2010; Vogel and Jurafsky, 2010), and neural network models (Mei et al.", "startOffset": 82, "endOffset": 137}, {"referenceID": 33, "context": ", 2009, 2010; Vogel and Jurafsky, 2010), and neural network models (Mei et al., 2016).", "startOffset": 67, "endOffset": 85}, {"referenceID": 58, "context": ", 2016b,a), caption generation (e.g., Chen et al., 2015, 2016; Xu et al., 2015), and visual reasoning (Johnson et al.", "startOffset": 31, "endOffset": 79}, {"referenceID": 19, "context": ", 2015), and visual reasoning (Johnson et al., 2016; Suhr et al., 2017).", "startOffset": 30, "endOffset": 71}, {"referenceID": 47, "context": ", 2015), and visual reasoning (Johnson et al., 2016; Suhr et al., 2017).", "startOffset": 30, "endOffset": 71}, {"referenceID": 39, "context": "Reinforcement learning with neural networks has been used for various NLP tasks, including text-based games (Narasimhan et al., 2015; He et al., 2016), information extraction (Narasimhan et al.", "startOffset": 108, "endOffset": 150}, {"referenceID": 17, "context": "Reinforcement learning with neural networks has been used for various NLP tasks, including text-based games (Narasimhan et al., 2015; He et al., 2016), information extraction (Narasimhan et al.", "startOffset": 108, "endOffset": 150}, {"referenceID": 40, "context": ", 2016), information extraction (Narasimhan et al., 2016), co-reference resolution (Clark and Manning, 2016), and chatbots (Li et al.", "startOffset": 32, "endOffset": 57}, {"referenceID": 15, "context": ", 2016), co-reference resolution (Clark and Manning, 2016), and chatbots (Li et al.", "startOffset": 33, "endOffset": 58}, {"referenceID": 29, "context": ", 2016), co-reference resolution (Clark and Manning, 2016), and chatbots (Li et al., 2016).", "startOffset": 73, "endOffset": 90}, {"referenceID": 42, "context": ", 2016) and solving memory puzzles (Oh et al., 2016).", "startOffset": 35, "endOffset": 52}, {"referenceID": 0, "context": "address the data efficiency problem by learning in a contextual bandit setting, which is known to be more tractable (Agarwal et al., 2014), and using reward shaping to increase exploration effectiveness.", "startOffset": 116, "endOffset": 138}, {"referenceID": 0, "context": "address the data efficiency problem by learning in a contextual bandit setting, which is known to be more tractable (Agarwal et al., 2014), and using reward shaping to increase exploration effectiveness. Zhu et al. (2017) address generalization of reinforcement learning to new target goals in visual search by providing the agent an image of the goal state.", "startOffset": 117, "endOffset": 222}, {"referenceID": 25, "context": "Reinforcement learning is extensively used in robotics (Kober et al., 2013).", "startOffset": 55, "endOffset": 75}, {"referenceID": 38, "context": "The agent also has access to K images of previous states and the previous action to distinguish between different stages of the execution (Mnih et al., 2015).", "startOffset": 138, "endOffset": 157}, {"referenceID": 16, "context": "We use a recurrent neural network (RNN; Elman, 1990) with a long shortterm memory (LSTM; Hochreiter and Schmidhuber, 1997) recurrence to map the instruction x\u0304 = \u3008x1, .", "startOffset": 34, "endOffset": 52}, {"referenceID": 18, "context": "We use a recurrent neural network (RNN; Elman, 1990) with a long shortterm memory (LSTM; Hochreiter and Schmidhuber, 1997) recurrence to map the instruction x\u0304 = \u3008x1, .", "startOffset": 82, "endOffset": 122}, {"referenceID": 39, "context": "The instruction representation x\u0304 is computed by applying the LSTM recurrence to generate a sequence of hidden states li = LSTM(\u03c8(xi), li\u22121), and computing the mean x\u0304 = 1 n \u2211n i=1 li (Narasimhan et al., 2015).", "startOffset": 184, "endOffset": 209}, {"referenceID": 37, "context": "sual state v (Mnih et al., 2013).", "startOffset": 13, "endOffset": 32}, {"referenceID": 55, "context": "We use policy gradient for reinforcement learning (Williams, 1992) to estimate the parameters", "startOffset": 50, "endOffset": 66}, {"referenceID": 50, "context": "The objective is an adaptation of the policy gradient objective defined by Sutton et al. (1999) to multiple starting states, one for each example.", "startOffset": 75, "endOffset": 96}, {"referenceID": 27, "context": "The primary theoretical advantage of a contextual bandit setting is much tighter sample complexity bounds when comparing upper bounds for contextual bandits (Langford and Zhang, 2007) to lower bounds (Krishnamurthy et al.", "startOffset": 157, "endOffset": 183}, {"referenceID": 26, "context": "The primary theoretical advantage of a contextual bandit setting is much tighter sample complexity bounds when comparing upper bounds for contextual bandits (Langford and Zhang, 2007) to lower bounds (Krishnamurthy et al., 2016) or upper bounds (Kearns et al.", "startOffset": 200, "endOffset": 228}, {"referenceID": 21, "context": ", 2016) or upper bounds (Kearns et al., 1999) for total reward maximization.", "startOffset": 24, "endOffset": 45}, {"referenceID": 56, "context": "To delay premature convergence we add an entropy term to the objective (Williams and Peng, 1991; Mnih et al., 2016).", "startOffset": 71, "endOffset": 115}, {"referenceID": 36, "context": "To delay premature convergence we add an entropy term to the objective (Williams and Peng, 1991; Mnih et al., 2016).", "startOffset": 71, "endOffset": 115}, {"referenceID": 36, "context": "Similar issues with vanilla policy gradient were recently reported for other tasks (Mnih et al., 2016).", "startOffset": 83, "endOffset": 102}, {"referenceID": 24, "context": "dients using ADAM (Kingma and Ba, 2014).", "startOffset": 18, "endOffset": 39}, {"referenceID": 24, "context": "ADAM(\u2206) applies a per-feature learning rate to the gradient \u2206 (Kingma and Ba, 2014).", "startOffset": 62, "endOffset": 83}, {"referenceID": 41, "context": "2 Ng et al. (1999) and Wiewiora et al.", "startOffset": 2, "endOffset": 19}, {"referenceID": 41, "context": "2 Ng et al. (1999) and Wiewiora et al. (2003) outline potential-based terms that realize sufficient conditions for safe shaping.", "startOffset": 2, "endOffset": 46}, {"referenceID": 41, "context": "For convenience, we briefly overview the theorems of Ng et al. (1999) and Wiewiora et al.", "startOffset": 53, "endOffset": 70}, {"referenceID": 41, "context": "For convenience, we briefly overview the theorems of Ng et al. (1999) and Wiewiora et al. (2003) in Appendix A.", "startOffset": 53, "endOffset": 97}, {"referenceID": 41, "context": "We design it to be a safe potential-based term (Ng et al., 1999):", "startOffset": 47, "endOffset": 64}, {"referenceID": 11, "context": "We incorporate complete trajectories by using a simplification of the shaping term introduced by Brys et al. (2015). Unlike F1, it requires access to the previous state and action.", "startOffset": 97, "endOffset": 116}, {"referenceID": 11, "context": "We incorporate complete trajectories by using a simplification of the shaping term introduced by Brys et al. (2015). Unlike F1, it requires access to the previous state and action. It is based on the look-back advice shaping term of Wiewiora et al. (2003), who introduced safe potential-based shaping that considers the previous state and action.", "startOffset": 97, "endOffset": 256}, {"referenceID": 8, "context": "Environment We use the environment of Bisk et al. (2016). The original task required predicting the source and target positions for a single block given an instruction.", "startOffset": 38, "endOffset": 57}, {"referenceID": 8, "context": "Data Bisk et al. (2016) collected a corpus of instructions paired with start and goal states.", "startOffset": 5, "endOffset": 24}, {"referenceID": 30, "context": "used SAIL navigation corpus (MacMahon et al., 2006; Chen and Mooney, 2011).", "startOffset": 28, "endOffset": 74}, {"referenceID": 12, "context": "used SAIL navigation corpus (MacMahon et al., 2006; Chen and Mooney, 2011).", "startOffset": 28, "endOffset": 74}, {"referenceID": 8, "context": "In contrast, Bisk et al. (2016) evaluate the selection", "startOffset": 13, "endOffset": 32}, {"referenceID": 38, "context": "actions, (c) SUPERVISED: supervised learning with maximum-likelihood estimate using stateaction pairs from the demonstrations, (d) DQN: deep Q-learning with both shaping terms (Mnih et al., 2015), and (e) REINFORCE: policy gra-", "startOffset": 176, "endOffset": 195}, {"referenceID": 50, "context": "dient with cumulative episodic reward with both shaping terms (Sutton et al., 1999).", "startOffset": 62, "endOffset": 83}, {"referenceID": 20, "context": "A likely explanation is test-time execution errors leading to unfamiliar states with poor later performance (Kakade and Langford, 2002), a form of the covariate shift problem.", "startOffset": 108, "endOffset": 135}, {"referenceID": 21, "context": "limited data due to relatively high sample complexity (Kearns et al., 1999; Krishnamurthy et al., 2016).", "startOffset": 54, "endOffset": 103}, {"referenceID": 26, "context": "limited data due to relatively high sample complexity (Kearns et al., 1999; Krishnamurthy et al., 2016).", "startOffset": 54, "endOffset": 103}, {"referenceID": 29, "context": "While the contribution of initialization is modest, it provides faster learning (Li et al., 2016).", "startOffset": 80, "endOffset": 97}], "year": 2017, "abstractText": "We propose to directly map raw visual observations and text input to actions for instruction execution. While existing approaches assume access to structured environment representations or use a pipeline of separately trained models, we learn a single model to jointly reason about linguistic and visual input. We use reinforcement learning in a contextual bandit setting to train a neural network agent. To guide the agent\u2019s exploration, we use reward shaping with different forms of supervision. Our approach does not require intermediate representations, planning procedures, or training different models. We evaluate in a simulated environment, and show significant improvements over supervised learning and common reinforcement learning variants.", "creator": "LaTeX with hyperref package"}}}