{"id": "1704.08847", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2017", "title": "Parseval Networks: Improving Robustness to Adversarial Examples", "abstract": "We introduce Parseval networks, a form of deep neural networks in which the Lipschitz constant of linear, convolutional and aggregation layers is constrained to be smaller than 1. Parseval networks are empirically and theoretically motivated by an analysis of the robustness of the predictions made by deep neural networks when their input is subject to an adversarial perturbation. The most important feature of Parseval networks is to maintain weight matrices of linear and convolutional layers to be (approximately) Parseval tight frames, which are extensions of orthogonal matrices to non-square matrices. We describe how these constraints can be maintained efficiently during SGD. We show that Parseval networks match the state-of-the-art in terms of accuracy on CIFAR-10/100 and Street View House Numbers (SVHN) while being more robust than their vanilla counterpart against adversarial examples. Incidentally, Parseval networks also tend to train faster and make a better usage of the full capacity of the networks.", "histories": [["v1", "Fri, 28 Apr 2017 08:43:55 GMT  (689kb,D)", "https://arxiv.org/abs/1704.08847v1", "submitted"], ["v2", "Tue, 2 May 2017 01:11:21 GMT  (689kb,D)", "http://arxiv.org/abs/1704.08847v2", "submitted"]], "COMMENTS": "submitted", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.CR cs.LG", "authors": ["moustapha ciss\u00e9", "piotr bojanowski", "edouard grave", "yann dauphin", "nicolas usunier"], "accepted": true, "id": "1704.08847"}, "pdf": {"name": "1704.08847.pdf", "metadata": {"source": "META", "title": "Parseval Networks: Improving Robustness to Adversarial Examples", "authors": ["Moustapha Cisse", "Piotr Bojanowski", "Edouard Grave", "Yann Dauphin", "Nicolas Usunier"], "emails": ["<moustaphacisse@fb.com>."], "sections": [{"heading": "1. Introduction", "text": "It is a question of whether and how such a development can occur. (...) It is a question of the extent to which such a development can occur. (...) It is a question of the extent to which such a development can occur. (...) It is a question of the extent to which such a development can occur. (...) It is a question of the extent to which such a development can occur. (...) It is a question of the extent to which such a development can occur. (...) It is a question of the extent to which such a development can occur. (...) It is a question of the extent to which such a development is even possible. (...)"}, {"heading": "2. Related work", "text": "Some authors argued that this sensitivity of deep networks is due to small changes in their input factors because neural networks only learn discriminatory information sufficient for good accuracy, rather than grasping the true concepts that define the classes (Fawzi et al., 2015; Nguyen et al., 2015). Strategies to improve the robustness of deep networks include defensive distillation (Papernot et al., 2016b) and various regulatory processes such as contractive networks (Gu & Rigazio, 2015). However, the majority of recent proposals rely on data augmentation (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Shaham et al., 2014; Kurakin certain isolation examples, we will consider parversation data during the online experiment as arival."}, {"heading": "3. Robustness in Neural Networks", "text": "We consider a multi-class prediction setting where we have Y classes in Y = {1,..., Y}, where W are the parameters to be learned, and a multi-class classifier is a function g = (x-RD, W-W) given to the (input, class) pair (x, y-RY) by a function g: RD \u00b7 W \u2192 RY. We take g as a neural network represented by a mathematical graph G = (N, E), which is a directed acyclic graph with a single root node, and each node n-N takes values out in Rd (n) and is a function of its children in the graph, with learnable parameters W (n): x-RIP (n) (n-RP) n: (n-RP)."}, {"heading": "3.1. Adversarial examples", "text": "In view of an input (traction or test) example (x, y), a counterexample is a disturbance of the input pattern x = x + \u03b4x, where \u03b4x is small enough so that x * (at least from the point of view of a human annotator) is almost indistinguishable from x, but the network predicts an incorrect designation. In view of the network parameters and structure g (., W) and a p-norm, the counterexample is formally defined as x = argmax x *. (2015) Shaham et al. (2015) propose to calculate the first-order Taylor character with an extension of x 7 (g (x, W), y), representing the strength of the opponent. (Since the optimization problem above is not convex, Shaham et al. (2015) result in the first-order Taylor character with an extension of x 7 (g (x, W), y) with a fast (x)."}, {"heading": "3.2. Generalization with adversarial examples", "text": "In the context of conflicting examples, there are two different generalization errors of interest: L (W) = E (x, y) \u2022 D ['(g (x, W), y)], Ladv (W, p,) = E (x, y) \u2022 D [maxx) = X (x, y).By definition, we have: L (W) \u2264 Ladv (W, p) + E (x, y) for each p and > 0. Conversely, we have: Ladv (W, p,) \u2264 L (W) + E (x, y) \u2012 D [maxx: x \u2212 x] p (g (x, W) p), the Lipschitz constant (in relation to the generalization of \"and g) is (in relation to the generalization of.\""}, {"heading": "3.3. Lipschitz constant of neural networks", "text": "s \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 (n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 m \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n"}, {"heading": "4. Parseval networks", "text": "The parseval regularization that we present in this section is a regularization scheme to make deep neural networks robust by limiting the Lipschitz constant (5) of each hidden layer to less than one, provided that the Lipschitz constant of the infant nodes is smaller than one. In this way, we avoid the exponential growth of the Lipschitz constant, and a common regularization scheme (i.e. the weight decay) on the last layer then controls the entire Lipschitz constant of the network. In order to enforce these limitations in practice, parseval networks use two ideas: maintaining oral rows in linear / conventional layers and performing convex combinations in aggregation layers. In the following, we will first explain the reasons for these limitations and then describe our approach to enforce the limitations efficiently during training."}, {"heading": "4.1. Parseval Regularization", "text": "Orthonormality of weight matrices: For linear layers, we must keep the spectral norm of the weight matrix at 1. Calculation of the largest singular value of weight matrices is not practicable in an SGD setting unless the rows of the matrix are held orthogonally. W is then approximately a parseval narrow frame (Kovac, evic, Chebira, 2008), hence the name of the parseval networks. For revolutionary layers, the matrix receives W, Idout, where I refer to the identity matrix. W is then approximately a parseval narrow frame (Kovac, evic, Chebira, 2008), hence the name of the parseval networks."}, {"heading": "4.2. Parseval Training", "text": "The first significant difference between the parameter updates and their parameters is the orthogonal limitation to the weight matrices. (This condition requires an optimization algorithm to the multiplicity of the orthogonal matrices, namely the unlimited function of interest by moving in the direction of the steeliest descent (given by the gradient of the function) while at the same time remaining on the manifold. (To guarantee that we remain in the parameter updates, we must define a feedback operator. There are several pullback operators for embedded submanifolds such as the boots manifold on the basis of Cayley. (Absiet al al al al al al al al al al al al al al al al al al)."}, {"heading": "5. Experimental evaluation", "text": "We evaluate the effectiveness of parseval networks using established image classification benchmark data sets, namely MNIST, CIFAR-10, CIFAR-100 (Krizhevsky, 2009) and Street View House Numbers (SVHN) (Netzer et al.). We train both fully connected networks and large residual networks. Details of data sets, models and training routines are summarized below."}, {"heading": "5.1. Datasets", "text": "CIFAR-10 and CIFAR-100 have 10 and 100 classes, respectively, and for these two sets of data we adopt the following standard preprocessing and data magnification scheme (Lin et al., 2013; He et al., 2016; Huang et al., 2016a; Zagoruyko & Komodakis, 2016): Each training image is initially replenished with 4 pixels on each side, the resulting image is randomly cropped to create a new 32 x 32 image, which is then flipped horizontally with a 0.5 probability. We also normalize each image with the mean and standard deviation of its channels. Following the same practice as (Huang et al., 2016a), we use 5K images from the training as an update set."}, {"heading": "5.2. Models and Implementation details", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1. CONVNETS", "text": "Models. For the CIFAR and SVHN data sets, we trained wide residual nets (Zagoruyko & Komodakis, 2016), as they perform on par with standard resnets (Er et al., 2016), while exercising faster thanks to a reduced depth. We used wide resnets of depth 28 and width 10 for both CIFAR-10 and CIFAR-100. For SVHN, we used wide resnet of depth 16 and width 4. For each architecture, we compared parseval networks with the vanilla model, which uses standard regulation in both adversarial and non-adversarial training settings. Training of networks with stochastic gradient descent with a dynamic of 0.9. On CIFAR data sets, the initial learning rate is set to 0.1 and by a factor of 0.2 for periods 60, 120 and 160, for a total of 200 epochs."}, {"heading": "5.2.2. FULLY CONNECTED", "text": "We use these models on MNIST and CIFAR10 mainly to show that the proposed approach is also useful on non-revolutionary networks. We compare a parseval network with vanilla models with and without weight loss regulation. For contractually trained models, we follow the previously described guidelines for the Convolutionary Networks. We train the models with SGD and divide the learning rate by two every 10 epochs. We use mini-batches of size 100 and train the model for 50 epochs. We selected the hyperparameters on the validation set and re-trained the model based on the merging of training and validation sets. The hyperparameters are \u03b2, the size of the subset S, the learning rate and the reduction rate."}, {"heading": "5.3. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.3.1. (QUASI)-ORTHOGONALITY.", "text": "First, we confirm that the parseval training (algorithm 1) actually yields (nearly) orthonormous weight matrices. To this end, we analyze the spectrum of weight matrices of the different models by plotting the histograms of their singular values and compare these histograms for parseval networks with networks trained with standard SGD with and without weight decay (SGD-wd and SGD). Histograms representing the distribution of singular values in layers 1 and 4 for the fully connected network (using S = 30%) trained on the CIFAR-10 dataset are shown in Fig. 2 (the numbers for Constitutional Networks are similar). The singular values obtained with our method are narrowly focused around 1. This experiment confirms that the weight matrices generated by the proposed optimization method are (almost) orthonormally motivated by weight matrices (the larger weight matrices are clearly motivated by the younger ones in the networks)."}, {"heading": "5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.", "text": "In fact, most of them are able to trump themselves when they don't see themselves able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves."}, {"heading": "5.3.3. BETTER USE OF CAPACITY", "text": "In view of the distribution of the singular values observed in Figure 2, we want to analyze the intrinsic dimensionality of the representation learned from the various networks at each level. To this end, we use the local covariance dimension (Dasgupta & Freund, 2008), which can be measured from the covariance matrix of the data. For each layer k of the fully connected network, we calculate the empirical covariance matrix 1n \u2211 n = 1 \u03c6k (x) \u03c6k (x) > and obtain its sorted eigenvalues \u03c31 \u2265 \u00b7 \u03c3d. For each layer and layer, we select the smallest integral dimension p so that we can p i = 1 \u03c3i \u2265 0.99 \u0445 d = 1 \u03c3i. This gives us the number of dimensions we need to explain 99% of the covariance. We can also calculate the same quantity for the examples of each layer by computing the codicity layers, the codicity of the calculation layers, only in the empirical estimation of the codicity of the examples."}, {"heading": "5.3.4. FASTER CONVERGENCE", "text": "Partial networks converge significantly faster than vanilla networks trained with batch normalization and dropout, as shown in Figure 4. Thanks to the orthogonalization step after each gradient update, the weight matrices are well conditioned at each step during optimization. We suspect that this is the main explanation of this phenomenon. In the case of conventional networks (resnets), faster convergence is not achieved at the expense of greater wall time, as the cost of the projection step is negligible compared to the total cost of transmission to the modern GPU architecture due to the small size of the filters."}, {"heading": "6. Conclusion", "text": "We have proposed an algorithm that enables us to optimize the model efficiently. Empirical results on three classification datasets with fully interconnected and broad residual networks illustrate the performance of our approach. As a by-product of the regulation we propose, the model travels faster and makes better use of its capacities. Further studies of this phenomenon will be left to future work."}], "references": [{"title": "Optimization algorithms on matrix manifolds", "author": ["Absil", "P-A", "Mahony", "Robert", "Sepulchre", "Rodolphe"], "venue": null, "citeRegEx": "Absil et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Absil et al\\.", "year": 2009}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Amodei", "Dario", "Anubhai", "Rishita", "Battenberg", "Eric", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan", "Chen", "Jingdong", "Chrzanowski", "Mike", "Coates", "Adam", "Diamos", "Greg"], "venue": "arXiv preprint arXiv:1512.02595,", "citeRegEx": "Amodei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Amodei et al\\.", "year": 2015}, {"title": "Fast projection onto the simplex and the\\ pmb {l} _\\ mathbf", "author": ["Condat", "Laurent"], "venue": "ball. Mathematical Programming,", "citeRegEx": "Condat and Laurent.,? \\Q2016\\E", "shortCiteRegEx": "Condat and Laurent.", "year": 2016}, {"title": "Random projection trees and low dimensional manifolds", "author": ["Dasgupta", "Sanjoy", "Freund", "Yoav"], "venue": "In Proceedings of the fortieth annual ACM symposium on Theory of computing,", "citeRegEx": "Dasgupta et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2008}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Denton", "Emily L", "Zaremba", "Wojciech", "Bruna", "Joan", "LeCun", "Yann", "Fergus", "Rob"], "venue": "In Adv. NIPS,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Fast monte carlo algorithms for matrices i: Approximating matrix multiplication", "author": ["Drineas", "Petros", "Kannan", "Ravi", "Mahoney", "Michael W"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Drineas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "Improving generalization performance using double backpropagation", "author": ["Drucker", "Harris", "Le Cun", "Yann"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Drucker et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Drucker et al\\.", "year": 1992}, {"title": "Efficient projections onto the l 1-ball for learning in high dimensions", "author": ["Duchi", "John", "Shalev-Shwartz", "Shai", "Singer", "Yoram", "Chandra", "Tushar"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "Analysis of classifiers\u2019 robustness to adversarial perturbations", "author": ["Fawzi", "Alhussein", "Omar", "Frossard", "Pascal"], "venue": "arXiv preprint arXiv:1502.02590,", "citeRegEx": "Fawzi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fawzi et al\\.", "year": 2015}, {"title": "Robustness of classifiers: from adversarial to random noise", "author": ["Fawzi", "Alhussein", "Moosavi-Dezfooli", "Seyed-Mohsen", "Frossard", "Pascal"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Fawzi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fawzi et al\\.", "year": 2016}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "In Proc. ICLR,", "citeRegEx": "Goodfellow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["Gu", "Shixiang", "Rigazio", "Luca"], "venue": "In ICLR workshop,", "citeRegEx": "Gu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Densely connected convolutional networks", "author": ["Huang", "Gao", "Liu", "Zhuang", "Weinberger", "Kilian Q", "van der Maaten", "Laurens"], "venue": "arXiv preprint arXiv:1608.06993,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Deep networks with stochastic depth", "author": ["Huang", "Gao", "Sun", "Yu", "Liu", "Zhuang", "Sedra", "Daniel", "Weinberger", "Kilian Q"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Independent component analysis: algorithms and applications", "author": ["Hyv\u00e4rinen", "Aapo", "Oja", "Erkki"], "venue": "Neural networks,", "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2000}, {"title": "An introduction to frames", "author": ["Kova\u010devi\u0107", "Jelena", "Chebira", "Amina"], "venue": "Foundations and Trends in Signal Processing,", "citeRegEx": "Kova\u010devi\u0107 et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kova\u010devi\u0107 et al\\.", "year": 2008}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex"], "venue": null, "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Adversarial machine learning at scale", "author": ["Kurakin", "Alexey", "Goodfellow", "Ian", "Bengio", "Samy"], "venue": "arXiv preprint arXiv:1611.01236,", "citeRegEx": "Kurakin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kurakin et al\\.", "year": 2016}, {"title": "Delving into transferable adversarial examples and black-box attacks", "author": ["Liu", "Yanpei", "Chen", "Xinyun", "Chang", "Song", "Dawn"], "venue": "CoRR, abs/1611.02770,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "A finite algorithm for finding the projection of a point onto the canonical simplex of? n", "author": ["Michelot", "Christian"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Michelot and Christian.,? \\Q1986\\E", "shortCiteRegEx": "Michelot and Christian.", "year": 1986}, {"title": "Distributional smoothing with virtual adversarial training", "author": ["Miyato", "Takeru", "Maeda", "Shin-ichi", "Koyama", "Masanori", "Nakae", "Ken", "Ishii", "Shin"], "venue": "In Proc. ICLR,", "citeRegEx": "Miyato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2015}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "author": ["Moosavi-Dezfooli", "Seyed-Mohsen", "Fawzi", "Alhussein", "Frossard", "Pascal"], "venue": "arXiv preprint arXiv:1511.04599,", "citeRegEx": "Moosavi.Dezfooli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Moosavi.Dezfooli et al\\.", "year": 2015}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Nguyen", "Anh", "Yosinski", "Jason", "Clune", "Jeff"], "venue": "In Proc. CVPR,", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["Papernot", "Nicolas", "McDaniel", "Patrick", "Goodfellow", "Ian", "Jha", "Somesh", "Z Berkay Celik", "Swami", "Ananthram"], "venue": "arXiv preprint arXiv:1602.02697,", "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["Papernot", "Nicolas", "McDaniel", "Patrick", "Wu", "Xi", "Jha", "Somesh", "Swami", "Ananthram"], "venue": "In Security and Privacy (SP),", "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "An algorithm for a singly constrained class of quadratic programs subject to upper and lower bounds", "author": ["Pardalos", "Panos M", "Kovoor", "Naina"], "venue": "Mathematical Programming,", "citeRegEx": "Pardalos et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Pardalos et al\\.", "year": 1990}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["Salimans", "Tim", "Kingma", "Diederik P"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Understanding adversarial training: Increasing local stability of neural nets through robust optimization", "author": ["Shaham", "Uri", "Yamada", "Yutaro", "Negahban", "Sahand"], "venue": "arXiv preprint arXiv:1511.05432,", "citeRegEx": "Shaham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shaham et al\\.", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian", "Fergus", "Rob"], "venue": "In Proc. ICLR,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Robustness and generalization", "author": ["Xu", "Huan", "Mannor", "Shie"], "venue": "Machine learning,", "citeRegEx": "Xu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2012}, {"title": "Wide residual networks", "author": ["Zagoruyko", "Sergey", "Komodakis", "Nikos"], "venue": "arXiv preprint arXiv:1605.07146,", "citeRegEx": "Zagoruyko et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "Deep neural networks achieve near-human accuracy on many perception tasks (He et al., 2016; Amodei et al., 2015).", "startOffset": 74, "endOffset": 112}, {"referenceID": 1, "context": "Deep neural networks achieve near-human accuracy on many perception tasks (He et al., 2016; Amodei et al., 2015).", "startOffset": 74, "endOffset": 112}, {"referenceID": 29, "context": "However, they lack robustness to small alterations of the inputs at test time (Szegedy et al., 2014).", "startOffset": 78, "endOffset": 100}, {"referenceID": 10, "context": "In practice, for a significant portion of inputs, a single step in the direction of the gradient sign is sufficient to generate an adversarial example (Goodfellow et al., 2015) that is even transferable from one network to another one trained for the same problem but with a different architecture (Liu et al.", "startOffset": 151, "endOffset": 176}, {"referenceID": 19, "context": ", 2015) that is even transferable from one network to another one trained for the same problem but with a different architecture (Liu et al., 2016; Kurakin et al., 2016).", "startOffset": 129, "endOffset": 169}, {"referenceID": 18, "context": ", 2015) that is even transferable from one network to another one trained for the same problem but with a different architecture (Liu et al., 2016; Kurakin et al., 2016).", "startOffset": 129, "endOffset": 169}, {"referenceID": 29, "context": "Whereas the earliest works on adversarial examples already suggested that their existence was related to the magnitude of the hidden activations gradient with respect to their inputs (Szegedy et al., 2014), they also empirically assessed that standard regularization schemes such as weight decay or training with random noise do not solve the problem (Goodfellow et al.", "startOffset": 183, "endOffset": 205}, {"referenceID": 10, "context": ", 2014), they also empirically assessed that standard regularization schemes such as weight decay or training with random noise do not solve the problem (Goodfellow et al., 2015; Fawzi et al., 2016).", "startOffset": 153, "endOffset": 198}, {"referenceID": 9, "context": ", 2014), they also empirically assessed that standard regularization schemes such as weight decay or training with random noise do not solve the problem (Goodfellow et al., 2015; Fawzi et al., 2016).", "startOffset": 153, "endOffset": 198}, {"referenceID": 10, "context": "It consists in generating adversarial examples on-line using the current network\u2019s parameters (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Szegedy et al., 2014; Kurakin et al., 2016) and adding them to the training data.", "startOffset": 94, "endOffset": 215}, {"referenceID": 21, "context": "It consists in generating adversarial examples on-line using the current network\u2019s parameters (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Szegedy et al., 2014; Kurakin et al., 2016) and adding them to the training data.", "startOffset": 94, "endOffset": 215}, {"referenceID": 22, "context": "It consists in generating adversarial examples on-line using the current network\u2019s parameters (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Szegedy et al., 2014; Kurakin et al., 2016) and adding them to the training data.", "startOffset": 94, "endOffset": 215}, {"referenceID": 29, "context": "It consists in generating adversarial examples on-line using the current network\u2019s parameters (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Szegedy et al., 2014; Kurakin et al., 2016) and adding them to the training data.", "startOffset": 94, "endOffset": 215}, {"referenceID": 18, "context": "It consists in generating adversarial examples on-line using the current network\u2019s parameters (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Szegedy et al., 2014; Kurakin et al., 2016) and adding them to the training data.", "startOffset": 94, "endOffset": 215}, {"referenceID": 28, "context": "This data augmentation method can be interpreted as a robust optimization procedure (Shaham et al., 2015).", "startOffset": 84, "endOffset": 105}, {"referenceID": 29, "context": "matrix could help in the context of robustness appeared as early as (Szegedy et al., 2014), but no experiment nor algorithm was proposed, and no clear conclusion was drawn on how to deal with convolutional layers.", "startOffset": 68, "endOffset": 90}, {"referenceID": 12, "context": "First, we provide a deeper analysis which applies to fully connected networks, convolutional networks, as well as Residual networks (He et al., 2016).", "startOffset": 132, "endOffset": 149}, {"referenceID": 29, "context": "Early papers on adversarial examples attributed the vulnerability of deep networks to high local variations (Szegedy et al., 2014; Goodfellow et al., 2015).", "startOffset": 108, "endOffset": 155}, {"referenceID": 10, "context": "Early papers on adversarial examples attributed the vulnerability of deep networks to high local variations (Szegedy et al., 2014; Goodfellow et al., 2015).", "startOffset": 108, "endOffset": 155}, {"referenceID": 8, "context": "Some authors argued that this sensitivity of deep networks to small changes in their inputs is because neural networks only learn the discriminative information sufficient to obtain good accuracy rather than capturing the true concepts defining the classes (Fawzi et al., 2015; Nguyen et al., 2015).", "startOffset": 257, "endOffset": 298}, {"referenceID": 23, "context": "Some authors argued that this sensitivity of deep networks to small changes in their inputs is because neural networks only learn the discriminative information sufficient to obtain good accuracy rather than capturing the true concepts defining the classes (Fawzi et al., 2015; Nguyen et al., 2015).", "startOffset": 257, "endOffset": 298}, {"referenceID": 10, "context": "However, the bulk of recent proposals relies on data augmentation (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Shaham et al., 2015; Szegedy et al., 2014; Kurakin et al., 2016).", "startOffset": 66, "endOffset": 208}, {"referenceID": 21, "context": "However, the bulk of recent proposals relies on data augmentation (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Shaham et al., 2015; Szegedy et al., 2014; Kurakin et al., 2016).", "startOffset": 66, "endOffset": 208}, {"referenceID": 22, "context": "However, the bulk of recent proposals relies on data augmentation (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Shaham et al., 2015; Szegedy et al., 2014; Kurakin et al., 2016).", "startOffset": 66, "endOffset": 208}, {"referenceID": 28, "context": "However, the bulk of recent proposals relies on data augmentation (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Shaham et al., 2015; Szegedy et al., 2014; Kurakin et al., 2016).", "startOffset": 66, "endOffset": 208}, {"referenceID": 29, "context": "However, the bulk of recent proposals relies on data augmentation (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Shaham et al., 2015; Szegedy et al., 2014; Kurakin et al., 2016).", "startOffset": 66, "endOffset": 208}, {"referenceID": 18, "context": "However, the bulk of recent proposals relies on data augmentation (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Shaham et al., 2015; Szegedy et al., 2014; Kurakin et al., 2016).", "startOffset": 66, "endOffset": 208}, {"referenceID": 28, "context": "Since the optimization problem above is non-convex, Shaham et al. (2015) propose to take the first order taylor expansion of x 7\u2192 `(g(x,W ), y) to compute \u03b4x by solving", "startOffset": 52, "endOffset": 73}, {"referenceID": 12, "context": "Aggregation layers/transfer functions: Layers that perform the sum of their inputs, as in Residual Netowrks (He et al., 2016), fall in the case where the values \u039b \u2032) p in (5) come into play.", "startOffset": 108, "endOffset": 125}, {"referenceID": 0, "context": "Optimization on matrix manifolds is a well-studied topic (see (Absil et al., 2009) for a comprehensive survey).", "startOffset": 62, "endOffset": 82}, {"referenceID": 0, "context": "There exist several pullback operators for embedded submanifolds such as the Stiefel manifold based for example on Cayley transforms (Absil et al., 2009).", "startOffset": 133, "endOffset": 153}, {"referenceID": 5, "context": "Provided the rows are carefully sampled, the procedure is an accurate Monte Carlo approximation of the regularizer loss function (Drineas et al., 2006).", "startOffset": 129, "endOffset": 151}, {"referenceID": 12, "context": ", their sum as in Residual networks (He et al., 2016).", "startOffset": 36, "endOffset": 53}, {"referenceID": 7, "context": "This is a well studied problem (Michelot, 1986; Pardalos & Kovoor, 1990; Duchi et al., 2008; Condat, 2016).", "startOffset": 31, "endOffset": 106}, {"referenceID": 7, "context": ",K)|1+k\u03b1k > \u2211 j\u2264k \u03b1j}, the optimal thresholding is given by (Duchi et al., 2008):", "startOffset": 60, "endOffset": 80}, {"referenceID": 7, "context": "In this work, we use the method detailed above (Duchi et al., 2008) to perform the projection of the coefficient \u03b1 after every gradient update step.", "startOffset": 47, "endOffset": 67}, {"referenceID": 12, "context": "For these two datasets, we adopt the following standard preprocessing and data augmentation scheme (Lin et al., 2013; He et al., 2016; Huang et al., 2016a; Zagoruyko & Komodakis, 2016): Each training image is first zero-padded with 4 pixels on each side.", "startOffset": 99, "endOffset": 184}, {"referenceID": 12, "context": "For the CIFAR and SVHN datasets, we trained wide residual networks (Zagoruyko & Komodakis, 2016) as they perform on par with standard resnets (He et al., 2016) while being faster to train thanks to a reduced depth.", "startOffset": 142, "endOffset": 159}, {"referenceID": 10, "context": "In all cases, We also adversarially trained each of the models on CIFAR-10 and CIFAR-100 following the guidelines in (Goodfellow et al., 2015; Shaham et al., 2015; Kurakin et al., 2016).", "startOffset": 117, "endOffset": 185}, {"referenceID": 28, "context": "In all cases, We also adversarially trained each of the models on CIFAR-10 and CIFAR-100 following the guidelines in (Goodfellow et al., 2015; Shaham et al., 2015; Kurakin et al., 2016).", "startOffset": 117, "endOffset": 185}, {"referenceID": 18, "context": "In all cases, We also adversarially trained each of the models on CIFAR-10 and CIFAR-100 following the guidelines in (Goodfellow et al., 2015; Shaham et al., 2015; Kurakin et al., 2016).", "startOffset": 117, "endOffset": 185}, {"referenceID": 18, "context": "In particular, we replace 50% of the examples of every minibatch by their adversarially perturbed version generated using the one-step method to avoid label leaking (Kurakin et al., 2016).", "startOffset": 165, "endOffset": 187}, {"referenceID": 4, "context": "This observation has motivated recent work on compressing deep neural networks (Denton et al., 2014).", "startOffset": 79, "endOffset": 100}, {"referenceID": 18, "context": "Following common practice (Kurakin et al., 2016), we use the fast gradient sign method to generate the adversarial examples (using \u2016.", "startOffset": 26, "endOffset": 48}, {"referenceID": 12, "context": "In comparison, the best performance achieved by a vanilla wide resnet (Zagoruyko & Komodakis, 2016) and a pre-activation resnet (He et al., 2016) are respectively 81.", "startOffset": 128, "endOffset": 145}], "year": 2017, "abstractText": "We introduce Parseval networks, a form of deep neural networks in which the Lipschitz constant of linear, convolutional and aggregation layers is constrained to be smaller than 1. Parseval networks are empirically and theoretically motivated by an analysis of the robustness of the predictions made by deep neural networks when their input is subject to an adversarial perturbation. The most important feature of Parseval networks is to maintain weight matrices of linear and convolutional layers to be (approximately) Parseval tight frames, which are extensions of orthogonal matrices to non-square matrices. We describe how these constraints can be maintained efficiently during SGD. We show that Parseval networks match the state-of-the-art in terms of accuracy on CIFAR-10/100 and Street View House Numbers (SVHN), while being more robust than their vanilla counterpart against adversarial examples. Incidentally, Parseval networks also tend to train faster and make a better usage of the full capacity of the networks.", "creator": "LaTeX with hyperref package"}}}