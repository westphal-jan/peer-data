{"id": "1704.08960", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2017", "title": "Neural Word Segmentation with Rich Pretraining", "abstract": "Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks.", "histories": [["v1", "Fri, 28 Apr 2017 14:46:25 GMT  (336kb,D)", "http://arxiv.org/abs/1704.08960v1", "Accepted by ACL 2017"]], "COMMENTS": "Accepted by ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jie yang", "yue zhang", "fei dong"], "accepted": true, "id": "1704.08960"}, "pdf": {"name": "1704.08960.pdf", "metadata": {"source": "CRF", "title": "Neural Word Segmentation with Rich Pretraining", "authors": ["Jie Yang", "Yue Zhang", "Fei Dong"], "emails": ["dong}@mymail.sutd.edu.sg", "zhang@sutd.edu.sg"], "sections": [{"heading": "1 Introduction", "text": "There is a current shift in research attention in the area of word segmentation from statistical methods to deep learning (Zheng et al., 2013; Pei et al., 2014; Morita et al., 2015; Chen et al., 2015b; Cai and Zhao, 2016; Zhang et al., 2016b). Neural network models have been exploited due to their strength in the non-sparse representation of learning and non-linear power in the feature combination, which has led to progress in many NLP tasks. Previously, neural word segmentors have provided comparable accuracy to the best statistical models."}, {"heading": "2 Related Work", "text": "Our work on statistical word segmentation dates back to the 1990s (Sproat et al., 1996). Our approaches include sequence labeling models (Xue et al., 2003) using CRFs (Peng et al., 1https: / / github.com / SUTDNLP / LibN3L2004; Zhao et al., 2006) and maximum margin models using word characteristics (Zhang and Clark, 2007; Sun et al., 2009; Sun, 2010). Semi-supervised methods were applied to both character-based and word-based models, examining external training data for better segmentation (Sun and Xu, 2011; Wang et al., 2012; Zhang et al., 2013)."}, {"heading": "3 Model", "text": "Our segmentor works incrementally from left to right, as shown in the example in Table 1. > At each step, the state consists of a sequence of words that have been fully recognized, referred to as W = [w \u2212 k, w \u2212 k + 1,..., w \u2212 1], a current partially recognized word P, and a sequence of next incoming characters referred to as C = [c0, c1,..., cm], as shown in Figure 1. Faced with an input set, W and P are initialized to [] and B, respectively, and C contains all input characters. At each step, a decision is made about c0, either by appending it as part of P, or by separating it as the beginning of a new word. The incremental process repeats itself until C is empty and P is again null (C = [], P = \u03c6). Formally, the process can be viewed as a state in which a tuple S = < W, P, C, > transition to a system (SEP) is shown."}, {"heading": "3.1 Representation Learning", "text": "We are examining two different approaches to encoding incoming characters, namely a window approach and an LSTM approach. For the former, we are following previous methods (Xue et al., 2003; Pei et al., 2014), using a five-character window [c \u2212 2, c \u2212 1, c0, c2] to represent incoming characters. Figure 3, a multi-layer perceptron (MLP) is used to represent a five-character window vector DC consisting of single character vector representations Vc \u2212 2, Vc \u2212 1, Vc0, Vc2.DC = MLP ([Vc \u2212 2; Vc0; Vc0; Vc2; Vc2]))) For the latter, we are following the recent work (Chen et al., 2015b; Zhang et al., 2016b), using a bi-directional LSTM model."}, {"heading": "3.2 Pretraining", "text": "In fact, most people who live and work in the United States also live in the United States and in other countries, so that they are able to survive on their own. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "4 Decoding and Training", "text": "To train the main segmentor, we adopt the global transition-based learning and beam search strategy of Zhang and Clark (2011). For decoding, the standard beam search is used, in which the B-best partial performance hypotheses are maintained in each step in an agenda. Initially, the agenda contains only the initial state. At each step, all hypotheses in the agenda are expanded by using all possible actions and B-best rated resulting hypotheses as an agenda for the next step. In training, the same decoding process is applied to each training example (xi, yi). At step j, when the gold standard sequence of transitional actions falls out of the agenda, the maximum margin update is performed by taking the current best hypothesis y-j in the beam as a negative example, and jij as a positive example."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experimental Settings", "text": "To test the robustness of our model, we additionally use SIGHAN 2005 Bake-off (Emerson, 2005) and NLPCC 2016 Shared Task for Weibo segmentation (Qiu et al., 2016) as test data sets, using the standard splits. To pre-embed words, characters and character bigrams, we use Chinese gigaword (simplified Chinese sections) 4, automatically segmented using ZPar 0.6 off-theshelf (Zhang and Clark, 2007), the statics of which are shown in Table 3.For pre-embedding words, characters and character bigrams, we use punctuation classification data from the gigaword corpus and use the word-based ZPar monogram (Zhang and Clark, 2007), the statistics of which are shown in Table 3.1."}, {"heading": "5.2 Development Experiments", "text": "We conduct development experiments to test the usefulness of different context representations, network configurations, and different preparation methods."}, {"heading": "5.2.1 Context Representations", "text": "The influence of character and word context representations are examined empirically by comparing the context of the network structures forXC and XW in Figure 1, respectively. All experiments in this section are performed with a beam size of 8. Character context. We fix the word representation XW to a two-word window and compare different context representations. The results are shown in Table 4, where \"no character\" represents our model without XC \u2212 5-character window \"also a five-character window context,\" char LSTM \"character LSTM context and4https: / / catalog.ldc.upenn.edu / LDC2011T13 5http: / / www.icl.pku.edu.cn / icl res\" 5-char window + LSTM \"stands for a combination, detailed in Section 3.1\" -char emb \"and\" - bichar emb. \""}, {"heading": "5.2.2 Stuctured Learning and Inference", "text": "We test the effectiveness of structured learning and conclusions by measuring the impact of beam size on the baseline segmentor. Figure 4 shows the F-scores based on different number of training processes with beam size 1,2,4,8 and 16, respectively. If beam size is 1, the conclusion is local and greedy. As the beam size increases, the more global structural ambiguities can be resolved, as the learning is designed to guide the search. A contrast between beam size 1 and 2 demonstrates the usefulness of structured learning and conclusion. As the beam size increases, the gain decreases by doubling the beam size. For the remaining experiments, we select a beam size of 8 to find a compromise between speed and accuracy."}, {"heading": "5.2.3 Pretraining Results", "text": "Table 6 shows the effectiveness of the rich pre-training from Dc to the development set. Specifically, through the use of punctuation information, the F score increases from 95.86% to 96.25%, with a relative error reduction of 9.4%. This is consistent with the observation of Sun and Xu (2011), which show that punctuation is more effective than semi-monitored data for a statistical word segmentation model when compared to mutual information and access diversity. Automatically segmented data6, heterogeneous segmentation, and POS information increase the F score to 96.26%, 96.27%, and 96.22%, respectively, demonstrating the relevance of all information sources for neural segmentation, which coincides with observations for statistical word segmentation (Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2013). Finally, the F score is further improved by integrating all of the above information into a multi-task 948% with a multi-learning reduction."}, {"heading": "5.2.4 Comparision with Zhang et al. (2016b)", "text": "Both our model and Zhang et al. (2016b) use global learning and beam search, but our network is different. Zhang et al. (2016b) use the action history with the LSTM encoder, while we use subtitle rather than action information. In addition, the character and character bigram embedding is finely tuned in our model, while Zhang et al. (2016b) specify the embedding that is determined during the training.6By using ZPar alone, we obtain an auto-segmented result of 96.02%, less than by using results from comparing with ZPar and the CRF segmentor output. We examine the F dimensional distribution with respect to the set length in our base model, the multitask pretraining model, and Zhang et al. (2016b) In particular, we cluster the sentences in the development data sets into 6 categories based on their length and rate their F1 values relative to their error formation, as shown in our models."}, {"heading": "5.3 Final Results", "text": "In fact, the number of people who are able to surpass themselves has increased sharply in recent years. (...) The number of people who are able to move in the world has doubled in the last ten years. (...) The number of people who are able to move in the world has doubled. (...) The number of people who are able to move in the world has increased. (...) The number of people who are able to move in the world has increased. (...) The number of people who are able to move in the world has increased. (...) The number of people who are able to move in the world has increased. (...) The number of people who are able to move in the world has increased. (...) The number of people who are able to move in the world has increased. (...) The number of people who are able to move in the world. (...) The number of people who are able to move in the world. (...) The number of people who are able to move. (...) The number of people who are able to move."}, {"heading": "6 Conclusion", "text": "We explored rich external resources to improve neural word segmentation by developing a globally optimized beam search model that uses both character and word contexts. Using each type of external resource as an additional classification task, we used neural multi-task learning to prepare a set of common parameters for character contexts. Results show that abundant pretraining leads to a relative error reduction of 15.4%, and our model delivers results that compete with the best systems on six different benchmarks."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their insightful comments and support of NSFC 61572245. We would like to thank Meishan Zhang for his insightful discussion and support in coding. Yue Zhang is the corresponding author."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "ACL. Association for Computational Linguistics, pages", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Practical recommendations for gradient-based training of deep architectures", "author": ["Yoshua Bengio."], "venue": "Neural networks: Tricks of the trade, Springer, pages 437\u2013478.", "citeRegEx": "Bengio.,? 2012", "shortCiteRegEx": "Bengio.", "year": 2012}, {"title": "Neural word segmentation learning for chinese", "author": ["Deng Cai", "Hai Zhao."], "venue": "ACL. Association for Computational Linguistics, pages 409\u2013420. https://doi.org/10.18653/v1/P16-1039.", "citeRegEx": "Cai and Zhao.,? 2016", "shortCiteRegEx": "Cai and Zhao.", "year": 2016}, {"title": "Gated recursive neural network for chinese word segmentation", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Xuanjing Huang."], "venue": "ACL. Association for Computational Linguistics. https://doi.org/10.3115/v1/P15-1168.", "citeRegEx": "Chen et al\\.,? 2015a", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Long shortterm memory neural networks for chinese word segmentation", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang."], "venue": "EMNLP. Association for Computational Linguistics, pages 1385\u20131394.", "citeRegEx": "Chen et al\\.,? 2015b", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Incremental parsing with the perceptron algorithm", "author": ["Michael Collins", "Brian Roark."], "venue": "ACL. Association for Computational Linguistics, page 111. http://aclweb.org/anthology/P04-1015.", "citeRegEx": "Collins and Roark.,? 2004", "shortCiteRegEx": "Collins and Roark.", "year": 2004}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12(Aug):2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research 12(Jul):2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "The second international chinese word segmentation bakeoff", "author": ["Thomas Emerson."], "venue": "Proceedings of the fourth SIGHAN workshop on Chinese language Processing. volume 133.", "citeRegEx": "Emerson.,? 2005", "shortCiteRegEx": "Emerson.", "year": 2005}, {"title": "Recurrent nets that time and count", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks, 2000. IJCNN 2000, Proceedings of the IEEE-INNSENNS International Joint Conference on. IEEE, volume 3, pages 189\u2013194.", "citeRegEx": "Gers and Schmidhuber.,? 2000", "shortCiteRegEx": "Gers and Schmidhuber.", "year": 2000}, {"title": "Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging: a case study", "author": ["Wenbin Jiang", "Liang Huang", "Qun Liu."], "venue": "ACL-IJCNLP. Association for Computational Linguistics, pages 522\u2013530.", "citeRegEx": "Jiang et al\\.,? 2009", "shortCiteRegEx": "Jiang et al\\.", "year": 2009}, {"title": "Punctuation as implicit annotations for chinese word segmentation", "author": ["Zhongguo Li", "Maosong Sun."], "venue": "Computational Linguistics 35(4):505\u2013512. http://aclweb.org/anthology/J09-4006.", "citeRegEx": "Li and Sun.,? 2009", "shortCiteRegEx": "Li and Sun.", "year": 2009}, {"title": "Unsupervised domain adaptation for joint segmentation and pos-tagging", "author": ["Yang Liu", "Yue Zhang."], "venue": "COLING. pages 745\u2013754. http://aclweb.org/anthology/C12-2073.", "citeRegEx": "Liu and Zhang.,? 2012", "shortCiteRegEx": "Liu and Zhang.", "year": 2012}, {"title": "Feature-based neural language model and chinese word segmentation", "author": ["Mairgup Mansur", "Wenzhe Pei", "Baobao Chang."], "venue": "IJCNLP. pages 1271\u2013 1277. http://aclweb.org/anthology/I13-1181.", "citeRegEx": "Mansur et al\\.,? 2013", "shortCiteRegEx": "Mansur et al\\.", "year": 2013}, {"title": "Morphological analysis for unsegmented languages using recurrent neural network language model", "author": ["Hajime Morita", "Daisuke Kawahara", "Sadao Kurohashi."], "venue": "EMNLP. Association for Computational Linguistics.", "citeRegEx": "Morita et al\\.,? 2015", "shortCiteRegEx": "Morita et al\\.", "year": 2015}, {"title": "Chinese part-ofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based? In EMNLP", "author": ["Hwee Tou Ng", "Jin Kiat Low."], "venue": "Association for Computational Linguistics, pages 277\u2013284. http://aclweb.org/anthology/W04-3236.", "citeRegEx": "Ng and Low.,? 2004", "shortCiteRegEx": "Ng and Low.", "year": 2004}, {"title": "Max-margin tensor neural network for chinese word segmentation", "author": ["Wenzhe Pei", "Tao Ge", "Baobao Chang."], "venue": "ACL. Association for Computational Linguistics, pages 293\u2013303. https://doi.org/10.3115/v1/P14-1028.", "citeRegEx": "Pei et al\\.,? 2014", "shortCiteRegEx": "Pei et al\\.", "year": 2014}, {"title": "Chinese segmentation and new word detection using conditional random fields", "author": ["Fuchun Peng", "Fangfang Feng", "Andrew McCallum."], "venue": "COLING. page 562. http://aclweb.org/anthology/C04-1081.", "citeRegEx": "Peng et al\\.,? 2004", "shortCiteRegEx": "Peng et al\\.", "year": 2004}, {"title": "Overview of the nlpcc-iccpol 2016 shared task: Chinese word segmentation for micro-blog texts", "author": ["Xipeng Qiu", "Peng Qian", "Zhan Shi."], "venue": "International Conference on Computer Processing of Oriental Languages. Springer, pages 901\u2013906.", "citeRegEx": "Qiu et al\\.,? 2016", "shortCiteRegEx": "Qiu et al\\.", "year": 2016}, {"title": "Profiting from mark-up: Hyper-text annotations for guided parsing", "author": ["Valentin I Spitkovsky", "Daniel Jurafsky", "Hiyan Alshawi."], "venue": "ACL. Association for Computational Linguistics, pages 1278\u2013 1287. http://aclweb.org/anthology/P10-1130.", "citeRegEx": "Spitkovsky et al\\.,? 2010", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2010}, {"title": "A stochastic finitestate word-segmentation algorithm for chinese", "author": ["Richard Sproat", "William Gale", "Chilin Shih", "Nancy Chang."], "venue": "Computational linguistics 22(3):377\u2013404. http://aclweb.org/anthology/J96-3004.", "citeRegEx": "Sproat et al\\.,? 1996", "shortCiteRegEx": "Sproat et al\\.", "year": 1996}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Word-based and characterbased word segmentation models: Comparison and combination", "author": ["Weiwei Sun."], "venue": "COLING. pages 1211\u20131219. http://aclweb.org/anthology/C10-2139.", "citeRegEx": "Sun.,? 2010", "shortCiteRegEx": "Sun.", "year": 2010}, {"title": "Enhancing chinese word segmentation using unlabeled data", "author": ["Weiwei Sun", "Jia Xu."], "venue": "EMNLP. Association for Computational Linguistics, pages 970\u2013 979. http://aclweb.org/anthology/D11-1090.", "citeRegEx": "Sun and Xu.,? 2011", "shortCiteRegEx": "Sun and Xu.", "year": 2011}, {"title": "Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection", "author": ["Xu Sun", "Houfeng Wang", "Wenjie Li."], "venue": "ACL. Association for Computational Linguistics, pages 253\u2013262.", "citeRegEx": "Sun et al\\.,? 2012", "shortCiteRegEx": "Sun et al\\.", "year": 2012}, {"title": "A discriminative latent variable chinese segmenter with hybrid word/character information", "author": ["Xu Sun", "Yaozhong Zhang", "Takuya Matsuzaki", "Yoshimasa Tsuruoka", "Jun\u2019ichi Tsujii"], "venue": "In NAACLHLT . Association for Computational Linguistics,", "citeRegEx": "Sun et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2009}, {"title": "A conditional random field word segmenter for sighan bakeoff 2005", "author": ["Huihsin Tseng", "Pichuan Chang", "Galen Andrew", "Daniel Jurafsky", "Christopher Manning."], "venue": "Proceedings of the fourth SIGHAN workshop on Chinese language Processing.", "citeRegEx": "Tseng et al\\.,? 2005", "shortCiteRegEx": "Tseng et al\\.", "year": 2005}, {"title": "Two knives cut better than one: Chinese word segmentation with dual decomposition", "author": ["Mengqiu Wang", "Rob Voigt", "Christopher D Manning."], "venue": "ACL. Association for Computational Linguistics, pages 193\u2013198. https://doi.org/10.3115/v1/P14-", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Improving chinese word segmentation and pos tagging with semi-supervised methods using large auto", "author": ["Yiou Wang", "Yoshimasa Tsuruoka", "Wenliang Chen", "Yujie Zhang", "Kentaro Torisawa"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Sequence-to-sequence learning as beam-search optimization", "author": ["Sam Wiseman", "Alexander M Rush."], "venue": "EMNLP. Association for Computational Linguistics, pages 1296\u20131306. http://aclweb.org/anthology/D16-1137.", "citeRegEx": "Wiseman and Rush.,? 2016", "shortCiteRegEx": "Wiseman and Rush.", "year": 2016}, {"title": "Word segmentation on micro-blog texts with external lexicon and heterogeneous data", "author": ["Qingrong Xia", "Zhenghua Li", "Jiayuan Chao", "Min Zhang."], "venue": "International Conference on Computer Processing of Oriental Languages. Springer.", "citeRegEx": "Xia et al\\.,? 2016", "shortCiteRegEx": "Xia et al\\.", "year": 2016}, {"title": "Dependencybased gated recursive neural network for chinese word segmentation", "author": ["Jingjing Xu", "Xu Sun."], "venue": "ACL. Association for Computational Linguistics, page 567. https://doi.org/10.18653/v1/P16-2092.", "citeRegEx": "Xu and Sun.,? 2016", "shortCiteRegEx": "Xu and Sun.", "year": 2016}, {"title": "The penn chinese treebank: Phrase structure annotation of a large corpus", "author": ["Naiwen Xue", "Fei Xia", "Fu-Dong Chiou", "Marta Palmer."], "venue": "Natural language engineering 11(02):207\u2013238.", "citeRegEx": "Xue et al\\.,? 2005", "shortCiteRegEx": "Xue et al\\.", "year": 2005}, {"title": "Chinese word segmentation as character tagging", "author": ["Nianwen Xue"], "venue": "Computational Linguistics and Chinese Language Processing 8(1):29\u201348.", "citeRegEx": "Xue,? 2003", "shortCiteRegEx": "Xue", "year": 2003}, {"title": "Exploring representations from unlabeled data with co-training for chinese word segmentation", "author": ["Longkai Zhang", "Houfeng Wang", "Xu Sun", "Mairgup Mansur."], "venue": "EMNLP. Association for Computational Linguistics, pages 311\u2013321.", "citeRegEx": "Zhang et al\\.,? 2013", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Libn3l: a lightweight package for neural nlp", "author": ["Meishan Zhang", "Jie Yang", "Zhiyang Teng", "Yue Zhang."], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation. https://doi.org/10.1145/322234.322243.", "citeRegEx": "Zhang et al\\.,? 2016a", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Character-level chinese dependency parsing", "author": ["Meishan Zhang", "Yue Zhang", "Wanxiang Che", "Ting Liu."], "venue": "ACL. Association for Computational Linguistics, pages 1326\u20131336. https://doi.org/10.3115/v1/P14-1125.", "citeRegEx": "Zhang et al\\.,? 2014", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Transition-based neural word segmentation", "author": ["Meishan Zhang", "Yue Zhang", "Guohong Fu."], "venue": "ACL. Association for Computational Linguistics. https://doi.org/10.18653/v1/P16-1040.", "citeRegEx": "Zhang et al\\.,? 2016b", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Subword-based tagging by conditional random fields for chinese word segmentation", "author": ["Ruiqiang Zhang", "Genichiro Kikui", "Eiichiro Sumita."], "venue": "NAACL. Association for Computational Linguistics, pages 193\u2013196.", "citeRegEx": "Zhang et al\\.,? 2006", "shortCiteRegEx": "Zhang et al\\.", "year": 2006}, {"title": "Chinese segmentation with a word-based perceptron algorithm", "author": ["Yue Zhang", "Stephen Clark."], "venue": "ACL. Association for Computational Linguistics, volume 45, page 840. http://aclweb.org/anthology/P07-1106.", "citeRegEx": "Zhang and Clark.,? 2007", "shortCiteRegEx": "Zhang and Clark.", "year": 2007}, {"title": "Joint word segmentation and pos tagging using a single perceptron", "author": ["Yue Zhang", "Stephen Clark."], "venue": "ACL. Association for Computational Linguistics, pages 888\u2013896. http://aclweb.org/anthology/P081101.", "citeRegEx": "Zhang and Clark.,? 2008", "shortCiteRegEx": "Zhang and Clark.", "year": 2008}, {"title": "Syntactic processing using the generalized perceptron and beam search", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Computational linguistics 37(1):105\u2013151. https://doi.org/10.1162/coli a 00037.", "citeRegEx": "Zhang and Clark.,? 2011", "shortCiteRegEx": "Zhang and Clark.", "year": 2011}, {"title": "Effective tag set selection in chinese word segmentation via conditional random field modeling", "author": ["Hai Zhao", "Chang-Ning Huang", "Mu Li", "Bao-Liang Lu."], "venue": "PACLIC. Citeseer, volume 20, pages 87\u201394. http://aclweb.org/anthology/Y06-1012.", "citeRegEx": "Zhao et al\\.,? 2006", "shortCiteRegEx": "Zhao et al\\.", "year": 2006}, {"title": "Deep learning for chinese word segmentation and pos tagging", "author": ["Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu."], "venue": "EMNLP. Association for Computational Linguistics, pages 647\u2013657. http://aclweb.org/anthology/D13-1061.", "citeRegEx": "Zheng et al\\.,? 2013", "shortCiteRegEx": "Zheng et al\\.", "year": 2013}, {"title": "A neural probabilistic structured-prediction model for transition-based dependency parsing", "author": ["Hao Zhou", "Yue Zhang", "Shujian Huang", "Jiajun Chen."], "venue": "ACL. Association for Computational Linguistics, pages 1213\u20131222.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 43, "context": "There has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning (Zheng et al., 2013; Pei et al., 2014; Morita et al., 2015; Chen et al., 2015b; Cai and Zhao, 2016; Zhang et al., 2016b).", "startOffset": 130, "endOffset": 250}, {"referenceID": 16, "context": "There has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning (Zheng et al., 2013; Pei et al., 2014; Morita et al., 2015; Chen et al., 2015b; Cai and Zhao, 2016; Zhang et al., 2016b).", "startOffset": 130, "endOffset": 250}, {"referenceID": 14, "context": "There has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning (Zheng et al., 2013; Pei et al., 2014; Morita et al., 2015; Chen et al., 2015b; Cai and Zhao, 2016; Zhang et al., 2016b).", "startOffset": 130, "endOffset": 250}, {"referenceID": 4, "context": "There has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning (Zheng et al., 2013; Pei et al., 2014; Morita et al., 2015; Chen et al., 2015b; Cai and Zhao, 2016; Zhang et al., 2016b).", "startOffset": 130, "endOffset": 250}, {"referenceID": 2, "context": "There has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning (Zheng et al., 2013; Pei et al., 2014; Morita et al., 2015; Chen et al., 2015b; Cai and Zhao, 2016; Zhang et al., 2016b).", "startOffset": 130, "endOffset": 250}, {"referenceID": 37, "context": "There has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning (Zheng et al., 2013; Pei et al., 2014; Morita et al., 2015; Chen et al., 2015b; Cai and Zhao, 2016; Zhang et al., 2016b).", "startOffset": 130, "endOffset": 250}, {"referenceID": 43, "context": "\u89d2(corner)\u201d (Zheng et al., 2013), which is infeasible by using sparse one-hot character features.", "startOffset": 11, "endOffset": 31}, {"referenceID": 13, "context": "In addition to character embeddings, distributed representations of character bigrams (Mansur et al., 2013; Pei et al., 2014) and words (Morita et al.", "startOffset": 86, "endOffset": 125}, {"referenceID": 16, "context": "In addition to character embeddings, distributed representations of character bigrams (Mansur et al., 2013; Pei et al., 2014) and words (Morita et al.", "startOffset": 86, "endOffset": 125}, {"referenceID": 14, "context": ", 2014) and words (Morita et al., 2015; Zhang et al., 2016b) have also been shown to improve segmentation accuracies.", "startOffset": 18, "endOffset": 60}, {"referenceID": 37, "context": ", 2014) and words (Morita et al., 2015; Zhang et al., 2016b) have also been shown to improve segmentation accuracies.", "startOffset": 18, "endOffset": 60}, {"referenceID": 43, "context": "With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a), as well as LSTMs on characters (Chen et al.", "startOffset": 204, "endOffset": 283}, {"referenceID": 13, "context": "With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a), as well as LSTMs on characters (Chen et al.", "startOffset": 204, "endOffset": 283}, {"referenceID": 16, "context": "With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a), as well as LSTMs on characters (Chen et al.", "startOffset": 204, "endOffset": 283}, {"referenceID": 3, "context": "With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a), as well as LSTMs on characters (Chen et al.", "startOffset": 204, "endOffset": 283}, {"referenceID": 4, "context": ", 2015a), as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al.", "startOffset": 41, "endOffset": 79}, {"referenceID": 31, "context": ", 2015a), as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al.", "startOffset": 41, "endOffset": 79}, {"referenceID": 14, "context": ", 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016; Zhang et al., 2016b).", "startOffset": 37, "endOffset": 99}, {"referenceID": 2, "context": ", 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016; Zhang et al., 2016b).", "startOffset": 37, "endOffset": 99}, {"referenceID": 37, "context": ", 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016; Zhang et al., 2016b).", "startOffset": 37, "endOffset": 99}, {"referenceID": 16, "context": "For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al.", "startOffset": 97, "endOffset": 135}, {"referenceID": 4, "context": "For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al.", "startOffset": 97, "endOffset": 135}, {"referenceID": 2, "context": ", 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al., 2016b).", "startOffset": 76, "endOffset": 117}, {"referenceID": 37, "context": ", 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al., 2016b).", "startOffset": 76, "endOffset": 117}, {"referenceID": 0, "context": "Previous research has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with findings on other NLP tasks, such as parsing (Andor et al., 2016).", "startOffset": 214, "endOffset": 234}, {"referenceID": 11, "context": "On the other hand, statistical segmentation research has exploited raw texts for semi-supervised learning, by collecting clues from raw texts more thoroughly such as mutual information and punctuation (Li and Sun, 2009; Sun and Xu, 2011), and making use of selfpredictions (Wang et al.", "startOffset": 201, "endOffset": 237}, {"referenceID": 23, "context": "On the other hand, statistical segmentation research has exploited raw texts for semi-supervised learning, by collecting clues from raw texts more thoroughly such as mutual information and punctuation (Li and Sun, 2009; Sun and Xu, 2011), and making use of selfpredictions (Wang et al.", "startOffset": 201, "endOffset": 237}, {"referenceID": 28, "context": "On the other hand, statistical segmentation research has exploited raw texts for semi-supervised learning, by collecting clues from raw texts more thoroughly such as mutual information and punctuation (Li and Sun, 2009; Sun and Xu, 2011), and making use of selfpredictions (Wang et al., 2011; Liu and Zhang, 2012).", "startOffset": 273, "endOffset": 313}, {"referenceID": 12, "context": "On the other hand, statistical segmentation research has exploited raw texts for semi-supervised learning, by collecting clues from raw texts more thoroughly such as mutual information and punctuation (Li and Sun, 2009; Sun and Xu, 2011), and making use of selfpredictions (Wang et al., 2011; Liu and Zhang, 2012).", "startOffset": 273, "endOffset": 313}, {"referenceID": 15, "context": "It has also utilised heterogenous annotations such as POS (Ng and Low, 2004; Zhang and Clark, 2008) and segmentation under different ar X iv :1 70 4.", "startOffset": 58, "endOffset": 99}, {"referenceID": 40, "context": "It has also utilised heterogenous annotations such as POS (Ng and Low, 2004; Zhang and Clark, 2008) and segmentation under different ar X iv :1 70 4.", "startOffset": 58, "endOffset": 99}, {"referenceID": 10, "context": "standards (Jiang et al., 2009).", "startOffset": 10, "endOffset": 30}, {"referenceID": 0, "context": "(2016b), we adopt a globally optimised beam-search framework for neural structured prediction (Andor et al., 2016; Zhou et al., 2015; Wiseman and Rush, 2016), which allows word information to be modelled explicitly.", "startOffset": 94, "endOffset": 157}, {"referenceID": 44, "context": "(2016b), we adopt a globally optimised beam-search framework for neural structured prediction (Andor et al., 2016; Zhou et al., 2015; Wiseman and Rush, 2016), which allows word information to be modelled explicitly.", "startOffset": 94, "endOffset": 157}, {"referenceID": 29, "context": "(2016b), we adopt a globally optimised beam-search framework for neural structured prediction (Andor et al., 2016; Zhou et al., 2015; Wiseman and Rush, 2016), which allows word information to be modelled explicitly.", "startOffset": 94, "endOffset": 157}, {"referenceID": 6, "context": "We adopt a multi-task learning strategy (Collobert et al., 2011), casting each external source of information as a auxiliary classification task, sharing a five-character window network.", "startOffset": 40, "endOffset": 64}, {"referenceID": 1, "context": "Following Cai and Zhao (2016) and Zhang et al.", "startOffset": 10, "endOffset": 30}, {"referenceID": 1, "context": "Following Cai and Zhao (2016) and Zhang et al. (2016b), we adopt a globally optimised beam-search framework for neural structured prediction (Andor et al.", "startOffset": 10, "endOffset": 55}, {"referenceID": 35, "context": "Our implementation is based on LibN3L1 (Zhang et al., 2016a).", "startOffset": 39, "endOffset": 60}, {"referenceID": 20, "context": "Work on statistical word segmentation dates back to the 1990s (Sproat et al., 1996).", "startOffset": 62, "endOffset": 83}, {"referenceID": 39, "context": ", 2006) and max-margin structured models leveraging word features (Zhang and Clark, 2007; Sun et al., 2009; Sun, 2010).", "startOffset": 66, "endOffset": 118}, {"referenceID": 25, "context": ", 2006) and max-margin structured models leveraging word features (Zhang and Clark, 2007; Sun et al., 2009; Sun, 2010).", "startOffset": 66, "endOffset": 118}, {"referenceID": 22, "context": ", 2006) and max-margin structured models leveraging word features (Zhang and Clark, 2007; Sun et al., 2009; Sun, 2010).", "startOffset": 66, "endOffset": 118}, {"referenceID": 23, "context": "Semisupervised methods have been applied to both character-based and word-based models, exploring external training data for better segmentation (Sun and Xu, 2011; Wang et al., 2011; Liu and Zhang, 2012; Zhang et al., 2013).", "startOffset": 145, "endOffset": 223}, {"referenceID": 28, "context": "Semisupervised methods have been applied to both character-based and word-based models, exploring external training data for better segmentation (Sun and Xu, 2011; Wang et al., 2011; Liu and Zhang, 2012; Zhang et al., 2013).", "startOffset": 145, "endOffset": 223}, {"referenceID": 12, "context": "Semisupervised methods have been applied to both character-based and word-based models, exploring external training data for better segmentation (Sun and Xu, 2011; Wang et al., 2011; Liu and Zhang, 2012; Zhang et al., 2013).", "startOffset": 145, "endOffset": 223}, {"referenceID": 34, "context": "Semisupervised methods have been applied to both character-based and word-based models, exploring external training data for better segmentation (Sun and Xu, 2011; Wang et al., 2011; Liu and Zhang, 2012; Zhang et al., 2013).", "startOffset": 145, "endOffset": 223}, {"referenceID": 22, "context": "Closest in spirit to our work, Sun and Xu (2011) empirically studied the use of various external resources for enhancing a statistical segmentor, including character mutual information, access variety information, punctuation and other statistical information.", "startOffset": 31, "endOffset": 49}, {"referenceID": 13, "context": "Our model inherits from previous findings on context representations, such as character windows (Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) and LSTMs (Chen et al.", "startOffset": 96, "endOffset": 155}, {"referenceID": 16, "context": "Our model inherits from previous findings on context representations, such as character windows (Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) and LSTMs (Chen et al.", "startOffset": 96, "endOffset": 155}, {"referenceID": 3, "context": "Our model inherits from previous findings on context representations, such as character windows (Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) and LSTMs (Chen et al.", "startOffset": 96, "endOffset": 155}, {"referenceID": 4, "context": ", 2015a) and LSTMs (Chen et al., 2015b; Xu and Sun, 2016).", "startOffset": 19, "endOffset": 57}, {"referenceID": 31, "context": ", 2015a) and LSTMs (Chen et al., 2015b; Xu and Sun, 2016).", "startOffset": 19, "endOffset": 57}, {"referenceID": 2, "context": ", 2014; Chen et al., 2015a) and LSTMs (Chen et al., 2015b; Xu and Sun, 2016). Similar to Zhang et al. (2016b) and Cai and Zhao (2016), we use word context on top of character context.", "startOffset": 8, "endOffset": 110}, {"referenceID": 2, "context": "(2016b) and Cai and Zhao (2016), we use word context on top of character context.", "startOffset": 12, "endOffset": 32}, {"referenceID": 2, "context": "(2016b) and Cai and Zhao (2016), allowing a central sub module, namely a fivecharacter context window, to be pretrained.", "startOffset": 12, "endOffset": 32}, {"referenceID": 33, "context": "Similar to Zhang et al. (2016b) and Cai and Zhao (2016), our model is a global structural model, using the overall score to disambiguate states, which correspond to sequences of inter-dependent transition actions.", "startOffset": 11, "endOffset": 32}, {"referenceID": 2, "context": "(2016b) and Cai and Zhao (2016), our model is a global structural model, using the overall score to disambiguate states, which correspond to sequences of inter-dependent transition actions.", "startOffset": 12, "endOffset": 32}, {"referenceID": 16, "context": "For the former, we follow prior methods (Xue et al., 2003; Pei et al., 2014), using five-character window [c\u22122, c\u22121, c0, c1, c2] to represent incoming characters.", "startOffset": 40, "endOffset": 76}, {"referenceID": 4, "context": "For the latter, we follow recent work (Chen et al., 2015b; Zhang et al., 2016b), using a bidirectional LSTM to encode input character sequence.", "startOffset": 38, "endOffset": 79}, {"referenceID": 37, "context": "For the latter, we follow recent work (Chen et al., 2015b; Zhang et al., 2016b), using a bidirectional LSTM to encode input character sequence.", "startOffset": 38, "endOffset": 79}, {"referenceID": 9, "context": "The LSTM variation with coupled input and forget gate but without peephole connections is applied (Gers and Schmidhuber, 2000)", "startOffset": 98, "endOffset": 126}, {"referenceID": 39, "context": "For the former, we follow prior methods (Zhang and Clark, 2007; Sun, 2010), using the two-word window [w\u22122, w\u22121] to represent recognized words.", "startOffset": 40, "endOffset": 74}, {"referenceID": 22, "context": "For the former, we follow prior methods (Zhang and Clark, 2007; Sun, 2010), using the two-word window [w\u22122, w\u22121] to represent recognized words.", "startOffset": 40, "endOffset": 74}, {"referenceID": 33, "context": "For the latter, we follow Zhang et al. (2016b) and Cai and Zhao (2016), using an uni-directional LSTM on words that have been recognized.", "startOffset": 26, "endOffset": 47}, {"referenceID": 2, "context": "(2016b) and Cai and Zhao (2016), using an uni-directional LSTM on words that have been recognized.", "startOffset": 12, "endOffset": 32}, {"referenceID": 23, "context": "Although raw texts do not contain explicit word boundary information, statistics such as mutual information between consecutive characters can be useful features for guiding segmentation (Sun and Xu, 2011).", "startOffset": 187, "endOffset": 205}, {"referenceID": 11, "context": "We therefore consider a more explicit clue for pretraining our character window network, namely punctuations (Li and Sun, 2009).", "startOffset": 109, "endOffset": 127}, {"referenceID": 19, "context": "Punctuation can serve as a type of explicit markup (Spitkovsky et al., 2010), indicating that the two characters on its left and right belong to two different words.", "startOffset": 51, "endOffset": 76}, {"referenceID": 12, "context": "Large texts automatically segmented by a baseline segmentor can be used for self-training (Liu and Zhang, 2012) or deriving statistical features (Wang et al.", "startOffset": 90, "endOffset": 111}, {"referenceID": 28, "context": "Large texts automatically segmented by a baseline segmentor can be used for self-training (Liu and Zhang, 2012) or deriving statistical features (Wang et al., 2011).", "startOffset": 145, "endOffset": 164}, {"referenceID": 10, "context": "There has been investigation on leveraging two corpora under different annotation standards to improve statistical segmentation (Jiang et al., 2009).", "startOffset": 128, "endOffset": 148}, {"referenceID": 15, "context": "Previous research has shown that POS information is closely related to segmentation (Ng and Low, 2004; Zhang and Clark, 2008).", "startOffset": 84, "endOffset": 125}, {"referenceID": 40, "context": "Previous research has shown that POS information is closely related to segmentation (Ng and Low, 2004; Zhang and Clark, 2008).", "startOffset": 84, "endOffset": 125}, {"referenceID": 6, "context": "Neural model have been shown capable of doing multi-task learning via parameter sharing (Collobert et al., 2011).", "startOffset": 88, "endOffset": 112}, {"referenceID": 39, "context": "To train the main segmentor, we adopt the global transition-based learning and beam-search strategy of Zhang and Clark (2011). For decoding, standard beam search is used, where the B best partial output hypotheses at each step are maintained in an agenda.", "startOffset": 103, "endOffset": 126}, {"referenceID": 5, "context": "The strategy above is early-update (Collins and Roark, 2004).", "startOffset": 35, "endOffset": 60}, {"referenceID": 7, "context": "We use Adagrad (Duchi et al., 2011) to optimize model parameters, with an initial learning rate \u03b1.", "startOffset": 15, "endOffset": 35}, {"referenceID": 21, "context": "L2 regularization and dropout (Srivastava et al., 2014) on input are used to reduce overfitting, with a L2 weight \u03bb and a dropout rate p.", "startOffset": 30, "endOffset": 55}, {"referenceID": 1, "context": "0 fanin+fanout (Bengio, 2012).", "startOffset": 15, "endOffset": 29}, {"referenceID": 1, "context": "0 fanin+fanout (Bengio, 2012). We fine-tune character and character bigram embeddings, but not word embeddings, acccording to Zhang et al. (2016b).", "startOffset": 16, "endOffset": 147}, {"referenceID": 32, "context": "0 (CTB6) (Xue et al., 2005) as our main dataset.", "startOffset": 9, "endOffset": 27}, {"referenceID": 36, "context": "Training, development and test set splits follow previous work (Zhang et al., 2014).", "startOffset": 63, "endOffset": 83}, {"referenceID": 8, "context": "In order to verify the robustness of our model, we additionally use SIGHAN 2005 bake-off (Emerson, 2005) and NLPCC 2016 shared task for Weibo segmentation (Qiu et al.", "startOffset": 89, "endOffset": 104}, {"referenceID": 18, "context": "In order to verify the robustness of our model, we additionally use SIGHAN 2005 bake-off (Emerson, 2005) and NLPCC 2016 shared task for Weibo segmentation (Qiu et al., 2016) as test datasets, where the standard splits are used.", "startOffset": 155, "endOffset": 173}, {"referenceID": 39, "context": "6 off-theshelf (Zhang and Clark, 2007), the statictics of which are shown in Table 3.", "startOffset": 15, "endOffset": 38}, {"referenceID": 26, "context": "For pretraining character representations, we extract punctuation classification data from the Gigaword corpus, and use the word-based ZPar and a standard character-based CRF model (Tseng et al., 2005) to obtain automatic segmentation results.", "startOffset": 181, "endOffset": 201}, {"referenceID": 8, "context": "The standard word precision, recall and F1 measure (Emerson, 2005) are used to evaluate segmentation performances.", "startOffset": 51, "endOffset": 66}, {"referenceID": 37, "context": "This shows that word contexts are far less important in our model compared to character contexts, and also compared to word contexts in previous word-based segmentors (Zhang et al., 2016b; Cai and Zhao, 2016).", "startOffset": 167, "endOffset": 208}, {"referenceID": 2, "context": "This shows that word contexts are far less important in our model compared to character contexts, and also compared to word contexts in previous word-based segmentors (Zhang et al., 2016b; Cai and Zhao, 2016).", "startOffset": 167, "endOffset": 208}, {"referenceID": 2, "context": ", 2016b; Cai and Zhao, 2016). This is likely due to the difference in our neural network structures, and that we fine-tune both character and character bigram embeddings, which significantly enlarges the adjustable parameter space as compared with Zhang et al. (2016b). The fact that word contexts can contribute relatively less than characters in a word is also not surprising in the sense that word-based neural segmentors do not outperform the best character-based models by large margins.", "startOffset": 9, "endOffset": 269}, {"referenceID": 39, "context": "This is consistent with previous findings of statistical word segmentation (Zhang and Clark, 2007), which adopt a 2-word context.", "startOffset": 75, "endOffset": 98}, {"referenceID": 10, "context": "22%, respectively, showing the relevance of all information sources to neural segmentation, which is consistent with observations made for statistical word segmentation (Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2013).", "startOffset": 169, "endOffset": 228}, {"referenceID": 28, "context": "22%, respectively, showing the relevance of all information sources to neural segmentation, which is consistent with observations made for statistical word segmentation (Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2013).", "startOffset": 169, "endOffset": 228}, {"referenceID": 34, "context": "22%, respectively, showing the relevance of all information sources to neural segmentation, which is consistent with observations made for statistical word segmentation (Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2013).", "startOffset": 169, "endOffset": 228}, {"referenceID": 21, "context": "the observation of Sun and Xu (2011), who show that punctuation is more effective compared with mutual information and access variety as semisupervised data for a statistical word segmentation model.", "startOffset": 19, "endOffset": 37}, {"referenceID": 34, "context": "4 Comparision with Zhang et al. (2016b)", "startOffset": 19, "endOffset": 40}, {"referenceID": 34, "context": "Both our model and Zhang et al. (2016b) use global learning and beam search, but our network is different.", "startOffset": 19, "endOffset": 40}, {"referenceID": 34, "context": "Both our model and Zhang et al. (2016b) use global learning and beam search, but our network is different. Zhang et al. (2016b) utilizes the action history with LSTM encoder, while we use partial word rather than action information.", "startOffset": 19, "endOffset": 128}, {"referenceID": 34, "context": "Both our model and Zhang et al. (2016b) use global learning and beam search, but our network is different. Zhang et al. (2016b) utilizes the action history with LSTM encoder, while we use partial word rather than action information. Besides, the character and character bigram embeddings are fine-tuned in our model while Zhang et al. (2016b) set the embeddings fixed during training.", "startOffset": 19, "endOffset": 343}, {"referenceID": 34, "context": "We study the F-measure distribution with respect to sentence length on our baseline model, multitask pretraining model and Zhang et al. (2016b). In particular, we cluster the sentences in the development dataset into 6 categories based on their length and evaluate their F1-values, respectively.", "startOffset": 123, "endOffset": 144}, {"referenceID": 34, "context": "We study the F-measure distribution with respect to sentence length on our baseline model, multitask pretraining model and Zhang et al. (2016b). In particular, we cluster the sentences in the development dataset into 6 categories based on their length and evaluate their F1-values, respectively. As shown in Figure 5, the models give different error distributions, with our models being more robust to the sentence length compared with Zhang et al. (2016b). Their model is better on very short sentences, but worse on all other cases.", "startOffset": 123, "endOffset": 457}, {"referenceID": 23, "context": "Our final results compare favourably to the best statistical models, including those using semisupervised learning (Sun and Xu, 2011; Wang et al., 2011), and those leveraging joint POS and syntactic information (Zhang et al.", "startOffset": 115, "endOffset": 152}, {"referenceID": 28, "context": "Our final results compare favourably to the best statistical models, including those using semisupervised learning (Sun and Xu, 2011; Wang et al., 2011), and those leveraging joint POS and syntactic information (Zhang et al.", "startOffset": 115, "endOffset": 152}, {"referenceID": 36, "context": ", 2011), and those leveraging joint POS and syntactic information (Zhang et al., 2014).", "startOffset": 66, "endOffset": 86}, {"referenceID": 30, "context": "44%, which is higher than the neural segmentor of Zhang et al. (2016b), which gives the best accuracies among pure neural segments on this dataset.", "startOffset": 50, "endOffset": 71}, {"referenceID": 22, "context": "In comparison, Sun and Xu (2011) investigated heterogenous semi-supervised learning on a stateof-the-art statistical model, obtaining a relative error reduction of 13.", "startOffset": 15, "endOffset": 33}, {"referenceID": 22, "context": "In comparison, Sun and Xu (2011) investigated heterogenous semi-supervised learning on a stateof-the-art statistical model, obtaining a relative error reduction of 13.8%. Our findings show that external data can be as useful for neural segmentation as for statistical segmentation. Our final results compare favourably to the best statistical models, including those using semisupervised learning (Sun and Xu, 2011; Wang et al., 2011), and those leveraging joint POS and syntactic information (Zhang et al., 2014). In addition, it also outperforms the best neural models, in particular Zhang et al. (2016b)*, which is a hybrid neural and statistical model, integrating man-", "startOffset": 15, "endOffset": 607}, {"referenceID": 3, "context": "0 Chen et al. (2015a) window 95.", "startOffset": 2, "endOffset": 22}, {"referenceID": 23, "context": "7%, respectivley, which are comparable with those statistical semi-supervised methods (Sun and Xu, 2011; Wang et al., 2011).", "startOffset": 86, "endOffset": 123}, {"referenceID": 28, "context": "7%, respectivley, which are comparable with those statistical semi-supervised methods (Sun and Xu, 2011; Wang et al., 2011).", "startOffset": 86, "endOffset": 123}, {"referenceID": 23, "context": "Compared with discrete semisupervised methods (Sun and Xu, 2011; Wang et al., 2011), our semi-supervised model is free from hand-crafted features.", "startOffset": 46, "endOffset": 83}, {"referenceID": 28, "context": "Compared with discrete semisupervised methods (Sun and Xu, 2011; Wang et al., 2011), our semi-supervised model is free from hand-crafted features.", "startOffset": 46, "endOffset": 83}, {"referenceID": 2, "context": ", 2015a,b) in our research, since lexicons might cover different OOV in the training and test data, and hence directly affecting the accuracies, which makes it relatively difficult to compare different methods fairly unless a single lexicon is used for all methods, as observed by Cai and Zhao (2016). We notice that both PKU dataset and our heterogenous data are based on the news of People\u2019s Daily.", "startOffset": 281, "endOffset": 301}, {"referenceID": 22, "context": "1 \u2013 Sun et al. (2009) 95.", "startOffset": 4, "endOffset": 22}, {"referenceID": 22, "context": "1 \u2013 Sun et al. (2009) 95.2 97.3 \u2013 94.6 \u2013 Sun (2010) 95.", "startOffset": 4, "endOffset": 52}, {"referenceID": 22, "context": "1 \u2013 Sun et al. (2009) 95.2 97.3 \u2013 94.6 \u2013 Sun (2010) 95.2 96.9 95.2 95.6 \u2013 Wang et al. (2014) 95.", "startOffset": 4, "endOffset": 93}, {"referenceID": 22, "context": "1 \u2013 Sun et al. (2009) 95.2 97.3 \u2013 94.6 \u2013 Sun (2010) 95.2 96.9 95.2 95.6 \u2013 Wang et al. (2014) 95.3 97.4 95.4 94.7 \u2013 Xia et al. (2016) \u2013 \u2013 \u2013 \u2013 95.", "startOffset": 4, "endOffset": 133}, {"referenceID": 30, "context": "Xia et al. (2016) achieved the best results on this dataset by using a statistical model with features learned using external lexicons, the CTB7 corpus and the People Daily corpus.", "startOffset": 0, "endOffset": 18}, {"referenceID": 30, "context": "Xia et al. (2016) achieved the best results on this dataset by using a statistical model with features learned using external lexicons, the CTB7 corpus and the People Daily corpus. Similar to Table 7, our method gives the best accuracies on all corpora except for MSR, where it underperforms the hybrid model of Zhang et al. (2016b) by 0.", "startOffset": 0, "endOffset": 333}], "year": 2017, "abstractText": "Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks.", "creator": "LaTeX with hyperref package"}}}