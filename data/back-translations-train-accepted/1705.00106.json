{"id": "1705.00106", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Apr-2017", "title": "Learning to Ask: Neural Question Generation for Reading Comprehension", "abstract": "We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e., grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).", "histories": [["v1", "Sat, 29 Apr 2017 01:08:48 GMT  (227kb,D)", "http://arxiv.org/abs/1705.00106v1", "Accepted to ACL 2017, 11 pages"]], "COMMENTS": "Accepted to ACL 2017, 11 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["xinya du", "junru shao", "claire cardie"], "accepted": true, "id": "1705.00106"}, "pdf": {"name": "1705.00106.pdf", "metadata": {"source": "CRF", "title": "Learning to Ask: Neural Question Generation for Reading Comprehension", "authors": ["Xinya Du", "Junru Shao", "Claire Cardie"], "emails": ["cardie}@cs.cornell.edu", "yz_sjr@sjtu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "A key application of the question is the field of education, in which questions are asked about the way in which questions are asked, in which questions are asked about the way in which they are asked (e.g. the question about the way in which questions are asked, in the way in which they are asked), the question about the way in which the questions that are asked are as important as the question about the way in which the questions are asked, the question about the way in which the questions that are asked, how they are asked, how they are asked, how they are asked, the question about the way in which the questions are asked is as important as the question about the way in which the questions are asked."}, {"heading": "2 Related Work", "text": "Recently, many new datasets have been published, and in most of these datasets, the questions are generated synthetically. For example, bAbI (Weston et al., 2016) is a fully synthetic dataset with 20 different tasks. Hermann et al. (2015) published a corpus of cloze-style questions, replacing units with placeholders in abstract summaries of CNN / Daily Mail news articles. Chen et al. (2016) claim that CNN / Daily Mail datasets are simpler than previously thought, and their system almost reaches the limit. (2013) curates MCTest, which pairs crowd-worker questions with four possible answers. Although MCTest contains challenging natural questions, it is too small to answer data-demanding questions."}, {"heading": "3 Task Definition", "text": "In this section we define the task of the question. For an input set x, our goal is to generate a natural question y in relation to information in the sentence, y can be a sequence of arbitrary length: [y1,..., y | y |]. Suppose the length of the input set is M, then x could be represented as a sequence of tokens [x1,..., xM]. The QG task is defined as finding y, so: y = argmax yP (y | x) (1), where P (y | x) is the conditional log probability of the predicted sequence y in relation to the input x. In Section 4.1, we will discuss the global attention mechanism for modeling P (y | x)."}, {"heading": "4 Model", "text": "Our model is partly inspired by the way a human would solve the task. To ask a natural question, people usually pay attention to specific parts of the input set and link context information from the paragraph. We model conditional probability using the RNN encoder decoder architecture (Bahdanau et al., 2015; Cho et al., 2014) and use the global attention mechanism (Luong et al., 2015a) to focus the model on specific elements of input when generating each word during decoding."}, {"heading": "4.1 Decoder", "text": "Similar to Sutskever et al. (2014) and Chopra et al. (2016), we factor the condition in Equation 1 into a product of word-level predictions: P (y | x) = | y | x = 1 P (yt | x, y < t), predicting the probability of each yt on the basis of all words previously generated (i.e. y < t), and the input sentence x. More specifically, P (yt | x, y < t) = softmax (Wstanh (Wt [ht; ct])) (2), where the recursive neural network is the variable in time step t and ct is the attention-based coding of x in time step t (Section 4.2). Ws and Wt are parameters to learn, the model of LSTM1 (yt \u2212 1, ht \u2212 1) (3)."}, {"heading": "4.2 Encoder", "text": "The attention-based sentence encoder is used in both of our models, while the paragraph encoder is used only in the model that contains paragraph-level information. \u2212 Attention-based sentence encoder: We use a bidirectional LSTM to encode the sentence, \u2212 \u2192 bt = \u2212 \u2212 \u2212 \u2212 \u2212 LSTM2 (xt, \u2212 \u2212 \u2212 \u2212 LSTM2 (xt, \u2212 bt + 1), where \u2212 bt is the hidden state in due course is step t for forward LSTM, \u2190 \u2212 bt for reverse pass. To get the attention-based encoding of x at the decryption time step, namely ct, we first get the context-dependent token representation by bt = [\u2212 bt; bt;), then we take the weighted average over the process LSTM, \u00b7 t = 1,..., | x |), ct = x, tbi (4).Attention is calculated by paragraph encoding (we)."}, {"heading": "4.3 Training and Inference", "text": "When we give a training corpus of sentence-question pairs: S = {(x (i), y (i))} S i = 1, the training objective of our models is to minimize the negative log probability of training data with respect to all parameters as indicated by \u03b8, L = \u2212 S \u2211 i = 1logP (y (i) | x (i); \u03b8) = \u2212 S \u2211 i = 1 | y (i) | \u2211 j = 1 logP (y (i) j | x (i), y (i) < j; \u03b8) Once the model is trained, we make conclusions by means of beam search. The beam search is indicated by the possible path number k. Since there may be many rare words in the input set that are not in the target page dictionary, while many UNK characters are output during decoding."}, {"heading": "5 Experimental Setup", "text": "We experiment with our model of generating neural questions based on the processed SQuAD dataset. In this section, we first describe the corpus of the task. Then, we provide implementation details for our neural generation model, the baselines to be compared and their experimental settings. Finally, we introduce the evaluation methods using automatic metrics and human raters."}, {"heading": "5.1 Dataset", "text": "We use the SQuAD dataset (Rajpurkar et al., 2016) to extract sentences and couple them with the sentence pairs. We train our models with the sentence pairs. The dataset contains 536 articles with over 100k questions asked about the articles. The authors employ crowd-workers from Amazon Mechanical Turks to create questions based on the Wikipedia articles. Since there is a hidden part of the original SQuAD that we do not have access to, we will henceforth treat the accessible parts (x 90%) as the entire dataset. We will initially list Stanford CoreNLP (Manning et al., 2014) for pre-processing. Since there is a hidden part of the original SQuAD that we do not have access to, we will treat the accessible parts as the entire dataset. We will list Stanford CoreNLP et al. (Manning et al., 2014) for pre-processing."}, {"heading": "5.2 Implementation Details", "text": "We implement our Model 2 in Torch7 3 on the newly released OpenNMT system (Klein et al., 2017).For source-side Vocabulary V, we retain only the 45k most common tokens (including < SOS >, < EOS > and placeholders).For target-side Vocabulary U, we similarly retain the 28k most common tokens. All other tokens outside the vocabulary list are replaced by the UNK symbol.We select word embedding of 300 dimensions and use pre-embedded Glove.840B.300d (Pennington et al., 2014) for initialization. We set the word representation during training by the hidden LSTM symbol to 600 and set the number of layers of LSTMs to 2 in both the encoder and decoder."}, {"heading": "5.3 Baselines", "text": "To prove the effectiveness of our system, we compare it with several competing systems. Next, we briefly present their approaches and experimental settings to perform them for our problem. Their results are shown in Table 2.IR, stands for our information retrieval baselines. Similar to Rush et al. (2015), we implement the IR baselines to control memorization of questions from the training set. We use two metrics to calculate the distance between a question and the input set, i.e., BM-25 (Robertson and Walker, 1994) and to edit the distance (Levenshtein, 1966). According to the metric, the system retrieves the training sets to find the question with the highest score.MOSES + (Koehn et al., 2007) is a widely used phrase-based statistical machine translation system. Here, we treat sentences as source-language text, treat questions as source-text, and we conduct source-text translation questions and translate sentences."}, {"heading": "5.4 Automatic Evaluation", "text": "We use the review package by Chen et al. (2015), which was originally used to evaluate captions. It includes the review scripts BLEU 1, BLEU 2, BLEU 3, BLEU 4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and ROUGEL (Lin, 2004). BLEU measures the average n-gram precision against a number of reference sets, with a penalty for sentences that are too short. BLEU-n is a BLEU score that uses up to n-grams for the counting of coexistence. METEOR is a memory-oriented metric that calculates the similarity between generations and references by taking into account synonyms, stems and paraphrases. ROUGE is commonly used to evaluate n-gram memories of summaries with gold standard sentences as references."}, {"heading": "5.5 Human Evaluation", "text": "In order to measure the quality of the questions generated by our system and the H & S system, we also conduct human evaluation studies: we look at two modalities: naturalness, which indicates grammatical and linguistic ability; and difficulty, which measures the syntactical divergence of sentence and question and the reasoning required to answer the question. We randomly select 100 sentence-question pairs: We ask four professional English speakers to rate the pairs on a scale of 1 to 5 (5 for the best) in relation to the above modalities."}, {"heading": "6 Results and Analysis", "text": "In fact, it is not as if we are able to solve the problem, but rather that it is a problem for which there is no solution. \"And further:\" It is not as if there is a solution. \"And further:\" It is not as if there is a solution. \"And further:\" It is not as if there is a solution. \"And further:\" It is not as if there is a solution. \"And further:\" It is not as if there is a solution. \"It is not as if there is a solution, but it is as if there is a solution.\""}, {"heading": "7 Conclusion and Future Work", "text": "We have presented a fully data-driven neural networking approach to automatically generating questions for reading comprehension. We use an attention-based neural networking approach to the task and investigate the effect of encoding information at the sentence or paragraph level. Our best model achieves excellence in both automatic ratings and human ratings. At this point, we point to several interesting future research directions. Currently, our paragraph model does not perform best in all question categories. We would like to explore how we can better use paragraph-level information to improve the performance of the QG system in all categories. Furthermore, it would be interesting to include mechanisms for other language generation tasks (e.g. copying mechanisms for dialogue generation) in our model in order to further improve the quality of the questions generated."}, {"heading": "Acknowledgments", "text": "We would like to thank the anonymous ACL reviewers Kai Sun and Yao Cheng for their helpful suggestions, Victoria Litvinova for their careful proofreading and Xanda Schofield, Wil Thoma-5The IDs of the examined questions are provided at https: / / github.com / xinyadu / nqg / blob / master / examined-question-ids.txt.son, Hubert Lin and Junxian He for the human reviews."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations Workshop (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A thorough examination of the cnn/daily mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Microsoft coco captions: Data collection and evaluation server", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C Lawrence Zitnick."], "venue": "arXiv preprint arXiv:1504.00325 .", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "author": ["Sumit Chopra", "Michael Auli", "Alexander M. Rush."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for", "citeRegEx": "Chopra et al\\.,? 2016", "shortCiteRegEx": "Chopra et al\\.", "year": 2016}, {"title": "Artificial paranoia", "author": ["Kenneth Mark Colby", "Sylvia Weber", "Franklin Dennis Hilf."], "venue": "Artificial Intelligence 2(1):1\u201325. https://doi.org/10.1016/00043702(71)90002-6.", "citeRegEx": "Colby et al\\.,? 1971", "shortCiteRegEx": "Colby et al\\.", "year": 1971}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Michael Denkowski", "Alon Lavie."], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation. Association for Computational Lin-", "citeRegEx": "Denkowski and Lavie.,? 2014", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "Scalable modified kneser-ney language model estimation", "author": ["Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn."], "venue": "Proceedings of the 51st Annual Meeting of the", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Good question! statistical ranking for question generation", "author": ["Michael Heilman", "Noah A. Smith."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Heilman and Smith.,? 2010", "shortCiteRegEx": "Heilman and Smith.", "year": 2010}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems (NIPS). pages", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Summarizing source code using a neural attention model", "author": ["Srinivasan Iyer", "Ioannis Konstas", "Alvin Cheung", "Luke Zettlemoyer."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Iyer et al\\.,? 2016", "shortCiteRegEx": "Iyer et al\\.", "year": 2016}, {"title": "Opennmt: Open-source toolkit for neural machine translation", "author": ["Guillaume Klein", "Yoon Kim", "Yuntian Deng", "Jean Senellart", "Alexander M. Rush."], "venue": "ArXiv e-prints .", "citeRegEx": "Klein et al\\.,? 2017", "shortCiteRegEx": "Klein et al\\.", "year": 2017}, {"title": "Deep questions without deep understanding", "author": ["Igor Labutov", "Sumit Basu", "Lucy Vanderwende."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Con-", "citeRegEx": "Labutov et al\\.,? 2015", "shortCiteRegEx": "Labutov et al\\.", "year": 2015}, {"title": "Binary codes capable of correcting deletions, insertions and reversals", "author": ["Vladimir I Levenshtein."], "venue": "Soviet physics doklady. volume 10, page 707.", "citeRegEx": "Levenshtein.,? 1966", "shortCiteRegEx": "Levenshtein.", "year": 1966}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop. Association for Com-", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Generating natural language questions to support learning on-line", "author": ["David Lindberg", "Fred Popowich", "John Nesbit", "Phil Winne."], "venue": "Proceedings of the 14th European Workshop on Natural Language Generation. Association for Computa-", "citeRegEx": "Lindberg et al\\.,? 2013", "shortCiteRegEx": "Lindberg et al\\.", "year": 2013}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Compu-", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computa-", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Christopher Manning", "Mihai Surdeanu", "John Bauer", "Jenny F.", "Steven B.", "David M."], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: Sys-", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Linguistic considerations in automatic question generation", "author": ["Karen Mazidi", "Rodney D. Nielsen."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational", "citeRegEx": "Mazidi and Nielsen.,? 2014", "shortCiteRegEx": "Mazidi and Nielsen.", "year": 2014}, {"title": "Computeraided generation of multiple-choice tests", "author": ["Ruslan Mitkov", "Le An Ha."], "venue": "Jill Burstein and Claudia Leacock, editors, Proceedings of the HLT-NAACL 03 Workshop on Building Educational Applications Using", "citeRegEx": "Mitkov and Ha.,? 2003", "shortCiteRegEx": "Mitkov and Ha.", "year": 2003}, {"title": "Generating natural questions about", "author": ["Nasrin Mostafazadeh", "Ishan Misra", "Jacob Devlin", "Margaret Mitchell", "Xiaodong He", "Lucy Vanderwende"], "venue": null, "citeRegEx": "Mostafazadeh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mostafazadeh et al\\.", "year": 2016}, {"title": "Generating instruction automatically for the reading strategy of selfquestioning", "author": ["Jack Mostow", "Wei Chen."], "venue": "Proceedings of the 2nd Workshop on Question Generation (AIED 2009). pages 465\u2013472.", "citeRegEx": "Mostow and Chen.,? 2009", "shortCiteRegEx": "Mostow and Chen.", "year": 2009}, {"title": "Ms marco: A human generated machine reading comprehension dataset", "author": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng."], "venue": "arXiv preprint arXiv:1611.09268 .", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguistics. Associ-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa-", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "MCTest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher J.C. Burges", "Erin Renshaw."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language", "citeRegEx": "Richardson et al\\.,? 2013", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval", "author": ["Stephen E. Robertson", "Steve Walker."], "venue": "Proceedings of the 17th Annual International ACM SIGIR Conference on Re-", "citeRegEx": "Robertson and Walker.,? 1994", "shortCiteRegEx": "Robertson and Walker.", "year": 1994}, {"title": "The first question generation shared task", "author": ["Vasile Rus", "Brendan Wyse", "Paul Piwek", "Mihai Lintean", "Svetlana Stoyanchev", "Cristian Moldovan"], "venue": null, "citeRegEx": "Rus et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rus et al\\.", "year": 2010}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computa-", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus", "author": ["Iulian Vlad Serban", "Alberto Garc\u00eda-Dur\u00e1n", "Caglar Gulcehre", "Sungjin Ahn", "Sarath Chandar", "Aaron Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems (NIPS). pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "The importance of being important: Question generation", "author": ["Lucy Vanderwende."], "venue": "Proceedings of the 1st Workshop on the Question Generation Shared Task Evaluation Challenge, Arlington, VA.", "citeRegEx": "Vanderwende.,? 2008", "shortCiteRegEx": "Vanderwende.", "year": 2008}, {"title": "Eliza&mdash;a computer program for the study of natural language communication between man and machine", "author": ["Joseph Weizenbaum."], "venue": "Commun. ACM 9(1):36\u201345. https://doi.org/10.1145/365153.365168.", "citeRegEx": "Weizenbaum.,? 1966", "shortCiteRegEx": "Weizenbaum.", "year": 1966}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merri\u00ebnboer", "Armand Joulin", "Tomas Mikolov."], "venue": "International Conference on Learning Represen-", "citeRegEx": "Weston et al\\.,? 2016", "shortCiteRegEx": "Weston et al\\.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio."], "venue": "ICML. volume 14, pages 77\u201381.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "One key application of question generation is in the area of education \u2014 to generate questions for reading comprehension materials (Heilman and Smith, 2010).", "startOffset": 131, "endOffset": 156}, {"referenceID": 22, "context": ", asking questions to start a conversation or to request feedback (Mostafazadeh et al., 2016)) or, arguably, as a clinical tool for evaluating or improving mental health (Weizenbaum, 1966; Colby et al.", "startOffset": 66, "endOffset": 93}, {"referenceID": 35, "context": ", 2016)) or, arguably, as a clinical tool for evaluating or improving mental health (Weizenbaum, 1966; Colby et al., 1971).", "startOffset": 84, "endOffset": 122}, {"referenceID": 5, "context": ", 2016)) or, arguably, as a clinical tool for evaluating or improving mental health (Weizenbaum, 1966; Colby et al., 1971).", "startOffset": 84, "endOffset": 122}, {"referenceID": 27, "context": ", SQuAD (Rajpurkar et al., 2016) and MS MARCO (Nguyen et al.", "startOffset": 8, "endOffset": 32}, {"referenceID": 24, "context": ", 2016) and MS MARCO (Nguyen et al., 2016), has spurred research in these areas.", "startOffset": 21, "endOffset": 42}, {"referenceID": 20, "context": ", Mitkov and Ha (2003); Rus et al.", "startOffset": 2, "endOffset": 23}, {"referenceID": 20, "context": ", Mitkov and Ha (2003); Rus et al. (2010). The success of these approaches hinges critically on the existence of well-designed rules for declarative-to-interrogative sentence transformation, typically based on deep linguistic knowledge.", "startOffset": 2, "endOffset": 42}, {"referenceID": 8, "context": "To improve over a purely rule-based system, Heilman and Smith (2010) introduced an overgenerate-and-rank approach that generates multiple questions from an input sentence using a rule-based approach and then ranks them using a supervised learning-based ranker.", "startOffset": 44, "endOffset": 69}, {"referenceID": 33, "context": "More specifically, inspired by the recent success in neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), summarization (Rush et al.", "startOffset": 80, "endOffset": 127}, {"referenceID": 0, "context": "More specifically, inspired by the recent success in neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), summarization (Rush et al.", "startOffset": 80, "endOffset": 127}, {"referenceID": 31, "context": ", 2015), summarization (Rush et al., 2015; Iyer et al., 2016), and image caption generation (Xu et al.", "startOffset": 23, "endOffset": 61}, {"referenceID": 11, "context": ", 2015), summarization (Rush et al., 2015; Iyer et al., 2016), and image caption generation (Xu et al.", "startOffset": 23, "endOffset": 61}, {"referenceID": 37, "context": ", 2016), and image caption generation (Xu et al., 2015), we tackle question generation using a conditional neural language model with a global attention mechanism (Luong et al.", "startOffset": 38, "endOffset": 55}, {"referenceID": 17, "context": ", 2015), we tackle question generation using a conditional neural language model with a global attention mechanism (Luong et al., 2015a).", "startOffset": 115, "endOffset": 136}, {"referenceID": 27, "context": "In evaluations on the SQuAD dataset (Rajpurkar et al., 2016) using three automatic evaluation metrics, we find that our system significantly outperforms a collection of strong baselines, including an information retrieval-based system (Robertson and Walker, 1994), a statistical machine translation approach (Koehn et al.", "startOffset": 36, "endOffset": 60}, {"referenceID": 29, "context": ", 2016) using three automatic evaluation metrics, we find that our system significantly outperforms a collection of strong baselines, including an information retrieval-based system (Robertson and Walker, 1994), a statistical machine translation approach (Koehn et al.", "startOffset": 182, "endOffset": 210}, {"referenceID": 8, "context": ", 2007), and the overgenerate-and-rank approach of Heilman and Smith (2010). Human evaluations also rated our generated questions as more grammatical, fluent, and challenging (in terms of syntactic divergence from the original reading passage and reasoning needed to answer) than the state-of-theart Heilman and Smith (2010) system.", "startOffset": 51, "endOffset": 76}, {"referenceID": 8, "context": ", 2007), and the overgenerate-and-rank approach of Heilman and Smith (2010). Human evaluations also rated our generated questions as more grammatical, fluent, and challenging (in terms of syntactic divergence from the original reading passage and reasoning needed to answer) than the state-of-theart Heilman and Smith (2010) system.", "startOffset": 51, "endOffset": 325}, {"referenceID": 36, "context": "For example, bAbI (Weston et al., 2016) is a fully synthetic dataset featuring 20 different tasks.", "startOffset": 18, "endOffset": 39}, {"referenceID": 1, "context": "Chen et al. (2016) claim that the CNN/Daily Mail dataset is easier than previ-", "startOffset": 0, "endOffset": 19}, {"referenceID": 15, "context": "ously thought, and their system almost reaches the ceiling performance. Richardson et al. (2013) curated MCTest, in which crowdworker questions are paired with four answer choices.", "startOffset": 54, "endOffset": 97}, {"referenceID": 27, "context": "Recently, Rajpurkar et al. (2016) released the Stanford Question Answering Dataset1 (SQuAD), which overcomes the aforementioned small size and (semi-)synthetic issues.", "startOffset": 10, "endOffset": 34}, {"referenceID": 27, "context": "Recently, Rajpurkar et al. (2016) released the Stanford Question Answering Dataset1 (SQuAD), which overcomes the aforementioned small size and (semi-)synthetic issues. The questions are posed by crowd workers and are of relatively high quality. We use SQuAD in our work, and similarly, we focus on the generation of natural questions for reading comprehension materials, albeit via automatic means. Question Generation has attracted the attention of the natural language generation (NLG) community in recent years, since the work of Rus et al. (2010).", "startOffset": 10, "endOffset": 551}, {"referenceID": 23, "context": "A lot of research has focused on first manually constructing question templates, and then applying them to generate questions (Mostow and Chen, 2009; Lindberg et al., 2013; Mazidi and Nielsen, 2014).", "startOffset": 126, "endOffset": 198}, {"referenceID": 16, "context": "A lot of research has focused on first manually constructing question templates, and then applying them to generate questions (Mostow and Chen, 2009; Lindberg et al., 2013; Mazidi and Nielsen, 2014).", "startOffset": 126, "endOffset": 198}, {"referenceID": 20, "context": "A lot of research has focused on first manually constructing question templates, and then applying them to generate questions (Mostow and Chen, 2009; Lindberg et al., 2013; Mazidi and Nielsen, 2014).", "startOffset": 126, "endOffset": 198}, {"referenceID": 13, "context": "Labutov et al. (2015) use crowdsourcing to collect a set of templates and then rank the relevant templates for the text of another domain.", "startOffset": 0, "endOffset": 22}, {"referenceID": 22, "context": "Mostafazadeh et al. (2016) introduce visual question generation task, to explore the deep connection between language and vision.", "startOffset": 0, "endOffset": 27}, {"referenceID": 22, "context": "Mostafazadeh et al. (2016) introduce visual question generation task, to explore the deep connection between language and vision. Serban et al. (2016) propose generating simple factoid ques-", "startOffset": 0, "endOffset": 151}, {"referenceID": 0, "context": "We model the conditional probability using RNN encoder-decoder architecture (Bahdanau et al., 2015; Cho et al., 2014), and adopt the global attention mechanism (Luong et al.", "startOffset": 76, "endOffset": 117}, {"referenceID": 3, "context": "We model the conditional probability using RNN encoder-decoder architecture (Bahdanau et al., 2015; Cho et al., 2014), and adopt the global attention mechanism (Luong et al.", "startOffset": 76, "endOffset": 117}, {"referenceID": 17, "context": ", 2014), and adopt the global attention mechanism (Luong et al., 2015a) to make the model focus on certain elements of the input when generating each word during decoding.", "startOffset": 50, "endOffset": 71}, {"referenceID": 32, "context": "Similar to Sutskever et al. (2014) and Chopra et al.", "startOffset": 11, "endOffset": 35}, {"referenceID": 4, "context": "(2014) and Chopra et al. (2016), we factorize the the conditional in equa-", "startOffset": 11, "endOffset": 32}, {"referenceID": 10, "context": "here, LSTM is the Long Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997).", "startOffset": 56, "endOffset": 90}, {"referenceID": 17, "context": "Unlike Luong et al. (2015b), we use a simpler replacing strategy for our task.", "startOffset": 7, "endOffset": 28}, {"referenceID": 27, "context": "With the SQuAD dataset (Rajpurkar et al., 2016), we extract sentences and pair them with the ques-", "startOffset": 23, "endOffset": 47}, {"referenceID": 19, "context": "We first run Stanford CoreNLP (Manning et al., 2014) for pre-processing: tokenization and sentence splitting.", "startOffset": 30, "endOffset": 52}, {"referenceID": 12, "context": "We implement our models 2 in Torch7 3 on top of the newly released OpenNMT system (Klein et al., 2017).", "startOffset": 82, "endOffset": 102}, {"referenceID": 26, "context": "300d pre-trained embeddings (Pennington et al., 2014) for initialization.", "startOffset": 28, "endOffset": 53}, {"referenceID": 29, "context": ", BM-25 (Robertson and Walker, 1994) and edit distance (Levenshtein, 1966).", "startOffset": 8, "endOffset": 36}, {"referenceID": 14, "context": ", BM-25 (Robertson and Walker, 1994) and edit distance (Levenshtein, 1966).", "startOffset": 55, "endOffset": 74}, {"referenceID": 14, "context": "IR stands for our information retrieval baselines. Similar to Rush et al. (2015), we implement the IR baselines to control memorizing questions from the training set.", "startOffset": 44, "endOffset": 81}, {"referenceID": 7, "context": "get side texts with KenLM (Heafield et al., 2013), and tune the system with MERT on dev set.", "startOffset": 26, "endOffset": 49}, {"referenceID": 33, "context": "Seq2seq (Sutskever et al., 2014) is a basic encoder-decoder sequence learning system for machine translation.", "startOffset": 8, "endOffset": 32}, {"referenceID": 25, "context": "The package includes BLEU 1, BLEU 2, BLEU 3, BLEU 4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and ROUGEL (Lin, 2004) evaluation scripts.", "startOffset": 52, "endOffset": 75}, {"referenceID": 6, "context": ", 2002), METEOR (Denkowski and Lavie, 2014) and ROUGEL (Lin, 2004) evaluation scripts.", "startOffset": 16, "endOffset": 43}, {"referenceID": 15, "context": ", 2002), METEOR (Denkowski and Lavie, 2014) and ROUGEL (Lin, 2004) evaluation scripts.", "startOffset": 55, "endOffset": 66}], "year": 2017, "abstractText": "We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentencevs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequenceto-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e., grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).", "creator": "LaTeX with hyperref package"}}}