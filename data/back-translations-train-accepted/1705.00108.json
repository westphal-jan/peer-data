{"id": "1705.00108", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Apr-2017", "title": "Semi-supervised sequence tagging with bidirectional language models", "abstract": "Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pre- trained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.", "histories": [["v1", "Sat, 29 Apr 2017 01:13:04 GMT  (131kb,D)", "http://arxiv.org/abs/1705.00108v1", "To appear in ACL 2017"]], "COMMENTS": "To appear in ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["matthew e peters", "waleed ammar", "chandra bhagavatula", "russell power"], "accepted": true, "id": "1705.00108"}, "pdf": {"name": "1705.00108.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised sequence tagging with bidirectional language models", "authors": ["Matthew E. Peters", "Waleed Ammar", "Chandra Bhagavatula", "Russell Power"], "emails": ["matthewp@allenai.org", "waleeda@allenai.org", "chandrab@allenai.org", "russellp@allenai.org"], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that we're going to be able to hide, and that we're going to be able to hide, \"he said.\" We've got to be able to hide, \"he said.\" We've got to be able to hide, \"he said.\" We've got to be able to hide, \"he said."}, {"heading": "2 Language model augmented sequence taggers (TagLM)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Overview", "text": "The main components of our language modeled sequence tagger (TagLM) are in Fig. 1. After word embedding before training and a neural LM on large, unlabeled corpora (Step 1), we extract the word and LM embedding for each token in a given input sequence (Step 2) and use it in the monitored sequence tagging model (Step 3)."}, {"heading": "2.2 Baseline sequence tagging model", "text": "Our starting sequence tagging model is a hierarchical neural tagging model, narrowly based on a series of recent studies (Ma and Hovy, 2016; Lample et al., 2016; Yang et al., 2017; Chiu and Nichols, 2016) (left side of Figure 2). In view of a set of tokens (t1, t2,., tN), it initially forms a representation, xk, for each token by concatenating a character-based representation ck with a token embedding wk: ck = C (tk; empirical representation) wk = E (tk; empirical representation) xk = [ck; wk] the character representation ck captures morphological information and is either a revolutionary neural network (CNN) (Ma and Hovy, 2016; Chiu and Nichols, 2016) or RNN (Yang et al., 2017; Lample et al., 2016)."}, {"heading": "2.3 Bidirectional LM", "text": "A language model calculates the probability of a token sequence (t1, t2,.., tN) p (t1, t2,.., tN) = N \u0433k = 1 p (tk | t1, t2,..., tk \u2212 1).Recent neural language models (Jo \u0301 zefowicz et al., 2016) use an architecture similar to our base sequence tagger, in which they pass a token representation (either from a CNN via characters or as token embedding) through several layers of LSTMs to embed the story (t1, t2,. \u2212 tk) in a fixed dimensional vector \u2212 \u2192 h LMk. This is the forward-directed LM embedding of the token at position k and is the output of the topmost LSTM layer in the language model. Finally, the language model predicts the probability of the lath form tk + 1 using a softmax layer over words in the vocabulary."}, {"heading": "2.4 Combining LM with sequence model", "text": "Our combined system, TagLM, uses the LM embedding as additional input into the sequence marking model. Specifically, we associate the LM embedding hLM with the output of one of the bidirectional RNN layers in the sequence model. In our experiments, we found that the introduction of LM embedding at the output of the first layer yielded the best performance. Formally, we simply replace (2) withhk, 1 = [\u2212 \u2192 h k, 1; \u2190 \u2212 h k, 1; h LM k]. (3) There are alternative ways to add the LM embedding to the sequence model. One option adds to the sequence model a nonlinear mapping after the concatenation and before the second RNN (e.g. Replace (3) by f ([\u2212 h k, 1; \u2190 \u2212 h k, 1; h LM k]), where f is a nonlinear function.) Another option adds a merit-like mechanism to the M so that we do not include it in the future mechanism of embedding LM."}, {"heading": "3 Experiments", "text": "We evaluate our approach to two well-benchmarked sequence tagging tasks, the CoNLL 2003 NGG task (Sang and Meulder, 2003) and the CoNLL 2000 chunking task (Sang and Buchholz, 2000). We report on the official evaluation of metric (micro-averaged F1) tasks. In both cases, we use the BIOES labeling scheme for output labels, the following previous work showing that it outperforms other options (e.g. Ratinov and Roth, 2009). After Chiu and Nichols (2016), we use the Sennawort embeddings (Collobert et al., 2011) and pre-edit the text by lowercasing all tokens and replacing all digits with 0.CoNLL 2003. The CoNLL 2003 NER task consists of Reuters RCV1 corpus tagged with four different entity types (PER, LOC, MISC)."}, {"heading": "3.1 Overall system results", "text": "Tables 1 and 2 compare the results of TagLM with previously published state-of-the-art results without additional labeled data or task-specific gazetteers. Tables 3 and 4 compare the results of TagLM with other systems containing additional labeled data or gazetteers. In both tasks, TagLM represents a new state of the art using bidirectional LMs (the forward CNN-BIG-LSTM and the backward LSTM-2048-512). In the CoNLL 2003 NER task, our model is 91.93 F1, which is a statistically significant increase over the previous best result of 91.62 \u00b1 0.33 of Chiu and Nichols (2016) working with gazetteers (at 95%, bilateral Welch t test, p = 0.021).In the CoNLL 2000 chunking task, TagLM achieves an average increase of 96.37 mean F1 and exceeds all previously published results without additional data by more than 1% absolute F1."}, {"heading": "3.2 Analysis", "text": "To decipher the characteristics of our LM-extended sequence taggers, we have a number of additional experiments at the CoNLL 2003 NER task.How to use LM embeddings? In this experiment, we link the LM embeddings at different locations in the base sequence tagger. In particular, we have used the LM embeddings hLMk to extend the input of the first RNN layer; i.e., we have to extend the meaning of the first RNN layer; i.e., hk, hk, 1 = [1]."}, {"heading": "4 Related work", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "5 Conclusion", "text": "Our method significantly exceeds the current state of the art in two popular datasets for NER and Chunking. Our analysis shows that adding a backward-facing LM in addition to traditional forward-facing LMs continuously improves performance, and the proposed method is robust even when the LM is trained on unlabeled data from another domain, or when the base model is trained on a large number of highlighted examples."}, {"heading": "Acknowledgments", "text": "We thank Chris Dyer, Julia Hockenmaier, Jayant Krishnamurthy, Matt Gardner and Oren Etzioni for comments on previous drafts that have resulted in significant improvements in the final version."}], "references": [{"title": "The AI2 system at SemEval-2017 Task 10 (ScienceIE): semisupervised end-to-end entity and relation extraction", "author": ["Waleed Ammar", "Matthew E. Peters", "Chandra Bhagavatula", "Russell Power."], "venue": "ACL workshop (SemEval).", "citeRegEx": "Ammar et al\\.,? 2017", "shortCiteRegEx": "Ammar et al\\.", "year": 2017}, {"title": "A highperformance semi-supervised learning method for text chunking", "author": ["Rie Kubota Ando", "Tong Zhang."], "venue": "ACL.", "citeRegEx": "Ando and Zhang.,? 2005", "shortCiteRegEx": "Ando and Zhang.", "year": 2005}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin."], "venue": "JMLR.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["Avrim Blum", "Tom Mitchell."], "venue": "COLT .", "citeRegEx": "Blum and Mitchell.,? 1998", "shortCiteRegEx": "Blum and Mitchell.", "year": 1998}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn."], "venue": "CoRR abs/1312.3005.", "citeRegEx": "Chelba et al\\.,? 2014", "shortCiteRegEx": "Chelba et al\\.", "year": 2014}, {"title": "Named entity recognition with bidirectional LSTM-CNNs", "author": ["Jason Chiu", "Eric Nichols."], "venue": "TACL.", "citeRegEx": "Chiu and Nichols.,? 2016", "shortCiteRegEx": "Chiu and Nichols.", "year": 2016}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "SSST@EMNLP.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins."], "venue": "EMNLP.", "citeRegEx": "Collins.,? 2002", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa."], "venue": "JMLR.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Semisupervised sequence learning", "author": ["Andrew M. Dai", "Quoc V. Le."], "venue": "NIPS.", "citeRegEx": "Dai and Le.,? 2015", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard M Schwartz", "John Makhoul."], "venue": "ACL.", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Bidirectional language model for handwriting recognition", "author": ["Volkmar Frinken", "Alicia Forn\u00e9s", "Josep Llad\u00f3s", "Jean-Marc Ogier."], "venue": "Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syn-", "citeRegEx": "Frinken et al\\.,? 2012", "shortCiteRegEx": "Frinken et al\\.", "year": 2012}, {"title": "A joint many-task model: Growing a neural network for multiple nlp tasks", "author": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher."], "venue": "CoRR abs/1611.01587.", "citeRegEx": "Hashimoto et al\\.,? 2016", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2016}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["Felix Hill", "Kyunghyun Cho", "Anna Korhonen."], "venue": "HLT-NAACL.", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Exploring the limits of language modeling", "author": ["Rafal J\u00f3zefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."], "venue": "CoRR abs/1602.02410.", "citeRegEx": "J\u00f3zefowicz et al\\.,? 2016", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Li Fei-Fei."], "venue": "ICLR workshop.", "citeRegEx": "Karpathy et al\\.,? 2015", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "ICLR.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Skip-thought vectors", "author": ["Jamie Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "NIPS.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Statistical machine translation", "author": ["Philipp Koehn."], "venue": "Cambridge University Press.", "citeRegEx": "Koehn.,? 2009", "shortCiteRegEx": "Koehn.", "year": 2009}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando Pereira."], "venue": "ICML.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Neural architectures for named entity recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."], "venue": "NAACL-HLT .", "citeRegEx": "Lample et al\\.,? 2016", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V. Le", "Tomas Mikolov."], "venue": "ICML.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Semi-supervised sequence modeling with syntactic topic models", "author": ["Wei Li", "Andrew McCallum."], "venue": "AAAI.", "citeRegEx": "Li and McCallum.,? 2005", "shortCiteRegEx": "Li and McCallum.", "year": 2005}, {"title": "Assessing the ability of LSTMs to learn syntax-sensitive dependencies", "author": ["Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg."], "venue": "TACL.", "citeRegEx": "Linzen et al\\.,? 2016", "shortCiteRegEx": "Linzen et al\\.", "year": 2016}, {"title": "Joint entity recognition and disambiguation", "author": ["Gang Luo", "Xiaojiang Huang", "Chin-Yew Lin", "Zaiqing Nie."], "venue": "EMNLP.", "citeRegEx": "Luo et al\\.,? 2015", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "End-to-end sequence labeling via bi-directional LSTM-CNNsCRF", "author": ["Xuezhe Ma", "Eduard H. Hovy."], "venue": "ACL.", "citeRegEx": "Ma and Hovy.,? 2016", "shortCiteRegEx": "Ma and Hovy.", "year": 2016}, {"title": "context2vec: Learning generic context embedding with bidirectional lstm", "author": ["Oren Melamud", "Jacob Goldberger", "Ido Dagan."], "venue": "CoNLL.", "citeRegEx": "Melamud et al\\.,? 2016", "shortCiteRegEx": "Melamud et al\\.", "year": 2016}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "Interspeech.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "NIPS.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Syntax-based semi-supervised named entity tagging", "author": ["Behrang Mohit", "Rebecca Hwa."], "venue": "ACL.", "citeRegEx": "Mohit and Hwa.,? 2005", "shortCiteRegEx": "Mohit and Hwa.", "year": 2005}, {"title": "Text classification from labeled and unlabeled documents using em", "author": ["Kamal Nigam", "Andrew Kachites McCallum", "Sebastian Thrun", "Tom Mitchell."], "venue": "Machine learning .", "citeRegEx": "Nigam et al\\.,? 2000", "shortCiteRegEx": "Nigam et al\\.", "year": 2000}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A bidirectional recurrent neural language model for machine translation", "author": ["\u00c1lvaro Peris", "Francisco Casacuberta."], "venue": "Procesamiento del Lenguaje Natural .", "citeRegEx": "Peris and Casacuberta.,? 2015", "shortCiteRegEx": "Peris and Casacuberta.", "year": 2015}, {"title": "Limitations of co-training for natural language learning from large datasets", "author": ["David Pierce", "Claire Cardie."], "venue": "EMNLP.", "citeRegEx": "Pierce and Cardie.,? 2001", "shortCiteRegEx": "Pierce and Cardie.", "year": 2001}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Lev-Arie Ratinov", "Dan Roth."], "venue": "CoNLL.", "citeRegEx": "Ratinov and Roth.,? 2009", "shortCiteRegEx": "Ratinov and Roth.", "year": 2009}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Hasim Sak", "Andrew W. Senior", "Franoise Beaufays."], "venue": "INTERSPEECH.", "citeRegEx": "Sak et al\\.,? 2014", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "Introduction to the CoNLL-2000 shared task chunking", "author": ["Erik F. Tjong Kim Sang", "Sabine Buchholz."], "venue": "CoNLL/LLL.", "citeRegEx": "Sang and Buchholz.,? 2000", "shortCiteRegEx": "Sang and Buchholz.", "year": 2000}, {"title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition", "author": ["Erik F. Tjong Kim Sang", "Fien De Meulder."], "venue": "CoNLL.", "citeRegEx": "Sang and Meulder.,? 2003", "shortCiteRegEx": "Sang and Meulder.", "year": 2003}, {"title": "Semi-supervised learning and domain adaptation in natural language processing", "author": ["Anders S\u00f8gaard."], "venue": "Synthesis Lectures on Human Language Technologies .", "citeRegEx": "S\u00f8gaard.,? 2013", "shortCiteRegEx": "S\u00f8gaard.", "year": 2013}, {"title": "Deep multi-task learning with low level tasks supervised at lower layers", "author": ["Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "ACL.", "citeRegEx": "S\u00f8gaard and Goldberg.,? 2016", "shortCiteRegEx": "S\u00f8gaard and Goldberg.", "year": 2016}, {"title": "Semi-supervised structured output learning based on a hybrid generative and discriminative approach", "author": ["Jun Suzuki", "Akinori Fujino", "Hideki Isozaki."], "venue": "EMNLP-CoNLL.", "citeRegEx": "Suzuki et al\\.,? 2007", "shortCiteRegEx": "Suzuki et al\\.", "year": 2007}, {"title": "Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data", "author": ["Jun Suzuki", "Hideki Isozaki."], "venue": "ACL.", "citeRegEx": "Suzuki and Isozaki.,? 2008", "shortCiteRegEx": "Suzuki and Isozaki.", "year": 2008}, {"title": "Transfer learning for sequence tagging with hierarchical recurrent networks", "author": ["Zhilin Yang", "Ruslan Salakhutdinov", "William W. Cohen."], "venue": "ICLR.", "citeRegEx": "Yang et al\\.,? 2017", "shortCiteRegEx": "Yang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 30, "context": "Many prior studies have shown that they capture useful semantic and syntactic information (Mikolov et al., 2013; Pennington et al., 2014) and including them in NLP systems has been shown to be enormously helpful for a variety of downstream tasks (Collobert et al.", "startOffset": 90, "endOffset": 137}, {"referenceID": 33, "context": "Many prior studies have shown that they capture useful semantic and syntactic information (Mikolov et al., 2013; Pennington et al., 2014) and including them in NLP systems has been shown to be enormously helpful for a variety of downstream tasks (Collobert et al.", "startOffset": 90, "endOffset": 137}, {"referenceID": 8, "context": ", 2014) and including them in NLP systems has been shown to be enormously helpful for a variety of downstream tasks (Collobert et al., 2011).", "startOffset": 116, "endOffset": 140}, {"referenceID": 44, "context": "Accordingly, current state of the art sequence tagging models typically include a bidirectional recurrent neural network (RNN) that encodes token sequences into a context sensitive representation before making token specific predictions (Yang et al., 2017; Ma and Hovy, 2016; Lample et al., 2016; Hashimoto et al., 2016).", "startOffset": 237, "endOffset": 320}, {"referenceID": 27, "context": "Accordingly, current state of the art sequence tagging models typically include a bidirectional recurrent neural network (RNN) that encodes token sequences into a context sensitive representation before making token specific predictions (Yang et al., 2017; Ma and Hovy, 2016; Lample et al., 2016; Hashimoto et al., 2016).", "startOffset": 237, "endOffset": 320}, {"referenceID": 22, "context": "Accordingly, current state of the art sequence tagging models typically include a bidirectional recurrent neural network (RNN) that encodes token sequences into a context sensitive representation before making token specific predictions (Yang et al., 2017; Ma and Hovy, 2016; Lample et al., 2016; Hashimoto et al., 2016).", "startOffset": 237, "endOffset": 320}, {"referenceID": 12, "context": "Accordingly, current state of the art sequence tagging models typically include a bidirectional recurrent neural network (RNN) that encodes token sequences into a context sensitive representation before making token specific predictions (Yang et al., 2017; Ma and Hovy, 2016; Lample et al., 2016; Hashimoto et al., 2016).", "startOffset": 237, "endOffset": 320}, {"referenceID": 44, "context": "Previous work has explored methods for jointly learning the bidirectional RNN with supplemental labeled data from other tasks (e.g., S\u00f8gaard and Goldberg, 2016; Yang et al., 2017).", "startOffset": 126, "endOffset": 179}, {"referenceID": 27, "context": "Our baseline sequence tagging model is a hierarchical neural tagging model, closely following a number of recent studies (Ma and Hovy, 2016; Lample et al., 2016; Yang et al., 2017; Chiu and Nichols, 2016) (left side of Figure 2).", "startOffset": 121, "endOffset": 204}, {"referenceID": 22, "context": "Our baseline sequence tagging model is a hierarchical neural tagging model, closely following a number of recent studies (Ma and Hovy, 2016; Lample et al., 2016; Yang et al., 2017; Chiu and Nichols, 2016) (left side of Figure 2).", "startOffset": 121, "endOffset": 204}, {"referenceID": 44, "context": "Our baseline sequence tagging model is a hierarchical neural tagging model, closely following a number of recent studies (Ma and Hovy, 2016; Lample et al., 2016; Yang et al., 2017; Chiu and Nichols, 2016) (left side of Figure 2).", "startOffset": 121, "endOffset": 204}, {"referenceID": 5, "context": "Our baseline sequence tagging model is a hierarchical neural tagging model, closely following a number of recent studies (Ma and Hovy, 2016; Lample et al., 2016; Yang et al., 2017; Chiu and Nichols, 2016) (left side of Figure 2).", "startOffset": 121, "endOffset": 204}, {"referenceID": 27, "context": "The character representation ck captures morphological information and is either a convolutional neural network (CNN) (Ma and Hovy, 2016; Chiu and Nichols, 2016) or RNN (Yang et al.", "startOffset": 118, "endOffset": 161}, {"referenceID": 5, "context": "The character representation ck captures morphological information and is either a convolutional neural network (CNN) (Ma and Hovy, 2016; Chiu and Nichols, 2016) or RNN (Yang et al.", "startOffset": 118, "endOffset": 161}, {"referenceID": 44, "context": "The character representation ck captures morphological information and is either a convolutional neural network (CNN) (Ma and Hovy, 2016; Chiu and Nichols, 2016) or RNN (Yang et al., 2017; Lample et al., 2016).", "startOffset": 169, "endOffset": 209}, {"referenceID": 22, "context": "The character representation ck captures morphological information and is either a convolutional neural network (CNN) (Ma and Hovy, 2016; Chiu and Nichols, 2016) or RNN (Yang et al., 2017; Lample et al., 2016).", "startOffset": 169, "endOffset": 209}, {"referenceID": 8, "context": "The token embeddings, wk, are obtained as a lookup E(\u00b7, \u03b8w), initialized using pre-trained word embeddings, and fine tuned during training (Collobert et al., 2011).", "startOffset": 139, "endOffset": 163}, {"referenceID": 6, "context": "In this paper, we use L = 2 layers of RNNs in all experiments and parameterize Ri as either Gated Recurrent Units (GRU) (Cho et al., 2014) or Long Short-Term Memory units (LSTM) (Hochreiter and Schmidhuber, 1997) depending on the task.", "startOffset": 120, "endOffset": 138}, {"referenceID": 14, "context": ", 2014) or Long Short-Term Memory units (LSTM) (Hochreiter and Schmidhuber, 1997) depending on the task.", "startOffset": 47, "endOffset": 81}, {"referenceID": 21, "context": "Accordingly, we add another layer with parameters for each label bigram, computing the sentence conditional random field (CRF) loss (Lafferty et al., 2001) using the forward-backward algorithm at training time, and using the Viterbi algorithm to find the most likely tag sequence at test time, similar to Collobert et al.", "startOffset": 132, "endOffset": 155}, {"referenceID": 8, "context": ", 2001) using the forward-backward algorithm at training time, and using the Viterbi algorithm to find the most likely tag sequence at test time, similar to Collobert et al. (2011).", "startOffset": 157, "endOffset": 181}, {"referenceID": 15, "context": "Recent state of the art neural language models (J\u00f3zefowicz et al., 2016) use a similar architecture to our baseline sequence tagger where they pass a token representation (either from a CNN over characters or as token embeddings) through multiple layers of LSTMs to embed the history (t1, t2, .", "startOffset": 47, "endOffset": 72}, {"referenceID": 39, "context": "We evaluate our approach on two well benchmarked sequence tagging tasks, the CoNLL 2003 NER task (Sang and Meulder, 2003) and the CoNLL 2000 Chunking task (Sang and Buchholz, 2000).", "startOffset": 97, "endOffset": 121}, {"referenceID": 38, "context": "We evaluate our approach on two well benchmarked sequence tagging tasks, the CoNLL 2003 NER task (Sang and Meulder, 2003) and the CoNLL 2000 Chunking task (Sang and Buchholz, 2000).", "startOffset": 155, "endOffset": 180}, {"referenceID": 8, "context": "Following Chiu and Nichols (2016), we use the Senna word embeddings (Collobert et al., 2011) and pre-processed the text by lowercasing all tokens and replacing all digits with 0.", "startOffset": 68, "endOffset": 92}, {"referenceID": 5, "context": "Following Chiu and Nichols (2016), we use the Senna word embeddings (Collobert et al.", "startOffset": 10, "endOffset": 34}, {"referenceID": 44, "context": "Following previous work (Yang et al., 2017; Chiu and Nichols, 2016) we trained on both the train and development sets after tuning hyperparameters on the development set.", "startOffset": 24, "endOffset": 67}, {"referenceID": 5, "context": "Following previous work (Yang et al., 2017; Chiu and Nichols, 2016) we trained on both the train and development sets after tuning hyperparameters on the development set.", "startOffset": 24, "endOffset": 67}, {"referenceID": 44, "context": "The hyperparameters for our baseline model are similar to Yang et al. (2017). We use two bidirectional GRUs with 80 hidden units and 25 dimensional character embeddings for the token character encoder.", "startOffset": 58, "endOffset": 77}, {"referenceID": 27, "context": "Following Ma and Hovy (2016) we added 50% dropout to the character embeddings, the input to each LSTM layer (but not recurrent connections) and to the output of the final LSTM layer.", "startOffset": 10, "endOffset": 29}, {"referenceID": 4, "context": "The primary bidirectional LMs we used in this study were trained on the 1B Word Benchmark (Chelba et al., 2014), a publicly available benchmark for largescale language modeling.", "startOffset": 90, "endOffset": 111}, {"referenceID": 4, "context": "The primary bidirectional LMs we used in this study were trained on the 1B Word Benchmark (Chelba et al., 2014), a publicly available benchmark for largescale language modeling. The training split has approximately 800 million tokens, about a 4000X increase over the number training tokens in the CoNLL datasets. J\u00f3zefowicz et al. (2016) explored several model architectures and released their best single model and training recipes.", "startOffset": 91, "endOffset": 338}, {"referenceID": 4, "context": "The primary bidirectional LMs we used in this study were trained on the 1B Word Benchmark (Chelba et al., 2014), a publicly available benchmark for largescale language modeling. The training split has approximately 800 million tokens, about a 4000X increase over the number training tokens in the CoNLL datasets. J\u00f3zefowicz et al. (2016) explored several model architectures and released their best single model and training recipes. Following Sak et al. (2014), they used linear projection layers at the output of each LSTM layer to reduce the computation time but still maintain a large LSTM state.", "startOffset": 91, "endOffset": 462}, {"referenceID": 15, "context": "In addition to CNN-BIG-LSTM from J\u00f3zefowicz et al. (2016),1 we used the same corpus to train two additional language models with fewer parameters: forward LSTM-2048-512 and backward LSTM-2048-512.", "startOffset": 33, "endOffset": 58}, {"referenceID": 15, "context": "In addition to CNN-BIG-LSTM from J\u00f3zefowicz et al. (2016),1 we used the same corpus to train two additional language models with fewer parameters: forward LSTM-2048-512 and backward LSTM-2048-512. Both language models use token embeddings as input to a single layer LSTM with 2048 units and a 512 dimension projection layer. We closely followed the procedure outlined in J\u00f3zefowicz et al. (2016), except we used synchronous parameter updates across four GPUs instead of asynchronous updates across 32 GPUs and ended training after 10 epochs.", "startOffset": 33, "endOffset": 396}, {"referenceID": 15, "context": "tree/master/lm_1b Due to different implementations, the perplexity of the forward LM with similar configurations in J\u00f3zefowicz et al. (2016) is different (45.", "startOffset": 116, "endOffset": 141}, {"referenceID": 5, "context": "Model F1\u00b1 std Chiu and Nichols (2016) 90.", "startOffset": 14, "endOffset": 38}, {"referenceID": 5, "context": "Model F1\u00b1 std Chiu and Nichols (2016) 90.91\u00b1 0.20 Lample et al. (2016) 90.", "startOffset": 14, "endOffset": 71}, {"referenceID": 5, "context": "Model F1\u00b1 std Chiu and Nichols (2016) 90.91\u00b1 0.20 Lample et al. (2016) 90.94 Ma and Hovy (2016) 91.", "startOffset": 14, "endOffset": 96}, {"referenceID": 44, "context": "Model F1\u00b1 std Yang et al. (2017) 94.", "startOffset": 14, "endOffset": 33}, {"referenceID": 18, "context": "All experiments use the Adam optimizer (Kingma and Ba, 2015) with gradient norms clipped at 5.", "startOffset": 39, "endOffset": 60}, {"referenceID": 5, "context": "Following Chiu and Nichols (2016), we train each final model configuration ten times with different random seeds and report the mean and standard deviation F1.", "startOffset": 10, "endOffset": 34}, {"referenceID": 5, "context": "33 from Chiu and Nichols (2016) that used gazetteers (at 95%, two-sided Welch t-test, p = 0.", "startOffset": 8, "endOffset": 32}, {"referenceID": 12, "context": "77 in Hashimoto et al. (2016) that jointly trains with Penn Treebank (PTB) POS tags is statistically significant at 95% (p < 0.", "startOffset": 6, "endOffset": 30}, {"referenceID": 44, "context": "75 improvement in F1 when including supervised labels from the PTB POS tags or CoNLL 2003 entities (Yang et al., 2017; S\u00f8gaard and Goldberg, 2016; Hashimoto et al., 2016).", "startOffset": 99, "endOffset": 170}, {"referenceID": 41, "context": "75 improvement in F1 when including supervised labels from the PTB POS tags or CoNLL 2003 entities (Yang et al., 2017; S\u00f8gaard and Goldberg, 2016; Hashimoto et al., 2016).", "startOffset": 99, "endOffset": 170}, {"referenceID": 12, "context": "75 improvement in F1 when including supervised labels from the PTB POS tags or CoNLL 2003 entities (Yang et al., 2017; S\u00f8gaard and Goldberg, 2016; Hashimoto et al., 2016).", "startOffset": 99, "endOffset": 170}, {"referenceID": 40, "context": "For example, Yang et al. (2017) noted an improvement of only 0.", "startOffset": 13, "endOffset": 32}, {"referenceID": 5, "context": "06 F1 in the NER task when transfer learning from both CoNLL 2000 chunks and PTB POS tags and Chiu and Nichols (2016) reported an increase of 0.", "startOffset": 94, "endOffset": 118}, {"referenceID": 5, "context": "06 Chiu and Nichols (2016) with gazetteers 90.", "startOffset": 3, "endOffset": 27}, {"referenceID": 12, "context": "75 Hashimoto et al. (2016) jointly trained with PTB-POS 95.", "startOffset": 3, "endOffset": 27}, {"referenceID": 40, "context": "results are consistent with S\u00f8gaard and Goldberg (2016) who found that chunking performance was sensitive to the level at which additional POS supervision was added.", "startOffset": 28, "endOffset": 56}, {"referenceID": 44, "context": "To test this hypothesis, we replicated the setup from Yang et al. (2017) that samples 1% of the CoNLL 2003 training set and compared the performance of TagLM to our baseline without LM.", "startOffset": 54, "endOffset": 73}, {"referenceID": 44, "context": "To test this hypothesis, we replicated the setup from Yang et al. (2017) that samples 1% of the CoNLL 2003 training set and compared the performance of TagLM to our baseline without LM. In this scenario, test F1 increased 3.35% (from 67.66 to 71.01%) compared to an increase of 1.06% F1 for a similar comparison with the full training dataset. The analogous increases in Yang et al. (2017) are 3.", "startOffset": 54, "endOffset": 390}, {"referenceID": 0, "context": "05%) for entity extraction over our baseline without LM embeddings and it was a major component in our winning submission to ScienceIE, Scenario 1 (Ammar et al., 2017).", "startOffset": 147, "endOffset": 167}, {"referenceID": 21, "context": "Instead of using a LM, Li and McCallum (2005) uses a probabilistic generative model to infer contextsensitive latent variables for each token, which are then used as extra features in a supervised CRF tagger (Lafferty et al., 2001).", "startOffset": 208, "endOffset": 231}, {"referenceID": 3, "context": "Other semisupervised learning methods for structured prediction problems include co-training (Blum and Mitchell, 1998; Pierce and Cardie, 2001), expectation maximization (Nigam et al.", "startOffset": 93, "endOffset": 143}, {"referenceID": 35, "context": "Other semisupervised learning methods for structured prediction problems include co-training (Blum and Mitchell, 1998; Pierce and Cardie, 2001), expectation maximization (Nigam et al.", "startOffset": 93, "endOffset": 143}, {"referenceID": 32, "context": "Other semisupervised learning methods for structured prediction problems include co-training (Blum and Mitchell, 1998; Pierce and Cardie, 2001), expectation maximization (Nigam et al., 2000; Mohit and Hwa, 2005), structural learning (Ando and Zhang, 2005) and maximum discriminant functions (Suzuki et al.", "startOffset": 170, "endOffset": 211}, {"referenceID": 31, "context": "Other semisupervised learning methods for structured prediction problems include co-training (Blum and Mitchell, 1998; Pierce and Cardie, 2001), expectation maximization (Nigam et al., 2000; Mohit and Hwa, 2005), structural learning (Ando and Zhang, 2005) and maximum discriminant functions (Suzuki et al.", "startOffset": 170, "endOffset": 211}, {"referenceID": 1, "context": ", 2000; Mohit and Hwa, 2005), structural learning (Ando and Zhang, 2005) and maximum discriminant functions (Suzuki et al.", "startOffset": 50, "endOffset": 72}, {"referenceID": 42, "context": ", 2000; Mohit and Hwa, 2005), structural learning (Ando and Zhang, 2005) and maximum discriminant functions (Suzuki et al., 2007; Suzuki and Isozaki, 2008).", "startOffset": 108, "endOffset": 155}, {"referenceID": 43, "context": ", 2000; Mohit and Hwa, 2005), structural learning (Ando and Zhang, 2005) and maximum discriminant functions (Suzuki et al., 2007; Suzuki and Isozaki, 2008).", "startOffset": 108, "endOffset": 155}, {"referenceID": 21, "context": "Besides pre-trained word embeddings, our method is most closely related to Li and McCallum (2005). Instead of using a LM, Li and McCallum (2005) uses a probabilistic generative model to infer contextsensitive latent variables for each token, which are then used as extra features in a supervised CRF tagger (Lafferty et al.", "startOffset": 75, "endOffset": 98}, {"referenceID": 21, "context": "Besides pre-trained word embeddings, our method is most closely related to Li and McCallum (2005). Instead of using a LM, Li and McCallum (2005) uses a probabilistic generative model to infer contextsensitive latent variables for each token, which are then used as extra features in a supervised CRF tagger (Lafferty et al.", "startOffset": 75, "endOffset": 145}, {"referenceID": 40, "context": "io/ in (S\u00f8gaard, 2013).", "startOffset": 7, "endOffset": 22}, {"referenceID": 19, "context": "LM embeddings are related to a class of methods (e.g., Le and Mikolov, 2014; Kiros et al., 2015; Hill et al., 2016) for learning sentence and document encoders from unlabeled data, which can be used for text classification and textual entailment among other tasks.", "startOffset": 48, "endOffset": 115}, {"referenceID": 13, "context": "LM embeddings are related to a class of methods (e.g., Le and Mikolov, 2014; Kiros et al., 2015; Hill et al., 2016) for learning sentence and document encoders from unlabeled data, which can be used for text classification and textual entailment among other tasks.", "startOffset": 48, "endOffset": 115}, {"referenceID": 9, "context": "Dai and Le (2015) pre-trained LSTMs using language models and sequence autoencoders then fine tuned the weights for classification tasks.", "startOffset": 0, "endOffset": 18}, {"referenceID": 20, "context": "LMs have always been a critical component in statistical machine translation systems (Koehn, 2009).", "startOffset": 85, "endOffset": 98}, {"referenceID": 2, "context": "Recently, neural LMs (Bengio et al., 2003; Mikolov et al., 2010) have also been integrated in neural machine translation systems (e.", "startOffset": 21, "endOffset": 64}, {"referenceID": 29, "context": "Recently, neural LMs (Bengio et al., 2003; Mikolov et al., 2010) have also been integrated in neural machine translation systems (e.", "startOffset": 21, "endOffset": 64}, {"referenceID": 10, "context": ", 2010) have also been integrated in neural machine translation systems (e.g., Kalchbrenner and Blunsom, 2013; Devlin et al., 2014) to score candidate translations.", "startOffset": 72, "endOffset": 131}, {"referenceID": 33, "context": "Most similar to our formulation, Peris and Casacuberta (2015) used a bidirectional neural LM in a statistical machine translation system for instance selection.", "startOffset": 33, "endOffset": 62}, {"referenceID": 11, "context": "Frinken et al. (2012) also used a bidirectional n-gram LM for handwriting recognition.", "startOffset": 0, "endOffset": 22}, {"referenceID": 24, "context": "Linzen et al. (2016) showed that single LSTM units can learn to predict singular-plural distinctions.", "startOffset": 0, "endOffset": 21}, {"referenceID": 17, "context": "Karpathy et al. (2015) visualized character level LSTM states and showed that individual cells capture long-range dependencies such as line lengths, quotes and brackets.", "startOffset": 0, "endOffset": 23}, {"referenceID": 7, "context": "However, many other sequence tagging models have been proposed in the literature for this class of problems (e.g., Lafferty et al., 2001; Collins, 2002).", "startOffset": 108, "endOffset": 152}], "year": 2017, "abstractText": "Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.", "creator": "LaTeX with hyperref package"}}}