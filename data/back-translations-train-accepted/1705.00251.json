{"id": "1705.00251", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Apr-2017", "title": "Lifelong Learning CRF for Supervised Aspect Extraction", "abstract": "This paper makes a focused contribution to supervised aspect extraction. It shows that if the system has performed aspect extraction from many past domains and retained their results as knowledge, Conditional Random Fields (CRF) can leverage this knowledge in a lifelong learning manner to extract in a new domain markedly better than the traditional CRF without using this prior knowledge. The key innovation is that even after CRF training, the model can still improve its extraction with experiences in its applications.", "histories": [["v1", "Sat, 29 Apr 2017 23:33:13 GMT  (35kb)", "http://arxiv.org/abs/1705.00251v1", "Accepted at ACL 2017. arXiv admin note: text overlap witharXiv:1612.07940"]], "COMMENTS": "Accepted at ACL 2017. arXiv admin note: text overlap witharXiv:1612.07940", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lei shu", "hu xu", "bing liu"], "accepted": true, "id": "1705.00251"}, "pdf": {"name": "1705.00251.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["liub}@uic.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.00 251v 1 [cs.C L] 29 Apr 201 7This paper makes a focused contribution to monitored aspect extraction. It shows that if the system has performed aspect extraction from many past domains and retained its results as knowledge, Conditional Random Fields (CRF) can use this knowledge in a lifetime way to extract significantly better in a new domain than traditional CRF without using this prior knowledge. The key innovation is that the model can still improve its extraction after CRF training through experience in its applications."}, {"heading": "1 Introduction", "text": "It extracts the opinion objectives from the previous text. For example, from the sentence \"The battery is good,\" it aims to extract the \"battery\" which is a product feature, also as a spectral aspect aspect aspect aspect (and non-spectral aspect and non-spectral aspect and non-spectral aspect and non-spectral aspect and non-spectral aspect (and non-spectral aspect and non-spectral aspect) spectral aspect (and non-spectral aspect and non-spectral aspect and non-spectral aspect)"}, {"heading": "2 Conditional Random Fields", "text": "CRF learns from an observation sequence x to estimate a label sequence y: p (y | x; FF), where the label FF is a set of weights. Let l be the l-th position in the sequence. Core components of CRF are a set of markup functions F = {fh (yl, yl \u2212 1, xl)} H h = 1 and their corresponding weights \u03b8 = {\u03b8h} H h = 1. Labeling functions: We use two types of markup functions (FF). One is labeling function (LL) FF: fLLij (yl \u2212 1) = 1 {yl \u2212 i} 1 {Label = j} H h, (1) where Y is the set of markup functions, and the other is labeling function. The other is labeling word-word (LW) FF: fLLLLLij (yl \u2212 1) = 1 {yl \u2212 yl the label value of {P-1} and {P-1} labeling function."}, {"heading": "3 General Dependency Feature (G)", "text": "Feature G uses generalized dependency relationships. What is interesting about this feature is that it enables L-CRF to use past knowledge in its sequence prediction at test date to perform much better, and this will become clear shortly. This feature takes a dependency pattern as its value, which is generalized from dependency relationships. The general dependency feature (G) of the variable xl takes a number of characteristic values V G. Each characteristic value vG is a dependency pattern. LabelG (LG) FF is defined as: fLG ivG (yl, xl) = 1 {yl = i} 1 {x G l = v G}, \u0399i-Y, \u043avG and VG. (4) Such an FF returns 1 if the dependency characteristic of the variable xl corresponds to a dependency pattern vG and the variable corresponds to the value i."}, {"heading": "3.1 Dependency Relation", "text": "Dependency relationships have proven useful in many emotional analysis applications (Johansson and Moschitti, 2010; Jakob and Gurevych, 2010).A dependency relationship 1 is a quintuple: (type, gov, govpos, dep, deppos), where type is the type of dependency relationship, gov is the governor's word, govpos is the POS tag of the governor's word, dep is the dependent word, and deppos is the POS tag of the dependent word. The l-th word can be either the governor's word or the dependent word in a dependency relationship."}, {"heading": "3.2 Dependency Pattern", "text": "We generalize dependency relationships in dependency patterns with the following steps: 1We obtain dependency relationships with the help of Stanford CoreNLP: http: / / stanfordnlp.github.io / CoreNLP / 1. For each dependency relationship, replace the current word (gubernatorial word or dependent word) and its POS tag with a placeholder, as we already mark the word (W) and the POS tag (P).2. Replace the contextual word (the word other than the l word) in each dependency relationship with a knowledge slab to form a more general dependency pattern. Let the set of aspects commented on in the training data be Kt. If the contextual word appears in the dependency relationship, we replace it with a knowledge slabel \"A\" (aspect); otherwise \"O\" (other).We are working on the sentence \"annels.\" The battery of this camera is great. \""}, {"heading": "4 The Proposed L-CRF Algorithm", "text": "In fact, it is the case that we will be able to solve the problem, \"he said in an interview with the German Press Agency.\" We will be able to solve the problem, \"he said.\" We will be able to solve it, \"he said.\" We will be able to solve it. \""}, {"heading": "5 Experiments", "text": "We are now evaluating the proposed L-CRF method and comparing it with baselines."}, {"heading": "5.1 Evaluation Datasets", "text": "We use two types of data for our experiments: the first type consists of seven (7) annotated benchmark verification records across 7 areas (product types); as they are annotated, they are used in training and testing; the first four data sets are from (Hu and Liu, 2004), which actually have 5 data sets across 4 areas; as we are mainly interested in domain level results, we have not used any of the domain repeated records; the last three data sets from three domains (products) are from (Liu et al., 2016); these data sets are used to evaluate our CRF training data Dt and test data Dn + 1; the annotation details are in Table 2.The second type contains 50 blank verification records from 50 domains or product types (Chen and Liu, 2014); each data set has 1,000 ratings; they are used as the previous domain data, i.e. D1, Dn, as they are not used for testing (Chen, 2014 and Liu)."}, {"heading": "5.2 Baseline Methods", "text": "We compare L-CRF with CRF. We will not compare with unattended methods that can be improved through lifelong learning (Chen et al., 2014; Liu et al., 2016). The frequency threshold \u03bb in algorithm 1 used in our experiment to assess which extracted aspects are considered reliable is empirical to 2. CRF: We use the linear chain CRF of 2. Note that CRF uses all traits, including dependency traits, such as the proposed L-CRF, but not the 50 unlabeled domains used for lifelong learning, CRF + R: It treats the reliable aspect K as a dictionary. It adds those reliable aspects in K that are not extracted from CRF but are included in the test data to the final results. We would like to see if the inclusion of K in the CRF extraction through dependency patterns in L-CRF is actually necessary."}, {"heading": "5.3 Experiment Setting", "text": "To compare systems that use the same training and test data, we use 200 records for training and 200 records for testing for each record, to avoid distortions to any data set or domain, because we combine multiple domain records for CRF training. We performed both cross-domain and cross-domain tests. Our problem is crossdomain. In-domain is used for completeness. In both cases, we assume that the extraction has been performed for the 50 domains. Cross-domain experiments: We combine 6 cross-domain records for training (1200 records) and tests on the 7th domain (not used in training), resulting in 7 cross-domain results. This set of tests is particularly interesting because it is desirable to use the tracked model in cross-domain situations to save manual labeling efforts. In-domain experiments: We train and test on the same 6 domains (1200 records for training and 1200 sets for testing)."}, {"heading": "5.4 Results and Analysis", "text": "All experiment results are shown in Table 3.Cross-domain: Each \u2212 X in column 1 means domain X is not used in training. X in column 2 means domain X is used in testing. We can see that L-CRF is significantly better than CRF and CRF + R in F1. CRF + R is very poor due to lack of precision, which shows that handling the reliable aspects that K has set as a dictionary is not a good idee.In-domain: \u2212 X in training and test columns means that the other 6 domains are used both in training and testing (i.e. in-domain)."}, {"heading": "6 Conclusion", "text": "This paper proposed a method of lifelong learning to enable CRF to use the findings from the extraction results of earlier areas (unlabeled) to improve its extraction. Experimental results showed the effectiveness of L-CRF. The current approach does not change the CRF model itself. In our future work, we plan to modify CRF so that it can take into account earlier extraction results as well as the knowledge of previous CRF models."}, {"heading": "Acknowledgments", "text": "This work was partially supported by grants from the National Science Foundation (NSF) under grant numbers IIS-1407927 and IIS-1650900."}], "references": [{"title": "An unsupervised aspect-sentiment model for online reviews", "author": ["Samuel Brody", "Noemie Elhadad."], "venue": "NAACL \u201910. pages 804\u2013812.", "citeRegEx": "Brody and Elhadad.,? 2010", "shortCiteRegEx": "Brody and Elhadad.", "year": 2010}, {"title": "Topic modeling using topics from many domains, lifelong learning and big data", "author": ["Zhiyuan Chen", "Bing Liu."], "venue": "ICML \u201914. pages 703\u2013711.", "citeRegEx": "Chen and Liu.,? 2014", "shortCiteRegEx": "Chen and Liu.", "year": 2014}, {"title": "Lifelong Machine Learning", "author": ["Zhiyuan Chen", "Bing Liu."], "venue": "Morgan & Claypool Publishers.", "citeRegEx": "Chen and Liu.,? 2016", "shortCiteRegEx": "Chen and Liu.", "year": 2016}, {"title": "Lifelong learning for sentiment classification", "author": ["Zhiyuan Chen", "Nianzu Ma", "Bing Liu."], "venue": "Volume 2: Short Papers page 750.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Aspect extraction with automated prior knowledge learning", "author": ["Zhiyuan Chen", "Arjun Mukherjee", "Bing Liu."], "venue": "ACL \u201914. pages 347\u2013358.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Hierarchical sequential learning for extracting opinions and their attributes", "author": ["Yejin Choi", "Claire Cardie."], "venue": "ACL \u201910. pages 269\u2013274.", "citeRegEx": "Choi and Cardie.,? 2010", "shortCiteRegEx": "Choi and Cardie.", "year": 2010}, {"title": "Fine granular aspect analysis using latent structural models", "author": ["Lei Fang andMinlie Huang."], "venue": "ACL \u201912. pages 333\u2013337.", "citeRegEx": "Huang.,? 2012", "shortCiteRegEx": "Huang.", "year": 2012}, {"title": "Mining and summarizing customer reviews", "author": ["Minqing Hu", "Bing Liu."], "venue": "KDD \u201904. pages 168\u2013 177.", "citeRegEx": "Hu and Liu.,? 2004", "shortCiteRegEx": "Hu and Liu.", "year": 2004}, {"title": "Extracting opinion targets in a single- and cross-domain setting with conditional random fields", "author": ["Niklas Jakob", "Iryna Gurevych."], "venue": "EMNLP \u201910. pages 1035\u20131045.", "citeRegEx": "Jakob and Gurevych.,? 2010", "shortCiteRegEx": "Jakob and Gurevych.", "year": 2010}, {"title": "Aspect and sentiment unification model for online review analysis", "author": ["Yohan Jo", "Alice H. Oh."], "venue": "InWSDM \u201911. pages 815\u2013824.", "citeRegEx": "Jo and Oh.,? 2011", "shortCiteRegEx": "Jo and Oh.", "year": 2011}, {"title": "Syntactic and semantic structure for opinion expression detection", "author": ["Richard Johansson", "Alessandro Moschitti."], "venue": "Proceedings of the Fourteenth Conference on Computational Natural Language Learning. pages 67\u201376.", "citeRegEx": "Johansson and Moschitti.,? 2010", "shortCiteRegEx": "Johansson and Moschitti.", "year": 2010}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando CN Pereira."], "venue": "ICML \u201901. pages 282\u2013289.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Sentiment analysis with global topics and local dependency", "author": ["Fangtao Li", "Minlie Huang", "Xiaoyan Zhu."], "venue": "AAAI \u201910. pages 1371\u20131376.", "citeRegEx": "Li et al\\.,? 2010", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Joint sentiment/topic model for sentiment analysis", "author": ["Chenghua Lin", "Yulan He."], "venue": "CIKM \u201909. pages 375\u2013384.", "citeRegEx": "Lin and He.,? 2009", "shortCiteRegEx": "Lin and He.", "year": 2009}, {"title": "Sentiment Analysis and Opinion Mining", "author": ["Bing Liu."], "venue": "Morgan & Claypool Publishers.", "citeRegEx": "Liu.,? 2012", "shortCiteRegEx": "Liu.", "year": 2012}, {"title": "Opinion target extraction using partially-supervised word alignment model", "author": ["Kang Liu", "Liheng Xu", "Yang Liu", "Jun Zhao."], "venue": "IJCAI \u201913. pages 2134\u2013 2140.", "citeRegEx": "Liu et al\\.,? 2013", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Improving opinion aspect extraction using semantic similarity and aspect associations", "author": ["Qian Liu", "Bing Liu", "Yuanlin Zhang", "Doo Soon Kim", "Zhiqiang Gao."], "venue": "Thirtieth AAAI Conference on Artificial Intelligence.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Topic sentiment mixture: Modeling facets and opinions in weblogs", "author": ["Qiaozhu Mei", "Xu Ling", "Matthew Wondra", "Hang Su", "ChengXiang Zhai."], "venue": "WWW \u201907. pages 171\u2013180.", "citeRegEx": "Mei et al\\.,? 2007", "shortCiteRegEx": "Mei et al\\.", "year": 2007}, {"title": "Open domain targeted sentiment", "author": ["Margaret Mitchell", "Jacqui Aguilar", "Theresa Wilson", "Benjamin Van Durme."], "venue": "ACL \u201913. pages 1643\u20131654.", "citeRegEx": "Mitchell et al\\.,? 2013", "shortCiteRegEx": "Mitchell et al\\.", "year": 2013}, {"title": "Neverending learning", "author": ["A Saparov", "M Greaves", "J Welling."], "venue": "AAAI\u20192015.", "citeRegEx": "Saparov et al\\.,? 2015", "shortCiteRegEx": "Saparov et al\\.", "year": 2015}, {"title": "ILDA: interdependent lda model for learning latent aspects and their ratings from online product reviews", "author": ["Samaneh Moghaddam", "Martin Ester."], "venue": "SIGIR \u201911. pages 665\u2013674.", "citeRegEx": "Moghaddam and Ester.,? 2011", "shortCiteRegEx": "Moghaddam and Ester.", "year": 2011}, {"title": "Aspect extraction through semi-supervised modeling", "author": ["Arjun Mukherjee", "Bing Liu."], "venue": "ACL \u201912. volume 1, pages 339\u2013348.", "citeRegEx": "Mukherjee and Liu.,? 2012", "shortCiteRegEx": "Mukherjee and Liu.", "year": 2012}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang."], "venue": "IEEE Transactions on knowledge and data engineering 22(10):1345\u20131359.", "citeRegEx": "Pan and Yang.,? 2010", "shortCiteRegEx": "Pan and Yang.", "year": 2010}, {"title": "Extracting product features and opinions from reviews", "author": ["Ana-Maria Popescu", "Oren Etzioni."], "venue": "HLT-EMNLP \u201905. pages 339\u2013346.", "citeRegEx": "Popescu and Etzioni.,? 2005", "shortCiteRegEx": "Popescu and Etzioni.", "year": 2005}, {"title": "A rule-based approach to aspect extraction from product reviews", "author": ["Soujanya Poria", "Erik Cambria", "Lun-Wei Ku", "Chen Gui", "Alexander Gelbukh."], "venue": "SocialNLP \u201914. pages 28\u201337.", "citeRegEx": "Poria et al\\.,? 2014", "shortCiteRegEx": "Poria et al\\.", "year": 2014}, {"title": "Opinion word expansion and target extraction through double propagation", "author": ["Guang Qiu", "Bing Liu", "Jiajun Bu", "Chun Chen."], "venue": "Computational Linguistics 37(1):9\u201327.", "citeRegEx": "Qiu et al\\.,? 2011", "shortCiteRegEx": "Qiu et al\\.", "year": 2011}, {"title": "Ella: An efficient lifelong learning algorithm", "author": ["Paul Ruvolo", "Eric Eaton."], "venue": "ICML (1) 28:507\u2013515.", "citeRegEx": "Ruvolo and Eaton.,? 2013", "shortCiteRegEx": "Ruvolo and Eaton.", "year": 2013}, {"title": "Lifelong-rl: Lifelong relaxation labeling for separating entities and aspects in opinion targets", "author": ["Lei Shu", "Bing Liu", "Hu Xu", "Annice Kim."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Shu et al\\.,? 2016", "shortCiteRegEx": "Shu et al\\.", "year": 2016}, {"title": "Lifelong learning algorithms", "author": ["Sebastian Thrun"], "venue": "Machine Learning. Citeseer,", "citeRegEx": "Thrun.,? \\Q1998\\E", "shortCiteRegEx": "Thrun.", "year": 1998}, {"title": "Jointly modeling aspects and opinions", "author": ["ing Li"], "venue": null, "citeRegEx": "Li.,? \\Q2010\\E", "shortCiteRegEx": "Li.", "year": 2010}, {"title": "Multi-aspect opinion polling", "author": ["Muhua Zhu"], "venue": null, "citeRegEx": "Zhu.,? \\Q2009\\E", "shortCiteRegEx": "Zhu.", "year": 2009}, {"title": "Semi-supervised learning literature", "author": ["Xiaojin Zhu"], "venue": null, "citeRegEx": "1802", "shortCiteRegEx": "1802", "year": 2005}], "referenceMentions": [{"referenceID": 14, "context": "Aspect extraction is a key task of opinion mining (Liu, 2012).", "startOffset": 50, "endOffset": 61}, {"referenceID": 7, "context": "The unsupervised approach includes methods such as frequent pattern mining (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhu et al., 2009), syntactic rules-based extraction (Zhuang et al.", "startOffset": 75, "endOffset": 138}, {"referenceID": 23, "context": "The unsupervised approach includes methods such as frequent pattern mining (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhu et al., 2009), syntactic rules-based extraction (Zhuang et al.", "startOffset": 75, "endOffset": 138}, {"referenceID": 25, "context": ", 2009), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Zhang et al., 2010; Qiu et al., 2011; Poria et al., 2014), topic modeling (Mei et al.", "startOffset": 42, "endOffset": 159}, {"referenceID": 24, "context": ", 2009), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Zhang et al., 2010; Qiu et al., 2011; Poria et al., 2014), topic modeling (Mei et al.", "startOffset": 42, "endOffset": 159}, {"referenceID": 17, "context": ", 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al.", "startOffset": 24, "endOffset": 276}, {"referenceID": 12, "context": ", 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al.", "startOffset": 24, "endOffset": 276}, {"referenceID": 0, "context": ", 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al.", "startOffset": 24, "endOffset": 276}, {"referenceID": 20, "context": ", 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al.", "startOffset": 24, "endOffset": 276}, {"referenceID": 21, "context": ", 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al.", "startOffset": 24, "endOffset": 276}, {"referenceID": 13, "context": ", 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al.", "startOffset": 24, "endOffset": 276}, {"referenceID": 9, "context": ", 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al.", "startOffset": 24, "endOffset": 276}, {"referenceID": 15, "context": ", 2016), word alignment (Liu et al., 2013), label propagation (Zhou et al.", "startOffset": 24, "endOffset": 42}, {"referenceID": 27, "context": ", 2013), label propagation (Zhou et al., 2013; Shu et al., 2016), and others (Zhao et al.", "startOffset": 27, "endOffset": 64}, {"referenceID": 8, "context": "This paper focuses on the supervised approach (Jakob and Gurevych, 2010; Choi and Cardie, 2010; Mitchell et al., 2013) using Conditional Random Fields (CRF) (Lafferty et al.", "startOffset": 46, "endOffset": 118}, {"referenceID": 5, "context": "This paper focuses on the supervised approach (Jakob and Gurevych, 2010; Choi and Cardie, 2010; Mitchell et al., 2013) using Conditional Random Fields (CRF) (Lafferty et al.", "startOffset": 46, "endOffset": 118}, {"referenceID": 18, "context": "This paper focuses on the supervised approach (Jakob and Gurevych, 2010; Choi and Cardie, 2010; Mitchell et al., 2013) using Conditional Random Fields (CRF) (Lafferty et al.", "startOffset": 46, "endOffset": 118}, {"referenceID": 11, "context": ", 2013) using Conditional Random Fields (CRF) (Lafferty et al., 2001).", "startOffset": 46, "endOffset": 69}, {"referenceID": 1, "context": "The improvement is possible because although every product (domain) is different, there is a fair amount of aspects sharing across domains (Chen and Liu, 2014).", "startOffset": 139, "endOffset": 159}, {"referenceID": 2, "context": "Due to leveraging the knowledge gained from the past to help the new domain extraction, we are using the idea of lifelong machine learning (LML) (Chen and Liu, 2016; Thrun, 1998; Silver et al., 2013), which is a continuous learning paradigm that retains the knowledge learned in the past and uses it to help future learning and problem solving with possible adaptations.", "startOffset": 145, "endOffset": 199}, {"referenceID": 28, "context": "Due to leveraging the knowledge gained from the past to help the new domain extraction, we are using the idea of lifelong machine learning (LML) (Chen and Liu, 2016; Thrun, 1998; Silver et al., 2013), which is a continuous learning paradigm that retains the knowledge learned in the past and uses it to help future learning and problem solving with possible adaptations.", "startOffset": 145, "endOffset": 199}, {"referenceID": 4, "context": "There are prior LML works for aspect extraction (Chen et al., 2014; Liu et al., 2016), but they were all unsupervised methods.", "startOffset": 48, "endOffset": 85}, {"referenceID": 16, "context": "There are prior LML works for aspect extraction (Chen et al., 2014; Liu et al., 2016), but they were all unsupervised methods.", "startOffset": 48, "endOffset": 85}, {"referenceID": 3, "context": "Supervised LML methods exist (Chen et al., 2015; Ruvolo and Eaton, 2013), but they are for classification rather than for sequence learning or labeling like CRF.", "startOffset": 29, "endOffset": 72}, {"referenceID": 26, "context": "Supervised LML methods exist (Chen et al., 2015; Ruvolo and Eaton, 2013), but they are for classification rather than for sequence learning or labeling like CRF.", "startOffset": 29, "endOffset": 72}, {"referenceID": 22, "context": "LML is related to transfer learning and multi-task learning (Pan and Yang, 2010), but they are also quite different (see (Chen and Liu, 2016) for details).", "startOffset": 60, "endOffset": 80}, {"referenceID": 2, "context": "LML is related to transfer learning and multi-task learning (Pan and Yang, 2010), but they are also quite different (see (Chen and Liu, 2016) for details).", "startOffset": 121, "endOffset": 141}, {"referenceID": 8, "context": "Following the previous work in (Jakob and Gurevych, 2010), we use the feature set {W, -1W, +1W, P, -1P, +1P, G}, where W is the word and P is its POS-tag, -1W is the previous word, -1P is its POS-tag, +1W is the next word, +1P is its POS-tag, and G is the generalized dependency feature.", "startOffset": 31, "endOffset": 57}, {"referenceID": 10, "context": "Dependency relations have been shown useful in many sentiment analysis applications (Johansson and Moschitti, 2010; Jakob and Gurevych, 2010).", "startOffset": 84, "endOffset": 141}, {"referenceID": 8, "context": "Dependency relations have been shown useful in many sentiment analysis applications (Johansson and Moschitti, 2010; Jakob and Gurevych, 2010).", "startOffset": 84, "endOffset": 141}, {"referenceID": 7, "context": "The first 4 datasets are from (Hu and Liu, 2004), which actually has 5 datasets from 4 domains.", "startOffset": 30, "endOffset": 48}, {"referenceID": 16, "context": "The last 3 datasets of three domains (products) are from (Liu et al., 2016).", "startOffset": 57, "endOffset": 75}, {"referenceID": 1, "context": "The second type has 50 unlabeled review datasets from 50 domains or types of products (Chen and Liu, 2014).", "startOffset": 86, "endOffset": 106}, {"referenceID": 4, "context": "We will not compare with unsupervised methods, which have been shown improvable by lifelong learning (Chen et al., 2014; Liu et al., 2016).", "startOffset": 101, "endOffset": 138}, {"referenceID": 16, "context": "We will not compare with unsupervised methods, which have been shown improvable by lifelong learning (Chen et al., 2014; Liu et al., 2016).", "startOffset": 101, "endOffset": 138}], "year": 2017, "abstractText": "This paper makes a focused contribution to supervised aspect extraction. It shows that if the system has performed aspect extraction from many past domains and retained their results as knowledge, Conditional Random Fields (CRF) can leverage this knowledge in a lifelong learning manner to extract in a new domain markedly better than the traditional CRF without using this prior knowledge. The key innovation is that even after CRF training, the model can still improve its extraction with experiences in its applications.", "creator": "LaTeX with hyperref package"}}}