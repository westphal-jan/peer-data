{"id": "1705.00424", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2017", "title": "Model Transfer for Tagging Low-resource Languages using a Bilingual Dictionary", "abstract": "Cross-lingual model transfer is a compelling and popular method for predicting annotations in a low-resource language, whereby parallel corpora provide a bridge to a high-resource language and its associated annotated corpora. However, parallel data is not readily available for many languages, limiting the applicability of these approaches. We address these drawbacks in our framework which takes advantage of cross-lingual word embeddings trained solely on a high coverage bilingual dictionary. We propose a novel neural network model for joint training from both sources of data based on cross-lingual word embeddings, and show substantial empirical improvements over baseline techniques. We also propose several active learning heuristics, which result in improvements over competitive benchmark methods.", "histories": [["v1", "Mon, 1 May 2017 05:58:56 GMT  (106kb,D)", "http://arxiv.org/abs/1705.00424v1", "5 pages with 2 pages reference. Accepted to appear in ACL 2017"]], "COMMENTS": "5 pages with 2 pages reference. Accepted to appear in ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["meng fang", "trevor cohn"], "accepted": true, "id": "1705.00424"}, "pdf": {"name": "1705.00424.pdf", "metadata": {"source": "CRF", "title": "Model Transfer for Tagging Low-resource Languages using a Bilingual Dictionary", "authors": ["Meng Fang"], "emails": ["meng.fang@unimelb.edu.au,", "t.cohn@unimelb.edu.au"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules they have set themselves, and they will be able to play by the rules they have set themselves."}, {"heading": "2 Related work", "text": "Traditionally, probabilistic models are a popular choice, such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Lafferty et al., 2001). Recently, neural network models for POS marking have been developed and achieved good performance, such as RNN and Bidirectional Short-Term Memory (BiLSTM) and CRF-BiLSTM models (Mikolov et al., 2010; Huang et al., 2015). Thus, the CRFBiLSTM POS marker achieved the most advanced identification performance on Penn treebank WSJ corpus (Huang et al., 2015).However, these models are rarely used in languages with limited resources because the data is limited, and parallel data therefore seems to be the most realistic additional source of information for developing NLP systems in languages with limited resources."}, {"heading": "3 Model", "text": "We now describe the model of POS labeling in a language that is about transferring information that is in the language and in the language. [2] Our model uses an explicit multi-layer perceptron when it comes to generating gold-standard keywords. [3] The multi-layer perceptron keywords can capture both language-specific and consistent speech errors. [4] We project a number of active learning methods to further reduce the need for annotation-related learning."}, {"heading": "4 Experiments", "text": "This year it is more than ever before."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a method for identifying a language with limited resources, without the need for bilingual parallel corpora. We introduced a new method of cross-language remote monitoring based on a bilingual dictionary. In addition, deep neural network models with limited monitoring can be effective by incorporating remote monitoring, in the form of model transfer with cross-language word embedding. We demonstrate that traditional strategies for scanning uncertainty in settings with limited resources do not work well, and introduce new methods based on word type tagging. Overall, our approach leads to consistent and substantial improvements over benchmark methods."}, {"heading": "Acknowledgments", "text": "This work was funded by the Defense Advanced Research Projects Agency Information Innovation Office (I2O) under DARPA / I2O's Low Resource Languages for Emergent Incidents (LORELEI) program under contract number HR0011-15-C-0114. The views expressed are those of the author and do not reflect the official policy or position of the Department of Defense or the U.S. Government. Trevor Cohn was supported by the Australian Research Council Future Fellowship (project number FT130101105)."}], "references": [{"title": "Multilingual projection for parsing truly low-resource languages. Transactions of the Association for Computational Linguistics", "author": ["\u017deljko Agi\u0107", "Anders Johannsen", "Barbara Plank", "H\u00e9ctor Alonso Mart\u0131\u0301nez", "Natalie Schluter", "Anders S\u00f8gaard"], "venue": null, "citeRegEx": "Agi\u0107 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agi\u0107 et al\\.", "year": 2016}, {"title": "Massively multilingual word embeddings", "author": ["Waleed Ammar", "George Mulcaire", "Yulia Tsvetkov", "Guillaume Lample", "Chris Dyer", "Noah A Smith."], "venue": "Transactions of the Association for Computational Linguistics 4:431\u2013444.", "citeRegEx": "Ammar et al\\.,? 2016", "shortCiteRegEx": "Ammar et al\\.", "year": 2016}, {"title": "Conll-x shared task on multilingual dependency parsing", "author": ["Sabine Buchholz", "Erwin Marsi."], "venue": "Proceedings of the Tenth Conference on Computational Natural Language Learning. Association for Computational Linguistics, pages 149\u2013164.", "citeRegEx": "Buchholz and Marsi.,? 2006", "shortCiteRegEx": "Buchholz and Marsi.", "year": 2006}, {"title": "Cross-lingual morphological tagging for low-resource languages", "author": ["Jan Buys", "Jan A. Botha."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics, Berlin, Ger-", "citeRegEx": "Buys and Botha.,? 2016", "shortCiteRegEx": "Buys and Botha.", "year": 2016}, {"title": "Unsupervised part-of-speech tagging with bilingual graph-based projections", "author": ["Dipanjan Das", "Slav Petrov."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-", "citeRegEx": "Das and Petrov.,? 2011", "shortCiteRegEx": "Das and Petrov.", "year": 2011}, {"title": "Learning when to trust distant supervision: An application to lowresource pos tagging using cross-lingual projection", "author": ["Meng Fang", "Trevor Cohn."], "venue": "Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning", "citeRegEx": "Fang and Cohn.,? 2016", "shortCiteRegEx": "Fang and Cohn.", "year": 2016}, {"title": "Learning a part-of-speech tagger from two hours of annotation", "author": ["Dan Garrette", "Jason Baldridge."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Garrette and Baldridge.,? 2013", "shortCiteRegEx": "Garrette and Baldridge.", "year": 2013}, {"title": "Token and type", "author": ["McDonald", "Joakim Nivre"], "venue": null, "citeRegEx": "McDonald and Nivre.,? \\Q2013\\E", "shortCiteRegEx": "McDonald and Nivre.", "year": 2013}, {"title": "Ten pairs to tag\u2013 multilingual pos tagging via coarse mapping between embeddings", "author": ["Yuan Zhang", "David Gaddy", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Asso-", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Prior work on low-resource NLP has primarily focused on exploiting parallel corpora to project information between a high- and low-resource language (Yarowsky and Ngai, 2001; T\u00e4ckstr\u00f6m et al., 2013; Guo et al., 2015; Agi\u0107 et al., 2016; Buys and Botha, 2016).", "startOffset": 149, "endOffset": 257}, {"referenceID": 3, "context": "Prior work on low-resource NLP has primarily focused on exploiting parallel corpora to project information between a high- and low-resource language (Yarowsky and Ngai, 2001; T\u00e4ckstr\u00f6m et al., 2013; Guo et al., 2015; Agi\u0107 et al., 2016; Buys and Botha, 2016).", "startOffset": 149, "endOffset": 257}, {"referenceID": 4, "context": "resource language (Das and Petrov, 2011; Zhang et al., 2016; Fang and Cohn, 2016).", "startOffset": 18, "endOffset": 81}, {"referenceID": 8, "context": "resource language (Das and Petrov, 2011; Zhang et al., 2016; Fang and Cohn, 2016).", "startOffset": 18, "endOffset": 81}, {"referenceID": 5, "context": "resource language (Das and Petrov, 2011; Zhang et al., 2016; Fang and Cohn, 2016).", "startOffset": 18, "endOffset": 81}, {"referenceID": 1, "context": "The first two resources are used as a form of distant supervision through learning crosslingual word embeddings over the monolingual corpora and bilingual dictionary (Ammar et al., 2016).", "startOffset": 166, "endOffset": 186}, {"referenceID": 4, "context": "Parallel data therefore appears to be the most realistic additional source of information for developing NLP systems in low-resource languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2013; Fang and Cohn, 2016; Zhang et al., 2016).", "startOffset": 143, "endOffset": 255}, {"referenceID": 5, "context": "Parallel data therefore appears to be the most realistic additional source of information for developing NLP systems in low-resource languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2013; Fang and Cohn, 2016; Zhang et al., 2016).", "startOffset": 143, "endOffset": 255}, {"referenceID": 8, "context": "Parallel data therefore appears to be the most realistic additional source of information for developing NLP systems in low-resource languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2013; Fang and Cohn, 2016; Zhang et al., 2016).", "startOffset": 143, "endOffset": 255}, {"referenceID": 4, "context": "Parallel data therefore appears to be the most realistic additional source of information for developing NLP systems in low-resource languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2013; Fang and Cohn, 2016; Zhang et al., 2016). Yarowsky and Ngai (2001) pioneered the use of parallel data for projecting POS tag information from one language to another language.", "startOffset": 169, "endOffset": 282}, {"referenceID": 4, "context": "Parallel data therefore appears to be the most realistic additional source of information for developing NLP systems in low-resource languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2013; Fang and Cohn, 2016; Zhang et al., 2016). Yarowsky and Ngai (2001) pioneered the use of parallel data for projecting POS tag information from one language to another language. Das and Petrov (2011) used parallel data and exploited graph-based label propagation to expand the coverage of labelled tokens.", "startOffset": 169, "endOffset": 413}, {"referenceID": 4, "context": "Parallel data therefore appears to be the most realistic additional source of information for developing NLP systems in low-resource languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2013; Fang and Cohn, 2016; Zhang et al., 2016). Yarowsky and Ngai (2001) pioneered the use of parallel data for projecting POS tag information from one language to another language. Das and Petrov (2011) used parallel data and exploited graph-based label propagation to expand the coverage of labelled tokens. T\u00e4ckstr\u00f6m et al. (2013) constructed tag dictionaries by projecting tag information from a highresource language to a low-resource language via alignments in the parallel text.", "startOffset": 169, "endOffset": 543}, {"referenceID": 8, "context": "Zhang et al. (2016) used a few word translations pairs to find a linear transfor-", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Our approach extends the work of Fang and Cohn (2016), who present a model based on distant supervision in the form of cross-lingual projection and use projected tags generated from parallel corpora as distant annotations.", "startOffset": 33, "endOffset": 54}, {"referenceID": 1, "context": "We use the embeddings from Ammar et al. (2016) which trains monolingual word2vec distributional representations, which are then projected into a common space, learned from bilingual dictionaries.", "startOffset": 27, "endOffset": 47}, {"referenceID": 5, "context": "This component allows for a more expressive label mapping than Fang and Cohn (2016)\u2019s linear matrix translation.", "startOffset": 63, "endOffset": 84}, {"referenceID": 6, "context": "FREQTYPE Select the most frequent unannotated word type (Garrette and Baldridge, 2013), in which case all token instances are", "startOffset": 56, "endOffset": 86}, {"referenceID": 2, "context": "X datasets of European languages (Buchholz and Marsi, 2006), comprising Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es) and Swedish (sv).", "startOffset": 33, "endOffset": 59}, {"referenceID": 4, "context": "Turkish data was drawn from CoNLL 20032 and Malagasy data was collected from Das and Petrov (2011), in both cases using the same training configuration as above.", "startOffset": 77, "endOffset": 99}, {"referenceID": 1, "context": "For cross-lingual word embeddings, we evaluate two techniques from Ammar et al. (2016): CCA-based word embeddings and clusterbased word embeddings.", "startOffset": 67, "endOffset": 87}, {"referenceID": 5, "context": "For a more direct comparison, we include BILSTMDEBIAS (Fang and Cohn, 2016), applied using our proposed cross-lingual supervision based on dic-", "startOffset": 54, "endOffset": 75}, {"referenceID": 5, "context": "BILSTM-DEBIAS (Fang and Cohn, 2016)", "startOffset": 14, "endOffset": 35}], "year": 2017, "abstractText": "Cross-lingual model transfer is a compelling and popular method for predicting annotations in a low-resource language, whereby parallel corpora provide a bridge to a high-resource language and its associated annotated corpora. However, parallel data is not readily available for many languages, limiting the applicability of these approaches. We address these drawbacks in our framework which takes advantage of cross-lingual word embeddings trained solely on a high coverage bilingual dictionary. We propose a novel neural network model for joint training from both sources of data based on cross-lingual word embeddings, and show substantial empirical improvements over baseline techniques. We also propose several active learning heuristics, which result in improvements over competitive benchmark methods.", "creator": "LaTeX with hyperref package"}}}