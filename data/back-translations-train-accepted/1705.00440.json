{"id": "1705.00440", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2017", "title": "Data Augmentation for Low-Resource Neural Machine Translation", "abstract": "The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.", "histories": [["v1", "Mon, 1 May 2017 08:12:15 GMT  (174kb,D)", "http://arxiv.org/abs/1705.00440v1", "5 pages, 2 figures, Accepted at ACL 2017"]], "COMMENTS": "5 pages, 2 figures, Accepted at ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marzieh fadaee", "arianna bisazza", "christof monz"], "accepted": true, "id": "1705.00440"}, "pdf": {"name": "1705.00440.pdf", "metadata": {"source": "CRF", "title": "Data Augmentation for Low-Resource Neural Machine Translation", "authors": ["Marzieh Fadaee", "Arianna Bisazza", "Christof Monz"], "emails": ["m.fadaee@uva.nl", "a.bisazza@uva.nl", "c.monz@uva.nl"], "sections": [{"heading": null, "text": "The quality of a Neural Machine Translation system largely depends on the availability of larger parallel corpora, but this is not the case for language pairs with limited resources, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel approach to data expansion that targets low-frequency words by generating new pairs of sentences that contain rare words in new synthetically generated contexts. Experimental results on simulated settings with limited resources show that our method improves translation quality by up to 2.9 BLEU points above the baseline and up to 3.2 BLEU via backtranslation."}, {"heading": "1 Introduction", "text": "In image processing, for example, training data is supplemented by horizontal flipping, random cropping, tilting, and altering the RGB channels of the original image (Krizhevsky et al., 2012; Chatfield et al., 2014). Since the content of the new image is still the same, the label of the original image remains intact (see Figure 1). While data augmentation has become a standard technique for training deep networks for image processing, it is not common practice in training networks for NLP tasks such as machine translation. Neural machine translation (NMT) (Bahdanau et al., 2015; Sutskever et al., 2014; Cho et al., 2014) is a sequence architecture in which an encoder represents the source and anticipates the hidden states."}, {"heading": "2 Translation Data Augmentation", "text": "Considering a source and target sentence pair (S, T), we want to modify it in a way that preserves the semantic equivalence between S and T while diversifying the training examples as much as possible. We can imagine a number of ways to do this, such as paraphrasing (parts of) S or T. However, paraphrasing is a difficult task in itself and there is no guarantee of bringing useful new information into the training data. Instead, we choose to focus on a subset of the vocabulary that we know to be poorly modelled by our baseline NMT system, namely words that rarely occur in the parallel world. Thus, the goal of our data augmentation techniques is to provide novel contexts for rare words in which a common word can be replaced by a rare word and thus its corresponding word in the other language is replaced by that rare word."}, {"heading": "3 Evaluation", "text": "In this section, we evaluate the benefits of our approach in a simulated resource-poor NMT scenario."}, {"heading": "3.1 Data and experimental setup", "text": "In order to simulate a resource-poor setting, we sample 10% of the English\u00d8German WMT15 training data and report on the latest tests in 2014, 2015 and 2016 (Bojar et al., 2016). For reference, we also provide the result of our baseline system on the full data. As an NMT system, we use a 4-layer Attention-based encoder decoder model, as described in (Luong et al., 2015), which was trained with hidden dimensions 1000, batch size 80 for 20 epochs. In all experiments, the NMT vocabulary is limited to the most common 30K words in both languages. Note that data augmentation does not introduce new words into the vocabulary. In all experiments, we process source and target language data with bytepair coding (BPE) (Sennrich et al al al., 2016b) by using 30K merge operations."}, {"heading": "3.2 Results", "text": "All translation results are presented in Table 2. As expected, the low-resource baseline performs much worse than the full data system, underscoring the importance of respectable training data for the NMT. Next, we note that both the back-translation and our proposed TDA method significantly improve translation quality. However, overall, TDA achieves the best results and significantly outperforms the back-translation in all test sentences. This is an important finding, considering that our method involves only minor changes to the original training sentences and does not involve a costly translation process. Improvements are consistent in both translation directions, regardless of whether rare word substitutions are applied first to the source or to the target page. We also note that changing multiple words in a sentence works slightly better than changing just one, which indicates that addressing rarer words is preferable, although the extended sentences will most likely be noisy. To verify that the gains are not due to a single word substitution, it is more likely that the repetitions are the result of a single word."}, {"heading": "4 Analysis of the Results", "text": "A desirable effect of our method is to increase the number of rare words generated by the NMT system in the test phase. To investigate the effects of expanding the training data by creating contexts for rare words on the target page, Table 3 provides an example of the German translation. We see that the base model is not able to generate the rare word centimeters as a correct translation of the German word. However, this word is not rare in the training data of the TDAr\u011b1 model and is generated during the translation. Table 3 also provides several instances of augmented training sentences aiming at the word centimeters. Note that some augmented sentences are nonsensical (e.g. the speed limit of five centimeters per hour), the NMT system benefits from the new context for the rare word and is able to align augmented sentences to the word centimeters."}, {"heading": "5 Conclusion", "text": "We have proposed a simple but effective approach to augmenting Neural Machine Translation training data for language pairs with limited resources. By using language models trained on large amounts of monolingual data, we generate new sentence pairs that contain rare words in new synthetically generated contexts. We show that this approach leads to fewer words being generated during translation, resulting in higher translation quality. In particular, we report significant improvements in simulated English and German settings that exceed another recently proposed data augmentation technology."}, {"heading": "Acknowledgments", "text": "This research was partially funded by the Dutch Organization for Scientific Research (NWO) under project numbers 639,022,213 and 639,021,646, as well as a research award from the Google Faculty. We also thank NVIDIA for their hardware support, Ke Tran for providing the base system for neural machine translation, and the anonymous reviewers for their helpful comments."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Findings of the 2016 conference on machine translation", "author": ["Matt Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri."], "venue": "Proceedings of the First Conference on Ma-", "citeRegEx": "Post et al\\.,? 2016", "shortCiteRegEx": "Post et al\\.", "year": 2016}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["Ken Chatfield", "Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman."], "venue": "Proceedings of the British Machine Vision Conference. BMVA Press.", "citeRegEx": "Chatfield et al\\.,? 2014", "shortCiteRegEx": "Chatfield et al\\.", "year": 2014}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "B van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A neural network model for lowresource universal dependency parsing", "author": ["Long Duong", "Trevor Cohn", "Steven Bird", "Paul Cook."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for", "citeRegEx": "Duong et al\\.,? 2015", "shortCiteRegEx": "Duong et al\\.", "year": 2015}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["Chris Dyer", "Victor Chahuneau", "Noah A. Smith."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human", "citeRegEx": "Dyer et al\\.,? 2013", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning.", "citeRegEx": "Jozefowicz et al\\.,? 2015", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Process-", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Compu-", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Improved statistical machine translation using monolingually-derived paraphrases", "author": ["Yuval Marton", "Chris Callison-Burch", "Philip Resnik."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Marton et al\\.,? 2009", "shortCiteRegEx": "Marton et al\\.", "year": 2009}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Sennrich et al\\.,? 2016a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Sennrich et al\\.,? 2016b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Sys-", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Transfer learning for low-resource neural machine translation", "author": ["Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computa-", "citeRegEx": "Zoph et al\\.,? 2016", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "In image processing the training data is augmented by, for instance, horizontally flipping, random cropping, tilting, and altering the RGB channels of the original images (Krizhevsky et al., 2012; Chatfield et al., 2014).", "startOffset": 171, "endOffset": 220}, {"referenceID": 2, "context": "In image processing the training data is augmented by, for instance, horizontally flipping, random cropping, tilting, and altering the RGB channels of the original images (Krizhevsky et al., 2012; Chatfield et al., 2014).", "startOffset": 171, "endOffset": 220}, {"referenceID": 0, "context": "Neural Machine Translation (NMT) (Bahdanau et al., 2015; Sutskever et al., 2014; Cho et al., 2014) is a sequence-to-sequence architecture where an encoder builds up a representation of the source sentence and a decoder, using the previous A boy is holding a bat.", "startOffset": 33, "endOffset": 98}, {"referenceID": 14, "context": "Neural Machine Translation (NMT) (Bahdanau et al., 2015; Sutskever et al., 2014; Cho et al., 2014) is a sequence-to-sequence architecture where an encoder builds up a representation of the source sentence and a decoder, using the previous A boy is holding a bat.", "startOffset": 33, "endOffset": 98}, {"referenceID": 3, "context": "Neural Machine Translation (NMT) (Bahdanau et al., 2015; Sutskever et al., 2014; Cho et al., 2014) is a sequence-to-sequence architecture where an encoder builds up a representation of the source sentence and a decoder, using the previous A boy is holding a bat.", "startOffset": 33, "endOffset": 98}, {"referenceID": 15, "context": "As a result NMT falls short of reaching state-of-the-art performances for these language pairs (Zoph et al., 2016).", "startOffset": 95, "endOffset": 114}, {"referenceID": 12, "context": "Recently Sennrich et al. (2016a) proposed a method to back-translate sentences from monolingual data and augment the bitext with the resulting pseudo parallel corpora.", "startOffset": 9, "endOffset": 33}, {"referenceID": 10, "context": "We simulate a low-resource setting as done in the literature (Marton et al., 2009; Duong et al., 2015) and obtain substantial improvements for translating English\u00d1German and German\u00d1English.", "startOffset": 61, "endOffset": 102}, {"referenceID": 4, "context": "We simulate a low-resource setting as done in the literature (Marton et al., 2009; Duong et al., 2015) and obtain substantial improvements for translating English\u00d1German and German\u00d1English.", "startOffset": 61, "endOffset": 102}, {"referenceID": 6, "context": "To this end, rather than relying on linguistic resources which are not available for many languages, we rely on LSTM language models (LM) (Hochreiter and Schmidhuber, 1997; Jozefowicz et al., 2015) trained on large amounts of monolingual data in both forward and backward directions.", "startOffset": 138, "endOffset": 197}, {"referenceID": 7, "context": "To this end, rather than relying on linguistic resources which are not available for many languages, we rely on LSTM language models (LM) (Hochreiter and Schmidhuber, 1997; Jozefowicz et al., 2015) trained on large amounts of monolingual data in both forward and backward directions.", "startOffset": 138, "endOffset": 197}, {"referenceID": 5, "context": "We use fast-align (Dyer et al., 2013) to extract word alignments and a bilingual lexicon with lexical translation probabilities from the low-resource bitext.", "startOffset": 18, "endOffset": 37}, {"referenceID": 9, "context": "As NMT system we use a 4-layer attentionbased encoder-decoder model as described in (Luong et al., 2015) trained with hidden dimension 1000, batch size 80 for 20 epochs.", "startOffset": 84, "endOffset": 104}, {"referenceID": 13, "context": "In all experiments we preprocess source and target language data with Bytepair encoding (BPE) (Sennrich et al., 2016b) using 30K merge operations.", "startOffset": 94, "endOffset": 118}, {"referenceID": 12, "context": "We also compare our approach to Sennrich et al. (2016a) by back-translating monolingual data and adding it to the parallel training data.", "startOffset": 32, "endOffset": 56}, {"referenceID": 11, "context": "We measure translation quality by singlereference case-insensitive BLEU (Papineni et al., 2002) computed with the multi-bleu.", "startOffset": 72, "endOffset": 95}, {"referenceID": 12, "context": "Back-translation refers to the work of Sennrich et al. (2016a). Statistically significant improvements are marked IJ at the p \u0103 .", "startOffset": 39, "endOffset": 63}], "year": 2017, "abstractText": "The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.", "creator": "LaTeX with hyperref package"}}}