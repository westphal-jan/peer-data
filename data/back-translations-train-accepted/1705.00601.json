{"id": "1705.00601", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2017", "title": "The Promise of Premise: Harnessing Question Premises in Visual Question Answering", "abstract": "In this paper, we make a simple but important observation -- questions about images often contain premises -- objects and relationships implied by the question -- and that reasoning about premises can help Visual Question Answering (VQA) models respond more intelligently to irrelevant or previously unseen questions.", "histories": [["v1", "Mon, 1 May 2017 17:41:37 GMT  (7761kb,D)", "https://arxiv.org/abs/1705.00601v1", "submitted to EMNLP 2017"], ["v2", "Thu, 17 Aug 2017 18:12:18 GMT  (7970kb,D)", "http://arxiv.org/abs/1705.00601v2", "Published at EMNLP 2017"]], "COMMENTS": "submitted to EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["aroma mahendru", "viraj prabhu", "akrit mohapatra", "dhruv batra", "stefan lee"], "accepted": true, "id": "1705.00601"}, "pdf": {"name": "1705.00601.pdf", "metadata": {"source": "CRF", "title": "The Promise of Premise: Harnessing Question Premises in Visual Question Answering", "authors": ["Aroma Mahendru", "Viraj Prabhu", "Akrit Mohapatra", "Dhruv Batra", "Stefan Lee"], "emails": ["akrit}@vt.edu,", "dbatra@gatech.edu,", "steflee@vt.edu"], "sections": [{"heading": null, "text": "We find that a visual question is irrelevant to an image if at least one of its assumptions is wrong (i.e. not shown in the image). We use this observation to construct a dataset for question relevance predictions and explanations (QRPE) by looking for false assumptions. We train novel models to detect question relevance and show that models that think about assumptions consistently outperform models that don't. We also find that when standard VQA models are forced to think about assumptions during training, it can lead to improvements in tasks that require compositional thinking."}, {"heading": "1 Introduction", "text": "This year, it will be able to reactivate the erroneous orders."}, {"heading": "2 Related Work", "text": "Visual Question Answering: Starting with simple word bags and CNN + LSTM models (Antol et al., 2015), VQA architectures have seen considerable innovation, with many powerful models integrating attention mechanisms (via the image, the question, or both) to focus on important structures (Fukui et al., 2016; Lu et al., 2017), and some designed with compositionality in mind (Andreas et al., 2016; Hendricks et al., 2016). However, improving compositionality or performance through data augmentation remains a largely unexplored area. Some other recent work has developed models that provide explanations in natural language for their results (Park et al., 2016; Wang et al., 2016), but there has been no work to generate explanations for irrelevant questions or false assumptions."}, {"heading": "3 Extracting Premises of a Question", "text": "In fact, the fact is that most of them will be able to feel as if they are able, able to move, and that they will be able to be able to be able to be able, to be able to be able to be able."}, {"heading": "4 Question Relevance Prediction and Explanation (QRPE) Dataset", "text": "In this section, we are curating a new dataset for the relevance of questions in VQA, which we call the Question Relevance Prediction and Explanation (QRPE). We plan to publish QRPE publicly to support future efforts. To train and evaluate models for the detection of irrelevant questions, we want to create a dataset of tuples (I +, Q, P, I \u2212) consisting of a natural language question for which Q is relevant, and an image for which Q is irrelevant because it is wrong. While there is no need to collect a relevant and irrelevant image for each question, we argue that this is an easy way to offset the bias of the image."}, {"heading": "4.1 Dataset Construction", "text": "We automatically curate our QRPE dataset based on existing annotations in COCO > > > Multiple Objects (Lin et al., 2014) and Visual Genome (Krishna et al., 2016). COCO is a set of over 300,000 images annotated with object segmentation and presence information for 80 classes, as well as text descriptions of the image content. Visual Genome builds on this dataset and provides more detailed object, attribute and relationship annotations for over 100,000 COCO images. We use these data sources to extract first and second order premises from VQA questions, which are also based on COCO images."}, {"heading": "4.2 Exploring the Dataset", "text": "Figure 3 shows sample data sets (I +, Q, P, I \u2212) from our data set. These examples illustrate the difficulty of our data set. For example, the images in the second column differ only in the presence of the water bottle and the images in the fourth column differ in the color of the equipment. Both are fine details of the image content. The QRPE data set contains 53,911 (I +, Q, P, I \u2212) tuples generated from as many premises. In total, it contains 1530 unique premises and 28,853 unique questions. Among the 53,911 premises, 3876 are second-class object premises, while the remaining 50,035 are first-order object / scenery premises. We divide our data set into two parts - a training set of 35,486 tuples generated from the VQA training set and a validation set of 18,425 tuples."}, {"heading": "4.3 Comparison to VTFQ", "text": "We contrast our approach with the VTFQ dataset by Ray et al. (2016). As discussed earlier, VTFQ was collected by selecting a random question and an image from the VQA dataset and asking human commentators to report whether the question was relevant and thus producing a pair. This approach leads to irrelevant picture-question pairs that clearly have nothing to do with the visual content of the image. To quantify this effect and compare it with QRPE, we pair each irrelevant picture-question pair (I \u2212, Q) from VTFQ with a relevant image from the VQA dataset. Specifically, we find the next neighbor question Qnn in the VQA dataset to Q based on an average of the word2vec (Mikolov et al., 2013) from the VTFQA dataset a relevant image."}, {"heading": "5 Question Relevance Detection", "text": "In this section we present a simple baseline for irrelevant questions on the QRPE question QRPE question and show that explicit consideration of premises improves both our new model and existing methods. Formally, we consider the binary classification task of predicting whether a question Qi from an image-question pair (Ii, Qi) is relevant to the image Ii.A Simple Premise-Aware Model based on the Deeper LSTM model. Like the standard VQA task, question-relevance recognition also requires a prediction based on a coded image and a question. In this sense, we start with a linear approach based on the Deeper LSTM VQA model of the architecture by Antol et al. (2015) This model encodes image I via a VGNet and question Q with an LSTM over a hot word encoding."}, {"heading": "5.1 Question Relevance Explanation", "text": "In addition to identifying whether a question is irrelevant to an image, we can indicate why it has a significant use for the real world. From the perspective of interpretability, reporting on which premise is wrong is more informative than simply answering the question negatively, as it can help correct the questioner's misunderstanding of the scene. We propose to generate such explanations by identifying the respective question premises that do not apply to an image. By constructing, irrelevant images in the QRPE datasets are selected based on the negation of a single premise - we are now using our datasets to train models to detect false premises, and using the premises that are classified as irrelevant to generate Tempirical explanations in the natural language. Figure 5 illustrates the task for false premise detection. In the face of a question-image pair, say: \"What color is the tie < &; The &; The &; The &; the &; the &; cat is &; the target &; &; the &; the &; the cat in the building at all)."}, {"heading": "6 Premise-Based Visual Question Answering Data Augmentation", "text": "In this section, we will develop a premise for the prevalence of VQs that raises simple, ready-made questions contained in complex, grounded questions."}, {"heading": "7 Conclusions and Future Work", "text": "In this paper, we have made the simple observation that questions about images often contain premises implied by the question, and that thinking about premises can help VQA models respond more intelligently to irrelevant or novel questions. We are developing a system to automatically extract these question premises. Using these premises, we have automatically created a new dataset for question relevance predictions and explanations (QRPE) consisting of 53,911 questions, relevant images, and irrelevant image triplets. In addition, we are training new question relevance prediction models and showing that models that use premise information outperform models that do not. Furthermore, we have shown that questions generated from premises can be an effective data augmentation technique for VQA tasks that require compositional considerations."}, {"heading": "A Compositional VQA Split", "text": "In this section, we will give details of the Compositional VQA Split introduced by Agrawal et al. (2017), on which we perform our data amplification experiments (Section 6).The compositional splits were created by rearranging the training and validation splits of the VQA dataset (Antol et al., 2015).These splits were created so that the question-answer pairs in the Compositional Test Split (e.g. question: \"What color is the plate?,\" answer: \"Green\") are not to be seen in the Compositional Train Split, but the concepts that make up the test QA pairs (e.g. \"Plate,\" \"Green\") in the Compositional Train Split (e.g. question: \"What color is the apple?,\" Answer: \"Green,\" Question: \"How many plates are on the table?,\" Answer: \"4\"), as far as possible."}, {"heading": "B Question Generation", "text": "This year it is more than ever before."}], "references": [{"title": "C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset", "author": ["A. Agrawal", "A. Kembhavi", "D. Batra", "D. Parikh"], "venue": "arXiv preprint arXiv:1704.08243", "citeRegEx": "Agrawal et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2017}, {"title": "Spice: Semantic propositional image caption evaluation", "author": ["Peter Anderson", "Basura Fernando", "Mark Johnson", "Stephen Gould."], "venue": "European Conference on Computer Vision, pages 382\u2013398. Springer.", "citeRegEx": "Anderson et al\\.,? 2016", "shortCiteRegEx": "Anderson et al\\.", "year": 2016}, {"title": "Neural module networks", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 39\u201348.", "citeRegEx": "Andreas et al\\.,? 2016", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Vqa: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh."], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2425\u20132433.", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Automatic annotation of structured facts in images", "author": ["Mohamed Elhoseiny", "Scott Cohen", "Walter Chang", "Brian Price", "Ahmed Elgammal."], "venue": "arXiv preprint arXiv:1604.00466.", "citeRegEx": "Elhoseiny et al\\.,? 2016", "shortCiteRegEx": "Elhoseiny et al\\.", "year": 2016}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach."], "venue": "arXiv preprint arXiv:1606.01847.", "citeRegEx": "Fukui et al\\.,? 2016", "shortCiteRegEx": "Fukui et al\\.", "year": 2016}, {"title": "Deep compositional captioning: Describing novel object categories without paired training data", "author": ["Lisa Anne Hendricks", "Subhashini Venugopalan", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko", "Trevor Darrell."], "venue": "Proceedings of the IEEE", "citeRegEx": "Hendricks et al\\.,? 2016", "shortCiteRegEx": "Hendricks et al\\.", "year": 2016}, {"title": "Image retrieval using scene graphs", "author": ["Justin Johnson", "Ranjay Krishna", "Michael Stark", "Li-Jia Li", "David Shamma", "Michael Bernstein", "Li FeiFei."], "venue": "Conference on Computer Vision and Pattern Recognition, pages 3668\u20133678.", "citeRegEx": "Johnson et al\\.,? 2015", "shortCiteRegEx": "Johnson et al\\.", "year": 2015}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Fei-Fei Li."], "venue": "CVPR.", "citeRegEx": "Karpathy and Li.,? 2015", "shortCiteRegEx": "Karpathy and Li.", "year": 2015}, {"title": "Multimodal residual learning for visual qa", "author": ["Jin-Hwa Kim", "Sang-Woo Lee", "Donghyun Kwak", "MinOh Heo", "Jeonghee Kim", "Jung-Woo Ha", "ByoungTak Zhang."], "venue": "Advances in Neural Information Processing Systems, pages 361\u2013369.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Visual genome: Connecting language and vision", "author": ["Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma", "Michael Bernstein", "Li Fei-Fei"], "venue": null, "citeRegEx": "Krishna et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krishna et al\\.", "year": 2016}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C. Lawrence Zitnick."], "venue": "European Conference on Computer Vision (ECCV).", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Deeper lstm and normalized cnn visual question answering model", "author": ["Jiasen Lu", "Xiao Lin", "Dhruv Batra", "Devi Parikh."], "venue": "https://github.com/ VT-vision-lab/VQA_LSTM_CNN.", "citeRegEx": "Lu et al\\.,? 2015", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning", "author": ["Jiasen Lu", "Caiming Xiong", "Devi Parikh", "Richard Socher."], "venue": "CVPR.", "citeRegEx": "Lu et al\\.,? 2017", "shortCiteRegEx": "Lu et al\\.", "year": 2017}, {"title": "Hierarchical question-image coattention for visual question answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh."], "venue": "Advances In Neural Information Processing Systems, pages 289\u2013297.", "citeRegEx": "Lu et al\\.,? 2016", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "A multiworld approach to question answering about realworld scenes based on uncertain input", "author": ["Mateusz Malinowski", "Mario Fritz."], "venue": "Advances in Neural Information Processing Systems, pages 1682\u20131690.", "citeRegEx": "Malinowski and Fritz.,? 2014", "shortCiteRegEx": "Malinowski and Fritz.", "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["Mateusz Malinowski", "Marcus Rohrbach", "Mario Fritz."], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1\u20139.", "citeRegEx": "Malinowski et al\\.,? 2015", "shortCiteRegEx": "Malinowski et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Attentive explanations: Justifying decisions and pointing to the evidence", "author": ["Rohrbach."], "venue": "arXiv preprint arXiv:1612.04757.", "citeRegEx": "Rohrbach.,? 2016", "shortCiteRegEx": "Rohrbach.", "year": 2016}, {"title": "Question relevance in vqa: Identifying non-visual and false-premise questions", "author": ["Arijit Ray", "Gordon Christie", "Mohit Bansal", "Dhruv Batra", "Devi Parikh."], "venue": "EMNLP.", "citeRegEx": "Ray et al\\.,? 2016", "shortCiteRegEx": "Ray et al\\.", "year": 2016}, {"title": "The ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC2012)", "author": ["Olga Russakovsky", "Jia Deng", "Jonathan Krause", "Alex Berg", "Li Fei-Fei."], "venue": "http://www.imagenet.org/challenges/LSVRC/2012/.", "citeRegEx": "Russakovsky et al\\.,? 2012", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2012}, {"title": "Generating semantically precise scene graphs from textual descriptions for improved image retrieval", "author": ["Sebastian Schuster", "Ranjay Krishna", "Angel Chang", "Li Fei-Fei", "Christopher D Manning."], "venue": "Proceedings of the Fourth Workshop on Vision and", "citeRegEx": "Schuster et al\\.,? 2015", "shortCiteRegEx": "Schuster et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman."], "venue": "CoRR, abs/1409.1556.", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Fvqa: Factbased visual question answering", "author": ["Peng Wang", "Qi Wu", "Chunhua Shen", "Anton van den Hengel", "Anthony Dick."], "venue": "arXiv preprint arXiv:1606.05433.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "What value do explicit high level concepts have in vision to language problems", "author": ["Qi Wu", "Chunhua Shen", "Lingqiao Liu", "Anthony Dick", "Anton van den Hengel"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Scene graph generation by iterative message passing", "author": ["Danfei Xu", "Yuke Zhu", "Christopher Choy", "Li FeiFei."], "venue": "Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Xu et al\\.,? 2017", "shortCiteRegEx": "Xu et al\\.", "year": 2017}, {"title": "Measuring machine intelligence through visual question answering", "author": ["C Lawrence Zitnick", "Aishwarya Agrawal", "Stanislaw Antol", "Margaret Mitchell", "Dhruv Batra", "Devi Parikh."], "venue": "arXiv preprint arXiv:1608.08716.", "citeRegEx": "Zitnick et al\\.,? 2016", "shortCiteRegEx": "Zitnick et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "Despite significant progress on VQA benchmarks (Antol et al., 2015), current models still present a number of unintelligent and problematic tendencies.", "startOffset": 47, "endOffset": 67}, {"referenceID": 19, "context": "Like Ray et al. (2016), we argue that practical VQA systems must be able to identify and explain irrelevant questions.", "startOffset": 5, "endOffset": 23}, {"referenceID": 1, "context": "We develop a premise extraction pipeline based on SPICE (Anderson et al., 2016) and demonstrate how these premises can be used to improve modern VQA models in the face of irrelevant or previously unseen questions.", "startOffset": 56, "endOffset": 79}, {"referenceID": 3, "context": "We collect the QRPE dataset by taking each image-question pair in the VQA dataset (Antol et al., 2015) and finding the most visually similar other image for which exactly one of the question premises is false.", "startOffset": 82, "endOffset": 102}, {"referenceID": 19, "context": "For context, the only other existing irrelevant question detection dataset (Ray et al., 2016) collected irrelevant question-image pairs by human verification of random pairs.", "startOffset": 75, "endOffset": 93}, {"referenceID": 3, "context": "Visual Question Answering: Starting from simple bag-of-word and CNN+LSTM models (Antol et al., 2015), VQA architectures have seen considerable innovation.", "startOffset": 80, "endOffset": 100}, {"referenceID": 5, "context": "Many top-performing models integrate attention mechanisms (over the image, the question, or both) to focus on important structures (Fukui et al., 2016; Lu et al., 2016, 2017), and some have been designed with compositionality in mind (Andreas et al.", "startOffset": 131, "endOffset": 174}, {"referenceID": 2, "context": ", 2016, 2017), and some have been designed with compositionality in mind (Andreas et al., 2016; Hendricks et al., 2016).", "startOffset": 73, "endOffset": 119}, {"referenceID": 6, "context": ", 2016, 2017), and some have been designed with compositionality in mind (Andreas et al., 2016; Hendricks et al., 2016).", "startOffset": 73, "endOffset": 119}, {"referenceID": 23, "context": "Some other recent work has developed models which produce natural language explanations for their outputs (Park et al., 2016; Wang et al., 2016), but there has not been work on generating explanations for irrelevant questions or false premises.", "startOffset": 106, "endOffset": 144}, {"referenceID": 19, "context": "Question Relevance: Most related to our work is that of Ray et al. (2016), which introduced the task of irrelevant question detection for VQA.", "startOffset": 56, "endOffset": 74}, {"referenceID": 25, "context": "2016); however, recent work has begun extending these techniques to visual domains (Xu et al., 2017; Johnson et al., 2015).", "startOffset": 83, "endOffset": 122}, {"referenceID": 7, "context": "2016); however, recent work has begun extending these techniques to visual domains (Xu et al., 2017; Johnson et al., 2015).", "startOffset": 83, "endOffset": 122}, {"referenceID": 10, "context": "Additionally, the Visual Genome (Krishna et al., 2016) dataset contains dense image annotations for objects and their", "startOffset": 32, "endOffset": 54}, {"referenceID": 1, "context": "Premise Extraction: To extract premises from questions, we use the semantic tuple extraction pipeline used in the SPICE metric (Anderson et al., 2016).", "startOffset": 127, "endOffset": 150}, {"referenceID": 21, "context": "image captioning, SPICE transforms a sentence into a scene graph using the Stanford Scene Graph Parser (Schuster et al., 2015) and then extracts semantic tuples from this representation.", "startOffset": 103, "endOffset": 126}, {"referenceID": 3, "context": "We base our dataset on the existing VQA corpus (Antol et al., 2015), taking the human-generated (and therefore relevant) image-question pairs from VQA as I+ and Q.", "startOffset": 47, "endOffset": 67}, {"referenceID": 22, "context": "Furthermore, we sort this subset of irrelevant image by their visual distance to the source image I+ based on image encodings from a VGGNet (Simonyan and Zisserman, 2014) pretrained on ImageNet (Russakovsky et al.", "startOffset": 140, "endOffset": 170}, {"referenceID": 20, "context": "Furthermore, we sort this subset of irrelevant image by their visual distance to the source image I+ based on image encodings from a VGGNet (Simonyan and Zisserman, 2014) pretrained on ImageNet (Russakovsky et al., 2012).", "startOffset": 194, "endOffset": 220}, {"referenceID": 11, "context": "We curate our QRPE dataset automatically from existing annotations in COCO (Lin et al., 2014) and Visual Genome (Krishna et al.", "startOffset": 75, "endOffset": 93}, {"referenceID": 10, "context": ", 2014) and Visual Genome (Krishna et al., 2016).", "startOffset": 26, "endOffset": 48}, {"referenceID": 11, "context": "existential premises), we consider only the 80 classes present in COCO (Lin et al., 2014).", "startOffset": 71, "endOffset": 89}, {"referenceID": 10, "context": "jects), we rely on Visual Genome (Krishna et al., 2016) annotations for object and attribute labels.", "startOffset": 33, "endOffset": 55}, {"referenceID": 19, "context": "We contrast our approach to the VTFQ dataset of Ray et al. (2016). As discussed prior, VTFQ was collected by selecting a random question and image from the VQA set and asking human annotators to report if the question was relevant, producing a pair.", "startOffset": 48, "endOffset": 66}, {"referenceID": 17, "context": "Specifically, we find the nearest neighbor question Qnn in the VQA dataset to Q based on an average of the word2vec (Mikolov et al., 2013) embedding of each word, and select the image on which Qnn was asked as I+ to form (I+, Q, P, I\u2212) tuples like in our proposed dataset.", "startOffset": 116, "endOffset": 138}, {"referenceID": 3, "context": "With this in mind, we begin with a straight-forward approach based on the Deeper LSTM VQA model architecture of Antol et al. (2015). This model encodes the image I via a VGGNet and the question Q with an LSTM over one-hot word encodings.", "startOffset": 112, "endOffset": 132}, {"referenceID": 12, "context": "We also extend the attention based Hierarchical Co-Attention VQA model of Lu et al. (2016) for the task of question rele-", "startOffset": 74, "endOffset": 91}, {"referenceID": 19, "context": "We compare our approaches with the best performing model of Ray et al. (2016). This model (which we denote QC-Sim) uses a pretrained captioning model to automatically provide natural language image descriptions and reasons about relevance based on a learned similarity between the question and image caption.", "startOffset": 60, "endOffset": 78}, {"referenceID": 8, "context": "Specifically, the approach uses NeuralTalk2 (Karpathy and Li, 2015) trained on the MS COCO dataset (Lin et al.", "startOffset": 44, "endOffset": 67}, {"referenceID": 11, "context": "Specifically, the approach uses NeuralTalk2 (Karpathy and Li, 2015) trained on the MS COCO dataset (Lin et al., 2014) to generate a caption for each image.", "startOffset": 99, "endOffset": 117}, {"referenceID": 17, "context": "Both the caption and question are embedded as a fixed length vector through an encoding LSTM (with words being represented as word2vec (Mikolov et al., 2013) vectors).", "startOffset": 135, "endOffset": 157}, {"referenceID": 22, "context": "tron that takes one-hot encodings of premises and VGGNet (Simonyan and Zisserman, 2014) image features as input to predict whether the premise is grounded in the image or not.", "startOffset": 57, "endOffset": 87}, {"referenceID": 0, "context": "We evaluate multiple models with and without premise augmentation on two splits of the VQA dataset - the standard split and the compositional split of Agrawal et al. (2017). The compositional split is specifically designed to test a model\u2019s ability to generalize to unseen/rarely seen combinations of concepts at test time.", "startOffset": 151, "endOffset": 173}, {"referenceID": 12, "context": "We evaluate the Deeper LSTM model of Lu et al. (2015) on the standard and compositional splits with two augmentation strategies - All which includes the entire set of premise questions and Top-1k-A which includes only questions with answers in the top 1000 most common VQA answers.", "startOffset": 37, "endOffset": 54}, {"referenceID": 3, "context": "Table 3: Accuracy on the standard and compositional VQA validation sets for different augmentation strategies for DeeperLSTM(Antol et al., 2015).", "startOffset": 124, "endOffset": 144}, {"referenceID": 12, "context": "DeeperLSTM(Lu et al., 2015) 46.", "startOffset": 10, "endOffset": 27}, {"referenceID": 14, "context": "HieCoAtt(Lu et al., 2016) 50.", "startOffset": 8, "endOffset": 25}, {"referenceID": 2, "context": "NMN(Andreas et al., 2016) 49.", "startOffset": 3, "endOffset": 25}, {"referenceID": 5, "context": "MCB(Fukui et al., 2016) 50.", "startOffset": 3, "endOffset": 23}], "year": 2017, "abstractText": "In this paper, we make a simple observation that questions about images often contain premises \u2013 objects and relationships implied by the question \u2013 and that reasoning about premises can help Visual Question Answering (VQA) models respond more intelligently to irrelevant or previously unseen questions. When presented with a question that is irrelevant to an image, state-of-the-art VQA models will still answer purely based on learned language biases, resulting in nonsensical or even misleading answers. We note that a visual question is irrelevant to an image if at least one of its premises is false (i.e. not depicted in the image). We leverage this observation to construct a dataset for Question Relevance Prediction and Explanation (QRPE) by searching for false premises. We train novel question relevance detection models and show that models that reason about premises consistently outperform models that do not. We also find that forcing standard VQA models to reason about premises during training can lead to improvements on tasks requiring compositional reasoning.", "creator": "LaTeX with hyperref package"}}}