{"id": "1705.00678", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2017", "title": "Twin Learning for Similarity and Clustering: A Unified Kernel Approach", "abstract": "Many similarity-based clustering methods work in two separate steps including similarity matrix computation and subsequent spectral clustering. However, similarity measurement is challenging because it is usually impacted by many factors, e.g., the choice of similarity metric, neighborhood size, scale of data, noise and outliers. Thus the learned similarity matrix is often not suitable, let alone optimal, for the subsequent clustering. In addition, nonlinear similarity often exists in many real world data which, however, has not been effectively considered by most existing methods. To tackle these two challenges, we propose a model to simultaneously learn cluster indicator matrix and similarity information in kernel spaces in a principled way. We show theoretical relationships to kernel k-means, k-means, and spectral clustering methods. Then, to address the practical issue of how to select the most suitable kernel for a particular clustering task, we further extend our model with a multiple kernel learning ability. With this joint model, we can automatically accomplish three subtasks of finding the best cluster indicator matrix, the most accurate similarity relations and the optimal combination of multiple kernels. By leveraging the interactions between these three subtasks in a joint framework, each subtask can be iteratively boosted by using the results of the others towards an overall optimal solution. Extensive experiments are performed to demonstrate the effectiveness of our method.", "histories": [["v1", "Mon, 1 May 2017 19:33:27 GMT  (39kb,D)", "http://arxiv.org/abs/1705.00678v1", "Published in AAAI 2017"], ["v2", "Wed, 3 May 2017 00:29:13 GMT  (39kb,D)", "http://arxiv.org/abs/1705.00678v2", "Published in AAAI 2017"]], "COMMENTS": "Published in AAAI 2017", "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["zhao kang", "chong peng", "qiang cheng"], "accepted": true, "id": "1705.00678"}, "pdf": {"name": "1705.00678.pdf", "metadata": {"source": "CRF", "title": "Twin Learning for Similarity and Clustering: A Unified Kernel Approach", "authors": ["Zhao Kang", "Chong Peng", "Qiang Cheng"], "emails": ["qcheng}@siu.edu"], "sections": [{"heading": "Introduction", "text": "Clustering is a fundamental topic in data mining and machine learning (Peng et al. 2016). It divides the data into different groups so that the objects within a group are similar to each other and different from those in other groups. Various methods have been proposed in recent decades. Some well-known algorithms include k-mean clustering (MacQueen 1967), spectral clustering (Ng et al. 2002) and hierarchical clustering (Johnson 1967).Thanks to its simplicity and effectiveness, the Kmeans algorithm is widely used. However, it does not succeed in identifying arbitrarily shaped clusters. Kernel k means (Scholkopf, Smola and Mu \ufffd ller 1998) are designed to capture non-linear structural information hidden in data sets. Kernel-based learning methods are required to specify a kernel, which means that one assumes a certain form of larval formation."}, {"heading": "Contributions", "text": "Unlike existing clustering algorithms, which operate in two separate steps, we simultaneously learn similarity matrix and cluster indicators by imposing a ranking constraint on the laplac matrix of the learned similarity matrix. By using the intrinsic interactions between learning similarity and cluster indicators, we seamlessly integrate them into a common framework in which the result of one task is used to improve the other. To capture the nonlinear structural information inherent in many elements of the real world, we develop our method directly in a core space known for its ability to explore nonlinear relationships. We design an efficient algorithm to find an optimal solution for our model and demonstrate the theoretical analyses of the means of kernel teaching - 170."}, {"heading": "Clustering with Single Kernel", "text": "According to the self-expressed property (Elhamifar and Vidal 2009), Xi \u2248 \u2211 j Xjzij, s.t. Z i 1 = 1, 0 \u2264 zij \u2264 1, (1), where Zji is the weight for j-th sample. Similar data points should receive larger weights and the weights should be smaller for less similar points. Thus, Z is also called the similarity matrix, which represents the global structure of the data. Note: (1) is in a similar spirit of Locally Linear Embedding (LLE) (Roweis and Saul 2000), which assumes that the data points are on a variety and each data point can be expressed as a linear combination of its closest neighbors. The difference to LLE lies in the fact that we do not specify a neighborhood that is automatically determined by our method. To obtain Z, we solve the following problem: min Z - X \u2212 XZ 2F +."}, {"heading": "Optimization Algorithm", "text": "We use an alternating optimization strategy for (7). When Z is fixed, (7) becomes min PTP = ITr (PTLP). (8) The optimal solution P is obtained by the c eigenvectors of L corresponding to the c smallest eigenvalues. If P is fixed, (7) can be reformulated in columns as follows: min ZiKii \u2212 2Ki,: Zi + ZTi KZi + \u03b1ZTi Zi + \u03b22 dTi Zi, s.t. ZTi 1 = 1, 0 \u2264 zij \u2264 1, (9) where di-ZiKii \u2212 2Ki is a vector, where the j element is dij = \u0438 Pi,: \u2212 Pj. (9) To get the important equation in the spectral analysis, (9) the important equation in i, j1 2 \u0438 Pi,: \u2212 Pj \u2212 2zij = Tr (PTLP) (10) can be used."}, {"heading": "Theoretical Analysis of SCSK Model", "text": "In this section we present a theoretical analysis of SCSK and its links to k-averages, k-averages and SC.algorithm 1. The algorithm of SCSK input: kernel matrixK, parameters \u03b1 > 0, \u03b2 > 0."}, {"heading": "REPEAT", "text": "1: Update P, which is composed of the c eigenvectors of L = D \u2212 ZT + Z2, which correspond to the c smallest eigenvalues. 2: For each i, update the i-th column of Z by solving problem (11)."}, {"heading": "Connection to Kernel K-means and K-means", "text": "Here we present a theorem stating the equivalence of SCSK and kernel k means, k means and k means under certain conditions. Theorem 2. If \u03b1 \u2192 \u221e, the problem (4) is equivalent to kernel k means problematic. Proof. The constraint rank (L) = n \u2212 c in (4) makes the solution Z block diagonal. Let Zi-Rni \u00b7 ni denote the similarity matrix of the i-th component of Z, where ni is the number of data samples in the component. Problem (4) is equivalent to solving the following problem for each i: min Zi block (Xi) \u2212 \u03c6 (Xi) Zi-2F s.t. (Zi) T1 = 1, 0 \u2264 Zi \u2264 1, (12) where Xi consists of the samples corresponding to the i-th component of Z."}, {"heading": "Connection to Spectral Clustering", "text": "With a predefined similarity Z, spectral clustering should solve the following problem: min PTP = ITr (PTLP). (16) The optimal solution P is achieved by the c eigenvectors of L, which correspond to the c smallest eigenvalues. In general, P cannot be used directly for clustering, since Z does not have exactly c related components. In order to obtain the final cluster results, k means or other discretion procedures must be performed on P (Huang, Nie and Huang 2013). In our proposed algorithm, the similarity matrix Z is not predefined like the existing spectral cluster methods in the literature. Similarity matrix Z is also learned taking into account the present cluster task, in contrast to the existing subspace cluster methods in the literature, which focus only on learning the similarity matrix Z without the effect of clustering on Z (Peng et al. 2015)."}, {"heading": "Clustering with Multiple Kernels", "text": "Although the model (7) can automatically learn the similarity matrix and cluster indicator matrix, its performance is largely determined by the choice of kernel. It is often impractical to search extensively for the most suitable kernel. Furthermore, real data sets are often generated from different sources along with heterogeneous properties, and the method of a single kernel may not be able to make full use of such information. Suppose there is a total number of different kernel functions {Ki} ri = 1. Accordingly, there would be r different kernel spaces known as {Hi} ri = 1. An extended Hilbert space (H = r i = 1Hi, can be obtained by concatenating all kernel spaces and by using the mapping of Kims (Kims) as {Hi} ri = 1."}, {"heading": "Optimization", "text": "Problem (18) can be solved by alternatively updating Z, P and w while keeping the other variables constant. (1) Optimization with respect to Z and P when w is fixed: We can directly calculate Kw, and the optimization problem is exactly (7). Therefore, we only need to use algorithm 1 with Kw as the input kernel matrix. (2) Optimization with respect to w when Z and P are fixed: Solution (18) with respect to w can be rewritten as (Cai et al. 2013). (19) The Lagrange function of (19) isJ (w) = wTh + g: 1 wihi s.t. r: i = 1 \u221a wi = 1, wi \u2265 0, (19) where hi = Tr (Ki \u2212 2KiZ + ZTKiZ). (19) The Lagrange function of (w) = wTh + g: i (1 \u2212 r: i) (1 \u2212 r: 1. = 1."}, {"heading": "Experiments", "text": "In this section, we will show the effectiveness of the proposed method on real benchmark datasets."}, {"heading": "Data Sets", "text": "In total, there are eight benchmark datasets used in our experiments. Table 1 summarizes the statistics of these datasets, including five image datasets and the other three text datasets. The five image datasets consist of four commonly used facial databases (ORL2, YALE3, AR4 (Martinez and Benavente 2007) and JAFFE5) and one binary alpha dataset BA6."}, {"heading": "Experiment Setup", "text": "To evaluate the effectiveness of multiple kernel learning, we design 12 kernels, among which: seven Gaussian kernels of the form K (x, y) = exp (\u2212 x \u2212 y, 22 / (td2max)), where dmax varies the maximum distance between samples and t, using the sentence {0.01, 0.05, 0.1, 1, 10, 50, 100}; one linear kernel K (x, y) = (a + xTy) b with a = {0, 1} and b = {2, 4}; one linear kernel K (x, y) = xTy; four polynomial kernels K (x, y) = (a + xTy) b with a = {0, 1}."}, {"heading": "Clustering Result", "text": "Table 2 shows the cluster results in terms of accuracy, NMI and purity of all data sets. The big difference between the best and average results confirms the fact that the choice of the kernel has a huge influence on the performance of individual kernel methods. This difference motivates the development of several kernel learning methods. Furthermore, several kernel cluster approaches usually improve the results over individual kernel cluster methods. 8http: / / imp.iis.sinica.edu.tw / IVCLab / research / Sean / mkfc / code 9http: / / imp.iis.edu.tw / IVCLab / research / Sean / aasc / code10https: / / github.com / csliangdu / RMKM KM"}, {"heading": "Parameter Selection", "text": "In our models there are two parameters \u03b1 and \u03b2: We vary \u03b1 in the range of {1e-5, 1e-4, 1e-3, 0.01, 0.1, 1, 10, 100} and \u03b2 in the range of {1e-6, 1e-5}. Figure 1 shows how the cluster results of the SCMK differ with \u03b1 and \u03b2 in terms of Acc, NMI and purity in the JAFFE and YALE datasets. We can observe that the performance of the SCMK is very stable in relation to a wide range of \u03b1 values and responds more sensitively to the value of \u03b2."}, {"heading": "Conclusion", "text": "In this paper, we first propose a cluster method to simultaneously perform similarity learning and cluster indicator matrix construction. In our method, similarity learning and cluster indicator learning are integrated within a framework; the method can easily be extended to core spaces to capture nonlinear structural information; the proposed method's links to kernel-k-means, -kmeans, and -spectral clusters are also established; and, to avoid a comprehensive search for the best kernel, we incorporate several kernel learning processes into our model. Similarity learning, cluster indicator construction, and core weight learning can be improved by using the results of the other two. Extensive experiments have been conducted on real benchmark datasets to demonstrate the superior performance of our method."}, {"heading": "Acknowledgements", "text": "This work is supported by the US National Science Foundation Grants IIS 1218712. Q. Cheng is the corresponding author."}], "references": [{"title": "Locality preserving nonnegative matrix factorization", "author": ["Cai"], "venue": "In IJCAI,", "citeRegEx": "Cai,? \\Q2009\\E", "shortCiteRegEx": "Cai", "year": 2009}, {"title": "Heterogeneous image features integration via multimodal semi-supervised learning model", "author": ["Cai"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Cai,? \\Q2013\\E", "shortCiteRegEx": "Cai", "year": 2013}, {"title": "Robust multiple kernel k-means using 2; 1-norm", "author": ["Du"], "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,", "citeRegEx": "Du,? \\Q2015\\E", "shortCiteRegEx": "Du", "year": 2015}, {"title": "R", "author": ["E. Elhamifar", "Vidal"], "venue": "2009. Sparse subspace clustering. In Computer Vision and Pattern Recognition, 2009. CVPR", "citeRegEx": "Elhamifar and Vidal 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "On a theorem of weyl concerning eigenvalues of linear transformations i", "author": ["K. Fan"], "venue": "Proceedings of the National Academy of Sciences of the United States of America", "citeRegEx": "Fan,? \\Q1949\\E", "shortCiteRegEx": "Fan", "year": 1949}, {"title": "Affinity aggregation for spectral clustering", "author": ["Chuang Huang", "H.-C. Chen 2012a] Huang", "Y.-Y. Chuang", "C.-S. Chen"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Multiple kernel fuzzy clustering", "author": ["Chuang Huang", "H.-C. Chen 2012b] Huang", "Y.-Y. Chuang", "C.-S. Chen"], "venue": "IEEE Transactions on Fuzzy Systems", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Spectral rotation versus k-means in spectral clustering", "author": ["Nie Huang", "J. Huang 2013] Huang", "F. Nie", "H. Huang"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "A new simplex sparse learning model to measure data similarity for clustering", "author": ["Nie Huang", "J. Huang 2015] Huang", "F. Nie", "H. Huang"], "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "S", "author": ["Johnson"], "venue": "C.", "citeRegEx": "Johnson 1967", "shortCiteRegEx": null, "year": 1967}, {"title": "and Cheng", "author": ["Z. Kang"], "venue": "Q.", "citeRegEx": "Kang and Cheng 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Robust subspace clustering via smoothed rank approximation", "author": ["Peng Kang", "Z. Cheng 2015a] Kang", "C. Peng", "Q. Cheng"], "venue": "IEEE Signal Processing Letters", "citeRegEx": "Kang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2015}, {"title": "Robust subspace clustering via tighter rank approximation", "author": ["Peng Kang", "Z. Cheng 2015b] Kang", "C. Peng", "Q. Cheng"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,", "citeRegEx": "Kang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2015}, {"title": "Multi-subspace representation and discovery", "author": ["Luo"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Luo,? \\Q2011\\E", "shortCiteRegEx": "Luo", "year": 2011}, {"title": "R", "author": ["A. Martinez", "Benavente"], "venue": "2007. The ar face database,", "citeRegEx": "Martinez and Benavente 2007", "shortCiteRegEx": null, "year": 1998}, {"title": "The laplacian spectrum of graphs. Graph theory, combinatorics, and applications 2(871-898):12", "author": ["Mohar"], "venue": null, "citeRegEx": "Mohar,? \\Q1991\\E", "shortCiteRegEx": "Mohar", "year": 1991}, {"title": "M", "author": ["Ng, A.Y.", "Jordan"], "venue": "I.; Weiss, Y.; et al.", "citeRegEx": "Ng et al. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "M", "author": ["F. Nie", "X. Wang", "Jordan"], "venue": "I.; and Huang, H.", "citeRegEx": "Nie et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Clustering and projected clustering with adaptive neighbors", "author": ["Wang Nie", "F. Huang 2014] Nie", "X. Wang", "H. Huang"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Nie et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nie et al\\.", "year": 2014}, {"title": "Subspace clustering using log-determinant rank approximation", "author": ["Peng"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Peng,? \\Q2015\\E", "shortCiteRegEx": "Peng", "year": 2015}, {"title": "Nonnegative matrix factorization with integrated graph and feature learning", "author": ["Peng"], "venue": "ACM Transactions on Intelligent Systems and Technology", "citeRegEx": "Peng,? \\Q2016\\E", "shortCiteRegEx": "Peng", "year": 2016}, {"title": "L", "author": ["S.T. Roweis", "Saul"], "venue": "K.", "citeRegEx": "Roweis and Saul 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem. Neural computation 10(5):1299\u20131319", "author": ["Smola Sch\u00f6lkopf", "B. M\u00fcller 1998] Sch\u00f6lkopf", "A. Smola", "K.-R. M\u00fcller"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1998}, {"title": "Discriminative unsupervised dimensionality reduction", "author": ["Wang"], "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,", "citeRegEx": "Wang,? \\Q2015\\E", "shortCiteRegEx": "Wang", "year": 2015}, {"title": "Image clustering using local discriminant models and global integration", "author": ["Yang"], "venue": "IEEE Transactions on Image Processing", "citeRegEx": "Yang,? \\Q2010\\E", "shortCiteRegEx": "Yang", "year": 2010}, {"title": "J", "author": ["S. Yu", "L. Tranchevent", "X. Liu", "W. Glanzel", "Suykens"], "venue": "A.; De Moor, B.; and Moreau, Y.", "citeRegEx": "Yu et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Cheung", "author": ["H. Zeng"], "venue": "Y.-m.", "citeRegEx": "Zeng and Cheung 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "A general kernelization framework for learning algorithms based on kernel", "author": ["Nie Zhang", "C. Xiang 2010] Zhang", "F. Nie", "S. Xiang"], "venue": "pca. Neurocomputing", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}], "referenceMentions": [], "year": 2017, "abstractText": "Many similarity-based clustering methods work in two separate steps including similarity matrix computation and subsequent spectral clustering. However, similarity measurement is challenging because it is usually impacted by many factors, e.g., the choice of similarity metric, neighborhood size, scale of data, noise and outliers. Thus the learned similarity matrix is often not suitable, let alone optimal, for the subsequent clustering. In addition, nonlinear similarity often exists in many real world data which, however, has not been effectively considered by most existing methods. To tackle these two challenges, we propose a model to simultaneously learn cluster indicator matrix and similarity information in kernel spaces in a principled way. We show theoretical relationships to kernel k-means, k-means, and spectral clustering methods. Then, to address the practical issue of how to select the most suitable kernel for a particular clustering task, we further extend our model with a multiple kernel learning ability. With this joint model, we can automatically accomplish three subtasks of finding the best cluster indicator matrix, the most accurate similarity relations and the optimal combination of multiple kernels. By leveraging the interactions between these three subtasks in a joint framework, each subtask can be iteratively boosted by using the results of the others towards an overall optimal solution. Extensive experiments are performed to demonstrate the effectiveness of our method. Introduction Clustering is a fundamental topic in data mining and machine learning (Peng et al. 2016). It partitions data points into different groups, such that the objects within a group are similar to one another and different from those in other groups. Various methods have been proposed over the past decades. Some well-known algorithms include k-means clustering (MacQueen 1967), spectral clustering (Ng et al. 2002), and hierarchical clustering (Johnson 1967). Thanks to the simplicity and the effectiveness, the kmeans algorithm is widely used. However, it fails to identify arbitrarily shaped clusters. Kernel k-means (Sch\u00f6lkopf, Smola, and M\u00fcller 1998) has been developed to capture nonlinear structure information hidden in data sets. Kernelbased learning methods requires one to specify a kernel, which means one assumes a certain shape of the underlying Copyright c \u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. data space. Thus the performance of kernel-based methods are largely affected by the choice of kernel. Spectral clustering does a low-dimensional embedding of the similarity matrix of the data before performing k-means clustering (Ng et al. 2002). The similarity between every pair of points, as an input, leverages the manifold information in this clustering model. Thus similarity-based clustering methods usually show better performance than k-means algorithm. However, the performance of this kind of methods is largely determined by the similarity matrix (Huang, Nie, and Huang 2015). Any variations during the similarity measurement, such as metric, neighborhood size, and data scale, may lead to suboptimal performance. Recently, self-expression has been successfully utilized in subspace recovery (Elhamifar and Vidal 2009; Luo et al. 2011), low rank representation (Kang, Peng, and Cheng 2015b; Kang, Peng, and Cheng 2015a), and recommender systems (Kang and Cheng 2016). It represents each data point in terms of the other points. By solving an optimization problem, the similarity information is automatically learned from the data. This approach can not only reveal low-dimensional structure, but also be robust to noise and data scale (Huang, Nie, and Huang 2015).", "creator": "LaTeX with hyperref package"}}}