{"id": "1705.00753", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2017", "title": "A Teacher-Student Framework for Zero-Resource Neural Machine Translation", "abstract": "While end-to-end neural machine translation (NMT) has made remarkable progress recently, it still suffers from the data scarcity problem for low-resource language pairs and domains. In this paper, we propose a method for zero-resource NMT by assuming that parallel sentences have close probabilities of generating a sentence in a third language. Based on this assumption, our method is able to train a source-to-target NMT model (\"student\") without parallel corpora available, guided by an existing pivot-to-target NMT model (\"teacher\") on a source-pivot parallel corpus. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs.", "histories": [["v1", "Tue, 2 May 2017 01:14:06 GMT  (161kb,D)", "http://arxiv.org/abs/1705.00753v1", "Accepted as a long paper by ACL 2017"]], "COMMENTS": "Accepted as a long paper by ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yun chen", "yang liu", "yong cheng", "victor o k li"], "accepted": true, "id": "1705.00753"}, "pdf": {"name": "1705.00753.pdf", "metadata": {"source": "CRF", "title": "A Teacher-Student Framework for Zero-Resource Neural Machine Translation", "authors": ["Yun Chen", "Yang Liu", "Yong Cheng", "Victor O.K. Li"], "emails": ["yun.chencreek@gmail.com;", "liuyang2011@tsinghua.edu.cn;", "chengyong3001@gmail.com;", "vli@eee.hku.hk"], "sections": [{"heading": "1 Introduction", "text": "Although NMT has achieved high translation performance in resource-rich language pairs such as English-French and German-English (Luong et al., 2015; Jean et al., 2015; Wu et al., 2016; Johnson et al., 2016), it still suffers from the unavailability of large-scale parallel corporations for translating low-resource languages. Due to the large parameter space, neural models generally learn poorly from low-resource events, resulting in poor choice for low-resource language pairs. Zoph et al. (2016) shows that NMT achieves high translation quality."}, {"heading": "2 Background", "text": "This is why it has come to this. (...) It is not that we are able to translate the results. (...) It is not that we are able to translate them. (...) It is not that we are able to translate them. (...) It is that we do not translate them. (...) It is that we do not translate them. (...) It is that we do not translate them. (...) It is that we translate them. (...) It is that we translate them. (...) It is that we translate them. (...) It is that we translate them. (...) It is that we translate them. (...) It is that we translate them. (...) It is that we translate them. (...) It is that we translate them. (...) It is that we translate them. (...) It is that we translate them. (...) It is that we translate them. (...) It is that we translate them. (...) It is that we translate them. (...) It is that we translate them. (... It is that we translate them. (...) It is that we translate them. (...) It is that we translate them. (... It is that we translate them."}, {"heading": "3 Approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Assumptions", "text": "In this work, we propose to model the intended neural translation from source to target directly, based on a teacher-student model. The basic idea is to use a pre-trained pivot-to-target model (\"teacher\") to guide the learning process of a source-to-target model (\"student\") without training data being available on a source pivot-based parallel corpus. An advantage of our approach is that equation (1) can be used as a decision rule for decrypting the error propagation problem that occurs through two-step decryption in pivot-based approaches. As shown in Figure 1 (b), we still assume that a source pivot parallel corpus (Dx) and a pivot target parallel corpus (x) x x x x-target parallel corpus (Dz) and a pivot target parallel corpus (x) x x x) are available, as opposed to the vot modal x x x-based x x x-model x x x x x-target corpus x, x x x x x x x-base x x x x x x x-base x x x x x x x-model x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3.2 Sentence-Level Teaching", "text": "In view of a source-pivoting parallel corpus Dx, z, our training objective is defined on the basis of assumption 1 as follows: JSENT (\u03b8x \u2192 y) = \u2211 < x, z > Dx, z KL (P (y | z; 2001), (5), summing up the KL divergence across all possible targets: KL (P (y | z; 2001). (6) Since the teacher model parameters are fixed, the training objective can be written as equivalent to JSENT (201x \u2192 y)."}, {"heading": "3.3 Word-Level Teaching", "text": "Instead of minimizing the KL divergence between teacher and learning models at the sentence level, we define a training goal at the word level based on assumption 2: JWORD (Java) = Java < x, z > Java (Java). (10) Equation (9) suggests that the teacher model P (y | z, y < j; j; vokaly; vokaly; vokaly; vokaly; vokaly; vokaly; vokaly; z; vokaly; vokaly; vokaly; z; x) \"the student model P (y, y < j; & ltx; p; p; x) at the education level P (z)."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Setup", "text": "We evaluate our approach at Europarl (Koehn, 2005) and WMT Corpora (WMT Corpora). To compare with the pivotbased methods, we use the same dataset as (Cheng et al., 2016a). All sentences are symbolized by the tokenize.perl script. All experiments treat English as the script language and French as the target language. To avoid the trilingual corpus composed of the source language corpus, we evaluate our proposed methods in Spanish-French (Es-Fr) and German-French (De-Fr) translation tasks in azero resource scenario. To avoid the trilingual corpus composed of the source language pivot and Pivottarget Corpora, we divide the overlapping pivot sentences of the original source pivot and Pivot target group into two equal parts and merge them separately with the non-overlapping parts for each language pair."}, {"heading": "4.2 Assumptions Verification", "text": "To verify the assumptions in Section 3.1, we train a source-to-target translation model P (y | x; \u03b8x \u2192 y) and a pivot-to-target translation model P (y | z; \u03b8z \u2192 y) using the trilingual Europarl corpus. We then measure the deviation at sentence and word level KL by measuring the deviation from the source totarget model P (y | x; \u03b8x \u2192 y) in different iterations to the trained target corpus P (y | z; ctuz \u2192 y) by using JSENT (Equation (5) and JWORD3We can also adopt sampling and k-best list for approximation. Random sampling results in a large deviation (Sutskever et al., 2014; Ranzato et al., 2015; BLZ et al., 2016) for sentence-level teaching."}, {"heading": "4.3 Results on the Europarl Corpus", "text": "The aforementioned suggestions from the period from 1900 to 1900 extended to the years 1900 and 1900 to 1900."}, {"heading": "4.4 Results on the WMT Corpus", "text": "The method of word sampling performs best in our five proposed approaches based on experiments on the Europarl corpus. To further verify this approach, we are conducting experiments on the large-scale WMT corpus for Spanish-French translation. Table 5 shows the results of our method of word sampling compared to other state-of-the-art baselines. Cheng et al. (2016a) use the same data sets and the same pre-processing as ours. Firat et al. (2016b) use a much larger training scope. [5] Our method achieves a significant improvement over the pivot baseline by + 3.46 BLEU points for Newstest2012 and + 5.84 BLEU points for Newstest2013 over many BLEU points. Note that both methods depend on a source-pivot-target decryption path."}, {"heading": "4.5 Results with Small Source-Pivot Data", "text": "Specifically, the size of the source pivot corpus is orders of magnitude smaller than that of the pivot target corpus. To accomplish this task, we combine our best-performing word sampling method with the initialization and parameter freezing strategy proposed in (Zoph et al., 2016). In the experiments, the Europarl corpus is used. We set the size of the German-language training data to 100 K and use the same teacher model that was trained with 900 K English-French sentences. Table 7 gives the BLEU score of our method in German-French translation compared to three other methods. Note that our task is much more difficult than learning (Zoph)."}, {"heading": "5 Related Work", "text": "The formation of NMT models in a zero-resource scenario by using other languages has attracted intense attention in recent years. Firat et al. (2016b) proposed an approach that provides the multipath, multilingual NMT model for zero-resource translation proposed by (Firat et al., 2016a), using the multilingual NMT model trained by other language pairs to create a pseudo-parallel corpus and refine the attention mechanism of the multipath NMT model to enable resource-free translation. Several authors proposed a universal encoder decoder network in multilingual scenarios to perform zero-shot learning (Johnson et al., 2016; Ha et al., 2016). This universal model extracts translation knowledge from multiple languages, making zero-resource translation feasible."}, {"heading": "6 Conclusion", "text": "In this paper, we propose a novel framework to train the learning process of the student model without parallel corpus, which is available on a source and rotating parallel corpus under the guidance of the already trained teacher model. We introduce teachings at sentence and word level to guide the learning process of the student model. Experiments on the Europarliament and WMT corpus across language boundaries show that our proposed sampling method at word level can significantly exceed the state-of-the-art pivot-based methods and multilingual methods in terms of translation quality and decryption efficiency. We also analyze resource-free translation with small source data and combine our word level sampling method with the initialization and freezing of parameters proposed by (Zoph et al., 2016). The experiments on the Europarl corpus show that our approach achieves a significant improvement over the pivot-based sampling method."}, {"heading": "Acknowledgments", "text": "This work was carried out during Yun Chen's visit to Tsinghua University and is partially supported by the National Natural Science Foundation of China (No.61522204, No.61331013) and the 863 Program (2015AA015407)."}], "references": [{"title": "Do deep nets really need to be deep", "author": ["Jimmy Ba", "Rich Caurana"], "venue": null, "citeRegEx": "Ba and Caurana.,? \\Q2014\\E", "shortCiteRegEx": "Ba and Caurana.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of ICLR .", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Phrase-based statistical machine translation with pivot languages", "author": ["Nicola Bertoldi", "Madalina Barbaiani", "Marcello Federico", "Roldano Cattoni."], "venue": "IWSLT .", "citeRegEx": "Bertoldi et al\\.,? 2008", "shortCiteRegEx": "Bertoldi et al\\.", "year": 2008}, {"title": "Model compression", "author": ["Cristian Bucila", "Rich Caruana", "Alexandru Niculescu-Mizil."], "venue": "KDD.", "citeRegEx": "Bucila et al\\.,? 2006", "shortCiteRegEx": "Bucila et al\\.", "year": 2006}, {"title": "Neural machine translation with pivot languages", "author": ["Yong Cheng", "Yang Liu", "Qian Yang", "Maosong Sun", "Wei Xu."], "venue": "CoRR abs/1611.04928.", "citeRegEx": "Cheng et al\\.,? 2016a", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Semisupervised learning for neural machine translation", "author": ["Yong Cheng", "Wei Xu", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Machine translation by triangulation: Making effective use of multi-parallel corpora", "author": ["Trevor Cohn", "Mirella Lapata."], "venue": "ACL.", "citeRegEx": "Cohn and Lapata.,? 2007", "shortCiteRegEx": "Cohn and Lapata.", "year": 2007}, {"title": "Catalanenglish statistical machine translation without parallel corpus: bridging through spanish", "author": ["Adri\u00e0 de Gispert", "Jos\u00e9 B. Mari\u00f1o."], "venue": "Proceedings of 5th International Conference on Language Resources and Evaluation (LREC). Citeseer, pages", "citeRegEx": "Gispert and Mari\u00f1o.,? 2006", "shortCiteRegEx": "Gispert and Mari\u00f1o.", "year": 2006}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "HLT-NAACL.", "citeRegEx": "Firat et al\\.,? 2016a", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Zero-resource translation with multi-lingual neural machine translation", "author": ["Orhan Firat", "Baskaran Sankaran", "Yaser Al-Onaizan", "Fatos T. Yarman-Vural", "Kyunghyun Cho."], "venue": "EMNLP.", "citeRegEx": "Firat et al\\.,? 2016b", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Toward multilingual neural machine translation with universal encoder and decoder", "author": ["Thanh-Le Ha", "Jan Niehues", "Alexander H. Waibel."], "venue": "CoRR abs/1611.04798.", "citeRegEx": "Ha et al\\.,? 2016", "shortCiteRegEx": "Ha et al\\.", "year": 2016}, {"title": "Dual learning for machine translation", "author": ["Di He", "Yingce Xia", "Tao Qin", "Liwei Wang", "Nenghai Yu", "Tie-Yan Liu", "Wei-Ying Ma."], "venue": "NIPS.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey E. Hinton", "Oriol Vinyals", "Jeffrey Dean."], "venue": "CoRR abs/1503.02531.", "citeRegEx": "Hinton et al\\.,? 2015", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "ACL.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Language independent connectivity strength features for phrase pivot statistical machine translation", "author": ["Ahmed El Kholy", "Nizar Habash", "Gregor Leusch", "Evgeny Matusov", "Hassan Sawaf"], "venue": null, "citeRegEx": "Kholy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kholy et al\\.", "year": 2013}, {"title": "Sequencelevel knowledge distillation", "author": ["Yoon Kim", "Alexander M. Rush."], "venue": "EMNLP.", "citeRegEx": "Kim and Rush.,? 2016", "shortCiteRegEx": "Kim and Rush.", "year": 2016}, {"title": "Europarl: a parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": null, "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Learning small-size dnn with outputdistribution-based criteria", "author": ["Jinyu Li", "Rui Zhao", "Jui-Ting Huang", "Yifan Gong."], "venue": "INTERSPEECH.", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "ACL.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Zeroresource machine translation by multimodal encoder-decoder network with multimedia pivot", "author": ["Hideki Nakayama", "Noriki Nishida."], "venue": "CoRR abs/1611.04503.", "citeRegEx": "Nakayama and Nishida.,? 2016", "shortCiteRegEx": "Nakayama and Nishida.", "year": 2016}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "ACL.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "ACL.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Sequence level training with recurrent neural networks. CoRR abs/1511.06732", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Minimum risk training for neural machine translation", "author": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": null, "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A comparison of pivot methods for phrase-based statistical machine translation", "author": ["Masao Utiyama", "Hitoshi Isahara."], "venue": "HLT-NAACL.", "citeRegEx": "Utiyama and Isahara.,? 2007", "shortCiteRegEx": "Utiyama and Isahara.", "year": 2007}, {"title": "Pivot language approach for phrase-based statistical machine translation", "author": ["Hua Wu", "Haifeng Wang."], "venue": "Machine Translation 21:165\u2013181.", "citeRegEx": "Wu and Wang.,? 2007", "shortCiteRegEx": "Wu and Wang.", "year": 2007}, {"title": "Revisiting pivot language approach for machine translation", "author": ["Hua Wu", "Haifeng Wang."], "venue": "ACL/IJCNLP.", "citeRegEx": "Wu and Wang.,? 2009", "shortCiteRegEx": "Wu and Wang.", "year": 2009}, {"title": "Using context vectors in improving a machine translation system with bridge language", "author": ["Samira Tofighi Zahabi", "Somayeh Bakhshaei", "Shahram Khadivi."], "venue": "ACL.", "citeRegEx": "Zahabi et al\\.,? 2013", "shortCiteRegEx": "Zahabi et al\\.", "year": 2013}, {"title": "Improving pivot-based statistical machine translation using random walk", "author": ["Xiaoning Zhu", "Zhongjun He", "Hua Wu", "Haifeng Wang", "Conghui Zhu", "Tiejun Zhao."], "venue": "EMNLP.", "citeRegEx": "Zhu et al\\.,? 2013", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}, {"title": "Transfer learning for low-resource neural machine translation", "author": ["Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight."], "venue": "EMNLP.", "citeRegEx": "Zoph et al\\.,? 2016", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), which directly models the translation process in an end-to-end way, has attracted intensive attention from the community.", "startOffset": 33, "endOffset": 112}, {"referenceID": 26, "context": "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), which directly models the translation process in an end-to-end way, has attracted intensive attention from the community.", "startOffset": 33, "endOffset": 112}, {"referenceID": 1, "context": "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), which directly models the translation process in an end-to-end way, has attracted intensive attention from the community.", "startOffset": 33, "endOffset": 112}, {"referenceID": 19, "context": "Although NMT has achieved state-of-the-art translation performance on resource-rich language pairs such as English-French and German-English (Luong et al., 2015; Jean et al., 2015; Wu et al., 2016; Johnson et al., 2016), it still suffers from the unavailability of large-scale parallel corpora for translating low-resource languages.", "startOffset": 141, "endOffset": 219}, {"referenceID": 13, "context": "Although NMT has achieved state-of-the-art translation performance on resource-rich language pairs such as English-French and German-English (Luong et al., 2015; Jean et al., 2015; Wu et al., 2016; Johnson et al., 2016), it still suffers from the unavailability of large-scale parallel corpora for translating low-resource languages.", "startOffset": 141, "endOffset": 219}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2015), which directly models the translation process in an end-to-end way, has attracted intensive attention from the community. Although NMT has achieved state-of-the-art translation performance on resource-rich language pairs such as English-French and German-English (Luong et al., 2015; Jean et al., 2015; Wu et al., 2016; Johnson et al., 2016), it still suffers from the unavailability of large-scale parallel corpora for translating low-resource languages. Due to the large parameter space, neural models usually learn poorly from low-count events, resulting in a poor choice for low-resource language pairs. Zoph et al. (2016) indicate that NMT obtains much worse translation quality than a statistical machine translation (SMT) system on low-resource languages.", "startOffset": 8, "endOffset": 659}, {"referenceID": 10, "context": "Another direction is to develop a universal NMT model in multilingual scenarios (Johnson et al., 2016; Ha et al., 2016).", "startOffset": 80, "endOffset": 119}, {"referenceID": 8, "context": "Firat et al. (2016b) present a multi-way, multilingual model with shared attention to achieve zeroresource translation.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "Another direction is to achieve source-to-target NMT without parallel data via a pivot, which is either text (Cheng et al., 2016a) or image (Nakayama and Nishida, 2016).", "startOffset": 109, "endOffset": 130}, {"referenceID": 20, "context": ", 2016a) or image (Nakayama and Nishida, 2016).", "startOffset": 18, "endOffset": 46}, {"referenceID": 31, "context": "However, pivot-based approaches usually need to divide the decoding process into two steps, which is not only more computationally expensive, but also potentially suffers from the error propagation problem (Zhu et al., 2013).", "startOffset": 206, "endOffset": 224}, {"referenceID": 4, "context": "Another direction is to achieve source-to-target NMT without parallel data via a pivot, which is either text (Cheng et al., 2016a) or image (Nakayama and Nishida, 2016). Cheng et al. (2016a) propose a pivot-based method for zeroresource NMT: it first translates the source language to a pivot language, which is then translated to the target language.", "startOffset": 110, "endOffset": 191}, {"referenceID": 4, "context": "Another direction is to achieve source-to-target NMT without parallel data via a pivot, which is either text (Cheng et al., 2016a) or image (Nakayama and Nishida, 2016). Cheng et al. (2016a) propose a pivot-based method for zeroresource NMT: it first translates the source language to a pivot language, which is then translated to the target language. Nakayama and Nishida (2016) show that using multimedia information as pivot also benefits zero-resource translation.", "startOffset": 110, "endOffset": 380}, {"referenceID": 4, "context": "Compared with pivot-based approaches (Cheng et al., 2016a), our method allows direct parameter estimation of the intended NMT model, without the need to divide decoding into two steps.", "startOffset": 37, "endOffset": 58}, {"referenceID": 26, "context": "Neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) advocates the use of neural networks to model the translation process in an end-to-end manner.", "startOffset": 27, "endOffset": 74}, {"referenceID": 1, "context": "Neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) advocates the use of neural networks to model the translation process in an end-to-end manner.", "startOffset": 27, "endOffset": 74}, {"referenceID": 32, "context": "Zoph et al. (2016) report that NMT obtains much lower BLEU scores than SMT if only small-scale parallel corpora are available.", "startOffset": 0, "endOffset": 19}, {"referenceID": 6, "context": "translating zero-resource language pairs (de Gispert and Mari\u00f1o, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013).", "startOffset": 41, "endOffset": 222}, {"referenceID": 27, "context": "translating zero-resource language pairs (de Gispert and Mari\u00f1o, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013).", "startOffset": 41, "endOffset": 222}, {"referenceID": 28, "context": "translating zero-resource language pairs (de Gispert and Mari\u00f1o, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013).", "startOffset": 41, "endOffset": 222}, {"referenceID": 2, "context": "translating zero-resource language pairs (de Gispert and Mari\u00f1o, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013).", "startOffset": 41, "endOffset": 222}, {"referenceID": 29, "context": "translating zero-resource language pairs (de Gispert and Mari\u00f1o, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013).", "startOffset": 41, "endOffset": 222}, {"referenceID": 30, "context": "translating zero-resource language pairs (de Gispert and Mari\u00f1o, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013).", "startOffset": 41, "endOffset": 222}, {"referenceID": 15, "context": "translating zero-resource language pairs (de Gispert and Mari\u00f1o, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013).", "startOffset": 41, "endOffset": 222}, {"referenceID": 4, "context": "As pivotbased methods are agnostic to model structures, they have been adapted to NMT recently (Cheng et al., 2016a; Johnson et al., 2016).", "startOffset": 95, "endOffset": 138}, {"referenceID": 4, "context": "Figure 1(a) illustrates the basic idea of pivotbased approaches to zero-resource NMT (Cheng et al., 2016a).", "startOffset": 85, "endOffset": 106}, {"referenceID": 31, "context": "The above two-step decoding process potentially suffers from the error propagation problem (Zhu et al., 2013): the translation errors made in the first step (i.", "startOffset": 91, "endOffset": 109}, {"referenceID": 25, "context": "To address this problem, it is possible to construct a sub-space by either sampling (Shen et al., 2016), generating a k-best list (Cheng et al.", "startOffset": 84, "endOffset": 103}, {"referenceID": 16, "context": ", 2016b) or mode approximation (Kim and Rush, 2016).", "startOffset": 31, "endOffset": 51}, {"referenceID": 17, "context": "We evaluate our approach on the Europarl (Koehn, 2005) and WMT corpora.", "startOffset": 41, "endOffset": 54}, {"referenceID": 4, "context": "To compare with pivotbased methods, we use the same dataset as (Cheng et al., 2016a).", "startOffset": 63, "endOffset": 84}, {"referenceID": 22, "context": "1 The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.", "startOffset": 49, "endOffset": 72}, {"referenceID": 24, "context": "To deal with out-of-vocabulary words, we adopt byte pair encoding (BPE) (Sennrich et al., 2016) to split words into sub-words.", "startOffset": 72, "endOffset": 95}, {"referenceID": 9, "context": "We leverage an open-source NMT toolkit dl4mt implemented by Theano 2 for all the experiments and compare our approach with state-of-the-art multilingual methods (Firat et al., 2016b) and pivot-based methods (Cheng et al.", "startOffset": 161, "endOffset": 182}, {"referenceID": 4, "context": ", 2016b) and pivot-based methods (Cheng et al., 2016a).", "startOffset": 33, "endOffset": 54}, {"referenceID": 16, "context": "Sentence-Level Teaching: for simplicity, we use the mode as suggested in (Kim and Rush, 2016) to approximate the target sentence space in calculating the expected gradients with respect to the expectation in Equation (7).", "startOffset": 73, "endOffset": 93}, {"referenceID": 26, "context": "Random sampling brings a large variance (Sutskever et al., 2014; Ranzato et al., 2015; He et al., 2016) for sentence-level teaching.", "startOffset": 40, "endOffset": 103}, {"referenceID": 23, "context": "Random sampling brings a large variance (Sutskever et al., 2014; Ranzato et al., 2015; He et al., 2016) for sentence-level teaching.", "startOffset": 40, "endOffset": 103}, {"referenceID": 11, "context": "Random sampling brings a large variance (Sutskever et al., 2014; Ranzato et al., 2015; He et al., 2016) for sentence-level teaching.", "startOffset": 40, "endOffset": 103}, {"referenceID": 21, "context": "where Yk is the k-best list from beam search of the teacher model and \u03b1 is a hyperparameter controling the sharpness of the distribution (Och, 2003).", "startOffset": 137, "endOffset": 148}, {"referenceID": 4, "context": "Table 3 gives BLEU scores on the Europarl corpus of our best performing sentence-level method (sent-beam) and word-level method (word-sampling) compared with pivot-based methods (Cheng et al., 2016a).", "startOffset": 178, "endOffset": 199}, {"referenceID": 4, "context": "We use the same data preprocessing as in (Cheng et al., 2016a).", "startOffset": 41, "endOffset": 62}, {"referenceID": 4, "context": "Existing zero-resource NMT systems Cheng et al. (2016a)\u2020 pivot 6.", "startOffset": 35, "endOffset": 56}, {"referenceID": 4, "context": "Existing zero-resource NMT systems Cheng et al. (2016a)\u2020 pivot 6.78M 9.29M 24.60 Cheng et al. (2016a)\u2020 likelihood 6.", "startOffset": 35, "endOffset": 102}, {"referenceID": 4, "context": "Existing zero-resource NMT systems Cheng et al. (2016a)\u2020 pivot 6.78M 9.29M 24.60 Cheng et al. (2016a)\u2020 likelihood 6.78M 9.29M 100K 25.78 Firat et al. (2016b) one-to-one 34.", "startOffset": 35, "endOffset": 158}, {"referenceID": 4, "context": "Existing zero-resource NMT systems Cheng et al. (2016a)\u2020 pivot 6.78M 9.29M 24.60 Cheng et al. (2016a)\u2020 likelihood 6.78M 9.29M 100K 25.78 Firat et al. (2016b) one-to-one 34.71M 65.77M 17.59 17.61 Firat et al. (2016b)\u2020 many-to-one 34.", "startOffset": 35, "endOffset": 216}, {"referenceID": 16, "context": "Kim and Rush (2016) claim a similar observation in data distillation for NMT and provide an explanation that student distributions are more peaked for sentence-level methods.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "Table 6: Examples and corresponding sentence BLEU scores of translations using the pivot and likelihood methods in (Cheng et al., 2016a) and the proposed word-sampling method.", "startOffset": 115, "endOffset": 136}, {"referenceID": 4, "context": "We observe that our approach generates better translations than the methods in (Cheng et al., 2016a).", "startOffset": 79, "endOffset": 100}, {"referenceID": 4, "context": "Table 6 shows translation examples of the pivot and likelihood methods proposed in (Cheng et al., 2016a) and our proposed word-sampling method.", "startOffset": 83, "endOffset": 104}, {"referenceID": 4, "context": "Cheng et al. (2016a) use the same datasets and the same preprocessing as ours.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "Cheng et al. (2016a) use the same datasets and the same preprocessing as ours. Firat et al. (2016b) utilize a much larger training set.", "startOffset": 0, "endOffset": 100}, {"referenceID": 32, "context": "Transfer represents the transfer learning method in (Zoph et al., 2016).", "startOffset": 52, "endOffset": 71}, {"referenceID": 32, "context": "To fulfill this task, we combine our best performing word-sampling method with the initialization and parameter freezing strategy proposed in (Zoph et al., 2016).", "startOffset": 142, "endOffset": 161}, {"referenceID": 32, "context": "Note that our task is much harder than transfer learning (Zoph et al., 2016) since the latter depends on a parallel German-French corpus.", "startOffset": 57, "endOffset": 76}, {"referenceID": 8, "context": "(2016b) proposed an approach which delivers the multiway, multilingual NMT model proposed by (Firat et al., 2016a) for zero-resource translation.", "startOffset": 93, "endOffset": 114}, {"referenceID": 10, "context": "Several authors proposed a universal encoder-decoder network in multilingual scenarios to perform zero-shot learning (Johnson et al., 2016; Ha et al., 2016).", "startOffset": 117, "endOffset": 156}, {"referenceID": 6, "context": "This idea is widely used in SMT (de Gispert and Mari\u00f1o, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013).", "startOffset": 32, "endOffset": 213}, {"referenceID": 27, "context": "This idea is widely used in SMT (de Gispert and Mari\u00f1o, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013).", "startOffset": 32, "endOffset": 213}, {"referenceID": 28, "context": "This idea is widely used in SMT (de Gispert and Mari\u00f1o, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013).", "startOffset": 32, "endOffset": 213}, {"referenceID": 2, "context": "This idea is widely used in SMT (de Gispert and Mari\u00f1o, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013).", "startOffset": 32, "endOffset": 213}, {"referenceID": 29, "context": "This idea is widely used in SMT (de Gispert and Mari\u00f1o, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013).", "startOffset": 32, "endOffset": 213}, {"referenceID": 30, "context": "This idea is widely used in SMT (de Gispert and Mari\u00f1o, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013).", "startOffset": 32, "endOffset": 213}, {"referenceID": 15, "context": "This idea is widely used in SMT (de Gispert and Mari\u00f1o, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013).", "startOffset": 32, "endOffset": 213}, {"referenceID": 3, "context": "Our work is also related to knowledge distillation, which trains a compact model to approximate the function learned by a larger, more complex model or an ensemble of models (Bucila et al., 2006; Ba and Caurana, 2014; Li et al., 2014; Hinton et al., 2015).", "startOffset": 174, "endOffset": 255}, {"referenceID": 0, "context": "Our work is also related to knowledge distillation, which trains a compact model to approximate the function learned by a larger, more complex model or an ensemble of models (Bucila et al., 2006; Ba and Caurana, 2014; Li et al., 2014; Hinton et al., 2015).", "startOffset": 174, "endOffset": 255}, {"referenceID": 18, "context": "Our work is also related to knowledge distillation, which trains a compact model to approximate the function learned by a larger, more complex model or an ensemble of models (Bucila et al., 2006; Ba and Caurana, 2014; Li et al., 2014; Hinton et al., 2015).", "startOffset": 174, "endOffset": 255}, {"referenceID": 12, "context": "Our work is also related to knowledge distillation, which trains a compact model to approximate the function learned by a larger, more complex model or an ensemble of models (Bucila et al., 2006; Ba and Caurana, 2014; Li et al., 2014; Hinton et al., 2015).", "startOffset": 174, "endOffset": 255}, {"referenceID": 1, "context": "Firat et al. (2016b) proposed an approach which delivers the multiway, multilingual NMT model proposed by (Firat et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "This idea is widely used in SMT (de Gispert and Mari\u00f1o, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013). Cheng et al. (2016a) propose pivot-based NMT by simultaneously improving source-to-pivot and pivot-to-target translation quality in order to improve source-to-target translation quality.", "startOffset": 131, "endOffset": 236}, {"referenceID": 1, "context": "This idea is widely used in SMT (de Gispert and Mari\u00f1o, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013). Cheng et al. (2016a) propose pivot-based NMT by simultaneously improving source-to-pivot and pivot-to-target translation quality in order to improve source-to-target translation quality. Nakayama and Nishida (2016) achieve zero-resource machine translation by utilizing image as a pivot and training multimodal encoders to share common semantic representation.", "startOffset": 131, "endOffset": 430}, {"referenceID": 0, "context": ", 2006; Ba and Caurana, 2014; Li et al., 2014; Hinton et al., 2015). Kim and Rush (2016) first introduce knowledge distillation in neural machine translation.", "startOffset": 8, "endOffset": 89}, {"referenceID": 32, "context": "We also analyze zero-resource translation with small source-pivot data, and combine our wordlevel sampling method with initialization and parameter freezing suggested by (Zoph et al., 2016).", "startOffset": 170, "endOffset": 189}], "year": 2017, "abstractText": "While end-to-end neural machine translation (NMT) has made remarkable progress recently, it still suffers from the data scarcity problem for low-resource language pairs and domains. In this paper, we propose a method for zero-resource NMT by assuming that parallel sentences have close probabilities of generating a sentence in a third language. Based on this assumption, our method is able to train a source-to-target NMT model (\u201cstudent\u201d) without parallel corpora available, guided by an existing pivot-to-target NMT model (\u201cteacher\u201d) on a source-pivot parallel corpus. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs.", "creator": "LaTeX with hyperref package"}}}