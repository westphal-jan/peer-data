{"id": "1705.00823", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2017", "title": "STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset", "abstract": "In recent years, automatic generation of image descriptions (captions), that is, image captioning, has attracted a great deal of attention. In this paper, we particularly consider generating Japanese captions for images. Since most available caption datasets have been constructed for English language, there are few datasets for Japanese. To tackle this problem, we construct a large-scale Japanese image caption dataset based on images from MS-COCO, which is called STAIR Captions. STAIR Captions consists of 820,310 Japanese captions for 164,062 images. In the experiment, we show that a neural network trained using STAIR Captions can generate more natural and better Japanese captions, compared to those generated using English-Japanese machine translation after generating English captions.", "histories": [["v1", "Tue, 2 May 2017 07:07:55 GMT  (1701kb,D)", "http://arxiv.org/abs/1705.00823v1", "Accepted as ACL2017 short paper. 5 pages"]], "COMMENTS": "Accepted as ACL2017 short paper. 5 pages", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["yuya yoshikawa", "yutaro shigeto", "akikazu takeuchi"], "accepted": true, "id": "1705.00823"}, "pdf": {"name": "1705.00823.pdf", "metadata": {"source": "META", "title": "STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset", "authors": ["Yuya Yoshikawa", "Yutaro Shigeto", "Akikazu Takeuchi"], "emails": ["yoshikawa@stair.center", "shigeto@stair.center", "takeuchi@stair.center"], "sections": [{"heading": "1 Introduction", "text": "Integrated processing of natural language and images has attracted attention in recent years, and the 2011 Workshop on Vision and Language has since become an annual event1. In this research area, methods for automatically generating captions (Karpathy and Fei-Fei, 2015; Donahue et al., 2015; Vinyals et al., 2015; Mao et al., 2015) have attracted a lot of attention. Captions are designed to automatically generate a caption for a particular image. By improving the quality of captions, image search by means of natural sentences and image recognition for1 In recent years, it has been held as a joint workshop, such as EMNLP and ACL; https: / / vision.cs.hacettepe. edu.tr / vl2017 / visually impaired people by providing captions as captions."}, {"heading": "2 Related Work", "text": "Some English caption datasets have been proposed (Krishna et al., 2016; Kuznetsova et al., 2013; Ordonez et al., 2011; Vedantam et al., ar Xiv: 170 5.00 823v 1 [cs.C L] 2M ay2 0172015; Isola et al., 2014) Representative examples are PASCAL (Rashtchian et al., 2010), Flickr3k (Rashtchian et al., 2010; Hodosh et al., 2013), Flickr30k (Young et al., 2014) - an extension of Flickr3k - and MS-COCO (Microsoft Common Objects in Context) (Lin et al., 2014). As described in section 3, we comment on Japanese captions for the images in MS-COCO. Capdating Captions of the Japanese Captions, STmiaki MS-COCO et al."}, {"heading": "3 STAIR Captions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Annotation Procedure", "text": "We commented on all the images (164,062 images) in the 2014 edition of MS-COCO. For each image, we provided five Japanese captions, so the total number of captions was 820,310. In accordance with the rules for publishing data sets based on MS-COCO, the Japanese captions we created for the test images are excluded from the public part of the STAIR captions. To efficiently comment on captions, we first developed a web system for captions annotations. Figure 1 shows the example of the annotation screen on the web system. Each commentator looks at the displayed image and writes the corresponding Japanese description in the text field below the image. By2http: / / mscococo.org / extern / pressing the \"Send\" button completes a single task and the next task is started. In order to comment simultaneously and unexplicitly on the captions, we have asked part-time workers to include the captions and the captions based on the image caption (not to include a Japanese caption)."}, {"heading": "3.2 Statistics", "text": "This section presents the quantitative characteristics of STAIR captions and compares them with YJ captions (Miyazaki and Shimizu, 2016), a set of Japanese captions for the images in MS-COCO as well as in STAIR captions. Table 1 summarizes the statistics of the data sets. Compared to YJ captions, the Japanese captions in STAIR captions total 6.23x and 6.19x, respectively. In the public part of STAIR captions, the figures for the images and Japanese captions are 4.65x and 4.67x larger than those in YJ captions. The fact that the number of images and captions in STAIR captions is large is an important factor in captions creation because it reduces the possibility of unknown scenes and objects appearing in the test images. STAIR captions vocabulary is 2.69x larger than that of YJ captions, because of the large number of STAIR captions and STAIR captions is possible."}, {"heading": "4 Image Caption Generation", "text": "In this section, we briefly consider the method proposed by Karpathy and Fei-Fei (2015) for generating captions used in our experiments (Section 5), which consists of a Convolutionary Neural Network (CNN) and Long-Term Memory (LSTM) 3. Specifically, CNN first extracts captions from a given image, and then LSTM generates a caption from the extracted features. Let me be an image, and the corresponding caption is Y = (y1, y2, \u00b7, yn). Subsequently, the caption is defined as: x (im) = CNN (I), h0 = tanh (W (im) x (im) + b (im)))))), c0 = 0, ht, ct = LSTM (xt, ht \u2212 1, ct \u2212 1) (t \u2265 1), yt = softmax (Woht + bo), where CNN (\u00b7) is a function that outputs the image attributes extracted from CNN."}, {"heading": "5 Experiments", "text": "In this section, we will conduct an experiment to generate Japanese captions using STAIR Captions. The aim of this experiment is to demonstrate the need for a Japanese captions dataset. Specifically, we will quantify and qualitatively evaluate how fluent Japanese captions can be generated using a neural network-based captions generation model trained on STAIR Captions."}, {"heading": "5.1 Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1.1 Evaluation Measure", "text": "According to the literature (Chen et al., 2015; Karpathy and Fei-Fei, 2015) we use BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and CIDER (Vedantam et al., 2015) as evaluation benchmarks. Although BLEU and ROUGE were originally developed to evaluate machine translation and text collection, we use them here because they are often used to measure the quality of caption generation."}, {"heading": "5.1.2 Comparison Methods", "text": "In this experiment, we evaluate the following caption generation methods. \u2022 En-Generator \u2192 MT: A pipeline method of English caption generation and English machine translation. \u2022 In the test phase, we first generate an English caption through the trained neural network and then translate the generated caption into Japanese by machine translation. \u2022 Yes-Generator: This method forms a neural network using the STAIR captions. Unlike MSCOCO \u2192 MT, this method directly generates a Japanese caption from a given image. As mentioned in Section 4, we use the method proposed by Karpathy and Fei File (2015) as caption generator generation models for both En generators."}, {"heading": "5.1.3 Dataset Separation", "text": "After the experimental setting in the previous studies (Chen et al., 2015; Karpathy and Fei-Fei, 2015), we used 123,287 images included in the MS-COCO training and validation sets and associated Japanese captions. We divided the data set into three parts, i.e. 113,287 images for the training set, 5,000 images for the validation set, and 5,000 images for the test set. Hyperparameters of the neural network were adjusted based on the CIDEr values using the validation set. As a pre-processing, we applied morphological analysis to the Japanese captions using MeCab6.5 http: / / www.robots.ox.ac.uk / ~ vgg / research / very _ deep / 6http: / / taku910.github.io / mecab /"}, {"heading": "5.2 Results", "text": "Table 2 summarizes the experimental results, showing that the Yes generator, i.e. the approach in which Japanese captions were used as training data, outperformed the En generator \u2192 MT, which was trained without Japanese captions. Table 3 shows two examples in which Yes generator generated appropriate captions, while Engenerator \u2192 MT generated unnatural captions. In the example at the top of Table 3, En generator initially generated the term \"A double-decker bus.\" MT translated the term with \"En generator,\" but the translation is word for word and unsuitable as a Japanese term. In contrast, Jager generator generated \"a two-decker bus,\" which is appropriate as a Japanese translation of a double-decker bus. In the example at the bottom of the table, En generator generated the wrong caption by translating \"A bundle of food\" with \"Yes generator.\""}, {"heading": "6 Conclusion", "text": "The total number of Japanese captions is 820,310. To our knowledge, STAIRCaptions is currently the largest Japanese captions dataset. In our experiment, we compared the performance of Japanese captions generation using a neural network-based model with and without STAIR captions to highlight the need for Japanese captions. As a result, we demonstrated the need for STAIR captions. In addition, we confirmed that Japanese captions can be generated simply by adapting the existing captions generation method. In future work, we will analyze the experimental results in more detail. In addition, by using both Japanese and English captions, we will develop multilingual captions generation models."}], "references": [{"title": "Microsoft COCO captions: Data collection and evaluation server", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C. Lawrenc Zitnick."], "venue": "arXiv preprint 1504.00325.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell."], "venue": "Proceedings of the IEEE", "citeRegEx": "Donahue et al\\.,? 2015", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "Multi30k: Multilingual english-german image descriptions", "author": ["D. Elliott", "S. Frank", "K. Sima\u2019an", "L. Specia"], "venue": "In Workshop on Vision and Language", "citeRegEx": "Elliott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2016}, {"title": "The IAPR Benchmark: A new evaluation resource for visual information systems", "author": ["Michael Grubinger", "Paul Clough", "Henning M\u00fcller", "Thomas Deselaers."], "venue": "Proceedings of the Fifth International Conference on Language Resources and Evaluation", "citeRegEx": "Grubinger et al\\.,? 2006", "shortCiteRegEx": "Grubinger et al\\.", "year": 2006}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier."], "venue": "Journal of Artificial Intelligence Research 47:853\u2013899.", "citeRegEx": "Hodosh et al\\.,? 2013", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "What makes a photograph memorable", "author": ["Phillip Isola", "Jianxiong Xiao", "Devi Parikh", "Antonio Torralba"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "Isola et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Isola et al\\.", "year": 2014}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei."], "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR). pages 3128\u20133137.", "citeRegEx": "Karpathy and Fei.Fei.,? 2015", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Visual Genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["Fei-Fei Li"], "venue": null, "citeRegEx": "Li.,? \\Q2016\\E", "shortCiteRegEx": "Li.", "year": 2016}, {"title": "Generalizing image captions for image-text parallel corpus", "author": ["Polina Kuznetsova", "Vicente Ordonez", "Alexander Berg", "Tamara Berg", "Yejin Choi."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL). pages", "citeRegEx": "Kuznetsova et al\\.,? 2013", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2013}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Proceedings of the Workshop on Text Summarization Branches Out. pages 25\u201326.", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Microsoft COCO: Common objects in context", "author": ["TY Lin", "M Maire", "S Belongie", "J Hays", "P Perona."], "venue": "European Conference on Computer Vision (ECCV). pages 740\u2013755.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (M-RNN)", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan Yuille."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Mao et al\\.,? 2015", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Crosslingual image caption generation", "author": ["TakashiMiyazaki andNobuyuki Shimizu."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL). pages 1780\u20131790.", "citeRegEx": "Shimizu.,? 2016", "shortCiteRegEx": "Shimizu.", "year": 2016}, {"title": "Im2Text: Describing images using 1 million captioned dhotographs", "author": ["Vicente Ordonez", "Girish Kulkarni", "Tamara L Berg."], "venue": "Advances in Neural Information Processing Systems (NIPS). pages 1143\u2013 1151.", "citeRegEx": "Ordonez et al\\.,? 2011", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting of the Association of Computational Linguistics (ACL). pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Collecting image annotations using Amazon\u2019s mechanical turk", "author": ["Cyrus Rashtchian", "Peter Young", "Micah Hodosh", "Julia Hockenmaier."], "venue": "NAACL HLT Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk. pages 139\u2013", "citeRegEx": "Rashtchian et al\\.,? 2010", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "CIDEr: Consensus-based image description evaluation", "author": ["RamakrishnaVedantam", "C. Lawrence Zitnick", "andDevi Parikh"], "venue": "In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "RamakrishnaVedantam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "RamakrishnaVedantam et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR). pages 3156\u20133164.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."], "venue": "Transactions of the Association for Computational Linguistics 2:67\u201378.", "citeRegEx": "Young et al\\.,? 2014", "shortCiteRegEx": "Young et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "In this research area, methods to automatically generate image descriptions (captions), that is, image captioning, have attracted a great deal of attention (Karpathy and Fei-Fei, 2015; Donahue et al., 2015; Vinyals et al., 2015; Mao et al., 2015) .", "startOffset": 156, "endOffset": 246}, {"referenceID": 1, "context": "In this research area, methods to automatically generate image descriptions (captions), that is, image captioning, have attracted a great deal of attention (Karpathy and Fei-Fei, 2015; Donahue et al., 2015; Vinyals et al., 2015; Mao et al., 2015) .", "startOffset": 156, "endOffset": 246}, {"referenceID": 17, "context": "In this research area, methods to automatically generate image descriptions (captions), that is, image captioning, have attracted a great deal of attention (Karpathy and Fei-Fei, 2015; Donahue et al., 2015; Vinyals et al., 2015; Mao et al., 2015) .", "startOffset": 156, "endOffset": 246}, {"referenceID": 11, "context": "In this research area, methods to automatically generate image descriptions (captions), that is, image captioning, have attracted a great deal of attention (Karpathy and Fei-Fei, 2015; Donahue et al., 2015; Vinyals et al., 2015; Mao et al., 2015) .", "startOffset": 156, "endOffset": 246}, {"referenceID": 10, "context": "\u2022 We constructed a large-scale Japanese image caption dataset, STAIR Captions, which consists of Japanese captions for all the images in MS-COCO (Lin et al., 2014) (Section 3).", "startOffset": 145, "endOffset": 163}, {"referenceID": 15, "context": "Representative examples are PASCAL (Rashtchian et al., 2010), Flickr3k (Rashtchian et al.", "startOffset": 35, "endOffset": 60}, {"referenceID": 15, "context": ", 2010), Flickr3k (Rashtchian et al., 2010; Hodosh et al., 2013), Flickr30k (Young et al.", "startOffset": 18, "endOffset": 64}, {"referenceID": 4, "context": ", 2010), Flickr3k (Rashtchian et al., 2010; Hodosh et al., 2013), Flickr30k (Young et al.", "startOffset": 18, "endOffset": 64}, {"referenceID": 18, "context": ", 2013), Flickr30k (Young et al., 2014) \u2014an extension of Flickr3k\u2014, and MS-COCO (Microsoft Common Objects in Context) (Lin et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 10, "context": ", 2014) \u2014an extension of Flickr3k\u2014, and MS-COCO (Microsoft Common Objects in Context) (Lin et al., 2014).", "startOffset": 86, "endOffset": 104}, {"referenceID": 3, "context": "Recently, a few caption datasets in languages other than English have been constructed (Miyazaki and Shimizu, 2016; Grubinger et al., 2006; Elliott et al., 2016).", "startOffset": 87, "endOffset": 161}, {"referenceID": 2, "context": "Recently, a few caption datasets in languages other than English have been constructed (Miyazaki and Shimizu, 2016; Grubinger et al., 2006; Elliott et al., 2016).", "startOffset": 87, "endOffset": 161}, {"referenceID": 2, "context": ", 2006; Elliott et al., 2016). In particular, the study of Miyazaki and Shimizu (2016) is closest to the present study.", "startOffset": 8, "endOffset": 87}, {"referenceID": 6, "context": "In this section, we briefly review the caption generation method proposed by Karpathy and Fei-Fei (2015), which is used in our experiments (Section 5).", "startOffset": 77, "endOffset": 105}, {"referenceID": 0, "context": "Following the literature (Chen et al., 2015; Karpathy and Fei-Fei, 2015), we use BLEU (Papineni et al.", "startOffset": 25, "endOffset": 72}, {"referenceID": 6, "context": "Following the literature (Chen et al., 2015; Karpathy and Fei-Fei, 2015), we use BLEU (Papineni et al.", "startOffset": 25, "endOffset": 72}, {"referenceID": 14, "context": ", 2015; Karpathy and Fei-Fei, 2015), we use BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and CIDEr (Vedantam et al.", "startOffset": 49, "endOffset": 72}, {"referenceID": 9, "context": ", 2002), ROUGE (Lin, 2004), and CIDEr (Vedantam et al.", "startOffset": 15, "endOffset": 26}, {"referenceID": 6, "context": "As mentioned in Section 4, we used the method proposed by Karpathy and Fei-Fei (2015) as caption generation models for both En-generator \u2192 MT and Ja-generator.", "startOffset": 58, "endOffset": 86}, {"referenceID": 0, "context": "Following the experimental setting in the previous studies (Chen et al., 2015; Karpathy and Fei-Fei, 2015), we used 123,287 images included in the MS-COCO training and validation sets and their corresponding Japanese captions.", "startOffset": 59, "endOffset": 106}, {"referenceID": 6, "context": "Following the experimental setting in the previous studies (Chen et al., 2015; Karpathy and Fei-Fei, 2015), we used 123,287 images included in the MS-COCO training and validation sets and their corresponding Japanese captions.", "startOffset": 59, "endOffset": 106}], "year": 2017, "abstractText": "In recent years, automatic generation of image descriptions (captions), that is, image captioning, has attracted a great deal of attention. In this paper, we particularly consider generating Japanese captions for images. Since most available caption datasets have been constructed for English language, there are few datasets for Japanese. To tackle this problem, we construct a large-scale Japanese image caption dataset based on images from MS-COCO, which is called STAIRCaptions. STAIRCaptions consists of 820,310 Japanese captions for 164,062 images. In the experiment, we show that a neural network trained using STAIR Captions can generate more natural and better Japanese captions, compared to those generated using English-Japanese machine translation after generating English captions.", "creator": "LaTeX with hyperref package"}}}