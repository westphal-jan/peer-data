{"id": "1705.00861", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2017", "title": "Deep Neural Machine Translation with Linear Associative Unit", "abstract": "Deep Neural Networks (DNNs) have provably enhanced the state-of-the-art Neural Machine Translation (NMT) with their capability in modeling complex functions and capturing complex linguistic structures. However NMT systems with deep architecture in their encoder or decoder RNNs often suffer from severe gradient diffusion due to the non-linear recurrent activations, which often make the optimization much more difficult. To address this problem we propose novel linear associative units (LAU) to reduce the gradient propagation length inside the recurrent unit. Different from conventional approaches (LSTM unit and GRU), LAUs utilizes linear associative connections between input and output of the recurrent unit, which allows unimpeded information flow through both space and time direction. The model is quite simple, but it is surprisingly effective. Our empirical study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art.", "histories": [["v1", "Tue, 2 May 2017 08:58:17 GMT  (235kb,D)", "http://arxiv.org/abs/1705.00861v1", "10 pages, ACL 2017"]], "COMMENTS": "10 pages, ACL 2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["mingxuan wang", "zhengdong lu", "jie zhou", "qun liu"], "accepted": true, "id": "1705.00861"}, "pdf": {"name": "1705.00861.pdf", "metadata": {"source": "CRF", "title": "Deep Neural Machine Translation with Linear Associative Unit", "authors": ["Mingxuan Wang", "Zhengdong Lu", "Jie Zhou", "Qun Liu"], "emails": ["xuanswang@tencent.com"], "sections": [{"heading": "1 Introduction", "text": "He has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Zhang et al., 2016; Zhang et al., 2016; Zhang et al., 2015; Meng et al., 2015). Unlike traditional statistical machine translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008), which consists of several separately coordinated components, NMT aims to build on a single and large neural network to insert text directly into the corresponding source text. Typical NMT models consist of two recurrent neural networks (RNNs)."}, {"heading": "2 Neural machine translation", "text": "A typical neural machine translation system is a single and large neural network that generalizes the conditional probability p (y | x) of translating a source set x = {x1, x2, \u00b7 \u00b7, xTx} directly to a target set y = {y1, y2, \u00b7 \u00b7, yTy}.Attention-based NMT, with RNNsearch as its most popular representative, generalizes the conventional concept of the encoder decoder when using an array of vectors to represent the source set and dynamically address the relevant segments thereof during decoding.The process can be explicitly divided into a coding part and an attention mechanism. First, the model encodes the source set x into a sequence of vectors c = {h1, h2, hTx}."}, {"heading": "3 Model Description", "text": "In this section, we discuss Linear Associative Unit (LAU) to facilitate the formation of deep stack RNNs. Based on this idea, we propose DEEPLAU, a neural machine translation model with a deep encoder and decoder."}, {"heading": "3.1 Recurrent Layers", "text": "A recursive neural network (Williams and Zipser, 1989) is a class of neural networks with recurring connections and a state (or more complex memory-like extension), and the information of the past is built up by recursive connections, which makes RNN applicable to sequential prediction tasks of any length. In a sequence of vectors x = {x1, x2, \u00b7 \u00b7, xT} input, a standard RNN calculates the sequence of hidden states h = {h1, h2, \u00b7, hT} by iterating the following sequence from t = 1 to t = T: ht = \u03c6 (xt, ht \u2212 1) (1) \u03c6 is normally a non-linear function like the composition of a logistic sigmoid with affine transformation."}, {"heading": "3.2 Gated Recurrent Unit", "text": "The effects of long-term dependencies are dropped exponentially in relation to the propagation length of the slope, a problem that has been extensively investigated by (Hochreiter and Schmidhuber, 1997; Pascanu et al., 2013b). A successful approach is to design a more sophisticated activation function than a usual activation function consisting of propagation functions to control the flow of information and reduce the propagation path. There is a long workbook aimed at solving this problem, with the Long-Term Storage Units (LSTM) being the most prominent examples and the latest propagation unit (GRU) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014). RNNs that either use these recurring units \u2212 xxt \u2212 as the most recent unit."}, {"heading": "3.3 Linear Associative Unit", "text": "It is not as if it is a matter of a way in which there is a direct contribution from the input H (xt). More precisely, it contains the nonlinear information of the input H (xt). (6) It contains the nonlinear information of the input and the input H (xt). (6) It is not as if there is a direct transfer from the input H (xt). (7) Where the non-linear information of the input xt and the previous hidden state.h). (7) Where the non-linear information of the input xt and the input xt are produced. (7) Where the non-linear abstractions of the input xt and the previous hidden states are produced. (7) Where the non-linear abstractions of the input xt and the previous hidden states are produced."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Setup", "text": "To demonstrate the usefulness of our approach, we also provide results on two other translation tasks: English-French, English-German. The evaluation metric is BLEU2 (Papineni et al., 2002). For Chinese-English tasks, we use NIST-BLEU random words. For other tasks, we have the reference and evaluate performance with multi-bleu.pl. The metrics are exactly the same as in previous work.LDC corpora3, with 27.9M Chinese words and 34.5M English words. We choose NIST 2002 (MT02) as the test set and NIST 2003 (MT03)."}, {"heading": "4.2 Training details", "text": "Specifically, we limit the source and target vocabulary to the most common 30K words in Chinese-English and English-French. For English-German, we set the source and target vocabulary to 120K and 80K, respectively. In all experiments, the dimensions of word embedding and recurring hidden states are also set to 512. The dimension of ct is also of size 512. Note that our network is narrower than most previous work in which hidden states of sizing 1024 are used. We initialize parameters by taking each element from the Gaussian distribution with the mean 0 and the variance 0.042. Parameter optimization is performed using stochastic ancestry rates."}, {"heading": "4.3 Results on Chinese-English Translation", "text": "DEEPLAU clearly leads to a remarkable improvement over its competitors. Compared to DEEPGRU, the BLEU value is + 4.89 BLEU with an average of four test sets, which shows the modelling performance of the liner association connections. We suggest this is because the LAUs use adaptive gate functionality, which is based on the input and enables it to automatically decide how much linear information should be transferred to the next step. To demonstrate the performance of DEEPLAU, we also make a comparison with previous work. Our best single model intersects both a phrase-based MT system (Moses) and an open source attention-based NMT system (Groundhog) by + 7.3 and + 6.8 BLEU points on average. The result is also better than some other state-of-the-art variants of attention-based NMT mode (Moses) with large margins. After PosEU and an overall gain of 11.EU and DE68, the total score exceeds the BLU + 4.U."}, {"heading": "4.4 Results on English-German Translation", "text": "We compare our NMT systems with various other systems, including the winning system in WMT14 (Buck et al., 2014), a phrase-based system whose language models were trained on a huge monolingual text, the Common Crawl Corpus. For end-to-end NMT systems, Wu et al. (2016) is currently the SOTA system to the best of our knowledge and about 4 BLEU points above the previously best reported results, although Zhou et al. (2016) is a much deeper neural network4. Following Wu et al. (2016), the BLEU score represents the average score of 8 models we train. Our approach achieves comparable results with SOTA system. As shown in Table 2, DeepLAU performs better than the word-based model and even not much worse than the best workpiece models achieved by Wu et al. (2016)."}, {"heading": "4.5 Results on English-French Translation", "text": "Luong et al. (2014) achieved BLEU values of 30.4 with a six-layer deep encoder decoder model. The two attention models RNNSearch and RNNsearch-LV achieved BLEU values of 28.5 and 32.7, respectively. The best single NMT deep-att model to date with an 18-layer encoder and 7-layer decoder achieved BLEU values of 35.9. For DEEPLAU, we obtained the BLEU value of 35.1 with a 4-layer encoder and 4-layer decoder, which is equivalent to the SOTA system in terms of BLEU."}, {"heading": "4.6 Analysis", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "5 Conclusion", "text": "We propose a Linear Associative Unit (LAU) that merges linear and nonlinear transformations within the recurrent unit, allowing gradients to decompose much more slowly than the standard deep networks that allow us to build a deep neural network for machine translation. Our empirical study shows that it can significantly improve the performance of NMT."}, {"heading": "6 acknowledge", "text": "Wang's work is partially supported by the National Science Foundation for Deep Semantics Based Uighur to Chinese Machine Translation (ID 61662077), and Qun Liu's work is partially supported by the Science Foundation Ireland at the ADAPT Centre for Digital Content Technology (www.adaptcentre.ie) at Dublin City University, which is co-funded by the SFI Research Centres Programme (Grant 13 / RC / 2106) from the European Regional Development Fund."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "N-gram counts and language models from the common crawl", "author": ["Christian Buck", "Kenneth Heafield", "Bas Van Ooyen."], "venue": "LREC. Citeseer, volume 2, page 4.", "citeRegEx": "Buck et al\\.,? 2014", "shortCiteRegEx": "Buck et al\\.", "year": 2014}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, pages 263\u2013270.", "citeRegEx": "Chiang.,? 2005", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv preprint arXiv:1308.0850 .", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka."], "venue": "arXiv preprint arXiv:1410.5401 .", "citeRegEx": "Graves et al\\.,? 2014", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "arXiv preprint arXiv:1512.03385 .", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 770\u2013778.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves."], "venue": "arXiv preprint arXiv:1507.01526 .", "citeRegEx": "Kalchbrenner et al\\.,? 2015", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Treeto-string alignment template for statistical machine translation", "author": ["Yang Liu", "Qun Liu", "Shouxun Lin."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the As-", "citeRegEx": "Liu et al\\.,? 2006", "shortCiteRegEx": "Liu et al\\.", "year": 2006}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1508.04025 .", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "arXiv preprint arXiv:1410.8206 .", "citeRegEx": "Luong et al\\.,? 2014", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Neural transformation machine: A new architecture for sequenceto-sequence learning", "author": ["Fandong Meng", "Zhengdong Lu", "Zhaopeng Tu", "Hang Li", "Qun Liu."], "venue": "CoRR abs/1506.06442. http://arxiv.org/abs/1506.06442.", "citeRegEx": "Meng et al\\.,? 2015", "shortCiteRegEx": "Meng et al\\.", "year": 2015}, {"title": "Forest-based translation", "author": ["Haitao Mi", "Liang Huang", "Qun Liu."], "venue": "ACL. pages 192\u2013 199.", "citeRegEx": "Mi et al\\.,? 2008", "shortCiteRegEx": "Mi et al\\.", "year": 2008}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. Association", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1312.6026 .", "citeRegEx": "Pascanu et al\\.,? 2013a", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "ICML (3) 28:1310\u20131318.", "citeRegEx": "Pascanu et al\\.,? 2013b", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Minimum risk training for neural machine translation", "author": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."], "venue": "arXiv preprint arXiv:1512.02433 .", "citeRegEx": "Shen et al\\.,? 2015", "shortCiteRegEx": "Shen et al\\.", "year": 2015}, {"title": "Training very deep networks", "author": ["Rupesh K Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "Advances in neural information processing systems. pages 2377\u20132385.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Inceptionv4, inception-resnet and the impact of residual connections on learning", "author": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke", "Alex Alemi."], "venue": "arXiv preprint arXiv:1602.07261 .", "citeRegEx": "Szegedy et al\\.,? 2016", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "ArXiv eprints, January .", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "A learning algorithm for continually running fully recurrent neural networks", "author": ["Ronald J Williams", "David Zipser."], "venue": "Neural computation 1(2):270\u2013280.", "citeRegEx": "Williams and Zipser.,? 1989", "shortCiteRegEx": "Williams and Zipser.", "year": 1989}, {"title": "Maximum entropy based phrase reordering model for statistical machine translation", "author": ["Deyi Xiong", "Qun Liu", "Shouxun Lin."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual", "citeRegEx": "Xiong et al\\.,? 2006", "shortCiteRegEx": "Xiong et al\\.", "year": 2006}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701 .", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Variational neural machine translation", "author": ["Biao Zhang", "Deyi Xiong", "Jinsong Su."], "venue": "arXiv preprint arXiv:1605.07869 .", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Exploiting source-side monolingual data in neural machine translation", "author": ["Jiajun Zhang", "Chengqing Zong."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Zhang and Zong.,? 2016", "shortCiteRegEx": "Zhang and Zong.", "year": 2016}, {"title": "Deep recurrent models with fastforward connections for neural machine translation", "author": ["Jie Zhou", "Ying Cao", "Xuguang Wang", "Peng Li", "Wei Xu."], "venue": "arXiv preprint arXiv:1606.04199 .", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015).", "startOffset": 160, "endOffset": 313}, {"referenceID": 20, "context": "Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015).", "startOffset": 160, "endOffset": 313}, {"referenceID": 28, "context": "Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015).", "startOffset": 160, "endOffset": 313}, {"referenceID": 24, "context": "Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015).", "startOffset": 160, "endOffset": 313}, {"referenceID": 29, "context": "Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015).", "startOffset": 160, "endOffset": 313}, {"referenceID": 9, "context": "Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015).", "startOffset": 160, "endOffset": 313}, {"referenceID": 15, "context": "Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015).", "startOffset": 160, "endOffset": 313}, {"referenceID": 11, "context": "Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text.", "startOffset": 66, "endOffset": 155}, {"referenceID": 2, "context": "Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text.", "startOffset": 66, "endOffset": 155}, {"referenceID": 12, "context": "Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text.", "startOffset": 66, "endOffset": 155}, {"referenceID": 26, "context": "Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text.", "startOffset": 66, "endOffset": 155}, {"referenceID": 16, "context": "Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text.", "startOffset": 66, "endOffset": 155}, {"referenceID": 22, "context": "Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 230, "endOffset": 277}, {"referenceID": 0, "context": "Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 230, "endOffset": 277}, {"referenceID": 6, "context": "Driven by the breakthrough achieved in computer vision (He et al., 2015; Srivastava et al., 2015), research in NMT has recently turned towards studying Deep Neural Networks (DNNs).", "startOffset": 55, "endOffset": 97}, {"referenceID": 21, "context": "Driven by the breakthrough achieved in computer vision (He et al., 2015; Srivastava et al., 2015), research in NMT has recently turned towards studying Deep Neural Networks (DNNs).", "startOffset": 55, "endOffset": 97}, {"referenceID": 19, "context": "One reasonable explanation is the notorious problem of vanishing/exploding gradients which was first studied in the context of vanilla RNNs (Pascanu et al., 2013b).", "startOffset": 140, "endOffset": 163}, {"referenceID": 6, "context": "Most prevalent approaches to solve this problem rely on short-cut connections between adjacent layers such as residual or fastforward connections (He et al., 2015; Srivastava et al., 2015; Zhou et al., 2016).", "startOffset": 146, "endOffset": 207}, {"referenceID": 21, "context": "Most prevalent approaches to solve this problem rely on short-cut connections between adjacent layers such as residual or fastforward connections (He et al., 2015; Srivastava et al., 2015; Zhou et al., 2016).", "startOffset": 146, "endOffset": 207}, {"referenceID": 30, "context": "Most prevalent approaches to solve this problem rely on short-cut connections between adjacent layers such as residual or fastforward connections (He et al., 2015; Srivastava et al., 2015; Zhou et al., 2016).", "startOffset": 146, "endOffset": 207}, {"referenceID": 6, "context": "Driven by the breakthrough achieved in computer vision (He et al., 2015; Srivastava et al., 2015), research in NMT has recently turned towards studying Deep Neural Networks (DNNs). Wu et al. (2016) and Zhou et al.", "startOffset": 56, "endOffset": 198}, {"referenceID": 6, "context": "Driven by the breakthrough achieved in computer vision (He et al., 2015; Srivastava et al., 2015), research in NMT has recently turned towards studying Deep Neural Networks (DNNs). Wu et al. (2016) and Zhou et al. (2016) found that deep architectures in both the encoder and decoder are essential for capturing subtle irregularities in the source and target languages.", "startOffset": 56, "endOffset": 221}, {"referenceID": 0, "context": "This is called automatic alignment (Bahdanau et al., 2014) or attention mechanism (Luong et al.", "startOffset": 35, "endOffset": 58}, {"referenceID": 13, "context": ", 2014) or attention mechanism (Luong et al., 2015), but it is essentially reading with content-based addressing defined in (Graves et al.", "startOffset": 31, "endOffset": 51}, {"referenceID": 5, "context": ", 2015), but it is essentially reading with content-based addressing defined in (Graves et al., 2014).", "startOffset": 80, "endOffset": 101}, {"referenceID": 6, "context": "In computer vision, models with more than 100 convolutional layers have outperformed shallow ones by a big margin on a series of image tasks (He et al., 2015; Srivastava et al., 2015).", "startOffset": 141, "endOffset": 183}, {"referenceID": 21, "context": "In computer vision, models with more than 100 convolutional layers have outperformed shallow ones by a big margin on a series of image tasks (He et al., 2015; Srivastava et al., 2015).", "startOffset": 141, "endOffset": 183}, {"referenceID": 6, "context": "In computer vision, models with more than 100 convolutional layers have outperformed shallow ones by a big margin on a series of image tasks (He et al., 2015; Srivastava et al., 2015). Following similar ideas of building deep CNNs, some promising improvements have also been achieved on building deep NMT systems. Zhou et al. (2016) proposed a new type of linear connections between adjacent layers to simplify the training of deeply stacked RNNs.", "startOffset": 142, "endOffset": 333}, {"referenceID": 6, "context": "In computer vision, models with more than 100 convolutional layers have outperformed shallow ones by a big margin on a series of image tasks (He et al., 2015; Srivastava et al., 2015). Following similar ideas of building deep CNNs, some promising improvements have also been achieved on building deep NMT systems. Zhou et al. (2016) proposed a new type of linear connections between adjacent layers to simplify the training of deeply stacked RNNs. Similarly, Wu et al. (2016) introduced residual connections to their deep neural machine translation system and achieve great improvements.", "startOffset": 142, "endOffset": 476}, {"referenceID": 25, "context": "A recurrent neural network (Williams and Zipser, 1989) is a class of neural network that has recurrent connections and a state (or its more sophisticated memory-like extension).", "startOffset": 27, "endOffset": 54}, {"referenceID": 8, "context": "The problem was explored in depth by (Hochreiter and Schmidhuber, 1997; Pascanu et al., 2013b).", "startOffset": 37, "endOffset": 94}, {"referenceID": 19, "context": "The problem was explored in depth by (Hochreiter and Schmidhuber, 1997; Pascanu et al., 2013b).", "startOffset": 37, "endOffset": 94}, {"referenceID": 8, "context": "There is a long thread of work aiming to solve this problem, with the long short-term memory units (LSTM) being the most salient examples and gated recurrent unit (GRU) being the most recent one (Hochreiter and Schmidhuber, 1997; Cho et al., 2014).", "startOffset": 195, "endOffset": 247}, {"referenceID": 3, "context": "There is a long thread of work aiming to solve this problem, with the long short-term memory units (LSTM) being the most salient examples and gated recurrent unit (GRU) being the most recent one (Hochreiter and Schmidhuber, 1997; Cho et al., 2014).", "startOffset": 195, "endOffset": 247}, {"referenceID": 30, "context": "Following Zhou et al. (2016), we choose another bidirectional approach to process the sequence in order to learn more temporal dependencies in this work.", "startOffset": 10, "endOffset": 29}, {"referenceID": 0, "context": "It is intuitively beneficial to exploit the information of yt\u22121 when reading from the source sentence representation, which is missing from the implementation of attention-based NMT in (Bahdanau et al., 2014).", "startOffset": 185, "endOffset": 208}, {"referenceID": 17, "context": "The evaluation metric is BLEU2 (Papineni et al., 2002).", "startOffset": 31, "endOffset": 54}, {"referenceID": 13, "context": "For English-German, to compare with the results reported by previous work (Luong et al., 2015; Zhou et al., 2016; Jean et al., 2015), we used the same subset of the WMT 2014 training corpus that contains 4.", "startOffset": 74, "endOffset": 132}, {"referenceID": 30, "context": "For English-German, to compare with the results reported by previous work (Luong et al., 2015; Zhou et al., 2016; Jean et al., 2015), we used the same subset of the WMT 2014 training corpus that contains 4.", "startOffset": 74, "endOffset": 132}, {"referenceID": 9, "context": "For English-German, to compare with the results reported by previous work (Luong et al., 2015; Zhou et al., 2016; Jean et al., 2015), we used the same subset of the WMT 2014 training corpus that contains 4.", "startOffset": 74, "endOffset": 132}, {"referenceID": 22, "context": "To compare with the results reported by previous work on end-toend NMT (Sutskever et al., 2014; Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2014; Zhou et al., 2016), we used the same subset of the WMT 2014 training corpus that contains 12M sentence pairs with 304M English words and 348M French words.", "startOffset": 71, "endOffset": 176}, {"referenceID": 0, "context": "To compare with the results reported by previous work on end-toend NMT (Sutskever et al., 2014; Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2014; Zhou et al., 2016), we used the same subset of the WMT 2014 training corpus that contains 12M sentence pairs with 304M English words and 348M French words.", "startOffset": 71, "endOffset": 176}, {"referenceID": 9, "context": "To compare with the results reported by previous work on end-toend NMT (Sutskever et al., 2014; Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2014; Zhou et al., 2016), we used the same subset of the WMT 2014 training corpus that contains 12M sentence pairs with 304M English words and 348M French words.", "startOffset": 71, "endOffset": 176}, {"referenceID": 14, "context": "To compare with the results reported by previous work on end-toend NMT (Sutskever et al., 2014; Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2014; Zhou et al., 2016), we used the same subset of the WMT 2014 training corpus that contains 12M sentence pairs with 304M English words and 348M French words.", "startOffset": 71, "endOffset": 176}, {"referenceID": 30, "context": "To compare with the results reported by previous work on end-toend NMT (Sutskever et al., 2014; Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2014; Zhou et al., 2016), we used the same subset of the WMT 2014 training corpus that contains 12M sentence pairs with 304M English words and 348M French words.", "startOffset": 71, "endOffset": 176}, {"referenceID": 0, "context": "Our training procedure and hyper parameter choices are similar to those used by (Bahdanau et al., 2014).", "startOffset": 80, "endOffset": 103}, {"referenceID": 18, "context": "To avoid gradient explosion, the gradients of the cost function which had `2 norm larger than a predefined threshold \u03c4 were normalized to the threshold (Pascanu et al., 2013a).", "startOffset": 152, "endOffset": 175}, {"referenceID": 1, "context": "We compare our NMT systems with various other systems including the winning system in WMT14 (Buck et al., 2014), a phrase-based system whose language models were trained on a huge monolingual text, the Common Crawl corpus.", "startOffset": 92, "endOffset": 111}, {"referenceID": 1, "context": "We compare our NMT systems with various other systems including the winning system in WMT14 (Buck et al., 2014), a phrase-based system whose language models were trained on a huge monolingual text, the Common Crawl corpus. For end-toend NMT systems, to the best of our knowledge, Wu et al. (2016) is currently the SOTA system and about 4 BLEU points on top of previously best reported results even though Zhou et al.", "startOffset": 93, "endOffset": 297}, {"referenceID": 1, "context": "We compare our NMT systems with various other systems including the winning system in WMT14 (Buck et al., 2014), a phrase-based system whose language models were trained on a huge monolingual text, the Common Crawl corpus. For end-toend NMT systems, to the best of our knowledge, Wu et al. (2016) is currently the SOTA system and about 4 BLEU points on top of previously best reported results even though Zhou et al. (2016) used a much deeper neural network4.", "startOffset": 93, "endOffset": 424}, {"referenceID": 30, "context": "It is also worth mentioning that the result reported by Zhou et al. (2016) does not include PosUnk, and this comparison is not fair enough.", "startOffset": 56, "endOffset": 75}, {"referenceID": 14, "context": "Enc-Dec (Luong et al., 2014) 30.", "startOffset": 8, "endOffset": 28}, {"referenceID": 0, "context": "RNNsearch (Bahdanau et al., 2014) 28.", "startOffset": 10, "endOffset": 33}, {"referenceID": 9, "context": "5 RNNsearch-LV (Jean et al., 2015) 32.", "startOffset": 15, "endOffset": 34}, {"referenceID": 30, "context": "7 Deep-Att (Zhou et al., 2016) 35.", "startOffset": 11, "endOffset": 30}, {"referenceID": 13, "context": "Luong et al. (2014) achieves BLEU score of 30.", "startOffset": 0, "endOffset": 20}, {"referenceID": 13, "context": "Luong et al. (2014) achieves BLEU score of 30.4 with a six layers deep Encoder-Decoder model. The two attention models, RNNSearch and RNNsearch-LV achieve BLEU scores of 28.5 and 32.7 respectively. The previous best single NMT Deep-Att model with an 18 layers encoder and 7 layers decoder achieves BLEU score of 35.9. For DEEPLAU, we obtain the BLEU score of 35.1 with a 4 layers encoder and 4 layers decoder, which is on par with the SOTA system in terms of BLEU. Note that Zhou et al. (2016) utilize a much larger depth as well as external alignment model and extensive regularization to achieve their best results.", "startOffset": 0, "endOffset": 494}, {"referenceID": 23, "context": "\u2022 Residual Networks (ResNet) are among the pioneering works (Szegedy et al., 2016; He et al., 2016) that utilize extra identity connections to enhance information flow such that very deep neural networks can be effectively optimized.", "startOffset": 60, "endOffset": 99}, {"referenceID": 7, "context": "\u2022 Residual Networks (ResNet) are among the pioneering works (Szegedy et al., 2016; He et al., 2016) that utilize extra identity connections to enhance information flow such that very deep neural networks can be effectively optimized.", "startOffset": 60, "endOffset": 99}, {"referenceID": 6, "context": ", 2016; He et al., 2016) that utilize extra identity connections to enhance information flow such that very deep neural networks can be effectively optimized. Share the similar idea, Wu et al. (2016) introduced to leverage residual connections to train deep RNNs.", "startOffset": 8, "endOffset": 200}, {"referenceID": 30, "context": "\u2022 Fast Forward (F-F) connections were proposed to reduce the propagation path length which is the pioneer work to simplify the training of deep NMT model (Zhou et al., 2016).", "startOffset": 154, "endOffset": 173}], "year": 2017, "abstractText": "Deep Neural Networks (DNNs) have provably enhanced the state-of-the-art Neural Machine Translation (NMT) with their capability in modeling complex functions and capturing complex linguistic structures. However NMT systems with deep architecture in their encoder or decoder RNNs often suffer from severe gradient diffusion due to the non-linear recurrent activations, which often make the optimization much more difficult. To address this problem we propose novel linear associative units (LAU) to reduce the gradient propagation length inside the recurrent unit. Different from conventional approaches (LSTM unit and GRU), LAUs utilizes linear associative connections between input and output of the recurrent unit, which allows unimpeded information flow through both space and time direction. The model is quite simple, but it is surprisingly effective. Our empirical study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art.", "creator": "LaTeX with hyperref package"}}}