{"id": "1705.01020", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2017", "title": "Modeling Source Syntax for Neural Machine Translation", "abstract": "Even though a linguistics-free sequence to sequence model in neural machine translation (NMT) has certain capability of implicitly learning syntactic information of source sentences, this paper shows that source syntax can be explicitly incorporated into NMT effectively to provide further improvements. Specifically, we linearize parse trees of source sentences to obtain structural label sequences. On the basis, we propose three different sorts of encoders to incorporate source syntax into NMT: 1) Parallel RNN encoder that learns word and label annotation vectors parallelly; 2) Hierarchical RNN encoder that learns word and label annotation vectors in a two-level hierarchy; and 3) Mixed RNN encoder that stitchingly learns word and label annotation vectors over sequences where words and labels are mixed. Experimentation on Chinese-to-English translation demonstrates that all the three proposed syntactic encoders are able to improve translation accuracy. It is interesting to note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best performance with an significant improvement of 1.4 BLEU points. Moreover, an in-depth analysis from several perspectives is provided to reveal how source syntax benefits NMT.", "histories": [["v1", "Tue, 2 May 2017 15:21:46 GMT  (785kb,D)", "http://arxiv.org/abs/1705.01020v1", "Accepted by ACL 2017"]], "COMMENTS": "Accepted by ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["junhui li", "deyi xiong", "zhaopeng tu", "muhua zhu", "min zhang", "guodong zhou"], "accepted": true, "id": "1705.01020"}, "pdf": {"name": "1705.01020.pdf", "metadata": {"source": "CRF", "title": "Modeling Source Syntax for Neural Machine Translation", "authors": ["Junhui Li", "Deyi Xiong", "Zhaopeng Tu", "Muhua Zhu", "Min Zhang", "Guodong Zhou"], "emails": ["gdzhou}@suda.edu.cn", "tuzhaopeng@gmail.com,", "muhuazhu@tencent.com"], "sections": [{"heading": "1 Introduction", "text": "The fact is that it will be able to retaliate, to retaliate, \"he says.\" It's the way it is, \"he says.\" It's the way it is, \"he says.\" But it's the way it is, \"he says."}, {"heading": "2 Attention-based NMT", "text": "As background and starting line, we briefly describe in this section the NMT model with an attention mechanism by Bahdanau et al. (2015), which mainly consists of an encoder and a decoder, as shown in Figure 2. Encoding a source set is done by manually examining 200 randomly translated noun phrases: one reads an input sequence x = (x1,..., xm) from left to right and outputs a forward sequence of hidden states (\u2212 \u2192 h1,..., \u2212 hm), while the other uses recursive neuronal networks (referred to as bi-RNN) and outputs a reverse sequence (\u2190 \u2212 h1,..., \u2212 hm). Each source word is represented as hj (also as a hidden word \u2212 hm)."}, {"heading": "3 NMT with Source Syntax", "text": "Traditional NMT models treat a sentence as a sequence of words, ignoring external knowledge and failing to effectively capture various types of inherent structure of the sentence. To make external knowledge, especially the syntax on the source side, usable, we focus on the parse tree of a sentence and propose three different NMT models that explicitly include the syntactic structure in the encoding. Our goal is to convey to the NMT model the structural context of each word in its corresponding parse tree, with the goal that the learned annotation vectors (h1,..., hm) encode not only the information of words and their environment, but also the structural context in the parse tree. In the rest of this section, we use English sentences as examples to explain our methods."}, {"heading": "3.1 Syntax Representation", "text": "In order to obtain the structural context of a word in its parse tree, the model should ideally not only capture and remember the entire parse tree structure, but also distinguish the contexts of two different words. Given the lack of efficient ways to model structural information directly, an alternative way is to linearize the parse tree into a sequence of structural labels and to learn the structural context through the sequence of structural labels. Figure 3 (c) shows the structural labeling sequence of Figure 3 (b) in a simple way according to a depth traverse order. Note, however, that the linearization of a parse tree in a depth traverse order in a sequence of structural labels is also presented in the most recent advances in the field of neural syntactic parsing (Vinyals et al, 2015; Choe and Charniak, 2016), the linearized labeling sequence of a parse tree in a structural sequence is first traversed in a structural sequence."}, {"heading": "3.2 RNN Encoders with Source Syntax", "text": "In the second half of the year, which marks the first three months of the new millennium in the US and the United States, there will be a further tightening of monetary policy in the US in the second half of the year."}, {"heading": "3.3 Comparison of RNN Encoders with Source Syntax", "text": "Although all three encoders model both the word sequence and the structural label sequence, the differences in their respective model architecture are related to the degree of coupling of the two sequences: \u2022 In the Parallel RNN encoder, the word RNN and the structure designation RNN operate in parallel, i.e. the error signal propagated back from the word sequence would have no direct effect on the structure label RNN and vice versa. In contrast, the Hierarchical RNN encoder closely links the structural label sequence and word sequence. Therefore, the degree of coupling of the word and the structure label vectors and thus the structure label sequences in these three encoders has a direct impact."}, {"heading": "4 Experimentation", "text": "We have presented our approaches to integrating source syntax into NMT encoders and evaluate their effectiveness in translating from Chinese to English in this section."}, {"heading": "4.1 Experimental Settings", "text": "This year, it will be able to introduce the aforementioned brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated braingeeenlcsrteeaeFl."}, {"heading": "4.2 Experiment Results", "text": "The three proposed models lead to new parameters in different ways. As a base model, RNNSearch has 60.6M parameters. Due to the similarity with the infrastructure, the parallel RNN system and the hierarchical RNN system lead to an introduction of 8https: / / github.com / nyu-thub.com / dl4mt-tutorialthe similar size of additional parameters, resulting from the RNN model introduce7https: / / github.com / cdec 8https: / github.com / nyu-tutorialthe."}, {"heading": "5 Analysis", "text": "As the proposed mixed RNN system performs best, we will continue to look at the RNNSearch system and the mixed RNN system to learn more about how syntactical information helps with translation."}, {"heading": "5.1 Effects on Long Sentences", "text": "Following Bahdanau et al. (2015), we group sentences of similar length and calculate BLEU values. Figure 6 shows the BLEU values over different lengths of input sentences. It shows that the mixed RNN system outperforms RNSearch over sentences of different lengths. It also shows that performance decreases significantly when the length of input sentences increases. This performance trend over length is in line with the results in (Cho et al., 2014a; Tu et al., 2016, 2017a). We also observe that NMT systems perform surprisingly poorly when sentences exceed 50 length, especially when compared to the performance of the SMT system (i.e. the Cdecs). We think that the bad behavior of NMT systems towards long sentences (e.g. a length of 50) is due to the following two reasons: (1) the maximum length of source sentences is set at 50 in training, whereby the learned models are unwilling to overset the sentences over the maximum length of the sentences (MT); (prematurely)"}, {"heading": "5.2 Analysis on Word Alignment", "text": "To test this hypothesis, we conduct word alignment task experiments on the Liu and Sun (2015) evaluation dataset, which contains 900 manually aligned Chinese-English sentence pairs. To obtain automatic alignments between input sentences and their reference translations, we force the decoder to output reference translations. To evaluate the alignment error rate (AER) (Och and Ney, 2003), we report in Table 2.Table 2 that the source syntax information improves the attention model as expected by maintaining an annotation vector that summarizes structural information for each source word."}, {"heading": "5.3 Analysis on Phrase Alignment", "text": "The above subsection examines word alignment performance at the word level. In this subsection, we turn to phrase alignment analysis by moving from word unit to phrase unit. In the face of a source phrase XP, we use word alignment to check whether the phrase is translated continuously (con.) or not translated at all (un.) Although the maximum source length limit in the mixed RNN system is set at 150, it contains about 50 words at maximum length (dis.) or if it is not translated at all (un.) There are some phrases, such as noun sentences (nPs), prepositional phrases (PPs), which we normally expect to be translated continuously. In relation to several such phrases, Table 3 shows how these phrases are translated. It is clear from the table that translations of the RNNSearch system do not particularly well respect source syntax. For example, in RNSearch translations,% 53.6% of the NNN system also sets after translations, and% 33.6% of the NPs are translated continuously."}, {"heading": "5.4 Analysis on Over Translation", "text": "To estimate the translation generated by NMT, we propose a ratio of translation (RED): RED = \u2211 wi t (wi) | w | (1), where | w | is the number of eligible words, t (wi) is the time of translation for word wi. Given a word w and its translation e = e1e2... en, we have: t (w) = | e | \u2212 | uniq (e) | (2), where | e | is the number of words in w's translation e, while | uniq (e) | is the number of unique words in e. For example, when a source word is translated as hong kong hong kong kong, we say that it is translated twice. Table 4 presents ROT grouped by some typical POS tags. It is not surprising that RNNSearch system has a high RED in terms of POS tags of NR (ordinary noun) and CD (cardinal number)."}, {"heading": "5.5 Analysis on Rare Word Translation", "text": "Given a rare word w, we examine whether it is translated into a non-UNK word (Non-UNK), UNK (UNK) or not at all (Un.).Table 5 shows how rare words are translated on the source page. The four POS tags listed in the table account for about 90% of all rare words on the target page. It shows that in the mixed RNN system, rather rare words on the source page are translated into UNK on the target page. Note that our approach is compatible with open vocabulary approaches. Taking the sub-vocabulary approach (Sennrich et al., 2016) as an example, it is difficult to get its correct non-UNK translation when a rare word on the source page is substituted as UNK, our approach is compatible with approaches to open vocabularies (Sennrich et al., 2016) when a rare word on the source page is divided into multiple word units (VB)."}, {"heading": "6 Related Work", "text": "In general, NMT can provide a flexible mechanism for adding linguistic knowledge, thanks to its strong ability to represent automatically learnable features. Eriguchi et al. (2016), however, propose a tree equation model that learns annotation vectors not only for terminal words, but also for non-terminal nodes. They also allow the attention model to align target words with non-terminal nodes. Our approach is similar to theirs by using source-side phrase parse tree. However, our mixed RNN system incorporates syntax information stylistically by learning annotation vectors of syntactic labels and words, but is still a sequence model, with no additional parameters and less training time. Sennrich and Haddow (2016) define some linguistically motivated features attached to each word."}, {"heading": "7 Conclusion", "text": "In this paper, we have investigated whether and how the source syntax NMT can explicitly help improve its translation accuracy. To obtain syntactic knowledge, we linear a parse tree into a structural label sequence and let the model automatically learn useful information from it. Specifically, we have described three different models to capture syntax knowledge, i.e. parallel RNN, hierarchical RNN and mixed RNN. Experiments with Chinese-English translation show that all proposed models bring improvements over a state-of-the-art base system NMT. It is also interesting that the simplest model (i.e. mixed RNN) achieves the best performance, resulting in significant improvements of 1.4 BLEU points for NIST MT 02 to 05. In this paper, we have also analyzed the translation behavior of our improved system against the state-of-the-art NMT base system. Our analysis shows that there is still much scope for NT synchronization of future control functions."}, {"heading": "Acknowledgments", "text": "The authors thank three anonymous reviewers for their helpful comments and also thank Xing Wang, Xiangyu Duan, Zhengxian Gong for useful discussions. This work was supported by the National Natural Science Foundation of China (grant no. 61525205, 61331011, 61401295)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of ICLR 2015.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "Computational Linguistics 33(2):201\u2013228.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "On the properties of neural machinetranslation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proceedings of SSST 2014. pages 103\u2013111.", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings", "citeRegEx": "Cho et al\\.,? 2014b", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Parsing as language modeling", "author": ["Do Kook Choe", "Eugene Charniak."], "venue": "Proceedings of EMNLP 2016. pages 2331\u20132336.", "citeRegEx": "Choe and Charniak.,? 2016", "shortCiteRegEx": "Choe and Charniak.", "year": 2016}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models", "author": ["Chris Dyer", "Adam Lopez", "Juri Ganitkevitch", "Jonathan Weese", "Ferhan Ture", "Phil Blunsom", "Hendra Setiawan", "Vladimir Eidelman", "Philip Resnik"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "Tree-to-sequence attentional neural machine translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Proceedings of ACL 2016. pages 823\u2013833.", "citeRegEx": "Eriguchi et al\\.,? 2016", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "Factored neural machine translation", "author": ["Mercedes Garc\u0131\u0301a-Mart\u0131\u0301nez", "Loic Barrault", "Fethi Bougares"], "venue": null, "citeRegEx": "Garc\u0131\u0301a.Mart\u0131\u0301nez et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Mart\u0131\u0301nez et al\\.", "year": 2016}, {"title": "Improving neural networks", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Montreal neural machine translation systems for wmt\u201915", "author": ["S\u00e9bastien Jean", "Orhan Firat", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of WMT 2015. pages 134\u2013140.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "Proceedings of EMNLP 2004. pages 388\u2013395.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Modeling syntactic and semantic structures in hierarchical phrase-based translation", "author": ["Junhui Li", "Philip Resnik", "Hal Daum\u00e9 III."], "venue": "Proceedings of HLT-NAACL 2013. pages 540\u2013549.", "citeRegEx": "Li et al\\.,? 2013", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Treeto-string alignment template for statistical machine translation", "author": ["Yang Liu", "Qun Liu", "Shouxun Lin."], "venue": "Proceedings of ACL-COLING 2006. pages 609\u2013616.", "citeRegEx": "Liu et al\\.,? 2006", "shortCiteRegEx": "Liu et al\\.", "year": 2006}, {"title": "Contrastive unsupervised word alignment with non-local features", "author": ["Yang Liu", "Maosong Sun."], "venue": "Proceedings of AAAI 2015. pages 857\u2013868.", "citeRegEx": "Liu and Sun.,? 2015", "shortCiteRegEx": "Liu and Sun.", "year": 2015}, {"title": "Stanford neural machine translation systems for spoken language domains", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "Proceedings of IWSLT 2015. pages 76\u201379.", "citeRegEx": "Luong and Manning.,? 2015", "shortCiteRegEx": "Luong and Manning.", "year": 2015}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of EMNLP 2015. pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Soft syntactic constraints for hierarchical phrased-based translation", "author": ["Yuval Marton", "Philip Resnik."], "venue": "Proceedings of ACL-HLT 2008. pages 1003\u20131011.", "citeRegEx": "Marton and Resnik.,? 2008", "shortCiteRegEx": "Marton and Resnik.", "year": 2008}, {"title": "Supervised attentions for neural machine translation", "author": ["Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah."], "venue": "Proceedings of EMNLP 2016. pages 2283\u20132288.", "citeRegEx": "Mi et al\\.,? 2016", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Franz J. Och", "Hermann Ney."], "venue": "Computational Linguistics 29(1):19\u201351.", "citeRegEx": "Och and Ney.,? 2003", "shortCiteRegEx": "Och and Ney.", "year": 2003}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of ACL 2002. pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Improved inference for unlexicalized parsing", "author": ["Slav Petrov", "Dan Klein."], "venue": "Proceedings of HLTNAACL 2007. pages 404\u2013411.", "citeRegEx": "Petrov and Klein.,? 2007", "shortCiteRegEx": "Petrov and Klein.", "year": 2007}, {"title": "Linguistic input features improve neural machine translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "Proceedings of the First Conference on Machine Translation. pages 83\u201391.", "citeRegEx": "Sennrich and Haddow.,? 2016", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of ACL 2016. pages 1715\u20131725.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "A new string-to-dependency machine translation algorithm with a target dependency language model", "author": ["Libin Shen", "Jinxi Xu", "Ralph Weischedel."], "venue": "Proceedings of ACL-HLT 2008. pages 577\u2013585.", "citeRegEx": "Shen et al\\.,? 2008", "shortCiteRegEx": "Shen et al\\.", "year": 2008}, {"title": "Does string-based neural MT learn source syntax? In Proceedings of EMNLP 2016", "author": ["Xing Shi", "Inkit Padhi", "Kevin Knight."], "venue": "pages 1526\u20131534.", "citeRegEx": "Shi et al\\.,? 2016", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "Context gates for neural machine translation", "author": ["Zhaopeng Tu", "Yang Liu", "Zhengdong Lu", "Xiaohua Liu", "Hang Li."], "venue": "Transactions of the Association for Computational Linguistics 5:87\u201399.", "citeRegEx": "Tu et al\\.,? 2017a", "shortCiteRegEx": "Tu et al\\.", "year": 2017}, {"title": "Neural machine translation with reconstruction", "author": ["Zhaopeng Tu", "Yang Liu", "Lifeng Shang", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of AAAI 2017. pages 3097\u20133103.", "citeRegEx": "Tu et al\\.,? 2017b", "shortCiteRegEx": "Tu et al\\.", "year": 2017}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of ACL 2016. pages 76\u201385.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Proceedings of NIPS 2015.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Neural machine translation advised by statistical machine translation", "author": ["Xing Wang", "Zhengdong Lu", "Zhaopeng Tu", "Hang Li", "Deyi Xiong", "Min Zhang."], "venue": "Proceedings of AAAI 2017. pages 3330\u2013 3336.", "citeRegEx": "Wang et al\\.,? 2017", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "The Penn Chinese Treebank: Phrase structure annotation of a large corpus", "author": ["Nianwen Xue", "Fei Xia", "Fu-Dong Chiou", "Martha Palmer."], "venue": "Natural Language Engineering 11(2):207\u2013238.", "citeRegEx": "Xue et al\\.,? 2005", "shortCiteRegEx": "Xue et al\\.", "year": 2005}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015).", "startOffset": 26, "endOffset": 113}, {"referenceID": 9, "context": "on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015).", "startOffset": 26, "endOffset": 113}, {"referenceID": 15, "context": "on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015).", "startOffset": 26, "endOffset": 113}, {"referenceID": 14, "context": "on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015).", "startOffset": 26, "endOffset": 113}, {"referenceID": 0, "context": "on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015). However, Shi et al. (2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus.", "startOffset": 27, "endOffset": 142}, {"referenceID": 11, "context": ", 2008; Li et al., 2013). While it is yet to be seen how syntax can benefit NMT effectively, we find that translations of NMT sometimes fail to well respect source syntax. Figure 1 (a) shows a Chinese-to-English translation example of NMT. In this example, the NMT seq2seq model incorrectly translates the Chinese noun phrase (i.e., \u65b0 \u751f/xinsheng \u94f6\u884c/yinhang) into a discontinuous phrase in English (i.e., new ... bank) due to the failure of capturing the internal syntactic structure in the input Chinese sentence. Statistics on our development set show that one forth of Chinese noun phrases are translated into discontinuous phrases in English, indicating the substantial disrespect of syntax in NMT translation.1 Figure 1 (b) shows another example with over translation, where the noun phrase \u4e24/liang \u4e2a/ge \u5973\u5b69/nvhai is translated twice in English. Similar to discontinuous translation, over translation usually happens along with the disrespect of syntax which results in the repeated translation of the same source words in multiple positions of the target sentence. In this paper we are not aiming at solving any particular issue, either the discontinuous translation or the over translation. Alternatively, we address how to incorporate explicitly the source syntax to improve the NMT translation accuracy with the expectation of alleviating the issues above in general. Specifically, rather than directly assigning each source word with manually designed syntactic labels, as Sennrich and Haddow (2016) do, we linearize a phrase parse tree into a structural label sequence and let the model automatically learn useful syntactic information.", "startOffset": 8, "endOffset": 1508}, {"referenceID": 0, "context": "As a background and a baseline, in this section, we briefly describe the NMT model with an attention mechanism by Bahdanau et al. (2015), which mainly consists of an encoder and a decoder, as shown in Figure 2.", "startOffset": 114, "endOffset": 137}, {"referenceID": 28, "context": "Note that linearizing a parse tree in a depth-first traversal order into a sequence of structural labels has also been widely adopted in recent advances in neural syntactic parsing (Vinyals et al., 2015; Choe and Charniak, 2016), suggesting that the linearized sequence can be viewed as an alternative to its tree structure.", "startOffset": 181, "endOffset": 228}, {"referenceID": 4, "context": "Note that linearizing a parse tree in a depth-first traversal order into a sequence of structural labels has also been widely adopted in recent advances in neural syntactic parsing (Vinyals et al., 2015; Choe and Charniak, 2016), suggesting that the linearized sequence can be viewed as an alternative to its tree structure.", "startOffset": 181, "endOffset": 228}, {"referenceID": 20, "context": "5 To get the source syntax for sentences on the source-side, we parse the Chinese sentences with Berkeley Parser 6 (Petrov and Klein, 2007) trained on Chinese TreeBank 7.", "startOffset": 115, "endOffset": 139}, {"referenceID": 30, "context": "0 (Xue et al., 2005).", "startOffset": 2, "endOffset": 20}, {"referenceID": 19, "context": "We use the case insensitive 4-gram NIST BLEU score (Papineni et al., 2002) for the translation task.", "startOffset": 51, "endOffset": 74}, {"referenceID": 0, "context": "All the other settings are the same as in Bahdanau et al.(2015).", "startOffset": 42, "endOffset": 64}, {"referenceID": 5, "context": "\u2022 cdec (Dyer et al., 2010): an open source hierarchical phrase-based SMT system (Chiang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data.", "startOffset": 7, "endOffset": 26}, {"referenceID": 1, "context": ", 2010): an open source hierarchical phrase-based SMT system (Chiang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data.", "startOffset": 61, "endOffset": 75}, {"referenceID": 0, "context": "\u2022 RNNSearch: a re-implementation of the attentional NMT system (Bahdanau et al., 2015) with slight changes taken from dl4mt tutorial.", "startOffset": 63, "endOffset": 86}, {"referenceID": 3, "context": "8 For the activation function f of an RNN, RNNSearch uses the gated recurrent unit (GRU) recently proposed by (Cho et al., 2014b).", "startOffset": 110, "endOffset": 129}, {"referenceID": 8, "context": "It incorporates dropout (Hinton et al., 2012) on the output layer and improves the attention model by feeding the lastly generated word.", "startOffset": 24, "endOffset": 45}, {"referenceID": 31, "context": "We use AdaDelta (Zeiler, 2012) to optimize model parameters in training with the mini-batch size of 80.", "startOffset": 16, "endOffset": 30}, {"referenceID": 10, "context": "01, tested by bootstrap resampling (Koehn, 2004).", "startOffset": 35, "endOffset": 48}, {"referenceID": 17, "context": "This is very consistent with other studies on Chinese-to-English translation (Mi et al., 2016; Tu et al., 2017b; Wang et al., 2017).", "startOffset": 77, "endOffset": 131}, {"referenceID": 26, "context": "This is very consistent with other studies on Chinese-to-English translation (Mi et al., 2016; Tu et al., 2017b; Wang et al., 2017).", "startOffset": 77, "endOffset": 131}, {"referenceID": 29, "context": "This is very consistent with other studies on Chinese-to-English translation (Mi et al., 2016; Tu et al., 2017b; Wang et al., 2017).", "startOffset": 77, "endOffset": 131}, {"referenceID": 0, "context": "Following Bahdanau et al. (2015), we group sentences of similar lengths together and compute BLEU scores.", "startOffset": 10, "endOffset": 33}, {"referenceID": 2, "context": "This performance trend over the length is consistent with the findings in (Cho et al., 2014a; Tu et al., 2016, 2017a).", "startOffset": 74, "endOffset": 117}, {"referenceID": 18, "context": "To evaluate alignment performance, we report the alignment error rate (AER) (Och and Ney, 2003) in Table 2.", "startOffset": 76, "endOffset": 95}, {"referenceID": 13, "context": "To test this hypothesis, we carry out experiments of the word alignment task on the evaluation dataset from Liu and Sun (2015), which contains 900 manually aligned Chinese-English sentence pairs.", "startOffset": 108, "endOffset": 127}, {"referenceID": 22, "context": "word approach (Sennrich et al., 2016) as an example, for a word on the source side which is divided into several subword units, we can synthesize subPOS nodes that cover these units.", "startOffset": 14, "endOffset": 37}, {"referenceID": 6, "context": "Eriguchi et al. (2016) propose a tree-tosequence model that learns annotation vectors not only for terminal words, but also for non-terminal nodes.", "startOffset": 0, "endOffset": 23}, {"referenceID": 6, "context": "Eriguchi et al. (2016) propose a tree-tosequence model that learns annotation vectors not only for terminal words, but also for non-terminal nodes. They also allow the attention model to align target words to non-terminal nodes. Our approach is similar to theirs by using source-side phrase parse tree. However, our Mixed RNN system, for example, incorporates syntax information by learning annotation vectors of syntactic labels and words stitchingly, but is still a sequenceto-sequence model, with no extra parameters and with less increased training time. Sennrich and Haddow (2016) define a few linguistically motivated features that are attached to each individual words.", "startOffset": 0, "endOffset": 586}], "year": 2017, "abstractText": "Even though a linguistics-free sequence to sequence model in neural machine translation (NMT) has certain capability of implicitly learning syntactic information of source sentences, this paper shows that source syntax can be explicitly incorporated into NMT effectively to provide further improvements. Specifically, we linearize parse trees of source sentences to obtain structural label sequences. On the basis, we propose three different sorts of encoders to incorporate source syntax into NMT: 1) Parallel RNN encoder that learns word and label annotation vectors parallelly; 2) Hierarchical RNN encoder that learns word and label annotation vectors in a two-level hierarchy; and 3) Mixed RNN encoder that stitchingly learns word and label annotation vectors over sequences where words and labels are mixed. Experimentation on Chinese-to-English translation demonstrates that all the three proposed syntactic encoders are able to improve translation accuracy. It is interesting to note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best performance with an significant improvement of 1.4 BLEU points. Moreover, an in-depth analysis from several perspectives is provided to reveal how source syntax benefits NMT.", "creator": "LaTeX with hyperref package"}}}