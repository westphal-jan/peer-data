{"id": "1705.01359", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2017", "title": "FOIL it! Find One mismatch between Image and Language caption", "abstract": "In this paper, we aim to understand whether current language and vision (LaVi) models truly grasp the interaction between the two modalities. To this end, we propose an extension of the MSCOCO dataset, FOIL-COCO, which associates images with both correct and \"foil\" captions, that is, descriptions of the image that are highly similar to the original ones, but contain one single mistake (\"foil word\"). We show that current LaVi models fall into the traps of this data and perform badly on three tasks: a) caption classification (correct vs. foil); b) foil word detection; c) foil word correction. Humans, in contrast, have near-perfect performance on those tasks. We demonstrate that merely utilising language cues is not enough to model FOIL-COCO and that it challenges the state-of-the-art by requiring a fine-grained understanding of the relation between text and image.", "histories": [["v1", "Wed, 3 May 2017 11:07:13 GMT  (2478kb,D)", "http://arxiv.org/abs/1705.01359v1", "To appear at ACL 2017"]], "COMMENTS": "To appear at ACL 2017", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.MM", "authors": ["ravi shekhar", "sandro pezzelle", "yauhen klimovich", "aur\u00e9lie herbelot", "moin nabi", "enver sangineto", "raffaella bernardi"], "accepted": true, "id": "1705.01359"}, "pdf": {"name": "1705.01359.pdf", "metadata": {"source": "CRF", "title": "FOIL it! Find One mismatch between Image and Language caption", "authors": ["Ravi Shekhar", "Sandro Pezzelle", "Yauhen Klimovich", "Aur\u00e9lie Herbelot", "Moin Nabi", "Enver Sangineto", "Raffaella Bernardi"], "emails": ["firstname.lastname@unitn.it"], "sections": [{"heading": "1 Introduction", "text": "Most human language comprehension is based on perception and what people actually learn. There is a sense of community that is communicated in NLP and AI, so there is a growing interest in combining information from language and vision in NLP and in AI in 2016. (2016) The primary test sets of language and vision (LaVi) models have therefore been called \"Visual Question Answering\" (VQA) (e.g. Antol et al. (2015); Malinowski et al. (2014); Malinowski et al. (2015); Gao et al. (2015); Ren et al. (2015) and \"Image Captioning\" (IC) (e.g. Hodosh et al. (2013); Fang et al. (2015); Chen and Lawrence Zitnick. (2015); Karpathy et al. (2015); Vinyals et al. (2015)). While some models have been extremely successful at these tasks, it remains unclear how the learners will be interpreted and what will actually learn."}, {"heading": "2 Related Work", "text": "In IC (Fang et al., 2015; Chen and Lawrence Zitnick, 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; 1The dataset is available from https: / / foilunitn. github.io / Wang et al., 2016), the goal is to generate a caption for a particular image so that it is both semantically and syntactically correct and correctly describes the content of that image. In VQA et al., 2015; Malinowski et al., 2015; Ren et al., 2015; the system attempts to answer open questions about the content of the image."}, {"heading": "3 Dataset", "text": "In this section we describe how to automatically generate FOIL COCO datapoints."}, {"heading": "4 Experiments and Results", "text": "We perform three tasks, which are presented below: Task 1 (T1): Correction vs. Slide Classification On the basis of an image and a caption, the model is asked to mark whether the caption is correct or incorrect. The goal is to understand whether LaVi models can detect discrepancies between their rough representations of language and visual input. Task 2 (T2): Slide word recognition In order to systematically check the performance of the system on the basis of different prior information, we test two different settings: The slide must be selected under (a) only the nouns or (b) all the substantive words in the caption. Task 3 (T3): Slide word correction In order to systematically check an image, a slide label and the slide word, the model must recognize the slide and make the correction.The goal is to check whether the visual representation of the system is good enough to allow us to select the required words from the models to correct the overall form."}, {"heading": "4.1 Models", "text": "We evaluate both VQA and IC models against our tasks. For the former, we use two of the three models that are evaluated in (Goyal et al., 2016a) against a balanced VQA dataset. For the latter, we use the multimodal bidirectional LSTM proposed in (Wang et al., 2016) and adapted for our tasks. LSTM + norm I: We use the best functioning VQA model in (Antol et al., 2015) (deeper LSTM + norm I). This model uses a two-dimensional LongShort Term Memory (LSTM) to encode the questions and the last fully connected layer of VGNet images. Both image embedding and image embedding are projected into a 1024-dimensional feature space. Subsequently (Antol et al., 2015) we normalized the image function before we project them."}, {"heading": "4.2 Results", "text": "As shown in Table 2, the FOIL-COCO dataset is challenging. On T1, for which the probability level is 50.00%, the \"blind\" linguistic-only model performs poorly with an accuracy of 55.62% (25.04% for slide labels), showing that the language bias is minimal. By adding visual information, CNN + LSTM, the overall accuracy increases by 5.45% (7.94% for slide labels.) and reaches 61.07% (or 32.98%). Both SoA-VQA and IC models perform significantly worse than humans on T1 and T2. VQA systems exhibit a strong propensity for correct slide labels and poor overall performance. They identify only 34.51% (LSTM + Norm I) and 36.38% (HieCoAtt) of the wrong slide labels (T1) and T2. On the other hand, the overall VA model shows a propensity for and poor slide labeling."}, {"heading": "5 Analysis", "text": "In fact, it is so that most of them are able to survive themselves, and that they do not. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "6 Conclusion", "text": "We have introduced FOIL-COCO, a large data set of images associated with both correct and foil labels. Error production is automatically generated but carefully thought through, making the task of foil detection particularly challenging. By linking the data set to a series of tasks, we enable the diagnosis of various errors of current LaVi systems, from their rough understanding of the correspondence between text and vision to their understanding of language and image structure. Our hypothesis is that systems that, like humans, deeply integrate language and visual modalities should easily recognize foil labels. The SoA-LaVi models we tested fall through this test, implying that they do not integrate the two modalities. To complete the analysis of these results, we plan to perform another task, which is to ask the system to detect the area in the image that generates a discrepancy with the foil word (the small red box that produces the bird)."}, {"heading": "Acknowledgments", "text": "We are grateful to the Erasmus Mundus European Master in Language and Communication Technologies (EM LCT) for the scholarship for the third author. We also thank NVIDIA Corporation for supporting the GPUs used in our research."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "In this paper, we aim to understand<lb>whether current language and vision<lb>(LaVi) models truly grasp the interac-<lb>tion between the two modalities. To this end, we propose an extension of the MS-<lb>COCO dataset, FOIL-COCO, which asso-<lb>ciates images with both correct and \u2018foil\u2019<lb>captions, that is, descriptions of the im-<lb>age that are highly similar to the original<lb>ones, but contain one single mistake (\u2018foil word\u2019). We show that current LaVi mod-<lb>els fall into the traps of this data and per-<lb>form badly on three tasks: a) caption clas-<lb>sification (correct vs. foil); b) foil word<lb>detection; c) foil word correction. Humans, in contrast, have near-perfect per-<lb>formance on those tasks. We demonstrate<lb>that merely utilising language cues is not<lb>enough to model FOIL-COCO and that it<lb>challenges the state-of-the-art by requiring a fine-grained understanding of the rela-<lb>tion between text and image.", "creator": "LaTeX with hyperref package"}}}