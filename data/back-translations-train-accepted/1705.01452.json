{"id": "1705.01452", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2017", "title": "Chunk-Based Bi-Scale Decoder for Neural Machine Translation", "abstract": "In typical neural machine translation~(NMT), the decoder generates a sentence word by word, packing all linguistic granularities in the same time-scale of RNN. In this paper, we propose a new type of decoder for NMT, which splits the decode state into two parts and updates them in two different time-scales. Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated. In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model.", "histories": [["v1", "Wed, 3 May 2017 14:39:56 GMT  (1131kb,D)", "http://arxiv.org/abs/1705.01452v1", "Accepted as a short paper by ACL 2017"]], "COMMENTS": "Accepted as a short paper by ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hao zhou", "zhaopeng tu", "shujian huang", "xiaohua liu", "hang li", "jiajun chen"], "accepted": true, "id": "1705.01452"}, "pdf": {"name": "1705.01452.pdf", "metadata": {"source": "CRF", "title": "Chunk-Based Bi-Scale Decoder for Neural Machine Translation", "authors": ["Hao Zhou", "Zhaopeng Tu", "Shujian Huang", "Xiaohua Liu", "Hang Li", "Jiajun Chen"], "emails": ["zhouh@nlp.nju.edu.cn", "tuzhaopeng@gmail.com", "huangsh@nlp.nju.edu.cn", "liuxiaohua3@huawei.com", "hangli.hl@huawei.com", "chenjj@nlp.nju.edu.cn"], "sections": [{"heading": null, "text": "In typical Neural Machine Translation (NMT), the decoder generates a sentence word for word and packages all linguistic granularities into the same timescale of RNN. In this work, we propose a new type of decoder for NMT that splits the decoder state into two parts and updates it into two different timescales. Specifically, we first predict a chunk timescale state for phrasal modeling, generating multiple word-timescale states. In this way, the target sentence is translated hierarchically from chunks to words, using information in different granularities. Experiments show that our proposed model significantly improves translation performance compared to the modern NMT model."}, {"heading": "1 Introduction", "text": "The current work on neural machine translation (NMT) proposes the encoder decoder framework for machine translation (Kalchburner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which uses a recursive neural network (RNN) to model the source context context information and an RNN decoder to generate translations that differ significantly from previous statistical machine translation systems (Koehn et al, 2003; Chiang, 2005), which is then expanded to include an attention mechanism that dynamically acquires the source context information in different decoding steps (Bahdanau et al, 2014; Luong et al al, 2015)."}, {"heading": "2 Standard Neural Machine Translation Model", "text": "In general, the neural machine translation system models the conditional probability of the translation y word for word (Bahdanau et al., 2014). Formally, with an input sequence x = [x1, x2,.., xJ] and the previously generated sequence y < t = [y1, y2,.., yt \u2212 1], the probability of the next target word yt isP (yt | x) = softmax (f (eyt \u2212 1, st, ct)) (1), where f (\u00b7) is a non-linear function, eyt \u2212 1 is the embedding of yt \u2212 1; st is the decoding state at the time step t calculated byst = g (st \u2212 1, eyt \u2212 1, ct) (2). Here g (\u00b7) is a transition function of the decoder RNN \u2212 ct, the context vector that is byct = J tant (st \u2212 1, hott \u2212 1, a search), where (n) is an exercise."}, {"heading": "3 Chunk-Based Bi-Scale Neural Machine Translation Model", "text": "Instead of the word-based decoder, we suggest using a chunk-based bi-scale decoder = multiple attention that generates a translation hierarchically with chunk and word time-scales, as shown in Figure 1. Intuitively, we first create a chunk state with the attention model that extracts the source context for the current phrasal scope. Then, we generate multiple lexical words based on the same chunk state that does not require attention operations. A chunk boundary gate determines whether the chunk state should be updated at each step or not. Formally, the probability of the next word is yt isP (yt | x) = softmax (f \u2212 1, pt)))) (6) st \u2212 1, eyt \u2212 1, eyt \u2212 1, pt (pt), pt \u2212 1, pt (7) here is the chunk state at each step."}, {"heading": "4 Experiments", "text": "We conduct experiments on a Chinese-English translation task. Our training data consists of 1.16M2 pairs of sentences extracted from LDC corpora, with 25.1M Chinese words and 27.1M English words, respectively. We select the data set NIST 2002 (MT02) as development set and the data set NIST 2003 (MT03), 2004 (MT04) 2005 (MT05) as test set. Furthermore, we evaluate our model using the translation task WMT German-English, Neuestest2014 (DE14) as development set and Neuest2012 (DE1213) as test set. The English sentences are labeled with a neural chunker implemented according to Zhou et al. (2015). We use the case-insensitive 4-gram model NIST BLEU as a yardstick (Papineni et al., 2002). In the training, we limit the source and target vocabularies to the most frequent sentences with 30,000 Sentences each."}, {"heading": "4.1 Results on Chinese-English", "text": "This year it is more than ever before."}, {"heading": "4.2 Results on German-English", "text": "We evaluate our model using the WMT15 translation task from German to English. We note that our proposed chunk-based NMT model also achieves significant improvements in accuracy in German English. However, the BLEU score gains are not as significant as in Chinese-English. We speculate that the difference between Chinese and English is greater than between German and English. The chunk-based NMT model may be more useful for bilingual data with greater differences."}, {"heading": "5 Related Work", "text": "NMT with Various Granularities. A number of previous works suggest using other granularities in addition to words for NMT. By further exploiting the character level (Ling et al., 2015; Costajussa et al., 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016), or the subordinate level (Sennrich et al., 2016; Sennrich et al., 2016; Garc\u0131 a-Mart\u00ed \u0301 nez et al., 2016), the corresponding NMT models capture the information within the word and alleviate the problem of unknown words. While most of them focus on the decomposition of words into characters or subordinates, our work aims to compose words into phrases. Incorporate syntactic information in NMT syntactical information used in SMT (Liu et al., 2006; Marton and Resal., 2008) to synchronize words in synrases."}, {"heading": "6 Conclusion", "text": "We propose a chunk-based bi-scale decoder for neural machine translation, where the target sentence is hierarchically translated into words by chunks, using information in different granularities. Experiments show that our proposed model exceeds the standard baseline of attention-based neural machine translation. Future work includes the task of tagging chunk data, adopting reinforcement to automatically explore the limits of the phrase (Mou et al., 2016). Our code is available at https: / / github.com / zhouh / chunk-nmt.Thanks go to the anonymous reviewers for their insightful comments. We also thank Lili Mou for helpful discussions and Hongjie Ji, Zhenting Yu, Xiaoxue Hou and Wei Zou for their help with data processing and subjective evaluation. This work was partially founded by the Foundation C97127122301 National Science Foundation of China and Wei Zou."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR 2015.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, pages 263\u2013270.", "citeRegEx": "Chiang.,? 2005", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Character-based neural machine translation", "author": ["R. Marta Costa-juss\u00e0", "R. Jos\u00e9 A. Fonollosa."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics,", "citeRegEx": "Costa.juss\u00e0 and Fonollosa.,? 2016", "shortCiteRegEx": "Costa.juss\u00e0 and Fonollosa.", "year": 2016}, {"title": "Tree-to-sequence attentional neural machine translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Associ-", "citeRegEx": "Eriguchi et al\\.,? 2016", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "Factored neural machine translation", "author": ["Mercedes Garc\u0131\u0301a-Mart\u0131\u0301nez", "Lo\u0131\u0308c Barrault", "Fethi Bougares"], "venue": null, "citeRegEx": "Garc\u0131\u0301a.Mart\u0131\u0301nez et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Mart\u0131\u0301nez et al\\.", "year": 2016}, {"title": "Chunk-based decoder for neural machine translation", "author": ["Shonosuke Ishiwatari", "Jingtao Yao", "Shujie Liu", "Mu Li", "Ming Zhou", "Naoki Yoshinaga", "Masaru Kitsuregawa", "Weijia Jia."], "venue": "Proceedings of the 55th annual meeting of the Association for Compu-", "citeRegEx": "Ishiwatari et al\\.,? 2017", "shortCiteRegEx": "Ishiwatari et al\\.", "year": 2017}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Fully character-level neural machine translation without explicit segmentation", "author": ["Jason Lee", "Kyunghyun Cho", "Thomas Hofmann."], "venue": "arXiv preprint arXiv:1610.03017 .", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black."], "venue": "arXiv preprint arXiv:1511.04586 .", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Treeto-string alignment template for statistical machine translation", "author": ["Yang Liu", "Qun Liu", "Shouxun Lin."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Compu-", "citeRegEx": "Liu et al\\.,? 2006", "shortCiteRegEx": "Liu et al\\.", "year": 2006}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "International Conference on Learning Representations (ICLR). San Juan, Puerto Rico.", "citeRegEx": "Luong et al\\.,? 2016", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Thang Luong", "Hieu Pham", "D. Christopher Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Soft syntactic constraints for hierarchical phrased-based translation", "author": ["Yuval Marton", "Philip Resnik."], "venue": "ACL. pages 1003\u20131011.", "citeRegEx": "Marton and Resnik.,? 2008", "shortCiteRegEx": "Marton and Resnik.", "year": 2008}, {"title": "Coupling distributed and symbolic execution for natural language queries", "author": ["Lili Mou", "Zhengdong Lu", "Hang Li", "Zhi Jin."], "venue": "arXiv preprint arXiv:1612.02741 .", "citeRegEx": "Mou et al\\.,? 2016", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Linguistic input features improve neural machine translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "Proceedings of the First Conference on Machine Translation. Association for Computational Linguistics, Berlin, Germany, pages 83\u201391.", "citeRegEx": "Sennrich and Haddow.,? 2016", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "A new string-to-dependency machine translation algorithm with a target dependency language model", "author": ["Libin Shen", "Jinxi Xu", "Ralph M Weischedel."], "venue": "ACL. pages 577\u2013585.", "citeRegEx": "Shen et al\\.,? 2008", "shortCiteRegEx": "Shen et al\\.", "year": 2008}, {"title": "Does string-based neural mt learn source syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "author": ["Xing Shi", "Inkit Padhi", "Kevin Knight."], "venue": "Association for Computational Linguistics, pages 1526\u2013", "citeRegEx": "Shi et al\\.,? 2016", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "Syntactically guided neural machine translation", "author": ["Felix Stahlberg", "Eva Hasler", "Aurelien Waite", "Bill Byrne."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Associa-", "citeRegEx": "Stahlberg et al\\.,? 2016", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Context gates for neural machine translation", "author": ["Zhaopeng Tu", "Yang Liu", "Zhengdong Lu", "Xiaohua Liu", "Hang Li."], "venue": "Transactions of the Association for Computational Linguistics 5:87\u201399.", "citeRegEx": "Tu et al\\.,? 2017a", "shortCiteRegEx": "Tu et al\\.", "year": 2017}, {"title": "Neural machine translation with reconstruction", "author": ["Zhaopeng Tu", "Yang Liu", "Lifeng Shang", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of AAAI 2017. pages 3097\u20133103.", "citeRegEx": "Tu et al\\.,? 2017b", "shortCiteRegEx": "Tu et al\\.", "year": 2017}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Asso-", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Graph-based dependency parsing with bidirectional lstm", "author": ["Wenhui Wang", "Baobao Chang."], "venue": "Proceedings of ACL. volume 1, pages 2306\u20132315.", "citeRegEx": "Wang and Chang.,? 2016", "shortCiteRegEx": "Wang and Chang.", "year": 2016}, {"title": "A neural probabilistic structuredprediction model for transition-based dependency parsing", "author": ["Hao Zhou", "Yue Zhang", "Shujian Huang", "Jiajun Chen."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al.", "startOffset": 126, "endOffset": 200}, {"referenceID": 2, "context": "Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al.", "startOffset": 126, "endOffset": 200}, {"referenceID": 24, "context": "Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al.", "startOffset": 126, "endOffset": 200}, {"referenceID": 10, "context": ", 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005).", "startOffset": 235, "endOffset": 269}, {"referenceID": 1, "context": ", 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005).", "startOffset": 235, "endOffset": 269}, {"referenceID": 0, "context": "This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015).", "startOffset": 138, "endOffset": 181}, {"referenceID": 15, "context": "This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015).", "startOffset": 138, "endOffset": 181}, {"referenceID": 12, "context": "Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016).", "startOffset": 196, "endOffset": 388}, {"referenceID": 4, "context": "Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016).", "startOffset": 196, "endOffset": 388}, {"referenceID": 3, "context": "Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016).", "startOffset": 196, "endOffset": 388}, {"referenceID": 14, "context": "Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016).", "startOffset": 196, "endOffset": 388}, {"referenceID": 11, "context": "Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016).", "startOffset": 196, "endOffset": 388}, {"referenceID": 19, "context": "Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016).", "startOffset": 196, "endOffset": 388}, {"referenceID": 20, "context": "Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016).", "startOffset": 196, "endOffset": 388}, {"referenceID": 6, "context": "Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016).", "startOffset": 196, "endOffset": 388}, {"referenceID": 9, "context": "However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (Koehn et al., 2007).", "startOffset": 133, "endOffset": 153}, {"referenceID": 5, "context": "Instead of incorporating source side linguistic information (Eriguchi et al., 2016; Sennrich and Haddow, 2016), our model incorporates linguistic knowledges in the target side (for deciding chunks), which will guide the translation more in line with linguistic grammars.", "startOffset": 60, "endOffset": 110}, {"referenceID": 19, "context": "Instead of incorporating source side linguistic information (Eriguchi et al., 2016; Sennrich and Haddow, 2016), our model incorporates linguistic knowledges in the target side (for deciding chunks), which will guide the translation more in line with linguistic grammars.", "startOffset": 60, "endOffset": 110}, {"referenceID": 0, "context": "Generally, neural machine translation system directly models the conditional probability of the translation y word by word (Bahdanau et al., 2014).", "startOffset": 123, "endOffset": 146}, {"referenceID": 28, "context": "In the UPDATE operation, ept\u22121 is the representation of last chunk, which is computed by the LSTM-minus approach (Wang and Chang, 2016):", "startOffset": 113, "endOffset": 135}, {"referenceID": 18, "context": "We use the case-insensitive 4-gram NIST BLEU score as our evaluation metric (Papineni et al., 2002).", "startOffset": 76, "endOffset": 99}, {"referenceID": 27, "context": "The English sentences are labeled by a neural chunker, which is implemented according to Zhou et al. (2015). We use the case-insensitive 4-gram NIST BLEU score as our evaluation metric (Papineni et al.", "startOffset": 89, "endOffset": 108}, {"referenceID": 0, "context": "All the other settings are the same as in Bahdanau et al. (2014).", "startOffset": 42, "endOffset": 65}, {"referenceID": 9, "context": "We list the BLEU score of our proposed model in Table 1, comparing with Moses (Koehn et al., 2007) and dl4mt3 (Bahdanau et al.", "startOffset": 78, "endOffset": 98}, {"referenceID": 0, "context": ", 2007) and dl4mt3 (Bahdanau et al., 2014), which are state-of-the-art models of SMT and NMT, respective.", "startOffset": 19, "endOffset": 42}, {"referenceID": 24, "context": "The chunk attention could be considered as a compromise approach between encoding the whole source sentence into decoder without attention (Sutskever et al., 2014) and utilizing word level attention at each step (Bahdanau et al.", "startOffset": 139, "endOffset": 163}, {"referenceID": 0, "context": ", 2014) and utilizing word level attention at each step (Bahdanau et al., 2014).", "startOffset": 56, "endOffset": 79}, {"referenceID": 12, "context": "By further exploiting the character level (Ling et al., 2015; Costajuss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016), or the sub-word level (Sennrich and Haddow, 2016; Sennrich et al.", "startOffset": 42, "endOffset": 151}, {"referenceID": 3, "context": "By further exploiting the character level (Ling et al., 2015; Costajuss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016), or the sub-word level (Sennrich and Haddow, 2016; Sennrich et al.", "startOffset": 42, "endOffset": 151}, {"referenceID": 14, "context": "By further exploiting the character level (Ling et al., 2015; Costajuss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016), or the sub-word level (Sennrich and Haddow, 2016; Sennrich et al.", "startOffset": 42, "endOffset": 151}, {"referenceID": 11, "context": "By further exploiting the character level (Ling et al., 2015; Costajuss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016), or the sub-word level (Sennrich and Haddow, 2016; Sennrich et al.", "startOffset": 42, "endOffset": 151}, {"referenceID": 19, "context": ", 2016), or the sub-word level (Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016) information, the corresponding NMT models capture the information inside the word and alleviate the problem of unknown words.", "startOffset": 31, "endOffset": 113}, {"referenceID": 20, "context": ", 2016), or the sub-word level (Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016) information, the corresponding NMT models capture the information inside the word and alleviate the problem of unknown words.", "startOffset": 31, "endOffset": 113}, {"referenceID": 6, "context": ", 2016), or the sub-word level (Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016) information, the corresponding NMT models capture the information inside the word and alleviate the problem of unknown words.", "startOffset": 31, "endOffset": 113}, {"referenceID": 13, "context": "Incorporating Syntactic Information in NMT Syntactic information has been widely used in SMT (Liu et al., 2006; Marton and Resnik, 2008; Shen et al., 2008), and a lot of previous work explore to incorporate the syntactic information in NMT, which shows the effectiveness of the syntactic information (Stahlberg et al.", "startOffset": 93, "endOffset": 155}, {"referenceID": 16, "context": "Incorporating Syntactic Information in NMT Syntactic information has been widely used in SMT (Liu et al., 2006; Marton and Resnik, 2008; Shen et al., 2008), and a lot of previous work explore to incorporate the syntactic information in NMT, which shows the effectiveness of the syntactic information (Stahlberg et al.", "startOffset": 93, "endOffset": 155}, {"referenceID": 21, "context": "Incorporating Syntactic Information in NMT Syntactic information has been widely used in SMT (Liu et al., 2006; Marton and Resnik, 2008; Shen et al., 2008), and a lot of previous work explore to incorporate the syntactic information in NMT, which shows the effectiveness of the syntactic information (Stahlberg et al.", "startOffset": 93, "endOffset": 155}, {"referenceID": 23, "context": ", 2008), and a lot of previous work explore to incorporate the syntactic information in NMT, which shows the effectiveness of the syntactic information (Stahlberg et al., 2016).", "startOffset": 152, "endOffset": 176}, {"referenceID": 11, "context": "Incorporating Syntactic Information in NMT Syntactic information has been widely used in SMT (Liu et al., 2006; Marton and Resnik, 2008; Shen et al., 2008), and a lot of previous work explore to incorporate the syntactic information in NMT, which shows the effectiveness of the syntactic information (Stahlberg et al., 2016). Shi et al. (2016) give some empirical results that the deep networks of NMT are able to capture some useful syntactic information implicitly.", "startOffset": 94, "endOffset": 344}, {"referenceID": 11, "context": "Incorporating Syntactic Information in NMT Syntactic information has been widely used in SMT (Liu et al., 2006; Marton and Resnik, 2008; Shen et al., 2008), and a lot of previous work explore to incorporate the syntactic information in NMT, which shows the effectiveness of the syntactic information (Stahlberg et al., 2016). Shi et al. (2016) give some empirical results that the deep networks of NMT are able to capture some useful syntactic information implicitly. Luong et al. (2016) propose to use a multi-task framework for NMT and neural parsing, achieving promising results.", "startOffset": 94, "endOffset": 488}, {"referenceID": 5, "context": "Eriguchi et al. (2016) propose a string-totree NMT system by end-to-end training.", "startOffset": 0, "endOffset": 23}, {"referenceID": 5, "context": "Eriguchi et al. (2016) propose a string-totree NMT system by end-to-end training. Different to previous work, we try to incorporate the syntactic information in the target side of NMT. Ishiwatari et al. (2017) concurrently propose to use chunk-based decoder to cope with the problem of free word-order languages.", "startOffset": 0, "endOffset": 210}, {"referenceID": 17, "context": "Future work includes abandoning labeled chunk data, adopting reinforcement learning to explore the boundaries of phrase automatically (Mou et al., 2016).", "startOffset": 134, "endOffset": 152}], "year": 2017, "abstractText": "In typical neural machine translation (NMT), the decoder generates a sentence word by word, packing all linguistic granularities in the same timescale of RNN. In this paper, we propose a new type of decoder for NMT, which splits the decode state into two parts and updates them in two different time-scales. Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated. In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model.", "creator": "LaTeX with hyperref package"}}}