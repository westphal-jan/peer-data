{"id": "1705.01991", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2017", "title": "Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU", "abstract": "Attentional sequence-to-sequence models have become the new standard for machine translation, but one challenge of such models is a significant increase in training and decoding cost compared to phrase-based systems. Here, we focus on efficient decoding, with a goal of achieving accuracy close the state-of-the-art in neural machine translation (NMT), while achieving CPU decoding speed/throughput close to that of a phrasal decoder.", "histories": [["v1", "Thu, 4 May 2017 19:50:35 GMT  (137kb,AD)", "http://arxiv.org/abs/1705.01991v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jacob devlin"], "accepted": true, "id": "1705.01991"}, "pdf": {"name": "1705.01991.pdf", "metadata": {"source": "CRF", "title": "Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU", "authors": ["Jacob Devlin"], "emails": ["jdevlin@microsoft.com"], "sections": [{"heading": null, "text": "We approach this problem from two angles: First, we describe several techniques for accelerating an NMT beam detector that achieve 4.4 times the acceleration over a very efficient base decoder without changing the decoder output. Second, we propose a simple but powerful network architecture that uses an RNN layer (GRU / LSTM) at the bottom, followed by a series of stacked, fully interconnected layers that are applied at any timeframe. This architecture achieves a similar accuracy to a deeply recurring model, at a small fraction of the training and decoding cost. By combining these techniques, our best system achieves a very competitive accuracy of 38.3 BLEU on the WMT English NewsTest2014, while decoding is done at 100 words / sec on a single-wire CPU. We believe that this is the best published accuracy / speed of an NMT system."}, {"heading": "1 Introduction", "text": "One of the biggest challenges is the high training and decryption costs of this neural machine translation (NMT), which is often at least one order of magnitude higher than a phrase system based on the same data. Phrase-like MT systems were able to achieve single-thread decryption speeds of 100-500 words / sec on decades-old CPUs (Quirk and Moore, 2007), while Jean et al. (2015) reported single-thread decryption speeds of 8-10 words / sec on a flat NMT system. (2016) Was able to achieve CPU decryption speeds of 100 words / sec for a deep model."}, {"heading": "2 Data Set", "text": "The dataset on which we are evaluating this work is the English-French NewsTest2014 from WMT, which contains 380M words of parallel training data and a set of 3003 sentences. NewsTest2013 is used for validation purposes. To compare our architecture with previous work, we train a word-based system without data expansion techniques. The network architecture is very similar to Bahdanau et al. (2014), and specific layer size / depth details are provided in the following sections. We use an 80k source / target vocabulary and perform standard radio substitutions (Jean et al., 2015) on foreign words. Training is performed using an in-house toolkit."}, {"heading": "3 Baseline Decoder", "text": "Our base decoder is a standard beam search decoder (Sutskever et al., 2014) with several simple performance optimizations: \u2022 It is written in pure C + +, with no heap assignment during the core search. \u2022 A candidate list is used to reduce the Softmax output from 80k to ~ 500. We perform word alignment (Brown et al., 1993) during training and keep the 20 context-free top translations for each source word in the test set. \u2022 The Intel MKL library is used for matrix multiplication, as it is the fastest floating-point multiplication library for CPUs. \u2022 An early stop is performed when the top sub-hypothesis has a log score of \u03b4 = 3.0, which is worse than the best accomplished hypothesis. \u2022 Batching matrix multiplication is used when possible. Since each sentence is decoded separately, we can only stack the source as well as the hypothesis in the base decoder."}, {"heading": "4 Decoder Speed Improvements", "text": "This section describes a number of accelerations that can be applied to a CPU-based attention-oriented sequenceto sequence beam decoder. Crucially, none of these accelerations affect the actual mathematical calculation of the decoder, so they can be applied to any network architecture, with the assurance that they will not affect outcomes.11Some accelerations apply quantifications that result in small random disturbances, but these alter the BLEU score by less than 02. The model used here is similar to the original implementation by Bahdanau et al. (2014).The exact target-GRU equation is dij = tanh (Wahi \u2212 1 + Vaxi) \u00b7 tanh (Uasj)."}, {"heading": "4.1 16-Bit Matrix Multiplication", "text": "Although CPU-based matrix multiplication libraries are highly optimized, they typically only work on 32 / 64-bit floats, although DNNs can almost always work with much less precision without degrading accuracy (Han et al., 2016). However, low-precision (1-bit to 7-bit) mathematics is hard to implement efficiently on the CPU, and even 8-bit mathematics has limited support for vectorized (SIMD) instruction sets. In this case, we use 16-bit fixed-point numbers as they have first-class SIMD support and require minimal changes in training. Training is still done with 32-bit floats, but we cut the weights to the range [-1.0, 1.0] of relay activation to [0.0, 10.0] to ensure that all values are multiplicable in 16-bit with high precision 16-bits."}, {"heading": "4.2 Pre-Compute Embeddings", "text": "In the first hidden level on the source and target pages, xi corresponds to word embedding. Since this is a closed set of values that are fixed after training, the vectors V xi can be pre-calculated for each word in the vocabulary (Devlin et al., 2014) and stored in a reference table. This can only be applied to the first hidden level. Pre-calculation increases the storage costs of the model, since we need to store per word instead of e r \u00d7 3 floats. However, if we only calculate the k-most common words (e.g. k = 8,000), this reduces the preview memory by 90%, but still results in 95% + token coverage due to the Zipser language distribution."}, {"heading": "4.3 Pre-Compute Attention", "text": "The calculation of the attention context in the GRU can be refracted as follows: Uci = U (\u2211 j \u03b1ijsj) = \u2211 j \u03b1ij (Usj) It is crucial that the hidden vector representation sj depends only on the source sentence, while aij depends on the target hypothesis. Therefore, the original calculation Uci requires total | T | \u00b7 b multiplications per sentence, but the refractory versionUsj requires only total | S | multiplications. The expectation of \u03b1 still has to be calculated at each target time, but this is much cheaper than the multiplication with U."}, {"heading": "4.4 SSE & Lookup Tables", "text": "For the elementary vector functions used in the GRU, we can use vectorized commands (SSE / AVX) for adding and multiplying functions and reference tables for sigmoid and tane. Reference implementations in C + + are included in the supplementary material."}, {"heading": "4.5 Merge Recurrent States", "text": "In the GRU equation, xi represents the previously generated word for the first hidden target plane, and hi \u2212 1 encodes up to two words before the current word. Thus, if two partial hypotheses in the bar differ only by the last output word, their Hi \u2212 1 vectors are identical. Therefore, we can only perform the matrix multiplication Whi \u2212 1 on the unique Hi \u2212 1 vectors in the bar at each target point in time. At a bar size of b = 6, we have measured that the ratio of Hi \u2212 1 to total Hi \u2212 1 is approximately 70% averaged over multiple language pairs, and this can only be applied to the first hidden target plane."}, {"heading": "4.6 Speedup Results", "text": "The NMT architecture studied here uses 3-layer 512-dimensional bidirectional GRU for the source and a 1-layer 1024-dimensional attention GRU for the target. Each set becomes independent with a beam of 6. Because these accelerations are all mathematical identities without quantization noise, all results reach 36.2 BLEU and are 99.9% + identical. The biggest improvement is the 16-bit matrix multiplication, but all accelerations make a significant contribution. Overall, we are able to achieve 4.4-fold acceleration via a fast baseline decoder. Although the absolute speed is impressive, the model uses only one target layer and is several BLEU behind the SOTA, so the next goal is to maximize model accuracy and still achieve speeds greater than some targets, such as 100 / sec."}, {"heading": "5 Model Improvements", "text": "In NMT, as in many other deep learning tasks, accuracy can be greatly improved by adding more hidden layers, but training and decoding time increase significantly (Luong et al., 2014; Zhou et al., 2016; Wu et al., 2016). However, this work has shown that it is difficult to replace the target side of the model with CNN layers while maintaining high accuracy. Using a recurring target is particularly important to ensure attention-oriented coverage. Here, we suggest a mixed model that uses an RNN layer at the bottom."}, {"heading": "5.1 Model Results", "text": "We have found that the benefit of using RNN + FC layers at source is minimal, so we only perform ablation at the target. For the source, we use a 3-layer 512-dim bidi GRU in all models (S1) - (S6).Model (S1) and (S2) are one and two layer baselines. Model (S4), which uses 7 intermediate layers FC, has similar decoding costs for (S2), while the improvement over (S1) is doubled to 1.2 BLEU. We see minimal benefit from using a GRU on the top layer (S5) or using more FC layers (S6). In (E1) and (E2) we present 2 and 3 model sets of (S4), which are trained from scratch (S4) with different random seeds."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Peter F Brown", "Vincent J Della Pietra", "Stephen A Della Pietra", "Robert L Mercer."], "venue": "Computational linguistics 19(2):263\u2013311.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Fast and robust neural network joint models", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard M Schwartz", "John Makhoul"], "venue": null, "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Edinburgh\u2019s phrase-based machine translation systems for wmt-14", "author": ["Nadir Durrani", "Barry Haddow", "Philipp Koehn", "Kenneth Heafield."], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation. pages 97\u2013104.", "citeRegEx": "Durrani et al\\.,? 2014", "shortCiteRegEx": "Durrani et al\\.", "year": 2014}, {"title": "A convolutional encoder model for neural machine translation", "author": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N Dauphin."], "venue": "arXiv preprint arXiv:1611.02344 .", "citeRegEx": "Gehring et al\\.,? 2016", "shortCiteRegEx": "Gehring et al\\.", "year": 2016}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J. Dally."], "venue": "ICLR .", "citeRegEx": "Han et al\\.,? 2016", "shortCiteRegEx": "Han et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 770\u2013778.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "CoRR .", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Neural machine translation in linear time", "author": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "Aaron van den Oord", "Alex Graves", "Koray Kavukcuoglu."], "venue": "arXiv preprint arXiv:1610.10099 .", "citeRegEx": "Kalchbrenner et al\\.,? 2016", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Sequencelevel knowledge distillation", "author": ["Yoon Kim", "Alexander M Rush."], "venue": "arXiv preprint arXiv:1606.07947 .", "citeRegEx": "Kim and Rush.,? 2016", "shortCiteRegEx": "Kim and Rush.", "year": 2016}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "ACL 2015 .", "citeRegEx": "Luong et al\\.,? 2014", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Faster beamsearch decoding for phrasal statistical machine translation", "author": ["Chris Quirk", "Robert Moore."], "venue": "Machine Translation Summit XI .", "citeRegEx": "Quirk and Moore.,? 2007", "shortCiteRegEx": "Quirk and Moore.", "year": 2007}, {"title": "http://www-lium", "author": ["Holger Schwenk."], "venue": "univ-lemans.fr/schwenk/cslm_joint_ paper. [Online; accessed 03-September-2014].", "citeRegEx": "Schwenk.,? 2014", "shortCiteRegEx": "Schwenk.", "year": 2014}, {"title": "Highway networks", "author": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "arXiv preprint arXiv:1505.00387 .", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "NIPS .", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Deep recurrent models with fast-forward connections for neural machine translation", "author": ["Jie Zhou", "Ying Cao", "Xuguang Wang", "Peng Li", "Wei Xu."], "venue": "arXiv preprint arXiv:1606.04199 .", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 11, "context": "For instance, phrasal MT systems were able achieve single-threaded decoding speeds of 100-500 words/sec on decade-old CPUs (Quirk and Moore, 2007), while Jean et al.", "startOffset": 123, "endOffset": 146}, {"referenceID": 9, "context": "There has been recent work in speeding up decoding by reducing the search space (Kim and Rush, 2016), but little in computational improvements.", "startOffset": 80, "endOffset": 100}, {"referenceID": 7, "context": "For instance, phrasal MT systems were able achieve single-threaded decoding speeds of 100-500 words/sec on decade-old CPUs (Quirk and Moore, 2007), while Jean et al. (2015) reported single-threaded decoding speeds of 8-10 words/sec on a shallow NMT system.", "startOffset": 154, "endOffset": 173}, {"referenceID": 7, "context": "For instance, phrasal MT systems were able achieve single-threaded decoding speeds of 100-500 words/sec on decade-old CPUs (Quirk and Moore, 2007), while Jean et al. (2015) reported single-threaded decoding speeds of 8-10 words/sec on a shallow NMT system. Wu et al. (2016) was able to reach CPU decoding speeds of 100 words/sec for a deep model, but used 44 CPU cores to do so.", "startOffset": 154, "endOffset": 274}, {"referenceID": 7, "context": "We use an 80k source/target vocab and perform standard unk-replacement (Jean et al., 2015) on out-of-vocabulary words.", "startOffset": 71, "endOffset": 90}, {"referenceID": 0, "context": "The network architecture is very similar to Bahdanau et al. (2014), and specific details of layer size/depth are provided in subsequent sections.", "startOffset": 44, "endOffset": 67}, {"referenceID": 14, "context": "Our baseline decoder is a standard beam search decoder (Sutskever et al., 2014) with several straightforward performance optimizations:", "startOffset": 55, "endOffset": 79}, {"referenceID": 1, "context": "We run word alignment (Brown et al., 1993) on the training and keep the top 20 context-free translations for each source word in the test sentence.", "startOffset": 22, "endOffset": 42}, {"referenceID": 0, "context": "The model used here is similar to the original implementation of Bahdanau et al. (2014). The exact target GRU equation is:", "startOffset": 65, "endOffset": 88}, {"referenceID": 5, "context": "Although CPU-based matrix multiplication libraries are highly optimized, they typically only operate on 32/64-bit floats, even though DNNs can almost always operate on much lower precision without degredation of accuracy (Han et al., 2016).", "startOffset": 221, "endOffset": 239}, {"referenceID": 2, "context": "Since this is a closed set of values that are fixed after training, the vectors V xi can be pre-computed (Devlin et al., 2014) for each word in the vocabulary and stored in a lookup table.", "startOffset": 105, "endOffset": 126}, {"referenceID": 12, "context": "Basic Phrase-Based MT (Schwenk, 2014) 33.", "startOffset": 22, "endOffset": 37}, {"referenceID": 3, "context": "1 SOTA Phrase-Based MT (Durrani et al., 2014) 37.", "startOffset": 23, "endOffset": 45}, {"referenceID": 10, "context": "0 6-Layer Non-Attentional Seq-to-Seq LSTM (Luong et al., 2014) 33.", "startOffset": 42, "endOffset": 62}, {"referenceID": 7, "context": "GRU, w/ Large Vocab (Jean et al., 2015) 34.", "startOffset": 20, "endOffset": 39}, {"referenceID": 15, "context": "LSTM (Zhou et al., 2016) 39.", "startOffset": 5, "endOffset": 24}, {"referenceID": 10, "context": "In NMT, like in many other deep learning tasks, accuracy can be greatly improved by adding more hidden layers, but training and decoding time increase significantly (Luong et al., 2014; Zhou et al., 2016; Wu et al., 2016).", "startOffset": 165, "endOffset": 221}, {"referenceID": 15, "context": "In NMT, like in many other deep learning tasks, accuracy can be greatly improved by adding more hidden layers, but training and decoding time increase significantly (Luong et al., 2014; Zhou et al., 2016; Wu et al., 2016).", "startOffset": 165, "endOffset": 221}, {"referenceID": 4, "context": "Several past works have noted that convolutional neural networks (CNNs) are significantly less expensive than RNNs, and replaced the source and/or target side with a CNN-based architecture (Gehring et al., 2016; Kalchbrenner et al., 2016).", "startOffset": 189, "endOffset": 238}, {"referenceID": 8, "context": "Several past works have noted that convolutional neural networks (CNNs) are significantly less expensive than RNNs, and replaced the source and/or target side with a CNN-based architecture (Gehring et al., 2016; Kalchbrenner et al., 2016).", "startOffset": 189, "endOffset": 238}, {"referenceID": 6, "context": "To avoid vanishing gradients, we use ResNetstyle skip connections (He et al., 2016).", "startOffset": 66, "endOffset": 83}, {"referenceID": 13, "context": "These allow very deep models to be trained from scratch and do not require any additional matrix multiplications, unlike highway networks (Srivastava et al., 2015).", "startOffset": 138, "endOffset": 163}, {"referenceID": 6, "context": "hi = AttGRU(h B i\u22121, xi, S) hi = relu(W hi ) hi = relu(W hi ) hi = relu(W hi + h 1 i ) hi = relu(W hi ) hi = relu(W hi + h 3 i ) hi = tanh(W hi ) or GRU(h T i\u22121, h 5 i ) yi = softmax(V h T i ) We follow He et al. (2016) and only use skip connections on every other FC layer, but do not use batch normalization.", "startOffset": 203, "endOffset": 220}, {"referenceID": 15, "context": "Zhou et al. (2016) has a similar computational cost to (S7), but we were not able to replicate those results in terms of accuracy.", "startOffset": 0, "endOffset": 19}], "year": 2017, "abstractText": "Attentional sequence-to-sequence models have become the new standard for machine translation, but one challenge of such models is a significant increase in training and decoding cost compared to phrase-based systems. Here, we focus on efficient decoding, with a goal of achieving accuracy close the state-of-the-art in neural machine translation (NMT), while achieving CPU decoding speed/throughput close to that of a phrasal decoder. We approach this problem from two angles: First, we describe several techniques for speeding up an NMT beam search decoder, which obtain a 4.4x speedup over a very efficient baseline decoder without changing the decoder output. Second, we propose a simple but powerful network architecture which uses an RNN (GRU/LSTM) layer at bottom, followed by a series of stacked fully-connected layers applied at every timestep. This architecture achieves similar accuracy to a deep recurrent model, at a small fraction of the training and decoding cost. By combining these techniques, our best system achieves a very competitive accuracy of 38.3 BLEU on WMT EnglishFrench NewsTest2014, while decoding at 100 words/sec on single-threaded CPU. We believe this is the best published accuracy/speed trade-off of an NMT system.", "creator": "LaTeX with hyperref package"}}}