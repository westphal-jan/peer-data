{"id": "1705.02073", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2017", "title": "Cross-lingual Distillation for Text Classification", "abstract": "Cross-lingual text classification(CLTC) is the task of classifying documents written in different languages into the same taxonomy of categories. This paper presents a novel approach to CLTC that builds on model distillation, which adapts and extends a framework originally proposed for model compression. Using soft probabilistic predictions for the documents in a label-rich language as the (induced) supervisory labels in a parallel corpus of documents, we train classifiers successfully for new languages in which labeled training data are not available. An adversarial feature adaptation technique is also applied during the model training to reduce distribution mismatch. We conducted experiments on two benchmark CLTC datasets, treating English as the source language and German, French, Japan and Chinese as the unlabeled target languages. The proposed approach had the advantageous or comparable performance of the other state-of-art methods.", "histories": [["v1", "Fri, 5 May 2017 03:36:11 GMT  (301kb,D)", "http://arxiv.org/abs/1705.02073v1", "Accepted at ACL 2017"]], "COMMENTS": "Accepted at ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ruochen xu", "yiming yang"], "accepted": true, "id": "1705.02073"}, "pdf": {"name": "1705.02073.pdf", "metadata": {"source": "CRF", "title": "Cross-lingual Distillation for Text Classification", "authors": ["Ruochen Xu", "Yiming Yang"], "emails": ["ruochenx@cs.cmu.edu", "yiming@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2 Related Work", "text": "Such work can be outlined in view of the representative work in the CLTC and recent progress in deep learning for knowledge distillation."}, {"heading": "2.1 CLTC Methods", "text": "The work of Bel et al. (Bel et al., 2003) was the first effort to solve the CLTC problem. They translated the target language documents into the source language using a bilingual dictionary. They built a multilingual classification by translating subjective words and phrases into the source language. The classifier was then applied to these translated documents. Shi et al. (Shi et al., 2010) also used a bilingual dictionary. Instead of translating the documents, they tried to translate the classification model from the source language into the target language. Prettenhofer and Stein. (Prettenhofer and Stein, 2010) also used the bilingual dictionary."}, {"heading": "2.2 Knowledge Distillation", "text": "The idea of distilling knowledge in a neural network was suggested by Hinton et al. (Hinton et al., 2015), in which they introduced a pupil-teacher paradigm. Once the cumbersome teacher network was trained, the pupil network was trained according to softer predictions of the teacher network. In the field of computer vision, it was empirically confirmed that the pupil network trained by distillation performs better than that trained with hard labels. (Hinton et al., 2015; Romero et al., 2014; Ba and Caruana, 2014). Gupta et al. (Gupta et al., 2015) transfers the supervision between images of various modalities (e.g. from the RGB image to the depth image). There is also some recent work applying distillation in the field of natural language."}, {"heading": "3 Preliminary", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Task and Notation", "text": "CLTC aims to use the training data in the source language to create a model that is applicable in the target language. In our setting, we have the test data in the source language Lsrc = {xi, yi} Li = 1, where xi is the labeled document in the source language and yi is the markup vector. We then have our test data in the target language, indicated by Ttgt = {x \u2032 i} Ti = 1. Our framework can also use unlabeled documents from both languages in transductive learning settings. We use Usrc = {xi} Mi = 1 to denote unlabeled documents in the source language, Utgt = {x \u2032 i} Ni = 1 to denote target-language unlabeled documents, and Uparl = {(xi, x \u2032 i)} Pi = 1 to denote an unlabeled bilingual parallel corpus where xi and x \u2032 i are interlinked document translations. We assume that the test document is not labeled in the target language and the target language."}, {"heading": "3.2 Convolutional Neural Network (CNN) as a Plug-in Classifier", "text": "We use a state-of-the-art CNN-based Neural Network Classifier (Kim, 2014) as a plug-in classifier in our framework. Instead of using a bag-of-words representation for each document, the CNN model links the word embedding (vertical vectors) of each input document into an n \u00d7 k matrix, where n is the length (number of word oc currencies) of the document, and k is the dimension of the word embedding. Denoting byx1: n = x1-x2... xnas is the resulting matrix, with the shortcut operator. One-dimensional volume filter w \u0432Rhk with window size h works on each consecutive h-word, with non-linear function f and bias. For windows of size h started at index i, the feature after the volume filter is: ci = f (w \u00b7 xi: i \u2212 Dab)."}, {"heading": "4 Proposed Framework", "text": "Let us present two versions of our cross-language knowledge distillation model, the vanilla version and the full version with customized functions. Both are supported by the proposed framework: the former is referred to as CLD-KCNN and the latter as CLDFA-KCNN."}, {"heading": "4.1 Vanilla Distillation", "text": "Without losing generality, we assume that we learn a multi-class classification for the target language. We have y = 1, 2,..., where v is the set of all possible classes. We assume that the base classification network generates a real number of logits for each class. Logits are then converted into probabilities for each class by producing the logits through a linear transformation that takes into account the characteristics of each class and creates a vector of size. (1) The logits are converted into probabilities of the classes by the softmax layer, adding each QJ to all other Logits.pj (qj / T.) The first step of our framework is to train the source language classification on the Lsc. We use standard temperature and are usually set to 1. With a higher value of T produces a softer probability distribution over Classes.The first step of our framework is to train the source language classification on the Lsc."}, {"heading": "4.2 Distillation with Adversarial Feature Adaptation", "text": "Although the distillation of the project data in Figures 1 and 2. It is fairly obvious that there can be no general distribution mismatch problem. For example, the marginal feature distributions of source language documents in Lsrc and Uparl may be different, such are the distributions of target language documents in Uparl and Ttgt. To further illustrate our point, we trained a CNN classifier according to Equation 2 and used the features extracted from Gf to present the source language documents in both Lsrc and Uparl. Then we projected the high-dimensional features onto a two-dimensional space via t-Distributed Stochastic Neighbor Embedding (t-SNE) (Maaten and Hinton, 2008)."}, {"heading": "5 Experiments and Discussions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Dataset", "text": "(1) Amazon ReviewsWe used the multilingual multi-domain Amazon review dataset created by Prettenhofer and Stein (Prettenhofer and Stein, 2010).The dataset contains Amazon reviews in three domains: book, DVD and music. Each domain has the reviews in four different languages: English, German, French and Japanese. We treated English as the source language resp. the rest three as the target languages. This results in 9 tasks (the product of the 3 domains and the 3 target languages) in total. For each task there are 1000 positive and 1000 negative ratings in English resp. in the target language. (Prettenhofer and Stein, 2010) thus 2000 parallel evaluations per task created using Google Translate 1 and used by us for transnational distillation. There are also several thousand unlabeled evaluations in each language. The statistics of the unlabeled data are summarized in Table 1."}, {"heading": "5.2 Baselines", "text": "We compare the proposed method with other state-of-the-art methods as outlined below. (1) Parallel corpus-based CLTC methods in this category all use an unlabeled parallel corpus. Methods named PL-LSI (Littman1translate.google.com 2https: / / pypi.python.org / pypi / tinysegmenteret al., 1998), PL-OPCA (Platt et al., 2010) and PL-KCAA (Vinokourov et al., 2002) learn latent document representations in a common low-dimensional space by performing latent semantic indexing (LSI), the oriented Principal Component Analysis (OPCA) and a kernel (namely KCAA) for parallel text. PL-MC (Xiao and Guo, 2013) recovers missing features via the matrix, which uses SCLum and Lum for creating a common space."}, {"heading": "5.3 Implementation Detail", "text": "We performed word2vec (Mikolov et al., 2013) 3 on the tokenized, unlabeled corpus.The learned word embeddings are used to initialize the word embedding matrix, which maps input words to word embeddings and integrates them into the input matrix.We have refined the source language classifier on the English training data with a 5-fold cross validation. For the English-Chinese Yelp hotel rating dataset, the temperature T (Section 4.1) in the distillation is matched to the validation specified in the target language. For Amazon rating datasets, since there is no standard validation, we set the temperature from low to high in {1, 3, 5, 10} and take the average among all predictions. 3https: / / code.google.com / archive / p / word2vec /"}, {"heading": "5.4 Main Results", "text": "In Tables 2 and 3, we compare the results of our methods (the vanilla version CLD-KCNN and the full version CLDFA-KCNN) with those of other methods based on published results in the literature. The basic methods differ in these two tables because they were previously evaluated (by their authors) using different benchmark data sets. CLDFA-KCNN clearly outperformed the other methods in all but one task in these two data sets, showing that knowledge distillation is successfully performed in our approach. We should also point out that the four basic methods (PL-LSI, PL-KCCA, PL-KCCA and PL-MC) also exceed the target language in narrowing the distribution differences between the parallel corpus and the traction / test data in the target domain."}, {"heading": "6 Conclusion", "text": "This paper presents a novel framework for distilling discriminatory knowledge between languages and provides effective and efficient algorithmic solutions to address domain / distribution mismatches in CLTC. The outstanding performance of our approach is evident in our evaluation of two CLTC benchmark datasets compared to other state-of-the-art methods."}, {"heading": "Acknowledgement", "text": "We thank the reviewers for their helpful comments. This work is supported in part by the Defense Advanced Research Projects Agency Information Innovation Oce (I2O), the Low Resource Languages for Emergent Incidents (LORELEI) Program, issued by DARPA / I2O under contract number HR0011-15-C-0114, and by the National Science Foundation (NSF) under grant number IIS-1546329."}], "references": [{"title": "Do deep nets really need to be deep? In Advances in neural information processing systems", "author": ["Jimmy Ba", "Rich Caruana."], "venue": "pages 2654\u20132662.", "citeRegEx": "Ba and Caruana.,? 2014", "shortCiteRegEx": "Ba and Caruana.", "year": 2014}, {"title": "Cross-lingual text categorization", "author": ["Nuria Bel", "Cornelis HA Koster", "Marta Villegas."], "venue": "Research and Advanced Technology for Digital Libraries pages 126\u2013139.", "citeRegEx": "Bel et al\\.,? 2003", "shortCiteRegEx": "Bel et al\\.", "year": 2003}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["Avrim Blum", "Tom Mitchell."], "venue": "Proceedings of the eleventh annual conference on Computational learning theory. ACM, pages 92\u2013100.", "citeRegEx": "Blum and Mitchell.,? 1998", "shortCiteRegEx": "Blum and Mitchell.", "year": 1998}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["Minmin Chen", "Zhixiang Xu", "Kilian Weinberger", "Fei Sha."], "venue": "arXiv preprint arXiv:1206.4683 .", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Adversarial deep averaging networks for cross-lingual sentiment classification", "author": ["Xilun Chen", "Ben Athiwaratkun", "Yu Sun", "Kilian Weinberger", "Claire Cardie."], "venue": "arXiv preprint arXiv:1606.01614 .", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12(Aug):2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Semi-supervised sequence learning", "author": ["Andrew M Dai", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems. pages 3079\u20133087.", "citeRegEx": "Dai and Le.,? 2015", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Knowledge transfer across multilingual corpora via latent topics", "author": ["Wim De Smet", "Jie Tang", "Marie-Francine Moens."], "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, pages 549\u2013560.", "citeRegEx": "Smet et al\\.,? 2011", "shortCiteRegEx": "Smet et al\\.", "year": 2011}, {"title": "Is machine translation ripe for cross-lingual sentiment classification", "author": ["Kevin Duh", "Akinori Fujino", "Masaaki Nagata"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies:", "citeRegEx": "Duh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duh et al\\.", "year": 2011}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Yaroslav Ganin", "Victor Lempitsky."], "venue": "arXiv preprint arXiv:1409.7495 .", "citeRegEx": "Ganin and Lempitsky.,? 2014", "shortCiteRegEx": "Ganin and Lempitsky.", "year": 2014}, {"title": "Cross language text classification via subspace co-regularized multiview learning", "author": ["Yuhong Guo", "Min Xiao."], "venue": "arXiv preprint arXiv:1206.6481 .", "citeRegEx": "Guo and Xiao.,? 2012a", "shortCiteRegEx": "Guo and Xiao.", "year": 2012}, {"title": "Transductive representation learning for cross-lingual text classification", "author": ["Yuhong Guo", "Min Xiao."], "venue": "Data Mining (ICDM), 2012 IEEE 12th International Conference on. IEEE, pages 888\u2013893.", "citeRegEx": "Guo and Xiao.,? 2012b", "shortCiteRegEx": "Guo and Xiao.", "year": 2012}, {"title": "Cross modal distillation for supervision transfer", "author": ["Saurabh Gupta", "Judy Hoffman", "Jitendra Malik."], "venue": "arXiv preprint arXiv:1507.00448 .", "citeRegEx": "Gupta et al\\.,? 2015", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean."], "venue": "arXiv preprint arXiv:1503.02531 .", "citeRegEx": "Hinton et al\\.,? 2015", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Iyyer et al\\.,? 2015", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Improving bilingual projections via sparse covariance matrices", "author": ["Jagadeesh Jagarlamudi", "Raghavendra Udupa", "Hal Daum\u00e9 III", "Abhijit Bhole."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Associa-", "citeRegEx": "Jagarlamudi et al\\.,? 2011", "shortCiteRegEx": "Jagarlamudi et al\\.", "year": 2011}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["Rie Johnson", "Tong Zhang."], "venue": "arXiv preprint arXiv:1412.1058 .", "citeRegEx": "Johnson and Zhang.,? 2014", "shortCiteRegEx": "Johnson and Zhang.", "year": 2014}, {"title": "Supervised and semi-supervised text categorization using lstm for region embeddings", "author": ["Rie Johnson", "Tong Zhang."], "venue": "Proceedings of The 33rd International Conference on Machine Learning. pages 526\u2013534.", "citeRegEx": "Johnson and Zhang.,? 2016", "shortCiteRegEx": "Johnson and Zhang.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882 .", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Sequencelevel knowledge distillation", "author": ["Yoon Kim", "Alexander M Rush."], "venue": "arXiv preprint arXiv:1606.07947 .", "citeRegEx": "Kim and Rush.,? 2016", "shortCiteRegEx": "Kim and Rush.", "year": 2016}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["Siwei Lai", "Liheng Xu", "Kang Liu", "Jun Zhao."], "venue": "AAAI. pages 2267\u20132273.", "citeRegEx": "Lai et al\\.,? 2015", "shortCiteRegEx": "Lai et al\\.", "year": 2015}, {"title": "An empirical study on sentiment classification of chinese review using word embedding", "author": ["Yiou Lin", "Hang Lei", "Jia Wu", "Xiaoyu Li."], "venue": "arXiv preprint arXiv:1511.01665 .", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Automatic cross-language information retrieval using latent semantic indexing", "author": ["Michael L Littman", "Susan T Dumais", "Thomas K Landauer."], "venue": "Cross-language information retrieval, Springer, pages 51\u201362.", "citeRegEx": "Littman et al\\.,? 1998", "shortCiteRegEx": "Littman et al\\.", "year": 1998}, {"title": "Joint bilingual sentiment classification with unlabeled parallel corpora", "author": ["Bin Lu", "Chenhao Tan", "Claire Cardie", "Benjamin K Tsou."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language", "citeRegEx": "Lu et al\\.,? 2011", "shortCiteRegEx": "Lu et al\\.", "year": 2011}, {"title": "Visualizing data using t-sne", "author": ["Laurens van der Maaten", "Geoffrey Hinton."], "venue": "Journal of Machine Learning Research 9(Nov):2579\u20132605.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Cross-lingual mixture model for sentiment classification", "author": ["Xinfan Meng", "Furu Wei", "Xiaohua Liu", "Ming Zhou", "Ge Xu", "Houfeng Wang."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-", "citeRegEx": "Meng et al\\.,? 2012", "shortCiteRegEx": "Meng et al\\.", "year": 2012}, {"title": "Learning multilingual subjective language via cross-lingual projections", "author": ["Rada Mihalcea", "Carmen Banea", "Janyce M Wiebe"], "venue": null, "citeRegEx": "Mihalcea et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2007}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distilling word embeddings: An encoding approach", "author": ["Lili Mou", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin."], "venue": "arXiv preprint arXiv:1506.04488 .", "citeRegEx": "Mou et al\\.,? 2015", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "Translingual document representations from discriminative projections", "author": ["John C Platt", "Kristina Toutanova", "Wen-tau Yih."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Compu-", "citeRegEx": "Platt et al\\.,? 2010", "shortCiteRegEx": "Platt et al\\.", "year": 2010}, {"title": "Crosslanguage text classification using structural correspondence learning", "author": ["Peter Prettenhofer", "Benno Stein."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguis-", "citeRegEx": "Prettenhofer and Stein.,? 2010", "shortCiteRegEx": "Prettenhofer and Stein.", "year": 2010}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.6550 .", "citeRegEx": "Romero et al\\.,? 2014", "shortCiteRegEx": "Romero et al\\.", "year": 2014}, {"title": "Cross language text classification by model translation and semi-supervised learning", "author": ["Lei Shi", "Rada Mihalcea", "Mingjun Tian."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association", "citeRegEx": "Shi et al\\.,? 2010", "shortCiteRegEx": "Shi et al\\.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Um-corpus: A large english-chinese parallel corpus for statistical machine translation", "author": ["Liang Tian", "Derek F Wong", "Lidia S Chao", "Paulo Quaresma", "Francisco Oliveira", "Lu Yi."], "venue": "LREC. pages 1837\u20131842.", "citeRegEx": "Tian et al\\.,? 2014", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Inferring a semantic representation of text via cross-language correlation analysis", "author": ["Alexei Vinokourov", "John Shawe-Taylor", "Nello Cristianini."], "venue": "NIPS. volume 1, page 4.", "citeRegEx": "Vinokourov et al\\.,? 2002", "shortCiteRegEx": "Vinokourov et al\\.", "year": 2002}, {"title": "Co-training for cross-lingual sentiment classification", "author": ["Xiaojun Wan."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL", "citeRegEx": "Wan.,? 2009", "shortCiteRegEx": "Wan.", "year": 2009}, {"title": "A novel two-step method for cross language representation learning", "author": ["Min Xiao", "Yuhong Guo."], "venue": "Advances in Neural Information Processing Systems. pages 1259\u20131267.", "citeRegEx": "Xiao and Guo.,? 2013", "shortCiteRegEx": "Xiao and Guo.", "year": 2013}, {"title": "Cross-lingual text classification via model translation with limited dictionaries", "author": ["Ruochen Xu", "Yiming Yang", "Hanxiao Liu", "Andrew Hsi."], "venue": "Proceedings of the 25th ACM International on Conference on Information and Knowledge Management.", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Hierarchical attention networks for document classification", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Dependency sensitive convolutional neural networks for modeling sentences and documents", "author": ["Rui Zhang", "Honglak Lee", "Dragomir Radev."], "venue": "arXiv preprint arXiv:1611.02361 .", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "Advances in neural information processing systems. pages 649\u2013657.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Learning bilingual sentiment word embeddings for cross-language sentiment classification", "author": ["Huiwei Zhou", "Long Chen", "Fulin Shi", "Degen Huang."], "venue": "ACL.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "2016a. Cross-lingual sentiment classification with bilingual document representation learning", "author": ["Xinjie Zhou", "Xianjun Wan", "Jianguo Xiao"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "2016b. Attention-based lstm network for cross-lingual sentiment classification", "author": ["Xinjie Zhou", "Xiaojun Wan", "Jianguo Xiao"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 38, "context": "Existing methods in CLTC use either a bilingual dictionary or a parallel corpus to bridge language barriers and to translate classification models (Xu et al., 2016) or text data(Zhou et al.", "startOffset": 147, "endOffset": 164}, {"referenceID": 8, "context": "Parallel-corpus based methods, although more effective in deploying context (when combined with word embedding in particular), often have an issue of domain mismatch or distribution mismatch if the available source-language training data, the parallel corpus (human-aligned or machine-translation induced one) and the target documents of interest are not in exactly the same domain and genre(Duh et al., 2011).", "startOffset": 391, "endOffset": 409}, {"referenceID": 13, "context": "It is inspired by the recent work in model compression (Hinton et al., 2015) where a large ensemble model is transformed to a compact (small) model.", "startOffset": 55, "endOffset": 76}, {"referenceID": 1, "context": "(Bel et al., 2003) was the first effort to solve CLTC problem.", "startOffset": 0, "endOffset": 18}, {"referenceID": 26, "context": "(Mihalcea et al., 2007) built cross-lingual classifier by translating subjectivity words and phrases in the source language into the target language.", "startOffset": 0, "endOffset": 23}, {"referenceID": 32, "context": "(Shi et al., 2010) also utilized a bilingual dictionary.", "startOffset": 0, "endOffset": 18}, {"referenceID": 30, "context": "(Prettenhofer and Stein, 2010) also used the bilingual dictionary as a word translation oracle and built their CLTC system on structural correspondence learning, a theory for domain adaptation.", "startOffset": 0, "endOffset": 30}, {"referenceID": 38, "context": "A more recent work by (Xu et al., 2016) extended seminal bilingual dictionaries with unlabeled corpora in low-resource languages.", "startOffset": 22, "endOffset": 39}, {"referenceID": 4, "context": "(Chen et al., 2016) used bilingual word embedding to map documents in source and target language into the same semantic space,", "startOffset": 0, "endOffset": 19}, {"referenceID": 36, "context": "For example, Wan (Wan, 2009) used machine translation systems to give each document a source-language and a target-language version, where one version is machine-translated from the another one.", "startOffset": 17, "endOffset": 28}, {"referenceID": 2, "context": "A co-training (Blum and Mitchell, 1998) algorithm was applied on two versions of both source and target documents to iterative train classifiers in both languages.", "startOffset": 14, "endOffset": 39}, {"referenceID": 37, "context": ", 2009), matrix completion(Xiao and Guo, 2013) and multi-view coregularization(Guo and Xiao, 2012a).", "startOffset": 26, "endOffset": 46}, {"referenceID": 10, "context": ", 2009), matrix completion(Xiao and Guo, 2013) and multi-view coregularization(Guo and Xiao, 2012a).", "startOffset": 78, "endOffset": 99}, {"referenceID": 11, "context": "Another branch of CLTC methods focuses on representation learning or the mapping of the induced representations in cross-language settings (Guo and Xiao, 2012b; Zhou et al., 2016a, 2015, 2016b; Xiao and Guo, 2013; Jagarlamudi et al., 2011; De Smet et al., 2011; Vinokourov et al., 2002; Platt et al., 2010; Littman et al., 1998).", "startOffset": 139, "endOffset": 328}, {"referenceID": 37, "context": "Another branch of CLTC methods focuses on representation learning or the mapping of the induced representations in cross-language settings (Guo and Xiao, 2012b; Zhou et al., 2016a, 2015, 2016b; Xiao and Guo, 2013; Jagarlamudi et al., 2011; De Smet et al., 2011; Vinokourov et al., 2002; Platt et al., 2010; Littman et al., 1998).", "startOffset": 139, "endOffset": 328}, {"referenceID": 15, "context": "Another branch of CLTC methods focuses on representation learning or the mapping of the induced representations in cross-language settings (Guo and Xiao, 2012b; Zhou et al., 2016a, 2015, 2016b; Xiao and Guo, 2013; Jagarlamudi et al., 2011; De Smet et al., 2011; Vinokourov et al., 2002; Platt et al., 2010; Littman et al., 1998).", "startOffset": 139, "endOffset": 328}, {"referenceID": 35, "context": "Another branch of CLTC methods focuses on representation learning or the mapping of the induced representations in cross-language settings (Guo and Xiao, 2012b; Zhou et al., 2016a, 2015, 2016b; Xiao and Guo, 2013; Jagarlamudi et al., 2011; De Smet et al., 2011; Vinokourov et al., 2002; Platt et al., 2010; Littman et al., 1998).", "startOffset": 139, "endOffset": 328}, {"referenceID": 29, "context": "Another branch of CLTC methods focuses on representation learning or the mapping of the induced representations in cross-language settings (Guo and Xiao, 2012b; Zhou et al., 2016a, 2015, 2016b; Xiao and Guo, 2013; Jagarlamudi et al., 2011; De Smet et al., 2011; Vinokourov et al., 2002; Platt et al., 2010; Littman et al., 1998).", "startOffset": 139, "endOffset": 328}, {"referenceID": 22, "context": "Another branch of CLTC methods focuses on representation learning or the mapping of the induced representations in cross-language settings (Guo and Xiao, 2012b; Zhou et al., 2016a, 2015, 2016b; Xiao and Guo, 2013; Jagarlamudi et al., 2011; De Smet et al., 2011; Vinokourov et al., 2002; Platt et al., 2010; Littman et al., 1998).", "startOffset": 139, "endOffset": 328}, {"referenceID": 25, "context": "(Meng et al., 2012) and Lu et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 23, "context": "(Lu et al., 2011) used a parallel corpus to learn word alignment probabilities in a pre-processing step.", "startOffset": 0, "endOffset": 17}, {"referenceID": 22, "context": "Some other work attempts to find a languageinvariant (or interlingua) representation for words or documents in different languages using various techniques, such as latent semantic indexing (Littman et al., 1998), kernel canonical correlation analysis (Vinokourov et al.", "startOffset": 190, "endOffset": 212}, {"referenceID": 35, "context": ", 1998), kernel canonical correlation analysis (Vinokourov et al., 2002), matrix completion(Xiao and Guo, 2013), principal component analysis (Platt et al.", "startOffset": 47, "endOffset": 72}, {"referenceID": 37, "context": ", 2002), matrix completion(Xiao and Guo, 2013), principal component analysis (Platt et al.", "startOffset": 26, "endOffset": 46}, {"referenceID": 29, "context": ", 2002), matrix completion(Xiao and Guo, 2013), principal component analysis (Platt et al., 2010) and Bayesian graphical models (De Smet et al.", "startOffset": 77, "endOffset": 97}, {"referenceID": 13, "context": "The idea of distilling knowledge in a neural network was proposed by Hinton et al (Hinton et al., 2015), in which they introduced a student-teacher paradigm.", "startOffset": 82, "endOffset": 103}, {"referenceID": 13, "context": "(Hinton et al., 2015; Romero et al., 2014; Ba and Caruana, 2014).", "startOffset": 0, "endOffset": 64}, {"referenceID": 31, "context": "(Hinton et al., 2015; Romero et al., 2014; Ba and Caruana, 2014).", "startOffset": 0, "endOffset": 64}, {"referenceID": 0, "context": "(Hinton et al., 2015; Romero et al., 2014; Ba and Caruana, 2014).", "startOffset": 0, "endOffset": 64}, {"referenceID": 12, "context": "(Gupta et al., 2015) transfers supervision between images from different modalities(e.", "startOffset": 0, "endOffset": 20}, {"referenceID": 28, "context": "(Mou et al., 2015) distilled task specific knowledge from a set of high-dimensional embeddings to a lowdimensional space.", "startOffset": 0, "endOffset": 18}, {"referenceID": 19, "context": "(Kim and Rush, 2016) applied knowledge distillation approaches in the field of machine translation to reduce the size of neural machine translation model.", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "We use a state-of-the-art CNN-based neural network classifier (Kim, 2014) as the plug-in classifier in our framework.", "startOffset": 62, "endOffset": 73}, {"referenceID": 5, "context": "A max-over-time pooling (Collobert et al., 2011) is applied on c over all possible positions such that each filter extracts one feature.", "startOffset": 24, "endOffset": 48}, {"referenceID": 16, "context": "There is a large family of neural classifiers for monolingual text classification that could be used in our framework as well, including other convolutional neural networks by (Johnson and Zhang, 2014), the recurrent neural networks by (Lai et al.", "startOffset": 176, "endOffset": 201}, {"referenceID": 20, "context": "There is a large family of neural classifiers for monolingual text classification that could be used in our framework as well, including other convolutional neural networks by (Johnson and Zhang, 2014), the recurrent neural networks by (Lai et al., 2015; Zhang et al., 2016; Johnson and Zhang, 2016; Sutskever et al., 2014; Dai and Le, 2015), the attention mechanism by (Yang et al.", "startOffset": 236, "endOffset": 341}, {"referenceID": 40, "context": "There is a large family of neural classifiers for monolingual text classification that could be used in our framework as well, including other convolutional neural networks by (Johnson and Zhang, 2014), the recurrent neural networks by (Lai et al., 2015; Zhang et al., 2016; Johnson and Zhang, 2016; Sutskever et al., 2014; Dai and Le, 2015), the attention mechanism by (Yang et al.", "startOffset": 236, "endOffset": 341}, {"referenceID": 17, "context": "There is a large family of neural classifiers for monolingual text classification that could be used in our framework as well, including other convolutional neural networks by (Johnson and Zhang, 2014), the recurrent neural networks by (Lai et al., 2015; Zhang et al., 2016; Johnson and Zhang, 2016; Sutskever et al., 2014; Dai and Le, 2015), the attention mechanism by (Yang et al.", "startOffset": 236, "endOffset": 341}, {"referenceID": 33, "context": "There is a large family of neural classifiers for monolingual text classification that could be used in our framework as well, including other convolutional neural networks by (Johnson and Zhang, 2014), the recurrent neural networks by (Lai et al., 2015; Zhang et al., 2016; Johnson and Zhang, 2016; Sutskever et al., 2014; Dai and Le, 2015), the attention mechanism by (Yang et al.", "startOffset": 236, "endOffset": 341}, {"referenceID": 6, "context": "There is a large family of neural classifiers for monolingual text classification that could be used in our framework as well, including other convolutional neural networks by (Johnson and Zhang, 2014), the recurrent neural networks by (Lai et al., 2015; Zhang et al., 2016; Johnson and Zhang, 2016; Sutskever et al., 2014; Dai and Le, 2015), the attention mechanism by (Yang et al.", "startOffset": 236, "endOffset": 341}, {"referenceID": 39, "context": ", 2014; Dai and Le, 2015), the attention mechanism by (Yang et al., 2016), the deep dense network by (Iyyer et al.", "startOffset": 54, "endOffset": 73}, {"referenceID": 14, "context": ", 2016), the deep dense network by (Iyyer et al., 2015), and more.", "startOffset": 35, "endOffset": 55}, {"referenceID": 24, "context": "Then we projected the highdimensional features onto a 2-dimensional space via t-Distributed Stochastic Neighbor Embedding (t-SNE)(Maaten and Hinton, 2008).", "startOffset": 129, "endOffset": 154}, {"referenceID": 9, "context": "We adapt the adversarial training method by (Ganin and Lempitsky, 2014) to the cross-lingual settings in our problems.", "startOffset": 44, "endOffset": 71}, {"referenceID": 9, "context": "We jointly optimize \u03b8f , \u03b8y, \u03b8d through the gradient reversal layer(Ganin and Lempitsky, 2014).", "startOffset": 67, "endOffset": 94}, {"referenceID": 30, "context": "We used the multilingual multi-domain Amazon review dataset created by Prettenhofer and Stein (Prettenhofer and Stein, 2010).", "startOffset": 94, "endOffset": 124}, {"referenceID": 30, "context": "(Prettenhofer and Stein, 2010) also provides 2000 parallel reviews per task,", "startOffset": 0, "endOffset": 30}, {"referenceID": 4, "context": "This dataset was firstly used for CLTC by (Chen et al., 2016).", "startOffset": 42, "endOffset": 61}, {"referenceID": 41, "context": "(Zhang et al., 2015).", "startOffset": 0, "endOffset": 20}, {"referenceID": 21, "context": "The Chinese data includes 20k labeled Chinese hotel reviews and 1037k unlabeled ones from (Lin et al., 2015).", "startOffset": 90, "endOffset": 108}, {"referenceID": 4, "context": "Following the approach by (Chen et al., 2016), we use 10k of labeled Chinese data as validation set and another 10k hotel reviews as held-out test data.", "startOffset": 26, "endOffset": 45}, {"referenceID": 34, "context": "We a random sample of 500k parallel sentences from UM-courpus(Tian et al., 2014), which is a general-purpose corpus designed for machine translation.", "startOffset": 61, "endOffset": 80}, {"referenceID": 29, "context": ", 1998), PL-OPCA (Platt et al., 2010) and PL-KCAA (Vinokourov et al.", "startOffset": 17, "endOffset": 37}, {"referenceID": 35, "context": ", 2010) and PL-KCAA (Vinokourov et al., 2002) learn latent document representations in a shared lowdimensional space by performing the Latent Semantic Indexing (LSI), the Oriented Principal Component Analysis (OPCA) and a kernel (namely KCAA) for the parallel text.", "startOffset": 20, "endOffset": 45}, {"referenceID": 37, "context": "PL-MC (Xiao and Guo, 2013) recovers missing features via matrix Completion, and also uses LSI to induce a latent space for parallel text.", "startOffset": 6, "endOffset": 26}, {"referenceID": 4, "context": "The prediction on each translated document is made by a source-language classifier, which can be a Logistic Regression model (MT+LR) (Chen et al., 2016) or a deep averaging network (MT+DAN) (Chen et al.", "startOffset": 133, "endOffset": 152}, {"referenceID": 4, "context": ", 2016) or a deep averaging network (MT+DAN) (Chen et al., 2016).", "startOffset": 45, "endOffset": 64}, {"referenceID": 4, "context": "Similar to our approach, the adversarial Deep Averaging Network (ADAN) also exploits adversarial training for CLTC (Chen et al., 2016).", "startOffset": 115, "endOffset": 134}, {"referenceID": 3, "context": "mSDA (Chen et al., 2012) is a domain adaptation method based on", "startOffset": 5, "endOffset": 24}, {"referenceID": 3, "context": "We show the results reported by (Chen et al., 2012), where they used bilingual word embedding as input for mSDA.", "startOffset": 32, "endOffset": 51}, {"referenceID": 27, "context": "We ran word2vec(Mikolov et al., 2013) 3 on the tokenized unlabeled corpus.", "startOffset": 15, "endOffset": 37}, {"referenceID": 37, "context": "We should also point out that in Table 2, the four baseline methods (PL-LSI, PL-KCCA, PL-OPCA and PL-MC) were evaluated under the condition of using additional 100 labeled target documents for training, according to the author\u2019s report (Xiao and Guo, 2013).", "startOffset": 236, "endOffset": 256}], "year": 2017, "abstractText": "Cross-lingual text classification(CLTC) is the task of classifying documents written in different languages into the same taxonomy of categories. This paper presents a novel approach to CLTC that builds on model distillation, which adapts and extends a framework originally proposed for model compression. Using soft probabilistic predictions for the documents in a label-rich language as the (induced) supervisory labels in a parallel corpus of documents, we train classifiers successfully for new languages in which labeled training data are not available. An adversarial feature adaptation technique is also applied during the model training to reduce distribution mismatch. We conducted experiments on two benchmark CLTC datasets, treating English as the source language and German, French, Japan and Chinese as the unlabeled target languages. The proposed approach had the advantageous or comparable performance of the other state-of-art methods.", "creator": "LaTeX with hyperref package"}}}