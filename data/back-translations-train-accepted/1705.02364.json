{"id": "1705.02364", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2017", "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data", "abstract": "Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference dataset can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks.", "histories": [["v1", "Fri, 5 May 2017 18:54:39 GMT  (542kb)", "http://arxiv.org/abs/1705.02364v1", "11 pages, 8 figures"], ["v2", "Tue, 9 May 2017 13:16:21 GMT  (542kb)", "http://arxiv.org/abs/1705.02364v2", "11 pages, 8 figures"], ["v3", "Tue, 27 Jun 2017 13:15:01 GMT  (542kb)", "http://arxiv.org/abs/1705.02364v3", "11 pages, 8 figures"], ["v4", "Fri, 21 Jul 2017 09:50:36 GMT  (740kb)", "http://arxiv.org/abs/1705.02364v4", "Accepted to EMNLP 2017"]], "COMMENTS": "11 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexis conneau", "douwe kiela", "holger schwenk", "lo\u00efc barrault", "antoine bordes"], "accepted": true, "id": "1705.02364"}, "pdf": {"name": "1705.02364.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Holger Schwenk"], "emails": ["aconneau@fb.com", "dkiela@fb.com", "schwenk@fb.com", "loic.barrault@univ-lemans.fr", "abordes@fb.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.02 364v 1 [cs.C L] 5M ay2 017embeddings that were previously trained unsupervised on large corpora as basic features. However, efforts to obtain embeddings for larger pieces of text such as sentences have not been as successful. Several attempts to learn unsupervised sentence representations have not been sufficiently powerful to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data from the Stanford Natural Language Inference dataset can consistently outperform unsupervised methods such as SkipThought vectors (Kiros et al., 2015) in a variety of transfer tasks. Similar to how Computervision uses ImageNet to obtain features that can then be transferred to other tasks, our work tends to show the suitability of natural linguistic inferences for transferring learning tasks to other NLP tasks."}, {"heading": "1 Introduction", "text": "Distributed representations of words (or word embedding) (Bengio et al., 2003; Hill et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) have shown that they provide useful functions for various tasks in natural language processing and computer vision. While there is consensus on the usefulness of word embedding and how to learn it, this is not yet clear in terms of representations that carry the meaning of a full sentence. That is, how to capture the relationships between multiple words and phrases in a single vector remains a question that needs to be resolved. In this paper, we examine the task of universal representations of sentences, i.e., an encoder model trained on a large corpus and then transferred to other tasks."}, {"heading": "2 Related work", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush."}, {"heading": "3 Approach", "text": "This work combines two lines of research that we describe below: First, we explain how the NLI task can be used to train universal sentence encoding models using the SNLI task; then we describe the architectures we have studied for the sentence encoder, which we believe covers a suitable range of currently used sentence encoders; in particular, we examine standard recurring models such as LSTMs and GRUs, for which we examine averages and maximum dimensions across the hidden representations; a self-aware network that incorporates different views of the sentence; and a hierarchical convolutionary network that can be viewed as a construction-based method that fuses different levels of abstraction."}, {"heading": "3.1 The Natural Language Inference task", "text": "The SNLI dataset consists of 570k human-generated English sentence pairs manually labeled with one of three categories: entropy, contradiction, and neutrality. It captures natural language conclusions, also known as Recognizing Textual Entailment (RTE) in previous incarnations, and represents one of the largest high-quality, explicitly constructed resources for understanding sentence semantics. We believe that the semantic nature of the NLI makes it a good candidate for learning universal sentence embeddings in a supervised manner. That is, we are looking at the v1 of the dataset. A larger, recently released v2 version will be left as future work. To show that sentence coders trained on natural language conclusions are able to learn sentence representations that capture universally useful phrases. Models can be trained on SNLI in two different ways: (i) Sentence-based models that explicitly represent the encoding the sentences."}, {"heading": "3.2 Sentence encoder architectures", "text": "There is a wide variety of neural networks for encrypting fixed-size sentences, and it is not yet clear which of them best captures generally useful information. We compare 7 different architectures: standard recurrent encoders with long short-term memory (LSTM) or gated recurrent units (GRU), concatenation of the last hidden states from forward and backward GRU, bi-directional LSTMs (BiLSTM) with mean or maximum pooling, standalone networks, and hierarchical wave networks."}, {"heading": "3.2.1 LSTM and GRU", "text": "Our first and simplest encoders use recursive neural networks that use either LSTM or GRU modules (Cho et al., 2014), as in sequence to sequence encoders (Sutskever et al., 2014). For a sequence of T-words (w1,..., wT), the network calculates a set of hidden T-representations h1,...., hT, with ht = \u2212 \u2212 \u2192 LSTM (w1,..., wT) (or uses GRU units instead). A set is represented by the last hidden vector hT. We also consider a BiGRU-last model that links the last hidden state of a forward-directed GRU and the last hidden state of a backward-directed GRU to have the same architecture as with SkipThought vectors."}, {"heading": "3.2.2 BiLSTM with mean/max pooling", "text": "For a sequence of T-words {wt} t = 1,..., T a bidirectional LSTM calculates a set of T-vectors {ht} t. For t [1,..., T], ht is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions: \u2212 \u2192 ht = \u2212 \u2212 \u2212 \u2212 \u2192 LSTMt (w1,.., wT) \u2190 \u2212 ht = \u2190 \u2212 \u2212 LSTMt (w1,..., wT) ht = [\u2212 \u2192 ht, \u2190 \u2212 ht] We experiment with two ways to combine the different number of {ht} t to a fixed vector, either by selecting the maximum value over each dimension of the hidden units (max pooling) (Collobert and Weston, 2008) or by looking at the average of the representations (mean pooling)."}, {"heading": "3.2.3 Self-attentive network", "text": "The self-aware sentence encoder (Liu et al., 2016; Lin et al., 2017) uses an attention mechanism on the hidden states of a BiLSTM to generate a representation u of an input sentence. The attention mechanism is defined as: h-i = tanh (Whi + bw) \u03b1i = h-Ti uw-i-h-T i uwu = \u2211 t\u03b1ihiwhere {h1,.., hT} are the output hidden vectors of a BiLSTM. These are applied to an affine transformation (W, bw) which returns a set of keys (barh1,.., barhT). The {\u03b1i} represent the score of similarity between the keys and an learned context query vector, etc. These weights are used to generate the final representation u, which is represented by a weighted linear combination of hidden vectors following the Lw-4, which is a multiple one in the architecture we can use in Lw-4."}, {"heading": "3.2.4 Hierarchical ConvNet", "text": "One of the currently best functioning models on SNLI is a Convolutionary Architecture called AdaSent (Zhao et al., 2015), which concatenates different representations of sentences at different levels of abstraction. Inspired by this architecture, we present a faster version, consisting of 4 Convolutionary Layers. At each level, a representation ui is calculated by a max pooling operation using the feature maps (see Figure 3).The final representation u = [u1, u2, u3, u4] concatenates representations at different levels of the input set, thus capturing hierarchical abstractions of an input set in a specified size."}, {"heading": "3.3 Training details", "text": "For all of our SNLI-trained models, we use SGD with a learning rate of 0.1 and a weight degradation of 0.99. At each epoch, we divide the learning rate by 5 as development accuracy decreases. We use 64-size minibatches and the training is terminated when the learning rate drops below the threshold of 10 \u2212 5. For the classifier, we use a multi-layer perceptron with a hidden layer of 512 hidden units. We use 300-dimensional open source GloVe vectors trained on Common Crawl840B2 as fixed word embedding and initialize other word vectors to random values sampled by U (\u2212 0.1, 0.1). Input inputs are pre-processed with the Penn treadmill token from the NLTK toolkit."}, {"heading": "4 Evaluation of sentence representations", "text": "Our goal is to obtain universally valid sentence embeddings that capture generic information that is useful for a wide range of tasks. To evaluate the quality of these representations, we use them as features in 12 transfer tasks. We present our sentence embedding evaluation process in this section. We construct a sentence evaluation tool to automate the evaluation of all the tasks mentioned in this paper. The tool uses a set of binary classification tasks (see Table 1) that covers various types of sentence classification, including sentiment analysis (MR, SST), question type (TREC), product evaluation (CR), subjectivity / objectivity (SUBJ), and opinion polarity (MPQA)."}, {"heading": "5 Empirical results", "text": "In this section, we refer to \"micro\" and \"macro\" averages of development results for transfer tasks whose measurement value is accuracy: We calculate a \"macro\" total value, which corresponds to the classic average of development accuracies, and the \"micro\" value, which is the sum of development accuracies, weighted by the number of development patterns."}, {"heading": "5.1 Architecture impact", "text": "As a matter of fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "5.2 Task transfer", "text": "We report on the different types of learning that we have pursued over the past few years. \"Over the past few years, we have repeatedly set out in search of new paths,\" he says, \"but we have not yet understood why it has come to this point.\" \"We have repeatedly set out in search of new paths,\" he says, \"but we have not yet understood why it has come to this point.\" \"We have not yet understood why it has come to this point.\" \"We have not yet understood why it has come to this point.\" \"We have not yet understood why it has.\" \"We have not yet understood why it has come to this point.\" \"We have not yet understood.\" We have not yet understood. \"We have not yet understood.\" We have not yet understood. \"We have not yet understood.\""}, {"heading": "6 Conclusion", "text": "This paper examines the effects of training set embedding with monitored data through testing on 12 different transfer tasks. We have shown that models learned on SNLI can perform better than models trained under unattended conditions or for other monitored tasks. By studying different architectures, we have shown that a BiLSTM network with maximum pooling produces the best current universal sentence encoding methods and surpasses existing approaches such as SkipThought vectors. We believe that this work only scratches the surface of possible combinations of models and tasks for learning general sentence embedding. Larger data sets based on understanding natural language for sentences could take sentence embedding quality to the next level. Thus, future work should confirm our findings about the latest new version of SNLI, MultiNLI4, and also examine how the recently introduced gated convolutional architectures (Dauphin et al., 2016) perform in transfer scenarios."}, {"heading": "7 Appendix", "text": "Max pooling visualization for BiLSTM-max trained and untrained In Table 7 and Table 8, we examine how the Max pooling operation selects information from the hidden states of BiLSTM, for our trained and untrained BiLSTMmax models (for both models word embedding is initialized with GloVe vectors).For each step t, we report how often the Max pooling operation has selected the hidden state ht (which can be seen as sentence representation around word wt).Without training, Max pooling proceeds fairly evenly across hidden states, although it consistently seems to focus more on the first and last hidden state. When the model is trained, it learns to focus on specific words that carry the meaning of the sentence. Note that each hidden state also contains information from the sentence at different levels, which explains why the trained model also includes information from all hidden states."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference dataset can consistently outperform unsupervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks.", "creator": "LaTeX with hyperref package"}}}