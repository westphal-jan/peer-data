{"id": "1705.02925", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2017", "title": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment", "abstract": "Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase(PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive embeddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.", "histories": [["v1", "Mon, 8 May 2017 15:40:51 GMT  (198kb,D)", "http://arxiv.org/abs/1705.02925v1", "ACL 2017"]], "COMMENTS": "ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["pradeep dasigi", "waleed ammar", "chris dyer", "eduard h hovy"], "accepted": true, "id": "1705.02925"}, "pdf": {"name": "1705.02925.pdf", "metadata": {"source": "CRF", "title": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment", "authors": ["Pradeep Dasigi", "Waleed Ammar", "Chris Dyer", "Eduard Hovy"], "emails": ["pdasigi@cs.cmu.edu,", "wammar@allenai.org,", "cdyer@cs.cmu.edu,", "hovy@cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Most word embedding models define a single vector for each word we use, but not for each word we use."}, {"heading": "2 WordNet-Grounded Context-Sensitive Token Embeddings", "text": "In this section, we focus on defining our context-sensitive token embeddings. First, we describe how we ground word types using WordNet concepts. Then, we describe our model of context-sensitive token embeddings as a weighted sum of WordNet concept embeddings."}, {"heading": "2.1 WordNet Grounding", "text": "We use WordNet to assign each word type to a set of synsets, including possible generalizations or abstractions. Among the relationships between different synsets defined in WordNet, we focus on the hypernymy relationship to model the generalization and selective preferences between words, which is especially important for predicting PP attachments (Resnik, 1993). To ground a word type, we identify the set of (direct and indirect) hypernyms of the WordNet sense of that word. A simplified grounding of the word \"pool\" is illustrated in Figure 1. This grounding is the key to our model of embedding tokens, which is described in the following subsections."}, {"heading": "2.2 Context-Sensitive Token Embeddings", "text": "Our goal is to define a context-sensitive model of embedding that can be used as a substitute for traditional type terms. (We) are not able to define these terms. (We) are not able to capture the list of hypernims for a syntax. (We) are not able to capture the list of hypernims for a syntax. (We) are not able to define the type of hypernims for a syntax. (We) are not able to define the type of hypernims for a syntax. (We) are not able to define the type of hypernims for a syntax. (We) are not able to use the type of hypernims for a syntax. (Each WordNet synset is associated with a set of parameters against Rn that represent their embedding. (These parameters are similar to those of Rothe and Protectors (2015).Embedding model."}, {"heading": "3 PP Attachment", "text": "Disambiguity of PP attachments is an important and challenging NLP problem. Since modeling hypernymic and selective preferences is critical to the successful prediction of PP attachments (Resnik, 1993), it lends itself well to evaluating our WordNet-based context-sensitive embeddings. Figure 3, reproduced by Belinkov et al. (2014), illustrates an example of the PP attachment problem. The accuracy of a competing English dependency saver in predicting the header of an ambiguous prepositional phrase is 88.5%, significantly lower than the general unlabeled attachment precision of the same parser (94.2%).4This section formally defines the problem of PP attachment disambiguity, describes our base model, and then shows how to integrate token-level embedding into the model."}, {"heading": "3.1 Problem Definition", "text": "We follow the definition of the PP binding problem by Belinkov et al. (2014). Considering a preposition p and a mechanism of soft attention typically used to explicitly represent the meaning of each item in a sequence, it can also be applied to non-sequential elements. See Table 2 in paragraph 4 for detailed results. Its direct dependency ratio d in the prepositional phrase (PP) is our goal to identify the correct header for the PP among an ordered list of candidate header words. Each example in turn, validation and test sets consist of an input tupel < h, p, d > and an output index k to identify the correct header among the candidates in h. Note that the order of words that make up each < h, p, d > is the same as in the corresponding original sentence."}, {"heading": "3.2 Model Definition", "text": "In fact, we are going to be able to be able to be able to be able to be able to be able to be able to be able to be able to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position."}, {"heading": "4 Experiments", "text": "We used the English PP attachment dataset that we created and provided. (2014) The training and test splits each contain 33,359 and 1951 marked examples. As explained in \u00a7 3.1, the input for each example is 1) an ordered list of candidate headers, 2) the preposition, and 3) the direct-dependent use of the preposition. The headers are either nouns or verbs, and the dependency is always a noun. All examples in this dataset have at least two candidate headers. As discussed in Belinkov et al, this dataset is a more realistic PP attachment task than the RRR dataset al. 1994) The RRR dataset is a binary classification task with exactly two header candidates in all examples. The context for each example in the RRR dataset is also limited, which undermines the purpose of our context-sensitive datasets."}, {"heading": "4.1 PP Attachment Results", "text": "Table 1 shows that our proposed token-level embedding scheme OntoLSTM-PP or PP outperforms the better variant of our base line LSTM-PP (with GloVe retro initialization) by an absolute accuracy difference of 4.9% or a relative error reduction of 32%. OntoLSTM-PP also outperforms HPCD (full), the best result so far on this dataset. Initializing the word embedding with GloVeretro (the WordNet as in Faruqui et al. (2015)) instead of GloVe amounts to a small improvement compared to the improvements achieved with OntoLSTM-PP. This result illustrates that our approach of dynamically selecting a context-sensitive distribution via synsets is a more effective way to make use of Word Networks. We are having an impact on dependency analyses. Following Belinkov et al (2014) we used BBG Parfix (2014), we modified BG Ral (2014)."}, {"heading": "4.2 Analysis", "text": "This year, more than ever before in the history of a country in which it is a country in which it is a country in which it is not a country in which it is not a country, but a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a"}, {"heading": "5 Related Work", "text": "This work is related to various lines of research within the NLP community: Dealing with synonymic and homonymic reference systems in word representations both in the context of distributed embeddings and more traditional vector spaces; hybrid models of distribution- and knowledge-based semantics 1993; and selective preferences and their relationship to syntactic and semantic relationships. The need to go beyond a single vector per word type has been known for some time, and many efforts have focused on building multiprototype vector space models of meaning (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Jauhar et al., 2015; Neelakantan et al., 2015; Arora et al., 2016, etc.). However, the goal of all these approaches is to obtain multi-meaningful word vector spaces, either by incorporating meaningful information or other types of external contexts. The number of predictors, the classes, are predetermined based on."}, {"heading": "6 Conclusion", "text": "In this paper, we proposed grounding lexical elements that recognize the semantic ambiguity of word types using WordNet, and a method to learn a context-sensitive distribution of their representations. We also demonstrated how the proposed representation can be integrated with recursive neural networks to disambiguate prepositional attachments. We provided a detailed qualitative and quantitative analysis of the proposed model. Implementation and code availability. The models are implemented with keras (Chollet, 2015), and the functionality is available at http: / / github.com / pdasigi / onto-lstm in the form of keras layers to make it easier to embed the proposed embedding model into other NLP problems. This approach can be extended to other NLP tasks that may benefit from the use of coding layers in the form of keras layers."}, {"heading": "Acknowledgements", "text": "The first author is supported by a grant from the Allen Institute for Artificial Intelligence. We thank Matt Gardner, Jayant Krishnamurthy, Julia Hockenmaier, Oren Etzioni, Hector Liu, Filip Ilievski and anonymous reviewers for their comments."}], "references": [{"title": "Improving parsing and pp attachment performance with sense information", "author": ["Eneko Agirre."], "venue": "ACL. Citeseer.", "citeRegEx": "Agirre.,? 2008", "shortCiteRegEx": "Agirre.", "year": 2008}, {"title": "Linear algebraic structure of word senses, with applications to polysemy", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski."], "venue": "arXiv preprint arXiv:1601.03764 .", "citeRegEx": "Arora et al\\.,? 2016", "shortCiteRegEx": "Arora et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A linear dynamical system model for text", "author": ["David Belanger", "Sham M. Kakade."], "venue": "ICML.", "citeRegEx": "Belanger and Kakade.,? 2015", "shortCiteRegEx": "Belanger and Kakade.", "year": 2015}, {"title": "Exploring compositional architectures and word vector representations for prepositional phrase attachment", "author": ["Yonatan Belinkov", "Tao Lei", "Regina Barzilay", "Amir Globerson."], "venue": "Transactions of the Association for Computational Linguistics 2:561\u2013572.", "citeRegEx": "Belinkov et al\\.,? 2014", "shortCiteRegEx": "Belinkov et al\\.", "year": 2014}, {"title": "A rule-based approach to prepositional phrase attachment disambiguation", "author": ["Eric Brill", "Philip Resnik."], "venue": "Proceedings of the 15th conference on Computational linguistics-Volume 2. Association for Computational Linguistics, pages 1198\u20131204.", "citeRegEx": "Brill and Resnik.,? 1994", "shortCiteRegEx": "Brill and Resnik.", "year": 1994}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "EMNLP. pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun."], "venue": "EMNLP. pages 1025\u20131035.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet."], "venue": "https://github. com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard H. Hovy", "Noah A. Smith."], "venue": "NAACL.", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Ontologically grounded multi-sense representation learning for semantic vector space models", "author": ["Sujay Kumar Jauhar", "Chris Dyer", "Eduard H. Hovy."], "venue": "NAACL.", "citeRegEx": "Jauhar et al\\.,? 2015", "shortCiteRegEx": "Jauhar et al\\.", "year": 2015}, {"title": "Embedding a semantic network in a word space", "author": ["Richard Johansson", "Luis Nieto Pina."], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics\u2013Human Language Technologies.", "citeRegEx": "Johansson and Pina.,? 2015", "shortCiteRegEx": "Johansson and Pina.", "year": 2015}, {"title": "A large-scale classification of english verbs", "author": ["Karin Kipper", "Anna Korhonen", "Neville Ryant", "Martha Palmer."], "venue": "Language Resources and Evaluation 42(1):21\u201340.", "citeRegEx": "Kipper et al\\.,? 2008", "shortCiteRegEx": "Kipper et al\\.", "year": 2008}, {"title": "Neural architectures for named entity recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."], "venue": "NAACL.", "citeRegEx": "Lample et al\\.,? 2016", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Low-rank tensors for scoring dependency structures", "author": ["Tao Lei", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola."], "venue": "ACL. Association for Computational Linguistics.", "citeRegEx": "Lei et al\\.,? 2014", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."], "venue": "CoRR abs/1310.4546.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller."], "venue": "Communications of the ACM 38(11):39\u2013", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space", "author": ["Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum."], "venue": "arXiv preprint arXiv:1504.06654 .", "citeRegEx": "Neelakantan et al\\.,? 2015", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A maximum entropy model for prepositional phrase attachment", "author": ["Adwait Ratnaparkhi", "Jeff Reynar", "Salim Roukos."], "venue": "Proceedings of the workshop on Human Language Technology.", "citeRegEx": "Ratnaparkhi et al\\.,? 1994", "shortCiteRegEx": "Ratnaparkhi et al\\.", "year": 1994}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Joseph Reisinger", "Raymond J Mooney."], "venue": "HLT-ACL.", "citeRegEx": "Reisinger and Mooney.,? 2010", "shortCiteRegEx": "Reisinger and Mooney.", "year": 2010}, {"title": "Semantic classes and syntactic ambiguity", "author": ["Philip Resnik."], "venue": "Proceedings of the workshop on Human Language Technology. Association for Computational Linguistics.", "citeRegEx": "Resnik.,? 1993", "shortCiteRegEx": "Resnik.", "year": 1993}, {"title": "Autoextend: Extending word embeddings to embeddings for synsets and lexemes", "author": ["Sascha Rothe", "Hinrich Sch\u00fctze."], "venue": "ACL.", "citeRegEx": "Rothe and Sch\u00fctze.,? 2015", "shortCiteRegEx": "Rothe and Sch\u00fctze.", "year": 2015}, {"title": "Word representations via gaussian embedding", "author": ["Luke Vilnis", "Andrew McCallum."], "venue": "ICLR.", "citeRegEx": "Vilnis and McCallum.,? 2015", "shortCiteRegEx": "Vilnis and McCallum.", "year": 2015}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Mo Yu", "Mark Dredze."], "venue": "ACL.", "citeRegEx": "Yu and Dredze.,? 2014", "shortCiteRegEx": "Yu and Dredze.", "year": 2014}, {"title": "Selectional preferences for semantic role classification", "author": ["Benat Zapirain", "Eneko Agirre", "Lluis Marquez", "Mihai Surdeanu."], "venue": "Computational Linguistics .", "citeRegEx": "Zapirain et al\\.,? 2013", "shortCiteRegEx": "Zapirain et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 17, "context": "In this work, we represent a word token in a given context by estimating a context-sensitive probability distribution over relevant concepts in WordNet (Miller, 1995) and use the expected value (i.", "startOffset": 152, "endOffset": 166}, {"referenceID": 22, "context": "Among the labeled relations defined in WordNet between different synsets, we focus on the hypernymy relation to help model generalization and selectional preferences between words, which is especially important for predicting PP attachments (Resnik, 1993).", "startOffset": 241, "endOffset": 255}, {"referenceID": 23, "context": "This parameterization is similar to that of Rothe and Sch\u00fctze (2015).", "startOffset": 44, "endOffset": 69}, {"referenceID": 2, "context": "This component is inspired by the soft attention often used in neural machine translation (Bahdanau et al., 2014).", "startOffset": 90, "endOffset": 113}, {"referenceID": 22, "context": "hypernymy and selectional preferences is critical for successful prediction of PP attachments (Resnik, 1993), it is a good fit for evaluating our WordNet-grounded context-sensitive embeddings.", "startOffset": 94, "endOffset": 108}, {"referenceID": 4, "context": "Figure 3, reproduced from Belinkov et al. (2014), illustrates an example of the PP attachment prediction problem.", "startOffset": 26, "endOffset": 49}, {"referenceID": 4, "context": "We follow Belinkov et al. (2014)\u2019s definition of the PP attachment problem.", "startOffset": 10, "endOffset": 33}, {"referenceID": 4, "context": "Note: This figure and caption have been reproduced from Belinkov et al. (2014).", "startOffset": 56, "endOffset": 79}, {"referenceID": 4, "context": "This model is inspired by the Head-Prep-ChildTernary model of Belinkov et al. (2014). The main difference is that we replace the input features for each token with the output bi-RNN vectors.", "startOffset": 62, "endOffset": 85}, {"referenceID": 4, "context": "We used the English PP attachment dataset created and made available by Belinkov et al. (2014). The training and test splits contain 33,359 and 1951 labeled examples respectively.", "startOffset": 72, "endOffset": 95}, {"referenceID": 20, "context": "(2014), this dataset is a more realistic PP attachment task than the RRR dataset (Ratnaparkhi et al., 1994).", "startOffset": 81, "endOffset": 107}, {"referenceID": 4, "context": "As discussed in Belinkov et al. (2014), this dataset is a more realistic PP attachment task than the RRR dataset (Ratnaparkhi et al.", "startOffset": 16, "endOffset": 39}, {"referenceID": 23, "context": "The synset embedding parameters are initialized using the synset vectors obtained by running AutoExtend (Rothe and Sch\u00fctze, 2015) on 100dimensional GloVe (Pennington et al.", "startOffset": 104, "endOffset": 129}, {"referenceID": 19, "context": "The synset embedding parameters are initialized using the synset vectors obtained by running AutoExtend (Rothe and Sch\u00fctze, 2015) on 100dimensional GloVe (Pennington et al., 2014) vectors for WordNet 3.", "startOffset": 154, "endOffset": 179}, {"referenceID": 13, "context": "The input representations are enriched using syntactic context information, POS, WordNet and VerbNet (Kipper et al., 2008) information and the distance of the head word from the PP is explicitly encoded in composition architecture.", "startOffset": 101, "endOffset": 122}, {"referenceID": 8, "context": "In our experiments, we compare our proposed model, OntoLSTM-PP with three baselines \u2013 LSTM-PP initialized with GloVe embedding, LSTM-PP initialized with GloVe vectors retrofitted to WordNet using the approach of Faruqui et al. (2015) (henceforth referred to as GloVe-retro), and finally the best performing standalone PP attachment system from Belinkov et al.", "startOffset": 212, "endOffset": 234}, {"referenceID": 4, "context": "(2015) (henceforth referred to as GloVe-retro), and finally the best performing standalone PP attachment system from Belinkov et al. (2014), referred to as HPCD (full) in the paper.", "startOffset": 117, "endOffset": 140}, {"referenceID": 9, "context": "Initializing the word embeddings with GloVeretro (which uses WordNet as described in Faruqui et al. (2015)) instead of GloVe amounts to a small improvement, compared to the improvements ob-", "startOffset": 85, "endOffset": 107}, {"referenceID": 15, "context": "(2014), we used RBG parser (Lei et al., 2014), and modified it by adding a binary feature indicating the PP attachment predictions from our model.", "startOffset": 27, "endOffset": 45}, {"referenceID": 4, "context": "We compare four ways to compute the additional binary features: 1) the predictions of the best standalone system HPCD (full) in Belinkov et al. (2014), 2) the predictions of our baseline model LSTM-PP, 3) the predictions of our improved model OntoLSTM-PP, and 4) the gold labels Oracle PP.", "startOffset": 128, "endOffset": 151}, {"referenceID": 9, "context": "GloVe-retro is GloVe vectors retrofitted (Faruqui et al., 2015) to WordNet 3.", "startOffset": 41, "endOffset": 63}, {"referenceID": 23, "context": "1, and GloVe-extended refers to the synset embeddings obtained by running AutoExtend (Rothe and Sch\u00fctze, 2015) on GloVe.", "startOffset": 85, "endOffset": 110}, {"referenceID": 4, "context": "Table 1: Results on Belinkov et al. (2014)\u2019s PPA test set.", "startOffset": 20, "endOffset": 43}, {"referenceID": 4, "context": "We also note that, although we use the same predictions of the HPCD (full) model in Belinkov et al. (2014)5, we report different results than Belinkov et al.", "startOffset": 84, "endOffset": 107}, {"referenceID": 4, "context": "We also note that, although we use the same predictions of the HPCD (full) model in Belinkov et al. (2014)5, we report different results than Belinkov et al. (2014). For example, the unlabeled attachment score (UAS) of the baselines RBG and RBG + HPCD (full) are 94.", "startOffset": 84, "endOffset": 165}, {"referenceID": 4, "context": "We also note that, although we use the same predictions of the HPCD (full) model in Belinkov et al. (2014)5, we report different results than Belinkov et al. (2014). For example, the unlabeled attachment score (UAS) of the baselines RBG and RBG + HPCD (full) are 94.17 and 94.19, respectively, in Table 2, compared to 93.96 and 94.05, respectively, in Belinkov et al. (2014). This is due to the use of different versions of the RBG parser.", "startOffset": 84, "endOffset": 375}, {"referenceID": 3, "context": "Other work not necessarily related to multisense vectors, but still related to our work includes Belanger and Kakade (2015)\u2019s work which proposed a Gaussian linear dynamical system for estimating token-level word embeddings, and Vilnis and McCallum (2015)\u2019s work which proposed mapping each word type to a density instead of a point in a space to account for uncertainty in meaning.", "startOffset": 97, "endOffset": 124}, {"referenceID": 3, "context": "Other work not necessarily related to multisense vectors, but still related to our work includes Belanger and Kakade (2015)\u2019s work which proposed a Gaussian linear dynamical system for estimating token-level word embeddings, and Vilnis and McCallum (2015)\u2019s work which proposed mapping each word type to a density instead of a point in a space to account for uncertainty in meaning.", "startOffset": 97, "endOffset": 256}, {"referenceID": 23, "context": "Related to the idea of concept embeddings is Rothe and Sch\u00fctze (2015) who estimated WordNet synset representations, given pre-trained typelevel word embeddings.", "startOffset": 45, "endOffset": 70}, {"referenceID": 25, "context": "Yu and Dredze (2014) extended the CBOW model", "startOffset": 0, "endOffset": 21}, {"referenceID": 16, "context": "(Mikolov et al., 2013) by adding an extra term in the training objective for generating words conditioned on similar words according to a lexicon.", "startOffset": 0, "endOffset": 22}, {"referenceID": 16, "context": "(2015) extended the skipgram model (Mikolov et al., 2013) by representing word senses as latent variables in the generation pro-", "startOffset": 35, "endOffset": 57}, {"referenceID": 11, "context": "Jauhar et al. (2015) extended the skipgram model (Mikolov et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "Faruqui et al. (2015) used belief propagation to update pre-trained word embeddings on a graph that encodes lexical relationships in the ontology.", "startOffset": 0, "endOffset": 22}, {"referenceID": 9, "context": "Faruqui et al. (2015) used belief propagation to update pre-trained word embeddings on a graph that encodes lexical relationships in the ontology. Similarly, Johansson and Pina (2015) improved word embeddings by representing each sense of the word in a way that reflects the topology of the semantic network they belong to, and then representing the words as convex combinations of their senses.", "startOffset": 0, "endOffset": 184}, {"referenceID": 5, "context": "Resnik (1993); Brill and Resnik (1994);", "startOffset": 15, "endOffset": 39}, {"referenceID": 8, "context": "The models are implemented using Keras (Chollet, 2015), and the functionality is available at https://github.", "startOffset": 39, "endOffset": 54}], "year": 2017, "abstractText": "Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive embeddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.", "creator": "LaTeX with hyperref package"}}}