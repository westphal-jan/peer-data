{"id": "1705.03122", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2017", "title": "Convolutional Sequence to Sequence Learning", "abstract": "The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.", "histories": [["v1", "Mon, 8 May 2017 23:25:30 GMT  (1489kb,D)", "http://arxiv.org/abs/1705.03122v1", null], ["v2", "Fri, 12 May 2017 16:14:26 GMT  (492kb,D)", "http://arxiv.org/abs/1705.03122v2", null], ["v3", "Tue, 25 Jul 2017 01:40:57 GMT  (492kb,D)", "http://arxiv.org/abs/1705.03122v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jonas gehring", "michael auli", "david grangier", "denis yarats", "yann n dauphin"], "accepted": true, "id": "1705.03122"}, "pdf": {"name": "1705.03122.pdf", "metadata": {"source": "META", "title": "Convolutional Sequence to Sequence Learning", "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N. Dauphin"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "This year, it is only a matter of time before there is an agreement, until there is an agreement."}, {"heading": "2. Recurrent Sequence to Sequence Learning", "text": "The RNN encoder processes an input sequence x = (x1,.., xm) of m elements and returns state representations z = (z1..., zm). The RNN encoder takes z and generates the output sequence y = (y1,.., yn) from left to right, one element at a time. To generate output yi + 1, the decoder calculates a new hidden state hi + 1 based on the previous state hi + 1, an embedding gi of the previous word yi, and a conditional input ci derived from the encoder output z. Based on this generic formulation, various encoder decoder architectures are proposed."}, {"heading": "3. A Convolutional Architecture", "text": "Next, we introduce a fully revolutionary architecture for sequence modeling. Instead of relying on RNNs to calculate intermediate states z and decoder states h, we use Convolutionary Neural Networks (CNN)."}, {"heading": "3.1. Position Embeddings", "text": "First, we embed input elements x = (x1,.., xm) in the distribution space as w = (w1,..., wm), where wj-Rf embeds a column in an embedding matrix D-RV-f. In addition, we give our model an orderly sense by embedding the absolute position of the input elements p = (p1,..., pm) where pj-Rf. Both are combined to get input element presentations e = (w1 + p1,..., wm + pm). Similarly, we proceed with output elements already generated by the decoder network to get output element presentations that are embedded back into the decoder network g = (g1,.., gn). Position embedding is useful in our architecture because it gives our model a sense of what part of the sequence in the input or output it currently has to do (\u00a7 5.3)."}, {"heading": "3.2. Convolutional Block Structure", "text": "The two encoders and decoder networks share a simple block structure, which is based on a fixed number of input elements. (D) D \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"\" i \"i\" i \"i\" i \"i\" \"i\" i \"i\" i \"\" \"i\" i \"i\" i \"i\" i \"i\" \"i\" i \"i\" i \"\" i \"i\" i \"i\" \"i\" i \"i\" \"i\" i \"i\" i \"\" \"i\" i \"i\" i \"\" i \"\" i \"i\" i \"i\" \"i\" i \"i\" i \"i\" \"\" \"i\" i \"i\" i \"i\" \"i\" i \"i\" \"i\" i \"i\" i \"i\" \"\" \"\" i \"i\" i \"i\" i \"\" \"i\" i \"\" i \"i\" \"\" i \"i\" i \"\" i \"i\" i \"\" i \"\" i \"\" i \"i\" i \"\" \"i\" \"i\" \"\" i \"\" i \"\" \"i\" \"i\" \"\" i \"\" i \"i\" \"\" \"i\" \"\" \"\" \"i\" \"\" \"\" \"i\" \"\" i \"\" \"\" i \"i\" \"\" \"i\" \"\" \"\" \"\" \"\" i \"\" \"\" \"i\" \"i\" \"\" \"\" i \"\" \"\" \"\" \"i\" \"\" \"\" \"\" i \"\" \""}, {"heading": "3.3. Multi-step Attention", "text": "We introduce a separate attention mechanism for each decoder layer. To calculate the attention, we combine the current decoder state hli with an embedding of the previous element gi: dli = W l, i + b, d + gi (1) For decoder layer l, the attention of the state i and the source element j is summed up as a dot product between the decoder state outputs and each output z \u2212 j of the last encoder block u: alij = exp (dli \u00b7 zuj).t The conditional input cli for the current decoder layer is a weighted sum of the encoder layer outputs as well as the input element embeddings ej (Figure 1, center-right).cli The condition-layer-En-En-En-En-En-En-En-En-En-En-En-En-En-En-En-n-n-En-En-n-En-n-n-n-En-n-n En-En-n-En-n-n-En-n En-n-n En-n-n En-n En-n-En-En-n-En-En-n-n-En-En-n-En-n-En-n-En-En-n-En-n-En-n-En-n-n-n-En-n-En-n-En-En-n-n-En-n-n-En-n-n-n-En-En-n-n-n-En-n-n-n-n-En-n-n-n-n-En-n-n-n-En-n-n-n-n-n-n-En-n-n-n-n-n-n-En-En-n-n-n-n-n-n-n-n-n-n-n-En-n-n-n-n-n-n-n-n-n"}, {"heading": "3.4. Normalization Strategy", "text": "We stabilize learning by careful weight initialization (\u00a7 3.5) and by scaling parts of the network to ensure that the variance within the network does not change dramatically. In particular, we scale the output of residual blocks and attention to maintain the variance of activations. We multiply the sum of the input and output of a residual block by \u221a 0.5 to halve the variance of the sum. This assumes that both sums have the same variance, which is not always true but effective in practice.The conditional input cli generated by the attention is a weighted sum of m vectors (2) and we counteract a change in variance by scaling with m \u221a 1 / m; we multiply the inputs to their original size, provided that the attention values are evenly distributed. This is generally not the case, but we found that this strategy works well in practice."}, {"heading": "3.5. Initialization", "text": "The motivation for our initialization is the same as for normalization: maintaining the variance of activations throughout the forward and backward gear. All embedding is initialized by a normal distribution with a mean of 0 and a standard deviation of 0.1. For layers whose output is not fed directly to a gated linear unit, we initialize the weights of N (0, \u221a 1 / nl), where nl is the number of input connections for each neuron, ensuring that the variance of a normally distributed input is maintained. For layers following GLU activation, we propose a weight initialization scheme by shifting the derivatives to (He et al., 2015b; Glorot & Bengio, 2010; Appendix A)."}, {"heading": "4. Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Datasets", "text": "We are looking at three major WMT translation tasks as well as a text summary task. WMT '16 English-Romanian. We use the same data and pre-processing as Sennrich et al. (2016b), but remove sentences with more than 175 words. This results in 2.8M sentence pairs for training and we evaluate on newstestest2016.22We followed the pre-processing of https: / / github. com / rsennrich / wmt16 scripts / blob / 80e21e5 / We are experimenting with word-based models using a source word vocabulary of 200K types and a target word vocabulary of 80K types. We are also looking at a common source and target byte pair encoding with 40K types (Sennrich et al / 80e21e5; b).WMT' 14 English-German."}, {"heading": "4.2. Model Parameters and Optimization", "text": "We use 512 hidden units for both encoders and decoders, unless otherwise specified. All embedding, including those produced by the decoder before the final linear layer, have dimensionality 512; we use the same dimensionalities for the linear layers used between the hidden and embedded quantities (\u00a7 3.2). We train our revolutionary models using the accelerated gradient method (Sutskever et al., 2013) with a pulse value of 0.99 and renormous gradients when their standard exceeds 0.1 (Pascanu et al al., 2013). We use a learning rate of 0.25 and once the validation stops perplexity, we reduce the learning rate by an order of magnitude after which it falls."}, {"heading": "4.3. Evaluation", "text": "We report on average results over three runs of each model, each differing only in the original random seeding. Translations are generated by a beam search and we normalize the log probability values by sentence length. We use a beam of width 5. We divide the log probabilities of the final hypothesis in the beam search by their length | y |. For WMT '14 English-German, we tune a length normalization constant on a separate development set (newstest2015), and we normalize the log probabilities by | y | \u03b1 (Wu et al., 2016). On other data sets, we found no benefit from tuning this constant. In word-based models, we perform unknown word exchanges based on attention values by generation (Jean et al., 2015) by looking up the source word with the maximum attention rating in a precompiled dictionary. If the dictionary does not include translation, we simply copy the source word to the English word most frequently."}, {"heading": "5. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Recurrent vs. Convolutional Models", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "5.2. Generation Speed", "text": "Next, we evaluate the deduction speed of our architecture on the development set of the WMT '14 Anglo-French task, which is the concatenation of newstest2012 and newstest2013; it includes 6003 sets. We measure the generation speed on both GPU and CPU hardware. Specifically, we measure the GPU speed on three generations of Nvidia cards: a GTX-1080ti, an M40, and an older K40 card. Note that the CPU timings are measured on a host with 48 hyperthreaded cores (Intel Xeon E5-2680 @ 2.50 GHz) with 40 workers. In all settings, we stack up to 128 sets, composing stacks with sentences of the same length. Note that the majority of batches are smaller because the size of the development set is smaller. We experiment with size 5 bars as well as greedy PU searches, i.e bars of size 1: In order to make the times of the previous generation, we do not compose the GTU quickly, but rather recompose the results of the previous states, we do not compose the GTU."}, {"heading": "5.3. Position Embeddings", "text": "In the following sections we analyze the design decisions in our architecture. The other results in this paper are based on the Anglo-German task WMT '14 with 13 encoder layers at core size 3 and 5 decoder layers at core size 5. We use a target vocabulary of 160K words and a vocabulary selection (Mi et al., 2016; L'Hostis et al., 2016) to reduce the size of the output layer that speeds up training and testing. The average vocabulary size for each training session is about 20K target words. All numbers are averaged over three runs (\u00a7 4) and BLEU is reported on new releases in 2014 before replacing unknown words. We start with an experiment that removes the position embedding from the encoder and decoder (\u00a7 3.1). These embedding allow our model to identify what proportion of the source and target sequence it performs, but also does not include an explicit limitation of the two sentences that is helpful."}, {"heading": "1,2,3,4,5 6.65 21.63", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1,2,3,4 6.70 21.54", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1,2,3 6.95 21.36", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1,2 6.92 21.47", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1,3,5 6.97 21.10", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 7.15 21.26", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 7.09 21.30", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 7.11 21.19", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 7.19 21.31", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 7.66 20.24", "text": "Simple addition of the input word embedding, which is a negligible effort."}, {"heading": "5.4. Multi-step Attention", "text": "The mechanism of multiple attention (\u00a7 3.3) calculates for each decoder layer a separate source context vector. The calculation also takes into account contexts calculated for previous decoder layers of the current time step, as well as previous time steps located within the receiver field of the decoder. How can multiple attention be compared with attention in fewer layers or even only in a single layer, as is customary? Table 4 shows that attention in all decoder layers achieves the best validation perplexity (PPL). Furthermore, removing more and more attention layers decreases the accuracy, both in terms of BLEU and PPL. Compared to the rest of the network, the computational effort for attention is very low. Attention training in all five decoder layers processes an average of 3624 targets per second on a single GPU, compared to 3772 words per second for attention in a single layer."}, {"heading": "5.5. Kernel size and Depth", "text": "Figure 2 shows accuracy when we change the number of layers in the encoder or decoder. The core width for layers in the encoder is 3 and for the decoder 5 layers. Deeper architectures are particularly advantageous for the encoder, but not so for the decoder. Decoder setups with two layers already work well, while the accuracy of the encoder with more and more layers is steadily increasing up to 9 layers as the accuracy begins to place. Apart from increasing the depth of the networks, we can also change the core width. Table 6 shows that encoders with narrow cores and many layers perform better than wider cores. These networks can also be faster, as the workload for calculating a kernel that operates over 3 input elements is less than half as compared to cores over 7 elements. A similar picture results for decoder networks with large core sizes (Table 7). Dauphin et al. (2016) shows that contextual sizes often range from 20 to very accurate."}, {"heading": "5.6. Summarization", "text": "The current best models for this task are recursive neural networks that either optimize evaluation metrics (Shen et al., 2016) or address specific summary problems such as avoiding repeated generations (Suzuki & Nagata, 2017) We use a standardized probability training and a simple model with six layers each in the encoder and decoder, hidden size 256, batch size 128, and we trained on a single GPU in one night. Table 5 shows that our probability-trained model (RNN MLE) surpasses the probability-trained model of Shen et al. (2016) and is not far behind the best models for this task, which benefit from task-specific optimization and model structure."}, {"heading": "3 20.61 21.17 21.63", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 20.80 21.02 21.42", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 20.81 21.30 21.09", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 21.10 21.71 21.62", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 21.09 21.63 21.24", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 21.40 21.31 21.33", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6. Conclusion and Future Work", "text": "Our model is based on gating and performs several attention steps. We reach a new state of the art based on several public translation benchmark data sets. In the Anglo-Romanian task WMT '16 we exceed the previous best result by 1.8 BLEU, in the Anglo-French translation WMT' 14 we improve the LSTM model by Wu et al. (2016) by 1.5 BLEU, and in the Anglo-German translation WMT '14 we exceed the state of the art by 0.5 BLEU. In the future we would like to apply Convolutionary Architectures to other sequences to solve learning problems that could also benefit from learning hierarchical representations."}, {"heading": "A. Weight Initialization", "text": "We derive a weight initialization scheme tailored to the GLU activation function, similar to Glorot & Bengio (2010); Er et al. (2015b) by focusing on the variance of activations within the network for both forward and reverse gears; we also describe how we define the weight initialization for dropout.A.1. In advance, we assume that the inputs xl of a revolutionary layer l and their weights Wl are independently and identically distributed (i.i.d.), the variance of their output as yl = Wlxl + bl, isV ar [yl] = nlV ar [yl] = nlV ar ar ar [wlxl] (3), where nl is the number of inputs on the layer; for onedimensional convolutionary layers with kernel width k and input dimension c, this is kc. We take the notation in (He et al., 2015b), i.e."}, {"heading": "B. Upper Bound on Squared Sigmoid", "text": "The sigmoid function \u03c3 (x) can be expressed as a hyperbolic tangent by using the identity tanh (x) = 2\u03c3 (2x) \u2212 1. The derivative tanh \u2032 (x) = 1 \u2212 tanh2 (x), and with tanh (x) [0, 1], x \u2265 0 it holds thattanh \u2032 (x) \u2264 1, x \u2265 0 (34) \u0445 x 0 tanh \u2032 (x) dx \u2264 x 0 1 dx (35) tanh (x) \u2264 x, x \u2265 0 (36) We can express this relation with \u03c3 (x) as follows: 2\u03c3 (x) \u2212 1 \u2264 1 2 x, x \u2265 0 (37) Both terms of this inequality have rotational symmetry w.r.t 0, and therefore (2\u03c3 (x) \u2212 1) 2 \u2264 (1 2 x) 2 \u0445 x (38)."}, {"heading": "C. Attention Visualization", "text": "Figure 3 shows attention scores for a generated sentence from the English-German task of WMT '14. The model used for this plot has 8 decoder levels and an 80K BPE vocabulary. Attention is transmitted in different decoder levels that capture different parts of the source set. Levels 1, 3 and 6 have a linear alignment; the first level shows the clearest alignment, although somewhat remote and often pays attention to the corresponding source word of the previously generated target. Levels 2 and 8 do not have a clear structure and presumably collect information about the entire source set; the fourth level shows high alignment values for nouns such as \"festival,\" \"path\" and \"work\" for both the generated target words and for their preceding words. Note that these preceding words in German depend on the gender and object relationship of the respective source sentence and that attention in Levels 5 and 7 is focused on \"built,\" which in the German translation is repeated from the beginning of the second generation to the end of the sentence, or that the higher interpretation may result in the 11th sentence."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Findings of the 2016 conference on machine translation", "author": ["Rubino", "Rapha\u00ebl", "Scarton", "Carolina", "Specia", "Lucia", "Turchi", "Marco", "Verspoor", "Karin M", "Zampieri", "Marcos"], "venue": "In Proc. of WMT,", "citeRegEx": "Rubino et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rubino et al\\.", "year": 2016}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Attention-based models for speech recognition", "author": ["Chorowski", "Jan K", "Bahdanau", "Dzmitry", "Serdyuk", "Dmitriy", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Torch7: A Matlab-like Environment for Machine Learning", "author": ["Collobert", "Ronan", "Kavukcuoglu", "Koray", "Farabet", "Clement"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Language modeling with gated linear units", "author": ["Dauphin", "Yann N", "Fan", "Angela", "Auli", "Michael", "Grangier", "David"], "venue": "arXiv preprint arXiv:1612.08083,", "citeRegEx": "Dauphin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2016}, {"title": "A Simple, Fast, and Effective Reparameterization of IBM Model 2", "author": ["Dyer", "Chris", "Chahuneau", "Victor", "Smith", "Noah A"], "venue": "In Proc. of ACL,", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Finding Structure in Time", "author": ["Elman", "Jeffrey L"], "venue": "Cognitive Science,", "citeRegEx": "Elman and L.,? \\Q1990\\E", "shortCiteRegEx": "Elman and L.", "year": 1990}, {"title": "A Convolutional Encoder Model for Neural Machine Translation", "author": ["Gehring", "Jonas", "Auli", "Michael", "Grangier", "David", "Dauphin", "Yann N"], "venue": "arXiv preprint arXiv:1611.02344,", "citeRegEx": "Gehring et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gehring et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Deep Residual Learning for Image Recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proc. of CVPR,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Montreal Neural Machine Translation systems for WMT15", "author": ["Jean", "S\u00e9bastien", "Firat", "Orhan", "Cho", "Kyunghyun", "Memisevic", "Roland", "Bengio", "Yoshua"], "venue": "In Proc. of WMT,", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Convolutional networks for images, speech, and time series", "author": ["LeCun", "Yann", "Bengio", "Yoshua"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "LeCun et al\\.,? \\Q1995\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1995}, {"title": "Vocabulary Selection Strategies for Neural Machine Translation", "author": ["L\u2019Hostis", "Gurvan", "Grangier", "David", "Auli", "Michael"], "venue": "arXiv preprint arXiv:1610.00072,", "citeRegEx": "L.Hostis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "L.Hostis et al\\.", "year": 2016}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Lin", "Chin-Yew"], "venue": "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,", "citeRegEx": "Lin and Chin.Yew.,? \\Q2004\\E", "shortCiteRegEx": "Lin and Chin.Yew.", "year": 2004}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong", "Minh-Thang", "Pham", "Hieu", "Manning", "Christopher D"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Encoding Source Language with Convolutional Neural Network for Machine Translation", "author": ["Meng", "Fandong", "Lu", "Zhengdong", "Wang", "Mingxuan", "Li", "Hang", "Jiang", "Wenbin", "Liu", "Qun"], "venue": "In Proc. of ACL,", "citeRegEx": "Meng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2015}, {"title": "Vocabulary Manipulation for Neural Machine Translation", "author": ["Mi", "Haitao", "Wang", "Zhiguo", "Ittycheriah", "Abe"], "venue": "In Proc. of ACL,", "citeRegEx": "Mi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Key-value memory networks for directly reading documents", "author": ["Miller", "Alexander H", "Fisch", "Adam", "Dodge", "Jesse", "Karimi", "Amir-Hossein", "Bordes", "Antoine", "Weston", "Jason"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Miller et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Abstractive text summarization using sequence-to-sequence rnns and beyond", "author": ["Nallapati", "Ramesh", "Zhou", "Bowen", "Gulcehre", "Caglar", "Xiang", "Bing"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Nallapati et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "Pixel recurrent neural networks", "author": ["Oord", "Aaron van den", "Kalchbrenner", "Nal", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["Oord", "Aaron van den", "Kalchbrenner", "Nal", "Vinyals", "Oriol", "Espeholt", "Lasse", "Graves", "Alex", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1606.05328,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Rush", "Alexander M", "Chopra", "Sumit", "Weston", "Jason"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["Salimans", "Tim", "Kingma", "Diederik P"], "venue": "arXiv preprint arXiv:1602.07868,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Japanese and korean voice search", "author": ["Schuster", "Mike", "Nakajima", "Kaisuke"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Schuster et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 2012}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["Sennrich", "Rico", "Haddow", "Barry", "Birch", "Alexandra"], "venue": "In Proc. of ACL,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural headline generation with sentence-wise optimization", "author": ["Shen", "Shiqi", "Zhao", "Yu", "Liu", "Zhiyuan", "Sun", "Maosong"], "venue": "arXiv preprint arXiv:1604.01904,", "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent Neural Networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "End-to-end Memory Networks", "author": ["Sukhbaatar", "Sainbayar", "Weston", "Jason", "Fergus", "Rob", "Szlam", "Arthur"], "venue": "In Proc. of NIPS, pp", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George E", "Hinton", "Geoffrey E"], "venue": "In ICML,", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In Proc. of NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Cutting-off redundant repeating generations for neural abstractive summarization", "author": ["Suzuki", "Jun", "Nagata", "Masaaki"], "venue": "arXiv preprint arXiv:1701.00138,", "citeRegEx": "Suzuki et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Suzuki et al\\.", "year": 2017}, {"title": "Phoneme Recognition using Time-delay Neural Networks", "author": ["Waibel", "Alex", "Hanazawa", "Toshiyuki", "Hinton", "Geoffrey", "Shikano", "Kiyohiro", "Lang", "Kevin J"], "venue": "IEEE transactions on acoustics, speech, and signal processing,", "citeRegEx": "Waibel et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Waibel et al\\.", "year": 1989}, {"title": "Neural Machine Translation with Recurrent Attention Modeling", "author": ["Yang", "Zichao", "Hu", "Zhiting", "Deng", "Yuntian", "Dyer", "Chris", "Smola", "Alex"], "venue": "arXiv preprint arXiv:1607.05108,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation", "author": ["Zhou", "Jie", "Cao", "Ying", "Wang", "Xuguang", "Li", "Peng", "Xu", "Wei"], "venue": "arXiv preprint arXiv:1606.04199,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "2015b) by focusing on the variance", "author": ["He"], "venue": null, "citeRegEx": "He,? \\Q2015\\E", "shortCiteRegEx": "He", "year": 2015}], "referenceMentions": [{"referenceID": 34, "context": "Sequence to sequence learning has been successful in many tasks such as machine translation, speech recognition (Sutskever et al., 2014; Chorowski et al., 2015) and text summarization (Rush et al.", "startOffset": 112, "endOffset": 160}, {"referenceID": 3, "context": "Sequence to sequence learning has been successful in many tasks such as machine translation, speech recognition (Sutskever et al., 2014; Chorowski et al., 2015) and text summarization (Rush et al.", "startOffset": 112, "endOffset": 160}, {"referenceID": 26, "context": ", 2015) and text summarization (Rush et al., 2015; Nallapati et al., 2016; Shen et al., 2016) amongst others.", "startOffset": 31, "endOffset": 93}, {"referenceID": 22, "context": ", 2015) and text summarization (Rush et al., 2015; Nallapati et al., 2016; Shen et al., 2016) amongst others.", "startOffset": 31, "endOffset": 93}, {"referenceID": 30, "context": ", 2015) and text summarization (Rush et al., 2015; Nallapati et al., 2016; Shen et al., 2016) amongst others.", "startOffset": 31, "endOffset": 93}, {"referenceID": 0, "context": "The dominant approach to date encodes the input sequence with a series of bi-directional recurrent neural networks (RNN) and generates a variable length output with another set of decoder RNNs, both of which interface via a soft-attention mechanism (Bahdanau et al., 2014; Luong et al., 2015).", "startOffset": 249, "endOffset": 292}, {"referenceID": 18, "context": "The dominant approach to date encodes the input sequence with a series of bi-directional recurrent neural networks (RNN) and generates a variable length output with another set of decoder RNNs, both of which interface via a soft-attention mechanism (Bahdanau et al., 2014; Luong et al., 2015).", "startOffset": 249, "endOffset": 292}, {"referenceID": 36, "context": "Convolutional neural networks are less common for sequence modeling, despite several advantages (Waibel et al., 1989; LeCun & Bengio, 1995).", "startOffset": 96, "endOffset": 139}, {"referenceID": 19, "context": "Gated convolutions have been previously explored for machine translation by Meng et al. (2015) but their evaluation was restricted to a small dataset and the model was used in tandem with a traditional count-based model.", "startOffset": 76, "endOffset": 95}, {"referenceID": 8, "context": "tures which are partially convolutional have shown strong performance on larger tasks but their decoder is still recurrent (Gehring et al., 2016).", "startOffset": 123, "endOffset": 145}, {"referenceID": 5, "context": "Our model is equipped with gated linear units (Dauphin et al., 2016) and residual connections (He et al.", "startOffset": 46, "endOffset": 68}, {"referenceID": 34, "context": "Sequence to sequence modeling has been synonymous with recurrent neural network based encoder-decoder architectures (Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 116, "endOffset": 163}, {"referenceID": 0, "context": "Sequence to sequence modeling has been synonymous with recurrent neural network based encoder-decoder architectures (Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 116, "endOffset": 163}, {"referenceID": 2, "context": "Models without attention consider only the final encoder state zm by setting ci = zm for all i (Cho et al., 2014), or simply initialize the first decoder state with zm (Sutskever et al.", "startOffset": 95, "endOffset": 113}, {"referenceID": 34, "context": ", 2014), or simply initialize the first decoder state with zm (Sutskever et al., 2014), in which case ci is not used.", "startOffset": 62, "endOffset": 86}, {"referenceID": 0, "context": "Architectures with attention (Bahdanau et al., 2014; Luong et al., 2015) compute ci as a weighted sum of (z1.", "startOffset": 29, "endOffset": 72}, {"referenceID": 18, "context": "Architectures with attention (Bahdanau et al., 2014; Luong et al., 2015) compute ci as a weighted sum of (z1.", "startOffset": 29, "endOffset": 72}, {"referenceID": 2, "context": "Popular choices for recurrent networks in encoder-decoder models are long short term memory networks (LSTM; Hochreiter & Schmidhuber, 1997) and gated recurrent units (GRU; Cho et al., 2014).", "startOffset": 166, "endOffset": 189}, {"referenceID": 0, "context": "Most recent approaches also rely on bi-directional encoders to build representations of both past and future contexts (Bahdanau et al., 2014; Zhou et al., 2016; Wu et al., 2016).", "startOffset": 118, "endOffset": 177}, {"referenceID": 38, "context": "Most recent approaches also rely on bi-directional encoders to build representations of both past and future contexts (Bahdanau et al., 2014; Zhou et al., 2016; Wu et al., 2016).", "startOffset": 118, "endOffset": 177}, {"referenceID": 38, "context": "Models with many layers often rely on shortcut or residual connections (He et al., 2015a; Zhou et al., 2016; Wu et al., 2016).", "startOffset": 71, "endOffset": 125}, {"referenceID": 5, "context": "We choose gated linear units (GLU; Dauphin et al., 2016) as non-linearity which implement a simple gating mechanism over the output of the convolution Y = [A B] \u2208 R:", "startOffset": 29, "endOffset": 56}, {"referenceID": 22, "context": "A similar nonlinearity has been introduced in Oord et al. (2016b) who apply tanh toA but Dauphin et al.", "startOffset": 46, "endOffset": 66}, {"referenceID": 5, "context": "(2016b) who apply tanh toA but Dauphin et al. (2016) shows that GLUs perform better in the context of language modelling.", "startOffset": 31, "endOffset": 53}, {"referenceID": 21, "context": "We found adding ej to be beneficial and it resembles key-value memory networks where the keys are the z j and the values are the z j + ej (Miller et al., 2016).", "startOffset": 138, "endOffset": 159}, {"referenceID": 32, "context": "This can be seen as attention with multiple \u2019hops\u2019 (Sukhbaatar et al., 2015) compared to single step attention (Bahdanau et al.", "startOffset": 51, "endOffset": 76}, {"referenceID": 0, "context": ", 2015) compared to single step attention (Bahdanau et al., 2014; Luong et al., 2015; Zhou et al., 2016; Wu et al., 2016).", "startOffset": 42, "endOffset": 121}, {"referenceID": 18, "context": ", 2015) compared to single step attention (Bahdanau et al., 2014; Luong et al., 2015; Zhou et al., 2016; Wu et al., 2016).", "startOffset": 42, "endOffset": 121}, {"referenceID": 38, "context": ", 2015) compared to single step attention (Bahdanau et al., 2014; Luong et al., 2015; Zhou et al., 2016; Wu et al., 2016).", "startOffset": 42, "endOffset": 121}, {"referenceID": 37, "context": "Overall, our attention mechanism considers which words we previously attended to (Yang et al., 2016) and performs multiple attention \u2019hops\u2019 per time step.", "startOffset": 81, "endOffset": 100}, {"referenceID": 29, "context": "We use the same data and pre-processing as Sennrich et al. (2016b) but remove sentences with more than 175 words.", "startOffset": 43, "endOffset": 67}, {"referenceID": 18, "context": "We use the same setup as Luong et al. (2015) which comprises 4.", "startOffset": 25, "endOffset": 45}, {"referenceID": 26, "context": ", 2003) and pre-process it identically to Rush et al. (2015) resulting in 3.", "startOffset": 42, "endOffset": 61}, {"referenceID": 26, "context": ", 2003) and pre-process it identically to Rush et al. (2015) resulting in 3.8M training examples and 190K for validation. We evaluate on the DUC-2004 test data comprising 500 article-title pairs (Over et al., 2007) and report three variants of recall-based ROUGE (Lin, 2004), namely, ROUGE-1 (unigrams), ROUGE-2 (bigrams), and ROUGE-L (longest-common substring). We also evaluate on a Gigaword test set of 2000 pairs which is identical to the one used by Rush et al. (2015) and we report F1 ROUGE similar to prior work.", "startOffset": 42, "endOffset": 474}, {"referenceID": 26, "context": ", 2003) and pre-process it identically to Rush et al. (2015) resulting in 3.8M training examples and 190K for validation. We evaluate on the DUC-2004 test data comprising 500 article-title pairs (Over et al., 2007) and report three variants of recall-based ROUGE (Lin, 2004), namely, ROUGE-1 (unigrams), ROUGE-2 (bigrams), and ROUGE-L (longest-common substring). We also evaluate on a Gigaword test set of 2000 pairs which is identical to the one used by Rush et al. (2015) and we report F1 ROUGE similar to prior work. Similar to Shen et al. (2016) we use a source and target vocabulary of 30K words and require outputs to be at least 14 words long.", "startOffset": 42, "endOffset": 550}, {"referenceID": 33, "context": "We train our convolutional models with Nesterov\u2019s accelerated gradient method (Sutskever et al., 2013) using a momentum value of 0.", "startOffset": 78, "endOffset": 102}, {"referenceID": 25, "context": "1 (Pascanu et al., 2013).", "startOffset": 2, "endOffset": 24}, {"referenceID": 4, "context": "All models are implemented in Torch (Collobert et al., 2011) and trained on a single Nvidia M40 GPU except for WMT\u201914 EnglishFrench for which we use a multi-GPU setup on a single machine.", "startOffset": 36, "endOffset": 60}, {"referenceID": 14, "context": "For word-based models, we perform unknown word replacement based on attention scores after generation (Jean et al., 2015).", "startOffset": 102, "endOffset": 121}, {"referenceID": 6, "context": "Dictionaries were extracted from the word aligned training data that we obtained with fast align (Dyer et al., 2013).", "startOffset": 97, "endOffset": 116}, {"referenceID": 6, "context": "Dictionaries were extracted from the word aligned training data that we obtained with fast align (Dyer et al., 2013). Each source word is mapped to the target word it is most frequently aligned to. In our multi-step attention (\u00a73.3) we simply average the attention scores over all layers. Finally, we compute case-sensitive tokenized BLEU, except for WMT\u201916 English-Romanian where we use detokenized BLEU to be comparable with Sennrich et al. (2016b).4", "startOffset": 98, "endOffset": 451}, {"referenceID": 28, "context": "On WMT\u201916 English-Romanian translation we compare to Sennrich et al. (2016b) which is the winning entry on this language pair at WMT\u201916 (Bojar et al.", "startOffset": 53, "endOffset": 77}, {"referenceID": 0, "context": "Their model implements the attention-based sequence to sequence architecture of Bahdanau et al. (2014) and uses GRU cells both in the encoder and decoder.", "startOffset": 80, "endOffset": 103}, {"referenceID": 18, "context": "On WMT\u201914 English to German translation we compare to the following prior work: Luong et al. (2015) is based on a four layer LSTM attention model, ByteNet (Kalchbrenner et al.", "startOffset": 80, "endOffset": 100}, {"referenceID": 20, "context": "We use a target vocabulary of 160K words as well as vocabulary selection (Mi et al., 2016; L\u2019Hostis et al., 2016) to decrease the size of the output layer which speeds up training and testing.", "startOffset": 73, "endOffset": 113}, {"referenceID": 16, "context": "We use a target vocabulary of 160K words as well as vocabulary selection (Mi et al., 2016; L\u2019Hostis et al., 2016) to decrease the size of the output layer which speeds up training and testing.", "startOffset": 73, "endOffset": 113}, {"referenceID": 5, "context": "Dauphin et al. (2016) shows that context sizes of 20 words are often sufficient to achieve very good accuracy on language modeling for English.", "startOffset": 0, "endOffset": 22}, {"referenceID": 30, "context": "The current best models on this task are recurrent neural networks which either optimize the evaluation metric (Shen et al., 2016) or address specific problems of summarization such as avoiding repeated generations (Suzuki & Nagata, 2017).", "startOffset": 111, "endOffset": 130}, {"referenceID": 30, "context": "The current best models on this task are recurrent neural networks which either optimize the evaluation metric (Shen et al., 2016) or address specific problems of summarization such as avoiding repeated generations (Suzuki & Nagata, 2017). We use standard likelhood training for our model and a simple model with six layers in the encoder and decoder each, hidden size 256, batch size 128, and we trained on a single GPU in one night. Table 5 shows that our likelhood trained model outperforms the likelihood trained model (RNN MLE) of Shen et al. (2016) and is not far behind the best models on this task which benefit from task-specific optimization and model structure.", "startOffset": 112, "endOffset": 555}, {"referenceID": 30, "context": "RNN MLE (Shen et al., 2016) 24.", "startOffset": 8, "endOffset": 27}, {"referenceID": 30, "context": "56 RNN MRT (Shen et al., 2016) 30.", "startOffset": 11, "endOffset": 30}], "year": 2017, "abstractText": "The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks.1 Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT\u201914 EnglishGerman and WMT\u201914 English-French translation at an order of magnitude faster speed, both on GPU and CPU.", "creator": "LaTeX with hyperref package"}}}