{"id": "1705.03551", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2017", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension", "abstract": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study.\\footnote{Data and code available at --", "histories": [["v1", "Tue, 9 May 2017 21:35:07 GMT  (1148kb,D)", "http://arxiv.org/abs/1705.03551v1", "Accepted at ACL 2017"], ["v2", "Sat, 13 May 2017 21:12:37 GMT  (1149kb,D)", "http://arxiv.org/abs/1705.03551v2", "Added references, fixed typos, minor baseline update"]], "COMMENTS": "Accepted at ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mandar joshi", "eunsol choi", "daniel s weld", "luke zettlemoyer"], "accepted": true, "id": "1705.03551"}, "pdf": {"name": "1705.03551.pdf", "metadata": {"source": "CRF", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension", "authors": ["Mandar Joshi", "Eunsol Choi", "Daniel S. Weld", "Luke Zettlemoyer", "Paul G. Allen"], "emails": ["lsz}@cs.washington.edu", "lukez@allenai.org"], "sections": [{"heading": "1 Introduction", "text": "In fact, we are in a position to go in search of a solution that enables us, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in the position we are in."}, {"heading": "2 Overview", "text": "We assume that access to a data set of tuples {(qi, ai, Tue) | i = 1.. n}, where ai is a text string that defines the correct answer to the question Qi. However, according to recent formulations (Rajpurkar et al., 2016), we assume that ai appears as a substring for a document in the series Di.2. However, we differ in defining Di as a series of documents in which previous work proceeded from a single document (Hermann et al., 2015) or even from a short paragraph (Rajpurkar et al., 2016). Data and distanced supervision Our evidence documents are automatically collected from Wikipedia or more general web search results (details in Section 3). Since we collect evidence using an automated process, the documents are not guaranteed to provide all the facts necessary to answer the question."}, {"heading": "3 Dataset Collection", "text": "We collected a large dataset to support the task of reading comprehension described above. First, we collected question and answer pairs from 14 websites with trivial and quiz leagues. We removed questions with less than four tokens, as these were generally either too simple or too vague. Then, we collected textual evidence to answer questions with two sources: documents from web search results and Wikipedia articles for entities in the question. To exclude the quiz websites, we removed all pages from the quiz websites we had scraped off, and each page whose url contained the keywords quiz, question or answer. We then combed through the top 10 search results pages and edited PDF and other poorly formatted documents. The search results contain various documents such as blog articles, news articles, and encyclopedic pages for the entities mentioned in the question."}, {"heading": "4 Dataset Analysis", "text": "A quantitative and qualitative analysis of TriviaQA shows that it contains complex questions about a variety of entities that can be answered with the help of evidence documents.Question and Answer Analysis TriviaQA questions written by TriviaQA enthusiasts cover various topics of interest to people.The average length of the questions is 14 characters, indicating that many questions are highly compositional. For qualitative analysis, we sampled 200 pairs of questions and manually analyzed their characteristics. Over 73.5% of these questions contain sentences that describe a fine-grained category to which the answer belongs, while 15.5% indicate a coarse-grained category (one person, organization, place, and miscellaneous).Questions often involve arguing over timeframes, as well as making comparisons comparisons.A summary of the analysis is presented in Table 3.Answers in 3.Answers in TriviaQA, we belong to a diverse group of titles, 2.985% of WikiA responses to the person, and 2.985% of WikiA responses to the person."}, {"heading": "5 Baseline methods", "text": "To quantify the difficulty of the dataset for current methods, we present results on neural and other models. We used a random baseline and a simple classifier inspired by previous work (Wang et al., 2015; Chen et al., 2016), and compared them with BiDAF (Seo et al., 2017), one of the most powerful models for the SQuAD dataset."}, {"heading": "5.1 Random entity baseline", "text": "In this heuristic approach, we first construct a candidate response set using entities assigned to the provided Wikipedia pages for a given question (an average of 1.8 per question) and then randomly select a candidate who does not appear in the question. If there is no such candidate, we select any candidate from the set of candidates."}, {"heading": "5.2 Entity classifier", "text": "More formally, for a question Qi, an answer a + i, and an evidence document Di, we want to learn a scoring functionality such as this score (a + i | qi, Di) > Score (a \u2212 i | qi, Di), where a \u2212 i is any other candidate than the answer. The functionality is learned with LambdaMART (Wu et al., 2010) 5, an advanced tree-based ranking algorithm. This is similar to previous scale classifiers for QA (Chen et al., 2016; Wang et al., 2015) and uses context and Wikipedia catalog functions. To construct the candidates \"answers, we consider sentences that have at least one word in common with the question. We then add all 5We use the RankLib implementation https: / / sourceforge.net / p / lemur / wiki / RankLib / n-gram (n-1, 5]) that occurs in these sentences and some Wikipedia articles."}, {"heading": "5.3 Neural model", "text": "Recurring Neural Network Models (RNNs) (Hermann et al., 2015; Chen et al., 2016) were very effective in reading comprehension. For our task, we modified the BiDAF model (Seo et al., 2017), which uses a sequence of context words as input and displays the start and end positions of the predicted answer in context. The model uses an RNN at the character, symbol and phrase level to encode context and question, and uses an attention mechanism between question and context. Independent of the evidence document, the TriviaQA does not contain the exact range of responses. To overcome this, we shorten the initial match of the response sequence in the evidence document. Designed for a dataset where the evidence document is a single paragraph (an average of 122 words), the BiDAF model does not scale to long documents. To overcome this, we shorten the first document of evidence to 800 words."}, {"heading": "6 Experiments", "text": "An evaluation of our baselines shows that our two tasks are challenging and that the TriviaQA dataset supports important future work."}, {"heading": "6.1 Evaluation Metrics", "text": "We use the same evaluation metrics as SQuAD - exact match (EM) and F1 over words in the answers. For questions that are numerical and 6Using a designated entity recognition system to generate candidate entities is not practical, as answers are common nouns or phrases.7We have found that splitting documents into smaller subdocuments impairs performance, since the majority of subdocuments do not include the answers in FreeForm, we use a single given answer as the basic truth. For questions that have Wikipedia entities as answers, we use Wikipedia aliases as a valid answer along with the given answer. Since Wikipedia and the Web vary considerably in style and content, we report on the performance of each source separately. While we use Wikipedia at question level, we rate facts that are required to answer a question as usually only given once."}, {"heading": "6.2 Experimental Setup", "text": "We randomly divide QA pairs in the data set into train (80%), development (10%) and test set (10%). In addition to evaluating the remote monitoring, we evaluate baselines on verified subsets (see section 4) of the development and test partitions. Table 6 contains the number of questions and documents for each task. We trained the unit classifier on the basis of a sample of 50,000 questions from the training set. To train BiDAF in the web area, we used a random sample of 80,000 documents. For both areas, we used only those (training) documents in which the answer appears in the first 400 tokens to keep the training time manageable."}, {"heading": "6.3 Results", "text": "For both Wikipedia and web documents, BiDAF (40%) performs better than the classifier (23%), and the oracle value is the upper limit of exact match accuracy. [8] All models fall well behind the human baseline of 79.7% on the Wikipedia domain and 75.4% on the Web domain. We analyze the performance of BiDAF on the development theorem using Wikipedia as a source of evidence by question and answer type. The accuracy of the system steadily decreased with the length of the questions - by 50% for questions with 5 or fewer words to 32% for 20 or more words. This suggests that longer compositional questions are more difficult for current methods."}, {"heading": "6.4 Error analysis", "text": "Our qualitative error analysis shows that the composition of the questions and lexical variation, as well as the low signal-to-noise ratio in (complete) documents, still pose a challenge to current methods.We sampled 100 false BiDAF predictions from the development package and used Wikipedia evidence documents for manual analysis.We found that 19 examples in none of the provided documents lacked evidence, 3 had false basic truths and 3 were valid answers that were not included in the answer key. In addition, 12 predictions were partially correct (Napoleonic vs. Napoleonic wars). This appears to be consistent with the human performance of 79.7%. Incidentally, we classified each example into one or more categories that are included in Table 8. Distractoral units refer to the presence of beings that resemble the basic truth. For example, for the question Rebecca Front plays Detective Chief Superintendent Trucent Innocent in which television series? Rebecca will describe the first of the roles of each of the question, the question for both of which makes the answer more difficult."}, {"heading": "7 Related work", "text": "Recent interest in answering questions has led to the creation of several datasets, which are either limited in scope or suffer from distortions resulting from their construction process. We group existing datasets according to their respective tasks and compare them to TriviaQA. Analysis is summarized in Table 1."}, {"heading": "7.1 Reading comprehension", "text": "The researchers constructed Cloze-style datasets (Hill et al., 2015; Hermann et al., 2015; Paperno et al., 2016; Onishi et al., 2016) in which the task is to predict missing words, often entities, in a document. Cloze-style datasets are easier to construct automatically, but do not include natural-language questions. Data sets with natural-language questions include MCTest (Richardson et al., 2013), SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2016). MCTest is limited to only 2,640 multiple-choice questions. SQuAD contains 100K crowd-sourced questions and answers paired with short Wikipedia passages. NewsQA uses crowdsourcing to construct questions exclusively from news reviews to control potential biases. The crucial difference between QA / QA-QA-QA-text / QA-QA-QA-QA-QA-quelltext questions, as well as the recent QA-QA-QA-QA-QA-QA-quelltext questions, are limited."}, {"heading": "7.2 Open domain question answering", "text": "The recently published MS Marco dataset (Nguyen et al., 2016) also contains independently written questions and documents from the search results. However, the questions in the dataset are derived from search logs and the answers are crowdsourced. On the other hand, trivia enthusiasts have provided both questions and answers for our datasets. In answering questions from the knowledge base, questions from the natural language are converted into logical forms that can be executed via a KB. Proposed datasets (Cai and Yates, 2013; Berant et al., 2013; Bordes et al., 2015) are either limited in scope or complexity of the questions and can only retrieve facts covered by the KB.A standard task for IR-style QA. (Voorhees and Tice, 2000), which include questions from different areas but are limited in size."}, {"heading": "8 Conclusion and Future Work", "text": "To our knowledge, TriviaQA is the first dataset in which questions are written independently of the evidence provided by TriviaQA enthusiasts. Evidence comes from two areas - web search results and Wikipedia pages - with widely differing levels of information redundancy. Current baseline results indicate that TriviaQA is a demanding testing environment that deserves a significant future study. Although TriviaQA is not the focus of this paper, it does provide a benchmark for a variety of other tasks such as IR-style answers, QA via structured KBs, and joint modelling of KBs and text, with much more data than previously available."}, {"heading": "Acknowledgments", "text": "This work was supported by the DARPA contract FA8750-13-2-0019, the WRF / Cable Professorship, gifts from Google and Tencent and an Allen Distinguished Investigator Award. The authors thank Minjoon Seo for the BiDAF code and Noah Smith, Srinivasan Iyer, Mark Yatskar, Nicholas FitzGerald, Antoine Bosselut, Dallas Card and anonymous critics for helpful comments."}], "references": [{"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston."], "venue": "CoRR abs/1506.02075. https://arxiv.org/abs/1506.02075.", "citeRegEx": "Bordes et al\\.,? 2015", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Large-scale semantic parsing via schema matching and lexicon extension", "author": ["Qingqing Cai", "Alexander Yates."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computa-", "citeRegEx": "Cai and Yates.,? 2013", "shortCiteRegEx": "Cai and Yates.", "year": 2013}, {"title": "A thorough examination of the cnn/daily mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Searchqa: A new q&a dataset augmented with context from a search engine", "author": ["Matthew Dunn", "Levent Sagun", "Mike Higgins", "Ugur Guney", "Volkan Cirik", "Kyunghyun Cho."], "venue": "CoRR https://arxiv.org/abs/1704.05179.", "citeRegEx": "Dunn et al\\.,? 2017", "shortCiteRegEx": "Dunn et al\\.", "year": 2017}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni."], "venue": "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM,", "citeRegEx": "Fader et al\\.,? 2014", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "Tagme: On-the-fly annotation of short text fragments (by wikipedia entities)", "author": ["Paolo Ferragina", "Ugo Scaiella."], "venue": "Proceedings of the 19th ACM International Conference on Information and Knowledge Management. ACM, New", "citeRegEx": "Ferragina and Scaiella.,? 2010", "shortCiteRegEx": "Ferragina and Scaiella.", "year": 2010}, {"title": "Building watson: An overview of the deepqa", "author": ["David Ferrucci", "Eric Brown", "Jennifer Chu-Carroll", "James Fan", "David Gondek", "Aditya A. Kalyanpur", "Adam Lally", "J. William Murdock", "Eric Nyberg", "John Prager", "Nico Schlaefer", "Chris Welty"], "venue": null, "citeRegEx": "Ferrucci et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ferrucci et al\\.", "year": 2010}, {"title": "Teaching machines to read and comprehend", "author": ["Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems. http://arxiv.org/abs/1506.03340.", "citeRegEx": "Suleyman and Blunsom.,? 2015", "shortCiteRegEx": "Suleyman and Blunsom.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "CoRR https://arxiv.org/abs/1511.02301.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S. Weld."], "venue": "Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Hoffmann et al\\.,? 2011", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Knowledge graph and corpus driven segmentation and answer inference for telegraphic entityseeking queries", "author": ["Mandar Joshi", "Uma Sawant", "Soumen Chakrabarti."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language", "citeRegEx": "Joshi et al\\.,? 2014", "shortCiteRegEx": "Joshi et al\\.", "year": 2014}, {"title": "Race: Large-scale reading comprehension dataset from examinations", "author": ["Guokun Lai", "Qizhe Xie", "Hanxiao Liu", "Yiming Yang", "Eduard Hovy."], "venue": "CoRR https://arxiv.org/abs/1704.04683.", "citeRegEx": "Lai et al\\.,? 2017", "shortCiteRegEx": "Lai et al\\.", "year": 2017}, {"title": "Dataset and neural recurrent sequence labeling model for open-domain factoid question answering", "author": ["Peng Li", "Wei Li", "Zhengyan He", "Xuguang Wang", "Ying Cao", "Jie Zhou", "Wei Xu."], "venue": "CoRR https://arxiv.org/abs/1607.06275.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "MS MARCO: A human generated machine reading comprehension dataset", "author": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng."], "venue": "Workshop in Advances in Neural Information Processing Systems.", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Who did what: A large-scale person-centered cloze dataset", "author": ["Takeshi Onishi", "Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David McAllester."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association", "citeRegEx": "Onishi et al\\.,? 2016", "shortCiteRegEx": "Onishi et al\\.", "year": 2016}, {"title": "The lambada dataset: Word prediction requiring a broad discourse", "author": ["Denis Paperno", "Germ\u00e1n Kruszewski", "Angeliki Lazaridou", "Ngoc Quan Pham", "Raffaella Bernardi", "Sandro Pezzelle", "Marco Baroni", "Gemma Boleda", "Raquel Fernandez"], "venue": null, "citeRegEx": "Paperno et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Paperno et al\\.", "year": 2016}, {"title": "Compositional semantic parsing on semi-structured tables", "author": ["Panupong Pasupat", "Percy Liang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Confer-", "citeRegEx": "Pasupat and Liang.,? 2015", "shortCiteRegEx": "Pasupat and Liang.", "year": 2015}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computa-", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "MCTest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher J.C. Burges", "Erin Renshaw."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Richardson et al\\.,? 2013", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum."], "venue": "Proceedings of the 2010 European Conference on Machine Learning and Knowledge Discovery in", "citeRegEx": "Riedel et al\\.,? 2010", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Learning joint query interpretation and response ranking", "author": ["Uma Sawant", "Soumen Chakrabarti."], "venue": "Proceedings of the 22Nd International Conference on World Wide Web. ACM, New York, NY, USA, WWW \u201913, pages 1099\u20131110.", "citeRegEx": "Sawant and Chakrabarti.,? 2013", "shortCiteRegEx": "Sawant and Chakrabarti.", "year": 2013}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR). https://arxiv.org/abs/1611.01603.", "citeRegEx": "Seo et al\\.,? 2017", "shortCiteRegEx": "Seo et al\\.", "year": 2017}, {"title": "Newsqa: A machine comprehension dataset", "author": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman."], "venue": "CoRR https://arxiv.org/abs/1611.09830.", "citeRegEx": "Trischler et al\\.,? 2016", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Building a question answering test collection", "author": ["Ellen M. Voorhees", "Dawn M. Tice."], "venue": "Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, New", "citeRegEx": "Voorhees and Tice.,? 2000", "shortCiteRegEx": "Voorhees and Tice.", "year": 2000}, {"title": "Adapting boosting for information retrieval measures", "author": ["Qiang Wu", "Christopher J. Burges", "Krysta M. Svore", "Jianfeng Gao."], "venue": "Inf. Retr. 13(3):254\u2013270. https://doi.org/10.1007/s10791-009-9112-1.", "citeRegEx": "Wu et al\\.,? 2010", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio."], "venue": "Proceedings of the", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yi Yang", "Wen-tau Yih", "Christopher Meek."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Hierarchical attention networks for document classification", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "Recently, significant progress has been made by introducing large new reading comprehension datasets that primarily focus on one of the challenges listed above, for example by crowdsourcing the gathering of question answer pairs (Rajpurkar et al., 2016) or using cloze-style sentences instead of questions (Hermann et al.", "startOffset": 229, "endOffset": 253}, {"referenceID": 15, "context": ", 2016) or using cloze-style sentences instead of questions (Hermann et al., 2015; Onishi et al., 2016) (see Table 1 for more examples).", "startOffset": 60, "endOffset": 103}, {"referenceID": 18, "context": "SQuAD (Rajpurkar et al., 2016) 3 3 3 7 7 MS Marco (Nguyen et al.", "startOffset": 6, "endOffset": 30}, {"referenceID": 14, "context": ", 2016) 3 3 3 7 7 MS Marco (Nguyen et al., 2016) 3 3 7 3 3 NewsQA(Trischler et al.", "startOffset": 27, "endOffset": 48}, {"referenceID": 23, "context": ", 2016) 3 3 7 3 3 NewsQA(Trischler et al., 2016) 3 3 3 7* 7 WikiQA (Yang et al.", "startOffset": 24, "endOffset": 48}, {"referenceID": 28, "context": ", 2016) 3 3 3 7* 7 WikiQA (Yang et al., 2016) 7 7 7 3 7 TREC (Voorhees and Tice, 2000) 7 3 3 3 3", "startOffset": 26, "endOffset": 45}, {"referenceID": 24, "context": ", 2016) 7 7 7 3 7 TREC (Voorhees and Tice, 2000) 7 3 3 3 3", "startOffset": 23, "endOffset": 48}, {"referenceID": 18, "context": "For example, our dataset contains three times as many questions that require inference over multiple sentences than the recently released SQuAD (Rajpurkar et al., 2016) dataset.", "startOffset": 144, "endOffset": 168}, {"referenceID": 3, "context": "Finally, we present baseline experiments on the TriviaQA dataset, including a linear classifier inspired by work on CNN Dailymail and MCTest (Chen et al., 2016; Richardson et al., 2013) and a state-of-the-art neural network baseline (Seo et al.", "startOffset": 141, "endOffset": 185}, {"referenceID": 19, "context": "Finally, we present baseline experiments on the TriviaQA dataset, including a linear classifier inspired by work on CNN Dailymail and MCTest (Chen et al., 2016; Richardson et al., 2013) and a state-of-the-art neural network baseline (Seo et al.", "startOffset": 141, "endOffset": 185}, {"referenceID": 22, "context": ", 2013) and a state-of-the-art neural network baseline (Seo et al., 2017).", "startOffset": 55, "endOffset": 73}, {"referenceID": 18, "context": "Following recent formulations (Rajpurkar et al., 2016), we further assume that ai appears as a substring for some document in the set Di.", "startOffset": 30, "endOffset": 54}, {"referenceID": 18, "context": ", 2015) or even just a short paragraph (Rajpurkar et al., 2016).", "startOffset": 39, "endOffset": 63}, {"referenceID": 20, "context": "For example, our data would also support multiinstance learning, which makes the at least once assumption, from relation extraction (Riedel et al., 2010; Hoffmann et al., 2011) or many other possibilities.", "startOffset": 132, "endOffset": 176}, {"referenceID": 10, "context": "For example, our data would also support multiinstance learning, which makes the at least once assumption, from relation extraction (Riedel et al., 2010; Hoffmann et al., 2011) or many other possibilities.", "startOffset": 132, "endOffset": 176}, {"referenceID": 6, "context": "We therefore collected an additional set of evidence documents by applying TAGME, an off-the-shelf entity linker (Ferragina and Scaiella, 2010), to find Wikipedia entities mentioned in the question, and added the corresponding pages as evidence documents.", "startOffset": 113, "endOffset": 143}, {"referenceID": 3, "context": "We used a random entity baseline and a simple classifier inspired from previous work (Wang et al., 2015; Chen et al., 2016), and compare these to BiDAF (Seo et al.", "startOffset": 85, "endOffset": 123}, {"referenceID": 22, "context": ", 2016), and compare these to BiDAF (Seo et al., 2017), one of the best performing models for the SQuAD dataset.", "startOffset": 36, "endOffset": 54}, {"referenceID": 25, "context": "The function score is learnt using LambdaMART (Wu et al., 2010) 5, a boosted tree based ranking algorithm.", "startOffset": 46, "endOffset": 63}, {"referenceID": 3, "context": "This is similar to previous entity-centric classifiers for QA (Chen et al., 2016; Wang et al., 2015), and uses context and Wikipedia catalog based features.", "startOffset": 62, "endOffset": 100}, {"referenceID": 3, "context": "Recurrent neural network models (RNNs) (Hermann et al., 2015; Chen et al., 2016) have been very effective for reading comprehension.", "startOffset": 39, "endOffset": 80}, {"referenceID": 22, "context": "For our task, we modified the BiDAF model (Seo et al., 2017), which takes a sequence of context words as input and outputs the start and end positions of the predicted answer in the context.", "startOffset": 42, "endOffset": 60}, {"referenceID": 9, "context": "Researchers have constructed cloze-style datasets (Hill et al., 2015; Hermann et al., 2015; Paperno et al., 2016; Onishi et al., 2016), where the task is to predict missing words, often entities, in a document.", "startOffset": 50, "endOffset": 134}, {"referenceID": 16, "context": "Researchers have constructed cloze-style datasets (Hill et al., 2015; Hermann et al., 2015; Paperno et al., 2016; Onishi et al., 2016), where the task is to predict missing words, often entities, in a document.", "startOffset": 50, "endOffset": 134}, {"referenceID": 15, "context": "Researchers have constructed cloze-style datasets (Hill et al., 2015; Hermann et al., 2015; Paperno et al., 2016; Onishi et al., 2016), where the task is to predict missing words, often entities, in a document.", "startOffset": 50, "endOffset": 134}, {"referenceID": 19, "context": "Datasets with natural language questions include MCTest (Richardson et al., 2013), SQuAD (Rajpurkar et al.", "startOffset": 56, "endOffset": 81}, {"referenceID": 18, "context": ", 2013), SQuAD (Rajpurkar et al., 2016), and NewsQA (Trischler et al.", "startOffset": 15, "endOffset": 39}, {"referenceID": 23, "context": ", 2016), and NewsQA (Trischler et al., 2016).", "startOffset": 20, "endOffset": 44}, {"referenceID": 12, "context": "Other recently released datasets include (Lai et al., 2017).", "startOffset": 41, "endOffset": 59}, {"referenceID": 14, "context": "The recently released MS Marco dataset (Nguyen et al., 2016) also contains independently authored questions and documents drawn from the search results.", "startOffset": 39, "endOffset": 60}, {"referenceID": 2, "context": "Proposed datasets (Cai and Yates, 2013; Berant et al., 2013; Bordes et al., 2015) are either limited in scale or in the complexity of questions, and can only retrieve facts covered by the KB.", "startOffset": 18, "endOffset": 81}, {"referenceID": 0, "context": "Proposed datasets (Cai and Yates, 2013; Berant et al., 2013; Bordes et al., 2015) are either limited in scale or in the complexity of questions, and can only retrieve facts covered by the KB.", "startOffset": 18, "endOffset": 81}, {"referenceID": 1, "context": "Proposed datasets (Cai and Yates, 2013; Berant et al., 2013; Bordes et al., 2015) are either limited in scale or in the complexity of questions, and can only retrieve facts covered by the KB.", "startOffset": 18, "endOffset": 81}, {"referenceID": 24, "context": "A standard task for open domain IR-style QA is the annual TREC competitions (Voorhees and Tice, 2000), which contains questions from various domains but is limited in size.", "startOffset": 76, "endOffset": 101}, {"referenceID": 7, "context": "the IBM Watson system for Jeopardy! (Ferrucci et al., 2010).", "startOffset": 36, "endOffset": 59}, {"referenceID": 4, "context": "Other datasets includes SearchQA (Dunn et al., 2017) where Jeopardy! questions are paired with search engine snippets, the WikiQA dataset (Yang et al.", "startOffset": 33, "endOffset": 52}, {"referenceID": 27, "context": ", 2017) where Jeopardy! questions are paired with search engine snippets, the WikiQA dataset (Yang et al., 2015) for answer sentence selection, and the Chinese language WebQA", "startOffset": 93, "endOffset": 112}, {"referenceID": 13, "context": "(Li et al., 2016) dataset, which focuses on the task", "startOffset": 0, "endOffset": 17}, {"referenceID": 5, "context": "Other recent approaches attempt to combine structured high precision KBs with semistructured information sources like OpenIE triples (Fader et al., 2014), HTML tables (Pasupat and Liang, 2015), and large (and noisy) corpora (Sawant and Chakrabarti, 2013; Joshi et al.", "startOffset": 133, "endOffset": 153}, {"referenceID": 17, "context": ", 2014), HTML tables (Pasupat and Liang, 2015), and large (and noisy) corpora (Sawant and Chakrabarti, 2013; Joshi et al.", "startOffset": 21, "endOffset": 46}, {"referenceID": 21, "context": ", 2014), HTML tables (Pasupat and Liang, 2015), and large (and noisy) corpora (Sawant and Chakrabarti, 2013; Joshi et al., 2014; Xu et al., 2015).", "startOffset": 78, "endOffset": 145}, {"referenceID": 11, "context": ", 2014), HTML tables (Pasupat and Liang, 2015), and large (and noisy) corpora (Sawant and Chakrabarti, 2013; Joshi et al., 2014; Xu et al., 2015).", "startOffset": 78, "endOffset": 145}, {"referenceID": 26, "context": ", 2014), HTML tables (Pasupat and Liang, 2015), and large (and noisy) corpora (Sawant and Chakrabarti, 2013; Joshi et al., 2014; Xu et al., 2015).", "startOffset": 78, "endOffset": 145}], "year": 2017, "abstractText": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K questionanswer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a featurebased classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study.1", "creator": "LaTeX with hyperref package"}}}