{"id": "1705.03773", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2017", "title": "Flexible and Creative Chinese Poetry Generation Using Neural Memory", "abstract": "It has been shown that Chinese poems can be successfully generated by sequence-to-sequence neural models, particularly with the attention mechanism. A potential problem of this approach, however, is that neural models can only learn abstract rules, while poem generation is a highly creative process that involves not only rules but also innovations for which pure statistical models are not appropriate in principle. This work proposes a memory-augmented neural model for Chinese poem generation, where the neural model and the augmented memory work together to balance the requirements of linguistic accordance and aesthetic innovation, leading to innovative generations that are still rule-compliant. In addition, it is found that the memory mechanism provides interesting flexibility that can be used to generate poems with different styles.", "histories": [["v1", "Wed, 10 May 2017 13:55:53 GMT  (374kb,D)", "http://arxiv.org/abs/1705.03773v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["jiyuan zhang", "yang feng", "dong wang", "yang wang", "andrew abel", "shiyue zhang", "andi zhang"], "accepted": true, "id": "1705.03773"}, "pdf": {"name": "1705.03773.pdf", "metadata": {"source": "CRF", "title": "Flexible and Creative Chinese Poetry Generation Using Neural Memory", "authors": ["Jiyuan Zhang", "Yang Feng", "Dong Wang", "Yang Wang", "Andrew Abel", "Shiyue Zhang", "Andi Zhang"], "emails": ["ml@pku.edu.cn,", "wangdong99@mails.tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Related Work", "text": "The first approach is based on rules and / or templates. For example, phrase search (Tosa et al., 2009; Wu et al., 2009), word association norm (Wang et al., 2009), template search (Oliveira, 2012), genetic search (Zhou et al., 2010), text summary (Yan et al., 2013). Another approach involves various SMT methods, e.g. (Jiang and Zhou, 2008; He et al., 2012). One drawback that the above methods share is that they are based on the surface shapes of words or characters that do not have a deep understanding of the meaning of a poem. More recently, neural models have been the subject of much attention. A clear advantage of neural methods is that they \"discover\" the meaning of words or signs, and can therefore more deeply understand the meaning of a poem."}, {"heading": "3 Memory-augmented neural model", "text": "In this section we first present the idea of memory enhancement and then describe the model structure and the training method."}, {"heading": "3.1 Memory augmentation", "text": "The idea of memory expansion is illustrated in Fig. 1. It contains two components, the component of the neural model on the left and the memory component on the right. In this work, the attention-based model of generation RNN presented by Wang et al., 2016a is used as a component of the neural model, although any neural model is suitable. The memory component contains a series of \"direct\" mappings from input to output and can therefore be used to store some specific cases of the generation that cannot be represented by the neural model. In poetry generation, the memory stores the information about which character is to be generated in a particular context. Output of the two components is then integrated, resulting in a consolidated output.There are several ways to understand the memory-extended neural model. First, it can be seen as a way to combine reason (neural) and knowledge (memory)."}, {"heading": "3.2 Model structure", "text": "Using the Chinese poetry model shown in Fig. 1 as an example, this section discusses the creation of a memory-enhanced neural model (mi = mi). First, the neural model part is an attention-based sequence-to-sequence model (Bahdanau et al., 2014).The encoder is a bidirectional RNN (with GRU units) that converts the in-put theme-specific memory designated by the corresponding embedding of compositional characters (x1, y2,..., xN) into a sequence of hidden states (h1, h2,..., hN), which generates the prediction of the entire quatraine model based on the status of the previous generation, yst \u2212 1, as well as all \u2212 1 states."}, {"heading": "3.3 Model Training", "text": "The training algorithm follows the scheme defined in (Wang et al., 2016a), which uses the cross entropy between the distributions of Chinese characters given by the decoder and the basic truth as an objective function. Optimization uses the SGD algorithm together with AdaDelta to adjust the learning rate (Zeiler, 2012)."}, {"heading": "4 Memory augmentation for Chinese poetry generation", "text": "This section describes how the memory mechanism can be used to solve a trade-off between the requirements for compliant generation and aesthetic innovation, and how it can also be used for more interesting things, such as style transfers."}, {"heading": "4.1 Memory for innovative generation", "text": "In this section, we describe how the storage mechanism promotes innovation."}, {"heading": "4.2 Memory for style transfer", "text": "The effect of memory is easy to control. For example, the complexity of the behavior can be controlled by the memory size, the marked distortion by the memory selection, and the strength of the impact by the weighting parameter \u03b2. This means that the memory mechanism is very flexible and can be used to produce poems with desired characteristics. In this thesis, we use these capabilities to generate poems with different styles. This was illustrated in Figure (g), where the energy function of the style memory shown in Figure (d) tends to a certain style, and once it is added to the energy function of the one-iteration model, the resulting energy function shown in Figure (g) receives lower values in places that correspond to the locations of the memory, which promotes the generation of poems with similar styles as the poems in memory."}, {"heading": "5 Experiments", "text": "This section describes the experiments and results carried out in this paper. Here, the basic system was a reproduction of the Attention-based System presented in (Wang et al., 2016a). The model in this system has proven to be quite flexible and powerful: it can generate different genres of Chinese poetry, and in the generation of quatrains it has been shown to be capable of deceiving human experts in many cases (Wang et al., 2016a) and the authors have performed a thorough comparison with competitive methods mentioned in the related work of this paper. We obtained the database and source code (in theano), and reproduced their system with Tensorflow from Google2. We did not make comparisons with some previous methods such as NNLM, RNNPG, as they were fully compared in (Wang et al., 2016a), and all of them were much worse than the attention-based system."}, {"heading": "5.1 Datasets", "text": "Our CPC dataset contains 284,899 traditional Chinese poems in various genres, including Tang quadruplets, Song iambics, Yuan songs, and Ming and Qing poems. This large amount of data ensures reliable learning for the semantic content of most Chinese characters. Our second dataset is a Chinese quadruplet corpus (CQC), which we gathered from the Internet and which consists of 13, 299 quadruplets and 65, 560 quadruplets. This corpus was used to train the attention-based RNN baseline. We filtered out the poems whose letters are all from quantum sources."}, {"heading": "5.2 Evaluation Process", "text": "We invited 34 experts to evaluate the quality of poetry production. In the innovation experiment, the evaluation consisted of a comparison between different systems and configurations in terms of the five metrics; the innovation questions presented two poems to the expert and asked him to assess which of the poems was better in terms of the five metrics; in the style transfer experiment, the evaluation was performed by determining the style of a poem produced; the evaluation was conducted online, with each questionnaire containing 11 questions focused on innovation and 4 questions related to style transfer; each of the style transfer questions presented a single poem to the expert and asked him to rate it between 1 and 5, with a higher score in terms of compliance, aesthetic innovation, 3http: / / vivi.cslt.org Consistency of the scenario and fluxity was better; they were also asked to specify the style of the poem."}, {"heading": "5.3 Innovation experiment", "text": "This experiment focuses on the contribution of memory to innovative poetry generation. We experimented with two configurations: one is with a one-iteration model (C1) and the other is with an overloaded model (C \u221e). The memory was generated from the 500 quatrains in MEM-I, and the weight factor was empirically defined as 16 for C1 and 49 for C. The themes of the generation were therefore selected in pairs and each pair of configurations contains at least 180 ratings. The results are shown in Table 2, where the preference ratio for each pair of configurations with the same theme was generated, the experts were asked to choose which they preferred. The rating is therefore paired and each pair of configurations contains at least 180 ratings. The results are shown in the preference rate for each pair of configurations, which was not tested in terms of the 5 metrics."}, {"heading": "5.4 Style-transfer experiment", "text": "In the second experiment, the storage mechanism is used to generate poems in different styles. We chose three styles: pastoral, battlefield, and romantic. A stylistic memory, which we call style memory, was constructed for each style by the corresponding quatrains in the MEM-S dataset. The system with an iteration model C1 was used as a basis. Two types of themes were used in the experiment, one is general and the other is stylistically biased. Experiments then examine whether the storage mechanism can produce a clear style when the theme is general, and can be transferred to another if the theme is stylistically biased."}, {"heading": "5.5 Examples", "text": "Table 5 to Table 7 shows sample poems generated by the system C1, C1 + Mem, and C1 + Style Mem, in which case the style is said to be romantic; the three poems were generated with the same very general theme (\"(itself)\")."}, {"heading": "6 Conclusions", "text": "Experimental results showed that memory can stimulate innovation from two opposite directions: either by encouraging the creative generation of regularly trained models, or by encouraging compliance with rules for revised models. Both strategies work well, although the former produced poetry that was preferred by experts in our experiments; and we found that memory can be used to flexibly change the style of the generated poems; the experts we worked with believe that the current generation is comparable to today's experienced amateur poets; future work will look at a better memory selection scheme; and other regulatory methods (e.g. standard or drop-out) are also interesting and could alleviate the problem of overmatching."}, {"heading": "Acknowledgments", "text": "This work was supported by the National Natural Science Foundation of China (NSFC) as part of Project No.61371136, No.61633013, No.61472428."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka."], "venue": "arXiv preprint arXiv:1410.5401 .", "citeRegEx": "Graves et al\\.,? 2014", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Generating Chinese classical poems with statistical machine translation models", "author": ["Jing He", "Ming Zhou", "Long Jiang."], "venue": "Twenty-Sixth AAAI Conference on Artificial Intelligence.", "citeRegEx": "He et al\\.,? 2012", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "Generating Chinese couplets using a statistical mt approach", "author": ["Long Jiang", "Ming Zhou."], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics. Association for Computational Linguistics, volume 1, pages 377\u2013384.", "citeRegEx": "Jiang and Zhou.,? 2008", "shortCiteRegEx": "Jiang and Zhou.", "year": 2008}, {"title": "Gaiku: Generating haiku with word associations norms", "author": ["Yael Netzer", "David Gabay", "Yoav Goldberg", "Michael Elhadad."], "venue": "Proceedings of the Workshop on Computational Approaches to Linguistic Creativity. Association for Computational Lin-", "citeRegEx": "Netzer et al\\.,? 2009", "shortCiteRegEx": "Netzer et al\\.", "year": 2009}, {"title": "Poetryme: a versatile platform for poetry generation", "author": ["H Oliveira."], "venue": "Proceedings of the ECAI 2012 Workshop on Computational Creativity, Concept Invention, and General Intelligence.", "citeRegEx": "Oliveira.,? 2012", "shortCiteRegEx": "Oliveira.", "year": 2012}, {"title": "English Translation for Tang Poems (Ying Yi Tang Shi San Bai Shou)", "author": ["Yihe Tang."], "venue": "Tianjin People Publisher.", "citeRegEx": "Tang.,? 2005", "shortCiteRegEx": "Tang.", "year": 2005}, {"title": "Hitch haiku: An interactive supporting system for composing haiku poem", "author": ["Naoko Tosa", "Hideto Obara", "Michihiko Minoh."], "venue": "Entertainment Computing-ICEC 2008 pages 209\u2013216. Springer.", "citeRegEx": "Tosa et al\\.,? 2009", "shortCiteRegEx": "Tosa et al\\.", "year": 2009}, {"title": "A Summary of Rhyming Constraints of Chinese Poems (Shi Ci Ge Lv Gai Yao), volume 1", "author": ["Li Wang."], "venue": "Beijin Press.", "citeRegEx": "Wang.,? 2002", "shortCiteRegEx": "Wang.", "year": 2002}, {"title": "Can machine generate traditional Chinese poetry? a feigenbaum test", "author": ["Qixin Wang", "Tianyi Luo", "Dong Wang."], "venue": "BICS 2016.", "citeRegEx": "Wang et al\\.,? 2016a", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Chinese song iambics generation with neural attention-based model", "author": ["Qixin Wang", "Tianyi Luo", "Dong Wang", "Chao Xing."], "venue": "IJCAI 16.", "citeRegEx": "Wang et al\\.,? 2016b", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Chinese poetry generation with planning based neural network", "author": ["Zhe Wang", "Wei He", "Hua Wu", "Haiyang Wu", "Wei Li", "Haifeng Wang", "Enhong Chen."], "venue": "COLING 2016.", "citeRegEx": "Wang et al\\.,? 2016c", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."], "venue": "arXiv preprint arXiv:1410.3916 .", "citeRegEx": "Weston et al\\.,? 2014", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "New hitch haiku: An interactive renku poem composition supporting tool applied for sightseeing navigation system", "author": ["Xiaofeng Wu", "Naoko Tosa", "Ryohei Nakatsu."], "venue": "Entertainment Computing-ICEC 2009 pages 191\u2013196. Springer.", "citeRegEx": "Wu et al\\.,? 2009", "shortCiteRegEx": "Wu et al\\.", "year": 2009}, {"title": "i, Poet: Automatic poetry composition through recurrent neural networks with iterative polishing schema", "author": ["Rui Yan."], "venue": "IJCAI2016.", "citeRegEx": "Yan.,? 2016", "shortCiteRegEx": "Yan.", "year": 2016}, {"title": "i, Poet: automatic Chinese poetry composition through a generative summarization framework under constrained optimization", "author": ["Rui Yan", "Han Jiang", "Mirella Lapata", "Shou-De Lin", "Xueqiang Lv", "Xiaoming Li."], "venue": "Proceedings of the Twenty-Third", "citeRegEx": "Yan et al\\.,? 2013", "shortCiteRegEx": "Yan et al\\.", "year": 2013}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701 .", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Chinese poetry generation with recurrent neural networks", "author": ["Xingxing Zhang", "Mirella Lapata."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 670\u2013680.", "citeRegEx": "Zhang and Lapata.,? 2014", "shortCiteRegEx": "Zhang and Lapata.", "year": 2014}, {"title": "Genetic algorithm and its implementation of automatic generation of Chinese Songci", "author": ["Cheng-Le Zhou", "Wei You", "Xiaojun Ding."], "venue": "Journal of Software 21(3):427\u2013437.", "citeRegEx": "Zhou et al\\.,? 2010", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "Among the various genres, perhaps the most popular one is the quatrain, a special style with a strict structure (four lines with five or seven characters per line), a regulated rhythmical form (the last characters in the second and fourth lines must follow the same rhythm), and a required tonal pattern (tones of characters in some positions should satisfy a predefined regulation) (Wang, 2002).", "startOffset": 383, "endOffset": 395}, {"referenceID": 6, "context": "The translation is from (Tang, 2005).", "startOffset": 24, "endOffset": 36}, {"referenceID": 18, "context": "For example, rule-based methods (Zhou et al., 2010), statistical machine translation (SMT) models (Jiang and Zhou, 2008; He et al.", "startOffset": 32, "endOffset": 51}, {"referenceID": 3, "context": ", 2010), statistical machine translation (SMT) models (Jiang and Zhou, 2008; He et al., 2012) and neural models (Zhang and Lapata, 2014; Wang et al.", "startOffset": 54, "endOffset": 93}, {"referenceID": 2, "context": ", 2010), statistical machine translation (SMT) models (Jiang and Zhou, 2008; He et al., 2012) and neural models (Zhang and Lapata, 2014; Wang et al.", "startOffset": 54, "endOffset": 93}, {"referenceID": 9, "context": ", rule-based or SMT), the neural model approach tends to generate more fluent poems and some generations are so natural that even professional poets can not tell they are the work of machines (Wang et al., 2016a).", "startOffset": 192, "endOffset": 212}, {"referenceID": 7, "context": "For example, phrase search (Tosa et al., 2009; Wu et al., 2009), word association norm (Netzer et al.", "startOffset": 27, "endOffset": 63}, {"referenceID": 13, "context": "For example, phrase search (Tosa et al., 2009; Wu et al., 2009), word association norm (Netzer et al.", "startOffset": 27, "endOffset": 63}, {"referenceID": 4, "context": ", 2009), word association norm (Netzer et al., 2009), template search (Oliveira, 2012), genetic search (Zhou et al.", "startOffset": 31, "endOffset": 52}, {"referenceID": 5, "context": ", 2009), template search (Oliveira, 2012), genetic search (Zhou et al.", "startOffset": 25, "endOffset": 41}, {"referenceID": 18, "context": ", 2009), template search (Oliveira, 2012), genetic search (Zhou et al., 2010), text summarization (Yan et al.", "startOffset": 58, "endOffset": 77}, {"referenceID": 15, "context": ", 2010), text summarization (Yan et al., 2013).", "startOffset": 28, "endOffset": 46}, {"referenceID": 3, "context": ", (Jiang and Zhou, 2008; He et al., 2012).", "startOffset": 2, "endOffset": 41}, {"referenceID": 2, "context": ", (Jiang and Zhou, 2008; He et al., 2012).", "startOffset": 2, "endOffset": 41}, {"referenceID": 13, "context": "The first study we have found in this direction is the work by Zhang and Lapata (2014), which proposed an RNN-based approach that produces each new line character-by-character using a recurrent neural network (RNN), with all the lines generated already (in the form of a vector) as a contextual input.", "startOffset": 63, "endOffset": 87}, {"referenceID": 8, "context": "Wang et al. (2016b) proposed a much simpler neural model that treats a poem as an entire character sequence, and poem generation is conducted character-by-character.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "(2016b) utilized the neural attention mechanism (Bahdanau et al., 2014) by which human intention is encoded by an RNN to guide the generation.", "startOffset": 48, "endOffset": 71}, {"referenceID": 7, "context": "To avoid theme drift caused by this long-sequence generation, Wang et al. (2016b) utilized the neural attention mechanism (Bahdanau et al.", "startOffset": 62, "endOffset": 82}, {"referenceID": 0, "context": "(2016b) utilized the neural attention mechanism (Bahdanau et al., 2014) by which human intention is encoded by an RNN to guide the generation. The same model was used by Wang et al. (2016a) for Chinese quatrain generation.", "startOffset": 49, "endOffset": 190}, {"referenceID": 0, "context": "(2016b) utilized the neural attention mechanism (Bahdanau et al., 2014) by which human intention is encoded by an RNN to guide the generation. The same model was used by Wang et al. (2016a) for Chinese quatrain generation. Yan (2016) proposed a hierarchical RNN model that conducts iterative generation.", "startOffset": 49, "endOffset": 234}, {"referenceID": 0, "context": "(2016b) utilized the neural attention mechanism (Bahdanau et al., 2014) by which human intention is encoded by an RNN to guide the generation. The same model was used by Wang et al. (2016a) for Chinese quatrain generation. Yan (2016) proposed a hierarchical RNN model that conducts iterative generation. Recently, Wang et al. (2016c) proposed a similar sequence generation model, but with the difference that attention is placed not only on the human input, but also on all the characters that have been generated so far.", "startOffset": 49, "endOffset": 334}, {"referenceID": 8, "context": "Our system was built following the model structure and training strategy proposed by Wang et al. (2016a) due to its simplicity and demonstrated quality, but the memory mechanism is general and can be applied to any of the models presented above.", "startOffset": 85, "endOffset": 105}, {"referenceID": 12, "context": ", 2014, 2016) and memory network (Weston et al., 2014).", "startOffset": 33, "endOffset": 54}, {"referenceID": 9, "context": "In this work, the attention-based RNN generation model presented by (Wang et al., 2016a) is used as the neural model component, although any neural model is suitFigure 1: The memory-augmented neural model used for Chinese poetry generation.", "startOffset": 68, "endOffset": 88}, {"referenceID": 11, "context": "Note that this memory-augmented neural model is inspired by and related to the memory network proposed by Weston et al.(2014) and Graves et al.", "startOffset": 106, "endOffset": 126}, {"referenceID": 1, "context": "(2014) and Graves et al.(2016), but we more focus on an accompanying memory that plays the role of assistance and regularization.", "startOffset": 11, "endOffset": 31}, {"referenceID": 0, "context": "Firstly, the neural model part is an attentionbased sequence-to-sequence model (Bahdanau et al., 2014).", "startOffset": 79, "endOffset": 102}, {"referenceID": 9, "context": "The training algorithm follows the scheme defined in (Wang et al., 2016a), where the cross entropy between the distributions over Chinese characters given by the decoder and the ground truth is used as the objective function.", "startOffset": 53, "endOffset": 73}, {"referenceID": 16, "context": "The optimization uses the SGD algorithm together with AdaDelta to adjust the learning rate (Zeiler, 2012).", "startOffset": 91, "endOffset": 105}, {"referenceID": 9, "context": "Here, The baseline system was a reproduction of the Attention-based system presented in (Wang et al., 2016a).", "startOffset": 88, "endOffset": 108}, {"referenceID": 9, "context": "ble and powerful: it can generate different genres of Chinese poems, and when generating quatrains it has been shown to be able to fool human experts in many cases (Wang et al., 2016a) and the authors had did a thorough comparison with competitive methods mentioned in the related work of this paper.", "startOffset": 164, "endOffset": 184}, {"referenceID": 9, "context": "We didn\u2019t make comparisons with some previous methods such as NNLM, SMT, RNNPG as they had been fully compared in (Wang et al., 2016a) and all of them were much worse than the attention-based system.", "startOffset": 114, "endOffset": 134}, {"referenceID": 11, "context": "We also reproduced the model in (Wang et al., 2016c) with the help of the first author.", "startOffset": 32, "endOffset": 52}, {"referenceID": 11, "context": "It should be also emphasized that the memory approach proposed in this paper is a general technique and is complementary to other efforts such as the planning approach (Wang et al., 2016c) and the recursive approach (Yan, 2016).", "startOffset": 168, "endOffset": 188}, {"referenceID": 14, "context": ", 2016c) and the recursive approach (Yan, 2016).", "startOffset": 36, "endOffset": 47}], "year": 2017, "abstractText": "It has been shown that Chinese poems can be successfully generated by sequence-to-sequence neural models, particularly with the attention mechanism. A potential problem of this approach, however, is that neural models can only learn abstract rules, while poem generation is a highly creative process that involves not only rules but also innovations for which pure statistical models are not appropriate in principle. This work proposes a memory-augmented neural model for Chinese poem generation, where the neural model and the augmented memory work together to balance the requirements of linguistic accordance and aesthetic innovation, leading to innovative generations that are still rule-compliant. In addition, it is found that the memory mechanism provides interesting flexibility that can be used to generate poems with different styles.", "creator": "LaTeX with hyperref package"}}}