{"id": "1705.03919", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2017", "title": "A Minimal Span-Based Neural Constituency Parser", "abstract": "In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).", "histories": [["v1", "Wed, 10 May 2017 18:44:15 GMT  (30kb)", "http://arxiv.org/abs/1705.03919v1", "To appear in ACL 2017"]], "COMMENTS": "To appear in ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mitchell stern", "jacob andreas", "dan klein"], "accepted": true, "id": "1705.03919"}, "pdf": {"name": "1705.03919.pdf", "metadata": {"source": "CRF", "title": "A Minimal Span-Based Neural Constituency Parser", "authors": ["Mitchell Stern", "Jacob Andreas Dan Klein"], "emails": ["mitchell@cs.berkeley.edu", "jda@cs.berkeley.edu", "klein@cs.berkeley.edu"], "sections": [{"heading": null, "text": "We show that this model is not only compatible with classical dynamic programming techniques, but also allows for a novel greedy top-down inference algorithm based on recursive partitioning of the input. We show empirically that both prediction models compete with recent work and, combined with fundamental enhancements to the scoring model, are capable of delivering state-of-the-art single-model performance on Penn Treebank (91.79 F1) and strong performance on French Treebank (82.23 F1)."}, {"heading": "1 Introduction", "text": "This paper presents a minimal but surprisingly effective span-based neural model for parsing constituencies. In recent years, there has also been a great deal of interest in analyzing architectures based on recursive neural networks (RNN) (Vinyals et al., 2015). Despite evidence that linear RNN decoders are implicitly capable of respecting some non-trivial well-designed constraints on structured outputs (Graves, 2013), researchers have consistently found that the best performance is achieved by systems that explicitly require the decoder to generate well-shaped tree structures (Chen and Manning, 2014). There are two general approaches to ensure this structural consistency (Graves, 2013). Most commonly, encoding the output as a result of operations within a transition system in which trees are built step by step. This transforms the analyzing problem back into a sequence-sequence-only-sequence-sequence-only-action-guaranteed-to-produce-good-sequence-sequence-sequence-sequence-sequence-sequence-sequence-response."}, {"heading": "2 Model", "text": "The fact is that we see ourselves as being able to be in a position, and that we will be able to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position where we are."}, {"heading": "3 Chart Parsing", "text": "Our basic model is compatible with traditional diagram-based dynamic programming. If a constituency tree T is represented by its designated spans T: = {(it, (it, jt))): t = 1,..., | T |}, then the value of a tree is defined as the sum of its components label and span scores, stree (T) = \u2211 (, (i, j)) \u0441T [slabel (i, j,) + sspan (i, j)]. To find the tree with the highest score for a given set, we use a modified CKY recursion. As with classical diagram parsing, the runtime of our method is O (n3) for a set of length n."}, {"heading": "3.1 Dynamic Program for Inference", "text": "The base case is a margin (i, i + 1) that consists of a single word. Since every valid tree must contain all singleton scans, possibly with blank labels, we do not have to consider the span score in this case and only make a single maximization using the selection of the label: sbest (i, i + 1) = max [slabel (i, i + 1,). For a general margin (i, j) we define the span score (i, k, j) as the sum of its partial scores, ssplit (i, k, j) = ssplit (i, k) + sspan (k, j). (1) For simplicity we also define an extended split score including the scores of the corresponding sub-trees, s-split (i, k, j) = ssplit (i, k, j) = ssplit (i, labels) + sbest (i), k + scores on general scores, we can add scores on common scores, best on common scores, j) = ssplit (i, k-scores on common scores on common scores, we can then use these scores on common scores."}, {"heading": "3.2 Margin Training", "text": "The training of the model under this inference scheme is carried out using a margin-based approach. On submission of an example set and the corresponding parse tree T \u043a, we calculate the best prediction under the current model using the dynamic program described above using the dynamic program T-stree = argmax T [stree (T)] described above. If T-stree (T-stree), then our prediction was correct and no changes had to be made. Otherwise, a hinge penalty of the form max (0, 1 \u2212 stree (T-stree) + stree (T-stree)) arises to encourage the model to maintain a distance of at least 1 between the gold tree and the best alternative. The loss to be minimized is then the sum of the penalties in all training examples. Preparatory work has shown that it may be advantageous in a variety of applications to include a structured loss function in this margin target, whereby the leaf above the T (T) is replaced by a T (T)."}, {"heading": "4 Top-Down Parsing", "text": "This year, it will be able to fix the mentioned bugs in order to fix them."}, {"heading": "4.1 Margin Training", "text": "As with the chart analysis formula, we also use a margin-based method to learn within the topdown model. However, instead of demanding a separation between the values full of trees, we apply a local margin at each decision point. For a span (i, j) that occurs in the gold tree, we allow the correct designation and the splitting point and allow the predictions made by calculating the maximizations in Equation (3) to be imposed on us. Similarly, if k 6 = k, which means that the prediction is wrong, we incur a hinged penalty of Formmax (0, 1 \u2212 ssplit (i, j,) + slabel (i, j,) + slabel (i, j, j).In order to reduce the loss of any hand before we receive the appropriate sum of training points, we apply a hinge penalty to Formmax (0, 1 \u2212 ssplit (i, k, j) + ssplit (i, k,) + slabel (i, j, j)."}, {"heading": "4.2 Training with Exploration", "text": "The hinge penalties given above are defined only for spans (i, j) that appear in the example tree. < b) The model must therefore be limited during the training period to follow decisions that accurately reproduce the gold tree, since supervision cannot be provided otherwise. As a result, the model is never exposed to its mistakes, which can lead to a lack of calibration and poor performance during the test period. To work around this problem, a dynamic oracle can be defined to inform the model of correct behavior, even after it has deviated from the gold tree. Cross and Huang (2016) propose such an oracle for a related transition-based parsing system and prove its optimism for F1 metrics on labeled spans. We adapt their result here to obtain a dynamic oracle for the current model with similar warranties. The oracle for labeling decisions carries no modification over the correct label for the label, if the label is the correct label for the label, or the label is the label."}, {"heading": "5 Scoring and Loss Alternatives", "text": "The model presented in Section 2 is intended to be as simple as possible. However, there are many variations of the label and span scoring functions that could be explored; here we discuss some of the options."}, {"heading": "5.1 Top-Middle-Bottom Label Scoring", "text": "Our basic model treats the empty label, the elementary nonterminals, and the simple chains as atomic units, thus obscuring the similarities between simple chains and their nonterminal components, or between different undermined chains with common prefixes or suffixes. To address this lack of structure, we consider an alternative evaluation scheme in which labels are predicted in three parts: a top nonterminal chain, a middle nonterminal chain, and a bottom nonterminal chain (each of which may be empty).1 This not only allows parameters to be divided between labels with common subcomponents, but also has the added advantage of allowing the model to decompose novel nonterminal chains at test time.1In detail, the model decomposes as (X, Z1- \u00b7 \u00b7 \u00b7 -SK, Y), X-Y decomposes as (X, \u2205), X-Y decomposes as (X, \u2205), Y, as (X, \u2205), Y, (X, \u2205), and K-SY labels."}, {"heading": "5.2 Left and Right Span Scoring", "text": "The basic model uses the same chip scoring function sspan to assign a score to the left and right segments of a given span. A simple extension is to replace them with a pair of different left and right feedback forward networks of the same shape, which results in the decomposition distribution (i, k, j) = left (i, k) + right (k, j)."}, {"heading": "5.3 Span Concatenation Scoring", "text": "Since chip scores are only used to evaluate splits in our model, we also consider direct scoring splitting by feeding the concatenation of the chip representations of the left and right subspans through a single feedback network, with givingssplit (i, k, j) = v s g (Ws [sik; skj] + bs), similar to the structural scoring function used by Cross and Huang (2016), although their concatenation also includes features for the outer spreads (0, i) and (j, n)."}, {"heading": "5.4 Deep Biaffine Span Scoring", "text": "Inspired by the success of deep biaffin scoring in the recent work of Dozat and Manning (2016) on dependency parsing, we also consider a split-scoring function of a similar form for our model. Specifically, we allow hik = fleft (sik) and hkj = Schreck (skj) to be deep left and right voltage representations, which we obtain by passing the child vectors through appropriate left and right feedback networks. Then, we define the biaffine split-scoring function split (i, k, j) = h'ikWshkj + v'lefthik + v'righthkj, which consists of the sum of a biline form between the two hidden representations together with two internal products."}, {"heading": "5.5 Structured Label Loss", "text": "The three-page label evaluation scheme described in Section 5.1 provides a way to incorporate the label structure into the model. In addition, we consider a structured hamming loss on labels. Specifically, we define the loss in the face of two labels consisting of zero or more nonterminals as: \"1,\" \"2\" and \"1,\" with each label treated as a plurality of nonterminals. This structured loss can be incorporated into the training process using the methods described in Sections 3.2 and 4.1."}, {"heading": "6 Experiments", "text": "We first describe the general setup used for our experiments."}, {"heading": "7 Related Work", "text": "Many early successful approaches to constituency analysis focused on the rich modeling of correlations in production space, typically by developing proabilistic context-free grammars with state spaces enriched to capture long-distance dependencies and lexical phenomena (Collins, 2003; Klein and Manning, 2003; Petrov and Klein, 2007). In contrast, the approach described here continues a recent series of work on direct modeling of input correlations by using rich characteristic representations to parameterize local potentials that interact with a comparatively unrestricted structured decoder. As mentioned in the introduction, this class of function-based tree assessment functions can be implemented either with a linear transition system (Chen and Manning, 2014) or with a global decoder (Finkel et al., 2008). Kiperwasser and Goldberg (2016) describe an approach that is closely related to our 2016, but easy to decode modal models (both 2016 and 2014)."}, {"heading": "8 Conclusion", "text": "Our model supports both accurate diagram-based decoding and a novel top-down inference process. Both approaches deliver state-of-the-art performance at Penn Treebank, and our best model delivers competitive performance at French Treebank. Our experiments show that many of the key insights from recent neural transition-based approaches to parsing can be easily ported into the diagram parsing setting, resulting in two extremely simple models that still deliver outstanding performance."}, {"heading": "Acknowledgments", "text": "We would like to thank Nick Altieri and the anonymous reviewers for their valuable comments and suggestions. MS is supported by an NSF Graduate Research Fellowship. YES is supported by a Facebook Graduate Scholarship and a Berkeley AI / Huawei Fellowship."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Training with exploration improves a greedy stack-lstm parser", "author": ["Miguel Ballesteros", "Yoav Goldberg", "Chris Dyer", "Noah A Smith."], "venue": "arXiv preprint arXiv:1603.03793 .", "citeRegEx": "Ballesteros et al\\.,? 2016", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2016}, {"title": "The imswroc\u0142aw-szeged-cis entry at the spmrl 2014 shared task: Reranking and morphosyntax meet unlabeled", "author": ["Anders Bj\u00f6rkelund", "Ozlem Cetinoglu", "Agnieszka Falenska", "Rich\u00e1rd Farkas", "Thomas M\u00fcller", "Wolfgang Seeker", "Zsolt Sz\u00e1nt\u00f3"], "venue": null, "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2014}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "EMNLP. pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Head-driven statistical models for natural language parsing", "author": ["Michael Collins."], "venue": "Computational linguistics 29(4):589\u2013637.", "citeRegEx": "Collins.,? 2003", "shortCiteRegEx": "Collins.", "year": 2003}, {"title": "Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles", "author": ["James Cross", "Liang Huang."], "venue": "EMNLP.", "citeRegEx": "Cross and Huang.,? 2016", "shortCiteRegEx": "Cross and Huang.", "year": 2016}, {"title": "Deep biaffine attention for neural dependency parsing", "author": ["Timothy Dozat", "Christopher D. Manning."], "venue": "CoRR abs/1611.01734. http://arxiv.org/abs/1611.01734.", "citeRegEx": "Dozat and Manning.,? 2016", "shortCiteRegEx": "Dozat and Manning.", "year": 2016}, {"title": "Neural crf parsing", "author": ["Greg Durrett", "Dan Klein."], "venue": "arXiv preprint arXiv:1507.03641 .", "citeRegEx": "Durrett and Klein.,? 2015", "shortCiteRegEx": "Durrett and Klein.", "year": 2015}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A Smith."], "venue": "arXiv preprint arXiv:1602.07776 .", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Efficient, feature-based, conditional random field parsing", "author": ["Jenny Rose Finkel", "Alex Kleeman", "Christopher D Manning."], "venue": "ACL. volume 46, pages 959\u2013967.", "citeRegEx": "Finkel et al\\.,? 2008", "shortCiteRegEx": "Finkel et al\\.", "year": 2008}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Training deterministic parsers with non-deterministic oracles", "author": ["Yoav Goldberg", "Joakim Nivre."], "venue": "Transactions of the association for Computational Linguistics 1:403\u2013414.", "citeRegEx": "Goldberg and Nivre.,? 2013", "shortCiteRegEx": "Goldberg and Nivre.", "year": 2013}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv preprint arXiv:1308.0850 .", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Less grammar, more features", "author": ["David Leo Wright Hall", "Greg Durrett", "Dan Klein."], "venue": "ACL (1). pages 228\u2013237.", "citeRegEx": "Hall et al\\.,? 2014", "shortCiteRegEx": "Hall et al\\.", "year": 2014}, {"title": "Discriminative training of a neural network statistical parser", "author": ["James Henderson."], "venue": "Proceedings of the 42nd AnnualMeeting on Association for Computational Linguistics. Association for Computational Linguistics, page 95.", "citeRegEx": "Henderson.,? 2004", "shortCiteRegEx": "Henderson.", "year": 2004}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6980. http://arxiv.org/abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Simple and accurate dependency parsing using bidirectional lstm feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "arXiv preprint arXiv:1603.04351 .", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D Manning."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. pages 423\u2013430.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Shift-reduce constituent parsing with neural lookahead features", "author": ["Jiangming Liu", "Yue Zhang"], "venue": null, "citeRegEx": "Liu and Zhang.,? \\Q2016\\E", "shortCiteRegEx": "Liu and Zhang.", "year": 2016}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Comput. Linguist. 19(2):313\u2013330. http://dl.acm.org/citation.cfm?id=972470.972475.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Dynet: The dynamic neural network toolkit", "author": ["Lingpeng Kong", "Adhiguna Kuncoro", "Gaurav Kumar", "Chaitanya Malaviya", "Paul Michel", "Yusuke Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin."], "venue": "arXiv preprint", "citeRegEx": "Kong et al\\.,? 2017", "shortCiteRegEx": "Kong et al\\.", "year": 2017}, {"title": "Improved inference for unlexicalized parsing", "author": ["Slav Petrov", "Dan Klein."], "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics. Assocation for Computational", "citeRegEx": "Petrov and Klein.,? 2007", "shortCiteRegEx": "Petrov and Klein.", "year": 2007}, {"title": "Introducing the spmrl 2014 shared task on parsing morphologically-rich languages", "author": ["Djam\u00e9 Seddah", "Sandra K\u00fcbler", "Reut Tsarfaty."], "venue": "Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and", "citeRegEx": "Seddah et al\\.,? 2014", "shortCiteRegEx": "Seddah et al\\.", "year": 2014}, {"title": "Learning structured prediction models: A large margin approach", "author": ["Ben Taskar", "Vassil Chatalbashev", "Daphne Koller", "Carlos Guestrin."], "venue": "Proceedings of the 22Nd International Conference on Machine Learning. ACM,", "citeRegEx": "Taskar et al\\.,? 2005", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "Optimal shift-reduce constituent parsing with structured perceptron", "author": ["Le Quang Thang", "Hiroshi Noji", "Yusuke Miyao."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. volume 1, pages 1534\u20131544.", "citeRegEx": "Thang et al\\.,? 2015", "shortCiteRegEx": "Thang et al\\.", "year": 2015}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the As-", "citeRegEx": "Toutanova et al\\.,? 2003", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems. pages 2773\u20132781.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "A unified tagging solution: Bidirectional LSTM recurrent neural network with word embedding", "author": ["Peilu Wang", "Yao Qian", "Frank K. Soong", "Lei He", "Hai Zhao."], "venue": "CoRR abs/1511.00215. http://arxiv.org/abs/1511.00215.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Graph-based dependency parsing with bidirectional LSTM", "author": ["Wenhui Wang", "Baobao Chang."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016,", "citeRegEx": "Wang and Chang.,? 2016", "shortCiteRegEx": "Wang and Chang.", "year": 2016}, {"title": "Sequence-to-sequence learning as beam-search optimization", "author": ["Sam Wiseman", "Alexander M Rush."], "venue": "arXiv preprint arXiv:1606.02960 .", "citeRegEx": "Wiseman and Rush.,? 2016", "shortCiteRegEx": "Wiseman and Rush.", "year": 2016}], "referenceMentions": [{"referenceID": 26, "context": "Recent years have seen a great deal of interest in parsing architectures that make use of recurrent neural network (RNN) representations of input sentences (Vinyals et al., 2015).", "startOffset": 156, "endOffset": 178}, {"referenceID": 12, "context": "Despite evidence that linear RNN decoders are implicitly able to respect some nontrivial well-formedness constraints on structured outputs (Graves, 2013), researchers have consistently found that the best performance is achieved by systems that explicitly require the decoder to generate well-formed tree structures (Chen and Manning, 2014).", "startOffset": 139, "endOffset": 153}, {"referenceID": 3, "context": "Despite evidence that linear RNN decoders are implicitly able to respect some nontrivial well-formedness constraints on structured outputs (Graves, 2013), researchers have consistently found that the best performance is achieved by systems that explicitly require the decoder to generate well-formed tree structures (Chen and Manning, 2014).", "startOffset": 316, "endOffset": 340}, {"referenceID": 24, "context": "However, transition-based models do not admit fast dynamic programs and require careful feature engineering to support exact search-based inference (Thang et al., 2015).", "startOffset": 148, "endOffset": 168}, {"referenceID": 29, "context": "Moreover, models with recurrent state require complex training procedures to benefit from anything other than greedy decoding (Wiseman and Rush, 2016).", "startOffset": 126, "endOffset": 150}, {"referenceID": 9, "context": "An alternative line of work focuses on chart parsers, which use log-linear or neural scoring potentials to parameterize a tree-structured dynamic program for maximization or marginalization (Finkel et al., 2008; Durrett and Klein, 2015).", "startOffset": 190, "endOffset": 236}, {"referenceID": 7, "context": "An alternative line of work focuses on chart parsers, which use log-linear or neural scoring potentials to parameterize a tree-structured dynamic program for maximization or marginalization (Finkel et al., 2008; Durrett and Klein, 2015).", "startOffset": 190, "endOffset": 236}, {"referenceID": 13, "context": "pre-specification of a complete context-free grammar for generating output structures and initial pruning of the output space with a weaker model (Hall et al., 2014).", "startOffset": 146, "endOffset": 165}, {"referenceID": 5, "context": "On the Penn Treebank, our approach outperforms a number of recent models for chart-based and transition-based parsing\u2014including the state-ofthe-art models of Cross and Huang (2016) and Liu and Zhang (2016)\u2014achieving an F1 score of 91.", "startOffset": 158, "endOffset": 181}, {"referenceID": 5, "context": "On the Penn Treebank, our approach outperforms a number of recent models for chart-based and transition-based parsing\u2014including the state-ofthe-art models of Cross and Huang (2016) and Liu and Zhang (2016)\u2014achieving an F1 score of 91.", "startOffset": 158, "endOffset": 206}, {"referenceID": 0, "context": "Given that a span\u2019s correct label and its quality as a constituent depend heavily on the context in which it appears, we naturally turn to recurrent neural networks as a starting point, since they have previously been shown to capture contextual information suitable for use in a variety of natural language applications (Bahdanau et al., 2014; Wang et al., 2015) In particular, we run a bidirectional LSTM over the input to obtain context-sensitive forward and backward encodings for each position i, denoted by fi and bi, respectively.", "startOffset": 321, "endOffset": 363}, {"referenceID": 27, "context": "Given that a span\u2019s correct label and its quality as a constituent depend heavily on the context in which it appears, we naturally turn to recurrent neural networks as a starting point, since they have previously been shown to capture contextual information suitable for use in a variety of natural language applications (Bahdanau et al., 2014; Wang et al., 2015) In particular, we run a bidirectional LSTM over the input to obtain context-sensitive forward and backward encodings for each position i, denoted by fi and bi, respectively.", "startOffset": 321, "endOffset": 363}, {"referenceID": 0, "context": "Given that a span\u2019s correct label and its quality as a constituent depend heavily on the context in which it appears, we naturally turn to recurrent neural networks as a starting point, since they have previously been shown to capture contextual information suitable for use in a variety of natural language applications (Bahdanau et al., 2014; Wang et al., 2015) In particular, we run a bidirectional LSTM over the input to obtain context-sensitive forward and backward encodings for each position i, denoted by fi and bi, respectively. Our representation of the span (i, j) is then the concatenatation the vector differences fj \u2212 fi and bi \u2212 bj . This corresponds to a bidirectional version of the LSTMMinus features first proposed by Wang and Chang (2016). On top of this base, our label and span scoring functions are implemented as one-layer feedforward networks, taking as input the concatenated span difference and producing as output either a vector of label scores or a single span score.", "startOffset": 322, "endOffset": 759}, {"referenceID": 5, "context": "Our model shares several features in common with that of Cross and Huang (2016). In particular, our representation of spans and the form of our label scoring function were directly inspired by their work, as were our handling of unary chains and our use of an empty label.", "startOffset": 57, "endOffset": 80}, {"referenceID": 23, "context": "1 to support loss-augmented decoding (Taskar et al., 2005).", "startOffset": 37, "endOffset": 58}, {"referenceID": 5, "context": "Cross and Huang (2016) propose such an oracle for a related transition-based parsing system, and prove its optimality for the F1 metric on labeled spans.", "startOffset": 0, "endOffset": 23}, {"referenceID": 5, "context": "The proof of correctness is similar to the proof in Cross and Huang (2016); we refer to the Dynamic Oracle section in their paper for a more detailed discussion.", "startOffset": 52, "endOffset": 75}, {"referenceID": 5, "context": "This is similar to the structural scoring function used by Cross and Huang (2016), although whereas they additionally include features for the outside spans (0, i) and (j, n) in their concatenation, we omit these from our implementation, finding that they do not improve performance.", "startOffset": 59, "endOffset": 82}, {"referenceID": 6, "context": "Inspired by the success of deep biaffine scoring in recent work by Dozat and Manning (2016) for dependency parsing, we also consider a split scoring function of a similar form for our model.", "startOffset": 67, "endOffset": 92}, {"referenceID": 19, "context": "We use the Penn Treebank (Marcus et al., 1993) for our English experiments, with standard splits of sections 2-21 for training, section 22 for development, and section 23 for testing.", "startOffset": 25, "endOffset": 46}, {"referenceID": 22, "context": "We use the French Treebank from the SPMRL 2014 shared task (Seddah et al., 2014) with its provided splits for our French experiments.", "startOffset": 59, "endOffset": 80}, {"referenceID": 25, "context": "We use automatically predicted tags for training and testing, obtaining predicted part-ofspeech tags for the Penn Treebank using the Stanford tagger (Toutanova et al., 2003) with 10-way jackknifing, and using the provided predicted partof-speech and morphological tags for the French Treebank.", "startOffset": 149, "endOffset": 173}, {"referenceID": 10, "context": "All parameters (including word and tag embeddings) are randomly initialized using Glorot initialization (Glorot and Bengio, 2010), and are tuned on development set performance.", "startOffset": 104, "endOffset": 129}, {"referenceID": 15, "context": "We use the Adam optimizer (Kingma and Ba, 2014) with its default settings for optimization, with a batch size of 10.", "startOffset": 26, "endOffset": 47}, {"referenceID": 18, "context": "7 reported by Liu and Zhang (2016), demonstrating that our simple neural parsing system is already capable of achieving strong results.", "startOffset": 14, "endOffset": 35}, {"referenceID": 5, "context": "8 Cross and Huang (2016) 90.", "startOffset": 2, "endOffset": 25}, {"referenceID": 5, "context": "8 Cross and Huang (2016) 90.5 92.1 91.3 Liu and Zhang (2016) 91.", "startOffset": 2, "endOffset": 61}, {"referenceID": 5, "context": "25 Cross and Huang (2016) 81.", "startOffset": 3, "endOffset": 26}, {"referenceID": 5, "context": "Although we fall short of the scores obtained by Cross and Huang (2016), we achieve competitive performance relative to the neural CRF parser of Durrett and Klein (2015).", "startOffset": 49, "endOffset": 72}, {"referenceID": 5, "context": "Although we fall short of the scores obtained by Cross and Huang (2016), we achieve competitive performance relative to the neural CRF parser of Durrett and Klein (2015). 7 Related Work", "startOffset": 49, "endOffset": 170}, {"referenceID": 4, "context": "Many early successful approaches to constituency parsing focused on rich modeling of correlations in the output space, typically by engineering proabilistic context-free grammars with state spaces enriched to capture long-distance dependencies and lexical phenomena (Collins, 2003; Klein and Manning, 2003; Petrov and Klein, 2007).", "startOffset": 266, "endOffset": 330}, {"referenceID": 17, "context": "Many early successful approaches to constituency parsing focused on rich modeling of correlations in the output space, typically by engineering proabilistic context-free grammars with state spaces enriched to capture long-distance dependencies and lexical phenomena (Collins, 2003; Klein and Manning, 2003; Petrov and Klein, 2007).", "startOffset": 266, "endOffset": 330}, {"referenceID": 21, "context": "Many early successful approaches to constituency parsing focused on rich modeling of correlations in the output space, typically by engineering proabilistic context-free grammars with state spaces enriched to capture long-distance dependencies and lexical phenomena (Collins, 2003; Klein and Manning, 2003; Petrov and Klein, 2007).", "startOffset": 266, "endOffset": 330}, {"referenceID": 3, "context": "As noted in the introduction, this class of feature-based tree scoring functions can be implemented with either a linear transition system (Chen and Manning, 2014) or a global decoder (Finkel et al.", "startOffset": 139, "endOffset": 163}, {"referenceID": 9, "context": "As noted in the introduction, this class of feature-based tree scoring functions can be implemented with either a linear transition system (Chen and Manning, 2014) or a global decoder (Finkel et al., 2008).", "startOffset": 184, "endOffset": 205}, {"referenceID": 13, "context": "Kiperwasser and Goldberg (2016) describe an approach closely related to ours but targeted at dependency formalisms, and which easily accommodates both sparse log-linear scoring models (Hall et al., 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al.", "startOffset": 184, "endOffset": 203}, {"referenceID": 14, "context": ", 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al., 2016).", "startOffset": 35, "endOffset": 78}, {"referenceID": 1, "context": ", 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al., 2016).", "startOffset": 35, "endOffset": 78}, {"referenceID": 16, "context": "the recurrent representation of spans (Kiperwasser and Goldberg, 2016), and the use of a dynamic oracle and exploration policy during training (Goldberg and Nivre, 2013)) and extends these insights to span-oriented models, which support a wider range of decoding procedures.", "startOffset": 38, "endOffset": 70}, {"referenceID": 11, "context": "the recurrent representation of spans (Kiperwasser and Goldberg, 2016), and the use of a dynamic oracle and exploration policy during training (Goldberg and Nivre, 2013)) and extends these insights to span-oriented models, which support a wider range of decoding procedures.", "startOffset": 143, "endOffset": 169}, {"referenceID": 2, "context": "As noted in the introduction, this class of feature-based tree scoring functions can be implemented with either a linear transition system (Chen and Manning, 2014) or a global decoder (Finkel et al., 2008). Kiperwasser and Goldberg (2016) describe an approach closely related to ours but targeted at dependency formalisms, and which easily accommodates both sparse log-linear scoring models (Hall et al.", "startOffset": 140, "endOffset": 239}, {"referenceID": 1, "context": ", 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al., 2016). The best-performing constituency parsers in the last two years have largely been transition-based rather than global; examples include the models of Dyer et al. (2016), Cross and Huang (2016) and Liu and Zhang (2016).", "startOffset": 53, "endOffset": 248}, {"referenceID": 1, "context": ", 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al., 2016). The best-performing constituency parsers in the last two years have largely been transition-based rather than global; examples include the models of Dyer et al. (2016), Cross and Huang (2016) and Liu and Zhang (2016).", "startOffset": 53, "endOffset": 272}, {"referenceID": 1, "context": ", 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al., 2016). The best-performing constituency parsers in the last two years have largely been transition-based rather than global; examples include the models of Dyer et al. (2016), Cross and Huang (2016) and Liu and Zhang (2016). The present work takes many of the insights developed in these models (e.", "startOffset": 53, "endOffset": 297}, {"referenceID": 1, "context": ", 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al., 2016). The best-performing constituency parsers in the last two years have largely been transition-based rather than global; examples include the models of Dyer et al. (2016), Cross and Huang (2016) and Liu and Zhang (2016). The present work takes many of the insights developed in these models (e.g. the recurrent representation of spans (Kiperwasser and Goldberg, 2016), and the use of a dynamic oracle and exploration policy during training (Goldberg and Nivre, 2013)) and extends these insights to span-oriented models, which support a wider range of decoding procedures. Our approach differs from other recent chart-based neural models (e.g. Durrett and Klein (2015)) in the use of a recurrent input representation, structured loss function, and comparatively simple parameterization of the scoring function.", "startOffset": 53, "endOffset": 745}], "year": 2017, "abstractText": "In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).", "creator": "LaTeX with hyperref package"}}}