{"id": "1705.04146", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2017", "title": "Program Induction by Rationale Generation:Learning to Solve and Explain Algebraic Word Problems", "abstract": "Solving algebraic word problems requires executing a series of arithmetic operations---a program---to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.", "histories": [["v1", "Thu, 11 May 2017 13:04:47 GMT  (1699kb,D)", "http://arxiv.org/abs/1705.04146v1", null], ["v2", "Tue, 10 Oct 2017 14:57:26 GMT  (1699kb,D)", "http://arxiv.org/abs/1705.04146v2", null], ["v3", "Mon, 23 Oct 2017 16:45:03 GMT  (1699kb,D)", "http://arxiv.org/abs/1705.04146v3", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG", "authors": ["wang ling", "dani yogatama", "chris dyer", "phil blunsom"], "accepted": true, "id": "1705.04146"}, "pdf": {"name": "1705.04146.pdf", "metadata": {"source": "CRF", "title": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems", "authors": ["Wang Ling", "Dani Yogatama", "Chris Dyer", "Phil Blunsom"], "emails": ["lingwang@google.com", "dyogatama@google.com", "cdyer@google.com", "pblunsom@google.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "2 Dataset", "text": "We created a dataset of 100,000 problems with the annotations shown in Figure 1. Each question is divided into four parts, two inputs and two outputs: the description of the problem that we will call a question, and the possible (multiple-choice) response options that are called options. Our goal is to generate the description of the rationality used to achieve the correct answer, which is called rationality and the correct option designation. Problem 1 illustrates an example of an algebra problem that needs to be translated into an expression (i.e. (27x + 17y) / (x + y) = 23) and then the desired amount (x / y) for which to be solved. Problem 2 is an example that could be solved by multi-level arithmetic operations proposed in (Roy and Roth, 2015). Finally, problem 3 describes a problem that is solved by testing each of the options that were not addressed in the past."}, {"heading": "2.1 Construction", "text": "First, we collect a set of 34,202 seed problems, which consist of multiple option problems covering a wide range of topics and difficulty levels. Examples of tests with such problems are the GMAT (Graduate Management Admission Test) and the GRE (General Test). Many websites contain sample mathematical questions in such tests, where the answer is supported by rationality.Next, we turned to crowdsourcing to generate new questions.We create a task in which users receive a set of 5 questions from our seed datasets. Then we ask the Turker to select one of the questions and write a similar question.We also force the answers and rationalities to deviate from the original question to avoid rewriting the original question. Once again, we manually check a subset of tasks for each Turker for quality control. The type of questions generated with this method varies. Some Turkers suggest small changes in the Wa questions (e.g., the problems before a valid problem is changed to a 3)."}, {"heading": "2.2 Statistics", "text": "The descriptive statistics of the data set are shown in Figure 1. In total, we collected 104,519 problems (34,202 seed problems and 70,318 crowdsourcing problems); we removed 500 problems as a heldout set (250 for development and 250 for testing); since replicas of the heldout problems may be present in the training set, these were removed manually by listing the closest problems in the training set in relation to the character-based Levenstein distance for each heldout instance; after filtering, 100,949 problems remained in the training set; we also show the average number of tokens (total number of tokens in question, options and reasoning) and the vocabulary size of the questions and reasoning; and finally, we provide the same statistics exclusively for tokens that are numerical values and tokens that are not. Figure 2 shows the distribution of examples based on the total number of tokens."}, {"heading": "3 Model", "text": "Creating explanations for mathematical problems is a challenge, as it requires models that learn to perform mathematical operations with a finer grain, since each step must be explained within the solution. For example, in problem 1, the equation (27x + 17y) / (x + y) = 23 must be solved in order to obtain the answer. In previous work (Kushman et al., 2014), this could be done by feeding the equation into an expression solver to perform x / y = 3 / 2. However, this would skip the intermediate steps 27x + 17y = 23x + 23y and 4x = 6y, which also need to be generated in our problem. We propose a model that collectively learns to generate the text in rationality and perform the mathematical operations necessary to solve the problem by generating a program that contains both statements that produce results and statements that are simply generated by the following instructions."}, {"heading": "3.1 Problem Definition", "text": "In traditional sequence models (Sutskever et al., 2014; Bahdanau et al., 2014), the goal is to predict the output sequence y = y1,.., y | y | from the input sequence x = x1,..., x | x |, with the lengths y | and | x |. In our specific problem, we get the problem and the set of options, and we want to predict the rationality and the right option. We use x as a word sequence in the problem, concatenated with words in each of the options separated by a special tag. Note that knowledge of the possible options is required, as some problems are solved by the process of elimination or by testing each of the options (e.g. problem 3). We want to create y, which is the word sequence in rationality. We also add the correct option as the last word in y, which is interpreted as the selected option. & ltts. < < < < < < < < < < < < < < < < < < <"}, {"heading": "3.2 Generating Programs to Generate Rationales", "text": "We want to create a latent sequence of program statements, z = z1,.. z | z |, with the length | z | that y generate when executed. We express z as a program that can access x, y and the memory buffer. Upon completion of the execution, we expect the sequence of output tokens in the output vector y.Table 2 to be an example of a sequence of statements that would generate an dump from problem 2, where the columns x, z, v, and r denote the input sequence (program), the values of the statement execution, and where each value vi is written to generate the output or memory. In this example, statements from indexes 1 to 14 simply indicate each position with the observed output y1, y14, y14."}, {"heading": "3.3 Generating and Executing Instructions", "text": "In our model, the programs consist of sequences of statements, for example, we now turn to the question of how we define each value, depending on the text program specification and program history. The zi statement is a tuple consisting of an operation (oi), an ordered sequence of its arguments (ai), and a decision about where its results will be placed (ri) (will it be appended in output or in a memory buffer m?), and the result of applying the operation to its arguments (vi). That is, zi = (oi, ai, vi).Formally, oi is an element of the prespecified operations O, for example, which contains a series of operations O that include, div, Str to Float, etc. The number of arguments needed by oi is given by argc (oi), e.g. argc (add) = 2 and argc (log) = 1. The arguments are ai = ai, argc (oi)."}, {"heading": "4 Inducing Programs while Learning", "text": "In fact, it is in such a way that most of the people who are in a position are able to put themselves in the world, to put themselves in the world in which they live, in the world in which they live, in the world in which they live, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the, in the world, in the world, in the world, in the world, in the world, in the, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the, in the world, in the world, in the world, in the world, in the world, in the, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world,"}, {"heading": "5 Staged Back-propagation", "text": "As shown in Figure 2, mathematical basics with more than 200 characters are not uncommon, and with additional intermediate instructions, the size z can easily exceed 400. This poses a practical challenge for the formation of the model. For both the attention and copy mechanisms, for each zi statement, the model must calculate the probability distribution between all participating units c conditioned on the previous state hi \u2212 1. For the attention model and the input copy mechanisms c = x0, i \u2212 1, and for the output copy mechanism c = z. These operations generally involve an exponential number of matrix multiplications as the size of c and z grows. For example, during the compilation of probabilities for the input copy mechanisms in Equation 1, the affinity function f between the current context q and a given input uk is generally implemented."}, {"heading": "6 Experiments", "text": "We apply our model to the task of generating justifications for solving mathematical problems by evaluating it both in terms of the quality of the justifications and the model's ability to obtain correct answers."}, {"heading": "6.1 Baselines", "text": "As a starting point, we use the attention-based sequence to sequence the sequence model proposed by Bahdanau et al. (2014) and propose extensions that allow the model to copy from the input (Ling et al., 2016) and from the output (Merity et al., 2016)."}, {"heading": "6.2 Hyperparameters", "text": "We used a two-layer LSTM with a hidden size of H = 200 and word embeddings with a size of 200. The number of layers that graph G expands during sampling D is set to 5. Decoding is done with a beam of 200. In terms of Softmax vocabulary and embeddings, we retain the most common 20,000 word types and replace the remaining words with an unknown character. During the training, the model only learns to predict a word as an unknown character when there is no other alternative to generating the word."}, {"heading": "6.3 Evaluation Metrics", "text": "The rationalities are evaluated with average helplessness at the sentence level and BLEU4 (Papineni et al., 2002). If a model cannot generate a token to calculate helplessness, we predict an unknown token. This benefits the baselines because they are less expressive. Since the helplessness of our model depends on the latent program that is generated, we decipher our model to generate rationality, while maximizing the probability of the program. This corresponds to the method used to obtain sample programs described in Section 4, but we choose the most likely instructions for each timestamp instead of sampling. Finally, the correctness of the answer is evaluated by calculating the percentage of questions where the chosen option matches the correct one."}, {"heading": "6.4 Results", "text": "The aforementioned results, looking for a solution to the problem, are usually able to find a solution to the problem that has occurred in the past. \"The problem is that it has occurred in the past,\" he says. \"The problem is that it is a problem that has occurred time and again in the past.\""}, {"heading": "6.5 Discussion.", "text": "Although we show that our model can outperform the models created to date, the generation of complex justifications as correctly presented in Figure 1 is still an unsolved problem, as each additional step makes the problem more complex in both inference and decoding. Nevertheless, this is the first result to show that it is possible to solve mathematical problems this way, and we believe that this modeling approach and this dataset will drive work on this problem."}, {"heading": "7 Related Work", "text": "Extensive efforts have been made in the field of mathematical problem solving (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015) aimed at finding the right answer to a given mathematical problem. Other work has focused on learning to translate mathematical expressions into formal languages (Roy et al., 2016), but our approach is strongly linked to work on sequence transduction using the encoder decoder paradigm (Sutskever et al., 2014; Bahdanau et al., 2014; Kalchbrenner and Blunsom, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al, 2012; Berant et al al al al, 2016 Lik al, 2013; Andreas et al, 2015)."}, {"heading": "8 Conclusion", "text": "In this thesis we dealt with the problem of generating justifications for mathematical problems, where the task is not only to obtain the correct answer to the problem, but also to provide a description of the method for solving the problem. To this end, we collect 100,000 pairs of questions and justifications and propose a model that can be used to generate natural language and perform calculations in the same decoding process. Experiments show that our method surpasses existing neural models both in terms of the fluidity of the justifications generated and in terms of the ability to solve the problem."}], "references": [{"title": "Semantic parsing as machine translation", "author": ["Jacob Andreas", "Andreas Vlachos", "Stephen Clark."], "venue": "Proc. of ACL.", "citeRegEx": "Andreas et al\\.,? 2013", "shortCiteRegEx": "Andreas et al\\.", "year": 2013}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."], "venue": "Proc. of EMNLP.", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Hybrid computing using a neural network with dynamic external memory", "author": ["Yori Zwols", "Georg Ostrovski", "Adam Cain", "Helen King", "Christopher Summerfield", "Phil Blunsom", "Koray Kavukcuoglu", "Demis Hassabis."], "venue": "Nature 538(7626):471\u2013", "citeRegEx": "Zwols et al\\.,? 2016", "shortCiteRegEx": "Zwols et al\\.", "year": 2016}, {"title": "Rationalization: A neural machine translation approach to generating natural language explanations", "author": ["Brent Harrison", "Upol Ehsan", "Mark O. Riedl."], "venue": "CoRR abs/1702.07826.", "citeRegEx": "Harrison et al\\.,? 2017", "shortCiteRegEx": "Harrison et al\\.", "year": 2017}, {"title": "Generating visual explanations", "author": ["Lisa Anne Hendricks", "Zeynep Akata", "Marcus Rohrbach", "Jeff Donahue", "Bernt Schiele", "Trevor Darrell."], "venue": "Proc. ECCV .", "citeRegEx": "Hendricks et al\\.,? 2016", "shortCiteRegEx": "Hendricks et al\\.", "year": 2016}, {"title": "Learning to solve arithmetic word problems with verb categorization", "author": ["Mohammad Javad Hosseini", "Hannaneh Hajishirzi", "Oren Etzioni", "Nate Kushman."], "venue": "Proc. of EMNLP.", "citeRegEx": "Hosseini et al\\.,? 2014", "shortCiteRegEx": "Hosseini et al\\.", "year": 2014}, {"title": "Semantic parsing with bayesian tree transducers", "author": ["Bevan Keeley Jones", "Mark Johnson", "Sharon Goldwater."], "venue": "Proc. of ACL.", "citeRegEx": "Jones et al\\.,? 2012", "shortCiteRegEx": "Jones et al\\.", "year": 2012}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proc. of EMNLP.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Learning to automatically solve algebra word problems", "author": ["Nate Kushman", "Yoav Artzi", "Luke Zettlemoyer", "Regina Barzilay."], "venue": "Proc. of ACL.", "citeRegEx": "Kushman et al\\.,? 2014", "shortCiteRegEx": "Kushman et al\\.", "year": 2014}, {"title": "Rationalizing neural predictions", "author": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proc. of EMNLP.", "citeRegEx": "Lei et al\\.,? 2016", "shortCiteRegEx": "Lei et al\\.", "year": 2016}, {"title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision", "author": ["Chen Liang", "Jonathan Berant", "Quoc Le", "Kenneth D. Forbus", "Ni Lao."], "venue": "arXiv 1611.00020.", "citeRegEx": "Liang et al\\.,? 2016", "shortCiteRegEx": "Liang et al\\.", "year": 2016}, {"title": "Latent predictor networks for code generation", "author": ["Wang Ling", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Andrew Senior", "Fumin Wang", "Phil Blunsom."], "venue": "Proc. of ACL.", "citeRegEx": "Ling et al\\.,? 2016", "shortCiteRegEx": "Ling et al\\.", "year": 2016}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher."], "venue": "arXiv 1609.07843.", "citeRegEx": "Merity et al\\.,? 2016", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V. Le", "Ilya Sutskever."], "venue": "Proc. ICLR.", "citeRegEx": "Neelakantan et al\\.,? 2016", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proc. of ACL.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Language to code: Learning semantic parsers for if-this-then-that recipes", "author": ["Chris Quirk", "Raymond Mooney", "Michel Galley."], "venue": "Proc. of ACL.", "citeRegEx": "Quirk et al\\.,? 2015", "shortCiteRegEx": "Quirk et al\\.", "year": 2015}, {"title": "Neural programmer-interpreters", "author": ["Scott E. Reed", "Nando de Freitas."], "venue": "Proc. of ICLR.", "citeRegEx": "Reed and Freitas.,? 2016", "shortCiteRegEx": "Reed and Freitas.", "year": 2016}, {"title": "Solving general arithmetic word problems", "author": ["Subhro Roy", "Dan Roth."], "venue": "Proc. of EMNLP.", "citeRegEx": "Roy and Roth.,? 2015", "shortCiteRegEx": "Roy and Roth.", "year": 2015}, {"title": "Equation parsing: Mapping sentences to grounded equations", "author": ["Subhro Roy", "Shyam Upadhyay", "Dan Roth."], "venue": "Proc. of EMNLP.", "citeRegEx": "Roy et al\\.,? 2016", "shortCiteRegEx": "Roy et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "arXiv 1409.3215.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Proc. of NIPS.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "As a proxy for the richness of the real world, a series of papers have used natural language specifications of algebraic word problems, and solved these by either learning to fill in templates that can be solved with equation solvers (Hosseini et al., 2014; Kushman et al., 2014) or inferring and modeling operation sequences (programs) that lead to the final an-", "startOffset": 234, "endOffset": 279}, {"referenceID": 8, "context": "As a proxy for the richness of the real world, a series of papers have used natural language specifications of algebraic word problems, and solved these by either learning to fill in templates that can be solved with equation solvers (Hosseini et al., 2014; Kushman et al., 2014) or inferring and modeling operation sequences (programs) that lead to the final an-", "startOffset": 234, "endOffset": 279}, {"referenceID": 17, "context": "swer (Roy and Roth, 2015).", "startOffset": 5, "endOffset": 25}, {"referenceID": 4, "context": "This work is thus related to models that can explain or rationalize their decisions (Hendricks et al., 2016; Harrison et al., 2017).", "startOffset": 84, "endOffset": 131}, {"referenceID": 3, "context": "This work is thus related to models that can explain or rationalize their decisions (Hendricks et al., 2016; Harrison et al., 2017).", "startOffset": 84, "endOffset": 131}, {"referenceID": 17, "context": "Problem 2 is an example that could be solved by multi-step arithmetic operations proposed in (Roy and Roth, 2015).", "startOffset": 93, "endOffset": 113}, {"referenceID": 8, "context": "In previous work (Kushman et al., 2014), this could be done by feeding the equation into an expression solver to obtain x/y = 3/2.", "startOffset": 17, "endOffset": 39}, {"referenceID": 11, "context": "This decision is modeled using a latent predictor network (Ling et al., 2016), where the control over which method used to generate ai,j is governed by a latent variable qi,j \u2208 {SOFTMAX, COPY-INPUT, COPY-OUTPUT}.", "startOffset": 58, "endOffset": 77}, {"referenceID": 20, "context": "\u2022 If qi,j = COPY-INPUT, ai,j is obtained by copying an element from the input vector with a pointer network (Vinyals et al., 2015) over input words x1, .", "startOffset": 108, "endOffset": 130}, {"referenceID": 10, "context": "This is analogous to the question answering work in Liang et al. (2016), where the query that", "startOffset": 52, "endOffset": 72}, {"referenceID": 17, "context": "In Roy and Roth (2015) this problem is also addressed by adding prior knowledge to constrain the exponential space.", "startOffset": 3, "endOffset": 23}, {"referenceID": 19, "context": "trast, the sequence-to-sequence model proposed in (Sutskever et al., 2014), does not suffer from these issues as each timestamp is dependent only on the previous state hi\u22121.", "startOffset": 50, "endOffset": 74}, {"referenceID": 11, "context": "(2014), and proposed augmentations, allowing it to copy from the input (Ling et al., 2016) and from the output (Merity et al.", "startOffset": 71, "endOffset": 90}, {"referenceID": 12, "context": ", 2016) and from the output (Merity et al., 2016).", "startOffset": 28, "endOffset": 49}, {"referenceID": 14, "context": "The evaluation of the rationales is performed with average sentence level perplexity and BLEU4 (Papineni et al., 2002).", "startOffset": 95, "endOffset": 118}, {"referenceID": 5, "context": "Extensive efforts have been made in the domain of math problem solving (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015), which aim at obtaining the correct answer to a given math", "startOffset": 71, "endOffset": 136}, {"referenceID": 8, "context": "Extensive efforts have been made in the domain of math problem solving (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015), which aim at obtaining the correct answer to a given math", "startOffset": 71, "endOffset": 136}, {"referenceID": 17, "context": "Extensive efforts have been made in the domain of math problem solving (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015), which aim at obtaining the correct answer to a given math", "startOffset": 71, "endOffset": 136}, {"referenceID": 18, "context": "Other work has focused on learning to map math expressions into formal languages (Roy et al., 2016).", "startOffset": 81, "endOffset": 99}, {"referenceID": 6, "context": "som, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al.", "startOffset": 81, "endOffset": 210}, {"referenceID": 1, "context": "som, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al.", "startOffset": 81, "endOffset": 210}, {"referenceID": 0, "context": "som, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al.", "startOffset": 81, "endOffset": 210}, {"referenceID": 15, "context": "som, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al.", "startOffset": 81, "endOffset": 210}, {"referenceID": 10, "context": "som, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al.", "startOffset": 81, "endOffset": 210}, {"referenceID": 13, "context": "som, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al.", "startOffset": 81, "endOffset": 210}, {"referenceID": 8, "context": "Lei et al. (2016), jointly modeled both a classification decision, and the selection of the most relevant subsection of a document for making the classification decision.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "Hendricks et al. (2016) generate textual explanations for visual classification problems, but in contrast to our model, they first generate an answer, and then, conditional on the answer, generate an explanation.", "startOffset": 0, "endOffset": 24}], "year": 2017, "abstractText": "Solving algebraic word problems requires executing a series of arithmetic operations\u2014a program\u2014to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.", "creator": "LaTeX with hyperref package"}}}