{"id": "1705.04925", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2017", "title": "Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization", "abstract": "In many modern machine learning applications, structures of underlying mathematical models often yield nonconvex optimization problems. Due to the intractability of nonconvexity, there is a rising need to develop efficient methods for solving general nonconvex problems with certain performance guarantee. In this work, we investigate the accelerated proximal gradient method for nonconvex programming (APGnc). The method compares between a usual proximal gradient step and a linear extrapolation step, and accepts the one that has a lower function value to achieve a monotonic decrease. In specific, under a general nonsmooth and nonconvex setting, we provide a rigorous argument to show that the limit points of the sequence generated by APGnc are critical points of the objective function. Then, by exploiting the Kurdyka-{\\L}ojasiewicz (\\KL) property for a broad class of functions, we establish the linear and sub-linear convergence rates of the function value sequence generated by APGnc. We further propose a stochastic variance reduced APGnc (SVRG-APGnc), and establish its linear convergence under a special case of the \\KL property. We also extend the analysis to the inexact version of these methods and develop an adaptive momentum strategy that improves the numerical performance.", "histories": [["v1", "Sun, 14 May 2017 07:22:20 GMT  (107kb,D)", "http://arxiv.org/abs/1705.04925v1", "Accepted in ICML 2017, 9 papes, 4 figures"]], "COMMENTS": "Accepted in ICML 2017, 9 papes, 4 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qunwei li", "yi zhou", "yingbin liang", "pramod k varshney"], "accepted": true, "id": "1705.04925"}, "pdf": {"name": "1705.04925.pdf", "metadata": {"source": "META", "title": "Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization", "authors": ["Qunwei Li", "Yi Zhou", "Yingbin Liang", "Pramod K. Varshney"], "emails": ["<qli33@syr.edu>."], "sections": [{"heading": "1. Introduction", "text": "Many problems in machine learning, Data Mining, and Signal Processing (x) (x) (x). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c).). (c). (c). (c). (c).). (c). (c). (c).). (c). (c). (c). (c).). (c). (c).). (c). (c). (c). (c).). (c).). (c). (c). (c).). (c). (c). (c). (c). (c).). (c). (c).).). (c). (c). (c).).). (c). (c).). (c).). (c.). (c).).). (c). (.).)."}, {"heading": "1.1. Main Contributions", "text": "This paper presents the convergence analysis of the APGNC algorithms for the nonconvex problems of the KL system as well as the imprecise situation. We also examine the stochastic variance of the APGNC algorithm and its imprecise situation. Our contributions are summarized as a consequential error. (P) We show that the boundary points of the sequences generated by APGNC are critical points of the objective function and the stochastic variance is reduced. (P)"}, {"heading": "1.2. Comparison to Related Work", "text": "The original accelerated gradient method for minimizing a single smooth convex function originates from (Nesterov, 1983) and is expanded as APG into the composite minimization framework in (Beck & Teboulle, 2009b; Tseng, 2010) While these APG variants generate a sequence of function values that can oscillate, (Beck & Teboulle, 2009a) another variant of APG is proposed that generates a non-increasing sequence of function values. Then, (Li & Lin, 2015) another sequence of function values is generated."}, {"heading": "2. Preliminaries and Assumptions", "text": "In this section, we first present some technical definitions that will be useful later, and then describe the assumptions about the problem (P) that we are looking at in this section. (P) The assumptions about the problem (P) that we are looking at in this section are as follows. (P) The assumptions about the problem (P) that we are looking at in this section are: (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P, (P), (P), (P), (P, P, (P), (P), (P), (P, (P), (P), (P), (P, (P), (P), (P, (P), (P), (P, (), (P, (P), (P, P, (, P, P), (, P, P, P, P (, P, P), (), (P, (P, P, P, P, P), (P, P, P, (P, P, P, P, P, P, P, P, P), (P, P, P (P, P, P, P, P, P, P, P, P, P, P, P), (P, P, P, P (P, P, P, P, P, P), (P, P, P, P, P (P, P, P, P), (P, P, P, P, P, P, P, P, P, P, P, P), (P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P"}, {"heading": "3. Main Results", "text": "In this section we provide our main results for convergence analysis of APGnc and SVRG-APGnc as well as imprecise variants of these algorithms. All proofs of the theorems are contained in supplementary materials."}, {"heading": "3.1. Convergence Analysis", "text": "In this sub-section, we characterize the convergence of APGnc. Our first result characterizes the behavior of the boundary points generated by APGnc.Theorem. (Let us assume that the order of the order 1, 2, 3, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8"}, {"heading": "3.2. APGnc with Adaptive Momentum", "text": "The original APGnc sets the impulse parameter \u03b2k = k + 3, which theoretically can only be justified for convex problems. At this point, we propose an alternative choice of the impulse step size, which is more intuitive for non-convex problems, and point to the resulting algorithm as APGnc + (see algorithm 4).The idea is to increase the impulse \u03b2 to further exploit the possibility of acceleration when the extrapolation step vk reaches a lower functional value. Since the proofs of theorem 1 and theorem 2 do not depend on the exact value of the impulse step size, APGnc and APGnc + have the same convergence rate at order level. However, in Section 4, we show that APGnc + is improving numerically compared to APGnc."}, {"heading": "3.3. Inexact APGnc", "text": "We are not going to be able to make a decision on whether or not we are going to make a decision on whether or not we are going to make a decision on whether or not we are going to make a decision on whether or not we are going to make a decision on whether or not we are going to make a decision on whether or not we are going to make a decision on whether or not we are going to make a decision on whether or not to make a decision on whether or not we are going to make a decision on whether or not to make a decision on whether or not to make a decision on whether or not to make a decision on whether or not to make a decision on whether or not to make a decision on whether or not to make a decision on whether or not to make a decision on whether or not to make a decision on whether or not to make a decision on whether or not to make a decision on whether or not to make a decision."}, {"heading": "3.4. Stochastic Variance Reduced APGnc", "text": "In this subsection, we examine the reduced APGnc stochastic variance algorithm, known as SVRG-APGnc. The most important steps are outlined in Algorithm 5. \u2212 The main difference of APGnc is that the single proximal gradient step is replaced by a loop of stochastic proximal gradient steps, the reduced variance gradients.Due to the stochastic nature of the algorithm, the iteration sequence may not remain stable in the local K L region, and therefore the standard K L approach fails. We will then focus on analyzing the specific but important case of the global K L property at 270. \u2212 In fact, if g = 0, the K L property is reduced in such a case to the well-known polyak Lojasiewicz (PL) property, which we examined in (Karimi et al., 2016). Various non-convex problems have been demonstrated to fulfill this property like quadratic phase loss function."}, {"heading": "3.5. Inexact SVRG-APGnc", "text": "We will further examine the inexact SVRG APGnc algorithm, and the setting of inaccuracy is the same as in Section 3.3. Here we will focus on the case where g is convex and ek = 0. Suppose that the K L property is globally satisfied with \u03b8 = 1 / 2. Set \u03b7 = \u03c1 / L, in which B < 1 / 2 and 8\u03c12m2 + \u2264 1. Let us assume that m \u2212 1 x x x x x = 0 3E [tk] \u2264 m \u2212 1 x x \u00b2 t + 1k \u00b2 2] is convergence in any way > 0, and define x x x T + 1k = proxiv \u2212 f (xtk)."}, {"heading": "4. Experiments", "text": "In this section, we compare the efficiency of APGnc and SVRG-APGnc with other competitive methods by numerical experiments. In particular, we focus on the problem of non-negative principle component analysis (NNPCA), which can be formulated asmin x \u2265 0 \u2212 1 2 xT (n \u2211 i = 1 ziz T i) x + \u03b3 \u00b2 x 2. (10) It can be equivalent to asmin x \u2212 1 2 xT (n \u2211 i = 1 ziz T i) x + \u03b3 2 + 1 {x \u2265 0}. (11) Here, f corresponds to the first two terms, and g is the indicator of the non-negative orthant, i.e. 1 {x \u2265 0}. This problem is not convex due to the negative sign and meets assumption 1. In particular, it meets the K L property because it is square."}, {"heading": "4.1. Comparison among APG variants", "text": "We first compare the deterministic APG-like methods in algorithms 2-4 and the standard method for the proximal gradient. The original APG in algorithm 1 is not taken into account because it is not a descent method and has no convergence guarantee in non-convex cases. We use a fixed step variable \u03b7 = 0.05 / L. In Figure 1 (a) we show the performance comparison of the methods when there is no error in the gradient or proximal calculation. We can see that APGnc and APGnc + outperform all other APG variants. In particular, APGnc + performs best with our adaptive dynamic strategy and justifies its empirical advantage. We note that the mAPG requires two transitions over all samples in each GnGnGnc calculation and is therefore less efficient than the exact APG study in the exact APG-1 method."}, {"heading": "4.2. Comparison among SVRG-APG variants", "text": "We then compare the performance between SVRGAPGnc, SVRG-APGnc + and the original proximalSVRG methods and select the step size \u03b7 = 1 / 8mL with m = n. The results are shown in Figures 3 and 4. In the error-free fall in Figure 3 (a), we can see that the SVRG-AGPnc + method outperforms the others due to the adaptive momentum, and the SVRG-APGnc method also behaves better than the original proximal SVRG method. In the inaccurate case, we set the proximal error as k = min (1100k3, 10 \u2212 7). You can see from Figure 3 (b) that the performance is deteriorated compared to the exact case and settles to a different local minimum. In this result, all methods are no longer monotonous due to the inaccuracy and stochastic nature of SVRG. Nevertheless, the SVRG-APGnc + provides the best results, although we compare the SVRG + RAPG method with the SVRPG (SVRPG)."}, {"heading": "5. Conclusion", "text": "In this paper, we have presented a comprehensive analysis of APGnc's convergence properties, as well as its inaccurate and stochastic variance reduction using the K L property, and proposed an improved APGnc + algorithm by adjusting the impulse parameter. We have shown that APGnc has the same convergence guarantee and the same order of convergence rate as mAPG, but is more computationally efficient and adaptable. In order to use the K L property for accelerated algorithms in situations with inaccurate errors and / or stochastic, variance-reduced gradients, we have developed novel convergence analysis techniques that may be useful for formulating other algorithms for non-convex problems."}], "references": [{"title": "On the convergence of the proximal algorithm for nonsmooth functions involving analytic features", "author": ["H. Attouch", "J. Bolte"], "venue": "Mathematical Programming,", "citeRegEx": "Attouch and Bolte,? \\Q2009\\E", "shortCiteRegEx": "Attouch and Bolte", "year": 2009}, {"title": "Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward-backward splitting, and regularized Gauss-Seidel methods", "author": ["H. Attouch", "J. Bolte", "B. Svaiter"], "venue": "Mathematical Programming,", "citeRegEx": "Attouch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Attouch et al\\.", "year": 2013}, {"title": "Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems", "author": ["A. Beck", "M. Teboulle"], "venue": "Transactions on Image Processing,", "citeRegEx": "Beck and Teboulle,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle", "year": 2009}, {"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal of Image Science.,", "citeRegEx": "Beck and Teboulle,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle", "year": 2009}, {"title": "The Lojasiewicz inequality for nonsmooth subanalytic functions with applications to subgradient dynamical systems", "author": ["J. Bolte", "A. Daniilidis", "A. Lewis"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Bolte et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bolte et al\\.", "year": 2007}, {"title": "Proximal alternating linearized minimization for nonconvex and nonsmooth problems", "author": ["J. Bolte", "S. Sabach", "M. Teboulle"], "venue": "Mathematical Programming,", "citeRegEx": "Bolte et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bolte et al\\.", "year": 2014}, {"title": "Splitting methods with variable metric for kurdyka\u2013 lojasiewicz functions and general convergence rates", "author": ["P. Frankel", "G. Garrigos", "J. Peypouquet"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Frankel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Frankel et al\\.", "year": 2015}, {"title": "Identity matters in deep learning", "author": ["M. Hardt", "T. Ma"], "venue": "Arxiv preprint,", "citeRegEx": "Hardt and Ma,? \\Q2016\\E", "shortCiteRegEx": "Hardt and Ma", "year": 2016}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Johnson and Zhang,? \\Q2013\\E", "shortCiteRegEx": "Johnson and Zhang", "year": 2013}, {"title": "Linear convergence of gradient and proximal-gradient methods under the Polyak- Lojasiewicz condition. Machine Learning and Knowledge Discovery in Databases", "author": ["H. Karimi", "J. Nutini", "M. Schmidt"], "venue": "European Conference,", "citeRegEx": "Karimi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Karimi et al\\.", "year": 2016}, {"title": "On gradients of functions definable in o-minimal structures", "author": ["K. Kurdyka"], "venue": "Annales de l\u2019institut Fourier,", "citeRegEx": "Kurdyka,? \\Q1998\\E", "shortCiteRegEx": "Kurdyka", "year": 1998}, {"title": "Calculus of the exponent of Kurdyka- Lojasiewicz inequality and its applications to linear convergence of first-order methods", "author": ["G. Li", "T. Kei"], "venue": "ArXiv preprint,", "citeRegEx": "Li and Kei,? \\Q2016\\E", "shortCiteRegEx": "Li and Kei", "year": 2016}, {"title": "Accelerated proximal gradient methods for nonconvex programming", "author": ["H. Li", "Z. Lin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Li and Lin,? \\Q2015\\E", "shortCiteRegEx": "Li and Lin", "year": 2015}, {"title": "A method of solving a convex programming problem with convergence rateO(1/k)", "author": ["Y. Nesterov"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "Nesterov,? \\Q1983\\E", "shortCiteRegEx": "Nesterov", "year": 1983}, {"title": "Stochastic variance reduction for nonconvex optimization", "author": ["S. Reddi", "A. Hefny", "S. Sra", "B. Poczos", "A. Smola"], "venue": "ArXiv preprint,", "citeRegEx": "Reddi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reddi et al\\.", "year": 2016}, {"title": "Proximal stochastic methods for nonsmooth nonconvex finitesum optimization", "author": ["S. Reddi", "S. Sra", "B. Poczos", "A. Smola"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Reddi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reddi et al\\.", "year": 2016}, {"title": "Convergence rates of inexact proximal-gradient methods for convex optimization", "author": ["M. Schmidt", "N.L. Roux", "F.R. Bach"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Schmidt et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2011}, {"title": "Approximation accuracy, gradient methods, and error bound for structured convex optimization", "author": ["P. Tseng"], "venue": "Mathematical Programming,", "citeRegEx": "Tseng,? \\Q2010\\E", "shortCiteRegEx": "Tseng", "year": 2010}, {"title": "URL https://arxiv", "author": ["Q. Yao", "Kwok", "J.T. More efficient accelerated proximal algorithm for nonconvex problems. ArXiv preprint", "December"], "venue": "org/abs/1612.09069.", "citeRegEx": "Yao et al\\.,? 2016", "shortCiteRegEx": "Yao et al\\.", "year": 2016}, {"title": "Geometrical properties and accelerated gradient solvers of non-convex phase retrieval", "author": ["Y. Zhou", "H. Zhang", "Y. Liang"], "venue": "The 54th Annual Allerton Conference,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "Improved SVRG for nonstrongly-convex or sum-of-non-convex objectives", "author": ["Z. Zhu", "Y. Yuan"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Zhu and Yuan,? \\Q2016\\E", "shortCiteRegEx": "Zhu and Yuan", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "The reader can refer to (Tseng, 2010) for other variants of APG.", "startOffset": 24, "endOffset": 37}, {"referenceID": 13, "context": "APG algorithms: The original accelerated gradient method for minimizing a single smooth convex function dates back to (Nesterov, 1983), and is further extended as APG in the composite minimization framework in (Beck & Teboulle, 2009b; Tseng, 2010).", "startOffset": 118, "endOffset": 134}, {"referenceID": 17, "context": "APG algorithms: The original accelerated gradient method for minimizing a single smooth convex function dates back to (Nesterov, 1983), and is further extended as APG in the composite minimization framework in (Beck & Teboulle, 2009b; Tseng, 2010).", "startOffset": 210, "endOffset": 247}, {"referenceID": 4, "context": "Nonconvex optimization under K L: The K L property (Bolte et al., 2007) is an extension of the Lojasiewicz gradient inequality ( Lojasiewicz, 1965) to the nonsmooth case.", "startOffset": 51, "endOffset": 71}, {"referenceID": 5, "context": "Many first-order descent methods, under the K L property, can be shown to converge to a critical point (Attouch & Bolte, 2009; Attouch et al., 2010; Bolte et al., 2014) with different types of asymptotic convergence rates.", "startOffset": 103, "endOffset": 168}, {"referenceID": 1, "context": "Inexact algorithms under K L: (Attouch et al., 2013; Frankel et al., 2015) studied the inexact proximal algorithm under the K L property.", "startOffset": 30, "endOffset": 74}, {"referenceID": 6, "context": "Inexact algorithms under K L: (Attouch et al., 2013; Frankel et al., 2015) studied the inexact proximal algorithm under the K L property.", "startOffset": 30, "endOffset": 74}, {"referenceID": 14, "context": "Recently, SVRG was further studied for smooth nonconvex optimization in Reddi et al. (2016a). Then in (Reddi et al.", "startOffset": 72, "endOffset": 93}, {"referenceID": 5, "context": "The proximal map is a popular tool to handle the nonsmooth part of the objective function, and is the key component of proximal-like algorithms (Beck & Teboulle, 2009b; Bolte et al., 2014).", "startOffset": 144, "endOffset": 188}, {"referenceID": 5, "context": "Definition 5 (Uniformized K L property, (Bolte et al., 2014)).", "startOffset": 40, "endOffset": 60}, {"referenceID": 10, "context": "The above definition is a modified version of the original K L property (Bolte et al., 2010; Kurdyka, 1998), and is more convenient for our analysis later.", "startOffset": 72, "endOffset": 107}, {"referenceID": 4, "context": "The K L property is a generalization of the Lojasiewicz gradient inequality to nonsmooth functions (Bolte et al., 2007), and it is a powerful tool to analyze a class of first-order descent algorithms (Attouch & Bolte, 2009; Attouch et al.", "startOffset": 99, "endOffset": 119}, {"referenceID": 5, "context": ", 2007), and it is a powerful tool to analyze a class of first-order descent algorithms (Attouch & Bolte, 2009; Attouch et al., 2010; Bolte et al., 2014).", "startOffset": 88, "endOffset": 153}, {"referenceID": 5, "context": "For a more detailed discussion and a list of examples of K L functions, see (Bolte et al., 2014) and (Attouch et al.", "startOffset": 76, "endOffset": 96}, {"referenceID": 1, "context": "The inexact proximal algorithm has been studied in (Attouch et al., 2013) for nonconvex functions under the K L property.", "startOffset": 51, "endOffset": 73}, {"referenceID": 9, "context": "In fact, if g = 0, the K L property in such a case reduces to the well known Polyak- Lojasiewicz (PL) inequality studied in (Karimi et al., 2016).", "startOffset": 124, "endOffset": 145}, {"referenceID": 19, "context": "Various nonconvex problems have been shown to satisfy this property such as quadratic phase retrieval loss function (Zhou et al., 2016) and neural network loss function (Hardt & Ma, 2016).", "startOffset": 116, "endOffset": 135}, {"referenceID": 9, "context": ", 2016b) studied proximal gradient algorithm; (2) the K L property with \u03b8 = 12 here is different from the generalized PL inequality for composite functions adopted by (Karimi et al., 2016).", "startOffset": 167, "endOffset": 188}], "year": 2017, "abstractText": "In many modern machine learning applications, structures of underlying mathematical models often yield nonconvex optimization problems. Due to the intractability of nonconvexity, there is a rising need to develop efficient methods for solving general nonconvex problems with certain performance guarantee. In this work, we investigate the accelerated proximal gradient method for nonconvex programming (APGnc) (Yao & Kwok, 2016). The method compares between a usual proximal gradient step and a linear extrapolation step, and accepts the one that has a lower function value to achieve a monotonic decrease. In specific, under a general nonsmooth and nonconvex setting, we provide a rigorous argument to show that the limit points of the sequence generated by APGnc are critical points of the objective function. Then, by exploiting the KurdykaLojasiewicz (K L) property for a broad class of functions, we establish the linear and sub-linear convergence rates of the function value sequence generated by APGnc. We further propose a stochastic variance reduced APGnc (SVRGAPGnc), and establish its linear convergence under a special case of the K L property. We also extend the analysis to the inexact version of these methods and develop an adaptive momentum strategy that improves the numerical performance.", "creator": "LaTeX with hyperref package"}}}