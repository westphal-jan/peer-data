{"id": "1705.05363", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2017", "title": "Curiosity-driven Exploration by Self-supervised Prediction", "abstract": "In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at", "histories": [["v1", "Mon, 15 May 2017 17:56:22 GMT  (2221kb,D)", "http://arxiv.org/abs/1705.05363v1", "In ICML 2017. Website atthis https URL"]], "COMMENTS": "In ICML 2017. Website atthis https URL", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.RO stat.ML", "authors": ["deepak pathak", "pulkit agrawal", "alexei a efros", "trevor darrell"], "accepted": true, "id": "1705.05363"}, "pdf": {"name": "1705.05363.pdf", "metadata": {"source": "META", "title": "Curiosity-driven Exploration by Self-supervised Prediction", "authors": ["Deepak Pathak", "Pulkit Agrawal", "Alexei A. Efros", "Trevor Darrell"], "emails": ["<pathak@berkeley.edu>."], "sections": [{"heading": "1. Introduction", "text": "This year, it is only a matter of time before agreement is reached."}, {"heading": "2. Curiosity-Driven Exploration", "text": "Our agent consists of two subsystems: a reward generator that emits a curious intrinsic reward signal, and a policy that emits a sequence of actions to maximize that reward signal. In addition to the intrinsic rewards, the agent can also optionally receive an extrinsic reward from the environment. Let the intrinsic reward for curiosity generated by the agent in due course be different, and the extrinsic reward must be ret. The political subsystem is trained to maximize the sum of these two rewards rt = rit + r e t, usually (if not always) zero. We represent the policy p through a deep neural network of prevP parameters. Given the agent in the state st, it executes the action at a time when the policy (st; p) is affected."}, {"heading": "2.1. Prediction error as curiosity reward", "text": "Making predictions in the raw sensory space (e.g. if the agent matches images) is undesirable, not only because it is difficult to predict pixels directly, but also because it is unclear whether predicting pixels is the right target for optimization at all. To see why, consider the use of prediction errors in the pixel space as a reward for curiosity. Imagine a scenario in which the agent observes the movement of tree leaves in a breeze. As it is inherently difficult to model the breeze, it is even more difficult to predict the pixel position of each individual leaf. This implies that the pixel prediction error remains high and the agent always remains curious about the leaves. However, the movement of the leaves is undesirable for the agent as a result of continued curiosity about them. The underlying problem is that the agent is unaware that some parts of the state space can be expanded, and thus the agent falls into the trap of curiosity."}, {"heading": "2.2. Self-supervised prediction for exploration", "text": "We suggest that such a trait space can be learned by forming a deep neural network with two submodules: however, the first submodule encodes the raw state (st) into a trait vector (st) and the second submodule takes as inputs the trait coding (st + 1) of two consequent states and predicts the action (at) that the agent takes to move from the state st to st + 1. Training of this neural network amounts to the learning function g defined as: an input the trait coding (st + 1) of two consequent states and predicts the action (at). Training of this neural network amounts to the learning function g as follows: a)."}, {"heading": "3. Experimental Setup", "text": "The first environment we evaluate is the VizDoom (Kempka et al., 2016) game. We look at the Doom 3-D navigation task, in which the agent's action space consists of four discrete actions - move forward, move to the left and not perform any action. Our test setup in all experiments is the \"DoomMyWayHome v0\" environment, which is available as part of OpenAI Gym (Brockman et al., 2016). Episodes are either terminated when the agent finds the vest or when the agent exceeds a maximum of 2100 time steps. The map consists of 9 rooms connected by corridors and the agent is asked to reach some fixed destinations."}, {"heading": "4. Experiments", "text": "We evaluate qualitatively and quantitatively the performance of the learned policy with and without the proposed intrinsic curiosity signal in two environments, VizDoom and Super Mario Bros. Three general scenarios are evaluated: a) low extrinsic reward for reaching a goal (Section 4.1); b) exploration without extrinsic reward (Section 4.2); and c) generalization to new scenarios (Section 4.3). In VizDoom, the generalization is evaluated on a novel map with new textures, while in Mario it is evaluated on subsequent game levels."}, {"heading": "4.1. Sparse Extrinsic Reward Setting", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live."}, {"heading": "4.2. No Reward Setting", "text": "A good exploration policy is one that allows the agent to visit as many states as possible without getting promoted. To test whether our agent can operate a good exploration policy, we have sent him in both cases in search of environmental rewards."}, {"heading": "4.3. Generalization to Novel Scenarios", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country."}, {"heading": "5. Related Work", "text": "Exploration driven by curiosity is a well-researched topic in reinforcement theory, and a good summary can be found instead in (Oudeyer & Kaplan, 2009; Oudeyer et al., 2007). Schmidhuber (1991; 2010) and Wang et al. (2011) use surprise and compression advances as intrinsic rewards. Classical work by Kearns et al. (1999) and Brafman et al. (2002) suggests exploration algorithms that are polynomial in the number of state spatial parameters. Others use empowerment, which is the gain of information based on the entropy of actions, as intrinsic rewards (Klyubin et al., 2005; Mohamed & Rezende, 2015). Stage of predictive error in the feature space of an auto encoder as a measure of interesting states to explore."}, {"heading": "6. Discussion", "text": "This year, it is only a matter of time before an agreement is reached."}], "references": [{"title": "Learning to see by moving", "author": ["Agrawal", "Pulkit", "Carreira", "Joao", "Malik", "Jitendra"], "venue": "In ICCV,", "citeRegEx": "Agrawal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2015}, {"title": "Learning to poke by poking: Experiential learning of intuitive physics", "author": ["Agrawal", "Pulkit", "Nair", "Ashvin", "Abbeel", "Pieter", "Malik", "Jitendra", "Levine", "Sergey"], "venue": null, "citeRegEx": "Agrawal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2016}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["Schaul", "Tom", "Saxton", "David", "Munos", "Remi"], "venue": "In NIPS,", "citeRegEx": "Schaul et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2016}, {"title": "R-max-a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["Brafman", "Ronen I", "Tennenholtz", "Moshe"], "venue": null, "citeRegEx": "Brafman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Brafman et al\\.", "year": 2002}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["Clevert", "Djork-Arn\u00e9", "Unterthiner", "Thomas", "Hochreiter", "Sepp"], "venue": null, "citeRegEx": "Clevert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clevert et al\\.", "year": 2015}, {"title": "Unsupervised visual representation learning by context prediction", "author": ["Doersch", "Carl", "Gupta", "Abhinav", "Efros", "Alexei A"], "venue": "In ICCV,", "citeRegEx": "Doersch et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Doersch et al\\.", "year": 2015}, {"title": "Learning to act by predicting the future", "author": ["Dosovitskiy", "Alexey", "Koltun", "Vladlen"], "venue": null, "citeRegEx": "Dosovitskiy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2016}, {"title": "Ex2: Exploration with exemplar models for deep reinforcement learning", "author": ["Fu", "Justin", "Co-Reyes", "John D", "Levine", "Sergey"], "venue": null, "citeRegEx": "Fu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Fu et al\\.", "year": 2017}, {"title": "Unsupervised feature learning from temporal data", "author": ["Goroshin", "Ross", "Bruna", "Joan", "Tompson", "Jonathan", "Eigen", "David", "LeCun", "Yann"], "venue": null, "citeRegEx": "Goroshin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goroshin et al\\.", "year": 2015}, {"title": "Variational intrinsic control", "author": ["Gregor", "Karol", "Rezende", "Danilo Jimenez", "Wierstra", "Daan"], "venue": "ICLR Workshop,", "citeRegEx": "Gregor et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2017}, {"title": "Vime: Variational information maximizing exploration", "author": ["Houthooft", "Rein", "Chen", "Xi", "Duan", "Yan", "Schulman", "John", "De Turck", "Filip", "Abbeel", "Pieter"], "venue": null, "citeRegEx": "Houthooft et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Houthooft et al\\.", "year": 2016}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["Jaderberg", "Max", "Mnih", "Volodymyr", "Czarnecki", "Wojciech Marian", "Schaul", "Tom", "Leibo", "Joel Z", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": null, "citeRegEx": "Jaderberg et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2017}, {"title": "Learning image representations tied to ego-motion", "author": ["Jayaraman", "Dinesh", "Grauman", "Kristen"], "venue": "In ICCV,", "citeRegEx": "Jayaraman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jayaraman et al\\.", "year": 2015}, {"title": "Forward models: Supervised learning with a distal teacher", "author": ["Jordan", "Michael I", "Rumelhart", "David E"], "venue": "Cognitive science,", "citeRegEx": "Jordan et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1992}, {"title": "Efficient reinforcement learning in factored mdps", "author": ["Kearns", "Michael", "Koller", "Daphne"], "venue": "In IJCAI,", "citeRegEx": "Kearns et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1999}, {"title": "Vizdoom: A doom-based ai research platform for visual reinforcement learning", "author": ["Kempka", "Micha\u0142", "Wydmuch", "Marek", "Runc", "Grzegorz", "Toczek", "Jakub", "Ja\u015bkowski", "Wojciech"], "venue": null, "citeRegEx": "Kempka et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kempka et al\\.", "year": 2016}, {"title": "Empowerment: A universal agentcentric measure of control", "author": ["Klyubin", "Alexander S", "Polani", "Daniel", "Nehaniv", "Chrystopher L"], "venue": "In Evolutionary Computation,", "citeRegEx": "Klyubin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Klyubin et al\\.", "year": 2005}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": null, "citeRegEx": "Lillicrap et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2016}, {"title": "Learning and exploration in action-perception loops", "author": ["Little", "Daniel Y", "Sommer", "Friedrich T"], "venue": "Closing the Loop Around Neural Systems,", "citeRegEx": "Little et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Little et al\\.", "year": 2014}, {"title": "Exploration in model-based reinforcement learning by empirically estimating learning progress", "author": ["Lopes", "Manuel", "Lang", "Tobias", "Toussaint", "Marc", "Oudeyer", "Pierre-Yves"], "venue": "In NIPS,", "citeRegEx": "Lopes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lopes et al\\.", "year": 2012}, {"title": "Learning to navigate in complex environments", "author": ["Mirowski", "Piotr", "Pascanu", "Razvan", "Viola", "Fabio", "Soyer", "Hubert", "Ballard", "Andy", "Banino", "Andrea", "Denil", "Misha", "Goroshin", "Ross", "Sifre", "Laurent", "Kavukcuoglu", "Koray"], "venue": null, "citeRegEx": "Mirowski et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mirowski et al\\.", "year": 2017}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Badia", "Adria Puigdomenech", "Mirza", "Mehdi", "Graves", "Alex", "Lillicrap", "Timothy P", "Harley", "Tim", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Variational information maximisation for intrinsically motivated reinforcement learning", "author": ["Mohamed", "Shakir", "Rezende", "Danilo Jimenez"], "venue": "In NIPS,", "citeRegEx": "Mohamed et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2015}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["Oh", "Junhyuk", "Guo", "Xiaoxiao", "Lee", "Honglak", "Lewis", "Richard L", "Singh", "Satinder"], "venue": "In NIPS,", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Deep exploration via bootstrapped dqn", "author": ["Osband", "Ian", "Blundell", "Charles", "Pritzel", "Alexander", "Van Roy", "Benjamin"], "venue": "In NIPS,", "citeRegEx": "Osband et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2016}, {"title": "What is intrinsic motivation? a typology of computational approaches", "author": ["Oudeyer", "Pierre-Yves", "Kaplan", "Frederic"], "venue": "Frontiers in neurorobotics,", "citeRegEx": "Oudeyer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Oudeyer et al\\.", "year": 2009}, {"title": "Intrinsic motivation systems for autonomous mental development", "author": ["Oudeyer", "Pierre-Yves", "Kaplan", "Frdric", "Hafner", "Verena V"], "venue": "Evolutionary Computation,", "citeRegEx": "Oudeyer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Oudeyer et al\\.", "year": 2007}, {"title": "Super mario bros", "author": ["Paquette", "Philip"], "venue": "in openai gym. github:ppaquette/gym-super-mario,", "citeRegEx": "Paquette and Philip.,? \\Q2016\\E", "shortCiteRegEx": "Paquette and Philip.", "year": 2016}, {"title": "Context encoders: Feature learning by inpainting", "author": ["Pathak", "Deepak", "Krahenbuhl", "Philipp", "Donahue", "Jeff", "Darrell", "Trevor", "Efros", "Alexei A"], "venue": null, "citeRegEx": "Pathak et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pathak et al\\.", "year": 2016}, {"title": "An analytic solution to discrete bayesian reinforcement learning", "author": ["Poupart", "Pascal", "Vlassis", "Nikos", "Hoey", "Jesse", "Regan", "Kevin"], "venue": "In ICML,", "citeRegEx": "Poupart et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Poupart et al\\.", "year": 2006}, {"title": "Intrinsic and extrinsic motivations: Classic definitions and new directions", "author": ["Richard Ryan", "Edward L. Deci"], "venue": "Contemporary Educational Psychology,", "citeRegEx": "Ryan and Deci,? \\Q2000\\E", "shortCiteRegEx": "Ryan and Deci", "year": 2000}, {"title": "A possibility for implementing curiosity and boredom in model-building neural controllers", "author": ["Schmidhuber", "Jurgen"], "venue": "In From animals to animats: Proceedings of the first international conference on simulation of adaptive behavior,", "citeRegEx": "Schmidhuber and Jurgen.,? \\Q1991\\E", "shortCiteRegEx": "Schmidhuber and Jurgen.", "year": 1991}, {"title": "Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010)", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q2010\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 2010}, {"title": "Loss is its own reward: Self-supervision for reinforcement learning", "author": ["Shelhamer", "Evan", "Mahmoudieh", "Parsa", "Argus", "Max", "Darrell", "Trevor"], "venue": null, "citeRegEx": "Shelhamer et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Shelhamer et al\\.", "year": 2017}, {"title": "Curiosity and motivation", "author": ["Silvia", "Paul J"], "venue": "In The Oxford Handbook of Human Motivation,", "citeRegEx": "Silvia and J.,? \\Q2012\\E", "shortCiteRegEx": "Silvia and J.", "year": 2012}, {"title": "Intrinsically motivated reinforcement learning", "author": ["Singh", "Satinder P", "Barto", "Andrew G", "Chentanez", "Nuttapong"], "venue": "In NIPS,", "citeRegEx": "Singh et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2005}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["Stadie", "Bradly C", "Levine", "Sergey", "Abbeel", "Pieter"], "venue": "NIPS Workshop,", "citeRegEx": "Stadie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stadie et al\\.", "year": 2015}, {"title": "An information-theoretic approach to curiosity-driven reinforcement learning", "author": ["Still", "Susanne", "Precup", "Doina"], "venue": "Theory in Biosciences,", "citeRegEx": "Still et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Still et al\\.", "year": 2012}, {"title": "Reinforcement driven information acquisition in nondeterministic environments", "author": ["Storck", "Jan", "Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "In ICANN,", "citeRegEx": "Storck et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Storck et al\\.", "year": 1995}, {"title": "Intrinsic motivation and automatic curricula via asymmetric self-play", "author": ["Sukhbaatar", "Sainbayar", "Kostrikov", "Ilya", "Szlam", "Arthur", "Fergus", "Rob"], "venue": null, "citeRegEx": "Sukhbaatar et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2017}, {"title": "Planning to be surprised: Optimal bayesian exploration in dynamic environments", "author": ["Sun", "Yi", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In AGI,", "citeRegEx": "Sun et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2011}, {"title": "Unsupervised learning of visual representations using videos", "author": ["Wang", "Xiaolong", "Gupta", "Abhinav"], "venue": "In ICCV,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "An internal model for sensorimotor integration", "author": ["Wolpert", "Daniel M", "Ghahramani", "Zoubin", "Jordan", "Michael I"], "venue": "Science-AAAS-Weekly Paper Edition,", "citeRegEx": "Wolpert et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Wolpert et al\\.", "year": 1995}], "referenceMentions": [{"referenceID": 21, "context": "the running score in an Atari game (Mnih et al., 2015), or the distance between a robot arm and an object in a reaching task (Lillicrap et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 17, "context": ", 2015), or the distance between a robot arm and an object in a reaching task (Lillicrap et al., 2016).", "startOffset": 78, "endOffset": 102}, {"referenceID": 10, "context": "its knowledge about the environment) (Houthooft et al., 2016; Mohamed & Rezende, 2015; Schmidhuber, 1991; 2010; Singh et al., 2005; Stadie et al., 2015).", "startOffset": 37, "endOffset": 152}, {"referenceID": 36, "context": "its knowledge about the environment) (Houthooft et al., 2016; Mohamed & Rezende, 2015; Schmidhuber, 1991; 2010; Singh et al., 2005; Stadie et al., 2015).", "startOffset": 37, "endOffset": 152}, {"referenceID": 37, "context": "its knowledge about the environment) (Houthooft et al., 2016; Mohamed & Rezende, 2015; Schmidhuber, 1991; 2010; Singh et al., 2005; Stadie et al., 2015).", "startOffset": 37, "endOffset": 152}, {"referenceID": 19, "context": "However, estimating learnability is a non-trivial problem (Lopes et al., 2012).", "startOffset": 58, "endOffset": 78}, {"referenceID": 22, "context": "We first compare the performance of an A3C agent (Mnih et al., 2016) with and without the curiosity signal on 3-D navigation tasks with sparse extrinsic reward in the VizDoom environment.", "startOffset": 49, "endOffset": 68}, {"referenceID": 22, "context": "Our curiosity reward model can potentially be used with a range of policy learning methods; in the experiments discussed here, we use the asynchronous advantage actor critic policy gradient (A3C) (Mnih et al., 2016) for policy learning.", "startOffset": 196, "endOffset": 215}, {"referenceID": 0, "context": "The use of inverse models has been investigated to learn features for recognition tasks (Agrawal et al., 2015; Jayaraman & Grauman, 2015).", "startOffset": 88, "endOffset": 137}, {"referenceID": 0, "context": "The use of inverse models has been investigated to learn features for recognition tasks (Agrawal et al., 2015; Jayaraman & Grauman, 2015). Agrawal et al. (2016) constructed a joint inverse-forward model to learn feature representation for the task of pushing objects.", "startOffset": 89, "endOffset": 161}, {"referenceID": 15, "context": "Environments The first environment we evaluate on is the VizDoom (Kempka et al., 2016) game.", "startOffset": 65, "endOffset": 86}, {"referenceID": 22, "context": "Training details All agents in this work are trained using visual inputs that are pre-processed in manner similar to (Mnih et al., 2016).", "startOffset": 117, "endOffset": 136}, {"referenceID": 21, "context": "Closely following (Mnih et al., 2015; 2016), we use action repeat of four during training time in VizDoom and action repeat of six in Mario.", "startOffset": 18, "endOffset": 43}, {"referenceID": 4, "context": "An exponential linear unit (ELU; (Clevert et al., 2015)) is used after each convolution layer.", "startOffset": 33, "endOffset": 55}, {"referenceID": 37, "context": "Note that ICM-pixels is representative of previous methods which compute information gain by directly using the observation space (Schmidhuber, 2010; Stadie et al., 2015).", "startOffset": 130, "endOffset": 170}, {"referenceID": 10, "context": "Finally, we include comparison with state-of-the-art exploration methods based on variational information maximization (VIME) (Houthooft et al., 2016) which is trained with TRPO.", "startOffset": 126, "endOffset": 150}, {"referenceID": 10, "context": "Comparison to TRPO-VIME We now compare our curious agent against variational information maximization agent trained with TRPO (Houthooft et al., 2016) for the VizDoom \u201csparse\u201d reward setup described above.", "startOffset": 126, "endOffset": 150}, {"referenceID": 7, "context": "The hyperparameters and accuracy for the TRPO and VIME results follow from the concurrent work (Fu et al., 2017).", "startOffset": 95, "endOffset": 112}, {"referenceID": 20, "context": "There are several prior works that use reinforcement learning to navigate in 3D environments from pixel inputs or playing ATARI games such as (Mirowski et al., 2017; Mnih et al., 2015; 2016), but they rely on intermediate external rewards provided by the environment.", "startOffset": 142, "endOffset": 190}, {"referenceID": 21, "context": "There are several prior works that use reinforcement learning to navigate in 3D environments from pixel inputs or playing ATARI games such as (Mirowski et al., 2017; Mnih et al., 2015; 2016), but they rely on intermediate external rewards provided by the environment.", "startOffset": 142, "endOffset": 190}, {"referenceID": 27, "context": "Curiosity-driven exploration is a well studied topic in the reinforcement learning literature and a good summary can be found in (Oudeyer & Kaplan, 2009; Oudeyer et al., 2007).", "startOffset": 129, "endOffset": 175}, {"referenceID": 16, "context": "Others have used empowerment, which is the information gain based on entropy of actions, as intrinsic rewards (Klyubin et al., 2005; Mohamed & Rezende, 2015).", "startOffset": 110, "endOffset": 157}, {"referenceID": 24, "context": "State visitation counts have also been investigated for exploration (Bellemare et al., 2016; Oh et al., 2015; Tang et al., 2016).", "startOffset": 68, "endOffset": 128}, {"referenceID": 21, "context": "Curiosity-driven exploration is a well studied topic in the reinforcement learning literature and a good summary can be found in (Oudeyer & Kaplan, 2009; Oudeyer et al., 2007). Schmidhuber (1991; 2010) and Sun et al. (2011) use surprise and compression progress as intrinsic rewards.", "startOffset": 154, "endOffset": 224}, {"referenceID": 13, "context": "Classic work of Kearns et al. (1999) and Brafman et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 3, "context": "(1999) and Brafman et al. (2002) propose exploration algorithms polynomial in the number of state space parameters.", "startOffset": 11, "endOffset": 33}, {"referenceID": 3, "context": "(1999) and Brafman et al. (2002) propose exploration algorithms polynomial in the number of state space parameters. Others have used empowerment, which is the information gain based on entropy of actions, as intrinsic rewards (Klyubin et al., 2005; Mohamed & Rezende, 2015). Stadie et al. (2015) use prediction error in the feature space of an auto-encoder as a measure of interesting states to explore.", "startOffset": 11, "endOffset": 296}, {"referenceID": 3, "context": "(1999) and Brafman et al. (2002) propose exploration algorithms polynomial in the number of state space parameters. Others have used empowerment, which is the information gain based on entropy of actions, as intrinsic rewards (Klyubin et al., 2005; Mohamed & Rezende, 2015). Stadie et al. (2015) use prediction error in the feature space of an auto-encoder as a measure of interesting states to explore. State visitation counts have also been investigated for exploration (Bellemare et al., 2016; Oh et al., 2015; Tang et al., 2016). Osband et al. (2016) train multiple value functions and makes", "startOffset": 11, "endOffset": 555}, {"referenceID": 39, "context": "Many approaches measure information gain for exploration (Little & Sommer, 2014; Still & Precup, 2012; Storck et al., 1995).", "startOffset": 57, "endOffset": 123}, {"referenceID": 1, "context": "Our approach of jointly training forward and inverse models for learning a feature space has similarities to (Agrawal et al., 2016; Jordan & Rumelhart, 1992; Wolpert et al., 1995), but these works use the learned models of dynamics for planning a sequence of actions instead of exploration.", "startOffset": 109, "endOffset": 179}, {"referenceID": 43, "context": "Our approach of jointly training forward and inverse models for learning a feature space has similarities to (Agrawal et al., 2016; Jordan & Rumelhart, 1992; Wolpert et al., 1995), but these works use the learned models of dynamics for planning a sequence of actions instead of exploration.", "startOffset": 109, "endOffset": 179}, {"referenceID": 0, "context": "The idea of using a proxy task to learn a semantic feature embedding has been used in a number of works on self-supervised learning in computer vision (Agrawal et al., 2015; Doersch et al., 2015; Goroshin et al., 2015; Jayaraman & Grauman, 2015; Pathak et al., 2016; Wang & Gupta, 2015).", "startOffset": 151, "endOffset": 286}, {"referenceID": 5, "context": "The idea of using a proxy task to learn a semantic feature embedding has been used in a number of works on self-supervised learning in computer vision (Agrawal et al., 2015; Doersch et al., 2015; Goroshin et al., 2015; Jayaraman & Grauman, 2015; Pathak et al., 2016; Wang & Gupta, 2015).", "startOffset": 151, "endOffset": 286}, {"referenceID": 8, "context": "The idea of using a proxy task to learn a semantic feature embedding has been used in a number of works on self-supervised learning in computer vision (Agrawal et al., 2015; Doersch et al., 2015; Goroshin et al., 2015; Jayaraman & Grauman, 2015; Pathak et al., 2016; Wang & Gupta, 2015).", "startOffset": 151, "endOffset": 286}, {"referenceID": 29, "context": "The idea of using a proxy task to learn a semantic feature embedding has been used in a number of works on self-supervised learning in computer vision (Agrawal et al., 2015; Doersch et al., 2015; Goroshin et al., 2015; Jayaraman & Grauman, 2015; Pathak et al., 2016; Wang & Gupta, 2015).", "startOffset": 151, "endOffset": 286}, {"referenceID": 6, "context": "Houthooft et al. (2016) use an exploration strategy that maximizes information gain about the agent\u2019s belief of the environment\u2019s dynamics.", "startOffset": 0, "endOffset": 24}, {"referenceID": 11, "context": "Several methods propose improving data efficiency of RL algorithms using self-supervised prediction based auxiliary tasks (Jaderberg et al., 2017; Shelhamer et al., 2017).", "startOffset": 122, "endOffset": 170}, {"referenceID": 34, "context": "Several methods propose improving data efficiency of RL algorithms using self-supervised prediction based auxiliary tasks (Jaderberg et al., 2017; Shelhamer et al., 2017).", "startOffset": 122, "endOffset": 170}, {"referenceID": 36, "context": "Sukhbaatar et al. (2017) generates supervision for pre-training via asymmetric self-play between two agents to improve data efficiency during fine-tuning.", "startOffset": 0, "endOffset": 25}, {"referenceID": 7, "context": "Fu et al. (2017) learn discriminative models, and Gregor et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 7, "context": "Fu et al. (2017) learn discriminative models, and Gregor et al. (2017) use empowerment based measure to tackle exploration in sparse reward setups.", "startOffset": 0, "endOffset": 71}, {"referenceID": 10, "context": "We demonstrate that our agent significantly outperforms the baseline A3C with no curiosity, a recently proposed VIME (Houthooft et al., 2016) formulation for exploration, and a baseline pixel-predicting formulation.", "startOffset": 117, "endOffset": 141}], "year": 2017, "abstractText": "In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent\u2019s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.", "creator": "LaTeX with hyperref package"}}}