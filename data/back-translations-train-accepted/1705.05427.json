{"id": "1705.05427", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2017", "title": "Repeated Inverse Reinforcement Learning", "abstract": "How detailed should we make the goals we prescribe to AI agents acting on our behalf in complex environments? Detailed and low-level specification of goals can be tedious and expensive to create, and abstract and high-level goals could lead to negative surprises as the agent may find behaviors that we would not want it to do, i.e., lead to unsafe AI. One approach to addressing this dilemma is for the agent to infer human goals by observing human behavior. This is the Inverse Reinforcement Learning (IRL) problem. However, IRL is generally ill-posed for there are typically many reward functions for which the observed behavior is optimal. While the use of heuristics to select from among the set of feasible reward functions has led to successful applications of IRL to learning from demonstration, such heuristics do not address AI safety. In this paper we introduce a novel repeated IRL problem that captures an aspect of AI safety as follows. The agent has to act on behalf of a human in a sequence of tasks and wishes to minimize the number of tasks that it surprises the human. Each time the human is surprised the agent is provided a demonstration of the desired behavior by the human. We formalize this problem, including how the sequence of tasks is chosen, in a few different ways and provide some foundational results.", "histories": [["v1", "Mon, 15 May 2017 20:06:35 GMT  (59kb)", "https://arxiv.org/abs/1705.05427v1", "The first two authors contributed equally to this work"], ["v2", "Thu, 18 May 2017 19:32:27 GMT  (59kb)", "http://arxiv.org/abs/1705.05427v2", "The first two authors contributed equally to this work"]], "COMMENTS": "The first two authors contributed equally to this work", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["kareem amin", "nan jiang", "satinder singh"], "accepted": true, "id": "1705.05427"}, "pdf": {"name": "1705.05427.pdf", "metadata": {"source": "META", "title": "Repeated Inverse Reinforcement Learning", "authors": ["Kareem Amin", "Nan Jiang", "Satinder Singh"], "emails": ["<nanjiang@umich.edu>."], "sections": [{"heading": null, "text": "ar Xiv: 170 5.05 427v 2 [cs.A I] 1 8M ay2 017scribe to AI agents acting in our order in complex environment? Detailed & low specifications of objectives can be tedious and expensive to create, and abstract & high-level objectives could lead to negative surprises as the agent finds behaviors that we do not want to lead to an unsafe AI. One approach to solving this dilemma is for the agent to derive human objectives by observing human behavior. this is the Inverse Reinforcement Learning (IRL) problem. However, IRL is generally poorly positioned as there are typically many reward functions for which the observed behavior is optimal. While the use of heuristics to select from a range of feasible reward functions has led to successful applications of IRL to learn from the demonstration, such heuristics do not address the safety of AI. In this paper we present a repeatable aspect of IRL security as a consequence."}, {"heading": "1. Introduction", "text": "One challenge in building AI agents who learn from experience is how to set their goals or rewards. In this context, an interesting answer to this question is the reversal of RL (or IRL), in which the agent derives the * Equal contribution 1Google Research, New York. This work was done as a postdoctoral researcher at the University of Michigan, Ann Arbor, at the University of Michigan, Ann Arbor. Responding to: Nan Jiang < nanjiang @ umich.edu > This paper extends to the following arXiv paper by the authors: https: / arxiv.org / abs / 1609.rewards a human by observing human politics (Ng & Russell, 2000). Unfortunately, the IRL problem is poorly positioned for many reward functions for which the observed behavior in a single task is optimal (Abbeel Ng, 2004)."}, {"heading": "2. Markov Decision Processes (MDPs)", "text": "We are interested in environments that can be represented as MDPs. An MDP is defined by its state space S, action space A, initial state distribution \u00b5 \u0445 (S), transition (or dynamic) function P: S \u00b7 A \u2192 \u2206 (S), reward function Y: S \u2192 R and discount factor \u03b3 (0, 1). A policy \u03c0: S \u2192 A describes the behavior of an actor by indicating the measures to be taken in each state. Similarly, the (normalized) value function or long-term usefulness of \u03c0 is defined as V \u03c0 (s) = (1 \u2212 \u03b3) E [\u2211 t = 1 \u03b3t \u2212 1Y (st) | s0 = s0 = s; \u03c0].1 Similarly, the Q value function Q\u03c0 (s, a) = (1 \u2212 \u03b3) E [\u2211 t = 1 \u03b3t \u2212 1Y (st) | s0 = s, a0 = a; p-value S)."}, {"heading": "3. Problem setup", "text": "Here we define the repeated IRL problem. The human reward function incorporates his / her security concerns and intrinsic / general preferences. (This approach is unknown to the agent and is the subject of interest in this, i.e., if the agent were aware of this reward, the concerns raised in this paper would be1Here we differentiate (w.l.o.g.) from the common IRL literature by assuming that reward after transition.solved. We assume that the human cannot communicate directly to the agent, but can evaluate the agent's behavior in a task as well as demonstrate optimal behavior. Formally, a task is defined by a pair (E, R), where E = (S, A, P, \u03b3) is the task environment without a reward function, and R is the task-specific reward function (task-specific reward function). We assume that all tasks share the same task."}, {"heading": "4. The challenge of identifying rewards", "text": "Note that it is impossible that when we observe human behavior in a single task, we have two equivalence positions at once. This is because all equivalence positions are fundamentally indistinguishable from an infinite number of reward functions.Definition 1. We argue that the task of identifying the reward function should amount only to identifying the (behavioral) equivalence class to which the equivalence class belongs. Specifically, identifying the equivalence class is sufficient to obtain a complete generalization of new tasks. Any remaining unidentifiability is merely representative and of no real consequence."}, {"heading": "5. Agent chooses the tasks", "text": "In this section, the protocol provides that the agent selects a sequence of tasks {(Et, Rt)}. For each task (Et, Rt), the human being reveals \u03c0-t, which is optimal for the environment Et and reward functions \u03b8 + Rt. Our goal is to design an algorithm that selects {(Et, Rt)} and identifies \u03b8 with as few tasks as possible with a desired accuracy."}, {"heading": "5.1. Omnipotent identification algorithm", "text": "According to Theorem 1, the algorithm boils down to a binary search for each component of the system by manipulating the task reward Rt. 2. See the proof for the specification of the algorithm. Theorem 1: If the algorithm boils down to a binary search for the task reward Rt. 2, there is an algorithm that provides the proof for the specification of the algorithm. Theorem 1: If the algorithm boils down to demonstrations for O (log (1 / 2)). Proof. The algorithm selects the following fixed environment in all tasks: for each s \"S\" {sref}, let one action be a self-loop, and the other action be the transition to sref. \"In sref,\" all actions cause self-loop."}, {"heading": "6. Nature chooses the tasks", "text": "While Theorem 1 provides a strong identification guarantee, it also relies on a strong assumption that {(Et, Rt)} can be chosen arbitrarily by the agent. (In this section, we let the nature presented for the2While we present a proof that Rt is being manipulated, provide only slightly more complex evidence for the setting in which all Rt are exactly zero and the manipulation is limited to the environment; see full details in the previous version of the paper on arXiv (Amin & Singh, 2016).For the purpose of analysis, choose {(Et, Rt)}. Generally speaking, we cannot get any identification guarantees in such an adversarial setup. As an example, if Rt, 0, and Et remain the same over time, we are essentially back to the classic IRL setting and suffer from the degeneration problem."}, {"heading": "6.1. The linear bandit setting", "text": "Linear bandit setting defines a finite field of action with the size of the problem. Each task is called a pair (X, R). X = [1) \u00b7 \u00b7 \u00b7 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "6.2. Ellipsoid Algorithm for Repeated Inverse Reinforcement Learning", "text": "We suggest algorithm 1 and provide the error contained in the following theory. < / p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p.\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\"."}, {"heading": "6.3. Lower bound", "text": "In Section 5, we get an O (log (1 / 2)), which is tied to the number of demonstrations that do not depend on the inefficiency of the algorithm. (1) We resolve this problem by proving a lower limit by showing that errors are inevitable in the worst-case scenario when nature chooses the tasks. (1) We provide a sketch of evidence below, and the full proof is moved to Appendix 1. (Theorem 3) For every randomized algorithm implementation in linear bandit setting, there are always existing problems. (1) While our algorithm 1 is deterministic, randomization is often critical for online learning in general (Shalev-Shwartz, 2011). (Xt, Rt), which exists before executing algorithm 1. (1) While our algorithms 1 are deterministic, randomization is often critical for online learning in general (Shalev-Shwartz, 2011)."}, {"heading": "6.4. On identification when Nature Chooses Tasks", "text": "While we have the number of total errors (cT0, cT0, cT0, cT0).Proposition 2, cT2).Proposition 2, cT2).Proposition 2, cT3, cT3, cT3, cT3, cT6, cT3, cT3, cT3, cT6, c6, c6, c6, c7, c7, c7, c7, c6, c6, c7, c6, c7, c7, c6, c7, c7, c7, c7, c7, c7, c7, c7, c7, c8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7, c8, 8, 8, 8, 8, 8, 8, 8, 8, c7, 7, 7, 7, c8, 7, 7, c8, 8, 8, 8, 8, 8, 8, c8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, c7, c7, c7, c7, c7, c7, c7, c7, c7, c7, c7, c7, c7, c7, c7, c7, c7, c7, c7, c7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, c8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,"}, {"heading": "7. Working with trajectories", "text": "In previous sections, we have assumed that the agent evaluates the agent's performance when he has made only one mistake, and indicates the optimal policy regarding the agent's occupation situation. (In practice, we would instead like to assume that the agent is a parallel version of Theorem 2. Unlike in traditional IRL, where the agent also acts, which leads to many subjectivities.) First, the overall reward for the agent's individual actions is a random variable and may differ from the expected value of his policies. Therefore, it is generally impossible to determine whether the agent's actions are nearly optimal, and instead, we assume that the human action taken by the agent in the trajectory is nearly optimal: when the agent is in a state, a mistake is made when he is counted, and when he is executed only then. (In previous sections, we assume that the agent's actions are approximately optimal)."}, {"heading": "8. Related work & Conclusions", "text": "Most of the existing work in the IRL has focused on deriving the reward function from data in a fixed environment (Ng & Russell, 2000; Abbeel & Ng, 2004; Coates et al., 2008; Ziebart et al., 2008; Ramachandran & Amir, 2007; Syed & Schapire, 2007; Regan & Boutilier, 2010); there are also applications where methods for MDPs from a single environment have been adapted to multiple environments (Ziebart et al., 2008); however, all of these work consider the objective of mimicking optimal behavior in the depicted environment (Ratliff et al., 2006) and are not aimed at generalizing new tasks. In the economic literature, the problem of inferring the usefulness of an agent from behavior has long been investigated under the heading of usefulness or preference determination."}, {"heading": "A. Proof of Lemma 2", "text": "The construction is as follows: select sref as the initial state and let absorb all other states. Let R \"(sref) = 0 and R\" confined to S\\ {sref} match R. The remaining work is to arrange the transition distribution of each action in sref so that the induced state allocation matches exactly one column of X. Attach each action a, and let x be the characteristic to which we want to associate a. The state distribution of (sref, a) is as follows: With probability p = 1 \u2212 x x 11 \u2212 x x 1, the next state is sref itself, and the probability of transition to the last state in S\\ {sref} is 1 \u2212 3 x 1x (j). In view of this, it is easy to verify that it is a valid distribution."}, {"heading": "B. Theorem 4 is tight in the worst case", "text": "We show that the theorem is narrow to a constant factor in the worst case. Leave X = [U \u2212 U] where Ud \u00b7 d is an orthonormal matrix, i.e. K = 2d. This is a valid choice of X, because its column vector x meets the x value. However, since U is arbitrary, we choose its first line to be 1 d / \u221a 2. Then we choose an ellipsoid center c and an ellipsoid center c, which differ in distance from each other, and show that an error is impossible. Specifically, we let c be equal except for the first line in which they differ from each other."}, {"heading": "D. Proof of Theorem 3", "text": "We will prove that there is a strategy of selection (Xt, Rt) \u2212 so that the expected number of errors of an algorithm is not lower than the average (in terms of detecting errors). In our construction, Xt = [0d, ejt] is where jt is any index that needs to be specified. Hence, each round of the agent is essentially asked whether he (jt) is more than average (in terms of detecting errors). The strategy of the opponent goes in phases, and Rt remains the same in each phase. Each phase has d rounds in which jt is rendered."}, {"heading": "E. Proof of Theorem 5", "text": "Since the update rule is still in the format of a central section through the ellipsoid, it is important to show that the update rule can still be used in the form of a central section through the ellipsoid, Lemma 1. (It remains to show that the update rule preserves the entire volume around it, and then we can follow the same argument as for Theorem 2.Fixing a mini-batch, let t0 be the round on which the last update occurs, and that the last update occurs, and that the last update occurs, namely in the amount of 1, c = CT0, c = CT0. Note that the number of the current mini-batch and cannot be changed, and ct = c Similar. For each i = 1, 2,., n, define z, Hi as the expected value of z, Hi, where the expectation in terms of the randomness of the trajectory is produced by humans, and let us be the Infinite Step Maintenance."}], "references": [{"title": "Apprenticeship Learning via Inverse Reinforcement Learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In Proceedings of the 21st International Conference on Machine learning,", "citeRegEx": "Abbeel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2004}, {"title": "An application of reinforcement learning to aerobatic helicopter flight", "author": ["Abbeel", "Pieter", "Coates", "Adam", "Quigley", "Morgan", "Ng", "Andrew Y"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Abbeel et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2007}, {"title": "Towards resolving unidentifiability in inverse reinforcement learning", "author": ["Amin", "Kareem", "Singh", "Satinder"], "venue": "arXiv preprint arXiv:1601.06569,", "citeRegEx": "Amin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Amin et al\\.", "year": 2016}, {"title": "Concrete problems in ai safety", "author": ["Amodei", "Dario", "Olah", "Chris", "Steinhardt", "Jacob", "Christiano", "Paul", "Schulman", "John", "Man\u00e9", "Dan"], "venue": "arXiv preprint arXiv:1606.06565,", "citeRegEx": "Amodei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Amodei et al\\.", "year": 2016}, {"title": "Ethical issues in advanced artificial intelligence. Science Fiction and Philosophy: From Time Travel to Superintelligence", "author": ["Bostrom", "Nick"], "venue": null, "citeRegEx": "Bostrom and Nick.,? \\Q2003\\E", "shortCiteRegEx": "Bostrom and Nick.", "year": 2003}, {"title": "Making rational decisions using adaptive utility elicitation", "author": ["Chajewska", "Urszula", "Koller", "Daphne", "Parr", "Ronald"], "venue": "In AAAI/IAAI, pp", "citeRegEx": "Chajewska et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Chajewska et al\\.", "year": 2000}, {"title": "Geometric algorithms and combinatorial optimization, volume 2", "author": ["Gr\u00f6tschel", "Martin", "Lov\u00e1sz", "L\u00e1szl\u00f3", "Schrijver", "Alexander"], "venue": "Springer Science & Business Media,", "citeRegEx": "Gr\u00f6tschel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gr\u00f6tschel et al\\.", "year": 2012}, {"title": "Cooperative inverse reinforcement learning", "author": ["Hadfield-Menell", "Dylan", "Russell", "Stuart J", "Abbeel", "Pieter", "Dragan", "Anca"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hadfield.Menell et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hadfield.Menell et al\\.", "year": 2016}, {"title": "Algorithms for inverse reinforcement learning", "author": ["Ng", "AndrewY", "Russell", "Stuart J"], "venue": "In Proceedings of the 17th International Conference onMachine Learning,", "citeRegEx": "Ng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2000}, {"title": "Bayesian inverse reinforcement learning", "author": ["Ramachandran", "Deepak", "Amir", "Eyal"], "venue": "Urbana, 51:61801,", "citeRegEx": "Ramachandran et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ramachandran et al\\.", "year": 2007}, {"title": "Maximum margin planning", "author": ["Ratliff", "Nathan D", "Bagnell", "J Andrew", "Zinkevich", "Martin A"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "Ratliff et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2006}, {"title": "Regret-based reward elicitation for markov decision processes", "author": ["Regan", "Kevin", "Boutilier", "Craig"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Regan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Regan et al\\.", "year": 2009}, {"title": "Robust policy computation in reward-uncertain mdps using nondominated policies", "author": ["Regan", "Kevin", "Boutilier", "Craig"], "venue": "In AAAI,", "citeRegEx": "Regan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Regan et al\\.", "year": 2010}, {"title": "Eliciting additive reward functions for markov decision processes", "author": ["Regan", "Kevin", "Boutilier", "Craig"], "venue": "In IJCAI Proceedings-International Joint Conference on Artificial Intelligence,", "citeRegEx": "Regan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Regan et al\\.", "year": 2011}, {"title": "Preference elicitation and inverse reinforcement learning", "author": ["Rothkopf", "Constantin A", "Dimitrakakis", "Christos"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Rothkopf et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rothkopf et al\\.", "year": 2011}, {"title": "Research priorities for robust and beneficial artificial intelligence", "author": ["Russell", "Stuart", "Dewey", "Daniel", "Tegmark", "Max"], "venue": "AI Magazine,", "citeRegEx": "Russell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russell et al\\.", "year": 2015}, {"title": "Online learning and online convex optimization", "author": ["Shalev-Shwartz", "Shai"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz and Shai.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz and Shai.", "year": 2011}, {"title": "A game-theoretic approach to apprenticeship learning", "author": ["Syed", "Umar", "Schapire", "Robert E"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Syed et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Syed et al\\.", "year": 2007}, {"title": "Theory of games and economic behavior (60th Anniversary Commemorative Edition)", "author": ["Von Neumann", "John", "Morgenstern", "Oskar"], "venue": "Princeton university press,", "citeRegEx": "Neumann et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Neumann et al\\.", "year": 2007}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Ziebart", "Brian D", "Maas", "Andrew L", "Bagnell", "J Andrew", "Dey", "Anind K"], "venue": "In AAAI,", "citeRegEx": "Ziebart et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 15, "context": "This is particularly relevant because IRL is a possible approach to the concern about aligning the agent\u2019s values/goals with those of humans for AI safety as society deploys more capable learning agents that impact more people in more ways (Russell et al., 2015; Amodei et al., 2016).", "startOffset": 240, "endOffset": 283}, {"referenceID": 3, "context": "This is particularly relevant because IRL is a possible approach to the concern about aligning the agent\u2019s values/goals with those of humans for AI safety as society deploys more capable learning agents that impact more people in more ways (Russell et al., 2015; Amodei et al., 2016).", "startOffset": 240, "endOffset": 283}, {"referenceID": 19, "context": "Most existing work in IRL focused on inferring the reward function using data acquired from a fixed environment (Ng & Russell, 2000; Abbeel & Ng, 2004; Coates et al., 2008; Ziebart et al., 2008; Ramachandran & Amir, 2007; Syed & Schapire, 2007; Regan & Boutilier, 2010).", "startOffset": 112, "endOffset": 269}, {"referenceID": 10, "context": "There is prior work on using data collected from multiple \u2014 but exogenously fixed\u2014 environments to predict agent behavior (Ratliff et al., 2006).", "startOffset": 122, "endOffset": 144}, {"referenceID": 19, "context": "There are also applications where methods for single-environment MDPs have been adapted to multiple environments (Ziebart et al., 2008).", "startOffset": 113, "endOffset": 135}, {"referenceID": 15, "context": "The issue of reward misspecification is often mentioned in AI safety articles (e.g., Bostrom, 2003; Russell et al., 2015; Amodei et al., 2016).", "startOffset": 78, "endOffset": 142}, {"referenceID": 3, "context": "The issue of reward misspecification is often mentioned in AI safety articles (e.g., Bostrom, 2003; Russell et al., 2015; Amodei et al., 2016).", "startOffset": 78, "endOffset": 142}, {"referenceID": 3, "context": ", 2015; Amodei et al., 2016). These articles mostly discuss the ethical concerns and possible research directions, while our paper develops mathematical formulations and algorithmic solutions. Recently, Hadfield-Menell et al. (2016) proposed cooperative inverse reinforcement learning, where the human and the agent act in the same environment, allowing the human to actively resolve the agent\u2019s uncertainty on the reward function.", "startOffset": 8, "endOffset": 233}], "year": 2017, "abstractText": "How detailed should we make the goals we prescribe to AI agents acting on our behalf in complex environments? Detailed & low-level specification of goals can be tedious and expensive to create, and abstract & high-level goals could lead to negative surprises as the agent may find behaviors that we would not want it to do, i.e., lead to unsafe AI. One approach to addressing this dilemma is for the agent to infer human goals by observing human behavior. This is the Inverse Reinforcement Learning (IRL) problem. However, IRL is generally ill-posed for there are typically many reward functions for which the observed behavior is optimal. While the use of heuristics to select from among the set of feasible reward functions has led to successful applications of IRL to learning from demonstration, such heuristics do not address AI safety. In this paper we introduce a novel repeated IRL problem that captures an aspect of AI safety as follows. The agent has to act on behalf of a human in a sequence of tasks and wishes to minimize the number of tasks that it surprises the human. Each time the human is surprised the agent is provided a demonstration of the desired behavior by the human. We formalize this problem, including how the sequence of tasks is chosen, in a few different ways and provide some foundational results.", "creator": "LaTeX with hyperref package"}}}