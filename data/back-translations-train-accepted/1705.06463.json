{"id": "1705.06463", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2017", "title": "Universal Dependencies Parsing for Colloquial Singaporean English", "abstract": "Singlish can be interesting to the ACL community both linguistically as a major creole based on English, and computationally for information extraction and sentiment analysis of regional social media. We investigate dependency parsing of Singlish by constructing a dependency treebank under the Universal Dependencies scheme, and then training a neural network model by integrating English syntactic knowledge into a state-of-the-art parser trained on the Singlish treebank. Results show that English knowledge can lead to 25% relative error reduction, resulting in a parser of 84.47% accuracies. To the best of our knowledge, we are the first to use neural stacking to improve cross-lingual dependency parsing on low-resource languages. We make both our annotation and parser available for further research.", "histories": [["v1", "Thu, 18 May 2017 08:27:42 GMT  (890kb,D)", "http://arxiv.org/abs/1705.06463v1", "Accepted by ACL 2017"]], "COMMENTS": "Accepted by ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hongmin wang", "yue zhang", "guangyong leonard chan", "jie yang", "hai leong chieu"], "accepted": true, "id": "1705.06463"}, "pdf": {"name": "1705.06463.pdf", "metadata": {"source": "CRF", "title": "Universal Dependencies Parsing for Colloquial Singaporean English", "authors": ["Hongmin Wang", "Yue Zhang", "GuangYong Leonard Chan", "Jie Yang", "Hai Leong Chieu"], "emails": ["zhang}@sutd.edu.sg", "yang@mymail.sutd.edu.sg", "chaileon}@dso.org.sg"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. Most of them are not able to survive themselves, because they are able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves, and most of them are able to survive themselves."}, {"heading": "2 Related Work", "text": "Neural networks have led to significant advances in the development of dependency analyses, including transformation-based analyses (Chen and Manning, 2014; Zhou et al., 2015; Weiss et al., 2015; Ballesteros et al., 2015; Andor et al., 2016) and graph-based analyses (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017). Specifically, the biaffine attention method used by Dozat and Manning (2017) uses deep bidirectional long-term memory (bi-LSTM) networks for high-grade nonlinear data extraction that produce the most powerful graph-based English dependency models. We are adopting this model as the basis for our Singlish Parser. Our work is one of a number of papers on transferential learning for parsing that leverages use English resources in universal dependencies to improve the accuracy of languages."}, {"heading": "3 Singlish Dependency Treebank", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Universal Dependencies for Singlish", "text": "Since English is the primary genesis of Singlish, we choose English as the source of lexical function transfer to support Singlish Dependency Parsing. Universal Dependencies offers a range of multilingual treebanks with cross-lingual consistent lexical annotations designed to support the development and evaluation of domain-specific systems, such as multilingual parsers (Nivre et al., 2016). The current version of Universal Dependencies includes not only important treebanks for 47 languages, but also their siblings for domain-specific corporas and dialects. With coordinated initiatives to create transfer-friendly treebanks, we adopt the Universal Dependencies Protocol for the construction of the Singlish Dependency Treebank, both as a new resource for these resource-poor languages and to facilitate knowledge transfer from English."}, {"heading": "3.2 Challenges and Solutions for Annotating Singlish", "text": "In fact, most of them are able to play by the rules that they have set themselves in order to play by the rules."}, {"heading": "3.3 Data Selection and Annotation", "text": "After the comparison, we chose SG Talk Forum7 as our data source due to its relative abundance of Singlish content. We crawled 84,459 posts with the Scrapy Framework8 from pages dated to December 25, 2016, and kept sentences ranging in length from 5 to 50, which are a total of 58,310. Sentences are inversely sorted according to the logarithmic probability of the sentence given by an English language model that is provided with the KenLM toolkit (Heafield et al., 2013) 9 normalized sentences of the sentence length, so that most of standard English can be selected."}, {"heading": "4 Part-of-Speech Tagging", "text": "In order to obtain automatically predicted POS tags as features for a base English dependency parser, we train a POS tagger for UD-Eng using the base model of Chen et al. (2016), represented in Figure 3. The bi-LSTM networks with a CRF layer (bi-LSTM-CRF) have 12https: / / www.singlishdictionary.com / 12https: / / en.wikipedia.org / wiki / Singlish _ vocabulary 13http: / / www.nltk.org / api / nltk. tokenize.html 14http: / / nlp.nju.edu.cn / tanggc / tools / Dependencywer.exBased on this model, we have created a tag layer for the English version of Singgc / tools / Dependencywer.ex."}, {"heading": "4.1 Base Bi-LSTM-CRF POS Tagger", "text": "Input Layer: Each token is represented as a vector by linking a word from a lookup table with a weighted average of its character embeddings, which is given by the attention model of Bahdanau et al. (2014). Characteristic Layer: According to Chen et al. (2016), the input layer generates a dense representation of the current input token by concatenating its word vector and the tokens for its surrounding context in a finite-size window. Characteristic Layer: This layer uses a bi-LSTM network to encode the input into a sequence of hidden vectors that embody global context information. According to Chen et al. (2016), we adopt bi-LSTM with peephole connections (Graves and Schmidhuber, 2005).Output Layer: This is a CRF layer to predict the POS tags for the input words by maximizing the conditional probability of the sequence of predefined input sentences."}, {"heading": "4.2 POS Tagger with Neural Stacking", "text": "As shown in Figure 4, the distributed vector representation of the target word at the input level of the Singlish Tagger is supplemented by concatenating the emission vector generated by the English Tagger with the original word and character-based embedding before the concatenation is applied within a context window in Section 4.1. During the training, the loss is propagated back to all traceable parameters in both the Singlish Tagger and the pre-trained feature level of the Basic English Tagger. At test time, the input set is fed to the integrated tagger model as a whole for inference."}, {"heading": "4.3 Results", "text": "We use publicly available source code 15 from Chen et al. (2016) to train a single-layer biLSTM CRF-based POS marking on UD-Eng using 50-dimensional pre-formed SENNA word embedding (Collobert et al., 2011). We set the hidden layer size to 300, the initial learning rate for Adagrad (Duchi et al., 2011) to 0.01, the regulation parameter \u03bb to 10 \u2212 6 and the failure rate to 15%. The label indicates 94.84% accuracy in the UD-Eng test after 24 epochs, selected according to development tests comparable to the state-of-the-art accuracy of 95.17% reported by Plank et al. (2016)."}, {"heading": "5 Dependency Parsing", "text": "We use the Dozat and Manning (2017) parser17 as our base model, as illustrated in Figure 5, and apply neural stacking to achieve improvements over the base parser. Both the base and neural stacking models consist of an input layer, a feature layer, and an output layer."}, {"heading": "5.1 Base Parser with Bi-affine Attentions", "text": "Input Layer: This layer encodes the current input word by combining a pre-trained word embedding with a traceable word embedding and the embedding of POS tags from the respective reference tables.Feature level: The two recursive vectors generated from each input vector by the multi-layer bi-LSTM network are provided by a series of parallel multi-layer perceptrons (MLP) 16We find empirically that the use of ICE-SIN embedding in neural stack models works better than the use of English SENNA embedding. Similar results are found for the parser, of which more details are given in Section 6.17https: / / github.com / tdozat / Parserlayers. Following Dozat and Manning (2017), we adopt Cif-LSTM cells (Greff et al., 2016)."}, {"heading": "5.2 Parser with Neural Stacking", "text": "Inspired by the idea of neural stacking at the trait level (Chen et al., 2016; Zhang and Weiss, 2016), we link the pre-trained word embeddings, traceable word and tag embeddings with the two recursive state vectors on the last bi-LSTM layer of the English tagger as the input vector for each target word. To maintain the syntactical knowledge maintained by the English tagger, the trait vectors from its MLP layer are added to the trace vectors produced by the Singlish parser, as shown in Figure 6, and the scoring tensor of the Singlish parser is initialized with that of the trained English tagger. Loss is propagated backwards by traversing all forward paths to all detectable parameters for the training, and the entire model is collectively used as inference."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Experimental Settings", "text": "We train an English parser on UD-Eng with the default model settings in Dozat and Manning (2017). It achieves a UAS of 88.83% and an LAS of 85.20%, which is close to the state of the art 85.90% LAS on UD-Eng reported by Ammar et al. (2016), and the main difference is caused by the fact that we do not use fine-grained POS tags. We use the same settings for a basic saver Singlish. We try to better configure the number of bi-LSTM layers and the hidden dimension based on the specified development performance, but the default settings turn out to be the best. Therefore, we stick to all default hyperparameters in Dozat and Manning (2017) to train the Singlish parameters. We experimented with different word embedding, as with the raw text sources summarized in Table 3 and described further in Section 6.2."}, {"heading": "6.2 Investigating Distributed Lexical Characteristics", "text": "In order to learn the characteristics of distributed lexical semantics for Singlish, we compare the performance of the Singlish dependency parser using several sets of pre-formed word embeddings: GloVe6B, large-format English word embeddings 18; ICE-SIN, Singlish word embeddings trained with GloVe (Pennington et al., 2014) on the ICE-SIN corpus (Nihilani, 1992; Ooi, 1997); Giga100M, a small-format English word embeddings trained with GloVe (Pennington et al., 2014) with the same settings on a comparable size of English data randomly selected from the English Gigaword Fifth Edition for a fair comparison with ICE-SIN embeddings. First, the English Giga100M embeddings improve the Singlish parser slightly from the baseline, without pre-formed embeddings, and also the UringEnglish-Parlliser directly compared to the English Singlish-6."}, {"heading": "6.3 Knowledge Transfer Using Neural Stacking", "text": "We train a neural stacking parser with singlish ICE-SIN embedded, which achieves the best performance of all models, with a UAS of 84.47%, represented as a \"stack ICE-SIN\" in Table 4, which corresponds to a relative error reduction of 25.01% compared to the baseline, showing that knowledge of English can be successfully incorporated to improve the singlish parser. To further evaluate the effectiveness of the neural stack model, we also trained a base model with the combination of UD-Eng and Singlish tree-18Trained with Wikipedia 2014, the Gigaword. Downloadable at http: / / nlp.stanford.edu / data / glove.6B.zipbank, represented as an \"ENG-plus-SIN\" in Table 4, which is still surpassed by the neural stack model. In addition, we performed a 5-fold validation for the base stack with singular SIN-5 and ENIN-4 represented as a relative improvement. \""}, {"heading": "6.4 Improvements over Grammar Types", "text": "To analyze the sources for Singlish parsing improvements using various model configurations, we perform error analyses of 5 syntactic categories19, including 4 types of grammars mentioned in Section 3.220, and 1 for all other cases, including sentences that contain imported vocabulary but are expressed in basic English syntax. The number of sentences and results in each group of the test set are in Table 6.The neural stack model results in the largest improvement across all categories, except for incomplete UAS performance in \"NP deletion\" cases, which explains the significant general discrepancies. Comparing the base model with ICE-SIN embeddings with the UD-Eng-trained base saver, which includes syntactical and semantic knowledge in Singlish and English, the former outperforms the latter in all 4 types of Singlish grammars, but not in the remaining samples. This indicates that basic English parsing contributes mainly to the basic English phrases in all 4 types of Singlish syntax."}, {"heading": "7 Conclusion", "text": "We have investigated the dependency analysis for Singlish, an important English-based Creole language, using annotations to a 10,986-word Singlish dependency tree, and developed an advanced parser, drawing on knowledge transmitted from a 20-times larger English tree bank, Universal Dependencies. We demonstrate the effectiveness of using neural stacks for function transfer by increasing Singlish dependency analysis from 79.29% to 84.47%, with a relative error reduction of 25.01% compared to the parser with all available Singlish resources. We publish the commented Singlish dependency tree, the trained model, and source code for the open-access parser. Possible future work includes extending the study to other regional languages such as Malay and Indonesian."}, {"heading": "Acknowledgments", "text": "Yue Zhang is the corresponding author. This research is supported by IGDSS1603031 from Temasek Laboratories @ SUTD. We appreciate anonymous reviewers for their insightful comments that have helped improve the work, and Zhiyang Teng, Jiangming Liu, Yupeng Liu and Enrico Santus for their constructive discussions."}, {"heading": "A Statistics of Singlish Dependency Treebank", "text": "Si \"s sla\" hc \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i.\" S \"D\" i \"h\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i"}], "references": [{"title": "Many languages, one parser", "author": ["Waleed Ammar", "George Mulcaire", "Miguel Ballesteros", "Chris Dyer", "Noah Smith."], "venue": "Transactions of the Association of Computational Linguistics 4:431\u2013444. http://aclweb.org/anthology/Q16-1031.", "citeRegEx": "Ammar et al\\.,? 2016", "shortCiteRegEx": "Ammar et al\\.", "year": 2016}, {"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "Proceedings of the ACL 2016. Association", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint abs/1409.0473. http://arxiv.org/abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Miguel Ballesteros", "Chris Dyer", "A. Noah Smith."], "venue": "Proceedings of the EMNLP 2015. Association for Computational Linguistics, pages 349\u2013359.", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher Manning."], "venue": "Proceedings of the EMNLP 2014. Association for Computational Linguistics, pages 740\u2013750. https://doi.org/10.3115/v1/D14-1082.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Neural network for heterogeneous annotations", "author": ["Hongshen Chen", "Yue Zhang", "Qun Liu."], "venue": "Proceedings of the EMNLP 2016. Association for Computational Linguistics, pages 731\u2013741. http://aclweb.org/anthology/D16-1070.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction", "author": ["Shay Cohen", "A. Noah Smith."], "venue": "Proceedings of the NAACL-HLT 2009. Association for Computational Linguistics, pages 74\u201382.", "citeRegEx": "Cohen and Smith.,? 2009", "shortCiteRegEx": "Cohen and Smith.", "year": 2009}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Deep biaffine attention for neural dependency parsing", "author": ["Timothy Dozat", "Christopher D. Manning."], "venue": "International Conference on Learning Representations 2017. volume abs/1611.01734. http://arxiv.org/abs/1611.01734.", "citeRegEx": "Dozat and Manning.,? 2017", "shortCiteRegEx": "Dozat and Manning.", "year": 2017}, {"title": "The roles of singapore standard", "author": [], "venue": null, "citeRegEx": "1244", "shortCiteRegEx": "1244", "year": 2009}, {"title": "Scalable modified kneser-ney language model estimation", "author": ["Kenneth Heafield", "Ivan Pouzyrevsky", "H. Jonathan Clark", "Philipp Koehn."], "venue": "Proceedings of the ACL 2013 (Short Papers). Association for Computational Linguistics, pages 690\u2013696.", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Proceedings of the Sixth Workshop on Statistical Machine Translation, Association for Computational Linguistics, chapter CMU Haitian Creole-English", "author": ["Sanjika Hewavitharana", "Nguyen Bach", "Qin Gao", "Vamshi Ambati", "Stephan Vogel"], "venue": null, "citeRegEx": "Hewavitharana et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hewavitharana et al\\.", "year": 2011}, {"title": "Bidirectional LSTM-CRF models for sequence tagging", "author": ["Zhiheng Huang", "Wei Xu", "Kai Yu."], "venue": "arXiv preprint abs/1508.01991. http://arxiv.org/abs/1508.01991.", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Bootstrapping parsers via syntactic projection across parallel texts", "author": ["Rebecca Hwa", "Philip Resnik", "Amy Weinberg", "Clara Cabezas", "Okan Kolak."], "venue": "Natural Language Engineering 11(3):311\u2013 325. https://doi.org/10.1017/S1351324905003840.", "citeRegEx": "Hwa et al\\.,? 2005", "shortCiteRegEx": "Hwa et al\\.", "year": 2005}, {"title": "Simple and accurate dependency parsing using bidirectional lstm feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "Transactions of the Association of Computational Linguistics 4:313\u2013 327. http://aclweb.org/anthology/Q16-1023.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Word order in french, spanish and italian: A grammaticalization account", "author": ["Karen Lahousse", "B\u00e9atrice Lamiroy."], "venue": "Folia Linguistica 46(2):387\u2013415.", "citeRegEx": "Lahousse and Lamiroy.,? 2012", "shortCiteRegEx": "Lahousse and Lamiroy.", "year": 2012}, {"title": "Modelling variation in Singapore English", "author": ["Jakob R.E. Leimgruber."], "venue": "Ph.D. thesis, Oxford University.", "citeRegEx": "Leimgruber.,? 2009", "shortCiteRegEx": "Leimgruber.", "year": 2009}, {"title": "Singapore english", "author": ["Jakob R.E. Leimgruber."], "venue": "Language and Linguistics Compass 5(1):47\u201362. https://doi.org/10.1111/j.1749-818X.2010.00262.x.", "citeRegEx": "Leimgruber.,? 2011", "shortCiteRegEx": "Leimgruber.", "year": 2011}, {"title": "When are tree structures necessary for deep learning of representations? In Proceedings of the EMNLP 2015", "author": ["Jiwei Li", "Thang Luong", "Dan Jurafsky", "Eduard Hovy."], "venue": "Association for Computational Linguistics, pages 2304\u20132314.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Mergers and acquisitions: on the ages and origins of singapore english particles", "author": ["Lisa Lim."], "venue": "World Englishes 26(4):446\u2013473.", "citeRegEx": "Lim.,? 2007", "shortCiteRegEx": "Lim.", "year": 2007}, {"title": "Parsing universal dependencies without training", "author": ["H\u00e9ctor Mart\u0131\u0301nez Alonso", "\u017deljko Agi\u0107", "Barbara Plank", "Anders S\u00f8gaard"], "venue": "In Proceedings of the EACL 2017. Association for Computational Linguistics,", "citeRegEx": "Alonso et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Alonso et al\\.", "year": 2017}, {"title": "Multi-source transfer of delexicalized dependency parsers", "author": ["Ryan McDonald", "Slav Petrov", "Keith Hall."], "venue": "Proceedings of the EMNLP 2011. Association for Computational Linguistics, pages 62\u201372. http://aclweb.org/anthology/D11-1006.", "citeRegEx": "McDonald et al\\.,? 2011", "shortCiteRegEx": "McDonald et al\\.", "year": 2011}, {"title": "Dynamics of a contact continuum: Singaporean English", "author": ["Ho Mian-Lian", "John T. Platt."], "venue": "Oxford University Press, USA.", "citeRegEx": "Mian.Lian and Platt.,? 1993", "shortCiteRegEx": "Mian.Lian and Platt.", "year": 1993}, {"title": "End-to-end relation extraction using lstms on sequences and tree structures", "author": ["Makoto Miwa", "Mohit Bansal."], "venue": "Proceedings of the ACL 2016. Association for Computational Linguistics, pages 1105\u2013 1116. https://doi.org/10.18653/v1/P16-1105.", "citeRegEx": "Miwa and Bansal.,? 2016", "shortCiteRegEx": "Miwa and Bansal.", "year": 2016}, {"title": "Selective sharing for multilingual dependency parsing", "author": ["Tahira Naseem", "Regina Barzilay", "Amir Globerson."], "venue": "Proceedings of the ACL 2012. Association for Computational Linguistics, pages 629\u2013637. http://aclweb.org/anthology/P12-1066.", "citeRegEx": "Naseem et al\\.,? 2012", "shortCiteRegEx": "Naseem et al\\.", "year": 2012}, {"title": "Using universal linguistic knowledge to guide grammar induction", "author": ["Tahira Naseem", "Harr Chen", "Regina Barzilay", "Mark Johnson."], "venue": "Proceedings of the EMNLP 2010. Association for Computational Linguistics, Cambridge, MA, pages 1234\u2013", "citeRegEx": "Naseem et al\\.,? 2010", "shortCiteRegEx": "Naseem et al\\.", "year": 2010}, {"title": "The international computerized corpus of english", "author": ["Paroo Nihilani."], "venue": "Words in a cultural context. Singapore: UniPress pages 84\u201388.", "citeRegEx": "Nihilani.,? 1992", "shortCiteRegEx": "Nihilani.", "year": 1992}, {"title": "Universal dependencies v1: A multilingual", "author": ["Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Yoav Goldberg", "Jan Hajic", "Christopher D. Manning", "Ryan McDonald", "Slav Petrov", "Sampo Pyysalo", "Natalia Silveira", "Reut Tsarfaty", "Daniel Zeman"], "venue": null, "citeRegEx": "Nivre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2016}, {"title": "The CoNLL 2007 shared task on dependency parsing", "author": ["Joakim Nivre", "Johan Hall", "Sandra K\u00fcbler", "Ryan McDonald", "Jens Nilsson", "Sebastian Riedel", "Deniz Yuret."], "venue": "Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007. As-", "citeRegEx": "Nivre et al\\.,? 2007", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "Analysing the Singapore ICE corpus for lexicographic evidence", "author": ["Vincent B Y Ooi."], "venue": "ENGLISH LANGUAGE & LITERATURE. http://scholarbank.nus.edu.sg/handle/10635/133118.", "citeRegEx": "Ooi.,? 1997", "shortCiteRegEx": "Ooi.", "year": 1997}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the EMNLP 2014. Association for Computational Linguistics, pages 1532\u20131543. https://doi.org/10.3115/v1/D14-1162.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "author": ["Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proceedings of the ACL 2016 (Short Papers). Associa-", "citeRegEx": "Plank et al\\.,? 2016", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Troll detection by domain-adapting sentiment analysis", "author": ["Chun-Wei Seah", "Hai Leong Chieu", "Kian Ming Adam Chai", "Loo-Nin Teow", "Lee Wei Yeong."], "venue": "18th International Conference on Information Fusion (Fusion) 2015. IEEE, pages 792\u2013799.", "citeRegEx": "Seah et al\\.,? 2015", "shortCiteRegEx": "Seah et al\\.", "year": 2015}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "D. Christopher Manning", "Andrew Ng", "Christopher Potts."], "venue": "Proceedings of the EMNLP 2013. Asso-", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Two baselines for unsupervised dependency parsing", "author": ["Anders S\u00f8gaard."], "venue": "Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure. Association for Computational Linguistics, pages 81\u201383.", "citeRegEx": "S\u00f8gaard.,? 2012a", "shortCiteRegEx": "S\u00f8gaard.", "year": 2012}, {"title": "Unsupervised dependency parsing without training", "author": ["Anders S\u00f8gaard."], "venue": "Natural Language Engineering 18(2):187203. https://doi.org/10.1017/S1351324912000022.", "citeRegEx": "S\u00f8gaard.,? 2012b", "shortCiteRegEx": "S\u00f8gaard.", "year": 2012}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Oscar T\u00e4ckstr\u00f6m", "Ryan McDonald", "Jakob Uszkoreit."], "venue": "Proceedings of the NAACL-HLT 2012. Association for Computational Linguistics, pages 477\u2013487.", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? 2012", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "Proceedings of the ACL-IJCNLP 2015. Association for Computational Linguistics, pages 323\u2013333.", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Hierarchical low-rank tensors for multilingual transfer parsing", "author": ["Yuan Zhang", "Regina Barzilay."], "venue": "Proceedings of the EMNLP 2015. Association for Computational Linguistics, pages 1857\u2013 1867. https://doi.org/10.18653/v1/D15-1213.", "citeRegEx": "Zhang and Barzilay.,? 2015", "shortCiteRegEx": "Zhang and Barzilay.", "year": 2015}, {"title": "Stackpropagation: Improved representation learning for syntax", "author": ["Yuan Zhang", "David Weiss."], "venue": "Proceedings of the 54th ACL. Association for Computational Linguistics, pages 1557\u2013 1566. https://doi.org/10.18653/v1/P16-1147.", "citeRegEx": "Zhang and Weiss.,? 2016", "shortCiteRegEx": "Zhang and Weiss.", "year": 2016}, {"title": "A neural probabilistic structuredprediction model for transition-based dependency parsing", "author": ["Hao Zhou", "Yue Zhang", "Shujian Huang", "Jiajun Chen."], "venue": "Proceedings of the ACL-IJCNLP 2015. Association for Computational Linguistics, pages", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "This work highlights the importance of NLP tools on creoles in crisis situations for emergency relief (Hu et al., 2011; Hewavitharana et al., 2011).", "startOffset": 102, "endOffset": 147}, {"referenceID": 23, "context": "Since dependency parsers are important for tasks such as information extraction (Miwa and Bansal, 2016) and discourse parsing (Li et al.", "startOffset": 80, "endOffset": 103}, {"referenceID": 18, "context": "Since dependency parsers are important for tasks such as information extraction (Miwa and Bansal, 2016) and discourse parsing (Li et al., 2015), this hinders the development of such downstream applications for Singlish in written forms and thus makes it crucial to build a dependency parser that can perform well natively on Singlish.", "startOffset": 126, "endOffset": 143}, {"referenceID": 16, "context": "Singlish is one of the major languages in Singapore, with borrowed vocabulary and grammars1 from a number of languages including Malay, Tamil, and Chinese dialects such as Hokkien, Cantonese and Teochew (Leimgruber, 2009, 2011), and it has been increasingly used in written forms on web media. Fluent English speakers unfamiliar with Singlish would find the creole hard to comprehend (Harada, 2009). Correspondingly, fundamental English NLP components such as POS taggers and dependency parsers perform poorly on such Singlish texts as shown in Table 2 and 4. For example, Seah et al. (2015) adapted the Socher et al.", "startOffset": 204, "endOffset": 592}, {"referenceID": 16, "context": "Singlish is one of the major languages in Singapore, with borrowed vocabulary and grammars1 from a number of languages including Malay, Tamil, and Chinese dialects such as Hokkien, Cantonese and Teochew (Leimgruber, 2009, 2011), and it has been increasingly used in written forms on web media. Fluent English speakers unfamiliar with Singlish would find the creole hard to comprehend (Harada, 2009). Correspondingly, fundamental English NLP components such as POS taggers and dependency parsers perform poorly on such Singlish texts as shown in Table 2 and 4. For example, Seah et al. (2015) adapted the Socher et al. (2013) sentiment analysis engine to the Singlish vocabulary, but failed to adapt the parser.", "startOffset": 204, "endOffset": 625}, {"referenceID": 16, "context": "We follow Leimgruber (2011) in using \u201cgrammar\u201d to describe \u201csyntactic constructions\u201d and we do not differentiate the two expressions in this paper.", "startOffset": 10, "endOffset": 28}, {"referenceID": 27, "context": "We categorize the challenges and formalize their interpretation using Universal Dependencies (Nivre et al., 2016), which extends to the creation of a Singlish dependency treebank with 1,200 sentences.", "startOffset": 93, "endOffset": 113}, {"referenceID": 8, "context": "In particular, we train a basic Singlish parser with the best off-the-shelf neural dependency parsing model using biaffine attention (Dozat and Manning, 2017), and improve it with knowledge transfer by adopting neural stacking (Chen et al.", "startOffset": 133, "endOffset": 158}, {"referenceID": 5, "context": "In particular, we train a basic Singlish parser with the best off-the-shelf neural dependency parsing model using biaffine attention (Dozat and Manning, 2017), and improve it with knowledge transfer by adopting neural stacking (Chen et al., 2016; Zhang and Weiss, 2016) to integrate the English syntax.", "startOffset": 227, "endOffset": 269}, {"referenceID": 39, "context": "In particular, we train a basic Singlish parser with the best off-the-shelf neural dependency parsing model using biaffine attention (Dozat and Manning, 2017), and improve it with knowledge transfer by adopting neural stacking (Chen et al., 2016; Zhang and Weiss, 2016) to integrate the English syntax.", "startOffset": 227, "endOffset": 269}, {"referenceID": 4, "context": "Since POS tags are important features for dependency parsing (Chen and Manning, 2014; Dyer et al., 2015), we train a POS tagger for Singlish following the same idea by integrating English POS knowledge using neural stacking.", "startOffset": 61, "endOffset": 104}, {"referenceID": 4, "context": "Neural networks have led to significant advance in the performance for dependency parsing, including transition-based parsing (Chen and Manning, 2014; Zhou et al., 2015; Weiss et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017).", "startOffset": 126, "endOffset": 254}, {"referenceID": 40, "context": "Neural networks have led to significant advance in the performance for dependency parsing, including transition-based parsing (Chen and Manning, 2014; Zhou et al., 2015; Weiss et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017).", "startOffset": 126, "endOffset": 254}, {"referenceID": 37, "context": "Neural networks have led to significant advance in the performance for dependency parsing, including transition-based parsing (Chen and Manning, 2014; Zhou et al., 2015; Weiss et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017).", "startOffset": 126, "endOffset": 254}, {"referenceID": 3, "context": "Neural networks have led to significant advance in the performance for dependency parsing, including transition-based parsing (Chen and Manning, 2014; Zhou et al., 2015; Weiss et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017).", "startOffset": 126, "endOffset": 254}, {"referenceID": 1, "context": "Neural networks have led to significant advance in the performance for dependency parsing, including transition-based parsing (Chen and Manning, 2014; Zhou et al., 2015; Weiss et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017).", "startOffset": 126, "endOffset": 254}, {"referenceID": 14, "context": ", 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017).", "startOffset": 33, "endOffset": 90}, {"referenceID": 8, "context": ", 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017).", "startOffset": 33, "endOffset": 90}, {"referenceID": 1, "context": ", 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017). In particular, the biaffine attention method of Dozat and Manning (2017) uses deep bi-directional long short-term memory (bi-LSTM) networks for highorder non-linear feature extraction, producing the highest-performing graph-based English dependency parser.", "startOffset": 8, "endOffset": 185}, {"referenceID": 13, "context": "Our work belongs to a line of work on transfer learning for parsing, which leverages English resources in Universal Dependencies to improve the parsing accuracies of low-resource languages (Hwa et al., 2005; Cohen and Smith, 2009; Ganchev et al., 2009).", "startOffset": 189, "endOffset": 252}, {"referenceID": 6, "context": "Our work belongs to a line of work on transfer learning for parsing, which leverages English resources in Universal Dependencies to improve the parsing accuracies of low-resource languages (Hwa et al., 2005; Cohen and Smith, 2009; Ganchev et al., 2009).", "startOffset": 189, "endOffset": 252}, {"referenceID": 36, "context": "Subsequent work considered syntactic similarities between languages for better feature transfer (T\u00e4ckstr\u00f6m et al., 2012; Naseem et al., 2012; Zhang and Barzilay, 2015).", "startOffset": 96, "endOffset": 167}, {"referenceID": 24, "context": "Subsequent work considered syntactic similarities between languages for better feature transfer (T\u00e4ckstr\u00f6m et al., 2012; Naseem et al., 2012; Zhang and Barzilay, 2015).", "startOffset": 96, "endOffset": 167}, {"referenceID": 38, "context": "Subsequent work considered syntactic similarities between languages for better feature transfer (T\u00e4ckstr\u00f6m et al., 2012; Naseem et al., 2012; Zhang and Barzilay, 2015).", "startOffset": 96, "endOffset": 167}, {"referenceID": 6, "context": ", 2005; Cohen and Smith, 2009; Ganchev et al., 2009). Seminal work employed statistical models. McDonald et al. (2011) investigated delexicalized transfer, where word-based features are removed from a statistical model for English, so that POS and dependency label knowledge can be utilized for training a model for lowresource language.", "startOffset": 8, "endOffset": 119}, {"referenceID": 0, "context": "Recently, a line of work leverages neural network models for multi-lingual parsing (Guo et al., 2015; Duong et al., 2015; Ammar et al., 2016).", "startOffset": 83, "endOffset": 141}, {"referenceID": 27, "context": "This gives consistency in tokens, POS and dependency labels thanks to the availability of Universal Dependencies (Nivre et al., 2016).", "startOffset": 113, "endOffset": 133}, {"referenceID": 5, "context": "Neural stacking was previously used for cross-annotation (Chen et al., 2016) and crosstask (Zhang and Weiss, 2016) joint-modelling on monolingual treebanks.", "startOffset": 57, "endOffset": 76}, {"referenceID": 39, "context": ", 2016) and crosstask (Zhang and Weiss, 2016) joint-modelling on monolingual treebanks.", "startOffset": 22, "endOffset": 45}, {"referenceID": 28, "context": "Besides these three dimensions in dealing with heterogeneous text data, another popular area of research is on the topic of domain adaption, which is commonly associated with crosslingual problems (Nivre et al., 2007).", "startOffset": 197, "endOffset": 217}, {"referenceID": 25, "context": "Unsupervised rule-based approaches also offer an competitive alternative for cross-lingual dependency parsing (Naseem et al., 2010; Gillenwater et al., 2010; Gelling et al., 2012; S\u00f8gaard, 2012a,b; Mart\u0131\u0301nez Alonso et al., 2017), and recently been benchmarked for the Universal Dependencies formalism by exploiting the linguistic constraints in the Universal Dependencies to improve the robustness against error propagation and domain adaption (Mart\u0131\u0301nez Alonso et al.", "startOffset": 110, "endOffset": 228}, {"referenceID": 27, "context": "Universal Dependencies provides a set of multilingual treebanks with cross-lingually consistent dependency-based lexicalist annotations, designed to aid development and evaluation for cross-lingual systems, such as multilingual parsers (Nivre et al., 2016).", "startOffset": 236, "endOffset": 256}, {"referenceID": 16, "context": "The deviations of Singlish from English come from both the lexical and the grammatical levels (Leimgruber, 2009, 2011), which bring challenges for analysis on Singlish using English NLP tools. The former involves imported vocabularies from the first languages of the local people and the latter can be represented by a set of relatively localized features which collectively form 5 unique grammars of Singlish according to Leimgruber (2011). We find empirically that all these deviations can be accommodated by applying the existing English dependency relation definitions while ensuring consistency with the annotations in other non-English UD treebanks, which are explained with examples as follows.", "startOffset": 95, "endOffset": 441}, {"referenceID": 27, "context": "Although the \u201cdislocated\u201d (dislocated elements) relation in UD is also used for preposed elements, but it captures the ones \u201cthat do not fulfill the usual core grammatical relations of a sentence\u201d and \u201cnot for a topic-marked noun that is also the subject of the sentence\u201d (Nivre et al., 2016).", "startOffset": 272, "endOffset": 292}, {"referenceID": 17, "context": "It may be regarded as a branch of \u201cTopic-prominence\u201d but is a distinctive feature of Singlish with relatively high frequency of usage (Leimgruber, 2011).", "startOffset": 134, "endOffset": 152}, {"referenceID": 17, "context": "Inversion: Inversion in Singlish involves either keeping the subject and verb in interrogative sentences in the same order as in statements, or tag questions in polar interrogatives (Leimgruber, 2011).", "startOffset": 182, "endOffset": 200}, {"referenceID": 19, "context": "Discourse particles: Usage of clausal-final discourse particles, which originates from Hokkien and Cantonese, is one of the most typical feature of Singlish (Leimgruber, 2009, 2011; Lim, 2007).", "startOffset": 157, "endOffset": 192}, {"referenceID": 10, "context": "Sentences are reversely sorted according to the log likelihood of the sentence given by an English language model trained using the KenLM toolkit (Heafield et al., 2013)9 normalized by the sentence length, so that those most different from standard English can be chosen.", "startOffset": 146, "endOffset": 169}, {"referenceID": 12, "context": "The bi-LSTM networks with a CRF layer (bi-LSTM-CRF) have shown state-of-the-art performance by globally optimizing the tag sequence (Huang et al., 2015; Chen et al., 2016).", "startOffset": 132, "endOffset": 171}, {"referenceID": 5, "context": "The bi-LSTM networks with a CRF layer (bi-LSTM-CRF) have shown state-of-the-art performance by globally optimizing the tag sequence (Huang et al., 2015; Chen et al., 2016).", "startOffset": 132, "endOffset": 171}, {"referenceID": 5, "context": "In order to obtain automatically predicted POS tags as features for a base English dependency parser, we train a POS tagger for UD-Eng using the baseline model of Chen et al. (2016), depicted in Figure 3.", "startOffset": 163, "endOffset": 182}, {"referenceID": 5, "context": "Based on this English POS tagging model, we train a POS tagger for Singlish using the featurelevel neural stacking model of Chen et al. (2016). Both the English and Singlish models consist of an input layer, a feature layer, and an output layer.", "startOffset": 124, "endOffset": 143}, {"referenceID": 2, "context": "Input Layer: Each token is represented as a vector by concatenating a word embedding from a lookup table with a weighted average of its character embeddings given by the attention model of Bahdanau et al. (2014). Following Chen et al.", "startOffset": 189, "endOffset": 212}, {"referenceID": 2, "context": "Input Layer: Each token is represented as a vector by concatenating a word embedding from a lookup table with a weighted average of its character embeddings given by the attention model of Bahdanau et al. (2014). Following Chen et al. (2016), the input layer produces a dense representation for the current input token by concatenating its word vector and the ones for its surrounding context tokens in a window of finite size.", "startOffset": 189, "endOffset": 242}, {"referenceID": 5, "context": "Following Chen et al. (2016), we adopt bi-LSTM with peephole connections (Graves and Schmidhuber, 2005).", "startOffset": 10, "endOffset": 29}, {"referenceID": 5, "context": "We adopt the deep integration neural stacking structure presented in Chen et al. (2016). As shown in Figure 4, the distributed vector representation for the target word at the input layer of the Singlish Tagger is augmented by concatenating the emission vector produced by the English Tagger with the original word and character-based embeddings, before applying the concatenation within a context window in section 4.", "startOffset": 69, "endOffset": 88}, {"referenceID": 7, "context": "(2016) to train a 1-layer biLSTM-CRF based POS tagger on UD-Eng, using 50-dimension pre-trained SENNA word embeddings (Collobert et al., 2011).", "startOffset": 118, "endOffset": 142}, {"referenceID": 5, "context": "We use the publicly available source code15 by Chen et al. (2016) to train a 1-layer biLSTM-CRF based POS tagger on UD-Eng, using 50-dimension pre-trained SENNA word embeddings (Collobert et al.", "startOffset": 47, "endOffset": 66}, {"referenceID": 5, "context": "We use the publicly available source code15 by Chen et al. (2016) to train a 1-layer biLSTM-CRF based POS tagger on UD-Eng, using 50-dimension pre-trained SENNA word embeddings (Collobert et al., 2011). We set the hidden layer size to 300, the initial learning rate for Adagrad (Duchi et al., 2011) to 0.01, the regularization parameter \u03bb to 10\u22126, and the dropout rate to 15%. The tagger gives 94.84% accuracy on the UD-Eng test set after 24 epochs, chosen according to development tests, which is comparable to the stateof-the-art accuracy of 95.17% reported by Plank et al. (2016). We use these settings to perform 10fold jackknifing of POS tagging on the UD-Eng training set, with an average accuracy of 95.", "startOffset": 47, "endOffset": 583}, {"referenceID": 26, "context": "Similarly, we trained a POS tagger using the Singlish dependency treebank alone with pretrained word embeddings on The Singapore Component of the International Corpus of English (ICE-SIN) (Nihilani, 1992; Ooi, 1997), which consists of both spoken and written texts.", "startOffset": 188, "endOffset": 215}, {"referenceID": 29, "context": "Similarly, we trained a POS tagger using the Singlish dependency treebank alone with pretrained word embeddings on The Singapore Component of the International Corpus of English (ICE-SIN) (Nihilani, 1992; Ooi, 1997), which consists of both spoken and written texts.", "startOffset": 188, "endOffset": 215}, {"referenceID": 8, "context": "We adopt the Dozat and Manning (2017) parser17 as our base model, as displayed in Figure 5, and apply neural stacking to achieve improvements over the baseline parser.", "startOffset": 13, "endOffset": 38}, {"referenceID": 8, "context": "Following Dozat and Manning (2017), we adopt Cif-LSTM cells (Greff et al.", "startOffset": 10, "endOffset": 35}, {"referenceID": 5, "context": "Inspired by the idea of feature-level neural stacking (Chen et al., 2016; Zhang and Weiss, 2016), we concatenate the pre-trained word embedding, trainable word and tag embeddings, with the two recurrent state vectors at the last bi-LSTM layer of the English Tagger as the input vector for each target word.", "startOffset": 54, "endOffset": 96}, {"referenceID": 39, "context": "Inspired by the idea of feature-level neural stacking (Chen et al., 2016; Zhang and Weiss, 2016), we concatenate the pre-trained word embedding, trainable word and tag embeddings, with the two recurrent state vectors at the last bi-LSTM layer of the English Tagger as the input vector for each target word.", "startOffset": 54, "endOffset": 96}, {"referenceID": 8, "context": "We train an English parser on UD-Eng with the default model settings in Dozat and Manning (2017).", "startOffset": 72, "endOffset": 97}, {"referenceID": 0, "context": "90% LAS on UD-Eng reported by Ammar et al. (2016), and the main difference is caused by us not using fine-grained POS tags.", "startOffset": 30, "endOffset": 50}, {"referenceID": 0, "context": "90% LAS on UD-Eng reported by Ammar et al. (2016), and the main difference is caused by us not using fine-grained POS tags. We apply the same settings for a baseline Singlish parser. We attempt to choose a better configuration of the number of bi-LSTM layers and the hidden dimension based on the development set performance, but the default settings turn out to perform the best. Thus we stick to all default hyper-parameters in Dozat and Manning (2017) for training the Singlish parsers.", "startOffset": 30, "endOffset": 455}, {"referenceID": 30, "context": "In order to learn characteristics of distributed lexical semantics for Singlish, we compare performances of the Singlish dependency parser using several sets of pre-trained word embeddings: GloVe6B, large-scale English word embeddings18; ICE-SIN, Singlish word embeddings trained using GloVe (Pennington et al., 2014) on the ICE-SIN (Nihilani, 1992; Ooi, 1997) corpus; Giga100M, a small-scale English word embeddings trained using GloVe (Pennington et al.", "startOffset": 292, "endOffset": 317}, {"referenceID": 26, "context": ", 2014) on the ICE-SIN (Nihilani, 1992; Ooi, 1997) corpus; Giga100M, a small-scale English word embeddings trained using GloVe (Pennington et al.", "startOffset": 23, "endOffset": 50}, {"referenceID": 29, "context": ", 2014) on the ICE-SIN (Nihilani, 1992; Ooi, 1997) corpus; Giga100M, a small-scale English word embeddings trained using GloVe (Pennington et al.", "startOffset": 23, "endOffset": 50}, {"referenceID": 30, "context": ", 2014) on the ICE-SIN (Nihilani, 1992; Ooi, 1997) corpus; Giga100M, a small-scale English word embeddings trained using GloVe (Pennington et al., 2014) with the same settings on a comparable size of English data randomly selected from the English Gigaword Fifth Edition for a fair comparison with ICE-SIN embeddings.", "startOffset": 127, "endOffset": 152}], "year": 2017, "abstractText": "Singlish can be interesting to the ACL community both linguistically as a major creole based on English, and computationally for information extraction and sentiment analysis of regional social media. We investigate dependency parsing of Singlish by constructing a dependency treebank under the Universal Dependencies scheme, and then training a neural network model by integrating English syntactic knowledge into a state-ofthe-art parser trained on the Singlish treebank. Results show that English knowledge can lead to 25% relative error reduction, resulting in a parser of 84.47% accuracies. To the best of our knowledge, we are the first to use neural stacking to improve cross-lingual dependency parsing on low-resource languages. We make both our annotation and parser available for further research.", "creator": "LaTeX with hyperref package"}}}