{"id": "1705.06476", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2017", "title": "ParlAI: A Dialog Research Software Platform", "abstract": "We introduce ParlAI (pronounced \"par-lay\"), an open-source software platform for dialog research implemented in Python, available at", "histories": [["v1", "Thu, 18 May 2017 08:54:47 GMT  (1008kb,D)", "http://arxiv.org/abs/1705.06476v1", null], ["v2", "Fri, 9 Jun 2017 18:35:03 GMT  (1008kb,D)", "http://arxiv.org/abs/1705.06476v2", null], ["v3", "Thu, 10 Aug 2017 04:17:48 GMT  (1020kb,D)", "http://arxiv.org/abs/1705.06476v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexander h miller", "will feng", "adam fisch", "jiasen lu", "dhruv batra", "antoine bordes", "devi parikh", "jason weston"], "accepted": true, "id": "1705.06476"}, "pdf": {"name": "1705.06476.pdf", "metadata": {"source": "CRF", "title": "ParlAI: A Dialog Research Software Platform", "authors": ["Alexander H. Miller", "Will Feng", "Adam Fisch", "Jiasen Lu", "Dhruv Batra", "Antoine Bordes"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The purpose of language is to achieve communication goals, which usually involve a dialogue between two or more communicators (Kristall, 2004). Therefore, trying to solve the dialogue is a fundamental goal for researchers in the NLP community. From a machine learning perspective, building a dialogical system can also be done for various reasons, mainly because the solution involves most of the sub-goals of the field, and in many cases these sub-tasks are directly geared to the task. On the one hand, dialogue can be considered a single task (speaking is learned) and on the other hand, thousands of related tasks that require different skills, all with the same input and output format."}, {"heading": "2 Goals", "text": "The objectives of ParlAI are the following: A unified framework for the development of dialog models. ParlAI aims to standardize as far as possible input formats for dialog data sets passed on to machine learning agents in a single format and standardize evaluation frameworks and metrics. Researchers can submit their new tasks and training code for agents to the repository to share with others in order to enhance reproducibility and better facilitate follow-up of research. General dialogue with many different skills. ParlAI includes a seamless combination of real and simulated language data sets and encourages the development and evaluation of multitask models by building them as easily as individual tasks. This should reduce the revision of model design to specific data sets and encourage models that perform task transfer, an important prerequisite for a general dialog agent. Real dialogue with humans allows the development and evaluation of multitask models by making it as easy as possible to create individual model assignments."}, {"heading": "3 General Properties of ParlAI", "text": "ParlAI consists of a number of tasks and agents that can be used to solve them. All tasks in ParlAI have a unified format (API) that makes it easy to apply an agent to a task or multiple tasks at once. Tasks include both permanently installed monitored / imitated learning data sets (i.e. conversation logs), interactive (online or reinforcement learning) tasks, and real language and simulation tasks that can all be trained seamlessly. ParlAI also supports other media, such as images and text to answer visual questions (Antol et al., 2015) or visually grounded dialogs (Das et al., 2017). ParlAI automatically downloads tasks and data sets when they are used for the first time. One or more Mechanical Turkers can be embedded within an environment (task) to collect, train, or evaluate learning agents. Examples are included in the first version of Training with Pych Torch and Parliamentary Torch is included in other languages."}, {"heading": "4 Worlds, Agents and Teachers", "text": "The most important concepts (classes) in ParlAI are worlds, agents and teachers: \u2022 world - the environment. This can be from very simple, e.g. just two agents talking to each other, to much more complex, e.g. several agents in an interactive environment. \u2022 agent - an agent who can act (in particular speak) in the world. \u2022 teacher - a kind of agent who talks to the learner to teach him something, e.g. one of the tasks in Fig. 1. After defining a world and the agents in it, a main loop can be executed for training, testing or displays calling the function world. Parley () to perform a time step of the world. Sample code for displaying data is given in Fig. 6, and the output of data is in Fig. 5."}, {"heading": "5 Actions and Observations", "text": "All agents (including teachers) speak to each other in a single common format - the observation / action object (a Python dictate) is used to pass text, labels and rewards between agents; the same type of object is used when speaking (acting) or listening (observing), but with a different view (i.e. with different values in the fields); therefore, the object is returned by agent.act () and passed to agent.observe (), see Fig. 6. The message fields are as follows: \u2022 Text: a speech act. \u2022 id: the identity of the speaker. \u2022 Reward: a real reward assigned to the recipient of the message. \u2022 episode done: indicating the end of a dialogue. For monitored records, there are some additional fields that can be used: \u2022 label: a set of answers that the speaker expects to receive in response."}, {"heading": "6 Code Structure", "text": "The ParlAI code base has five main directories: \u2022 core: the primary code for the platform. \u2022 agents: contains agents who can interact with the worlds / tasks (e.g. learning models). \u2022 examples: contains examples of different main lines (display data, training and evaluation). \u2022 tasks: contains code for the various tasks available within ParlAI. \u2022 mturk: contains code for setting up Mechanical Turk as well as sample tasks from MTurk."}, {"heading": "6.1 Core", "text": "The core library contains the following files: \u2022 agents.py: defines the base class Agent for all agents implementing the Observe () and Act () methods, the teacher class that also outputs metrics, and MultiTaskTeacher for multi-task training. \u2022 dialog teacher.py: the base class for fixed-protocol dialogs. \u2022 dict.py: code for building language dictionaries. \u2022 metrics.py: calculates exact matches, F1 and ranking metrics for evaluation. \u2022 params.py: uses Argparse to interpret command line arguments for ParlAI. \u2022 worlds.py: defines the base class World, DialogPartnerWorld for two speakers, MultiAgentDialogWorld for more than two and two containers that can include a selected environment: BatchWorld for batch training and HogwildWorld for training across multiple threads."}, {"heading": "6.2 Agents", "text": "The agents directory contains machine learning agents. Currently available in this directory: \u2022 drqa: an attentive LSTM model DrQA (Chen et al., 2017) implemented in PyTorch that achieves competitive results on SQuAD (Rajpurkar et al., 2016) among other datasets. \u2022 memnn: Code for an end-to-end storage network (Sukhbaatar et al., 2015) in Lua Torch. \u2022 remote agent: base class for all agents connecting via ZeroMQ. \u2022 ir baseline: simple information retrieval (IR) baseline that scores candidate response with TFIDF-weighted matching. \u2022 repeat label: basic class for simply repeating all data sent to it (e.g. for debugging)."}, {"heading": "6.3 Examples", "text": "This directory contains examples of different main networks:. \u2022 Display data: Display data of a specific task provided on the command line. \u2022 Display model: shows the predictions of a provided model. \u2022 Evaluation model: Calculate calculation metrics for a particular model on a given task. \u2022 Memnn luatorch cpu: Training an end-to-end storage network (Sukhbaatar et al., 2015). \u2022 drqa: Training the attentive LSTM DrQA model (Chen et al., 2017). For example, you can display 10 random examples from the bAbI tasks (Weston et al., 2015): python display data.py -t babi -n 10Display multitasking bAbI and SQuAD (Rajpurkar et al., 2016) simultaneously: python display data.py -t babi, squadEvaluate an IR baseline model on Submodal Queval: Python-Python-Python-modelindt -baseline."}, {"heading": "6.4 Tasks", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "6.5 Mechanical Turk", "text": "An important part of ParlAI is its seamless integration with Mechanical Turk for data collection, training or evaluation. Human Turkers are also considered agents in ParlAI and can therefore receive and send via the same interface: using the fields of the observation / action diktat. We provide two examples in the first release: (i) qa collector: an agent who talks to Turkers to collect question and answer pairs given via a context paragraph to build a QA dataset, see Fig. 2. (ii) Model Evaluator: an agent who collects ratings from Turkers on the performance of a Bot on a given task. Running a new MTurk task involves executing a master file (such as mturk.py) and defining several task-specific parameters for the world and the agent. (s) You can ask the agent to work on the tasks to describe the collection mode."}, {"heading": "7 Demonstrative Experiment", "text": "To demonstrate ParlAI in action, we give our knowledge the results in Table 1 of the DrQA, an attentive single-task LSTM architecture and multi-task training on the SQuAD and bAbI tasks, a combination that has not been shown before with any method. At the same time, this experiment demonstrates the power of ParlAI - how easy it is to set up this experiment - and the limitations of current methods. Almost all methods that work well with SQuAD are designed to predict a phrase from the given context (they are called start and end indexes in training). Therefore, these models cannot be applied to all dialog data sets, e.g. some of the bAbI tasks include yes / no questions where yes and no do not occur in context, underscoring that researchers should not focus on a single data set. ParlAI does not offer start and end label indices because its API is only dialogue, see Figure 3. This is a deliberate choice / overlay."}, {"heading": "8 Related Software", "text": "There are many existing independent dialog data sets and training codes for individual models that work on some of them. Many are slightly different (different formats with different types of monitoring), and ParlAI is trying to unify this fragmented landscape. There are some existing software platforms that are related to each other in scope but not in specialization. OpenAI's Gym and Universe6 are toolkits for developing and comparing reinforcement learning algorithms (RL). Gym is for games like Pong or Go, and Universe is for online games and websites. None focuses on dialogue or covers the case of monitored data sets, as we do. CommAI7 is a framework that uses textual communication for the purpose of developing artificial general intelligence through incremental tasks that test increasingly complex skills, as described in (Mikolov et al., 2015). CommAI is in an RL setting and contains only synthetic data sets, rather than natural language sets, as we do here."}, {"heading": "9 Conclusion and Outlook", "text": "ParlAI is a framework that allows the research community to share existing and new tasks for dialogue, collect and evaluate agents who learn from it, and conversations between agents and people about Mechanical Turk. We hope that this tool will enable the systematic development and evaluation of dialogue agents, contribute to advancing the state of the art in dialogue, and benefit the field as a whole."}, {"heading": "Acknowledgments", "text": "We would like to thank Mike Lewis, Denis Yarats, Douwe Kiela, Michael Auli, Y-Lan Boureau, Arthur Szlam, Marc'Aurelio Ranzato, Yuandong Tian, Maximilian Nickel, Martin Raison, Myle Ott, Marco Baroni, Leon Bottou and other members of the FAIR team for the discussions that helped to set up ParlAI."}], "references": [{"title": "VQA: Visual Question Answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh."], "venue": "Proceedings of the IEEE International Conference on Computer Vision. pages 2425\u20132433.", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Semantic parsing on freebase from questionanswer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."], "venue": "EMNLP. volume 2, page 6.", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Learning end-to-end goal-oriented dialog", "author": ["Antoine Bordes", "Jason Weston."], "venue": "arXiv preprint arXiv:1605.07683 .", "citeRegEx": "Bordes and Weston.,? 2016", "shortCiteRegEx": "Bordes and Weston.", "year": 2016}, {"title": "Reading wikipedia to answer open-domain questions", "author": ["Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes."], "venue": "arXiv:1704.00051 .", "citeRegEx": "Chen et al\\.,? 2017", "shortCiteRegEx": "Chen et al\\.", "year": 2017}, {"title": "The Cambridge encyclopedia of the English language", "author": ["David Crystal."], "venue": "Ernst Klett Sprachen.", "citeRegEx": "Crystal.,? 2004", "shortCiteRegEx": "Crystal.", "year": 2004}, {"title": "Learning cooperative visual dialog agents with deep reinforcement learning", "author": ["Abhishek Das", "Satwik Kottur", "Jos\u00e9 MF Moura", "Stefan Lee", "Dhruv Batra."], "venue": "arXiv preprint arXiv:1703.06585 .", "citeRegEx": "Das et al\\.,? 2017", "shortCiteRegEx": "Das et al\\.", "year": 2017}, {"title": "Tracking the world state with recurrent entity networks", "author": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun."], "venue": "arXiv preprint arXiv:1612.03969 .", "citeRegEx": "Henaff et al\\.,? 2016", "shortCiteRegEx": "Henaff et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems. pages 1693\u20131701.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1511.02301 .", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1506.08909 .", "citeRegEx": "Lowe et al\\.,? 2015", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "A roadmap towards machine intelligence", "author": ["Tomas Mikolov", "Armand Joulin", "Marco Baroni."], "venue": "arXiv preprint arXiv:1511.08130 .", "citeRegEx": "Mikolov et al\\.,? 2015", "shortCiteRegEx": "Mikolov et al\\.", "year": 2015}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "arXiv:1606.05250 .", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Query-reduction networks for question answering", "author": ["Minjoon Seo", "Sewon Min", "Ali Farhadi", "Hannaneh Hajishirzi."], "venue": "arXiv preprint arXiv:1606.04582 .", "citeRegEx": "Seo et al\\.,? 2016", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "End-to-end memory networks. In Advances in neural information processing systems", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": null, "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merri\u00ebnboer", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv:1502.05698 .", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "The purpose of language is to accomplish communication goals, which typically involve a dialog between two or more communicators (Crystal, 2004).", "startOffset": 129, "endOffset": 144}, {"referenceID": 1, "context": "For example, methods that do not generalize beyond WebQuestions (Berant et al., 2013) because they specialize on knowledge bases only, SQuAD (Rajpurkar et al.", "startOffset": 64, "endOffset": 85}, {"referenceID": 11, "context": ", 2013) because they specialize on knowledge bases only, SQuAD (Rajpurkar et al., 2016) because they predict start and end context indices (see Sec.", "startOffset": 63, "endOffset": 87}, {"referenceID": 14, "context": "7), or bAbI (Weston et al., 2015) because they use supporting facts or make use of its simulated nature.", "startOffset": 12, "endOffset": 33}, {"referenceID": 0, "context": "images as well as text for visual question answering (Antol et al., 2015) or visually grounded dialog (Das et al.", "startOffset": 53, "endOffset": 73}, {"referenceID": 5, "context": ", 2015) or visually grounded dialog (Das et al., 2017).", "startOffset": 36, "endOffset": 54}, {"referenceID": 3, "context": "Currently available within this directory: \u2022 drqa: an attentive LSTM model DrQA (Chen et al., 2017) implemented in PyTorch that has competitive results on SQuAD (Rajpurkar et al.", "startOffset": 80, "endOffset": 99}, {"referenceID": 11, "context": ", 2017) implemented in PyTorch that has competitive results on SQuAD (Rajpurkar et al., 2016) amongst other datasets.", "startOffset": 69, "endOffset": 93}, {"referenceID": 13, "context": "\u2022 memnn: code for an end-to-end memory network (Sukhbaatar et al., 2015) in Lua Torch.", "startOffset": 47, "endOffset": 72}, {"referenceID": 13, "context": "\u2022 memnn luatorch cpu: training an end-toend memory network (Sukhbaatar et al., 2015).", "startOffset": 59, "endOffset": 84}, {"referenceID": 3, "context": "\u2022 drqa: training the attentive LSTM DrQA model of (Chen et al., 2017).", "startOffset": 50, "endOffset": 69}, {"referenceID": 14, "context": "For example, one can display 10 random examples from the bAbI tasks (Weston et al., 2015):", "startOffset": 68, "endOffset": 89}, {"referenceID": 11, "context": "Display multitasking bAbI and SQuAD (Rajpurkar et al., 2016) at the same time:", "startOffset": 36, "endOffset": 60}, {"referenceID": 11, "context": "Over 20 tasks are supported in the first release, including popular datasets such as SQuAD (Rajpurkar et al., 2016), bAbI tasks (Weston et al.", "startOffset": 91, "endOffset": 115}, {"referenceID": 14, "context": ", 2016), bAbI tasks (Weston et al., 2015), QACNN and QADailyMail (Hermann et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 7, "context": ", 2015), QACNN and QADailyMail (Hermann et al., 2015), CBT (Hill et al.", "startOffset": 31, "endOffset": 53}, {"referenceID": 8, "context": ", 2015), CBT (Hill et al., 2015), bAbI Dialog tasks (Bordes and Weston, 2016), Ubuntu (Lowe et al.", "startOffset": 13, "endOffset": 32}, {"referenceID": 2, "context": ", 2015), bAbI Dialog tasks (Bordes and Weston, 2016), Ubuntu (Lowe et al.", "startOffset": 27, "endOffset": 52}, {"referenceID": 9, "context": ", 2015), bAbI Dialog tasks (Bordes and Weston, 2016), Ubuntu (Lowe et al., 2015) and VQA (Antol et al.", "startOffset": 61, "endOffset": 80}, {"referenceID": 0, "context": ", 2015) and VQA (Antol et al., 2015).", "startOffset": 16, "endOffset": 36}, {"referenceID": 3, "context": "5 EM, see (Chen et al., 2017), which is still in the range of many existing well performing methods, see https://stanford-qa.", "startOffset": 10, "endOffset": 29}, {"referenceID": 12, "context": "Overall, while DrQA can solve some of the bAbI tasks and performs well on SQuAD, it does not match the best performing methods on bAbI (Seo et al., 2016; Henaff et al., 2016), and multitasking does not help.", "startOffset": 135, "endOffset": 174}, {"referenceID": 6, "context": "Overall, while DrQA can solve some of the bAbI tasks and performs well on SQuAD, it does not match the best performing methods on bAbI (Seo et al., 2016; Henaff et al., 2016), and multitasking does not help.", "startOffset": 135, "endOffset": 174}, {"referenceID": 10, "context": "CommAI7 is a framework that uses textual communication for the goal of developing artificial general intelligence through incremental tasks that test increasingly more complex skills, as described in (Mikolov et al., 2015).", "startOffset": 200, "endOffset": 222}], "year": 2017, "abstractText": "We introduce ParlAI (pronounced \u201cparlay\u201d), an open-source software platform for dialog research implemented in Python, available at http://parl.ai. Its goal is to provide a unified framework for training and testing of dialog models, including multitask training, and integration of Amazon Mechanical Turk for data collection, human evaluation, and online/reinforcement learning. Over 20 tasks are supported in the first release, including popular datasets such as SQuAD, bAbI tasks, MCTest, WikiQA, QACNN, QADailyMail, CBT, bAbI Dialog, Ubuntu, OpenSubtitles and VQA. Included are examples of training neural models with PyTorch and Lua Torch, including both batch and hogwild training of memory networks and attentive LSTMs.", "creator": "LaTeX with hyperref package"}}}