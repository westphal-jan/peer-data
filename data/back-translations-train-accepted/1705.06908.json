{"id": "1705.06908", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Unbiased estimates for linear regression via volume sampling", "abstract": "Given a full rank matrix $X$ with more columns than rows consider the task of estimating the pseudo inverse $X^+$ based on the pseudo inverse of a sampled subset of columns (of size at least the number of rows). We show that this is possible if the subset of columns is chosen proportional to the squared volume spanned by the rows of the chosen submatrix (ie, volume sampling). The resulting estimator is unbiased and surprisingly the covariance of the estimator also has a closed form: It equals a specific factor times $X^+X^{+\\top}$.", "histories": [["v1", "Fri, 19 May 2017 09:43:41 GMT  (23kb)", "http://arxiv.org/abs/1705.06908v1", null], ["v2", "Wed, 7 Jun 2017 22:42:47 GMT  (23kb)", "http://arxiv.org/abs/1705.06908v2", null], ["v3", "Tue, 13 Jun 2017 22:36:50 GMT  (23kb)", "http://arxiv.org/abs/1705.06908v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["michal derezinski", "manfred k warmuth"], "accepted": true, "id": "1705.06908"}, "pdf": {"name": "1705.06908.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["mderezin@ucsc.edu", "manfred@ucsc.edu"], "sections": [{"heading": null, "text": "The question we have asked ourselves is whether we are going to make a fundamental connection between linear minimum squares and volume based on all the column captions we apply based on these new formulas. We are using our methods to get an algorithm for the volume that is faster than the state type and for the total loss of the estimated minimum squares. We are assuming that a basic connection is made between linear minimum squares and volume. We are using our methods to get an algorithm for volume sampling that is responsible for the total loss of the estimated minimum squares."}, {"heading": "2 Related Work", "text": "Volume sampling is an extension of a determinantal point process [13], which has been given a lot of attention in the literature with many applications to machine learning, including recommendation systems [9] and clustering [11]. Many exact and approximate methods for efficiently generating samples from this distribution have been proposed [5, 12], making it a useful tool in designing randomized algorithms. Most of these methods focus on sampling s \u2264 d elements. In this paper, we examine volume sampling sampling sets of size d, which have been proposed in [1] and with applications in graph theory, linear regression, matrix approximation and more. The only known polynomial time algorithms for the size s > d volume sampling was recently proposed in [14] with the time complexity O (n4s) and we offer a new algorithm with runtimeO (n \u2212 s + d) to factor selectivity of at least one of the 2.n4s problem."}, {"heading": "3 Unbiased estimators", "text": "Let n be an integer dimension. For each subset S '1.. n} of size s, we obtain a matrix formula FS = | |. Our goal is to sample a series S of size s using a sampling method.We represent the sample using a directed acyclic graph (dag), where a single root node corresponds to the complete set [1.. n]. Starting from the root, we walk along the edges of the graph, removing iteratively elements from the set S. Specifically, we consider a dag with levels s = n, n \u2212 1,..., which contains (n s) nodes for the sets S' {1.. n} of size s. Each node S on level s > d has s directed edges on the nodes S \u2212 n."}, {"heading": "3.1 Volume sampling", "text": "The following episode shows the ability shown above to calculate the normalization constant for this distribution. If s = d, the Korollars provide a novel minimum proof for the Cauchy-Binet formula: \"S:\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s"}, {"heading": "3.2 Expectation formulas for volume sampling", "text": "The expectation formulas for two choices of FS are proven in the next two theorems. By Lemma 1, it is sufficient to show FS = i S P (i | S) FS \u2212 i for volume sampling. Let's introduce a bit more notation first. Remember that XS is the sub-matrix of columns indexed by S {1.. n} (see Figure 1). Consider a version of X in which all but the columns of S are null. This matrix equals XIS to an n-dimensional diagonal matrix with (IS) ii = 1 if i S and 0 otherwise.Theorem 3 Let X Rd be a wide full matrix."}, {"heading": "If volume sampling does not have full support then the matrix equality \u201c=\u201d is replaced by the positive-definite inequality \u201c \u201d.", "text": "Sampling Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling-Sampling"}, {"heading": "4.1 Proof of Theorem 5", "text": "We use the following Lemma 1-out loss for linear regression [3]: Lemma 9 Let w * \u2212 i represent the least quadratic solution to the problem (X \u2212 i, y \u2212 i). Then we have the formula L (w *) = L (w * \u2212 i) \u2212 x i (XX i) (w i), where the number i (w \u2212 i) def = (x i w \u2212 yi) 2.In particular, if X + 1 has columns and X \u2212 i is a full d dmatrix, thenL (w i) = x d i (w i) and Lemma 9 results in the following formula: det (X X) L (w y y y 2)."}, {"heading": "5 Efficient algorithm for volume sampling", "text": "In this section, we propose an algorithm that performs an exact inversion of the conversion modalities that we are able to realize the conversion modalities for all the desired quantities. (...) This addresses the question that occurs in time O (nd3). (...) We propose a new method that uses our techniques to achieve the time complexity O (n \u2212 s + d) nd. (n \u2212 s) nd, a direct improvement by a factor of at least n2. Our algorithm also offers an improvement for certain regimes. (...) If n = o (d2), then our algorithm runs in time O (nd3) = o (nd3), faster than the method that we have proposed."}, {"heading": "6 Conclusions", "text": "We developed exact formulas for E [(XIS) +) and E [(XIS) +) 2] when the subset S of the s column indices is sampled in proportion to the volume det (XSX S). The formulas apply to each specified size s. These new expectation formulas imply that the solution w \u0445 S is unbiased for a volume sampled partial problem of a linear regression problem. We also gave a formula that relates the loss of the partial problem to the optimal loss (i.e. E (L (w) S) = (d + 1) L (w)). However, this result applies only to the sample size s = d. It is an open problem to have such an exact expectation formula for s > d.A natural algorithm is to draw k samples of size d and return w (d), where S1: k = i Si."}], "references": [{"title": "Faster subset selection for matrices and applications", "author": ["Haim Avron", "Christos Boutsidis"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Rich coresets for constrained linear regression", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "CoRR, abs/1202.3505,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "Gabor Lugosi"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L. Clarkson", "David P. Woodruff"], "venue": "In Proceedings of the Forty-fifth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Efficient volume sampling for row/column subset selection", "author": ["Amit Deshpande", "Luis Rademacher"], "venue": "In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Matrix approximation and projective clustering via volume sampling", "author": ["Amit Deshpande", "Luis Rademacher", "Santosh Vempala", "GrantWang"], "venue": "In Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithm,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Fast approximation of matrix coherence and statistical leverage", "author": ["Petros Drineas", "Malik Magdon-Ismail", "Michael W. Mahoney", "David P. Woodruff"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Klimko, editors. Theory of optimal experiments. Probability and mathematical statistics", "author": ["Valeri Vadimovich Fedorov", "W.J. Studden", "E.M"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1972}, {"title": "Bayesian low-rank determinantal point processes", "author": ["Mike Gartrell", "Ulrich Paquet", "Noam Koenigstein"], "venue": "In Proceedings of the 10th ACM Conference on Recommender Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Optimal column-based low-rank matrix reconstruction", "author": ["Venkatesan Guruswami", "Ali Kemal Sinop"], "venue": "In Proceedings of the Twenty-third Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Fast determinantal point process sampling with application to clustering", "author": ["Byungkon Kang"], "venue": "In Proceedings of the 26th International Conference on Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "k-DPPs: Fixed-Size Determinantal Point Processes", "author": ["Alex Kulesza", "Ben Taskar"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Determinantal Point Processes for Machine Learning", "author": ["Alex Kulesza", "Ben Taskar"], "venue": "Now Publishers Inc.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Column Subset Selection via Polynomial Time Dual Volume Sampling", "author": ["C. Li", "S. Jegelka", "S. Sra"], "venue": "ArXiv e-prints,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Randomized algorithms for matrices and data", "author": ["Michael W. Mahoney"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Pool-based active learning in approximate linear regression", "author": ["Masashi Sugiyama", "Shinichi Nakajima"], "venue": "Mach. Learn.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Linear regression without correspondence. personal communication, 2017", "author": ["Xiaorui Sun", "Daniel Hsu"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2017}], "referenceMentions": [{"referenceID": 1, "context": "This question is a version of the \u201cminimal coresets\u201d open problem posed in [2].", "startOffset": 75, "endOffset": 78}, {"referenceID": 6, "context": "Also any iid sampling of S (such as the commonly used leverage scores [7]) requires at least \u03a9(d log d) examples to achieve a finite factor.", "startOffset": 70, "endOffset": 73}, {"referenceID": 12, "context": "Volume sampling is an extension of a determinantal point process [13], which has been given a lot of attention in the literature with many applications to machine learning, including recommendation systems [9] and clustering [11].", "startOffset": 65, "endOffset": 69}, {"referenceID": 8, "context": "Volume sampling is an extension of a determinantal point process [13], which has been given a lot of attention in the literature with many applications to machine learning, including recommendation systems [9] and clustering [11].", "startOffset": 206, "endOffset": 209}, {"referenceID": 10, "context": "Volume sampling is an extension of a determinantal point process [13], which has been given a lot of attention in the literature with many applications to machine learning, including recommendation systems [9] and clustering [11].", "startOffset": 225, "endOffset": 229}, {"referenceID": 4, "context": "Many exact and approximate methods for efficiently generating samples from this distribution have been proposed [5, 12], making it a useful tool in the design of randomized algorithms.", "startOffset": 112, "endOffset": 119}, {"referenceID": 11, "context": "Many exact and approximate methods for efficiently generating samples from this distribution have been proposed [5, 12], making it a useful tool in the design of randomized algorithms.", "startOffset": 112, "endOffset": 119}, {"referenceID": 0, "context": "In this paper, we study volume sampling sets of size s \u2265 d, which has been proposed in [1] and motivated with applications in graph theory, linear regression, matrix approximation and more.", "startOffset": 87, "endOffset": 90}, {"referenceID": 13, "context": "The only known polynomial time algorithm for size s > d volume sampling was recently proposed in [14] with time complexityO(ns).", "startOffset": 97, "endOffset": 101}, {"referenceID": 7, "context": "The problem of selecting a subset of input vectors for solving a linear regression task has been extensively studied in statistics literature under the terms optimal design [8] and pool-based active learning [16].", "startOffset": 173, "endOffset": 176}, {"referenceID": 15, "context": "The problem of selecting a subset of input vectors for solving a linear regression task has been extensively studied in statistics literature under the terms optimal design [8] and pool-based active learning [16].", "startOffset": 208, "endOffset": 212}, {"referenceID": 14, "context": "A related task has been explored in the field of computational geometry, where efficient algorithms are sought for approximately solving linear regression and matrix approximation [15, 4, 2].", "startOffset": 180, "endOffset": 190}, {"referenceID": 3, "context": "A related task has been explored in the field of computational geometry, where efficient algorithms are sought for approximately solving linear regression and matrix approximation [15, 4, 2].", "startOffset": 180, "endOffset": 190}, {"referenceID": 1, "context": "A related task has been explored in the field of computational geometry, where efficient algorithms are sought for approximately solving linear regression and matrix approximation [15, 4, 2].", "startOffset": 180, "endOffset": 190}, {"referenceID": 5, "context": "In this line of work, volume sampling size s \u2264 d has been used by [6, 10] for matrix approximation.", "startOffset": 66, "endOffset": 73}, {"referenceID": 9, "context": "In this line of work, volume sampling size s \u2264 d has been used by [6, 10] for matrix approximation.", "startOffset": 66, "endOffset": 73}, {"referenceID": 6, "context": "Another common sampling technique is based on statistical leverage scores [7], which have been effectively used for the task of linear regression.", "startOffset": 74, "endOffset": 77}, {"referenceID": 0, "context": "However, [1] discuss many problems where controlling the pseudo-inverse of a submatrix is essential.", "startOffset": 9, "endOffset": 12}, {"referenceID": 0, "context": "This norm formula has been shown in [1], with numerous applications.", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "can findw S for which L(w \u2217 S) is only a multiplicative factor away from L(w ) (independent of the number of input vectors n)? This question was posed as an open problem by [2].", "startOffset": 173, "endOffset": 176}, {"referenceID": 2, "context": "We use the following lemma regarding the leave-one-out loss for linear regression [3]:", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "This addresses the question posed by [1], asking for a polynomial-time algorithm for the case when s > d.", "startOffset": 37, "endOffset": 40}, {"referenceID": 4, "context": "[5, 10] gave an algorithm for the case when s = d, which runs in time O(nd).", "startOffset": 0, "endOffset": 7}, {"referenceID": 9, "context": "[5, 10] gave an algorithm for the case when s = d, which runs in time O(nd).", "startOffset": 0, "endOffset": 7}, {"referenceID": 13, "context": "Recently, [14] offered an algorithm for arbitrary s, which has complexity O(ns).", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "We propose a new method, which uses our techniques to achieve the time complexity O((n \u2212 s+ d)nd), a direct improvement over [14] by a factor of at least n.", "startOffset": 125, "endOffset": 129}, {"referenceID": 4, "context": "Namely, when n = o(d), then our algorithm runs in time O(nd) = o(nd), faster than the method proposed by [5].", "startOffset": 105, "endOffset": 108}, {"referenceID": 16, "context": "Our loss expectation formula has already been applied by [17] to the task of linear regression without correspondence.", "startOffset": 57, "endOffset": 61}], "year": 2017, "abstractText": "Given a full rank matrix X with more columns than rows consider the task of estimating the pseudo inverseX based on the pseudo inverse of a sampled subset of columns (of size at least the number of rows). We show that this is possible if the subset of columns is chosen proportional to the squared volume spanned by the rows of the chosen submatrix (ie, volume sampling). The resulting estimator is unbiased and surprisingly the covariance of the estimator also has a closed form: It equals a specific factor timesXX. Pseudo inverse plays an important part in solving the linear least squares problem, where we try to predict a label for each column of X. We assume labels are expensive and we are only given the labels for the small subset of columns we sample fromX. Using our methods we show that the weight vector of the solution for the sub problem is an unbiased estimator of the optimal solution for the whole problem based on all column labels. We believe that these new formulas establish a fundamental connection between linear least squares and volume sampling. We use our methods to obtain an algorithm for volume sampling that is faster than state-of-the-art and for obtaining bounds for the total loss of the estimated least-squares solution on all labeled columns.", "creator": "LaTeX with hyperref package"}}}