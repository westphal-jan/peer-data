{"id": "1705.07041", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Posterior sampling for reinforcement learning: worst-case regret bounds", "abstract": "We present an algorithm based on posterior sampling (aka Thompson sampling) that achieves near-optimal worst-case regret bounds when the underlying Markov Decision Process (MDP) is communicating with a finite, though unknown, diameter. Our main result is a high probability regret upper bound of $\\tilde{O}(D\\sqrt{SAT})$ for any communicating MDP with $S$ states, $A$ actions and diameter $D$, when $T\\ge S^5A$. Here, regret compares the total reward achieved by the algorithm to the total expected reward of an optimal infinite-horizon undiscounted average reward policy, in time horizon $T$. This result improves over the best previously known upper bound of $\\tilde{O}(DS\\sqrt{AT})$ achieved by any algorithm in this setting, and matches the dependence on $S$ in the established lower bound of $\\Omega(\\sqrt{DSAT})$ for this problem. Our techniques involve proving some novel results about the anti-concentration of Dirichlet distribution, which may be of independent interest.", "histories": [["v1", "Fri, 19 May 2017 15:10:21 GMT  (48kb)", "http://arxiv.org/abs/1705.07041v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shipra agrawal", "randy jia"], "accepted": true, "id": "1705.07041"}, "pdf": {"name": "1705.07041.pdf", "metadata": {"source": "CRF", "title": "Posterior sampling for reinforcement learning: worst-case regret bounds", "authors": ["Shipra Agrawal"], "emails": ["sa3305@columbia.edu", "rqj2000@columbia.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.07 041v 1 [cs.L G] 19 May 2eter. Our main result is a high probability that remorse compares the upper limit of the O value (D \u221a SAT) for all communicating MDP with S states, A actions, and diameter D, if T \u2265 S5A. In this context, remorse compares the total reward achieved by the algorithm with the expected total reward of an optimal, undiscounted average reward policy over the time horizon T. This result improves on the best known upper limit of the O value (DS \u221a AT) achieved by any algorithm in this environment, and corresponds to the dependence on S in the established lower limit of the O value (\u221a DSAT) for this problem. Our techniques include the detection of some new results on the anticoncentration of the Dirichlet distribution, which may be of independent interest."}, {"heading": "1 Introduction", "text": "This requires the algorithm to manage the problem of learning and planning in sequential decision-making systems when the underlying system dynamics are unknown and may need to be learned by trying out different options and observing their results. A typical model for the problem of sequential decision-making is a Markov Decision Process (MDP) that proceeds in discrete increments of time. Each time the system is in any state, the decision maker can take all available measures to obtain a (possibly stochastic) reward. The system then passes to the next state according to a fixed state transitional distribution. The reward and the next state depend on the current state and action, but are independent of all previous states and actions. In amplifying learning problems, the underlying state transitional distributions and / or reward distributions are unknown, and must be learned using the observed rewards and state transitions, while the goal is maximizing the cumulative reward."}, {"heading": "2 Preliminaries and Problem Definition", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Markov Decision Process (MDP)", "text": "We consider a Markov decision-making process M defined by tuples {S, A, P, r, s1}, where S is a finite state space of magnitude S, A is a finite action space of magnitude A, P: S \u00b7 A \u2192 S is the transition model, r: S \u00b7 A \u2192 [0, 1] is the reward function, and s1 is the initial state. When an action A is taken in a state S, a reward is generated, and the system transition to the next state S is probable Ps, a (s), where a (s), S Ps, a (s) \"communicating\" MDPs with finite \"diameter.\" Below we define communicating MDPs and remember some useful known results for such MDPs. Definition 1 (Politics): A deterministic policy sp: S \u2192 A is an illustration of state space to action."}, {"heading": "2.2 The reinforcement learning problem", "text": "The reinforcement learning problem occurs in rounds t = 1,.., T. The learning agent assumes a state s1 at round t = 1. At the beginning of each round t and observes the reward state rst, where r and P are the reward function or transition model for a communicating MDP M with diameterD. (The assumption of known and deterministic rewards was made here only for the sake of simplicity of presentation, since the unknown transition model is the main cause of difficulties with this problem.) Our algorithm and results can be extended to limited stochastic rewards when using standard Thompson sampling in MAB, e.g. the use of the unknown transition model in this problem. (Our algorithm and results can be extended to limited stochastic rewards when using standard Thompson sampling in MAB.) The learning agent starts at a state s1 at round t = 1,. (The learning agent starts at round t = 1)."}, {"heading": "3 Algorithm Description", "text": "It is not that we index the number of time steps in these countries where the next state i, i.e., a transition from state s to i, was observed. We index the states from 1 to S so that the states from 1 to S, i.e. that the states from 1 to S, one (i) s, one (i) s, one (i) s, one (i) s, one (i) s, one (i) s, one (i) s, one (i) s, one (i) s, one (i) s, one (i) s, one (i) s, one (i) s, one (i) s, one (i) s, one (i) s, one (i) s, one (i) s, one)."}, {"heading": "4 Regret Bounds", "text": "Theorem 1. For all communicating MDP M with S states, A actions and diameter D, with probability 1 \u2212 \u03c1. the regret of algorithm 1 in time T \u2265 CDA log2 (T / \u03c1) is limited as follows: R (T, M) \u2264 O (D \u221a SAT + DS7 / 4A3 / 4 + DS5 / 2A), where C is an absolute constant. For T \u2265 S5A this implies a regret limit of R (T, M) \u2264 O (D \u221a SAT). Here O \u2212 hides logarithmic factors in S, A, T, \u03c1 and absolute constants. The rest of this section is devoted to the proof of the above theorem. Here we provide a sketch of the evidence and discuss some of the key lemmas, all missing details are provided in the supplementary material."}, {"heading": "4.1 Proof of Theorem 1", "text": "As defined in paragraph 2, the regret is R (T, M) by R (T, M) = K (T) = K (T) = K (S) = K (S) = K (S) = K (S) = K (S) = K (S) = K (S) = K (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s) s (S) s) s (S) s) s (S) s) s (S) s) s (S) s (S) s) s (S) s (S) s (S) s) s (S) s (S) s) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s) s (S) s (S) s (S) s) s (S) s (S) s) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S (S) s) s (S (S) s (S) s (S) s (S (S) s) s (S (S) s (S (S) s) s (S (s) s) s (S (S (S) s) s (S, S (S (S (S (S) s) s) s (S, S (S (S (S (S) s) s, S (S (S (s) s) s, S (S (S, S (S (s) s), S (S (S (S (S, S (S) s), S (S (S (s), S (S (s), S (S (S (S) s) s, S (S (S, S (S (S) s) s), S (S (S (S, S, S (s) a) s) s (S (S (S, S (s) a) s) s (S ("}, {"heading": "4.2 Main lemmas", "text": "Lemma 4.1. Suppose T \u2265 CDA log2 (T / \u043c) for a sufficiently large constant C. Then, with probability 1 \u2212 \u03c1, for each epoch k, the diameter of the MDP M-k is limited by 2D. Lemma 4.2. With probability 1 \u2212 \u03c1, for each epoch k, the optimal gain from the extended MDP M-k is satisfactory. (D log2 (T / \u03c1). SA T), where the optimal gain from the MDP M and D is the diameter. Proof. Let us refer to the bias vector for an optimal policy of the MDP M (let us refer to Lemma 2.1 in the preliminary section). Since it is a fixed (albeit unknown) vector with | hi \u2212 hj | \u2264 D, we can apply Lemma 4.3 to reference the bias vector for an optimal lemon (2.1) in the preliminary section of the MDP."}, {"heading": "5 Conclusions", "text": "We presented an algorithm inspired by posterior sampling that, at worst, achieves near-optimal remorse limits for the affirmation problem in communicating MDPs in a non-episodic, non-discounted average reward situation. Our algorithm can be considered a more efficient randomized version of the UCRL2 algorithm by Jaksch et al. [2010], with randomization using posterior sampling being the key to improving the remorse bounce provided by our algorithm. Our analysis shows that posterior sampling provides the right level of uncertainty in the samples so that an optimistic policy can be achieved without excessive overestimation."}, {"heading": "A Anti-concentration of Dirichlet distribution", "text": "We take the following general result based on the Dirichlet distributions used to prove optimism. (PropositionA.1. Consider a random vector p generated from the Dirichlet distribution with parameters (mp + 1,..., mp + S) where any fixed h (0, D] S, with probability p (1 / S) \u2212 p, (p + p) Th. + p \u00b2 S), i < SD (2 / p), i (2 / p), i (2 / p), i (p \u00b2 S), i (p \u00b2 S), i (p \u00b2 S), i), i (hi \u2212 H \u00b2 i), i (H \u00b2 i), i (H \u00b2 i), i (H \u00b2 i), i (H \u00b2 i), i (H \u00b2 i), i (H \u00b2 i)."}, {"heading": "B Optimism", "text": "In this section we prove the following lemmas.Lemma 4.2. With probability 1 \u2212 Q Q, for each epoch Q, the optimal gain \u03bb k of extended MDP-M-k satisfaction. (This is the optimal amplification of MDP-M satisfaction.) Let us leave h to obtain the bias vector for optimal political satisfaction. (There is a sample vectorQj 2.1 in this section.) Since h is a fixed (though unknown) vector with | hi \u2212 hj, we can use Lemma 4.3 to obtain such satisfaction for all s. (There is a sample vectorQj 2.1 in this section.) ks, one for some j satisfaction {1,..., such a satisfaction (Qj, ks, a) Th satisfaction."}, {"heading": "C Deviation bounds", "text": "(D) D (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n n (S) n n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n (S) n n (S) n (S) n (S) n (S) n n (S) n (S) n (S) n (S) n (S) n (S) n) n (S) n (S) n (S) n) n (S) n (S) n) n (S) n (S) n) n (S) n (S) n) n (S) n (S) n (S) n (S) n (S) n (S) n) n (S) n) n (S) n) n (S) n (S) n) n (S) n) n (S) n (S) n (S) n) n (S) n) n) n (S n) n (S n) n) n (S (S) n) n (S n) n) n n (S n) n (S n) n (S n) n) n n (S n) n (S n) n (S n) n (S n) n (S n) n n (S n) n (S n) n (S n (S n) n) n (S n) n (S n) n (S n (S n) n) n (S n) n (S n) n) n n (S n (S n) n (S n) n) n"}, {"heading": "D Diameter of the extended MDP M\u0303k", "text": "Let's assume there's an example vector Qj, ks, ks, s, s, c, s, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c,"}, {"heading": "E Useful deviation inequalities", "text": "Fact 3 (Bernstein's Inequality, from Seldin et al. [2012] Lem 11 / Cor 12]. Let Z1, Z2,..., Zn be a limited differential sequence, so that Zi | \u2264 K and E [Zi | Fi \u2212 1] = 0. Define Mn = 1 Zi and Vn = 1 E [(Zi) 2 | Fi \u2212 1]. For each c > 1 and 2% (0, 1), with a probability of more than 1 \u2212 1, if the probability is higher than 1 \u2212 2, if the probability is higher, that the probability is higher, that the probability is higher than the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that probability is higher, that probability is higher, that probability is higher, that probability is higher, that probability is higher, that probability is higher, that probability is higher, that probability is higher, that probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher, that the probability is higher is higher, that the probability is higher, that is higher, that is higher, that the probability is higher, the higher, that is higher, the higher"}], "references": [{"title": "Bayesian optimal control of smoothly parameterized systems: The lazy posterior sampling algorithm", "author": ["Yasin Abbasi-Yadkori", "Csaba Szepesvari"], "venue": "arXiv preprint arXiv:1406.3926,", "citeRegEx": "Abbasi.Yadkori and Szepesvari.,? \\Q2014\\E", "shortCiteRegEx": "Abbasi.Yadkori and Szepesvari.", "year": 2014}, {"title": "Handbook of mathematical functions: with formulas, graphs, and mathematical tables, volume 55", "author": ["Milton Abramowitz", "Irene A Stegun"], "venue": "Courier Corporation,", "citeRegEx": "Abramowitz and Stegun.,? \\Q1964\\E", "shortCiteRegEx": "Abramowitz and Stegun.", "year": 1964}, {"title": "Analysis of Thompson Sampling for the Multi-armed Bandit Problem", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Agrawal and Goyal.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2012}, {"title": "Thompson sampling for contextual bandits with linear payoffs", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Agrawal and Goyal.,? \\Q2013\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2013}, {"title": "Further Optimal Regret Bounds for Thompson Sampling", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In AISTATS,", "citeRegEx": "Agrawal and Goyal.,? \\Q2013\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2013}, {"title": "A Bayesian sampling approach to exploration in reinforcement learning", "author": ["John Asmuth", "Lihong Li", "Michael L Littman", "Ali Nouri", "David Wingate"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Asmuth et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Asmuth et al\\.", "year": 2009}, {"title": "Minimax regret bounds for reinforcement learning", "author": ["Mohammad Gheshlaghi Azar", "Ian Osband", "R\u00e9mi Munos"], "venue": "arXiv preprint arXiv:1703.05449,", "citeRegEx": "Azar et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2017}, {"title": "REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs", "author": ["Peter L Bartlett", "Ambuj Tewari"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Bartlett and Tewari.,? \\Q2009\\E", "shortCiteRegEx": "Bartlett and Tewari.", "year": 2009}, {"title": "R-max-a general polynomial time algorithm for nearoptimal reinforcement learning", "author": ["Ronen I Brafman", "Moshe Tennenholtz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Brafman and Tennenholtz.,? \\Q2002\\E", "shortCiteRegEx": "Brafman and Tennenholtz.", "year": 2002}, {"title": "Prior-free and prior-dependent regret bounds for Thompson sampling", "author": ["S\u00e9bastien Bubeck", "Che-Yu Liu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bubeck and Liu.,? \\Q2013\\E", "shortCiteRegEx": "Bubeck and Liu.", "year": 2013}, {"title": "Optimal adaptive policies for Markov decision processes", "author": ["Apostolos N Burnetas", "Michael N Katehakis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Burnetas and Katehakis.,? \\Q1997\\E", "shortCiteRegEx": "Burnetas and Katehakis.", "year": 1997}, {"title": "An empirical evaluation of Thompson sampling", "author": ["Olivier Chapelle", "Lihong Li"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Chapelle and Li.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle and Li.", "year": 2011}, {"title": "Sample complexity of episodic fixed-horizon reinforcement learning", "author": ["Christoph Dann", "Emma Brunskill"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dann and Brunskill.,? \\Q2015\\E", "shortCiteRegEx": "Dann and Brunskill.", "year": 2015}, {"title": "Introduction to probability", "author": ["Charles Miller Grinstead", "James Laurie Snell"], "venue": "American Mathematical Soc.,", "citeRegEx": "Grinstead and Snell.,? \\Q2012\\E", "shortCiteRegEx": "Grinstead and Snell.", "year": 2012}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "On the sample complexity of reinforcement learning", "author": ["ShamMachandranath Kakade"], "venue": "PhD thesis,", "citeRegEx": "Kakade,? \\Q2003\\E", "shortCiteRegEx": "Kakade", "year": 2003}, {"title": "Thompson Sampling: An Optimal Finite Time Analysis", "author": ["Emilie Kaufmann", "Nathaniel Korda", "R\u00e9mi Munos"], "venue": "In International Conference on Algorithmic Learning Theory (ALT),", "citeRegEx": "Kaufmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "Finite-sample convergence rates for Q-learning and indirect algorithms", "author": ["Michael J Kearns", "Satinder P Singh"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Kearns and Singh.,? \\Q1999\\E", "shortCiteRegEx": "Kearns and Singh.", "year": 1999}, {"title": "Multi-armed bandits in metric spaces", "author": ["Robert Kleinberg", "Aleksandrs Slivkins", "Eli Upfal"], "venue": "In Proceedings of the fortieth annual ACM symposium on Theory of computing,", "citeRegEx": "Kleinberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2008}, {"title": "Why is posterior sampling better than optimism for reinforcement learning", "author": ["Ian Osband", "Benjamin Van Roy"], "venue": "arXiv preprint arXiv:1607.00215,", "citeRegEx": "Osband and Roy.,? \\Q2016\\E", "shortCiteRegEx": "Osband and Roy.", "year": 2016}, {"title": "More) efficient reinforcement learning via posterior sampling", "author": ["Ian Osband", "Dan Russo", "Benjamin Van Roy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Osband et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2013}, {"title": "Generalization and exploration via randomized value functions", "author": ["Ian Osband", "Benjamin Van Roy", "Zheng Wen"], "venue": "arXiv preprint arXiv:1402.0635,", "citeRegEx": "Osband et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2014}, {"title": "Markov decision processes: discrete stochastic dynamic programming", "author": ["Martin L Puterman"], "venue": null, "citeRegEx": "Puterman.,? \\Q2014\\E", "shortCiteRegEx": "Puterman.", "year": 2014}, {"title": "Learning to Optimize Via Posterior Sampling", "author": ["Daniel Russo", "Benjamin Van Roy"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Russo and Roy.,? \\Q2014\\E", "shortCiteRegEx": "Russo and Roy.", "year": 2014}, {"title": "An Information-Theoretic Analysis of Thompson Sampling", "author": ["Daniel Russo", "Benjamin Van Roy"], "venue": "Journal of Machine Learning Research (to appear),", "citeRegEx": "Russo and Roy.,? \\Q2015\\E", "shortCiteRegEx": "Russo and Roy.", "year": 2015}, {"title": "PAC-Bayesian inequalities for martingales", "author": ["Yevgeny Seldin", "Fran\u00e7ois Laviolette", "Nicolo Cesa-Bianchi", "John Shawe-Taylor", "Peter Auer"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Seldin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Seldin et al\\.", "year": 2012}, {"title": "An improvement of convergence rate estimates in the Lyapunov theorem", "author": ["I.G. Shevtsova"], "venue": null, "citeRegEx": "Shevtsova.,? \\Q2010\\E", "shortCiteRegEx": "Shevtsova.", "year": 2010}, {"title": "A theoretical analysis of model-based interval estimation", "author": ["Alexander L Strehl", "Michael L Littman"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Strehl and Littman.,? \\Q2005\\E", "shortCiteRegEx": "Strehl and Littman.", "year": 2005}, {"title": "An analysis of model-based interval estimation for Markov decision processes", "author": ["Alexander L Strehl", "Michael L Littman"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Strehl and Littman.,? \\Q2008\\E", "shortCiteRegEx": "Strehl and Littman.", "year": 2008}, {"title": "Optimistic linear programming gives logarithmic regret for irreducible MDPs", "author": ["Ambuj Tewari", "Peter L Bartlett"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tewari and Bartlett.,? \\Q2008\\E", "shortCiteRegEx": "Tewari and Bartlett.", "year": 2008}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William R Thompson"], "venue": null, "citeRegEx": "Thompson.,? \\Q1933\\E", "shortCiteRegEx": "Thompson.", "year": 1933}, {"title": "In Section D, we prove Lemma 4.1 bounding the diameter of extended MDP with high probability. And, in Section E we list some known results (or easy corollaries of known results) that are utilized in our proofs. A Anti-concentration of Dirichlet distribution We prove the following general result on anti-concentration", "author": ["Osband"], "venue": null, "citeRegEx": "Osband,? \\Q2014\\E", "shortCiteRegEx": "Osband", "year": 2014}, {"title": "Gaussian vs Dirichlet optimism, from Osband et al", "author": ["Lemma E"], "venue": "Let Y = PV for V \u2208 [0,", "citeRegEx": "E.3,? \\Q2014\\E", "shortCiteRegEx": "E.3", "year": 2014}], "referenceMentions": [{"referenceID": 30, "context": "More recently, posterior sampling, aka Thompson Sampling [Thompson, 1933], has emerged as another popular algorithm design principle in MAB, owing its popularity to a simple and extendible algorithmic structure, an attractive empirical performance [Chapelle and Li, 2011, Kaufmann et al.", "startOffset": 57, "endOffset": 73}, {"referenceID": 14, "context": "A finite diameter is understood to be necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al., 2010].", "startOffset": 118, "endOffset": 139}, {"referenceID": 1, "context": "ably optimal performance bounds that have been recently obtained for many variations of MAB [Agrawal and Goyal, 2012, 2013b,a, Russo and Van Roy, 2015, 2014, Bubeck and Liu, 2013]. In this approach, the algorithm maintains a Bayesian posterior distribution for the expected reward of every action; then at any given step, it generates an independent sample from each of these posteriors and takes the action with the highest sample value. We consider the reinforcement learning problem with finite states S and finite actions A in a similar regret based framework, where the total reward of the reinforcement learning algorithm is compared to the total expected reward achieved by a single benchmark policy over a time horizon T . In our setting, the benchmark policy is the infinite-horizon undiscounted average reward optimal policy for the underlying MDP, under the assumption that the MDP is communicating with (unknown) finite diameter D. The diameter D is an upper bound on the time it takes to move from any state s to any other state s using an appropriate policy, for each pair s, s. A finite diameter is understood to be necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al., 2010]. The UCRL2 algorithm of Jaksch et al. [2010], which is based on the optimism principle, achieved the best previously known upper bound of \u00d5(DS \u221a AT ) for this problem.", "startOffset": 93, "endOffset": 1278}, {"referenceID": 1, "context": "ably optimal performance bounds that have been recently obtained for many variations of MAB [Agrawal and Goyal, 2012, 2013b,a, Russo and Van Roy, 2015, 2014, Bubeck and Liu, 2013]. In this approach, the algorithm maintains a Bayesian posterior distribution for the expected reward of every action; then at any given step, it generates an independent sample from each of these posteriors and takes the action with the highest sample value. We consider the reinforcement learning problem with finite states S and finite actions A in a similar regret based framework, where the total reward of the reinforcement learning algorithm is compared to the total expected reward achieved by a single benchmark policy over a time horizon T . In our setting, the benchmark policy is the infinite-horizon undiscounted average reward optimal policy for the underlying MDP, under the assumption that the MDP is communicating with (unknown) finite diameter D. The diameter D is an upper bound on the time it takes to move from any state s to any other state s using an appropriate policy, for each pair s, s. A finite diameter is understood to be necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al., 2010]. The UCRL2 algorithm of Jaksch et al. [2010], which is based on the optimism principle, achieved the best previously known upper bound of \u00d5(DS \u221a AT ) for this problem. A similar bound was achieved by Bartlett and Tewari [2009], though assuming the knowledge of the diameter D.", "startOffset": 93, "endOffset": 1460}, {"referenceID": 1, "context": "ably optimal performance bounds that have been recently obtained for many variations of MAB [Agrawal and Goyal, 2012, 2013b,a, Russo and Van Roy, 2015, 2014, Bubeck and Liu, 2013]. In this approach, the algorithm maintains a Bayesian posterior distribution for the expected reward of every action; then at any given step, it generates an independent sample from each of these posteriors and takes the action with the highest sample value. We consider the reinforcement learning problem with finite states S and finite actions A in a similar regret based framework, where the total reward of the reinforcement learning algorithm is compared to the total expected reward achieved by a single benchmark policy over a time horizon T . In our setting, the benchmark policy is the infinite-horizon undiscounted average reward optimal policy for the underlying MDP, under the assumption that the MDP is communicating with (unknown) finite diameter D. The diameter D is an upper bound on the time it takes to move from any state s to any other state s using an appropriate policy, for each pair s, s. A finite diameter is understood to be necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al., 2010]. The UCRL2 algorithm of Jaksch et al. [2010], which is based on the optimism principle, achieved the best previously known upper bound of \u00d5(DS \u221a AT ) for this problem. A similar bound was achieved by Bartlett and Tewari [2009], though assuming the knowledge of the diameter D. Jaksch et al. [2010] also established a worst-case lower bound of \u03a9( \u221a DSAT ) on the regret of any algorithm for this problem.", "startOffset": 93, "endOffset": 1531}, {"referenceID": 1, "context": "ably optimal performance bounds that have been recently obtained for many variations of MAB [Agrawal and Goyal, 2012, 2013b,a, Russo and Van Roy, 2015, 2014, Bubeck and Liu, 2013]. In this approach, the algorithm maintains a Bayesian posterior distribution for the expected reward of every action; then at any given step, it generates an independent sample from each of these posteriors and takes the action with the highest sample value. We consider the reinforcement learning problem with finite states S and finite actions A in a similar regret based framework, where the total reward of the reinforcement learning algorithm is compared to the total expected reward achieved by a single benchmark policy over a time horizon T . In our setting, the benchmark policy is the infinite-horizon undiscounted average reward optimal policy for the underlying MDP, under the assumption that the MDP is communicating with (unknown) finite diameter D. The diameter D is an upper bound on the time it takes to move from any state s to any other state s using an appropriate policy, for each pair s, s. A finite diameter is understood to be necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al., 2010]. The UCRL2 algorithm of Jaksch et al. [2010], which is based on the optimism principle, achieved the best previously known upper bound of \u00d5(DS \u221a AT ) for this problem. A similar bound was achieved by Bartlett and Tewari [2009], though assuming the knowledge of the diameter D. Jaksch et al. [2010] also established a worst-case lower bound of \u03a9( \u221a DSAT ) on the regret of any algorithm for this problem. Our main contribution is a posterior sampling based algorithm with a high probability worst-case regret upper bound of \u00d5(D \u221a SAT + DSAT ), which is \u00d5(D \u221a SAT ) when T \u2265 SA. This improves the previously best known upper bound for this problem by a factor of \u221a S, and matches the dependence on S in the lower bound, for large enough T . Our algorithm uses an \u2018optimistic version\u2019 of the posterior sampling heuristic, while utilizing several ideas from the algorithm design structure in Jaksch et al. [2010], such as an epoch based execution and the extended MDP construction.", "startOffset": 93, "endOffset": 2142}, {"referenceID": 1, "context": "ably optimal performance bounds that have been recently obtained for many variations of MAB [Agrawal and Goyal, 2012, 2013b,a, Russo and Van Roy, 2015, 2014, Bubeck and Liu, 2013]. In this approach, the algorithm maintains a Bayesian posterior distribution for the expected reward of every action; then at any given step, it generates an independent sample from each of these posteriors and takes the action with the highest sample value. We consider the reinforcement learning problem with finite states S and finite actions A in a similar regret based framework, where the total reward of the reinforcement learning algorithm is compared to the total expected reward achieved by a single benchmark policy over a time horizon T . In our setting, the benchmark policy is the infinite-horizon undiscounted average reward optimal policy for the underlying MDP, under the assumption that the MDP is communicating with (unknown) finite diameter D. The diameter D is an upper bound on the time it takes to move from any state s to any other state s using an appropriate policy, for each pair s, s. A finite diameter is understood to be necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al., 2010]. The UCRL2 algorithm of Jaksch et al. [2010], which is based on the optimism principle, achieved the best previously known upper bound of \u00d5(DS \u221a AT ) for this problem. A similar bound was achieved by Bartlett and Tewari [2009], though assuming the knowledge of the diameter D. Jaksch et al. [2010] also established a worst-case lower bound of \u03a9( \u221a DSAT ) on the regret of any algorithm for this problem. Our main contribution is a posterior sampling based algorithm with a high probability worst-case regret upper bound of \u00d5(D \u221a SAT + DSAT ), which is \u00d5(D \u221a SAT ) when T \u2265 SA. This improves the previously best known upper bound for this problem by a factor of \u221a S, and matches the dependence on S in the lower bound, for large enough T . Our algorithm uses an \u2018optimistic version\u2019 of the posterior sampling heuristic, while utilizing several ideas from the algorithm design structure in Jaksch et al. [2010], such as an epoch based execution and the extended MDP construction. The algorithm proceeds in epochs, where in the beginning of every epoch, it generates \u03c8 = \u00d5(S) sample transition probability vectors from a posterior distribution for every state and action, and solves an extended MDP with \u03c8A actions and S states formed using these samples. The optimal policy computed for this extended MDP is used throughout the epoch. Posterior Sampling for Reinforcement Learning (PSRL) approach has been used previously in Osband et al. [2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework.", "startOffset": 93, "endOffset": 2677}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework.", "startOffset": 8, "endOffset": 45}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework.", "startOffset": 8, "endOffset": 72}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of \u00d5( \u221a HSAT ) when T \u2265 HSA.", "startOffset": 8, "endOffset": 458}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of \u00d5( \u221a HSAT ) when T \u2265 HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H .", "startOffset": 8, "endOffset": 636}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of \u00d5( \u221a HSAT ) when T \u2265 HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm\u2019s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper.", "startOffset": 8, "endOffset": 1289}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of \u00d5( \u221a HSAT ) when T \u2265 HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm\u2019s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants.", "startOffset": 8, "endOffset": 1463}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of \u00d5( \u221a HSAT ) when T \u2265 HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm\u2019s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants.", "startOffset": 8, "endOffset": 1494}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of \u00d5( \u221a HSAT ) when T \u2265 HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm\u2019s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants. Strong PAC bounds have been provided in Kearns and Singh [1999], Brafman and Tennenholtz [2002], Kakade et al.", "startOffset": 8, "endOffset": 1680}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of \u00d5( \u221a HSAT ) when T \u2265 HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm\u2019s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants. Strong PAC bounds have been provided in Kearns and Singh [1999], Brafman and Tennenholtz [2002], Kakade et al.", "startOffset": 8, "endOffset": 1712}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of \u00d5( \u221a HSAT ) when T \u2265 HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm\u2019s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants. Strong PAC bounds have been provided in Kearns and Singh [1999], Brafman and Tennenholtz [2002], Kakade et al. [2003], Asmuth et al.", "startOffset": 8, "endOffset": 1734}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of \u00d5( \u221a HSAT ) when T \u2265 HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm\u2019s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants. Strong PAC bounds have been provided in Kearns and Singh [1999], Brafman and Tennenholtz [2002], Kakade et al. [2003], Asmuth et al. [2009], Dann and Brunskill [2015].", "startOffset": 8, "endOffset": 1756}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of \u00d5( \u221a HSAT ) when T \u2265 HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm\u2019s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants. Strong PAC bounds have been provided in Kearns and Singh [1999], Brafman and Tennenholtz [2002], Kakade et al. [2003], Asmuth et al. [2009], Dann and Brunskill [2015]. There, the aim is to bound the performance of the policy Worst-case regret is a strictly stronger notion of regret in case the reward distribution function is known and only the transition probability distribution is unknown, as we will assume here for the most part.", "startOffset": 8, "endOffset": 1783}, {"referenceID": 27, "context": "Strehl and Littman [2005], Strehl and Littman [2008] provide an optimistic algorithm for bounding regret in a discounted reward setting, but the definition of regret is slightly different in that it measures the difference between the rewards of an optimal policy and the rewards of the learning algorithm along the trajectory taken by the learning algorithm.", "startOffset": 0, "endOffset": 26}, {"referenceID": 27, "context": "Strehl and Littman [2005], Strehl and Littman [2008] provide an optimistic algorithm for bounding regret in a discounted reward setting, but the definition of regret is slightly different in that it measures the difference between the rewards of an optimal policy and the rewards of the learning algorithm along the trajectory taken by the learning algorithm.", "startOffset": 0, "endOffset": 53}, {"referenceID": 22, "context": "For a communicating MDP M with diameter D: (a) (Puterman [2014] Theorem 8.", "startOffset": 48, "endOffset": 64}, {"referenceID": 22, "context": "For a communicating MDP M with diameter D: (a) (Puterman [2014] Theorem 8.1.2, Theorem 8.3.2) The optimal (maximum) gain \u03bb is state independent and is achieved by a deterministic stationary policy \u03c0, i.e., there exists a deterministic policy \u03c0 such that \u03bb := max s\u2032\u2208S max \u03c0 \u03bb(s) = \u03bb \u2217 (s), \u2200s \u2208 S. Here, \u03c0 is referred to as an optimal policy for MDP M. (b) (Tewari and Bartlett [2008], Theorem 4) The optimal gain \u03bb satisfies the following equations, \u03bb = min h\u2208RS max s,a rs,a + P T s,ah\u2212 hs = max a rs,a + P T s,ah \u2217 \u2212 h\u2217s, \u2200s (1) where h, referred to as the bias vector of MDP M, satisfies: max s hs \u2212min s hs \u2264 D.", "startOffset": 48, "endOffset": 385}, {"referenceID": 2, "context": ", using the techniques in Agrawal and Goyal [2013b].) The agent can use the past observations to learn the underlyingMDPmodel and decide future actions.", "startOffset": 26, "endOffset": 52}, {"referenceID": 2, "context": ", using the techniques in Agrawal and Goyal [2013b].) The agent can use the past observations to learn the underlyingMDPmodel and decide future actions. The goal is to maximize the total reward \u2211T t=1 rst,at , or equivalently, minimize the total regret over a time horizon T , defined as R(T,M) := T\u03bb \u2212\u2211Tt=1 rst,at (2) where \u03bb is the optimal gain of MDP M. We present an algorithm for the learning agent with a near-optimal upper bound on the regret R(T,M) for any communicating MDP M with diameter D, thus bounding the worst-case regret over this class of MDPs. 3 Algorithm Description Our algorithm combines the ideas of Posterior sampling (aka Thompson Sampling) with the extended MDP construction used in Jaksch et al. [2010]. Below we describe the main components of our algorithm.", "startOffset": 26, "endOffset": 730}, {"referenceID": 2, "context": ", using the techniques in Agrawal and Goyal [2013b].) The agent can use the past observations to learn the underlyingMDPmodel and decide future actions. The goal is to maximize the total reward \u2211T t=1 rst,at , or equivalently, minimize the total regret over a time horizon T , defined as R(T,M) := T\u03bb \u2212\u2211Tt=1 rst,at (2) where \u03bb is the optimal gain of MDP M. We present an algorithm for the learning agent with a near-optimal upper bound on the regret R(T,M) for any communicating MDP M with diameter D, thus bounding the worst-case regret over this class of MDPs. 3 Algorithm Description Our algorithm combines the ideas of Posterior sampling (aka Thompson Sampling) with the extended MDP construction used in Jaksch et al. [2010]. Below we describe the main components of our algorithm. Some notations: N t s,a denotes the total number of times the algorithm visited state s and played action a until before time t, and N t s,a(i) denotes the number of time steps among these N t s,a steps where the next state was i, i.e., a transition from state s to i was observed. We index the states from 1 to S, so that \u2211S i=1 N t s,a(i) = N t s,a for any t. We use the symbol 1 to denote the vector of all 1s, and 1i to denote the vector with 1 at the i th coordinate and 0 elsewhere. Doubling epochs: Our algorithm uses the epoch based execution framework of Jaksch et al. [2010]. An epoch is a group of consecutive rounds.", "startOffset": 26, "endOffset": 1372}, {"referenceID": 14, "context": "Extended MDP: The policy \u03c0\u0303k to be used in epoch k is computed as the optimal policy of an extended MDP M\u0303k defined by the sampled transition probability vectors, using the construction of Jaksch et al. [2010]. Given sampled vectors Q s,a, j = 1, .", "startOffset": 189, "endOffset": 210}, {"referenceID": 14, "context": "This line of argument bears similarities to the analysis of UCRL2 in Jaksch et al. [2010], but with tighter deviation bounds that we are able to guarantee due to the use of posterior sampling instead of deterministic optimistic bias used in UCRL2.", "startOffset": 69, "endOffset": 90}, {"referenceID": 20, "context": "The derivation of this bound utilizes and extends the stochastic optimism technique from Osband et al. [2014]. For s, a with N \u03c4k s,a \u2264 \u03b7, P\u0303s,a = Q s,a is a sample from the simple optimistic sampling, where we can only show the following weaker bound, but since this is used only while N \u03c4k s,a is small, the total contribution of this deviation will be small: max h\u2208[0,2D]S (P\u0303 k s,a \u2212 Ps,a)h \u2264 \u00d5 (", "startOffset": 89, "endOffset": 110}, {"referenceID": 14, "context": "Our algorithm may be viewed as a more efficient randomized version of the UCRL2 algorithm of Jaksch et al. [2010], with randomization via posterior sampling forming the key to the \u221a S factor improvement in the regret bound provided by our algorithm.", "startOffset": 93, "endOffset": 114}, {"referenceID": 20, "context": "Here, we utilize the stochastic optimism technique from Osband et al. [2014]. In Section D, we prove Lemma 4.", "startOffset": 56, "endOffset": 77}, {"referenceID": 22, "context": "6) in Puterman [2014]).", "startOffset": 6, "endOffset": 22}, {"referenceID": 31, "context": "1 Dirichlet concentration A similar result as the lemma below for concentration of Dirichlet random vectors was proven in Osband and Van Roy [2016]. We include (an expanded version of) the proof for completeness.", "startOffset": 122, "endOffset": 148}, {"referenceID": 31, "context": "1 Dirichlet concentration A similar result as the lemma below for concentration of Dirichlet random vectors was proven in Osband and Van Roy [2016]. We include (an expanded version of) the proof for completeness. Lemma C.1 (Osband and Van Roy [2016]).", "startOffset": 122, "endOffset": 250}, {"referenceID": 20, "context": "Now, since they have the same mean, from equivalence condition for stochastic optimism (Condition 3 in Lemma 3 of Osband et al. [2014]) E[DYv \u2212 p\u0303 v|p\u0303 v] = 0 for all values of v, p\u0303 v.", "startOffset": 114, "endOffset": 135}, {"referenceID": 20, "context": "Now, since they have the same mean, from equivalence condition for stochastic optimism (Condition 3 in Lemma 3 of Osband et al. [2014]) E[DYv \u2212 p\u0302 v|p\u0302 v] = 0 for all values of v, p\u0303 v.", "startOffset": 114, "endOffset": 135}, {"referenceID": 13, "context": "Then we notice that (I \u2212 Q\u2020\u03c0\u0303) is precisely the fundamental matrix of this absorbing Markov chain and hence exists and is non-negative (see Grinstead and Snell [2012], Theorem 11.", "startOffset": 140, "endOffset": 167}, {"referenceID": 25, "context": "E Useful deviation inequalities Fact 3 (Bernstein\u2019s Inequality, from Seldin et al. [2012] Lem 11/Cor 12).", "startOffset": 69, "endOffset": 90}, {"referenceID": 18, "context": "Fact 4 (Multiplicative Chernoff Bound, Kleinberg et al. [2008] Lemma 4.", "startOffset": 39, "endOffset": 63}, {"referenceID": 25, "context": "56 (see Shevtsova [2010]).", "startOffset": 8, "endOffset": 25}, {"referenceID": 1, "context": "Fact 7 (Abramowitz and Stegun [1964] 26.", "startOffset": 8, "endOffset": 37}, {"referenceID": 20, "context": "3 (Gaussian vs Dirichlet optimism, from Osband et al. [2014] Lemma 1).", "startOffset": 40, "endOffset": 61}, {"referenceID": 20, "context": "3 (Gaussian vs Dirichlet optimism, from Osband et al. [2014] Lemma 1). Let Y = PV for V \u2208 [0, 1] fixed and P \u223c Dirichlet(\u03b1) with \u03b1 \u2208 R + and \u2211S i=1 \u03b1i \u2265 2. Let X \u223c N(\u03bc, \u03c3) with \u03bc = \u2211S i=1 \u03b1iVi \u2211 S i=1 \u03b1i , \u03c3 = ( \u2211S i=1 \u03b1i) , thenX is stochastically optimistic for Y . Lemma E.4 (Gaussian vs Beta optimism, Osband et al. [2014] Lemma 6).", "startOffset": 40, "endOffset": 327}, {"referenceID": 20, "context": "3 (Gaussian vs Dirichlet optimism, from Osband et al. [2014] Lemma 1). Let Y = PV for V \u2208 [0, 1] fixed and P \u223c Dirichlet(\u03b1) with \u03b1 \u2208 R + and \u2211S i=1 \u03b1i \u2265 2. Let X \u223c N(\u03bc, \u03c3) with \u03bc = \u2211S i=1 \u03b1iVi \u2211 S i=1 \u03b1i , \u03c3 = ( \u2211S i=1 \u03b1i) , thenX is stochastically optimistic for Y . Lemma E.4 (Gaussian vs Beta optimism, Osband et al. [2014] Lemma 6). Let \u1ef8 \u223c Beta(\u03b1, \u03b2) for any \u03b1, \u03b2 > 0 and X \u223c N( \u03b1 \u03b1+\u03b2 , 1 \u03b1+\u03b2 ). Then X is stochastically optimistic for \u1ef8 whenever \u03b1+ \u03b2 \u2265 2. Lemma E.5 (Dirichlet vs Beta optimism, Osband et al. [2014] Lemma 5).", "startOffset": 40, "endOffset": 522}, {"referenceID": 20, "context": "3 (Gaussian vs Dirichlet optimism, from Osband et al. [2014] Lemma 1). Let Y = PV for V \u2208 [0, 1] fixed and P \u223c Dirichlet(\u03b1) with \u03b1 \u2208 R + and \u2211S i=1 \u03b1i \u2265 2. Let X \u223c N(\u03bc, \u03c3) with \u03bc = \u2211S i=1 \u03b1iVi \u2211 S i=1 \u03b1i , \u03c3 = ( \u2211S i=1 \u03b1i) , thenX is stochastically optimistic for Y . Lemma E.4 (Gaussian vs Beta optimism, Osband et al. [2014] Lemma 6). Let \u1ef8 \u223c Beta(\u03b1, \u03b2) for any \u03b1, \u03b2 > 0 and X \u223c N( \u03b1 \u03b1+\u03b2 , 1 \u03b1+\u03b2 ). Then X is stochastically optimistic for \u1ef8 whenever \u03b1+ \u03b2 \u2265 2. Lemma E.5 (Dirichlet vs Beta optimism, Osband et al. [2014] Lemma 5). Let y = p v for some random variable p \u223c Dirichlet(\u03b1) and constants v \u2208 Rd and \u03b1 \u2208 N . Without loss of generality, assume v1 \u2264 v2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 vd. Let \u03b1\u0303 = \u2211d i=1 \u03b1i(vi\u2212v1)/(vd\u2212v1) and \u03b2\u0303 = \u2211d i=1 \u03b1i(vd\u2212vi)/(vd\u2212 v1). Then, there exists a random variable p\u0303 \u223c Beta(\u03b1\u0303, \u03b2\u0303) such that, for \u1ef9 = p\u0303vd + (1 \u2212 p\u0303)v1, E[\u1ef9|y] = E[y]. Lemma E.6. If E[X ] = E[Y ] and X is stochastically optimistic for Y , then \u2212X is stochastically optimistic for \u2212Y . Proof. By Lemma 3.3 in Osband et al. [2014], X stochastically optimistic for Y is equivalent to having X =D Y + A + W with A \u2265 0 and E[W |Y + A] = 0 for all values y + a.", "startOffset": 40, "endOffset": 1015}, {"referenceID": 20, "context": "Then stochastic optimism follows from applying the optimism equivalence condition from Lemma 3 (Condition 3) of Osband et al. [2014].", "startOffset": 112, "endOffset": 133}], "year": 2017, "abstractText": "We present an algorithm based on posterior sampling (aka Thompson sampling) that achieves near-optimal worst-case regret bounds when the underlying Markov Decision Process (MDP) is communicating with a finite, though unknown, diameter. Our main result is a high probability regret upper bound of \u00d5(D \u221a SAT ) for any communicating MDP with S states, A actions and diameter D, when T \u2265 SA. Here, regret compares the total reward achieved by the algorithm to the total expected reward of an optimal infinite-horizon undiscounted average reward policy, in time horizon T . This result improves over the best previously known upper bound of \u00d5(DS \u221a AT ) achieved by any algorithm in this setting, and matches the dependence on S in the established lower bound of \u03a9( \u221a DSAT ) for this problem. Our techniques involve proving some novel results about the anticoncentration of Dirichlet distribution, which may be of independent interest.", "creator": "LaTeX with hyperref package"}}}