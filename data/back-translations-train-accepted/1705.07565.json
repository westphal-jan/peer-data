{"id": "1705.07565", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon", "abstract": "How to develop slim and accurate deep neural networks has become crucial for real- world applications, especially for those employed in embedded systems. Though previous work along this research line has shown some promising results, most existing methods either fail to significantly compress a well-trained deep network or require a heavy retraining process for the pruned deep network to re-boost its prediction performance. In this paper, we propose a new layer-wise pruning method for deep neural networks. In our proposed method, parameters of each individual layer are pruned independently based on second order derivatives of a layer-wise error function with respect to the corresponding parameters. We prove that the final prediction performance drop after pruning is bounded by a linear combination of the reconstructed errors caused at each layer. Therefore, there is a guarantee that one only needs to perform a light retraining process on the pruned network to resume its original prediction performance. We conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our pruning method compared with several state-of-the-art baseline methods.", "histories": [["v1", "Mon, 22 May 2017 05:54:37 GMT  (139kb,D)", "http://arxiv.org/abs/1705.07565v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["xin dong", "shangyu chen", "sinno jialin pan"], "accepted": true, "id": "1705.07565"}, "pdf": {"name": "1705.07565.pdf", "metadata": {"source": "CRF", "title": "Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon", "authors": ["Xin Dong", "Shangyu Chen"], "emails": ["n1503521a@e.ntu.edu.sg", "schen025@e.ntu.edu.sg", "sinnopan@ntu.edu.sg"], "sections": [{"heading": null, "text": "Although previous work along this line of research has shown some promising results, most existing methods are either unable to significantly compress a well-trained deep network or require a heavy retraining process for the truncated deep network to increase its predictive performance again. In this paper, we propose a new layer-by-layer prediction method for deep neural networks. In our proposed method, the parameters of each individual layer are truncated independently based on derivatives of a second-order layered error function with respect to the corresponding parameters. We prove that the final power decline after truncation is limited by a linear combination of reconstructed errors caused on each layer. Therefore, there is a guarantee that only a light retraining process needs to be performed on the truncated network to restore its original predictive performance."}, {"heading": "1 Introduction", "text": "It is indeed the case that we are able to go in search of a solution."}, {"heading": "2 Related Works", "text": "In the past, however, with a relatively small size of training data, the calculation of the Hessian inversion across all parameters is expensive. In the OBD, however, the Hessian matrix is limited to a diagonal matrix in order to make it compressible."}, {"heading": "3 Layer-wise Optimal Brain Surgeon", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Problem Statement", "text": "In the face of a training of n instances, {(xj, yj)} nj = 1, and a well-trained deep neural network of L layers (without the input layer) 1. Specify the input and output of the deep neural network with X = [x1,..., xn]. For layer l, we denote the input and output of the layer with Yl \u2212 1 = [yl \u2212 11,..., y l \u2212 1 n], Rml \u2212 1 \u00b7 n and Yl = [yl1,..., yln], where yli can be seen as the representation of xi in layer l, and Y0 = X, YL = Y and m0 = d. With forward speed, we have Yl = 1 \u00b7 n and Yl = [yl1,..., yln] where Zl = Wl > Yl \u2212 1 where Wl \u2212 Rml is the matrix of the layer parameters."}, {"heading": "3.2 Layer-Wise Error", "text": "Suppose we set the q-te element of the plane named by the plane, and get a new parameter vector called by the plane. (Suppose we get a new output for the plane named by the plane.) Consider the root of the middle quadratic error between Yl and Yl \u2212 1 over the entire training data as a layered error: \u03b5l = 1 n. \"(Note: For each single parameter vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector"}, {"heading": "3.3 Layer-Wise Error Propagation and Accumulation", "text": "To do this, we show below how the layer-by-layer errors spread to the final output layer, and the accumulated error over multiple layers will not explode. Theorem 3.2. Given a truncated deep network over layer-by-layer pruning that was introduced in Section 3.2, each layer has its own layer-by-layer error, and the accumulated error over multiple layers will not explode. Theorem 3.2. Given a truncated deep network over layer-by-layer pruning that was introduced in Section 3.2, each layer has its own layer-by-layer error, and the cumulative error of the ultimate network performance will not explode."}, {"heading": "3.4 The Proposed Algorithm", "text": "In this section we present an efficient algorithm that can reduce the size of the Hessian matrix and thereby accelerate the calculation on its inversion. For each level l, according to the definition of the error function used in Lemma 3.1, the first derivative of the error function in relation to the Hessian matrix is normally so large that it can reduce the size of the Hessian matrix and thus accelerate the calculation on its inversion. The first derivative of the error function in relation to the Hessian matrix is not as large as it is in relation to the Hessian matrix."}, {"heading": "4 Experiments", "text": "In this section, we review the effectiveness of our proposed layer-wise OBS (L-OBS) using different architectures of deep neural networks in terms of compression rate (CR), error rate before retraining, and the number of iterations required for retraining to achieve satisfactory performance. In this section, CR is defined as the ratio of the number of parameters obtained to the number of original parameters. We perform comparative results of L-OBS with the following pruning approaches: 1) Randomly prune, 2) OBD [12], 3) LWC [9], 4) DNS [11], and 5) Net-Trim [6]. Deep architectures used for experiments include: LeNet-300-100 [2] and LeNet-5 [2] on the MNIST dataset, CIFAR-Net2 [22] on the CIFAR-10 dataset, AlexNet [23] and VG-16 [2] on the FIST-22 dataset."}, {"heading": "4.1 Overall Comparison Results", "text": "Indeed, it is not as if it is not, as if it is not, as if it is not, as if.............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "5 Conclusion", "text": "We have proposed a novel L-OBS framework for pruning parameters, based on second-order information about the layer-by-layer error function and providing a theoretical guarantee of the total error in terms of the reconstructed errors for each layer. Our proposed L-OBS can prune a significant number of parameters with a tiny drop in performance and reduce or even omit retraining. More importantly, it identifies and preserves the really important part of networks during pruning compared to previous methods, which can help immerse you in the nature of neural networks."}], "references": [{"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Ultrastructural evidence for synaptic scaling across the wake/sleep", "author": ["Luisa de Vivo", "Michele Bellesi", "William Marshall", "Eric A Bushong", "Mark H Ellisman", "Giulio Tononi", "Chiara Cirelli"], "venue": "cycle. Science,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2017}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Nando de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Net-trim: A layer-wise convex pruning of deep neural networks", "author": ["A. Nguyen N. Aghasi", "J. Romberg"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Pruning algorithms-a survey", "author": ["Russell Reed"], "venue": "IEEE transactions on Neural Networks,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Sparsifying neural network connections for face recognition", "author": ["Yi Sun", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Dynamic network surgery for efficient dnns", "author": ["Yiwen Guo", "Anbang Yao", "Yurong Chen"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S Denker", "Sara A Solla", "Richard E Howard", "Lawrence D Jackel"], "venue": "In NIPs,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1989}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["Babak Hassibi", "David G Stork"], "venue": "Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1993}, {"title": "The incredible shrinking neural network: New perspectives on learning representations through the lens of pruning", "author": ["Nikolas Wolfe", "Aditya Sharma", "Lukas Drude", "Bhiksha Raj"], "venue": "arXiv preprint arXiv:1701.04465,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Network trimming: A data-driven neuron pruning approach towards efficient deep architectures", "author": ["Hengyuan Hu", "Rui Peng", "Yu-Wing Tai", "Chi-Keung Tang"], "venue": "arXiv preprint arXiv:1607.03250,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Pruning filters for efficient convnets", "author": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "venue": "arXiv preprint arXiv:1608.08710,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Convolutional neural networks with low-rank regularization", "author": ["Cheng Tai", "Tong Xiao", "Yi Zhang", "Xiaogang Wang"], "venue": "arXiv preprint arXiv:1511.06067,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Sparse convolutional neural networks", "author": ["Baoyuan Liu", "Min Wang", "Hassan Foroosh", "Marshall Tappen", "Marianna Pensky"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Convex analysis", "author": ["R Tyrrell Rockafellar"], "venue": "princeton landmarks in mathematics", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u00edn Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In practice, the size of deep neural networks has been being tremendously increased, from LeNet-5 with less than 1M parameters [2] to VGG-16 with 133M parameters [3].", "startOffset": 127, "endOffset": 130}, {"referenceID": 1, "context": "In practice, the size of deep neural networks has been being tremendously increased, from LeNet-5 with less than 1M parameters [2] to VGG-16 with 133M parameters [3].", "startOffset": 162, "endOffset": 165}, {"referenceID": 2, "context": "On one hand, in neuroscience, recent studies point out that there are significant redundant neurons in human brain, and memory may have relation with vanishment of specific synapses [4].", "startOffset": 182, "endOffset": 185}, {"referenceID": 3, "context": "On the other hand, in machine learning, both theoretical analysis and empirical experiments have shown the evidence of redundancy in several deep models [5, 6].", "startOffset": 153, "endOffset": 159}, {"referenceID": 4, "context": "On the other hand, in machine learning, both theoretical analysis and empirical experiments have shown the evidence of redundancy in several deep models [5, 6].", "startOffset": 153, "endOffset": 159}, {"referenceID": 5, "context": "Recent work mainly focuses on developing efficient algorithms to obtain a near-optimal pruning solution [7, 8, 9, 10, 11].", "startOffset": 104, "endOffset": 121}, {"referenceID": 6, "context": "Recent work mainly focuses on developing efficient algorithms to obtain a near-optimal pruning solution [7, 8, 9, 10, 11].", "startOffset": 104, "endOffset": 121}, {"referenceID": 7, "context": "Recent work mainly focuses on developing efficient algorithms to obtain a near-optimal pruning solution [7, 8, 9, 10, 11].", "startOffset": 104, "endOffset": 121}, {"referenceID": 8, "context": "Recent work mainly focuses on developing efficient algorithms to obtain a near-optimal pruning solution [7, 8, 9, 10, 11].", "startOffset": 104, "endOffset": 121}, {"referenceID": 9, "context": "Recent work mainly focuses on developing efficient algorithms to obtain a near-optimal pruning solution [7, 8, 9, 10, 11].", "startOffset": 104, "endOffset": 121}, {"referenceID": 4, "context": "Instead of consuming efforts on a whole deep network, a layer-wise pruning method, Net-Trim, was proposed to learn sparse parameters by minimizing reconstructed error for each individual layer [6].", "startOffset": 193, "endOffset": 196}, {"referenceID": 7, "context": "However, as Net-Trim adopts `1-norm to induce sparsity for pruning, it fails to obtain high compression ratio compared with other methods [9, 11].", "startOffset": 138, "endOffset": 145}, {"referenceID": 9, "context": "However, as Net-Trim adopts `1-norm to induce sparsity for pruning, it fails to obtain high compression ratio compared with other methods [9, 11].", "startOffset": 138, "endOffset": 145}, {"referenceID": 10, "context": "To achieve our first goal, we borrow an idea from some classic pruning approaches for shallow neural networks, such as optimal brain damage (OBD) [12] and optimal brain surgeon (OBS) [13].", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "To achieve our first goal, we borrow an idea from some classic pruning approaches for shallow neural networks, such as optimal brain damage (OBD) [12] and optimal brain surgeon (OBS) [13].", "startOffset": 183, "endOffset": 187}, {"referenceID": 4, "context": "To achieve our second goal, based on the theoretical results in [6], we provide a proof on the bound of performance drop before and after pruning in terms of the reconstructed errors for each layer.", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "Pruning methods have been widely used for model compression in early neural networks [7] and modern deep neural networks [6, 8, 9, 10, 11].", "startOffset": 85, "endOffset": 88}, {"referenceID": 4, "context": "Pruning methods have been widely used for model compression in early neural networks [7] and modern deep neural networks [6, 8, 9, 10, 11].", "startOffset": 121, "endOffset": 138}, {"referenceID": 6, "context": "Pruning methods have been widely used for model compression in early neural networks [7] and modern deep neural networks [6, 8, 9, 10, 11].", "startOffset": 121, "endOffset": 138}, {"referenceID": 7, "context": "Pruning methods have been widely used for model compression in early neural networks [7] and modern deep neural networks [6, 8, 9, 10, 11].", "startOffset": 121, "endOffset": 138}, {"referenceID": 8, "context": "Pruning methods have been widely used for model compression in early neural networks [7] and modern deep neural networks [6, 8, 9, 10, 11].", "startOffset": 121, "endOffset": 138}, {"referenceID": 9, "context": "Pruning methods have been widely used for model compression in early neural networks [7] and modern deep neural networks [6, 8, 9, 10, 11].", "startOffset": 121, "endOffset": 138}, {"referenceID": 7, "context": "[9] proposed to delete unimportant parameters based on magnitude of their absolute values, and retrain the remaining ones to recover the original prediction performance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "However, as pointed out by pioneer research work [12, 13], parameters with low magnitude of their absolute values can be necessary for low error.", "startOffset": 49, "endOffset": 57}, {"referenceID": 11, "context": "However, as pointed out by pioneer research work [12, 13], parameters with low magnitude of their absolute values can be necessary for low error.", "startOffset": 49, "endOffset": 57}, {"referenceID": 12, "context": "Therefore, magnitude-based approaches may eliminate wrong parameters, resulting in a big prediction performance drop right after pruning, and poor robustness before retraining [14].", "startOffset": 176, "endOffset": 180}, {"referenceID": 13, "context": "criteria [15, 16], the significant drop of prediction performance after pruning still remains.", "startOffset": 9, "endOffset": 17}, {"referenceID": 14, "context": "criteria [15, 16], the significant drop of prediction performance after pruning still remains.", "startOffset": 9, "endOffset": 17}, {"referenceID": 9, "context": "[11] introduced a mask matrix to indicate the state of network connection for dynamically pruning after each gradient decent step.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Besides Net-trim, which is a layer-wise pruning method discussed in the previous section, there is some other work proposed to induce sparsity or low-rank approximation on certain layers for pruning [17, 18].", "startOffset": 199, "endOffset": 207}, {"referenceID": 16, "context": "Besides Net-trim, which is a layer-wise pruning method discussed in the previous section, there is some other work proposed to induce sparsity or low-rank approximation on certain layers for pruning [17, 18].", "startOffset": 199, "endOffset": 207}, {"referenceID": 4, "context": "However, as the `0-norm or the `1-norm sparsity-induced regularization term increases difficulty in optimization, the pruned deep neural networks using these methods either obtain much smaller compression ratio [6] compared with direct pruning methods or require retraining of the whole network to prevent accumulation of errors [10].", "startOffset": 211, "endOffset": 214}, {"referenceID": 8, "context": "However, as the `0-norm or the `1-norm sparsity-induced regularization term increases difficulty in optimization, the pruned deep neural networks using these methods either obtain much smaller compression ratio [6] compared with direct pruning methods or require retraining of the whole network to prevent accumulation of errors [10].", "startOffset": 329, "endOffset": 333}, {"referenceID": 17, "context": "For convenience in presentation and proof, we define nonlinear activation function \u03c3(\u00b7) as the rectified linear unit (ReLU) [19].", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "This idea has been adopted by some methods [14].", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "Following [12, 13], the error function can be approximated by functional Taylor series as follows,", "startOffset": 10, "endOffset": 18}, {"referenceID": 11, "context": "Following [12, 13], the error function can be approximated by functional Taylor series as follows,", "startOffset": 10, "endOffset": 18}, {"referenceID": 18, "context": "It can be shown that the optimization problem (4) can be solved by the Lagrange multipliers method [20].", "startOffset": 99, "endOffset": 103}, {"referenceID": 11, "context": "Even in the late-stage of pruning when this difference is not small, we can still ignore the corresponding term [13].", "startOffset": 112, "endOffset": 116}, {"referenceID": 8, "context": "\u03a8t = 1 t \u2211t j=1\u03c8 j with \u03a8\u22121 0 =\u03b1I, \u03b1 \u2208 [10, 10], and \u03a8\u22121 = \u03a8\u22121 n .", "startOffset": 39, "endOffset": 47}, {"referenceID": 8, "context": "\u03a8t = 1 t \u2211t j=1\u03c8 j with \u03a8\u22121 0 =\u03b1I, \u03b1 \u2208 [10, 10], and \u03a8\u22121 = \u03a8\u22121 n .", "startOffset": 39, "endOffset": 47}, {"referenceID": 10, "context": "We conduct comparison results of L-OBS with the following pruning approaches: 1) Randomly prune, 2) OBD [12], 3) LWC [9], 4) DNS [11], and 5) Net-Trim [6].", "startOffset": 104, "endOffset": 108}, {"referenceID": 7, "context": "We conduct comparison results of L-OBS with the following pruning approaches: 1) Randomly prune, 2) OBD [12], 3) LWC [9], 4) DNS [11], and 5) Net-Trim [6].", "startOffset": 117, "endOffset": 120}, {"referenceID": 9, "context": "We conduct comparison results of L-OBS with the following pruning approaches: 1) Randomly prune, 2) OBD [12], 3) LWC [9], 4) DNS [11], and 5) Net-Trim [6].", "startOffset": 129, "endOffset": 133}, {"referenceID": 4, "context": "We conduct comparison results of L-OBS with the following pruning approaches: 1) Randomly prune, 2) OBD [12], 3) LWC [9], 4) DNS [11], and 5) Net-Trim [6].", "startOffset": 151, "endOffset": 154}, {"referenceID": 0, "context": "The deep architectures used for experiments include: LeNet-300-100 [2] and LeNet-5 [2] on the MNIST dataset, CIFAR-Net2 [22] on the CIFAR-10 dataset, AlexNet [23] and VGG-16 [3] on the ImageNet ILSVRC-2012 dataset.", "startOffset": 67, "endOffset": 70}, {"referenceID": 0, "context": "The deep architectures used for experiments include: LeNet-300-100 [2] and LeNet-5 [2] on the MNIST dataset, CIFAR-Net2 [22] on the CIFAR-10 dataset, AlexNet [23] and VGG-16 [3] on the ImageNet ILSVRC-2012 dataset.", "startOffset": 83, "endOffset": 86}, {"referenceID": 19, "context": "The deep architectures used for experiments include: LeNet-300-100 [2] and LeNet-5 [2] on the MNIST dataset, CIFAR-Net2 [22] on the CIFAR-10 dataset, AlexNet [23] and VGG-16 [3] on the ImageNet ILSVRC-2012 dataset.", "startOffset": 120, "endOffset": 124}, {"referenceID": 20, "context": "The deep architectures used for experiments include: LeNet-300-100 [2] and LeNet-5 [2] on the MNIST dataset, CIFAR-Net2 [22] on the CIFAR-10 dataset, AlexNet [23] and VGG-16 [3] on the ImageNet ILSVRC-2012 dataset.", "startOffset": 158, "endOffset": 162}, {"referenceID": 1, "context": "The deep architectures used for experiments include: LeNet-300-100 [2] and LeNet-5 [2] on the MNIST dataset, CIFAR-Net2 [22] on the CIFAR-10 dataset, AlexNet [23] and VGG-16 [3] on the ImageNet ILSVRC-2012 dataset.", "startOffset": 174, "endOffset": 177}, {"referenceID": 9, "context": "Results of retraining iterations of DNS are reported from [11] and the other experiments are implemented based on TensorFlow.", "startOffset": 58, "endOffset": 62}, {"referenceID": 21, "context": "[24] In addition, in scenario requiring high pruning ratio, L-OBS can be quite flexibly adopted to its iterative version-doing pruning and light retraining alternately, to obtain higher pruning ratio with relative higher cost of pruning.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "How to develop slim and accurate deep neural networks has become crucial for realworld applications, especially for those employed in embedded systems. Though previous work along this research line has shown some promising results, most existing methods either fail to significantly compress a well-trained deep network or require a heavy retraining process for the pruned deep network to re-boost its prediction performance. In this paper, we propose a new layer-wise pruning method for deep neural networks. In our proposed method, parameters of each individual layer are pruned independently based on second order derivatives of a layer-wise error function with respect to the corresponding parameters. We prove that the final prediction performance drop after pruning is bounded by a linear combination of the reconstructed errors caused at each layer. Therefore, there is a guarantee that one only needs to perform a light retraining process on the pruned network to resume its original prediction performance. We conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our pruning method compared with several state-of-the-art baseline methods.", "creator": "LaTeX with hyperref package"}}}