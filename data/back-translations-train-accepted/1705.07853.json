{"id": "1705.07853", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "Nonparametric Online Regression while Learning the Metric", "abstract": "We study algorithms for online nonparametric regression that learn the directions along which the regression function is smoother. Our algorithm learns the Mahalanobis metric based on the gradient outer product matrix $\\boldsymbol{G}$ of the regression function (automatically adapting to the effective rank of this matrix), while simultaneously bounding the regret ---on the same data sequence--- in terms of the spectrum of $\\boldsymbol{G}$. As a preliminary step in our analysis, we generalize a nonparametric online learning algorithm by Hazan and Megiddo by enabling it to compete against functions whose Lipschitzness is measured with respect to an arbitrary Mahalanobis metric.", "histories": [["v1", "Mon, 22 May 2017 16:58:13 GMT  (20kb,D)", "http://arxiv.org/abs/1705.07853v1", null], ["v2", "Mon, 23 Oct 2017 15:51:33 GMT  (166kb,D)", "http://arxiv.org/abs/1705.07853v2", "To appear in NIPS 2017"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ilja kuzborskij", "nicol\\`o cesa-bianchi"], "accepted": true, "id": "1705.07853"}, "pdf": {"name": "1705.07853.pdf", "metadata": {"source": "CRF", "title": "Nonparametric Online Regression while Learning the Metric", "authors": ["Ilja Kuzborskij"], "emails": ["ilja.kuzborskij@idiap.ch", "nicolo.cesa-bianchi@unimi.it"], "sections": [{"heading": "1 Introduction", "text": "The learner (\"t\") is an agent who interacts with an unknown and arbitrary environment (\"t\"). (\"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t.\" t \"t\" t \"t\" t \"t\" t \"t\" t. \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t. \"t\" t \"t\" t \"t\" t \"t.\" t \"t\" t \"t\" t \"t.\" t \"t\" t \"t\" t. \"t\" t \"t.\" t. \"t.\" t. \"t.\" t. \"t.\" t. \"t.\" t. \"t.\" t. \"t.\" t."}, {"heading": "2 Related works", "text": "However, it is a simple and efficient algorithm that adapts well to the design of extensions that exhibit different forms of adaptation to the data sequence. For example, the paper has derived a variant that automatically adapts to the intrusiveness of the diverse data. Our work mainly aims at restoring an optimal function from a rich class under reasonable probable assumptions. In online learning, the non-parametric approach was indeed investigated in [15, 16, 17] by Vovk, which has looked at regression problems in large spaces and proven limits of regret. [7] The non-parametric online algorithms of [7] are known to have a suboptimal regret for Lipschitz classes of functions. However, it is a simple and efficient algorithm that adapts well to the design of extensions that exhibit different forms of adaptation to the data sequence."}, {"heading": "3 Online nonparametric learning with ellipsoid packing", "text": "In this section we present a variant (algorithm 1) of the non-parametric regression algorithms introduced in relation to each individual contrition. (algorithm 1) Since our analysis is invariable in order to realize the rescalation of the matrix M without loss of generality, we assume that M has a spectral unit radius (i.e., the property 1 = 1). Algorithm 1 sequentially constructs a packaging of the X ellipsoids centered on a substance of the past. At the end of the step, the label of the current instance is predicted based on the average of the labels of past substances fallen within the ellipsoids whose center xs is closest to xt in the Mahalanobis metric. If xt was outside the next ellipsoid, then a new ellipsoid with center xt is created."}, {"heading": "4 Learning while learning the metric", "text": "In this section, we assume that instances of xt realizations of i.i.d. random variables Xt are drawn according to a fixed and unknown distribution that has a continuous density on its basis. We also assume that the labels yt are generated according to the noise model yt = f0 (xt) + 0 (xt), where f0 is a random variable that we have bound with high probability. We now show how the non-parametric online learning algorithm (algorithm 1) of Section 3 can be combined with an algorithm that learns an estimate."}, {"heading": "5 Conclusions and future work", "text": "We have presented an efficient algorithm for non-parametric online regression that adapts to the directions along which the regression function f0 is smoother. It does this by learning the Mahalanobis metric by estimating the outer product matrix with the gradient E. As a preliminary result, we analyzed the regret of a generalized version of the algorithm from [7] and recorded situations in which one competes with functions with directional lip coating in relation to any Mahalanobis metric. Our main result is then obtained by a phase-by-phase algorithm that estimates the outer product matrix with the same sequence while performing non-parametric regression online. Both algorithms automatically adapt to the effective rank of the metric. This work could be extended by examining a variant of Algorithm 1 for classification in which ball radii shrink at an uneven rate."}, {"heading": "A Nonparametric gradient learning", "text": "In this section we describe a non-parametric gradient learning algorithm introduced in [14]. Furthermore, we assume that instances xt are realizations of i.i.d. random variables Xt, which are drawn after a fixed and unknown distribution, which has a continuous density of their support. Names yt are generated according to the noise model yt = f (xt) + \u03bd (xt), in which Constantia (x) is a substantial zero mean variable for all x values X. The algorithm calculates a sequence of estimates f: 1, f: 2, f: 2,. the regression function f: 0 by kernel regression. Let Xn: The data is observed so far and let y1,."}, {"heading": "B Proofs from Section 3", "text": "Let us consider a pair of standards (Volumetric packing bound) = Land Use Plan = Land Use Plan = Land Use Plan = Land Use Plan = Land Use Plan = Land Use Plan = Land Use Plan = Land Use Plan (Land Use Plan) = Land Use Plan (Land Use Plan) = Land Use Plan (Land Use Plan) = Land Use Plan (Land Use Plan) = Land Use Plan (Land Use Plan) = Land Use Plan (Land Use Plan) = Land Use Plan (Land Use Plan) and Land Use Plan (Land Use Plan). Let us consider the Land Use Plan (Land Use Plan) + Land Use Plan (Land Use Plan)."}, {"heading": "C Proofs from Section 4", "text": "Lemma 5. Leave \u00b5d, \u03b1 > 0 and d \u2265 1. Then the derivative of F (t) = (\u00b5d + 2 (T0 + t) \u2212 \u03b1) t 2 1 + dis results positive for all t \u2265 1, if T0 \u2265 (d + 12\u00b5d) 1 / \u03b1.Evidence. We have this F \u2032 (t) \u2265 0, if and only ift \u2264 2 (T0 + t) \u03b1 (d + 1) (1 + (T0 + t) \u03b1\u00b5d) implicitly implied by t \u2264 2\u00b5d (T0 + t) 1 + \u03b1\u03b1 (d + 1) or equivalent T0 \u2265 A1 / (1 + \u03b1) t1 / (1 + \u03b1) \u2212 \u2212 t, where the derivative is implicated \u03b1 (d + 1) / (2\u00b5d), the right side A1 / (1 + \u03b1) t1 / \u03b1 (\u03b1) is a concave function of t. This results in the maximum \u2212 \u2212 \u2212 t where the \u03b1 + \u03b1 (\u03b1 + 1) + \u03b1 (\u03b1) (\u03b1 + 1) (\u03b1) is \u03b1 + \u03b1 (\u03b1) (\u03b1 + 1) (\u03b1)."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>We study algorithms for online nonparametric regression that learn the directions along which the<lb>regression function is smoother. Our algorithm learns the Mahalanobis metric based on the gradient<lb>outer product matrix G of the regression function (automatically adapting to the effective rank of this<lb>matrix), while simultaneously bounding the regret \u2014on the same data sequence\u2014 in terms of the<lb>spectrum of G. As a preliminary step in our analysis, we generalize a nonparametric online learning<lb>algorithm by Hazan and Megiddo by enabling it to compete against functions whose Lipschitzness is<lb>measured with respect to an arbitrary Mahalanobis metric.", "creator": "LaTeX with hyperref package"}}}