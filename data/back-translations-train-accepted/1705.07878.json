{"id": "1705.07878", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning", "abstract": "High network communication cost for synchronizing gradients and parameters is the well-known bottleneck of distributed training. In this work, we propose TernGrad that uses ternary gradients to accelerate distributed deep learning in data parallelism. Our approach requires only three numerical levels {-1,0,1} which can aggressively reduce the communication time. We mathematically prove the convergence of TernGrad under the assumption of a bound on gradients. Guided by the bound, we propose layer-wise ternarizing and gradient clipping to improve its convergence. Our experiments show that applying TernGrad on AlexNet does not incur any accuracy loss and can even improve accuracy. The accuracy loss of GoogLeNet induced by TernGrad is less than 2% on average. Finally, a performance model is proposed to study the scalability of TernGrad. Experiments show significant speed gains for various deep neural networks.", "histories": [["v1", "Mon, 22 May 2017 17:42:15 GMT  (690kb,D)", "http://arxiv.org/abs/1705.07878v1", "9 pages"], ["v2", "Wed, 24 May 2017 06:41:05 GMT  (693kb,D)", "http://arxiv.org/abs/1705.07878v2", "11 pages"], ["v3", "Mon, 4 Sep 2017 23:49:08 GMT  (693kb,D)", "http://arxiv.org/abs/1705.07878v3", "NIPS 2017 Oral"], ["v4", "Mon, 18 Sep 2017 16:21:51 GMT  (727kb,D)", "http://arxiv.org/abs/1705.07878v4", "NIPS 2017 Oral"], ["v5", "Tue, 31 Oct 2017 16:36:41 GMT  (694kb,D)", "http://arxiv.org/abs/1705.07878v5", "NIPS 2017 Oral"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG cs.DC cs.NE", "authors": ["wei wen", "cong xu", "feng yan", "chunpeng wu", "yandan wang", "yiran chen", "hai li"], "accepted": true, "id": "1705.07878"}, "pdf": {"name": "1705.07878.pdf", "metadata": {"source": "CRF", "title": "TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning", "authors": ["Wei Wen", "Cong Xu", "Feng Yan"], "emails": ["wei.wen@duke.edu", "cong.xu@hpe.com", "fyan@unr.edu", "chunpeng.wu@duke.edu", "yaw46@pitt.edu", "yiran.chen@duke.edu", "hai.li@duke.edu"], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that this is a very complex matter."}, {"heading": "2 Related work", "text": "The method reduced gradient communication and achieved a speed gain of 22% for these parsimony-based methods without affecting translation quality. An earlier study by Garg et al. [22] took the same approach, but aimed at accelerating disparities. Our proposed methods are orthogonal without affecting quantity."}, {"heading": "3 Problem Formulation and Our Approach", "text": "In fact, most of them are able to survive on their own."}, {"heading": "3.2 Convergence Analysis and Gradient Bound", "text": "We analyze the convergence of TernGrad within the framework of online learning systems. (1) We analyze the convergence of TernGrad within the framework of online learning systems. (1) We analyze the convergence of TernGrad within the framework of online learning systems. (1) We analyze the convergence of TernGrad within the framework of online learning systems. (2) We analyze the convergence of TernGrad within the framework of online learning systems. (2) We analyze the convergence of TernGrad. (2) We analyze the convergence of TernGrad. (1) We analyze the convergence of TernGrad. (2) We analyze the convergence of TernGrad. (2) We analyze the convergence of TernGrad. (2) We analyze the convergence of TernGrad. (1) The convergence of Tr. (2) is the convergence of Tr. (1) The convergence of TernGrad."}, {"heading": "3.3 Feasibility Considerations", "text": "The gradients of the TernGrad in assumption 3 are stronger than the gradients set in the standard GOGA. Shifting the two boundaries could improve the convergence of TernGrad. In assumption 3, max (G) is the maximum absolute value of all gradients in the DNN. So the maximum (G) gradient analysis could be relatively much larger than most gradients in a large DNN, implying that the gradient distribution bound in TernGrad becomes much stronger. Given the situation, we propose a layered TernGrad truncation to reduce max (G) and thus shrink the gap between these two gradients. Layered tnarization is proposed based on the observation that the gradients in each layer are changed. Instead of assuming a large global maximum scaler, we use the gradients in each layer."}, {"heading": "4 Experiments", "text": "We will first examine the convergence of TernGrad in the context of various training programs on relatively small databases and show the results in Section 4.1. Then, the scalability of TernGrad to large-scale, distributed, deep learning will be investigated and discussed in Section 4.2. the experiments will be conducted by TensorFlow [2]. We will maintain the exponential moving average of the parameters by using an exponential decay of 0.9999 [15]. Accuracy will be evaluated by the final averaged parameters, which results in a slightly better accuracy in our experiments. To be fair: In each pair of comparison experiments with suspended or ternary gradients, all other training hyper parameters are the same unless explicitly indicated differences. In experiments, if SGD is adopted with dynamics, a dynamic value of 0.9 is used."}, {"heading": "4.1 Integrating with Various Training Schemes", "text": "We study the convergence of TernGrad with LeNet on MNIST and a ConvNet [31] (called CifarNet) on CIFAR-10. LeNet is trained without data multiplication. During the training CifarNet, images are randomly cropped to 24 \u00d7 24 images and mirrored. Brightness and contrast are also randomly adjusted. During the testing of CifarNet, only medium fruit is used. Our experiments cover the scope of the SGD optimizers via Vanilla SGD, SGD with Dynamics [32] and Adam [33]. Figure 3 shows the results of LeNet. All are trained with polynomic LR decay with weight decay of 0.0005. The base learning rates of Dynamics SGD and Vanilla SGD are 0.01 and 0.1, respectively. Given the total minibatch size M and the number of workers N, the minibatch size per worker alizes."}, {"heading": "4.2 Scaling to Large-scale Deep Learning", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "5 Performance Model and Discussion", "text": "Our proposed TernGrad requires only three numerical levels {\u2212 1, 0, 1}, which can aggressively reduce communication time. Furthermore, our experiments in Section 4 show that TernGrad can converge within the same iterations to approximately the same accuracy as its corresponding baseline. Consequently, a dramatic improvement in throughput is expected on the distributed DNN education. Due to resource and time constraints, we are unfortunately unable to perform the training of more DNN models such as VggNet-A [39] and distributed training of 8 workers. We plan to continue the experiments in our future work. We opt to use a performance model to perform scalability analysis of DNN models if they are up to 512 GPUs with and without the use of TernGrad. Three neural network models - AlexNet, GoogLeNet and VggNet-A - are being investigated."}], "references": [{"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Marc'aurelio Ranzato", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V. Le", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u00edn Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint:1603.04467,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Deep learning with cots hpc systems", "author": ["Adam Coates", "Brody Huval", "Tao Wang", "David Wu", "Bryan Catanzaro", "Ng Andrew"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Benjamin Recht", "Christopher Re", "Stephen Wright", "Feng Niu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Project adam: Building an efficient and scalable deep learning training system", "author": ["Trishul M Chilimbi", "Yutaka Suzue", "Johnson Apacible", "Karthik Kalyanaraman"], "venue": "In OSDI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Petuum: A new platform for distributed machine learning on big data", "author": ["Eric P Xing", "Qirong Ho", "Wei Dai", "Jin Kyu Kim", "Jinliang Wei", "Seunghak Lee", "Xun Zheng", "Pengtao Xie", "Abhimanu Kumar", "Yaoliang Yu"], "venue": "IEEE Transactions on Big Data,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Sparknet: Training deep networks in spark", "author": ["Philipp Moritz", "Robert Nishihara", "Ion Stoica", "Michael I Jordan"], "venue": "arXiv preprint:1511.06051,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems", "author": ["Tianqi Chen", "Mu Li", "Yutian Li", "Min Lin", "Naiyan Wang", "Minjie Wang", "Tianjun Xiao", "Bing Xu", "Chiyuan Zhang", "Zheng Zhang"], "venue": "arXiv preprint:1512.01274,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Deep learning with elastic averaging sgd", "author": ["Sixin Zhang", "Anna E Choromanska", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Scaling Distributed Machine Learning with System and Algorithm Co-design", "author": ["Mu Li"], "venue": "PhD thesis,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2017}, {"title": "Scaling distributed machine learning with the parameter server", "author": ["Mu Li", "David G Andersen", "Jun Woo Park", "Alexander J Smola", "Amr Ahmed", "Vanja Josifovski", "James Long", "Eugene J Shekita", "Bor-Yiing Su"], "venue": "In OSDI,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Communication efficient distributed machine learning with the parameter server", "author": ["Mu Li", "David G Andersen", "Alexander J Smola", "Kai Yu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "More effective distributed ml via a stale synchronous parallel parameter server", "author": ["Qirong Ho", "James Cipar", "Henggang Cui", "Seunghak Lee", "Jin Kyu Kim", "Phillip B Gibbons", "Garth A Gibson", "Greg Ganger", "Eric P Xing"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Parallelized stochastic gradient descent", "author": ["Martin Zinkevich", "Markus Weimer", "Lihong Li", "Alex J Smola"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Revisiting distributed synchronous sgd", "author": ["Xinghao Pan", "Jianmin Chen", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "venue": "arXiv preprint:1702.05800,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2017}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Learning structured sparsity in deep neural networks", "author": ["Wei Wen", "Chunpeng Wu", "Yandan Wang", "Yiran Chen", "Hai Li"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Faster cnns with direct sparse convolutions and guided pruning", "author": ["J Park", "S Li", "W Wen", "PTP Tang", "H Li", "Y Chen", "P Dubey"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}, {"title": "Binarized neural networks", "author": ["Itay Hubara", "Matthieu Courbariaux", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["Mohammad Rastegari", "Vicente Ordonez", "Joseph Redmon", "Ali Farhadi"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients", "author": ["Shuchang Zhou", "Yuxin Wu", "Zekun Ni", "Xinyu Zhou", "He Wen", "Yuheng Zou"], "venue": "arXiv preprint arXiv:1606.06160,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Parallel coordinate descent for l1-regularized loss minimization", "author": ["Joseph K Bradley", "Aapo Kyrola", "Danny Bickson", "Carlos Guestrin"], "venue": "arXiv preprint arXiv:1105.5379,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Sparse communication for distributed gradient descent", "author": ["Alham Fikri Aji", "Kenneth Heafield"], "venue": "arXiv preprint:1704.05021,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2017}, {"title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns", "author": ["Frank Seide", "Hao Fu", "Jasha Droppo", "Gang Li", "Dong Yu"], "venue": "In Interspeech,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Qsgd: Communication-optimal stochastic gradient descent, with applications to training neural networks", "author": ["Ryota Tomioka", "Milan Vojnovic"], "venue": "arXiv preprint:1610.02132,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2017}, {"title": "Deep learning with limited numerical precision", "author": ["Suyog Gupta", "Ankur Agrawal", "Kailash Gopalakrishnan", "Pritish Narayanan"], "venue": "In ICML,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Gradient descent with sparsification: an iterative algorithm for sparse recovery with restricted isometry property", "author": ["Rahul Garg", "Rohit Khandekar"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Online learning and stochastic approximations. On-line learning in neural networks", "author": ["L\u00e9on Bottou"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1998}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V Le", "Ilya Sutskever", "Lukasz Kaiser", "Karol Kurach", "James Martens"], "venue": "arXiv preprint:1511.06807,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "On the momentum term in gradient descent learning algorithms", "author": ["Ning Qian"], "venue": "Neural networks,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1999}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint:1412.6980,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "On large-batch training for deep learning: Generalization gap and sharp minima", "author": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2017}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint:1502.03167,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint:1409.1556,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Performance modeling and scalability optimization of distributed deep learning systems", "author": ["Feng Yan", "Olatunji Ruwase", "Yuxiong He", "Trishul M. Chilimbi"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "The training of large-scale models with huge amounts of data are often carried on distributed systems [1][2][3][4][5][6][7][8][9], where data parallelism is adopted to exploit the compute capability empowered by multiple workers [10].", "startOffset": 102, "endOffset": 105}, {"referenceID": 1, "context": "The training of large-scale models with huge amounts of data are often carried on distributed systems [1][2][3][4][5][6][7][8][9], where data parallelism is adopted to exploit the compute capability empowered by multiple workers [10].", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "The training of large-scale models with huge amounts of data are often carried on distributed systems [1][2][3][4][5][6][7][8][9], where data parallelism is adopted to exploit the compute capability empowered by multiple workers [10].", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "The training of large-scale models with huge amounts of data are often carried on distributed systems [1][2][3][4][5][6][7][8][9], where data parallelism is adopted to exploit the compute capability empowered by multiple workers [10].", "startOffset": 111, "endOffset": 114}, {"referenceID": 4, "context": "The training of large-scale models with huge amounts of data are often carried on distributed systems [1][2][3][4][5][6][7][8][9], where data parallelism is adopted to exploit the compute capability empowered by multiple workers [10].", "startOffset": 114, "endOffset": 117}, {"referenceID": 5, "context": "The training of large-scale models with huge amounts of data are often carried on distributed systems [1][2][3][4][5][6][7][8][9], where data parallelism is adopted to exploit the compute capability empowered by multiple workers [10].", "startOffset": 117, "endOffset": 120}, {"referenceID": 6, "context": "The training of large-scale models with huge amounts of data are often carried on distributed systems [1][2][3][4][5][6][7][8][9], where data parallelism is adopted to exploit the compute capability empowered by multiple workers [10].", "startOffset": 120, "endOffset": 123}, {"referenceID": 7, "context": "The training of large-scale models with huge amounts of data are often carried on distributed systems [1][2][3][4][5][6][7][8][9], where data parallelism is adopted to exploit the compute capability empowered by multiple workers [10].", "startOffset": 123, "endOffset": 126}, {"referenceID": 8, "context": "The training of large-scale models with huge amounts of data are often carried on distributed systems [1][2][3][4][5][6][7][8][9], where data parallelism is adopted to exploit the compute capability empowered by multiple workers [10].", "startOffset": 126, "endOffset": 129}, {"referenceID": 9, "context": "The training of large-scale models with huge amounts of data are often carried on distributed systems [1][2][3][4][5][6][7][8][9], where data parallelism is adopted to exploit the compute capability empowered by multiple workers [10].", "startOffset": 229, "endOffset": 233}, {"referenceID": 3, "context": "However, as the scale of distributed systems grows up, the extensive gradient and parameter synchronizations prolong the communication time and even amortize the savings of computation time [4][11][12].", "startOffset": 190, "endOffset": 193}, {"referenceID": 10, "context": "However, as the scale of distributed systems grows up, the extensive gradient and parameter synchronizations prolong the communication time and even amortize the savings of computation time [4][11][12].", "startOffset": 193, "endOffset": 197}, {"referenceID": 11, "context": "However, as the scale of distributed systems grows up, the extensive gradient and parameter synchronizations prolong the communication time and even amortize the savings of computation time [4][11][12].", "startOffset": 197, "endOffset": 201}, {"referenceID": 0, "context": "A common approach to overcome such a network bottleneck is asynchronous SGD [1][4][7][12][13][14], which continues computation by using stale values without waiting for the completeness of synchronization.", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "A common approach to overcome such a network bottleneck is asynchronous SGD [1][4][7][12][13][14], which continues computation by using stale values without waiting for the completeness of synchronization.", "startOffset": 79, "endOffset": 82}, {"referenceID": 6, "context": "A common approach to overcome such a network bottleneck is asynchronous SGD [1][4][7][12][13][14], which continues computation by using stale values without waiting for the completeness of synchronization.", "startOffset": 82, "endOffset": 85}, {"referenceID": 11, "context": "A common approach to overcome such a network bottleneck is asynchronous SGD [1][4][7][12][13][14], which continues computation by using stale values without waiting for the completeness of synchronization.", "startOffset": 85, "endOffset": 89}, {"referenceID": 12, "context": "A common approach to overcome such a network bottleneck is asynchronous SGD [1][4][7][12][13][14], which continues computation by using stale values without waiting for the completeness of synchronization.", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "A common approach to overcome such a network bottleneck is asynchronous SGD [1][4][7][12][13][14], which continues computation by using stale values without waiting for the completeness of synchronization.", "startOffset": 93, "endOffset": 97}, {"referenceID": 14, "context": "The inconsistency of parameters across computing workers, however, can degrade training accuracy and incur occasional divergence [15][16].", "startOffset": 129, "endOffset": 133}, {"referenceID": 15, "context": "From the perspective of inference acceleration, sparse and quantized Deep Neural Networks (DNNs) have been widely studied, such as [17][18][19][20][21][22].", "startOffset": 131, "endOffset": 135}, {"referenceID": 16, "context": "From the perspective of inference acceleration, sparse and quantized Deep Neural Networks (DNNs) have been widely studied, such as [17][18][19][20][21][22].", "startOffset": 135, "endOffset": 139}, {"referenceID": 17, "context": "From the perspective of inference acceleration, sparse and quantized Deep Neural Networks (DNNs) have been widely studied, such as [17][18][19][20][21][22].", "startOffset": 139, "endOffset": 143}, {"referenceID": 18, "context": "From the perspective of inference acceleration, sparse and quantized Deep Neural Networks (DNNs) have been widely studied, such as [17][18][19][20][21][22].", "startOffset": 143, "endOffset": 147}, {"referenceID": 19, "context": "From the perspective of inference acceleration, sparse and quantized Deep Neural Networks (DNNs) have been widely studied, such as [17][18][19][20][21][22].", "startOffset": 147, "endOffset": 151}, {"referenceID": 20, "context": "From the perspective of inference acceleration, sparse and quantized Deep Neural Networks (DNNs) have been widely studied, such as [17][18][19][20][21][22].", "startOffset": 151, "endOffset": 155}, {"referenceID": 3, "context": "Researches such as sparse logistic regression and Lasso optimization problems [4][12][23] took advantage of the sparsity inherent in models and achieved remarkable speedup for distributed training.", "startOffset": 78, "endOffset": 81}, {"referenceID": 11, "context": "Researches such as sparse logistic regression and Lasso optimization problems [4][12][23] took advantage of the sparsity inherent in models and achieved remarkable speedup for distributed training.", "startOffset": 81, "endOffset": 85}, {"referenceID": 21, "context": "Researches such as sparse logistic regression and Lasso optimization problems [4][12][23] took advantage of the sparsity inherent in models and achieved remarkable speedup for distributed training.", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "For instance, Aji and Heafield [24] proposed to heuristically sparsify dense gradients by dropping off small values in order to reduce gradient communication.", "startOffset": 31, "endOffset": 35}, {"referenceID": 20, "context": "For the same purpose, quantizing gradients to low-precision values with smaller bit width has also been extensively studied [22][25][26][27].", "startOffset": 124, "endOffset": 128}, {"referenceID": 23, "context": "For the same purpose, quantizing gradients to low-precision values with smaller bit width has also been extensively studied [22][25][26][27].", "startOffset": 128, "endOffset": 132}, {"referenceID": 24, "context": "For the same purpose, quantizing gradients to low-precision values with smaller bit width has also been extensively studied [22][25][26][27].", "startOffset": 132, "endOffset": 136}, {"referenceID": 25, "context": "For the same purpose, quantizing gradients to low-precision values with smaller bit width has also been extensively studied [22][25][26][27].", "startOffset": 136, "endOffset": 140}, {"referenceID": 22, "context": "Aji and Heafield [24] proposed a heuristic gradient sparsification method that truncated the smallest gradients and transmitted only the remaining large ones.", "startOffset": 17, "endOffset": 21}, {"referenceID": 26, "context": "[28] adopted the similar approach, but targeted at sparsity recovery instead of training acceleration.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "DoReFa-Net [22] derived from AlexNet reduced the bit widths of weights, activations and gradients to 1, 2 and 6, respectively.", "startOffset": 11, "endOffset": 15}, {"referenceID": 25, "context": "[27] successfully trained neural networks on MNIST and CIFAR-10 datasets using 16-bit numerical precision for an energy-efficient hardware accelerator.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] applied 1-bit SGD to accelerate distributed training and empirically verified its effectiveness in speech applications.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "So it cannot yield any speed benefit on convolutional neural networks [26].", "startOffset": 70, "endOffset": 74}, {"referenceID": 23, "context": "Moreover, \u201ccold start\u201d of the method [25] requires floating-point gradients to converge to a good initial point for the following 1-bit SGD.", "startOffset": 37, "endOffset": 41}, {"referenceID": 24, "context": "[26] presented QSGD that explores the trade-off between accuracy and gradient precision.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Note that parameter server in most implementations [1][12] are used to preserve shared parameters, while here we utilize it in a slightly different way to maintain shared gradients.", "startOffset": 51, "endOffset": 54}, {"referenceID": 11, "context": "Note that parameter server in most implementations [1][12] are used to preserve shared parameters, while here we utilize it in a slightly different way to maintain shared gradients.", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "Note that our proposed TernGrad can be integrated with many settings like Asynchronous SGD [1][4], even though the scope of this paper only focuses on the distributed SGD in Figure 1.", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "Note that our proposed TernGrad can be integrated with many settings like Asynchronous SGD [1][4], even though the scope of this paper only focuses on the distributed SGD in Figure 1.", "startOffset": 94, "endOffset": 97}, {"referenceID": 24, "context": "This stochastic rounding, instead of deterministic one, is chosen by both our study and QSGD [26], as stochastic rounding has an unbiased expectation and has been successfully studied for low-precision processing [20][27].", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "This stochastic rounding, instead of deterministic one, is chosen by both our study and QSGD [26], as stochastic rounding has an unbiased expectation and has been successfully studied for low-precision processing [20][27].", "startOffset": 213, "endOffset": 217}, {"referenceID": 25, "context": "This stochastic rounding, instead of deterministic one, is chosen by both our study and QSGD [26], as stochastic rounding has an unbiased expectation and has been successfully studied for low-precision processing [20][27].", "startOffset": 217, "endOffset": 221}, {"referenceID": 27, "context": "In General Online Gradient Algorithm (GOGA) [29], parameter is updated at learning rate \u03b3t as wt+1 = wt \u2212 \u03b3tgt = wt \u2212 \u03b3t \u00b7 \u2207wQ(zt,wt), (4)", "startOffset": 44, "endOffset": 48}, {"referenceID": 27, "context": "The convergence analysis of TernGrad is adapted from the convergence proof of GOGA presented in [29], which uses the following two assumptions.", "startOffset": 96, "endOffset": 100}, {"referenceID": 27, "context": "Bottou [29] proved Lemma 1.", "startOffset": 7, "endOffset": 11}, {"referenceID": 27, "context": "Comparing with the gradient bound of standard GOGA E { ||g|| } \u2264 A+B ||w \u2212w\u2217|| [29], the bound in Assumption 3 is stronger because max(abs(g)) \u00b7 ||g||1 \u2265 ||g||.", "startOffset": 79, "endOffset": 83}, {"referenceID": 28, "context": "A side benefit of our work is that, by following the similar proof procedure, we can prove the convergence of GOGA when Gaussian noise N (0, \u03c3) is added to gradients [30], under the gradient bound of E { ||g|| } \u2264 A + B ||w \u2212w\u2217|| \u2212 \u03c3.", "startOffset": 166, "endOffset": 170}, {"referenceID": 28, "context": "Although the bound is also stronger, Gaussian noise encourages active exploration of parameter space and improves accuracy as was empirically studied in [30].", "startOffset": 153, "endOffset": 157}, {"referenceID": 29, "context": "Specifically, we used a CNN [31] trained on CIFAR-10 by momentum SGD with staircase learning rate and obtained the optimal c = 2.", "startOffset": 28, "endOffset": 32}, {"referenceID": 1, "context": "The experiments are performed by TensorFlow[2].", "startOffset": 43, "endOffset": 46}, {"referenceID": 14, "context": "9999 [15].", "startOffset": 5, "endOffset": 9}, {"referenceID": 29, "context": "1 Integrating with Various Training Schemes We study the convergence of TernGrad using LeNet on MNIST and a ConvNet [31] (named as CifarNet) on CIFAR-10.", "startOffset": 116, "endOffset": 120}, {"referenceID": 30, "context": "Our experiments cover the scope of SGD optimizers over vanilla SGD, SGD with momentum [32] and Adam [33].", "startOffset": 86, "endOffset": 90}, {"referenceID": 31, "context": "Our experiments cover the scope of SGD optimizers over vanilla SGD, SGD with momentum [32] and Adam [33].", "startOffset": 100, "endOffset": 104}, {"referenceID": 32, "context": "This is because parameters are updated less frequently and large-batch training tends to converge to poorer sharp minima [34].", "startOffset": 121, "endOffset": 125}, {"referenceID": 32, "context": "However, the noise inherent in TernGrad can help converge to better flat minimizers [34], which could explain the smaller accuracy gap between the baseline and TernGrad when the mini-batch size is 2048.", "startOffset": 84, "endOffset": 88}, {"referenceID": 33, "context": "99% in ResNet-152 [35].", "startOffset": 18, "endOffset": 22}, {"referenceID": 34, "context": "All DNNs are trained by momentum SGD with Batch Normalization [36] on convolutional layers.", "startOffset": 62, "endOffset": 66}, {"referenceID": 35, "context": "GoogLeNet is trained by polynomial LR decay and data augmentation in [37].", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "To overcome this problem, we increase the learning rate for large-batch scenario [10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 28, "context": "92% when mini-batch size is 1024, because its inherent randomness encourages to escape from poorer sharp minima [30][34].", "startOffset": 112, "endOffset": 116}, {"referenceID": 32, "context": "92% when mini-batch size is 1024, because its inherent randomness encourages to escape from poorer sharp minima [30][34].", "startOffset": 116, "endOffset": 120}, {"referenceID": 36, "context": "Due to the resource and time constraint, unfortunately, we aren\u2019t able to perform the training of more DNN models like VggNet-A [39] and distributed training beyond 8 workers.", "startOffset": 128, "endOffset": 132}, {"referenceID": 37, "context": "Here, we extend the performance model that was initially developed for CPU-based deep learning systems [40] to estimate the performance of distributed GPUs/machines.", "startOffset": 103, "endOffset": 107}], "year": 2017, "abstractText": "High network communication cost for synchronizing gradients and parameters is the well-known bottleneck of distributed training. In this work, we propose TernGrad that uses ternary gradients to accelerate distributed deep learning in data parallelism. Our approach requires only three numerical levels {\u22121, 0, 1}, which can aggressively reduce the communication time. We mathematically prove the convergence of TernGrad under the assumption of a bound on gradients. Guided by the bound, we propose layer-wise ternarizing and gradient clipping to improve its convergence. Our experiments show that applying TernGrad on AlexNet doesn\u2019t incur any accuracy loss and can even improve accuracy. The accuracy loss of GoogLeNet induced by TernGrad is less than 2% on average. Finally, a performance model is proposed to study the scalability of TernGrad. Experiments show significant speed gains for various deep neural networks.", "creator": "LaTeX with hyperref package"}}}