{"id": "1705.08439", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Thinking Fast and Slow with Deep Learning and Tree Search", "abstract": "Solving sequential decision making problems, such as text parsing, robotic control, and game playing, requires a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration, a novel algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. In contrast, standard Deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that our method substantially outperforms Policy Gradients in the board game Hex, winning 84.4% of games against it when trained for equal time.", "histories": [["v1", "Tue, 23 May 2017 17:48:51 GMT  (650kb,D)", "http://arxiv.org/abs/1705.08439v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["thomas anthony", "zheng tian", "david barber"], "accepted": true, "id": "1705.08439"}, "pdf": {"name": "1705.08439.pdf", "metadata": {"source": "CRF", "title": "Thinking Fast and Slow with Deep Learning and Tree Search", "authors": ["Thomas W. Anthony"], "emails": ["thomas.anthony.14@ucl.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "According to experts, human thinking consists of two different types of thinking. System 1 is a fast, unconscious, and automatic mode of thinking, also known as intuition or heuristic process. System 2, an evolutionary new process reserved for humans, is a slow, explicit, and rules-based mode of reasoning. When you learn to complete a challenging planning task, such as a board game, people take advantage of both processes: strong intuitions enable more effective analytical reasoning by quickly selecting interesting moves to look at."}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Markov Decision Processes", "text": "In a final state sT, an episodic reward R is observed, which we intend to maximize. 1 We can easily extend it to two players, perfect information, and zero-sum games by learning guidelines for both players simultaneously, aimed at maximizing the reward for the respective player. We refer to a distribution of actions available in a state as guidelines and call them \u03c0 (a | s). The value function V \u03c0 (s) is the mean reward resulting from what begins in a state. By Q\u03c0 (s, a), we mean the expected reward when we take action in a state and then follow action. The advantage function A\u03c0 (s, a) = Q\u03c0 (s, a) \u2212 maxa \u2032 Q \u03c0 (s, a) represents the value of an action relative to the best action available. This advantage function can be composed as a reward (s, a)."}, {"heading": "2.2 Imitation Learning", "text": "The policy we learn through this mimicry is called apprenticeship policy. We create a data set with states of the expert game, together with some target data that we try to predict from the expert. Several target data have been used. The simplest approach is simply to ask the expert to name an optimal step \u03c0 (a | s) [5]. Once we can predict expert movements, we can take the actions that we think the expert would most likely have taken. Another approach is to estimate the action-value function Qjunction (s, a) [6]. We can then predict this function and act greedily in relation to it. Unlike the first method, this goal is cost-sensitive, i.e. the apprentice can exchange predictive errors for costs."}, {"heading": "3 Expert iteration", "text": "Compared to IL techniques, Expert Iteration (ExIt) is enriched by an Expert Improvement Step. Improving the Expert Player and then solving the Imitation Learning Problem allows us to exploit the fast convergence properties of Imitation Learning even in contexts where no strong player was originally known. To solve such problems, researchers resort to RL algorithms, which often suffer from slow convergence, high variance, and local minima.For each i iteration, the algorithm proceeds as follows: We create a number of states through the self-play of the apprentice. In each of these states, we calculate an Expert Step."}, {"heading": "3.1 Choice of expert and apprentice", "text": "The learning rate of ExIt is determined by two factors: the size of the achievement gap between the apprentice policy and the improved expert; and how close the performance of the new apprentice is to the expert he learns from. The former induces an upper limit on the performance of the new apprentice at each iteration, while the latter describes how close we are to that upper limit. Choosing both the expert and the apprentice can have a significant impact on both factors, so they must be considered jointly. The role of the expert is to conduct exploration and thus accurately determine strong movements from a single position. The role of the apprentice is to generalize the strategies the expert discovers across the state and to quickly access this strong policy for boatstrapping. The canonical choice of the expert is a tree search algorithm. The search takes into account the exact dynamics of the wild tree locally for the state concerned and can be seen as analogous to the predictive human games that players engage in planning their movements."}, {"heading": "3.2 Distributed Expert Iteration", "text": "ExIt spends most of its runtime generating a large dataset of expert plans, because our tree search is orders of magnitude slower than evaluations of neural networks (in our example, the 250,000 samples generated per iteration account for 98% of runtime).Creating this dataset is an embarrassingly parallel task, and the plans created can be grouped together by a target that measures well under 1KB, meaning that ExIt can be trivially paralleled across distributed architectures, even at very low bandwidth."}, {"heading": "3.3 Online expert iteration", "text": "Since the creation of our data sets is computationally intensive, this can significantly increase the runtime of the algorithm. The online version of ExIt mitigates this by aggregating all the data sets generated so far with each iteration. In other words, instead of training them on Di, we train them on D = j \u2264 iDj. Such data set aggregation is similar to the DAgger algorithm [5]. In fact, the removal of the Expert Improvement step from the Online ExIt reduces it to DAgger. Data set aggregation in Online ExIt allows us to request fewer movement options from the expert for each iteration, while maintaining a large dataset at the same time. By increasing the frequency at which improvements can be made, the apprentice in Online ExIt can generalize the expert more quickly, and thus the expert improves faster, which should lead to a higher playing quality in the dataset."}, {"heading": "4 Imitation Learning in the game Hex", "text": "The algorithmic requirements of the ExIt algorithm are an imitation learning procedure and an expert improvement procedure. In this section, we develop the techniques for our imitation learning step and test them for Imitation Learning of Monte Carlo Tree Search (MCTS). We use this test because our intended expert in ExIt is a version of Neural MCTS, which is described in Section 5."}, {"heading": "4.1 Preliminaries", "text": "HexHex is a two-player based game played on a hexagonal grid. Players, identified by the colors black and white, alternately place stones of their color into empty squares. The black player wins if there is otherwise a sequence of adjacent black stones connecting the northern edge of the board to the southern edge. White wins if he reaches a sequence of adjacent white stones running from the western edge to the eastern edge. (See Figure 1).Hex has a deep strategy that challenges it to the Deep Reinforcement Learning algorithms; its large set of actions and connection-based rules mean that he shares similar challenges for AI to Go. Moreover, because the winning condition is mutually exclusive (e.g. if black cannot have a winner), its rules are simple and game order permutations are irrelevant to the outcome of a game, games can be efficiently simulated, making it an ideal test bed for Reinforcement Learning."}, {"heading": "4.2 Imitation Learning from Monte Carlo Tree Search", "text": "In this section, we will select a standardized neural network to mimic an MCTS expert. [11] We have a similar approach to Atari games, but their results show that the performance of the learned neural network has actually fallen short of the MCTS expert, even if there is a large dataset of 800,000 MCTS moves. [10] Our network architecture is essentially based on this performance. [12] We use ADAM as our optimization goals. [11] The learning goal we use was simply the movement chosen by MCTS, so sL is just an extension after several visions. [12] Our network architecture is described in the appendix. We use ADAM [12] as the optimization of the distribution of TargetsIn Guo et al. [11], the learning goal chosen by MCTS."}, {"heading": "4.3 Results of Imitation Learning", "text": "Based on our original record of 100,000 MCTS moves, CAT performs marginally better than TPT in predicting the turn selected by MCTS, with prediction errors of 52.2% and 50.0%, respectively, and prediction errors of 70.9% and 69.1%. Despite similar prediction errors, the TPT network wins 93% of games against the CAT network, confirming that TPT's cost-consciousness brings about a significant performance improvement. We continued to train the TPT network using the DAgger algorithm and created three additional batches of 100,000 moves in a row, resulting in an improvement of 120 Elo over the first TPT network. Our last DAgger TPT network achieved similar performance to the MCTS it was emulated, winning just over half of the games played between them (87 / 162)."}, {"heading": "5 Expert Improvement in Hex", "text": "In this section, we describe our Neural MCTS (N-MCTS) algorithm, which uses such apprenticeship networks to improve search quality. As the apprentice network has effectively generalized our policies, we can make quick assessments of the plausibility of actions at the beginning of the search. As the search progresses, we discover improvements in this apprenticeship policy, which is only available to human players through Lookahead.As discussed in Section 4.2, the objectives of tree policy can be seen as a monotonous transformation of the benefit function, which informs the decision on how we use the neural network to improve our tree policy.When a node is expanded, we use the neural network to estimate A (s, a) for each available action, and add the estimates to the UCT formula to give our neural network-supported UCT formula."}, {"heading": "6 Performance of Expert Iteration in Hex", "text": "In sections 4 and 5, we have developed the two most important processes required for the ExIt algorithm. In this section, we present the performance of the resulting algorithm."}, {"heading": "6.1 ExIt parameters", "text": "We test both the batch version of the ExIt algorithm and the online version. In the batch version, we perform two iterations. In each iteration, we spend the same processing time creating our data. Our first iteration generated a dataset of 291276 expert strokes and the second 231012 strokes. The lower number of strokes was due to the fact that the second iteration expert conducted a deeper search and therefore reached the expansion threshold somewhat more frequently. Between the first and second iteration, we also revised the parameters of the N-MCTS expert to make the most of the new NN-MCTS parameters. N-MCTS parameters are included in the appendix. In the online version, we start by creating a dataset of 40176 expert strokes. With each iteration, we increase the size of this dataset by 3240 strokes of the latest expert. We solve the monitored learning problem from scratch."}, {"heading": "6.2 Benchmark algorithm", "text": "We compare ExIt with the policy gradient algorithm from Silver et al. [13], which provided state-of-the-art performance for a neural network player in the board game Go. In Silver et al. [13], the algorithm was initialized by a network trained to predict human moves from a corpus of 30 million positions, and then REINFORCE [3] was used. Such a scheme, the initialization of imitation learning followed by an improvement in reinforcement learning, is a common approach when well-known experts are not strong enough. In our tests, both the REINFORCE algorithm and ExIt are initialized to the strongest TPT network learned in Section 4. For REINFORCE, the network weights were stored after each training period, and after completion of the training, we searched for the most powerful network among the learned that corrects itself for the highly varied updates that the policy gradient algorithms suffer from."}, {"heading": "6.3 Results", "text": "Since Hex has a strong advantage for the first player, we measure performance between two networks by playing two games based on each possible opening move, with each player playing one black and one white. Our results are shown in Figure 2. They show that Expert Iteration significantly exceeds political gradients. In a direct game against the strongest political gradient network, batch ExIt won 74.7% of the games. Online ExIt beat the strongest political gradient network in 84.4% of games. The results show that ExIt is a much stronger learning algorithm than political gradients. Online Exit is initially able to quickly learn strong game, indicating an advantage over batch mode. However, as the number of iterations grows, so does the size of the dataset for the online algorithm. Training on this large dataset starts to take a long time, so improvements become flatter."}, {"heading": "7 Related work", "text": "ExIt has several connections to existing RL algorithms resulting from different expert class decisions. Unlike Q schemes, for example, DQN [4] can be considered a special case of ExIt, where the expert performs a one-step single-step outlook; the apprentice is represented by the function Qi (s, a) = QTB (s, a), and the learning objectives of the i + 1 iteration are given by Q\u03c0 i (st, a) = rt + Qi (st + 1, a) (rt denotes the sampled instant reward); we can also recreate a version of the iteration of policy [14] by using Monte Carlo Search as our expert; in this case, it is easy to see that tree hunting in Monte Carlo offers a stronger plan than Monte Carlo Search.Previous work has also attempted to achieve imitation experiences that surpass the original expert. Silver et al. [13] use imitation-driven learning followed by Reinforcement Learning."}, {"heading": "8 Conclusion and Future Work", "text": "We have introduced a new Reinforcement Learning algorithm, Expert Iteration, which is motivated by the dual process theory of human thought. ExIt dismantles the Reinforcement Learning problem by separating the problems of generalization and planning. Planning is done on a case-by-case basis, and only when a strong plan is found is the resulting policy generalized, enabling long-term planning and leading to faster learning, especially when challenging problems. ExIt is almost embarrassingly comparable, even for distributed systems with low network bandwidth. By contrast, standard Reinforcement Learning methods often need to communicate weight updates in order to be parallelized, and this communication is usually limited by network bandwidth. We show that this algorithm significantly outperforms a variant of the REINFORCE algorithm in learning to play the board game Hex."}, {"heading": "A Fast calculation of expert moves", "text": "Since the computation of neural networks in the batch is faster and is executed on a GPU, most implementations of Neural-MCTS compute their neural networks asynchronously: When a node is expanded, the position is added to a GPU spreadsheet queue, but the search continues. Once the length of the queue reaches the desired batch size B, the policy of the neural network for the first B states in the queue can be calculated, and the information is added to the corresponding nodes. Compared to waiting until the evaluation has taken place, this asynchronous computation of the neural network significantly increases the rate at which MCTS iterations can take place: Batching neural network computations improves the GPU throughput, and the CPU never waits idle for evaluations. However, since the search continues before the evaluation, suboptimal movements are made in the tree, where we can simultaneously compute the CTS, where we can't compute the previous information from three times in the CPU."}, {"heading": "B Monte Carlo Tree Search Parameters and Rapid Action Value Estimation (RAVE)", "text": "RAVE is a technique for providing estimates of the values of movements in the search tree faster in the early stages of exploring a state than with UCT alone.This is important because the Monte Carlo value heuristically requires multiple samples to achieve a low variance estimate of the value, which is particularly problematic when there are many actions available. (RAVE) A common feature of many games is that a move that is strong at a time t2, probably also at a time t1 < t2. For example, in stone placement games such as Go and Hex, if a cell is useful, it may also have been beneficial to claim it earlier. RAVE tries to use this heuristic approach for many actions by a single UCT. RAVE statistics nRAVE (s), nRAVE (s, a) and rRAVE (s, a) are stored that correspond to those used in normal UCT statistics."}, {"heading": "C Neural Network Architecture", "text": "In fact, it is so that most of them are able to move, while they are able to move, most of them are able to move, most of them are able to move, most of them are able to move, most of them are not able to move, most of them are able to move, most of them are able to move, most of them are able to move, most of them are able to move, most of them are not able to move, most of them are not able to move, most of them are not able to move, most of them are able to move, most of them are not able to move."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Solving sequential decision making problems, such as text parsing, robotic<lb>control, and game playing, requires a combination of planning policies and gen-<lb>eralisation of those plans. In this paper, we present Expert Iteration, a novel al-<lb>gorithm which decomposes the problem into separate planning and generalisation<lb>tasks. Planning new policies is performed by tree search, while a deep neural net-<lb>work generalises those plans. In contrast, standard Deep Reinforcement Learning<lb>algorithms rely on a neural network not only to generalise plans, but to discover<lb>them too. We show that our method substantially outperforms Policy Gradients in<lb>the board game Hex, winning 84.4% of games against it when trained for equal<lb>time.", "creator": "LaTeX with hyperref package"}}}