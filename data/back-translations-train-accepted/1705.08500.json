{"id": "1705.08500", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Selective Classification for Deep Neural Networks", "abstract": "Selective classification techniques (also known as reject option) have not yet been considered in the context of deep neural networks (DNNs). These techniques can potentially significantly improve DNNs prediction performance by trading-off coverage. In this paper we propose a method to construct a selective classifier given a trained neural network. Our method allows a user to set a desired risk level. At test time, the classifier rejects instances as needed, to grant the desired risk (with high probability). Empirical results over CIFAR and ImageNet convincingly demonstrate the viability of our method, which opens up possibilities to operate DNNs in mission-critical applications. For example, using our method an unprecedented 2% error in top-5 ImageNet classification can be guaranteed with probability 99.9%, and almost 60% test coverage.", "histories": [["v1", "Tue, 23 May 2017 19:43:56 GMT  (198kb,D)", "https://arxiv.org/abs/1705.08500v1", null], ["v2", "Thu, 1 Jun 2017 14:10:34 GMT  (198kb,D)", "http://arxiv.org/abs/1705.08500v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["yonatan geifman", "ran el-yaniv"], "accepted": true, "id": "1705.08500"}, "pdf": {"name": "1705.08500.pdf", "metadata": {"source": "CRF", "title": "Selective Classification for Deep Neural Networks", "authors": ["Yonatan Geifman"], "emails": ["yonatan.g@cs.technion.ac.il", "rani@cs.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "While we cannot rely on an illusionary, elusive concept of \"singularity,\" in which the question is to what extent it is actually a pure \"system,\" the question is why it has come to this point: \"What is it?,\" \"What is it?,\" \"What is it?,\" \"What is it?,\" \"What is it?,\" \"What is it?,\" \"What is it?,\" \"What is it?,\" What is it?, \"What is it?,\" \"What is it?,\" What is it?, \"What is it?,\" What is it?, \"What is it?,\" What is it?, \"what is it?,\" what is it?, \"what is it?,\" what is it?, \"what is it?,\" what is it?, \"what is it?"}, {"heading": "2 Problem Setting", "text": "Let X be a attribute space (e.g. raw image data) and Y, a finite label set, Y = {1, 2, 3,.., k} representing k classes. Let P (X, Y) be a distribution over X > Y. A classifier f is a function f: X \u2192 Y, and the actual risk of f w.r.t. P is R (f | P) = 1 (X, Y) ['(f (x), y)], where \": Y \u00b7 Y \u2192 R + is a given risk function, for example the 0 / 1 error. Given a labeled set Sm = {(xi, yi)} mi = 1 (X \u00b7 Y) sampled i.d. by P (X, Y)]] the empirical risk is f-identifier f x, x x."}, {"heading": "3 Selection with Guaranteed Risk Control", "text": "In this section, we present a general technique for constructing a selection function with guaranteed performance, based on a predefined classification f = >. We assume that the selection function f (x1) is no higher in the sense than the confidence in the prediction f (x1). In this section, we do not deal with the question of what is a good selection option (which is discussed in Section 4); our goal is to generate a selection function g (x2), with guaranteed performance for a given performance. To commemorate this work, the loss function is taken as the default 0 / 1 loss function (unless explicitly stated otherwise)."}, {"heading": "4 Confidence-Rate Functions for Neural Networks", "text": "In this section we will look at two confidence functions based on previous work."}, {"heading": "5 Empirical Results", "text": "In Section 4, we introduced the SR and MC dropout confidence rate function, which is defined for a particular model f. We trained VGG models [17] for CIFAR-10, CIFAR-100 and ImageNet. Whilst Theorem 3.2 always applies, we find that the limit of the resulting selective classifier can be far from the target risk at severe distortion (far from ideal). In Figure 2, we present the risk coverage curves obtained for each of the three datasets. These curves were achieved by calculating a validation risk and coverage with MC dropout confidence rates for many validation values. It is obvious that the risk coverage profile for SR and MC dropout datasets is almost identical."}, {"heading": "5.1 Selective Guaranteed Risk for CIFAR-10", "text": "We used the VGG-16 architecture [17] and adapted it to the CIFAR-10 dataset by adding massive dropouts, just as described in [15]. We used data augmentation that included horizontal flips, vertical and horizontal shifts and rotations, and trained with SGD at a dynamics of 0.9, an initial learning rate of 0.1, and a weight loss of 0.0005. We applied the SGR algorithm to f10 with the SR confidence assessment function, and traced for 250 epochs. With this setting, we achieved a validation accuracy of 93.54 and used the resulting network f10 as the basis for our selective classification. We applied the SGR algorithm to f10 with the SR confidence function, where the SR rating function, the training set for SGR 10, was randomly divided into two parts, the SGAR standardization."}, {"heading": "5.2 Selective Guaranteed Risk for CIFAR-100", "text": "Using the same VGG architecture (now adapted to 100 classes), we trained a model for CIFAR-100 using the same data expansion routine as in the CIFAR-10 experiment. Following exactly the same experimental design as in the CFAR-10 case, we also obtained the results of Table 2Here, SGR generated narrow limits that were very close to the desired target risk, and the limits were never violated by the actual risk. Furthermore, we see again that it is possible to drastically reduce the risk if only moderate compromises are made in coverage. While the architecture used is remarkably small and the number of reported experiments is small (6-7 lines in each table), we did not perform a bonferroni correction (which can be easily added)."}, {"heading": "5.3 Selective Guaranteed Risk for ImageNet", "text": "We used an already trained Image-Net VGG-16 model based on ILSVRC2014 [16]. We repeated the same experimental design, but now the size of the training and test set was about 25,000. SGR results for both the TOP-1 and TOP-5 classification tasks are summarized in Tables 3 and 4 respectively. We also implemented the RESNET-50 architecture [12] to see if qualitatively similar results can be achieved with a different architecture. RESNET-50 results for ImageNet TOP-1 and TOP-5 classification tasks are summarized in Tables 5 and 6 respectively. These results show that our selective classifiers are highly effective even for the demanding mageNet with both the VGGG and RESNET architectures, and easily surpass the best-known results for ImageNet with appropriate coverage compromises."}, {"heading": "6 Concluding Remarks", "text": "We have presented an algorithm for learning a selective classifier whose risk can be fully controlled and guaranteed with high confidence. Our empirical study validated this algorithm using challenging image classification data sets and showed that guaranteed risk control is achievable. Our methods can be applied immediately by deep learning practitioners who help them cope with mission critical tasks. We believe that our work is only the first significant step in this direction and many research questions remain open. Starting point of our approach is a trained neural classifier f (supposedly trained to optimize risks under full coverage).While the rejection mechanisms we are considering are highly effective, it may be possible to identify superior mechanisms for a particular classifier f. However, we believe that the most difficult open question would be to identify both the classifier f and the selective function in order to train the selective context to optimize the coverage for a particular classifier."}, {"heading": "Acknowledgments", "text": "This research was supported by the Israel Science Foundation (grant no. 1890 / 14)."}], "references": [{"title": "An optimum character recognition system using decision functions", "author": ["Chao K Chow"], "venue": "IRE Transactions on Electronic Computers,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1957}, {"title": "A method for improving classification reliability of multilayer perceptrons", "author": ["Luigi Pietro Cordella", "Claudio De Stefano", "Francesco Tortorella", "Mario Vento"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "Boosting with abstention", "author": ["Corinna Cortes", "Giulia DeSalvo", "Mehryar Mohri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "To reject or not to reject: that is the question-an answer in case of neural classifiers", "author": ["Claudio De Stefano", "Carlo Sansone", "Mario Vento"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "On the foundations of noise-free selective classification", "author": ["R. El-Yaniv", "Y. Wiener"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Active learning via perfect selective classification", "author": ["Ran El-Yaniv", "Yair Wiener"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Generalization bounds for averaged classifiers", "author": ["Yoav Freund", "Yishay Mansour", "Robert E Schapire"], "venue": "Annals of Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Support vector machines with embedded reject option. In Pattern recognition with support vector machines", "author": ["Giorgio Fumera", "Fabio Roli"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Dropout as a bayesian approximation: representing model uncertainty in deep learning", "author": ["Yarin Gal", "Zoubin Ghahramani"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Distribution-free performance bounds with the resubstitution error estimate", "author": ["O. Gascuel", "G. Caraux"], "venue": "Pattern Recognition Letters,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1992}, {"title": "The Relationship Between Agnostic Selective Classification and Active", "author": ["R. Gelbhart", "R. El-Yaniv"], "venue": "ArXiv e-prints,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "The nearest neighbor classification rule with a reject option", "author": ["Martin E Hellman"], "venue": "IEEE Transactions on Systems Science and Cybernetics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1970}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Very deep convolutional neural network based image classification using small training sample size", "author": ["Shuying Liu", "Weihong Deng"], "venue": "In Pattern Recognition (ACPR),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "ImageNet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei- Fei"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "A risk bound for ensemble classification with a reject option", "author": ["Kush R Varshney"], "venue": "In Statistical Signal Processing Workshop (SSP), 2011 IEEE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Wide residual networks", "author": ["Sergey Zagoruyko", "Nikos Komodakis"], "venue": "arXiv preprint arXiv:1605.07146,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "The subfield dealing with such capabilities in machine learning is called selective prediction (also known as prediction with a reject option), which has been around for 60 years [1, 5].", "startOffset": 179, "endOffset": 185}, {"referenceID": 4, "context": "The subfield dealing with such capabilities in machine learning is called selective prediction (also known as prediction with a reject option), which has been around for 60 years [1, 5].", "startOffset": 179, "endOffset": 185}, {"referenceID": 7, "context": "The literature on the reject option is quite extensive and mainly discusses rejection mechanisms for various hypothesis classes and learning algorithms, such as SVM, boosting, and nearestneighbors [8, 13, 3].", "startOffset": 197, "endOffset": 207}, {"referenceID": 12, "context": "The literature on the reject option is quite extensive and mainly discusses rejection mechanisms for various hypothesis classes and learning algorithms, such as SVM, boosting, and nearestneighbors [8, 13, 3].", "startOffset": 197, "endOffset": 207}, {"referenceID": 2, "context": "The literature on the reject option is quite extensive and mainly discusses rejection mechanisms for various hypothesis classes and learning algorithms, such as SVM, boosting, and nearestneighbors [8, 13, 3].", "startOffset": 197, "endOffset": 207}, {"referenceID": 1, "context": "Existing NN works consider a cost-based rejection model [2, 4], whereby the costs of misclassification and abstaining must be specified, and a rejection mechanism is optimized for these costs.", "startOffset": 56, "endOffset": 62}, {"referenceID": 3, "context": "Existing NN works consider a cost-based rejection model [2, 4], whereby the costs of misclassification and abstaining must be specified, and a rejection mechanism is optimized for these costs.", "startOffset": 56, "endOffset": 62}, {"referenceID": 4, "context": ") Here we consider the alternative risk-coverage view for selective classification discussed in [5].", "startOffset": 96, "endOffset": 99}, {"referenceID": 17, "context": "Ensemble techniques have been considered for selective (and confidence-rated) prediction, where rejection mechanisms are typically based on the ensemble statistics [18, 7].", "startOffset": 164, "endOffset": 171}, {"referenceID": 6, "context": "Ensemble techniques have been considered for selective (and confidence-rated) prediction, where rejection mechanisms are typically based on the ensemble statistics [18, 7].", "startOffset": 164, "endOffset": 171}, {"referenceID": 8, "context": "Recently, Gal and Ghahramani [9] proposed an ensemble-like method for measuring uncertainty in DNNs, which bypasses the need to train several ensemble members.", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "A selective classifier [5] is a pair (f, g), where f is a classifier, and g : X \u2192 {0, 1} is a selection function, which serves as a binary qualifier for f as follows,", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "The entire performance profile of such a classifier can be specified by its risk-coverage curve, defined to be risk as a function of coverage [5].", "startOffset": 142, "endOffset": 145}, {"referenceID": 9, "context": "1 (Gascuel and Caraux, 1992, [10]) Let P be any distribution and consider a classifier f whose true error w.", "startOffset": 29, "endOffset": 33}, {"referenceID": 9, "context": "As discussed in [10], the analytic bounds derived using, e.", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "In this section we consider two confidence-rate functions, \u03baf , based on previous work [9, 2].", "startOffset": 87, "endOffset": 93}, {"referenceID": 1, "context": "In this section we consider two confidence-rate functions, \u03baf , based on previous work [9, 2].", "startOffset": 87, "endOffset": 93}, {"referenceID": 1, "context": "1 The first confidence-rate function we consider has been around in the NN folklore for years, and is explicitly mentioned by [2, 4] in the context of reject option.", "startOffset": 126, "endOffset": 132}, {"referenceID": 3, "context": "1 The first confidence-rate function we consider has been around in the NN folklore for years, and is explicitly mentioned by [2, 4] in the context of reject option.", "startOffset": 126, "endOffset": 132}, {"referenceID": 8, "context": "Softmax responses are often treated as probabilities (responses are positive and sum to 1), but some authors criticize this approach [9].", "startOffset": 133, "endOffset": 136}, {"referenceID": 8, "context": "The MC-dropout technique we consider was recently proposed to quantify uncertainty in neural networks [9].", "startOffset": 102, "endOffset": 105}, {"referenceID": 16, "context": "We trained VGG models [17] for CIFAR-10, CIFAR-100 and ImageNet.", "startOffset": 22, "endOffset": 26}, {"referenceID": 13, "context": "We now consider CIFAR-10; see [14] for details.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "We used the VGG-16 architecture [17] and adapted it to the CIFAR-10 dataset by adding massive dropout, exactly as described in [15].", "startOffset": 32, "endOffset": 36}, {"referenceID": 14, "context": "We used the VGG-16 architecture [17] and adapted it to the CIFAR-10 dataset by adding massive dropout, exactly as described in [15].", "startOffset": 127, "endOffset": 131}, {"referenceID": 18, "context": "85% using the wide residual network architecture [19].", "startOffset": 49, "endOffset": 53}, {"referenceID": 15, "context": "We used an already trained Image-Net VGG-16 model based on ILSVRC2014 [16].", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "We also implemented the RESNET-50 architecture [12] in order to see if qualitatively similar results can be obtained with a different architecture.", "startOffset": 47, "endOffset": 51}, {"referenceID": 5, "context": "Selective classification is intimately related to active learning in the context of linear classifiers [6, 11].", "startOffset": 103, "endOffset": 110}, {"referenceID": 10, "context": "Selective classification is intimately related to active learning in the context of linear classifiers [6, 11].", "startOffset": 103, "endOffset": 110}], "year": 2017, "abstractText": "Selective classification techniques (also known as reject option) have not yet been considered in the context of deep neural networks (DNNs). These techniques can potentially significantly improve DNNs prediction performance by trading-off coverage. In this paper we propose a method to construct a selective classifier given a trained neural network. Our method allows a user to set a desired risk level. At test time, the classifier rejects instances as needed, to grant the desired risk (with high probability). Empirical results over CIFAR and ImageNet convincingly demonstrate the viability of our method, which opens up possibilities to operate DNNs in mission-critical applications. For example, using our method an unprecedented 2% error in top-5 ImageNet classification can be guaranteed with probability 99.9%, and almost 60% test coverage.", "creator": "LaTeX with hyperref package"}}}