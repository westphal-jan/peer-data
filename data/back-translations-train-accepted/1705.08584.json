{"id": "1705.08584", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2017", "title": "MMD GAN: Towards Deeper Understanding of Moment Matching Network", "abstract": "Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD-GAN. The new distance measure in MMD-GAN is a meaningful loss that enjoys the advantage of weak topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works.", "histories": [["v1", "Wed, 24 May 2017 02:20:29 GMT  (6792kb,D)", "http://arxiv.org/abs/1705.08584v1", "submitted to NIPS 2017"]], "COMMENTS": "submitted to NIPS 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chun-liang li", "wei-cheng chang", "yu cheng", "yiming yang", "barnab\\'as p\\'oczos"], "accepted": true, "id": "1705.08584"}, "pdf": {"name": "1705.08584.pdf", "metadata": {"source": "CRF", "title": "MMD GAN: Towards Deeper Understanding of Moment Matching Network", "authors": ["Chun-Liang Li", "Wei-Cheng Chang", "Yu Cheng", "Yiming Yang", "Barnab\u00e1s P\u00f3czos"], "emails": ["chunlial@cs.cmu.edu", "wchang2@cs.cmu.edu", "yiming@cs.cmu.edu", "bapoczos@cs.cmu.edu", "chengyu@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "The nature of the unattended learning models of the underlying distribution PX of the data X [1, 2] uses deep generative model [1, 2] to approximate the distribution of complex datasets with promising results. However, modelling arbitrary density is a statistically challenging task [3]. In many applications, such as image caption generation [4], an accurate density estimate is not even necessary, as we are only interested in the sample from the approximate distribution. Instead of estimating the density of PX, Generative Adversarial Network (GAN) [5] starts from a base distribution PZ over Z, like the Gaussian distribution, then forms a transformation network, similar to PX, where Pledge is the underlying distribution of gifts (z) and z PZ. During training, we require GAN-based algorithms to be an auxiliary network for estimating the distance between PX and PZ."}, {"heading": "2 GAN, Two-Sample Test and GMMN", "text": "Suppose that we are given data {xi} ni = 1, in which xi-X and xi-X-PX give two-Q values as a sample. If we are interested in the sample of PX, it is not necessary to estimate the density of PX. Instead, Generative Adversarial Network (GAN) [5] trains a generator, which is parameterized during the training on the basis of its samples {x} ni = 1 and {g\u03b8 (zj) nj = 1. [5] To measure the similarity between PX and PTB on the basis of their samples, one trains the discriminator f\u03c6, which is parameterized by means of the training."}, {"heading": "2.1 MMD with Kernel Learning", "text": "In practice, we use finite samples from distributions to estimate the MMD distance. Given X = {x1, \u00b7 Q = 11, \u00b7 \u00b7 \u00b7 P = {y1, \u00b7 \u00b7 \u00b7, yn} \u0445 Q, an estimator of Mk (P, Q) isM-k (X, Y) = 1 (n 2) \u2211 i = i \"k (xi, x\" i) \u2212 2 (n 2) \u2211 i = j \"(xi, yj) + 1 (n 2) \u2211 j 6 = j\" k \"(yj\" j).Due to the sampling variance, M (X, Y) cannot even be zero if P = Q. We then conduct hypotheses tests with zero hypotheses H0: P = Q. For a given permissible probability of false repulsion, we can only reject H0, which implicitly implies P 6 = Q if M (X, Y) > c\u03b1."}, {"heading": "2.2 Properties of MMD with Kernel Learning", "text": "[8] Discuss different distances between distributions assumed by existing deep learning algorithms, and show that many of them are discontinuous, such as Jensen-Shannon divergence [5] and total variation [7], with the exception of the Waterstone distance. Discontinuity makes the descent of the gradient unfeasible for training. Of (3), we train g\u03b8 by minimizing max\u03c6Mf\u03c6 (PX, P\u03b8). Next, we show max\u03c6Mf\u03c6 (PX, P\u03b8) also the advantage of being a continuous and differentiated target in education under mild assumptions. Assumption 2. g: Z \u00d7 Rm \u2192 X is local Lipschitz, where Z Rd."}, {"heading": "3 MMD GAN", "text": "To (3) approximate, we use neural networks for parameterization (X) and parameterization (Z. For g\u03b8, the assumption is local to Lipschitz, where commonly used neural networks meet this limitation. Also, the gradient f \u2212 1 of such gradient f \u2212 1 (x)) must be limited, which can be done by cliffs. The non-trivial part of the gradient is f\u03c6. For an injective function f \u2212 1 of such gradient, there is a function f \u2212 1 (x) = x, x \u2212 1 (f (z))))). The non-trivial part of the gradient is f\u03c6 (z), which can be approximated by an autocoder. We treat the gradient f \u2212 1 of such gradient as an encoder, and train the corresponding decoders fdec. The objective (3) is to be relaxed."}, {"heading": "3.1 Feasible Set Reduction", "text": "According to Theorem 5, there is f \"\u03c6,\" i.e. that Mf\u03c6 (Pr, PTB) = Mf \"\u03c6 (Pr, PTB) and Ex [f\u03c6 (x)] Ez [f\u03c6\" (gTB (z)). With Theorem 5, we could reduce the realizable theorem of \u03c6 (PTB) during optimization by solving the problem of limited optimization by reproofing. We loosen the limitation by ordinal regression [20] to bemin \u2082 max \"MFT (PR, PTB) + \u03bbmin (E [fBO (x)] \u2212 E [fBO (z)], 0), 3Note that the injection is not necessary."}, {"heading": "4 Related Works", "text": "There is a current rise in the improvement of GAN [5]. We are reviewing some related work here. Connection with WGAN: \"There are only two ways.\" (\"There is only one way back.\") \"There is only one way back.\" (\"There is no way back.\") \"There is only one way back.\" (\"There is no way back.\") \"There is no way back.\" (\"There is no way back.\") \"(\" There is no way back. \")\" (\"There is no way back.\") \"(\" There is no way back. \"(\" There is no way back. \")\" (\"There is no way back.\") \"(\" There is no way back. \"(\" There is no way back. \")\" (\"There is no way back.\" \"(\" There is no way back. \")\" (\"There is no way back.\" (\"There is no way back.\") \"(\" There is no way back. \"(\" There is no way back. \")\" (\"There is no way back.\") \"(\" There is no way back. \"(\" There is no way back. \")\" (\"There is no way back.\" (\")\" (\"There is no way back.\") \"(\" There is no way back. \"(\" There is no way. \")\" (\"There is no way back.\" (\"There is no way back.\"). \"(\""}, {"heading": "5 Experiment", "text": "apapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapapap"}, {"heading": "5.1 Qualitative Analysis", "text": "The aforementioned hsrc\u00fceFnlhsrc\u00fceGsrteh nvo rf\u00fc ide nlrgne\u00fceaeFnln ni rde eeisrcnlhsrteeGe ni nde nlrgne\u00fceaeFnln ni red nlrgne\u00fceaeFngn in rde eeirg\u00dfeG, nlrteew sasd hsci-eaJnlhsrsrteeeeaeaeeaeJnlrrrrgnea ni nlrrgneaeaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "5.2 Quantitative Analysis", "text": "To quantify the quality and variety of samples generated, we calculate the incentive score [26] on CIFAR-10 images. The incentive score is used for GANs to measure the quality and variety of samples on the prefabricated incentive model [26]. Models that generate collapsed samples have a relatively low score. Table 1 lists the results for 50K samples generated by various unattended generative models trained on CIFAR-10 datasets. Incention scores of [32, 33, 26] come directly from the corresponding references. Although both WGAN and MMD-GAN can produce sharp images, as we show in Section 5.1, our score is better than other GAN techniques with the exception of DFM [32]. This seems to empirically confirm that a higher order of correspondence between the real data and falsified sample previews will result in more diversified sample images."}, {"heading": "5.3 Stability of MMD GAN", "text": "Figure 4 shows the evolution of the MMD-GAN distance during training for MNIST, CelebA and LSUN datasets. We indicate the average of the M-f\u03c6 (PX, P\u03b8) with a moving average to reduce the variance caused by stochastic training in the minibatch. We observe throughout the training process that samples generated from the same sound vector over multiple iterations remain similar in nature (facial identity, bedroom style, as details and background develop. This qualitative observation points to a valuable stability of the training process. The decreasing curve with improved image quality supports the weak \"topology\" depicted in Theorem 4. Furthermore, we can see from the graph that the model converges very quickly. In Figure 4b, for example, it converges to CelebA datasets shortly after tens of thousands of generators."}, {"heading": "5.4 Computation Issue", "text": "The time complexity of each iteration is O (B) for WGAN and O (KB2) for our proposed MMD-GAN with a mixture of K-RBF cores. The quadratic complexity O (B2) of MMD-GAN is introduced by the calculation of the kernel matrix, which is sometimes criticized for not being applicable in practice for large batch sizes. However, we first point out that there are several recent works, such as EBGAN [7], which also match the relationship between the batch sizes in pairs, which also leads to O (B2) complexity. Empirically, we find that under the GPU environment, the highly parallel matrix operation greatly mitigates the quadratic time by reducing the quadratic time to almost linear time with modest B. Figure 3 the computational time per generator iterations with titanium X."}, {"heading": "6 Discussion", "text": "We are introducing a new, MMD-trained, deep generative model with hostile learned cores. We are further investigating its theoretical properties and proposing a practical implementation of MMD-GAN, which can be trained with much smaller batches than GMMN and has a competitive performance compared to modern GANs. We can consider MMD-GAN as the first practical step towards connecting the moment matching network and GAN. An important direction is the application of developed tools in moment matching [15] to general GAN work based on the connections shown by MMD-GAN."}, {"heading": "A Technical Proof", "text": "Since MMD is a probable Metrik (11), we have the triangular inequality for each Mf. \u2212 predetermined breaking point (PX, PTB). \u2212 predetermined breaking point (PX, PTB). \u2212 predetermined breaking point (PTB). \u2212 predetermined breaking point (PX, PTB). \u2212 predetermined breaking point (PTB). \u2212 predetermined breaking point (PX, PTB). \u2212 predetermined breaking point (PTB). \u2212 predetermined breaking point (PX, PTB). \u2212 predetermined breaking point (PX, PTB). \u2212 predetermined breaking point (PTB). \u2212 predetermined breaking point (PX, PTB). \u2212 predetermined breaking point (PX, PTB). \u2212 predetermined breaking point (PTB). \u2212. \u2212. \u2212."}, {"heading": "B Property of MMD with Fixed and Learned Kernels", "text": "In [8] they propose a counterexample to show the discontinuity of MMD by assuming that Q = L2. However, it is known that L2 is not included in RKHS, so the counterexample discussed is not appropriate. (10) \u2212 P \u2212 P FrameworkFrom integral probability metrics (IPM), the probabilitic distance can definingH = L2. (P, Q) = supf: F Ex \u0445 P [f x) \u2212 P \u2212 P FrameworkFrom integral probability metrics (IPM), the probabilitic distance is defined as \u2206 (P, Q) = supf: F Ex \u0445 P [f x) \u2212 Ey \u00b2 P = kernel Q [f (y). (f) \u2212 P \u2212 P FrameworkFrom integral probability metrics (IPM), we can recover several distances, such as total variation, Wasserstein distance and MMMMD distance."}], "references": [{"title": "Deep boltzmann machines", "author": ["Ruslan Salakhutdinov", "Geoffrey Hinton"], "venue": "In AISTATS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "In ICLR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "All of statistics: a concise course in statistical inference", "author": ["Larry Wasserman"], "venue": "Springer Science & Business Media,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "f-gan: Training generative neural samplers using variational divergence minimization", "author": ["Sebastian Nowozin", "Botond Cseke", "Ryota Tomioka"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Energy-based Generative Adversarial Network", "author": ["J. Zhao", "M. Mathieu", "Y. LeCun"], "venue": "In ICLR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Generative moment matching networks", "author": ["Yujia Li", "Kevin Swersky", "Richard Zemel"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Training generative neural networks via maximum mean discrepancy optimization", "author": ["Gintare Karolina Dziugaite", "Daniel M. Roy", "Zoubin Ghahramani"], "venue": "In UAI,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A kernel two-sample test", "author": ["Arthur Gretton", "Karsten M. Borgwardt", "Malte J. Rasch", "Bernhard Sch\u00f6lkopf", "Alexander Smola"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop", "author": ["Fisher Yu", "Ari Seff", "Yinda Zhang", "Shuran Song", "Thomas Funkhouser", "Jianxiong Xiao"], "venue": "arXiv preprint arXiv:1506.03365,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Deep learning face attributes in the wild", "author": ["Ziwei Liu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In CVPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Generative models and model criticism via optimized maximum mean discrepancy", "author": ["Dougal J. Sutherland", "Hsiao-Yu Fish Tung", "Heiko Strathmann", "Soumyajit De", "Aaditya Ramdas", "Alexander J. Smola", "Arthur Gretton"], "venue": "In ICLR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Kernel mean embedding of distributions: A review and beyonds", "author": ["Krikamol Muandet", "Kenji Fukumizu", "Bharath Sriperumbudur", "Bernhard Sch\u00f6lkopf"], "venue": "arXiv preprint arXiv:1605.09522,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Kernel choice and classifiability for rkhs embeddings of probability distributions", "author": ["Kenji Fukumizu", "Arthur Gretton", "Gert R Lanckriet", "Bernhard Sch\u00f6lkopf", "Bharath K Sriperumbudur"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Optimal kernel choice for large-scale two-sample tests", "author": ["A. Gretton", "B. Sriperumbudur", "D. Sejdinovic", "H. Strathmann", "S. Balakrishnan", "M. Pontil", "K. Fukumizu"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Optimal kernel choice for large-scale two-sample tests", "author": ["Arthur Gretton", "Dino Sejdinovic", "Heiko Strathmann", "Sivaraman Balakrishnan", "Massimiliano Pontil", "Kenji Fukumizu", "Bharath K Sriperumbudur"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Deep kernel learning", "author": ["Andrew Gordon Wilson", "Zhiting Hu", "Ruslan Salakhutdinov", "Eric P Xing"], "venue": "In AISTATS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Support vector learning for ordinal regression", "author": ["Ralf Herbrich", "Thore Graepel", "Klaus Obermayer"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Generalization and equilibrium in generative adversarial nets (gans)", "author": ["Sanjeev Arora", "Rong Ge", "Yingyu Liang", "Tengyu Ma", "Yi Zhang"], "venue": "arXiv preprint arXiv:1703.00573,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2017}, {"title": "Mcgan: Mean and covariance feature matching gan", "author": ["Youssef Mroueh", "Tom Sercu", "Vaibhava Goel"], "venue": "arxiv pre-print", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2017}, {"title": "Central moment discrepancy (cmd) for domain-invariant representation learning", "author": ["Werner Zellinger", "Thomas Grubinger", "Edwin Lughofer", "Thomas Natschl\u00e4ger", "Susanne Saminger-Platz"], "venue": "arXiv preprint arXiv:1702.08811,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2017}, {"title": "Adversarial generator-encoder networks", "author": ["Dmitry Ulyanov", "Andrea Vedaldi", "Victor Lempitsky"], "venue": "arXiv preprint arXiv:1704.02304,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2017}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "In NIPS,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Began: Boundary equilibrium generative adversarial networks", "author": ["David Berthelot", "Tom Schumm", "Luke Metz"], "venue": "arXiv preprint arXiv:1703.10717,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2017}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1998}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "In ICLR,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural networks for machine learning,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Improving generative adversarial networks with denoising feature matching", "author": ["D Warde-Farley", "Y Bengio"], "venue": "In ICLR,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2017}, {"title": "Adversarially learned inference", "author": ["Vincent Dumoulin", "Ishmael Belghazi", "Ben Poole", "Alex Lamb", "Martin Arjovsky", "Olivier Mastropietro", "Aaron Courville"], "venue": "In ICLR,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "Deep generative model [1, 2] uses deep learning to approximate the distribution of complex datasets with promising results.", "startOffset": 22, "endOffset": 28}, {"referenceID": 1, "context": "Deep generative model [1, 2] uses deep learning to approximate the distribution of complex datasets with promising results.", "startOffset": 22, "endOffset": 28}, {"referenceID": 2, "context": "However, modeling arbitrary density is a statistically challenging task [3].", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "In many applications, such as caption generation [4], accurate density estimation is not even necessary since we are only interested in sampling from the approximated distribution.", "startOffset": 49, "endOffset": 52}, {"referenceID": 4, "context": "Rather than estimating the density of PX , Generative Adversarial Network (GAN) [5] starts from a base distribution PZ over Z , such as Gaussian distribution, then trains a transformation network g\u03b8 such that P\u03b8 \u2248 PX , where P\u03b8 is the underlying distribution of g\u03b8(z) and z \u223c PZ .", "startOffset": 80, "endOffset": 83}, {"referenceID": 4, "context": "Different probabilistic (pseudo) metrics have been studied [5\u20138] under GAN framework.", "startOffset": 59, "endOffset": 64}, {"referenceID": 5, "context": "Different probabilistic (pseudo) metrics have been studied [5\u20138] under GAN framework.", "startOffset": 59, "endOffset": 64}, {"referenceID": 6, "context": "Different probabilistic (pseudo) metrics have been studied [5\u20138] under GAN framework.", "startOffset": 59, "endOffset": 64}, {"referenceID": 7, "context": "Instead of training an auxiliary network f\u03c6 for measuring the distance between PX and P\u03b8, Generative moment matching network (GMMN) [9, 10] uses kernel maximum mean discrepancy (MMD) [11], which is the centerpiece of nonparametric two-sample test, to determine the distribution distances.", "startOffset": 132, "endOffset": 139}, {"referenceID": 8, "context": "Instead of training an auxiliary network f\u03c6 for measuring the distance between PX and P\u03b8, Generative moment matching network (GMMN) [9, 10] uses kernel maximum mean discrepancy (MMD) [11], which is the centerpiece of nonparametric two-sample test, to determine the distribution distances.", "startOffset": 132, "endOffset": 139}, {"referenceID": 9, "context": "Instead of training an auxiliary network f\u03c6 for measuring the distance between PX and P\u03b8, Generative moment matching network (GMMN) [9, 10] uses kernel maximum mean discrepancy (MMD) [11], which is the centerpiece of nonparametric two-sample test, to determine the distribution distances.", "startOffset": 183, "endOffset": 187}, {"referenceID": 9, "context": "[11] shows even the simple Gaussian kernel enjoys the strong theoretical guarantees (Theorem 1).", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "There is no promising empirical results comparable with GAN on challenging benchmarks [12, 13].", "startOffset": 86, "endOffset": 94}, {"referenceID": 11, "context": "There is no promising empirical results comparable with GAN on challenging benchmarks [12, 13].", "startOffset": 86, "endOffset": 94}, {"referenceID": 7, "context": "it also requires larger batch size than GAN needs for training, which is considered to be less efficient [9, 10, 14, 8] In this work, we try to improve GMMN and consider using MMD with adversarially learned kernels instead of fixed Gaussian kernels to have better hypothesis testing power.", "startOffset": 105, "endOffset": 119}, {"referenceID": 8, "context": "it also requires larger batch size than GAN needs for training, which is considered to be less efficient [9, 10, 14, 8] In this work, we try to improve GMMN and consider using MMD with adversarially learned kernels instead of fixed Gaussian kernels to have better hypothesis testing power.", "startOffset": 105, "endOffset": 119}, {"referenceID": 12, "context": "it also requires larger batch size than GAN needs for training, which is considered to be less efficient [9, 10, 14, 8] In this work, we try to improve GMMN and consider using MMD with adversarially learned kernels instead of fixed Gaussian kernels to have better hypothesis testing power.", "startOffset": 105, "endOffset": 119}, {"referenceID": 13, "context": "The unified view shows more connections between moment matching and GAN, which can potentially inspire new algorithms based on well-developed tools in statistics [15].", "startOffset": 162, "endOffset": 166}, {"referenceID": 4, "context": "Instead, Generative Adversarial Network (GAN) [5] trains a generator g\u03b8 parametrized by \u03b8 to transform samples z \u223c PZ , where z \u2208 Z , into g\u03b8(z) \u223c P\u03b8 such that P\u03b8 \u2248 PX .", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "To measure the similarity between PX and P\u03b8 via their samples {x}i=1 and {g\u03b8(zj)}j=1 during the training, [5] trains the discriminator f\u03c6 parametrized by \u03c6 for help.", "startOffset": 106, "endOffset": 109}, {"referenceID": 9, "context": "One way to conduct two sample test is via kernel maximum mean discrepancy (MMD) [11].", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "[11] Given a kernel k, if k is a characteristic kernel, then Mk(P,Q) = 0 iff P = Q.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Based on Theorem 1, [9, 10] propose generative moment-matching network (GMMN), which trains g\u03b8 by", "startOffset": 20, "endOffset": 27}, {"referenceID": 8, "context": "Based on Theorem 1, [9, 10] propose generative moment-matching network (GMMN), which trains g\u03b8 by", "startOffset": 20, "endOffset": 27}, {"referenceID": 9, "context": "Please refer to [11] for more details.", "startOffset": 16, "endOffset": 20}, {"referenceID": 9, "context": "By [11, 17] if f is a injective function and k is characteristic, then the resulted kernel k\u0303 = k \u25e6 f , where k\u0303(x, x\u2032) = k(f(x), f(x\u2032)) is still characteristic.", "startOffset": 3, "endOffset": 11}, {"referenceID": 15, "context": "By [11, 17] if f is a injective function and k is characteristic, then the resulted kernel k\u0303 = k \u25e6 f , where k\u0303(x, x\u2032) = k(f(x), f(x\u2032)) is still characteristic.", "startOffset": 3, "endOffset": 11}, {"referenceID": 16, "context": "Note that [18] consider the linear combination of characteristic kernels, which can also be incorporated into the discussed composition kernels.", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "A more general composition kernel is studied in [19].", "startOffset": 48, "endOffset": 52}, {"referenceID": 4, "context": "[8] discuss different distances between distributions adopted by existing deep learning algorithms, and show many of them are discontinuous, such as Jensen-Shannon divergence [5] and Total variation [7], except for Wasserstein distance.", "startOffset": 175, "endOffset": 178}, {"referenceID": 6, "context": "[8] discuss different distances between distributions adopted by existing deep learning algorithms, and show many of them are discontinuous, such as Jensen-Shannon divergence [5] and Total variation [7], except for Wasserstein distance.", "startOffset": 199, "endOffset": 202}, {"referenceID": 2, "context": "Considering n \u2192 \u221e, under mild Assumption, max\u03c6Mf\u03c6(PX ,Pn)\u2192 0\u21d0\u21d2 Pn D \u2212\u2192 PX , where D \u2212\u2192 means converging in distribution [3].", "startOffset": 120, "endOffset": 123}, {"referenceID": 14, "context": "Please refer to [16] for more rigorous discussions.", "startOffset": 16, "endOffset": 20}, {"referenceID": 4, "context": "The proposed algorithm is similar to GAN [5], which aims to optimize two neural networks g\u03b8 and f\u03c6 in a minmax formulation, while the meaning of the objective is different.", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "In [5], f\u03c6 is a discriminator (binary) classifier to distinguish two distributions.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "From this perspective, [9] is the special case of MMD-GAN if f\u03c6 is the identity mapping function.", "startOffset": 23, "endOffset": 26}, {"referenceID": 18, "context": "We relax the constraint by ordinal regression [20] to be min \u03b8 max \u03c6 Mf\u03c6(Pr,P\u03b8) + \u03bbmin ( E[f\u03c6(x)]\u2212 E[f\u03c6(g\u03b8(z))], 0 ) ,", "startOffset": 46, "endOffset": 50}, {"referenceID": 4, "context": "There has been a recent surge on improving GAN [5].", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "If we treat f\u03c6(x) as the data transform function, WGAN can be treated as first-order moment matching (linear kernel) while MMD GAN aims to match infinite order of moments with Gaussian kernel form Taylor expansion [9].", "startOffset": 214, "endOffset": 217}, {"referenceID": 19, "context": "In practice, [21] show neural networks does not have enough capacity to approximate Wasserstein distance.", "startOffset": 13, "endOffset": 17}, {"referenceID": 20, "context": "[22] also propose McGAN that matches second order moment from the primal-dual norm perspective.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "However, the proposed algorithm requires matrix (tensor) decompositions because of exact moment matching [23], which is hard to scale to higher order moment matching.", "startOffset": 105, "endOffset": 109}, {"referenceID": 6, "context": "Difference from Other Works with Autoencoders: Energy-based GAN (EBGAN) [7] also utilizes the autoencoder (AE) in its discriminator from the energy model perspective, which minimizes the reconstruction error of real samples x while maximize the reconstruction error of generated samples g\u03b8(z).", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "The other line of works include [2, 24, 9], which aims to match the AE codespace f(x), and utilize the decoder fdec(\u00b7).", "startOffset": 32, "endOffset": 42}, {"referenceID": 7, "context": "The other line of works include [2, 24, 9], which aims to match the AE codespace f(x), and utilize the decoder fdec(\u00b7).", "startOffset": 32, "endOffset": 42}, {"referenceID": 1, "context": "[2, 24] match the distribution of f(x) and z via different distribution distances and generate data (e.", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[9] use MMD to match f(x) and g(z), and generate data via fdec(g(z)).", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "[25] is similar to MMD GAN but it considers KL-divergence without showing continuity and weak\u2217 topology guarantee as we prove in Section 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[26] propose use linear kernel to match first moment of its discriminator\u2019s latent features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] consider the variance of empirical MMD score during the training.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Also, [14] only improve the latent feature matching in [26] by using kernel MMD, instead of proposing an adversarial training framework as we studied in Section 2.", "startOffset": 6, "endOffset": 10}, {"referenceID": 23, "context": "Also, [14] only improve the latent feature matching in [26] by using kernel MMD, instead of proposing an adversarial training framework as we studied in Section 2.", "startOffset": 55, "endOffset": 59}, {"referenceID": 24, "context": "[27] use Wasserstein distance to match the distribution of autoencoder loss instead of data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "One can consider to extend [27] to higher order matching based on the proposed MMD-GAN.", "startOffset": 27, "endOffset": 31}, {"referenceID": 25, "context": "We train MMD GAN for image generation on the MNIST [28], CIFAR-10 [29], CelebA [13], and LSUN bedrooms [12] datasets, where the size of training instances are 50K, 50K, 160K, 3M respectively.", "startOffset": 51, "endOffset": 55}, {"referenceID": 26, "context": "We train MMD GAN for image generation on the MNIST [28], CIFAR-10 [29], CelebA [13], and LSUN bedrooms [12] datasets, where the size of training instances are 50K, 50K, 160K, 3M respectively.", "startOffset": 66, "endOffset": 70}, {"referenceID": 11, "context": "We train MMD GAN for image generation on the MNIST [28], CIFAR-10 [29], CelebA [13], and LSUN bedrooms [12] datasets, where the size of training instances are 50K, 50K, 160K, 3M respectively.", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "We train MMD GAN for image generation on the MNIST [28], CIFAR-10 [29], CelebA [13], and LSUN bedrooms [12] datasets, where the size of training instances are 50K, 50K, 160K, 3M respectively.", "startOffset": 103, "endOffset": 107}, {"referenceID": 27, "context": "Network architecture: In our experiments, we follow the architecture of DCGAN [30] to design g\u03b8 by its generator and f\u03c6 by its discriminator except for expanding the output layer of f\u03c6 to be h dimensions.", "startOffset": 78, "endOffset": 82}, {"referenceID": 8, "context": "Similar to the prior MMD seminal papers [10, 9, 14], we consider a mixture of K RBF kernels k(x, x\u2032) = \u2211K q=1 k\u03c3q (x, x \u2032) where k\u03c3q is a Gaussian kernel with bandwidth parameter \u03c3q.", "startOffset": 40, "endOffset": 51}, {"referenceID": 7, "context": "Similar to the prior MMD seminal papers [10, 9, 14], we consider a mixture of K RBF kernels k(x, x\u2032) = \u2211K q=1 k\u03c3q (x, x \u2032) where k\u03c3q is a Gaussian kernel with bandwidth parameter \u03c3q.", "startOffset": 40, "endOffset": 51}, {"referenceID": 12, "context": "Similar to the prior MMD seminal papers [10, 9, 14], we consider a mixture of K RBF kernels k(x, x\u2032) = \u2211K q=1 k\u03c3q (x, x \u2032) where k\u03c3q is a Gaussian kernel with bandwidth parameter \u03c3q.", "startOffset": 40, "endOffset": 51}, {"referenceID": 28, "context": "Hyper-parameters: We use RMSProp [31] with learning rate of 0.", "startOffset": 33, "endOffset": 37}, {"referenceID": 27, "context": "We consider recent state-of-art WGAN [8] based on DCGAN structure [30], because of the connection with MMD-GAN discussed in Section 4.", "startOffset": 66, "endOffset": 70}, {"referenceID": 23, "context": "To quantitatively measure the quality and diversity of generated samples, we compute the inception score [26] on CIFAR-10 images.", "startOffset": 105, "endOffset": 109}, {"referenceID": 23, "context": "The inception score is used for GANs to measure samples quality and diversity on the pretrained inception model [26].", "startOffset": 112, "endOffset": 116}, {"referenceID": 29, "context": "The inception scores of [32, 33, 26] are directly derived from the corresponding references.", "startOffset": 24, "endOffset": 36}, {"referenceID": 30, "context": "The inception scores of [32, 33, 26] are directly derived from the corresponding references.", "startOffset": 24, "endOffset": 36}, {"referenceID": 23, "context": "The inception scores of [32, 33, 26] are directly derived from the corresponding references.", "startOffset": 24, "endOffset": 36}, {"referenceID": 29, "context": "1, our score is better than other GAN techniques except for DFM [32].", "startOffset": 64, "endOffset": 68}, {"referenceID": 29, "context": "20 DFM [32] 7.", "startOffset": 7, "endOffset": 11}, {"referenceID": 30, "context": "72 ALI [33] 5.", "startOffset": 7, "endOffset": 11}, {"referenceID": 23, "context": "34 Improved GANs [26] 4.", "startOffset": 17, "endOffset": 21}, {"referenceID": 6, "context": "However, we first point that there are several recent works, such as EBGAN [7], also matching pairwise relation between samples of batch size, leading to O(B) complexity as well.", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "GMMN in its references [9], the time per iteration becomes 4.", "startOffset": 23, "endOffset": 26}], "year": 2017, "abstractText": "Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD-GAN. The new distance measure in MMD-GAN is a meaningful loss that enjoys the advantage of weak\u2217 topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works.", "creator": "LaTeX with hyperref package"}}}