{"id": "1705.08947", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2017", "title": "Deep Voice 2: Multi-Speaker Neural Text-to-Speech", "abstract": "We introduce a technique for augmenting neural text-to-speech (TTS) with lowdimensional trainable speaker embeddings to generate different voices from a single model. As a starting point, we show improvements over the two state-ofthe-art approaches for single-speaker neural TTS: Deep Voice 1 and Tacotron. We introduce Deep Voice 2, which is based on a similar pipeline with Deep Voice 1, but constructed with higher performance building blocks and demonstrates a significant audio quality improvement over Deep Voice 1. We improve Tacotron by introducing a post-processing neural vocoder, and demonstrate a significant audio quality improvement. We then demonstrate our technique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron on two multi-speaker TTS datasets. We show that a single neural TTS system can learn hundreds of unique voices from less than half an hour of data per speaker, while achieving high audio quality synthesis and preserving the speaker identities almost perfectly.", "histories": [["v1", "Wed, 24 May 2017 19:53:13 GMT  (1150kb,D)", "http://arxiv.org/abs/1705.08947v1", "Submitted to NIPS 2017"], ["v2", "Wed, 20 Sep 2017 21:43:18 GMT  (1145kb,D)", "http://arxiv.org/abs/1705.08947v2", "Accepted in NIPS 2017"]], "COMMENTS": "Submitted to NIPS 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sercan arik", "gregory diamos", "rew gibiansky", "john miller", "kainan peng", "wei ping", "jonathan raiman", "yanqi zhou"], "accepted": true, "id": "1705.08947"}, "pdf": {"name": "1705.08947.pdf", "metadata": {"source": "CRF", "title": "Deep Voice 2: Multi-Speaker Neural Text-to-Speech", "authors": ["Sercan \u00d6. Ar\u0131k", "Gregory Diamos", "Kainan Peng", "Yanqi Zhou"], "emails": ["sercanarik@baidu.com", "gregdiamos@baidu.com", "gibianskyandrew@baidu.com", "millerjohn@baidu.com", "pengkainan@baidu.com", "pingwei01@baidu.com", "jonathanraiman@baidu.com", "zhouyanqi@baidu.com"], "sections": [{"heading": "1 Introduction", "text": "Most TTS systems are built with a single voice, and multiple voices are provided by having different language databases or model parameters. As a result, developing a TTS system with support for multiple voices requires much more data and development effort than a system that only supports a single voice. In this work, we show that we can build all neural TTS systems with multiple speakers that share the vast majority of parameters between different speakers. We show that not only a single model can generate speech from multiple different voices, but also that significantly less data is needed per speaker than when training individual voice systems. Specifically, we will make the following contributions: 1. We will present Deep Voice 2, an improved architecture based on Deep Voice 1 (Arik et al., 2017).2 We are running a WaveNet-based (Oet al., 2016) spectrogram-based on Deep Voice 2, which is based on Deep Voice 3."}, {"heading": "2 Related Work", "text": "We discuss the related work relevant to each of our claims in Section 1, in order, starting with the synthesis of neural speakers and the transition to multilingual speech synthesis and metrics for generative model quality. In terms of speech synthesis of individual speakers, deep learning has been used for a variety of subcomponents, including duration prediction (Zen et al., 2016), basic frequency prediction (Ronanki et al., 2016), acoustic modelling (Zen and Sak, 2015), and more recently authoritative audio waveform generation (e.g., Oord et al., 2016; Mehri et al., 2016). Our contributions are based on fully neural TTS systems, including Deep Voice 1 (Arik et al., 2017), Tacotron et al (Wang et al., 2017), and Char2Wav (Sotelo et al., 2017)."}, {"heading": "3 Single-Speaker Deep Voice 2", "text": "In this section we present Deep Voice 2, a neural TTS system based on Deep Voice 1 (Arik et al., 2017). We consider the general structure of Deep Voice 1 (Arik et al., 2017) as illustrated in Fig. 1 (the corresponding training pipeline is illustrated in Appendix A.) Our primary motivation for presenting an enhanced single speaker model is to use it as the starting point for a high-quality multispeaker model.A major difference between Deep Voice 2 and Deep Voice 1 is the separation of phoneme duration and frequency models. Deep Voice 1 has a single model to jointly use phonemes DurationConv-BN-ResPhoneme 1 Phoneme n Stacked Bi-GRU Bi-GRU FC! + F0 Filter Bank FC Voiced Speaker... FC Softsign) +) 1 + 1 + FC Softsign 1 + Frequency Upsign."}, {"heading": "3.1 Segmentation model", "text": "The estimation of phoneme locations is treated in Deep Voice 2 as an unattended learning problem, similar to Deep Voice 1. The segmentation model is a convolutional-recursive architecture with connectionist time classification (CTC), which is used to classify phoneme pairs, which are then used to extract the boundaries between them. The most important architectural changes in Deep Voice 2 are the addition of batch normalization and residual connections in the revolutionary layers. Specifically, the segmentation model of Deep Voice 1 calculates the output of each layer ash (l) = Relu (W (l) \u0445 h (l) + b (l))))), (1) where h (l) is the output of the Lth layer, W (l) is the evolution filter bank, b (l) is the bias vector, and vice versa is the confusion operator. In contrast, Deep Voice 2 makes the relational layer (2015) = (\u2212 B)."}, {"heading": "3.2 Duration Model", "text": "In Deep Voice 2, we formulate the duration prediction as a sequence marking problem. We discredit the duration of the phoneme in log-scale buckets and assign each input phoneme the bucket label corresponding to its duration. We model the sequence using a conditional random field (CRF) with paired potentials at the output level (Lample et al., 2016). During inference, we decrypt the CRF using the Viterbi forward backward algorithm. We find that quantifying the duration prediction and introducing the paired dependence that the CRF implies improves synthesis sequence."}, {"heading": "3.3 Frequency Model", "text": "After decoding from the duration model, the input functions using the predicted phoneme durations are upsample.2 Deep Voice 2 Frequency Model1 We calculate the smoothed normalized audio performance as p [n] = (x [n] 2 / xmax2).g [n] where x [n] exceeds the audio signal, g [n] is the impulse response of a Gaussian filter, xmax is the maximum value of x [n] and \u0445 is a one-dimensional folding operation. We assign the silent phoneme limits when p [n] exceeds a fixed threshold. The optimal parameter values for the Gaussian filter and the threshold depend on the dataset and audio sampling rate.2Each frame is designed to be 10 milliseconds. E.g. if a phoneme is repeated correspondingly for 20 milliseconds, the input functions are repeated in milliseconds."}, {"heading": "3.4 Vocal Model", "text": "The vocal model of Deep Voice 2 is based on a WaveNet architecture (Oord et al., 2016) with a two-layer bi-directional QRNN conditioning network (Bradbury et al., 2016) similar to Deep Voice 1. However, we remove the 1 \u00d7 1 fold between the gated tanh nonlinearity and the residual connection. In addition, we use the same conditioner bias for each layer of WaveNet instead of creating a separate bias for each layer as was the case in Deep Voice 1.3."}, {"heading": "4 Multi-Speaker Models with Trainable Speaker Embeddings", "text": "Unlike previous work, our approach does not rely on individual FCC speakers storing their respective weight matrices or layers in a very low dimensional vector, and therefore there is an almost complete weight distribution between speakers. We use loudspeaker embeddings to produce recurrent neural networks (RNN) initial states, nonlinearity, and multiplicative gating factors that are used throughout the networks. Loudspeaker embeddings are randomly initialized with an even distribution via [\u2212 0.1] and trained connections via back propagation. To sample each speaker's unique language signature, we are able to integrate loudspeaker embeddings into multiple parts of the model."}, {"heading": "4.1 Multi-Speaker Deep Voice 2", "text": "The Deep Voice 2 models have separate speaker embedding for each model, but can be considered as parts of a larger speaker embedding that are trained independently of each other."}, {"heading": "4.1.1 Segmentation Model", "text": "In the multi-speaker segmentation model, we use feature gating in the residual connections of the folding layers. Instead of equation 2, we multiply the batch-normalized activations with a site-specific speaker embedding: h (l) = relu (h (l \u2212 1) + BN (W \u0445 h (l \u2212 1) \u00b7 gs), (5) where gs is a site-specific speaker embedding. The same site-specific embedding is used for all winding layers. In addition, we initialize each of the recurring layers with a second site-specific embedding. Likewise, each layer uses the same site-specific embedding instead of having a separate embedding per layer."}, {"heading": "4.1.2 Duration Model", "text": "The multi-speaker continuous model uses speaker-dependent recursive initialization and input expansion. A site-specific embedding is used to initialize RNN hidden states, and another site-specific embedding is provided as input into the first RNN layer by concatenating with the feature vectors."}, {"heading": "4.1.3 Frequency Model", "text": "As described in Section 3.3, the recurring and revolutionary output layers in the single-speaker frequency model predict a normalized frequency, which is then converted to true F0 by a fixed linear transformation. Linear transformation depends on the mean and standard deviation of the observed F0 for the speaker. These values vary greatly between speakers: male speakers, for example, tend to have a much lower mean F0. To better adapt to these variations, we make the mean and standard deviation parameters of the model and multiply them by scaling the terms that depend on the speaker embedding."}, {"heading": "4.1.4 Vocal Model", "text": "The multi-loudspeaker vocal model uses only an input magnification, with the site-specific loudspeaker linked to each input frame of the conditioner. This differs from the global conditioning suggested in Oord et al. (2016), and allows the loudspeaker to also influence the local conditioning network.Without embedding loudspeakers, the vocal model can still produce reasonably distinct-sounding voices due to the discretionary features offered by the frequency and duration models."}, {"heading": "4.2 Multi-Speaker Tacotron", "text": "In addition to expanding Deep Voice 2 to include speaker embedding, we are also expanding Tacotron (Wang et al., 2017), a sequence-to-sequence character-to-waveform model. If we train Tacotron variants with multiple speakers, we find that the performance of the model strongly depends on the hyperparameters of the model, and that some models often fail to learn the attention mechanisms for a small subset of speakers. We also find that if the language in each audio clip does not begin at the same time, the models are much less likely to converge into a significant attention curve and recognizable speech; therefore, in each audio clip we trim all initial and final silence. Due to the sensitivity of the model to hyperparameters and data preprocessing, we believe that an additional tuning may be necessary to achieve maximum quality. Therefore, our work is focused on showing that Tacotron is able to handle speakers like Deep 2 is capable of multiple architectures in both Voice architectures."}, {"heading": "4.2.1 Character-to-Spectrogram Model", "text": "The Tacotron character-to-spectrogram architecture consists of a folding bench highway GRU (CBHG) encoder, an attention decoder, and a CBHG post-processing network. Due to the complexity of the architecture, we do not provide a full description and instead focus on our modifications. We note that embedding speakers in the CBHG post-processing network reduces output quality, whereas embedding speakers in the character encoder is necessary. Without a speaker-dependent CBHG encoder, the model is unable to learn its attention mechanism and cannot generate meaningful output (see Appendix C.2 for loudspeaker-dependent attention visualizations). To condition the encoder on the speaker, we use a location-specific embedding as an additional input in each CBHN high-location initialize each second layer of the RHG and NHN state."}, {"heading": "4.2.2 Spectrogram-to-Waveform Model", "text": "The original Tacotron implementation in (Wang et al., 2017) uses the Griffin-Lim algorithm to convert spectrograms into time-limited audio waveforms by iteratively estimating the unknown phases.5 We observe that minor noise in the input spectrogram leads to noticeable estimation errors in the GriffinLim algorithm and deteriorates the audio quality generated. To generate higher audio quality with Tacotron, we train a WaveNet-based neural vocoder instead of Griffin Lim to convert from linear spectrograms to audio waveforms. The model used corresponds to the Deep Voice 2 vocal model, but uses linearly scaled log-magnitude spectrograms instead of phoneme identity and F0 as input. The combined Tacotron-WaveNet model is shown in Fig. 3."}, {"heading": "5 Results", "text": "In this section we will present the results of speech synthesis with one loudspeaker as well as with multiple loudspeakers based on the described architectures. All hyperparameters of the model are presented in Appendix B."}, {"heading": "5.1 Single-Speaker Speech Synthesis", "text": "We train Deep Voice 1, Deep Voice 2 and Tacotron on an internal English language database with approximately 20 hours of single speaker data. Intermediate evaluations of models in Deep Voice 1 and Deep Voice 2 can be found in Table 3 within Appendix A. We perform an MOS evaluation using the CrowdMOS framework (Ribeiro et al., 2011) to compare the quality of samples (Table 1). Results conclusively show that the architectural improvements in Deep Voice 2 bring significant quality gains over Deep Voice 1. They also show that converting Tacotron-generated spectrograms into audio using WaveNet is preferable to the iterative Griffin-Lim algorithm."}, {"heading": "5.2 Multi-Speaker Speech Synthesis", "text": "We train all the aforementioned models on the VCTK dataset with 44 hours speaking time, which includes 108 speakers with about 400 utterances. We also train all the models on an internal audiobook dataset, which contains 477 speakers with 30 minutes speaking time each. Consistent sample quality of our models indicates that our architectures can easily learn hundreds of voices with different accents and cadences. We also observe that the learned embeddings are located in a significant space (see Fig. 4)."}, {"heading": "6 Conclusion", "text": "In this paper, we explore how the pipelines of fully neural speech synthesis can be extended to text-to-speech multi-speaker embedding through low-dimensional, traceable speaker embedding. First, we introduce Deep Voice 2, an improved multi-speaker embedding model. Next, we demonstrate the applicability of our technique by training both Deep Voice 2 with multiple speakers and Tacotron multi-speaker models and evaluating their quality using MOS. Finally, we use our speaker embedding technique to create high-quality text-to-speech systems, and finally demonstrate that models of neural speech synthesis can effectively learn from small amounts of data distributed across hundreds of different speakers. The results presented in this paper suggest many directions for future research."}, {"heading": "B Model Hyperparameters", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move,"}, {"heading": "D Speaker Discriminative Model", "text": "To calculate the accuracy of the classification of multiple loudspeakers, we use a discriminatory model for loudspeakers based on the basic truth set of multiple loudspeakers. Although the use of another discriminatory model such as Deep Speaker (Li et al., 2017) or other methods would also suffice, we choose our own discriminatory model based on deep learning. We note that our accuracy results on the test set are equivalent to the state-of-the-art literacy classification methods for loudspeakers. Our architecture is illustrated in Fig. 10. We use receiver coefficients (MFCCs), which are calculated to a constant sampling frequency after re-sampling the input frequency. We then use two-dimensional revolutionary layers that entangle over time and the receiver frequency bands, with a relay nonlinearity cut to a maximum of six after each constant layer."}], "references": [{"title": "Deep voice: Real-time neural text-to-speech", "author": ["S.O. Arik", "M. Chrzanowski", "A. Coates", "G. Diamos", "A. Gibiansky", "Y. Kang", "X. Li", "J. Miller", "J. Raiman", "S. Sengupta", "M. Shoeybi"], "venue": null, "citeRegEx": "Arik et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Arik et al\\.", "year": 2017}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Multi-speaker modeling and speaker adaptation for DNN-based TTS synthesis", "author": ["Y. Fan", "Y. Qian", "F.K. Soong", "L. He"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Fan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2015}, {"title": "Voice conversion from unaligned corpora using variational autoencoding wasserstein generative adversarial networks", "author": ["C.-C. Hsu", "H.-T. Hwang", "Y.-C. Wu", "Y. Tsao", "H.-M. Wang"], "venue": null, "citeRegEx": "Hsu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2017}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Neural architectures for named entity recognition", "author": ["G. Lample", "M. Ballesteros", "K. Kawakami", "S. Subramanian", "C. Dyer"], "venue": "In Proc. NAACL-HLT,", "citeRegEx": "Lample et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Deep speaker: an end-to-end neural speaker embedding system", "author": ["C. Li", "X. Ma", "B. Jiang", "X. Li", "X. Zhang", "X. Liu", "Y. Cao", "A. Kannan", "Z. Zhu"], "venue": "arXiv preprint arXiv:1705.02304,", "citeRegEx": "Li et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "SampleRNN: An unconditional end-to-end neural audio generation model", "author": ["S. Mehri", "K. Kumar", "I. Gulrajani", "R. Kumar", "S. Jain", "J. Sotelo", "A. Courville", "Y. Bengio"], "venue": null, "citeRegEx": "Mehri et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mehri et al\\.", "year": 2016}, {"title": "Wavenet: A generative model for raw audio", "author": ["A. v. d. Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": null, "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Speaker verification using adapted gaussian mixture models", "author": ["D.A. Reynolds", "T.F. Quatieri", "R.B. Dunn"], "venue": "Digital signal processing,", "citeRegEx": "Reynolds et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Reynolds et al\\.", "year": 2000}, {"title": "Crowdmos: An approach for crowdsourcing mean opinion score studies", "author": ["F. Ribeiro", "D. Flor\u00eancio", "C. Zhang", "M. Seltzer"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Ribeiro et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ribeiro et al\\.", "year": 2011}, {"title": "Median-based generation of synthetic speech durations using a non-parametric approach", "author": ["S. Ronanki", "O. Watts", "S. King", "G.E. Henter"], "venue": null, "citeRegEx": "Ronanki et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ronanki et al\\.", "year": 2016}, {"title": "Improved techniques for training gans", "author": ["T. Salimans", "I. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford", "X. Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Char2wav: End-to-end speech synthesis", "author": ["J. Sotelo", "S. Mehri", "K. Kumar", "J.F. Santos", "K. Kastner", "A. Courville", "Y. Bengio"], "venue": "In ICLR2017 workshop submission,", "citeRegEx": "Sotelo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sotelo et al\\.", "year": 2017}, {"title": "Tacotron: Towards end-to-end speech synthesis", "author": ["Y. Wang", "R. Skerry-Ryan", "D. Stanton", "Y. Wu", "R.J. Weiss", "N. Jaitly", "Z. Yang", "Y. Xiao", "Z. Chen", "S. Bengio"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "A study of speaker adaptation for DNN-based speech synthesis", "author": ["Z. Wu", "P. Swietojanski", "C. Veaux", "S. Renals", "S. King"], "venue": "In INTERSPEECH,", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": "Robust speaker-adaptive hmm-based text-to-speech synthesis", "author": ["J. Yamagishi", "T. Nose", "H. Zen", "Z.-H. Ling", "T. Toda", "K. Tokuda", "S. King", "S. Renals"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Yamagishi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yamagishi et al\\.", "year": 2009}, {"title": "On the training of dnn-based average voice model for speech synthesis", "author": ["S. Yang", "Z. Wu", "L. Xie"], "venue": "In Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2016 Asia-Pacific,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis", "author": ["H. Zen", "H. Sak"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Zen and Sak.,? \\Q2015\\E", "shortCiteRegEx": "Zen and Sak.", "year": 2015}, {"title": "Fast, compact, and high quality LSTM-RNN based statistical parametric speech synthesizers for mobile devices", "author": ["H. Zen", "Y. Agiomyrgiannakis", "N. Egberts", "F. Henderson", "P. Szczepaniak"], "venue": null, "citeRegEx": "Zen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zen et al\\.", "year": 2016}, {"title": "Discriminative Model To compute multi-speaker classification accuracy, we use a speaker discriminative model trained on the ground truth data set of multiple speakers. Although using another discriminator model such as Deep Speaker (Li et al., 2017) or other methods would also suffice, we choose to create our own deep learning based discriminative model", "author": ["D Speaker"], "venue": null, "citeRegEx": "Speaker,? \\Q2017\\E", "shortCiteRegEx": "Speaker", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "We present Deep Voice 2, an improved architecture based on Deep Voice 1 (Arik et al., 2017).", "startOffset": 72, "endOffset": 91}, {"referenceID": 9, "context": "We introduce a WaveNet-based (Oord et al., 2016) spectrogram-to-audio neural vocoder, and use it with Tacotron (Wang et al.", "startOffset": 29, "endOffset": 48}, {"referenceID": 15, "context": ", 2016) spectrogram-to-audio neural vocoder, and use it with Tacotron (Wang et al., 2017) as a replacement for Griffin-Lim audio generation.", "startOffset": 70, "endOffset": 89}, {"referenceID": 20, "context": "With regards to single-speaker speech synthesis, deep learning has been used for a variety of subcomponents, including duration prediction (Zen et al., 2016), fundamental frequency prediction (Ronanki et al.", "startOffset": 139, "endOffset": 157}, {"referenceID": 12, "context": ", 2016), fundamental frequency prediction (Ronanki et al., 2016), acoustic modeling (Zen and Sak, 2015), and more recently autoregressive sample-bysample audio waveform generation (e.", "startOffset": 42, "endOffset": 64}, {"referenceID": 19, "context": ", 2016), acoustic modeling (Zen and Sak, 2015), and more recently autoregressive sample-bysample audio waveform generation (e.", "startOffset": 27, "endOffset": 46}, {"referenceID": 8, "context": ", 2016), acoustic modeling (Zen and Sak, 2015), and more recently autoregressive sample-bysample audio waveform generation (e.g., Oord et al., 2016; Mehri et al., 2016).", "startOffset": 123, "endOffset": 168}, {"referenceID": 0, "context": "Our contributions build upon recent work in entirely neural TTS systems, including Deep Voice 1 (Arik et al., 2017), Tacotron (Wang et al.", "startOffset": 96, "endOffset": 115}, {"referenceID": 15, "context": ", 2017), Tacotron (Wang et al., 2017), and Char2Wav (Sotelo et al.", "startOffset": 18, "endOffset": 37}, {"referenceID": 14, "context": ", 2017), and Char2Wav (Sotelo et al., 2017).", "startOffset": 22, "endOffset": 43}, {"referenceID": 3, "context": "More recently, speaker adaptation has been tackled with generative adversarial networks (GANs) (Hsu et al., 2017).", "startOffset": 95, "endOffset": 113}, {"referenceID": 0, "context": "Our contributions build upon recent work in entirely neural TTS systems, including Deep Voice 1 (Arik et al., 2017), Tacotron (Wang et al., 2017), and Char2Wav (Sotelo et al., 2017). While these works focus on building single-speaker TTS systems, our paper focuses on extending neural TTS systems to handle multiple speakers with less data per speaker. Our work is not the first to attempt a multi-speaker TTS system. For instance, in traditional HMMbased TTS synthesis (e.g., Yamagishi et al., 2009), an average voice model is trained using multiple speakers\u2019 data, which is then adapted to different speakers. DNN-based systems (e.g., Yang et al., 2016) have also been used to build average voice models, with i-vectors representing speakers as additional inputs and separate output layers for each target speaker. Similarly, Fan et al. (2015) uses a shared hidden representation among different speakers with speaker-dependent output layers predicting vocoder parameters (e.", "startOffset": 97, "endOffset": 846}, {"referenceID": 0, "context": "Our contributions build upon recent work in entirely neural TTS systems, including Deep Voice 1 (Arik et al., 2017), Tacotron (Wang et al., 2017), and Char2Wav (Sotelo et al., 2017). While these works focus on building single-speaker TTS systems, our paper focuses on extending neural TTS systems to handle multiple speakers with less data per speaker. Our work is not the first to attempt a multi-speaker TTS system. For instance, in traditional HMMbased TTS synthesis (e.g., Yamagishi et al., 2009), an average voice model is trained using multiple speakers\u2019 data, which is then adapted to different speakers. DNN-based systems (e.g., Yang et al., 2016) have also been used to build average voice models, with i-vectors representing speakers as additional inputs and separate output layers for each target speaker. Similarly, Fan et al. (2015) uses a shared hidden representation among different speakers with speaker-dependent output layers predicting vocoder parameters (e.g., line spectral pairs, aperiodicity parameters, etc.). For further context, Wu et al. (2015) empirically studies DNN-based multi-speaker modeling.", "startOffset": 97, "endOffset": 1072}, {"referenceID": 0, "context": "In this section, we present Deep Voice 2, a neural TTS system based on Deep Voice 1 (Arik et al., 2017).", "startOffset": 84, "endOffset": 103}, {"referenceID": 0, "context": "We keep the general structure of the Deep Voice 1 (Arik et al., 2017), as depicted in Fig.", "startOffset": 50, "endOffset": 69}, {"referenceID": 4, "context": "In contrast, Deep Voice 2\u2019s segmentation model layers instead compute h = relu ( h(l\u22121) +BN ( W (l) \u2217 h(l\u22121) )) , (2) where BN is batch normalization (Ioffe and Szegedy, 2015).", "startOffset": 150, "endOffset": 175}, {"referenceID": 6, "context": "We model the sequence by a conditional random field (CRF) with pairwise potentials at output layer (Lample et al., 2016).", "startOffset": 99, "endOffset": 120}, {"referenceID": 1, "context": "consists of multiple layers: firstly, bidirectional gated recurrent unit (GRU) layers (Cho et al., 2014) generate hidden states from the input features.", "startOffset": 86, "endOffset": 104}, {"referenceID": 9, "context": "4 Vocal Model The Deep Voice 2 vocal model is based on a WaveNet architecture (Oord et al., 2016) with a two-layer bidirectional QRNN (Bradbury et al.", "startOffset": 78, "endOffset": 97}, {"referenceID": 9, "context": "This differs from the global conditioning suggested in Oord et al. (2016) and allows the speaker embedding to influence the local conditioning network as well.", "startOffset": 55, "endOffset": 74}, {"referenceID": 15, "context": "2 Multi-Speaker Tacotron In addition to extending Deep Voice 2 with speaker embeddings, we also extend Tacotron (Wang et al., 2017), a sequence-to-sequence character-to-waveform model.", "startOffset": 112, "endOffset": 131}, {"referenceID": 15, "context": "2 Spectrogram-to-Waveform Model The original Tacotron implementation in (Wang et al., 2017) uses the Griffin-Lim algorithm to convert spectrograms to time-domain audio waveforms by iteratively estimating the unknown phases.", "startOffset": 72, "endOffset": 91}, {"referenceID": 11, "context": "We run an MOS evaluation using the crowdMOS framework (Ribeiro et al., 2011) to compare the quality of samples (Table 1).", "startOffset": 54, "endOffset": 76}], "year": 2017, "abstractText": "We introduce a technique for augmenting neural text-to-speech (TTS) with lowdimensional trainable speaker embeddings to generate different voices from a single model. As a starting point, we show improvements over the two state-ofthe-art approaches for single-speaker neural TTS: Deep Voice 1 and Tacotron. We introduce Deep Voice 2, which is based on a similar pipeline with Deep Voice 1, but constructed with higher performance building blocks and demonstrates a significant audio quality improvement over Deep Voice 1. We improve Tacotron by introducing a post-processing neural vocoder, and demonstrate a significant audio quality improvement. We then demonstrate our technique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron on two multi-speaker TTS datasets. We show that a single neural TTS system can learn hundreds of unique voices from less than half an hour of data per speaker, while achieving high audio quality synthesis and preserving the speaker identities almost perfectly.", "creator": "LaTeX with hyperref package"}}}