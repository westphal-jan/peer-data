{"id": "1705.08982", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2017", "title": "Modeling the Intensity Function of Point Process Via Recurrent Neural Networks", "abstract": "Event sequence, asynchronously generated with random timestamp, is ubiquitous among applications. The precise and arbitrary timestamp can carry important clues about the underlying dynamics, and has lent the event data fundamentally different from the time-series whereby series is indexed with fixed and equal time interval. One expressive mathematical tool for modeling event is point process. The intensity functions of many point processes involve two components: the background and the effect by the history. Due to its inherent spontaneousness, the background can be treated as a time series while the other need to handle the history events. In this paper, we model the background by a Recurrent Neural Network (RNN) with its units aligned with time series indexes while the history effect is modeled by another RNN whose units are aligned with asynchronous events to capture the long-range dynamics. The whole model with event type and timestamp prediction output layers can be trained end-to-end. Our approach takes an RNN perspective to point process, and models its background and history effect. For utility, our method allows a black-box treatment for modeling the intensity which is often a pre-defined parametric form in point processes. Meanwhile end-to-end training opens the venue for reusing existing rich techniques in deep network for point process modeling. We apply our model to the predictive maintenance problem using a log dataset by more than 1000 ATMs from a global bank headquartered in North America.", "histories": [["v1", "Wed, 24 May 2017 22:23:14 GMT  (1292kb,D)", "http://arxiv.org/abs/1705.08982v1", "Accepted at Thirty-First AAAI Conference on Artificial Intelligence (AAAI17)"]], "COMMENTS": "Accepted at Thirty-First AAAI Conference on Artificial Intelligence (AAAI17)", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["shuai xiao", "junchi yan", "xiaokang yang", "hongyuan zha", "stephen m chu"], "accepted": true, "id": "1705.08982"}, "pdf": {"name": "1705.08982.pdf", "metadata": {"source": "CRF", "title": "Modeling The Intensity Function Of Point Process Via Recurrent Neural Networks", "authors": ["Shuai Xiao", "Junchi Yan", "Stephen M. Chu", "Xiaokang Yang", "Hongyuan Zha"], "emails": ["benjaminforever@sjtu.edu.cn,", "xkyang@sjtu.edu.cn,", "jcyan@sei.ecnu.edu.cn,", "schu@us.ibm.com,", "zha@cc.gatech.edu"], "sections": [{"heading": "Introduction", "text": "In fact, most of them are able to outdo themselves, and they are able to outdo themselves, \"he said in an interview with the\" New York Times, \"\" I don't think they are able to outdo themselves. \""}, {"heading": "Related Work and Motivation", "text": "We look at the related concepts and work in this section, which focuses mainly on recursive neural networks (RNMs) and their applications in time series and sequence data. Then, we give our point of view on existing point-process methods and their connection to RNNs. All these observations actually motivate the work of this paper (LSTM). The building blocks of our model are the recurrent neural networks (RNNs), whose next states and outcomes depend on the current network states and input, which are more general models than the feed-forward networks. RNNs have long been researched in perceptual applications, but it can be very difficult to learn RNNs to develop long dynamics that may lead to the disappearance of networks."}, {"heading": "Network Structure and End-to-End Learning", "text": "We take a sequence {x} Tt = 1 as input, the RNN generates the hidden states (h) Tt = 1 and the results of a sequence (Elman 1990). \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp # p # p # p # p # p # p \u2212 p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p"}, {"heading": "Experiments on Real-world Data", "text": "In fact, most of them are able to survive on their own."}, {"heading": "Results and discussion", "text": "As shown in Fig.3, we are testing two architectures of the event type prediction layer, i.e. hierarchical prediction layer (Fig.3 (a)) and flat independent prediction histories (Fig.3 (b)). The main type comprises \"ticket\" and \"error\" and the subtype comprises \"ticket\" and the other six subtypes under \"error,\" as we have described them earlier in the paper.Confusion matrix The confusion matrix for the six subtypes under \"error\" event, and for the two main types \"ticket\" and \"error\" are shown in Fig.4 by various methods. We are making observations and analyses based on the results: 1) As shown in Fig.3, the flat architecture that directly surpasses the main types of the hierarchical prediction methods in the RNN hierarchy."}, {"heading": "Conclusion", "text": "We use Fig.5 to complete and further position our model in developing (implicit and explicit) modelling of the intensity function of the point process. In fact, the Hawkes process uses a fully explicit parametric model, and RMTPP misses the characteristics of dense time series to model time-varying base intensity, taking on a partially parametric form. We take another step through a complete implicit mapping model. Our model (see Fig.2) is simple, general, and can be learned end-to-end through standard back propagation, opening up new opportunities to transfer progress in neural network learning to the field of point process modeling and application. Clearly, the representative study in this paper points to its high potential for real-world problems, even if we do not have expert knowledge of the problem. This is in contrast to existing point process models, where an assumption of dynamics often needs to be specified beforehand."}], "references": [{"title": "Survival and event history analysis: a process point of view", "author": ["Borgan Aalen", "O. Gjessing 2008] Aalen", "O. Borgan", "H. Gjessing"], "venue": null, "citeRegEx": "Aalen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Aalen et al\\.", "year": 2008}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio"], "venue": null, "citeRegEx": "Bengio,? \\Q2015\\E", "shortCiteRegEx": "Bengio", "year": 2015}, {"title": "and Zhang", "author": ["R. Chandra"], "venue": "M.", "citeRegEx": "Chandra and Zhang 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Recurrent neural networks for multivariate time series with missing values", "author": ["Che"], "venue": null, "citeRegEx": "Che,? \\Q2016\\E", "shortCiteRegEx": "Che", "year": 2016}, {"title": "F", "author": ["P.A. Chen", "L.C. Chang", "Chang"], "venue": "J.", "citeRegEx": "Chen. Chang. and Chang 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "W", "author": ["E. Choi", "M.T. Bahadori", "A. Schuetz", "Stewart"], "venue": "F.; and Sun, J.", "citeRegEx": "Choi et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung"], "venue": null, "citeRegEx": "Chung,? \\Q2014\\E", "shortCiteRegEx": "Chung", "year": 2014}, {"title": "L", "author": ["J.T. Connor", "R.D. Martin", "Atlas"], "venue": "E.", "citeRegEx": "Connor. Martin. and Atlas 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Y", "author": ["Dauphin"], "venue": "N.; de Vries, H.; Chung, J.; and Bengio, Y.", "citeRegEx": "Dauphin et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent marked temporal point processes: Embedding event history to vectore", "author": ["Du"], "venue": null, "citeRegEx": "Du,? \\Q2016\\E", "shortCiteRegEx": "Du", "year": 2016}, {"title": "J", "author": ["Elman"], "venue": "L.", "citeRegEx": "Elman 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "T", "author": ["S. Ertekin", "C. Rudin", "McCormick"], "venue": "H.", "citeRegEx": "Ertekin. Rudin. and McCormick 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting clinical events by combining static and dynamic information using recurrent neural networks", "author": ["Esteban"], "venue": null, "citeRegEx": "Esteban,? \\Q2016\\E", "shortCiteRegEx": "Esteban", "year": 2016}, {"title": "Continuous-time predictive-maintenance scheduling for a deteriorating system", "author": ["Grall"], "venue": "IEEE transactions on Reliability", "citeRegEx": "Grall,? \\Q2002\\E", "shortCiteRegEx": "Grall", "year": 2002}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["rahman Mohamed Graves", "A. Hinton 2014] Graves", "A. rahman Mohamed", "G. Hinton"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor"], "venue": null, "citeRegEx": "Gregor,? \\Q2015\\E", "shortCiteRegEx": "Gregor", "year": 2015}, {"title": "Prediction of chaotic time series based on the recurrent predictor neural network", "author": ["Han"], "venue": "IEEE Transactions on Signal Processing", "citeRegEx": "Han,? \\Q2004\\E", "shortCiteRegEx": "Han", "year": 2004}, {"title": "A", "author": ["Hawkes"], "venue": "G.", "citeRegEx": "Hawkes 1971", "shortCiteRegEx": null, "year": 1971}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "and Westcott", "author": ["V. Isham"], "venue": "M.", "citeRegEx": "Isham and Westcott 1979", "shortCiteRegEx": null, "year": 1979}, {"title": "H", "author": ["A. Jain", "A. Singh", "Koppula"], "venue": "S.; Soh, S.; and Saxena, A.", "citeRegEx": "Jain et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "J", "author": ["Kingman"], "venue": "F. C.", "citeRegEx": "Kingman 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun"], "venue": "Proceedings of the IEEE 86(11):2278\u20132324", "citeRegEx": "LeCun,? \\Q1998\\E", "shortCiteRegEx": "LeCun", "year": 1998}, {"title": "and Mohler", "author": ["E. Lewis"], "venue": "E.", "citeRegEx": "Lewis and Mohler 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "P", "author": ["E. Lewis", "G. Mohler", "Brantingham"], "venue": "J.; and Bertozzi, A.", "citeRegEx": "Lewis et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Firebird: Predicting fire risk and prioritizing fire inspections in atlanta", "author": ["Madaio"], "venue": null, "citeRegEx": "Madaio,? \\Q2016\\E", "shortCiteRegEx": "Madaio", "year": 2016}, {"title": "R", "author": ["Mobley"], "venue": "K.", "citeRegEx": "Mobley 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "C", "author": ["Montgomery, D.C.", "Jennings"], "venue": "L.; and Kulahci, M.", "citeRegEx": "Montgomery. Jennings. and Kulahci 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Mikolov Pascanu", "R. Bengio 2013] Pascanu", "T. Mikolov", "Y. Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "A", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "Berg"], "venue": "C.; and Fei-Fei, L.", "citeRegEx": "Russakovsky et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling and predicting popularity dynamics via reinforced poisson processes", "author": ["Shen"], "venue": null, "citeRegEx": "Shen,? \\Q2014\\E", "shortCiteRegEx": "Shen", "year": 2014}, {"title": "and Babaoglu", "author": ["A. Sirbu"], "venue": "O.", "citeRegEx": "Sirbu and Babaoglu 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "M", "author": ["D.L. Snyder", "Miller"], "venue": "I.", "citeRegEx": "Snyder and Miller 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Q", "author": ["I. Sutskever", "O. Vinyals", "Le."], "venue": "V.", "citeRegEx": "Sutskever. Vinyals. and Le. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Z", "author": ["Tripathi, S.", "Lipton"], "venue": "C.; Belongie, S.; and Nguyen, T.", "citeRegEx": "Tripathi et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Patient flow prediction via discriminative learning of mutuallycorrecting processes", "author": ["Xu"], "venue": "IEEE transactions on Knowledge and Data Engineering", "citeRegEx": "Xu,? \\Q2016\\E", "shortCiteRegEx": "Xu", "year": 2016}, {"title": "and Zha", "author": ["Yang", "S.-h."], "venue": "H.", "citeRegEx": "Yang and Zha 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning social infectivity in sparse low-rank networks using multi-dimensional hawkes processe", "author": ["Zha Zhou", "K. Song 2013a] Zhou", "H. Zha", "L. Song"], "venue": "In AISTATS", "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}, {"title": "Learning triggering kernels for multi-dimensional hawkes processes", "author": ["Zha Zhou", "K. Song 2013b] Zhou", "H. Zha", "L. Song"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}], "referenceMentions": [], "year": 2017, "abstractText": "Event sequence, asynchronously generated with random timestamp, is ubiquitous among applications. The precise and arbitrary timestamp can carry important clues about the underlying dynamics, and has lent the event data fundamentally different from the time-series whereby series is indexed with fixed and equal time interval. One expressive mathematical tool for modeling event is point process. The intensity functions of many point processes involve two components: the background and the effect by the history. Due to its inherent spontaneousness, the background can be treated as a time series while the other need to handle the history events. In this paper, we model the background by a Recurrent Neural Network (RNN) with its units aligned with time series indexes while the history effect is modeled by another RNN whose units are aligned with asynchronous events to capture the long-range dynamics. The whole model with event type and timestamp prediction output layers can be trained end-to-end. Our approach takes an RNN perspective to point process, and models its background and history effect. For utility, our method allows a black-box treatment for modeling the intensity which is often a pre-defined parametric form in point processes. Meanwhile end-to-end training opens the venue for reusing existing rich techniques in deep network for point process modeling. We apply our model to the predictive maintenance problem using a log dataset by more than 1000 ATMs from a global bank headquartered in North America.", "creator": "LaTeX with hyperref package"}}}