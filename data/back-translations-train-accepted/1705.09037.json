{"id": "1705.09037", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "Deriving Neural Architectures from Sequence and Graph Kernels", "abstract": "The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects (cf. filters in CNN) via the kernels. Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training. We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art or competitive results across these applications. We also draw connections to existing architectures such as LSTMs.", "histories": [["v1", "Thu, 25 May 2017 03:58:10 GMT  (539kb,D)", "https://arxiv.org/abs/1705.09037v1", "to appear at ICML 2017; includes additional discussions"], ["v2", "Wed, 14 Jun 2017 14:34:24 GMT  (1618kb,D)", "http://arxiv.org/abs/1705.09037v2", "extended version of ICML 2017 camera ready"], ["v3", "Mon, 30 Oct 2017 13:56:23 GMT  (1618kb,D)", "http://arxiv.org/abs/1705.09037v3", "extended version of ICML 2017 camera ready"]], "COMMENTS": "to appear at ICML 2017; includes additional discussions", "reviews": [], "SUBJECTS": "cs.NE cs.CL cs.LG", "authors": ["tao lei", "wengong jin", "regina barzilay", "tommi s jaakkola"], "accepted": true, "id": "1705.09037"}, "pdf": {"name": "1705.09037.pdf", "metadata": {"source": "META", "title": "Deriving Neural Architectures from Sequence and Graph Kernels", "authors": ["Tao Lei", "Wengong Jin", "Regina Barzilay", "Tommi Jaakkola"], "emails": ["<taolei@csail.mit.edu>,", "gong@csail.mit.edu>."], "sections": [{"heading": "1. Introduction", "text": "In fact, it is the case that it is a form and manner in which one considers oneself to be in a position to surpass oneself. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "2. From String Kernels to Sequence NNs", "text": "Notations We define a sequence (or string) of symbols (e.g. a sentence) as x1: L = {xi} Li = 1 in which xi \u00b2 Rd represents its ith element and | x | = L specifies the length. Whenever it is clear from the context, we will omit the subscript and directly use x (and y) to specify a sequence. For a pair of vectors (or matrices) u, v, we designate < u, v > = x, k ukvk as their internal product. For a kernel function Ki (\u00b7) with subscript i we use (\u00b7) to name the underlying mapping, i.e. Ki, y) = < digits (x), i (y) > = \"We (x) > digits (y) > i (y).Kernel String Kernel measures the similarity between two sequences by counting divided subsequences (see Lodhi)."}, {"heading": "2.1. Single Layer as Kernel Computation", "text": "Now let us specify how the proposed class string kernel (x1: t) is embedded in the calculation. For the j-th line of the matrix W (j) \u00b7 \u00b7 activation functions (j) \u00b7 q functions (n), let us let cj [t] [i] [i] be the state vector cj [t], w (j) i) i represent the i-th line of the matrix W (j). Let us define wi, j = {w (1) i, w (2) i,..., w (j) i} as a \"reference sequence,\" constructing the i-th line of each matrix W (1),.., W (j).Theorem 1. Let us let x1: t be the prefix of x, consisting of the first t tokens, and Kj is the string kernel of the j-gram shown in Eq. (3) Then let us evaluate [cj [t], cj [t] the kernel [i] t, [i] t."}, {"heading": "2.2. Deep Networks as Deep Kernel Construction", "text": "We address the case where multiple layers of the same module are stacked to construct deeper networks, i.e. the initial states h (l) [t] of the l-th layer are fed to the (l + 1) th layer as an input sequence. We show that the layer stacking corresponds to the recursive kernel construction (i.e. (l + 1) - th kernel is defined above the l-th kernel tested for forward-facing networks (Zhang et al., 2016). We first generalize the sequence definition to allow for recursive construction. Note that the definition in Equation (3) uses the linear kernel (internal product) < xi, yk > as a \"subroutine\" to measure the similarity between substructures (e.g. tokens) within the sequences."}, {"heading": "3. From Graph Kernels to Graph NNs", "text": "In the previous section, we encode the calculation of the sequence nuclei into neural modules and demonstrate possible extensions using different base nuclei. The same ideas apply to deriving neural architectures from sequence and graph nuclei to other types of nuclei and data. Specifically, in this section, we derive neural components for graphs. Notations A graph is defined as G = (V, E), with each vertex v \u00b2 V being associated with the characteristic vector fv. The neighbor of the node v is referred to as N (v). Following previous notations, we use for each core function K \u0445 (\u00b7, \u00b7) with underlying mapping function \u03c6 (\u00b7) K \u0445, \u043c (\u00b7, \u00b7) to denote the post-activation nucleus arising from the composite underlying mapping component."}, {"heading": "3.1. Random Walk Kernel NNs", "text": "We start from a random-walk graph (Ga \ufffd rtner et al., 2003), which counts the common paths in two graphs. Formally, Pn (G) is the set of paths x = x1 \u00b7 \u00b7 xn, in which the two graphs G and G \ufffd 4 are defined. A random-walk graph n-th order is defined as: KnW (G, G \ufffd) = x-Pn (G) x-Pn (G) x-Pn (G) x-Pn (G) n-i = 1 < fxi, fyi > (5), in which fxi \"Rd\" is the feature vector of the node xi in the walk. Now we show how to realize the above graph with a neural module. In view of a graph G, the proposed neural module is: c1 [v] = node [v] [v = vector of the kernel [vv = x]."}, {"heading": "3.2. Unified View of Graph Kernels", "text": "The derivation of the neural module mentioned above could be extended to other classes of graph nuclei, such as sub-tree nuclei (cf. Ramon & Ga \ufffd rtner, 2003; Vishwanathan et al., 2010). Generally, most of these core functions factorize graphs into local substructures, e.g. K (G, G) = v \u00b2 v \u00b2 Kloc (v, v \u00b2) (7), where Kloc (v, v \u00b2) measures the similarity between local substructures centered with nodes v and v \u00b2. For example, the random nucleus node can be defined as equivalent to Knloc (v, v \u00b2) = < fv, fv \u00b2 > if n = 1 < fv \u00b2, fv \u00b2 >, fv \u00b2, u \u00b2 n (v), (v \u00b2), if the kernel (v \u00b2) or kernel (n \u2212 1loc), if n > Other nuclei such as sub-tree nuclei."}, {"heading": "3.3. Deep Graph Kernels and NNs", "text": "According to Section 2, we could stack several graph kernels NNs to form a deep network, i.e.: c (l) 1 [v] = W (l, 1) h (l \u2212 1) [v] c (l) j [v] = W (l, j) h (l \u2212 1) [v]] p (l \u2212 1) h (l) [v] p (l) p (l) n [v] p (l) n [v]) p (l) n [v]) 1 \u2264 l (l \u2212 1) p (l) p (l \u00b7 l) n [v] p (l \u00b7 l) n [v] p (l < j \u2264 nThe local core function is recursively defined in two dimensions: Depth (l) h (Term h) and Width (Term cj)."}, {"heading": "3.4. Connection to Weisfeiler-Lehman Kernel", "text": "The above Weisfeiler-Lehman borrows concepts from the WeisfeilerLehman."}, {"heading": "4. Adaptive Decay with Neural Gates", "text": "The sequence and the graph kernel (and its neural components) discussed so far use a constant decay value \u03bb independent of the current input. However, this is often not the case, since the meaning of the input may vary depending on the context or application. An extension consists in using neural gates that control the decay factor adaptively. At this point, we give two illustrative examples: gated string kernel NN By replacing the constant decay factor with a sigmoid gate, we modify our single-layer sequence module as: \u03bbt = \u03c3 (U [xt, ht \u2212 1] + b) c1 [t] = \u03bbt c1 [t \u2212 1] + (W (1) xt) cj [t] = \u03bbt \u2212 1] + (t \u2212 1] W (t \u2212 t \u2212 t \u2212 n) xt x (t \u2212 t \u2212 n) xt (t \u2212 n) xt (t \u2212 1 t \u2212 n that G \u2212 1 t \u2212 n, [t \u2212 1] xt + 1 cxt [t] j \u2212 1 t \u2212 t \u2212 n that G \u2212 t \u2212 n \u2212 n \u2212 n that G \u2212 t \u2212 t \u2212 t \u2212 t \u2212 n \u2212 n that G \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 n that G \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 n that G \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 n that G \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 n that G \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 n that G \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 n that G \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 n that G \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 n that G \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 n that G \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 n that G \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 n that G \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 n that g \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 t \u2212 n"}, {"heading": "5. Related Work", "text": "In fact, most of them are able to reform themselves, and they are able to reform themselves, \"he told the German Press Agency.\" We have to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules. \"He added:\" We have to play by the rules we know and we have to play by the rules we have imposed on ourselves. \"He added:\" We have to play by the rules. \""}, {"heading": "6. Experiments", "text": "In this section, we apply the proposed sequence and graph modules to different tasks and evaluate their performance empirically compared to other models of neural networks. These tasks include speech modelling, sentiment classification and molecular regression."}, {"heading": "6.1. Language Modeling on PTB", "text": "Dataset and Setup We use the Penn Tree Bank (PTB) corpus as a benchmark (5.5 million). The dataset contains a total of about 1 million tokens. We use the standard train / development / test split of this dataset with a vocabulary of size 10,000.Model Configuration Following standard practice, we use SGD with an initial learning rate of 1.0 and reduce the learning rate by a constant factor after a particular epoch. We propagate the gradient with a roll size of 35 and use Dropout (Hinton et al., 2012) as regulation. Unless otherwise specified, we train 3-layer networks with n = 1 and normalized adaptive decay.5 Following (Zilly et al., 2016) we add highway connections (Srivastava et al al al al al al al., 2015) within each layer: c (l) [t] = [t] [t] = [t] c."}, {"heading": "6.2. Sentiment Classification", "text": "We use the benchmark Stanford Sentiment Treebank (Socher et al., 2013). The data set consists of 11855 analyzed English sentences, which are commented both at the root (i.e. sentence) level and at the phrase level with fine-grained 5-grade labels. We use the standard split for training, development, and testing. Following previous work, we also evaluate our model on the binary classification variant of this benchmark, ignoring all neutral sentences. Derive Neural architectures from sequence and graph kernelsFollowing recent work by DAN (Iyyer et al., 2015) and RLSTM (Tai et al., 2015), we use the publicly available 300-dimensional GloVe word vector model (Pennington et al., 2014). Unlike previous work that fine-tuned the word vector model, we normalize the vectors (i.e. we have only displayed the 300-dimensional word light vectors as the best hidden layer)."}, {"heading": "6.3. Molecular Graph Regression", "text": "Dataset and setup We evaluate our graphical NN models on the benchmark of the Harvard Clean Energy Project used in Dai et al. (2016); Duvenaud et al. (2015) as their evaluation dataset. This dataset contains 2.3 million candidate molecules, each molecule labeled with its value for energy conversion (PCE). We follow exactly the same tensile test split as Dai et al. (2016) and the same sampling procedure on the training data (but not the test data) to focus the algorithm more on molecules with higher PCE values as the data are unevenly distributed. We use the same functionality as in Duvenaud et al. (2015) for atoms and bonds. Initial atomic characteristics include the atomic element, its degree, the number of attached hydrogens, its implicit value, and an aromaticity optimizer."}, {"heading": "7. Conclusion", "text": "By linking kernel and neural operations, we have a \"template\" for deriving new families of neural architectures for sequences and graphs. We hope that the theoretical view of neural kernel networks can be helpful for future model research."}, {"heading": "Acknowledgement", "text": "We thank Prof. Le Song for sharing the Harvard Clean Energy Project dataset. We also thank Yu Zhang, Vikas Garg, David Alvarez, Tianxiao Shen, Karthik Narasimhan and the reviewers for their helpful comments. This work was supported by the DARPA Make-It Program under contract ARO W911NF-16-2-0023."}, {"heading": "A. Examples of kernel / neural variants", "text": "Our theoretical results apply to some other variants of sequence cores and their associated neural components. In this section we give some examples. Table 4 shows three network variants corresponding to three realizations of string cores provided in Table 5.Connection to LSTMs. Interestingly, many recent work through empirical research has achieved similar RNN architectures. For example, Greff et al. (2015) found that simplification of LSTMs by removing the input gate or coupling with the Forge Gate did not significantly change performance. However, the Forge Gate (according to the decay factor \u03bb in our notation) is crucial for performance. This is consistent with our theoretical analysis and empirical results in Figure 3. Furthermore, Balduzzi & Ghifary (2016) and Lee et al. (2017) both suggest that a linear additive state calculation is not sufficient to provide a competitive performance in comparison to sufficiency Wc1 = ST1 (STc1) (Wc2 = STc1)."}, {"heading": "B. Proof of Theorem 1", "text": "We first generalise the core definition in Eq. < < < < > \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "C. Proof of Theorem 2", "text": "The first review of necessary concepts and notations for the simplicity of reading. Similar to the evidence in Appendix B is the generalized string Kernel K (l) and K (l) \u03c3 in Eq. (???) can be defined with the underlying correlations. (l) (x) (x) (x) (x) \u00b7 \u00b7 i1) (l). ((\u00b7 \u00b7 \u00b7 \u00b7 i1). (). () (\u00b7 \u00b7 i1). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.).). (.). (.).). (.).). (.).). (.).). (.).). (.).).). (.).). (.).). (.).).). (.).). (.).).). (.). (.).).).). (.).).). (.).). (.).). (.).). (.). (.).). (.). (.).).). (.).). (.).).).). (.).). (.).). (.). (.).).).).). (.).).). (.).).). (.).). (. (.).). (.).).)."}, {"heading": "D. Proof for Theorem 3", "text": "Remember that the kernel random diagram is defined as follows: Kn (G, G) = 1 < fxi, fyi > \u03c6n (G) = 2 x x Pn (G) fx0 fx1 \u00b7 \u00b7 \u00b7 fxnand single-layer graph NN calculates its states as follows: cn [v] = 2 x N (v) cn \u2212 1 [u] W (n) fv = 2 x N (un \u2212 1) \u00b7 0 x N (u1) W (0) fu0 W (n) fu1 \u00b7 W (n) fun = 1 x (u) W (n) fun = 1 x (u) W (n) u = u0 \u00b7 \u00b7 un Pn (G, v) un N (G, v) W (0) fu0 W (n) fu1 \u00b7 n W (n) fu1 \u00b7 fu1 \u00b7 W (n) fun = 1 x (u) W (n) u = u0 \u00b7 u (n)."}, {"heading": "E. Proof for Theorem 4", "text": "For clarity we return the kernel definition and theorem as follows: K (L, n) (G, v) (G, v) (v) = v (K, n) loc (V, n) loc (V, v) loc (V, v) loc (V, v) nel loc (V, n) loc (V) loc (V) loc (V) loc (V) loc (V) loc (V) loc (V) loc (V) loc (V) loc (V) loc (V) loc (G) loc (G) loc (G) nel loc (V) loc (V) loc (K) loc (V) loc (V) loc (V) loc (V) loc (V) loc (V) loc (V) loc (V) loc (G) loc (G) loc (G) loc (G) loc (V) loc (G) loc (G), loc (G) loc (G) loc (G) loc (G) loc (G) loc (G) loc (G) loc (G) loc (G) loc (G) loc (G (G) loc (G) loc (G) loc (G) loc (G (G) loc (G) loc (G) loc (G (G) loc (G) loc (G (G) loc (G) loc (G (G) loc (G) loc (G) loc (G) loc (G (G) loc (G (G) loc (G) loc (G (G) loc) loc (G (G (G) loc) loc (G (G) loc (G) loc (G (G) loc) loc (G (G (G) loc) loc (G (G (G (G) loc) loc) loc (G (G (G) loc) loc (G (G (G) loc) loc) loc (G (G (G) loc) loc (G"}], "references": [{"title": "Deep convolutional networks are hierarchical kernel machines", "author": ["Anselmi", "Fabio", "Rosasco", "Lorenzo", "Tan", "Cheston", "Poggio", "Tomaso"], "venue": null, "citeRegEx": "Anselmi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anselmi et al\\.", "year": 2015}, {"title": "Strongly-typed recurrent neural networks", "author": ["Balduzzi", "David", "Ghifary", "Muhammad"], "venue": "In Proceedings of 33th International Conference on Machine Learning (ICML),", "citeRegEx": "Balduzzi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Balduzzi et al\\.", "year": 2016}, {"title": "Spectral networks and locally connected networks on graphs", "author": ["Bruna", "Joan", "Zaremba", "Wojciech", "Szlam", "Arthur", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1312.6203,", "citeRegEx": "Bruna et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bruna et al\\.", "year": 2013}, {"title": "Long short-term memory networks for machine reading", "author": ["Cheng", "Jianpeng", "Dong", "Li", "Lapata", "Mirella"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Kernel methods for deep learning", "author": ["Cho", "Youngmin", "Saul", "Lawrence K"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Cho et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2009}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung", "Junyoung", "Gulcehre", "Caglar", "Cho", "KyungHyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Discriminative embeddings of latent variable models for structured data", "author": ["Dai", "Hanjun", "Bo", "Song", "Le"], "venue": "arXiv preprint arXiv:1603.05629,", "citeRegEx": "Dai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2016}, {"title": "Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity", "author": ["Daniely", "Amit", "Frostig", "Roy", "Singer", "Yoram"], "venue": null, "citeRegEx": "Daniely et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2016}, {"title": "Recurrent neural network grammars", "author": ["Dyer", "Chris", "Kuncoro", "Adhiguna", "Ballesteros", "Miguel", "Smith", "Noah A"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Dyer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Gal", "Yarin", "Ghahramani", "Zoubin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Gal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2016}, {"title": "On graph kernels: Hardness results and efficient alternatives", "author": ["G\u00e4rtner", "Thomas", "Flach", "Peter", "Wrobel", "Stefan"], "venue": "In Learning Theory and Kernel Machines,", "citeRegEx": "G\u00e4rtner et al\\.,? \\Q2003\\E", "shortCiteRegEx": "G\u00e4rtner et al\\.", "year": 2003}, {"title": "Lstm: A search space odyssey", "author": ["Greff", "Klaus", "Srivastava", "Rupesh Kumar", "Koutn\u0131\u0301k", "Jan", "Steunebrink", "Bas R", "Schmidhuber", "J\u00fcrgen"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Steps toward deep kernel methods from infinite neural networks", "author": ["Hazan", "Tamir", "Jaakkola", "Tommi"], "venue": "arXiv preprint arXiv:1508.05133,", "citeRegEx": "Hazan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2015}, {"title": "Improper deep kernels", "author": ["Heinemann", "Uri", "Livni", "Roi", "Eban", "Elad", "Elidan", "Gal", "Globerson", "Amir"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Heinemann et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Heinemann et al\\.", "year": 2016}, {"title": "Deep convolutional networks on graph-structured data", "author": ["Henaff", "Mikael", "Bruna", "Joan", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1506.05163,", "citeRegEx": "Henaff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Henaff et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Irsoy", "Ozan", "Cardie", "Claire"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Irsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Irsoy et al\\.", "year": 2014}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Iyyer", "Mohit", "Manjunatha", "Varun", "Boyd-Graber", "Jordan", "Daum\u00e9 III", "Hal"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Kalchbrenner", "Nal", "Grefenstette", "Edward", "Blunsom", "Phil"], "venue": "In Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Kim", "Yoon"], "venue": "In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP", "citeRegEx": "Kim and Yoon.,? \\Q2014\\E", "shortCiteRegEx": "Kim and Yoon.", "year": 2014}, {"title": "Character-aware neural language models", "author": ["Kim", "Yoon", "Jernite", "Yacine", "Sontag", "David", "Rush", "Alexander M"], "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy Lei"], "venue": "In International Conference on Learning Representation,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Kumar", "Ankit", "Irsoy", "Ozan", "Ondruska", "Peter", "Iyyer", "Mohit", "James Bradbury", "Ishaan Gulrajani", "Zhong", "Victor", "Paulus", "Romain", "Socher", "Richard"], "venue": null, "citeRegEx": "Kumar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2016}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Quoc", "Mikolov", "Tomas"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Recurrent additive networks", "author": ["Lee", "Kenton", "Levy", "Omer", "Zettlemoyer", "Luke"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2017}, {"title": "Semi-supervised question retrieval with gated convolutions", "author": ["Lei", "Tao", "Joshi", "Hrishikesh", "Barzilay", "Regina", "Jaakkola", "Tommi", "Tymoshenko", "Katerina", "Moschitti", "Alessandro", "Marquez", "Lluis"], "venue": "arXiv preprint arXiv:1512.05726,", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Gated graph sequence neural networks", "author": ["Li", "Yujia", "Tarlow", "Daniel", "Brockschmidt", "Marc", "Zemel", "Richard"], "venue": "arXiv preprint arXiv:1511.05493,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Text classification using string kernels", "author": ["Lodhi", "Huma", "Saunders", "Craig", "Shawe-Taylor", "John", "Cristianini", "Nello", "Watkins", "Chris"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lodhi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lodhi et al\\.", "year": 2002}, {"title": "Pointer sentinel mixture models", "author": ["Merity", "Stephen", "Xiong", "Caiming", "Bradbury", "James", "Socher", "Richard"], "venue": "arXiv preprint arXiv:1609.07843,", "citeRegEx": "Merity et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation. volume", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": null, "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Using the output embedding to improve language models", "author": ["Press", "Ofir", "Wolf", "Lior"], "venue": "arXiv preprint arXiv:1608.05859,", "citeRegEx": "Press et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Press et al\\.", "year": 2016}, {"title": "Expressivity versus efficiency of graph kernels", "author": ["Ramon", "Jan", "G\u00e4rtner", "Thomas"], "venue": "In First international workshop on mining graphs, trees and sequences,", "citeRegEx": "Ramon et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ramon et al\\.", "year": 2003}, {"title": "Learning kernel-based halfspaces with the 01 loss", "author": ["Shalev-Shwartz", "Shai", "Shamir", "Ohad", "Sridharan", "Karthik"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Weisfeiler-lehman graph kernels", "author": ["Shervashidze", "Nino", "Schweitzer", "Pascal", "Leeuwen", "Erik Jan van", "Mehlhorn", "Kurt", "Borgwardt", "Karsten M"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Shervashidze et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shervashidze et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Socher", "Richard", "Perelygin", "Alex", "Wu", "Jean", "Chuang", "Jason", "Manning", "Christopher D", "Ng", "Andrew Y", "Potts", "Christopher"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Training very deep networks", "author": ["Srivastava", "Rupesh K", "Greff", "Klaus", "Schmidhuber", "J\u00fcrgen"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Improved semantic representations from treestructured long short-term memory networks", "author": ["Tai", "Kai Sheng", "Socher", "Richard", "Manning", "Christopher D"], "venue": "In Proceedings of the 53th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Value iteration networks", "author": ["Tamar", "Aviv", "Levine", "Sergey", "Abbeel", "Pieter", "Wu", "Yi", "Thomas", "Garrett"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tamar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tamar et al\\.", "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "`1regularized neural networks are improperly learnable in polynomial time", "author": ["Zhang", "Yuchen", "Lee", "Jason D", "Jordan", "Michael I"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Recurrent Highway Networks", "author": ["Zilly", "Julian Georg", "Srivastava", "Rupesh Kumar", "Koutn\u0131\u0301k", "Jan", "Schmidhuber", "J\u00fcrgen"], "venue": "arXiv preprint arXiv:1607.03474,", "citeRegEx": "Zilly et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zilly et al\\.", "year": 2016}, {"title": "Neural architecture search with reinforcement learning", "author": ["Zoph", "Barret", "Le", "Quoc V"], "venue": "arXiv preprint arXiv:1611.01578,", "citeRegEx": "Zoph et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}, {"title": "2017) both suggest that a linear additive state computation suffices to provide competitive performance compared to LSTMs", "author": ["Lee"], "venue": "Ghifary", "citeRegEx": "Lee,? \\Q2017\\E", "shortCiteRegEx": "Lee", "year": 2017}], "referenceMentions": [{"referenceID": 5, "context": "For instance, LSTM (Hochreiter & Schmidhuber, 1997), GRU (Chung et al., 2014) and other complex recurrent units (Zoph & Le, 2016) can be easily adapted to embed structured objects such as sentences (Tai et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 37, "context": ", 2014) and other complex recurrent units (Zoph & Le, 2016) can be easily adapted to embed structured objects such as sentences (Tai et al., 2015) or molecules (Li et al.", "startOffset": 128, "endOffset": 146}, {"referenceID": 27, "context": ", 2015) or molecules (Li et al., 2015; Dai et al., 2016) into vector spaces suitable for later processing by standard predictive methods.", "startOffset": 21, "endOffset": 56}, {"referenceID": 6, "context": ", 2015) or molecules (Li et al., 2015; Dai et al., 2016) into vector spaces suitable for later processing by standard predictive methods.", "startOffset": 21, "endOffset": 56}, {"referenceID": 38, "context": "For example, value iteration calculations can be folded into convolutional architectures so as to optimize the representations to facilitate planning (Tamar et al., 2016).", "startOffset": 150, "endOffset": 170}, {"referenceID": 6, "context": "Similarly, inference calculations in graphical models about latent states of variables such as atom characteristics can be directly associated with embedding operations (Dai et al., 2016).", "startOffset": 169, "endOffset": 187}, {"referenceID": 28, "context": "For example, in a string kernel (Lodhi et al., 2002), S may refer to all possible subsequences while a graph kernel (Vishwanathan et al.", "startOffset": 32, "endOffset": 52}, {"referenceID": 40, "context": "Several studies have highlighted the relation between feed-forward neural architectures and kernels (Hazan & Jaakkola, 2015; Zhang et al., 2016) but we are unaware of any prior work pertaining to kernels associated with neural architectures for structured objects.", "startOffset": 100, "endOffset": 144}, {"referenceID": 28, "context": "String Kernel String kernel measures the similarity between two sequences by counting shared subsequences (see Lodhi et al. (2002)).", "startOffset": 111, "endOffset": 131}, {"referenceID": 33, "context": "It turns out that many activations are also functions in the reproducing kernel Hilbert space (RKHS) of certain kernel functions (see Shalev-Shwartz et al. (2011); Zhang et al.", "startOffset": 134, "endOffset": 163}, {"referenceID": 33, "context": "It turns out that many activations are also functions in the reproducing kernel Hilbert space (RKHS) of certain kernel functions (see Shalev-Shwartz et al. (2011); Zhang et al. (2016)).", "startOffset": 134, "endOffset": 184}, {"referenceID": 40, "context": "(l + 1)th kernel is defined on top of l-th kernel), which has been proven for feed-forward networks (Zhang et al., 2016).", "startOffset": 100, "endOffset": 120}, {"referenceID": 10, "context": "Random Walk Kernel NNs We start from random walk graph kernels (G\u00e4rtner et al., 2003), which count common walks in two graphs.", "startOffset": 63, "endOffset": 85}, {"referenceID": 34, "context": "This parameter tying mechanism allows our model to embed Weisfeiler-Lehman kernel (Shervashidze et al., 2011).", "startOffset": 82, "endOffset": 109}, {"referenceID": 34, "context": "Figure taken from Shervashidze et al. (2011) Note that our definition of r(v) is exactly the same as hv in Equation 9, with \u25e6 being additive composition.", "startOffset": 18, "endOffset": 45}, {"referenceID": 5, "context": "This includes recurrent modules with the ability to carry persistent memories such as LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Chung et al., 2014), as well as non-consecutive convolutional modules (RCNNs, Lei et al.", "startOffset": 132, "endOffset": 152}, {"referenceID": 0, "context": "Similar results have been made for convolutional neural nets (Anselmi et al., 2015), and general computational graphs (Daniely et al.", "startOffset": 61, "endOffset": 83}, {"referenceID": 7, "context": ", 2015), and general computational graphs (Daniely et al., 2016).", "startOffset": 42, "endOffset": 64}, {"referenceID": 40, "context": "While some prior work appeals to convex optimization through improper learning (Zhang et al., 2016; Heinemann et al., 2016) (since kernel space is larger), we use the proposed networks as building blocks in typical non-convex but flexible neural network training.", "startOffset": 79, "endOffset": 123}, {"referenceID": 13, "context": "While some prior work appeals to convex optimization through improper learning (Zhang et al., 2016; Heinemann et al., 2016) (since kernel space is larger), we use the proposed networks as building blocks in typical non-convex but flexible neural network training.", "startOffset": 79, "endOffset": 123}, {"referenceID": 3, "context": "This includes recurrent modules with the ability to carry persistent memories such as LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Chung et al., 2014), as well as non-consecutive convolutional modules (RCNNs, Lei et al. (2015)), and others.", "startOffset": 133, "endOffset": 229}, {"referenceID": 3, "context": "This includes recurrent modules with the ability to carry persistent memories such as LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Chung et al., 2014), as well as non-consecutive convolutional modules (RCNNs, Lei et al. (2015)), and others. More recently, Zoph & Le (2016) exemplified a reinforcement learning-based search algorithm to further optimize the design of such recurrent architectures.", "startOffset": 133, "endOffset": 275}, {"referenceID": 3, "context": "This includes recurrent modules with the ability to carry persistent memories such as LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Chung et al., 2014), as well as non-consecutive convolutional modules (RCNNs, Lei et al. (2015)), and others. More recently, Zoph & Le (2016) exemplified a reinforcement learning-based search algorithm to further optimize the design of such recurrent architectures. Our proposed neural networks offer similar state evolution and feature aggregation functionalities but derive the motivation for the operations involved from wellestablished kernel computations over sequences. Recursive neural networks are alternative architectures to model hierarchical structures such as syntax trees and logic forms. For instance, Socher et al. (2013) employs recursive networks for sentence classification, where each node in the dependency tree of the sentence is transformed into a vector representation.", "startOffset": 133, "endOffset": 771}, {"referenceID": 3, "context": "This includes recurrent modules with the ability to carry persistent memories such as LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Chung et al., 2014), as well as non-consecutive convolutional modules (RCNNs, Lei et al. (2015)), and others. More recently, Zoph & Le (2016) exemplified a reinforcement learning-based search algorithm to further optimize the design of such recurrent architectures. Our proposed neural networks offer similar state evolution and feature aggregation functionalities but derive the motivation for the operations involved from wellestablished kernel computations over sequences. Recursive neural networks are alternative architectures to model hierarchical structures such as syntax trees and logic forms. For instance, Socher et al. (2013) employs recursive networks for sentence classification, where each node in the dependency tree of the sentence is transformed into a vector representation. Tai et al. (2015) further proposed tree-LSTM, which incorporates LSTM-style architectures as the transformation unit.", "startOffset": 133, "endOffset": 945}, {"referenceID": 3, "context": "This includes recurrent modules with the ability to carry persistent memories such as LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Chung et al., 2014), as well as non-consecutive convolutional modules (RCNNs, Lei et al. (2015)), and others. More recently, Zoph & Le (2016) exemplified a reinforcement learning-based search algorithm to further optimize the design of such recurrent architectures. Our proposed neural networks offer similar state evolution and feature aggregation functionalities but derive the motivation for the operations involved from wellestablished kernel computations over sequences. Recursive neural networks are alternative architectures to model hierarchical structures such as syntax trees and logic forms. For instance, Socher et al. (2013) employs recursive networks for sentence classification, where each node in the dependency tree of the sentence is transformed into a vector representation. Tai et al. (2015) further proposed tree-LSTM, which incorporates LSTM-style architectures as the transformation unit. Dyer et al. (2015; 2016) recently introduced a recursive neural model for transitionbased language modeling and parsing. While not specifically discussed in the paper, our ideas do extend to similar neural components for hierarchical objects (e.g. trees). Graph Networks Most of the current graph neural architectures perform either convolutional or recurrent operations on graphs. Duvenaud et al. (2015) developed Neural Fingerprint for chemical compounds, where each convolution operation is a sum of neighbor node features, followed by a linear transformation.", "startOffset": 133, "endOffset": 1450}, {"referenceID": 1, "context": ", Bruna et al. (2013) and Henaff et al.", "startOffset": 2, "endOffset": 22}, {"referenceID": 1, "context": ", Bruna et al. (2013) and Henaff et al. (2015), rely on graph Laplacian or Fourier transform.", "startOffset": 2, "endOffset": 47}, {"referenceID": 1, "context": ", Bruna et al. (2013) and Henaff et al. (2015), rely on graph Laplacian or Fourier transform. For recurrent architectures, Li et al. (2015) proposed gated graph neural networks, where neighbor features are aggregated by GRU function.", "startOffset": 2, "endOffset": 140}, {"referenceID": 1, "context": ", Bruna et al. (2013) and Henaff et al. (2015), rely on graph Laplacian or Fourier transform. For recurrent architectures, Li et al. (2015) proposed gated graph neural networks, where neighbor features are aggregated by GRU function. Dai et al. (2016) considers a different architecture where a graph is viewed as a latent variable graphical model.", "startOffset": 2, "endOffset": 252}, {"referenceID": 1, "context": ", Bruna et al. (2013) and Henaff et al. (2015), rely on graph Laplacian or Fourier transform. For recurrent architectures, Li et al. (2015) proposed gated graph neural networks, where neighbor features are aggregated by GRU function. Dai et al. (2016) considers a different architecture where a graph is viewed as a latent variable graphical model. Their recurrent model is derived from Belief Propagation-like algorithms. Our approach is most closely related to Dai et al. (2016), in terms of neighbor feature aggregation and resulting recurrent architecture.", "startOffset": 2, "endOffset": 481}, {"referenceID": 1, "context": ", Bruna et al. (2013) and Henaff et al. (2015), rely on graph Laplacian or Fourier transform. For recurrent architectures, Li et al. (2015) proposed gated graph neural networks, where neighbor features are aggregated by GRU function. Dai et al. (2016) considers a different architecture where a graph is viewed as a latent variable graphical model. Their recurrent model is derived from Belief Propagation-like algorithms. Our approach is most closely related to Dai et al. (2016), in terms of neighbor feature aggregation and resulting recurrent architecture. Nonetheless, the focus of this paper is on providing a framework for how such recurrent networks could be derived from deep graph kernels. Kernels and Neural Nets Our work follows recent work demonstrating the connection between neural networks and kernels (Cho & Saul, 2009; Hazan & Jaakkola, 2015). For example, Zhang et al. (2016) showed that standard feedforward neural nets belong to a larger space of recursively constructed kernels (given certain activation functions).", "startOffset": 2, "endOffset": 895}, {"referenceID": 15, "context": "We back-propagate the gradient with an unroll size of 35 and use dropout (Hinton et al., 2012) as the regularization.", "startOffset": 73, "endOffset": 94}, {"referenceID": 41, "context": "5 Following (Zilly et al., 2016), we add highway connections (Srivastava et al.", "startOffset": 12, "endOffset": 32}, {"referenceID": 36, "context": ", 2016), we add highway connections (Srivastava et al., 2015) within each layer: c[t] = \u03bbt c[t\u2212 1] + (1\u2212 \u03bbt) (W(l)h(l\u22121)[t]) h[t] = ft c[t] + (1\u2212 ft) h(l\u22121)[t] where h[t] = xt, \u03bbt is the gated decay factor and ft is the transformation gate of highway connections.", "startOffset": 36, "endOffset": 61}, {"referenceID": 41, "context": "8 perplexity when the recurrence depth is increased to 4, being state-of-the-art and on par with the results reported in (Zilly et al., 2016; Zoph & Le, 2016).", "startOffset": 121, "endOffset": 158}, {"referenceID": 15, "context": "We back-propagate the gradient with an unroll size of 35 and use dropout (Hinton et al., 2012) as the regularization. Unless otherwise specified, we train 3-layer networks with n = 1 and normalized adaptive decay.5 Following (Zilly et al., 2016), we add highway connections (Srivastava et al., 2015) within each layer: c[t] = \u03bbt c[t\u2212 1] + (1\u2212 \u03bbt) (W(l)h(l\u22121)[t]) h[t] = ft c[t] + (1\u2212 ft) h(l\u22121)[t] where h[t] = xt, \u03bbt is the gated decay factor and ft is the transformation gate of highway connections.6 Results Table 1 compares our model with various state-ofthe-art models. Our small model with 5 million parameters achieves a test perplexity of 73.6, already outperforming many results achieved using much larger network. By increasing the network size to 20 million, we obtain a test perplexity of 69.2, with standard dropout. Adding variational dropout (Gal & Ghahramani, 2016) within the recurrent cells further improves the perplexity to 65.5. Finally, the model achieves 63.8 perplexity when the recurrence depth is increased to 4, being state-of-the-art and on par with the results reported in (Zilly et al., 2016; Zoph & Le, 2016). Note that Zilly et al. (2016) uses 10 neural layers and Zoph & Le (2016) adopts a complex recurrent cell found by reinforcement learning based search.", "startOffset": 74, "endOffset": 1171}, {"referenceID": 15, "context": "We back-propagate the gradient with an unroll size of 35 and use dropout (Hinton et al., 2012) as the regularization. Unless otherwise specified, we train 3-layer networks with n = 1 and normalized adaptive decay.5 Following (Zilly et al., 2016), we add highway connections (Srivastava et al., 2015) within each layer: c[t] = \u03bbt c[t\u2212 1] + (1\u2212 \u03bbt) (W(l)h(l\u22121)[t]) h[t] = ft c[t] + (1\u2212 ft) h(l\u22121)[t] where h[t] = xt, \u03bbt is the gated decay factor and ft is the transformation gate of highway connections.6 Results Table 1 compares our model with various state-ofthe-art models. Our small model with 5 million parameters achieves a test perplexity of 73.6, already outperforming many results achieved using much larger network. By increasing the network size to 20 million, we obtain a test perplexity of 69.2, with standard dropout. Adding variational dropout (Gal & Ghahramani, 2016) within the recurrent cells further improves the perplexity to 65.5. Finally, the model achieves 63.8 perplexity when the recurrence depth is increased to 4, being state-of-the-art and on par with the results reported in (Zilly et al., 2016; Zoph & Le, 2016). Note that Zilly et al. (2016) uses 10 neural layers and Zoph & Le (2016) adopts a complex recurrent cell found by reinforcement learning based search.", "startOffset": 74, "endOffset": 1214}, {"referenceID": 39, "context": "Model |\u03b8| PPL LSTM (large) (Zaremba et al., 2014) 66m 78.", "startOffset": 27, "endOffset": 49}, {"referenceID": 21, "context": "4 Character CNN (Kim et al., 2015) 19m 78.", "startOffset": 16, "endOffset": 34}, {"referenceID": 41, "context": "9 Variational RHN (Zilly et al., 2016) 23m 65.", "startOffset": 18, "endOffset": 38}, {"referenceID": 35, "context": "We use the Stanford Sentiment Treebank benchmark (Socher et al., 2013).", "startOffset": 49, "endOffset": 70}, {"referenceID": 31, "context": "Model Fine Binary RNN (Socher et al. (2011)) 43.", "startOffset": 23, "endOffset": 44}, {"referenceID": 31, "context": "Model Fine Binary RNN (Socher et al. (2011)) 43.2 82.4 RNTN (Socher et al. (2013)) 45.", "startOffset": 23, "endOffset": 82}, {"referenceID": 31, "context": "Model Fine Binary RNN (Socher et al. (2011)) 43.2 82.4 RNTN (Socher et al. (2013)) 45.7 85.4 DRNN (Irsoy & Cardie (2014)) 49.", "startOffset": 23, "endOffset": 121}, {"referenceID": 31, "context": "Model Fine Binary RNN (Socher et al. (2011)) 43.2 82.4 RNTN (Socher et al. (2013)) 45.7 85.4 DRNN (Irsoy & Cardie (2014)) 49.8 86.8 RLSTM (Tai et al. (2015)) 51.", "startOffset": 23, "endOffset": 157}, {"referenceID": 17, "context": "0 DCNN (Kalchbrenner et al. (2014)) 48.", "startOffset": 8, "endOffset": 35}, {"referenceID": 17, "context": "0 DCNN (Kalchbrenner et al. (2014)) 48.5 86.9 CNN-MC (Kim (2014)) 47.", "startOffset": 8, "endOffset": 65}, {"referenceID": 17, "context": "0 DCNN (Kalchbrenner et al. (2014)) 48.5 86.9 CNN-MC (Kim (2014)) 47.4 88.1 Bi-LSTM (Tai et al. (2015)) 49.", "startOffset": 8, "endOffset": 103}, {"referenceID": 3, "context": "5 LSTMN (Cheng et al. (2016)) 47.", "startOffset": 9, "endOffset": 29}, {"referenceID": 3, "context": "5 LSTMN (Cheng et al. (2016)) 47.9 87.0 PVEC (Le & Mikolov (2014)) 48.", "startOffset": 9, "endOffset": 66}, {"referenceID": 3, "context": "5 LSTMN (Cheng et al. (2016)) 47.9 87.0 PVEC (Le & Mikolov (2014)) 48.7 87.8 DAN (Iyyer et al. (2014)) 48.", "startOffset": 9, "endOffset": 102}, {"referenceID": 3, "context": "5 LSTMN (Cheng et al. (2016)) 47.9 87.0 PVEC (Le & Mikolov (2014)) 48.7 87.8 DAN (Iyyer et al. (2014)) 48.2 86.8 DMN (Kumar et al. (2016)) 52.", "startOffset": 9, "endOffset": 138}, {"referenceID": 18, "context": "Following the recent work of DAN (Iyyer et al., 2015) and RLSTM (Tai et al.", "startOffset": 33, "endOffset": 53}, {"referenceID": 37, "context": ", 2015) and RLSTM (Tai et al., 2015), we use the publicly available 300-dimensional GloVe word vectors (Pennington et al.", "startOffset": 18, "endOffset": 36}, {"referenceID": 30, "context": ", 2015), we use the publicly available 300-dimensional GloVe word vectors (Pennington et al., 2014).", "startOffset": 74, "endOffset": 99}, {"referenceID": 6, "context": "Model (Dai et al., 2016) |\u03b8| RMSE Mean Predicator 1 2.", "startOffset": 6, "endOffset": 24}, {"referenceID": 6, "context": "Molecular Graph Regression Dataset and Setup We further evaluate our graph NN models on the Harvard Clean Energy Project benchmark, which has been used in Dai et al. (2016); Duvenaud et al.", "startOffset": 155, "endOffset": 173}, {"referenceID": 6, "context": "Molecular Graph Regression Dataset and Setup We further evaluate our graph NN models on the Harvard Clean Energy Project benchmark, which has been used in Dai et al. (2016); Duvenaud et al. (2015) as their evaluation dataset.", "startOffset": 155, "endOffset": 197}, {"referenceID": 6, "context": "Molecular Graph Regression Dataset and Setup We further evaluate our graph NN models on the Harvard Clean Energy Project benchmark, which has been used in Dai et al. (2016); Duvenaud et al. (2015) as their evaluation dataset. This dataset contains 2.3 million candidate molecules, with each molecule labeled with its power conversion efficiency (PCE) value. We follow exactly the same train-test split as Dai et al. (2016), and the same re-sampling procedure on the training data (but not the test data) to make the algorithm put more Table 3: Experiments on Harvard Clean Energy Project.", "startOffset": 155, "endOffset": 423}, {"referenceID": 6, "context": "Molecular Graph Regression Dataset and Setup We further evaluate our graph NN models on the Harvard Clean Energy Project benchmark, which has been used in Dai et al. (2016); Duvenaud et al. (2015) as their evaluation dataset. This dataset contains 2.3 million candidate molecules, with each molecule labeled with its power conversion efficiency (PCE) value. We follow exactly the same train-test split as Dai et al. (2016), and the same re-sampling procedure on the training data (but not the test data) to make the algorithm put more Table 3: Experiments on Harvard Clean Energy Project. We report Root Mean Square Error(RMSE) on test set. The first block lists the results reported in Dai et al. (2016) for reference.", "startOffset": 155, "endOffset": 705}, {"referenceID": 6, "context": "Embedded Loopy BP (Dai et al., 2016) is a recurrent architecture, with 4 recurrent iterations.", "startOffset": 18, "endOffset": 36}], "year": 2017, "abstractText": "The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects (cf. filters in CNN) via the kernels. Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training. We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art results across these applications.", "creator": "LaTeX with hyperref package"}}}