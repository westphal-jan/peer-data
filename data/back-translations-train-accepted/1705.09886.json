{"id": "1705.09886", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2017", "title": "Convergence Analysis of Two-layer Neural Networks with ReLU Activation", "abstract": "In recent years, stochastic gradient descent (SGD) based techniques has become the standard tools for training neural networks. However, formal theoretical understanding of why SGD can train neural networks in practice is largely missing.", "histories": [["v1", "Sun, 28 May 2017 02:11:10 GMT  (1221kb,D)", "http://arxiv.org/abs/1705.09886v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuanzhi li", "yang yuan"], "accepted": true, "id": "1705.09886"}, "pdf": {"name": "1705.09886.pdf", "metadata": {"source": "CRF", "title": "Convergence Analysis of Two-layer Neural Networks with ReLU Activation", "authors": ["Yuanzhi Li", "Yang Yuan"], "emails": ["yuanzhil@cs.princeton.edu", "yangyuan@cs.cornell.edu"], "sections": [{"heading": null, "text": "In fact, it is such that we are able to maneuver ourselves into a situation, in which we see ourselves in a position, in which we see ourselves in a position, in which we are able to change the world, in which we are able to live, in which we are able to live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, we live, in which we live"}], "references": [{"title": "Learning polynomials with neural networks", "author": ["Alexandr Andoni", "Rina Panigrahy", "Gregory Valiant", "Li Zhang"], "venue": "In ICML, pages 1908\u20131916,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Provable bounds for learning some deep representations", "author": ["Sanjeev Arora", "Aditya Bhaskara", "Rong Ge", "Tengyu Ma"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Universal approximation bounds for superpositions of a sigmoidal function", "author": ["Andrew R. Barron"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1993}, {"title": "The loss surfaces of multilayer networks", "author": ["Anna Choromanska", "Mikael Henaff", "Micha\u00ebl Mathieu", "G\u00e9rard Ben Arous", "Yann LeCun"], "venue": "In AISTATS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Open problem: The landscape of the loss surfaces of multilayer networks", "author": ["Anna Choromanska", "Yann LeCun", "G\u00e9rard Ben Arous"], "venue": "In Proceedings of The 28th Conference on Learning Theory, COLT", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["George Cybenko"], "venue": "MCSS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity", "author": ["Amit Daniely", "Roy Frostig", "Yoram Singer"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Yann N Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio"], "venue": "NIPS", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John C. Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Escaping from saddle points - online stochastic gradient for tensor decomposition", "author": ["Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan"], "venue": "In COLT 2015,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In AISTATS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In AISTATS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Reliably learning the relu in polynomial time", "author": ["Surbhi Goel", "Varun Kanade", "Adam R. Klivans", "Justin Thaler"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Qualitatively characterizing neural network optimization problems", "author": ["Ian J. Goodfellow", "Oriol Vinyals"], "venue": "CoRR, abs/1412.6544,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In ICCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Kurt Hornik", "Maxwell B. Stinchcombe", "Halbert White"], "venue": "Neural Networks,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1989}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "author": ["Majid Janzamin", "Hanie Sedghi", "Anima Anandkumar"], "venue": "arXiv preprint arXiv:1506.08473,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Deep learning without poor local minima", "author": ["Kenji Kawaguchi"], "venue": "In NIPS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Efficient BackProp, pages 9\u201350", "author": ["Yann LeCun", "Leon Bottou", "Genevieve B. Orr", "Klaus Robert M\u00fcller"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "On the computational efficiency of training neural networks", "author": ["Roi Livni", "Shai Shalev-Shwartz", "Ohad Shamir"], "venue": "In NIPS,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F. Mont\u00fafar", "Razvan Pascanu", "KyungHyun Cho", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Expressiveness of rectifier networks", "author": ["Xingyuan Pan", "Vivek Srikumar"], "venue": "In ICML,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "author": ["Razvan Pascanu", "Guido Mont\u00fafar", "Yoshua Bengio"], "venue": "CoRR, abs/1312.6098,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Non-asymptotic theory of random matrices: extreme singular values", "author": ["M. Rudelson", "R. Vershynin"], "venue": "ArXiv e-prints,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Dynamics of on-line gradient descent learning for multilayer neural networks", "author": ["David Saad", "Sara A. Solla"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1996}, {"title": "On the quality of the initial basin in overspecified neural networks", "author": ["Itay Safran", "Ohad Shamir"], "venue": "In ICML,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M. Saxe", "James L. McClelland", "Surya Ganguli"], "venue": "CoRR, abs/1312.6120,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Provable methods for training neural networks with sparse connectivity", "author": ["Hanie Sedghi", "Anima Anandkumar"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Distribution-specific hardness of learning", "author": ["Ohad Shamir"], "venue": "neural networks. CoRR,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Training a single sigmoidal neuron is hard", "author": ["Jir\u00ed S\u00edma"], "venue": "Neural Computation,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2002}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George E. Dahl", "Geoffrey E. Hinton"], "venue": "In ICML,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Symmetry-breaking convergence analysis of certain two-layered neural networks with relu nonlinearity", "author": ["Yuandong Tian"], "venue": "In Submitted to ICLR", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Diversity leads to generalization in neural networks", "author": ["Bo Xie", "Yingyu Liang", "Le Song"], "venue": "In AISTATS,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2017}], "referenceMentions": [{"referenceID": 16, "context": "It is well known that neural networks have great expressive power [19, 6, 3, 7, 27].", "startOffset": 66, "endOffset": 83}, {"referenceID": 5, "context": "It is well known that neural networks have great expressive power [19, 6, 3, 7, 27].", "startOffset": 66, "endOffset": 83}, {"referenceID": 2, "context": "It is well known that neural networks have great expressive power [19, 6, 3, 7, 27].", "startOffset": 66, "endOffset": 83}, {"referenceID": 6, "context": "It is well known that neural networks have great expressive power [19, 6, 3, 7, 27].", "startOffset": 66, "endOffset": 83}, {"referenceID": 24, "context": "It is well known that neural networks have great expressive power [19, 6, 3, 7, 27].", "startOffset": 66, "endOffset": 83}, {"referenceID": 33, "context": ", SGD, Momentum [36], Adagrad [9], Adam [22]), but to the best of our knowledge, there were no theoretical guarantees that such methods will find good weights.", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": ", SGD, Momentum [36], Adagrad [9], Adam [22]), but to the best of our knowledge, there were no theoretical guarantees that such methods will find good weights.", "startOffset": 30, "endOffset": 33}, {"referenceID": 19, "context": ", SGD, Momentum [36], Adagrad [9], Adam [22]), but to the best of our knowledge, there were no theoretical guarantees that such methods will find good weights.", "startOffset": 40, "endOffset": 44}, {"referenceID": 34, "context": "For this basic network, it is known that even in the simplified setting where the weights are initialized symmetrically and the ground truth forms orthonormal basis, gradient descent might get stuck at saddle points [37].", "startOffset": 216, "endOffset": 220}, {"referenceID": 15, "context": "Inspired by the structure of residual network (ResNet) [18], we add an extra identity mapping for the hidden layer (see Figure 1).", "startOffset": 55, "endOffset": 59}, {"referenceID": 27, "context": "Following the standard setting [30, 37], we assume that there exists a two-layer teacher network with weight W\u2217.", "startOffset": 31, "endOffset": 39}, {"referenceID": 34, "context": "Following the standard setting [30, 37], we assume that there exists a two-layer teacher network with weight W\u2217.", "startOffset": 31, "endOffset": 39}, {"referenceID": 7, "context": "Another common belief is that neural network has lots of local minima and saddle points [8], so even if there exists a global minimum, we may not be able to arrive there.", "startOffset": 88, "endOffset": 91}, {"referenceID": 16, "context": "For example, two-layer network with sigmoid activations could approximate any continuous function [19, 6, 3].", "startOffset": 98, "endOffset": 108}, {"referenceID": 5, "context": "For example, two-layer network with sigmoid activations could approximate any continuous function [19, 6, 3].", "startOffset": 98, "endOffset": 108}, {"referenceID": 2, "context": "For example, two-layer network with sigmoid activations could approximate any continuous function [19, 6, 3].", "startOffset": 98, "endOffset": 108}, {"referenceID": 23, "context": "ReLU is the state-of-the-art activation function, and performs much better than sigmoid [26, 12], but fewer and weaker results on expressivity of feedforward ReLU networks are known [25, 28, 27].", "startOffset": 88, "endOffset": 96}, {"referenceID": 11, "context": "ReLU is the state-of-the-art activation function, and performs much better than sigmoid [26, 12], but fewer and weaker results on expressivity of feedforward ReLU networks are known [25, 28, 27].", "startOffset": 88, "endOffset": 96}, {"referenceID": 22, "context": "ReLU is the state-of-the-art activation function, and performs much better than sigmoid [26, 12], but fewer and weaker results on expressivity of feedforward ReLU networks are known [25, 28, 27].", "startOffset": 182, "endOffset": 194}, {"referenceID": 25, "context": "ReLU is the state-of-the-art activation function, and performs much better than sigmoid [26, 12], but fewer and weaker results on expressivity of feedforward ReLU networks are known [25, 28, 27].", "startOffset": 182, "endOffset": 194}, {"referenceID": 24, "context": "ReLU is the state-of-the-art activation function, and performs much better than sigmoid [26, 12], but fewer and weaker results on expressivity of feedforward ReLU networks are known [25, 28, 27].", "startOffset": 182, "endOffset": 194}, {"referenceID": 32, "context": "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].", "startOffset": 62, "endOffset": 74}, {"referenceID": 21, "context": "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].", "startOffset": 62, "endOffset": 74}, {"referenceID": 31, "context": "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].", "startOffset": 62, "endOffset": 74}, {"referenceID": 17, "context": "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].", "startOffset": 131, "endOffset": 147}, {"referenceID": 30, "context": "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].", "startOffset": 131, "endOffset": 147}, {"referenceID": 12, "context": "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].", "startOffset": 131, "endOffset": 147}, {"referenceID": 0, "context": "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].", "startOffset": 189, "endOffset": 195}, {"referenceID": 1, "context": "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].", "startOffset": 189, "endOffset": 195}, {"referenceID": 28, "context": "[31] proved that with high probability, there exists a continuous decreasing path from random initial point to the global minimum, but SGD may not follow this path.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "Some previous works simplified the model by ignoring the activation functions and considering deep linear networks [32, 21] or deep linear residual networks [16], which can only learn linear functions.", "startOffset": 115, "endOffset": 123}, {"referenceID": 18, "context": "Some previous works simplified the model by ignoring the activation functions and considering deep linear networks [32, 21] or deep linear residual networks [16], which can only learn linear functions.", "startOffset": 115, "endOffset": 123}, {"referenceID": 3, "context": "Some previous results are based on independent activation assumption that the activations of ReLU and the input are independent, which is unrealistic [4, 21].", "startOffset": 150, "endOffset": 157}, {"referenceID": 18, "context": "Some previous results are based on independent activation assumption that the activations of ReLU and the input are independent, which is unrealistic [4, 21].", "startOffset": 150, "endOffset": 157}, {"referenceID": 7, "context": "It is observed that saddle point is not a big problem for neural networks [8, 15].", "startOffset": 74, "endOffset": 81}, {"referenceID": 13, "context": "It is observed that saddle point is not a big problem for neural networks [8, 15].", "startOffset": 74, "endOffset": 81}, {"referenceID": 9, "context": "In general, if the objective is strict-saddle [10], SGD could escape all saddle points.", "startOffset": 46, "endOffset": 50}, {"referenceID": 34, "context": "1 (Eqn (13) from [37]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": "Randomly initializing the weights with O(1/ \u221a d) is standard in deep learning, see [23, 11, 17].", "startOffset": 83, "endOffset": 95}, {"referenceID": 10, "context": "Randomly initializing the weights with O(1/ \u221a d) is standard in deep learning, see [23, 11, 17].", "startOffset": 83, "endOffset": 95}, {"referenceID": 14, "context": "Randomly initializing the weights with O(1/ \u221a d) is standard in deep learning, see [23, 11, 17].", "startOffset": 83, "endOffset": 95}, {"referenceID": 26, "context": "It is also well known that if the entries are initialized with O(1/ \u221a d), the spectral norm of the random matrix is O(1) [29].", "startOffset": 121, "endOffset": 125}, {"referenceID": 3, "context": "The assumption that the input follows a Gaussian distribution is not necessarily true in practice (Although this is a common assumption appeared in the previous papers [4, 37, 38], and also considered plausible in [5]).", "startOffset": 168, "endOffset": 179}, {"referenceID": 34, "context": "The assumption that the input follows a Gaussian distribution is not necessarily true in practice (Although this is a common assumption appeared in the previous papers [4, 37, 38], and also considered plausible in [5]).", "startOffset": 168, "endOffset": 179}, {"referenceID": 35, "context": "The assumption that the input follows a Gaussian distribution is not necessarily true in practice (Although this is a common assumption appeared in the previous papers [4, 37, 38], and also considered plausible in [5]).", "startOffset": 168, "endOffset": 179}, {"referenceID": 4, "context": "The assumption that the input follows a Gaussian distribution is not necessarily true in practice (Although this is a common assumption appeared in the previous papers [4, 37, 38], and also considered plausible in [5]).", "startOffset": 214, "endOffset": 217}, {"referenceID": 29, "context": "Moreover, previous analyses either ignore the nonlinear activations and thus consider linear model [32, 21, 16], or directly [4, 21] or indirectly [37]1 assume that the activations are independent.", "startOffset": 99, "endOffset": 111}, {"referenceID": 18, "context": "Moreover, previous analyses either ignore the nonlinear activations and thus consider linear model [32, 21, 16], or directly [4, 21] or indirectly [37]1 assume that the activations are independent.", "startOffset": 99, "endOffset": 111}, {"referenceID": 3, "context": "Moreover, previous analyses either ignore the nonlinear activations and thus consider linear model [32, 21, 16], or directly [4, 21] or indirectly [37]1 assume that the activations are independent.", "startOffset": 125, "endOffset": 132}, {"referenceID": 18, "context": "Moreover, previous analyses either ignore the nonlinear activations and thus consider linear model [32, 21, 16], or directly [4, 21] or indirectly [37]1 assume that the activations are independent.", "startOffset": 125, "endOffset": 132}, {"referenceID": 34, "context": "Moreover, previous analyses either ignore the nonlinear activations and thus consider linear model [32, 21, 16], or directly [4, 21] or indirectly [37]1 assume that the activations are independent.", "startOffset": 147, "endOffset": 151}, {"referenceID": 4, "context": "As pointed out by [5], eliminating the unrealistic assumptions on activation independence is the central problem of analyzing the loss surface of neural network, which was not fully addressed by the previous analyses.", "startOffset": 18, "endOffset": 21}, {"referenceID": 15, "context": "1 Importance of identity mapping In this experiment, we compare the standard ResNet [18] and single skip model where identity mapping skips only one layer.", "startOffset": 84, "endOffset": 88}, {"referenceID": 0, "context": "References [1] Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Andrew R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Anna Choromanska, Mikael Henaff, Micha\u00ebl Mathieu, G\u00e9rard Ben Arous, and Yann LeCun.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Anna Choromanska, Yann LeCun, and G\u00e9rard Ben Arous.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] George Cybenko.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Amit Daniely, Roy Frostig, and Yoram Singer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] John C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Xavier Glorot and Yoshua Bengio.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Xavier Glorot, Antoine Bordes, and Yoshua Bengio.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Surbhi Goel, Varun Kanade, Adam R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] Ian J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] Kurt Hornik, Maxwell B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] Kenji Kawaguchi.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] Diederik P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] Yann LeCun, Leon Bottou, Genevieve B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24] Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25] Guido F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[26] Vinod Nair and Geoffrey E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[27] Xingyuan Pan and Vivek Srikumar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[28] Razvan Pascanu, Guido Mont\u00fafar, and Yoshua Bengio.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[29] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[30] David Saad and Sara A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31] Itay Safran and Ohad Shamir.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[32] Andrew M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[33] Hanie Sedghi and Anima Anandkumar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[34] Ohad Shamir.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[35] Jir\u00ed S\u00edma.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[36] Ilya Sutskever, James Martens, George E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[37] Yuandong Tian.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[38] Bo Xie, Yingyu Liang, and Le Song.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "In recent years, stochastic gradient descent (SGD) based techniques has become the standard tools for training neural networks. However, formal theoretical understanding of why SGD can train neural networks in practice is largely missing. In this paper, we make progress on understanding this mystery by providing a convergence analysis for SGD on a rich subset of two-layer feedforward networks with ReLU activations. This subset is characterized by a special structure called \u201cidentity mapping\u201d. We prove that, if input follows from Gaussian distribution, with standard O(1/ \u221a d) initialization of the weights, SGD converges to the global minimum in polynomial number of steps. Unlike normal vanilla networks, the \u201cidentity mapping\u201d makes our network asymmetric and thus the global minimum is unique. To complement our theory, we are also able to show experimentally that multi-layer networks with this mapping have better performance compared with normal vanilla networks. Our convergence theorem differs from traditional non-convex optimization techniques. We show that SGD converges to optimal in \u201ctwo phases\u201d: In phase I, the gradient points to the wrong direction, however, a potential function g gradually decreases. Then in phase II, SGD enters a nice one point convex region and converges. We also show that the identity mapping is necessary for convergence, as it moves the initial point to a better place for optimization. Experiment verifies our claims.", "creator": "LaTeX with hyperref package"}}}