{"id": "1705.10470", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Iterative Machine Teaching", "abstract": "In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner. We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers. We also validate our theoretical findings with extensive experiments on different data distribution and real image datasets.", "histories": [["v1", "Tue, 30 May 2017 06:35:29 GMT  (4740kb,D)", "http://arxiv.org/abs/1705.10470v1", "To appear in ICML 2017"], ["v2", "Tue, 13 Jun 2017 05:42:32 GMT  (9152kb,D)", "http://arxiv.org/abs/1705.10470v2", "ICML 2017 camera ready"]], "COMMENTS": "To appear in ICML 2017", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["weiyang liu", "bo dai", "ahmad humayun", "charlene tay", "chen yu", "linda b smith", "james m rehg", "le song"], "accepted": true, "id": "1705.10470"}, "pdf": {"name": "1705.10470.pdf", "metadata": {"source": "META", "title": "Iterative Machine Teaching", "authors": ["Weiyang Liu", "Bo Dai", "James M. Rehg", "Le Song"], "emails": ["<wyliu@gatech.edu,", "lsong@cc.gatech.edu>."], "sections": [{"heading": "1. Introduction", "text": "This year it is so far that it will only take a few weeks to reach an agreement."}, {"heading": "2. Related Work", "text": "Machine teaching problem is to find an optimal training set given a student model and a target. (Zhu, 2015) proposes a general teaching framework. (Zhu et al., 2016) considers Bayesian learner in an exponential family and expresses machine learning as an optimization problem compared to teaching examples that compensate for the future loss of the learner and the effort of the teacher. (Liu et al., 2015) The framework has been applied to security (Mei & Zhu, 2015), human computer interaction (Meek et al., 2016) and education (Khan et al., 2011). (Johns et al.) extends machine learning to interactive settings. However, this work ignores the fact that a student model is typically learned through an iterative algorithm, and we generally care more about how fast the student can learn from the teacher. (Cakmak & Thomaz, 2014)"}, {"heading": "3. Iterative Machine Teaching", "text": "The proposed iterative machine instruction is a general concept, and the work takes into account the following settings: > > student asset (> student asset). Generally, a student's assets include the initial parameter w0, loss function, optimization algorithm, representation (attribute), model, learning rate \u03b7t over time (and initial \u03b70) and the traceability of the parameter wt. Ideally, a teacher has access to all of them and can track the parameters and learning rate, while the worst case is that a teacher knows nothing about them. How practicable the instruction is depends on the prior knowledge and traceability of a teacher. The teacher presents an example as (x, y), while the student presents the same example as (x, y) (typically y = y). The representation x, x, x and x, can be the teacher, x and the teacher are x and the teacher."}, {"heading": "4. Teaching by an Omniscient Teacher", "text": "An omniscient teacher has access to the student's attribute space, model, loss function, and optimization algorithm. In particular, omniscient teacher (x, y) and student (x, y) share the same representation space, and the teacher's optimal model is also the same as the student's optimal model."}, {"heading": "4.1. Intuition and teaching algorithm", "text": "In fact, most of us are able to keep ourselves to the limits of our capabilities. & # 8222; This is an example of the ability to identify with others. & # 8220;. & # 8222; It is an example of & # 252; the ability to identify with others. & # 8220; & # 8220; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10;"}, {"heading": "4.2. Teaching monotonicity and universal speedup", "text": "In this section we identify general conditions under which we can guarantee that the iterative teaching algorithm always performs better than the random teacher. Definition 1 (teaching volume) For a specific loss function \"we first define a teaching volume function TV (w) with the model parameter w asTV (w) = max x x X, y Y {\u2212 throu2t T1 (x, y | w) + 2throutT2 (x, y | w)} (5) Theorem 2 (teaching monotony) Given a learning condition X and a loss function\" if the inequality w1 \u2212 w \u00b2 2 \u2212 TV (w1) \u2264 w2 \u2212 w \u00b2 TV (x, y | w) \u2212 TV (w2) \u2212 TV (6) for any learning condition w1, w2 \u2212 w2 \u2212 w \u00b2 it."}, {"heading": "4.3. Teaching capability and exponential speedup", "text": "The theory in the previous section assures that under certain conditions the all-knowing teacher can always lead to a faster convergence for the students. < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p \"p > p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \">\" p \"p\" p \">\" p \"p\" p \"p\" p \"p\" p \""}, {"heading": "5. Teaching by a less informative teacher", "text": "In order to make the teacher model useful in practice, we are designing two less informative teacher models that require less and less information from the student."}, {"heading": "5.1. The surrogate teacher", "text": "Suppose we can only query the functional output of what we learned < wt, x >, but we cannot directly access wt. How can we select the example? In this case, we suggest using the convexity of the loss function. (7) Taking pool-based teaching as an example, we can instead optimize the following substitute function: (xt, yt): (xt, yt) = argmin {x, y). (< w, x, y). (7) Taking pool-based teaching as an example, we can instead select the following substitute function: (xt, yt)."}, {"heading": "5.2. The imitation teacher", "text": "To this end, we present a teacher who learns to imitate the inner product performance < wt, x > of the student model while selecting examples in the teacher's own feature space; the teacher may be able to use active learning to imitate the student's inner product performance < wt, x >. In this imitation, the student model remains unchanged and the teacher model could be updated by multiple queries to the student (enter an example and see the student's inner product performance); we present a simpler and more direct teacher imitator (Alg. 2) that works in a similar way to stochastic mirror descent (Nemirovski et al., 2009; Hall & Willett, 2013); in concrete terms, the teacher first learns to imitate the teacher < wt, x > with the following iterative learning effect: vt + 1 = imitate = imitate (Hall & Willett, 2013)."}, {"heading": "6. Discussion", "text": "Optimization of the teacher model. For arbitrary loss functions, the optimal teacher model for a student model should find the learning sample sequence to achieve the fastest possible convergence. An exhaustive search for such an example sequence is mathematically impossible. For example, there are nT possible learning sequences (T is the iteration number) for n-size pool-based instruction. As a result, we must use the properties of the loss function to design the teacher model. The proposed teacher models are not necessarily optimal, but they are good enough under certain conditions for loss function, student model and training data. Theoretical aspects of the teacher model. Theoretical investigation of the teacher model includes determining the conditions for loss function and training data so that the teacher model is optimal, or demonstrably achieves a faster convergence rate, or converges faster than the random teacher. We would like these conditions to be sufficient and necessary, but sometimes the conditions of the teacher model in practice are sufficient."}, {"heading": "7. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1. Experimental details", "text": "Performance Measurement. We use three measurements to evaluate the convergence performance: objective value compared to the training set, difference between wt and w \u043a (\u0435wt \u2212 w \u0445 2) and classification accuracy in the test set. Parameters and setup. We give detailed test setups in Appendix B. In the first section we compare different teaching strategies, while in the remaining sections we evaluate practical teaching on a pool basis. To be fair, the learning rates are the same for all the compared methods."}, {"heading": "7.2. Comparison of different teaching strategies", "text": "First, we compare four different teaching strategies for the omniscient teacher. We consider two scenarios: first, that the dimension of the attribute space is smaller than the number of samples (the given characteristics are sufficient to represent the whole attribute), and second, that the attribute dimension is larger than the number of samples (the given characteristics are not sufficient to represent the whole attribute). In both of these scenarios, we find that synthesis-based teaching usually works best and always achieves exponential convergence. Combination-based teaching is exactly the same as synthesis-based teaching in the first scenario, but it is much worse than synthesis in the second scenario. Rescale pool-based teaching is also better than pool-based teaching. Empirically, the experiment confirms our theoretical results: the more flexible the teaching strategy is, the more convergence gain we can achieve."}, {"heading": "7.3. Teaching linear models on Gaussian data", "text": "This experiment examines the convergence of three typical linear models: First Regression (RR), Logistic Regression (LR) and Support Vector Machine (SVM) on Gaussian data. Note that SGD is executed on selected sets based on the unification of all samples selected by the omniscient teacher. For the scenario of different feature spaces, we use a random orthogonal projection matrix to generate the teacher's feature space based on the example provided by the teacher. All teachers use pool-based teaching strategies. For fair comparisons, we use the same random initialization and learning rate. Classes in the same feature space. The results in Fig. 3 show that the learner can converge much faster using the teacher's example by demonstrating the effectiveness of our teaching models. As expected, the omniscient teacher consistently achieves faster convergence than the substitute teacher by imitating the model."}, {"heading": "7.4. Teaching linear models on uniform spherical data", "text": "In this experiment, we use a different data distribution to further evaluate the teacher models. We will investigate LR and SVM by classifying uniform spherical data. Classes in the same attribute space. Fig. 5 shows that convergence is consistently improved, while we use omniscient teachers to provide examples to learners. We find that the importance of improvement is related to the distribution and loss function of the training data, as our theoretical results show. The substitute teacher generates less convergence gain in SVM, as the convergence bottom becomes very loose in this case. Overall, the omniscient teacher still shows strong teaching ability. What is more interesting is that we perform simple SGD runs on the sample selected by the omniscient teacher, and also achieve faster convergence, which shows that the chosen example rate in terms of convergence is better than the entire set. Classes in different attribute spaces. While teachers and students use different attribute spaces, one can see from Fig. 5 that the overall teacher performs poorly even in terms of the replacement teacher."}, {"heading": "7.5. Teaching Linear Classifiers on MNIST Dataset", "text": "We use random 24D characteristics to classify the numbers (0 / 1, 3 / 5 as examples). We generate the characteristics of the teacher using a random projection matrix from the characteristics of the original 24D student. Note that omniscient teacher and substitute teacher (the same room) assume that the teacher uses the characteristic space of the student, while substitute teacher (other room) and copycat teacher assume that the teacher uses his own space. Figure 6 shows that all of these teacher models generate significant convergence accelerations. We can see that the omniscient teacher converges the fastest as expected. Interestingly, our copycat teacher achieves a very similar convergence speed on the omniscient teacher under the condition that the teacher does not know the characteristic space of the student. In Figure 7, we also show some examples of the selected digit images of the teacher (0, 1, 9, and the teacher tends to select examples as examples)."}, {"heading": "7.6. Teaching Fully Connected Layers in Convolutional Neural Networks on CIFAR-10 Dataset", "text": "We expand our teacher models from binary classification to multi-class classification. We use the teacher model to teach the last fully connected layers in the convective neural network on the CIFAR-10 dataset. We first train three basic CNNs (6 / 9 / 12 convolution layers, detailed network configuration is given in the appendix) on CIFAR-10 without data augmentation and get the 83.5%, 86.1%, 87.2% accuracy. First, we applied the omniscient teacher and substitute teacher on the CNN-6 student with the optimal fully connected layer from the shared backprop training. This is essentially to teach the fully connected layer in the same feature room. Second, we applied the substitute teacher and the copycat teacher to the CNN-6 student using the parameters of optimal fully connected layers from CNN-9 and CNN-12."}, {"heading": "8. Concluding Remarks", "text": "This paper expands machine learning with a novel iterative framework. First, we work out the settings of iterative machine learning, then we examine two important properties of iterative machine learning: teaching monotony and teaching ability. Based on this framework, we propose three teacher models for graduate students and provide theoretical analyses for these models to demonstrate rapid convergence. Extensive experiments with both synthetic data and real image data confirm our theoretical insights and confirm the effectiveness of these teacher models."}, {"heading": "Acknowledgement", "text": "This project was partially supported by NSF IIS-1218749, NIH BIGDATA 1R01GM108341, NSF CAREER IIS1350983, NSF IIS-1639792 EAGER, ONR N00014-15-12340, Nvidia and Intel."}, {"heading": "A. Details of the Proof", "text": "The results of the study: wtg and wtg. \u2212 wp \u2212 kkg: wtg and wtg: wtg and wtg: wtg and wtg: wtg and wtg: wtg and wtg: wtg and wtg: wtg and wtg: wtg and wtg: wtg and wtg: wtg and wtg: wtg and wtg: wtg and wtg: wtg and wtg: wtg and wtg: wtg and wtg: wtg and wtg: wtg and tg: wtg and wtg: wtg and wg: wg tg: wg and tg: wg: wg and tg: wg and tg: wr wg and tg: wg and wg: wg and tg: wr wg and tg: wg and tg: wr and tg and wg: wr"}, {"heading": "B. Detailed Experimental Setting", "text": "We have used three linear models for the experiments.Specifically, the formulation of the ridge regression (RR), while the formulation of the ridge regression (RR) is ismin w Rd, b R1n n \u2211 i = 1 log (wTxi + b \u2212 yi), 2 + \u03bb 2, w 2The formulation of the logistic regression (SVM) ismin w Rd, b R1n n n, n i = 1 log (wTxi + b), 0) + \u03bb 2, w 2The formulation of the support vector machine (SVM) ismin w Rd R1n, n i = 1 max (wTxi + b), 0 w 2Comparison of different teaching strategies We use a linear regression model (ridge regression with \u03bb = 0) for this experiment. We set R as 1 and uniformly generate 30 data points as our knowledge pool for the teacher."}], "references": [{"title": "Data poisoning attacks against autoregressive models", "author": ["Alfeld", "Scott", "Zhu", "Xiaojin", "Barford", "Paul"], "venue": "In AAAI,", "citeRegEx": "Alfeld et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Alfeld et al\\.", "year": 2016}, {"title": "Explicit defense actions against test-set attacks", "author": ["Alfeld", "Scott", "Zhu", "Xiaojin", "Barford", "Paul"], "venue": "In AAAI,", "citeRegEx": "Alfeld et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Alfeld et al\\.", "year": 2017}, {"title": "Do deep nets really need to be deep? In Advances in neural information processing", "author": ["Ba", "Jimmy", "Caruana", "Rich"], "venue": null, "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "The true sample complexity of active learning", "author": ["Balcan", "Maria-Florina", "Hanneke", "Steve", "Vaughan", "Jennifer Wortman"], "venue": "Machine learning,", "citeRegEx": "Balcan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2010}, {"title": "Curriculum learning", "author": ["Bengio", "Yoshua", "Louradour", "J\u00e9r\u00f4me", "Collobert", "Ronan", "Weston", "Jason"], "venue": "In ICML,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Eliciting good teaching from humans for machine learners", "author": ["Cakmak", "Maya", "Thomaz", "Andrea L"], "venue": "Artificial Intelligence,", "citeRegEx": "Cakmak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cakmak et al\\.", "year": 2014}, {"title": "Recursive teaching dimension, vcdimension and sample compression", "author": ["Doliwa", "Thorsten", "Fan", "Gaojian", "Simon", "Hans Ulrich", "Zilles", "Sandra"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Doliwa et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Doliwa et al\\.", "year": 2014}, {"title": "On the complexity of teaching", "author": ["Goldman", "Sally A", "Kearns", "Michael J"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Goldman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Goldman et al\\.", "year": 1995}, {"title": "Online optimization in dynamic environments", "author": ["Hall", "Eric C", "Willett", "Rebecca M"], "venue": "arXiv preprint arXiv:1307.5944,", "citeRegEx": "Hall et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2013}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["Hinton", "Geoffrey", "Vinyals", "Oriol", "Dean", "Jeff"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Becoming the expert - interactive multi-class machine teaching", "author": ["Johns", "Edward", "Mac Aodha", "Oisin", "Brostow", "Gabriel J"], "venue": "In CVPR,", "citeRegEx": "Johns et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Johns et al\\.", "year": 2015}, {"title": "How do humans teach: On curriculum learning and teaching dimension", "author": ["Khan", "Faisal", "Mutlu", "Bilge", "Zhu", "Xiaojin"], "venue": "In NIPS,", "citeRegEx": "Khan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Khan et al\\.", "year": 2011}, {"title": "The teaching dimension of linear learners", "author": ["Liu", "Ji", "Zhu", "Xiaojin", "Ohannessian", "H Gorune"], "venue": "In ICML,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Analysis of a design pattern for teaching with features and labels", "author": ["Meek", "Christopher", "Simard", "Patrice", "Zhu", "Xiaojin"], "venue": "arXiv preprint arXiv:1611.05950,", "citeRegEx": "Meek et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Meek et al\\.", "year": 2016}, {"title": "Using machine teaching to identify optimal training-set attacks on machine learners", "author": ["Mei", "Shike", "Zhu", "Xiaojin"], "venue": "In AAAI,", "citeRegEx": "Mei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mei et al\\.", "year": 2015}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["Nemirovski", "Arkadi", "Juditsky", "Anatoli", "Lan", "Guanghui", "Shapiro", "Alexander"], "venue": "SIAM Journal on optimization,", "citeRegEx": "Nemirovski et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "A survey on transfer learning", "author": ["Pan", "Sinno Jialin", "Yang", "Qiang"], "venue": "IEEE Transactions on knowledge and data engineering,", "citeRegEx": "Pan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2010}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Romero", "Adriana", "Ballas", "Nicolas", "Kahou", "Samira Ebrahimi", "Chassang", "Antoine", "Gatta", "Carlo", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.6550,", "citeRegEx": "Romero et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2014}, {"title": "Teachability in computational learning", "author": ["Shinohara", "Ayumi", "Miyano", "Satoru"], "venue": "New Generation Computing,", "citeRegEx": "Shinohara et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Shinohara et al\\.", "year": 1991}, {"title": "Near-optimally teaching the crowd to classify", "author": ["Singla", "Adish", "Bogunovic", "Ilija", "Bartok", "Gabor", "Karbasi", "Amin", "Krause", "Andreas"], "venue": "In ICML, pp", "citeRegEx": "Singla et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Singla et al\\.", "year": 2014}, {"title": "Machine teaching for bayesian learners in the exponential family", "author": ["Zhu", "Xiaojin"], "venue": "In NIPS,", "citeRegEx": "Zhu and Xiaojin.,? \\Q2013\\E", "shortCiteRegEx": "Zhu and Xiaojin.", "year": 2013}, {"title": "Machine teaching: An inverse problem to machine learning and an approach toward optimal education", "author": ["Zhu", "Xiaojin"], "venue": "In AAAI,", "citeRegEx": "Zhu and Xiaojin.,? \\Q2015\\E", "shortCiteRegEx": "Zhu and Xiaojin.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Recently, there is a surge of interests in machine teaching which has found diverse applications in model compression (Bucila et al., 2006; Han et al., 2015; Ba & Caruana, 2014; Romero et al., 2014), transfer learning (Pan & Yang, 2010) and cyber-security problems (Alfeld et al.", "startOffset": 118, "endOffset": 198}, {"referenceID": 18, "context": "Recently, there is a surge of interests in machine teaching which has found diverse applications in model compression (Bucila et al., 2006; Han et al., 2015; Ba & Caruana, 2014; Romero et al., 2014), transfer learning (Pan & Yang, 2010) and cyber-security problems (Alfeld et al.", "startOffset": 118, "endOffset": 198}, {"referenceID": 0, "context": ", 2014), transfer learning (Pan & Yang, 2010) and cyber-security problems (Alfeld et al., 2016; 2017; Mei & Zhu, 2015).", "startOffset": 74, "endOffset": 118}, {"referenceID": 4, "context": "Furthermore, machine teaching is also closely related to other subjects of interests, such as curriculum learning (Bengio et al., 2009) and knowledge distilation (Hinton et al.", "startOffset": 114, "endOffset": 135}, {"referenceID": 10, "context": ", 2009) and knowledge distilation (Hinton et al., 2015).", "startOffset": 34, "endOffset": 55}, {"referenceID": 6, "context": "There are also many seminal theory work on analyzing the teaching dimension of different models (Shinohara & Miyano, 1991; Goldman & Kearns, 1995; Doliwa et al., 2014; Liu et al., 2016).", "startOffset": 96, "endOffset": 185}, {"referenceID": 13, "context": "There are also many seminal theory work on analyzing the teaching dimension of different models (Shinohara & Miyano, 1991; Goldman & Kearns, 1995; Doliwa et al., 2014; Liu et al., 2016).", "startOffset": 96, "endOffset": 185}, {"referenceID": 13, "context": "(Liu et al., 2016) provides the teaching dimension of several linear learners.", "startOffset": 0, "endOffset": 18}, {"referenceID": 14, "context": "The framework has been applied to security (Mei & Zhu, 2015), human computer interaction (Meek et al., 2016) and education (Khan et al.", "startOffset": 89, "endOffset": 108}, {"referenceID": 12, "context": ", 2016) and education (Khan et al., 2011).", "startOffset": 22, "endOffset": 41}, {"referenceID": 11, "context": "(Johns et al., 2015) further extends machine teaching to interactive settings.", "startOffset": 0, "endOffset": 20}, {"referenceID": 20, "context": "(Singla et al., 2014) consider the crowdsourcing problem and propose a sequential teaching algorithm that can teach crowd worker to better classify the query.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Therefore they have different sample complexity (Balcan et al., 2010; Zhu, 2013).", "startOffset": 48, "endOffset": 80}, {"referenceID": 4, "context": "Curriculum learning (Bengio et al., 2009) is a general training strategy that encourages to input training examples from easy ones to difficult ones.", "startOffset": 20, "endOffset": 41}, {"referenceID": 16, "context": "2) which works in a way similar to stochastic mirror descent (Nemirovski et al., 2009; Hall & Willett, 2013).", "startOffset": 61, "endOffset": 108}], "year": 2017, "abstractText": "In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner. We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers. We also validate our theoretical findings with extensive experiments on different data distribution and real image datasets.", "creator": "LaTeX with hyperref package"}}}