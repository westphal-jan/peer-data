{"id": "1705.10479", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets", "abstract": "Imitation learning has traditionally been applied to learn a single task from demonstrations thereof. The requirement of structured and isolated demonstrations limits the scalability of imitation learning approaches as they are difficult to apply to real-world scenarios, where robots have to be able to execute a multitude of tasks. In this paper, we propose a multi-modal imitation learning framework that is able to segment and imitate skills from unlabelled and unstructured demonstrations by learning skill segmentation and imitation learning jointly. The extensive simulation results indicate that our method can efficiently separate the demonstrations into individual skills and learn to imitate them using a single multi-modal policy. The video of our experiments is available at", "histories": [["v1", "Tue, 30 May 2017 07:15:11 GMT  (2769kb,D)", "http://arxiv.org/abs/1705.10479v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.LG", "authors": ["karol hausman", "yevgen chebotar", "stefan schaal", "gaurav sukhatme", "joseph lim"], "accepted": true, "id": "1705.10479"}, "pdf": {"name": "1705.10479.pdf", "metadata": {"source": "CRF", "title": "Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets", "authors": ["Karol Hausman", "Yevgen Chebotar", "Stefan Schaal", "Gaurav Sukhatme", "Joseph J. Lim"], "emails": ["limjj}@usc.edu"], "sections": [{"heading": "1 Introduction", "text": "One of the key factors for the use of robots in unstructured real-world environments is their ability to learn from data. In recent years, there have been several examples of robotic learning frameworks that deliver promising results, including: Learn reinforcement [29] - where a robot learns a skill based on its interaction with the environment and imitation. [27] The demonstration is usually conducted in the form of kinesthetic teachings that the user needs to provide the right training data. This limited skill imitation setup focuses on isolated demonstrations of a particular skill."}, {"heading": "2 Related Work", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "3 Preliminaries", "text": "Letter M = (S, A, P, R, p0, \u03b3, T) is a finite horizon Markov decision-making process (MDP) where S and A are state and action spaces, P: S \u00b7 A \u00b7 S \u2192 R + is a state transition probability function or system dynamics, R: S \u00b7 A \u2192 R is a reward function, p0: S \u2192 R + is an initial state distribution, \u03b3 is a reward discount factor, and T is a horizon. Let Mass = (s0, a0,., sT) be an orbit of states and actions, and R (\u03c4) = 0 \u03b3tR (st, at) the railway reward. The goal of reinforcing learning methods is to find parameters of a policy (a | s) that maximizes the expected discounted reward."}, {"heading": "4 Multi-modal Imitation Learning", "text": "The traditional imitation learning scenario described in Sec. 3 considers a problem of learning to mimic a skill from demonstrations. The demonstrations represent examples from a single expert policy \u03c0E1. In this work, we focus on a imitation learning situation in which we learn from unstructured and unlabeled demonstrations of different tasks (in this case, the demonstrations come from a series of expert policies \u03c0E2,. \u03c0Ek, where k may be unknown, which optimize different reward functions / tasks. We refer to this set of unstructured expert policies as a mixture of policies \u03c0E. We aim to segment the demonstrations of these policies into separate tasks and learn a multimodal policy that will be able to mimic all segmented tasks. To be able to learn multimodal policies, we expand the policy with a latent intention, which is distributed by a categorical or uniform distribution."}, {"heading": "4.1 Relation to InfoGAN", "text": "In this section, we present an alternative derivation of the optimization goal in Eq. (8) by extending the InfoGAN approach (11) presented in [7]. Following [7], we introduce the latent variable c as a means to capture the semantic characteristics of data distribution. In this case, however, the latent variables (intention) (approach) (approach) (approach) (approach) (approach) (approach) (approach) (approach) (approach) (approach) (approach) (approach) (approach) (approach) (approach) (approach) (approach) (approach) (approach)) (approach)), in order to prevent collapse into a single mode, the policy optimization goal c c c, approach) is extended by the mutual information I (c; G (approach), c)."}, {"heading": "5 Implementation", "text": "The first indicator that the training has become unstable is a high classification accuracy of the discriminator. In this case, it is difficult for the generator to develop a meaningful strategy because the discriminator's reward signal is flat and the generator's TRPO gradient disappears. In an extreme case, the discriminator assigns all generator samples to the same class, and it is impossible for TRPO to provide a meaningful gradient because all generator samples receive the same reward. Previous work suggests several ways to avoid this behavior, including using the Waterstone distance metric to improve convergence behavior [3] and adding instance noise to the discriminator's instances to avoid generative distributions. We find that adding the Gaussian noise helped us most to control the discriminator's performance and generate a smooth reward signal for generator policy."}, {"heading": "6 Experiments", "text": "Our experiments aim to answer the following questions: (1) Can we divide unstructured and unmarked demonstrations into skills and learn a multimodal policy that mimics them? (2) What impact do the costs of intention and prediction introduced have on the resulting strategies? (3) Can we autonomously discover and even achieve the number of skills presented in the demonstrations in different ways? (4) Does the presented method scale to high-dimensional strategies? (5) Can we use the proposed method to learn hierarchical strategies? We evaluate our method using a series of challenging simulated robotics tasks described below."}, {"heading": "6.1 Task setup", "text": "The actuator is a 2-DoF arm located in the center of the scene. There are several targets that are placed at random positions within the environment. The goal of the task is to discover the dependence of target selection on intention in the face of a data set that achieves movements to random targets and to learn a policy that is able to achieve various goals based on the stated intention. We evaluate the performance of our framework on environments with 1, 2 and 4 objectives. Walker-2D The Walker-2D (fig. 1 left) is a 6-DoF two-legged robot consisting of two legs and feet that are attached to a common base. The goal of this task is to learn a policy that can switch between three different behaviors depending on the discovered goals: running forward, running backwards and jumping. We use TRPO to train individual expert strategies and to create a combined data set that is used to learn a multimodal policy."}, {"heading": "6.2 Multi-Target Imitation Learning", "text": "In this case, the influence of the lateral aversion to the reactionary tendencies in the reactionary movement in the reactionary movement in the reactionary movement in the reactionary movement in the reactionary movement in the reactionary movement in the reactionary movement in the reactionary movement in the reactionary movement in the reactionary movement in the reactionary movement in the reactionary movement in the reactionary movement in the reactionary movement in the reactionary movement in the reactionary reactionary movement in the reactionary reactionary movement in the reactionary reactionary movement in the reactionary reactionary movement in the reactionary reactionary reactionary movement in the reactionary reactionary reactionary movement in the reactionary reactionary reactionary movement in the reactionary reactionary reactionary movement in the reactionary reactionary reactionary reactionary movement in the reactionary reactionary reactionary reactionary movement in the reactionary reactionary reactionary movement in the reactionary reactionary reactionary movement in the reactionary reactionary reactionary movement in the reactionary reactionary reactionary movement in the reactionary reactionary reactionary reactionary movement in the reactionary reactionary reactionary reactionary movement in the reactionary reactionary reactionary reactionary movement in the reactionary reactionary reactionary reactionary movement in the reactionary reactionary reactionary movement in the reactionary reactionary reactionary reactionary movement in the reactionary reactionary reactionary reactionary movement in the reactionary reactionary reactionary movement in the reactionary reactionary reactionary reactionary movement in the reactionary reactionary reactionary reactionary movement in the reactionary reactionary reactionary reactionary movement in the reactionary reactionary reactionary movement in the reactionary reactionary reactionary movement in the reactionary reactionary reactionary reactionary movement in the reactionary reactionary reactionary movement in the reactionary reactionary reactionary movement in the reactionary reactionary movement in the reactionary reac"}, {"heading": "6.3 Multi-Task Imitation Learning", "text": "We try to further understand whether our model extends to segmentation and imitation of strategies that perform different tasks. In particular, we evaluate whether our framework is capable of learning a multimodal policy on the Walker 2D task. We incorporate three different strategies - backwards, forwards and backwards - into an expert policy and try to recover them all through our methodology. Results are presented in Fig. 5 (left) The additional latent intentions lead to a policy that is capable of segmenting and imitating all three behaviors and achieving a performance similar to that of expert policy."}, {"heading": "7 Conclusions", "text": "We present a novel learning method for imitation that learns a multimodal stochastic policy that is able to imitate a number of automatically segmented tasks using a series of unstructured and unmarked demonstrations. The presented approach learns the concept of intention and is able to perform various tasks based on the political intentions entered. We evaluated our method using a series of simulation scenarios in which we show that it is able to segment the demonstrations into different tasks and learn a multimodal policy that imitates all segmented abilities. We also compared our method with a basic approach that performs imitation learning without explicitly separating the tasks. In future work, we plan to focus on autonomously discovering the number of tasks within the given demonstration pool and evaluating this method on real robots."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Pieter Abbeel", "Andrew Y. Ng"], "venue": "In Proc. ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "A survey of robot learning from demonstration", "author": ["Brenna D Argall", "Sonia Chernova", "Manuela Veloso", "Brett Browning"], "venue": "Robotics and autonomous systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Apprenticeship learning about multiple intentions", "author": ["Monica Babes", "Vukosi Marivate", "Kaushik Subramanian", "Michael L Littman"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Robot programming by demonstration", "author": ["Aude Billard", "Sylvain Calinon", "Ruediger Dillmann", "Stefan Schaal"], "venue": "In Springer handbook of robotics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Path integral guided policy search", "author": ["Yevgen Chebotar", "Mrinal Kalakrishnan", "Ali Yahya", "Adrian Li", "Stefan Schaal", "Sergey Levine"], "venue": "arXiv preprint arXiv:1610.00529,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets, 2016", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Emily L Denton", "Soumith Chintala", "Rob Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Bayesian multitask inverse reinforcement learning", "author": ["Christos Dimitrakakis", "Constantin A Rothkopf"], "venue": "In European Workshop on Reinforcement Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "One-shot imitation learning", "author": ["Yan Duan", "Marcin Andrychowicz", "Bradly Stadie", "Jonathan Ho", "Jonas Schneider", "Ilya Sutskever", "Pieter Abbeel", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1703.07326,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2017}, {"title": "A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models", "author": ["Chelsea Finn", "Paul Christiano", "Pieter Abbeel", "Sergey Levine"], "venue": "arXiv preprint arXiv:1611.03852,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Guided cost learning: Deep inverse optimal control via policy optimization", "author": ["Chelsea Finn", "Sergey Levine", "Pieter Abbeel"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Stochastic neural networks for hierarchical reinforcement learning", "author": ["Carlos Florensa", "Yan Duan", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1704.03012,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "Multi-level discovery of deep options", "author": ["Roy Fox", "Sanjay Krishnan", "Ion Stoica", "Ken Goldberg"], "venue": "arXiv preprint arXiv:1703.08294,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Generative adversarial nets", "author": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Generative adversarial imitation learning", "author": ["Jonathan Ho", "Stefano Ermon"], "venue": "CoRR, abs/1606.03476,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Learning force control policies for compliant manipulation", "author": ["Mrinal Kalakrishnan", "Ludovic Righetti", "Peter Pastor", "Stefan Schaal"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Towards learning hierarchical skills for multi-phase manipulation tasks", "author": ["Oliver Kroemer", "Christian Daniel", "Gerhard Neumann", "Herke Van Hoof", "Jan Peters"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Nonlinear inverse reinforcement learning with gaussian processes", "author": ["Sergey Levine", "Zoran Popovic", "Vladlen Koltun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["Michael Mathieu", "Camille Couprie", "Yann LeCun"], "venue": "arXiv preprint arXiv:1511.05440,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Learning to select and generalize striking movements in robot table tennis", "author": ["Katharina M\u00fclling", "Jens Kober", "Oliver Kroemer", "Jan Peters"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Algorithms for inverse reinforcement learning", "author": ["Andrew Y Ng", "Stuart J Russell"], "venue": "In Icml,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2000}, {"title": "Incremental semantically grounded learning from demonstration", "author": ["Scott Niekum", "Sachin Chitta", "Andrew G Barto", "Bhaskara Marthi", "Sarah Osentoski"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Connecting generative adversarial networks and actor-critic methods", "author": ["David Pfau", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1610.01945,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Efficient training of artificial neural networks for autonomous navigation", "author": ["Dean A Pomerleau"], "venue": "Neural Computation,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1991}, {"title": "Efficient reductions for imitation learning", "author": ["St\u00e9phane Ross", "Drew Bagnell"], "venue": "In AISTATS,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Is imitation learning the route to humanoid robots", "author": ["Stefan Schaal"], "venue": "Trends in cognitive sciences,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1999}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Pieter Abbeel", "Michael I. Jordan", "Philipp Moritz"], "venue": "ICML, volume 37 of JMLR Workshop and Conference Proceedings,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Amortised map inference for image super-resolution", "author": ["Casper Kaae S\u00f8nderby", "Jose Caballero", "Lucas Theis", "Wenzhe Shi", "Ferenc Husz\u00e1r"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Feudal networks for hierarchical reinforcement learning", "author": ["Alexander Sasha Vezhnevets", "Simon Osindero", "Tom Schaul", "Nicolas Heess", "Max Jaderberg", "David Silver", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1703.01161,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2017}, {"title": "Unpaired image-to-image translation using cycle-consistent adversarial networks", "author": ["Jun-Yan Zhu", "Taesung Park", "Phillip Isola", "Alexei A Efros"], "venue": "arXiv preprint arXiv:1703.10593,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2017}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Brian D. Ziebart", "Andrew L. Maas", "J. Andrew Bagnell", "Anind K. Dey"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}], "referenceMentions": [{"referenceID": 1, "context": "These include: reinforcement learning [29] - where a robot learns a skill based on its interaction with the environment and imitation learning [2, 5] - where a robot is presented with a demonstration of a skill that it should imitate.", "startOffset": 143, "endOffset": 149}, {"referenceID": 3, "context": "These include: reinforcement learning [29] - where a robot learns a skill based on its interaction with the environment and imitation learning [2, 5] - where a robot is presented with a demonstration of a skill that it should imitate.", "startOffset": 143, "endOffset": 149}, {"referenceID": 25, "context": "Traditionally, imitation learning has focused on using isolated demonstrations of a particular skill [27].", "startOffset": 101, "endOffset": 105}, {"referenceID": 23, "context": "Approaches that are suitable for this setting can be split into two categories: i) behavioral cloning [25], and ii) inverse reinforcement learning (IRL) [22].", "startOffset": 102, "endOffset": 106}, {"referenceID": 20, "context": "Approaches that are suitable for this setting can be split into two categories: i) behavioral cloning [25], and ii) inverse reinforcement learning (IRL) [22].", "startOffset": 153, "endOffset": 157}, {"referenceID": 24, "context": "While behavioral cloning aims at replicating the demonstrations exactly, it suffers from the covariance shift [26].", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "The majority of IRL works [16, 33, 1, 12, 19] introduce algorithms that can imitate a single skill from demonstrations thereof but they do not readily generalize to learning a multi-task policy from a set of unstructured demonstrations of various tasks.", "startOffset": 26, "endOffset": 45}, {"referenceID": 30, "context": "The majority of IRL works [16, 33, 1, 12, 19] introduce algorithms that can imitate a single skill from demonstrations thereof but they do not readily generalize to learning a multi-task policy from a set of unstructured demonstrations of various tasks.", "startOffset": 26, "endOffset": 45}, {"referenceID": 0, "context": "The majority of IRL works [16, 33, 1, 12, 19] introduce algorithms that can imitate a single skill from demonstrations thereof but they do not readily generalize to learning a multi-task policy from a set of unstructured demonstrations of various tasks.", "startOffset": 26, "endOffset": 45}, {"referenceID": 10, "context": "The majority of IRL works [16, 33, 1, 12, 19] introduce algorithms that can imitate a single skill from demonstrations thereof but they do not readily generalize to learning a multi-task policy from a set of unstructured demonstrations of various tasks.", "startOffset": 26, "endOffset": 45}, {"referenceID": 17, "context": "The majority of IRL works [16, 33, 1, 12, 19] introduce algorithms that can imitate a single skill from demonstrations thereof but they do not readily generalize to learning a multi-task policy from a set of unstructured demonstrations of various tasks.", "startOffset": 26, "endOffset": 45}, {"referenceID": 8, "context": "More recently, there has been work that tackles a problem similar to the one presented in this paper, where the authors consider a setting where there is a large set of tasks with many instantiations [10].", "startOffset": 200, "endOffset": 204}, {"referenceID": 7, "context": "Examples from this field include [9] and [4].", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "Examples from this field include [9] and [4].", "startOffset": 41, "endOffset": 44}, {"referenceID": 7, "context": "In [9], the authors present a Bayesian approach to the problem, while the method in [4] is based on an EM approach that clusters observed demonstrations.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "In [9], the authors present a Bayesian approach to the problem, while the method in [4] is based on an EM approach that clusters observed demonstrations.", "startOffset": 84, "endOffset": 87}, {"referenceID": 15, "context": "There has also been a separate line of work on learning from demonstration, which is then iteratively improved through reinforcement learning [17, 6, 21].", "startOffset": 142, "endOffset": 153}, {"referenceID": 4, "context": "There has also been a separate line of work on learning from demonstration, which is then iteratively improved through reinforcement learning [17, 6, 21].", "startOffset": 142, "endOffset": 153}, {"referenceID": 19, "context": "There has also been a separate line of work on learning from demonstration, which is then iteratively improved through reinforcement learning [17, 6, 21].", "startOffset": 142, "endOffset": 153}, {"referenceID": 21, "context": "Examples include [23, 18, 14, 31, 13].", "startOffset": 17, "endOffset": 37}, {"referenceID": 16, "context": "Examples include [23, 18, 14, 31, 13].", "startOffset": 17, "endOffset": 37}, {"referenceID": 12, "context": "Examples include [23, 18, 14, 31, 13].", "startOffset": 17, "endOffset": 37}, {"referenceID": 28, "context": "Examples include [23, 18, 14, 31, 13].", "startOffset": 17, "endOffset": 37}, {"referenceID": 11, "context": "Examples include [23, 18, 14, 31, 13].", "startOffset": 17, "endOffset": 37}, {"referenceID": 13, "context": "Generative Adversarial Networks (GANs) [15] have enjoyed success in various domains including image generation [8], image-image translation [32] and video prediction [20].", "startOffset": 39, "endOffset": 43}, {"referenceID": 6, "context": "Generative Adversarial Networks (GANs) [15] have enjoyed success in various domains including image generation [8], image-image translation [32] and video prediction [20].", "startOffset": 111, "endOffset": 114}, {"referenceID": 29, "context": "Generative Adversarial Networks (GANs) [15] have enjoyed success in various domains including image generation [8], image-image translation [32] and video prediction [20].", "startOffset": 140, "endOffset": 144}, {"referenceID": 18, "context": "Generative Adversarial Networks (GANs) [15] have enjoyed success in various domains including image generation [8], image-image translation [32] and video prediction [20].", "startOffset": 166, "endOffset": 170}, {"referenceID": 22, "context": "More recently, there have been works connecting GANs and other reinforcement learning and IRL methods [24, 11, 16].", "startOffset": 102, "endOffset": 114}, {"referenceID": 9, "context": "More recently, there have been works connecting GANs and other reinforcement learning and IRL methods [24, 11, 16].", "startOffset": 102, "endOffset": 114}, {"referenceID": 14, "context": "More recently, there have been works connecting GANs and other reinforcement learning and IRL methods [24, 11, 16].", "startOffset": 102, "endOffset": 114}, {"referenceID": 14, "context": "The works that are most closely related to this paper are [16] and [7].", "startOffset": 58, "endOffset": 62}, {"referenceID": 5, "context": "The works that are most closely related to this paper are [16] and [7].", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "In [7], the authors show a method that is able to learn disentangled representations and apply it to the problem of image generation.", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "In [16], the authors present an imitation learning GAN approach that serves as a basis for the development of our method.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "We provide an extensive evaluation of the hereby presented approach compared to the work in [16], which shows that our method, as opposed to [16], can handle unstructured demonstrations of different skills.", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": "We provide an extensive evaluation of the hereby presented approach compared to the work in [16], which shows that our method, as opposed to [16], can handle unstructured demonstrations of different skills.", "startOffset": 141, "endOffset": 145}, {"referenceID": 0, "context": "This approach is known as inverse reinforcement learning (IRL) [1].", "startOffset": 63, "endOffset": 66}, {"referenceID": 30, "context": "We will refer to this approach as the maximum-entropy IRL [33] with the optimization objective", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "Ho and Ermon [16] showed that it is possible to redefine the maximum-entropy IRL problem with multiple demonstrations sampled from a single expert policy \u03c0E1 as an optimization of GANs [15].", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "Ho and Ermon [16] showed that it is possible to redefine the maximum-entropy IRL problem with multiple demonstrations sampled from a single expert policy \u03c0E1 as an optimization of GANs [15].", "startOffset": 185, "endOffset": 189}, {"referenceID": 26, "context": "The generator can be trained using the trust region policy optimization (TRPO) algorithm [28] with the cost function log(Dw(s, a)).", "startOffset": 89, "endOffset": 93}, {"referenceID": 5, "context": "In order to be able to learn multi-modal policy distributions, we augment the policy input with a latent intention i distributed by a categorical or uniform distribution p(i), similar to [7].", "startOffset": 187, "endOffset": 190}, {"referenceID": 14, "context": "Here, we show an extension of the derivation presented in [16] (Eqs.", "startOffset": 58, "endOffset": 62}, {"referenceID": 5, "context": "(8) by extending the InfoGAN approach presented in [7].", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "Following [7], we introduce the latent variable c as a means to capture the semantic features of the data distribution.", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "Similarly to [7], to prevent collapsing to a single mode, the policy optimization objective is augmented with mutual information I(c;G(\u03c0 \u03b8, c)) between the latent variable and the state-action pairs generator G dependent on the policy distribution \u03c0 \u03b8.", "startOffset": 13, "endOffset": 16}, {"referenceID": 5, "context": "In order to compute I(c;G(\u03c0 \u03b8, c)), we follow the derivation from [7] that introduces a lower bound: I(c;G(\u03c0 \u03b8, c)) = H(c)\u2212H(c|G(\u03c0 \u03b8, c)) (11)", "startOffset": 66, "endOffset": 69}, {"referenceID": 27, "context": "These include leveraging the Wasserstein distance metric to improve the convergence behavior [3] and adding instance noise to the inputs of the discriminator to avoid degenerate generative distributions [30].", "startOffset": 203, "endOffset": 207}, {"referenceID": 27, "context": "During our experiments, we anneal the noise similar to [30], as the generator policy improves towards the end of the training.", "startOffset": 55, "endOffset": 59}, {"referenceID": 5, "context": "We use a softmax layer to represent categorical latent variables, and use a uniform distribution for continuous latent variables as proposed in [7].", "startOffset": 144, "endOffset": 147}, {"referenceID": 14, "context": "As a baseline, we present the results of the Reacher task achieved by the standard GAN imitation learning presented in [16] without the latent intention cost.", "startOffset": 119, "endOffset": 123}], "year": 2017, "abstractText": "Imitation learning has traditionally been applied to learn a single task from demonstrations thereof. The requirement of structured and isolated demonstrations limits the scalability of imitation learning approaches as they are difficult to apply to real-world scenarios, where robots have to be able to execute a multitude of tasks. In this paper, we propose a multi-modal imitation learning framework that is able to segment and imitate skills from unlabelled and unstructured demonstrations by learning skill segmentation and imitation learning jointly. The extensive simulation results indicate that our method can efficiently separate the demonstrations into individual skills and learn to imitate them using a single multi-modal policy. The video of our experiments is available at http://sites.google.com/view/nips17intentiongan.", "creator": "LaTeX with hyperref package"}}}