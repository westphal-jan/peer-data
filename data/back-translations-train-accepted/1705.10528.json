{"id": "1705.10528", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Constrained Policy Optimization", "abstract": "For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015, Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in high-dimensional control, but do not consider the constrained setting.", "histories": [["v1", "Tue, 30 May 2017 10:07:31 GMT  (916kb,D)", "http://arxiv.org/abs/1705.10528v1", "Accepted to ICML 2017"]], "COMMENTS": "Accepted to ICML 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["joshua achiam", "david held", "aviv tamar", "pieter abbeel"], "accepted": true, "id": "1705.10528"}, "pdf": {"name": "1705.10528.pdf", "metadata": {"source": "META", "title": "Constrained Policy Optimization", "authors": ["Joshua Achiam", "David Held", "Aviv Tamar", "Pieter Abbeel"], "emails": ["<jachiam@berkeley.edu>."], "sections": [{"heading": null, "text": "We propose Constrained Policy Optimization (CPO), the first general-purpose search algorithm for restricted reinforcement learning with guarantees of near-contingent satisfaction with each iteration. Our method allows us to train neural network strategies for high-dimensional control while providing guarantees of policy behavior throughout the training. Our guarantees are based on a new theoretical result that is of independent interest: We prove a limit on the expected returns of two strategies to an average divergence between them. We demonstrate the effectiveness of our approach to simulated robot motion tasks where the agent must meet safety-related constraints."}, {"heading": "1. Introduction", "text": "It is indeed the case that we see ourselves as being able to assert ourselves, that we are able to change the world, and that we are able to change the world in order to change it."}, {"heading": "2. Related Work", "text": "Security has long been a topic of interest in RL research, and a comprehensive overview of security in RL has been given by (Garc\u0131 \"a & Ferna\" ndez, 2015) Safe policy search methods have been suggested in previous work. Uchibe and Doya (2007) gave a political gradient algorithm that uses gradient projection to enforce active restrictions, but this approach suffers from the inability to prevent a policy from becoming unsafe at all. Bou Ammar et al. (2015) propose a theoretically motivated political gradient method for lifelong learning with safety restrictions, but their method involves an expensive inner loop optimization of a semi-defined program, making it unsuitable for the deep RL environment. Their method also assumes that safety restrictions are linear in political parameters, limiting the ability to act."}, {"heading": "3. Preliminaries", "text": "A Markov Decision Process (MDP) is a tuple (S, A, R, P, \u00b5), where S is the set of states, A is the set of actions, R: S \u00b7 A \u00b7 S \u00b7 S \u00b7 S \u00b7 S \u2192 [0, 1] is the transition probability function (where P (s) is the probability of transition to states, since the previous state was s and the actor acted in s, and \u00b5: S \u2192 [0, 1] is the initial state distribution. A stationary policy \u03c0: S \u2192 P (A) is a map of states to probability distributions of actions, where \u03c0 (a | s) denotes the probability of selecting an action in states s. We define the set of all stationary actions by yardsticks. In intensified learning, we aim to choose a policy that maximizes a unit of measurement, J (\u03c0), which is usually considered as an infinite horizon, an overall distance."}, {"heading": "4. Constrained Markov Decision Processes", "text": "Specifically, we supplement the MDP with a series of C additional cost functions, C1,..., Cm (each with a function Ci: S \u00b7 A \u00b7 S \u2192 R, which maps the transition stage to costs, like the usual reward), and limit d1,..., dm. Let us leave JCi (\u03c0) the expected discounted return of policy measures in relation to the cost function Ci: JCi (\u03c0) = E; vice versa, vice versa. The amount of feasible stationary measures for a CMDP is then.C = {\u03c0: These measures I, JCi (\u03c0) = E; vice versa, vice versa, and the amplification of the learning problem in a CMDP is an interchangeable value."}, {"heading": "5. Constrained Policy Optimization", "text": "For large or continuous MDPs, the solution to the exact optimal policy is insoluble due to the curse of dimensionality (Sutton & Barto, 1998).Political search algorithms approach this problem by seeking the optimal policy within a defined, objective selection of parameterized strategies with parameters \u03b8 (for example, neural networks of a fixed architecture).In local policy search (Peters & Schaal, 2008), policy is iteratively updated by measuring J (\u03c0) over a local neighborhood of the most recent iterated strategies with parameters \u03c0k: \u03c0k + 1 = arg maximum number of new strategies J (\u03c0) s.t. Politics is iteratively updated by representing a distance measurement, and \u03b4 > 0 is a step variable. If the goal is estimated by linearization in the region, as J (\u03c0k) + gT (\u03b8 \u2212 zipk), g is the political gradient, and the standard political gradient is achieved."}, {"heading": "5.1. Policy Performance Bounds", "text": "In this section, we present the theoretical basis for our approach - a new boundary for the difference in returns between two arbitrary strategies; this result, which is of independent interest, extends the work of (Kakade & Langford, 2002), (Pirotta et al., 2013), and (Schulman et al., 2015), which offers narrower boundaries. As we will show later, it also applies the theoretical boundaries for improving trust regions to the actual trust region algorithms that have proven successful in practice (Duan et al., 2016). In the context of limited political search, we use our results to propose policy updates that both improve expected returns and satisfy constraints. The following theorem links the difference in returns (or constraint returns) between two arbitrary strategies to an average divergence between den.Theorem 1. For each function f: S \u2192 R and any strategies that we define (f)."}, {"heading": "5.2. Trust Region Methods", "text": "(Schulman et al., 2015; 2016) have political updates of form\u03c0k + 1 = \u03c0arg max zepzepzepzepzepzepzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzippfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpfpf"}, {"heading": "5.3. Trust Region Optimization for Constrained MDPs", "text": "A Limited Political Optimization (CPO), which we present and justify in this section, is a political search algorithm for CMDPs with updates that roughly (3) solve with a specific choice of D. First, we describe an update of the political search for CMDPs that alleviates the problem of non-political evaluation and provides guarantees of monotonous performance improvement and satisfaction with limitations. Then, because the theoretically guaranteed update will include all possible steps in practice, we propose CPO as a practical approach based on trust region methods. By the conclusions 1, 2 and 3, for appropriate coefficients, \u03b2ik will make the updated CPO method + 1 = arg max."}, {"heading": "6. Practical Implementation", "text": "In this section, we show how to implement an approximation of the update (10) that can be efficiently calculated, even if the guidelines are optimized with thousands of parameters. In order to address the problem of approximation and sampling errors that occur in practice, as well as the potential violations described in Proposal 2, we also propose to tighten the restrictions by setting the ceilings of the aid costs instead of the aid costs themselves."}, {"heading": "6.1. Approximately Solving the CPO Update", "text": "For strategies with high-dimensional parameter spaces such as neural networks, (10) the problem (FIx) cannot be solved directly on the basis of computational costs. (For small steps, however, the objective and cost-related constraints are linearized by \u03c0k, and the KL divergence constraint is well approximated by the expansion of the second order. (In small steps, the KL divergence and its gradient are both zero. (10) Denociation of the object's gradient as g, the gradient of the constraint i as bi, the Hessian of the KL divergence as H, and definition of ci. = JCi (\u03c0k) \u2212 di, the approximation to (10) is: \u0445k + 1 = arg max of the constraint i as bi, the Hessian of the KL divergence as H, and the definition of CL divergence as H, and the definition of the definition of the neural networks."}, {"heading": "6.2. Feasibility", "text": "Sometimes (11) is still workable and the CPO can automatically recover from its bad step, but in the unworkable case a recovery method is necessary. In our experiments, where we have only one limitation, we recover by proposing an update to reduce the restriction value purely: \u03b8 \u0445 = actionk \u2212 \u221a 2\u03b4bTH \u2212 1b H \u2212 1b. (14) As before, a line search follows. This approach is based on using the limiting search direction as the intersection of the trust region and shrinking the restriction region to zero. We specify the pseudo code for our algorithm (for the individual case) as algorithm 1."}, {"heading": "6.3. Tightening Constraints via Cost Shaping", "text": "Due to the different approximations between (3) and our practical algorithm, it is important to build a security factor into the algorithm to minimize the likelihood of constraints. To this end, we choose to limit the limits to the original constraints, C + i, instead of the original constraints themselves. We do this by pricing: C + i (s, a, s) = Ci (s, a, s) + \u2206 i (s, a, s \u2032), (15) where \u2206 i: S \u00d7 A \u00d7 S \u2192 R + is useful with CI. In our experiments, where we have only one constraint, we divide states into safe states and insecure states, and the agent incurs security costs of 1 because he is in an uncertain state. We choose the probability of entering an uncertain state within a fixed time horizon, according to a learned model that is updated with each repetition."}, {"heading": "7. Connections to Prior Work", "text": "Our methodology has similar policy updates to those of Chow et al. (2015), but crucially, we differ in calculating the dual variables (Lagrange multipliers for the constraints); in Primary Dual Optimization (PDO), dual variables are set and learned simultaneously with the primary variables (Boyd et al., 2003); in a PDO algorithm to solve (3), dual variables would be updated according to the value k + 1 = (\u03bdk + \u03b1k (JC (\u03c0k) \u2212 d) +, (16), where \u03b1k is a learning rate; this approach does not guarantee that intermediate strategies meet constraints - only policy on convergence; in contrast, the CPO recalculates new dual variables from scratch each time they are updated to enforce constraints accurately."}, {"heading": "8. Experiments", "text": "In our experiments, we try to answer the following questions: \u2022 How does the CPO manage to enforce behavioral constraints when it comes to developing strategies for neural networks with thousands of parameters? \u2022 How does the CPO compare to a baseline that uses primary-dual optimization? \u2022 Does the CPO perform better in terms of constraints? \u2022 How much does it help to limit a cost cap (15) instead of directly limiting costs? \u2022 What are the benefits of using constraints instead of fixed penalties? We designed experiments that are easy to interpret and motivated by security. We consider two tasks and train several different agents (robots) for each task: \u2022 Circle: The agent is rewarded for running in a wide circle, but is forced to stay within a safe region that is smaller than the radius of the target. \u2022 Gather: The agent is rewarded for gathering green apples, and forced to avoid red bombs."}, {"heading": "8.1. Evaluating CPO and Comparison Analysis", "text": "The learning curves for CPO and PDO are compiled in Figure 1. Note that we evaluate algorithm performance based on C + return instead of C return (except in PointGather, where we did not use cost formation due to the short time horizon of this environment), because this is what the algorithm actually limits in these experiments. For our comparison, we implement PDO with (16) as the updating rule for the dual variables, using a constant learning rate \u03b1; details are available in supplementary material (Section 10.3.3). We emphasize that to make the comparison fair, we give PDO every advantage given to the CPO, including equivalent regional policy updates. To name the environments, we also include TRPO (trust regional policy optimization) that we perform (Schulman et al., 2015), which is a state of unrestricted learning algorithm."}, {"heading": "8.2. Ablation on Cost Shaping", "text": "In Figure 3, we compare the performance of the CPO with and without CPO modeling in constraint. Our benchmark is the C rate of return, the \"true\" constraint. CPO modeling helps, taking into account almost all of the approximation errors inherent in the CPO. However, even without CPO modeling, the CPO is almost necessarily satisfactory."}, {"heading": "8.3. Constraint vs. Fixed Penalty", "text": "In Figure 4, we compare the CPO to a fixed penalty method, in which strategies are learned using TRPO with rewards R (s, a, s \u00b2) + \u03bbC + (s, a, s \u00b2). We find that fixed penalties can be very sensitive to the choice of penalty coefficient: in the ant circle, a penalty coefficient of 1 leads to reward-maximizing strategies that accumulate massive restraint costs, while a coefficient of 5 (less than a size difference) leads to cost-minimizing strategies that never learn how to get rewards. In contrast, the CPO automatically selects penalty coefficients to achieve the desired trade-off between reward and restraint costs."}, {"heading": "9. Discussion", "text": "In this article, we have shown that a particular optimization problem leads to policy updates that are guaranteed to both improve returns and fulfill constraints, enabling the development of CPO, our policy search algorithm for CMDPs, which is similar in principle to the theoretical guaranteed algorithm. We have shown that CPO can train neural networking strategies with thousands of parameters on highly dimensionally limited control tasks, while maximizing reward and approximately satisfactory limitations. Our work represents a step toward the application of enhanced learning in the real world, where constraints on agent behavior are sometimes necessary for security reasons."}, {"heading": "Acknowledgements", "text": "The authors would like to pay tribute to Peter Chen, who independently and simultaneously deduced a political limit for improvement. Joshua Achiam is supported by TRUST (Team for Research in Ubiquitous Secure Technology), which is supported by the NSF (price number CCF-0424422). This project was also supported by Berkeley Deep Drive and Siemens."}, {"heading": "10. Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10.1. Proof of Policy Performance Bound", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10.1.1. PRELIMINARIES", "text": "Our analysis will make a comprehensive use of the discounted future distribution of states defined as asd\u03c0 (s) = finite transition (s) = finite transition (s) = finite transition (s) = finite transition (s) = finite transition (s) = finite transition (s) = finite transition (s) = finite transition (s)."}, {"heading": "10.1.2. MAIN RESULTS", "text": "In this section, we will deduce and introduce the new policy improvement strategy. We start with a dilemma: For each function f: S (R) and each policy p (S) and p (S). \u2212 f (s), (22) and p (f). \u2212 f (maxs), p (s), p (s), p (s), p (s), p (s), p (s), p (s), f (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p, p (s), p (s), p (s), p (s), p, p, p (s), p, p (s), p, p, p (s, p (s), p, p (s), p (s), p (s), p, p (s, p, p (s), p, p (s), p, p (s), p, p (s), p (s), p (p (s), p, p (s), p (p, p (s), p (s)."}, {"heading": "10.2. Proof of Analytical Solution to LQCLP", "text": "Height: Size: Size: Size = Size = Size = Size = Size = Size = Size = Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: View: Size: Size: View: Size: Size: Size: Size: Size: View: Size: Size: Size: Size: Size: View: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: View: Size: Size: Size: Size: Size: View: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size: Size:: Size: Size:::: Size::: Size::: Size::::: Size:::::::: Size::::::: Size: Size: Size: Size:::: Size: Size: Size:::: Size: Size:: Size: Size::: Size::::: View::::::::: View:::::::::: Size:: View::::: View::::::::::::: View:::::::::: View: Size::: Size: Size::"}, {"heading": "10.3. Experimental Parameters", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10.3.1. ENVIRONMENTS", "text": "In the Circle environments, reward and cost functions are as follows: R (s) = vT [\u2212 y, x] 1 + [x, y] 2 \u2212 d |, C (s) = 1 [| x | > xlim], where x, y are the coordinates in the plane, v is the velocity and d, xlim are the environmental parameters. We set these parameters on bePoint-mass Ant Humanoid d 15 10 10xlim 2.5 3 2.5In Point-Gather, the agent receives a reward of + 10 for collecting an apple and a cost of 1 for collecting a bomb. Two apples and eight bombs spawn on the map at the beginning of each episode. In Ant-Gather, reward and cost structure were the same, except that the agent also receives a reward of \u2212 10 for falling over (which leads to the end of the episode). Eight apples and eight bombs spawn on the map at the beginning of each episode."}, {"heading": "10.3.2. ALGORITHM PARAMETERS", "text": "In all experiments, we use Gaussian guidelines with mean vectors specified as outputs of neural networks, with deviations that are separately learnable parameters; the policy networks for all experiments have two hidden size layers (64, 32) with tanh activation functions; we use GAE-\u03bb (Schulman et al., 2016) to estimate the benefits and limitation benefits of neural network value functions; the value functions have the same architecture and activation functions as the political networks; we found that with different GAE values for the regular advantages and limitation advantages EC worked best; we refer to the GAE parameters used for the limitation benefits as squeeze GAEC. (s \u2192 U) We use neural networks with a single hidden size layer (32), with the output of a sigmoid unit; with each iteration, the error parameter is updated by a certain lineage count of 0.0 AGL."}, {"heading": "10.3.3. PRIMAL-DUAL OPTIMIZATION IMPLEMENTATION", "text": "Our primary dual implementation should be as close as possible to our CPO implementation, the main difference being that the dual variables for the constraints are state-based, learnable parameters, unlike in the CPO, where they are solved from scratch with each update. The update equations for our PDO implementation are state-related, learnable parameters, unlike in the CPO, where they are solved from scratch with each update.The update equations for our PDO implementation are state-stable + 1 = \u03b8k + s + s j \u221a 2\u03b4 (g \u2212 \u03bdkb) TH \u2212 1 (g \u2212 \u03bdkb) H \u2212 1 (g \u2212 \u03bdkb) \u03bdk + 1 = (\u03bdk + \u03b1 (JC (\u03c0k) \u2212 d))) +, where sj is used from the trace line search (s (0, 1) and j {0, 1, Cdub), this is CJ \u2212 folk + 1, where the learning folder is TRO, where TRO is TRO; where TRJ is TRO; where TRJ is TRO; where TRJ is TRO."}], "references": [{"title": "Constrained Markov Decision Processes", "author": ["Altman", "Eitan"], "venue": "pp. 260,", "citeRegEx": "Altman and Eitan.,? \\Q1999\\E", "shortCiteRegEx": "Altman and Eitan.", "year": 1999}, {"title": "Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret", "author": ["Bou Ammar", "Haitham", "Tutunov", "Rasul", "Eaton", "Eric"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Ammar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2015}, {"title": "Subgradient methods", "author": ["Boyd", "Stephen", "Xiao", "Lin", "Mutapcic", "Almir"], "venue": "Lecture Notes of Stanford EE392,", "citeRegEx": "Boyd et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2003}, {"title": "Risk-Constrained Reinforcement Learning with Percentile Risk Criteria", "author": ["Chow", "Yinlam", "Ghavamzadeh", "Mohammad", "Janson", "Lucas", "Pavone", "Marco"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chow et al\\.", "year": 2015}, {"title": "Information Theory: Coding Theorems for Discrete Memoryless Systems", "author": ["I Csiszar", "J. K\u00f6rner"], "venue": "Book, 244:452,", "citeRegEx": "Csiszar and K\u00f6rner,? \\Q1981\\E", "shortCiteRegEx": "Csiszar and K\u00f6rner", "year": 1981}, {"title": "Benchmarking Deep Reinforcement Learning for Continuous Control", "author": ["Duan", "Yan", "Chen", "Xi", "Schulman", "John", "Abbeel", "Pieter"], "venue": "The 33rd International Conference on Machine Learning (ICML", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "A Comprehensive Survey on Safe Reinforcement Learning", "author": ["Garc\u0131\u0301a", "Javier", "Fern\u00e1ndez", "Fernando"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Garc\u0131\u0301a et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Garc\u0131\u0301a et al\\.", "year": 2015}, {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "author": ["Gu", "Shixiang", "Lillicrap", "Timothy", "Ghahramani", "Zoubin", "Turner", "Richard E", "Levine", "Sergey"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Gu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2017}, {"title": "Probabilistically Safe Policy Transfer", "author": ["Held", "David", "Mccarthy", "Zoe", "Zhang", "Michael", "Shentu", "Fred", "Abbeel", "Pieter"], "venue": "In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Held et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Held et al\\.", "year": 2017}, {"title": "Doubly Robust Off-policy Value Evaluation for Reinforcement Learning", "author": ["Jiang", "Nan", "Li", "Lihong"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Jiang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2015}, {"title": "Approximately Optimal Approximate Reinforcement Learning", "author": ["Kakade", "Sham", "Langford", "John"], "venue": "Proceedings of the 19th International Conference on Machine Learning,", "citeRegEx": "Kakade et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2002}, {"title": "End-to-End Training of Deep Visuomotor Policies", "author": ["Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Lillicrap et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2016}, {"title": "Combating Deep Reinforcement Learning\u2019s Sisyphean Curse with Intrinsic Fear", "author": ["Lipton", "Zachary C", "Gao", "Jianfeng", "Li", "Lihong", "Chen", "Jianshu", "Deng"], "venue": "In arXiv,", "citeRegEx": "Lipton et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lipton et al\\.", "year": 2004}, {"title": "Human-level control through deep reinforcement learning", "author": ["Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Dharshan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dharshan et al\\.", "year": 2015}, {"title": "Asynchronous Methods for Deep Reinforcement Learning", "author": ["Mnih", "Volodymyr", "Badia", "Adri\u00e0 Puigdom\u00e8nech", "Mirza", "Mehdi", "Graves", "Alex", "Lillicrap", "Timothy P", "Harley", "Tim", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "URL http://arxiv.org/abs/1602", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Safe Exploration in Markov Decision Processes", "author": ["Moldovan", "Teodor Mihai", "Abbeel", "Pieter"], "venue": "Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Moldovan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Moldovan et al\\.", "year": 2012}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["Peters", "Jan", "Schaal", "Stefan"], "venue": "Neural Networks,", "citeRegEx": "Peters et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2008}, {"title": "Safe Policy Iteration", "author": ["Pirotta", "Matteo", "Restelli", "Marcello", "Calandriello", "Daniele"], "venue": "Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Pirotta et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pirotta et al\\.", "year": 2013}, {"title": "Trust Region Policy Optimization", "author": ["Schulman", "John", "Moritz", "Philipp", "Jordan", "Michael", "Abbeel", "Pieter"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation", "author": ["Schulman", "John", "Moritz", "Philipp", "Levine", "Sergey", "Jordan", "Michael", "Abbeel", "Pieter"], "venue": null, "citeRegEx": "Schulman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2016}, {"title": "Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving. arXiv, 2016", "author": ["Shalev-Shwartz", "Shai", "Shammah", "Shaked", "Shashua", "Amnon"], "venue": "URL http: //arxiv.org/abs/1610.03295", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2016}, {"title": "Mastering the game of Go with deep neural networks and tree search", "author": ["Sutskever", "Ilya", "Lillicrap", "Timothy", "Leach", "Madeleine", "Kavukcuoglu", "Koray", "Graepel", "Thore", "Hassabis", "Demis"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2016}, {"title": "Introduction to Reinforcement Learning", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": "ISSN 10743529", "citeRegEx": "Sutton et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1996}, {"title": "Constrained reinforcement learning from intrinsic and extrinsic rewards", "author": ["Uchibe", "Eiji", "Doya", "Kenji"], "venue": "IEEE 6th International Conference on Development and Learning, ICDL,", "citeRegEx": "Uchibe et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Uchibe et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 15, "context": "Recent advances in policy search algorithms (Mnih et al., 2016; Schulman et al., 2015; Lillicrap et al., 2016; Levine et al., 2016) have enabled new capabilities in highdimensional control, but do not consider the constrained setting.", "startOffset": 44, "endOffset": 131}, {"referenceID": 19, "context": "Recent advances in policy search algorithms (Mnih et al., 2016; Schulman et al., 2015; Lillicrap et al., 2016; Levine et al., 2016) have enabled new capabilities in highdimensional control, but do not consider the constrained setting.", "startOffset": 44, "endOffset": 131}, {"referenceID": 12, "context": "Recent advances in policy search algorithms (Mnih et al., 2016; Schulman et al., 2015; Lillicrap et al., 2016; Levine et al., 2016) have enabled new capabilities in highdimensional control, but do not consider the constrained setting.", "startOffset": 44, "endOffset": 131}, {"referenceID": 11, "context": "Recent advances in policy search algorithms (Mnih et al., 2016; Schulman et al., 2015; Lillicrap et al., 2016; Levine et al., 2016) have enabled new capabilities in highdimensional control, but do not consider the constrained setting.", "startOffset": 44, "endOffset": 131}, {"referenceID": 19, "context": ", 2015; 2016), robot locomotion and manipulation (Schulman et al., 2015; Levine et al., 2016; Lillicrap et al., 2016), and even Go at the human grandmaster level (Silver et al.", "startOffset": 49, "endOffset": 117}, {"referenceID": 11, "context": ", 2015; 2016), robot locomotion and manipulation (Schulman et al., 2015; Levine et al., 2016; Lillicrap et al., 2016), and even Go at the human grandmaster level (Silver et al.", "startOffset": 49, "endOffset": 117}, {"referenceID": 12, "context": ", 2015; 2016), robot locomotion and manipulation (Schulman et al., 2015; Levine et al., 2016; Lillicrap et al., 2016), and even Go at the human grandmaster level (Silver et al.", "startOffset": 49, "endOffset": 117}, {"referenceID": 15, "context": "Currently, policy search algorithms enjoy state-of-theart performance on high-dimensional control tasks (Mnih et al., 2016; Duan et al., 2016).", "startOffset": 104, "endOffset": 142}, {"referenceID": 5, "context": "Currently, policy search algorithms enjoy state-of-theart performance on high-dimensional control tasks (Mnih et al., 2016; Duan et al., 2016).", "startOffset": 104, "endOffset": 142}, {"referenceID": 3, "context": "Heuristic algorithms for policy search in CMDPs have been proposed (Uchibe & Doya, 2007), and approaches based on primal-dual methods can be shown to converge to constraint-satisfying policies (Chow et al., 2015), but there is currently no approach for policy search in continuous CMDPs that guarantees every policy during learning will satisfy constraints.", "startOffset": 193, "endOffset": 212}, {"referenceID": 18, "context": "This result, which is of independent interest, tightens known bounds for policy search using trust regions (Kakade & Langford, 2002; Pirotta et al., 2013; Schulman et al., 2015), and provides a tighter connection between the theory and practice of policy search for deep RL.", "startOffset": 107, "endOffset": 177}, {"referenceID": 19, "context": "This result, which is of independent interest, tightens known bounds for policy search using trust regions (Kakade & Langford, 2002; Pirotta et al., 2013; Schulman et al., 2015), and provides a tighter connection between the theory and practice of policy search for deep RL.", "startOffset": 107, "endOffset": 177}, {"referenceID": 1, "context": "Bou Ammar et al. (2015) propose a theoretically-motivated policy gradient method for lifelong learning with safety constraints, but their method involves an expensive inner loop optimization of a semi-definite program, making it unsuited for the deep RL setting.", "startOffset": 4, "endOffset": 24}, {"referenceID": 1, "context": "Bou Ammar et al. (2015) propose a theoretically-motivated policy gradient method for lifelong learning with safety constraints, but their method involves an expensive inner loop optimization of a semi-definite program, making it unsuited for the deep RL setting. Their method also assumes that safety constraints are linear in policy parameters, which is limiting. Chow et al. (2015) propose a primal-dual subgradient method for risk-constrained reinforcement learning which takes policy gradient steps on an objective that trades off return with risk, while simultaneously learning the trade-off coefficients (dual variables).", "startOffset": 4, "endOffset": 384}, {"referenceID": 8, "context": "Held et al. (2017) study the problem for robotic manipulation, but the assumptions they make restrict the applicability of their methods.", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "Held et al. (2017) study the problem for robotic manipulation, but the assumptions they make restrict the applicability of their methods. Lipton et al. (2017) use an \u2018intrinsic fear\u2019 heuristic, as opposed to constraints, to motivate agents to avoid rare but catastrophic events.", "startOffset": 0, "endOffset": 159}, {"referenceID": 8, "context": "Held et al. (2017) study the problem for robotic manipulation, but the assumptions they make restrict the applicability of their methods. Lipton et al. (2017) use an \u2018intrinsic fear\u2019 heuristic, as opposed to constraints, to motivate agents to avoid rare but catastrophic events. Shalev-Shwartz et al. (2016) avoid the problem of enforcing constraints on parametrized policies by decomposing \u2018desires\u2019 from trajectory planning; the neural network policy learns desires for behavior, while the trajectory planning algorithm (which is not learned) selects final behavior and enforces safety constraints.", "startOffset": 0, "endOffset": 308}, {"referenceID": 19, "context": "When the objective is estimated by linearizing around \u03c0k as J(\u03c0k) + g (\u03b8 \u2212 \u03b8k), g is the policy gradient, and the standard policy gradient update is obtained by choosing D(\u03c0, \u03c0k) = \u2016\u03b8 \u2212 \u03b8k\u20162 (Schulman et al., 2015).", "startOffset": 191, "endOffset": 214}, {"referenceID": 5, "context": "When using sampling to compute policy updates, as is typically done in high-dimensional control (Duan et al., 2016), this requires off-policy evaluation, which is known to be challenging (Jiang & Li, 2015).", "startOffset": 96, "endOffset": 115}, {"referenceID": 19, "context": "In this work, we take a different approach, motivated by recent methods for trust region optimization (Schulman et al., 2015).", "startOffset": 102, "endOffset": 125}, {"referenceID": 18, "context": "This result, which is of independent interest, extends the works of (Kakade & Langford, 2002), (Pirotta et al., 2013), and (Schulman et al.", "startOffset": 95, "endOffset": 117}, {"referenceID": 19, "context": ", 2013), and (Schulman et al., 2015), providing tighter bounds.", "startOffset": 13, "endOffset": 36}, {"referenceID": 5, "context": "As we show later, it also relates the theoretical bounds for trust region policy improvement with the actual trust region algorithms that have been demonstrated to be successful in practice (Duan et al., 2016).", "startOffset": 190, "endOffset": 209}, {"referenceID": 18, "context": "to get a second factor of maxsDTV (\u03c0\u2032||\u03c0)[s], we recover (up to assumption-dependent factors) the bounds given by Pirotta et al. (2013) as Corollary 3.", "startOffset": 114, "endOffset": 136}, {"referenceID": 18, "context": "to get a second factor of maxsDTV (\u03c0\u2032||\u03c0)[s], we recover (up to assumption-dependent factors) the bounds given by Pirotta et al. (2013) as Corollary 3.6, and by Schulman et al. (2015) as Theorem 1a.", "startOffset": 114, "endOffset": 184}, {"referenceID": 19, "context": "Trust region algorithms for reinforcement learning (Schulman et al., 2015; 2016) have policy updates of the form", "startOffset": 51, "endOffset": 80}, {"referenceID": 5, "context": "This is important for optimizing neural network policies, which are known to suffer from performance collapse after bad updates (Duan et al., 2016).", "startOffset": 128, "endOffset": 147}, {"referenceID": 19, "context": "Despite the approximation, trust region steps usually give monotonic improvements (Schulman et al., 2015; Duan et al., 2016) and have shown state-of-the-art performance in the deep RL setting (Duan et al.", "startOffset": 82, "endOffset": 124}, {"referenceID": 5, "context": "Despite the approximation, trust region steps usually give monotonic improvements (Schulman et al., 2015; Duan et al., 2016) and have shown state-of-the-art performance in the deep RL setting (Duan et al.", "startOffset": 82, "endOffset": 124}, {"referenceID": 5, "context": ", 2016) and have shown state-of-the-art performance in the deep RL setting (Duan et al., 2016; Gu et al., 2017), making the approach appealing for developing policy search methods for CMDPs.", "startOffset": 75, "endOffset": 111}, {"referenceID": 7, "context": ", 2016) and have shown state-of-the-art performance in the deep RL setting (Duan et al., 2016; Gu et al., 2017), making the approach appealing for developing policy search methods for CMDPs.", "startOffset": 75, "endOffset": 111}, {"referenceID": 19, "context": "Like (Schulman et al., 2015), we approximately compute them using the conjugate gradient method.", "startOffset": 5, "endOffset": 28}, {"referenceID": 2, "context": "In primal-dual optimization (PDO), dual variables are stateful and learned concurrently with the primal variables (Boyd et al., 2003).", "startOffset": 114, "endOffset": 133}, {"referenceID": 2, "context": "Our method has similar policy updates to primal-dual methods like those proposed by Chow et al. (2015), but crucially, we differ in computing the dual variables (the Lagrange multipliers for the constraints).", "startOffset": 84, "endOffset": 103}, {"referenceID": 5, "context": "Our experiments are implemented in rllab (Duan et al., 2016).", "startOffset": 41, "endOffset": 60}, {"referenceID": 19, "context": "To benchmark the environments, we also include TRPO (trust region policy optimization) (Schulman et al., 2015), a stateof-the-art unconstrained reinforcement learning algorithm.", "startOffset": 87, "endOffset": 110}], "year": 2017, "abstractText": "For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016; Schulman et al., 2015; Lillicrap et al., 2016; Levine et al., 2016) have enabled new capabilities in highdimensional control, but do not consider the constrained setting. We propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Our method allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of independent interest: we prove a bound relating the expected returns of two policies to an average divergence between them. We demonstrate the effectiveness of our approach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety.", "creator": "LaTeX with hyperref package"}}}