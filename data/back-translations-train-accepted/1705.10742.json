{"id": "1705.10742", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Generating Steganographic Text with LSTMs", "abstract": "Motivated by concerns for user privacy, we design a steganographic system (\"stegosystem\") that enables two users to exchange encrypted messages without an adversary detecting that such an exchange is taking place. We propose a new linguistic stegosystem based on a Long Short-Term Memory (LSTM) neural network. We demonstrate our approach on the Twitter and Enron email datasets and show that it yields high-quality steganographic text while significantly improving capacity (encrypted bits per word) relative to the state-of-the-art.", "histories": [["v1", "Tue, 30 May 2017 16:52:48 GMT  (75kb,D)", "http://arxiv.org/abs/1705.10742v1", "ACL 2017 Student Research Workshop"]], "COMMENTS": "ACL 2017 Student Research Workshop", "reviews": [], "SUBJECTS": "cs.AI cs.CR cs.MM", "authors": ["tina fang", "martin jaggi", "katerina argyraki"], "accepted": true, "id": "1705.10742"}, "pdf": {"name": "1705.10742.pdf", "metadata": {"source": "CRF", "title": "Generating Steganographic Text with LSTMs", "authors": ["Tina Fang", "Martin Jaggi", "Katerina Argyraki"], "emails": ["tbfang@edu.uwaterloo.ca", "martin.jaggi@epfl.ch", "katerina.argyraki@epfl.ch"], "sections": [{"heading": "1 Introduction", "text": "The business model behind modern communication systems (e-mail services or messaging services provided by social networks) is incompatible with end-to-end encryption of messages, which providers of these services can afford to offer free of charge because most of their users are willing to receive \"targeted ads\" (ads specifically selected to appeal to each user based on the needs that the user has implied through his messages).This model works as long as users communicate predominantly in plain text, which allows service providers to make well-founded assumptions about user needs under an authoritarian regime.This situation does not prevent users from encrypting a few sensitive messages, but it does take away some of the benefits of confidentiality from them. Imagine, for example, a scenario in which two users want to exchange banned ideas or organize prohibited events under an authoritarian regime; in a world where most communication takes place in plain text, encryption of a small part of the messages - which automatically makes them (and the users) suspicious."}, {"heading": "2 Linguistic Steganography", "text": "In this section we summarize related work (\u00a7 2.1) and then present a proposal (\u00a7 2.2)."}, {"heading": "2.1 Related Work", "text": "Traditional linguistic stegosystems are based on the modification of an existing cover text, e.g. by means of synonym substitution (Topkara et al., 2006; Chang and Clark, 2014) and / or paraphrase substitution (Chang and Clark, 2010).The idea is to encode the secret information in the transformation of the cover text, ideally without compromising its meaning or grammatical correctness. Of these systems, CoverTweet (Wilson et al., 2014) is a state-of-the-art cover modification stegosystem that uses Twitter as the cover medium; we compare it in our preliminary evaluation (\u00a7 4).ar Xiv: 170 5,10 742v 1 [cs.A I] 3 0M ay2 017Cover modification can introduce syntactical and semantic unnaturalness (Grosvald and Orgun, 2011)."}, {"heading": "2.2 Our Proposal: Steganographic LSTM", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "3 Steganographic LSTM Model", "text": "In this section we present further details about our system: how we modify the LSTM (\u00a7 3.1) and how we evaluate its results (\u00a7 3.2)."}, {"heading": "3.1 LSTM Modification", "text": "Text generation in the classical LSTM. The classical LSTMs generate words as follows (Sutskever et al., 2011): When specifying a word sequence (x1, x2,.., xT), the model has hidden states (h1,..., hT) and resulting output vectors (o1,..., oT). Each output vector has the length | V |, and each output vector element o (j) t is the unnormalized probability of the word j in the vocabulary. The normalized probabilities for each candidate word are determined by the following Softmax activation function: Softmax (ot) j: = exp (o (j) t) / \u2211 k exp (o (k) t).The LSTM then selects the word with the highest probability P [xt + 1 | x \u2264 t] as its next word. Text generation in our steganographic LSTM is limited by the split word."}, {"heading": "3.2 Evaluation Metrics", "text": "We use confusion to quantify the quality of the web text; and capacity (i.e., encrypted bits per output word) to quantify its efficiency in transmitting secret information. In Section 4, we also discuss our web text quality as empirically perceived bits per output word. Confusion is a default quantity for the quality of language models (Martin and Jurafsky, 2000), and it is defined as the average log probability per word on the valid dataset: exp (\u2212 1 / N \u2211 i ln p [wi]) (Jozefowicz et al., 2016). Lower perplexity indicates a better model. In our steganographic LSTM, we cannot use this parameter as it is: since we force p [wi] = 0 for wi / i WB, the corresponding p [wi] formation is undefined."}, {"heading": "4 Experiments", "text": "In this section, we present our preliminary experimental analysis: our Twitter and e-mail records (Section 4.1), details of the LSTMs used for our results (Section 4.2), and finally a discussion of our results (Section 4.3)."}, {"heading": "4.1 Datasets", "text": "Tweets and e-mails are among the most popular media of open communication and therefore provide a very realistic environment for hiding information. Therefore, we trained our LSTMs in these two domains, Twitter messages and Enron e-mails (Klimt and Yang, 2004), which differ greatly in message length and vocabulary size. For Twitter, we used the NLTK tokenizer to tokenise tweets (Bird, 2006) into words and punctuation marks. We normalized the content by replacing usernames and URLs with a username token (< user >) or a URL token (< url >). We used 600 thousand tweets with a total of 45 million words and a vocabulary of 225 thousand. For Enron, we cleaned and extracted e-mail messages (Zhou et al., 2007) from the Enron dataset and certified the messages into words and punctuations."}, {"heading": "4.2 Implementation Details", "text": "In both experiments, we implemented multi-layered LSTMs based on PyTorch2. We did not use prefabricated word embeddings (Mikolov et al., 2013; Pennington et al., 2014), but trained word embeddings of the dimension 200 from scratch. We optimized with Stochastic Gradient Descent and used a batch size of 20. The initial learning rate was 20 and the decay factor per epoch was 4. The decay of the learning rate only occurred when the validation loss did not improve. Model training was performed on an NVIDIA GeForce GTX TITAN X. For Twitter, we used a double-layer LSTM with 600 units, which was unrolled for 25 steps to reverse propagation. We cut the standard of gradients (Pascanu et al., 2013) at 0.25 and applied 20% drop-out (Srivastava et al., 2014). We broke the training after 12 epochs (10 hours) with no loss of validation (we used a backward conversion layer)."}, {"heading": "4.3 Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3.1 Tweets", "text": "In addition, we found empirically that the addition of 10 most common tokens from the Twitter corpus was sufficient to significantly improve the grammatical correctness and semantic reasonableness of 2https: / / github.com / pytorchtweets. Table 3 shows the relationship between capacity (bits per word) and quantitative text quality (perplexity), and also compares models with and without the addition of common tokens using perplexity and bits per word.Table 4 shows sample texts of LSTMs with and without common tokens added. To reflect the variation in the quality of tweets, we present tweets that are good and bad in quality.We created < user > > by replacing < user > tweets with the LSTM using sham usernames for a more realistic representation in Table 4."}, {"heading": "4.3.2 Emails", "text": "The biggest difference between emails and tweets is that emails have a much wider reach. For each category, we manually rate 60 randomly generated tweets based on grammatical correctness, semantic coherence and similarity to real tweets. We select tweets from the 25th, 50th and 75th percentiles and call them \"good,\" \"average\" or \"bad.\" We limit ourselves to tweets that are not linguistically objectionable.4We only present passages \"average\" in quality to save space. Context dependence, with cross-context sentences and paragraphs, is difficult to model, even for non-steganographic LSTMs. As the context dependence of the non-steganographic LSTM improves, the context dependence of the steganographic LSTL Ms should also improve."}, {"heading": "4.4 Comparison with Other Stegosystems", "text": "For all comparisons, we use our quadruple model without additional common tokens. Our model significantly improves the capacity of the modern Twitter bar system. Cover modification-based bar systems hide 1-2 bits per sentence (Chang and Clark, 2012). The state-of-the-art Twitter bar system hides 2.8 bits per tweet (Wil-son and Ker, 2016). Assuming 16.04 words per Tweet5, our quadruple system hides 32 bits per tweet, more than 11 times more than (Wilson and Ker, 2016).We expect the subjective quality of our generated tweets to be comparable to tweets from CoverTweet (2014).We present some examples from Table 6 to show that there is comparative potential. This contrasts with the previous notion that CoverTweets generation methods are deadly weak compared to human judges (Wilson et al, 2014). Coveret was tested to be safe from human judges."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we have opened up a new application of LSTMs, namely steganographic text generation. We presented our steganographic model based on existing voice modeling LSTMs and demonstrated that our model produces realistic tweets and e-mails while hiding information.Compared to modern steganographic systems, our system has the advantage of encoding much more information (about 2 bits per word).This advantage makes the system more usable and scalable in practice.In future work, we will formally evaluate the security of our system vis-\u00e0-vis human judges and other steganographic recognition methods (steganographic analysis) (Wilson et al., 2015; Kodovsky et al., 2012).If the setup is evaluated against an automated classifier, that of a generative adversarial network (Goodfellow et al., 2014), however, with additional conditions for the generator (the secret bits), the discriminator, will be unknown to us, and we will be able to use more personal tweeting instead."}], "references": [{"title": "Nltk: the natural language toolkit", "author": ["Steven Bird."], "venue": "Proceedings of the COLING/ACL on Interactive presentation sessions. Association for Computational Linguistics, pages 69\u201372.", "citeRegEx": "Bird.,? 2006", "shortCiteRegEx": "Bird.", "year": 2006}, {"title": "Linguistic steganography using automatically generated paraphrases", "author": ["Ching-Yun Chang", "Stephen Clark."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Chang and Clark.,? 2010", "shortCiteRegEx": "Chang and Clark.", "year": 2010}, {"title": "Adjective deletion for linguistic steganography and secret sharing", "author": ["Ching-Yun Chang", "Stephen Clark."], "venue": "COLING. pages 493\u2013510.", "citeRegEx": "Chang and Clark.,? 2012", "shortCiteRegEx": "Chang and Clark.", "year": 2012}, {"title": "Practical linguistic steganography using contextual synonym substitution and a novel vertex coding method", "author": ["Ching-Yun Chang", "Stephen Clark."], "venue": "Computational Linguistics 40(2):403\u2013448.", "citeRegEx": "Chang and Clark.,? 2014", "shortCiteRegEx": "Chang and Clark.", "year": 2014}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."], "venue": "Advances in neural information processing systems. pages 2672\u20132680.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Free from the cover text: a human-generated natural language approach to text-based steganography", "author": ["Michael Grosvald", "C Orhan Orgun."], "venue": "Journal of Information Hiding and Multimedia Signal Processing 2(2):133\u2013141.", "citeRegEx": "Grosvald and Orgun.,? 2011", "shortCiteRegEx": "Grosvald and Orgun.", "year": 2011}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."], "venue": "arXiv preprint arXiv:1602.02410 .", "citeRegEx": "Jozefowicz et al\\.,? 2016", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "The enron corpus: A new dataset for email classification research", "author": ["Bryan Klimt", "Yiming Yang."], "venue": "European Conference on Machine Learning. Springer, pages 217\u2013226.", "citeRegEx": "Klimt and Yang.,? 2004", "shortCiteRegEx": "Klimt and Yang.", "year": 2004}, {"title": "Ensemble classifiers for steganalysis of digital media", "author": ["Jan Kodovsky", "Jessica Fridrich", "Vojt\u011bch Holub."], "venue": "IEEE Transactions on Information Forensics and Security 7(2):432\u2013444.", "citeRegEx": "Kodovsky et al\\.,? 2012", "shortCiteRegEx": "Kodovsky et al\\.", "year": 2012}, {"title": "Speech and language processing", "author": ["James H Martin", "Daniel Jurafsky."], "venue": "International Edition 710.", "citeRegEx": "Martin and Jurafsky.,? 2000", "shortCiteRegEx": "Martin and Jurafsky.", "year": 2000}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "Interspeech. volume 2, page 3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "ICML (3) 28:1310\u20131318.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP. volume 14, pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "The laws of large numbers, volume 4", "author": ["P\u00e1l R\u00e9v\u00e9sz."], "venue": "Academic Press.", "citeRegEx": "R\u00e9v\u00e9sz.,? 2014", "shortCiteRegEx": "R\u00e9v\u00e9sz.", "year": 2014}, {"title": "Matryoshka: Hiding secret communication in plain sight", "author": ["Iris Safaka", "Christina Fragouli", "Katerina Argyraki."], "venue": "6th USENIX Workshop on Free and Open Communications on the Internet (FOCI 16). USENIX Association.", "citeRegEx": "Safaka et al\\.,? 2016", "shortCiteRegEx": "Safaka et al\\.", "year": 2016}, {"title": "Data privacy and security: encryption and information hiding", "author": ["David Salomon."], "venue": "Springer Science & Business Media.", "citeRegEx": "Salomon.,? 2003", "shortCiteRegEx": "Salomon.", "year": 2003}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton."], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11). pages 1017\u20131024.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions", "author": ["Umut Topkara", "Mercan Topkara", "Mikhail J Atallah."], "venue": "Proceedings of the 8th workshop on Multimedia and security. ACM,", "citeRegEx": "Topkara et al\\.,? 2006", "shortCiteRegEx": "Topkara et al\\.", "year": 2006}, {"title": "Steganographic generative adversarial networks", "author": ["Denis Volkhonskiy", "Ivan Nazarov", "Boris Borisenko", "Evgeny Burnaev."], "venue": "arXiv preprint arXiv:1703.05502 .", "citeRegEx": "Volkhonskiy et al\\.,? 2017", "shortCiteRegEx": "Volkhonskiy et al\\.", "year": 2017}, {"title": "Detection of steganographic techniques on twitter", "author": ["Alex Wilson", "Phil Blunsom", "Andrew Ker."], "venue": "EMNLP. pages 2564\u20132569.", "citeRegEx": "Wilson et al\\.,? 2015", "shortCiteRegEx": "Wilson et al\\.", "year": 2015}, {"title": "Linguistic steganography on twitter: hierarchical language modeling with manual interaction", "author": ["Alex Wilson", "Phil Blunsom", "Andrew D Ker."], "venue": "IS&T/SPIE Electronic Imaging. International Society for Optics and Photonics, pages", "citeRegEx": "Wilson et al\\.,? 2014", "shortCiteRegEx": "Wilson et al\\.", "year": 2014}, {"title": "Avoiding detection on twitter: embedding strategies for linguistic steganography", "author": ["Alex Wilson", "Andrew D Ker."], "venue": "Electronic Imaging 2016(8):1\u20139.", "citeRegEx": "Wilson and Ker.,? 2016", "shortCiteRegEx": "Wilson and Ker.", "year": 2016}, {"title": "Strategies for cleaning organizational emails with an application to enron email dataset", "author": ["Yingjie Zhou", "Mark Goldberg", "Malik Magdon-Ismail", "Al Wallace."], "venue": "5th Conf. of North American Association for Computational Social and Organizational", "citeRegEx": "Zhou et al\\.,? 2007", "shortCiteRegEx": "Zhou et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 20, "context": ", using synonym substitution (Topkara et al., 2006; Chang and Clark, 2014) and/or paraphrase substitution (Chang and Clark, 2010).", "startOffset": 29, "endOffset": 74}, {"referenceID": 3, "context": ", using synonym substitution (Topkara et al., 2006; Chang and Clark, 2014) and/or paraphrase substitution (Chang and Clark, 2010).", "startOffset": 29, "endOffset": 74}, {"referenceID": 1, "context": ", 2006; Chang and Clark, 2014) and/or paraphrase substitution (Chang and Clark, 2010).", "startOffset": 62, "endOffset": 85}, {"referenceID": 23, "context": "Of these systems, the most closely related to ours is CoverTweet (Wilson et al., 2014), a state-of-theart cover modification stegosystem that uses Twitter as the medium of cover; we compare to it in our preliminary evaluation (\u00a74).", "startOffset": 65, "endOffset": 86}, {"referenceID": 5, "context": "Cover modification can introduce syntactic and semantic unnaturalness (Grosvald and Orgun, 2011); to address this, Grovsald and Orgun proposed an alternative stegosystem where a human generates the stegotext manually, thus improving linguistic naturalness at the cost of human effort (Grosvald and Orgun, 2011).", "startOffset": 70, "endOffset": 96}, {"referenceID": 5, "context": "Cover modification can introduce syntactic and semantic unnaturalness (Grosvald and Orgun, 2011); to address this, Grovsald and Orgun proposed an alternative stegosystem where a human generates the stegotext manually, thus improving linguistic naturalness at the cost of human effort (Grosvald and Orgun, 2011).", "startOffset": 284, "endOffset": 310}, {"referenceID": 16, "context": "Matryoshka (Safaka et al., 2016) takes this further: in step 1, it generates candidate stegotext automatically based on an n-gram model of the English language; in step 2, it presents the candidate stegotext to the human user for polishing, i.", "startOffset": 11, "endOffset": 32}, {"referenceID": 4, "context": "have applied Generative Adversarial Networks (Goodfellow et al., 2014) to image steganography (Volkhonskiy et al.", "startOffset": 45, "endOffset": 70}, {"referenceID": 21, "context": ", 2014) to image steganography (Volkhonskiy et al., 2017), but we are not aware of any text stegosystem based on neural networks.", "startOffset": 31, "endOffset": 57}, {"referenceID": 6, "context": "Motivated by the fact that LSTMs (Hochreiter and Schmidhuber, 1997) constitute the state of the art in text generation (Jozefowicz et al.", "startOffset": 33, "endOffset": 67}, {"referenceID": 7, "context": "Motivated by the fact that LSTMs (Hochreiter and Schmidhuber, 1997) constitute the state of the art in text generation (Jozefowicz et al., 2016), we propose to automatically generate the stegotext from an LSTM (as opposed to an n-gram model).", "startOffset": 119, "endOffset": 144}, {"referenceID": 17, "context": "1 outlines the building blocks of a stegosystem (Salomon, 2003).", "startOffset": 48, "endOffset": 63}, {"referenceID": 11, "context": "The embedding algorithm uses a modified word-level LSTM for language modeling (Mikolov et al., 2010).", "startOffset": 78, "endOffset": 100}, {"referenceID": 19, "context": "Classic LSTMs generate words as follows (Sutskever et al., 2011): Given a word sequence (x1, x2, .", "startOffset": 40, "endOffset": 64}, {"referenceID": 10, "context": "Perplexity is a standard metric for the quality of language models (Martin and Jurafsky, 2000), and it is defined as the average per-word log-probability on the valid data set: exp(\u22121/N \u2211 i ln p[wi]) (Jozefowicz et al.", "startOffset": 67, "endOffset": 94}, {"referenceID": 7, "context": "Perplexity is a standard metric for the quality of language models (Martin and Jurafsky, 2000), and it is defined as the average per-word log-probability on the valid data set: exp(\u22121/N \u2211 i ln p[wi]) (Jozefowicz et al., 2016).", "startOffset": 200, "endOffset": 225}, {"referenceID": 15, "context": "By the Law of Large Numbers (R\u00e9v\u00e9sz, 2014), if we perform many stegotext-generating trials using different random secret data as input, the probability of each word will tend to the expected value, \u2211 p[wi, B]/2 |B|, Hence, we set p[wi] := \u2211 p[wi, B]/2 |B| instead of p[wi] = 0 for wi / \u2208WB .", "startOffset": 28, "endOffset": 42}, {"referenceID": 8, "context": "(Klimt and Yang, 2004), which vary greatly in message length and vocabulary size.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "For Twitter, we used the NLTK tokenizer to tokenize tweets (Bird, 2006) into words and punctuation marks.", "startOffset": 59, "endOffset": 71}, {"referenceID": 25, "context": "For Enron, we cleaned and extracted email message bodies (Zhou et al., 2007) from the Enron dataset, and we tokenized the messages into words and punctuation marks.", "startOffset": 57, "endOffset": 76}, {"referenceID": 12, "context": "We did not use pretrained word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and instead trained word embeddings of dimension 200 from scratch.", "startOffset": 42, "endOffset": 89}, {"referenceID": 14, "context": "We did not use pretrained word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and instead trained word embeddings of dimension 200 from scratch.", "startOffset": 42, "endOffset": 89}, {"referenceID": 13, "context": "We clipped the norm of the gradients (Pascanu et al., 2013) at 0.", "startOffset": 37, "endOffset": 59}, {"referenceID": 18, "context": "25 and applied 20% dropout (Srivastava et al., 2014).", "startOffset": 27, "endOffset": 52}, {"referenceID": 2, "context": "Cover modification based stegosystems hide 1-2 bits per sentence (Chang and Clark, 2012).", "startOffset": 65, "endOffset": 88}, {"referenceID": 24, "context": "8 bits of per tweet (Wilson and Ker, 2016).", "startOffset": 20, "endOffset": 42}, {"referenceID": 24, "context": "04 words per tweet5, our 4-bin system hides 32 bits per tweet, over 11 times higher than (Wilson and Ker, 2016).", "startOffset": 89, "endOffset": 111}, {"referenceID": 23, "context": "This contrasts the previous conception that cover generation methods are fatally weak against human judges (Wilson et al., 2014).", "startOffset": 107, "endOffset": 128}, {"referenceID": 1, "context": "Cover modification based stegosystems hide 1-2 bits per sentence (Chang and Clark, 2012). The state-of-the-art Twitter stegosystem hides 2.8 bits of per tweet (Wilson and Ker, 2016). Assuming 16.04 words per tweet5, our 4-bin system hides 32 bits per tweet, over 11 times higher than (Wilson and Ker, 2016). We hypothesize that the subjective quality of our generated tweets will be comparable to tweets produced by CoverTweet (2014). We present some examples6 in Table 6 to show there is potential for a comparison.", "startOffset": 66, "endOffset": 434}, {"referenceID": 23, "context": "This is not the case with existing cover modification systems, where capacity is bounded above by the number of transformation options (Wilson et al., 2014).", "startOffset": 135, "endOffset": 156}, {"referenceID": 22, "context": "In future work, we will formally evaluate our system\u2019s security against human judges and other steganography detection (steganalysis) methods (Wilson et al., 2015; Kodovsky et al., 2012).", "startOffset": 142, "endOffset": 186}, {"referenceID": 9, "context": "In future work, we will formally evaluate our system\u2019s security against human judges and other steganography detection (steganalysis) methods (Wilson et al., 2015; Kodovsky et al., 2012).", "startOffset": 142, "endOffset": 186}, {"referenceID": 4, "context": "When evaluated against an automated classifier, the setup becomes that of a Generative Adversarial Network (Goodfellow et al., 2014), though with additional conditions for the generator (the secret bits) which are unknown to the discriminator, and not necessarily employing joint training.", "startOffset": 107, "endOffset": 132}], "year": 2017, "abstractText": "Motivated by concerns for user privacy, we design a steganographic system (\u201cstegosystem\u201d) that enables two users to exchange encrypted messages without an adversary detecting that such an exchange is taking place. We propose a new linguistic stegosystem based on a Long ShortTerm Memory (LSTM) neural network. We demonstrate our approach on the Twitter and Enron email datasets and show that it yields high-quality steganographic text while significantly improving capacity (encrypted bits per word) relative to the state-of-the-art.", "creator": "LaTeX with hyperref package"}}}