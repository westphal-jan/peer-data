{"id": "1705.10829", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Accuracy First: Selecting a Differential Privacy Level for Accuracy-Constrained ERM", "abstract": "Traditional approaches to differential privacy assume a fixed privacy requirement $\\epsilon$ for a computation, and attempt to maximize the accuracy of the computation subject to the privacy constraint. As differential privacy is increasingly deployed in practical settings, it may often be that there is instead a fixed accuracy requirement for a given computation and the data analyst would like to maximize the privacy of the computation subject to the accuracy constraint. This raises the question of how to find and run a maximally private empirical risk minimizer subject to a given accuracy requirement. We propose a general \"noise reduction\" framework that can apply to a variety of private empirical risk minimization (ERM) algorithms, using them to \"search\" the space of privacy levels to find the empirically strongest one that meets the accuracy constraint, incurring only logarithmic overhead in the number of privacy levels searched. The privacy analysis of our algorithm leads naturally to a version of differential privacy where the privacy parameters are dependent on the data, which we term ex-post privacy, and which is related to the recently introduced notion of privacy odometers. We also give an ex-post privacy analysis of the classical AboveThreshold privacy tool, modifying it to allow for queries chosen depending on the database. Finally, we apply our approach to two common objectives, regularized linear and logistic regression, and empirically compare our noise reduction methods to (i) inverting the theoretical utility guarantees of standard private ERM algorithms and (ii) a stronger, empirical baseline based on binary search.", "histories": [["v1", "Tue, 30 May 2017 19:20:28 GMT  (115kb,D)", "http://arxiv.org/abs/1705.10829v1", "24 pages single-column"]], "COMMENTS": "24 pages single-column", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["katrina ligett", "seth neel", "aaron roth", "bo waggoner", "z steven wu"], "accepted": true, "id": "1705.10829"}, "pdf": {"name": "1705.10829.pdf", "metadata": {"source": "CRF", "title": "Accuracy First: Selecting a Differential Privacy Level for Accuracy-Constrained ERM", "authors": ["Katrina Ligett", "Seth Neel", "Aaron Roth", "Bo Waggoner", "Steven Wu"], "emails": [], "sections": [{"heading": "1 Introduction and Related Work", "text": "There are a number of current privacy requirements in theory and the practical necessities of applications. In this paper, we focus our attention on such an assumption in the field of private empirical risk mitigation (ERM): that the data analyst first selects a privacy requirement and then tries to obtain the best accuracy guarantee (or empirical performance) that it is tailored to this view given the privacy limitation chosen: the data analyst can select their privacy parameters via some exogenous processes and either insert them into a \"usage theory\" to improve their accuracy, or simply Xiv: 170 5,10 829v 1 [cs.L G] 30 Mdeploy algorithms privately and substantially."}, {"heading": "2 Privacy Background and Tools", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Differential Privacy and Ex-Post Privacy", "text": "Let us call X the data domain. We call two data sets D, D. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \"x.\" x. \""}, {"heading": "2.2 Differential Privacy Tools", "text": "There are two beautiful properties: Theorem 2.4 (Post Processing [6]). Let A: X \u0445 \u2192 O be some \u03b5-differential private algorithm, and let f: O \u00b2 O \u00b2 be some function. Then, the algorithm f: X \u00b2 privacy (privacy) is also \u03b5-differentiated privat.Post-processing implies that, for example, any decision based on the production of a differentiated private algorithm is also differentiated privat.Theorem 2.5 (composition [6]). Let A1: X \u00b2 O: O \u00b2 O \u00b2 O \u00b2 be algorithms based on the output of a differentiated private algorithm, respectively. Then, the algorithm A: X \u00b2 O \u00b2 O \u00b2 is defined as A (x) = (A1), A2 (x) is (x)."}, {"heading": "2.3 AboveThreshold with Private Queries", "text": "Our high-level approach to our last ERM problem will be as follows: Create a sequence of hypotheses that we would not publish at a point where there would be a restriction of privacy. (.) The classic top privacy threshold is \"good enough.\" The classic top privacy threshold [7] takes the index of the first query in a dataset and a sequence of queries and private expenditures to exceed a given threshold (with some errors due to noise). We would like to use AboveThreshold to perform these accuracy checks, but there is an important obstacle: for us, the \"queries\" themselves depend on private data. (2) A standard composition analysis would involve the first private release of all queries, then we run AboveThreshold on those queries (which are now public). Intuitively, it would be much better to generate and publish the queries."}, {"heading": "3 Noise-Reduction with Private ERM", "text": "In this section, we provide a general private ERM-Framework, which enables us to approach the best data protection guarantee available on the data. < < < < < < < < < < < < < < > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > >"}, {"heading": "3.1 Covariance Perturbation for Ridge Regression", "text": "In ridge regression we consider the squared loss function: \"(Xi, yi), \u03b8) = 1 2 (yi \u2212 < \u03b8, Xi >) 2, and thus empirical loss over the dataset is defined as:\" (X, yi). \"Since the optimal solution to the unrestricted problem\" 2 norms has no more than 1 / 2, where X has the (n, p) matrix with the row vectors X1,., Xn and y = (y1,., yn), we focus on optimization over the restricted set C = (y1,.,.). Since the optimal solution to the unrestricted problem has no more than \"2 norms\" (see appendix for aproof)."}, {"heading": "3.2 Output Perturbation for Logistic Regression", "text": "Next, we will show how to use the output disturbance method with noise reduction for the first regression problems.4 In this context, the input data consists of n marked examples (X1, y1),.., (Xn, yn), so for each i, Xi, yi) privacy guarantee 1 \u2264 1, and yi. \"The goal is to learn a linear classification using a weight vector for the examples from the two classes. We will consider the logistical loss function:\" (Xi, yi) = log (1 + exp) = exemplary privacy statement (\u2212 ii)."}, {"heading": "4 Experiments", "text": "To evaluate the methods described above, we performed empirical evaluations in two settings: We used ridge regression to predict the (log) popularity of posts on Twitter in the dataset of [1], with p = 77 characteristics and subsampled to n = 100,000 data points. Logistical regression was applied to classify network events in the KDD 99 Cup dataset [12] as innocent or malicious, with 38 characteristics and subsampled to 100,000 points. Details of parameters and methods appear in the appendix. In any case, we tested the average loss of privacy ex post for a number of input accuracy targets \u03b1 by setting a modest probability of failure \u03b3 = 0.1 (and we observed that excessive risks were concentrated well below \u03b1 / 2, indicating pessimistic analysis). The results show that our meta method represents a simple improvement over the ERM's large-benefit approach to the theory. \""}, {"heading": "Acknowledgements", "text": "This work was partially supported by NSF grants CNS-1253345, CNS-1513694, CNS-1254169, and CNS-1518941, the US-Israel Binational Science Foundation grants 2012348, the Israeli Science Foundation (ISF) grants # 1044 / 16, a subcontract for the DARPA Brandeis Project, the Warren Center for Data and Network Sciences, and the HUJI Cyber Security Research Center in collaboration with the Israel National Cyber Bureau in the Prime Minister's Office."}, {"heading": "A Missing Details and Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 AboveThreshold", "text": "We will instead analyze the algorithm that prints the entire prefix f1. We want for all results o = (t, f1,.., ft): Pr [IAT (D) = (t, f2,., ft): Pr [IAT (D) = (t, f2,., ft)]: We have directly from the privacy guarantee of the interactive AboveThreshold that for each set sequence of queries f1,...., ft., ft. (ft.): f1,., ft."}, {"heading": "A.2 Doubling Method", "text": "Let \u03b8 = argmin\u03b8 \u0445 Rp L (\u03b8). DoublingMethod accepts a list of privacy levels \u03b51 < \u03b52 < \u03b1. < \u03b5T, where \u03b5i = 2\u03b5i \u2212 1. We show in claim B.1 that 2 is the optimal factor to scale based on. It is also assumed that there is a probability of error, and a black box private ERM mechanism M, which has the following guarantee: Defining a dataset D, M assumes as input D and a privacy level, and generates a privacy-level hypothesis, so that the query f i (D) = L (D, \u03b52) = Privacy mechanism M, which has the following guarantee: Defining a dataset D, M assumes as input and a privacy-D."}, {"heading": "A.3 Ridge Regression", "text": "(Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads of State or Government) (Heads) (Heads of"}, {"heading": "A.4 Logistic Regression", "text": "In this subsection, the input file D consists of n marked examples (X1, y1),., (Xn, yn), so for each i, xi, xi, xi, xi, xi, 1, and yi, 1, and yi, 1,. We consider the logistic loss function: \"(Xi, yi) = log (1 + exp, yi) = log (1 + exp), and our empirical loss is defined as asL (\u03b8, D) = n, i = 1 log (1 + exp, yi)) + empirical loss is defined as\" 1-empirical loss \"caused by the following lemma.Lemma A.10. Let D and D be a pair of adjacent datasets. Let us be that we will be a pair of adjacent datasets."}, {"heading": "B Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Parameters and data", "text": "For the sake of simplicity and to avoid overfitting, we have defined the following parameters for both experiments: \u2022 n = 100,000 (number of data points) \u2022 \u03bb = 0.005 (regularization parameters) \u2022 \u03b3 = 0.10 (requested failure probability) \u2022 \u03b51 = 4E, where E is the inversion of the theory guarantee for the underlying algorithm. \u2022 \u03b5T = 1.0 / n. \u2022 \u03b1 = 0.005.00010,0015,.., 0.200 (requested exceeding of the error limit). For NoiseReduction, we select T = 1000 (maximum number of iterations) and set \u03b5t = \u03b51rt for the corresponding r, i.e. r = (\u03b5T \u03b51) is the doubling method, T is equal to the number of doubling steps until it is exceeded."}, {"heading": "B.2 Additional results", "text": "Figure 2 shows the empirical accuracy of the starting hypotheses to ensure that the algorithms achieve their theoretical guarantees. In fact, they perform significantly better, which is reasonable given the private testing methodology: set a threshold well below the goal \u03b1, add independent noise to each query, and only accept this if the query plus noise is lower than the threshold. Combined with the requirement to use limits, the accuracy tends to be significantly lower than \u03b1 and with a significantly higher probability than 1 \u2212 \u03b3. (Remember: This is not necessarily a good thing, since it probably costs a significant amount of additional privacy.) Figure 3 shows the breakdown of privacy losses between the \"privacy test\" and the \"hypotheses generator.\" In the case of NoiseReduction, this is an exceeding threshold \u03b5A, and determining the private method, covariance disturbance, or output permission is clearly a hypothesis based on a duplication of the private hypothesis."}, {"heading": "B.3 Supporting theory", "text": "Claim B.1. In the \"duplication method,\" increasing the factor 2 in each step guarantees the optimal subsequent loss of privacy. Proof. Suppose that \u03b5 would be the \"final\" level of privacy at which the algorithm would come to a standstill. If the factor 1 / r were increased for r < 1, the final loss could be as large as \u03b5% / r. The total loss is the sum of this loss and all previous losses, i.e., if t-steps were taken: (\u03b5 / r) + r \u00b7 (\u03b5 / r) + \u00b7 + rt \u2212 1 \u00b7 (\u03b5 \u0445 / r) = (\u03b5 \u043a / r) t \u2212 1 \u0445 j = 0 rj \u2192 (\u03b5 \u0445 / r). The final inequality implies that setting r = 0.5 and (1 / r) = 2 is optimal. The asymptotic \u2192 is justified by noting that the initial factor 1 can be chosen arbitrarily so that there are no parameters exceeding the 9."}], "references": [{"title": "Private empirical risk minimization, revisited", "author": ["Raef Bassily", "Adam D. Smith", "Abhradeep Thakurta"], "venue": "CoRR, abs/1405.7085,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Privacy-preserving logistic regression", "author": ["Kamalika Chaudhuri", "Claire Monteleoni"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Differentially private empirical risk minimization", "author": ["Kamalika Chaudhuri", "Claire Monteleoni", "Anand D. Sarwate"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Local privacy and statistical minimax rates", "author": ["John C. Duchi", "Michael I. Jordan", "Martin J. Wainwright"], "venue": "In 51st Annual Allerton Conference on Communication, Control, and Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith"], "venue": "In Theory of Cryptography Conference,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "The algorithmic foundations of differential privacy", "author": ["Cynthia Dwork", "Aaron Roth"], "venue": "Foundations and Trends\u00ae in Theoretical Computer Science,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Boosting and differential privacy", "author": ["Cynthia Dwork", "Guy N Rothblum", "Salil Vadhan"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Building a rappor with the unknown: Privacypreserving learning of associations and data dictionaries", "author": ["Giulia Fanti", "Vasyl Pihur", "\u00dalfar Erlingsson"], "venue": "Proceedings on Privacy Enhancing Technologies (PoPETS),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Apple\u2019s \u2019differential privacy\u2019 is about collecting your data\u2014but not your data", "author": ["Andy Greenberg"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Differentially private online learning", "author": ["Prateek Jain", "Pravesh Kothari", "Abhradeep Thakurta"], "venue": "In COLT 2012 - The 25th Annual Conference on Learning Theory, June", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Private convex optimization for empirical risk minimization with applications to high-dimensional regression", "author": ["Daniel Kifer", "Adam D. Smith", "Abhradeep Thakurta"], "venue": "In COLT 2012 - The 25th Annual Conference on Learning Theory, June 25-27,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Gradual release of sensitive data under differential privacy", "author": ["Fragkiskos Koufogiannis", "Shuo Han", "George J. Pappas"], "venue": "Journal of Privacy and Confidentiality,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Mechanism design via differential privacy", "author": ["Frank McSherry", "Kunal Talwar"], "venue": "In Foundations of Computer Science,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Privacy odometers and filters: Pay-as-you-go composition", "author": ["Ryan M Rogers", "Aaron Roth", "Jonathan Ullman", "Salil Vadhan"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Learning in a large function space: Privacy-preserving mechanisms for SVM learning", "author": ["Benjamin I.P. Rubinstein", "Peter L. Bartlett", "Ling Huang", "Nina Taft"], "venue": "CoRR, abs/0911.5708,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Is interaction necessary for distributed private learning", "author": ["Adam Smith", "Jalaj Upadhyay", "Abhradeep Thakurta"], "venue": "IEEE Symposium on Security and Privacy,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Stochastic gradient descent with differentially private updates", "author": ["Shuang Song", "Kamalika Chaudhuri", "Anand D. Sarwate"], "venue": "In IEEE Global Conference on Signal and Information Processing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": "Differential Privacy [6, 7] enjoys over a decade of study as a theoretical construct, and a much more recent set of large-scale practical deployments, including by Google [9] and Apple [10].", "startOffset": 21, "endOffset": 27}, {"referenceID": 5, "context": "Differential Privacy [6, 7] enjoys over a decade of study as a theoretical construct, and a much more recent set of large-scale practical deployments, including by Google [9] and Apple [10].", "startOffset": 21, "endOffset": 27}, {"referenceID": 7, "context": "Differential Privacy [6, 7] enjoys over a decade of study as a theoretical construct, and a much more recent set of large-scale practical deployments, including by Google [9] and Apple [10].", "startOffset": 171, "endOffset": 174}, {"referenceID": 8, "context": "Differential Privacy [6, 7] enjoys over a decade of study as a theoretical construct, and a much more recent set of large-scale practical deployments, including by Google [9] and Apple [10].", "startOffset": 185, "endOffset": 189}, {"referenceID": 1, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 72, "endOffset": 86}, {"referenceID": 2, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 72, "endOffset": 86}, {"referenceID": 10, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 72, "endOffset": 86}, {"referenceID": 14, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 72, "endOffset": 86}, {"referenceID": 15, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 112, "endOffset": 116}, {"referenceID": 0, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 144, "endOffset": 151}, {"referenceID": 12, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 144, "endOffset": 151}, {"referenceID": 0, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 185, "endOffset": 203}, {"referenceID": 3, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 185, "endOffset": 203}, {"referenceID": 9, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 185, "endOffset": 203}, {"referenceID": 16, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 185, "endOffset": 203}, {"referenceID": 11, "context": "One key technique that saves significant factors in privacy loss over naive search is the use of correlated noise generated by the method of [14], which formalizes the conceptual idea of \u201csubtracting\u201d noise without incurring additional privacy overhead.", "startOffset": 141, "endOffset": 145}, {"referenceID": 5, "context": "In order to select the most private of these queries that meets the accuracy requirement, we introduce a natural modification of the now-classic AboveThreshold algorithm [7], which iteratively checks a sequence of queries on a dataset and privately releases the index of the first to approximately exceed some fixed threshold.", "startOffset": 170, "endOffset": 173}, {"referenceID": 11, "context": "When combined with the above-mentioned correlated noise technique of [14], this gives an algorithm whose privacy loss is equal to that of the final hypothesis output \u2013 the previous ones coming \u201cfor free\u201d \u2013 plus the privacy loss of AboveThreshold.", "startOffset": 69, "endOffset": 73}, {"referenceID": 13, "context": "we introduce and apply a new, corresponding privacy notion, which we term ex-post privacy, and which is closely related to the recently introduced notion of \u201cprivacy odometers\u201d [16].", "startOffset": 177, "endOffset": 181}, {"referenceID": 2, "context": "This includes both direct applications of the Laplace mechanism, like output perturbation [4]; and more sophisticated methods like covariance perturbation [18], which perturbs the covariance matrix of the data and then performs an optimization using the noisy data.", "startOffset": 90, "endOffset": 93}, {"referenceID": 15, "context": "This includes both direct applications of the Laplace mechanism, like output perturbation [4]; and more sophisticated methods like covariance perturbation [18], which perturbs the covariance matrix of the data and then performs an optimization using the noisy data.", "startOffset": 155, "endOffset": 159}, {"referenceID": 4, "context": "1 (Differential Privacy [6]).", "startOffset": 24, "endOffset": 27}, {"referenceID": 13, "context": "[16] initiated the study of privacy odometers, which formalize this idea.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "4 (Post Processing [6]).", "startOffset": 19, "endOffset": 22}, {"referenceID": 4, "context": "5 (Composition [6]).", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "The composition theorem holds even if the composition is adaptive\u2014-see [8] for details.", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "6 ([6]).", "startOffset": 3, "endOffset": 6}, {"referenceID": 11, "context": "[14] study how to gradually release private data using the Laplace mechanism with an increasing sequence of \u03b5 values, with a privacy cost scaling only with the privacy of the marginal distribution on the least private release, rather than the sum of the privacy costs of independent releases.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Algorithm 1 Noise Reduction [14]: NR(v,\u2206, {\u03b5t}) Input: private vector v, sensitivity parameter \u2206, list \u03b51 < \u03b52 < \u00b7 \u00b7 \u00b7 < \u03b5T Set v\u0302T := v + Lap(\u2206/\u03b5T ) .", "startOffset": 28, "endOffset": 32}, {"referenceID": 11, "context": "7 ([14]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "\u201d The classical AboveThreshold algorithm [7] takes in a dataset and a sequence of queries and privately outputs the index of the first query to exceed a given threshold (with some error due to noise).", "startOffset": 41, "endOffset": 44}, {"referenceID": 5, "context": "The proof, which is a variant on the proof of privacy for AboveThreshold [7], appears in the appendix, along with an accuracy theorem for IAT.", "startOffset": 73, "endOffset": 76}, {"referenceID": 13, "context": ") 3This result does not follow from a straightforward application of privacy odometers from [16], because the privacy analysis of algorithms like the noise reduction technique is not compositional.", "startOffset": 92, "endOffset": 96}, {"referenceID": 11, "context": "We use Algorithm 1, the \u201cnoise reduction\u201d method of [14], for generating the sequence of hypotheses: we first compute a very private and noisy \u03b81, and then obtain the subsequent hypotheses by gradually \u201cde-noising\u201d \u03b81.", "startOffset": 52, "endOffset": 56}, {"referenceID": 15, "context": "In particular, our noise-reduction method is based on two private ERM algorithms: the recently introduced covariance perturbation technique of [18], and output perturbation [4].", "startOffset": 143, "endOffset": 147}, {"referenceID": 2, "context": "In particular, our noise-reduction method is based on two private ERM algorithms: the recently introduced covariance perturbation technique of [18], and output perturbation [4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 15, "context": "Before we formally introduce the covariance perturbation algorithm due to [18], observe that the optimal solution \u03b8\u2217 can be computed as \u03b8\u2217 = argmin \u03b8\u2208C L(\u03b8,D) = argmin \u03b8\u2208C (\u03b8T(XTX)\u03b8 \u2212 2\u3008XTy,\u03b8\u3009) 2n + \u03bb\u2016\u03b8\u20162 2 .", "startOffset": 74, "endOffset": 78}, {"referenceID": 15, "context": "Our analysis differs from the one in [18] in that their paper considers the \u201clocal privacy\u201d setting, and also adds Gaussian noise whereas we use Laplace.", "startOffset": 37, "endOffset": 41}, {"referenceID": 2, "context": "Our analysis deviates slightly from the one in [4] since we are adding Laplace noise (see the appendix).", "startOffset": 47, "endOffset": 50}, {"referenceID": 0, "context": "[2] Raef Bassily, Adam D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3] Kamalika Chaudhuri and Claire Monteleoni.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] Kamalika Chaudhuri, Claire Monteleoni, and Anand D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] John C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] Cynthia Dwork and Aaron Roth.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] Cynthia Dwork, Guy N Rothblum, and Salil Vadhan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] Giulia Fanti, Vasyl Pihur, and \u00dalfar Erlingsson.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] Andy Greenberg.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] Prateek Jain, Pravesh Kothari, and Abhradeep Thakurta.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13] Daniel Kifer, Adam D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] Fragkiskos Koufogiannis, Shuo Han, and George J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] Frank McSherry and Kunal Talwar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] Ryan M Rogers, Aaron Roth, Jonathan Ullman, and Salil Vadhan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] Benjamin I.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] Adam Smith, Jalaj Upadhyay, and Abhradeep Thakurta.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] Shuang Song, Kamalika Chaudhuri, and Anand D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "6 in [16] then says that the ex-post privacy loss of outputting k \u2264 T is \u2211k i=1[\u03b51 \u2217 2k\u22121 + 2\u2206 log(T /\u03b3) \u03b1 ] = 2k\u2206 log(T /\u03b3) \u03b1 + (2 k \u2212 1)\u03b51, as desired.", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "By Corollary 8 of [4], we can bound", "startOffset": 18, "endOffset": 21}], "year": 2017, "abstractText": "Traditional approaches to differential privacy assume a fixed privacy requirement \u03b5 for a computation, and attempt to maximize the accuracy of the computation subject to the privacy constraint. As differential privacy is increasingly deployed in practical settings, it may often be that there is instead a fixed accuracy requirement for a given computation and the data analyst would like to maximize the privacy of the computation subject to the accuracy constraint. This raises the question of how to find and run a maximally private empirical risk minimizer subject to a given accuracy requirement. We propose a general \u201cnoise reduction\u201d framework that can apply to a variety of private empirical risk minimization (ERM) algorithms, using them to \u201csearch\u201d the space of privacy levels to find the empirically strongest one that meets the accuracy constraint, incurring only logarithmic overhead in the number of privacy levels searched. The privacy analysis of our algorithm leads naturally to a version of differential privacy where the privacy parameters are dependent on the data, which we term ex-post privacy, and which is related to the recently introduced notion of privacy odometers. We also give an ex-post privacy analysis of the classical AboveThreshold privacy tool, modifying it to allow for queries chosen depending on the database. Finally, we apply our approach to two common objectives, regularized linear and logistic regression, and empirically compare our noise reduction methods to (i) inverting the theoretical utility guarantees of standard private ERM algorithms and (ii) a stronger, empirical baseline based on binary search.", "creator": "LaTeX with hyperref package"}}}