{"id": "1705.10886", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "High Dimensional Structured Superposition Models", "abstract": "High dimensional superposition models characterize observations using parameters which can be written as a sum of multiple component parameters, each with its own structure, e.g., sum of low rank and sparse matrices, sum of sparse and rotated sparse vectors, etc. In this paper, we consider general superposition models which allow sum of any number of component parameters, and each component structure can be characterized by any norm. We present a simple estimator for such models, give a geometric condition under which the components can be accurately estimated, characterize sample complexity of the estimator, and give high probability non-asymptotic bounds on the componentwise estimation error. We use tools from empirical processes and generic chaining for the statistical analysis, and our results, which substantially generalize prior work on superposition models, are in terms of Gaussian widths of suitable sets.", "histories": [["v1", "Tue, 30 May 2017 23:00:34 GMT  (358kb,D)", "http://arxiv.org/abs/1705.10886v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["qilong gu", "arindam banerjee"], "accepted": true, "id": "1705.10886"}, "pdf": {"name": "1705.10886.pdf", "metadata": {"source": "CRF", "title": "High Dimensional Structured Superposition Models", "authors": ["Qilong Gu", "Arindam Banerjee"], "emails": ["guxxx396@cs.umn.edu", "banerjee@cs.umn.edu"], "sections": [{"heading": "1 Introduction", "text": "For high-dimensional structured estimation problems [7, 27] considerable progress has been made in accurately estimating a sparse or structured parameter + Rp, even if the sample size n is much smaller than the ambient dimensionality of imbalances, i.e., n p. Instead of a single structure, such as thrift or low rank, we have seen interest in parameter estimation in recent years if the parameter inequality is an overlay or sum of multiple different structures, i.e. we are essentially generalizing the non-asymptotic estimation errors for such overlay models (i), and so on [1, 8, 9, 11, 15, 16, 19, 31, 32]. In this paper, we will generalize the non-asymptotic error analysis for such overlay models so that (i) the parameters imbalances can be the superposition of any number of components."}, {"heading": "2 Error Structure and Recovery Guarantees", "text": "In this section, we will begin with some basic results and, under appropriate assumptions, provide a deterministic limit for the component estimation of errors in superposition models. In the following, we will show that the assumptions made here are held with high probability as long as a purely geometric, non-probable state characterized by structural coherence (SC) is fulfilled. Let me be the error vector for component i. Let me be the error vector for component i of superposition. Our goal is to provide a preliminary understanding of the structure of error sets under which an overall component is bound."}, {"heading": "3 Geometry of Structural Coherence", "text": "In this section we give a geometric characterization of structural coherence (SC) in (9). We start with the simplest case of two vectors x, y. If they are not reflections from each other, i.e. x 6 = \u2212 y, then the following relation applies: sentence 2 If there is such a case < 1 such that \u2212 < x, y > \u00b2 x \u00b2 and C2 \u2212 2, then the sum x + y \u00b2 2 is more than 1 \u2212 kowski 2 (both cases). (12) Next, we generalize the condition from sentence 2 to vectors in two different cones C1 and C2. Given the cones, the definition of 0 = sup x x x, C1, Sp \u2212 1, y, C2 and C2, both of which are not structural. (12) We generalize the condition of sentence 2 to vectors in two different cones C1 and C2."}, {"heading": "4 Restricted Eigenvalue Condition for Superposition Models", "text": "We assume that the target situation is satisfied by the error constellation, i = 1,.., k, in this section we show that the general RE condition in (6) is highly likely to be met if the number of samples n in the Sub-Gaussian design matrix X-Rn \u00b7 p exceeds the sample complexity n0. We start with a restricted eigenvalue condition to H. For a random vector Z-Rp, we define the marginal tail function for an arbitrary Set-E asQi (E; Z) = inf u-EP (< Z), u > | p-e."}, {"heading": "5 General Error Bound", "text": "Remember that the error limit in Theorem 1 with respect to noise design (ND) with respect to interactions (IL) = inf s > 0 {s: supu \u00b2 sC1 + + 0. (24) In this section, we will give a characterization of the ND interaction that yields the final error rate as long as the error rate is n \u00b2 n \u00b2 -0, i.e., the sample complexity is met. (24) Let us be a centered sub-Gaussian vector, and its sub-Gaussian norm that yields the final error limit. (Let X be a set of smart i.e. Sub-Gaussian random matrix, for each row Z, its sub-Gaussian norm | | | Z-2 \u2264 x. The ND interaction can be limited by the following conclusion, and the proof of problem 3 is given in appendix."}, {"heading": "6 Accelerated Proximal Algorithm", "text": "In this section, we propose a general-purpose algorithm to solve the problem (2). For convenience, we specify f (\u03b8) = f (\u03b8) = f (\u2211 k i = 1 \u03b8i) = i \u2212 X\u03b8 (22) and i = {\u03b8i | Ri (\u03b8i) \u2264 Ri (\u03b8) = i). Although the standards Ri (.) may not be smooth, a general algorithm can be designed as long as the probable operators i (v) = argminu-i-u \u2212 v \u00b2 2 can be efficiently calculated for each set of i. The algorithm is simply the proximal gradient method [23], in which each component \u0445i is updated cyclically in each iteration (see algorithm 1): It does not determine + 1i = argmin \u0445i < vice versa, if we do not use it. \u2212 The algorithm is simply the proximal gradient method [23], in which each component is updated in each algorithm (see algorithm 1)."}, {"heading": "7 Noiseless Case: Comparing Estimators", "text": "In this section we present a comparative analysis of the estimators. (c) The estimators (29) must essentially show that the two estimators have similar recovery conditions, but the existing estimator (29) needs additional structure for the unique decomposition of the components. (29) The estimator (29) must consider the so-called \"infimal convolution\" (25, 32) across different norms in order to obtain a (unique) decomposition of the components. (30) The denoteR (29) must take into account the so-called \"infimal convolution\" (25, 32). (30) The results in [25] show that (30) the resolution of the components (29) can be rewritten."}, {"heading": "8 Related Work", "text": "Structured superposition models have been studied in recent literature. [32] Consider the superposition of structures when k = 2 and noise \u03c9 = 0, and assume specific structures such as sparse + sparse [14] and low + sparse [11]. [16] analyze errors associated with low and sparse matrix decomposition with noise. [15] propose an estimate for the decomposition of two generalized structured matrices, while one of them exhibits random rotation. [22] analyze the decomposition of a low matrix plus another matrix with generalized structure. [15] We propose an estimate for the decomposition of two generalized structured matrices, while one of them exhibits random rotation."}, {"heading": "9 Application of General Bound", "text": "In this section we explain the general error limits of the Morphological Component Analysis (MCA) as well as the low and sparse decomposition of the matrix. The evidence for this can be found in Appendix E."}, {"heading": "9.1 Morphological Component Analysis Using l1 Norm", "text": "In the morphological component-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector"}, {"heading": "9.3 Low-rank and Sparse Matrix Decomposition", "text": "If we have k > 1, the framework in [32] is not applicable, because the k support standard is non-decomposable. If k = 1, then the component standards are simplified in terms of sparseness to the value 1. Suppose we have a rank-r matrix L * and a sparse matrix S * with s-disparate entries, S *, L * Rd1 \u00b7 d2. Our observation Y results from the following problem Yi = < Xi, L * + S * > + Ei, i = 1,."}, {"heading": "10 Experimental Results", "text": "In this section, we confirm the theoretical results of this work with some simple experiments. We show our experimental results under different conditions. In our experiments, we focus on MCA when k = 2. The design matrixX are generated from the Gaussian distribution, making each entry of X subjects to N (0, 1). The noise \u03c9 is generated from the Gaussian distribution, making each entry of II subjects to N (0, 1). We implement our algorithm 1 in MATLAB. We use synthetic data in all our experiments and leave the true signal horizons 1 = (1,.., 1 s1, 0.., 0), QPhenomen2 = (1,.. s2, 0.. We generate our data in different ways for our three experiments."}, {"heading": "10.1 Recovery From Noisy Observation", "text": "In our first experiment, we test the effects of \u03c1 on the estimation error. We select three different matrices Q, and \u03c1 is given by the choice of Q. The first Q is given by random sample: we sample a random matrix Q, so that Qij > 0 and \u03c1 are undercut by (39). The second and third Q is given by identity matrix I and its negative \u2212 I; therefore, we are equal to 1 / 2 or 0. We select the dimension p = 1000 and leave s1 = s2 = 1. The number of samples n varied between 1 and 1000. The observation time is given by y = X (1 + 1) +. In this experiment, we generate for each n 100 pairs of X and W. For each (X, w) pair, we obtain a solution that lies between 1 and 2."}, {"heading": "10.2 Recovery From Noiseless Observation", "text": "In our second experiment, we test how the dimension p affects the successful restoration of the true value. In this experiment, we select different dimensions p with p = 20, p = 40, p = 80 and p = 160. We leave s1 = s2 = 1. To avoid the effects of \u03c1, we try 100 random orthogonal matrices Q. Observation y results from y = X (\u03b8 1 + \u03b8 2). We increase n from 1 to 40, and the graph we get is Figure 5. From Figure 5, we can find that the sample complexity required to regenerate increases between 1 and 2%, with the dimension p.10,3 recovery using the k-assisting normality. In this experiment, we solve the problem (Q \u00b7 2) and leave the transformation of 1 \u00b7 and 2 \u00b7 matrix."}, {"heading": "11 Conclusions", "text": "We present a simple estimator for general superimposition models and give a purely geometric characterization based on structural coherence when an accurate estimate of each component is possible. Furthermore, we determine the sample complexity of the estimator and upper limits on component-related estimation errors, and interestingly show that both depend on the widest Gaussian width among the spherical caps caused by the error cones that meet component standards. In the future, it will be interesting to study specific component structures that satisfy structural coherence, and also to expand our results to allow for more general measurement models.Recognition: The research has also been supported by NSF grants IIS-1563950, IIS-1447566, IIS1447574, IIS-1422557, CCF-1451986, CNS-1314560, IIS-1029711, NASA grants XQIS, IdoABS, Ido137BS and Yahoo-A27511."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Proof of Theorem 1", "text": "Theorem 11 (Deterministic limit) Suppose that the RE condition in (6) in C is met with the parameter \u0430. If we then have \"k\" = 1 \"i\" = 1 \"i\" 2 \"6\" sn \"(). Proof: Through the feasibility of\" c \"and the optimality of\" c \"we have\" Y \"-X\" 22 \"Y\" -X. \"22\" c. \"22\" c. \"22\" c. \"22\" c. \"22\" c. \"22\" c. \"22\" c. \"22\" c. \"22\" c. \"TX.\" 22 \"c.\" 6 \"c.\" (42) For all others \"c\" i. \""}, {"heading": "B Geometry of Structural Coherence", "text": "In this section, our goal is to characterize the geometric property of our SC condition. We assume a simple case if k = 2."}, {"heading": "B.1 Proof of Lemma 2", "text": "Lemma 6 If there is such a size that \u2212 < x, y > \u2264 \u043c, x, 2, 2, 2, 2, then there is such a size. (44) Proof: We know from [18] that a size of x + y, 22, (1) (x x x x, 22 +, y, 22) and (x, 2 +, y, 2) 2, 2, 2 (x, 22 +, y, 22) are combined, and we will come to the conclusion."}, {"heading": "B.2 Proof of Theorem 3", "text": "Theorem 12 (Structural Coherence (SC) Condition) Let me say it: = maxi \u03b4i with \u03b4i as defined in (14). If \u03b4 < 1 exists, so that for each,,, i \"Ci,,..., k, the condition of SC in (9) applies, i.e., the condition of SC in (9) applies:,,, i\" i \"i = 1\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i."}, {"heading": "C Restricted Eigenvalue Condition", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C.1 Proof of Lemma 2", "text": "Lemma 7 Let the sentences C and A be defined as in (7) and (19) respectively. (46) If the SC condition in (9) applies, then the boundary tail functions of the two sentences have the following relationship: Qi (H; Z) \u2265 Qi (A; Z). (46) Proof: By definition, we can ui-Ci, i = 1, 2,..., k and k i = 1 ui = u. Then | < Z, u > | = 1 ui-Z = 1 ui-Z, u-Z = 1 ui-2,."}, {"heading": "C.2 Proof of Theorem 4", "text": "Theorem 13 (Restricted Eigenvalue Condition) Let X be the sub-Gaussian design matrix that meets the above assumptions. If the SC condition (9) applies with \u03c1 > 0, then with a probability of at least 1 \u2212 exp (\u2212 t2 / 2), we have defined two sets C and A as they were previously defined; from Lemma (1) and lemma (2) we know that for each set of cases > 0, with a probability of at least 1 \u2212 e \u2212 t2 / 2inf and c3 positive constants are defined by \u03c3x, \u03c3\u043d and \u03b1.Proof: Let two sets C and A be defined as they were previously defined; from Lemma (1) and lemma (2) we know that for each individual group > 0, with a probability of at least 1 \u2212 e \u2212 t2 / 2inf and so on."}, {"heading": "C.3 Proof of Proposition 5", "text": "Sentence 14 If there is a matrix X, so that condition (6) applies to \"i-Ci,\" then SC (9) applies. Proof: If there is one, then there are some \"i-i-Ci,\" \"i-Ci,\" \"..., k not all zero,\" so that \"i-i-i-ki\" = 1 \"i-i\" 2 = 0, \"\" i-i \"= 0,\" meaning that \"X-i-ki\" = 1 \"i-2 = 0 applies to each X. That is a contradiction."}, {"heading": "D Error Bound", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D.1 Proof of Lemma 3", "text": "Let's design X-Rn-p so that there are a number of ways we can do it. Let's just do it. Let's do it the way we want to do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it. Let's do it."}, {"heading": "D.2 Proof of Lemma 4", "text": "Lemma 9 (Gaussian width limited) Let H and H be defined as in (7) and (8), respectively, then we have w (H) = O (maxiw (Ci-Sp \u2212 1) + \u221a log k) and w (H-Sp) = O (maxiw (Ci-Bp) + \u221a log k). Proof: By definition and by reason of the fact that the Gaussian width of the convex hull of the propositions corresponds to the Gaussian width of their union [10] w (H) = E-Sp u-H < u, g > = Emax i-Sp-1 < ui, g > \u2265 E-Sp-1 < ui-Sp-1 < ui-1 < sup-H < u, g > Emax-Sp-1 < ui, g > \u2265 E-Sp-2-p-p."}, {"heading": "D.3 Proof of Theorem 6", "text": "Theorem 15 For estimators (3), leave Ci = = = cone (6x2), leave Ci = = cone (6x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), x2, (2), (2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2),"}, {"heading": "E Examples", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "E.1 Structural Coherence For 1-sparse + 1-sparse MCA", "text": "Suggestion 16 Assume that both vector-sign and vector-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-sign-"}, {"heading": "E.2 Proof of Theorem 8", "text": "Theorem 17: If M \u2264 18 \u221a s1s2, then for problem (38) with high probability that it is a fault, namely a problem (1 = 2 + 2 = 2 = O (max {s1 log pn, \u221a s2 log pn}). Proof: If i = 2 = 1, 2, then we have < 1, 2 > = < 1, QTQ 2 > \u2264 max ij | Qij | 1 = 1 = 1 = 1 \u2264 1 = 1 \u2264 1 + 1% P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P..P.P.....P.P..P.P..P.P.P....P...P.P.P....P...P.P.P..P.P.P.P.P....P.P.P...P..P.P.P.P....P.P......P.P.P..P...P....P......P......P.........P.....P......P.P....P.......P......P.....P.....................P................P...."}, {"heading": "E.3 Proof of Theorem 9", "text": "Theorem 18: If we want to solve the problem (40) with a high probability, we must first describe the interaction between the cones: < 1, 2, 2, 2 > < 1, QTQ 2 >.because the k-shaped standard is an atomic standard and its atomic quantity is all k-sparse vectors. Result: We can first describe the interaction between the cones: < 1, 2 > = < 1, QTQ 2 >.because the k-shaped standard is an atomic standard and its atomic quantity is all k-sparse vectors."}, {"heading": "E.4 Proof of Theorem 10", "text": "Theorem 19 If there is a \u03c1 > 0 for problem (41), then with high probability. Proof: From [10] we know that for the error cone the d1 \u00b7 d2 rank-r matrix is its Gaussian width O (r (d1 + d2 \u2212 r). Therefore, the error limit when applying theorem 6, if it is reversed, is the limit between L \u2212 L * 2 + \u0445S * 2 = O (max {s log (d1d2 \u2212 s) n, \u221a r (d1 + d2 \u2212 r) n})."}, {"heading": "E.5 Additional Example for Low-rank ans Sparse Matrix Decomposition", "text": "Example 1 Suppose that noise \u03c9 = 0, S0 = L0 = 1 0. 0 0. 0 0. 0........ 0.. 0.. 0, (54) and M = S0 + L0, then the SC state of the problem (41) applies. Proof: Suppose the singular value of the decomposition of L * 0 is UVT. DenoteC \"S = cl.\" From [18] we know that SC is equivalent to \u2212 C \"S\" C \"L = {0}. To prove \u2212 C\" S \"C\" L = {0}, we need the following inequalities: < character (S \"0), < character (S\" 0), < character (S \"C\" C \"L = {0}."}, {"heading": "F Noiseless Case: Comparing Estimators", "text": "In this section, we will try to examine the structures that differ between problem (2) and problem (29), and the structures that they have in common."}, {"heading": "F.1 Proof of Lemma 5", "text": "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1"}, {"heading": "F.2 Proof of theorem 7", "text": "s a solution. Theorem 20 Given the fact that there is no solution, this is a solution. Theorem 20 Given the fact that there is no solution, this is a solution. square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square."}, {"heading": "F.3 Proof of Lemma 11", "text": "The proof: Without loss of universality, it is to be assumed that it is a unique decomposition, namely for each individual \"i\" Ci, if \"i\" i = 1 c \"i = 1 c\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i) i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i"}], "references": [{"title": "Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions", "author": ["Alekh Agarwal", "Sahand Negahban", "Martin J. Wainwright"], "venue": "The Annals of Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Sparse Prediction with the k-Support Norm", "author": ["Andreas Argyriou", "Rina Foygel", "Nathan Srebro"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Estimation with Norm Regularization", "author": ["Arindam Banerjee", "Sheng Chen", "Farideh Fazayeli", "Vidyashankar Sivakumar"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["Amir Beck", "Marc Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Simultaneous analysis of Lasso and Dantzig selector", "author": ["Peter J Bickel", "Yaacov Ritov", "Alexandre B Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Concentration Inequalities", "author": ["St\u00e9phane Boucheron", "G\u00e1bor Lugosi", "Pascal Massart"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Statistics for High Dimensional Data: Methods, Theory and Applications", "author": ["Peter Buhlmann", "Sara van de Geer"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Robust principal component analysis", "author": ["Emmanuel J. Cand\u00e8s", "Xiaodong Li", "Yi Ma", "John Wright"], "venue": "J. ACM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Latent variable graphical model selection via convex optimization", "author": ["Venkat Chandrasekaran", "Pablo A. Parrilo", "Alan S. Willsky"], "venue": "The Annals of Statistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1935}, {"title": "The Convex Geometry of Linear Inverse Problems", "author": ["Venkat Chandrasekaran", "Benjamin Recht", "Pablo A. Parrilo", "Alan S. Willsky"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Rank-Sparsity Incoherence for Matrix Decomposition", "author": ["Venkat Chandrasekaran", "Sujay Sanghavi", "Pablo a. Parrilo", "Alan S. Willsky"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Generalized dantzig selector: Application to the k-support norm", "author": ["Soumyadeep Chatterjee", "Sheng Chen", "Arindam Banerjee"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Structured estimation with atomic norms: General bounds and applications", "author": ["Sheng Chen", "Arindam Banerjee"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Uncertainty principles and ideal atomic decomposition", "author": ["David L. Donoho", "Xiaoming Huo"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Corrupted Sensing: Novel Guarantees for Separating Structured Signals", "author": ["R Foygel", "L Mackey"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Robust matrix decomposition with sparse corruptions", "author": ["Daniel Hsu", "Sham M Kakade", "Tong Zhang"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Probability in Banach Spaces: Isoperimetry and Processes, volume 23 of A Series of Modern Surveys in Mathematics", "author": ["Michel Ledoux", "Michel Talagrand"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "The achievable performance of convex demixing", "author": ["M B McCoy", "J A Tropp"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "A geometric analysis of convex demixing", "author": ["Michael B McCoy"], "venue": "Ph.D. Thesis, Caltech,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Learning without concentration", "author": ["Shahar Mendelson"], "venue": "J. ACM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "A Unified Framework for High-Dimensional Analysis ofM -Estimators with Decomposable Regularizers", "author": ["Sahand N Negahban", "Pradeep Ravikumar", "Martin J Wainwright", "Bin Yu"], "venue": "Statistical Science,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Sharp Time\u2013Data Tradeoffs for Linear Inverse Problems", "author": ["Samet Oymak", "Benjamin Recht", "Mahdi Soltanolkotabi"], "venue": "ArXiv e-prints,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Restricted Eigenvalue Properties for Correlated Gaussian Designs", "author": ["Garvesh Raskutti", "Martin J Wainwright", "Bin Yu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Convex Analysis", "author": ["R. Tyrrell Rockafellar"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1970}, {"title": "Upper and Lower Bounds for Stochastic Processes. A Series of Modern Surveys in Mathematics", "author": ["Michel Talagrand"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Regression Shrinkage and Selection via the Lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1996}, {"title": "Convex recovery of a structured signal from independent random linear measurements", "author": ["Joel A Tropp"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Roman Vershynin"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Estimation in high dimensions: a geometric perspective", "author": ["Roman Vershynin"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Compressive principal component pursuit", "author": ["John Wright", "Arvind Ganesh", "Kerui Min", "Yi Ma"], "venue": "IEEE International Symposium on Information Theory,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Dirty statistical models", "author": ["Eunho Yang", "Pradeep Ravikumar"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "For high-dimensional structured estimation problems [7, 27], considerable advances have been made in accurately estimating a sparse or structured parameter \u03b8 \u2208 Rp even when the sample size n is far smaller than the ambient dimensionality of \u03b8\u2217, i.", "startOffset": 52, "endOffset": 59}, {"referenceID": 25, "context": "For high-dimensional structured estimation problems [7, 27], considerable advances have been made in accurately estimating a sparse or structured parameter \u03b8 \u2208 Rp even when the sample size n is far smaller than the ambient dimensionality of \u03b8\u2217, i.", "startOffset": 52, "endOffset": 59}, {"referenceID": 0, "context": ", \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].", "startOffset": 81, "endOffset": 118}, {"referenceID": 7, "context": ", \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].", "startOffset": 81, "endOffset": 118}, {"referenceID": 8, "context": ", \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].", "startOffset": 81, "endOffset": 118}, {"referenceID": 10, "context": ", \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].", "startOffset": 81, "endOffset": 118}, {"referenceID": 14, "context": ", \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].", "startOffset": 81, "endOffset": 118}, {"referenceID": 15, "context": ", \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].", "startOffset": 81, "endOffset": 118}, {"referenceID": 17, "context": ", \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].", "startOffset": 81, "endOffset": 118}, {"referenceID": 18, "context": ", \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].", "startOffset": 81, "endOffset": 118}, {"referenceID": 29, "context": ", \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].", "startOffset": 81, "endOffset": 118}, {"referenceID": 30, "context": ", \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].", "startOffset": 81, "endOffset": 118}, {"referenceID": 13, "context": "Popular models such as Morphological Component Analysis (MCA) [14] and Robust PCA [8, 11] can be viewed as a special cases of this framework (see Section 9).", "startOffset": 62, "endOffset": 66}, {"referenceID": 7, "context": "Popular models such as Morphological Component Analysis (MCA) [14] and Robust PCA [8, 11] can be viewed as a special cases of this framework (see Section 9).", "startOffset": 82, "endOffset": 89}, {"referenceID": 10, "context": "Popular models such as Morphological Component Analysis (MCA) [14] and Robust PCA [8, 11] can be viewed as a special cases of this framework (see Section 9).", "startOffset": 82, "endOffset": 89}, {"referenceID": 21, "context": "In this paper, we focus on the case where \u03b1i = Ri(\u03b8 i ), noting that recent advances [22] can be used to extend our results to more general settings.", "startOffset": 85, "endOffset": 89}, {"referenceID": 2, "context": "where n is the sample size, k is the number of components, and w(Ci\u2229Bp) is the Gaussian width [3, 10, 30] of the intersection of the error cone Ci with the unit Euclidean ball Bp \u2286 Rp.", "startOffset": 94, "endOffset": 105}, {"referenceID": 9, "context": "where n is the sample size, k is the number of components, and w(Ci\u2229Bp) is the Gaussian width [3, 10, 30] of the intersection of the error cone Ci with the unit Euclidean ball Bp \u2286 Rp.", "startOffset": 94, "endOffset": 105}, {"referenceID": 28, "context": "where n is the sample size, k is the number of components, and w(Ci\u2229Bp) is the Gaussian width [3, 10, 30] of the intersection of the error cone Ci with the unit Euclidean ball Bp \u2286 Rp.", "startOffset": 94, "endOffset": 105}, {"referenceID": 20, "context": "Interestingly, the estimation error converges at the rate of 1 \u221a n , similar to the case of single parameter estimators [21, 3], and depends only logarithmically on the number of components k.", "startOffset": 120, "endOffset": 127}, {"referenceID": 2, "context": "Interestingly, the estimation error converges at the rate of 1 \u221a n , similar to the case of single parameter estimators [21, 3], and depends only logarithmically on the number of components k.", "startOffset": 120, "endOffset": 127}, {"referenceID": 2, "context": "Further, while dependency of the error on Gaussian width of the error has been shown in recent results involving a single parameter [3, 30], the bound in (3) depends on the maximum of the Gaussian width of individual error cones, not their sum.", "startOffset": 132, "endOffset": 139}, {"referenceID": 28, "context": "Further, while dependency of the error on Gaussian width of the error has been shown in recent results involving a single parameter [3, 30], the bound in (3) depends on the maximum of the Gaussian width of individual error cones, not their sum.", "startOffset": 132, "endOffset": 139}, {"referenceID": 23, "context": "In Section 7, we compare an estimator using \u201cinfimal convolution\u201d[25] of norms with our estimator (2) for the noiseless case.", "startOffset": 65, "endOffset": 69}, {"referenceID": 4, "context": "The lower bound is a generalized form of the restricted eigenvalue (RE) condition studied in the literature [5, 7, 24].", "startOffset": 108, "endOffset": 118}, {"referenceID": 6, "context": "The lower bound is a generalized form of the restricted eigenvalue (RE) condition studied in the literature [5, 7, 24].", "startOffset": 108, "endOffset": 118}, {"referenceID": 22, "context": "The lower bound is a generalized form of the restricted eigenvalue (RE) condition studied in the literature [5, 7, 24].", "startOffset": 108, "endOffset": 118}, {"referenceID": 4, "context": "If k = 1, the RE condition in (6) above simplifies to the widely studied RE condition in the current literature on Lasso-type and Dantzig-type estimators [5, 24, 3] where only one error cone is involved.", "startOffset": 154, "endOffset": 164}, {"referenceID": 22, "context": "If k = 1, the RE condition in (6) above simplifies to the widely studied RE condition in the current literature on Lasso-type and Dantzig-type estimators [5, 24, 3] where only one error cone is involved.", "startOffset": 154, "endOffset": 164}, {"referenceID": 2, "context": "If k = 1, the RE condition in (6) above simplifies to the widely studied RE condition in the current literature on Lasso-type and Dantzig-type estimators [5, 24, 3] where only one error cone is involved.", "startOffset": 154, "endOffset": 164}, {"referenceID": 0, "context": "We also note that the general RE condition as explicitly stated in (6) has been implicitly used in [1] and [32].", "startOffset": 99, "endOffset": 102}, {"referenceID": 30, "context": "We also note that the general RE condition as explicitly stated in (6) has been implicitly used in [1] and [32].", "startOffset": 107, "endOffset": 111}, {"referenceID": 4, "context": "Since most existing literature on high-dimensional structured models focus on the k = 1 setting [5, 24, 3], there was no reason to study the SC condition carefully.", "startOffset": 96, "endOffset": 106}, {"referenceID": 22, "context": "Since most existing literature on high-dimensional structured models focus on the k = 1 setting [5, 24, 3], there was no reason to study the SC condition carefully.", "startOffset": 96, "endOffset": 106}, {"referenceID": 2, "context": "Since most existing literature on high-dimensional structured models focus on the k = 1 setting [5, 24, 3], there was no reason to study the SC condition carefully.", "startOffset": 96, "endOffset": 106}, {"referenceID": 17, "context": "In Section 3, we present a geometric characterization of the SC condition [18], and illustrate that the condition is both necessary and sufficient for accurate recovery of each component.", "startOffset": 74, "endOffset": 78}, {"referenceID": 2, "context": "and design X [3, 20].", "startOffset": 13, "endOffset": 20}, {"referenceID": 19, "context": "and design X [3, 20].", "startOffset": 13, "endOffset": 20}, {"referenceID": 17, "context": "In recent work, [18] concluded that if \u03b4i < 1 for each i = 1, .", "startOffset": 16, "endOffset": 20}, {"referenceID": 26, "context": "Our analysis is based on the results and techniques in [28, 20], and we note that [3] has related results using mildly different techniques.", "startOffset": 55, "endOffset": 63}, {"referenceID": 19, "context": "Our analysis is based on the results and techniques in [28, 20], and we note that [3] has related results using mildly different techniques.", "startOffset": 55, "endOffset": 63}, {"referenceID": 2, "context": "Our analysis is based on the results and techniques in [28, 20], and we note that [3] has related results using mildly different techniques.", "startOffset": 82, "endOffset": 85}, {"referenceID": 26, "context": "(19) From [28, 20], one can obtain a lower bound to Q\u03be(A;Z) based on the Paley-Zygmund inequality.", "startOffset": 10, "endOffset": 18}, {"referenceID": 19, "context": "(19) From [28, 20], one can obtain a lower bound to Q\u03be(A;Z) based on the Paley-Zygmund inequality.", "startOffset": 10, "endOffset": 18}, {"referenceID": 2, "context": "The Gaussian width [3] of E is defined as w(E) = E sup u\u2208E \u3008g, u\u3009.", "startOffset": 19, "endOffset": 22}, {"referenceID": 24, "context": "One way to upper bound the supremum of a stochastic process is by generic chaining [26, 3, 28], and by using generic chaining we can upper bound the stochastic process by a Gaussian process, which is the Gaussian width.", "startOffset": 83, "endOffset": 94}, {"referenceID": 2, "context": "One way to upper bound the supremum of a stochastic process is by generic chaining [26, 3, 28], and by using generic chaining we can upper bound the stochastic process by a Gaussian process, which is the Gaussian width.", "startOffset": 83, "endOffset": 94}, {"referenceID": 26, "context": "One way to upper bound the supremum of a stochastic process is by generic chaining [26, 3, 28], and by using generic chaining we can upper bound the stochastic process by a Gaussian process, which is the Gaussian width.", "startOffset": 83, "endOffset": 94}, {"referenceID": 27, "context": "Let X \u2208 Rn\u00d7p be a random matrix where each row is an independent copy of the sub-Gaussian random vector Z \u2208 Rp, and where Z has sub-Gaussian norm |||Z|||\u03c82 \u2264 \u03c3x [29].", "startOffset": 161, "endOffset": 165}, {"referenceID": 19, "context": "Let \u03b1 = infu\u2208Sp\u22121 E[|\u3008Z, u\u3009|] so that \u03b1 > 0 [20, 28].", "startOffset": 44, "endOffset": 52}, {"referenceID": 26, "context": "Let \u03b1 = infu\u2208Sp\u22121 E[|\u3008Z, u\u3009|] so that \u03b1 > 0 [20, 28].", "startOffset": 44, "endOffset": 52}, {"referenceID": 2, "context": "The result can be viewed as a direct generalization of existing results for k = 1, when the SC condition is always satisfied, and the sample complexity and error is given by w(C1 \u2229 Sp\u22121) and w(C1 \u2229Bp) [3, 10].", "startOffset": 201, "endOffset": 208}, {"referenceID": 9, "context": "The result can be viewed as a direct generalization of existing results for k = 1, when the SC condition is always satisfied, and the sample complexity and error is given by w(C1 \u2229 Sp\u22121) and w(C1 \u2229Bp) [3, 10].", "startOffset": 201, "endOffset": 208}, {"referenceID": 3, "context": "To determine a proper \u03b7t+1, we use a backtracking step [4].", "startOffset": 55, "endOffset": 58}, {"referenceID": 3, "context": "Based on existing results [4], the basic method can be accelerated by setting the starting point of the next iteration \u03b8 i as a proper combination of \u03b8\u0303 i and \u03b8 t i .", "startOffset": 26, "endOffset": 29}, {"referenceID": 3, "context": "By [4], one can use the updates: \u03b8 i = \u03b8\u0303 t+1 i + \u03b1t \u2212 1 \u03b1t+1 (\u03b8\u0303 i \u2212 \u03b8 t i) , where \u03b1t+1 = 1 + \u221a 1 + 4\u03b12 t 2 .", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "Convergence of Algorithm 1 has been studied in [4].", "startOffset": 47, "endOffset": 50}, {"referenceID": 3, "context": "The work [4] also give the convergence rate of Algorithm 1, which is O(1/t2).", "startOffset": 9, "endOffset": 12}, {"referenceID": 23, "context": "The estimator (29) needs to consider the so-called \u201cinfimal convolution\u201d [25, 32] over different norms to get a (unique) decomposition of \u03b8 in terms of the components {\u03b8\u0302i}.", "startOffset": 73, "endOffset": 81}, {"referenceID": 30, "context": "The estimator (29) needs to consider the so-called \u201cinfimal convolution\u201d [25, 32] over different norms to get a (unique) decomposition of \u03b8 in terms of the components {\u03b8\u0302i}.", "startOffset": 73, "endOffset": 81}, {"referenceID": 23, "context": "Results in [25] show that (30) is also a norm.", "startOffset": 11, "endOffset": 15}, {"referenceID": 9, "context": "The problem (31) is a simple structured recovery problem, and is well studied [10, 28].", "startOffset": 78, "endOffset": 86}, {"referenceID": 26, "context": "The problem (31) is a simple structured recovery problem, and is well studied [10, 28].", "startOffset": 78, "endOffset": 86}, {"referenceID": 13, "context": "Early work focus on the case when k=2 and noise \u03c9 = 0, and assume specific structures such as sparse+sparse [14], and low-rank+sparse [11].", "startOffset": 108, "endOffset": 112}, {"referenceID": 10, "context": "Early work focus on the case when k=2 and noise \u03c9 = 0, and assume specific structures such as sparse+sparse [14], and low-rank+sparse [11].", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "[16] analyze error bound for low-rank and sparse matrix decomposition with noise.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] analyze the decomposition of a low-rank matrix plus another matrix with generalized structure.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[15] propose an estimator for the decomposition of two generalize structured matrices, while one of them has a random rotation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "In [31], the authors generalize the noiseless matrix decomposition problem to arbitrary number of superposition under random orthogonal measurement.", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "[32] consider the superposition of structures of structures captured by decomposable norm, while [18] consider general norms but with a different measurement model, involving componentwise random rotations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[32] consider the superposition of structures of structures captured by decomposable norm, while [18] consider general norms but with a different measurement model, involving componentwise random rotations.", "startOffset": 97, "endOffset": 101}, {"referenceID": 30, "context": "[32] consider a general framework for superposition model, and give a high-probability bound for the fol-", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] consider an estimator like (2), which is", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "1 Morphological Component Analysis Using l1 Norm In Morphological Component Analysis [14], we consider the following linear model y = X(\u03b8\u2217 1 + \u03b8 \u2217 2) + \u03c9, where vector \u03b81 is sparse and vector \u03b82 is sparse under a rotation Q.", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": "In [14], the authors introduced a quantity M = max i,j |Qij |.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "2 Morphological Component Analysis Using k-support Norm k-support norm [2] is another way to induce sparse solution instead of l1 norm.", "startOffset": 71, "endOffset": 74}, {"referenceID": 1, "context": "Recent works [2, 12] have shown that k-support norm has better statistical guarantee than l1 norm.", "startOffset": 13, "endOffset": 20}, {"referenceID": 11, "context": "Recent works [2, 12] have shown that k-support norm has better statistical guarantee than l1 norm.", "startOffset": 13, "endOffset": 20}, {"referenceID": 30, "context": "Therefore we can not apply the framework of [32] for this problem.", "startOffset": 44, "endOffset": 48}, {"referenceID": 7, "context": "3 Low-rank and Sparse Matrix Decomposition To recover a sparse matrix and low-rank matrix from their sum [8, 11], one can use k-support norm [2] to induce sparsity and nuclear norm to induce low-rank.", "startOffset": 105, "endOffset": 112}, {"referenceID": 10, "context": "3 Low-rank and Sparse Matrix Decomposition To recover a sparse matrix and low-rank matrix from their sum [8, 11], one can use k-support norm [2] to induce sparsity and nuclear norm to induce low-rank.", "startOffset": 105, "endOffset": 112}, {"referenceID": 1, "context": "3 Low-rank and Sparse Matrix Decomposition To recover a sparse matrix and low-rank matrix from their sum [8, 11], one can use k-support norm [2] to induce sparsity and nuclear norm to induce low-rank.", "startOffset": 141, "endOffset": 144}, {"referenceID": 30, "context": "If we have k > 1, the framework in [32] is not applicable, because k-support norm is not decomposable.", "startOffset": 35, "endOffset": 39}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "We set the matrix Q to be a p \u00d7 p discrete cosine transformation (DCT) matrix [14].", "startOffset": 78, "endOffset": 82}], "year": 2017, "abstractText": "High dimensional superposition models characterize observations using parameters which can be written as a sum of multiple component parameters, each with its own structure, e.g., sum of low rank and sparse matrices, sum of sparse and rotated sparse vectors, etc. In this paper, we consider general superposition models which allow sum of any number of component parameters, and each component structure can be characterized by any norm. We present a simple estimator for such models, give a geometric condition under which the components can be accurately estimated, characterize sample complexity of the estimator, and give high probability non-asymptotic bounds on the componentwise estimation error. We use tools from empirical processes and generic chaining for the statistical analysis, and our results, which substantially generalize prior work on superposition models, are in terms of Gaussian widths of suitable sets.", "creator": "LaTeX with hyperref package"}}}