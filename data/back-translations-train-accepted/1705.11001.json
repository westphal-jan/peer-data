{"id": "1705.11001", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Adversarial Ranking for Language Generation", "abstract": "Generative adversarial networks (GANs) have great successes on synthesizing data. However, the existing GANs restrict the discriminator to be a binary classifier, and thus limit their learning capacity for tasks that need to synthesize output with rich structures such as natural language descriptions. In this paper, we propose a novel generative adversarial network, RankGAN, for generating high-quality language descriptions. Rather than train the discriminator to learn and assign absolute binary predicate for individual data sample, the proposed RankGAN is able to analyze and rank a collection of human-written and machine-written sentences by giving a reference group. By viewing a set of data samples collectively and evaluating their quality through relative ranking scores, the discriminator is able to make better assessment which in turn helps to learn a better generator. The proposed RankGAN is optimized through the policy gradient technique. Experimental results on multiple public datasets clearly demonstrate the effectiveness of the proposed approach.", "histories": [["v1", "Wed, 31 May 2017 09:21:04 GMT  (592kb)", "http://arxiv.org/abs/1705.11001v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["kevin lin", "dianqi li", "xiaodong he", "zhengyou zhang", "ming-ting sun"], "accepted": true, "id": "1705.11001"}, "pdf": {"name": "1705.11001.pdf", "metadata": {"source": "CRF", "title": "Adversarial Ranking for Language Generation", "authors": ["Kevin Lin", "Dianqi Li", "Xiaodong He", "Zhengyou Zhang", "Ming-Ting Sun"], "emails": ["kvlin@uw.edu,", "dianqili@uw.edu,", "mts@uw.edu,", "xiaohe@microsoft.com", "zhang@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.11 001v 1 [cs.C L] 31 May 2"}, {"heading": "1 Introduction", "text": "In recent years, the number of those who are able to work in the USA, in Europe, in the USA, in Europe, in the USA, in Europe, in the USA, in the USA, in Europe, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA,"}, {"heading": "2 Related works", "text": "GANs: Recently, GANs [8] have been extensively researched due to the nature of unsupervised deep learning. Although GANs with computer vision applications have achieved great success [5, 13, 16, 23, 30], little progress has been made in processing natural language, as the individual sequences cannot be differentiated. To address the non-differentiable problem, SeqGAN [34] addresses this problem through the policy gradient inspired by enhanced learning. [28] The approach considers each word in the sentence as an action and calculates the reward of the sequence by searching for Monte Carlo (MC). Their method propagates the reward from the discriminator and encourages the generator to create human-like language sentences. Li et al. [17] Apply GANs with the policy gradient method to dialogue generation. They train a Seq2Seq model as a generator and build the discriminator on top of a hierarchical encoder."}, {"heading": "3 Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Overall architecture", "text": "In conventional GANs [8], the multilayer perceptron discriminator outputs a single probability distribution to indicate whether the unknown sequences come from the real data, rather than from the data synthesized by a generator. In contrast to conventional GANs, RankGAN consists of a sequence generator G and a ranking R, where the R can give a relative rank among the sequences when a reference is given. As illustrated in Figure 1, the learning objective of G is to produce a synthetic sentence that is ranked higher than those drawn from real data. However, the target of R ranks the synthetic sentence lower than human-written sentences. This can play as G and R a mini-maxim game with the objective function L: min \u03b8 max \u03c6 L (G\u03b8, R\u03c6) = E s \u00b2 Ph [logR\u03c6 (s, C \u2212) are the synthetic sentences."}, {"heading": "3.2 Rank score", "text": "Inspired by this, the relevance value of the input sequence s is measured with a reference set and the input sequence u as: \u03b1 (s | u) = Cosine (ys, yu) = ys \u00b7 yu-like formula is used to calculate the ranking score for a particular sequence s, where yu and ys are the embedded feature vectors of the reference set or input sequence, respectively."}, {"heading": "3.3 Training", "text": "In conventional settings, the GANs are designed for generating realistically evaluated image data, and thus generatorG\u03b8 consists of a series of differential functions with continuous parameters guided by the objective function. [1] Unfortunately, the synthetic data in text generation is based on discrete symbols that are difficult to update. [2] To solve this problem, we need to apply the policy gradient method, which is widely used in amplifying learning algorithms. [3] Compared with the typical amplification of learning algorithms, the existing sequence s1: t \u2212 1 = (w0, wt \u2212 1) is the current state in which the next word wt \u2212 1 is selected."}, {"heading": "4 Experimental results", "text": "Following the evaluation protocol in [34], we first conduct experiments with the data proposed in [34] and the simulator, then compare the performance of RankGAN with other state-of-the-art methods for several sets of public language data, including Chinese poems [36], COCO captions [18] and Shakespeare plays [26]."}, {"heading": "4.1 Simulation on synthetic data", "text": "In fact, most of us will be able to move to another world where we are able to understand the world and where we are able to change the world."}, {"heading": "4.2 Results on Chinese poems composition", "text": "In order to evaluate the performance of our speech generator, we compare our method with other approaches, including MLE and SeqGAN [34] based on real language data. We conduct experiments with the Chinese poetry data set [36], which contains 13, 123 five-word quatragepoems. Each poem has 4 sentences, and each sentence contains 5 words, making a total of 20 words. After standard pre-processing, which replaces the not frequently used words (appeared less than 5 times) with the special character UNK, we train our model on the data set and generate the poem. To keep the proposed method general, our model does not use prior knowledge such as phonology during learning. Following the evaluation protocol in [34, 36] we calculate the BLEU-2 score and estimate the similarity between the human written poem and the machine-generated one. Table 2 summarizes the BLEU-2 score of various methods."}, {"heading": "4.3 Results on COCO image captions", "text": "We test our method using the captions of the COCO dataset [18]. Captions are the human-written narrative sentences, and each sentence consists of at least 8 words and a maximum of 20 words. We randomly select 80,000 captions as a training set and select 5,000 captions to form the validation set. We replace the words that have appeared less than 5 times with the UNK character to measure the similarity between the generated sentences and the human-written sentences in the validation group. Table 3 shows the performance comparison of different methods. RankGAN performs better than the other methods in terms of different BLEU values. The results show that RankGAN is able to learn large sentences in the validation group by evaluating the human composition in group 1."}, {"heading": "4.4 Results on Shakespeare\u2019s plays", "text": "Finally, we explore the possibility of learning Shakespeare's lexical dependence and use the rare phrases. In this experiment, we train our model on the basis of the Romeo and Juliet play [26] to further validate the proposed method, which is divided into 2,500 training sets and 565 test sets. To learn the rare words in the script, we adjust the threshold from UNK from 5 to 2. Table 4 shows the performance comparison of the proposed RankGAN and the other methods, including MLE and SeqGAN. As can be seen, the proposed method consistently achieves higher BLEU values than the other methods in terms of the different n-gram criteria. The results indicate that the proposed RankGAN is able to capture the transition pattern between words, even if the training sets are new, delicate and complicated."}, {"heading": "5 Conclusion", "text": "We presented a new generative adversary network, RankGAN, to generate high-quality natural language descriptions. Instead of training the discriminator to assign an absolute binary predicate to real or synthesized data samples, we propose to rank the human-written sentences with a ranking higher than the machine-written sentences. Subsequently, we train the generator to synthesize sentences of natural language that can be rated higher than the human-written ones. By loosening the binary classification restriction and designing a relative space with rich information for the discriminator in the hostile learning environment, the proposed learning goal is advantageous for the synthesis of high-quality natural language sentences. Experimental results from several public data sets show that our method performs significantly better than previous state language generators."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments", "author": ["Satanjeev Banerjee", "Alon Lavie"], "venue": "In Proc. ACL workshops,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Generating sentences from a continuous space", "author": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio"], "venue": "Proc. CoNLL,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Towards diverse and natural image descriptions via a conditional gan", "author": ["Bo Dai", "Dahua Lin", "Raquel Urtasun", "Sanja Fidler"], "venue": "arXiv preprint arXiv:1703.06029,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2017}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Emily L Denton", "Soumith Chintala", "Rob Fergus"], "venue": "In Proc. NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "From captions to visual concepts and back", "author": ["Hao Fang", "Saurabh Gupta", "Forrest Iandola", "Rupesh K Srivastava", "Li Deng", "Piotr Doll\u00e1r", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John C Platt"], "venue": "In Proc. CVPR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Convolutional sequence to sequence learning", "author": ["Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N Dauphin"], "venue": "arXiv preprint arXiv:1705.03122,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Proc. NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": "In Proc. CIKM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "How (not) to train your generative model: Scheduled sampling, likelihood, adversary", "author": ["Ferenc Husz\u00e1r"], "venue": "arXiv preprint arXiv:1511.05101,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Image-to-image translation with conditional adversarial networks", "author": ["Phillip Isola", "Jun-Yan Zhu", "Tinghui Zhou", "Alexei A Efros"], "venue": "In Proc. CVPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "Optimizing search engines using clickthrough data", "author": ["Thorsten Joachims"], "venue": "In Proc. SIGKDD,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Gans for sequences of discrete elements with the gumbel-softmax distribution", "author": ["Matt J Kusner", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"], "venue": "arXiv preprint arXiv:1611.04051,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Photo-realistic single image superresolution using a generative adversarial network", "author": ["Christian Ledig", "Lucas Theis", "Ferenc Husz\u00e1r", "Jose Caballero", "Andrew Cunningham", "Alejandro Acosta", "Andrew Aitken", "Alykhan Tejani", "Johannes Totz", "Zehan Wang"], "venue": "arXiv preprint arXiv:1609.04802,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Adversarial learning for neural dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Tianlin Shi", "Alan Ritter", "Dan Jurafsky"], "venue": "arXiv preprint arXiv:1701.06547,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2017}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In Proc. ECCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Learning to rank for information retrieval", "author": ["Tie-Yan Liu"], "venue": "Foundations and Trends R  \u00a9 in Information Retrieval,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proc. ACL,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "Relative attributes", "author": ["Devi Parikh", "Kristen Grauman"], "venue": "In Proc. ICCV, pages 503\u2013510,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Generative adversarial text to image synthesis", "author": ["Scott Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee"], "venue": "In Proc. NIPS,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Generating recommendation dialogs by extracting information from user reviews", "author": ["Kevin Reschke", "Adam Vogel", "Dan Jurafsky"], "venue": "In ACL,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "The complete works of William Shakespeare", "author": ["William Shakespeare"], "venue": "Race Point Publishing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Proc. NIPS,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S Sutton", "David A McAllester", "Satinder P Singh", "Yishay Mansour"], "venue": "In NIPS,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1999}, {"title": "Improved regret bounds for oracle-based adversarial contextual bandits", "author": ["Vasilis Syrgkanis", "Haipeng Luo", "Akshay Krishnamurthy", "Robert E Schapire"], "venue": "In Proc. NIPS,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Cider: Consensus-based image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In Proc. CVPR,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey"], "venue": "arXiv preprint arXiv:1609.08144,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Improving neural machine translation with conditional sequence generative adversarial nets", "author": ["Zhen Yang", "Wei Chen", "Feng Wang", "Bo Xu"], "venue": "arXiv preprint arXiv:1703.04887,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2017}, {"title": "Seqgan: sequence generative adversarial nets with policy gradient", "author": ["Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu"], "venue": "In Proc. AAAI,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2017}, {"title": "Text understanding from scratch", "author": ["Xiang Zhang", "Yann LeCun"], "venue": "arXiv preprint arXiv:1502.01710,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Chinese poetry generation with recurrent neural networks", "author": ["Xingxing Zhang", "Mirella Lapata"], "venue": "In Proc. EMNLP,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Language generation plays an important role in natural language processing, which is essential to many applications such as machine translation [1], image captioning [6], and dialogue systems [25].", "startOffset": 144, "endOffset": 147}, {"referenceID": 5, "context": "Language generation plays an important role in natural language processing, which is essential to many applications such as machine translation [1], image captioning [6], and dialogue systems [25].", "startOffset": 166, "endOffset": 169}, {"referenceID": 23, "context": "Language generation plays an important role in natural language processing, which is essential to many applications such as machine translation [1], image captioning [6], and dialogue systems [25].", "startOffset": 192, "endOffset": 196}, {"referenceID": 8, "context": "Recent studies [9, 10, 27, 32] show that the recurrent neural networks (RNNs) and the long shortterm memory networks (LSTMs) can achieve impressive performances for the task of language generation.", "startOffset": 15, "endOffset": 30}, {"referenceID": 9, "context": "Recent studies [9, 10, 27, 32] show that the recurrent neural networks (RNNs) and the long shortterm memory networks (LSTMs) can achieve impressive performances for the task of language generation.", "startOffset": 15, "endOffset": 30}, {"referenceID": 25, "context": "Recent studies [9, 10, 27, 32] show that the recurrent neural networks (RNNs) and the long shortterm memory networks (LSTMs) can achieve impressive performances for the task of language generation.", "startOffset": 15, "endOffset": 30}, {"referenceID": 30, "context": "Recent studies [9, 10, 27, 32] show that the recurrent neural networks (RNNs) and the long shortterm memory networks (LSTMs) can achieve impressive performances for the task of language generation.", "startOffset": 15, "endOffset": 30}, {"referenceID": 19, "context": "In evaluation, metrics such as BLEU [21], METEOR [2], and CIDEr [31] are reported in the literature.", "startOffset": 36, "endOffset": 40}, {"referenceID": 1, "context": "In evaluation, metrics such as BLEU [21], METEOR [2], and CIDEr [31] are reported in the literature.", "startOffset": 49, "endOffset": 52}, {"referenceID": 29, "context": "In evaluation, metrics such as BLEU [21], METEOR [2], and CIDEr [31] are reported in the literature.", "startOffset": 64, "endOffset": 68}, {"referenceID": 7, "context": "[8] introduced the framework for generating the synthetic data that is similar to the real one.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Since then, GANs achieve great performance in computer vision tasks such as image synthesis [5, 13, 16, 23, 30].", "startOffset": 92, "endOffset": 111}, {"referenceID": 12, "context": "Since then, GANs achieve great performance in computer vision tasks such as image synthesis [5, 13, 16, 23, 30].", "startOffset": 92, "endOffset": 111}, {"referenceID": 15, "context": "Since then, GANs achieve great performance in computer vision tasks such as image synthesis [5, 13, 16, 23, 30].", "startOffset": 92, "endOffset": 111}, {"referenceID": 21, "context": "Since then, GANs achieve great performance in computer vision tasks such as image synthesis [5, 13, 16, 23, 30].", "startOffset": 92, "endOffset": 111}, {"referenceID": 28, "context": "Since then, GANs achieve great performance in computer vision tasks such as image synthesis [5, 13, 16, 23, 30].", "startOffset": 92, "endOffset": 111}, {"referenceID": 2, "context": ", text sequences [3]).", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "GANs assume the output of the discriminator to be a binary predicate indicating whether the given sentence is written by human or machine [4, 15, 17, 33, 34].", "startOffset": 138, "endOffset": 157}, {"referenceID": 14, "context": "GANs assume the output of the discriminator to be a binary predicate indicating whether the given sentence is written by human or machine [4, 15, 17, 33, 34].", "startOffset": 138, "endOffset": 157}, {"referenceID": 16, "context": "GANs assume the output of the discriminator to be a binary predicate indicating whether the given sentence is written by human or machine [4, 15, 17, 33, 34].", "startOffset": 138, "endOffset": 157}, {"referenceID": 31, "context": "GANs assume the output of the discriminator to be a binary predicate indicating whether the given sentence is written by human or machine [4, 15, 17, 33, 34].", "startOffset": 138, "endOffset": 157}, {"referenceID": 32, "context": "GANs assume the output of the discriminator to be a binary predicate indicating whether the given sentence is written by human or machine [4, 15, 17, 33, 34].", "startOffset": 138, "endOffset": 157}, {"referenceID": 27, "context": "During learning, we adopt the policy gradient technique [29] to overcome the non-differentiable problem.", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": "GANs: Recently, GANs [8] have been widely explored due to its nature of unsupervised deep learning.", "startOffset": 21, "endOffset": 24}, {"referenceID": 4, "context": "Though GANs achieve great successes on computer vision applications [5, 13, 16, 23, 30], there are only a few progresses in natural language processing because the discrete sequences are not differentiable.", "startOffset": 68, "endOffset": 87}, {"referenceID": 12, "context": "Though GANs achieve great successes on computer vision applications [5, 13, 16, 23, 30], there are only a few progresses in natural language processing because the discrete sequences are not differentiable.", "startOffset": 68, "endOffset": 87}, {"referenceID": 15, "context": "Though GANs achieve great successes on computer vision applications [5, 13, 16, 23, 30], there are only a few progresses in natural language processing because the discrete sequences are not differentiable.", "startOffset": 68, "endOffset": 87}, {"referenceID": 21, "context": "Though GANs achieve great successes on computer vision applications [5, 13, 16, 23, 30], there are only a few progresses in natural language processing because the discrete sequences are not differentiable.", "startOffset": 68, "endOffset": 87}, {"referenceID": 28, "context": "Though GANs achieve great successes on computer vision applications [5, 13, 16, 23, 30], there are only a few progresses in natural language processing because the discrete sequences are not differentiable.", "startOffset": 68, "endOffset": 87}, {"referenceID": 32, "context": "To tackle the non-differentiable problem, SeqGAN [34] addresses this issue by the policy gradient inspired from the reinforcement learning [28].", "startOffset": 49, "endOffset": 53}, {"referenceID": 26, "context": "To tackle the non-differentiable problem, SeqGAN [34] addresses this issue by the policy gradient inspired from the reinforcement learning [28].", "startOffset": 139, "endOffset": 143}, {"referenceID": 16, "context": "[17] apply GANs with the policy gradient method to dialogue generation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] show that it is possible to enhance the diversity of the generated image captions with conditional GANs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "[33] further proof that training a convolutional neural network (CNN) as a discriminator yields better performance than that of the recurrent neural network (RNN) for the task of machine translation (MT).", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Learning to rank: Learning to rank plays an essential role in Information Retrieval (IR) [20].", "startOffset": 89, "endOffset": 93}, {"referenceID": 10, "context": "The ranking technique has been proofed effective for searching documents [11] and images [22].", "startOffset": 73, "endOffset": 77}, {"referenceID": 20, "context": "The ranking technique has been proofed effective for searching documents [11] and images [22].", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "Given a reference, the desired information (such as click-through logs [14]) is incorporated into the ranking function which aims to encourage the relevant documents to be returned as early as possible.", "startOffset": 71, "endOffset": 75}, {"referenceID": 7, "context": "In conventional GANs [8], the discriminator with multilayer perceptrons outputs a single probability distribution to suggest whether the unknown sequences come from the real data rather than the data synthesized by a generator.", "startOffset": 21, "endOffset": 24}, {"referenceID": 9, "context": "In this paper, we design the generative model with the long short-term memory networks (LSTMs) [10].", "startOffset": 95, "endOffset": 99}, {"referenceID": 6, "context": "Recent studies show that the convolutional neural network can achieve high performance for machine translation [7, 33] and text classification [35].", "startOffset": 111, "endOffset": 118}, {"referenceID": 31, "context": "Recent studies show that the convolutional neural network can achieve high performance for machine translation [7, 33] and text classification [35].", "startOffset": 111, "endOffset": 118}, {"referenceID": 33, "context": "Recent studies show that the convolutional neural network can achieve high performance for machine translation [7, 33] and text classification [35].", "startOffset": 143, "endOffset": 147}, {"referenceID": 10, "context": "Following the ranking steps commonly used in Web search [11], the relevance score of the input sequence s given a reference u is measured as:", "startOffset": 56, "endOffset": 60}, {"referenceID": 26, "context": "The parameter \u03b3, whose value is set empirically during experiments, shares the similar idea with the Boltzmann exploration [28] method in reinforcement learning.", "startOffset": 123, "endOffset": 127}, {"referenceID": 7, "context": "In conventional settings, GANs are designed for generating real-valued image data and thus the generatorG\u03b8 consists of a series of differential functions with continuous parameters guided by the objective function from discriminatorD\u03c6 [8].", "startOffset": 235, "endOffset": 238}, {"referenceID": 27, "context": "To solve this issue, we adopt the Policy Gradient method [29], which has been widely used in reinforcement learning.", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "To relieve this problem, we utilize the Monte Carlo rollouts methods [4, 34] to simulate intermediate rewards when a sequence is incomplete.", "startOffset": 69, "endOffset": 76}, {"referenceID": 32, "context": "To relieve this problem, we utilize the Monte Carlo rollouts methods [4, 34] to simulate intermediate rewards when a sequence is incomplete.", "startOffset": 69, "endOffset": 76}, {"referenceID": 27, "context": "Refer to the proof in [29], the gradient of the objective function for generator G can be formulated as:", "startOffset": 22, "endOffset": 26}, {"referenceID": 3, "context": "Importantly, different from the gradient policy methods in other works [4, 19, 34], our method replaces the simple binary outputs with a ranking system based on multiple sentences, which can better reflect the quality of the imitate sentences and facilitate effective training of the generatorG.", "startOffset": 71, "endOffset": 82}, {"referenceID": 32, "context": "Importantly, different from the gradient policy methods in other works [4, 19, 34], our method replaces the simple binary outputs with a ranking system based on multiple sentences, which can better reflect the quality of the imitate sentences and facilitate effective training of the generatorG.", "startOffset": 71, "endOffset": 82}, {"referenceID": 22, "context": "This is similar to the finding in [24].", "startOffset": 34, "endOffset": 38}, {"referenceID": 32, "context": "Following the evaluation protocol in [34], we first carry out experiments on the data and simulator proposed in [34].", "startOffset": 37, "endOffset": 41}, {"referenceID": 32, "context": "Following the evaluation protocol in [34], we first carry out experiments on the data and simulator proposed in [34].", "startOffset": 112, "endOffset": 116}, {"referenceID": 34, "context": "Then, we compare the performance of RankGAN with other state-of-the-art methods on multiple public language datasets including Chinese poems [36], COCO captions [18], and Shakespear\u2019s plays [26].", "startOffset": 141, "endOffset": 145}, {"referenceID": 17, "context": "Then, we compare the performance of RankGAN with other state-of-the-art methods on multiple public language datasets including Chinese poems [36], COCO captions [18], and Shakespear\u2019s plays [26].", "startOffset": 161, "endOffset": 165}, {"referenceID": 24, "context": "Then, we compare the performance of RankGAN with other state-of-the-art methods on multiple public language datasets including Chinese poems [36], COCO captions [18], and Shakespear\u2019s plays [26].", "startOffset": 190, "endOffset": 194}, {"referenceID": 32, "context": "Table 1: The performance comparison of different methods on the synthetic data [34] in terms of the negative log-likelihood (NLL) scores.", "startOffset": 79, "endOffset": 83}, {"referenceID": 32, "context": "We first conduct the test on the dataset proposed in [34].", "startOffset": 53, "endOffset": 57}, {"referenceID": 32, "context": "Following the evaluation protocol in [34], we evaluate the machine-written sentences by stimulating the Turing test.", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "Following this, we take the sentences generated by RankGAN as the input of the oracle model, and estimate the average negative loglikelihood (NLL) [12].", "startOffset": 147, "endOffset": 151}, {"referenceID": 32, "context": "We compare our approach with the state-of-the-art methods including maximum likelihood estimation (MLE), policy gradient with BLEU (PG-BLEU), and SeqGAN [34].", "startOffset": 153, "endOffset": 157}, {"referenceID": 32, "context": "To evaluate the performance of our language generator, we compare our method with other approaches including MLE and SeqGAN [34] on the real-word language data.", "startOffset": 124, "endOffset": 128}, {"referenceID": 34, "context": "We conduct experiments on the Chinese poem dataset [36], which contains 13, 123 five-word quatrain poems.", "startOffset": 51, "endOffset": 55}, {"referenceID": 32, "context": "Following the evaluation protocol in [34, 36], we compute the BLEU-2 score and estimate the similarity between the human-written poem and the machine-created one.", "startOffset": 37, "endOffset": 45}, {"referenceID": 34, "context": "Following the evaluation protocol in [34, 36], we compute the BLEU-2 score and estimate the similarity between the human-written poem and the machine-created one.", "startOffset": 37, "endOffset": 45}, {"referenceID": 17, "context": "We test our method on the image captions provided by the COCO dataset [18].", "startOffset": 70, "endOffset": 74}, {"referenceID": 24, "context": "In this experiment, we train our model on the Romeo and Juliet play [26] to further validate the proposed method.", "startOffset": 68, "endOffset": 72}], "year": 2017, "abstractText": "Generative adversarial networks (GANs) have great successes on synthesizing data. However, the existing GANs restrict the discriminator to be a binary classifier, and thus limit their learning capacity for tasks that need to synthesize output with rich structures such as natural language descriptions. In this paper, we propose a novel generative adversarial network, RankGAN, for generating highquality language descriptions. Rather than train the discriminator to learn and assign absolute binary predicate for individual data sample, the proposed RankGAN is able to analyze and rank a collection of human-written and machine-written sentences by giving a reference group. By viewing a set of data samples collectively and evaluating their quality through relative ranking scores, the discriminator is able to make better assessment which in turn helps to learn a better generator. The proposed RankGAN is optimized through the policy gradient technique. Experimental results on multiple public datasets clearly demonstrate the effectiveness of the proposed approach.", "creator": "LaTeX with hyperref package"}}}