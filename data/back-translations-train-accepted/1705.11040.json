{"id": "1705.11040", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "End-to-end Differentiable Proving", "abstract": "We introduce neural networks for end-to-end differentiable theorem proving that operate on dense vector representations of symbols. These neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in Prolog. Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations. By using gradient descent, the resulting neural network can be trained to infer facts from a given incomplete knowledge base. It learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove facts, (iii) induce logical rules, and (iv) use provided and induced logical rules for complex multi-hop reasoning. We demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, on four benchmark knowledge bases while at the same time inducing interpretable function-free first-order logic rules.", "histories": [["v1", "Wed, 31 May 2017 11:40:57 GMT  (32kb,D)", "http://arxiv.org/abs/1705.11040v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG cs.LO", "authors": ["tim rockt\\\"aschel", "sebastian riedel"], "accepted": true, "id": "1705.11040"}, "pdf": {"name": "1705.11040.pdf", "metadata": {"source": "CRF", "title": "End-to-end Differentiable Proving", "authors": ["Tim Rockt\u00e4schel", "Sebastian Riedel"], "emails": ["tim.rocktaschel@cs.ox.ac.uk", "s.riedel@cs.ucl.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point that it will only be once before there is such a process, in which there is such a process."}, {"heading": "2 Background", "text": "In this section, we will briefly introduce the syntax of the KBs we use in the rest of the paper. An atom (short for atomic rule) consists of a predicate symbol and a list of terms. We will use lowercase letters to refer to predicates and constant symbols (e.g. Fatherland and BART), and capital letters for variables (e.g. X, Y, Z). As we look at only functional-free first order logic rules, a term can only be a constant or a variable. For example, [Grandfather, Q, BART] is an atom with the predicate grandfatherOf, and two terms that have variable leQ and the constant BART. We will consider rules of the form HEAD: - BODY, where BODY is a possibly empty conjunction of atoms, and HEAD is an atom. We call a rule without free variables a basic rule. All variables are universally quantified."}, {"heading": "3 Methods", "text": "In the following, we describe the recursive construction of NTPs - neural networks for end-to-end differentiable evidence that allow us to calculate the gradients of evidence successes related to vector representations of symbols. We define the construction of NTPs based on modules similar to the behavior of dynamic neural module networks [27]. Each module inputs discrete objects such as atoms and rules and a proof state. A proof-state S is a tuple consisting of the previously constructed substitution set and a neural network that gives a real success score of a (partial) proof. While discrete objects and the substitution sets are used only during the construction of the neural network (i.e. they define the topology of the network), after the construction of the network, a continuous proof-success score can be used for many different objectives in the formation and test time computation, and the substitution sets can be used equally to discrete and institute objects."}, {"heading": "3.1 Unification Module", "text": "The union of two atoms (predicates or constants) is checked for equality and the proof can be aborted if this verification fails. However, we want to be able to apply rules, even if symbols in the target and in the head are not the same but similar in meaning (e.g. grandfather and grandfather), and therefore replace the symbolic comparison with a calculation that measures the similarity of both symbols in a vector space. The module unifies a substitution set and creates a neural network for comparing the vector representations of non-variable symbols in two conceptual sequences. The signature of this module is L \u00d7 L \u00d7 S \u2192 S, where L is the domain of the lists of termets. Unification takes two atoms that represent a list of concepts and an upstream proof, and maps them into a new state of proof (substitution and proof)."}, {"heading": "3. unify\u03b8(H, [ ], S) = FAIL", "text": "4. unify\u03b8 (h: H, g: G, S) = unify\u03b8 (H, G, S) S \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" h h h h h h h h h \"p\" p \"p\" p \"p\" p \"p\" p \"p\" h \"h\" h h h h h h h h h \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"h\" p \"h h h h h\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" h h h h h h h \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" h h h h h h h h h h h h h \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"h\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"h h h h h h h h h h h\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \""}, {"heading": "3.2 OR Module", "text": "Based on Unify, we now define the module (s) that attempts to apply rules in a KB. The signature of or is L \u00b7 N \u00b7 S \u2192 SN, where L is the domain of target atoms and N is the domain of integers used to specify the maximum depth of proof of the neural network. Furthermore, N is the number of possible starting proofs for a target of a given structure and a provided KB.2 We implement or as 1. orK\u03b8 (G, d, S) = [S \u2032 | S \u2032 and K\u03b8 (B, d, unify\u03b8 (H, G, S))), H: -B, where H: -B denotes a rule in a given KB K with a header atom H and a list of body atoms B. Unlike the symbolic OR method, the module (s) is able to use the GrandfaterOf rule (H, G, S) if the subsymbolic representations of both predicates are similar."}, {"heading": "3.3 AND Module", "text": "For the implementation and definition of an auxiliary function called substitutions that applies substitutions to variables in an atom, if possible: substitutions (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) (substitutions) () (substitutions) () ((substitutions) () (substitutions) () (substitutions) () (substitutions) () (substitutions) () (substitutions) () (substitutions) () (substitutions) () () (substitutions) (() (substitutions) () (() (substitutions) () (() () (substitutions) () (substitutions) (() (() () () (() (() () (() (() () () (() (() (() () ((() () ((() () (() (() (() (() () (() (() (() () ((() () (() () (() (() (() (() (() ((() () () (() ((() (() (() (() (((() () (() ((() () (((() () (() (() (() ((() () (((() (() (() () (() (((()) () ((()) (((() ()) (((())) (((((())) ((("}, {"heading": "3.4 Proof Aggregation", "text": "Finally, we define the overall success of the proof of a target G using a KB K with the parameters \u03b8 asntpK\u03b8 (G, d) = argmax S-orK\u03b8 (G, d, (\u2205, 1) S 6 = FAILS\u03c1where d is a predefined maximum depth of proof and the initial proof state is an empty substitution quantity and a proof success of 1. Example Figure 1 illustrates an exemplary NTP calculation diagram designed for a toy KB. Note that such an NTP is constructed once before the training and can then be used to prove the goals of the structure [s, i, j] during training and testing, where s is the index of an entry predicate and i and j are indices of input constants. Final states of proof used in the evidence aggregation are underlined."}, {"heading": "3.5 Training Objective", "text": "Let K be the amount of known facts in a given KB. Normally, we do not observe any negative facts and therefore fall back on the sample of corrupted soil atoms as performed in previous work [29]. Specifically, for each [s, i, j], [s, i, j], [s, i, j], [s, i, j], 6, K by sample i, j, i, i, and j] from the set of constants we obtain corrupted soil atoms. These corrupted soil atoms are reevaluated in each iteration of the training, and we denote the set of known and corrupted soil atoms together with their target value (1.0 for known soil atoms and 0.0 for corrupted ones) as T. We use the negative protocol probability of the evidence as proof of the loss function of an NTP with parameters resulting from the composition."}, {"heading": "3.6 Computational Optimizations", "text": "Unlike the symbolic reverse concatenation, where a proof can be aborted once unification fails, the differentiated test only gives us a unification error for atoms whose identity does not match. In the appendix, we propose two optimizations to speed up NTPs: First, we use modern graphics processing units (GPUs) by processing a lot of evidence in parallel (appendix B); second, we use the small number of gradients caused by the min and max operations of unification or proof aggregation to derive a heuristics for a shortened forward and reverse flow that drastically reduces the number of gradients to be calculated (appendix C)."}, {"heading": "3.7 Neural Inductive Logic Programming", "text": "We can use NTPs for ILP [9] by gradient descent instead of a combinatorial search of the space of rules, such as that performed by the First Order Inductive Learner (FOIL) [30]. Specifically, we use the concept of Learning from entailment [9] to create rules by which we can detect known soil atoms, but which do not yield high success rates to corrupt soil atoms. Suppose: - s (X, Z), - Rk are representations of some unknown predicts with index r, s, and t. Previous knowledge of a transitivity between three unknown predictors can be specified by r (X, Y): - s (Z, Y). We call this a parameterized rule, because the corresponding predicts are unknown and their representations are learned from data. Such a rule can be used for evidence in training and testing time in the same way as any other given rule."}, {"heading": "3.8 Regularization by Neural Link Predictor", "text": "At the beginning of the training, all sub-symbolic representations are randomly initialized. Consequently, when unifying a target with all the facts in a KB, we get very loud results in the early stages of the training. Since only maximum success leads to gradient updates for the respective sub-symbolic representations, it can take a long time for NTPs to learn to place similar symbols close to each other in vector space. To accelerate the learning of sub-symbolic representations, we train NTPs together with ComplEx [7] (Appendix A). ComplEx and the NTP share the same parameters, which is feasible since the RBF kernel is also defined for complex vectors in Unify. While the NTP is responsible for multi-hop reasoning, the neural link prediction model learns to evaluate soil atoms locally."}, {"heading": "4 Experiments", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "5 Results and Discussion", "text": "Another method of differentiating rules for automated KB completion was recently introduced by [34] and our rating structure is in accordance with their Protocol II. We are therefore focusing on comparing NTPs with ComplEx.First, we note that vanilla NTPs alone do not always work well; they only perform better than ComplEx in countries S3 and nations, but not Kinship or UMLS. This shows the difficulty of learning sub-symbolic representations in differentiated evidence of uniformity alone, and the need to regulate NTPs using a neural link prediction model such as Kinship or UMLS alone shows that NTP prediction models such as ComplEx predictions outperform other models in most settings. The differences in the NPR rules are significant for NTP-1 prediction models to be comparable to ComplEx prediction models."}, {"heading": "6 Related Work", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "7 Conclusion and Future Work", "text": "To this end, we used Prologs backward-concatenation algorithm as a recipe for the recursive construction of neural networks that can be used to prove facts in a KB. Specifically, we introduced a differentiable unification operation between vector representations of symbols, and the constructed neural network enabled us to calculate the gradient of evidence sequences in relation to vector representations of symbols, thus enabling us to train subsymbolic representations at the end of facts in a KB. Furthermore, we were able to induce first-order logical rules of-order by means of gradient-pedigree based on templates for unknown rules of predefined structures. Benchmark-KB's model of page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-side-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-to-page-page-page-page-page-page-page-and-page-page-page-page-to-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-page-to-page-page-page-page-page-page-in-the-in-as-as-as-as-as-as-as-as-as-as-as-as-as-as-as-as-as-as"}, {"heading": "Acknowledgements", "text": "We thank Matko Bosnjak and Johannes Welbl for comments on the drafts of this work. This work was supported by a Google PhD Fellowship in Natural Language Processing, an Allen Distinguished Investigator Award and a Marie Curie Career Integration Award."}, {"heading": "A ComplEx", "text": "ComplEx [7] is a state-of-the-art prediction model for neural concatenations that represents symbols as complex vectors.The scoring function defined by ComplEx denotes the real part and imag (\u03b8i:) the imaginary part of a complex vector \u03b8i: \u0441Ck representing the symbol with the ith index.The scoring function is complex (s, i, j) = \u03c3 (real (\u03b8i:) > (real (\u03b8i:))) + real (successs:) > (imag (\u03b8i:)) imag (\u03b8i:))) + imag (successj:) > (real (successi:) imag (\u03b8i:)) > (imag (\u03b8i:)) > (2), where ComplEx maintains elementary multiplication and the sigmoid function."}, {"heading": "B Batch Proving", "text": "In fact, it is the case that we will be able to achieve the objectives mentioned without them being able to achieve them."}, {"heading": "D Training Details", "text": "We use ADAM [61] with an initial learning rate of 0.001 and a mini batch size of 50 (10 known and 40 corrupted atoms) for optimization. We apply a \"2 regularization of 0.01 to all model parameters and clip gradient values at [\u2212 1.0, 1.0]. All subsymbolic representations and rule parameters are initialized with Xavier initialization [62]. We use a maximum proof depth of d = 2 and add the following rule templates, where the number before the rule indicates how often a parameterized rule of the given structure is instantiated. Note that a rule template such as # 1 (X, Y), S2: - # 2 (X, Z, Y)."}], "references": [{"title": "Factorizing YAGO: scalable machine learning for linked data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 21st World Wide Web Conference", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum", "Benjamin M. Marlin"], "venue": "In Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Typed tensor decomposition of knowledge bases for relation extraction", "author": ["Kai-Wei Chang", "Wen-tau Yih", "Bishan Yang", "Christopher Meek"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Representing text for joint embedding of text and knowledge bases", "author": ["Kristina Toutanova", "Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Complex embeddings for simple link prediction", "author": ["Th\u00e9o Trouillon", "Johannes Welbl", "Sebastian Riedel", "\u00c9ric Gaussier", "Guillaume Bouchard"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Logic and Data Bases, Symposium on Logic and Data Bases, Centre d\u2019\u00e9tudes", "author": ["Herv\u00e9 Gallaire", "Jack Minker", "editors"], "venue": "et de recherches de Toulouse,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1977}, {"title": "Inductive logic programming", "author": ["Stephen Muggleton"], "venue": "New Generation Comput.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1991}, {"title": "Statistical predicate invention", "author": ["Stanley Kok", "Pedro M. Domingos"], "venue": "In Machine Learning, Proceedings of the Twenty-Fourth International Conference (ICML", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Improving learning and inference in a large knowledge-base using latent syntactic cues", "author": ["Matt Gardner", "Partha Pratim Talukdar", "Bryan Kisiel", "Tom M. Mitchell"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Incorporating vector space similarity in random walk inference over knowledge bases", "author": ["Matt Gardner", "Partha Pratim Talukdar", "Jayant Krishnamurthy", "Tom M. Mitchell"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Representing meaning with a combination of logical and distributional models", "author": ["Islam Beltagy", "Stephen Roller", "Pengxiang Cheng", "Katrin Erk", "Raymond J Mooney"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "Compositional vector space models for knowledge base completion", "author": ["Arvind Neelakantan", "Benjamin Roth", "Andrew McCallum"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Towards neural network-based reasoning", "author": ["Baolin Peng", "Zhengdong Lu", "Hang Li", "Kam-Fai Wong"], "venue": "CoRR, abs/1508.05508,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Chains of reasoning over entities, relations, and text using recurrent neural networks", "author": ["Rajarshi Das", "Arvind Neelakantan", "David Belanger", "Andrew McCallum"], "venue": "In Conference of the European Chapter of the Association for Computational Linguistics (EACL),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2017}, {"title": "Separating answers from queries for neural reading", "author": ["Dirk Weissenborn"], "venue": "comprehension. CoRR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Armand Joulin", "Tomas Mikolov"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V. Le", "Ilya Sutskever"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Neural programmer-interpreters", "author": ["Scott E. Reed", "Nando de Freitas"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Learning to learn by gradient descent by gradient descent", "author": ["Marcin Andrychowicz", "Misha Denil", "Sergio Gomez Colmenarejo", "Matthew W. Hoffman", "David Pfau", "Tom Schaul", "Nando de Freitas"], "venue": "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Programming with a differentiable forth interpreter", "author": ["Matko Bosnjak", "Tim Rockt\u00e4schel", "Jason Naradowsky", "Sebastian Riedel"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2017}, {"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "In NAACL HLT", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Radial basis functions, multi-variable functional interpolation and adaptive networks", "author": ["David S Broomhead", "David Lowe"], "venue": "Technical report, DTIC Document,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1988}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garc\u00eda-Dur\u00e1n", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Learning logical definitions from relations", "author": ["J. Ross Quinlan"], "venue": "Machine Learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1990}, {"title": "Joint information extraction and reasoning: A scalable statistical relational learning approach", "author": ["William Yang Wang", "William W. Cohen"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "On approximate reasoning capabilities of lowrank vector spaces", "author": ["Guillaume Bouchard", "Sameer Singh", "Theo Trouillon"], "venue": "In Proceedings of the 20015 AAAI Spring Symposium on Knowledge Representation and Reasoning (KRR): Integrating Symbolic and Neural Approaches. Citeseer,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Holographic embeddings of knowledge graphs", "author": ["Maximilian Nickel", "Lorenzo Rosasco", "Tomaso A. Poggio"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Differentiable learning of logical rules for knowledge base completion", "author": ["Fan Yang", "Zhilin Yang", "William W. Cohen"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2017}, {"title": "Neural-symbolic learning systems: foundations and applications", "author": ["Artur S. d\u2019Avila Garcez", "Krysia Broda", "Dov M. Gabbay"], "venue": "Springer Science & Business Media,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "An approach to combining explanation-based and neural learning algorithms", "author": ["Jude W Shavlik", "Geoffrey G Towell"], "venue": "Connection Science,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1989}, {"title": "Knowledge-based artificial neural networks", "author": ["Geoffrey G. Towell", "Jude W. Shavlik"], "venue": "Artif. Intell.,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1994}, {"title": "The connectionist inductive learning and logic programming system", "author": ["Artur S. d\u2019Avila Garcez", "Gerson Zaverucha"], "venue": "Appl. Intell.,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1999}, {"title": "Neurally motivated constraints on the working memory capacity of a production system for parallel processing: Implications of a connectionist model based on temporal synchrony", "author": ["Lokendra Shastri"], "venue": "In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society: July 29 to August", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1992}, {"title": "Neural prolog-the concepts, construction and mechanism", "author": ["Liya Ding"], "venue": "In Systems, Man and Cybernetics,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1995}, {"title": "Fast relational learning using bottom clause propositionalization with artificial neural networks", "author": ["Manoel V.M. Fran\u00e7a", "Gerson Zaverucha", "Artur S. d\u2019Avila Garcez"], "venue": "Machine Learning,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "Lifted relational neural networks", "author": ["Gustav Sourek", "Vojtech Aschenbrenner", "Filip Zelezn\u00fd", "Ondrej Kuzelka"], "venue": "In Proceedings of the NIPS Workshop on Cognitive Computation: Integrating Neural and Symbolic Approaches co-located with the 29th Annual Conference on Neural Information Processing Systems (NIPS", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Tensorlog: A differentiable deductive database", "author": ["William W. Cohen"], "venue": "CoRR, abs/1605.06523,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}, {"title": "Logic tensor networks: Deep learning and logical reasoning from data and knowledge. In Proceedings of the 11th International Workshop on Neural-Symbolic Learning and Reasoning (NeSy\u201916) co-located with the Joint Multi-Conference on Human-Level Artificial Intelligence (HLAI 2016)", "author": ["Luciano Serafini", "Artur S. d\u2019Avila Garcez"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Low-Dimensional Embeddings of Logic", "author": ["Tim Rockt\u00e4schel", "Matko Bosnjak", "Sameer Singh", "Sebastian Riedel"], "venue": "In ACL Workshop on Semantic Parsing (SP\u201914),", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "Injecting logical background knowledge into embeddings for relation extraction", "author": ["Tim Rockt\u00e4schel", "Sameer Singh", "Sebastian Riedel"], "venue": "In NAACL HLT", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "Order-embeddings of images and language", "author": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2016}, {"title": "Harnessing deep neural networks with logic rules. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016", "author": ["Zhiting Hu", "Xuezhe Ma", "Zhengzhong Liu", "Eduard H. Hovy", "Eric P. Xing"], "venue": "Long Papers,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2016}, {"title": "Lifted rule injection for relation embeddings", "author": ["Thomas Demeester", "Tim Rockt\u00e4schel", "Sebastian Riedel"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2016}, {"title": "Unification neural networks: unification by error-correction learning", "author": ["Ekaterina Komendantskaya"], "venue": "Logic Journal of the IGPL,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2011}, {"title": "A structured connectionist unification algorithm", "author": ["Steffen H\u00f6lldobler"], "venue": "In Proceedings of the 8th National Conference on Artificial Intelligence. Boston, Massachusetts, July", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1990}, {"title": "Learning first-order horn clauses from web text", "author": ["Stefan Schoenmackers", "Jesse Davis", "Oren Etzioni", "Daniel S. Weld"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2010}, {"title": "Meta-interpretive learning of higher-order dyadic datalog: Predicate invention revisited", "author": ["Stephen H Muggleton", "Dianhuan Lin", "Alireza Tamaddoni-Nezhad"], "venue": "Machine Learning,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1997}, {"title": "Efficient selectivity and backup operators in monte-carlo tree search", "author": ["R\u00e9mi Coulom"], "venue": "In Computers and Games, 5th International Conference,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2006}, {"title": "Bandit based monte-carlo planning", "author": ["Levente Kocsis", "Csaba Szepesv\u00e1ri"], "venue": "In Machine Learning: ECML 2006, 17th European Conference on Machine Learning, Berlin,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2006}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J. Maddison", "Arthur Guez", "Laurent Sifre", "George van den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Vedavyas Panneershelvam", "Marc Lanctot", "Sander Dieleman", "Dominik Grewe", "John Nham", "Nal Kalchbrenner", "Ilya Sutskever", "Timothy P. Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis"], "venue": "doi: 10.1038/nature16961", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2016}, {"title": "Towards \"alphachem\": Chemical synthesis planning with tree search and deep neural network", "author": ["Marwin H.S. Segler", "Mike Preu\u00df", "Mark P. Waller"], "venue": "policies. CoRR,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2017}, {"title": "Holstep: A machine learning dataset for higher-order logic theorem proving", "author": ["Cezary Kaliszyk", "Fran\u00e7ois Chollet", "Christian Szegedy"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "subsymbolic representations) for scoring fact triples [1\u20137].", "startOffset": 54, "endOffset": 59}, {"referenceID": 1, "context": "subsymbolic representations) for scoring fact triples [1\u20137].", "startOffset": 54, "endOffset": 59}, {"referenceID": 2, "context": "subsymbolic representations) for scoring fact triples [1\u20137].", "startOffset": 54, "endOffset": 59}, {"referenceID": 3, "context": "subsymbolic representations) for scoring fact triples [1\u20137].", "startOffset": 54, "endOffset": 59}, {"referenceID": 4, "context": "subsymbolic representations) for scoring fact triples [1\u20137].", "startOffset": 54, "endOffset": 59}, {"referenceID": 5, "context": "subsymbolic representations) for scoring fact triples [1\u20137].", "startOffset": 54, "endOffset": 59}, {"referenceID": 6, "context": "subsymbolic representations) for scoring fact triples [1\u20137].", "startOffset": 54, "endOffset": 59}, {"referenceID": 7, "context": "In contrast, symbolic theorem provers like Prolog [8] enable exactly this type of multi-hop reasoning.", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "Furthermore, Inductive Logic Programming (ILP) [9] builds upon such provers to learn interpretable rules from data and exploit these for reasoning.", "startOffset": 47, "endOffset": 50}, {"referenceID": 9, "context": "[10]), and when using subsymbolic representations they are not trained end-to-end from training data (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11\u201313]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 11, "context": "[11\u201313]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 12, "context": "[11\u201313]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 13, "context": "Neural multi-hop reasoning models [14\u201318] address the aforementioned limitations to some extent by encoding reasoning chains in a vector space or by iteratively refining subsymbolic representations of questions before comparison with answers.", "startOffset": 34, "endOffset": 41}, {"referenceID": 14, "context": "Neural multi-hop reasoning models [14\u201318] address the aforementioned limitations to some extent by encoding reasoning chains in a vector space or by iteratively refining subsymbolic representations of questions before comparison with answers.", "startOffset": 34, "endOffset": 41}, {"referenceID": 15, "context": "Neural multi-hop reasoning models [14\u201318] address the aforementioned limitations to some extent by encoding reasoning chains in a vector space or by iteratively refining subsymbolic representations of questions before comparison with answers.", "startOffset": 34, "endOffset": 41}, {"referenceID": 16, "context": "Neural multi-hop reasoning models [14\u201318] address the aforementioned limitations to some extent by encoding reasoning chains in a vector space or by iteratively refining subsymbolic representations of questions before comparison with answers.", "startOffset": 34, "endOffset": 41}, {"referenceID": 17, "context": "Our solution to this problem is inspired by recent neural network architectures like Neural Turing Machines [19], Memory Networks [20], Neural Stacks/Queues [21, 22], Neural Programmer [23], Neural Programmer-Interpreters [24], Hierarchical Attentive Memory [25] and the Differentiable Forth Interpreter [26].", "startOffset": 157, "endOffset": 165}, {"referenceID": 18, "context": "Our solution to this problem is inspired by recent neural network architectures like Neural Turing Machines [19], Memory Networks [20], Neural Stacks/Queues [21, 22], Neural Programmer [23], Neural Programmer-Interpreters [24], Hierarchical Attentive Memory [25] and the Differentiable Forth Interpreter [26].", "startOffset": 157, "endOffset": 165}, {"referenceID": 19, "context": "Our solution to this problem is inspired by recent neural network architectures like Neural Turing Machines [19], Memory Networks [20], Neural Stacks/Queues [21, 22], Neural Programmer [23], Neural Programmer-Interpreters [24], Hierarchical Attentive Memory [25] and the Differentiable Forth Interpreter [26].", "startOffset": 185, "endOffset": 189}, {"referenceID": 20, "context": "Our solution to this problem is inspired by recent neural network architectures like Neural Turing Machines [19], Memory Networks [20], Neural Stacks/Queues [21, 22], Neural Programmer [23], Neural Programmer-Interpreters [24], Hierarchical Attentive Memory [25] and the Differentiable Forth Interpreter [26].", "startOffset": 222, "endOffset": 226}, {"referenceID": 21, "context": "Our solution to this problem is inspired by recent neural network architectures like Neural Turing Machines [19], Memory Networks [20], Neural Stacks/Queues [21, 22], Neural Programmer [23], Neural Programmer-Interpreters [24], Hierarchical Attentive Memory [25] and the Differentiable Forth Interpreter [26].", "startOffset": 258, "endOffset": 262}, {"referenceID": 22, "context": "Our solution to this problem is inspired by recent neural network architectures like Neural Turing Machines [19], Memory Networks [20], Neural Stacks/Queues [21, 22], Neural Programmer [23], Neural Programmer-Interpreters [24], Hierarchical Attentive Memory [25] and the Differentiable Forth Interpreter [26].", "startOffset": 304, "endOffset": 308}, {"referenceID": 6, "context": "Our contributions are threefold: (i) we present the construction of NTPs inspired by Prolog\u2019s backward chaining algorithm and a differentiable unification operation using subsymbolic representations, (ii) we propose optimizations to this architecture by joint training with a neural link prediction model, batch proving, and approximate gradient calculation, and (iii) we experimentally show that NTPs can learn representations of symbols and function-free first-order rules of predefined structure, enabling them to learn complex multi-hop reasoning on benchmark KBs and to outperform ComplEx [7], a state-of-the-art neural link prediction model.", "startOffset": 594, "endOffset": 597}, {"referenceID": 7, "context": "Given a goal such as [grandfatherOf, Q, BART], we can use Prolog\u2019s backward chaining algorithm to find substitutions for Q [8].", "startOffset": 123, "endOffset": 126}, {"referenceID": 23, "context": "We define the construction of NTPs in terms of modules similar to dynamic neural module networks [27].", "startOffset": 97, "endOffset": 101}, {"referenceID": 24, "context": "Otherwise, the vector representations of the two non-variable symbols are compared using a Radial Basis Function (RBF) kernel [28] where \u03bc is a hyperparameter that we set to 1 \u221a 2 in our experiments.", "startOffset": 126, "endOffset": 130}, {"referenceID": 25, "context": "Usually, we do not observe negative facts and thus resort to sampling corrupted ground atoms as done in previous work [29].", "startOffset": 118, "endOffset": 122}, {"referenceID": 8, "context": "We can use NTPs for ILP [9] by gradient descent instead of a combinatorial search over the space of rules as, for example, done by the First Order Inductive Learner (FOIL) [30].", "startOffset": 24, "endOffset": 27}, {"referenceID": 26, "context": "We can use NTPs for ILP [9] by gradient descent instead of a combinatorial search over the space of rules as, for example, done by the First Order Inductive Learner (FOIL) [30].", "startOffset": 172, "endOffset": 176}, {"referenceID": 8, "context": "Specifically, we are using the concept of learning from entailment [9] to induce rules that let us prove known ground atoms, but that do not give high proof success scores to corrupted ground atoms.", "startOffset": 67, "endOffset": 70}, {"referenceID": 27, "context": "Inspired by [31], we use rule templates for convenience which define the structure of multiple parameterized rules in a concise way by specifying the number of parameterized rules that should be instantiated for a given rule structure.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "To speed up learning subsymbolic representations, we train NTPs jointly with ComplEx [7] (Appendix A).", "startOffset": 85, "endOffset": 88}, {"referenceID": 25, "context": "Consistent with previous work, we carry out experiments on four benchmark KBs and compare ComplEx with the NTP and NTP\u03bb in terms of area under the Precision-Recall-curve (AUC-PR) on the Countries KB, and Mean Reciprocal Rank (MRR) and HITS@m [29] on the other KBs.", "startOffset": 242, "endOffset": 246}, {"referenceID": 28, "context": "Countries The Countries KB is a dataset introduced by [32] for testing reasoning capabilities of neural link prediction models.", "startOffset": 54, "endOffset": 58}, {"referenceID": 29, "context": "We follow [33] and split countries randomly into a training set of 204 countries (train), a development set of 20 countries (dev), and a test set of 20 countries (test), such that every dev and test country has at least one neighbor in the training set.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "Kinship, Nations & UMLS We use the Nations, Alyawarra kinship (Kinship) and Unified Medical Language System (UMLS) KBs from [10].", "startOffset": 124, "endOffset": 128}, {"referenceID": 30, "context": "Another method for inducing rules in a differentiable way for automated KB completion has been introduced recently by [34] and our evaluation setup is equivalent to their Protocol II.", "startOffset": 118, "endOffset": 122}, {"referenceID": 31, "context": "Combining neural and symbolic approaches to relational learning and reasoning has a long tradition and let to various proposed architectures over the past decades (see [35] for a review).", "startOffset": 168, "endOffset": 172}, {"referenceID": 32, "context": ", EBL-ANN [36], KBANN [37] and C-ILP [38]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 33, "context": ", EBL-ANN [36], KBANN [37] and C-ILP [38]).", "startOffset": 22, "endOffset": 26}, {"referenceID": 34, "context": ", EBL-ANN [36], KBANN [37] and C-ILP [38]).", "startOffset": 37, "endOffset": 41}, {"referenceID": 35, "context": ", SHRUTI [39], Neural Prolog [40], CLIP++ [41], Lifted Relational Neural Networks [42], and TensorLog [43]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 36, "context": ", SHRUTI [39], Neural Prolog [40], CLIP++ [41], Lifted Relational Neural Networks [42], and TensorLog [43]).", "startOffset": 29, "endOffset": 33}, {"referenceID": 37, "context": ", SHRUTI [39], Neural Prolog [40], CLIP++ [41], Lifted Relational Neural Networks [42], and TensorLog [43]).", "startOffset": 42, "endOffset": 46}, {"referenceID": 38, "context": ", SHRUTI [39], Neural Prolog [40], CLIP++ [41], Lifted Relational Neural Networks [42], and TensorLog [43]).", "startOffset": 82, "endOffset": 86}, {"referenceID": 39, "context": ", SHRUTI [39], Neural Prolog [40], CLIP++ [41], Lifted Relational Neural Networks [42], and TensorLog [43]).", "startOffset": 102, "endOffset": 106}, {"referenceID": 40, "context": "Logic Tensor Networks [44] are in spirit similar to NTPs, but need to ground first-order logic rules.", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "Recent question-answering architectures such as Neural Reasoner [15], Query-Answer Neural Networks [17] or ReasoNet [18] translate query representations implicitly in a vector space without explicit rule representations and can thus not easily incorporate domain-specific knowledge.", "startOffset": 64, "endOffset": 68}, {"referenceID": 16, "context": "Recent question-answering architectures such as Neural Reasoner [15], Query-Answer Neural Networks [17] or ReasoNet [18] translate query representations implicitly in a vector space without explicit rule representations and can thus not easily incorporate domain-specific knowledge.", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "Furthermore, NTPs are related to path encoding models [14, 16], but instead of encoding paths to predict a target predicate, reasoning steps in NTPs are explicit and only the comparison of symbols uses subsymbolic representations.", "startOffset": 54, "endOffset": 62}, {"referenceID": 15, "context": "Furthermore, NTPs are related to path encoding models [14, 16], but instead of encoding paths to predict a target predicate, reasoning steps in NTPs are explicit and only the comparison of symbols uses subsymbolic representations.", "startOffset": 54, "endOffset": 62}, {"referenceID": 41, "context": "Another line of work [45\u201349] regularizes distributed representations via domain-specific rules, but does not learn such rules from data and only supports a restricted subset of first-order logic.", "startOffset": 21, "endOffset": 28}, {"referenceID": 42, "context": "Another line of work [45\u201349] regularizes distributed representations via domain-specific rules, but does not learn such rules from data and only supports a restricted subset of first-order logic.", "startOffset": 21, "endOffset": 28}, {"referenceID": 43, "context": "Another line of work [45\u201349] regularizes distributed representations via domain-specific rules, but does not learn such rules from data and only supports a restricted subset of first-order logic.", "startOffset": 21, "endOffset": 28}, {"referenceID": 44, "context": "Another line of work [45\u201349] regularizes distributed representations via domain-specific rules, but does not learn such rules from data and only supports a restricted subset of first-order logic.", "startOffset": 21, "endOffset": 28}, {"referenceID": 45, "context": "Another line of work [45\u201349] regularizes distributed representations via domain-specific rules, but does not learn such rules from data and only supports a restricted subset of first-order logic.", "startOffset": 21, "endOffset": 28}, {"referenceID": 46, "context": "NTPs are constructed from Prolog\u2019s backward chaining and are thus related to Unification Neural Networks [50, 51].", "startOffset": 105, "endOffset": 113}, {"referenceID": 47, "context": "NTPs are constructed from Prolog\u2019s backward chaining and are thus related to Unification Neural Networks [50, 51].", "startOffset": 105, "endOffset": 113}, {"referenceID": 26, "context": "As NTPs can learn rules from data, they are related to ILP systems such as FOIL [30], Sherlock [52] and meta-interpretive learning of higher-order dyadic Datalog (Metagol) [53].", "startOffset": 80, "endOffset": 84}, {"referenceID": 48, "context": "As NTPs can learn rules from data, they are related to ILP systems such as FOIL [30], Sherlock [52] and meta-interpretive learning of higher-order dyadic Datalog (Metagol) [53].", "startOffset": 95, "endOffset": 99}, {"referenceID": 49, "context": "As NTPs can learn rules from data, they are related to ILP systems such as FOIL [30], Sherlock [52] and meta-interpretive learning of higher-order dyadic Datalog (Metagol) [53].", "startOffset": 172, "endOffset": 176}, {"referenceID": 30, "context": "Recently, [34] introduced a differentiable rule learning system based on TensorLog and a neural network controller similar to LSTMs [54].", "startOffset": 10, "endOffset": 14}, {"referenceID": 50, "context": "Recently, [34] introduced a differentiable rule learning system based on TensorLog and a neural network controller similar to LSTMs [54].", "startOffset": 132, "endOffset": 136}, {"referenceID": 21, "context": "To overcome computational limitations of end-to-end differentiable proving, we want to investigate the use of hierarchical attention [25] and recent reinforcement learning methods such as Monte Carlo tree search [55, 56] that have been used for learning to play Go [57] and chemical synthesis planning [58].", "startOffset": 133, "endOffset": 137}, {"referenceID": 51, "context": "To overcome computational limitations of end-to-end differentiable proving, we want to investigate the use of hierarchical attention [25] and recent reinforcement learning methods such as Monte Carlo tree search [55, 56] that have been used for learning to play Go [57] and chemical synthesis planning [58].", "startOffset": 212, "endOffset": 220}, {"referenceID": 52, "context": "To overcome computational limitations of end-to-end differentiable proving, we want to investigate the use of hierarchical attention [25] and recent reinforcement learning methods such as Monte Carlo tree search [55, 56] that have been used for learning to play Go [57] and chemical synthesis planning [58].", "startOffset": 212, "endOffset": 220}, {"referenceID": 53, "context": "To overcome computational limitations of end-to-end differentiable proving, we want to investigate the use of hierarchical attention [25] and recent reinforcement learning methods such as Monte Carlo tree search [55, 56] that have been used for learning to play Go [57] and chemical synthesis planning [58].", "startOffset": 265, "endOffset": 269}, {"referenceID": 54, "context": "To overcome computational limitations of end-to-end differentiable proving, we want to investigate the use of hierarchical attention [25] and recent reinforcement learning methods such as Monte Carlo tree search [55, 56] that have been used for learning to play Go [57] and chemical synthesis planning [58].", "startOffset": 302, "endOffset": 306}, {"referenceID": 55, "context": "Lastly, we are interested in applying NTPs to automated proving of mathematical theorems, either in logical or natural language form, similar to recent approaches by [59] and [60].", "startOffset": 166, "endOffset": 170}], "year": 2017, "abstractText": "We introduce neural networks for end-to-end differentiable theorem proving that operate on dense vector representations of symbols. These neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in Prolog. Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations. By using gradient descent, the resulting neural network can be trained to infer facts from a given incomplete knowledge base. It learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove facts, (iii) induce logical rules, and (iv) use provided and induced logical rules for complex multi-hop reasoning. We demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, on four benchmark knowledge bases while at the same time inducing interpretable function-free first-order logic rules.", "creator": "LaTeX with hyperref package"}}}