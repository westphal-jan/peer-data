{"id": "1705.11192", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols", "abstract": "Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.", "histories": [["v1", "Wed, 31 May 2017 17:47:55 GMT  (788kb,D)", "http://arxiv.org/abs/1705.11192v1", "Submitted to NIPS 2017. The extended abstract was presented at ICLR 2017 workshop track"]], "COMMENTS": "Submitted to NIPS 2017. The extended abstract was presented at ICLR 2017 workshop track", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.CV cs.MA", "authors": ["serhii havrylov", "ivan titov"], "accepted": true, "id": "1705.11192"}, "pdf": {"name": "1705.11192.pdf", "metadata": {"source": "CRF", "title": "Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols", "authors": ["Serhii Havrylov"], "emails": ["s.havrylov@uva.nl", "ititov@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "With the rapid advances in machine learning in recent years, the goal of intelligent agents communicating with each other and communicating with humans has transformed from a hot topic of philosophical debate into a practical engineering problem. It is believed that supervised learning alone will not provide a solution to this challenge (Mikolov et al., 2015). Furthermore, even learning natural language from an interaction between humans and an agent may not require the most efficient and scalable approach. These, as well as other considerations (e.g. a better understanding of principles guiding the development and emergence of natural languages), have motivated previous research into setups in which agents invent a communication protocol that allows them to succeed in a given collaborative task (Kirby, 2002; Wagner et al., 2003) for an overview of previous work. We continue this line of research and look specifically at an attitude in which the collaborative task is a game. Neural network models have demonstrated how to successfully induce a communication protocol (for this communication protocol) in 2016."}, {"heading": "2 Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Agents\u2019 architectures", "text": "The sender and the receiver are implemented as LSTM networks (Hochreiter and Schmidhuber, 1997).Figure 1 shows the outline of the model architecture, in which diamond-shaped dashed and fixed arrows represent scanning, copying and deterministic functions respectively.The inputs to the sender are the target image t and the special token < S >, which marks the beginning of a message. In light of these inputs, the sender generates the next token wi in an order by scanning from the categorical distribution Cat (pti), where p t i = softmax (Wh s i + b).Here h s i is the hidden state of the sender's scanning capability and can be calculated as2 hsi = LSTM (h s i \u2212 1, wi \u2212 1).In the first step, we have h s 0 = p (f (t))), where the state L (t) is an affine of the image transformation characteristics (f)."}, {"heading": "2.2 Learning", "text": "The receiver model has already been studied by Chrupa\u0142a et al. (2015) and is known as Imaginet. It was used to learn visually grounded speech representations from coupled text and image input. The real challenge is to learn the sender agent. His arithmetic diagram contains samples that make him indistinguishable. In the following we will discuss methods for estimating the gradients of the loss function in Equation (1)."}, {"heading": "2.2.1 REINFORCE", "text": "REINFORCE is a method of probability ratio (Williams, 1992), which provides a simple method of estimating the gradients of the loss function with respect to the parameters of stochastic politics. We are interested in optimizing the loss function from Equation (1). The REINFORCE algorithm allows the use of gradient-based optimization methods by estimating gradients such as: \u2202 L\u03c6, \u03b8 \u03c6 = Ep\u03c6 (\u00b7 | t) [lt \u2202 log p\u03c6 (mt | t) 20s] (2) Where there is a learning signal, there is an inner part of the expectation in Equation (1). However, the precise calculation of the gradient may not be feasible due to the enormous number of message configurations. Usually, a Monte Carlo approximation of expectation is used. Training models with REINFORCE can be difficult due to the high variance of the estimator. We observed a more reliable learning when introducing the stabilization techniques proposed by Mnih and Gregor (2014)."}, {"heading": "2.2.2 Gumbel-softmax estimator", "text": "In the typical RL task formulation, an acting agent does not have access to the full environment specification, or, even if this is the case, the environment is indistinguishable. In our setup, an agent trained by a REINFORCE-like algorithm would make vastly insufficient use of the available information about the environment. However, as a solution, we consider the replacement of one-hot-coded symbols w, V, sampled from a categorical distribution with a continuous relaxation trick w obtained from the Gumbel Softmax distribution (Jang et al., 2016; Maddison et al., 2016). Consider a categorical distribution with event probabilities p1, p2,..., the Gumbel Softmax trick runs as follows: Get K samples {uk} Kk = 1 from a uniformly distributed variable u U (0, 1), transform each sample with the function glog = continuously distributed U (K) and then distribute a rubber-1."}, {"heading": "2.2.3 Straight-through Gumbel-softmax estimator", "text": "To avoid the problems mentioned above, we discredit w-passes with Argmax in advance, which then becomes an ordinary sample from the original categorical distribution. Nevertheless, we use continuous relaxation in the return, effectively assuming that \u2202 L passes w-passes w-passes. This distorted estimator is known as a continuous Gumbel Softmax (ST-GS) estimator (Jang et al., 2016; Bengio et al., 2013). As a result of applying this trick, there is no difference in the use of messages during the training and testing phases, which differs from previous differentiable frameworks for learning communication protocols (Foerster et al., 2016). Due to the use of ST-GS, the forward flow is not dependent on temperature. Nevertheless, it influences the gradient values during the backward run. As previously discussed, low values for this process provide better approximation values for argmax. As the derivation of a maximum state pass is not a problem anywhere other than the range of state passes, we would allow for a maximum in the limit of 1."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Tabula rasa communication", "text": "In fact, we are in a position to go in search of a solution that enables us, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in the position we are in."}, {"heading": "3.2 Qualitative analysis of the learned language", "text": "To better understand the nature of the language learned, we examined a small subset of sentences generated by the model with a maximum message length of 5. Figure 3 shows some examples from the MSCOCO 2014 validation set that correspond to the (5747 * * * *) code. 3 images in this subset depict animals. On the other hand, it appears that images for (* * * 5747 *) code do not correspond to a predefined category, suggesting that the developed language implements some kind of hierarchical encoding, which is interesting in itself since the model was not explicitly limited to the use of an animal in the image. The same figure shows that the message (5747 7125 * *) corresponds to a particular type of bear. The model suggests that the developed language implements some kind of hierarchical encoding."}, {"heading": "3.3 Indirect grounding of artificial language in natural language", "text": "Since the ultimate goal is to ensure that communication is achieved with a language that is understandable to humans, we should prefer protocols that resemble a natural language in a way. A possible solution is to use the Kullback-Leibler (KL) divergence regularization DKL (i.e. texts) and approximate the original KL divergence with DKL (m | t).Since we do not have access to pNL (m), we train a language model that uses available patterns (i.e., the original KL divergences with DKL (m | t).This regularization provides indirect monitoring by promoting generated messages in the natural language while maintaining high entropy for the communication protocol."}, {"heading": "3.4 Variational Autoencoder interpretation", "text": "The described indirect grounding of artificial language in a natural language can be interpreted as a specific instance of a variational autoencoder (VAE) (Kingma and Welling, 2013). There are no gold standard messages for images. A message can therefore be treated as a variable sequence of discrete latent variables in variable length. On the other hand, images are always given and therefore correspond to the observed variable within the VAE framework. The trained language model p\u03c9 (m) serves as a precursor of latent variables. The receiver agent corresponds to the generative part of the VAE, although it uses a slightly different loss for the reconstruction error (hinge loss instead of log probability)."}, {"heading": "3.5 Direct grounding of artificial language in natural language", "text": "Minimizing the KL deviation from the natural language to the learned protocol favors generated messages, both of which have a high probability of natural language distribution and high entropy, with the goal of ensuring that the statistical properties of the protocol are similar to those of natural language. However, words are unlikely to retain their original meaning (e.g. the word \"red\" must not refer to \"red\" in the protocol).To achieve this, we considered a more direct form of monitoring and additionally trained the sender on the picture captioning task (Vinyals et al., 2015).To simulate the semi-supervised setup, we divided the previously created training set into two parts. The randomly selected 25% of the records were used to train the sender on the picture captioning task Lcaption, and the remaining 75% were used to train the sender and the recipient to solve the reference game Lgame."}, {"heading": "4 ST-GS estimator as a pseudogradient estimator", "text": "Poljak and Tsypkin (1973) have shown that under certain assumptions about the learning rate, a very wide class of pseudogradient methods converge to the critical point of function J. To investigate whether the direction provided by ST-GS is a pseudogradient, we used a stochastic gradient estimator that can approximate a point product between arbitrary direction in the parameter space and the true gradient: J (u + \u03b4) \u2212 J (u \u2212 \u03b4) 2 = \u03b4T-J (u) + O (2) (5) In our case, J (u) is a Monte Carlo approximation of the equation (1). To reduce the deviation in the point product evaluation (Bhatnagar et al., 2012), the same Gumbel noise samples were used to evaluate J (u)."}, {"heading": "5 Related work", "text": "There is a long history of work on the emergence of language in multi-agent systems (Kirby, 2002; Wagner et al., 2003; Steels, 2005; Nolfi and Mirolli, 2009; Golland et al., 2010). The youngest generation relied on deep learning techniques. More specifically, Foerster et al. (2016) proposed differentiated interagent learning (DIAL), which used it to solve puzzles in a multi-agent setting. Actors in their work were able to communicate by sending one-bit messages. Jorge et al. (2016) chose DIAL to solve the interactive image search task with two actors involved in the task. These actors successfully developed a language consisting of unilaterally coded atomic symbols. In contrast, Lazaridou et al applied the political gradient method to learn agents involved in a reference game. Unlike symbols used by us, they used symbols."}, {"heading": "6 Conclusion", "text": "In this work, we have shown that agents modelled using neural networks can successfully invent a language consisting of sequences of discrete characters. Despite the widespread view that it is difficult to train such models, we have proposed an efficient learning strategy based on the consistent Gumbel Softmax estimator. We have conducted an analysis of the language learned and the corresponding learning dynamics. We have also considered two methods to inject knowledge of natural language. In future work, we would like to extend this approach to modelling practical dialogue systems, thereby reducing the need for human supervision."}, {"heading": "Acknowledgments", "text": "This project is supported by SAP ICN and ERC Starting Grant BroadSem (678254) and we thank Jelle Zuidema for her helpful comments."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Learning to communicate through interaction, rather than relying on explicit super-<lb>vision, is often considered a prerequisite for developing a general AI. We study a<lb>setting where two agents engage in playing a referential game and, from scratch,<lb>develop a communication protocol necessary to succeed in this game. Unlike<lb>previous work, we require that messages they exchange, both at train and test time,<lb>are in the form of a language (i.e. sequences of discrete symbols). We compare a<lb>reinforcement learning approach and one using a differentiable relaxation (straight-<lb>through Gumbel-softmax estimator (Jang et al., 2016)) and observe that the latter is<lb>much faster to converge and it results in more effective protocols. Interestingly, we<lb>also observe that the protocol we induce by optimizing the communication success<lb>exhibits a degree of compositionality and variability (i.e. the same information can<lb>be phrased in different ways), both properties characteristic of natural languages.<lb>As the ultimate goal is to ensure that communication is accomplished in natural<lb>language, we also perform experiments where we inject prior information about<lb>natural language into our model and study properties of the resulting protocol.", "creator": "LaTeX with hyperref package"}}}