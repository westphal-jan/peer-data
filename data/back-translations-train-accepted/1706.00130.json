{"id": "1706.00130", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "Teaching Machines to Describe Images via Natural Language Feedback", "abstract": "Robots will eventually be part of every household. It is thus critical to enable algorithms to learn from and be guided by non-expert users. In this paper, we bring a human in the loop, and enable a human teacher to give feedback to a learning agent in the form of natural language. We argue that a descriptive sentence can provide a much stronger learning signal than a numeric reward in that it can easily point to where the mistakes are and how to correct them. We focus on the problem of image captioning in which the quality of the output can easily be judged by non-experts. We propose a hierarchical phrase-based captioning model trained with policy gradients, and design a feedback network that provides reward to the learner by conditioning on the human-provided feedback. We show that by exploiting descriptive feedback our model learns to perform better than when given independently written human captions.", "histories": [["v1", "Thu, 1 Jun 2017 00:24:55 GMT  (3938kb,D)", "https://arxiv.org/abs/1706.00130v1", "13 pages"], ["v2", "Mon, 5 Jun 2017 16:47:40 GMT  (3938kb,D)", "http://arxiv.org/abs/1706.00130v2", "13 pages"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV cs.HC", "authors": ["huan ling", "sanja fidler"], "accepted": true, "id": "1706.00130"}, "pdf": {"name": "1706.00130.pdf", "metadata": {"source": "CRF", "title": "Teaching Machines to Describe Images via Natural Language Feedback", "authors": ["Huan Ling", "Sanja Fidler"], "emails": ["linghuan@cs.toronto.edu", "fidler@cs.toronto.edu"], "sections": [{"heading": "1 Introduction", "text": "In the era in which we are slowly entering people's lives, we will be able to get to grips with their needs."}, {"heading": "2 Related Work", "text": "In this context, it has to be stated that most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to move, to move, to move, to move, to move, to move, to dance, to move, to move, to move, to move, to move, to move, to move, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "3 Our Approach", "text": "Our framework consists of a new, phrase-based subtitling model that is trained with policy histories and incorporates the natural language feedback of a human teacher. Although there are a number of subtitling methods, we design our own, based on phrases and allowing for natural guidance from a non-expert. Specifically, we argue that the strongest learning signal is given when the feedback describes an error, such as a single wrong word or phrase in a caption. One example is in Fig. 1. This is the most effective way to teach a learning child. To avoid analyzing the sentences generated at test time, we aim to predict phrases directly with our subtitling model. First, we describe our phrase-based subtitler, then describe our feedback collection process, and finally, we suggest how to use feedback as a guideline for optimizing policy histories."}, {"heading": "3.1 Phrase-based Image Captioning", "text": "In fact, it is a very rare disease, but not a very rare disease, but a very strange disease that has developed in recent years, \"he said in an interview with the New York Times."}, {"heading": "3.2 Crowd-sourcing Human Feedback", "text": "We aim to put a person in a loop if we do not understand the caption model correctly. We create a web interface that allows us to collect larger-scale feedback information about AMT. Our interface is similar to the one shown in Fig. 1, and we provide additional visualizations in the appendix. We also make it available online on our project page. In particular, we take a snapshot of our model and generatecaptions for a subset of MS-COCO images [20] using greedy decoding. In our experiments, we take the model that was trained with MLE objectification. We do two rounds of remark. In the first round, the remark is shown an image and we are asked to judge the quality of the caption by choosing between: perfect, acceptable, grammatical errors minor or large errors. We asked the markers to choose minor and major errors when the subtitles contained errors in the semantics."}, {"heading": "3.3 Feedback Network", "text": "Our goal is to incorporate natural language feedback into the learning process, and the feedback we collect provides rich information on how to improve feedback: it conveys the position of the error and typically suggests how to correct it, as shown in Table 2. This provides a strong supervisory signal that we want to use within our RL framework. In particular, we design a neural network that provides additional reward based on the feedback rate, using the Feedback Network (FBN). We first explain our feedback network and show how we use its results in RL.Note that RL training requires us to generate examples (captions) from the model."}, {"heading": "3.4 Policy Gradient Optimization using Natural Language Feedback", "text": "We directly follow [29, 28] the optimization of the desired image processing quantities with the aim of obtaining the expected reward when we receive the complete reward, i.e., the reward can be any word from the vocabulary of automatic metrics, the weighted sum of which in our case is also the reward from the feedback. The goal for learning the parameters of the model is the expected reward that is obtained when completing the model. (ws1,., w s T) (w s T) (w s T) is the expected reward that we received."}, {"heading": "4 Experimental Results", "text": "In fact, most of them are able to outdo themselves."}, {"heading": "5 Conclusion", "text": "During this time, we have the opportunity to initiate a learning process in the form of a natural language, focusing on the problem of image editing."}, {"heading": "Acknowledgment", "text": "We are grateful for the support of NVIDIA for donating the GPUs used for this research. This work was partially supported by NSERC. We also thank Relu Patrascu for supporting the infrastructure."}, {"heading": "A Qualitative Examples: Phrase-based Captioning", "text": "We provide qualitative results from our phrase model in Fig. 6. The figure shows the attention cards, the generated phrase under each card and the predicted phrase name."}, {"heading": "B Feedback Crowd-Sourcing Interface", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C Examples of Collected Feedback", "text": "In Table 8 we show examples of collected feedback for our reference model (MLE)."}, {"heading": "D Qualitative Examples: RL that Incorporates Feedback", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Towards diverse and natural image descriptions via a conditional gan", "author": ["Bo Dai", "Dahua Lin", "Raquel Urtasun", "Sanja Fidler"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2017}, {"title": "Visual dialog", "author": ["A. Das", "S. Kottur", "K. Gupta", "A. Singh", "D. Yadav", "J.M. Moura", "D. Parikh", "D. Batra"], "venue": "arXiv:1611.08669", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Policy shaping: Integrating human feedback with reinforcement learning", "author": ["Shane Griffith", "Kaushik Subramanian", "Jonathan Scholz", "Charles L. Isbell", "Andrea Lockerd Thomaz"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Reinforcement learning via practice and critique advice", "author": ["K. Judah", "S. Roy", "A. Fern", "T. Dietterich"], "venue": "AAAI", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Beating atari with natural language guided reinforcement learning", "author": ["Russell Kaplan", "Christopher Sauer", "Alexander Sosa"], "venue": "In arXiv:1704.05539,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S. Zemel"], "venue": "CoRR, abs/1411.2539,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Cynthia Breazeal", "author": ["W. Bradley Knox"], "venue": ", and Peter Stone. Training a robot via human feedback: A case study. In International Conference on Social Robotics", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Reinforcement learning from simultaneous human and mdp reward", "author": ["W. Bradley Knox", "Peter Stone"], "venue": "In Intl. Conf. on Autonomous Agents and Multiagent Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Tega: A social robot", "author": ["Jacqueline Kory Westlund", "Jin Joo Lee", "Luke Plummer", "Fardad Faridi", "Jesse Gray", "Matt Berlin", "Harald Quintus-Bosz", "Robert Hartmann", "Mike Hess", "Stacy Dyer", "Kristopher dos Santos", "Sigurdhur \u00d6rn Adhalgeirsson", "Goren Gordon", "Samuel Spaulding", "Marayna Martinez", "Madhurima Das", "Maryam Archie", "Sooyeon Jeong", "Cynthia Breazeal"], "venue": "In International Conference on Human-Robot Interaction,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "A hierarchical approach for generating descriptive image paragraphs", "author": ["Jonathan Krause", "Justin Johnson", "Ranjay Krishna", "Li Fei-Fei"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Guiding a reinforcement learner with natural language advice: Initial results in robocup soccer", "author": ["G. Kuhlmann", "P. Stone", "R. Mooney", "J. Shavlik"], "venue": "AAAI Workshop on Supervisory Control of Learning and Adaptive Systems", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Phrase-based image captioning", "author": ["Remi Lebret", "Pedro O. Pinheiro", "Ronan Collobert"], "venue": "In arXiv:1502.03671,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Dialogue learning with human-in-the-loop", "author": ["Jiwei Li", "Alexander H. Miller", "Sumit Chopra", "Marc\u2019Aurelio Ranzato", "Jason Weston"], "venue": "In arXiv:1611.09823,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Michel Galley", "Jianfeng Gao", "Dan Jurafsky"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In ECCV,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Improved image captioning via policy gradient optimization of spider", "author": ["Siqi Liu", "Zhenhai Zhu", "Ning Ye", "Sergio Guadarrama", "Kevin Murphy"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Incorporating advice into agents that learn from reinforcements", "author": ["Richard Maclin", "Jude W. Shavlik"], "venue": "In National Conference on Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1994}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": "In ICLR,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Socially assistive robotics: Human augmentation vs. automation", "author": ["Maja J. Matari\u010d"], "venue": "Science Robotics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2017}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["Andrew Y. Ng", "Daishi Harada", "Stuart J. Russell"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "How to construct deep recurrent neural networks. In Association for Computational Linguistics (ACL) System Demonstrations", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "In arXiv:1511.06732,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Selfcritical sequence training for image captioning", "author": ["Steven J. Rennie", "Etienne Marcheret", "Youssef Mroueh", "Jarret Ross", "Vaibhava Goel"], "venue": "In arXiv:1612.00563,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J. Maddison", "Arthur Guez", "Laurent Sifre", "George van den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot", "Sander Dieleman", "Dominik Grewe", "John Nham", "Nal Kalchbrenner", "Ilya Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis"], "venue": "search. Nature,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Neuroaesthetics in fashion: Modeling the perception of beauty", "author": ["Edgar Simo-Serra", "Sanja Fidler", "Francesc Moreno-Noguer", "Raquel Urtasun"], "venue": "In CVPR,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "phi-lstm: A phrase-based hierarchical lstm model for image captioning", "author": ["Ying Hua Tan", "Chee Seng Chan"], "venue": "In ACCV,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Reinforcement learning with human teachers: Evidence of feedback and guidance", "author": ["A. Thomaz", "C. Breazeal"], "venue": "AAAI", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le"], "venue": "In arXiv:1506.05869,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Dialog-based language learning", "author": ["Jason Weston"], "venue": "In arXiv:1604.06045,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "In Machine Learning,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1992}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Training an adaptive dialogue policy for interactive learning of visually grounded word meanings", "author": ["Yanchao Yu", "Arash Eshghi", "Oliver Lemon"], "venue": "In Proc. of SIGDIAL,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "The burchak corpus: a challenge data set for interactive learning of visually grounded word meanings", "author": ["Yanchao Yu", "Arash Eshghi", "Gregory Mills", "Oliver Lemon"], "venue": "In Workshop on Vision and Language,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2017}], "referenceMentions": [{"referenceID": 32, "context": "is slowly finding its way into everyone\u2019s lives, be in the form of social bots [35, 2], personal assistants [24, 13, 31], or household robots [1], it becomes critical to allow non-expert users to teach and guide their robots [36, 18].", "startOffset": 79, "endOffset": 86}, {"referenceID": 21, "context": "is slowly finding its way into everyone\u2019s lives, be in the form of social bots [35, 2], personal assistants [24, 13, 31], or household robots [1], it becomes critical to allow non-expert users to teach and guide their robots [36, 18].", "startOffset": 108, "endOffset": 120}, {"referenceID": 10, "context": "is slowly finding its way into everyone\u2019s lives, be in the form of social bots [35, 2], personal assistants [24, 13, 31], or household robots [1], it becomes critical to allow non-expert users to teach and guide their robots [36, 18].", "startOffset": 108, "endOffset": 120}, {"referenceID": 28, "context": "is slowly finding its way into everyone\u2019s lives, be in the form of social bots [35, 2], personal assistants [24, 13, 31], or household robots [1], it becomes critical to allow non-expert users to teach and guide their robots [36, 18].", "startOffset": 108, "endOffset": 120}, {"referenceID": 33, "context": "is slowly finding its way into everyone\u2019s lives, be in the form of social bots [35, 2], personal assistants [24, 13, 31], or household robots [1], it becomes critical to allow non-expert users to teach and guide their robots [36, 18].", "startOffset": 225, "endOffset": 233}, {"referenceID": 15, "context": "is slowly finding its way into everyone\u2019s lives, be in the form of social bots [35, 2], personal assistants [24, 13, 31], or household robots [1], it becomes critical to allow non-expert users to teach and guide their robots [36, 18].", "startOffset": 225, "endOffset": 233}, {"referenceID": 27, "context": "There have been significant advances in a variety of domains such as games [30, 25], robotics [17], and even fields like vision and NLP [29, 19].", "startOffset": 75, "endOffset": 83}, {"referenceID": 22, "context": "There have been significant advances in a variety of domains such as games [30, 25], robotics [17], and even fields like vision and NLP [29, 19].", "startOffset": 75, "endOffset": 83}, {"referenceID": 14, "context": "There have been significant advances in a variety of domains such as games [30, 25], robotics [17], and even fields like vision and NLP [29, 19].", "startOffset": 94, "endOffset": 98}, {"referenceID": 26, "context": "There have been significant advances in a variety of domains such as games [30, 25], robotics [17], and even fields like vision and NLP [29, 19].", "startOffset": 136, "endOffset": 144}, {"referenceID": 16, "context": "There have been significant advances in a variety of domains such as games [30, 25], robotics [17], and even fields like vision and NLP [29, 19].", "startOffset": 136, "endOffset": 144}, {"referenceID": 31, "context": "Several works explored the idea of incorporating humans in the learning process, in order to help the reinforcement learning agent to learn faster [34, 12, 11, 6, 5].", "startOffset": 147, "endOffset": 165}, {"referenceID": 9, "context": "Several works explored the idea of incorporating humans in the learning process, in order to help the reinforcement learning agent to learn faster [34, 12, 11, 6, 5].", "startOffset": 147, "endOffset": 165}, {"referenceID": 8, "context": "Several works explored the idea of incorporating humans in the learning process, in order to help the reinforcement learning agent to learn faster [34, 12, 11, 6, 5].", "startOffset": 147, "endOffset": 165}, {"referenceID": 3, "context": "Several works explored the idea of incorporating humans in the learning process, in order to help the reinforcement learning agent to learn faster [34, 12, 11, 6, 5].", "startOffset": 147, "endOffset": 165}, {"referenceID": 2, "context": "Several works explored the idea of incorporating humans in the learning process, in order to help the reinforcement learning agent to learn faster [34, 12, 11, 6, 5].", "startOffset": 147, "endOffset": 165}, {"referenceID": 31, "context": "This feedback typically comes in the form of a simple numerical (or \u201cgood\u201d/\u201cbad\u201d) reward which is used to either shape the MDP reward [34] or directly shape the policy of the learner [5].", "startOffset": 134, "endOffset": 138}, {"referenceID": 2, "context": "This feedback typically comes in the form of a simple numerical (or \u201cgood\u201d/\u201cbad\u201d) reward which is used to either shape the MDP reward [34] or directly shape the policy of the learner [5].", "startOffset": 183, "endOffset": 186}, {"referenceID": 19, "context": "In pioneering work, [22] translated natural language advice into a short program which was used to bias action selection.", "startOffset": 20, "endOffset": 24}, {"referenceID": 19, "context": "While this is possible in limited domains such as in navigating a maze [22] or learning to play a soccer game [15], it can hardly scale to the real scenarios with large action spaces requiring versatile language feedback.", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "While this is possible in limited domains such as in navigating a maze [22] or learning to play a soccer game [15], it can hardly scale to the real scenarios with large action spaces requiring versatile language feedback.", "startOffset": 110, "endOffset": 114}, {"referenceID": 26, "context": "We show how to incorporate this information in Policy Gradient RL [29], and show that we can improve over RL that has access to the same amount of ground-truth captions.", "startOffset": 66, "endOffset": 70}, {"referenceID": 31, "context": "[34] exploits humans in the loop to teach an agent to cook in a virtual kitchen.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Reward shaping [26] is used to incorporate this information in the MDP.", "startOffset": 15, "endOffset": 19}, {"referenceID": 3, "context": "[6] iterates between \u201cpractice\u201d, during which the agent interacts with the real environment, and a critique session where a human labels any subset of the chosen actions as good or bad.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "In [12], the authors compare different ways of incorporating human feedback, including reward shaping, Q augmentation, action biasing, and control sharing.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "The same authors implement their TAMER framework on a real robotic platform [11].", "startOffset": 76, "endOffset": 80}, {"referenceID": 2, "context": "[5] proposes policy shaping which incorporates right/wrong feedback by utilizing it as direct policy labels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[22]\u2019s pioneering work translated advice to a short program which was then implemented as a neural network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] incorporated natural language advice for a RoboCup simulated soccer task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Parallel to our work, [7] exploits textual advice to improve training time of the A3C algorithm in playing an Atari game.", "startOffset": 22, "endOffset": 25}, {"referenceID": 33, "context": "Recently, [36, 18] incorporates human feedback to improve a text-based QA agent.", "startOffset": 10, "endOffset": 18}, {"referenceID": 15, "context": "Recently, [36, 18] incorporates human feedback to improve a text-based QA agent.", "startOffset": 10, "endOffset": 18}, {"referenceID": 5, "context": "This domain has received significant attention [8, 38, 10], achieving impressive performance on standard benchmarks.", "startOffset": 47, "endOffset": 58}, {"referenceID": 35, "context": "This domain has received significant attention [8, 38, 10], achieving impressive performance on standard benchmarks.", "startOffset": 47, "endOffset": 58}, {"referenceID": 7, "context": "This domain has received significant attention [8, 38, 10], achieving impressive performance on standard benchmarks.", "startOffset": 47, "endOffset": 58}, {"referenceID": 13, "context": "Our phrase model shares the most similarity with [16], but differs in that exploits attention [38], linguistic information, and RL to train.", "startOffset": 49, "endOffset": 53}, {"referenceID": 35, "context": "Our phrase model shares the most similarity with [16], but differs in that exploits attention [38], linguistic information, and RL to train.", "startOffset": 94, "endOffset": 98}, {"referenceID": 18, "context": "Several recent approaches trained the captioning model with policy gradients in order to directly optimize for the desired performance metrics [21, 29, 3].", "startOffset": 143, "endOffset": 154}, {"referenceID": 26, "context": "Several recent approaches trained the captioning model with policy gradients in order to directly optimize for the desired performance metrics [21, 29, 3].", "startOffset": 143, "endOffset": 154}, {"referenceID": 0, "context": "Several recent approaches trained the captioning model with policy gradients in order to directly optimize for the desired performance metrics [21, 29, 3].", "startOffset": 143, "endOffset": 154}, {"referenceID": 36, "context": "Related to our efforts is also work on dialogue based visual representation learning [39, 40], however this work tackles a simpler scenario, and employs a slightly more engineered approach.", "startOffset": 85, "endOffset": 93}, {"referenceID": 37, "context": "Related to our efforts is also work on dialogue based visual representation learning [39, 40], however this work tackles a simpler scenario, and employs a slightly more engineered approach.", "startOffset": 85, "endOffset": 93}, {"referenceID": 16, "context": "We stress that our work differs from the recent efforts in conversation modeling [19] or visual dialog [4] using Reinforcement Learning.", "startOffset": 81, "endOffset": 85}, {"referenceID": 1, "context": "We stress that our work differs from the recent efforts in conversation modeling [19] or visual dialog [4] using Reinforcement Learning.", "startOffset": 103, "endOffset": 106}, {"referenceID": 30, "context": "Our captioning model, forming the base of our approach, uses a hierarchical Recurrent Neural Network, similar to [33, 14].", "startOffset": 113, "endOffset": 121}, {"referenceID": 11, "context": "Our captioning model, forming the base of our approach, uses a hierarchical Recurrent Neural Network, similar to [33, 14].", "startOffset": 113, "endOffset": 121}, {"referenceID": 11, "context": "In [14], the authors use a two-level LSTM to generate paragraphs, while [33] uses it to generate sentences as a sequence of phrases.", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "In [14], the authors use a two-level LSTM to generate paragraphs, while [33] uses it to generate sentences as a sequence of phrases.", "startOffset": 72, "endOffset": 76}, {"referenceID": 35, "context": "Following [38], we use a convolutional neural network in order to extract a set of feature vectors a = (a1, .", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "fword LSTM, dim 256 fout deep decoder [27] fword\u2212phrase mean+3-lay.", "startOffset": 38, "endOffset": 42}, {"referenceID": 35, "context": "As in [38], ct denotes a context vector obtained by applying the attention mechanism to the image.", "startOffset": 6, "endOffset": 10}, {"referenceID": 35, "context": "Following [38], we use a deep output layer [27] in the LSTM and double stochastic attention.", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "Following [38], we use a deep output layer [27] in the LSTM and double stochastic attention.", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "To train our hierarchical model, we first process MS-COCO image caption data [20] using the Stanford Core NLP toolkit [23].", "startOffset": 77, "endOffset": 81}, {"referenceID": 20, "context": "To train our hierarchical model, we first process MS-COCO image caption data [20] using the Stanford Core NLP toolkit [23].", "startOffset": 118, "endOffset": 122}, {"referenceID": 6, "context": "We use the ADAM optimizer [9] with learning rate 0.", "startOffset": 26, "endOffset": 29}, {"referenceID": 17, "context": "captions for a subset of MS-COCO images [20] using greedy decoding.", "startOffset": 40, "endOffset": 44}, {"referenceID": 6, "context": "We use ADAM [9] with learning rate 0.", "startOffset": 12, "endOffset": 15}, {"referenceID": 26, "context": "We follow [29, 28] to directly optimize for the desired image captioning metrics using the Policy Gradient technique.", "startOffset": 10, "endOffset": 18}, {"referenceID": 25, "context": "We follow [29, 28] to directly optimize for the desired image captioning metrics using the Policy Gradient technique.", "startOffset": 10, "endOffset": 18}, {"referenceID": 26, "context": "For completeness, we briefly summarize it here [29].", "startOffset": 47, "endOffset": 51}, {"referenceID": 26, "context": ", the reward can be any of the automatic metrics, their weighted sum [29, 21], or in our case will also include the reward from feedback.", "startOffset": 69, "endOffset": 77}, {"referenceID": 18, "context": ", the reward can be any of the automatic metrics, their weighted sum [29, 21], or in our case will also include the reward from feedback.", "startOffset": 69, "endOffset": 77}, {"referenceID": 34, "context": "L(\u03b8) = \u2212Ews\u223cp\u03b8 [r(w)] (5) To optimize this objective, we follow the reinforce algorithm [37], as also used in [29, 28].", "startOffset": 88, "endOffset": 92}, {"referenceID": 26, "context": "L(\u03b8) = \u2212Ews\u223cp\u03b8 [r(w)] (5) To optimize this objective, we follow the reinforce algorithm [37], as also used in [29, 28].", "startOffset": 110, "endOffset": 118}, {"referenceID": 25, "context": "L(\u03b8) = \u2212Ews\u223cp\u03b8 [r(w)] (5) To optimize this objective, we follow the reinforce algorithm [37], as also used in [29, 28].", "startOffset": 110, "endOffset": 118}, {"referenceID": 26, "context": "We follow [29] to define the baseline b as the reward obtained by performing greedy decoding: b = r(\u0175), \u0175t = argmax p(wt|ht) \u2207\u03b8L(\u03b8) \u2248 \u2212(r(w)\u2212 r(\u0175))\u2207\u03b8 log p\u03b8(w) (8)", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "As in [28], we follow an annealing schedule.", "startOffset": 6, "endOffset": 10}, {"referenceID": 17, "context": "To validate our approach we use the MS-COCO dataset [20].", "startOffset": 52, "endOffset": 56}, {"referenceID": 29, "context": "For all the models (including baselines) we used a pre-trained VGG [32] network to extract image features.", "startOffset": 67, "endOffset": 71}, {"referenceID": 35, "context": "To sanity check our model we compare it to a flat approach (word-RNN only) [38].", "startOffset": 75, "endOffset": 79}, {"referenceID": 35, "context": "Overall, our model performs slightly worse than [38] (0.", "startOffset": 48, "endOffset": 52}, {"referenceID": 35, "context": "12 Table 4: Comparing performance of the flat captioning model [38], and different instantiations of our phrasebased captioning model.", "startOffset": 63, "endOffset": 67}, {"referenceID": 26, "context": "We then test the RL model by optimizing the metric wrt the 5GT captions (as in [29]).", "startOffset": 79, "endOffset": 83}], "year": 2017, "abstractText": "Robots will eventually be part of every household. It is thus critical to enable algorithms to learn from and be guided by non-expert users. In this paper, we bring a human in the loop, and enable a human teacher to give feedback to a learning agent in the form of natural language. We argue that a descriptive sentence can provide a much stronger learning signal than a numeric reward in that it can easily point to where the mistakes are and how to correct them. We focus on the problem of image captioning in which the quality of the output can easily be judged by non-experts. We propose a hierarchical phrase-based captioning model trained with policy gradients, and design a feedback network that provides reward to the learner by conditioning on the human-provided feedback. We show that by exploiting descriptive feedback our model learns to perform better than when given independently written human captions.", "creator": "LaTeX with hyperref package"}}}