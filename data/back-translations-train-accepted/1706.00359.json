{"id": "1706.00359", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "Discovering Discrete Latent Topics with Neural Variational Inference", "abstract": "Topic models have been widely explored as probabilistic generative models of documents. Traditional inference methods have sought closed-form derivations for updating the models, however as the expressiveness of these models grows, so does the difficulty of performing fast and accurate inference over their parameters. This paper presents alternative neural approaches to topic modelling by providing parameterisable distributions over topics which permit training by backpropagation in the framework of neural variational inference. In addition, with the help of a stick-breaking construction, we propose a recurrent network that is able to discover a notionally unbounded number of topics, analogous to Bayesian non-parametric topic models. Experimental results on the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the effectiveness and efficiency of these neural topic models.", "histories": [["v1", "Thu, 1 Jun 2017 15:55:42 GMT  (849kb,D)", "http://arxiv.org/abs/1706.00359v1", "ICML 2017"]], "COMMENTS": "ICML 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.IR cs.LG", "authors": ["yishu miao", "edward grefenstette", "phil blunsom"], "accepted": true, "id": "1706.00359"}, "pdf": {"name": "1706.00359.pdf", "metadata": {"source": "META", "title": "Discovering Discrete Latent Topics with Neural Variational Inference", "authors": ["Yishu Miao", "Edward Grefenstette", "Phil Blunsom"], "emails": ["<yishu.miao@cs.ox.ac.uk>."], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "2. Parameterising Topic Distributions", "text": "In order to reach an efficient conclusion, the Dirichlet distribution (or the Dirichlet process) (Teh et al., 2006) is used as a precursor to generating the parameters of the multinomic distribution (Lead & Lafferty, 2006). Using a conjugation before allows the tractable calculation of the posterior distribution via the latent variables. \"While alternatives have been investigated, such as the log-normal topic distribution (Lead & Lafferty, 2007), additional approximation (e.g. the Laplace approximation (Wang & Lead, 2013), a closed form of the derivatives is required."}, {"heading": "2.1. The Gaussian Softmax Construction", "text": "Deep learning generally uses an energy-based function to construct probability distributions (LeCun et al., 2006), passing a Gaussian random vector through a Softmax function to parameterise the distributions of multinomial document topics. Thus, \u03b8 \u0445 GGSM (\u00b50, \u03c320) is defined as: x \u0445 N (\u00b50, \u03c320) \u03b8 = softmax (WT1 x), where W1 is a linear transformation, and we leave the bias terms for the brevity. \u00b50 and \u03c320 are hyperparameters that we set for a zero mean and a Gaussian type unit variance."}, {"heading": "2.2. The Gaussian Stick Breaking Construction", "text": "In Bayesian's non-parametric models, the nitrogen fracture process (Sethuraman, 1994) is used as a constructive definition of the process of dilementlet in which sequentially drawn Beta1Through this presentation, we use isotropic Gaussian distributions. In our case, we use N (\u00b5, \u03c32) to represent the Gaussian distributions, where \u03c32 is the diagonal of the covariance matrix - random variables define fractions from a stick. In our case, we transform the modeling of multinomial probability parameters into the modeling of the logits of binomial probability parameters using Gaussian latent variables. Specifically, due to a Gaussian sample x, RH, the fraction proportions of RK \u2212 1 are generated into the fraction proportions of RK \u2212 1 using the sigmoid function of S1 = sigmoid function."}, {"heading": "2.3. The Recurrent Stick Breaking Construction", "text": "Recursive neural networks (RNN) are commonly used to model input sequences in the field of deep learning. Here, we consider stick-breaking construction to be a sequential pull from an RNN, which covers an unlimited number of pauses with a finite number of parameters. Due to a Gaussian latent variable x, the recursive neural network fSB (x) generates a sequence of binomial logs. fRNN (x) breaks down as follows: hk = RNNSB (hk \u2212 1) \u03b7k = sigmoid (hTk \u2212 1x), where hk is the output of the box state that we feed as input into the next state of the RNNSB. Figure 2 shows the recursive neural network structure. Now, the free structure of the GRSB (0, 20) is used as the sequence."}, {"heading": "3. Models", "text": "In view of the constructions for the distribution of themes described above, in this section we present our family of neural theme models and the corresponding methods of inference."}, {"heading": "3.1. Neural Topic Models", "text": "Suppose we have a finite number of topics K, the topic distribution via words that have assigned us a topic. (...) Suppose we have a finite number of topics K that are assigned to us. (...) Suppose we have a finite number of topics that are assigned to us. (...) Suppose we have a finite number of topics that are assigned to us. (...) Suppose we have a finite number of topics that are assigned to us. (...) Suppose we have a finite number of topics. (...) Suppose we have a finite number of topics. (...) Suppose we have a finite number of topics. (...) Suppose we have a finite number of topics. (...) Suppose we have a finite number of topics. (...) Suppose we have taken. (...) Suppose we have a finite number of topics."}, {"heading": "3.2. Recurrent Neural Topic Models", "text": "For the GSM and GSB models, the topic of vectors t-RK-H is predefined for the calculation of the topic distribution via words \u03b2. With the RSB construction, we can model an unlimited number of topics, but in addition to the topic RNNSB, which processes the topic proportions. For comparison, in finite neural topic models we have to introduce another neural network RNNTopic, in order to dynamically produce the topics t-R-R-H, in order to avoid the necessity of the topic of variable inference.For comparison, in finite neural topic models we have topic vectors t-RK-H, while in unlimited neural topic models the topics t-R-H are dynamically generated by RNNTopic and the order of the topics corresponds to the order of the states in RNNSB. The generation of \u03b2 follows: tk = RNNTopic (tk \u2212 1), \u03b2k = softv (topic Tv \u00b7 k, where we represent the Tv-K)."}, {"heading": "3.3. Topic vs. Document Models", "text": "In most topic models, documents are modeled by a mixture of topics, and each word is associated with a single topic that is latently variable. Miao et al. (2016) proposed a Neural Variation Document Model (NVDM) that was implemented as a variable auto-encoder (Kingma & Welling, 2014), which has a very similar neural structure to our models. The main difference is that NVDM uses a Softmax decoder (Equation (5)) to generate all the words of a document conditioned on an unnormalized vector: log p (wn | \u03b2, \u03b8) = log softmax (Slovakia \u00b7 \u03b2). (5) In the GSM construction, if we replace the generative distribution (Equation 4) with the above distribution (Equation 5) and remove the softmax function from the solution, it is reduced to a variant of the NVDM model (GM applies the theme and word processors directly)."}, {"heading": "4. Related Work", "text": "Beyond LDA, however, significant extensions have been attempted to capture topic correlations (Lead & Lafferty, 2007), to capture model-time dependencies (Lead & Lafferty, 2006), and to discover an unlimited number of topics (Teh et al., 2006).Thematic models have been expanded to include additional context information such as time (Wang & McCallum, 2006), authorship (Rosen-Zvi et al., 2004), and class names (Mcauliffe & Lead, 2008).Such extensions often require carefully tailored graphical models and associated inference algorithms to capture the desired context. Neural models provide a more general and extensible option, and a number of papers have attempted to use them (Mcauliffe & Lead, 2008)."}, {"heading": "5. Experiments", "text": "We are conducting an experimental evaluation using three sets of data: MXM2 Lyrics, 20NewsGroups3, and Reuters RCV1-v24 News. MXM is the official song collection of the Million Song Dataset with 210,519 training topics and 27,143 test datapoints. The 20NewsGroups Corpus is divided into 11,314 training and 7,531 test documents, while the RCV1-v2 Corpus is a larger collection with 794,414 training topics and 10,000 test cases from Reuters Newswire stories. We are using the original 5,000 vocabulary provided for MXM, while the other two datasets are used as vocabularies by stemming, filtering stop words, and the most common 2,005 and 10,000 words. 2http: / labrosa.ee.columbia.ee.ee.edu / millionsong / musixmatch (Bertin-Mahieal, Newsjason / WE / 204qe255.com)."}, {"heading": "5.1. Evaluation", "text": "In fact, most of us are able to put ourselves at the top of society, \"he said in an interview with\" Welt am Sonntag \":\" It is very important that people are able to put themselves at the top of society. \""}, {"heading": "6. Conclusion", "text": "In this paper, we have presented a family of neural theme models that use the Gaussian Softmax, Gaussian StickBreaking, and Recurrent Stick-Breaking constructions to parameterize the latent multinomial theme distributions of each document. By using the stick-breaking construct, we are able to build neural theme models that exhibit similarly sparse theme distributions as traditional Dirichlet multinomial models. By leveraging the ability of recursive neural networks to model sequences of unlimited length, we also present a truncation-free variation-free inference method that allows the number of themes to dynamically increase."}, {"heading": "A. Discovered Topics", "text": "Table 4 presents the topics based on the words with the highest probability (top 10 words) achieved by various neural topic models on 20NewsGroups dataset."}, {"heading": "B. Topic Diversity", "text": "A problem that exists in both probabilistic and neural topic models are redundant topics. In neural models, it simply regulates the distance between the individual topic vectors to diversify the topics. According to Xie et al. (2015), we apply such topic diversity regulation while performing neural variation conclusions. We use the cosinal distance to measure the distance between two topics a (ti, tj) = arccos (| ti \u00b7 \u03b2j | | ti | | | | tj | | | |) The mean angle of all pairs of K topics is D = 1K2 \u2211 i \u0445 j a (ti, tj), and the variance is \u03bd = 1K2 \u0445 i \u0445 j (a (ti, tj) \u2212 m) 2. We add the following topic diversity regulation to the variation target: J = L + \u0445 (ti, tj), where \u03bb is a hyperparameter for regulation that is empirically set as 0.1."}], "references": [{"title": "A variational bayesian framework for graphical models", "author": ["Attias", "Hagai"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Attias and Hagai.,? \\Q2000\\E", "shortCiteRegEx": "Attias and Hagai.", "year": 2000}, {"title": "Variational algorithms for approximate Bayesian inference", "author": ["Beal", "Matthew James"], "venue": "University of London,", "citeRegEx": "Beal and James.,? \\Q2003\\E", "shortCiteRegEx": "Beal and James.", "year": 2003}, {"title": "The million song dataset", "author": ["Bertin-Mahieux", "Thierry", "Ellis", "Daniel P.W", "Whitman", "Brian", "Lamere", "Paul"], "venue": "In Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR),", "citeRegEx": "Bertin.Mahieux et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bertin.Mahieux et al\\.", "year": 2011}, {"title": "Dynamic topic models", "author": ["Blei", "David M", "Lafferty", "John D"], "venue": "In Proceedings of ICML,", "citeRegEx": "Blei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2006}, {"title": "A correlated topic model of science", "author": ["Blei", "David M", "Lafferty", "John D"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Blei et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2007}, {"title": "Latent dirichlet allocation", "author": ["Blei", "David M", "Ng", "Andrew Y", "Jordan", "Michael I"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Truly nonparametric online variational inference for hierarchical dirichlet processes", "author": ["Bryant", "Michael", "Sudderth", "Erik B"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Bryant et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bryant et al\\.", "year": 2012}, {"title": "Inference for nonconjugate bayesian models using the gibbs sampler", "author": ["Carlin", "Bradley P", "Polson", "Nicholas G"], "venue": "Canadian Journal of statistics,", "citeRegEx": "Carlin et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Carlin et al\\.", "year": 1991}, {"title": "Topicrnn: A recurrent neural network with long-range semantic dependency", "author": ["Dieng", "Adji B", "Wang", "Chong", "Gao", "Jianfeng", "Paisley", "John"], "venue": "arXiv preprint arXiv:1611.01702,", "citeRegEx": "Dieng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dieng et al\\.", "year": 2016}, {"title": "Replicated softmax: an undirected topic model", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Hinton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2009}, {"title": "Online learning for latent dirichlet allocation", "author": ["Hoffman", "Matthew", "Bach", "Francis R", "Blei", "David M"], "venue": "In Proceedings of NIPS, pp", "citeRegEx": "Hoffman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2010}, {"title": "Probabilistic latent semantic indexing", "author": ["Hofmann", "Thomas"], "venue": "In Proceedings of SIGIR,", "citeRegEx": "Hofmann and Thomas.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann and Thomas.", "year": 1999}, {"title": "An introduction to variational methods for graphical models", "author": ["Jordan", "Michael I", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawrence K"], "venue": "Machine learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "A stick-breaking likelihood for categorical data analysis with latent gaussian models", "author": ["Khan", "Mohammad Emtiyaz", "Mohamed", "Shakir", "Marlin", "Benjamin M", "Murphy", "Kevin P"], "venue": "In Proceedings of AISTATS,", "citeRegEx": "Khan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Khan et al\\.", "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "In Proceedings of ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In Proceedings of ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "An introduction to latent semantic analysis", "author": ["Landauer", "Thomas K", "Foltz", "Peter W", "Laham", "Darrell"], "venue": "Discourse processes,", "citeRegEx": "Landauer et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1998}, {"title": "A neural autoregressive topic model", "author": ["Larochelle", "Hugo", "Lauly", "Stanislas"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Larochelle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2012}, {"title": "Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality", "author": ["Lau", "Jey Han", "Newman", "David", "Baldwin", "Timothy"], "venue": "In Proceedings of EACL,", "citeRegEx": "Lau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lau et al\\.", "year": 2014}, {"title": "A tutorial on energy-based learning", "author": ["LeCun", "Yann", "Chopra", "Sumit", "Hadsell", "Raia"], "venue": "Predicting structured data,", "citeRegEx": "LeCun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2006}, {"title": "Supervised topic models", "author": ["Mcauliffe", "Jon D", "Blei", "David M"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mcauliffe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mcauliffe et al\\.", "year": 2008}, {"title": "Neural variational inference for text processing", "author": ["Miao", "Yishu", "Yu", "Lei", "Blunsom", "Phil"], "venue": "In Proceedings of ICML,", "citeRegEx": "Miao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2016}, {"title": "Neural variational inference and learning in belief networks", "author": ["Mnih", "Andriy", "Gregor", "Karol"], "venue": "In Proceedings of ICML,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Deep generative models with stick-breaking priors", "author": ["Nalisnick", "Eric", "Smyth", "Padhraic"], "venue": "arXiv preprint arXiv:1605.06197,", "citeRegEx": "Nalisnick et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nalisnick et al\\.", "year": 2016}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "The author-topic model for authors and documents", "author": ["Rosen-Zvi", "Michal", "Griffiths", "Thomas", "Steyvers", "Mark", "Smyth", "Padhraic"], "venue": "In Proceedings of the 20th conference on Uncertainty in artificial intelligence,", "citeRegEx": "Rosen.Zvi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rosen.Zvi et al\\.", "year": 2004}, {"title": "A constructive definition of dirichlet priors", "author": ["Sethuraman", "Jayaram"], "venue": "Statistica sinica, pp", "citeRegEx": "Sethuraman and Jayaram.,? \\Q1994\\E", "shortCiteRegEx": "Sethuraman and Jayaram.", "year": 1994}, {"title": "Neural variational inference for topic models", "author": ["Srivastava", "Akash", "Sutton", "Charles"], "venue": "Bayesian deep learning workshop,", "citeRegEx": "Srivastava et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2016}, {"title": "Hierarchical dirichlet processes", "author": ["Teh", "Yee Whye", "Jordan", "Michael I", "Beal", "Matthew J", "Blei", "David M"], "venue": "Journal of the American Statistical Asociation,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Variational inference in nonconjugate models", "author": ["Wang", "Chong", "Blei", "David M"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Online variational inference for the hierarchical dirichlet process", "author": ["Wang", "Chong", "Paisley", "John William", "Blei", "David M"], "venue": "In Proceedings of AISTATS,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Topics over time: a non-markov continuous-time model of topical trends", "author": ["Wang", "Xuerui", "McCallum", "Andrew"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Wang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2006}, {"title": "Diversifying restricted boltzmann machine for document modeling", "author": ["Xie", "Pengtao", "Deng", "Yuntian", "Xing", "Eric"], "venue": "In Proceedings of KDD,", "citeRegEx": "Xie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "Starting with latent semantic analysis (LSA (Landauer et al., 1998)), models for uncovering the underlying semantic structure of a document collection have been widely applied in data mining, text processing and information retrieval.", "startOffset": 44, "endOffset": 67}, {"referenceID": 5, "context": "PLSA (Hofmann, 1999), LDA (Blei et al., 2003) and HDPs (Teh et al.", "startOffset": 26, "endOffset": 45}, {"referenceID": 29, "context": ", 2003) and HDPs (Teh et al., 2006)) provide a robust, scalable, and theoretically sound foundation for document modelling by introducing latent variables for each token to topic assignment.", "startOffset": 17, "endOffset": 35}, {"referenceID": 12, "context": "either Monte Carlo or Variational techniques (Jordan et al., 1999; Attias, 2000; Beal, 2003)).", "startOffset": 45, "endOffset": 92}, {"referenceID": 25, "context": "Neural variational inference (Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014) approximates the posterior of a generative model with a variational distribution parameterised by a neural network.", "startOffset": 29, "endOffset": 96}, {"referenceID": 25, "context": "For models with continuous latent variables associated with particular distributions, such as Gaussians, there exist reparameterisations (Kingma & Welling, 2014; Rezende et al., 2014) of the distribution permitting unbiased and low-variance estimates of the gradients with respect to the parameters of the inference network.", "startOffset": 137, "endOffset": 183}, {"referenceID": 22, "context": "Recently, algorithms such as REINFORCE have been used effectively to decrease variance and improve learning (Mnih & Gregor, 2014; Mnih et al., 2014).", "startOffset": 108, "endOffset": 148}, {"referenceID": 29, "context": "Finally, the Recurrent Stick Breaking process employs a recurrent neural network, again conditioned on the Gaussian draw, to progressively break the stick, yielding a neural analog of a Dirichlet Process topic model (Teh et al., 2006).", "startOffset": 216, "endOffset": 234}, {"referenceID": 21, "context": "Previous neural document models, such as the neural variational document model (NVDM) (Miao et al., 2016), belief networks document model (Mnih & Gregor, 2014), neural auto-regressive document model (Larochelle & Lauly, 2012) and replicated softmax (Hinton & Salakhutdinov, 2009), have not explicitly modelled latent topics.", "startOffset": 86, "endOffset": 105}, {"referenceID": 5, "context": "In probabilistic topic models, such as LDA (Blei et al., 2003), we use the latent variables \u03b8d and zn for the topic proportion of document d, and the topic assignment for the observed word wn, respectively.", "startOffset": 43, "endOffset": 62}, {"referenceID": 29, "context": "In order to facilitate efficient inference, the Dirichlet distribution (or Dirichlet process (Teh et al., 2006)) is employed as the prior to generate the parameters of the multinomial distribution \u03b8d for each document.", "startOffset": 93, "endOffset": 111}, {"referenceID": 21, "context": "To carry out neural variational inference (Miao et al., 2016), we construct an inference network q(\u03b8|\u03bc(d), \u03c3(d)) to approximate the posterior p(\u03b8|d), where \u03bc(d) and \u03c3(d) are functions of d that are implemented by multilayer perceptrons (MLP).", "startOffset": 42, "endOffset": 61}, {"referenceID": 19, "context": "In deep learning, an energy-based function is generally used to construct probability distributions (LeCun et al., 2006).", "startOffset": 100, "endOffset": 120}, {"referenceID": 13, "context": "In our case, following Khan et al. (2012), we transform the modelling of multinomial probability parameters into the modelling of the logits of binomial probability parameters using Gaussian latent variables.", "startOffset": 23, "endOffset": 42}, {"referenceID": 21, "context": "Following the framework of neural variational inference (Miao et al., 2016; Kingma & Welling, 2014; Rezende et al., 2014), we introduce an inference network conditioned on the observed document d to generate the variational parameters \u03bc(d) and \u03c3(d) so that we can estimate the lower bound by sampling \u03b8 from q(\u03b8|d) = G(\u03b8|\u03bc(d), \u03c3(d)).", "startOffset": 56, "endOffset": 121}, {"referenceID": 25, "context": "Following the framework of neural variational inference (Miao et al., 2016; Kingma & Welling, 2014; Rezende et al., 2014), we introduce an inference network conditioned on the observed document d to generate the variational parameters \u03bc(d) and \u03c3(d) so that we can estimate the lower bound by sampling \u03b8 from q(\u03b8|d) = G(\u03b8|\u03bc(d), \u03c3(d)).", "startOffset": 56, "endOffset": 121}, {"referenceID": 21, "context": "Miao et al. (2016) proposed a neural variational document model (NVDM) implemented as a variational auto-encoder (Kingma & Welling, 2014), which has a very similar neural structure to our models.", "startOffset": 0, "endOffset": 19}, {"referenceID": 29, "context": "Beyond LDA, significant extensions have sought to capture topic correlations (Blei & Lafferty, 2007), model temporal dependencies (Blei & Lafferty, 2006) and discover an unbounded number of topics (Teh et al., 2006).", "startOffset": 197, "endOffset": 215}, {"referenceID": 26, "context": "Topic models have been extended to capture extra context information such as time (Wang & McCallum, 2006), authorship (Rosen-Zvi et al., 2004), and class labels (Mcauliffe & Blei, 2008).", "startOffset": 118, "endOffset": 142}, {"referenceID": 21, "context": "Neural models provide a more generic and extendable option and a number of works have sought to leverage these, such as the Replicated Softmax (Hinton & Salakhutdinov, 2009), the Auto-Regressive Document Model (Larochelle & Lauly, 2012), Sigmoid Belief Document Model (Mnih & Gregor, 2014), Variational Auto-Encoder Document Model (NVDM) (Miao et al., 2016) and TopicRNN Model (Dieng et al.", "startOffset": 338, "endOffset": 357}, {"referenceID": 8, "context": ", 2016) and TopicRNN Model (Dieng et al., 2016).", "startOffset": 27, "endOffset": 47}, {"referenceID": 10, "context": "Finite Topic Model MXM 20News RCV1 50 200 50 200 50 200 GSM 306 272 822 830 717 602 GSB 309 296 838 826 788 634 RSB 311 297 835 822 750 628 OnlineLDA 312 342 893 1015 1062 1058 (Hoffman et al., 2010) NVLDA 330 357 1073 993 791 797 (Srivastava & Sutton, 2016)", "startOffset": 177, "endOffset": 199}, {"referenceID": 31, "context": "RSB-TF 303 825 622 HDP (Wang et al., 2011) 370 937 918", "startOffset": 23, "endOffset": 42}, {"referenceID": 10, "context": "We compare our neural topic models with the Gaussian Softmax (GSM), Gaussian Stick Breaking (GSB) and Recurrent Stick Breaking (RSB) constructions to the online variational LDA (onlineLDA) (Hoffman et al., 2010) and neural variational inference LDA (NVLDA) (Srivastava & Sutton, 2016) models.", "startOffset": 189, "endOffset": 211}, {"referenceID": 31, "context": "The lower section shows the results for the unbounded topic models, including our truncation-free RSB (RSB-TF) and the online HDP topic model (Wang et al., 2011).", "startOffset": 142, "endOffset": 161}, {"referenceID": 2, "context": "edu/millionsong/musixmatch (Bertin-Mahieux et al., 2011) http://qwone.", "startOffset": 27, "endOffset": 56}, {"referenceID": 21, "context": "Finite Document Model MXM 20News RCV1 50 200 50 200 50 200 GSM 270 267 787 829 653 521 GSB 285 275 816 815 712 544 RSB 286 283 785 792 662 534 NVDM 345 345 837 873 717 588 (Miao et al., 2016) ProdLDA 319 326 1009 989 780 788 (Srivastava & Sutton, 2016)", "startOffset": 172, "endOffset": 191}, {"referenceID": 2, "context": "edu/millionsong/musixmatch (Bertin-Mahieux et al., 2011) http://qwone.com/ jason/20Newsgroups http://trec.nist.gov/data/reuters/reuters.html We use the vocabulary provided by Srivastava & Sutton (2016) for direct comparison.", "startOffset": 28, "endOffset": 202}, {"referenceID": 21, "context": "The table compares the results for a fixed dimension latent variable, 50 or 200, achieved by our neural document models to Product of Experts LDA (prodLDA) (Srivastava & Sutton, 2016) and the Neural Variational Document Model (NVDM) (Miao et al., 2016).", "startOffset": 233, "endOffset": 252}, {"referenceID": 33, "context": "To alleviate the redundant topics issue, we also apply topic diversity regularisation (Xie et al., 2015) while carrying out neural variational inference.", "startOffset": 86, "endOffset": 104}, {"referenceID": 21, "context": "We follow the optimisation strategy of Miao et al. (2016) by alternately updating the model parameters and the inference network.", "startOffset": 39, "endOffset": 58}, {"referenceID": 21, "context": "Here we use the variational lower bound to estimate the document perplexity: exp(\u2212 1 D \u2211D d 1 Nd log p(d)) following Miao et al. (2016). Table 1 presents the test document perplexities of the topic models on the three datasets.", "startOffset": 117, "endOffset": 136}, {"referenceID": 29, "context": "as 5e\u22125), with the traditional non-parametric HDP topic model (Teh et al., 2006).", "startOffset": 62, "endOffset": 80}, {"referenceID": 18, "context": "a function of loglikelihood), we evaluate the topic observed coherence by normalised point-wise mutual information (NPMI) (Lau et al., 2014).", "startOffset": 122, "endOffset": 140}], "year": 2017, "abstractText": "Topic models have been widely explored as probabilistic generative models of documents. Traditional inference methods have sought closedform derivations for updating the models, however as the expressiveness of these models grows, so does the difficulty of performing fast and accurate inference over their parameters. This paper presents alternative neural approaches to topic modelling by providing parameterisable distributions over topics which permit training by backpropagation in the framework of neural variational inference. In addition, with the help of a stick-breaking construction, we propose a recurrent network that is able to discover a notionally unbounded number of topics, analogous to Bayesian non-parametric topic models. Experimental results on the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the effectiveness and efficiency of these neural topic models.", "creator": "LaTeX with hyperref package"}}}