{"id": "1706.00387", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning", "abstract": "Off-policy model-free deep reinforcement learning methods using previously collected data can improve sample efficiency over on-policy policy gradient techniques. On the other hand, on-policy algorithms are often more stable and easier to use. This paper examines, both theoretically and empirically, approaches to merging on- and off-policy updates for deep reinforcement learning. Theoretical results show that off-policy updates with a value function estimator can be interpolated with on-policy policy gradient updates whilst still satisfying performance bounds. Our analysis uses control variate methods to produce a family of policy gradient algorithms, with several recently proposed algorithms being special cases of this family. We then provide an empirical comparison of these techniques with the remaining algorithmic details fixed, and show how different mixing of off-policy gradient estimates with on-policy samples contribute to improvements in empirical performance. The final algorithm provides a generalization and unification of existing deep policy gradient techniques, has theoretical guarantees on the bias introduced by off-policy updates, and improves on the state-of-the-art model-free deep RL methods on a number of OpenAI Gym continuous control benchmarks.", "histories": [["v1", "Thu, 1 Jun 2017 17:00:52 GMT  (661kb,D)", "http://arxiv.org/abs/1706.00387v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO", "authors": ["shixiang gu", "timothy lillicrap", "zoubin ghahramani", "richard e turner", "bernhard sch\\\"olkopf", "sergey levine"], "accepted": true, "id": "1706.00387"}, "pdf": {"name": "1706.00387.pdf", "metadata": {"source": "META", "title": "Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning", "authors": ["Shixiang Gu"], "emails": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "bs@tuebingen.mpg.de", "svlevine@eecs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "Recent years have shown that deep networks can be successfully combined with RL techniques to solve difficult control problems, some of which include robotic control (Schulman et al., 2016; Lillicrap et al., Levine et al., 2016), computer games (Mnih et al., 2015), and board games (Silver et al., 2016). One of the easiest ways to learn a neural network policy is to collect a set of behaviors in which politics is used to act in the world, and then make a political gradient update, because all updates are made using data generated by the agent's current policies."}, {"heading": "2 Preliminaries", "text": "A key component of our interpolated gradient method is the use of control variables to mix probability gradient gradients with deterministic gradient estimates explicitly obtained by a critic of government policies. In this section, we summarize both probability and deterministic gradient methods and show how control variants can be used to combine these two approaches."}, {"heading": "2.1 On-Policy Likelihood Ratio Policy Gradient", "text": "At the time t, the RL agent in State st takes action in accordance with his political prediction \u03c0 (at | st), the state transition to st + 1, and the agent receives a reward r (st, at). For a parameterized political prediction, the objective is to maximize the cumulative cumulative future return J (\u03b8) = J (\u03c0) = Es0, a0, \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 [\u21a9 t = 0 \u03b3tr (st, at)]. The Monte Carlo method of political bias, such as REINFORCE (Williams, 1992) and TRPO (Schulman et al., 2015), uses the political bias of the RL target, as well as the results of these strategies (at | st) and the results of normalization (at) (Q (st, at) (st, at) \u2212 b (st) and variance (st)."}, {"heading": "2.2 Off-Policy Deterministic Policy Gradient", "text": "Political gradient methods with functional approximation (Sutton et al., 1999) or actor-critique methods (actor-critic methods) are a family of political gradient methods that first estimate the critic or the value of politics using Qw \u2248 Q\u03c0 and then greedily optimize policy approaches with respect to Qw. Although it is not necessary for such algorithms to be outside politics, we primarily analyze non-political variants, such as (Riedmiller, 2005; Degris et al., 2012; Heess et al., 2015; Lillicrap et al., 2016). For example, DDPG Lillicrap et al. (2016), which optimize a continuous deterministic policy, can be summarized on the basis of bias (at = st) = bias (p (st))) by the following actualization equations, in which Q \u2032 w describes the goal-Q network (Lillicrapet al., 2016) and embeds some external policy approaches (e.g., large-policy algorithms)."}, {"heading": "2.3 Off-Policy Control Variate Fitting", "text": "The Control Variant Method (Ross, 2006) is a general technique for reducing variance of a Monte Carlo estimator by using a correlated variable for which we know more information such as analytical expectations. Common control variants for RL include state bases for action, and one example may be a non-policy-adjusted critic Qw. Q-Prop (Gu et al., 2017), for example, used Q-Prop, the Taylor extension of the first order of Qw, as control varies, and showed an improvement in stability and sampling efficiency of political gradient methods."}, {"heading": "3 Interpolated Policy Gradient", "text": "Our proposed approach, the Interpolated Policy Gradient (IPG), mixes the probability gradient with the Q value, which provides an unbiased but highly variable gradient estimate, and the deterministic gradient with a non-policy adjusted criticQw value, which provides a low but tendentious gradient. IPG interpolates the two terms directly from Equation 1 and 3: \"PhenomenJ (\u03b8)\" (1 \u2212 \u03bd), \"Equity\" (at | st) A value (st, at) + \"Equity,\" \"Equity\" (5), where we generalize the deterministic policy gradient by the critic as \"Equity Q value\" (st) = \"Equity Log\" (st, \u00b7). This generalization is intended to make our analysis applicable to more general forms of the critique-based control variants discussed in the appendix."}, {"heading": "3.1 Control Variates for Interpolated Policy Gradient", "text": "The expression for the IPG with control variants is below, where A\u03c0w (st, at) = Qw (st, at) = Qw (st, at) = Qw (st, at) \u2212 Q \u00b2 w (st), HQ \u00b2 p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (st) p (st) p (st) p (p) p (p) p (st) p (st) p (st) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p) p (p) p \"p (p) p (p) p) p (p) p (p) (p) p) p (p) (p) p) p (p) (p) p) p (p) (p) p) p (p) (p) p) p (p) p (p) (p) p) p (p) p (p) p (p) p) p (p) p (p) p (p) p (p) p (p) (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) (p) p (p) (p) (p) (p) (p) p (p) (p) p (p) (p) (p) (p) (p (p) (p) p (p) (p) p (p) (p) (p) p (p) (p (p) p (p) (p) p (p (p) p) (p (p) (p) (p) (p) (p) (p) (p) p (p (p) p) (p) (p (p) (p) (p) (p) p (p (p) p (p) p (p) (p) (p (p (p)"}, {"heading": "3.2 Relationship to Prior Policy Gradient and Actor-Critic Methods", "text": "Crucial: IPG allows interpolation of a rich list of previous methods of the deep policy gradient using only three parameters: \u03b2, \u03bd and the use of control variables (CV). The link is summarized in Table 1 and the algorithm is presented in Algorithm 1. Importantly, a wide range of previous work has examined only cases of spectrum limitation, e.g. \u03bd = 0, 1, with or without control variant. Our work provides a thorough theoretical analysis of distortions and in some cases performance guarantees for each method in this spectrum and empirically shows that the most powerful algorithms are often in the middle of the spectrum. Algorithm 1 Interpolated policy gradient input \u03b2, \u03bd, useCV 1: Initialize w for critics Qw, Celsius for stochastic politics and Empirically shows that the buffers R are hesitant."}, {"heading": "3.3 \u03bd = 1: Actor-Critic methods", "text": "Before presenting our theoretical analysis, an important special case to be discussed is \u03bd = 1, which corresponds to a deterministic actor-critic method. Advantages of this special case include that politics can be deterministic and that learning can take place entirely outside politics, since it does not have to evaluate the Monte Carlo political critic Q *. Previous papers such as DDPG Lillicrap et al. (2016) and related Q-Learning methods have suggested aggressive non-political exploration strategies to exploit these properties of the algorithm. In this paper, we compare alternatives such as the use of non-political exploration and stochastic strategies with classic DDPG algorithm designs and show that performance in some areas may significantly deteriorate due to non-political exploration. Theoretically, we confirm this empirical observation by showing that the bias from non-political sampling in \u03b2 is increasingly monotonically interpreted with the total variation or KL divergence between the strategic actor and an alternative theoretical actor in both empirical and empirical explanatory outcomes."}, {"heading": "4 Theoretical Analysis", "text": "In this section, we present a theoretical analysis of the bias of the interpolated policy gradient. This is crucial because understanding the bias of the methods can improve our intuition about their performance and make it easier to design new algorithms in the future. Since IPG includes many earlier methods as special cases, our analysis applies to these methods and other incidents as well. First, we analyze a special case and derive results for the general IPG. All evidence can be found in the appendix."}, {"heading": "4.1 \u03b2 6= \u03c0, \u03bd = 0: Policy Gradient with Control Variate and Off-Policy Sampling", "text": "In this section, an analysis of the special case of IPG with \u03b2 6 = p = p = p = p = p = p = p = p = p = p = p = p, p = p = p = p = p, p = p = p = p = p, p = p = p, p = p = p = p, p = p = p, p = p = p = p, p = p = p, p = p = p, p = p = p = p = p, p = p = p = p = p, p = p = p = p, p = p = p = p = p, p = p = p = p = p, p = p = p = p = p, p = p = p = p = p, p = p = p = p, p = p = p = p, p = p = p = p, p = p = p = p = p, p = p = p = p = p = p, p = p = p = p = p = p = p, p = p = p = p = p = p = p = p, p = p = p = p = p = p = p = p = p, p = p = p = p = p = p = p = p = p = p = p, p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p, p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p, p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p, p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p."}, {"heading": "4.1.1 Monotonic Policy Improvement Guarantee", "text": "Some forms of political gradient methods have theoretical guarantees of monotonous convergence Kakade & Langford (2002); Schulman et al. (2015) Such guarantees often correspond to stable empirical performance in challenging problems, even if some of the constraints are relaxed in practice (Schulman et al., 2015; Duan et al., 2016; Gu et al., 2017). We can show that algorithm 2, which is a variant of IPG, guarantees monotonous convergence, as demonstrated in the Appendix. Algorithm 2 is often impracticable; IPG with confidence-regional updating, however, if \u03b2 6 = \u03c0, \u03bd = 1, CV = true approaches this monotonous algorithm, similar to how TRPO is an approximation of the theoretical monotonical algorithm 2 (Schulman et al. (2015)."}, {"heading": "4.2 General Bounds on the Interpolated Policy Gradient", "text": "Theorem 2. If \u03b4 = maxs, a | A\u03c0 (s, a) \u2212 A\u03c0 (s) p (s) p (s) p (s) p (s) p (s) p (s) p (s) p \u2212 p (s) p (s) p (s) p (s) p (s) p (s) p (s) p (s) p (s) p \u2212 p (s) p (s) p (s) p \u2212 p (s) p (s) p (s) p (s) p (p) p \u2212 p (p) p (s) p (p) p (s) p (s) p \u2212 p (s) p (p) p (p) p (p) p \u2212 p) p (s) p (p) p (p) p \u2212 p) p (s) p (p) p (p \u2212 p) p (p) p (p) p (p) p (p) p (p) p \u2212 p) p (s) p (p) p (p) p \u2212 p) p (s) p (p) p \u2212 p) p (s) p \u2212 p (s) p \u2212 p (s) p \u2212 p) p (s) p (p \u2212 p) p \u2212 p (s) p (p) p \u2212 p (p) p \u2212 p (p) p \u2212 p (p) p (p \u2212 p) p (p \u2212 p) p (p \u2212 p (p) p (p \u2212 p) p (p \u2212 p (p) p (p) p \u2212 p (p \u2212 p) p (p \u2212 p) p (p \u2212 p (p) p (p) p (p \u2212 p \u2212 p) p (p (p) p \u2212 p (p) p (p \u2212 p) p (p (p \u2212 p (p) p (p) p (p \u2212 p) p (p (p) p \u2212 p (p (p) p (p) p \u2212 p) p (p (p \u2212 p p) p (p (p) p \u2212 p) p (p) p (p (p (p) p (p) p (p) p \u2212 p (p \u2212 p p) p (p (p) p (p (p) p (p) p"}, {"heading": "5 Related Work", "text": "An overarching goal of this paper is the uniformity of political graduation in a single conceptual framework. Our analysis examines how politics relates to extra-parliamentary learning (see Table 1), with 0 < 1 and without the control mechanisms closely related to PGQ and ACER."}, {"heading": "6 Experiments", "text": "In this section, we show empirically that the three parameters of IPG can interpolate different behaviors and often perform better than previous methods limiting cases of this approach. Crucially, all methods have the same algorithmic structure as algorithm 1, and we consider the rest of the experimental details to be fixed. All experiments were conducted on MuJoCo domains in OpenAI Gym (Todorov et al., 2012; Brockman et al., 2016), with the results presented for the average of three seeds. Further experimental details are included in the appendix."}, {"heading": "6.1 \u03b2 6= \u03c0, \u03bd = 0, with the control variate", "text": "We are evaluating the performance of the IPG special case discussed in Section 4.1. This case is of particular interest because we can derive monotonous convergence results for a variant of this method under certain conditions, despite the presence of off-policy updates. Figure 1a shows performance on the HalfCheetah v1 domain when the batch size of the policy is 5000 transitions (i.e. 5 episodes). \"Last\" and \"Edge\" show whether \u03b2 samples are from recent transitions or uniformly from the experiential response. \"last05000\" would be equivalent to Q-Prop, since it is a comparison between \"IPG \u03b2rand05000\" and \"Q-Prop\" curves, observing that the same number of samples are randomly taken from the replay buffer to evaluate the critic forecast, rather than using the results of the policy."}, {"heading": "6.2 \u03b2 = \u03c0, \u03bd = 1", "text": "In this section, we evaluate empirically another specific case of IPG, where \u03b2 = \u03c0, which indicates a policy sample, and \u03bd = 1, which reduces to a trust-based region, is a policy variant of a deterministic actor-critic method. Although this algorithm performs updates of actor-critics, the use of a trust region is more akin to TRPO or Q-Prop than DDPG. Results for all areas are shown in Table 2. Figure 1b shows the learning curves at Ant-v1. Although IPG-\u03bd = 1 methods may be outside politics, the policy is updated every 5000 samples to keep them in line with other IPG methods, while DPG updates policy at every step in the environment and makes other design decisions Lillicrap et al. (2016). We see that in this area the standard DDPG is rewarded with an average of 1000, while IPG policy is mononotonically improved, which achieves a much better result."}, {"heading": "6.3 General Cases of Interpolated Policy Gradient", "text": "Table 2 shows the results for experiments in which we compare IPG methods with different values of \u03bd; additional results are provided in the appendix. \u03b2 6 = \u03c0 indicates that the method uses samples from the replay buffer used outside the guidelines, with the same batch size as the batch used within the guidelines for a fair comparison. We have analysed \u03bd = {0.2, 0.4, 0.6, 0.8} and found that \u03bd = 0.2 consistently performs better than Q-Prop, TRPO or previous actor-critic methods. This is consistent with the results in PGQ (O'Donoghue et al., 2017) and ACER (Wang et al., 2017), which found that their equivalent of \u03bd = 0.1 performs best according to their benchmarks. It is important that we have compared all methods with the same algorithm designs (exploration, policy, etc.), ProQ-IPG and without PO-TRP = TR\u03bd."}, {"heading": "7 Discussion", "text": "This family of algorithms unifies and interpolates the policy gradient and the deterministic policy gradient, and comprises a number of previous work as approximate borderline cases. Empirical results confirm that interpolated gradients have in many cases improved sampling efficiency and stability over previous state-of-the-art methods, and the theoretical results provide intuition for analyzing the cases in which the various methods perform well or poorly. We hope that this detailed analysis of interpolated gradient methods will not only provide more effective algorithms in practice, but also provide useful insights for future algorithm design."}, {"heading": "Acknowledgements", "text": "This work is supported by the Cambridge-T\u00fcbingen PhD Fellowship, the NSERC and the Google Faculty Award."}, {"heading": "8 Proof for Theorem 1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 Local approximation objective with bounded bias", "text": "In this section, we discuss the rationale behind this choice and refer to previous work (Kakade & Langford, 2002; Schulman et al., 2015). Firstly, the expected yield J (\u03c0) of a policy \u03c0 can be written as the sum of the expected return J (\u03c0) of another policy \u03c0 and the expected advantage between the two strategies in the equation, where A\u03c0 (st, at) is the advantage of the policy \u03c0., J (\u03c0) = J (\u03c0) of the expected return J (\u03c0) of the expected return J (sp., at). First: The anticipated return J (st, at)."}, {"heading": "8.2 Main proof for Theorem 1", "text": "(T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T"}, {"heading": "9 Proof for Monotonic Convergence in Algorithm 2", "text": "We can prove that the algorithm that guarantees a monotonic improvement by introducing the following logical, corollary 1.J (conceived) \u2265 M (conceived, conceived) \u2265 Mziping = 0, CV (conceived, conceived), J (conceived) = M (conceived, conceived) = M\u03b2, \u03bd = 0, CV (conceived, conceived) (14), whereas M (conceived, conceived) = J (conceived, conceived) \u2212 DmaxKL (conceived, conceived) \u2212 DmaxKL (conceived, conceived) \u2212 DmaxKL (conceived) \u2212 DmaxKL (conceived, conceived) \u2212 DmaxKL (conceived) \u2212 DmaxKL (conceived, conceived) \u2212 DmaxKst (conceived, conceived) \u2015 \u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441iciciciciciciciciciciciciciciciciciciciciciciciciciced) (conceived)"}, {"heading": "10 Proof for Theorem 2", "text": "We follow the same procedure as the proof of equivalence 15 for equivalence of equivalence, first deriving the boundaries between J (\u03c0, \u03c0) and the other local targets and then combining the results with the result. \u2212 To begin with the proof, we first deduce the boundary for the special case where it is a non-political actor-critic algorithm, and which is closely linked to the DDPG (Lillicrap et al., 2016), except that it does not use the target network and its application of a stochastic policy. \u2212 This is a non-political actor-critic algorithm closely linked to the DPG (Lillicrap et al., 2016). \u2212 We can introduce the following boundary on the local target J. \u2212 We cannot introduce the following boundary on the local target J. \u2212 We can introduce the following boundary on the local target J. \u2212 We can introduce the following boundary on the local target J."}, {"heading": "11 Control Variates for Policy Gradient", "text": "In this section, we describe control variants for other political gradient methods than the first order Taylor extension presented in Q-Prop (Gu et al., 2017)."}, {"heading": "11.1 Reparameterized Critic Control Variate", "text": "In order to reduce the deviation of the concept of correction, we estimate the political gradient, the repair parameterization trick (Kingma & Welling, 2014) can be applied to reduce the deviation. In a Gaussian policy, the deviation (at | st) = N (\u00b5\u03b8 (st), \u03c3\u03b8 (st))), Q \u0435\u043dw (st) = E\u03c0 [Qw (st, at)] = E \u0445 N (0,1) [Qw (st, \u00b5\u03b8 (st) + \u03c3 (st)] \u2248 1 mm \u0445i = 1Qw (st, \u00b5\u03b8 (st) + i\u03c3 (st))) (20)."}, {"heading": "11.2 Discrete Critic Control Variate", "text": "Imagine that \u03c0\u03b8 (st) \u0441Rk denotes a probability vector via discrete k actions, and Qw (st) \u0441Rk denotes the action-value function for k actions, as in DQN (Mnih et al., 2015).Q-\u03c0w (st) = \u03c0\u03b8 (st) T \u00b7 Qw (st). (21)"}, {"heading": "11.3 NAF Critic Control Variate", "text": "If the policy is a locally Gaussian policy, i.e. a policy of the \"normalized advantage function\" (NAF) (Gu et al., 2016), the square Qw can be applied directly, Qw (st, at) = Aw (st, at) + Vw (st) Aw (st, at) = \u2212 12 (at \u2212 \u00b5w (st)) TPw (st) (at \u2212 \u00b5w (st)). (22) Deterministic gradient expression leads to Q-\u03c0w (st) = Vw (st) \u2212 12 Tr (Pw (st)) \u2212 1 2 (\u00b5\u03b8 (st) \u2212 \u00b5w (st) (\u00b5\u03b8 (st) (st) \u2212 \u00b5w (st))). (23)"}, {"heading": "12 Supplementary Experimental Details", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "12.1 Hyperparameters", "text": "GAE (\u03bb = 0.97) (Schulman et al., 2016) is used for a fixed-layer Q function = 0.00. Trust-region update in TRPO is used as a political optimizer (Schulman et al., 2015). DDPG's standard Q-fitting routine (Lillicrap et al., 2016) is used to customize Qw, where Qw is trained with batch size 64, using experience replay of size 1e6 and target network with mass = 0.001. ADAM (Kingma & Ba, 2014) is used as an optimizer for Qw. The political network parameterizes a Gaussian politics with precision size (at | st) = N (\u00b5zipation (st)), where it is a two-layer neural network of size 100 \u2212 50 and tanh hidden non-linearity and linear output, and networking is a diagonal, state-independent variant."}, {"heading": "12.2 Additional Plot", "text": "Figure 2 shows an additional graph on Humanoid-v1."}], "references": [{"title": "Off-policy actor-critic", "author": ["Degris", "Thomas", "White", "Martha", "Sutton", "Richard S"], "venue": "arXiv preprint arXiv:1205.4839,", "citeRegEx": "Degris et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Degris et al\\.", "year": 2012}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Duan", "Yan", "Chen", "Xi", "Houthooft", "Rein", "Schulman", "John", "Abbeel", "Pieter"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Continuous deep q-learning with model-based acceleration", "author": ["Gu", "Shixiang", "Lillicrap", "Timothy", "Sutskever", "Ilya", "Levine", "Sergey"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Q-prop: Sample-efficient policy gradient with an off-policy critic", "author": ["Gu", "Shixiang", "Lillicrap", "Timothy", "Ghahramani", "Zoubin", "Turner", "Richard E", "Levine", "Sergey"], "venue": null, "citeRegEx": "Gu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2017}, {"title": "Learning continuous control policies by stochastic value gradients", "author": ["Heess", "Nicolas", "Wayne", "Gregory", "Silver", "David", "Lillicrap", "Tim", "Erez", "Tom", "Tassa", "Yuval"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Heess et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2015}, {"title": "A tutorial on mm algorithms", "author": ["Hunter", "David R", "Lange", "Kenneth"], "venue": "The American Statistician,", "citeRegEx": "Hunter et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hunter et al\\.", "year": 2004}, {"title": "Doubly robust off-policy value evaluation for reinforcement learning", "author": ["Jiang", "Nan", "Li", "Lihong"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Jiang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2016}, {"title": "On a connection between importance sampling and the likelihood ratio policy gradient", "author": ["Jie", "Tang", "Abbeel", "Pieter"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Jie et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jie et al\\.", "year": 2010}, {"title": "Plato: Policy learning using adaptive trajectory optimization", "author": ["Kahn", "Gregory", "Zhang", "Tianhao", "Levine", "Sergey", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1603.00622,", "citeRegEx": "Kahn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kahn et al\\.", "year": 2016}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["Kakade", "Sham", "Langford", "John"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Kakade et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2002}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Guided policy search", "author": ["Levine", "Sergey", "Koltun", "Vladlen"], "venue": "In International Conference on Machine Learning (ICML), pp", "citeRegEx": "Levine et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2013}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": null, "citeRegEx": "Lillicrap et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2016}, {"title": "Weighted importance sampling for off-policy learning with linear function approximation", "author": ["Mahmood", "A Rupam", "van Hasselt", "Hado P", "Sutton", "Richard S"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mahmood et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mahmood et al\\.", "year": 2014}, {"title": "Humanlevel control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Safe and efficient off-policy reinforcement learning", "author": ["Munos", "R\u00e9mi", "Stepleton", "Tom", "Harutyunyan", "Anna", "Bellemare", "Marc G"], "venue": "arXiv preprint arXiv:1606.02647,", "citeRegEx": "Munos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Munos et al\\.", "year": 2016}, {"title": "Pgq: Combining policy gradient and q-learning", "author": ["O\u2019Donoghue", "Brendan", "Munos", "Remi", "Kavukcuoglu", "Koray", "Mnih", "Volodymyr"], "venue": null, "citeRegEx": "O.Donoghue et al\\.,? \\Q2017\\E", "shortCiteRegEx": "O.Donoghue et al\\.", "year": 2017}, {"title": "Learning from scarce experience", "author": ["Peshkin", "Leonid", "Shelton", "Christian R"], "venue": "In Proceedings of the Nineteenth International Conference on Machine Learning,", "citeRegEx": "Peshkin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Peshkin et al\\.", "year": 2002}, {"title": "Relative entropy policy search", "author": ["Peters", "Jan", "M\u00fclling", "Katharina", "Altun", "Yasemin"], "venue": "In AAAI. Atlanta,", "citeRegEx": "Peters et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2010}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["Precup", "Doina"], "venue": "Computer Science Department Faculty Publication Series, pp", "citeRegEx": "Precup and Doina.,? \\Q2000\\E", "shortCiteRegEx": "Precup and Doina.", "year": 2000}, {"title": "Neural fitted q iteration\u2013first experiences with a data efficient neural reinforcement learning method", "author": ["Riedmiller", "Martin"], "venue": "In European Conference on Machine Learning,", "citeRegEx": "Riedmiller and Martin.,? \\Q2005\\E", "shortCiteRegEx": "Riedmiller and Martin.", "year": 2005}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross", "St\u00e9phane", "Gordon", "Geoffrey J", "Bagnell", "Drew"], "venue": "In AISTATS,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael I", "Moritz", "Philipp"], "venue": "In ICML, pp", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["Schulman", "John", "Moritz", "Philipp", "Levine", "Sergey", "Jordan", "Michael", "Abbeel", "Pieter"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Schulman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2016}, {"title": "Deterministic policy gradient algorithms", "author": ["Silver", "David", "Lever", "Guy", "Heess", "Nicolas", "Degris", "Thomas", "Wierstra", "Daan", "Riedmiller", "Martin"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Silver et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2014}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["Silver", "David", "Huang", "Aja", "Maddison", "Chris J", "Guez", "Arthur", "Sifre", "Laurent", "Van Den Driessche", "George", "Schrittwieser", "Julian", "Antonoglou", "Ioannis", "Panneershelvam", "Veda", "Lanctot", "Marc"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Sutton", "Richard S", "McAllester", "David A", "Singh", "Satinder P", "Mansour", "Yishay"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Bias in natural actor-critic algorithms", "author": ["Thomas", "Philip"], "venue": "In ICML, pp", "citeRegEx": "Thomas and Philip.,? \\Q2014\\E", "shortCiteRegEx": "Thomas and Philip.", "year": 2014}, {"title": "Data-efficient off-policy policy evaluation for reinforcement learning", "author": ["Thomas", "Philip", "Brunskill", "Emma"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Thomas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Thomas et al\\.", "year": 2016}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Todorov", "Emanuel", "Erez", "Tom", "Tassa", "Yuval"], "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Sample efficient actor-critic with experience replay", "author": ["Wang", "Ziyu", "Bapst", "Victor", "Heess", "Nicolas", "Mnih", "Volodymyr", "Munos", "Remi", "Kavukcuoglu", "Koray", "de Freitas", "Nando"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "J(\u03c0\u0303) = M (\u03c0\u0303, \u03c0\u0303) since \u03b6 = = 0 when \u03c0 = \u03c0\u0303. Given Corollary 1, we use minorization-maximization (MM) (Hunter & Lange, 2004) to derive Algorithm 2 in the main text, a policy iteration algorithm that allows using off-policy samples", "author": ["Schulman"], "venue": null, "citeRegEx": "Schulman,? \\Q2015\\E", "shortCiteRegEx": "Schulman", "year": 2015}], "referenceMentions": [{"referenceID": 25, "context": "Some of these include robotic control (Schulman et al., 2016; Lillicrap et al., 2016; Levine et al., 2016), computer games (Mnih et al.", "startOffset": 38, "endOffset": 106}, {"referenceID": 14, "context": "Some of these include robotic control (Schulman et al., 2016; Lillicrap et al., 2016; Levine et al., 2016), computer games (Mnih et al.", "startOffset": 38, "endOffset": 106}, {"referenceID": 13, "context": "Some of these include robotic control (Schulman et al., 2016; Lillicrap et al., 2016; Levine et al., 2016), computer games (Mnih et al.", "startOffset": 38, "endOffset": 106}, {"referenceID": 16, "context": ", 2016), computer games (Mnih et al., 2015), and board games (Silver et al.", "startOffset": 24, "endOffset": 43}, {"referenceID": 27, "context": ", 2015), and board games (Silver et al., 2016).", "startOffset": 25, "endOffset": 46}, {"referenceID": 28, "context": "Off-policy algorithms based on Q-learning and actor-critic learning (Sutton et al., 1999) have also proven to be an effective approach to deep reinforcement learning such as in (Mnih et al.", "startOffset": 68, "endOffset": 89}, {"referenceID": 16, "context": ", 1999) have also proven to be an effective approach to deep reinforcement learning such as in (Mnih et al., 2015) and (Lillicrap et al.", "startOffset": 95, "endOffset": 114}, {"referenceID": 14, "context": ", 2015) and (Lillicrap et al., 2016).", "startOffset": 12, "endOffset": 36}, {"referenceID": 3, "context": "Most recent research has worked with on-policy algorithms or off-policy algorithms, a few recent methods have sought to make use of both on- and off-policy data for learning (Gu et al., 2017; Wang et al., 2017; O\u2019Donoghue et al., 2017).", "startOffset": 174, "endOffset": 235}, {"referenceID": 32, "context": "Most recent research has worked with on-policy algorithms or off-policy algorithms, a few recent methods have sought to make use of both on- and off-policy data for learning (Gu et al., 2017; Wang et al., 2017; O\u2019Donoghue et al., 2017).", "startOffset": 174, "endOffset": 235}, {"referenceID": 18, "context": "Most recent research has worked with on-policy algorithms or off-policy algorithms, a few recent methods have sought to make use of both on- and off-policy data for learning (Gu et al., 2017; Wang et al., 2017; O\u2019Donoghue et al., 2017).", "startOffset": 174, "endOffset": 235}, {"referenceID": 32, "context": "The first approach is to mix some ratio of on- and off-policy gradients or update steps in order to update a policy, as in the ACER and PGQ algorithms (Wang et al., 2017; O\u2019Donoghue et al., 2017).", "startOffset": 151, "endOffset": 195}, {"referenceID": 18, "context": "The first approach is to mix some ratio of on- and off-policy gradients or update steps in order to update a policy, as in the ACER and PGQ algorithms (Wang et al., 2017; O\u2019Donoghue et al., 2017).", "startOffset": 151, "endOffset": 195}, {"referenceID": 3, "context": "In the second approach, an off-policy Q critic is trained but is used as a control variate to reduce on-policy gradient variance, as in the Q-prop algorithm (Gu et al., 2017).", "startOffset": 157, "endOffset": 174}, {"referenceID": 3, "context": "We show that a number of recent methods (Gu et al., 2017; Wang et al., 2017; O\u2019Donoghue et al., 2017) can be viewed as special cases of this more general family.", "startOffset": 40, "endOffset": 101}, {"referenceID": 32, "context": "We show that a number of recent methods (Gu et al., 2017; Wang et al., 2017; O\u2019Donoghue et al., 2017) can be viewed as special cases of this more general family.", "startOffset": 40, "endOffset": 101}, {"referenceID": 18, "context": "We show that a number of recent methods (Gu et al., 2017; Wang et al., 2017; O\u2019Donoghue et al., 2017) can be viewed as special cases of this more general family.", "startOffset": 40, "endOffset": 101}, {"referenceID": 24, "context": "Monte Carlo policy gradient methods, such as REINFORCE (Williams, 1992) and TRPO (Schulman et al., 2015), use the likelihood ratio policy gradient of the RL objective,", "startOffset": 81, "endOffset": 104}, {"referenceID": 28, "context": "Policy gradient methods with function approximation (Sutton et al., 1999), or actor-critic methods, are a family of policy gradient methods which first estimate the critic, or the value, of the policy by Qw \u2248 Q , and then greedily optimize the policy \u03c0\u03b8 with respect to Qw.", "startOffset": 52, "endOffset": 73}, {"referenceID": 0, "context": "While it is not necessary for such algorithms to be off-policy, we primarily analyze the off-policy variants, such as (Riedmiller, 2005; Degris et al., 2012; Heess et al., 2015; Lillicrap et al., 2016).", "startOffset": 118, "endOffset": 201}, {"referenceID": 4, "context": "While it is not necessary for such algorithms to be off-policy, we primarily analyze the off-policy variants, such as (Riedmiller, 2005; Degris et al., 2012; Heess et al., 2015; Lillicrap et al., 2016).", "startOffset": 118, "endOffset": 201}, {"referenceID": 14, "context": "While it is not necessary for such algorithms to be off-policy, we primarily analyze the off-policy variants, such as (Riedmiller, 2005; Degris et al., 2012; Heess et al., 2015; Lillicrap et al., 2016).", "startOffset": 118, "endOffset": 201}, {"referenceID": 0, "context": "While it is not necessary for such algorithms to be off-policy, we primarily analyze the off-policy variants, such as (Riedmiller, 2005; Degris et al., 2012; Heess et al., 2015; Lillicrap et al., 2016). For example, DDPG Lillicrap et al. (2016), which optimizes a continuous deterministic policy \u03c0\u03b8(at|st) = \u03b4(at = \u03bc\u03b8(st)), can be summarized by the following update equations, where Qw denotes the target Q network (Lillicrap", "startOffset": 137, "endOffset": 245}, {"referenceID": 24, "context": "\u03b2 \u03bd CV Examples - 0 No REINFORCE (Williams, 1992),TRPO (Schulman et al., 2015) \u03c0 0 Yes Q-Prop (Gu et al.", "startOffset": 55, "endOffset": 78}, {"referenceID": 3, "context": ", 2015) \u03c0 0 Yes Q-Prop (Gu et al., 2017) - 1 - DDPG (Silver et al.", "startOffset": 23, "endOffset": 40}, {"referenceID": 26, "context": ", 2017) - 1 - DDPG (Silver et al., 2014; Lillicrap et al., 2016),SVG(0) (Heess et al.", "startOffset": 19, "endOffset": 64}, {"referenceID": 14, "context": ", 2017) - 1 - DDPG (Silver et al., 2014; Lillicrap et al., 2016),SVG(0) (Heess et al.", "startOffset": 19, "endOffset": 64}, {"referenceID": 4, "context": ", 2016),SVG(0) (Heess et al., 2015) 6= \u03c0 - No \u2248PGQ (O\u2019Donoghue et al.", "startOffset": 15, "endOffset": 35}, {"referenceID": 18, "context": ", 2015) 6= \u03c0 - No \u2248PGQ (O\u2019Donoghue et al., 2017), \u2248ACER (Wang et al.", "startOffset": 23, "endOffset": 48}, {"referenceID": 32, "context": ", 2017), \u2248ACER (Wang et al., 2017)", "startOffset": 15, "endOffset": 34}, {"referenceID": 1, "context": "This usually makes off-policy algorithms less stable during learning, compared to on-policy algorithms using a large batch size for each update (Duan et al., 2016; Gu et al., 2017).", "startOffset": 144, "endOffset": 180}, {"referenceID": 3, "context": "This usually makes off-policy algorithms less stable during learning, compared to on-policy algorithms using a large batch size for each update (Duan et al., 2016; Gu et al., 2017).", "startOffset": 144, "endOffset": 180}, {"referenceID": 3, "context": "Q-Prop (Gu et al., 2017), for example, used Q\u0303w, the first-order Taylor expansion of Qw, as the control variates, and showed improvement in stability and sample efficiency of policy gradient methods.", "startOffset": 7, "endOffset": 24}, {"referenceID": 24, "context": "However, as we show in Section 4, we can bound the biases for all the cases, and in some cases, the algorithm still guarantees monotonic convergence as in Kakade & Langford (2002); Schulman et al. (2015).", "startOffset": 181, "endOffset": 204}, {"referenceID": 14, "context": "Prior work such as DDPG Lillicrap et al. (2016) and related Q-learning methods have proposed aggressive off-policy exploration strategy to exploit these properties of the algorithm.", "startOffset": 24, "endOffset": 48}, {"referenceID": 24, "context": "To analyze the bias for this gradient expression, we first introduce J\u0303(\u03c0, \u03c0\u0303), a local approximation to J(\u03c0), which has been used in prior theoretical work (Kakade & Langford, 2002; Schulman et al., 2015).", "startOffset": 157, "endOffset": 205}, {"referenceID": 20, "context": "This means that the algorithm fits well with policy gradient methods which constrain the KL divergence per policy update, such as covariant policy gradient (Bagnell & Schneider, 2003), natural policy gradient (Kakade & Langford, 2002), REPS (Peters et al., 2010), and trust-region policy optimization (TRPO) (Schulman et al.", "startOffset": 241, "endOffset": 262}, {"referenceID": 24, "context": ", 2010), and trust-region policy optimization (TRPO) (Schulman et al., 2015).", "startOffset": 53, "endOffset": 76}, {"referenceID": 24, "context": "Such guarantees often correspond to stable empirical performance on challenging problems, even when some of the constraints are relaxed in practice (Schulman et al., 2015; Duan et al., 2016; Gu et al., 2017).", "startOffset": 148, "endOffset": 207}, {"referenceID": 1, "context": "Such guarantees often correspond to stable empirical performance on challenging problems, even when some of the constraints are relaxed in practice (Schulman et al., 2015; Duan et al., 2016; Gu et al., 2017).", "startOffset": 148, "endOffset": 207}, {"referenceID": 3, "context": "Such guarantees often correspond to stable empirical performance on challenging problems, even when some of the constraints are relaxed in practice (Schulman et al., 2015; Duan et al., 2016; Gu et al., 2017).", "startOffset": 148, "endOffset": 207}, {"referenceID": 21, "context": "1 Monotonic Policy Improvement Guarantee Some forms of on-policy policy gradient methods have theoretical guarantees on monotonic convergence Kakade & Langford (2002); Schulman et al. (2015). Such guarantees often correspond to stable empirical performance on challenging problems, even when some of the constraints are relaxed in practice (Schulman et al.", "startOffset": 168, "endOffset": 191}, {"referenceID": 1, "context": ", 2015; Duan et al., 2016; Gu et al., 2017). We can show that Algorithm 2, which is a variant of IPG, guarantees monotonic convergence. The proof is provided in the appendix. Algorithm 2 is often impractical to implement; however, IPG with trust-region updates when \u03b2 6= \u03c0, \u03bd = 1,CV = true approximates this monotonic algorithm, similar to how TRPO is an approximation to the theoretically monotonic algorithm proposed by Schulman et al. (2015).", "startOffset": 8, "endOffset": 445}, {"referenceID": 3, "context": "Our analysis examines how Q-Prop (Gu et al., 2017), PGQ (O\u2019Donoghue et al.", "startOffset": 33, "endOffset": 50}, {"referenceID": 18, "context": ", 2017), PGQ (O\u2019Donoghue et al., 2017), and ACER (Wang et al.", "startOffset": 13, "endOffset": 38}, {"referenceID": 32, "context": ", 2017), and ACER (Wang et al., 2017), which are all recent works that combine on-policy with off-policy learning, are connected to each other (see Table 1).", "startOffset": 18, "endOffset": 37}, {"referenceID": 0, "context": "Asides from these more recent works, the use of off-policy samples with policy gradients has been a popular direction of research (Peshkin & Shelton, 2002; Jie & Abbeel, 2010; Degris et al., 2012; Levine & Koltun, 2013).", "startOffset": 130, "endOffset": 219}, {"referenceID": 0, "context": "Asides from these more recent works, the use of off-policy samples with policy gradients has been a popular direction of research (Peshkin & Shelton, 2002; Jie & Abbeel, 2010; Degris et al., 2012; Levine & Koltun, 2013). Most of these methods rely on variants of importance sampling (IS) to correct for bias. The use of importance sampling ensures unbiased estimates, but at the cost of considerable variance, as quantified by the ESS measure used by Jie & Abbeel (2010). Ignoring importance weights produces bias but, as shown in our analysis, this bias can be bounded.", "startOffset": 176, "endOffset": 471}, {"referenceID": 0, "context": "Asides from these more recent works, the use of off-policy samples with policy gradients has been a popular direction of research (Peshkin & Shelton, 2002; Jie & Abbeel, 2010; Degris et al., 2012; Levine & Koltun, 2013). Most of these methods rely on variants of importance sampling (IS) to correct for bias. The use of importance sampling ensures unbiased estimates, but at the cost of considerable variance, as quantified by the ESS measure used by Jie & Abbeel (2010). Ignoring importance weights produces bias but, as shown in our analysis, this bias can be bounded. Therefore, our IPG estimators have higher bias as the sampling distribution deviates from the policy, while IS methods have higher variance. Among these importance sampling methods, Levine & Koltun (2013) evaluates on tasks that are the most similar to our paper, but the focus is on using importance sampling to include demonstrations, rather than to speed up learning from scratch.", "startOffset": 176, "endOffset": 776}, {"referenceID": 15, "context": "Lastly, there are many methods that combine on- and off-policy data for policy evaluation (Precup, 2000; Mahmood et al., 2014; Munos et al., 2016), mostly through variants of importance sampling.", "startOffset": 90, "endOffset": 146}, {"referenceID": 17, "context": "Lastly, there are many methods that combine on- and off-policy data for policy evaluation (Precup, 2000; Mahmood et al., 2014; Munos et al., 2016), mostly through variants of importance sampling.", "startOffset": 90, "endOffset": 146}, {"referenceID": 0, "context": "Combining our methods with more sophisticated policy evaluation methods will likely lead to further improvements, as done in (Degris et al., 2012).", "startOffset": 125, "endOffset": 146}, {"referenceID": 0, "context": "Combining our methods with more sophisticated policy evaluation methods will likely lead to further improvements, as done in (Degris et al., 2012). A more detailed analysis of the effect of importance sampling on bias and variance is left to future work, where some of the relevant work includes Precup (2000); Jie & Abbeel (2010); Mahmood et al.", "startOffset": 126, "endOffset": 310}, {"referenceID": 0, "context": "Combining our methods with more sophisticated policy evaluation methods will likely lead to further improvements, as done in (Degris et al., 2012). A more detailed analysis of the effect of importance sampling on bias and variance is left to future work, where some of the relevant work includes Precup (2000); Jie & Abbeel (2010); Mahmood et al.", "startOffset": 126, "endOffset": 331}, {"referenceID": 0, "context": "Combining our methods with more sophisticated policy evaluation methods will likely lead to further improvements, as done in (Degris et al., 2012). A more detailed analysis of the effect of importance sampling on bias and variance is left to future work, where some of the relevant work includes Precup (2000); Jie & Abbeel (2010); Mahmood et al. (2014); Jiang & Li (2016); Thomas & Brunskill (2016).", "startOffset": 126, "endOffset": 354}, {"referenceID": 0, "context": "Combining our methods with more sophisticated policy evaluation methods will likely lead to further improvements, as done in (Degris et al., 2012). A more detailed analysis of the effect of importance sampling on bias and variance is left to future work, where some of the relevant work includes Precup (2000); Jie & Abbeel (2010); Mahmood et al. (2014); Jiang & Li (2016); Thomas & Brunskill (2016).", "startOffset": 126, "endOffset": 373}, {"referenceID": 0, "context": "Combining our methods with more sophisticated policy evaluation methods will likely lead to further improvements, as done in (Degris et al., 2012). A more detailed analysis of the effect of importance sampling on bias and variance is left to future work, where some of the relevant work includes Precup (2000); Jie & Abbeel (2010); Mahmood et al. (2014); Jiang & Li (2016); Thomas & Brunskill (2016).", "startOffset": 126, "endOffset": 400}, {"referenceID": 31, "context": "All experiments were performed on MuJoCo domains in OpenAI Gym (Todorov et al., 2012; Brockman et al., 2016), with results presented for the average over three seeds.", "startOffset": 63, "endOffset": 108}, {"referenceID": 16, "context": "This is the original motivation for experience replay in the DQN method (Mnih et al., 2015), and we have shown that such decorrelated off-policy samples can similarly produce gains for policy gradient algorithms.", "startOffset": 72, "endOffset": 91}, {"referenceID": 14, "context": "Although IPG-\u03bd=1 methods can be off-policy, the policy is updated every 5000 samples to keep it consistent with other IPG methods, while DDPG updates the policy on every step in the environment and makes other design choices Lillicrap et al. (2016). We see that, in this domain, standard DDPG becomes stuck with a mean reward of 1000, while IPG-\u03bd=1 improves monotonically, achieving a significantly better result.", "startOffset": 225, "endOffset": 249}, {"referenceID": 18, "context": "This is consistent with the results in PGQ (O\u2019Donoghue et al., 2017) and ACER (Wang et al.", "startOffset": 43, "endOffset": 68}, {"referenceID": 32, "context": ", 2017) and ACER (Wang et al., 2017), which found that their equivalent of \u03bd = 0.", "startOffset": 17, "endOffset": 36}, {"referenceID": 14, "context": "IPG-\u03bd=1 is a novel variant of the actor-critic method that differs from DDPG (Lillicrap et al., 2016) and SVG(0) (Heess et al.", "startOffset": 77, "endOffset": 101}, {"referenceID": 4, "context": ", 2016) and SVG(0) (Heess et al., 2015) due to the use of a trust region.", "startOffset": 19, "endOffset": 39}], "year": 2017, "abstractText": "Off-policy model-free deep reinforcement learning methods using previously collected data can improve sample efficiency over on-policy policy gradient techniques. On the other hand, on-policy algorithms are often more stable and easier to use. This paper examines, both theoretically and empirically, approaches to merging onand off-policy updates for deep reinforcement learning. Theoretical results show that off-policy updates with a value function estimator can be interpolated with on-policy policy gradient updates whilst still satisfying performance bounds. Our analysis uses control variate methods to produce a family of policy gradient algorithms, with several recently proposed algorithms being special cases of this family. We then provide an empirical comparison of these techniques with the remaining algorithmic details fixed, and show how different mixing of off-policy gradient estimates with on-policy samples contribute to improvements in empirical performance. The final algorithm provides a generalization and unification of existing deep policy gradient techniques, has theoretical guarantees on the bias introduced by off-policy updates, and improves on the state-of-the-art model-free deep RL methods on a number of OpenAI Gym continuous control benchmarks.", "creator": "LaTeX with hyperref package"}}}