{"id": "1706.00834", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2017", "title": "Online Dynamic Programming", "abstract": "We consider the problem of repeatedly solving a variant of the same dynamic programming problem in successive trials. An instance of the type of problems we consider is to find the optimal binary search tree. At the beginning of each trial, the learner probabilistically chooses a tree with the n keys at the internal nodes and the n + 1 gaps between keys at the leaves. It is then told the frequencies of the keys and gaps and is charged by the average search cost for the chosen tree. The problem is online because the frequencies can change between trials. The goal is to develop algorithms with the property that their total average search cost (loss) in all trials is close to the total loss of the best tree chosen in hind sight for all trials. The challenge, of course, is that the algorithm has to deal with exponential number of trees. We develop a methodology for tackling such problems for a wide class of dynamic programming algorithms. Our framework allows us to extend online learning algorithms like Hedge and Component Hedge to a significantly wider class of combinatorial objects than was possible before.", "histories": [["v1", "Fri, 2 Jun 2017 20:02:19 GMT  (1919kb,D)", "https://arxiv.org/abs/1706.00834v1", null], ["v2", "Wed, 7 Jun 2017 22:30:44 GMT  (1919kb,D)", "http://arxiv.org/abs/1706.00834v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["holakou rahmanian", "s v n vishwanathan", "manfred k warmuth"], "accepted": true, "id": "1706.00834"}, "pdf": {"name": "1706.00834.pdf", "metadata": {"source": "CRF", "title": "Online Dynamic Programming", "authors": ["Holakou Rahmanian"], "emails": ["holakou@ucsc.edu", "vishy@ucsc.edu", "manfred@ucsc.edu"], "sections": [{"heading": "1 Introduction", "text": "This year it is so far that it will be able to the mentioned rf\u00fc the mentioned rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the reG-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green"}, {"heading": "2 Background", "text": "Perhaps the simplest algorithms in online learning are the well-known so-called \"expert algorithms\" such as the Randomized Weighted Majority [17] or Hedge [9] algorithms. They track a probability vector across all experts and the weight wi of the expert i is proportional to exp (\u2212 \u03b7 L (i)), where L (i) is the cumulative loss of the expert i through experimentation and \u03b7 is a non-negative learning rate. In our application we use the typically exponential number of combinatorial objects as a group of experts. As [15] to make a clear distinction between component hedge, we henceforth call this algorithm Expanded Hedge (EH) in this paper."}, {"heading": "2.1 Learning Paths", "text": "The shortest online path has been studied both in the complete information setting [15, 21] and in various bandit settings [2, 3, 8, 11]. Specifically, the problem in the complete information setting is as follows: Consider a directed acyclic graph (DAG) G = (V, E) with the designated source node s-V and the sink node \u03c9-V. In each attempt, the algorithm predicts with a path from s to \u03c9. Then, for each edge e-E, the opponent indicates a loss' e-V [0, 1]. The loss of the algorithm results from the sum of the losses of the edges along the predicted path. The aim is to minimize the regret that is the difference between the total loss of the algorithm and that of the subsequently best chosen path. In the literature for this setting, there are two main algorithms:"}, {"heading": "2.1.1 Expanded Hedge on Paths", "text": "Takimoto and Warmuth [21] developed an algorithmic approach, namely path cores, to apply extended hedge efficiently to the path problem by exploiting the underlying structure of the paths. Essentially, path cores maintain a distribution of paths by keeping the weights at the edges along the paths normalized. In each experiment, the weight of each edge is updated multiplicatively by the associated exposed loss, followed by a normalization by weight shift [18]. Finally, to proceed from the distribution, we jump from the source to the next node according to the localized distribution over the outgoing edges at each node. The regret limit is similar to that of the hedge algorithm [9]: Theorem 1 (Takimoto-Warmuth [21]). Given a DAG G = (V, E) with named source nodes s-V \u2212 and the sink node-V, letN and L = the number of paths in total G-21 is guaranteed to search for the T and T [2]."}, {"heading": "2.1.2 Component Hedge on Paths", "text": "In each study, the weight of the individual components (i.e.) is multiplied by their associated exponential losses. Then, the weight vector is projected back onto the unit flow polytopic via relative entropy projection. To do this, Bregman iterative projection [5] is used, which basically forces the conservation of the individual nodes by specifying the geometric mean of the individual nodes."}, {"heading": "4 Dynamic Programming Games", "text": "In this section we focus on the online learning of \"U.\" (U). \"U.\" (U). \"U.\" (U). \"U.\" (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U) U). (U). (U). (U) U). (U). (U). (U). (U) U). (U). (U). (U). (U). (U). (U). (U) U U). (U). (U). (U) (U). (U). (U) (U). (U) (U) (U) U U). (U). (U). (U) (U) U). (U). (U) U). (U) (U). (U) U). (U). U U). (U U). (U). U. U. (U) U). (U) U. U. U. (U) U. U. U. U. (U) U. U. (U) U. U. U. U. (U) U U U. U. U U U U U U. (U U U U U. (U U U U U. U U U U U U U. (U). U U U U U U U U U U U. (U U U U U U U U U U U U. (U U U U U). (U). (U). U). U U U U U U U U U U U. U U U U U U U U U U U U U U U U U U U U U U U U U U U U"}, {"heading": "5 Online Learning of Binary Search Trees", "text": "s look at the online version of the optimal binary search tree problem [6]. Specifically, we get a sequence of n unique keys K1 < K2 <. < Kn to create a binary search tree (BST). In addition, we get n + 1 \"dummy keys\" D0,..., Dn that specify search errors and for all i [n] we have Tue \u2212 1 < Ki < Tue. In each attempt t the algorithm predicts with a BST \u03b3 (t). Then the opponent shows a probability vector '(t) = (p (t), q (t))) with p (t), [0, 1] n, q (t) n = deppert (i)."}, {"heading": "5.1 Challenges of the Original Space", "text": "Since the loss in relation to the depth of the keys in BST is linear, it may sound natural to work with the space of the depth sequences of the keys. Specifically, one now considers the convex shell of all \u03b3's in this space. To our knowledge, there is no well-known characterization of this polyp and we believe that if there is one, it will probably have exponentially many facets, making it impossible to apply CH-like algorithms directly to this problem in the original space. Furthermore, the algorithm of Suehiro et. al. [20] does not apply to this problem, since the polytopic is not a subset of a simplex and therefore cannot be characterized as a submodular base polytopic. Nevertheless, one can apply Follow the Perturbed Leader (FPT) [13] to this problem, since the FPT is not a subset of a simplex and therefore cannot be defined as a search result."}, {"heading": "5.2 The Dynamic Programming Game", "text": "We now go beyond the modules of the underlying dynamic programming game \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "6 Conclusions and Future Work", "text": "We have developed a general framework for online learning of combinatorial objects, whose offline optimization problems can be efficiently solved by a large class of dynamic programming algorithms. Several examples of such objects are discussed in the main part and in the appendix of this paper. Table 1 shows the performance of EH and CH in our framework (referred to by the prefix \"DP\") for various problems. See Appendix A. In Expanded Hedge, the predictions are accurate (i.e., renormalize a weight vector). In contrast, iterative Bregman predictions are commonly used for algorithms such as component hedge [12, 15] which are known to theoretically converge with the exact projection [4, 5] and are considered empirically very efficient [15]. However, the iterative character of the projection steps requires an analysis such as the one in Appendix E to bind the additional loss arising from the interruption of full convergence."}, {"heading": "Acknowledgments", "text": "Holakou Rahmanian and Manfred K. Warmuth were supported by the NSF scholarship IIS-1619271."}, {"heading": "A More Instantiations", "text": "We apply our method to a few more cases of combinatorial objects detected by dynamic programming. (A2A) (A1A) (A1A) (A1A) (A1A) (A1A) (A1A) (A1A) (A1A) (Chain Multiplication) (Chain Multiplication) (Chain Multiplication) (Chain Multiplication) (Chain Multiplication) (Chain Multiplication) (Chain Multiplication) (Chain Multiplication) (Chain Multiplication) (Chain Multiplication) (Chain Multiplication) (Chain Multiplication) (Chain Multiplication) (Chain Multiplication) () (Chain Multiplication) ()."}, {"heading": "B Generalized Weight Pushing Correctness", "text": "The weights w (t), E (t), E (t), E (t), E (t), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), W (v), E (v), E (v), E (e), E (v), E (v), E (v), E (e), E (v), E (v), E (e), E (v), E (e), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (v), E (e), E (v), E (v (v), E (e), E (v (v), E (e), E (v (v), E (v (v), E (e), E (v (e), E (v (v), E (v (e), E (v (v), E (e), E (v (e), E (v (v (v), E (v (v), E (v (v), E (v (v), E (v (v), E (v (v), E (v (v), E (e), E (v (v), E (v (v), E (v (v), E (v (v), E (v (v), E (v), E (v (v), E (v (v), E (v), E (v ("}, {"heading": "E Additional Loss with Approximate Projection", "text": "First, let us define the notation. Since we work with two vectors a and b, we say that each element of a is less than or equal to. (Suppose we use Bregman iterative projections and additional losses. (Suppose we have the following steps for predicting: 8This calculation can be performed as a predictive step. (Suppose we work with the exact projection w | F | F | + in 1 standard), that is, we have the following steps for predicting: 8This calculation can be performed as a predictive step. (Suppose we have the exact projection w + 1-2W). (Applied definition of F | F | + is a vector of all entries. (Applied decomposition method to w). (Applied decomposition method to w-W). (Applied decomposition method to F-Z). (Applied decomposition method to F-Z). (Applied decomposition method to F-Z)."}], "references": [{"title": "Improved bounds for online learning over the permutahedron and other ranking polytopes", "author": ["Nir Ailon"], "venue": "In AISTATS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Regret in online combinatorial optimization", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck", "G\u00e1bor Lugosi"], "venue": "Mathematics of Operations Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Online linear optimization and adaptive routing", "author": ["Baruch Awerbuch", "Robert Kleinberg"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Legendre functions and the method of random bregman projections", "author": ["Heinz H Bauschke", "Jonathan M Borwein"], "venue": "Journal of Convex Analysis,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming", "author": ["Lev M Bregman"], "venue": "USSR computational mathematics and mathematical physics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1967}, {"title": "Introduction to algorithms", "author": ["Thomas H.. Cormen", "Charles Eric Leiserson", "Ronald L Rivest", "Clifford Stein"], "venue": "MIT press Cambridge,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "On-line learning algorithms for path experts with non-additive losses", "author": ["Corinna Cortes", "Vitaly Kuznetsov", "Mehryar Mohri", "Manfred Warmuth"], "venue": "In Conference on Learning Theory,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "The price of bandit information for online optimization", "author": ["Varsha Dani", "Sham M Kakade", "Thomas P Hayes"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "Journal of computer and system sciences,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Solving combinatorial games using products, projections and lexicographically optimal bases", "author": ["Swati Gupta", "Michel Goemans", "Patrick Jaillet"], "venue": "arXiv preprint arXiv:1603.00522,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "The on-line shortest path problem under partial monitoring", "author": ["Andr\u00e1s Gy\u00f6rgy", "Tam\u00e1s Linder", "G\u00e1bor Lugosi", "Gy\u00f6rgy Ottucs\u00e1k"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Learning permutations with exponential weights", "author": ["David P Helmbold", "Manfred K Warmuth"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Efficient algorithms for online decision problems", "author": ["Adam Kalai", "Santosh Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Optimum follow the leader algorithm", "author": ["Dima Kuzmin", "Manfred K Warmuth"], "venue": "In Learning Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K Warmuth"], "venue": "Information and computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1994}, {"title": "Weighted automata algorithms. In Handbook of weighted automata, pages 213\u2013254", "author": ["Mehryar Mohri"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Online decision-making in general combinatorial spaces", "author": ["Arun Rajkumar", "Shivani Agarwal"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Online prediction under submodular constraints", "author": ["Daiki Suehiro", "Kohei Hatano", "Shuji Kijima", "Eiji Takimoto", "Kiyohito Nagano"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Path kernels and multiplicative updates", "author": ["Eiji Takimoto", "Manfred K Warmuth"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "Randomized online pca algorithms with regret bounds that are logarithmic in the dimension", "author": ["Manfred K Warmuth", "Dima Kuzmin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}], "referenceMentions": [{"referenceID": 8, "context": "Our framework allows us to extend online learning algorithms like Hedge [9] and Component Hedge [15] to a significantly wider class of combinatorial objects than was possible before.", "startOffset": 72, "endOffset": 75}, {"referenceID": 19, "context": "Such work includes k-sets [22], permutations [1, 12, 23] and paths [3, 7, 11, 16, 21].", "startOffset": 26, "endOffset": 30}, {"referenceID": 0, "context": "Such work includes k-sets [22], permutations [1, 12, 23] and paths [3, 7, 11, 16, 21].", "startOffset": 45, "endOffset": 56}, {"referenceID": 11, "context": "Such work includes k-sets [22], permutations [1, 12, 23] and paths [3, 7, 11, 16, 21].", "startOffset": 45, "endOffset": 56}, {"referenceID": 2, "context": "Such work includes k-sets [22], permutations [1, 12, 23] and paths [3, 7, 11, 16, 21].", "startOffset": 67, "endOffset": 85}, {"referenceID": 6, "context": "Such work includes k-sets [22], permutations [1, 12, 23] and paths [3, 7, 11, 16, 21].", "startOffset": 67, "endOffset": 85}, {"referenceID": 10, "context": "Such work includes k-sets [22], permutations [1, 12, 23] and paths [3, 7, 11, 16, 21].", "startOffset": 67, "endOffset": 85}, {"referenceID": 13, "context": "Such work includes k-sets [22], permutations [1, 12, 23] and paths [3, 7, 11, 16, 21].", "startOffset": 67, "endOffset": 85}, {"referenceID": 18, "context": "Such work includes k-sets [22], permutations [1, 12, 23] and paths [3, 7, 11, 16, 21].", "startOffset": 67, "endOffset": 85}, {"referenceID": 12, "context": "Follow the Perturbed Leader (FPL) [13] is a simple algorithm which adds random perturbation to the cumulative loss of each component, and then predicts with the combinatorial object with minimum perturbed loss.", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "to submodular base polytopes [10, 19, 20] are powerful generic techniques which keep track of component-wise weights in the convex hull of the objects for prediction.", "startOffset": 29, "endOffset": 41}, {"referenceID": 16, "context": "to submodular base polytopes [10, 19, 20] are powerful generic techniques which keep track of component-wise weights in the convex hull of the objects for prediction.", "startOffset": 29, "endOffset": 41}, {"referenceID": 17, "context": "to submodular base polytopes [10, 19, 20] are powerful generic techniques which keep track of component-wise weights in the convex hull of the objects for prediction.", "startOffset": 29, "endOffset": 41}, {"referenceID": 8, "context": "This graph representation allows us to extend both standard Hedge algorithm [9, 17] and Component Hedge to wide class of combinatorial objects like BST (see Section 5), Matrix-Chain Multiplication, Knapsack, Rod Cutting, and Weighted Interval Scheduling (see Appendix A).", "startOffset": 76, "endOffset": 83}, {"referenceID": 14, "context": "This graph representation allows us to extend both standard Hedge algorithm [9, 17] and Component Hedge to wide class of combinatorial objects like BST (see Section 5), Matrix-Chain Multiplication, Knapsack, Rod Cutting, and Weighted Interval Scheduling (see Appendix A).", "startOffset": 76, "endOffset": 83}, {"referenceID": 14, "context": "Perhaps the simplest algorithms in online learning are the well-known so-call \u201cexperts algorithms\u201d like the Randomized Weighted Majority [17] or Hedge [9] algorithms.", "startOffset": 137, "endOffset": 141}, {"referenceID": 8, "context": "Perhaps the simplest algorithms in online learning are the well-known so-call \u201cexperts algorithms\u201d like the Randomized Weighted Majority [17] or Hedge [9] algorithms.", "startOffset": 151, "endOffset": 154}, {"referenceID": 18, "context": "1 Learning Paths The online shortest path has been explored both in full information setting [15, 21] and various bandit settings [2, 3, 8, 11].", "startOffset": 93, "endOffset": 101}, {"referenceID": 1, "context": "1 Learning Paths The online shortest path has been explored both in full information setting [15, 21] and various bandit settings [2, 3, 8, 11].", "startOffset": 130, "endOffset": 143}, {"referenceID": 2, "context": "1 Learning Paths The online shortest path has been explored both in full information setting [15, 21] and various bandit settings [2, 3, 8, 11].", "startOffset": 130, "endOffset": 143}, {"referenceID": 7, "context": "1 Learning Paths The online shortest path has been explored both in full information setting [15, 21] and various bandit settings [2, 3, 8, 11].", "startOffset": 130, "endOffset": 143}, {"referenceID": 10, "context": "1 Learning Paths The online shortest path has been explored both in full information setting [15, 21] and various bandit settings [2, 3, 8, 11].", "startOffset": 130, "endOffset": 143}, {"referenceID": 0, "context": "Then, for each edge e \u2208 E, the adversary reveals a loss `e \u2208 [0, 1].", "startOffset": 61, "endOffset": 67}, {"referenceID": 18, "context": "1 Expanded Hedge on Paths Takimoto and Warmuth [21] developed an algorithmic approach, namely path kernels, to apply Expanded Hedge on path problem efficiently by exploiting the underlying structure of the paths.", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "In each trial, the weight of each edge is updated multiplicatively by its associated exponentiated loss followed by normalization via weight pushing [18].", "startOffset": 149, "endOffset": 153}, {"referenceID": 8, "context": "The regret bound is similar to the one of the Hedge algorithm [9]:", "startOffset": 62, "endOffset": 65}, {"referenceID": 18, "context": "Theorem 1 (Takimoto-Warmuth [21]).", "startOffset": 28, "endOffset": 32}, {"referenceID": 4, "context": "To do projection, iterative Bregman projection [5] is used which basically enforces flow conservation at each node by setting input and output flow to their geometric average as it cycles through the vertices.", "startOffset": 47, "endOffset": 50}, {"referenceID": 0, "context": "Then, for each edge e \u2208 E, the adversary reveals a loss ` e \u2208 [0, 1].", "startOffset": 62, "endOffset": 68}, {"referenceID": 15, "context": "Generalized Weight Pushing For efficient normalization, we generalize the weight pushing algorithm [18] for k-multipaths.", "startOffset": 99, "endOffset": 103}, {"referenceID": 8, "context": "Regret Bound In order to use the regret bound of Expanded Hedge [9], we have to initialize W (0) to the uniform distribution.", "startOffset": 64, "endOffset": 67}, {"referenceID": 4, "context": "To do this projection, iterative Bregman projection [5] can be used which cycles through the constraints and project the point to each constraint iteratively.", "startOffset": 52, "endOffset": 55}, {"referenceID": 5, "context": "Denote V as the set of all subproblems v in dynamic programming which needs to be solved in order to eventually solve P iteratively in bottom-up fashion [6].", "startOffset": 153, "endOffset": 156}, {"referenceID": 0, "context": "Now if the given loss function of the problem is in such way that `e \u2208 [0, 1], we can apply both algorithms CH and EH and their corresponding regret guarantees in Theorems 3 and 4, respectively 5.", "startOffset": 71, "endOffset": 77}, {"referenceID": 5, "context": "Consider the online version of optimal binary search tree problem [6].", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "Then the adversary reveals the a probability vector ` = (p, q) with p \u2208 [0, 1], q \u2208 [0, 1] and \u2211n i=1 p (t) i + \u2211n i=0 q (t) i = 1.", "startOffset": 72, "endOffset": 78}, {"referenceID": 0, "context": "Then the adversary reveals the a probability vector ` = (p, q) with p \u2208 [0, 1], q \u2208 [0, 1] and \u2211n i=1 p (t) i + \u2211n i=0 q (t) i = 1.", "startOffset": 84, "endOffset": 90}, {"referenceID": 17, "context": "[20] does not apply to this problem as the polytope is not a subset of a simplex, and consequently, cannot be characterized as submodular base polytope.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Nevertheless, one can apply Follow the Perturbed Leader (FPT) [13] to this problem.", "startOffset": 62, "endOffset": 66}, {"referenceID": 5, "context": "programming [6] with the recurrence relation below:", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "Regret Bound It is well-known that the number of binary trees with n nodes is the nth Catalan number [6].", "startOffset": 101, "endOffset": 104}, {"referenceID": 11, "context": "In contrast, iterative Bregman projections are often used for algorithms like Component Hedge [12, 15].", "startOffset": 94, "endOffset": 102}, {"referenceID": 3, "context": "These methods are known to converge to the exact projection theoretically [4, 5] and are reported to be empirically very efficient [15].", "startOffset": 74, "endOffset": 80}, {"referenceID": 4, "context": "These methods are known to converge to the exact projection theoretically [4, 5] and are reported to be empirically very efficient [15].", "startOffset": 74, "endOffset": 80}, {"referenceID": 0, "context": "References [1] Nir Ailon.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Jean-Yves Audibert, S\u00e9bastien Bubeck, and G\u00e1bor Lugosi.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Baruch Awerbuch and Robert Kleinberg.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Heinz H Bauschke and Jonathan M Borwein.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Lev M Bregman.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Thomas H.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri, and Manfred Warmuth.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Varsha Dani, Sham M Kakade, and Thomas P Hayes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Yoav Freund and Robert E Schapire.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Swati Gupta, Michel Goemans, and Patrick Jaillet.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Andr\u00e1s Gy\u00f6rgy, Tam\u00e1s Linder, G\u00e1bor Lugosi, and Gy\u00f6rgy Ottucs\u00e1k.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] David P Helmbold and Manfred K Warmuth.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Adam Kalai and Santosh Vempala.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] Dima Kuzmin and Manfred K Warmuth.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] Nick Littlestone and Manfred K Warmuth.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] Mehryar Mohri.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] Arun Rajkumar and Shivani Agarwal.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] Daiki Suehiro, Kohei Hatano, Shuji Kijima, Eiji Takimoto, and Kiyohito Nagano.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] Eiji Takimoto and Manfred K Warmuth.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] Manfred K Warmuth and Dima Kuzmin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Now consider the following online version of matrix-chain multiplication problem [6].", "startOffset": 81, "endOffset": 84}, {"referenceID": 5, "context": "The matrix-chain multiplication problem can be solved via dynamic programming [6] with the recurrence relation below:", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "Regret Bound It is well-known that the number of full parenthesizing of a sequence of n matrices is the nth Catalan number [6].", "startOffset": 123, "endOffset": 126}, {"referenceID": 0, "context": "Then the adversary reveals a profit vector p \u2208 [0, 1] in which the ith component \u2013 p i \u2013 is the profit of the ith item at trial t.", "startOffset": 47, "endOffset": 53}, {"referenceID": 17, "context": "[20] does not apply to this problem as the polytope is not a subset of a simplex, and consequently, cannot be characterized as submodular base polytope.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Nevertheless, one can apply Follow the Perturbed Leader (FPT) [13] to this problem.", "startOffset": 62, "endOffset": 66}, {"referenceID": 1, "context": "Figure 4: Example with C = 7 and h = [2, 3, 4].", "startOffset": 37, "endOffset": 46}, {"referenceID": 2, "context": "Figure 4: Example with C = 7 and h = [2, 3, 4].", "startOffset": 37, "endOffset": 46}, {"referenceID": 3, "context": "Figure 4: Example with C = 7 and h = [2, 3, 4].", "startOffset": 37, "endOffset": 46}, {"referenceID": 5, "context": "3 Rod Cutting Consider the online version of rod cutting problem [6].", "startOffset": 65, "endOffset": 68}, {"referenceID": 0, "context": "Then the adversary reveals a price vector p \u2208 [0, 1] in which the ith component \u2013 p i \u2013 is the price of the piece of length i at trial t.", "startOffset": 46, "endOffset": 52}, {"referenceID": 17, "context": "[20] does not apply to this problem as the polytope is not a subset of a simplex, and consequently, cannot be characterized as submodular base polytope.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Nevertheless, one can apply Follow the Perturbed Leader (FPT) [13] to this problem.", "startOffset": 62, "endOffset": 66}, {"referenceID": 5, "context": "The rod cutting problem can be solved via dynamic programming [6] with the recurrence relation below: OPT(i) = { 0 i = 0 max0\u2264j\u2264i{OPT (j) + p j\u2212i} i > 0 (9)", "startOffset": 62, "endOffset": 65}, {"referenceID": 10, "context": "[11], we can add O(n) vertices and edges (with gain zero) to make all paths equi-length.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "The number of possible cutting is called partition function which is approximately e \u221a /4n \u221a 3 [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 0, "context": "Then the adversary reveals a profit vector p \u2208 [0, 1] in which the ith component \u2013 p i \u2013 is the profit of including Ii in the scheduling at trial t.", "startOffset": 47, "endOffset": 53}, {"referenceID": 17, "context": "[20] does not apply to this problem as the polytope is not a subset of a simplex, and consequently, cannot be characterized as submodular base polytope.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Nevertheless, one can apply Follow the Perturbed Leader (FPT) [13] to this problem.", "startOffset": 62, "endOffset": 66}], "year": 2017, "abstractText": "We consider the problem of repeatedly solving a variant of the same dynamic programming problem in successive trials. An instance of the type of problems we consider is to find the optimal binary search tree. At the beginning of each trial, the learner probabilistically chooses a tree with the n keys at the internal nodes and the n+ 1 gaps between keys at the leaves. It is then told the frequencies of the keys and gaps and is charged by the average search cost for the chosen tree. The problem is online because the frequencies can change between trials. The goal is to develop algorithms with the property that their total average search cost (loss) in all trials is close to the total loss of the best tree chosen in hind sight for all trials. The challenge, of course, is that the algorithm has to deal with exponential number of trees. We develop a methodology for tackling such problems for a wide class of dynamic programming algorithms. Our framework allows us to extend online learning algorithms like Hedge [9] and Component Hedge [15] to a significantly wider class of combinatorial objects than was possible before.", "creator": "LaTeX with hyperref package"}}}