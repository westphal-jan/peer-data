{"id": "1706.01026", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2017", "title": "Adaptive Multiple-Arm Identification", "abstract": "We study the problem of selecting $K$ arms with the highest expected rewards in a stochastic $n$-armed bandit game. This problem has a wide range of applications, e.g., A/B testing, crowdsourcing, simulation optimization. Our goal is to develop a PAC algorithm, which, with probability at least $1-\\delta$, identifies a set of $K$ arms with the aggregate regret at most $\\epsilon$. The notion of aggregate regret for multiple-arm identification was first introduced in \\cite{Zhou:14} , which is defined as the difference of the averaged expected rewards between the selected set of arms and the best $K$ arms. In contrast to \\cite{Zhou:14} that only provides instance-independent sample complexity, we introduce a new hardness parameter for characterizing the difficulty of any given instance. We further develop two algorithms and establish the corresponding sample complexity in terms of this hardness parameter. The derived sample complexity can be significantly smaller than state-of-the-art results for a large class of instances and matches the instance-independent lower bound upto a $\\log(\\epsilon^{-1})$ factor in the worst case. We also prove a lower bound result showing that the extra $\\log(\\epsilon^{-1})$ is necessary for instance-dependent algorithms using the introduced hardness parameter.", "histories": [["v1", "Sun, 4 Jun 2017 05:04:52 GMT  (1000kb,D)", "http://arxiv.org/abs/1706.01026v1", "30 pages, 5 figures, preliminary version to appear in ICML 2017"]], "COMMENTS": "30 pages, 5 figures, preliminary version to appear in ICML 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jiecao chen", "xi chen", "qin zhang 0001", "yuan zhou"], "accepted": true, "id": "1706.01026"}, "pdf": {"name": "1706.01026.pdf", "metadata": {"source": "CRF", "title": "Adaptive Multiple-Arm Identification\u2217", "authors": ["Jiecao Chen", "Xi Chen", "Leonard N. Stern"], "emails": ["jiecchen@umail.iu.edu", "xchen3@stern.nyu.edu", "qzhangcs@indiana.edu", "yzhoucs@indiana.edu"], "sections": [{"heading": null, "text": "We are investigating the problem of selecting K-weapons with the highest expected rewards in a stochastic n-armed bandit game. This problem has a wide range of applications, e.g. A / B testing, crowdsourcing, simulation optimization. Our goal is to develop a PAC algorithm that identifies with a probability of at least 1 \u2212 \u03b4 a group of K-weapons with the highest overall regret. Unlike Zhou et al. [2014], which only provides an instance-independent sample complexity, we are introducing a new hardness parameter to characterize the difficulty of a given instance. We are further developing two algorithms and determining the corresponding sample complexity in relation to this hardness parameter."}, {"heading": "1 Introduction", "text": "Considering a number of alternatives of varying quality, the identification of high-quality alternatives via a sequential experiment is an important issue in multi-arm studies of worker pay (MAB) literature, also known as the \"pure exploration\" problem. This problem has a wide range of applications. The question is: how should the agent adaptively select which design should be displayed next so that the high-quality designs can be quickly and accurately identified? For another example, in crowdsourcing, it is crucial to identify high-quality workers from the alphabetical order. Preliminary version to appear in ICML 2017.ar Xiv: 170 6.01 026v 1 [csa pool of a large number of noisy workers."}, {"heading": "1.1 Summary of Main Results", "text": "Following the existing literature (see e.g. Bubeck et al. [2013]) we first define the gap of the i-th arm \u2206 i (K) = empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical (K) -empirical) (K (K) -empirical) (K (K) -empirical) (K (K (K) -empirical) -empirical) (K (K (K) -empirical) (K (K (K) -empirical) -empirical) (K (K (K (K) -empirical) -empirical) -empirical) (K (K (K (K (K) -empirical) -empirical) -empirical) (K (K (K (K (K) -empirical) -empirical) -empirical) (K (K (K (K (K))) (K (K (K (K (K))) -empirical) -empirical) (K (K (K (K (K"}, {"heading": "1.2 Review of and Comparison with Related Works", "text": "The problem of identifying the single best arm (i.e. the top K arms with K = 1) was a problem that has been solved for this system. It has been extensively investigated [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Chen et al., 2016b]. Specifically, if our problem reduces the identification of a -best arm, i.e. an arm whose expected reward differs from the best arm by an additive error of at most, with a probability of 1 \u2212 3. For this problem, Even-Dar et al. [2006] showed an algorithm with an instinct-dependent sample O (n 2 log) (and this was found asymptotically optimal of Mannor and Tsitsiklis [2004]."}, {"heading": "2.1 Correctness of Algorithm 1", "text": "Definition 2 Let it be the event, let it be the event, let it be the event, let it be an event, let it be an event. Definition 2 Let it be. Definition 2 Let it be the event. Definition 2 Let it be the event. Definition 3 Let it be an event. Definition 2 Let it be an event. Definition 2 Let it be. Definition 2 Let it be. Definition 2 Let it be. Definition 2 Let it be. Definition 2 Let it be. Definition 2 Let it be an event. Definition 2 Let it be an event. Definition 2 Let it be. Definition 2 Let it be. Definition 2 Let it be. Definition 2 Let it be. Definition 2 Let it be."}, {"heading": "2.2 Query Complexity of Algorithm 1", "text": "Recall (in the statement of Theorem 4) that t- {0, 1, 2,. \u00b7 \u00b7 K-1} is the largest integer satisfying \u2206 K \u2212 t \u00b7 t \u2264 K. (13) Lemma 6 \u2212 \u2212 \u2212 \u2212 \u2212 If the algorithm leaves the outer loop while the algorithm leaves the outer loop after the execution of round r, then we must satisfy each valid round r (14) Proof: We show that once 2 \u2212 r < K \u2212 t / 4, the algorithm will leave the outer loop after the execution of round r. So each valid round r must be two \u2212 r \u00b2 K \u2212 t / 8 and the problem persists trivially. To this end, we assume that we are now in round r and 2 \u2212 r < K \u2212 t / 4, we have this for each i-th Sr and i \u2264 K \u2212 t,."}, {"heading": "3.1.1 Correctness of Algorithm 2", "text": "The following problem is the key to proving the correctness. \u2022 Lemma 8 With the probability that at least 1 \u2212 r = \"Event\" - \"Event\" - \"Event\" (\"Event\") - \"Event\" (\"Event\") - \"Event\" (\"Event\") - \"Event\" (\"Event\") - \"Event\" (\"Event\") - \"Event\" (\"Event\") - \"Event\" (\"Event\") - \"Event\" (\"Event\") - \"Event\" (\"Event\") - \"Event\" (\"Event\") - \"Event\" (\"Event\") - \"(\" Event \") -\" (\"Event\") - \"(\" Event \") -\" (\"Event\") - \"(\") - () () () - () () - () () - () () - () () - () () - () - () () - () - () - () - () - () - () - () - () - () - () () - () - () - () () - () - () () - () - () () - () - () - () - () () - () () - () () - () - () - () () - () () - () - () - () () - () - () - () - () () - () - () - () - () - () - () - () () () - () () () () ()) () () ()) - () () - () - () () () ()) () () ()))) ((()))) ((()) ((((())))))) - ((((((()))))) ((((((()))))))))) (((((())))))) ((((((())))))))) ((((((())))))))) (((())))"}, {"heading": "3.1.2 Complexity Algorithm 2", "text": "We can sum up the total number of draws of the algorithm by simple summing of the number of draws in each cycle (log 1 (3 / 4) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "3.2 The Improved Algorithm", "text": "In this section we will introduce an improved algorithm that eliminates the log (n) factor in sample complexity. We will first introduce a few more subroutines (Lemma 10, Lemma 11 and Lemma 12) that will be useful for our improved algorithm. (Lemma 10 [Zhou et al., 2014] For a number of weapons S = [2 log]. (S] there is an algorithm called OptMAI. (S, K, \u03b4) which calculates arms with the probability 1-2, with the probability. (S] 2 log (2 log) pulls.The following two Lemmas show how to find a constant fraction of weapons in the range of Top-K weapons and a constant fraction of weapons outside the range of Top-K weapons."}, {"heading": "3.2.1 Correctness of Algorithm 5", "text": "Define E1 to be the event that all calls to the subroutine EstKthArm's at line 8 and line 9. Input: n: number of weapons; K and: see the error probability of all calls to EstKthArm by 4 \u00b7 number of calls to EstKthArm by 4 \u2212 number of calls to EstKthArm z = 1 \u2264 number of calls to EstKthArm z = 1 \u2264 number of calls to EstKthArm. Output: -top-K weapons 1 set S [n], r 7: ImprovedTopK, \u03b4) Input: n: number of weapons; K and: see the error probability of -top-K arms; Jackson: error probability of -top-K."}, {"heading": "3.2.2 Complexity of Algorithm 5", "text": "Throughout the analysis, we assume that the e-mail address is as follows: \"We assume that our task is to provide the e-mail address.\" We assume that the e-mail address is as follows: \"We are obliged to provide the e-mail address.\" We assume that the e-mail address is as follows: \"We assume that we provide the e-mail address.\" We assume that our e-mail address is sent to the e-mail address: \"We assume that we provide the e-mail address.\" We assume that we provide the e-mail address."}, {"heading": "4 A Lower Bound", "text": "In this section we will prove theorem 3. In section 4.1 we will introduce a lower limit for a coin toss problem. In section 4.2 we will reduce the proof for theorem 3 to the coin toss problem."}, {"heading": "4.1 The Coin-Tossing Problem", "text": "We say that a coin is p-distorted if the probability of a toss turning the head is p-distorted, and we call p the value of the coin. We say that a coin is p-distorted if the probability of a toss turning the head is p-distorted, and we call p the value of the coin. Places \u03b7 = 10 \u2212 4.Definition 3 (Coin-Tossing). \u2212 \u2212 \u2212 \u2212 In this problem we have the following theorem 7 Any algorithm that correctly solves the coin toss problem, we want to know its exact value by toss (1 \u2212), and we are allowed to waive 5 m and issue \"unknown\" with a probability of no more than 0.9 m. Since the input is distributed, we just have to focus on deterrent algorithms. Let m be the total number of toss of the coin and let B = (B1,., Bm)."}, {"heading": "4.2 The Reduction", "text": "We show a reduction from the coin toss problem to the -top-K arm problem. For technical reasons, we issue K = n / 2, and assume that K \u2265 cK for a sufficiently large constant cK. Lemma 19 If there is an algorithm for -top-K arms that succeeds with a probability of 0.9, then there is an algorithm for coin toss that succeeds with (1 \u2212) using O (C / n) rolls. In addition, the instances fed into the -top-K arms algorithm have the property that H (t) = 2 (n) for the coin toss solution (s). We prove Lemma 19 in two steps. We first perform an input reduction and then show that we can construct an efficient algorithm for coin toss using a -top-K arm. Given an input X for coin toss, we construct an input reduction (Y = 1)."}, {"heading": "5 Experiments", "text": "In this section, we present the experimental results. While our theorems are presented here in PAC form, it is generally difficult to verify them directly because the parameter is only an upper limit and the actual aggregate regret may differ from it. In our experiment, we convert our algorithms 1 to the fixed budget version (that is, we fix the budget of the number of draws and calculate the aggregate regret).We compare our algorithms 1 (AdaptiveTopK) with two state-of-the-art methods - OptMAI in Zhou et al. [2014] and CLUCB-PAC in Chen et al. [2014] The comparison between OptMAI / CLUCB-PAC and previous methods in Bubeck et al. [2013] and Kalyanakrishnan et al. [2012] we have already demonstrated in Zhou et al."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we have proposed two algorithms for a PAC version of the multi-armed identification problem in a stochastic multi-armed bandit (MAB) game. We have introduced a new hardness parameter for characterizing the difficulty of an instance when using overall regret as a benchmark, and have determined the instance-dependent complexity of the sample based on this hardness parameter. We have also set lower hardness parameters to represent the optimality of our algorithm in the worst case. Although we only consider the case when supporting the reward distribution to [0, 1] it is easy to extend our results to sub-Gaussian reward distributions. For future instructions, it is worth looking at the more general problem of pure exploration of MAB under matroid constraints, which include multi-armed identification as a special case, or other polynomial-time combinatory limitations such as matching."}], "references": [{"title": "Large-scale markov decision problems with KL control cost and its application to crowdsourcing", "author": ["Y. Abbasi-Yadkori", "P. Bartlett", "X. Chen", "A. Malek"], "venue": "In Proceedings of International Conference on Machine Learning (ICML),", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2015}, {"title": "Best arm identification in multi-armed bandits", "author": ["J. Audibert", "S. Bubeck", "R. Munos"], "venue": "In Proceedings of the Conference on Learning Theory (COLT),", "citeRegEx": "Audibert et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2010}, {"title": "Multiple identifications in multi-armed bandits", "author": ["S. Bubeck", "T. Wang", "N. Viswanathan"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Bubeck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2013}, {"title": "On top-k selection in multi-armed bandits and hidden bipartite graphs", "author": ["W. Cao", "J. Li", "Y. Tao", "Z. Li"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Cao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2015}, {"title": "Pure exploration of multi-armed bandit under matroid constraints", "author": ["L. Chen", "A. Gupta", "J. Li"], "venue": "In Proceedings of the Conference on Learning Theory (COLT),", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Towards instance optimal bounds for best arm identification", "author": ["L. Chen", "J. Li", "M. Qiao"], "venue": "arXiv preprint arXiv:1608.06031,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Combinatorial pure exploration of multi-armed bandits", "author": ["S. Chen", "T. Lin", "I. King", "M.R. Lyu", "W. Chen"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Statistical decision making for optimal budget allocation in crowd labeling", "author": ["X. Chen", "Q. Lin", "D. Zhou"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "PAC bounds for multi-armed bandit and markov decision processes", "author": ["E. Even-Dar", "S. Mannor", "Y. Mansour"], "venue": "In Proceedings of the Annual Conference on Learning Theory (COLT),", "citeRegEx": "Even.Dar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2002}, {"title": "Action elimination and stopping conditions for the multiarmed bandit and reinforcement learning problems", "author": ["E. Even-Dar", "S. Mannor", "Y. Mansour"], "venue": "Journal of machine learning research,", "citeRegEx": "Even.Dar et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2006}, {"title": "Generalization of a probability limit theorem of cramer", "author": ["W. Feller"], "venue": "Trans. Amer. Math. Soc,", "citeRegEx": "Feller.,? \\Q1943\\E", "shortCiteRegEx": "Feller.", "year": 1943}, {"title": "Multi-bandit best arm identification", "author": ["V. Gabillon", "M. Ghavamzadeh", "A. Lazaric", "S. Bubeck"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Gabillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2011}, {"title": "Best arm identification: A unified approach to fixed budget and fixed confidence", "author": ["V. Gabillon", "M. Ghavamzadeh", "A. Lazaric"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Gabillon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2012}, {"title": "Improved learning complexity in combinatorial pure exploration bandits", "author": ["V. Gabillon", "A. Lazaric", "M. Ghavamzadeh", "R. Ortner", "P. Bartlett"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Gabillon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2016}, {"title": "UCB : An optimal exploration algorithm for multi-armed bandits", "author": ["K. Jamieson", "M. Malloy", "R. Nowak", "S. Bubeck"], "venue": "In Proceedings of the Conference on Learning Theory (COLT),", "citeRegEx": "Jamieson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jamieson et al\\.", "year": 2014}, {"title": "Top arm identification in multi-armed bandits with batch arm pulls", "author": ["K.-S. Jun", "K. Jamieson", "R. Nowak", "X. Zhu"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Jun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jun et al\\.", "year": 2016}, {"title": "PAC subset selection in stochastic multi-armed bandits", "author": ["S. Kalyanakrishnan", "A. Tewari", "P. Auer", "P. Stone"], "venue": "In Proceedings of International Conference on Machine Learning (ICML),", "citeRegEx": "Kalyanakrishnan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kalyanakrishnan et al\\.", "year": 2012}, {"title": "Almost optimal exploration in multi-armed bandits", "author": ["Z. Karnin", "T. Koren", "O. Somekh"], "venue": "In Proceedings of International Conference on Machine Learning (ICML),", "citeRegEx": "Karnin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karnin et al\\.", "year": 2013}, {"title": "On the complexity of best arm identification in multi-armed bandit models", "author": ["E. Kaufmann", "O. Capp\u00e9", "A. Garivier"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kaufmann et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2016}, {"title": "The sample complexity of exploration in the multi-armed bandit problem", "author": ["S. Mannor", "J.N. Tsitsiklis"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mannor and Tsitsiklis.,? \\Q2004\\E", "shortCiteRegEx": "Mannor and Tsitsiklis.", "year": 2004}, {"title": "The probabilistic method", "author": ["J. Matousek", "J. Vondr\u00e1k"], "venue": "Lecture Notes,", "citeRegEx": "Matousek and Vondr\u00e1k.,? \\Q2008\\E", "shortCiteRegEx": "Matousek and Vondr\u00e1k.", "year": 2008}, {"title": "Simple bayesian algorithms for best arm identification", "author": ["D. Russo"], "venue": "In Proceedings of the Conference on Learning Theory (COLT),", "citeRegEx": "Russo.,? \\Q2016\\E", "shortCiteRegEx": "Russo.", "year": 2016}, {"title": "Cheap and fast\u2014but is it good?: Evaluating nonexpert annotations for natural language tasks", "author": ["R. Snow", "B. O\u2019Connor", "D. Jurafsky", "A.Y. Ng"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Snow et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2008}, {"title": "Best-arm identification in linear bandits", "author": ["M. Soare", "A. Lazaric", "R. Munos"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Soare et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Soare et al\\.", "year": 2014}, {"title": "Optimal PAC multiple arm identification with applications to crowdsourcing", "author": ["Y. Zhou", "X. Chen", "J. Li"], "venue": "In Proceedings of International Conference on Machine Learning (ICML),", "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 24, "context": "The notion of aggregate regret for multiple-arm identification was first introduced in Zhou et al. [2014] , which is defined as the difference of the averaged expected rewards between the selected set of arms and the best K arms.", "startOffset": 87, "endOffset": 106}, {"referenceID": 24, "context": "The notion of aggregate regret for multiple-arm identification was first introduced in Zhou et al. [2014] , which is defined as the difference of the averaged expected rewards between the selected set of arms and the best K arms. In contrast to Zhou et al. [2014] that only provides instance-independent sample complexity, we introduce a new hardness parameter for characterizing the difficulty of any given instance.", "startOffset": 87, "endOffset": 264}, {"referenceID": 24, "context": "To measure the quality of the selected arms, we adopt the notion of aggregate regret (or regret for short) from Zhou et al. [2014]. In particular, we assume that arms are ordered by their mean \u03b81 \u2265 \u03b82 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b8n so that the set of the best K arms is {1, .", "startOffset": 112, "endOffset": 131}, {"referenceID": 20, "context": "To solve this problem, Zhou et al. [2014] proposed the OptMAI algorithm and established its sample complexity \u0398 ( n 2 ( 1 + ln \u03b4 \u22121 K )) , which is shown to be asymptotically optimal.", "startOffset": 23, "endOffset": 42}, {"referenceID": 20, "context": "To solve this problem, Zhou et al. [2014] proposed the OptMAI algorithm and established its sample complexity \u0398 ( n 2 ( 1 + ln \u03b4 \u22121 K )) , which is shown to be asymptotically optimal. However, the algorithm and the corresponding sample complexity in Zhou et al. [2014] are non-adaptive to the underlying instance.", "startOffset": 23, "endOffset": 269}, {"referenceID": 4, "context": "Chen et al. [2014] developed the CLUCB-PAC algorithm and established an instance-dependent sample complexity for a more general class of problems, including the -top-K arm identification problem as one of the key examples.", "startOffset": 0, "endOffset": 19}, {"referenceID": 4, "context": "Chen et al. [2014] developed the CLUCB-PAC algorithm and established an instance-dependent sample complexity for a more general class of problems, including the -top-K arm identification problem as one of the key examples. When applying the CLUCB-PAC algorithm to identify -top-K arms, the sample complexity becomes O((logH ) +log \u03b4\u22121)H(0, ) where H ) = \u2211n i=1 min{(\u2206i) \u22122, \u22122}, \u2206i = \u03b8i\u2212 \u03b8K+1 for i \u2264 K, \u2206i = \u03b8K \u2212 \u03b8i for i > K. The reason why we adopt the notation H ) will be clear from Section 1.1. However, this bound may be improved for the following two reasons. First, intuitively, the hardness parameter H ) is the total number of necessary pulls needed for each arm to identify whether it is among the top-K arms or the rest so that the algorithm can decide whether to accept or reject the arm (when the arm\u2019s mean is -close to the boundary between the top-K arms and the rest arms, it can be either selected or rejected). However, in many cases, even if an arm\u2019s mean is -far from the boundary, we may still be vague about the comparison between its mean and the boundary, i.e. either selecting or rejecting the arm satisfies the aggregate regret bound. This may lead to fewer number of pulls and a smaller hardness parameter for the same instance. Second, the worst-case sample complexity for CLUCB-PAC becomes O((logn + log \u22121 + log \u03b4\u22121)n \u22122). When \u03b4 is a constant, this bound is logn times more than the best non-adaptive algorithm in Zhou et al. [2014].", "startOffset": 0, "endOffset": 1466}, {"referenceID": 4, "context": "Note that this bound has a similar form as the one in Chen et al. [2014], but as mentioned above, we have an \u221a", "startOffset": 54, "endOffset": 73}, {"referenceID": 24, "context": "1), the worst-case sample complexity of ImprovedTopK matches the best instance-independent shown in Zhou et al. [2014] up to an extra log( \u22121) factor (for constant \u03b4).", "startOffset": 100, "endOffset": 119}, {"referenceID": 2, "context": ", Bubeck et al. [2013]), we first define the gap of the i-th arm", "startOffset": 2, "endOffset": 23}, {"referenceID": 2, "context": ", Bubeck et al. [2013], Karnin et al.", "startOffset": 2, "endOffset": 23}, {"referenceID": 2, "context": ", Bubeck et al. [2013], Karnin et al. [2013]) is H\u0303 , \u2211n i=1 \u2206 \u22122 i .", "startOffset": 2, "endOffset": 45}, {"referenceID": 23, "context": ", linear contextual bandit [Soare et al., 2014], batch arm pulls [Jun et al.", "startOffset": 27, "endOffset": 47}, {"referenceID": 15, "context": ", 2014], batch arm pulls [Jun et al., 2016].", "startOffset": 25, "endOffset": 43}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]).", "startOffset": 37, "endOffset": 463}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]).", "startOffset": 37, "endOffset": 629}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al.", "startOffset": 37, "endOffset": 714}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) .", "startOffset": 37, "endOffset": 774}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122).", "startOffset": 37, "endOffset": 1026}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed.", "startOffset": 37, "endOffset": 1285}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al.", "startOffset": 37, "endOffset": 1657}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings.", "startOffset": 37, "endOffset": 1708}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al.", "startOffset": 37, "endOffset": 1784}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time.", "startOffset": 37, "endOffset": 1845}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.", "startOffset": 37, "endOffset": 1967}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret.", "startOffset": 37, "endOffset": 2284}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper.", "startOffset": 37, "endOffset": 2595}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper. Kalyanakrishnan et al. [2012] proposed the so-called Explore-K metric, which requires for each arm i in the selected set T to satisfy \u03b8i \u2265 \u03b8K \u2212 , where \u03b8K is the mean of the K-th best arm.", "startOffset": 37, "endOffset": 2706}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper. Kalyanakrishnan et al. [2012] proposed the so-called Explore-K metric, which requires for each arm i in the selected set T to satisfy \u03b8i \u2265 \u03b8K \u2212 , where \u03b8K is the mean of the K-th best arm. Cao et al. [2015] proposed a more restrictive notion of optimality\u2014Elementwise- -Optimal, which requires the mean reward of the i-th best arm in the selected set T be at least \u03b8i \u2212 for 1 \u2264 i \u2264 K.", "startOffset": 37, "endOffset": 2883}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper. Kalyanakrishnan et al. [2012] proposed the so-called Explore-K metric, which requires for each arm i in the selected set T to satisfy \u03b8i \u2265 \u03b8K \u2212 , where \u03b8K is the mean of the K-th best arm. Cao et al. [2015] proposed a more restrictive notion of optimality\u2014Elementwise- -Optimal, which requires the mean reward of the i-th best arm in the selected set T be at least \u03b8i \u2212 for 1 \u2264 i \u2264 K. It is clear that the Elementwise- -Optimal is a stronger guarantee than our -top-K in regret, while the latter is stronger than Explore-K. Chen et al. [2016a] further extended Cao et al.", "startOffset": 37, "endOffset": 3220}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper. Kalyanakrishnan et al. [2012] proposed the so-called Explore-K metric, which requires for each arm i in the selected set T to satisfy \u03b8i \u2265 \u03b8K \u2212 , where \u03b8K is the mean of the K-th best arm. Cao et al. [2015] proposed a more restrictive notion of optimality\u2014Elementwise- -Optimal, which requires the mean reward of the i-th best arm in the selected set T be at least \u03b8i \u2212 for 1 \u2264 i \u2264 K. It is clear that the Elementwise- -Optimal is a stronger guarantee than our -top-K in regret, while the latter is stronger than Explore-K. Chen et al. [2016a] further extended Cao et al. [2015] to pure exploration problems under matroid constraints.", "startOffset": 37, "endOffset": 3255}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper. Kalyanakrishnan et al. [2012] proposed the so-called Explore-K metric, which requires for each arm i in the selected set T to satisfy \u03b8i \u2265 \u03b8K \u2212 , where \u03b8K is the mean of the K-th best arm. Cao et al. [2015] proposed a more restrictive notion of optimality\u2014Elementwise- -Optimal, which requires the mean reward of the i-th best arm in the selected set T be at least \u03b8i \u2212 for 1 \u2264 i \u2264 K. It is clear that the Elementwise- -Optimal is a stronger guarantee than our -top-K in regret, while the latter is stronger than Explore-K. Chen et al. [2016a] further extended Cao et al. [2015] to pure exploration problems under matroid constraints. Audibert et al. [2010] and Bubeck et al.", "startOffset": 37, "endOffset": 3334}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper. Kalyanakrishnan et al. [2012] proposed the so-called Explore-K metric, which requires for each arm i in the selected set T to satisfy \u03b8i \u2265 \u03b8K \u2212 , where \u03b8K is the mean of the K-th best arm. Cao et al. [2015] proposed a more restrictive notion of optimality\u2014Elementwise- -Optimal, which requires the mean reward of the i-th best arm in the selected set T be at least \u03b8i \u2212 for 1 \u2264 i \u2264 K. It is clear that the Elementwise- -Optimal is a stronger guarantee than our -top-K in regret, while the latter is stronger than Explore-K. Chen et al. [2016a] further extended Cao et al. [2015] to pure exploration problems under matroid constraints. Audibert et al. [2010] and Bubeck et al. [2013] considered expected aggregate regret (i.", "startOffset": 37, "endOffset": 3359}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper. Kalyanakrishnan et al. [2012] proposed the so-called Explore-K metric, which requires for each arm i in the selected set T to satisfy \u03b8i \u2265 \u03b8K \u2212 , where \u03b8K is the mean of the K-th best arm. Cao et al. [2015] proposed a more restrictive notion of optimality\u2014Elementwise- -Optimal, which requires the mean reward of the i-th best arm in the selected set T be at least \u03b8i \u2212 for 1 \u2264 i \u2264 K. It is clear that the Elementwise- -Optimal is a stronger guarantee than our -top-K in regret, while the latter is stronger than Explore-K. Chen et al. [2016a] further extended Cao et al. [2015] to pure exploration problems under matroid constraints. Audibert et al. [2010] and Bubeck et al. [2013] considered expected aggregate regret (i.e. 1 K (\u2211K i=1 \u03b8i \u2212E (\u2211 i\u2208T \u03b8i )) , where the expectation is taken over the randomness of the algorithm. Note that this notion of expected aggregate regret is a weaker objective than the aggregate regret. Moreover, there are some other recent works studying the problem of best-arm identification in different setups, e.g., linear contextual bandit [Soare et al., 2014], batch arm pulls [Jun et al., 2016]. For our -top-K arm problem, the state-of-the-art instance-dependent sample complexity was given by Chen et al. [2014] (see Section B.", "startOffset": 37, "endOffset": 3924}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper. Kalyanakrishnan et al. [2012] proposed the so-called Explore-K metric, which requires for each arm i in the selected set T to satisfy \u03b8i \u2265 \u03b8K \u2212 , where \u03b8K is the mean of the K-th best arm. Cao et al. [2015] proposed a more restrictive notion of optimality\u2014Elementwise- -Optimal, which requires the mean reward of the i-th best arm in the selected set T be at least \u03b8i \u2212 for 1 \u2264 i \u2264 K. It is clear that the Elementwise- -Optimal is a stronger guarantee than our -top-K in regret, while the latter is stronger than Explore-K. Chen et al. [2016a] further extended Cao et al. [2015] to pure exploration problems under matroid constraints. Audibert et al. [2010] and Bubeck et al. [2013] considered expected aggregate regret (i.e. 1 K (\u2211K i=1 \u03b8i \u2212E (\u2211 i\u2208T \u03b8i )) , where the expectation is taken over the randomness of the algorithm. Note that this notion of expected aggregate regret is a weaker objective than the aggregate regret. Moreover, there are some other recent works studying the problem of best-arm identification in different setups, e.g., linear contextual bandit [Soare et al., 2014], batch arm pulls [Jun et al., 2016]. For our -top-K arm problem, the state-of-the-art instance-dependent sample complexity was given by Chen et al. [2014] (see Section B.2 in Appendix of their paper). More specifically, Chen et al. [2014] proposed CLUCB-PAC algorithms that finds -top-K arms with probability at least (1\u2212 \u03b4) using O (( log \u03b4\u22121 + logH ) ) H ) ) pulls.", "startOffset": 37, "endOffset": 4008}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper. Kalyanakrishnan et al. [2012] proposed the so-called Explore-K metric, which requires for each arm i in the selected set T to satisfy \u03b8i \u2265 \u03b8K \u2212 , where \u03b8K is the mean of the K-th best arm. Cao et al. [2015] proposed a more restrictive notion of optimality\u2014Elementwise- -Optimal, which requires the mean reward of the i-th best arm in the selected set T be at least \u03b8i \u2212 for 1 \u2264 i \u2264 K. It is clear that the Elementwise- -Optimal is a stronger guarantee than our -top-K in regret, while the latter is stronger than Explore-K. Chen et al. [2016a] further extended Cao et al. [2015] to pure exploration problems under matroid constraints. Audibert et al. [2010] and Bubeck et al. [2013] considered expected aggregate regret (i.e. 1 K (\u2211K i=1 \u03b8i \u2212E (\u2211 i\u2208T \u03b8i )) , where the expectation is taken over the randomness of the algorithm. Note that this notion of expected aggregate regret is a weaker objective than the aggregate regret. Moreover, there are some other recent works studying the problem of best-arm identification in different setups, e.g., linear contextual bandit [Soare et al., 2014], batch arm pulls [Jun et al., 2016]. For our -top-K arm problem, the state-of-the-art instance-dependent sample complexity was given by Chen et al. [2014] (see Section B.2 in Appendix of their paper). More specifically, Chen et al. [2014] proposed CLUCB-PAC algorithms that finds -top-K arms with probability at least (1\u2212 \u03b4) using O (( log \u03b4\u22121 + logH ) ) H ) ) pulls. Since we always have H ) \u2265 H ) \u2265 \u03a9(n) and H ) \u2265 (\u03a8 t) \u22122, our Theorem 1 is not worse than the bound in Chen et al. [2014]. Indeed, in many common settings, H ) can be much smaller than H ) so that Theorem 1 (and therefore Theorem 2) requires", "startOffset": 37, "endOffset": 4259}, {"referenceID": 16, "context": ", best-arm identification in Bayesian setup Russo [2016]).", "startOffset": 44, "endOffset": 57}, {"referenceID": 3, "context": "In crowdsourcing applications, Chen et al. [2015] and Abbasi-Yadkori et al.", "startOffset": 31, "endOffset": 50}, {"referenceID": 0, "context": "[2015] and Abbasi-Yadkori et al. [2015] also made this assumption for modeling workers\u2019 accuracy, which correspond to the expected rewards.", "startOffset": 11, "endOffset": 40}, {"referenceID": 4, "context": "The following lemma upper-bounds H ) for O(1)-spread arms, and shows the improvement of our algorithms compared to Chen et al. [2014] on O(1)-spread arms.", "startOffset": 115, "endOffset": 134}, {"referenceID": 2, "context": "Bubeck et al. [2013]. The algorithm goes by rounds for r = 1, 2, 3, .", "startOffset": 0, "endOffset": 21}, {"referenceID": 24, "context": "Lemma 10 [Zhou et al., 2014] For a set of arms S = {\u03b81 \u2265 .", "startOffset": 9, "endOffset": 28}, {"referenceID": 10, "context": "We will need the following anti-concentration result which is an easy consequence of Feller Feller [1943] (cf.", "startOffset": 85, "endOffset": 106}, {"referenceID": 10, "context": "We will need the following anti-concentration result which is an easy consequence of Feller Feller [1943] (cf. Matousek and Vondr\u00e1k. [2008]).", "startOffset": 85, "endOffset": 140}, {"referenceID": 20, "context": "Fact 1 (Matousek and Vondr\u00e1k. [2008]) Let Y be a sum of independent random variables, each attaining values in [0, 1], and let \u03c3 = \u221a Var[Y ] \u2265 200.", "startOffset": 8, "endOffset": 37}, {"referenceID": 18, "context": "We compare our Algorithm 1 (AdaptiveTopK ) with two state-of-the-art methods \u2013 OptMAI in Zhou et al. [2014] and CLUCB-PAC in Chen et al.", "startOffset": 89, "endOffset": 108}, {"referenceID": 3, "context": "[2014] and CLUCB-PAC in Chen et al. [2014]. The comparison between OptMAI /CLUCB-PAC and previous methods (e.", "startOffset": 24, "endOffset": 43}, {"referenceID": 2, "context": ", the methods in Bubeck et al. [2013] and Kalyanakrishnan et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 2, "context": ", the methods in Bubeck et al. [2013] and Kalyanakrishnan et al. [2012]) have already been demonstrated in Zhou et al.", "startOffset": 17, "endOffset": 72}, {"referenceID": 2, "context": ", the methods in Bubeck et al. [2013] and Kalyanakrishnan et al. [2012]) have already been demonstrated in Zhou et al. [2014] and Chen et al.", "startOffset": 17, "endOffset": 126}, {"referenceID": 2, "context": ", the methods in Bubeck et al. [2013] and Kalyanakrishnan et al. [2012]) have already been demonstrated in Zhou et al. [2014] and Chen et al. [2014], and thus omitted here for the", "startOffset": 17, "endOffset": 149}, {"referenceID": 22, "context": "\u2022 Rte: We generate \u03b8 from a real recognizing textual entailment (RTE) dataset Snow et al. [2008]. There are n = 164 workers and we set each \u03b8i be the true labeling accuracy of the i-th worker.", "startOffset": 78, "endOffset": 97}], "year": 2017, "abstractText": "We study the problem of selecting K arms with the highest expected rewards in a stochastic n-armed bandit game. This problem has a wide range of applications, e.g., A/B testing, crowdsourcing, simulation optimization. Our goal is to develop a PAC algorithm, which, with probability at least 1 \u2212 \u03b4, identifies a set of K arms with the aggregate regret at most . The notion of aggregate regret for multiple-arm identification was first introduced in Zhou et al. [2014] , which is defined as the difference of the averaged expected rewards between the selected set of arms and the best K arms. In contrast to Zhou et al. [2014] that only provides instance-independent sample complexity, we introduce a new hardness parameter for characterizing the difficulty of any given instance. We further develop two algorithms and establish the corresponding sample complexity in terms of this hardness parameter. The derived sample complexity can be significantly smaller than state-of-the-art results for a large class of instances and matches the instance-independent lower bound upto a log( \u22121) factor in the worst case. We also prove a lower bound result showing that the extra log( \u22121) is necessary for instance-dependent algorithms using the introduced hardness parameter.", "creator": "LaTeX with hyperref package"}}}