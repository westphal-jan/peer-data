{"id": "1706.01554", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2017", "title": "Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model", "abstract": "We present a novel training framework for neural sequence models, particularly for grounded dialog generation. The standard training paradigm for these models is maximum likelihood estimation (MLE), or minimizing the cross-entropy of the human responses. Across a variety of domains, a recurring problem with MLE trained generative neural dialog models (G) is that they tend to produce 'safe' and generic responses (\"I don't know\", \"I can't tell\"). In contrast, discriminative dialog models (D) that are trained to rank a list of candidate human responses outperform their generative counterparts; in terms of automatic metrics, diversity, and informativeness of the responses. However, D is not useful in practice since it can not be deployed to have real conversations with users.", "histories": [["v1", "Mon, 5 Jun 2017 22:50:37 GMT  (644kb,D)", "http://arxiv.org/abs/1706.01554v1", "11 pages, 3 figures"], ["v2", "Fri, 27 Oct 2017 20:27:07 GMT  (888kb,D)", "http://arxiv.org/abs/1706.01554v2", "11 pages, 3 figures"]], "COMMENTS": "11 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL", "authors": ["jiasen lu", "anitha kannan", "jianwei yang", "devi parikh", "dhruv batra"], "accepted": true, "id": "1706.01554"}, "pdf": {"name": "1706.01554.pdf", "metadata": {"source": "CRF", "title": "Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model", "authors": ["Jiasen Lu", "Anitha Kannan", "Jianwei Yang", "Devi Parikh", "Dhruv Batra"], "emails": ["jiasenlu@vt.edu,", "akannan@fb.com,", "dbatra}@gatech.edu", "recall@10)."], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to survive on their own are not able to survive on their own."}, {"heading": "2 Related Work", "text": "GANs for sequence generation. Generative Adversarial Networks (GANs) [16] have proven to be effective models for a wide range of applications that include continuous variables (e.g. images) c.f [10, 35, 22, 55]. More recently, they have also been used for discrete output ranges such as voice generation."}, {"heading": "3 Preliminaries: Visual Dialog", "text": "We start with the formal description of the visual dialog task introduced by Das et al. [7]. The machine learning task is as follows: A visual dialog model is entered as an image I, caption c describing the image, a dialog history up to turn t \u2212 1, H = (c'H0, (q1, a1) H1,.., (qt \u2212 1, at \u2212 1) Ht \u2212 1) and the following question qt at turn t. The visual dialog agent must return a valid answer to the question. In view of the problem, there are two major classes of methods - generative and discriminatory models. Generative models for visual dialogues are practiced by maximizing the log response sequence: When specifying the coded list of inputs (I, H, qt), these models are encoded both by input (I, qt, an additional candidate) and by some kind of response (1) to an effective list (1)."}, {"heading": "4 Approach: Backprop Through Discriminative Losses for Generative Training", "text": "In this section, we describe our approach to transferring knowledge from a discriminatory visual dialog model (D) to a generative visual dialog model (G). Figure 1 (a) shows the overview of our approach. In view of input image I, dialog storyH, and question qt, the encoder converts the input into a common representation et. Generator G takes et as input and generates a distribution via response sequences over a recursive neural network (specifically an LSTM). For each word in the response sequence, we use a Gumbel Softmax sampler S to scan the response marker from this distribution. The discriminator D in its default form assumes et, f (a), ground N \u2212 1 \"negative\" responses: a \u2212 t, i \u2212 1 i \u2212 1 i = 1 as input and learns an embedding space so that the similarity (et, f (a gt) > similarity (et, f (a, \u00b7 t), \u2212 where we embed the individual response = 1 \u2212 i and \u2212 G) occurs."}, {"heading": "4.1 History-Conditioned Image Attentive Encoder (HCIAE)", "text": "An important feature in dialogues is the use of co-references to avoid repeating units that can be resolved contextually. In fact, a model for correctly answering a question requires a reliable co-reference resolution mechanism. A common approach is to use an encoding architecture with an attention mechanism that implicitly performs co-reference resolution by identifying the part of the dialogue history that can help answer the current question [7, 38, 39, 32], using a holistic representation for the image. Intuitively, one would also expect the answer to be localized to regions in the image and consistent with the story visited. With this motivation, we propose a novel coding architecture (called HCIAE) that is presented in Figure 2. Our encoder first uses the current question to consider the exchange in the story."}, {"heading": "4.2 Discriminator Loss", "text": "The loss function for D must be conducive to knowledge transfer, and in particular it must promote perceptual similarities. Therefore, we use a metric-learning multi-class N pair loss [43] defined as: LD = Ln \u2212 pair ({et, agtt, {a \u2212 t, i} N \u2212 1 = 1}, f) = logistic loss log (1 + N \u2211 i = 1 Exp (e > t f (a \u2212 t, i) \u2212 e > t f (a gt) score margin))) (4), where f is an attention-based LSTM encoder for the answer. This attention may help the discriminator better deal with paraphrases about the answers, but the attention weight is learned by a 1-layer MLP via the LSTM output."}, {"heading": "4.3 Discriminant Perceptual Loss and Knowledge Transfer from D to G", "text": "At a high level, our approach to the transfer of knowledge from D to G is as follows: G repeatedly asks D with answers to a question it generates for an input embedding function to receive feedback and update itself. In each such update, the goal is to update its parameters to try and have a higher score than the correct answer, agtt, under D's learned embedding and scoring function. (5) where f is learned the embedding function learned by the discriminator as (4). (4) Intuitive, updating generator parameters to minimize LG can be interpreted as a learning sequence to produce a response sequence."}, {"heading": "5 Experiments", "text": "We evaluate our proposed approach on the VisDial dataset [7] collected by Das et al. by assembling two subjects on Amazon Mechanical Turk to chat over an image. One person was assigned the role of \"questioner\" and the other the role of \"responder.\" One worker (the questioner) sees only a single line of text describing an image (COCO caption [25]); the image remains hidden from the questioner. Their task is to ask questions about this hidden image in order to \"better imagine the scene.\" The second worker (the responder) sees the image and the caption and answers the questions. The two workers take turns taking questions and answers to questions for 10 rounds. We conduct experiments on VisDial v0.9 (the latest version available), the 83k dialogs on COCO train and 40k on COCO val images that we have superimposed."}, {"heading": "5.1 Results and Analysis", "text": "The question of whether this is a \"conspiracy\" does not arise because it is clear from the outset that it is a \"conspiracy,\" but rather a \"conspiracy,\" in which it is a \"conspiracy\" or \"conspiracy.\""}, {"heading": "5.2 Does updating discriminator help?", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we will be able, that we will be able, that we will be able, and that we will be able, that we will be able, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position."}, {"heading": "5.3 Qualitative Comparison", "text": "In Table 3, we present a few qualitative examples comparing the answers generated by G-MLE and G-DIS. G-MLE produces mostly \"safe\" and less informative answers such as \"yes\" and \"I can't say.\" Our proposed G-DIS model, on the other hand, does so less often and often produces more diverse but more informative answers (see in particular the second example)."}, {"heading": "6 Conclusion", "text": "Generative models for (visual) dialogues are typically trained with an MLE goal, and as a result they tend to build on secure and generic responses. On the other hand, discriminative (or on-demand) models significantly outperform their generative counterparts. However, discriminative models cannot be used as dialogue agents with a real user if conserved answers are not available. In this paper, we propose transferring knowledge from a powerful discriminatory visual dialogue model to a generative model. We also propose a novel visual dialogue that establishes the discrete distribution - specifically, an RNN that is expanded by a sequence of GS samplers coupled with an ST gradient estimator for end-to-end-to-to-differentiability. We also propose a novel visual dialogue that establishes the image attention determined by the history of the dialogue; and use a metric learning loss along with a self-attentive response to learn to put meaningful structures in the discriminator."}], "references": [{"title": "Vqa: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Yoshua Bengio", "Nicholas L\u00e9onard", "Aaron C. Courville"], "venue": "CoRR, abs/1308.3432,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Learning end-to-end goal-oriented dialog", "author": ["Antoine Bordes", "Jason Weston"], "venue": "arXiv preprint arXiv:1605.07683,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Abc-cnn: An attention based convolutional neural network for visual question answering", "author": ["Kan Chen", "Jiang Wang", "Liang-Chieh Chen", "Haoyuan Gao", "Wei Xu", "Ram Nevatia"], "venue": "arXiv preprint arXiv:1511.05960,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Net2net: Accelerating learning via knowledge transfer", "author": ["Tianqi Chen", "Ian Goodfellow", "Jonathon Shlens"], "venue": "arXiv preprint arXiv:1511.05641,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Towards diverse and natural image descriptions via a conditional gan", "author": ["Bo Dai", "Dahua Lin", "Raquel Urtasun", "Sanja Fidler"], "venue": "arXiv preprint arXiv:1703.06029,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Visual Dialog", "author": ["Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "Jos\u00e9 M.F. Moura", "Devi Parikh", "Dhruv Batra"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Learning cooperative visual dialog agents with deep reinforcement learning", "author": ["Abhishek Das", "Satwik Kottur", "Jos\u00e9 MF Moura", "Stefan Lee", "Dhruv Batra"], "venue": "arXiv preprint arXiv:1703.06585,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2017}, {"title": "Guesswhat?! visual object discovery through multi-modal dialogue", "author": ["Harm de Vries", "Florian Strub", "Sarath Chandar", "Olivier Pietquin", "Hugo Larochelle", "Aaron Courville"], "venue": "arXiv preprint arXiv:1611.08481,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Emily L. Denton", "Soumith Chintala", "Arthur Szlam", "Robert Fergus"], "venue": "Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description", "author": ["Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell"], "venue": "In CVPR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Generating images with perceptual similarity metrics based on deep networks", "author": ["Alexey Dosovitskiy", "Thomas Brox"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question answering", "author": ["Haoyuan Gao", "Junhua Mao", "Jie Zhou", "Zhiheng Huang", "Lei Wang", "Wei Xu"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "A neural algorithm of artistic style", "author": ["Leon A Gatys", "Alexander S Ecker", "Matthias Bethge"], "venue": "arXiv preprint arXiv:1508.06576,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Categorical reparameterization with gumbel-softmax", "author": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "venue": "arXiv preprint arXiv:1611.01144,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Perceptual losses for real-time style transfer and super-resolution", "author": ["Justin Johnson", "Alexandre Alahi", "Li Fei-Fei"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Gans for sequences of discrete elements with the gumbel-softmax distribution", "author": ["Matt J. Kusner", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Photo-realistic single image super-resolution using a generative adversarial network", "author": ["Christian Ledig", "Lucas Theis", "Ferenc Huszar", "Jose Caballero", "Andrew P. Aitken", "Alykhan Tejani", "Johannes Totz", "Zehan Wang", "Wenzhe Shi"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Adversarial learning for neural dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Tianlin Shi", "Alan Ritter", "Dan Jurafsky"], "venue": "arXiv preprint arXiv:1701.06547,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2017}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "In ACL 2004 Workshop,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau"], "venue": "arXiv preprint arXiv:1603.08023,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Optimization of image description metrics using policy gradient methods", "author": ["Siqi Liu", "Zhenhai Zhu", "Ning Ye", "Sergio Guadarrama", "Kevin Murphy"], "venue": "arXiv preprint arXiv:1612.00370,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Knowing when to look: Adaptive attention via A visual sentinel for image captioning", "author": ["Jiasen Lu", "Caiming Xiong", "Devi Parikh", "Richard Socher"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "The concrete distribution: A continuous relaxation of discrete random variables", "author": ["Chris J Maddison", "Andriy Mnih", "Yee Whye Teh"], "venue": "arXiv preprint arXiv:1611.00712,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["Mateusz Malinowski", "Marcus Rohrbach", "Mario Fritz"], "venue": "In ICCV,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Coherent dialogue with attention-based language models", "author": ["Hongyuan Mei", "Mohit Bansal", "Matthew R Walter"], "venue": "arXiv preprint arXiv:1611.06997,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Image-grounded conversations: Multimodal context for natural question and response generation", "author": ["Nasrin Mostafazadeh", "Chris Brockett", "Bill Dolan", "Michel Galley", "Jianfeng Gao", "Georgios P Spithourakis", "Lucy Vanderwende"], "venue": "arXiv preprint arXiv:1701.08251,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2017}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In ACL,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2002}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "CoRR, abs/1511.06434,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1511.06732,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Exploring models and data for image question answering", "author": ["Mengye Ren", "Ryan Kiros", "Richard Zemel"], "venue": "In NIPS,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "arXiv preprint arXiv:1507.04808,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1605.06069,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "venue": "In Thirty-First AAAI Conference on Artificial Intelligence,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2017}, {"title": "Speaking the same language: Matching machine to human captions by adversarial training", "author": ["Rakshith Shetty", "Marcus Rohrbach", "Lisa Anne Hendricks", "Mario Fritz", "Bernt Schiele"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2017}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Improved deep metric learning with multi-class n-pair loss objective", "author": ["Kihyuk Sohn"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian- Yun Nie", "Jianfeng Gao", "Bill Dolan"], "venue": "arXiv preprint arXiv:1506.06714,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "End-to-end optimization of goal-driven and visually grounded dialogue systems", "author": ["Florian Strub", "Harm de Vries", "Jeremie Mary", "Bilal Piot", "Aaron Courville", "Olivier Pietquin"], "venue": "arXiv preprint arXiv:1703.05423,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2017}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2014}, {"title": "Cider: Consensus-based image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In CVPR,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2015}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["Huijuan Xu", "Kate Saenko"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": "CoRR, abs/1502.03044,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2016}, {"title": "Seqgan: Sequence generative adversarial nets with policy gradient", "author": ["Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu"], "venue": "AAAI Conference on Artificial Intelligence,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2017}, {"title": "Energy-based generative adversarial", "author": ["Junbo Jake Zhao", "Micha\u00ebl Mathieu", "Yann LeCun"], "venue": "network. CoRR,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2016}, {"title": "Unpaired image-to-image translation using cycle-consistent adversarial networks", "author": ["Jun-Yan Zhu", "Taesung Park", "Phillip Isola", "Alexei A Efros"], "venue": "arXiv preprint arXiv:1703.10593,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2017}], "referenceMentions": [{"referenceID": 45, "context": "[47, 44, 46]) have emerged as the dominant paradigm across a variety of setting and datasets \u2013 from text-only dialog [44, 40, 23, 3] to more recently, visual dialog [7, 9, 8, 33, 45], where an agent must answer a sequence of questions grounded in an image, requiring it to reason about both visual content and the dialog history.", "startOffset": 0, "endOffset": 12}, {"referenceID": 42, "context": "[47, 44, 46]) have emerged as the dominant paradigm across a variety of setting and datasets \u2013 from text-only dialog [44, 40, 23, 3] to more recently, visual dialog [7, 9, 8, 33, 45], where an agent must answer a sequence of questions grounded in an image, requiring it to reason about both visual content and the dialog history.", "startOffset": 0, "endOffset": 12}, {"referenceID": 44, "context": "[47, 44, 46]) have emerged as the dominant paradigm across a variety of setting and datasets \u2013 from text-only dialog [44, 40, 23, 3] to more recently, visual dialog [7, 9, 8, 33, 45], where an agent must answer a sequence of questions grounded in an image, requiring it to reason about both visual content and the dialog history.", "startOffset": 0, "endOffset": 12}, {"referenceID": 42, "context": "[47, 44, 46]) have emerged as the dominant paradigm across a variety of setting and datasets \u2013 from text-only dialog [44, 40, 23, 3] to more recently, visual dialog [7, 9, 8, 33, 45], where an agent must answer a sequence of questions grounded in an image, requiring it to reason about both visual content and the dialog history.", "startOffset": 117, "endOffset": 132}, {"referenceID": 38, "context": "[47, 44, 46]) have emerged as the dominant paradigm across a variety of setting and datasets \u2013 from text-only dialog [44, 40, 23, 3] to more recently, visual dialog [7, 9, 8, 33, 45], where an agent must answer a sequence of questions grounded in an image, requiring it to reason about both visual content and the dialog history.", "startOffset": 117, "endOffset": 132}, {"referenceID": 21, "context": "[47, 44, 46]) have emerged as the dominant paradigm across a variety of setting and datasets \u2013 from text-only dialog [44, 40, 23, 3] to more recently, visual dialog [7, 9, 8, 33, 45], where an agent must answer a sequence of questions grounded in an image, requiring it to reason about both visual content and the dialog history.", "startOffset": 117, "endOffset": 132}, {"referenceID": 2, "context": "[47, 44, 46]) have emerged as the dominant paradigm across a variety of setting and datasets \u2013 from text-only dialog [44, 40, 23, 3] to more recently, visual dialog [7, 9, 8, 33, 45], where an agent must answer a sequence of questions grounded in an image, requiring it to reason about both visual content and the dialog history.", "startOffset": 117, "endOffset": 132}, {"referenceID": 6, "context": "[47, 44, 46]) have emerged as the dominant paradigm across a variety of setting and datasets \u2013 from text-only dialog [44, 40, 23, 3] to more recently, visual dialog [7, 9, 8, 33, 45], where an agent must answer a sequence of questions grounded in an image, requiring it to reason about both visual content and the dialog history.", "startOffset": 165, "endOffset": 182}, {"referenceID": 8, "context": "[47, 44, 46]) have emerged as the dominant paradigm across a variety of setting and datasets \u2013 from text-only dialog [44, 40, 23, 3] to more recently, visual dialog [7, 9, 8, 33, 45], where an agent must answer a sequence of questions grounded in an image, requiring it to reason about both visual content and the dialog history.", "startOffset": 165, "endOffset": 182}, {"referenceID": 7, "context": "[47, 44, 46]) have emerged as the dominant paradigm across a variety of setting and datasets \u2013 from text-only dialog [44, 40, 23, 3] to more recently, visual dialog [7, 9, 8, 33, 45], where an agent must answer a sequence of questions grounded in an image, requiring it to reason about both visual content and the dialog history.", "startOffset": 165, "endOffset": 182}, {"referenceID": 31, "context": "[47, 44, 46]) have emerged as the dominant paradigm across a variety of setting and datasets \u2013 from text-only dialog [44, 40, 23, 3] to more recently, visual dialog [7, 9, 8, 33, 45], where an agent must answer a sequence of questions grounded in an image, requiring it to reason about both visual content and the dialog history.", "startOffset": 165, "endOffset": 182}, {"referenceID": 43, "context": "[47, 44, 46]) have emerged as the dominant paradigm across a variety of setting and datasets \u2013 from text-only dialog [44, 40, 23, 3] to more recently, visual dialog [7, 9, 8, 33, 45], where an agent must answer a sequence of questions grounded in an image, requiring it to reason about both visual content and the dialog history.", "startOffset": 165, "endOffset": 182}, {"referenceID": 21, "context": "Across a variety of domains, a recurring problem with MLE trained neural dialog models is that they tend to produce \u2018safe\u2019 generic responses, such as \u2018Not sure\u2019 or \u2018I don\u2019t know\u2019 in text-only dialog [23], and \u2018I can\u2019t see\u2019 or \u2018I can\u2019t tell\u2019 in visual dialog [7, 8].", "startOffset": 199, "endOffset": 203}, {"referenceID": 6, "context": "Across a variety of domains, a recurring problem with MLE trained neural dialog models is that they tend to produce \u2018safe\u2019 generic responses, such as \u2018Not sure\u2019 or \u2018I don\u2019t know\u2019 in text-only dialog [23], and \u2018I can\u2019t see\u2019 or \u2018I can\u2019t tell\u2019 in visual dialog [7, 8].", "startOffset": 258, "endOffset": 264}, {"referenceID": 7, "context": "Across a variety of domains, a recurring problem with MLE trained neural dialog models is that they tend to produce \u2018safe\u2019 generic responses, such as \u2018Not sure\u2019 or \u2018I don\u2019t know\u2019 in text-only dialog [23], and \u2018I can\u2019t see\u2019 or \u2018I can\u2019t tell\u2019 in visual dialog [7, 8].", "startOffset": 258, "endOffset": 264}, {"referenceID": 34, "context": "One promising alternative to MLE training proposed by recent work [36, 27] is sequence-level training of neural sequence models, specifically, using reinforcement learning to optimize taskspecific sequence metrics such as BLEU [34], ROUGE [24], CIDEr [48].", "startOffset": 66, "endOffset": 74}, {"referenceID": 25, "context": "One promising alternative to MLE training proposed by recent work [36, 27] is sequence-level training of neural sequence models, specifically, using reinforcement learning to optimize taskspecific sequence metrics such as BLEU [34], ROUGE [24], CIDEr [48].", "startOffset": 66, "endOffset": 74}, {"referenceID": 32, "context": "One promising alternative to MLE training proposed by recent work [36, 27] is sequence-level training of neural sequence models, specifically, using reinforcement learning to optimize taskspecific sequence metrics such as BLEU [34], ROUGE [24], CIDEr [48].", "startOffset": 227, "endOffset": 231}, {"referenceID": 22, "context": "One promising alternative to MLE training proposed by recent work [36, 27] is sequence-level training of neural sequence models, specifically, using reinforcement learning to optimize taskspecific sequence metrics such as BLEU [34], ROUGE [24], CIDEr [48].", "startOffset": 239, "endOffset": 243}, {"referenceID": 46, "context": "One promising alternative to MLE training proposed by recent work [36, 27] is sequence-level training of neural sequence models, specifically, using reinforcement learning to optimize taskspecific sequence metrics such as BLEU [34], ROUGE [24], CIDEr [48].", "startOffset": 251, "endOffset": 255}, {"referenceID": 24, "context": "Unfortunately, in the case of dialog, all existing automatic metrics correlate poorly with human judgment [26], which renders this alternative infeasible for dialog models.", "startOffset": 106, "endOffset": 110}, {"referenceID": 14, "context": "In this paper, inspired by the success of adversarial training [16], we propose to train a generative visual dialog model (G) to produce sequences that score highly under a discriminative visual dialog model (D).", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "In this process, D learns a task-dependent perceptual similarity [12, 19, 15] and learns to recognize multiple correct responses in the feature space.", "startOffset": 65, "endOffset": 77}, {"referenceID": 17, "context": "In this process, D learns a task-dependent perceptual similarity [12, 19, 15] and learns to recognize multiple correct responses in the feature space.", "startOffset": 65, "endOffset": 77}, {"referenceID": 13, "context": "In this process, D learns a task-dependent perceptual similarity [12, 19, 15] and learns to recognize multiple correct responses in the feature space.", "startOffset": 65, "endOffset": 77}, {"referenceID": 15, "context": "In that sense, our proposed approach may be viewed as an instance of \u2018knowledge transfer\u2019 [17, 5] from D to G.", "startOffset": 90, "endOffset": 97}, {"referenceID": 4, "context": "In that sense, our proposed approach may be viewed as an instance of \u2018knowledge transfer\u2019 [17, 5] from D to G.", "startOffset": 90, "endOffset": 97}, {"referenceID": 16, "context": "We propose to leverage the recently proposed Gumbel-Softmax (GS) approximation to the discrete distribution [18, 30] \u2013 specifically, a Recurrent Neural Network (RNN) augmented with a sequence of GS samplers, which when coupled with the straight-through gradient estimator [2, 18] enables end-to-end differentiability.", "startOffset": 108, "endOffset": 116}, {"referenceID": 28, "context": "We propose to leverage the recently proposed Gumbel-Softmax (GS) approximation to the discrete distribution [18, 30] \u2013 specifically, a Recurrent Neural Network (RNN) augmented with a sequence of GS samplers, which when coupled with the straight-through gradient estimator [2, 18] enables end-to-end differentiability.", "startOffset": 108, "endOffset": 116}, {"referenceID": 1, "context": "We propose to leverage the recently proposed Gumbel-Softmax (GS) approximation to the discrete distribution [18, 30] \u2013 specifically, a Recurrent Neural Network (RNN) augmented with a sequence of GS samplers, which when coupled with the straight-through gradient estimator [2, 18] enables end-to-end differentiability.", "startOffset": 272, "endOffset": 279}, {"referenceID": 16, "context": "We propose to leverage the recently proposed Gumbel-Softmax (GS) approximation to the discrete distribution [18, 30] \u2013 specifically, a Recurrent Neural Network (RNN) augmented with a sequence of GS samplers, which when coupled with the straight-through gradient estimator [2, 18] enables end-to-end differentiability.", "startOffset": 272, "endOffset": 279}, {"referenceID": 6, "context": "7% on recall@5 on the VisDial dataset, essentially improving over state-of-the-art [7] by 2.", "startOffset": 83, "endOffset": 86}, {"referenceID": 14, "context": "Generative Adversarial Networks (GANs) [16] have shown to be effective models for a wide range of applications involving continuous variables (e.", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "f [10, 35, 22, 55].", "startOffset": 2, "endOffset": 18}, {"referenceID": 33, "context": "f [10, 35, 22, 55].", "startOffset": 2, "endOffset": 18}, {"referenceID": 20, "context": "f [10, 35, 22, 55].", "startOffset": 2, "endOffset": 18}, {"referenceID": 53, "context": "f [10, 35, 22, 55].", "startOffset": 2, "endOffset": 18}, {"referenceID": 5, "context": "image captioning [6, 41], dialog generation [23], or text generation [53] \u2013 by either viewing the generative model as a stochastic parametrized policy that is updated using REINFORCE with the discriminator providing the reward [53, 6, 41, 23], or (closer to our approach) through continuous relaxation of discrete variables through Gumbel-Softmax to enable backpropagating the response from the discriminator [21, 41].", "startOffset": 17, "endOffset": 24}, {"referenceID": 39, "context": "image captioning [6, 41], dialog generation [23], or text generation [53] \u2013 by either viewing the generative model as a stochastic parametrized policy that is updated using REINFORCE with the discriminator providing the reward [53, 6, 41, 23], or (closer to our approach) through continuous relaxation of discrete variables through Gumbel-Softmax to enable backpropagating the response from the discriminator [21, 41].", "startOffset": 17, "endOffset": 24}, {"referenceID": 21, "context": "image captioning [6, 41], dialog generation [23], or text generation [53] \u2013 by either viewing the generative model as a stochastic parametrized policy that is updated using REINFORCE with the discriminator providing the reward [53, 6, 41, 23], or (closer to our approach) through continuous relaxation of discrete variables through Gumbel-Softmax to enable backpropagating the response from the discriminator [21, 41].", "startOffset": 44, "endOffset": 48}, {"referenceID": 51, "context": "image captioning [6, 41], dialog generation [23], or text generation [53] \u2013 by either viewing the generative model as a stochastic parametrized policy that is updated using REINFORCE with the discriminator providing the reward [53, 6, 41, 23], or (closer to our approach) through continuous relaxation of discrete variables through Gumbel-Softmax to enable backpropagating the response from the discriminator [21, 41].", "startOffset": 69, "endOffset": 73}, {"referenceID": 51, "context": "image captioning [6, 41], dialog generation [23], or text generation [53] \u2013 by either viewing the generative model as a stochastic parametrized policy that is updated using REINFORCE with the discriminator providing the reward [53, 6, 41, 23], or (closer to our approach) through continuous relaxation of discrete variables through Gumbel-Softmax to enable backpropagating the response from the discriminator [21, 41].", "startOffset": 227, "endOffset": 242}, {"referenceID": 5, "context": "image captioning [6, 41], dialog generation [23], or text generation [53] \u2013 by either viewing the generative model as a stochastic parametrized policy that is updated using REINFORCE with the discriminator providing the reward [53, 6, 41, 23], or (closer to our approach) through continuous relaxation of discrete variables through Gumbel-Softmax to enable backpropagating the response from the discriminator [21, 41].", "startOffset": 227, "endOffset": 242}, {"referenceID": 39, "context": "image captioning [6, 41], dialog generation [23], or text generation [53] \u2013 by either viewing the generative model as a stochastic parametrized policy that is updated using REINFORCE with the discriminator providing the reward [53, 6, 41, 23], or (closer to our approach) through continuous relaxation of discrete variables through Gumbel-Softmax to enable backpropagating the response from the discriminator [21, 41].", "startOffset": 227, "endOffset": 242}, {"referenceID": 21, "context": "image captioning [6, 41], dialog generation [23], or text generation [53] \u2013 by either viewing the generative model as a stochastic parametrized policy that is updated using REINFORCE with the discriminator providing the reward [53, 6, 41, 23], or (closer to our approach) through continuous relaxation of discrete variables through Gumbel-Softmax to enable backpropagating the response from the discriminator [21, 41].", "startOffset": 227, "endOffset": 242}, {"referenceID": 19, "context": "image captioning [6, 41], dialog generation [23], or text generation [53] \u2013 by either viewing the generative model as a stochastic parametrized policy that is updated using REINFORCE with the discriminator providing the reward [53, 6, 41, 23], or (closer to our approach) through continuous relaxation of discrete variables through Gumbel-Softmax to enable backpropagating the response from the discriminator [21, 41].", "startOffset": 409, "endOffset": 417}, {"referenceID": 39, "context": "image captioning [6, 41], dialog generation [23], or text generation [53] \u2013 by either viewing the generative model as a stochastic parametrized policy that is updated using REINFORCE with the discriminator providing the reward [53, 6, 41, 23], or (closer to our approach) through continuous relaxation of discrete variables through Gumbel-Softmax to enable backpropagating the response from the discriminator [21, 41].", "startOffset": 409, "endOffset": 417}, {"referenceID": 52, "context": "One can also draw connections between our work and Energy Based GAN (EBGAN) [54] \u2013 without the adversarial training aspect.", "startOffset": 76, "endOffset": 80}, {"referenceID": 10, "context": ", image captioning [11, 13, 20, 49], visual question answering [1, 14, 31, 37], visual dialog [7, 9, 8, 45, 33] \u2013 typically involve attention mechanisms.", "startOffset": 19, "endOffset": 35}, {"referenceID": 18, "context": ", image captioning [11, 13, 20, 49], visual question answering [1, 14, 31, 37], visual dialog [7, 9, 8, 45, 33] \u2013 typically involve attention mechanisms.", "startOffset": 19, "endOffset": 35}, {"referenceID": 47, "context": ", image captioning [11, 13, 20, 49], visual question answering [1, 14, 31, 37], visual dialog [7, 9, 8, 45, 33] \u2013 typically involve attention mechanisms.", "startOffset": 19, "endOffset": 35}, {"referenceID": 0, "context": ", image captioning [11, 13, 20, 49], visual question answering [1, 14, 31, 37], visual dialog [7, 9, 8, 45, 33] \u2013 typically involve attention mechanisms.", "startOffset": 63, "endOffset": 78}, {"referenceID": 12, "context": ", image captioning [11, 13, 20, 49], visual question answering [1, 14, 31, 37], visual dialog [7, 9, 8, 45, 33] \u2013 typically involve attention mechanisms.", "startOffset": 63, "endOffset": 78}, {"referenceID": 29, "context": ", image captioning [11, 13, 20, 49], visual question answering [1, 14, 31, 37], visual dialog [7, 9, 8, 45, 33] \u2013 typically involve attention mechanisms.", "startOffset": 63, "endOffset": 78}, {"referenceID": 35, "context": ", image captioning [11, 13, 20, 49], visual question answering [1, 14, 31, 37], visual dialog [7, 9, 8, 45, 33] \u2013 typically involve attention mechanisms.", "startOffset": 63, "endOffset": 78}, {"referenceID": 6, "context": ", image captioning [11, 13, 20, 49], visual question answering [1, 14, 31, 37], visual dialog [7, 9, 8, 45, 33] \u2013 typically involve attention mechanisms.", "startOffset": 94, "endOffset": 111}, {"referenceID": 8, "context": ", image captioning [11, 13, 20, 49], visual question answering [1, 14, 31, 37], visual dialog [7, 9, 8, 45, 33] \u2013 typically involve attention mechanisms.", "startOffset": 94, "endOffset": 111}, {"referenceID": 7, "context": ", image captioning [11, 13, 20, 49], visual question answering [1, 14, 31, 37], visual dialog [7, 9, 8, 45, 33] \u2013 typically involve attention mechanisms.", "startOffset": 94, "endOffset": 111}, {"referenceID": 43, "context": ", image captioning [11, 13, 20, 49], visual question answering [1, 14, 31, 37], visual dialog [7, 9, 8, 45, 33] \u2013 typically involve attention mechanisms.", "startOffset": 94, "endOffset": 111}, {"referenceID": 31, "context": ", image captioning [11, 13, 20, 49], visual question answering [1, 14, 31, 37], visual dialog [7, 9, 8, 45, 33] \u2013 typically involve attention mechanisms.", "startOffset": 94, "endOffset": 111}, {"referenceID": 47, "context": "For image captioning, this may be attending to relevant regions in the image [49, 51, 28].", "startOffset": 77, "endOffset": 89}, {"referenceID": 49, "context": "For image captioning, this may be attending to relevant regions in the image [49, 51, 28].", "startOffset": 77, "endOffset": 89}, {"referenceID": 26, "context": "For image captioning, this may be attending to relevant regions in the image [49, 51, 28].", "startOffset": 77, "endOffset": 89}, {"referenceID": 3, "context": "For VQA, this may be attending to relevant image regions alone [4, 50, 52] or co-attending to image regions and question words/phrases [29].", "startOffset": 63, "endOffset": 74}, {"referenceID": 48, "context": "For VQA, this may be attending to relevant image regions alone [4, 50, 52] or co-attending to image regions and question words/phrases [29].", "startOffset": 63, "endOffset": 74}, {"referenceID": 50, "context": "For VQA, this may be attending to relevant image regions alone [4, 50, 52] or co-attending to image regions and question words/phrases [29].", "startOffset": 63, "endOffset": 74}, {"referenceID": 27, "context": "For VQA, this may be attending to relevant image regions alone [4, 50, 52] or co-attending to image regions and question words/phrases [29].", "startOffset": 135, "endOffset": 139}, {"referenceID": 6, "context": "In the context of visual dialog, [7] uses attention to identify utterances in the dialog history that may be useful for answering the current question.", "startOffset": 33, "endOffset": 36}, {"referenceID": 6, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "In fact, in the VisDial dataset [7] nearly all (98%) dialogs involve at least one pronoun.", "startOffset": 32, "endOffset": 35}, {"referenceID": 6, "context": "A common approach is to use an encoder architecture with an attention mechanism that implicitly performs co-reference resolution by identifying the portion of the dialog history that can help in answering the current question [7, 38, 39, 32].", "startOffset": 226, "endOffset": 241}, {"referenceID": 36, "context": "A common approach is to use an encoder architecture with an attention mechanism that implicitly performs co-reference resolution by identifying the portion of the dialog history that can help in answering the current question [7, 38, 39, 32].", "startOffset": 226, "endOffset": 241}, {"referenceID": 37, "context": "A common approach is to use an encoder architecture with an attention mechanism that implicitly performs co-reference resolution by identifying the portion of the dialog history that can help in answering the current question [7, 38, 39, 32].", "startOffset": 226, "endOffset": 241}, {"referenceID": 30, "context": "A common approach is to use an encoder architecture with an attention mechanism that implicitly performs co-reference resolution by identifying the portion of the dialog history that can help in answering the current question [7, 38, 39, 32].", "startOffset": 226, "endOffset": 241}, {"referenceID": 41, "context": "Therefore, we use a metric-learning multi-class N-pair loss [43] defined as:", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "To overcome this, we leverage the recently introduced continuous relaxation of the categorical distribution \u2013 the Gumbel-softmax distribution or the Concrete distribution [18, 30].", "startOffset": 171, "endOffset": 179}, {"referenceID": 28, "context": "To overcome this, we leverage the recently introduced continuous relaxation of the categorical distribution \u2013 the Gumbel-softmax distribution or the Concrete distribution [18, 30].", "startOffset": 171, "endOffset": 179}, {"referenceID": 1, "context": "When coupled with the straight-through gradient estimator [2, 18] this enables end-to-end differentiability.", "startOffset": 58, "endOffset": 65}, {"referenceID": 16, "context": "When coupled with the straight-through gradient estimator [2, 18] this enables end-to-end differentiability.", "startOffset": 58, "endOffset": 65}, {"referenceID": 6, "context": "We evaluate our proposed approach on the VisDial dataset [7], which was collected by Das et al.", "startOffset": 57, "endOffset": 60}, {"referenceID": 23, "context": "One worker (the questioner) sees only a single line of text describing an image (caption from COCO [25]); the image remains hidden to the questioner.", "startOffset": 99, "endOffset": 103}, {"referenceID": 6, "context": "We split the 83k into 82k for train, 1k for val, and use the 40k as test, in a manner consistent with [7].", "startOffset": 102, "endOffset": 105}, {"referenceID": 6, "context": "Following the evaluation protocol established in [7], we use a retrieval setting to evaluate the responses at each round in the dialog.", "startOffset": 49, "endOffset": 52}, {"referenceID": 40, "context": "We use VGG-19 [42] to get the representation of image.", "startOffset": 14, "endOffset": 18}, {"referenceID": 41, "context": "Following [43], we regularize the L norm of the embedding vectors to be small.", "startOffset": 10, "endOffset": 14}, {"referenceID": 6, "context": "We compare our proposed techniques to the current state-of-art generative and discriminative models developed in [7].", "startOffset": 113, "endOffset": 116}, {"referenceID": 6, "context": "Specifically, [7] introduced 3 encoding architectures \u2013 Late Fusion (LF), Hierarchical Recurrent Encoder (HRE), Memory Network (MN) \u2013 each trained with a generative (-G) and discriminative (-D) decoder.", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "Comparing this variant to the generative baselines from [7] establishes the improvement due to our encoder (HCIAE).", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "Comparing this variant to the discriminative baselines from [7] establishes the improvement due to our encoder (HCIAE) in the discriminative setting.", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": "LF-G [7] 0.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "07 HREA-G [7] 0.", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "79 MN-G [7] 0.", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "LF-D [7] 0.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "78 HREA-D [7] 0.", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "66 MN-D [7] 0.", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "self-attentive answer encoding: In the purely discriminative setting, our final discriminative model (HCIAE-D-NP-ATT) also beats the performance of the corresponding state-of-art models [7] by 2.", "startOffset": 186, "endOffset": 189}], "year": 2017, "abstractText": "We present a novel training framework for neural sequence models, particularly for grounded dialog generation. The standard training paradigm for these models is maximum likelihood estimation (MLE), or minimizing the cross-entropy of the human responses. Across a variety of domains, a recurring problem with MLE trained generative neural dialog models (G) is that they tend to produce \u2018safe\u2019 and generic responses (\u2018I don\u2019t know\u2019, \u2018I can\u2019t tell\u2019). In contrast, discriminative dialog models (D) that are trained to rank a list of candidate human responses outperform their generative counterparts; in terms of automatic metrics, diversity, and informativeness of the responses. However, D is not useful in practice since it can not be deployed to have real conversations with users. Our work aims to achieve the best of both worlds \u2013 the practical usefulness of G and the strong performance of D \u2013 via knowledge transfer from D to G. Our primary contribution is an end-to-end trainable generative visual dialog model, where G receives gradients from D as a perceptual (not adversarial) loss of the sequence sampled from G. We leverage the recently proposed Gumbel-Softmax (GS) approximation to the discrete distribution \u2013 specifically, a RNN augmented with a sequence of GS samplers, coupled with the straight-through gradient estimator to enable end-to-end differentiability. We also introduce a stronger encoder for visual dialog, and employ a self-attention mechanism for answer encoding along with a metric learning loss to aid D in better capturing semantic similarities in answer responses. Overall, our proposed model outperforms state-of-the-art on the VisDial dataset by a significant margin (2.67% on recall@10).", "creator": "LaTeX with hyperref package"}}}