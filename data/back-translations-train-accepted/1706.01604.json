{"id": "1706.01604", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2017", "title": "Hyperplane Clustering via Dual Principal Component Pursuit", "abstract": "We extend the theoretical analysis of a recently proposed single subspace learning algorithm, called Dual Principal Component Pursuit (DPCP), to the case where the data are drawn from of a union of hyperplanes. To gain insight into the properties of the $\\ell_1$ non-convex problem associated with DPCP, we develop a geometric analysis of a closely related continuous optimization problem. Then transferring this analysis to the discrete problem, our results state that as long as the hyperplanes are sufficiently separated, the dominant hyperplane is sufficiently dominant and the points are uniformly distributed inside the associated hyperplanes, then the non-convex DPCP problem has a unique global solution, equal to the normal vector of the dominant hyperplane. This suggests the correctness of a sequential hyperplane learning algorithm based on DPCP. A thorough experimental evaluation reveals that hyperplane learning schemes based on DPCP dramatically improve over the state-of-the-art methods for the case of synthetic data, while are competitive to the state-of-the-art in the case of 3D plane clustering for Kinect data.", "histories": [["v1", "Tue, 6 Jun 2017 04:27:24 GMT  (4582kb,D)", "http://arxiv.org/abs/1706.01604v1", null], ["v2", "Mon, 19 Jun 2017 19:20:23 GMT  (4582kb,D)", "http://arxiv.org/abs/1706.01604v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG stat.ML", "authors": ["manolis c tsakiris", "ren\u00e9 vidal"], "accepted": true, "id": "1706.01604"}, "pdf": {"name": "1706.01604.pdf", "metadata": {"source": "CRF", "title": "Hyperplane Clustering Via Dual Principal Component Pursuit", "authors": ["Manolis C. Tsakiris", "Ren\u00e9 Vidal"], "emails": ["M.TSAKIRIS@JHU.EDU", "RVIDAL@JHU.EDU"], "sections": [{"heading": null, "text": "When this assumption is violated, as in the case of hyperplanes, existing methods are either computationally too intensive (e.g. algebraic methods) or lack of theoretical support (e.g. K-hyperplanes or RANSAC).The main theoretical contribution of this paper is to extend the theoretical analysis of a recently proposed single subspace learning algorithm called Dual Principal Component Pursuit (DPCP) to the case in which the data is taken from a combination of hyperplanes. In order to gain insight into the expected properties of the non-convex '1 problem associated with DPCP (Discrete Problem), we develop a geometric analysis of a closely related PC continuous optimization problem. Then, this analysis is applied to the discrete problem that Hyperformative P, as long as the hyperformational threshold level is associated with DPCP (Discrete Problem), is applied to hyperformational sector and hyperformational sector points universed (x)."}, {"heading": "1. Introduction", "text": "Over the past fifteen years, the model of a unification of linear subspaces, also called subspace arrangement (Derksen, 2007), has gained popularity in pattern recognition and computer vision (Vidal, 2011), often replacing the classic model of a single linear subspace associated with the known Principal Component Analysis (PCA) (Hotelling, 1933; Pearson, 1901; Jolliffe, 2002), which has led to a variety of algorithms attempting to collect a collection of data from a subspace arrangement, leading to the challenging field of subspace clustering (Vidal, 2011). Such techniques can be iterative (Bradley and Mangasarian, 2000; Tseng, 2000; ar Xiv: 170 6.01 604v 1 [cs.C V] 6J un2 01Zhang et al., 2009), statistical (Tipping and Bishop, 1999b; Gruber and Wei\u00df, 2004), informational (Ma)."}, {"heading": "2. Prior Art", "text": "Suppose a single hyperplaneHi at a given time becomes specific between the RANdom Sampling Consensus (Fischler and Bolles, 1981), which attempts to identify a single hyperplaneHi. Suppose a single hyperplaneHi at a given time becomes specific between the RANdom Sampling Consensus algorithm (Fischler and Bolles, 1981)."}, {"heading": "3. Theoretical Contributions", "text": "In this section, we will develop the main theoretical contributions of this work, which deal with the properties of the non-convex minimization problem (9) and the recursion of linear programs (11) in the context of hyperplane clustering. Specifically, we are particularly interested in the development of conditions under which each global minimizer of the non-convex problem (9) is the normal vector to one of the hyperplanes of the underlying hyper-plane arrangement. To this end, it is instructive to examine a related ongoing problem, which is achieved by replacing each finite cluster within a hyperplane with the uniform measure on the unity sphere of the hyperplane (\u00a7 3.2). The main result in this direction is Theorem 4. Next, we will introduce certain uniformity parameters that measure the deviation of discrete quantities from their continuous counterparts, and adjust our analysis to the discrete case of interest (\u00a7 3.1)."}, {"heading": "3.1 Data model and the problem of hyperplane clustering", "text": "(Consider a Collection X = [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1], [x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1"}, {"heading": "3.2 Theoretical analysis of the continuous problem", "text": "As it turns out, certain important insights regarding the problem (9) regarding the formation of hyper-level clusters can be gained by investigating an associated ongoing problem (= >). (>) To see what this problem is, one has to bear in mind that the LHS of (12) exactly 1Ni-Ni-Ni-Ni-Ni-Ni-Ni-Ni-Ni-Ni-Ni-Ni-Ni-Ni-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X"}, {"heading": "3.3 Theoretical analysis of the discrete problem", "text": "We now turn our attention to the discrete problem of hyperplane clustering via DPCP, i.e. problems (9) and (11), in the case where X = [1,.., X \u2212 n], with X i as Ni-points in Hi, as described in \u00a7 3.1. As a first step of our analysis, we define certain uniformity parameters i, which serve as a link between the continuous and discrete ranges. To this end, it should be noted that for all i-points [n] and b-points SD \u2212 1, the quantity | X > i b-points can be defined, which serve as a link between the continuous and discrete ranges."}, {"heading": "4. Proofs", "text": "In this section we will demonstrate theorems 1-3 associated with the continuous problem (20) and theorems 6 and 7 associated with the discrete, non-convex minimization problem (9) and recursion of linear programs (11), respectively."}, {"heading": "4.1 Preliminaries on the continuous problem", "text": "We begin by stating that the objective function (20) is distinguishable everywhere except at points \u00b1 b1,.., \u00b1 bn, where its partial derivatives do not exist. For each b-SD-1 that differs from \u00b1 bi, the gradient at b is determined by b-J = \u2212 n-i = 1 b > i b (1 \u2212 (b > i b) 2) 1 2 bi. (45) Now, b-i is a global solution of (20) and suppose that b-i = 1 Ni (b > i b) [n]. Then b-i must fulfill the optimum condition of first order."}, {"heading": "4.2 Proof of Theorem 1", "text": "According to Lemma 8, any global solution must be in the plane chip (b1, b2), and so our problem becomes planar, i.e., we can just as well assume that the hyperplane arrangement b1, b2 is a line arrangement of R2. Note that the continuous objective function for two hyperplanes asJ (b) = N1 (1 \u2212 (b > 1 b) 2) 2) 2) 1 2 + N2 (1 \u2212 (b > 2 b) 2) 2) 1 2, b \u2212 S1 (49) b) can be a global solution and assume that b \u00b2 S1 (b > 1) = N1 (b > 1 b) 2) 2) 1 + N2 (b > 2) 2) 2, b \u2212 S1 \u2212 S1 (49) b) can be a global solution and that b \u00b2 b \u00b2 a (b) is a minimum of b, then we can replace b1, b2 (b2) with \u2212 b1 \u2212 b2, an operation that neither modifies the arrangement nor the goal."}, {"heading": "4.3 Proof of Theorem 2", "text": "For the sake of simplicity, we assume that n = 3, the general case follows in a similar way."}, {"heading": "4.4 Proof of Theorem 3", "text": "Without loss of universality, we can give an equiangular arrangement of three hyperplanes of RD = > b = 71 principal (with an equiangular arrangement of three levels of R3, with norms b1, b2, b3 given byb1: = \u00b5 [1 + \u03b1 \u03b1] > (63) b2: = \u00b5 [\u03b1 1 + \u03b1] > (64) b3: = \u00b5 [\u03b1 1 + \u03b1] > (65) \u2212 \u2212 \u2212 2 + 2\u03b12] \u2212 1 2, (66) with a positive real number determining the angle of the arrangement, however, lies (0, \u03c0 / 2] of the arrangement, given bycos [2]: = 2\u03b1 (1 + \u03b1) 2 + 2\u03b12 [2 + 2\u03b12] \u2212 2 + 2\u03b12] \u2212 2, (66) with a positive real number determining the angle of the two lies. (67) Since N1 = N2 = N2 = N2 = N3, so our objective function is > 1 b."}, {"heading": "4.5 Proof of Theorem 4", "text": "We start with two Lemmas.Lemma 12 Let us know that we don't know what to do. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) (...) (...) Let us know. (...) (...) Let us know. (...) Let us know. (...) (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...).). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...).). (...). (...)."}, {"heading": "4.6 Proof of Theorem 6", "text": "Let us first derive an upper limit: (1) max. (1) max. (1) max. (1) max. (1) max. (2) max. (2) max. (2) max. (2) max. (2) max. (1) max. (2) max. (2) max. (2) max. (2) max. (1) max. (2) max. (2) max. (2) max. (2) max. (1) max. (1) max. (1) max. (2) max. (2) max. (2) max. (1). (2) max. (1). (2) max. (2). (2) max. (2). (2) max. (2). (2) max. (2) max. (2). (2) max. (1). (2) max. (2) max. (1). (2) max. (2). (2) max. (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2).). (2). (2). (2). (2).). (2). (2). (2).). (2). (2). (2). (2). (2). (2). (2). (2). (2).). (2). (2). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3).). (3.). (3. (3.). (3.).). (3.).). (3. (3.).). ("}, {"heading": "4.7 Proof of Theorem 7", "text": "First, from the theory of the simple method it follows that ifnk + > 1 is reached by the simple method, then it will satisfy the conclusion of Lemma 15 in Appendix A. Then Lemma 16 guarantees that {nk} converges in a finite number of steps to a critical point of the problem (9); denote this point by n \u00b0. In other words, n \u00b0 will satisfy Equation (111) and it will have Unit '2 norm. Now, if n \u00b0 = \u00b1 bj for some j > 1, thenJ (n \u00b0 0) \u2265 J (bj), (145) or equivalent \u00b2 Nin the equation (0 \u00b0 I, n \u00b0 I = j Nib > j \u00b2 j \u00b2 i, bj \u00b2 s. (146) Let's replace the concentration modellation i, n \u00b2 0 = c = c \u00b2 \u00b2 Hi, i \u00b2 i \u00b2 i \u00b2 i, (145 \u00b0 I)."}, {"heading": "5. Algorithmic Contributions", "text": "There are at least two ways in which DPCP can be used to learn a hyperplane arrangement: either by a sequential (RANSAC-like) scheme or by an iterative (K-subspaces-like) scheme. These two cases are described below. Algorithm 1 Sequential hyperplane learning using DPCP 1: Method SHL-DPCP (X = [x1, x2,.., xN]: RD \u00b7 N, n) 2: i (0); 3: wj (1, j = 1,.., N); 4: for i = 1: n do 5: Y (w1x1 \u00b7 wNxN]; 6: bi (a) argminb (2).b > b > b = 1]; 7: wj (b) mink = 1,... b > k (b), i (b), i (b), i (b), i (b), i (b), i (i), i (b), i (b)."}, {"heading": "5.1 Sequential hyperplane learning via DPCP", "text": "Since the DPCP is essentially a single learning method in subspace, we may as well use it to learn n hyperplanes as RANSAC (Fischler and Bolles, 1981) is used: if we learn a hyperplane from the whole data set, remove the points near them, then we learn a second hyperplane, etc. The main weakness of this technique is well known and consists in its sensitivity to the threshold necessary to remove points. To alleviate the need to know a good threshold, we propose to replace the process of point removal with a process of appropriate weighting of points. Specifically, we assume that we solve the DPCP problem (9) on the entire data set X and obtain a unit '2-standard vector b1. Instead of replacing the points of X that are close to the hyperplane with a process of normal weighting the points (which would require a threshold b1), we calculate each point xj from its distance."}, {"heading": "5.2 Iterative hyperplane learning via DPCP", "text": "Another way to make hyperplane clustering via DPCP is to modify the classic K subspace = > > J method = > JP method = > JP method, 2000; Zhang et al., 2009) by calculating the normal vector of each cluster by DPCP. We call the resulting method IHL-DPCP; see Algorithm 2. It should be noted that since DPCP, the \"1 standard of distances to a hyperplane is minimized, with consistency dictating that the stop criterion for IHL-DPCP is regulated by the sum of all points of the distance to its assigned hyperplane (instead of the traditional sum of squares (Bradley and Mangasarian, 2000)); in other words, the global objective function minimized by IHL-DPCP."}, {"heading": "5.3 Solving the DPCP problem", "text": "Remember that the DPCP problem (9), which appears in algorithms 1 and 2 (with data matrices Y and Ci), is not convex. In Tsakiris and Vidal (2017b), we have described four different methods for solving this problem, which we will briefly examine here. Algorithm 5: Denoised Dual Principal Component Pursuit 1: Method DPCP-d (X, \u03b5, Tmax) 2: Calculation of a choleskytic factoring LL > = XX > + \u03b4ID; 3: k \u00b2 method; 3: k \u00b2 -J \u00b2 method; 4: b \u00b2 argminb \u00b2 method (X > b \u00b2 method) 2: Calculation of a choleskylike factoring LL > b > b \u00b2 method; 6: during k < Tmax and J > method 7: k \u00b2 -DPCP \u00b2 programs (X > b \u00b2 method)."}, {"heading": "6. Experimental evaluation", "text": "In this section we evaluate algorithms 1 and 2 experimentally using both synthetic (\u00a7 6.1) and real data (\u00a7 6.2)."}, {"heading": "6.1 Synthetic data", "text": "The coordinate dimension D of the data is inspired by large applications in which hyper-plane arrangements occur. In particular, we remember that \u2022 In 3D point cloud analysis, the coordinate dimension of the data is increased by 1 (see?), i.e., D = 4. \u2022 In two-view geometry, one works with correspondences between pairs of 3D points. Each such correspondence is treated as a point itself, like the product of the two associated 3D points, i.e. with coordinate dimension D = 9. Consequently, we choose D = 4, 9 and D = 30, where the choice of 30 is justified as sufficiently greater than 4 or 9."}, {"heading": "6.2 3D Plane clustering of real Kinect data", "text": "In fact, most of them are able to play by the rules that they have set themselves, and they are able to play by the rules that they have set themselves in order to play by the rules."}, {"heading": "7. Conclusions", "text": "We investigated theoretically and algorithmically the application of the recently proposed method of single subspace learning, Dual Principal Component Pursuit (DPCP), to the problem of clustering data close to a hyper-plane connection. We provided theoretical conditions under which the non-convex cosmos problem associated with DPCP allows for a unique (ominous) global solution corresponding to the normal vector of the underlying dominant hyperplane. We proposed sequential and parallel hyper-plane cluster methods that, based on synthetic data, dramatically improved modern methods such as RANSAC or REAPER, while competing with the latter in the case of learning 3D layers from real Kinect data."}, {"heading": "Acknowledgement", "text": "The authors thank Prof. Daniel P. Robinson of the Department of Applied Mathematics and Statistics at Johns Hopkins University for many useful conversations and for his many comments that have helped to improve this manuscript. Appendix A. Results on problems (9) and (11) following Spa \ufffd th and Watson (1987) In this section, we list three results that are important for our mathematical analysis that were already known in Spa \ufffd th and Watson (1987); detailed evidence can be found in Tsakiris and Vidal (2017b). Let Y have a D \u00b7 N matrix of complete rank D. Then we have the following. Lemma 14 Any global solution b \u00b2 to minb > b = 1% Y > b > 1, must be orthogonal to (D \u2212 1) linear independent points of Y."}], "references": [{"title": "Identification of switched linear systems via sparse optimization", "author": ["L. Bako"], "venue": null, "citeRegEx": "Bako.,? \\Q2011\\E", "shortCiteRegEx": "Bako.", "year": 2011}, {"title": "Factorization-based segmentation of motions", "author": ["T.E. Boult", "L.G. Brown"], "venue": "In IEEE Workshop on Motion Understanding,", "citeRegEx": "Boult and Brown.,? \\Q1991\\E", "shortCiteRegEx": "Boult and Brown.", "year": 1991}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Boykov et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Boykov et al\\.", "year": 2001}, {"title": "k-plane clustering", "author": ["P.S. Bradley", "O.L. Mangasarian"], "venue": "Journal of Global Optimization,", "citeRegEx": "Bradley and Mangasarian.,? \\Q2000\\E", "shortCiteRegEx": "Bradley and Mangasarian.", "year": 2000}, {"title": "Enhancing sparsity by reweighted `1 minimization", "author": ["E. Cand\u00e8s", "M. Wakin", "S. Boyd"], "venue": "Journal of Fourier Analysis and Applications,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2008}, {"title": "Iteratively reweighted algorithms for compressive sensing", "author": ["R. Chartrand", "W. Yin"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Chartrand and Yin.,? \\Q2008\\E", "shortCiteRegEx": "Chartrand and Yin.", "year": 2008}, {"title": "Spectral curvature clustering (SCC)", "author": ["G. Chen", "G. Lerman"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Chen and Lerman.,? \\Q2009\\E", "shortCiteRegEx": "Chen and Lerman.", "year": 2009}, {"title": "A multibody factorization method for independently moving objects", "author": ["J. Costeira", "T. Kanade"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Costeira and Kanade.,? \\Q1998\\E", "shortCiteRegEx": "Costeira and Kanade.", "year": 1998}, {"title": "Ideals, Varieties, and Algorithms", "author": ["D.A. Cox", "J. Little", "D. O\u2019Shea"], "venue": null, "citeRegEx": "Cox et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cox et al\\.", "year": 2007}, {"title": "Iteratively reweighted least squares minimization for sparse recovery", "author": ["I. Daubechies", "R. DeVore", "M. Fornasier", "C.S. G\u00fcnt\u00fcrk"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Daubechies et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Daubechies et al\\.", "year": 2010}, {"title": "Hilbert series of subspace arrangements", "author": ["H. Derksen"], "venue": "Journal of Pure and Applied Algebra,", "citeRegEx": "Derksen.,? \\Q2007\\E", "shortCiteRegEx": "Derksen.", "year": 2007}, {"title": "A flag representation for finite collections of subspaces of mixed dimensions", "author": ["B. Draper", "M. Kirby", "J. Marks", "T. Marrinan", "C. Peterson"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Draper et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Draper et al\\.", "year": 2014}, {"title": "Sparse subspace clustering", "author": ["E. Elhamifar", "R. Vidal"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Elhamifar and Vidal.,? \\Q2009\\E", "shortCiteRegEx": "Elhamifar and Vidal.", "year": 2009}, {"title": "Clustering disjoint subspaces via sparse representation", "author": ["E. Elhamifar", "R. Vidal"], "venue": "In IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Elhamifar and Vidal.,? \\Q2010\\E", "shortCiteRegEx": "Elhamifar and Vidal.", "year": 2010}, {"title": "Sparse subspace clustering: Algorithm, theory, and applications", "author": ["E. Elhamifar", "R. Vidal"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Elhamifar and Vidal.,? \\Q2013\\E", "shortCiteRegEx": "Elhamifar and Vidal.", "year": 2013}, {"title": "A closed form solution to robust subspace estimation and clustering", "author": ["P. Favaro", "R. Vidal", "A. Ravichandran"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Favaro et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Favaro et al\\.", "year": 2011}, {"title": "RANSAC random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography", "author": ["M.A. Fischler", "R.C. Bolles"], "venue": "Communications of the ACM,", "citeRegEx": "Fischler and Bolles.,? \\Q1981\\E", "shortCiteRegEx": "Fischler and Bolles.", "year": 1981}, {"title": "The fermat point of a spherical triangle", "author": ["K. Ghalieh", "M. Hajja"], "venue": "The Mathematical Gazette,", "citeRegEx": "Ghalieh and Hajja.,? \\Q1996\\E", "shortCiteRegEx": "Ghalieh and Hajja.", "year": 1996}, {"title": "Spherical designs, discrepancy and numerical integration", "author": ["P.J. Grabner", "R.F. Tichy"], "venue": "Math. Comp.,", "citeRegEx": "Grabner and Tichy.,? \\Q1993\\E", "shortCiteRegEx": "Grabner and Tichy.", "year": 1993}, {"title": "Discrepancies of point sequences on the sphere and numerical integration", "author": ["P.J. Grabner", "B. Klinger", "R.F. Tichy"], "venue": "Mathematical Research,", "citeRegEx": "Grabner et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Grabner et al\\.", "year": 1997}, {"title": "Multibody factorization with uncertainty and missing data using the EM algorithm", "author": ["A. Gruber", "Y. Weiss"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Gruber and Weiss.,? \\Q2004\\E", "shortCiteRegEx": "Gruber and Weiss.", "year": 2004}, {"title": "Analysis of a complex of statistical variables into principal components", "author": ["H. Hotelling"], "venue": "Journal of Educational Psychology,", "citeRegEx": "Hotelling.,? \\Q1933\\E", "shortCiteRegEx": "Hotelling.", "year": 1933}, {"title": "Principal Component Analysis", "author": ["I. Jolliffe"], "venue": "Springer-Verlag, 2nd edition,", "citeRegEx": "Jolliffe.,? \\Q2002\\E", "shortCiteRegEx": "Jolliffe.", "year": 2002}, {"title": "Motion segmentation by subspace separation and model selection", "author": ["K. Kanatani"], "venue": "In IEEE International Conference on Computer Vision,", "citeRegEx": "Kanatani.,? \\Q2001\\E", "shortCiteRegEx": "Kanatani.", "year": 2001}, {"title": "`p-recovery of the most significant subspace among multiple subspaces with outliers", "author": ["G. Lerman", "T. Zhang"], "venue": "Constructive Approximation,", "citeRegEx": "Lerman and Zhang.,? \\Q2014\\E", "shortCiteRegEx": "Lerman and Zhang.", "year": 2014}, {"title": "Robust computation of linear models by convex relaxation", "author": ["G. Lerman", "M.B. McCoy", "J.A. Tropp", "T. Zhang"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Lerman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lerman et al\\.", "year": 2015}, {"title": "Robust subspace segmentation by low-rank representation", "author": ["G. Liu", "Z. Lin", "Y. Yu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Robust recovery of subspace structures by low-rank representation", "author": ["G. Liu", "Z. Lin", "S. Yan", "J. Sun", "Y. Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Robust and efficient subspace segmentation via least squares regression", "author": ["C-Y. Lu", "H. Min", "Z-Q. Zhao", "L. Zhu", "D-S. Huang", "S. Yan"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Lu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2012}, {"title": "Identification of deterministic switched ARX systems via identification of algebraic varieties. In Hybrid Systems: Computation and Control, pages 449\u2013465", "author": ["Y. Ma", "R. Vidal"], "venue": null, "citeRegEx": "Ma and Vidal.,? \\Q2005\\E", "shortCiteRegEx": "Ma and Vidal.", "year": 2005}, {"title": "Segmentation of multivariate mixed data via lossy coding and compression", "author": ["Y. Ma", "H. Derksen", "W. Hong", "J. Wright"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Ma et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2007}, {"title": "On lines and planes of closest fit to systems of points in space", "author": ["K. Pearson"], "venue": "The London, Edinburgh and Dublin Philosphical Magazine and Journal of Science,", "citeRegEx": "Pearson.,? \\Q1901\\E", "shortCiteRegEx": "Pearson.", "year": 1901}, {"title": "Finding a sparse vector in a subspace: Linear sparsity using alternating directions", "author": ["Q. Qu", "J. Sun", "J. Wright"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Qu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qu et al\\.", "year": 2014}, {"title": "Segmentation and reconstruction of polyhedral building roofs from aerial lidar point clouds", "author": ["A. Sampath", "J. Shan"], "venue": "IEEE Transactions on Geoscience and Remote Sensing,", "citeRegEx": "Sampath and Shan.,? \\Q2010\\E", "shortCiteRegEx": "Sampath and Shan.", "year": 2010}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["N. Silberman", "P. Kohli", "D. Hoiem", "R. Fergus"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Silberman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Silberman et al\\.", "year": 2012}, {"title": "On orthogonal linear `1 approximation", "author": ["H. Sp\u00e4th", "G.A. Watson"], "venue": "Numerische Mathematik,", "citeRegEx": "Sp\u00e4th and Watson.,? \\Q1987\\E", "shortCiteRegEx": "Sp\u00e4th and Watson.", "year": 1987}, {"title": "An introduction to conditional random fields for relational learning, volume 2. Introduction to statistical relational learning", "author": ["C. Sutton", "A. McCallum"], "venue": null, "citeRegEx": "Sutton and McCallum.,? \\Q2006\\E", "shortCiteRegEx": "Sutton and McCallum.", "year": 2006}, {"title": "Probabilistic principal component analysis", "author": ["M. Tipping", "C. Bishop"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Tipping and Bishop.,? \\Q1999\\E", "shortCiteRegEx": "Tipping and Bishop.", "year": 1999}, {"title": "Mixtures of probabilistic principal component analyzers", "author": ["M. Tipping", "C. Bishop"], "venue": "Neural Computation,", "citeRegEx": "Tipping and Bishop.,? \\Q1999\\E", "shortCiteRegEx": "Tipping and Bishop.", "year": 1999}, {"title": "Abstract algebraic-geometric subspace clustering", "author": ["M.C. Tsakiris", "R. Vidal"], "venue": "In Asilomar Conference on Signals, Systems and Computers,", "citeRegEx": "Tsakiris and Vidal.,? \\Q2014\\E", "shortCiteRegEx": "Tsakiris and Vidal.", "year": 2014}, {"title": "Algebraic clustering of affine subspaces", "author": ["M.C. Tsakiris", "R. Vidal"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Tsakiris and Vidal.,? \\Q2017\\E", "shortCiteRegEx": "Tsakiris and Vidal.", "year": 2017}, {"title": "Dual principal component pursuit. arXiv:1510.04390v2 [cs.CV], 2017b", "author": ["M.C. Tsakiris", "R. Vidal"], "venue": null, "citeRegEx": "Tsakiris and Vidal.,? \\Q2017\\E", "shortCiteRegEx": "Tsakiris and Vidal.", "year": 2017}, {"title": "Filtrated algebraic subspace clustering", "author": ["M.C. Tsakiris", "R. Vidal"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Tsakiris and Vidal.,? \\Q2017\\E", "shortCiteRegEx": "Tsakiris and Vidal.", "year": 2017}, {"title": "Dual principal component pursuit", "author": ["M.C. Tsakiris", "R. Vidal"], "venue": "In ICCV Workshop on Robust Subspace Learning and Computer Vision,", "citeRegEx": "Tsakiris and Vidal.,? \\Q2015\\E", "shortCiteRegEx": "Tsakiris and Vidal.", "year": 2015}, {"title": "Filtrated spectral algebraic subspace clustering", "author": ["M.C. Tsakiris", "R. Vidal"], "venue": "In ICCV Workshop on Robust Subspace Learning and Computer Vision,", "citeRegEx": "Tsakiris and Vidal.,? \\Q2015\\E", "shortCiteRegEx": "Tsakiris and Vidal.", "year": 2015}, {"title": "Nearest q-flat to m points", "author": ["P. Tseng"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Tseng.,? \\Q2000\\E", "shortCiteRegEx": "Tseng.", "year": 2000}, {"title": "Subspace clustering", "author": ["R. Vidal"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Vidal.,? \\Q2011\\E", "shortCiteRegEx": "Vidal.", "year": 2011}, {"title": "Low rank subspace clustering (LRSC)", "author": ["R. Vidal", "P. Favaro"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Vidal and Favaro.,? \\Q2014\\E", "shortCiteRegEx": "Vidal and Favaro.", "year": 2014}, {"title": "Three-view multibody structure from motion", "author": ["R. Vidal", "R. Hartley"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Vidal and Hartley.,? \\Q2008\\E", "shortCiteRegEx": "Vidal and Hartley.", "year": 2008}, {"title": "Generalized Principal Component Analysis (GPCA)", "author": ["R. Vidal", "Y. Ma", "S. Sastry"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Vidal et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Vidal et al\\.", "year": 2003}, {"title": "Generalized Principal Component Analysis (GPCA)", "author": ["R. Vidal", "Y. Ma", "S. Sastry"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Vidal et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Vidal et al\\.", "year": 2005}, {"title": "Two-view multibody structure from motion", "author": ["R. Vidal", "Y. Ma", "S. Soatto", "S. Sastry"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Vidal et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Vidal et al\\.", "year": 2006}, {"title": "Multiframe motion segmentation with missing data using PowerFactorization, and GPCA", "author": ["R. Vidal", "R. Tron", "R. Hartley"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Vidal et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vidal et al\\.", "year": 2008}, {"title": "Generalized Principal Component Analysis", "author": ["R. Vidal", "Y. Ma", "S. Sastry"], "venue": null, "citeRegEx": "Vidal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vidal et al\\.", "year": 2016}, {"title": "A tutorial on spectral clustering", "author": ["U. von Luxburg"], "venue": "Statistics and Computing,", "citeRegEx": "Luxburg.,? \\Q2007\\E", "shortCiteRegEx": "Luxburg.", "year": 2007}, {"title": "Self scaled regularized robust regression", "author": ["Y. Wang", "C. Dicle", "M. Sznaier", "O. Camps"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Median k-flats for hybrid linear modeling with many outliers", "author": ["T. Zhang", "A. Szlam", "G. Lerman"], "venue": "In Workshop on Subspace Methods,", "citeRegEx": "Zhang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 10, "context": "Over the past fifteen years the model of a union of linear subspaces, also called a subspace arrangement (Derksen, 2007), has gained significant popularity in pattern recognition and computer vision (Vidal, 2011), often replacing the classical model of a single linear subspace, associated to the well-known Principal Component Analysis (PCA) (Hotelling, 1933; Pearson, 1901; Jolliffe, 2002).", "startOffset": 105, "endOffset": 120}, {"referenceID": 46, "context": "Over the past fifteen years the model of a union of linear subspaces, also called a subspace arrangement (Derksen, 2007), has gained significant popularity in pattern recognition and computer vision (Vidal, 2011), often replacing the classical model of a single linear subspace, associated to the well-known Principal Component Analysis (PCA) (Hotelling, 1933; Pearson, 1901; Jolliffe, 2002).", "startOffset": 199, "endOffset": 212}, {"referenceID": 21, "context": "Over the past fifteen years the model of a union of linear subspaces, also called a subspace arrangement (Derksen, 2007), has gained significant popularity in pattern recognition and computer vision (Vidal, 2011), often replacing the classical model of a single linear subspace, associated to the well-known Principal Component Analysis (PCA) (Hotelling, 1933; Pearson, 1901; Jolliffe, 2002).", "startOffset": 343, "endOffset": 391}, {"referenceID": 31, "context": "Over the past fifteen years the model of a union of linear subspaces, also called a subspace arrangement (Derksen, 2007), has gained significant popularity in pattern recognition and computer vision (Vidal, 2011), often replacing the classical model of a single linear subspace, associated to the well-known Principal Component Analysis (PCA) (Hotelling, 1933; Pearson, 1901; Jolliffe, 2002).", "startOffset": 343, "endOffset": 391}, {"referenceID": 22, "context": "Over the past fifteen years the model of a union of linear subspaces, also called a subspace arrangement (Derksen, 2007), has gained significant popularity in pattern recognition and computer vision (Vidal, 2011), often replacing the classical model of a single linear subspace, associated to the well-known Principal Component Analysis (PCA) (Hotelling, 1933; Pearson, 1901; Jolliffe, 2002).", "startOffset": 343, "endOffset": 391}, {"referenceID": 46, "context": "This has led to a variety of algorithms that attempt to cluster a collection of data drawn from a subspace arrangement, giving rise to the challenging field of subspace clustering (Vidal, 2011).", "startOffset": 180, "endOffset": 193}, {"referenceID": 20, "context": ", 2009), statistical (Tipping and Bishop, 1999b; Gruber and Weiss, 2004), informationtheoretic (Ma et al.", "startOffset": 21, "endOffset": 72}, {"referenceID": 30, "context": ", 2009), statistical (Tipping and Bishop, 1999b; Gruber and Weiss, 2004), informationtheoretic (Ma et al., 2007), algebraic (Vidal et al.", "startOffset": 95, "endOffset": 112}, {"referenceID": 1, "context": ", 2003, 2005; Tsakiris and Vidal, 2017c), spectral (Boult and Brown, 1991; Costeira and Kanade, 1998; Kanatani, 2001; Chen and Lerman, 2009), or based on sparse (Elhamifar and Vidal, 2009, 2010, 2013) and low-rank (Liu et al.", "startOffset": 51, "endOffset": 140}, {"referenceID": 7, "context": ", 2003, 2005; Tsakiris and Vidal, 2017c), spectral (Boult and Brown, 1991; Costeira and Kanade, 1998; Kanatani, 2001; Chen and Lerman, 2009), or based on sparse (Elhamifar and Vidal, 2009, 2010, 2013) and low-rank (Liu et al.", "startOffset": 51, "endOffset": 140}, {"referenceID": 23, "context": ", 2003, 2005; Tsakiris and Vidal, 2017c), spectral (Boult and Brown, 1991; Costeira and Kanade, 1998; Kanatani, 2001; Chen and Lerman, 2009), or based on sparse (Elhamifar and Vidal, 2009, 2010, 2013) and low-rank (Liu et al.", "startOffset": 51, "endOffset": 140}, {"referenceID": 6, "context": ", 2003, 2005; Tsakiris and Vidal, 2017c), spectral (Boult and Brown, 1991; Costeira and Kanade, 1998; Kanatani, 2001; Chen and Lerman, 2009), or based on sparse (Elhamifar and Vidal, 2009, 2010, 2013) and low-rank (Liu et al.", "startOffset": 51, "endOffset": 140}, {"referenceID": 26, "context": ", 2003, 2005; Tsakiris and Vidal, 2017c), spectral (Boult and Brown, 1991; Costeira and Kanade, 1998; Kanatani, 2001; Chen and Lerman, 2009), or based on sparse (Elhamifar and Vidal, 2009, 2010, 2013) and low-rank (Liu et al., 2010; Favaro et al., 2011; Liu et al., 2013; Vidal and Favaro, 2014) representation theory.", "startOffset": 214, "endOffset": 295}, {"referenceID": 15, "context": ", 2003, 2005; Tsakiris and Vidal, 2017c), spectral (Boult and Brown, 1991; Costeira and Kanade, 1998; Kanatani, 2001; Chen and Lerman, 2009), or based on sparse (Elhamifar and Vidal, 2009, 2010, 2013) and low-rank (Liu et al., 2010; Favaro et al., 2011; Liu et al., 2013; Vidal and Favaro, 2014) representation theory.", "startOffset": 214, "endOffset": 295}, {"referenceID": 27, "context": ", 2003, 2005; Tsakiris and Vidal, 2017c), spectral (Boult and Brown, 1991; Costeira and Kanade, 1998; Kanatani, 2001; Chen and Lerman, 2009), or based on sparse (Elhamifar and Vidal, 2009, 2010, 2013) and low-rank (Liu et al., 2010; Favaro et al., 2011; Liu et al., 2013; Vidal and Favaro, 2014) representation theory.", "startOffset": 214, "endOffset": 295}, {"referenceID": 47, "context": ", 2003, 2005; Tsakiris and Vidal, 2017c), spectral (Boult and Brown, 1991; Costeira and Kanade, 1998; Kanatani, 2001; Chen and Lerman, 2009), or based on sparse (Elhamifar and Vidal, 2009, 2010, 2013) and low-rank (Liu et al., 2010; Favaro et al., 2011; Liu et al., 2013; Vidal and Favaro, 2014) representation theory.", "startOffset": 214, "endOffset": 295}, {"referenceID": 51, "context": "Prominent applications include projective motion segmentation (Vidal et al., 2006; Vidal and Hartley, 2008), 3D point cloud analysis (Sampath and Shan, 2010) and hybrid system identification (Bako, 2011; Ma and Vidal, 2005).", "startOffset": 62, "endOffset": 107}, {"referenceID": 48, "context": "Prominent applications include projective motion segmentation (Vidal et al., 2006; Vidal and Hartley, 2008), 3D point cloud analysis (Sampath and Shan, 2010) and hybrid system identification (Bako, 2011; Ma and Vidal, 2005).", "startOffset": 62, "endOffset": 107}, {"referenceID": 33, "context": ", 2006; Vidal and Hartley, 2008), 3D point cloud analysis (Sampath and Shan, 2010) and hybrid system identification (Bako, 2011; Ma and Vidal, 2005).", "startOffset": 58, "endOffset": 82}, {"referenceID": 0, "context": ", 2006; Vidal and Hartley, 2008), 3D point cloud analysis (Sampath and Shan, 2010) and hybrid system identification (Bako, 2011; Ma and Vidal, 2005).", "startOffset": 116, "endOffset": 148}, {"referenceID": 29, "context": ", 2006; Vidal and Hartley, 2008), 3D point cloud analysis (Sampath and Shan, 2010) and hybrid system identification (Bako, 2011; Ma and Vidal, 2005).", "startOffset": 116, "endOffset": 148}, {"referenceID": 49, "context": ", 2003, 2005, 2008; Tsakiris and Vidal, 2014, 2015b, 2017c,a), which gives closed-form solutions by means of factorization (Vidal et al., 2003) or differentiation (Vidal et al.", "startOffset": 123, "endOffset": 143}, {"referenceID": 50, "context": ", 2003) or differentiation (Vidal et al., 2005) of polynomials.", "startOffset": 27, "endOffset": 47}, {"referenceID": 6, "context": "Another method that is theoretically justifiable for clustering hyperplanes is Spectral Curvature Clustering (SCC) (Chen and Lerman, 2009), which is based on computing a D-fold affinity between all D-tuples of points in the dataset.", "startOffset": 115, "endOffset": 138}, {"referenceID": 3, "context": "On the other hand, the intuitive classical method of K-hyperplanes (KH) (Bradley and Mangasarian, 2000), which alternates between assigning clusters and fitting a new normal vector to each cluster with PCA, is perhaps the most practical method for hyperplane clustering, since it is simple to implement, it is robust to noise and its complexity depends on the maximal allowed number of iterations.", "startOffset": 72, "endOffset": 103}, {"referenceID": 56, "context": "Median K-Flats (MKF) (Zhang et al., 2009) shares a similar objective function as KH, but uses the `1-norm instead of the `2norm, in an attempt to gain robustness to outliers.", "startOffset": 21, "endOffset": 41}, {"referenceID": 16, "context": "Finally, we note that any single subspace learning method, such as RANSAC (Fischler and Bolles, 1981) or REAPER (Lerman et al.", "startOffset": 74, "endOffset": 101}, {"referenceID": 25, "context": "Finally, we note that any single subspace learning method, such as RANSAC (Fischler and Bolles, 1981) or REAPER (Lerman et al., 2015), can be applied in a sequential fashion to learn a union of hyperplanes, by first learning the first dominant hyperplane, then removing the points lying close to it, then learning a second dominant hyperplane, and so on.", "startOffset": 112, "endOffset": 133}, {"referenceID": 0, "context": ", 2006; Vidal and Hartley, 2008), 3D point cloud analysis (Sampath and Shan, 2010) and hybrid system identification (Bako, 2011; Ma and Vidal, 2005). Even though in some ways hyperplane clustering is simpler than general subspace clustering, since, e.g., the dimensions of the subspaces are equal and known a-priori, modern self-expressiveness-based subspace clustering methods, such as Liu et al. (2013); Lu et al.", "startOffset": 117, "endOffset": 405}, {"referenceID": 0, "context": ", 2006; Vidal and Hartley, 2008), 3D point cloud analysis (Sampath and Shan, 2010) and hybrid system identification (Bako, 2011; Ma and Vidal, 2005). Even though in some ways hyperplane clustering is simpler than general subspace clustering, since, e.g., the dimensions of the subspaces are equal and known a-priori, modern self-expressiveness-based subspace clustering methods, such as Liu et al. (2013); Lu et al. (2012); Elhamifar and Vidal (2013), in principle do not apply in this case, because they require small relative subspace dimensions.", "startOffset": 117, "endOffset": 423}, {"referenceID": 0, "context": ", 2006; Vidal and Hartley, 2008), 3D point cloud analysis (Sampath and Shan, 2010) and hybrid system identification (Bako, 2011; Ma and Vidal, 2005). Even though in some ways hyperplane clustering is simpler than general subspace clustering, since, e.g., the dimensions of the subspaces are equal and known a-priori, modern self-expressiveness-based subspace clustering methods, such as Liu et al. (2013); Lu et al. (2012); Elhamifar and Vidal (2013), in principle do not apply in this case, because they require small relative subspace dimensions.", "startOffset": 117, "endOffset": 451}, {"referenceID": 16, "context": "A traditional way of clustering points lying close to a hyperplane arrangement is by means of the RANdom SAmpling Consensus algorithm (RANSAC) (Fischler and Bolles, 1981), which attempts to identify a single hyperplaneHi at a time.", "startOffset": 143, "endOffset": 170}, {"referenceID": 3, "context": "Another very popular method for hyperplane clustering is the so-called K-hyperplanes (KH), which was proposed by Bradley and Mangasarian (2000). KH attempts to minimize the non-convex objective function", "startOffset": 113, "endOffset": 144}, {"referenceID": 56, "context": "It is precisely the sensitivity to outliers of KH that Median K Flats (MKF) or Median K Hyperplanes (Zhang et al., 2009) attempts to address, by minimizing the non-", "startOffset": 100, "endOffset": 120}, {"referenceID": 50, "context": "This makes the optimization problem harder, and Zhang et al. (2009) propose to solve it by means of a stochastic gradient approach, which requires multiple restarts, as KH does.", "startOffset": 48, "endOffset": 68}, {"referenceID": 46, "context": "ASC was originally proposed in Vidal et al. (2003) precisely for the purpose of provably clustering hyperplanes, a problem which at the time was handled either by the intuitive RANSAC or K-Hyperplanes.", "startOffset": 31, "endOffset": 51}, {"referenceID": 46, "context": "This reduces the problem to that of factorizing p(x) to the product of linear factors, which was elegantly done in Vidal et al. (2003). When the data are contaminated by noise, the fitted polynomial need no longer be factorizable; this apparent difficulty was circumvented in Vidal et al.", "startOffset": 115, "endOffset": 135}, {"referenceID": 46, "context": "This reduces the problem to that of factorizing p(x) to the product of linear factors, which was elegantly done in Vidal et al. (2003). When the data are contaminated by noise, the fitted polynomial need no longer be factorizable; this apparent difficulty was circumvented in Vidal et al. (2005), where it was shown that the gradient of the polynomial evaluated at point xj is a good estimate for the normal vector of the hyperplane Hi that xj lies closest to.", "startOffset": 115, "endOffset": 296}, {"referenceID": 25, "context": "A recently proposed single subspace learning method that admits an interesting theoretical analysis is the so-called REAPER (Lerman et al., 2015).", "startOffset": 124, "endOffset": 145}, {"referenceID": 6, "context": ",xjD (see Chen and Lerman (2009) for an explicit formula) and \u03c3 is a tuning parameter.", "startOffset": 10, "endOffset": 33}, {"referenceID": 6, "context": ",xjD (see Chen and Lerman (2009) for an explicit formula) and \u03c3 is a tuning parameter. Intuitively, the polar curvature is a multiple of the volume of the simplex of the D points, which becomes zero if the points lie in the same hyperplane, and the further the points lie from any hyperplane the larger the volume becomes. SCC obtains the hyperplane clusters by unfolding the tensor A to an affinity matrix, upon which spectral clustering is applied. As with ASC, the main bottleneck of SCC is computational, since in principle all ( N D ) entries of the tensor need to be computed. Even though the combinatorial complexity of SCC can be reduced, this comes at the cost of significant performance degradation. RANSAC/KH Hybrids. Generally speaking, any single subspace learning method that is robust to outliers and can handle subspaces of high relative dimensions, can be used to perform hyperplane clustering, either via a RANSAC-style or a KH-style scheme or a combination of both. For example, ifM is a method that takes a dataset and fits to it a hyperplane, then one can useM to compute the first dominant hyperplane, remove the points in the dataset lying close to it, compute a second dominant hyperplane and so on (RANSAC-style). Alternatively, one can start with a random guess for n hyperplanes, cluster the data according to their distance to these hyperplanes, and then use M (instead of the classic SVD) to fit a new hyperplane to each cluster, and so on (KH-style). Even though a large variety of single subspace learning methods exist, e.g., see references in Lerman and Zhang (2014), only few such methods are potentially able to handle large relative dimensions and in particular hyperplanes.", "startOffset": 10, "endOffset": 1600}, {"referenceID": 25, "context": "As (6) is non-convex, Lerman et al. (2015) relax it to the convex semi-definite program", "startOffset": 22, "endOffset": 43}, {"referenceID": 16, "context": "Similarly to RANSAC (Fischler and Bolles, 1981) or REAPER (Lerman et al.", "startOffset": 20, "endOffset": 47}, {"referenceID": 25, "context": "Similarly to RANSAC (Fischler and Bolles, 1981) or REAPER (Lerman et al., 2015), DPCP (Tsakiris and Vidal, 2015a, 2017b) is another, recently proposed, single subspace learning method that can be applied to hyperplane clustering.", "startOffset": 58, "endOffset": 79}, {"referenceID": 24, "context": "This difficulty can be circumvented by solving (7) in an Iteratively Reweighted Least Squares (IRLS) fashion, for which convergence of the objective value to a neighborhood of the optimal value has been established in Lerman et al. (2015). Dual Principal Component Pursuit (DPCP).", "startOffset": 218, "endOffset": 239}, {"referenceID": 16, "context": "Similarly to RANSAC (Fischler and Bolles, 1981) or REAPER (Lerman et al., 2015), DPCP (Tsakiris and Vidal, 2015a, 2017b) is another, recently proposed, single subspace learning method that can be applied to hyperplane clustering. 5. Regression-type methods such as the one proposed in Wang et al. (2015) are also a possibility.", "startOffset": 21, "endOffset": 304}, {"referenceID": 30, "context": "Important examples that we do not compare with in this paper are the statistical-theoretic Mixtures of Probabilistic Principal Component Analyzers (Tipping and Bishop, 1999a), as well as the information-theoretic Agglomerative Lossy Compression (Ma et al., 2007).", "startOffset": 245, "endOffset": 262}, {"referenceID": 36, "context": "Even though no theory has been developed for this approach, experimental evidence in Tsakiris and Vidal (2017b) indicates convergence of such an IRLS scheme to the global minimizer of (9).", "startOffset": 85, "endOffset": 112}, {"referenceID": 30, "context": "Important examples that we do not compare with in this paper are the statistical-theoretic Mixtures of Probabilistic Principal Component Analyzers (Tipping and Bishop, 1999a), as well as the information-theoretic Agglomerative Lossy Compression (Ma et al., 2007). For an extensive account of these and other methods the reader is referred to Vidal et al. (2016).", "startOffset": 246, "endOffset": 362}, {"referenceID": 40, "context": "This can be verified computationally by checking that there is only one up to scale homogeneous polynomial of degree n that fits the data, see Vidal et al. (2005); Tsakiris and Vidal (2017c) for details.", "startOffset": 143, "endOffset": 163}, {"referenceID": 39, "context": "(2005); Tsakiris and Vidal (2017c) for details.", "startOffset": 8, "endOffset": 35}, {"referenceID": 39, "context": "(16) In the second equality above we made use of the rotational invariance of the sphere, as well as the fact that \u0124i \u223c= SD\u22122, which leads to (for details see the proof of Proposition 4 and Lemma 7 in Tsakiris and Vidal (2017b)) \u222b", "startOffset": 201, "endOffset": 228}, {"referenceID": 11, "context": "Medians in Riemmannian manifolds, and in particular in the Grassmannian manifold, are an active subject of research (Draper et al., 2014; Ghalieh and Hajja, 1996).", "startOffset": 116, "endOffset": 162}, {"referenceID": 17, "context": "Medians in Riemmannian manifolds, and in particular in the Grassmannian manifold, are an active subject of research (Draper et al., 2014; Ghalieh and Hajja, 1996).", "startOffset": 116, "endOffset": 162}, {"referenceID": 17, "context": "This is in striking similarity with the results regarding the Fermat point of planar and spherical triangles (Ghalieh and Hajja, 1996).", "startOffset": 109, "endOffset": 134}, {"referenceID": 19, "context": "2) that when the points X i are uniformly distributed in a deterministic sense (Grabner et al., 1997; Grabner and Tichy, 1993), i is small and in particular i \u2192 0 as Ni \u2192\u221e.", "startOffset": 79, "endOffset": 126}, {"referenceID": 18, "context": "2) that when the points X i are uniformly distributed in a deterministic sense (Grabner et al., 1997; Grabner and Tichy, 1993), i is small and in particular i \u2192 0 as Ni \u2192\u221e.", "startOffset": 79, "endOffset": 126}, {"referenceID": 37, "context": "Then it can be shown (see Tsakiris and Vidal (2017b) \u00a74.", "startOffset": 26, "endOffset": 53}, {"referenceID": 39, "context": "For details regarding the evaluation of this integral see Lemma 9 and its proof in Tsakiris and Vidal (2017b).", "startOffset": 83, "endOffset": 110}, {"referenceID": 8, "context": "Viewing the above system of equations as polynomial equations in the variables x1, x2, x3, y1, y2, y3, z, standard Groebner basis (Cox et al., 2007) computations reveal that the polynomial g := (1\u2212 z)(y 1 \u2212 y 2)(y 1 \u2212 y 3)(y 2 \u2212 y 3)(y1 + y2 + y3) (76) lies in the ideal generated by pi, qi, i = 1, 2, 3.", "startOffset": 130, "endOffset": 148}, {"referenceID": 16, "context": "Since at its core DPCP is a single subspace learning method, we may as well use it to learn n hyperplanes in the same way that RANSAC (Fischler and Bolles, 1981) is used: learn one hyperplane from the entire dataset, remove the points close to it, then learn a second hyperplane and so on.", "startOffset": 134, "endOffset": 161}, {"referenceID": 3, "context": "Another way to do hyperplane clustering via DPCP, is to modify the classic K-Subspaces (Bradley and Mangasarian, 2000; Tseng, 2000; Zhang et al., 2009) by computing the normal vector of each cluster by DPCP.", "startOffset": 87, "endOffset": 151}, {"referenceID": 45, "context": "Another way to do hyperplane clustering via DPCP, is to modify the classic K-Subspaces (Bradley and Mangasarian, 2000; Tseng, 2000; Zhang et al., 2009) by computing the normal vector of each cluster by DPCP.", "startOffset": 87, "endOffset": 151}, {"referenceID": 56, "context": "Another way to do hyperplane clustering via DPCP, is to modify the classic K-Subspaces (Bradley and Mangasarian, 2000; Tseng, 2000; Zhang et al., 2009) by computing the normal vector of each cluster by DPCP.", "startOffset": 87, "endOffset": 151}, {"referenceID": 3, "context": "It is worth noting that since DPCP minimizes the `1-norm of the distances of the points to a hyperplane, consistency dictates that the stopping criterion for IHL-DPCP be governed by the sum over all points of the distance of each point to its assigned hyperplane (instead of the traditional sum of squares (Bradley and Mangasarian, 2000; Tseng, 2000)); in other words the global objective function minimized by IHL-DPCP is the same as that of Median K-Flats (Zhang et al.", "startOffset": 306, "endOffset": 350}, {"referenceID": 45, "context": "It is worth noting that since DPCP minimizes the `1-norm of the distances of the points to a hyperplane, consistency dictates that the stopping criterion for IHL-DPCP be governed by the sum over all points of the distance of each point to its assigned hyperplane (instead of the traditional sum of squares (Bradley and Mangasarian, 2000; Tseng, 2000)); in other words the global objective function minimized by IHL-DPCP is the same as that of Median K-Flats (Zhang et al.", "startOffset": 306, "endOffset": 350}, {"referenceID": 56, "context": "It is worth noting that since DPCP minimizes the `1-norm of the distances of the points to a hyperplane, consistency dictates that the stopping criterion for IHL-DPCP be governed by the sum over all points of the distance of each point to its assigned hyperplane (instead of the traditional sum of squares (Bradley and Mangasarian, 2000; Tseng, 2000)); in other words the global objective function minimized by IHL-DPCP is the same as that of Median K-Flats (Zhang et al., 2009).", "startOffset": 458, "endOffset": 478}, {"referenceID": 39, "context": "In Tsakiris and Vidal (2017b) we described four distinct methods for solving it, which we briefly review here.", "startOffset": 3, "endOffset": 30}, {"referenceID": 4, "context": "A second approach, called DPCP-IRLS, is to solve (9) using a standard Iteratively Reweighted LeastSquares (IRLS) technique ((Cand\u00e8s et al., 2008; Daubechies et al., 2010; Chartrand and Yin, 2008)) as in Algorithm 4.", "startOffset": 124, "endOffset": 195}, {"referenceID": 9, "context": "A second approach, called DPCP-IRLS, is to solve (9) using a standard Iteratively Reweighted LeastSquares (IRLS) technique ((Cand\u00e8s et al., 2008; Daubechies et al., 2010; Chartrand and Yin, 2008)) as in Algorithm 4.", "startOffset": 124, "endOffset": 195}, {"referenceID": 5, "context": "A second approach, called DPCP-IRLS, is to solve (9) using a standard Iteratively Reweighted LeastSquares (IRLS) technique ((Cand\u00e8s et al., 2008; Daubechies et al., 2010; Chartrand and Yin, 2008)) as in Algorithm 4.", "startOffset": 124, "endOffset": 195}, {"referenceID": 31, "context": "The first method, which was first proposed in Sp\u00e4th and Watson (1987), consists of solving the recursion of linear programs (11) using any standard solver, such as Gurobi (Gurobi Optimization, 2015); we refer to such a method as DPCP-r, standing for relaxed DPCP (see Algorithm 3).", "startOffset": 46, "endOffset": 70}, {"referenceID": 4, "context": "A second approach, called DPCP-IRLS, is to solve (9) using a standard Iteratively Reweighted LeastSquares (IRLS) technique ((Cand\u00e8s et al., 2008; Daubechies et al., 2010; Chartrand and Yin, 2008)) as in Algorithm 4. A third method, first proposed in Qu et al. (2014), is to solve (9) approximately by applying alternative minimization on its denoised version", "startOffset": 125, "endOffset": 267}, {"referenceID": 39, "context": "which is in turn solved via alternating minimization; see Tsakiris and Vidal (2017b) for details.", "startOffset": 58, "endOffset": 85}, {"referenceID": 34, "context": "In this section we explore various Iterative Hyperplane Clustering10 (IHL) algorithms using the benchmark dataset NYUdepthV2 Silberman et al. (2012). This dataset consists of 1449 RGBd data instances acquired using the Microsoft kinect sensor.", "startOffset": 125, "endOffset": 149}, {"referenceID": 36, "context": ", bn that the algorithm produced to minimize a Conditional-Random-Field (Sutton and McCallum, 2006) type of energy function, given by", "startOffset": 72, "endOffset": 99}, {"referenceID": 2, "context": "The minimization of the energy function is done via GraphCuts (Boykov et al., 2001).", "startOffset": 62, "endOffset": 83}], "year": 2017, "abstractText": "State-of-the-art methods for clustering data drawn from a union of subspaces are based on sparse and low-rank representation theory. Existing results guaranteeing the correctness of such methods require the dimension of the subspaces to be small relative to the dimension of the ambient space. When this assumption is violated, as is, for example, in the case of hyperplanes, existing methods are either computationally too intense (e.g., algebraic methods) or lack theoretical support (e.g., K-hyperplanes or RANSAC). The main theoretical contribution of this paper is to extend the theoretical analysis of a recently proposed single subspace learning algorithm, called Dual Principal Component Pursuit (DPCP), to the case where the data are drawn from of a union of hyperplanes. To gain insight into the expected properties of the non-convex `1 problem associated with DPCP (discrete problem), we develop a geometric analysis of a closely related continuous optimization problem. Then transferring this analysis to the discrete problem, our results state that as long as the hyperplanes are sufficiently separated, the dominant hyperplane is sufficiently dominant and the points are uniformly distributed (in a deterministic sense) inside their associated hyperplanes, then the non-convex DPCP problem has a unique (up to sign) global solution, equal to the normal vector of the dominant hyperplane. This suggests a sequential hyperplane learning algorithm, which first learns the dominant hyperplane by applying DPCP to the data. In order to avoid hard thresholding of the points which is sensitive to the choice of the thresholding parameter, all points are weighted according to their distance to that hyperplane, and a second hyperplane is computed by applying DPCP to the weighted data, and so on. Experiments on corrupted synthetic data show that this DPCP-based sequential algorithm dramatically improves over similar sequential algorithms, which learn the dominant hyperplane via state-of-the-art single subspace learning methods (e.g., with RANSAC or REAPER). Finally, 3D plane clustering experiments on real 3D point clouds show that a K-Hyperplanes DPCP-based scheme, which computes the normal vector of each cluster via DPCP, instead of the classic SVD, is very competitive to state-of-the-art approaches (e.g., RANSAC or SVD-based K-Hyperplanes).", "creator": "LaTeX with hyperref package"}}}