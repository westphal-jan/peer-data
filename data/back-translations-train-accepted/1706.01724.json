{"id": "1706.01724", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2017", "title": "Deep Latent Dirichlet Allocation with Topic-Layer-Adaptive Stochastic Gradient Riemannian MCMC", "abstract": "It is challenging to develop stochastic gradient based scalable inference for deep discrete latent variable models (LVMs), due to the difficulties in not only computing the gradients, but also adapting the step sizes to different latent factors and hidden layers. For the Poisson gamma belief network (PGBN), a recently proposed deep discrete LVM, we derive an alternative representation that is referred to as deep latent Dirichlet allocation (DLDA). Exploiting data augmentation and marginalization techniques, we derive a block-diagonal Fisher information matrix and its inverse for the simplex-constrained global model parameters of DLDA. Exploiting that Fisher information matrix with stochastic gradient MCMC, we present topic-layer-adaptive stochastic gradient Riemannian (TLASGR) MCMC that jointly learns simplex-constrained global parameters across all layers and topics, with topic and layer specific learning rates. State-of-the-art results are demonstrated on big data sets.", "histories": [["v1", "Tue, 6 Jun 2017 12:15:42 GMT  (1176kb,D)", "http://arxiv.org/abs/1706.01724v1", "Appearing in ICML 2017"]], "COMMENTS": "Appearing in ICML 2017", "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.CO", "authors": ["yulai cong", "bo chen", "hongwei liu", "mingyuan zhou"], "accepted": true, "id": "1706.01724"}, "pdf": {"name": "1706.01724.pdf", "metadata": {"source": "CRF", "title": "Deep Latent Dirichlet Allocation with Topic-Layer-Adaptive Stochastic Gradient Riemannian MCMC", "authors": ["Yulai Cong", "Bo Chen", "Hongwei Liu", "Mingyuan Zhou"], "emails": ["<bchen@mail.xidian.edu.cn>,", "<mingyuan.zhou@mccombs.utexas.edu>."], "sections": [{"heading": null, "text": "The increasing amount and complexity of data requires large capacity models, such as deep discrete latent variable models (LVMs) for unmonitored data analysis (Hinton et al., 2006; Bengio et al., 2007; Srivastava et al., 2013; Ranganath et al., 2015; Patterson et al., 2013; Ma et al., 2014; Ma et al., 2015; Ma et al., most in-depth LVMs., 1National Laboratory of Radar Signal Processing, Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi'an, China. 2McCombs School of Business, The University of Texas at Austin, TX 78712, USA."}], "references": [{"title": "Natural gradient works efficiently in learning", "author": ["S. Amari"], "venue": "Neural Computation,", "citeRegEx": "Amari,? \\Q1998\\E", "shortCiteRegEx": "Amari", "year": 1998}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "In NIPS, pp", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Stochastic gradient Hamiltonian Monte Carlo", "author": ["T. Chen", "E.B. Fox", "C. Guestrin"], "venue": "In ICML, pp", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Fast simulation of hyperplane-truncated multivariate normal distributions", "author": ["Y. Cong", "B. Chen", "M. Zhou"], "venue": "Bayesian Analysis Advance Publication,", "citeRegEx": "Cong et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Cong et al\\.", "year": 2017}, {"title": "Bayesian sampling using stochastic gradient thermostats", "author": ["N. Ding", "Y. Fang", "R. Babbush", "C. Chen", "R.D. Skeel", "H. Neven"], "venue": "In NIPS,", "citeRegEx": "Ding et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "JMLR, 12(Jul):2121\u20132159,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Scalable deep Poisson factor analysis for topic modeling", "author": ["Z. Gan", "C. Chen", "R. Henao", "D. Carlson", "L. Carin"], "venue": "In ICML,", "citeRegEx": "Gan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gan et al\\.", "year": 2015}, {"title": "Scaling up natural gradient by sparsely factorizing the inverse Fisher matrix", "author": ["R.B. Grosse", "R. Salakhutdinov"], "venue": "In ICML,", "citeRegEx": "Grosse and Salakhutdinov,? \\Q2015\\E", "shortCiteRegEx": "Grosse and Salakhutdinov", "year": 2015}, {"title": "Deep Poisson factor modeling", "author": ["R. Henao", "Z. Gan", "J. Lu", "L. Carin"], "venue": "In NIPS, pp", "citeRegEx": "Henao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Henao et al\\.", "year": 2015}, {"title": "Replicated softmax: an undirected topic model", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "In NIPS, pp", "citeRegEx": "Hinton and Salakhutdinov,? \\Q2009\\E", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2009}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Online learning for latent Dirichlet allocation", "author": ["M.D. Hoffman", "F.R. Bach", "D.M. Blei"], "venue": "In NIPS,", "citeRegEx": "Hoffman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "Kingma and Ba,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Auto-encoding variational Bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "A neural autoregressive topic model", "author": ["H. Larochelle", "S. Lauly"], "venue": "In NIPS,", "citeRegEx": "Larochelle and Lauly,? \\Q2012\\E", "shortCiteRegEx": "Larochelle and Lauly", "year": 2012}, {"title": "Preconditioned stochastic gradient Langevin dynamics for deep neural networks", "author": ["C. Li", "C. Chen", "D. Carlson", "L. Carin"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Pachinko allocation: DAGstructured mixture models of topic correlations", "author": ["W. Li", "A. McCallum"], "venue": "In ICML, pp", "citeRegEx": "Li and McCallum,? \\Q2006\\E", "shortCiteRegEx": "Li and McCallum", "year": 2006}, {"title": "A complete recipe for stochastic gradient MCMC", "author": ["Y. Ma", "T. Chen", "E. Fox"], "venue": "In NIPS, pp", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Neural variational inference for text processing", "author": ["Y. Miao", "L. Yu", "P. Blunsom"], "venue": "In ICML,", "citeRegEx": "Miao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2016}, {"title": "Sparse stochastic inference for latent Dirichlet allocation", "author": ["D. Mimno", "M.D. Hoffman", "D.M. Blei"], "venue": "In ICML, pp", "citeRegEx": "Mimno et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mimno et al\\.", "year": 2012}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": null, "citeRegEx": "Mnih and Gregor,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor", "year": 2014}, {"title": "MCMC using Hamiltonian dynamics", "author": ["Neal", "R. M"], "venue": "Handbook of Markov Chain Monte Carlo,", "citeRegEx": "Neal and M,? \\Q2011\\E", "shortCiteRegEx": "Neal and M", "year": 2011}, {"title": "The discrete infinite logistic normal distribution for mixed-membership modeling", "author": ["J. Paisley", "C. Wang", "D. Blei"], "venue": "In AISTATS,", "citeRegEx": "Paisley et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Paisley et al\\.", "year": 2011}, {"title": "Revisiting natural gradient for deep networks", "author": ["R. Pascanu", "Y. Bengio"], "venue": null, "citeRegEx": "Pascanu and Bengio,? \\Q2013\\E", "shortCiteRegEx": "Pascanu and Bengio", "year": 2013}, {"title": "Stochastic gradient Riemannian Langevin dynamics on the probability simplex", "author": ["S. Patterson", "Y.W. Teh"], "venue": "In NIPS, pp", "citeRegEx": "Patterson and Teh,? \\Q2013\\E", "shortCiteRegEx": "Patterson and Teh", "year": 2013}, {"title": "A Bayesian nonparametric approach to image superresolution", "author": ["G. Polatkan", "M. Zhou", "L. Carin", "D. Blei", "I. Daubechies"], "venue": null, "citeRegEx": "Polatkan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Polatkan et al\\.", "year": 2015}, {"title": "Black box variational inference", "author": ["R. Ranganath", "S. Gerrish", "D.M. Blei"], "venue": "In AISTATS,", "citeRegEx": "Ranganath et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2014}, {"title": "Deep exponential families", "author": ["R. Ranganath", "L. Tang", "L. Charlin", "D.M. Blei"], "venue": "In AISTATS,", "citeRegEx": "Ranganath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In AISTATS,", "citeRegEx": "Salakhutdinov and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2009}, {"title": "Poisson\u2013gamma dynamical systems", "author": ["A. Schein", "M. Zhou", "H. Wallach"], "venue": "In NIPS,", "citeRegEx": "Schein et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schein et al\\.", "year": 2016}, {"title": "Stochastic quasi-Newton Langevin Monte Carlo", "author": ["U. Simsekli", "R. Badeau", "A.T. Cemgil", "Richard"], "venue": "In ICML,", "citeRegEx": "Simsekli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Simsekli et al\\.", "year": 2016}, {"title": "Learning sigmoid belief networks via Monte Carlo expectation maximization", "author": ["Z. Song", "R. Henao", "D. Carlson", "L. Carin"], "venue": "In AISTATS,", "citeRegEx": "Song et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Song et al\\.", "year": 2016}, {"title": "Modeling documents with deep Boltzmann machines", "author": ["N. Srivastava", "R.R. Salakhutdinov", "G.E. Hinton"], "venue": "In UAI,", "citeRegEx": "Srivastava et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2013}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural networks for machine learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Evaluation methods for topic models", "author": ["H.M. Wallach", "I. Murray", "R. Salakhutdinov", "D. Mimno"], "venue": "In ICML,", "citeRegEx": "Wallach et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wallach et al\\.", "year": 2009}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["M. Welling", "Teh", "Y.-W"], "venue": "In ICML, pp", "citeRegEx": "Welling et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2011}, {"title": "The IBP compound Dirichlet process and its application to focused topic modeling", "author": ["S. Williamson", "C. Wang", "K.A. Heller", "D.M. Blei"], "venue": "In ICML,", "citeRegEx": "Williamson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Williamson et al\\.", "year": 2010}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": null, "citeRegEx": "Zeiler,? \\Q2012\\E", "shortCiteRegEx": "Zeiler", "year": 2012}, {"title": "Negative binomial process count and mixture modeling", "author": ["M. Zhou", "L. Carin"], "venue": "PAMI, 37(2):307\u2013320,", "citeRegEx": "Zhou and Carin,? \\Q2015\\E", "shortCiteRegEx": "Zhou and Carin", "year": 2015}, {"title": "Betanegative binomial process and Poisson factor analysis", "author": ["M. Zhou", "L. Hannah", "D.B. Dunson", "L. Carin"], "venue": "In AISTATS,", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}, {"title": "Augmentable gamma belief networks", "author": ["M. Zhou", "Y. Cong", "B. Chen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "Priors for random count matrices derived from a family of negative binomial processes", "author": ["M. Zhou", "O. Padilla", "J.G. Scott"], "venue": "J. Amer. Statist. Assoc.,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "1/Kl for the lth layer, on extracting multilayer representations/topics from both the RCV1 and Wiki corpora", "author": ["Zhou"], "venue": null, "citeRegEx": "Zhou,? \\Q2016\\E", "shortCiteRegEx": "Zhou", "year": 2016}, {"title": "2016a), we plot 54 example topics of layer one in Figure 4, the top 30 topics of layer two in Figure 5, and the top 30 topics of layer three", "author": ["Zhou"], "venue": null, "citeRegEx": "Zhou,? \\Q2016\\E", "shortCiteRegEx": "Zhou", "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": "capacity models, such as deep discrete latent variable models (LVMs) for unsupervised data analysis (Hinton et al., 2006; Bengio et al., 2007; Srivastava et al., 2013; Ranganath et al., 2015; Zhou et al., 2016a), and scalable inference methods, such as stochastic gradient Markov chain Monte Carlo", "startOffset": 100, "endOffset": 211}, {"referenceID": 1, "context": "capacity models, such as deep discrete latent variable models (LVMs) for unsupervised data analysis (Hinton et al., 2006; Bengio et al., 2007; Srivastava et al., 2013; Ranganath et al., 2015; Zhou et al., 2016a), and scalable inference methods, such as stochastic gradient Markov chain Monte Carlo", "startOffset": 100, "endOffset": 211}, {"referenceID": 33, "context": "capacity models, such as deep discrete latent variable models (LVMs) for unsupervised data analysis (Hinton et al., 2006; Bengio et al., 2007; Srivastava et al., 2013; Ranganath et al., 2015; Zhou et al., 2016a), and scalable inference methods, such as stochastic gradient Markov chain Monte Carlo", "startOffset": 100, "endOffset": 211}, {"referenceID": 27, "context": "capacity models, such as deep discrete latent variable models (LVMs) for unsupervised data analysis (Hinton et al., 2006; Bengio et al., 2007; Srivastava et al., 2013; Ranganath et al., 2015; Zhou et al., 2016a), and scalable inference methods, such as stochastic gradient Markov chain Monte Carlo", "startOffset": 100, "endOffset": 211}, {"referenceID": 17, "context": "(SG-MCMC) that provides posterior samples in a non-batch learning setting (Welling & Teh, 2011; Patterson & Teh, 2013; Ma et al., 2015).", "startOffset": 74, "endOffset": 135}, {"referenceID": 10, "context": "such as deep belief network (DBN) (Hinton et al., 2006)", "startOffset": 34, "endOffset": 55}, {"referenceID": 1, "context": "manner (Bengio et al., 2007).", "startOffset": 7, "endOffset": 28}, {"referenceID": 17, "context": "Following a general framework for SG-MCMC (Ma et al., 2015), the block diagonal structure of the FIM of DLDA makes it be easily", "startOffset": 42, "endOffset": 59}, {"referenceID": 0, "context": "inverted to precondition the mini-batch based noisy gradients to exploit the second-order local curvature information, leading to topic-layer-adaptive step sizes based on the Riemannian manifold and the same asymptotic performance as a natural gradient based batch-learning algorithm (Amari, 1998; Pascanu & Bengio, 2013).", "startOffset": 284, "endOffset": 321}, {"referenceID": 17, "context": "work (Ma et al., 2015), we present topic-layer-adaptive", "startOffset": 5, "endOffset": 22}, {"referenceID": 43, "context": "of Zhou & Carin (2015) with a greedy layer-wise training", "startOffset": 3, "endOffset": 23}, {"referenceID": 40, "context": "Extensive experiments in Zhou et al. (2016a) show", "startOffset": 25, "endOffset": 45}, {"referenceID": 17, "context": "As in Theorem 1 of Ma et al. (2015), p (z |X ) is the stationary distribution of the dynamics defined by the stochastic differential equation (SDE) dz = f (z) dt + \u221a 2D (z)dW (t), if the deterministic drift f (z) is restricted to the form", "startOffset": 19, "endOffset": 36}, {"referenceID": 17, "context": "As shown in Ma et al. (2015), stochastic gradient Riemannian Langevin dynamics (SGRLD) of Patterson & Teh (2013) is a special case with D (z) = G(z)\u22121,Q (z) = 0, B\u0302t = 0, where G (z) denotes the Fisher information matrix (FIM).", "startOffset": 12, "endOffset": 29}, {"referenceID": 17, "context": "As shown in Ma et al. (2015), stochastic gradient Riemannian Langevin dynamics (SGRLD) of Patterson & Teh (2013) is a special case with D (z) = G(z)\u22121,Q (z) = 0, B\u0302t = 0, where G (z) denotes the Fisher information matrix (FIM).", "startOffset": 12, "endOffset": 113}, {"referenceID": 40, "context": "by letting K \u2192\u221e (Zhou et al., 2016b). When L = 1, the PGBN whose (rk,\u03c6k) are the points of a gamma process reduces to the gamma-negative binomial process PFA of Zhou & Carin (2015), whose alternative representation is provided in Corollary D.", "startOffset": 17, "endOffset": 181}, {"referenceID": 30, "context": "re-express the PGBN as DLDA is related to how Schein et al. (2016) re-express their Poisson\u2013gamma dynamic systems", "startOffset": 46, "endOffset": 67}, {"referenceID": 1, "context": "a greedy layer-wise manner (Bengio et al., 2007), which", "startOffset": 27, "endOffset": 48}, {"referenceID": 2, "context": "(2015), including as special examples stochastic gradient Hamiltonian Monte Carlo in Chen et al. (2014) and stochastic gradient thermostats in Ding et al.", "startOffset": 85, "endOffset": 104}, {"referenceID": 2, "context": "(2015), including as special examples stochastic gradient Hamiltonian Monte Carlo in Chen et al. (2014) and stochastic gradient thermostats in Ding et al. (2014), may", "startOffset": 85, "endOffset": 162}, {"referenceID": 3, "context": "using Theorem 2 of Cong et al. (2017), the special struc-", "startOffset": 19, "endOffset": 38}, {"referenceID": 3, "context": "can be found in Examples 1-3 of Cong et al. (2017).", "startOffset": 32, "endOffset": 51}, {"referenceID": 25, "context": "We update them using annealed weighting (Polatkan et al., 2015) as", "startOffset": 40, "endOffset": 63}, {"referenceID": 40, "context": "analysis (PFA) (Zhou et al., 2012) are equipped with scal-", "startOffset": 15, "endOffset": 34}, {"referenceID": 11, "context": "(Hoffman et al., 2010; Mimno et al., 2012) and SGRLD", "startOffset": 0, "endOffset": 42}, {"referenceID": 19, "context": "(Hoffman et al., 2010; Mimno et al., 2012) and SGRLD", "startOffset": 0, "endOffset": 42}, {"referenceID": 8, "context": "(2015) and Henao et al. (2015) are proposed to generalize", "startOffset": 11, "endOffset": 31}, {"referenceID": 26, "context": "Ranganath et al. (2015) propose deep exponential family (DEF), which differs from the PGBN in connecting adjacent", "startOffset": 0, "endOffset": 24}, {"referenceID": 26, "context": "layers via the gamma rate parameters and using black-box variational inference (BBVI) (Ranganath et al., 2014).", "startOffset": 86, "endOffset": 110}, {"referenceID": 10, "context": "Some commonly used neural networks, such as deep belief network (DBN) (Hinton et al., 2006) and deep Boltzmann machines (DBM) (Salakhutdinov & Hinton, 2009), have also been modified for text analysis (Hinton & Salakhutdinov, 2009; Larochelle & Lauly, 2012; Srivastava et al.", "startOffset": 70, "endOffset": 91}, {"referenceID": 33, "context": ", 2006) and deep Boltzmann machines (DBM) (Salakhutdinov & Hinton, 2009), have also been modified for text analysis (Hinton & Salakhutdinov, 2009; Larochelle & Lauly, 2012; Srivastava et al., 2013).", "startOffset": 116, "endOffset": 197}, {"referenceID": 38, "context": ", 2011), Adadelta (Zeiler, 2012), Adam (Kingma & Ba,", "startOffset": 18, "endOffset": 32}, {"referenceID": 15, "context": "ple, Li et al. (2016) show that preconditioning the gradients", "startOffset": 5, "endOffset": 22}, {"referenceID": 11, "context": "Wiki consists of 10 million documents randomly downloaded from Wikipedia using the scripts provided in Hoffman et al. (2010); as in Hoffman", "startOffset": 103, "endOffset": 125}, {"referenceID": 6, "context": "(2010), Gan et al. (2015), and Henao et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 6, "context": "(2010), Gan et al. (2015), and Henao et al. (2015), we use a vocabulary with 7,702 words and randomly select", "startOffset": 8, "endOffset": 51}, {"referenceID": 6, "context": "To make a fair comparison, these corpora, including the training/testing partitions, are set to be the same as those in Gan et al. (2015) and Henao et al.", "startOffset": 120, "endOffset": 138}, {"referenceID": 6, "context": "To make a fair comparison, these corpora, including the training/testing partitions, are set to be the same as those in Gan et al. (2015) and Henao et al. (2015). To be consistent with the settings of Gan et al.", "startOffset": 120, "endOffset": 162}, {"referenceID": 6, "context": "To make a fair comparison, these corpora, including the training/testing partitions, are set to be the same as those in Gan et al. (2015) and Henao et al. (2015). To be consistent with the settings of Gan et al. (2015) and Henao et al.", "startOffset": 120, "endOffset": 219}, {"referenceID": 6, "context": "To make a fair comparison, these corpora, including the training/testing partitions, are set to be the same as those in Gan et al. (2015) and Henao et al. (2015). To be consistent with the settings of Gan et al. (2015) and Henao et al. (2015), no precautions are taken in the scripts for Wikipedia to prevent a testing document from", "startOffset": 120, "endOffset": 243}, {"referenceID": 34, "context": ", in Wallach et al. (2009), Paisley et al.", "startOffset": 5, "endOffset": 27}, {"referenceID": 22, "context": "(2009), Paisley et al. (2011), and Zhou & Carin (2015).", "startOffset": 8, "endOffset": 30}, {"referenceID": 22, "context": "(2009), Paisley et al. (2011), and Zhou & Carin (2015). Although a good measure", "startOffset": 8, "endOffset": 55}, {"referenceID": 37, "context": "cused topic model (FTM) of Williamson et al. (2010), repli-", "startOffset": 27, "endOffset": 52}, {"referenceID": 6, "context": "(2015), DPFA of Gan et al. (2015), and DPFM of", "startOffset": 16, "endOffset": 34}, {"referenceID": 6, "context": "results are taken from Gan et al. (2015) and Henao et al.", "startOffset": 23, "endOffset": 41}, {"referenceID": 40, "context": "Note \u03b7 and Kl are set similar to that of DPFM for fair comparisons, while other hyperparameters follow Zhou et al. (2016a).", "startOffset": 103, "endOffset": 123}, {"referenceID": 8, "context": "(2015) and Henao et al. (2015). Note that for Wiki, DPFM", "startOffset": 11, "endOffset": 31}, {"referenceID": 40, "context": "4) Gibbs: the upward-downward Gibbs sampler in Zhou et al. (2016a).", "startOffset": 47, "endOffset": 67}], "year": 2017, "abstractText": "It is challenging to develop stochastic gradient based scalable inference for deep discrete latent variable models (LVMs), due to the difficulties in not only computing the gradients, but also adapting the step sizes to different latent factors and hidden layers. For the Poisson gamma belief network (PGBN), a recently proposed deep discrete LVM, we derive an alternative representation that is referred to as deep latent Dirichlet allocation (DLDA). Exploiting data augmentation and marginalization techniques, we derive a block-diagonal Fisher information matrix and its inverse for the simplex-constrained global model parameters of DLDA. Exploiting that Fisher information matrix with stochastic gradient MCMC, we present topic-layer-adaptive stochastic gradient Riemannian (TLASGR) MCMC that jointly learns simplex-constrained global parameters across all layers and topics, with topic and layer specific learning rates. State-of-the-art results are demonstrated on big data sets.", "creator": "LaTeX with hyperref package"}}}