{"id": "1706.01847", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2017", "title": "Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext", "abstract": "We consider the problem of learning general-purpose, paraphrastic sentence embeddings in the setting of Wieting et al. (2016b). We use neural machine translation to generate sentential paraphrases via back-translation of bilingual sentence pairs. We evaluate the paraphrase pairs by their ability to serve as training data for learning paraphrastic sentence embeddings. We find that the data quality is stronger than prior work based on bitext and on par with manually-written English paraphrase pairs, with the advantage that our approach can scale up to generate large training sets for many languages and domains. We experiment with several language pairs and data sources, and develop a variety of data filtering techniques. In the process, we explore how neural machine translation output differs from human-written sentences, finding clear differences in length, the amount of repetition, and the use of rare words.", "histories": [["v1", "Tue, 6 Jun 2017 16:36:41 GMT  (34kb,D)", "http://arxiv.org/abs/1706.01847v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["john wieting", "jonathan mallinson", "kevin gimpel"], "accepted": true, "id": "1706.01847"}, "pdf": {"name": "1706.01847.pdf", "metadata": {"source": "CRF", "title": "Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext", "authors": ["John Wieting", "Jonathan Mallinson", "Kevin Gimpel"], "emails": ["jwieting@ttic.edu,", "kgimpel@ttic.edu,", "j.mallinson@ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "This year, we have reached the point where we will be able to retaliate, \"he said in an interview with the German Press Agency.\" We have never hesitated so long, \"he said,\" but we have managed to make it as good as possible. \""}, {"heading": "2 Related Work", "text": "We describe the related work in the area of learning, embedding, work and the discovery of paraphrases DB 2001 and finally the work on the use of neural machine translations for embedding learners. Our learning and evaluation method is the same as that of Wieting et al. (2016b) and Wieting et al. (2016a), in which the aim is to learn paraphrastic sentence embedding concepts, which can be used for downstream tasks. They trained models on PPDB and evaluated them with a series of semantic textual similarities (STS) tasks and overarching semantic tasks. Others have begun to consider these embedding concepts as well (Arora et al al. 2017). Other work in the field of learning concepts of general sentence embedding concepts architectural concepts-concepts architectural-concepts-concepts-concepts architectural-concepts-concepts-concepts-concepts-concepts-architecture-concepts-concepts-concepts-concepts we have used autoencoders (Socher et al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al 2016, al al al al al al al al)."}, {"heading": "3 Neural Machine Translation", "text": "This year it has come to the point where we will be able to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We are able to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said."}, {"heading": "4 Models and Training", "text": "Our goal is to compare our paraphrase dataset with other datasets by using each to train sentence embedding, to record the models and the learning procedure. Therefore, we select models and a loss function from previous work (Wieting et al., 2016b; Wieting and Gimpel, 2017)."}, {"heading": "4.1 Models", "text": "We want to embed a word sequence s in a fixed-length vector. We call the tth word in s st, and we call its word embedding by xt. In this work, we focus on two models. The first model, which we call AVG, simply forms the mean of embedding xt of all words in s. The only parameters learned in this model are those in the word embedding itself, which are stored in the word embedding matrix Ww. This model was found by Wieting et al. (2016b) to perform very strongly semantic similarity tasks. The second model, the GATED RECURRENT AVERAGING NETWORK (GRAN and Gimpel, 2017), combines the advantages of AVG and long-term memory (LSTM) to perform recurrent neural networks (Hochreiter and Schmidhuber, 1997). It first uses an LSTM to generate a hidden vector, one step at a time, which becomes a Gb-1 for each word."}, {"heading": "4.2 Training", "text": "We follow the training sequence of Wieting et al. (2015) and Wieting et al. (2016b). The training data are a set of S paraphratic pairs < s1, s2 > and we optimize a margin-based loss: min Wc, Ww1 | S | (\u2211 < s1, s2 > Ps max (0, \u03b4 \u2212 cos (g (s1), g (s2))) + cos (g (s1), g (t1)))) + max (0, \u03b4 \u2212 cos (g (s1), g (s2) + cos (g (s2))))) + \u03bbc Wc 2 + & w Wwinitial \u2212 Wwinitial \u2212 Ww 2where g is the model (AVG or GRAN), \u03b4 is the margin, \u0441ltc and \u0441w are regulation parameters, Wwinitial is the starting word of the matrix, and t1 and t2 are \"negative\" examples that we take from a mini set (1)."}, {"heading": "5 Experiments", "text": "We are now examining how best to use our generated paraphrase data to train universal paraphrase embeddings. We are looking at 10 data sources: Common Crawl (CC), Europarl (EP) and News Commentary (News) from all three language pairs, as well as the 109 French-English data (Giga). From each data source, we extract 150,000 reference / retranslation pairs, 100,000 of which we use to extract training data for our sentence embeddings models, and the remaining 50,000 are used as traction / validation / test data for the reference classification and language models described below."}, {"heading": "5.1 Evaluation", "text": "We evaluate the quality of a paraphrase dataset by using the experimental setting of Wieting et al. (2016b). We use the paraphrases as training data to create paraphratic sentence embeddings, to evaluate the embeddings of the SemEval semantic text similarity (STS) tasks from 2012 to 2015 (Agirre et al., 2012, 2013, 2014, 2015), the SemEval 2015 Twitter task (Xu et al., 2015), and the SemEval 2014 SICK Semantic Relatedness task (Marelli et al., 2014). Considering two sets, the goal of the STS tasks is to predict their similarity on a scale of 0-5, where 0 indicates that the sentences are on different topics and 5 indicates that they are fully equivalent. As our test set is, we report on the average r of these 22 sentence similarity tasks. As development data, we use the STS tasks from 2016 (Agirre et al, the values are above the average of 2016)."}, {"heading": "5.2 Experimental Setup", "text": "For a fair comparison between different data sets and data set filtering methods described below, we only use 24,000 training examples for almost all experiments. Different filter methods produce different amounts of training data, and with 24,000 examples we can keep the amount of training data constant across filter methods. It also allows us to complete these several thousand experiments in a reasonable time. In Section 5.8 we discuss experiments that scale up to larger amounts of training data. We use PARAGRAM-SL999 embedding (Wieting et al., 2015) to initialize the word embedding matrix (Ww) for both models. For all experiments, we fix the minibatch size to 100, w to 0, \u03bbc to 0, and the margin to 0.4. We train AVG for 20 epochs, and the GRAN for 3, as it is converted much faster."}, {"heading": "5.3 Dataset Comparison", "text": "We first compare data sets and sample 24,000 set pairs from each PPDB, SimpWiki and each of our NMT datasets. The only hyperparameter we set for this experiment is the holding period, which we tune based on our development set. Results will be shown in Table 4.We find that the NMT datasets are all effective as training data and outperform PPDB when using GRAN in all cases. There are exceptions to the use of AVG, for which PPDB is quite strong. This makes sense because AVG is not sensitive to the word sequence, so the fragments in the PPDB do not cause any problems. However, when using the GRAN, which responds to the word order, the NMT data is consistently better than PPDB. They often exceed the performance of training on the SimpWiki data, which consists exclusively of human written records."}, {"heading": "5.4 Filtering Methods", "text": "Since we can access so much more NMT data than SimpWiki (which is limited to less than 200k sentence pairs), we will next experiment with several approaches to filtering the NMT data. First, we will look at filtering by the length described in Section 5.5. Then, we will look at filtering by several quality measures designed to find more natural and high-quality translations, as described in Section 5.6. Finally, we will look at several diversity measures. By diversity, we mean a measure of the lexical and syntactical difference between the reference and its paraphrase. We will describe these experiments in Section 5.7. We note that these filter methods are not all mutually exclusive and cannot be combined, although in this paper we will experiment with each and leave the combination to future work."}, {"heading": "5.5 Length Filtering", "text": "First, we look at filtering candidate set pairs by length, i.e. the number of tokens in the translation. The tunable parameters are the upper and lower limits of the displayed translation lengths. We are experimenting with a division of the length ranges, which results in Table 5. These results are averages of all language pairs and data sources of the training data for each displayed length range. We find it best to select NMT data where the translations have between 0 and 10 tokens, with performance decreasing with increasing sentence length. This applies to both the GRAN and AVG models. We filter the same data for SimpWiki, although the trend is not nearly as strong. This may be because the quality of the machine translation decreases with increasing sentence length. This trend appears even though the data sets with higher ranges have more tokens of training data, as only the number of training set pairs is kept constant over configurations."}, {"heading": "5.6 Quality Filtering", "text": "We also consider filtering using several measures of the \"quality\" of the back translation: \u2022 Translation cost: We use the cost (negative log probability) of the translation from the NMT system divided by the number of tokens in the translation. \u2022 Language model: We build a separate language model for each language / data pair of 40,000 references, separated from the 100,000 used for mining data. Due to the small data size, we train a 3 gram language model and use the KenLM toolkit (Heafield, 2011). \u2022 Reference / Translation Classification: Wetrain binary classifiers to predict whether a given sentence is a reference or a translation (described in Section 5.6.1). We use the probability of being a reference model as the basis for filtering. For the translation costs, we agree the upper limit of costs over the range [0.2, 1] with increments of 0.1."}, {"heading": "5.6.1 Reference/Translation Classification", "text": "In fact, we are able to set out in search of new paths that will lead us into the future."}, {"heading": "5.7 Diversity Filtering", "text": "We consider several filter criteria based on measures that favor certain disparities between the reference and its back translation: \u2022 n-gram overlap: Our n-gram overlap is calculated by counting n-gram overlaps of a given order both in the reference and in the translation, and then dividing the number of common n-gram overlaps by the total number of n-gram overlaps in the reference or translation, whichever is less. \u2022 BLEU rating: We use a smoothed sentence overlap BLEU variant by Nakov et al. (2012) that uses a smoothing for all n-gram lengths and also does not significantly vary the word length of overlaps and references. For both methods, the adjustable hyperparameters are the upper and lower limits of the above values."}, {"heading": "5.8 Scaling Up", "text": "Unlike the SimpWiki data, which is of course limited and only available for English, we can scale our approach. As we use data on which the NMT systems were trained and perform back translations, we can easily create large training sets of paraphratic sentence pairs for many languages and data ranges, which are only possible due to the availability of BitEdit. To test this, we took the matched filter sets and language / data pairs (only according to our development data set) and trained them on more data. These were the CC-CS data for GRAN and CC-DE for AVG models. We trained them with the same amount of sentence pairs from SimpWiki.6 We also compare with PPDB XL, and since the PPDB per example has fewer tokens, we use 6Since the CC-CS data was the smallest data set used to train the CS-NMT system for AVG models (see Table 3), and limited, we only used 100,000 pairs for the SimpWiki data for the GRA9.7K experiment, so that we have enough wiki data to use in the Wiki at least as we did in the 167,000 Wiki experiment."}, {"heading": "6 Conclusion", "text": "We have shown how back translation can be an effective technique for generating paraphrases by using it to create paraphratic embedding that is at least equivalent to human-written paraphrase data sets and significantly exceeds PPDB. We have also shown that filtering can improve the generated paraphrase corpus, and we have explored a variety of filtering techniques, including identifying features that distinguish NMT output from human-written sentences. Future work will aim to improve this process by, for example, developing more complex filtering techniques."}, {"heading": "Acknowledgments", "text": "Resources from the Argonne Leadership Computing Facility, an entity of the DOE Office of Science User Facility supported under the DE-AC02-06CH11357 contract, were used for this research. We thank the developers of Theano (Theano Development Team, 2016) and NVIDIA Corporation for donating GPUs used in this research."}], "references": [{"title": "SemEval-2014 task 10: Multilingual semantic textual similarity", "author": ["Eneko Agirre", "Carmen Banea", "Claire Cardie", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo", "Rada Mihalcea", "German Rigau", "Janyce Wiebe."], "venue": "Proceedings of the", "citeRegEx": "Agirre et al\\.,? 2014", "shortCiteRegEx": "Agirre et al\\.", "year": 2014}, {"title": "Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation", "author": ["Eneko Agirre", "Carmen Banea", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Rada Mihalcea", "German Rigau", "Janyce Wiebe."], "venue": "Proceedings of SemEval", "citeRegEx": "Agirre et al\\.,? 2016", "shortCiteRegEx": "Agirre et al\\.", "year": 2016}, {"title": "SEM 2013 shared task: Semantic textual similarity", "author": ["Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor GonzalezAgirre", "Weiwei Guo."], "venue": "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main", "citeRegEx": "Agirre et al\\.,? 2013", "shortCiteRegEx": "Agirre et al\\.", "year": 2013}, {"title": "SemEval-2012 task 6: A pilot on semantic textual similarity", "author": ["Eneko Agirre", "Mona Diab", "Daniel Cer", "Aitor Gonzalez-Agirre."], "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the", "citeRegEx": "Agirre et al\\.,? 2012", "shortCiteRegEx": "Agirre et al\\.", "year": 2012}, {"title": "A simple but tough-to-beat baseline for sentence embeddings", "author": ["Sanjeev Arora", "Yingyu Liang", "Tengyu Ma."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Arora et al\\.,? 2017", "shortCiteRegEx": "Arora et al\\.", "year": 2017}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Paraphrasing with bilingual parallel corpora", "author": ["Colin Bannard", "Chris Callison-Burch."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics.", "citeRegEx": "Bannard and Callison.Burch.,? 2005", "shortCiteRegEx": "Bannard and Callison.Burch.", "year": 2005}, {"title": "Extracting paraphrases from a parallel corpus", "author": ["Regina Barzilay", "Kathleen R McKeown."], "venue": "Proceedings of the 39th annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, pages 50\u201357.", "citeRegEx": "Barzilay and McKeown.,? 2001", "shortCiteRegEx": "Barzilay and McKeown.", "year": 2001}, {"title": "CzEng 1.6: Enlarged Czech-English Parallel Corpus with Processing Tools Dockered", "author": ["Ond\u0159ej Bojar", "Ond\u0159ej Du\u0161ek", "Tom Kocmi", "Jind\u0159ich Libovick\u00fd", "Michal Nov\u00e1k", "Martin Popel", "Roman Sudarikov", "Du\u0161an Vari\u0161"], "venue": null, "citeRegEx": "Bojar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2016}, {"title": "Text, Speech, and Dialogue: 19th International Conference, TSD 2016", "author": ["Ivan Kope\u010dek", "Karel Pala", "editors"], "venue": null, "citeRegEx": "Kope\u010dek et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kope\u010dek et al\\.", "year": 2016}, {"title": "Simple english wikipedia: a new text simplification task", "author": ["William Coster", "David Kauchak."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2. Asso-", "citeRegEx": "Coster and Kauchak.,? 2011", "shortCiteRegEx": "Coster and Kauchak.", "year": 2011}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["Bill Dolan", "Chris Quirk", "Chris Brockett."], "venue": "Proceedings of the 20th international conference on Computational Linguistics. Association for Compu-", "citeRegEx": "Dolan et al\\.,? 2004", "shortCiteRegEx": "Dolan et al\\.", "year": 2004}, {"title": "Automatically constructing a corpus of sentential paraphrases", "author": ["William B Dolan", "Chris Brockett."], "venue": "Proc. of IWP.", "citeRegEx": "Dolan and Brockett.,? 2005", "shortCiteRegEx": "Dolan and Brockett.", "year": 2005}, {"title": "Multiun: A multilingual corpus from united nation documents", "author": ["Andreas Eisele", "Yu Chen."], "venue": "LREC.", "citeRegEx": "Eisele and Chen.,? 2010", "shortCiteRegEx": "Eisele and Chen.", "year": 2010}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "The multilingual paraphrase database", "author": ["Juri Ganitkevitch", "Chris Callison-Burch."], "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC-2014).", "citeRegEx": "Ganitkevitch and Callison.Burch.,? 2014", "shortCiteRegEx": "Ganitkevitch and Callison.Burch.", "year": 2014}, {"title": "PPDB: The Paraphrase Database", "author": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch."], "venue": "Proceedings of HLT-NAACL.", "citeRegEx": "Ganitkevitch et al\\.,? 2013", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "KenLM: faster and smaller language model queries", "author": ["Kenneth Heafield."], "venue": "Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation. pages 187\u2013197.", "citeRegEx": "Heafield.,? 2011", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "Embedding word similarity with neural machine translation", "author": ["Felix Hill", "Kyunghyun Cho", "Sebastien Jean", "Coline Devin", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.6448 .", "citeRegEx": "Hill et al\\.,? 2014a", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Not all neural embeddings are born equal", "author": ["Felix Hill", "KyungHyun Cho", "Sebastien Jean", "Coline Devin", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1410.0718 .", "citeRegEx": "Hill et al\\.,? 2014b", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["Felix Hill", "Kyunghyun Cho", "Anna Korhonen."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8).", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S Zemel", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler."], "venue": "arXiv preprint arXiv:1506.06726 .", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn."], "venue": "MT summit. volume 5, pages 79\u201386.", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V. Le", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1405.4053 .", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Paraphrasing revisited with neural machine translation", "author": ["Jonathan Mallinson", "Rico Sennrich", "Mirella Lapata."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Pa-", "citeRegEx": "Mallinson et al\\.,? 2017", "shortCiteRegEx": "Mallinson et al\\.", "year": 2017}, {"title": "SemEval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Optimizing for sentence-level BLEU+1 yields short translations", "author": ["Preslav Nakov", "Francisco Guzman", "Stephan Vogel."], "venue": "Proceedings of COLING 2012. pages 1979\u20131994.", "citeRegEx": "Nakov et al\\.,? 2012", "shortCiteRegEx": "Nakov et al\\.", "year": 2012}, {"title": "Jointly optimizing word representations for lexical and sentential tasks with the c-phrase model", "author": ["Nghia The Pham", "Germ\u00e1n Kruszewski", "Angeliki Lazaridou", "Marco Baroni."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for", "citeRegEx": "Pham et al\\.,? 2015", "shortCiteRegEx": "Pham et al\\.", "year": 2015}, {"title": "Monolingual machine translation for paraphrase generation", "author": ["Chris Quirk", "Chris Brockett", "William Dolan."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Quirk et al\\.,? 2004", "shortCiteRegEx": "Quirk et al\\.", "year": 2004}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pages", "citeRegEx": "Sennrich et al\\.,? 2016a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pages 1715\u2013", "citeRegEx": "Sennrich et al\\.,? 2016b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H. Huang", "Jeffrey Pennington", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in Neural Information Processing Systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team."], "venue": "arXiv e-prints abs/1605.02688.", "citeRegEx": "Team.,? 2016", "shortCiteRegEx": "Team.", "year": 2016}, {"title": "Charagram: Embedding words and sentences via character n-grams", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Wieting et al\\.,? 2016a", "shortCiteRegEx": "Wieting et al\\.", "year": 2016}, {"title": "Towards universal paraphrastic sentence embeddings", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proceedings of International Conference on Learning Representations.", "citeRegEx": "Wieting et al\\.,? 2016b", "shortCiteRegEx": "Wieting et al\\.", "year": 2016}, {"title": "From paraphrase database to compositional paraphrase model and back", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu", "Dan Roth."], "venue": "Transactions of the ACL (TACL) .", "citeRegEx": "Wieting et al\\.,? 2015", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Revisiting recurrent networks for paraphrastic sentence embeddings", "author": ["John Wieting", "Kevin Gimpel."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Wieting and Gimpel.,? 2017", "shortCiteRegEx": "Wieting and Gimpel.", "year": 2017}, {"title": "SemEval-2015 task 1: Paraphrase and semantic similarity in Twitter (PIT)", "author": ["Wei Xu", "Chris Callison-Burch", "William B Dolan."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval).", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Extracting lexically divergent paraphrases from Twitter", "author": ["Wei Xu", "Alan Ritter", "Chris Callison-Burch", "William B. Dolan", "Yangfeng Ji."], "venue": "Transactions of the Association for Computational Linguistics 2:435\u2013448.", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "CoRR abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 37, "context": "We consider the problem of learning general-purpose, paraphrastic sentence embeddings in the setting of Wieting et al. (2016b). We use neural machine translation to generate sentential paraphrases", "startOffset": 104, "endOffset": 127}, {"referenceID": 37, "context": "Wieting et al. (2016b) developed paraphrastic sentence embeddings that are useful for semantic textual similarity tasks and can also be used as initialization for supervised semantic tasks.", "startOffset": 0, "endOffset": 23}, {"referenceID": 16, "context": "used the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013).", "startOffset": 36, "endOffset": 63}, {"referenceID": 15, "context": "Versions of PPDB have been released for several languages (Ganitkevitch and Callison-Burch, 2014).", "startOffset": 58, "endOffset": 97}, {"referenceID": 40, "context": "the fragmental nature of PPDB\u2019s pairs can be problematic, especially for recurrent networks (Wieting and Gimpel, 2017).", "startOffset": 92, "endOffset": 118}, {"referenceID": 10, "context": "Better performance can be achieved with a smaller set of sentence pairs derived from aligning Simple English and standard English Wikipedia (Coster and Kauchak, 2011).", "startOffset": 140, "endOffset": 166}, {"referenceID": 35, "context": "We turn to neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014; Sennrich et al., 2016a), which has matured recently to yield strong performance especially in terms of producing grammatical outputs.", "startOffset": 44, "endOffset": 115}, {"referenceID": 5, "context": "We turn to neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014; Sennrich et al., 2016a), which has matured recently to yield strong performance especially in terms of producing grammatical outputs.", "startOffset": 44, "endOffset": 115}, {"referenceID": 32, "context": "We turn to neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014; Sennrich et al., 2016a), which has matured recently to yield strong performance especially in terms of producing grammatical outputs.", "startOffset": 44, "endOffset": 115}, {"referenceID": 4, "context": "Others have begun to consider this setting as well (Arora et al., 2017).", "startOffset": 51, "endOffset": 71}, {"referenceID": 36, "context": "Our learning and evaluation setting is the same as that considered by Wieting et al. (2016b) and Wieting et al.", "startOffset": 70, "endOffset": 93}, {"referenceID": 36, "context": "Our learning and evaluation setting is the same as that considered by Wieting et al. (2016b) and Wieting et al. (2016a), in which the goal is to learn paraphrastic sentence embeddings that can be used for downstream tasks.", "startOffset": 70, "endOffset": 120}, {"referenceID": 24, "context": ", 2016), encoder-decoder architectures (Kiros et al., 2015), or other learning frameworks (Le and Mikolov, 2014; Pham et al.", "startOffset": 39, "endOffset": 59}, {"referenceID": 26, "context": ", 2015), or other learning frameworks (Le and Mikolov, 2014; Pham et al., 2015).", "startOffset": 38, "endOffset": 79}, {"referenceID": 30, "context": ", 2015), or other learning frameworks (Le and Mikolov, 2014; Pham et al., 2015).", "startOffset": 38, "endOffset": 79}, {"referenceID": 18, "context": ", 2011; Hill et al., 2016), encoder-decoder architectures (Kiros et al., 2015), or other learning frameworks (Le and Mikolov, 2014; Pham et al., 2015). Wieting et al. (2016b) and Hill et al.", "startOffset": 8, "endOffset": 175}, {"referenceID": 18, "context": ", 2011; Hill et al., 2016), encoder-decoder architectures (Kiros et al., 2015), or other learning frameworks (Le and Mikolov, 2014; Pham et al., 2015). Wieting et al. (2016b) and Hill et al. (2016) provide many empirical comparisons to this prior work.", "startOffset": 8, "endOffset": 198}, {"referenceID": 7, "context": "There is a rich history of research in generating or finding naturally-occurring sentential paraphrases (Barzilay and McKeown, 2001; Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004; Coster and Kauchak, 2011; Xu et al., 2014, 2015).", "startOffset": 104, "endOffset": 247}, {"referenceID": 11, "context": "There is a rich history of research in generating or finding naturally-occurring sentential paraphrases (Barzilay and McKeown, 2001; Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004; Coster and Kauchak, 2011; Xu et al., 2014, 2015).", "startOffset": 104, "endOffset": 247}, {"referenceID": 12, "context": "There is a rich history of research in generating or finding naturally-occurring sentential paraphrases (Barzilay and McKeown, 2001; Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004; Coster and Kauchak, 2011; Xu et al., 2014, 2015).", "startOffset": 104, "endOffset": 247}, {"referenceID": 31, "context": "There is a rich history of research in generating or finding naturally-occurring sentential paraphrases (Barzilay and McKeown, 2001; Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004; Coster and Kauchak, 2011; Xu et al., 2014, 2015).", "startOffset": 104, "endOffset": 247}, {"referenceID": 10, "context": "There is a rich history of research in generating or finding naturally-occurring sentential paraphrases (Barzilay and McKeown, 2001; Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004; Coster and Kauchak, 2011; Xu et al., 2014, 2015).", "startOffset": 104, "endOffset": 247}, {"referenceID": 6, "context": ", Bannard and Callison-Burch (2005), culmi-", "startOffset": 2, "endOffset": 36}, {"referenceID": 15, "context": "Our goals are highly similar to those of the PPDB project, which has also been produced for many languages (Ganitkevitch and Callison-Burch, 2014) since it only relies on the availability of bilingual text.", "startOffset": 107, "endOffset": 146}, {"referenceID": 14, "context": "used for learning embeddings for words and phrases (Faruqui et al., 2015; Wieting et al., 2015).", "startOffset": 51, "endOffset": 95}, {"referenceID": 39, "context": "used for learning embeddings for words and phrases (Faruqui et al., 2015; Wieting et al., 2015).", "startOffset": 51, "endOffset": 95}, {"referenceID": 14, "context": "used for learning embeddings for words and phrases (Faruqui et al., 2015; Wieting et al., 2015). However, when learning sentence embeddings, Wieting and Gimpel (2017) showed that PPDB is not as effective as sentential paraphrases, espe-", "startOffset": 52, "endOffset": 167}, {"referenceID": 39, "context": "For sentential paraphrases, Wieting and Gimpel (2017) used a dataset developed for text simplification by Coster and Kauchak (2011).", "startOffset": 28, "endOffset": 54}, {"referenceID": 10, "context": "For sentential paraphrases, Wieting and Gimpel (2017) used a dataset developed for text simplification by Coster and Kauchak (2011). It was created by aligning sentences from Simple English and standard English Wikipedia.", "startOffset": 106, "endOffset": 132}, {"referenceID": 31, "context": "Sutskever et al. (2014) trained NMT systems and visualized part of the space of the source language encoder for their English\u2192French system.", "startOffset": 0, "endOffset": 24}, {"referenceID": 18, "context": "Hill et al. (2016) evaluated the encoders of English-to-X NMT systems as sentence representations, finding them to perform poorly compared to several other methods based on unlabeled data.", "startOffset": 0, "endOffset": 19}, {"referenceID": 18, "context": "Hill et al. (2016) evaluated the encoders of English-to-X NMT systems as sentence representations, finding them to perform poorly compared to several other methods based on unlabeled data. Mallinson et al. (2017) adapted trained NMT models to produce sentence similarity scores in semantic evaluations.", "startOffset": 0, "endOffset": 213}, {"referenceID": 5, "context": "We generally followed the settings and training procedure from previous work (Bahdanau et al., 2014; Sennrich et al., 2016a).", "startOffset": 77, "endOffset": 124}, {"referenceID": 32, "context": "We generally followed the settings and training procedure from previous work (Bahdanau et al., 2014; Sennrich et al., 2016a).", "startOffset": 77, "endOffset": 124}, {"referenceID": 43, "context": "(Zeiler, 2012), a minibatch size of 80, and the training set was reshuffled between epochs.", "startOffset": 0, "endOffset": 14}, {"referenceID": 22, "context": "We trained a network for approximately 7 days on a single GPU (TITAN X), then the embedding layer was fixed and training continued, as suggested by Jean et al. (2015), for 12 hours.", "startOffset": 148, "endOffset": 167}, {"referenceID": 22, "context": "We trained a network for approximately 7 days on a single GPU (TITAN X), then the embedding layer was fixed and training continued, as suggested by Jean et al. (2015), for 12 hours. Additionally, the softmax was calculated over a filtered list of candidate translations. Following Jean et al. (2015), during decoding, we restrict the softmax layers\u2019 output vocabulary to include: the 10000 most common words, the top 25 unigram translations, and the gold translations\u2019 unigrams.", "startOffset": 148, "endOffset": 300}, {"referenceID": 25, "context": "The training data included: Europarl v7 (Koehn, 2005), the Common Crawl corpus, the UN corpus (Eisele and Chen, 2010), News Commentary v10, the 109 French-English corpus,", "startOffset": 40, "endOffset": 53}, {"referenceID": 13, "context": "The training data included: Europarl v7 (Koehn, 2005), the Common Crawl corpus, the UN corpus (Eisele and Chen, 2010), News Commentary v10, the 109 French-English corpus,", "startOffset": 94, "endOffset": 117}, {"referenceID": 8, "context": "0 (Bojar et al., 2016).", "startOffset": 2, "endOffset": 22}, {"referenceID": 32, "context": "Rare words were split into sub-word units, following Sennrich et al. (2016b). BLEU scores on the WMT2015 test set for each NMT system can be seen in Table 3.", "startOffset": 53, "endOffset": 77}, {"referenceID": 38, "context": "So we select models and a loss function from prior work (Wieting et al., 2016b; Wieting and Gimpel, 2017).", "startOffset": 56, "endOffset": 105}, {"referenceID": 40, "context": "So we select models and a loss function from prior work (Wieting et al., 2016b; Wieting and Gimpel, 2017).", "startOffset": 56, "endOffset": 105}, {"referenceID": 37, "context": "This model was found by Wieting et al. (2016b) to perform very strongly for semantic similarity tasks.", "startOffset": 24, "endOffset": 47}, {"referenceID": 40, "context": "ERAGING NETWORK (GRAN) (Wieting and Gimpel, 2017), combines the benefits of AVG and long short-term memory (LSTM) recurrent neural networks (Hochreiter and Schmidhuber, 1997).", "startOffset": 23, "endOffset": 49}, {"referenceID": 21, "context": "ERAGING NETWORK (GRAN) (Wieting and Gimpel, 2017), combines the benefits of AVG and long short-term memory (LSTM) recurrent neural networks (Hochreiter and Schmidhuber, 1997).", "startOffset": 140, "endOffset": 174}, {"referenceID": 37, "context": "We follow the training procedure of Wieting et al. (2015) and Wieting et al.", "startOffset": 36, "endOffset": 58}, {"referenceID": 37, "context": "We follow the training procedure of Wieting et al. (2015) and Wieting et al. (2016b). The training data is a set S of paraphrastic pairs \u3008s1, s2\u3009 and we optimize a margin-based loss: min Wc,Ww 1", "startOffset": 36, "endOffset": 85}, {"referenceID": 37, "context": "We evaluate the quality of a paraphrase dataset by using the experimental setting of Wieting et al. (2016b). We use the paraphrases as training data to create paraphrastic sentence embeddings, then", "startOffset": 85, "endOffset": 108}, {"referenceID": 41, "context": ", 2012, 2013, 2014, 2015), the SemEval 2015 Twitter task (Xu et al., 2015), and the SemEval 2014 SICK Semantic Relatedness task (Marelli et al.", "startOffset": 57, "endOffset": 74}, {"referenceID": 28, "context": ", 2015), and the SemEval 2014 SICK Semantic Relatedness task (Marelli et al., 2014).", "startOffset": 61, "endOffset": 83}, {"referenceID": 1, "context": "As development data, we use the 2016 STS tasks (Agirre et al., 2016), where the tuning criterion is the average Pearson\u2019s r over its 5 datasets.", "startOffset": 47, "endOffset": 68}, {"referenceID": 39, "context": "We use PARAGRAM-SL999 embeddings (Wieting et al., 2015) to initialize the word embedding matrix (Ww) for both models.", "startOffset": 33, "endOffset": 55}, {"referenceID": 23, "context": "For optimization we use Adam (Kingma and Ba, 2014) with a learning rate of 0.", "startOffset": 29, "endOffset": 50}, {"referenceID": 36, "context": "used by Wieting et al. (2016b) and Wieting et al.", "startOffset": 8, "endOffset": 31}, {"referenceID": 36, "context": "used by Wieting et al. (2016b) and Wieting et al. (2016a). PPDB comes in different sizes (S, M, L, XL, XXL, and XXXL), where each larger size subsumes all smaller ones.", "startOffset": 8, "endOffset": 58}, {"referenceID": 10, "context": "The other data source is the aligned Simple English / standard English Wikipedia data developed by Coster and Kauchak (2011) and used for learning paraphrastic sentence Lang.", "startOffset": 99, "endOffset": 125}, {"referenceID": 40, "context": "embeddings by Wieting and Gimpel (2017). We refer to this data source as \u201cSimpWiki\u201d.", "startOffset": 14, "endOffset": 40}, {"referenceID": 17, "context": "Due to the small data size, we train a 3-gram language model and use the KenLM toolkit (Heafield, 2011).", "startOffset": 87, "endOffset": 103}, {"referenceID": 39, "context": "We use PARAGRAM-SL999 embeddings (Wieting et al., 2015) to initialize the word embeddings for both models.", "startOffset": 33, "endOffset": 55}, {"referenceID": 29, "context": "level BLEU variant from Nakov et al. (2012) that uses smoothing for all n-gram lengths and also smooths the brevity penalty.", "startOffset": 24, "endOffset": 44}], "year": 2017, "abstractText": "We consider the problem of learning general-purpose, paraphrastic sentence embeddings in the setting of Wieting et al. (2016b). We use neural machine translation to generate sentential paraphrases via back-translation of bilingual sentence pairs. We evaluate the paraphrase pairs by their ability to serve as training data for learning paraphrastic sentence embeddings. We find that the data quality is stronger than prior work based on bitext and on par with manually-written English paraphrase pairs, with the advantage that our approach can scale up to generate large training sets for many languages and domains. We experiment with several language pairs and data sources, and develop a variety of data filtering techniques. In the process, we explore how neural machine translation output differs from humanwritten sentences, finding clear differences in length, the amount of repetition, and the use of rare words.1", "creator": "LaTeX with hyperref package"}}}