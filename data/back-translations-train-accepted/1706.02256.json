{"id": "1706.02256", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2017", "title": "A Mention-Ranking Model for Abstract Anaphora Resolution", "abstract": "Resolving abstract anaphora is an important, but difficult task for text understanding. With recent advances in representation learning this task becomes a tangible aim. A central property of abstract anaphora is that it establishes a relation between the anaphor embedded in the anaphoric sentence and its (typically non-nominal) antecedent. We propose an LSTM-based mention-ranking model that learns how abstract anaphors relate to their antecedents with a Siamese Net. We overcome the lack of training data by generating artificial anaphoric sentence-antecedent pairs. Our model outperforms state-of-the-art results on shell noun resolution. We also report first benchmark results on an abstract anaphora subset of the ARRAU corpus. This corpus presents a greater challenge due to a greater range of confounders. Our model is able to select syntactically plausible candidates and - if disregarding syntax - discriminates candidates using deeper features. Deeper inspection shows that the model is able to learn a relation between the anaphor in the anaphoric sentence and its antecedent.", "histories": [["v1", "Wed, 7 Jun 2017 16:58:59 GMT  (730kb,D)", "https://arxiv.org/abs/1706.02256v1", null], ["v2", "Fri, 21 Jul 2017 12:12:04 GMT  (1034kb,D)", "http://arxiv.org/abs/1706.02256v2", "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP). Copenhagen, Denmark"]], "reviews": [], "SUBJECTS": "cs.CL stat.ML", "authors": ["ana marasovic", "leo born", "juri opitz", "anette frank"], "accepted": true, "id": "1706.02256"}, "pdf": {"name": "1706.02256.pdf", "metadata": {"source": "CRF", "title": "A Mention-Ranking Model for Abstract Anaphora Resolution", "authors": ["Ana Marasovi\u0107", "Leo Born", "Juri Opitz", "Anette Frank"], "emails": ["marasovic@cl.uni-heidelberg.de", "born@cl.uni-heidelberg.de", "opitz@cl.uni-heidelberg.de", "frank@cl.uni-heidelberg.de"], "sections": [{"heading": null, "text": "A key characteristic of abstract anaphoras is that they establish a relationship between the anaphora embedded in the anaphorical sentence and their (typically non-nominal) prehistory. We propose a mention ranking model that examines the relationship between abstract anaphoras and their ancestors using an LSTM-Siamese mesh. We overcome the lack of training data by generating artificial anaphorical sentence pairs and precursor pairs. Our model exceeds the state of the art in terms of shell resolution. We also report initial benchmark results on an abstract anaphorical corpus subset of the ARRAU corpus. This corpus presents a greater challenge due to a mixture of nominal and pronominal anaphores and a wider range of constants. We have found models that work behind the individual anaphorical corpus and without synaphors, but for training the individual candidates without synaphores."}, {"heading": "1 Introduction", "text": "The current research in Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-Anaphora-An"}, {"heading": "2 Related and prior work", "text": "In fact, the fact is that most of them are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with The New York Times, in which he dealt with the question:\" What is this? \"\" What is this actually? \"\" What is this actually? \"\" What is this? \"\" What is this? \"\" What is this? \"he asked.\" What is this? \"\" What is this? \"he asked?\" What is this? \"he asked?\" What is this? \"he asked?\" What is this? \"he asked.\" What is this? \"he asked?\" he asked? \"What is this?\" he asked? \"What is this?\" he asked? \"What is this?\" he asked? \"What is this?\" he asked? \""}, {"heading": "3 Mention-Ranking Model", "text": "rE \"s tis rf\u00fc ide feeds for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green."}, {"heading": "4 Training data construction", "text": "We create large-scale training data for abstract anaphora resolution by using a common construction consisting of a verb with an embedded sentence (addendum or adverbial) (cf. fig. 2). We recognize this pattern in an analyzed corpus, \"cut\" the S constitution, and replace it with a suitable anaphorism to create the anaphoristic sentence (anaphrase), while S is the antecedent (antec). This method encompasses a wide range of anaphoraantedent constellations that create various semantic or discursive relationships harboring the verb and the embedded punishment."}, {"heading": "5 Experimental setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Datasets", "text": "This year, it has become one of the biggest hubs in the city's history."}, {"heading": "5.2 Baselines and evaluation metrics", "text": "Following KZH13, we report on the success of @ n (s @ n), which measures whether the precursor or a candidate who differs in a word14 belongs to n ranked candidates. Additionally, we report on the previous sentence baseline14 We received this information in personal communication with one of the authors. (PSBL), which selects the preceding sentence for the preceding sentence and the TAGBaseline (TAGBL), which randomly selects a candidate with the constituent marker in {S, VP, ROOT, SBAR}. For TAGBL, we report the average of 10 runs with 10 solid seeds. PSBL always performs worse on the ASN than the KTH13 model, so we report only for ARRAU-AA."}, {"heading": "5.3 Training details for our models", "text": "We recorded the performance using manually selected HPs and then tuned HPs using tree-structured plot estimators (TPE) (TPE) (Bergstra et al., 2011) 15. TPE selects HPs for the next (of 10) tracks based on the s @ 1 score based on the devset. We report on the best s @ 1 score tests in 10 studies when it is better than the scores of standard HPs and previous distributions for HPs used by TPE. The (exact) HPs we used can be found in complementary materials. To construct word vectors defined in Section 3, we used 100-dim. GloVe word beds on the Gigaington and Wikipedia were not replaced."}, {"heading": "6 Results and analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Results on shell noun resolution dataset", "text": "Table 3 shows the results of the Mention Ranking Model (MR-LSTM) on the ASN corpus using standard HP. Column 2 indicates which model produced the results: KZH13 refers to the best reported results in Kolhatkar et al. (2013b) and TAGBL is the abbreviation described in section 5.2. With respect to s @ 1 score, MR-LSTM exceeds both the results of KZH13 and TAGBL, without even requiring an HP vote. For the outlier reason, we have adjusted HPs (to ARRAU-AA) abbreviation for various architectural variants: the complete architecture, without embedding the context of the anapher (ctx), the anapher (aa), the two components of entering Em and the abbreviation (tag, cut), where only the abbreviation (to ARRAU-AA) for different architectural variants is used, the abbreviation of the tag (ctx), the abbreviation of the abbreviation of the anapher (ctx), the abbreviation of the abbreviation of the tag, the abbreviation of the abbreviation of the abbreviation of the tag, the abbreviation of the abbreviation of the abbreviation of the abbreviation of the tag, the abbreviation of the abbreviation of the abbreviation of the abbreviation (to ARRAU-AA), the abbreviation of the abbreviation of the abbreviation of the abbreviation (to the abbreviation of the abbreviation of the abbreviation of the tag, the abbreviation of the abbreviation of the abbreviation of the tag, the abbreviation, the abbreviation of the abbreviation of the abbreviation of the abbreviation of the abbreviation (to ARRAU-AA), the abbreviation of the abbreviation of the abbreviation of the abbreviation of the abbreviation (to the abbreviation of the abbreviation of the abbreviation (to"}, {"heading": "6.2 Results on the ARRAU corpus", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "6.3 Exploring the model", "text": "Finally, we analyze deeper aspects of the model: (1) whether a learned representation between the anaphorical clause and an antecedent establishes a relationship between a specific anaphorical clause we want to resolve and the antecedent, and (2) whether the maxmargin goal forces a separation of the common representations in common space. (1) We claim that by embedding both the anaphorical clause and the clause containing the anaphorical clause, we ensure that the learned relationship between the antecedent and anaphorical clause is dependent on the anaphorical clause under consideration. Fig. 3 illustrates the heatmap for an anaphorical clause with two anaphores. The i-th column of the heatmap corresponds to absolute differences between the output of the bi-LSTM for the i-th word in the anaphorical clause when the first vs. second anaphorical clause is resolved. Stronger colored inaphorical indicators of the heatmap represent the larger blue column, the blue column for the blue column, or the blue column for the first column."}, {"heading": "7 Conclusions", "text": "We presented a neural mention ranking model for the resolution of unrestricted abstract anaphoras and applied it to two sets of data with different types of abstract anaphoras: the shell noun dataset and a subset of ARRAU with (pro) nominal abstract anaphoras of any kind. To our knowledge, this work is the first dedicated to the unrestricted abstract anaphora resolution task with a neural network. Our model also exceeds the current results of the shell noun datasets. In this work, we investigated the use of purely artificially generated training data and how far it can take us. In future work, we plan to study mixtures of (more) artificial and natural data from different sources (e.g. ASN, CSN). On the more sophisticated ARRAU-AA model, we have found model variants that exceed the baseline for the entire and the nominal part of ARRAU-AA, although we do not train the models for nominal resolution."}, {"heading": "Acknowledgments", "text": "This work was supported by the Deutsche Forschungsgemeinschaft (German Research Foundation) within the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under funding number RTG 1994 / 1. We would like to thank anonymous reviewers for their useful comments and especially Todor Mihaylov for his advice on model implementation and everyone in the Computational Linguistics Group for the helpful discussion."}, {"heading": "A Pre-processing details", "text": "The number of cases differed from the reported numbers in KZH13 in 9 to 809 cases for training and 1 for the exam. The given sentences still contained the precursor, so we removed it from the sentence and converted the corresponding shell into \"this < shell noun >.\" An example of this process is: The decision to turn off the ventilator came after doctors found no brain activity. \u2192 This decision came after doctors found no brain activity. To use pre-trained word embeddings, we had to put all data in lowercase. Since we use an automatic parser to extract all the syntactical components, candidates with the same string appeared with different tags. We eliminated negative examples by checking which tag is used more frequently for candidates with the same POS tag than the duplicate candidate."}, {"heading": "B Hyperparameter details", "text": "Tables 6 and 7 give the coordinated HPs for the resolution of the shell contents and the resolution of abstract anaphores in ARRAU-AA for different model variants. Below is the list of all tunable HPs. \u2022 the dimensionality of the hidden states in the Bi-LSTM, hLSTM \u2022 the size of the first feed layer, hffl1 \u2022 the size of the second feed layer, hffl2 \u2022 the dimensionality of the day embedding, dTAG \u2022 the gradient clipping value, g \u2022 the frequency of the words in the vocabulary, fw \u2022 regulation coefficient, r \u2022 the probability of the output of Bi-LSTM, kLSTM \u2022 the probability of the input, kLSTM \u2022 the probability of the output of the first feed layer, kffl1 \u2022 the probability of the output of the first feed layer, kffl1 \u2022 the probability of the second of the first feed layer, kffl2We additionally give the number of the training (the number of parameters most trainable according to the GT1) (the X10)."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org", "author": ["van", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "van et al\\.,? \\Q2015\\E", "shortCiteRegEx": "van et al\\.", "year": 2015}, {"title": "Antecedent selection for sluicing: Structure and content", "author": ["Pranav Anand", "Daniel Hardt."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1234\u2013 1243, Austin, Texas.", "citeRegEx": "Anand and Hardt.,? 2016", "shortCiteRegEx": "Anand and Hardt.", "year": 2016}, {"title": "Reference to Abstract Objects in Discourse", "author": ["Nicholas Asher."], "venue": "Kluwer Academic Publishers.", "citeRegEx": "Asher.,? 1993", "shortCiteRegEx": "Asher.", "year": 1993}, {"title": "Algorithms for hyper-parameter optimization", "author": ["James Bergstra", "R\u00e9mi Bardenet", "Yoshua Bengio", "Bal\u00e1zs K\u00e9gl."], "venue": "Proceedings of the 35th Annual Conference on Neural Information Processing Systems (NIPS), Granada, Spain.", "citeRegEx": "Bergstra et al\\.,? 2011", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel R. Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts."], "venue": "Proceedings of the 54th Annual Meeting of the Association", "citeRegEx": "Bowman et al\\.,? 2016", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Resolving pronominal reference to abstract entities", "author": ["Donna K. Byron."], "venue": "Ph.D. thesis, University of Rochester, Rochester, New York.", "citeRegEx": "Byron.,? 2004", "shortCiteRegEx": "Byron.", "year": 2004}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Sumit Chopra", "Raia Hadsell", "Yann LeCun."], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages", "citeRegEx": "Chopra et al\\.,? 2005", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Entity-centric coreference resolution with model stacking", "author": ["Kevin Clark", "Christopher D. Manning."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natu-", "citeRegEx": "Clark and Manning.,? 2015", "shortCiteRegEx": "Clark and Manning.", "year": 2015}, {"title": "Deep reinforcement learning for mention-ranking coreference models", "author": ["Kevin Clark", "Christopher D. Manning."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), Austin, Texas.", "citeRegEx": "Clark and Manning.,? 2016a", "shortCiteRegEx": "Clark and Manning.", "year": 2016}, {"title": "Improving coreference resolution by learning entitylevel distributed representations", "author": ["Kevin Clark", "Christopher D. Manning."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), Berling, Germany.", "citeRegEx": "Clark and Manning.,? 2016b", "shortCiteRegEx": "Clark and Manning.", "year": 2016}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["Djork-Arn\u00e9 Clevert", "Thomas Unterthiner", "Sepp Hochreiter."], "venue": "Proceedings of the 4th International Conference on Learning Representations (ICLR), San Juan, Puerto", "citeRegEx": "Clevert et al\\.,? 2016", "shortCiteRegEx": "Clevert et al\\.", "year": 2016}, {"title": "Together we stand: Siamese networks for similar question retrieval", "author": ["Arpita Das", "Harish Yenala", "Manoj Kumar Chinnakotla", "Manish Shrivastava."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Das et al\\.,? 2016", "shortCiteRegEx": "Das et al\\.", "year": 2016}, {"title": "Annotating Abstract Anaphora", "author": ["Stefanie Dipper", "Heike Zinsmeister."], "venue": "Language Resources and Evaluation, 46(1):37\u201352.", "citeRegEx": "Dipper and Zinsmeister.,? 2012", "shortCiteRegEx": "Dipper and Zinsmeister.", "year": 2012}, {"title": "Dialogue acts, synchronising units and anaphora resolution", "author": ["Miriam Eckert", "Michael Strube"], "venue": null, "citeRegEx": "Eckert and Strube.,? \\Q2000\\E", "shortCiteRegEx": "Eckert and Strube.", "year": 2000}, {"title": "Framewise Phoneme Classification With Bidirectional LSTM And Other Neural Network Architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks, 18:602\u2013610.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "2015 IEEE International Conference on Computer Vision (ICCV).", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Recurrent orthogonal networks and long-memory tasks", "author": ["Mikael Henaff", "Arthur Szlam", "Yann LeCun."], "venue": "Proceedings of the the 33rd International Conference on Machine Learning (ICML), New York City, USA.", "citeRegEx": "Henaff et al\\.,? 2016", "shortCiteRegEx": "Henaff et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Resolving discourse-deictic pronouns: A two-stage approach to do it", "author": ["Sujay Kumar Jauhar", "Raul Guerra", "Edgar Gonz\u00e0lez Pellicer", "Marta Recasens."], "venue": "Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,", "citeRegEx": "Jauhar et al\\.,? 2015", "shortCiteRegEx": "Jauhar et al\\.", "year": 2015}, {"title": "Optimizing search engines using clickthrough data", "author": ["Thorsten Joachims."], "venue": "Proceedings of the 8th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 133\u2013142.", "citeRegEx": "Joachims.,? 2002", "shortCiteRegEx": "Joachims.", "year": 2002}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal J\u00f3zefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML), Lille, France.", "citeRegEx": "J\u00f3zefowicz et al\\.,? 2015", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proceedings of the 3rd International Conference on Learning Representations (ICLR), San Diego, USA.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D Manning."], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423\u2013430. Association for Computational Linguistics.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Resolving shell nouns", "author": ["Varada Kolhatkar", "Graeme Hirst."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 499\u2013510, Doha, Qatar.", "citeRegEx": "Kolhatkar and Hirst.,? 2014", "shortCiteRegEx": "Kolhatkar and Hirst.", "year": 2014}, {"title": "Annotating anaphoric shell nouns with their antecedents", "author": ["Varada Kolhatkar", "Heike Zinsmeister", "Graeme Hirst."], "venue": "Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 112\u2013121, Sofia, Bulgaria.", "citeRegEx": "Kolhatkar et al\\.,? 2013a", "shortCiteRegEx": "Kolhatkar et al\\.", "year": 2013}, {"title": "Interpreting anaphoric shell nouns using antecedents of cataphoric shell nouns as training data", "author": ["Varada Kolhatkar", "Heike Zinsmeister", "Graeme Hirst."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Kolhatkar et al\\.,? 2013b", "shortCiteRegEx": "Kolhatkar et al\\.", "year": 2013}, {"title": "Event Coreference Resolution with Multi-Pass Sieves", "author": ["Jing Lu", "Vincent Ng."], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC), pages 3996\u20134003, Portoroz.", "citeRegEx": "Lu and Ng.,? 2016", "shortCiteRegEx": "Lu and Ng.", "year": 2016}, {"title": "Visualizing data using t-sne", "author": ["Laurens v.d. Maaten", "Geoffrey Hinton."], "venue": "Journal of Machine Learning Research, 9(2579-2605):85.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Siamese recurrent architectures for learning sentence similarity", "author": ["Jonas Mueller", "Aditya Thyagarajan."], "venue": "Proceedings of the 13th Conference on Artificial Intelligence (AAAI), pages 2786\u20132792, Phoenix, Arizona.", "citeRegEx": "Mueller and Thyagarajan.,? 2016", "shortCiteRegEx": "Mueller and Thyagarajan.", "year": 2016}, {"title": "Fully Automatic Resolution of It, This and That in Unrestricted Multi-Party Dialog", "author": ["Christoph M\u00fcller."], "venue": "Ph.D. thesis, Universit\u00e4t T\u00fcbingen, T\u00fcbingen.", "citeRegEx": "M\u00fcller.,? 2008", "shortCiteRegEx": "M\u00fcller.", "year": 2008}, {"title": "Learning Text Similarity with Siamese Recurrent Networks", "author": ["Paul Neculoiu", "Maarten Versteegh", "Mihai Rotaru."], "venue": "Proceedings of the 1st Workshop on Representation Learning for NLP, pages 148\u2013 157, Berlin, Germany.", "citeRegEx": "Neculoiu et al\\.,? 2016", "shortCiteRegEx": "Neculoiu et al\\.", "year": 2016}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML), Atlanta, USA.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Anaphoric Annotation in the ARRAU Corpus", "author": ["Massimo Poesio", "Ron Artstein."], "venue": "Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC\u201908), Marrakech, Morocco.", "citeRegEx": "Poesio and Artstein.,? 2008", "shortCiteRegEx": "Poesio and Artstein.", "year": 2008}, {"title": "Unsupervised event coreference for abstract words", "author": ["Dheeraj Rajagopal", "Eduard Hovy", "Teruko Mitamura."], "venue": "Proceedings of EMNLP 2016 Workshop on Uphill Battles in Language Processing: Scaling Early Achievements to Robust Methods,", "citeRegEx": "Rajagopal et al\\.,? 2016", "shortCiteRegEx": "Rajagopal et al\\.", "year": 2016}, {"title": "A Systematic Study of Neural Discourse Models for Implicit Discourse Relation", "author": ["Attapol T. Rutherford", "Vera Demberg", "Nianwen Xue."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics.", "citeRegEx": "Rutherford et al\\.,? 2017", "shortCiteRegEx": "Rutherford et al\\.", "year": 2017}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15:1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Anaphoricity in Connectives: A Case Study on German", "author": ["Manfred Stede", "Yulia Grishina."], "venue": "Proceedings of the Coreference Resolution Beyond OntoNotes (CORBON) Workshop, San Diego, California.", "citeRegEx": "Stede and Grishina.,? 2016", "shortCiteRegEx": "Stede and Grishina.", "year": 2016}, {"title": "A machine learning approach to pronoun resolution in spoken dialogue", "author": ["Michael Strube", "Christoph M\u00fcller."], "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 168\u2013175, Sapporo, Japan.", "citeRegEx": "Strube and M\u00fcller.,? 2003", "shortCiteRegEx": "Strube and M\u00fcller.", "year": 2003}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "ARRAU: Linguistically-Motivated Annotation of Anaphoric Descriptions", "author": ["Olga Uryupina", "Ron Artstein", "Antonella Bristot", "Federica Cavicchio", "Kepa J Rodriguez", "Massimo Poesio."], "venue": "Proceedings of the Tenth International Conference on", "citeRegEx": "Uryupina et al\\.,? 2016", "shortCiteRegEx": "Uryupina et al\\.", "year": 2016}, {"title": "Structure and ostension in the interpretation of discourse deixis", "author": ["Bonnie Lynn Webber."], "venue": "Language and Cognitive processes, 6(2):107\u2013135.", "citeRegEx": "Webber.,? 1991", "shortCiteRegEx": "Webber.", "year": 1991}, {"title": "Learning anaphoricity and antecedent ranking features", "author": ["Sam Joshua Wiseman", "Alexander Matthew Rush", "Stuart Merrill Shieber", "Jason Weston"], "venue": null, "citeRegEx": "Wiseman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wiseman et al\\.", "year": 2015}, {"title": "End-to-end learning of semantic role labeling using recurrent neural networks", "author": ["Jie Zhou", "Wei Xu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natu-", "citeRegEx": "Zhou and Xu.,? 2015", "shortCiteRegEx": "Zhou and Xu.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Distinct from these are diverse types of abstract anaphora (AA) (Asher, 1993) where reference is made to propositions, facts, events or properties.", "startOffset": 64, "endOffset": 77}, {"referenceID": 23, "context": "While recent approaches address the resolution of selected abstract shell nouns (Kolhatkar and Hirst, 2014), we aim to resolve a wide range of abstract anaphors, such as the NP this trend in (1), as well as pronominal anaphors (this, that, or it).", "startOffset": 80, "endOffset": 107}, {"referenceID": 8, "context": "This enables us to use neural methods which have shown great success in related tasks: coreference resolution (Clark and Manning, 2016a), textual entailment (Bowman et al.", "startOffset": 110, "endOffset": 136}, {"referenceID": 4, "context": "This enables us to use neural methods which have shown great success in related tasks: coreference resolution (Clark and Manning, 2016a), textual entailment (Bowman et al., 2016), learning textual similarity (Mueller and Thyagarajan, 2016), and discourse relation sense classification (Rutherford et al.", "startOffset": 157, "endOffset": 178}, {"referenceID": 29, "context": ", 2016), learning textual similarity (Mueller and Thyagarajan, 2016), and discourse relation sense classification (Rutherford et al.", "startOffset": 37, "endOffset": 68}, {"referenceID": 36, "context": ", 2016), learning textual similarity (Mueller and Thyagarajan, 2016), and discourse relation sense classification (Rutherford et al., 2017).", "startOffset": 114, "endOffset": 139}, {"referenceID": 29, "context": ", 2015; Clark and Manning, 2015, 2016a,b) and combines it with a Siamese Net (Mueller and Thyagarajan, 2016), (Neculoiu et al.", "startOffset": 77, "endOffset": 108}, {"referenceID": 31, "context": ", 2015; Clark and Manning, 2015, 2016a,b) and combines it with a Siamese Net (Mueller and Thyagarajan, 2016), (Neculoiu et al., 2016) for", "startOffset": 110, "endOffset": 133}, {"referenceID": 41, "context": "Example drawn from ARRAU (Uryupina et al., 2016).", "startOffset": 25, "endOffset": 48}, {"referenceID": 23, "context": "ating training data is not confined to specific types of anaphora such as shell nouns (Kolhatkar and Hirst, 2014) or anaphoric connectives (Stede and Grishina, 2016).", "startOffset": 86, "endOffset": 113}, {"referenceID": 38, "context": "ating training data is not confined to specific types of anaphora such as shell nouns (Kolhatkar and Hirst, 2014) or anaphoric connectives (Stede and Grishina, 2016).", "startOffset": 139, "endOffset": 165}, {"referenceID": 34, "context": "Moreover, we report results of the model (trained on our newly constructed dataset) on unrestricted abstract anaphora instances from the ARRAU corpus (Poesio and Artstein, 2008; Uryupina et al., 2016).", "startOffset": 150, "endOffset": 200}, {"referenceID": 41, "context": "Moreover, we report results of the model (trained on our newly constructed dataset) on unrestricted abstract anaphora instances from the ARRAU corpus (Poesio and Artstein, 2008; Uryupina et al., 2016).", "startOffset": 150, "endOffset": 200}, {"referenceID": 24, "context": "We evaluate our model on the shell noun resolution dataset of Kolhatkar et al. (2013b) and show that it outperforms their state-of-the-art results.", "startOffset": 62, "endOffset": 87}, {"referenceID": 2, "context": "(2015) course properties (Asher, 1993; Webber, 1991).", "startOffset": 25, "endOffset": 52}, {"referenceID": 42, "context": "(2015) course properties (Asher, 1993; Webber, 1991).", "startOffset": 25, "endOffset": 52}, {"referenceID": 12, "context": "Annotation of abstract anaphora is also difficult for humans (Dipper and Zinsmeister, 2012), and thus, only", "startOffset": 61, "endOffset": 91}, {"referenceID": 41, "context": "We evaluate our models on a subset of the ARRAU corpus (Uryupina et al., 2016) that contains abstract anaphors and the shell noun corpus used in Kolhatkar et al.", "startOffset": 55, "endOffset": 78}, {"referenceID": 24, "context": ", 2016) that contains abstract anaphors and the shell noun corpus used in Kolhatkar et al. (2013b).3 We are not aware of other freely available abstract anaphora datasets.", "startOffset": 74, "endOffset": 99}, {"referenceID": 13, "context": "Early work (Eckert and Strube, 2000; Strube and M\u00fcller, 2003; Byron, 2004; M\u00fcller, 2008) has focused on spoken language, which exhibits specific properties.", "startOffset": 11, "endOffset": 88}, {"referenceID": 39, "context": "Early work (Eckert and Strube, 2000; Strube and M\u00fcller, 2003; Byron, 2004; M\u00fcller, 2008) has focused on spoken language, which exhibits specific properties.", "startOffset": 11, "endOffset": 88}, {"referenceID": 5, "context": "Early work (Eckert and Strube, 2000; Strube and M\u00fcller, 2003; Byron, 2004; M\u00fcller, 2008) has focused on spoken language, which exhibits specific properties.", "startOffset": 11, "endOffset": 88}, {"referenceID": 30, "context": "Early work (Eckert and Strube, 2000; Strube and M\u00fcller, 2003; Byron, 2004; M\u00fcller, 2008) has focused on spoken language, which exhibits specific properties.", "startOffset": 11, "endOffset": 88}, {"referenceID": 18, "context": "ing feature-based classifiers (Jauhar et al., 2015; Lu and Ng, 2016).", "startOffset": 30, "endOffset": 68}, {"referenceID": 26, "context": "ing feature-based classifiers (Jauhar et al., 2015; Lu and Ng, 2016).", "startOffset": 30, "endOffset": 68}, {"referenceID": 1, "context": "More related to our work is Anand and Hardt (2016) who present an antecedent ranking account for sluicing using classical machine learning based on a small training dataset.", "startOffset": 28, "endOffset": 51}, {"referenceID": 24, "context": "Closest to our work is Kolhatkar et al. (2013b)", "startOffset": 23, "endOffset": 48}, {"referenceID": 23, "context": "(KZH13) and Kolhatkar and Hirst (2014) (KH14) on shell noun resolution, using classical machine learning techniques.", "startOffset": 12, "endOffset": 39}, {"referenceID": 19, "context": "cataphoric antecedent) of CSNs and trained SVMrank (Joachims, 2002) on such instances.", "startOffset": 51, "endOffset": 67}, {"referenceID": 43, "context": "Most relevant for our task is the mention-ranking neural coreference model proposed in Clark and Manning (2015), and their improved model in Clark and Manning (2016a), which integrates a loss function (Wiseman et al., 2015) which learns distinct feature representations for anaphoricity detection and antecedent ranking.", "startOffset": 201, "endOffset": 223}, {"referenceID": 6, "context": "It is widely used in vision (Chopra et al., 2005), and in NLP for semantic similarity, entailment, query", "startOffset": 28, "endOffset": 49}, {"referenceID": 6, "context": "Most relevant for our task is the mention-ranking neural coreference model proposed in Clark and Manning (2015), and their improved model in Clark and Manning (2016a), which integrates a loss function (Wiseman et al.", "startOffset": 87, "endOffset": 112}, {"referenceID": 6, "context": "Most relevant for our task is the mention-ranking neural coreference model proposed in Clark and Manning (2015), and their improved model in Clark and Manning (2016a), which integrates a loss function (Wiseman et al.", "startOffset": 87, "endOffset": 167}, {"referenceID": 29, "context": "normalization and QA (Mueller and Thyagarajan, 2016; Neculoiu et al., 2016; Das et al., 2016).", "startOffset": 21, "endOffset": 93}, {"referenceID": 31, "context": "normalization and QA (Mueller and Thyagarajan, 2016; Neculoiu et al., 2016; Das et al., 2016).", "startOffset": 21, "endOffset": 93}, {"referenceID": 11, "context": "normalization and QA (Mueller and Thyagarajan, 2016; Neculoiu et al., 2016; Das et al., 2016).", "startOffset": 21, "endOffset": 93}, {"referenceID": 17, "context": "tence s and a candidate antecedent c using a bidirectional Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005).", "startOffset": 82, "endOffset": 146}, {"referenceID": 14, "context": "tence s and a candidate antecedent c using a bidirectional Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005).", "startOffset": 82, "endOffset": 146}, {"referenceID": 10, "context": "The resulting vector is fed into a feed-forward layer of exponential linear units (ELUs) (Clevert et al., 2016) to produce the final representation h\u0303s or h\u0303c of the sequence.", "startOffset": 89, "endOffset": 111}, {"referenceID": 40, "context": "From h\u0303c and h\u0303s we compute a vector hc,s = [|h\u0303c \u2212 h\u0303s|; h\u0303c h\u0303s] (Tai et al., 2015), where |\u2013| denotes the absolute values of the element-wise subtraction, and the element-wise multiplication.", "startOffset": 67, "endOffset": 85}, {"referenceID": 43, "context": "We train the described mention-ranking model with the max-margin training objective from Wiseman et al. (2015), used for the antecedent ranking subtask.", "startOffset": 89, "endOffset": 111}, {"referenceID": 28, "context": "Using this method we generated a dataset of artificial anaphoric sentence\u2013antecedent pairs from the WSJ part of the PTB Corpus (Marcus et al., 1993), automatically parsed using the Stanford Parser (Klein and Manning, 2003).", "startOffset": 127, "endOffset": 148}, {"referenceID": 22, "context": ", 1993), automatically parsed using the Stanford Parser (Klein and Manning, 2003).", "startOffset": 56, "endOffset": 81}, {"referenceID": 24, "context": "We follow the data preparation and evaluation protocol of Kolhatkar et al. (2013b) (KZH13).", "startOffset": 58, "endOffset": 83}, {"referenceID": 24, "context": "For validation, Kolhatkar et al. (2013a) crowdsourced annotations for the sentence which contains the antecedent, which KZH13 refer to as a broad region.", "startOffset": 16, "endOffset": 41}, {"referenceID": 41, "context": "12 Our test data for unrestricted abstract anaphora resolution is obtained from the ARRAU corpus (Uryupina et al., 2016).", "startOffset": 97, "endOffset": 120}, {"referenceID": 3, "context": "We recorded performance with manually chosen HPs and then tuned HPs with Tree-structured Parzen Estimators (TPE) (Bergstra et al., 2011)15.", "startOffset": 113, "endOffset": 136}, {"referenceID": 33, "context": "GloVe word embeddings pre-trained on the Gigaword and Wikipedia (Pennington et al., 2014), and did not fine-tune them.", "startOffset": 64, "endOffset": 89}, {"referenceID": 16, "context": "We initialized the weight matrices of the LSTMs with random orthogonal matrices (Henaff et al., 2016), all other weight matrices with the initialization proposed in He et al.", "startOffset": 80, "endOffset": 101}, {"referenceID": 15, "context": ", 2016), all other weight matrices with the initialization proposed in He et al. (2015). The first feed-forward layer size is set to a value in {400, qlog-U(200, 800)}, the second to a value in {1024, qlog-U(400, 2000)}.", "startOffset": 71, "endOffset": 88}, {"referenceID": 21, "context": "We trained our model in minibatches using Adam (Kingma and Ba, 2015) with the learning rate of 10\u22124 and maximal batch size 64.", "startOffset": 47, "endOffset": 68}, {"referenceID": 32, "context": "We clip gradients by global norm (Pascanu et al., 2013), with a clipping value in {1.", "startOffset": 33, "endOffset": 55}, {"referenceID": 37, "context": "Dropout (Srivastava et al., 2014) with a keep probability kp \u2208 {0.", "startOffset": 8, "endOffset": 33}, {"referenceID": 24, "context": "Column 2 states which model produced the results: KZH13 refers to the best reported results in Kolhatkar et al. (2013b) and TAGBL is the baseline described in Section 5.", "startOffset": 95, "endOffset": 120}], "year": 2017, "abstractText": "Resolving abstract anaphora is an important, but difficult task for text understanding. Yet, with recent advances in representation learning this task becomes a more tangible aim. A central property of abstract anaphora is that it establishes a relation between the anaphor embedded in the anaphoric sentence and its (typically non-nominal) antecedent. We propose a mention-ranking model that learns how abstract anaphors relate to their antecedents with an LSTM-Siamese Net. We overcome the lack of training data by generating artificial anaphoric sentence\u2013 antecedent pairs. Our model outperforms state-of-the-art results on shell noun resolution. We also report first benchmark results on an abstract anaphora subset of the ARRAU corpus. This corpus presents a greater challenge due to a mixture of nominal and pronominal anaphors and a greater range of confounders. We found model variants that outperform the baselines for nominal anaphors, without training on individual anaphor data, but still lag behind for pronominal anaphors. Our model selects syntactically plausible candidates and \u2013 if disregarding syntax \u2013 discriminates candidates using deeper features.", "creator": "LaTeX with hyperref package"}}}