{"id": "1706.02275", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2017", "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments", "abstract": "We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.", "histories": [["v1", "Wed, 7 Jun 2017 17:35:00 GMT  (3408kb,D)", "http://arxiv.org/abs/1706.02275v1", null], ["v2", "Wed, 21 Jun 2017 22:18:54 GMT  (3408kb,D)", "http://arxiv.org/abs/1706.02275v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["ryan lowe", "yi wu", "aviv tamar", "jean harb", "pieter abbeel", "igor mordatch"], "accepted": true, "id": "1706.02275"}, "pdf": {"name": "1706.02275.pdf", "metadata": {"source": "CRF", "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments", "authors": ["Ryan Lowe", "Yi Wu"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most of RL's successful strategies are located in the retail sector, where the behavior of other actors in the environment is largely inevitable. However, there are a number of important applications that involve interaction between different actors, where the behavior of actors is jointly developed. For example, the discovery of communication and language in practice is not necessarily, but the analysis of social dilemmas. [17] All operate in a multi-agent domain. Similar problems such as variations of hierarchical learning can also be seen as multipliers."}, {"heading": "2 Related Work", "text": "The simplest approach to learning in multi-agent settings is the use of independent learning agents. This has been tried with Q-Learning in [34], but in practice it does not succeed in preventing the naive application of experiential responses. Previous work has tried to do this by including other actors \"political parameters in the Q function [35], by explicitly adding the iteration index to the replay buffer, or by using the meaning of sampling settings [9], but these approaches have had mixed empirical success. Deep Q-Learning approaches have previously been studied in competing pong functions. The nature of interaction between actors can either be cooperative, competitive, or both algorithms are designed for only one type of interaction."}, {"heading": "3 Background", "text": "Markov Games In this thesis we consider an extension of the Markov decision-making processes (MDPs) as Q = Q = Q = Q = Q = partially observable Markov games [19]. A Markov game for N-agents is defined by a series of states describing the possible configurations of all agents, a series of actions A1,..., AN and a series of observations O1,..., ON for each agent. To choose actions, each agent uses a stochastic political procedure, whereby each agent can apply a stochastic procedure. [0, 1], which determines the next state according to the transitional state function T: S \u00b7 A1 \u00b7 AN 7 \u00b7 S. 2 Each agent receives rewards as a function of the state and agents. ri: S \u00d7 Ai 7 \u2192 R, and receives a private observation correlation with the state oi: S 7 \u2192 Oi. The initial states are determined by a distribution. [0, 1] Each agent aimed to maximize his own overall expectation."}, {"heading": "4 Methods", "text": "4.1. We do not assume that a certain communication method is ever applicable between agents (that we consider for a competitive communication).... m 1m Nc 1c Nl 1l MclaCabpoolFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCFCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC"}, {"heading": "4.2 Inferring Policies of Other Agents", "text": "To eliminate the assumption of knowing the policies of other agents, as required in Eq.6, each agent i can additionally maintain an approximation, which is achieved by maximizing the log probability of the actions of Agent j with an entropy regulator: L (Eoj) = \u2212 Eoj, aj [log \u00b5 ji (aj | oj) + \u03bbH (\u00b5 j i)], (7) where H is the entropy of policy distribution. With the approximate policies, y can be replaced in Eq.6 by an approximate value y, which is calculated as follows: y = ri + quQ \u00b5 \"i (x \u00b2, \u00b5 \u00b2 1i (o1),., \u00b5 \u00b2 i (oi),. Note:.\".,.,. in Equation y (oN). \""}, {"heading": "4.3 Agents with Policy Ensembles", "text": "As already mentioned, a recurring problem in the field of multi-agent reinforcement is the lack of stationarity of the environment due to the changing strategies of the actors. This is especially true in competitive environments where the actors can derive a strong policy by over-adapting to the behavior of their competitors. Such strategies are undesirable because they are fragile and can fail when the competitors change their strategies.To obtain strategies for multi-agents that are more resilient to changes in the policies of competing actors, we propose to train a collection of K different sub-strategies. In agent i we then randomly select a specific sub-policy for each agent to execute. Let us present this policy \u00b5i as an interplay of K different sub-strategies with sub-strategies k, which are referred to as sub-strategies k. For agent i we then maximize the overall goal: Je (\u00b5i) Ek = Sub-politics, if (\u00b5), \u00b5-policy."}, {"heading": "5.1 Environments", "text": "To conduct our experiments, we adopt the grounded communication environment proposed in [24], which consists of N agents and L markers inhabiting a two-dimensional world with continuous space and discrete time. Agents can take physical actions in the environment and communication actions that are transmitted to other agents. However, unlike [24], we do not assume that all agents have identical spaces for action and observation, or act according to the same policy. We also consider games that are both cooperative (all agents must maximize a common return) and competitive (agents have conflicting goals). Some environments require explicit communication between agents to achieve the best reward, while in other environments agents can only perform physical actions."}, {"heading": "5.2 Comparison to Traditional Reinforcement Learning Methods", "text": "In fact, it is as if most of us are able to obey the rules that they have imposed on themselves. (...) In fact, it is as if most of us are able to understand the rules. (...) In fact, it is as if they do not obey the rules. (...) In fact, it is as if they do not obey the rules. (...) It is not as if they obey the rules. (...) It is as if they obey the rules. (...) It is as if they obey the rules. (...) It is as if they obey the rules. (...) It is as if they obey the rules. (...). (...) It is as if they obey the rules. (...) It is as if they obey the rules. (...) It is as if they obey the rules. (...) It is as if they obey the rules. (...)"}, {"heading": "5.3 Effect of Learning Polices of Other Agents", "text": "We evaluate the effectiveness of learning the strategies of other actors in the cooperative communication environment by following the same hyperparameters as the previous experiments and \u03bb = 0.001 in Equation 7. Results are presented in Figure 7. We observe that learning with approximate strategies can achieve the same success rate as applying the true strategies without a significant slowdown in convergence, although the approximate listener policy that the speaker has learned does not fit perfectly with the strategies of other actors."}, {"heading": "5.4 Effect of Training with Policy Ensembles", "text": "We focus on the effectiveness of political ensembles in competitive environments, including restraint, cooperative navigation, and predator prey. To improve the rate of convergence, we enforce that the cooperative actors should adopt the same strategies for each episode, as well as for the opponents. To evaluate the approach, we measure the performance of ensemble policies and individual strategies in the roles of actors and opponents. Results are presented on the right side of Figure 3. We observe that actors with political ensembles are stronger than those with a single strategy. In particular, when ensemble agents are pitted against individual political opponents (second to left beam clusters), the ensemble agents far outperform the opponents compared to reversed roles (third to left beam clusters)."}, {"heading": "6 Conclusions and Future Work", "text": "Empirically, our method outperforms traditional RL algorithms in a variety of cooperative and competing multi-agent environments. We can further improve the performance of our method by training agents with an overall set of policies - an approach that we believe is universally applicable to all multi-agent algorithms. A disadvantage of our approach is that the input space of Q grows linearly (depending on what information is contained in x) with the number of Agent N. This could be remedied in practice, for example, by having a modular Q function that takes agents into account only in a specific neighborhood of a particular agent. We leave this study to future work."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Jacob Andreas, Smitha Milli, Jack Clark and others at OpenAI and UC Berkeley for interesting discussions related to this paper, as well as Jakub Pachocki, Yura Burda and Joelle Pineau for comments on the draft paper. We would like to thank Tambet Matiisen for providing the code base used for some early experiments related to this paper. Ryan Lowe is partially supported by a Vanier CGS Scholarship and the Samsung Advanced Institute of Technology. Finally, we would like to thank OpenAI for fostering a dedicated and productive research environment."}], "references": [{"title": "Learning to protect communications with adversarial neural cryptography", "author": ["M. Abadi", "D.G. Andersen"], "venue": "arXiv preprint arXiv:1610.06918", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning conventions in multiagent stochastic domains using likelihood estimates", "author": ["C. Boutilier"], "venue": "Proceedings of the Twelfth international conference on Uncertainty in artificial intelligence, pages 106\u2013114. Morgan Kaufmann Publishers Inc.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "A comprehensive survey of multiagent reinforcement learning", "author": ["L. Busoniu", "R. Babuska", "B. De Schutter"], "venue": "IEEE Transactions on Systems Man and Cybernetics Part C Applications and Reviews, 38(2):156", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Coordination in multiagent reinforcement learning: a bayesian approach", "author": ["G. Chalkiadakis", "C. Boutilier"], "venue": "Proceedings of the second international joint conference on Autonomous agents and multiagent systems, pages 709\u2013716. ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Feudal reinforcement learning", "author": ["P. Dayan", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 271\u2013271. Morgan Kaufmann Publishers", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1993}, {"title": "Counterfactual multi-agent policy gradients", "author": ["J. Foerster", "G. Farquhar", "T. Afouras", "N. Nardelli", "S. Whiteson"], "venue": "arXiv preprint arXiv:1705.08926", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2017}, {"title": "N", "author": ["J.N. Foerster", "Y.M. Assael"], "venue": "de Freitas, and S. Whiteson. Learning to communicate with deep multi-agent reinforcement learning. CoRR, abs/1605.06676", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Stabilising experience replay for deep multi-agent reinforcement learning", "author": ["J.N. Foerster", "N. Nardelli", "G. Farquhar", "P.H.S. Torr", "P. Kohli", "S. Whiteson"], "venue": "CoRR, abs/1702.08887", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2017}, {"title": "Predicting pragmatic reasoning in language games", "author": ["M.C. Frank", "N.D. Goodman"], "venue": "Science, 336(6084):998\u2013998", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in neural information processing systems, pages 2672\u20132680", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Cooperative multi-agent control using deep reinforcement learning", "author": ["J.K. Gupta", "M. Egorov", "M. Kochenderfer"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Online learning about other agents in a dynamic multiagent system", "author": ["J. Hu", "M.P. Wellman"], "venue": "Proceedings of the Second International Conference on Autonomous Agents, AGENTS \u201998, pages 239\u2013246, New York, NY, USA", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Categorical reparameterization with gumbel-softmax", "author": ["E. Jang", "S. Gu", "B. Poole"], "venue": "arXiv preprint arXiv:1611.01144", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "An algorithm for distributed reinforcement learning in cooperative multi-agent systems", "author": ["M. Lauer", "M. Riedmiller"], "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning, pages 535\u2013542. Morgan Kaufmann", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Multi-agent cooperation and the emergence of (natural) language", "author": ["A. Lazaridou", "A. Peysakhovich", "M. Baroni"], "venue": "arXiv preprint arXiv:1612.07182", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-agent reinforcement learning in sequential social dilemmas", "author": ["J.Z. Leibo", "V.F. Zambaldi", "M. Lanctot", "J. Marecki", "T. Graepel"], "venue": "CoRR, abs/1702.03037", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "arXiv preprint arXiv:1504.00702", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Markov games as a framework for multi-agent reinforcement learning", "author": ["M.L. Littman"], "venue": "Proceedings of the eleventh international conference on machine learning, volume 157, pages 157\u2013163", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "et al", "author": ["L. Matignon", "L. Jeanpierre", "A.-I. Mouaddib"], "venue": "Coordinated multi-robot exploration under communication constraints using decentralized markov decision processes. In AAAI", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Hysteretic q-learning: an algorithm for decentralized reinforcement learning in cooperative multi-agent teams", "author": ["L. Matignon", "G.J. Laurent", "N. Le Fort-Piat"], "venue": "Intelligent Robots and Systems, 2007. IROS 2007. IEEE/RSJ International Conference on, pages 64\u201369. IEEE", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Independent reinforcement learners in cooperative markov games: a survey regarding coordination problems", "author": ["L. Matignon", "G.J. Laurent", "N. Le Fort-Piat"], "venue": "The Knowledge Engineering Review, 27(01):1\u201331", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "et al", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Emergence of grounded compositional language in multi-agent populations", "author": ["I. Mordatch", "P. Abbeel"], "venue": "arXiv preprint arXiv:1703.04908", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep decentralized multi-task multi-agent reinforcement learning under partial observability", "author": ["S. Omidshafiei", "J. Pazis", "C. Amato", "J.P. How", "J. Vian"], "venue": "CoRR, abs/1703.06182", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2017}, {"title": "Cooperative multi-agent learning: The state of the art", "author": ["L. Panait", "S. Luke"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "Multiagent bidirectionallycoordinated nets for learning to play starcraft combat games", "author": ["P. Peng", "Q. Yuan", "Y. Wen", "Y. Yang", "Z. Tang", "H. Long", "J. Wang"], "venue": "CoRR, abs/1703.10069", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2017}, {"title": "G", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre"], "venue": "van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484 \u2013 489", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "et al", "author": ["S. Sukhbaatar", "R. Fergus"], "venue": "Learning multiagent communication with backpropagation. In Advances in Neural Information Processing Systems, pages 2244\u20132252", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Intrinsic motivation and automatic curricula via asymmetric self-play", "author": ["S. Sukhbaatar", "I. Kostrikov", "A. Szlam", "R. Fergus"], "venue": "arXiv preprint arXiv:1703.05407", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2017}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "volume 1. MIT press Cambridge", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R.S. Sutton", "D.A. McAllester", "S.P. Singh", "Y. Mansour"], "venue": "Advances in neural information processing systems, pages 1057\u20131063", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2000}, {"title": "Multiagent cooperation and competition with deep reinforcement learning", "author": ["A. Tampuu", "T. Matiisen", "D. Kodelja", "I. Kuzovkin", "K. Korjus", "J. Aru", "J. Aru", "R. Vicente"], "venue": "PloS one, 12(4):e0172395", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2017}, {"title": "Multi-agent reinforcement learning: Independent vs", "author": ["M. Tan"], "venue": "cooperative agents. In Proceedings of the tenth international conference on machine learning, pages 330\u2013337", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1993}, {"title": "Extending q-learning to general adaptive multi-agent systems", "author": ["G. Tesauro"], "venue": "Advances in neural information processing systems, pages 871\u2013878", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2004}, {"title": "Conjugate markov decision processes", "author": ["P.S. Thomas", "A.G. Barto"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 137\u2013144", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning, 8(3-4):229\u2013256", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1992}], "referenceMentions": [{"referenceID": 21, "context": "Reinforcement learning (RL) has recently been applied to solve challenging problems, from game playing [23, 28] to robotics [18].", "startOffset": 103, "endOffset": 111}, {"referenceID": 26, "context": "Reinforcement learning (RL) has recently been applied to solve challenging problems, from game playing [23, 28] to robotics [18].", "startOffset": 103, "endOffset": 111}, {"referenceID": 16, "context": "Reinforcement learning (RL) has recently been applied to solve challenging problems, from game playing [23, 28] to robotics [18].", "startOffset": 124, "endOffset": 128}, {"referenceID": 18, "context": "For example, multi-robot control [20], the discovery of communication and language [29, 8, 24], multiplayer games [27], and the analysis of social dilemmas [17] all operate in a multi-agent domain.", "startOffset": 33, "endOffset": 37}, {"referenceID": 27, "context": "For example, multi-robot control [20], the discovery of communication and language [29, 8, 24], multiplayer games [27], and the analysis of social dilemmas [17] all operate in a multi-agent domain.", "startOffset": 83, "endOffset": 94}, {"referenceID": 6, "context": "For example, multi-robot control [20], the discovery of communication and language [29, 8, 24], multiplayer games [27], and the analysis of social dilemmas [17] all operate in a multi-agent domain.", "startOffset": 83, "endOffset": 94}, {"referenceID": 22, "context": "For example, multi-robot control [20], the discovery of communication and language [29, 8, 24], multiplayer games [27], and the analysis of social dilemmas [17] all operate in a multi-agent domain.", "startOffset": 83, "endOffset": 94}, {"referenceID": 25, "context": "For example, multi-robot control [20], the discovery of communication and language [29, 8, 24], multiplayer games [27], and the analysis of social dilemmas [17] all operate in a multi-agent domain.", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "For example, multi-robot control [20], the discovery of communication and language [29, 8, 24], multiplayer games [27], and the analysis of social dilemmas [17] all operate in a multi-agent domain.", "startOffset": 156, "endOffset": 160}, {"referenceID": 4, "context": "Related problems, such as variants of hierarchical reinforcement learning [6] can also be seen as a multi-agent system, with multiple levels of hierarchy being equivalent to multiple agents.", "startOffset": 74, "endOffset": 77}, {"referenceID": 26, "context": "Additionally, multi-agent self-play has recently been shown to be a useful training paradigm [28, 30].", "startOffset": 93, "endOffset": 101}, {"referenceID": 28, "context": "Additionally, multi-agent self-play has recently been shown to be a useful training paradigm [28, 30].", "startOffset": 93, "endOffset": 101}, {"referenceID": 9, "context": "Applying these methods to competitive environments is also challenging from an optimization perspective, as evidenced by the notorious instability of adversarial training methods [11].", "startOffset": 179, "endOffset": 183}, {"referenceID": 28, "context": "The ability to act in mixed cooperative-competitive environments may be critical for intelligent agents; while competitive training provides a natural curriculum for learning [30], agents must also exhibit cooperative behavior (e.", "startOffset": 175, "endOffset": 179}, {"referenceID": 32, "context": "This was attempted with Q-learning in [34], but does not perform well in practice [22].", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "This was attempted with Q-learning in [34], but does not perform well in practice [22].", "startOffset": 82, "endOffset": 86}, {"referenceID": 33, "context": "Previous work has attempted to address this by inputting other agent\u2019s policy parameters to the Q function [35], explicitly adding the iteration index to the replay buffer, or using importance sampling [9], but these approaches had mixed empirical success.", "startOffset": 107, "endOffset": 111}, {"referenceID": 7, "context": "Previous work has attempted to address this by inputting other agent\u2019s policy parameters to the Q function [35], explicitly adding the iteration index to the replay buffer, or using importance sampling [9], but these approaches had mixed empirical success.", "startOffset": 202, "endOffset": 205}, {"referenceID": 31, "context": "Deep Q-learning approaches have previously been investigated in [33] to train competing Pong agents.", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "Most studied are cooperative settings, with strategies such as optimistic and hysteretic Q function updates [15, 21, 25], which assume that the actions of other agents are made to improve collective reward.", "startOffset": 108, "endOffset": 120}, {"referenceID": 19, "context": "Most studied are cooperative settings, with strategies such as optimistic and hysteretic Q function updates [15, 21, 25], which assume that the actions of other agents are made to improve collective reward.", "startOffset": 108, "endOffset": 120}, {"referenceID": 23, "context": "Most studied are cooperative settings, with strategies such as optimistic and hysteretic Q function updates [15, 21, 25], which assume that the actions of other agents are made to improve collective reward.", "startOffset": 108, "endOffset": 120}, {"referenceID": 10, "context": "Another approach is to indirectly arrive at cooperation via sharing of policy parameters [12], but this requires a homogeneous agent population.", "startOffset": 89, "endOffset": 93}, {"referenceID": 5, "context": "Concurrently to our work, [7] proposed a similar idea of using policy gradient methods with a centralized critic, and test their approach on a StarCraft micromanagement task.", "startOffset": 26, "endOffset": 29}, {"referenceID": 24, "context": "See [26, 4] for surveys of multi-agent learning approaches and applications.", "startOffset": 4, "endOffset": 11}, {"referenceID": 2, "context": "See [26, 4] for surveys of multi-agent learning approaches and applications.", "startOffset": 4, "endOffset": 11}, {"referenceID": 27, "context": "Recent work has focused on learning grounded cooperative communication protocols between agents to solve various tasks [29, 8, 24].", "startOffset": 119, "endOffset": 130}, {"referenceID": 6, "context": "Recent work has focused on learning grounded cooperative communication protocols between agents to solve various tasks [29, 8, 24].", "startOffset": 119, "endOffset": 130}, {"referenceID": 22, "context": "Recent work has focused on learning grounded cooperative communication protocols between agents to solve various tasks [29, 8, 24].", "startOffset": 119, "endOffset": 130}, {"referenceID": 1, "context": "The importance of such modeling has been recognized by both reinforcement learning [3, 5] and cognitive science communities [10].", "startOffset": 83, "endOffset": 89}, {"referenceID": 3, "context": "The importance of such modeling has been recognized by both reinforcement learning [3, 5] and cognitive science communities [10].", "startOffset": 83, "endOffset": 89}, {"referenceID": 8, "context": "The importance of such modeling has been recognized by both reinforcement learning [3, 5] and cognitive science communities [10].", "startOffset": 124, "endOffset": 128}, {"referenceID": 11, "context": "[13] stressed the importance of being robust to the decision making process of", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Markov Games In this work, we consider a multi-agent extension of Markov decision processes (MDPs) called partially observable Markov games [19].", "startOffset": 140, "endOffset": 144}, {"referenceID": 21, "context": "Q-Learning and DQN [23] are popular methods in reinforcement learning and have been previously applied to multi-agent settings [8, 35].", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": "Q-Learning and DQN [23] are popular methods in reinforcement learning and have been previously applied to multi-agent settings [8, 35].", "startOffset": 127, "endOffset": 134}, {"referenceID": 33, "context": "Q-Learning and DQN [23] are popular methods in reinforcement learning and have been previously applied to multi-agent settings [8, 35].", "startOffset": 127, "endOffset": 134}, {"referenceID": 32, "context": "Q-Learning can be directly applied to multi-agent settings by having each agent i learn an independently optimal function Qi [34].", "startOffset": 125, "endOffset": 129}, {"referenceID": 7, "context": "Another difficulty observed in [9] is that the experience replay buffer cannot be used in such a setting since in general, P (s\u2032|s, a,\u03c01, .", "startOffset": 31, "endOffset": 34}, {"referenceID": 30, "context": "Using the Q function defined previously, the gradient of the policy can be written as [32]: \u2207\u03b8J(\u03b8) = Es\u223cp\u03c0 ,a\u223c\u03c0\u03b8 [\u2207\u03b8 log\u03c0\u03b8(a|s)Q(s, a)], (2) where p is the state distribution.", "startOffset": 86, "endOffset": 90}, {"referenceID": 35, "context": "For example, one can simply use a sample return R = \u2211T i=t \u03b3 ri, which leads to the REINFORCE algorithm [37].", "startOffset": 104, "endOffset": 108}, {"referenceID": 29, "context": "temporal-difference learning [31]; this Q(s, a) is called the critic and leads to a variety of actor-critic algorithms [31].", "startOffset": 29, "endOffset": 33}, {"referenceID": 29, "context": "temporal-difference learning [31]; this Q(s, a) is called the critic and leads to a variety of actor-critic algorithms [31].", "startOffset": 119, "endOffset": 123}, {"referenceID": 21, "context": "DDPG also makes use of a target network, as in DQN [23].", "startOffset": 51, "endOffset": 55}, {"referenceID": 22, "context": "their own observations) at execution time, (2) we do not assume a differentiable model of the environment dynamics, unlike in [24], and (3) we do not assume any particular structure on the communication method between agents (that is, we don\u2019t assume a differentiable communication channel).", "startOffset": 126, "endOffset": 130}, {"referenceID": 6, "context": "Similarly to [8], we accomplish our goal by adopting the framework of centralized training with decentralized execution.", "startOffset": 13, "endOffset": 16}, {"referenceID": 22, "context": "To perform our experiments, we adopt the grounded communication environment proposed in [24], which consists of N agents and L landmarks inhabiting a two-dimensional world with continuous space and discrete time.", "startOffset": 88, "endOffset": 92}, {"referenceID": 22, "context": "Unlike [24], we do not assume that all agents have identical action and observation spaces, or act according to the same policy \u03c0 .", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "This is similar to the cryptography environment considered in [2].", "startOffset": 62, "endOffset": 65}, {"referenceID": 12, "context": "To support discrete communication messages, we use the Gumbel-Softmax estimator [14].", "startOffset": 80, "endOffset": 84}, {"referenceID": 14, "context": "[16]) may not generalize to more complex tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "We emphasize that we do not use any of the tricks required for the cryptography environment from [2], including modifying Eve\u2019s loss function, alternating agent and adversary training, and using a hybrid \u2018mix & transform\u2019 feed-forward and convolutional architecture.", "startOffset": 97, "endOffset": 100}], "year": 2017, "abstractText": "We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multiagent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.", "creator": "LaTeX with hyperref package"}}}