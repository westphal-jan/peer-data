{"id": "1706.02459", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization", "abstract": "Current Chinese social media text summarization models are based on an encoder-decoder framework. Although its generated summaries are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and summaries for Chinese social media summarization. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms baseline systems on a social media corpus.", "histories": [["v1", "Thu, 8 Jun 2017 07:05:56 GMT  (167kb)", "http://arxiv.org/abs/1706.02459v1", "Accepted by ACL"]], "COMMENTS": "Accepted by ACL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shuming ma", "xu sun", "jingjing xu", "houfeng wang", "wenjie li", "qi su"], "accepted": true, "id": "1706.02459"}, "pdf": {"name": "1706.02459.pdf", "metadata": {"source": "CRF", "title": "Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization", "authors": ["Shuming Ma", "Xu Sun", "Jingjing Xu", "Houfeng Wang", "Wenjie Li", "Qi Su"], "emails": ["sukia}@pku.edu.cn", "cswjli@comp.polyu.edu.hk"], "sections": [{"heading": null, "text": "ar Xiv: 170 6.02 459v 1 [cs.C L] 8J un2 017rization models are based on an encoderdecoder framework. Although its generated summaries are literally similar to source code, they have little semantic relevance. In this work, our goal is to improve the semantic relevance between source code and summaries for summaries in Chinese social media. We are introducing a semantic relevance model to promote a high semantic similarity between texts and summaries. In our model, the source code is represented by a gated attention encoder, while the summary representation is generated by a decoder. Furthermore, the similarity value between the representations during the training is maximized. Our experiments show that the proposed model outperforms base systems on a social media corpus."}, {"heading": "1 Introduction", "text": "For long and normal documents, the extractive summary performs satisfactorily by selecting a few sentences from the source code (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016), but it does not apply to the summary of Chinese social media texts, where the texts are comparatively short and often noisy. Therefore, an abstract text summary based on an encoder decoder framework is a better choice (Rush et al., 2015; Hu et al., 2015). For an extractive summary, the sentences selected have a high semantic relevance to the text. However, for an abstract text summary, current models tend to produce gramativities and coherent summaries, regardless of their semantic relevance with the source code. Figure 1 shows that the summary generated by a current model (RNN encoder e.coural) improves our source text component literally."}, {"heading": "2 Background: Chinese Abstractive Text Summarization", "text": "The current Chinese model of text summary in social media is based on an encoder-decoder framework. The encoder-decoder model is capable of compressing source code x = {x1, x2,..., xN} into a continuous vector representation with an encoder and then generating the summary y = {y1, y2,..., yM} with a decoder. In the previous work (Hu et al., 2015), the encoder is a bidirectionally controlled recursive neural network that maps source code into sentence vectors {h1, h2,..., hN}. The decoder is a unidirectional recursive neural network that produces the distribution of output words yt with a previous hidden state st \u2212 1 and word yt \u2212 1: p (yt | x) = softmaxf (st \u2212 1, yt \u2212 1) (1) (1), where the output function is the neural and the output function is the last one."}, {"heading": "3 Proposed Model", "text": "Figure 2 shows our proposed model. The model consists of three components: encoder, decoder and a similarity function. The encoder compresses source code into semantic vectors, and the decoder generates summaries and generates semantic vectors of the generated summaries. Finally, the similarity function evaluates the relevance between the sematic vectors of source code and generated summaries. Our training goal is to maximize the similarity value so that the generated summaries have a high semantic relevance to source code."}, {"heading": "3.1 Text Representation", "text": "There are several methods to represent a text or a sentence, such as the middle summary of the RNN output or the retention of the last state of the RNN. In our model, the source code is represented by a gated attention encoder (Hahn and Keller, 2016). Each coming word is fed into a gated attention network that measures its meaning. The gated attention network outputs the important score with a feedback network. At each step, there is a word vector et and its previous context vector inserted, then outputs the score \u03b2t. Then, the word vector et is multiplied by the score \u03b2t and inserted into the RNN encoder. However, this method wastes a lot of time as we select the last hN of the RNN encoder as the semantic vector of the source code Vt.A natural idea to get the semantic vector of a summary is to feed it into the encoder as well. However, this method wastes a lot of time because we code the last sentence of M twice (in fact, the last sentence contains the summary of M)."}, {"heading": "3.2 Semantic Relevance", "text": "Our goal is to calculate the semantic relevance of source code and generated summary taking into account the semantic vectors Vt and Vs. Here, we use cosinal similarity to measure the semantic relevance represented by a point product and a quantity: cos (Vs, Vt) = Vs \u00b7 Vt, Vs, Vs and Vt (5) source code and summary share the same language, so it is reasonable to assume that their semantic vectors are distributed in the same space. Cosmic similarity is a good way to measure the distance between two vectors in the same space."}, {"heading": "3.3 Training", "text": "Taking into account the model parameter \u03b8 and the input text x, the model generates the corresponding summary y and the semantic vector Vs and Vt. The aim is to minimize the loss function: L = \u2212 p (y | x; \u03b8) \u2212 \u03bbcos (Vs, Vt) (6), where p (y | x; \u03b8) is the conditional probability of summaries of given source code and is calculated by the encoder decoder model. cos (Vs, Vt) is cosmic similarity of the semantic vectors Vs and Vt. This term attempts to maximize the semantic relevance between source and target output."}, {"heading": "4 Experiments", "text": "In this section, we present the evaluation of our model and show its performance on a popular social media corpus. We also use a case to explain the semantic relevance between generated summary and source code."}, {"heading": "4.1 Dataset", "text": "Our data set is the Large Scale Chinese Short Text Summarization Dataset (LCSTS) created by Hu et al. (2015). The data set consists of more than 2.4 million summary text pairs created by a famous Chinese social media site called Sina Weibo1. It is divided into three parts, with 2,400,591 pairs in Part I, 10,666 pairs in Part II, and 1,106 pairs in Part III. All summary pairs in Part II and Part III are manually entered with relevant values from 1 to 5, and we reserve only pairs with values not less than 3. Following the previous work, we use Part I as a training set, Part II as a development set, and Part III as a test set."}, {"heading": "4.2 Experiment Setting", "text": "To reduce the risk of word segmentation errors (Xu and Sun, 2016), we use Chinese strings as both source and target output. We limit the size of the model vocabulary to 4000, which covers most of the common characters. Each character is represented by a randomly initialized word embedding. We adjust our parameters to the development set. In our model, the embed size is 400, the hidden state size of the encoder decoder 500, and the size of the gated attention network 1000. We use Adam Optimizer to learn the model parameters, and the stack size is set to 32. The parameter \u03bb is 0.0001. Both the encoder and the decoder are based on the LSTM unit. After previous work (Hu et al., 2015), our rating metric of ROUGE is F-Score: ROUGE-1, ROUGE-2, and ROUGE-L (Lin 2003, Hovy)."}, {"heading": "4.3 Baseline Systems", "text": "RNN. We call RNN the basic sequence sequence model with bidirectional GRU encoder and unidirectional GRU decoder. It is a widely used language-generated framework, so it is an important baseline.RNN context. RNN context is a sequence sequence sequence system with neural attention. Attention mechanisms help capture the context information of source texts. This model is a stronger base system."}, {"heading": "4.4 Results and Discussions", "text": "We refer to our proposed Semantic Relevance Based Neural Model as SRB. Furthermore, SRB with a gated attention encoder is referred to as + Attention. Table 11weibo.sina.com shows the results of our models and base systems. We can see that SRB generates both RNN and RNN context in the F-score of ROUGE-1, ROUGE-2 and ROUGE-L. It concludes that SRB generates more keywords and phrases. With a gated attention encoder, SRB achieves better performance with 33.3 F-Score of ROUGE-1, 20.0 ROUGE-2 and 30.1 ROUGE-L. It shows that gated attention reduces noisy and unimportant information so that the remaining information represents a clear idea of the source text. Better presentation of the encoder results in better seman-tic relevance of text."}, {"heading": "5 Related Work", "text": "Abstractive text summary has performed well thanks to the sequence sequence model (Sutskever et al., 2014) and the attention mechanism (Bahdanau et al., 2014). Rush et al. (2015) initially used an attention-based encoder to compress texts and a decoder for the language of neural networks to generate summaries. Following this work, a recurring encoder was introduced into the text summary and achieved better performance (Lopyrev, 2015; Chopra et al., 2016). In the direction of Chinese texts, Hu et al. (2015) constructed a large corpus of Chinese short text summaries. In order to deal with unknown word problems, Nallapati et al. (2016) proposed a generator-pointer model so that the decoder is able to generate words in source texts. Gu et al. (2016) minimize this problem by also incorporating the risk parameters."}, {"heading": "6 Conclusion", "text": "Our work aims to improve the semantic relevance of generated summaries and source code for the summary of Chinese social media texts. Our model is capable of converting text and summary into a dense vector and promoting a high similarity in their representation. Experiments show that our model outperforms basic systems and the generated summary has a higher semantic relevance."}, {"heading": "Acknowledgements", "text": "This work was partially supported by the Chinese High Technology Research and Development Program (863 Program, No. 2015AA015404) and the Chinese National Natural Science Foundation (No. 61673028). Xu Sun is the corresponding author of this paper."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Current Chinese social media text summarization models are based on an encoderdecoder framework. Although its generated summaries are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and summaries for Chinese social media summarization. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms baseline systems on a social media corpus.", "creator": "LaTeX with hyperref package"}}}