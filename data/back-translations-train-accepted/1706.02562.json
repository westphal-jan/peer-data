{"id": "1706.02562", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Pain-Free Random Differential Privacy with Sensitivity Sampling", "abstract": "Popular approaches to differential privacy, such as the Laplace and exponential mechanisms, calibrate randomised smoothing through global sensitivity of the target non-private function. Bounding such sensitivity is often a prohibitively complex analytic calculation. As an alternative, we propose a straightforward sampler for estimating sensitivity of non-private mechanisms. Since our sensitivity estimates hold with high probability, any mechanism that would be $(\\epsilon,\\delta)$-differentially private under bounded global sensitivity automatically achieves $(\\epsilon,\\delta,\\gamma)$-random differential privacy (Hall et al., 2012), without any target-specific calculations required. We demonstrate on worked example learners how our usable approach adopts a naturally-relaxed privacy guarantee, while achieving more accurate releases even for non-private functions that are black-box computer programs.", "histories": [["v1", "Thu, 8 Jun 2017 13:06:34 GMT  (121kb,D)", "http://arxiv.org/abs/1706.02562v1", "12 pages, 9 figures, 1 table; full report of paper accepted into ICML'2017"]], "COMMENTS": "12 pages, 9 figures, 1 table; full report of paper accepted into ICML'2017", "reviews": [], "SUBJECTS": "cs.LG cs.CR cs.DB stat.ML", "authors": ["benjamin i p rubinstein", "francesco ald\u00e0"], "accepted": true, "id": "1706.02562"}, "pdf": {"name": "1706.02562.pdf", "metadata": {"source": "META", "title": "Pain-Free Random Differential Privacy with Sensitivity Sampling", "authors": ["Benjamin I. P. Rubinstein", "Francesco Ald\u00e0"], "emails": ["<brubinstein@unimelb.edu.au>,", "<francesco.alda@rub.de>."], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "2. Background", "text": "We are interested in non-private mechanisms f: Dn \u2192 B that record databases in the product room via answers in a standardized space. < D (S) -D (S) -D (S) -D (S) -D (S) -D (S) -D (S) -D (S) -D (S) -D (S) -D (S) -D (S) -D (S) -D (S) -D (S) -D (S) -D (S) -D (S) -D (S) -D) -D (S) -D (S) -D (S) -D (S) -D (S) -D (S) -D -D) -D (D) -D (S) -D) (D) (D) -D (S) -D (D) -D (D) -D) (D) -D (D) (D)."}, {"heading": "3. Problem Statement", "text": "We consider a statistician who wishes to apply a differentiated private mechanism to a f: Dn \u2192 B whose sensitivity cannot be readily limited analytically (cf. example 6 or the case of a computer program). Instead, we assume that the statistician is able to extract Pn + 1 to Dn + 1 from any product room, can evaluate f arbitrarily (and in particular on the basis of the result of that sample), and is interested in applying a privatization mechanism with a guarantee of random differential privacy (definition 4). Note 7. Natural choices for P lend themselves to the sample or definition of random differential privacy. P could be taken as the underlying distribution from which a sensitive DB was drawn - in the case of sensitive training data but insensitive data sources; an alternative test distribution of interest in the case of domain adaptation; or P could be uniform or otherwise non-informative probability (see example 6)."}, {"heading": "4. Sensitivity-Induced Differential Privacy", "text": "If a privatization mechanism M is known to achieve a differentiated privacy, we can introduce the concept of sensitivity-induced privacy (). (D) Definition (D) Definition (D) Definition (D) Definition (D) Definition (D) Definition (D) Definition (D) Definition (D) Definition (D) Definition (D) Definition (D) Definition (D) Definition (D) Definition (D) Definition (D) Definition (D) (D) Definition) (D) (D) Definition) (D) Definition) (D) (D) (D)"}, {"heading": "5. The Sensitivity Sampler", "text": "Consider the privacy-insensitive independent sample D1,.., Dm \u0445 Pn + 1 of databases on n + 1 records, with algorithm 2 SAMPLE-THEN-RESPOND making privacy decisions. However, the randomized mechanism M \u2206: B \u2192 R; target mapping f: Dn \u2192 B, sample size m, distribution index k, distribution potential P set in SENSITIVITY SAMPLER (| D, m, k, P) response M \u0445 (D) P is chosen to correspond to the desired distribution of random differential privacy. A number of natural choices are available for P (cf. Remark 7). The main idea of SENSITIVITYSAMPLER is that for every extended database observation of D-CDN + 1, we explicitly CD.i.d. observations G1, we."}, {"heading": "5.1. Practicalities", "text": "SENSITIVITY AMPLER simplifies the application of differential privacy by avoiding the challenge of limited sensitivity. Therefore, it is important to examine all the practical problems that arise in implementing it. The algorithm itself comprises only a few main steps: the collection of databases, the measurement of sensitivity, the sorting, the identification of statistical research (inversion), followed by the sensitivity-induced private mechanism. Sampling. as discussed in Remark 7, a number of natural choices for sample distribution P could be made. Where a simulation process exists that can generate synthetic data, this could lead to an approximation to D. For example, in the Bayesian environment (Dimitrakakis et al., 2014) a number of natural options for sample distribution P could be used."}, {"heading": "6. Analysis", "text": "For the i.i.d. sample of sensitivities G1,., Gm drawn within algorithm 1; < Gm drawn within algorithm 1; < Gm drawn within algorithm 1, denotes the corresponding fixed unknown CDF, and the corresponding random empirical CDF, the random sensitivity of a (non-private, possibly deterministic) mapping f: Dn \u2192 R to achieve sensitivity, allowing the limitation of RDP theorem 15. Consider all non-private mappings f: Dn \u2192 B, any sensitivity-induced (, potentially deterministic) mapping B to (randomized) responses in R, any database D of n datasets, privacy parameters > 0, 1)."}, {"heading": "7. Experiments", "text": "We will now show the practical value of SENSITIVITY AMPLER. First, in Section 7.1, we will show how the sensitivity of SENSITIVITY AMPLER can quickly approach analytical sensitivity with high probability, and how it can be significantly lower than the global sensitivity in the worst case scenario in Section 7.2. Operating privatization mechanisms with lower sensitivity parameters can reduce benefit losses while maintaining (a weaker form of) differential privacy. We will present experimental evidence of these benefit savings in Section 7.3. While areas of application find the alternative balance to utility value attractive in themselves, it should be emphasized that a major advantage of SENSITIVITY AMPLER lies in its ease of implementation."}, {"heading": "7.1. Analytical RDP vs. Sampled Sensitivity", "text": "Consider Example 6: Private release of the sample mean f (D) = n \u2212 1 \u2211 n i = 1Di of a database D, which is generally obtained from Exp (1). Figure 4 shows with different probabilities \u03b3: the analytical limit between sensitivity and SENSITIVITY SAMPLER estimates for different sampling budgets on average over 50 repetitions. For fixed sampling budgets \u0445 \u0439 is estimated at lower limits for \u03b3, which quickly approach exactly."}, {"heading": "7.2. Global Sensitivity vs. Sampled Sensitivity", "text": "Let us now consider the challenging goal of privately publishing an SVM classifier that matches sensitive training data. By applying the Laplace mechanism for releasing the primary normal vector, Rubinstein et al. (2012) bound the sensitivity of the vector using the algorithmic stability of the SVM. In particular, a lengthy derivative notes that the sensitivity of the SVM in a statistically consistent formulation of the SVM with the convex L-lipschitz loss, the d-dimensional feature imaging with Supx k (x, x) and the regulation parameter C. While the original work (and others since) did not take into account the practical problem of releasing the unregulated bias term b, we can equate this sensitivity with a short argument in the appendix D.Proposition 19. For the SVM run with hinged loss, linear kernel, D = [1, d] the release of the SVM (400w) of the global NM position."}, {"heading": "7.3. Effect on Utility", "text": "Support the vector classification. We return to the same SVM setting as in the previous section, where d = 2, now presents the utility as a misclassification error (averaging over 500 repetitions) vs. data protection budget. Here, we set \u03b3 = 0.05 and also include the performance of the non-private SVM as a possible limit of benefit. See Figure 6. At very high levels of data protection, both private SVMs suffer the same bad error. However, at a lower level of data protection, the misclassification error of the SENSITIVITYSAMPLER decreases rapidly until it reaches the non-private rate. At the same time, the global sensitivity approach has a much higher value and suffers from a much slower decline. These results suggest that the SENSITIVITYSAMPLER can achieve a much better utility value in addition to sensitivity. Finally, we consider a one-dimensional (d = 1) KDE setting as a KDE setting. In Figure 7, we show the error (average of a repeatable stone of over 1000 amber = 0.0001)."}, {"heading": "8. Conclusion", "text": "In this paper, we propose SENSITIVITY SAMPLER, an algorithm for empirical estimation of sensitivity to the privatization of black box functions. Our work addresses an important usability gap in the differential privacy, where several generic privatization mechanisms exist complete with guarantees of privacy and utility, but require analytical limits on global sensitivity (a Lipschitz condition) for the non-private target. While this sensitivity is derived trivially for simple statistics, sensitivity derivations are tedious for state-of-the-art learners, e.g. in collaborative filtering (McSherry & Mironov, 2009), SVMs (Rubinstein et al., 2012; Chaudhuri et al., 2011), in model selection (Thakurta & Smith, 2013), in feature selection (Kifer et al., 2012), in the learning selection (Dmaladi, 2012), in the learning selection (Wang & al, 2015)."}, {"heading": "Acknowledgements", "text": "F. Alda and B. Rubinstein acknowledge the support of the DFG Research Training Group RTG 1817 / 1 and the Australian Research Council (DE160100584)."}, {"heading": "A. Proof of Proposition 8", "text": "Due to Pinsker's inequality, the product measurements have limited the total variation distance. A denotes the event that holds -DP (similar to (, \u03b4) DP) on n datasets in adjacent databases: A = {HQ, R, R (M (D), R) \u2264 e Pr (M (D), R)}. RDP wrt Q follows as Qn + 1 (A) \u2265 Pn + 1 (A) \u2212 \u221a (n + 1) \u0432 / 2 \u2265 1 \u2212 \u03b3 \u2212 \u221a (n + 1) \u0432 / 2."}, {"heading": "B. Optimising Sampler Performance with \u03c1", "text": "This section presents precise statements and evidence for the expressions found in Table 1.B.1. \u2212 k \u2212 for the expressions found in Table 1.B.1. \u2212 k \u2212 for the expressions found in Table 1.B.1. \u2212 k \u2212 for the expressions found in Table 1.B.1. \u2212 k \u2212 for the expressions found in Table 1.B.1. \u2212 k \u2212 for the expressions found in Table 1.B.1. \u2212 k \u2212 for the expressions found in Table 1.B.1. \u2212 k \u2212 for the expressions found in Table 1.B.1. \u2212 k \u2212 k \u2212 for the expressions found in Table 1.B.1. \u2212 k \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b b \u2212 b b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 table \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 f \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 f \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 f \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b"}, {"heading": "C. Global vs. Sampled Sensitivity: Sample Mean of Bounded Data", "text": "Consider the objective of releasing the sample mean f (D) = n \u2212 1 \u2211 n i = 1Di of a database D as in Example 6, but above domain D = [0, 1] d. Figure 9 shows: the (sharp) limit of global sensitivity for this target for use, for example, in the Laplace mechanism; and the sensitivity estimated by SENSITIVITY SAMPLER. D includes n = 500 points sampled from the uniform distribution over D, with SENSITIVITY SAMPLER running with optimized m under varying \u03b3 as indicated. Remarkable is the sensitivity reduction due to the sample (note the protocol scale). This experiment demonstrates sensitivity to different privacy guarantees (DP vs. RDP). In contrast to the same level of privacy (RDP) in Section 7.1, SENSIVITYSAMPLER quickly approaches the analytical approach."}, {"heading": "D. Proof of Proposition 19", "text": "From the solution b = yi \u2212 \u2211 n j = 1 \u03b1jyjk (Di, Dj) for some i [n], combined with the box constraints 0 \u2264 \u03b1j \u2264 C / n, the sensitivity of the bias can be limited to 2 + 2C \u221a d. Combined with the existing normal vector sensitivity, the result is."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Popular approaches to differential privacy, such as the Laplace and exponential mechanisms, calibrate randomised smoothing through global sensitivity of the target non-private function. Bounding such sensitivity is often a prohibitively complex analytic calculation. As an alternative, we propose a straightforward sampler for estimating sensitivity of non-private mechanisms. Since our sensitivity estimates hold with high probability, any mechanism that would be ( , \u03b4)differentially private under bounded global sensitivity automatically achieves ( , \u03b4, \u03b3)-random differential privacy (Hall et al., 2012), without any target-specific calculations required. We demonstrate on worked example learners how our usable approach adopts a naturally-relaxed privacy guarantee, while achieving more accurate releases even for non-private functions that are black-box computer programs.", "creator": "LaTeX with hyperref package"}}}