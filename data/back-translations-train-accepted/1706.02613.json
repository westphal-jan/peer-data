{"id": "1706.02613", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Decoupling \"when to update\" from \"how to update\"", "abstract": "Deep learning requires data. A useful approach to obtain data is to be creative and mine data from various sources, that were created for different purposes. Unfortunately, this approach often leads to noisy labels. In this paper, we propose a meta algorithm for tackling the noisy labels problem. The key idea is to decouple \"when to update\" from \"how to update\". We demonstrate the effectiveness of our algorithm by mining data for gender classification by combining the Labeled Faces in the Wild (LFW) face recognition dataset with a textual genderizing service, which leads to a noisy dataset. While our approach is very simple to implement, it leads to state-of-the-art results. We analyze some convergence properties of the proposed algorithm.", "histories": [["v1", "Thu, 8 Jun 2017 14:37:45 GMT  (781kb,D)", "http://arxiv.org/abs/1706.02613v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["eran malach", "shai shalev-shwartz"], "accepted": true, "id": "1706.02613"}, "pdf": {"name": "1706.02613.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Related Work", "text": "In fact, it is the case that you are able to be in a position without being able to move."}, {"heading": "3 Method", "text": "As already mentioned, to address the problem of loud labels, we propose to change the update rule, which is commonly used in the deep learning optimization algorithms, to decouple the decision of \"when to update\" from \"how to update.\" In our approach, the decision of \"when to update\" does not depend on the label. Instead, it depends on a disagreement between two different networks. This method could generally be regarded as a meta-algorithm that uses two base classifiers and performs updates according to a base learning algorithm, but only based on examples where there are discrepancies between the two classifiers. To formalize this, let X be an instance space and Y the label space, and assume that we perform examples from a distribution D over X \u00d7 Y, with possibly loud labels. We would like to train a classifier h that comes from a class hypothesis, and we rely on the current H, as well as a mini update h."}, {"heading": "4 Theoretical analysis", "text": "We focus on analyzing the properties of our algorithm for linearly separable data corrupted by random marking noise, and while using perception as the base algorithm, let's leave X = {x-Rd: [x-Rd:] x-Rd: [x-Rd:] x-Rd: [x-Rd:] x-Rd: [x-Rd:] x-Rd: [x-Rd:] x-Rd: [x-Rd] x-Rd: [x-Rd:] x-Rd: (x-Rd) x-Rd: (x-Rd) x-Rd: (x-Rd) x-Rd: (x-Rd) x-Rd: (x-R) x-Rd: (x-R) x-R: (x-R) x-R."}, {"heading": "5 Experiments", "text": "Our main experiment is to use our deep network algorithm in a real scenario with loud labels. Specifically, we use a hypotheses class of deep networks and a stochastic gradient descent with dynamics as the basis for updating the rule. The task is to classify facial images by gender. As training data, we use the Labeled Faces in the Wild (LFW) dataset, for which we had a label on the face name but no gender label. To construct gender labels, we used an external service that offers gender labels based on names. This process led to loud labels. We show that our method yields state-of-the-art results in this task compared with competing methods of noise robustness. In addition, we conducted controlled experiments to demonstrate the performance of our linear classification algorithm with different noise levels."}, {"heading": "5.1 Deep Learning", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "6 Discussion", "text": "We have shown that this simple approach leads to current results. Our theoretical analysis shows that this approach leads to a rapid convergence rate when the underlying updating rule is the perceptron. We have also shown that the proof that the method of an optimal solution is based on distribution assumptions must be based on distribution assumptions. There are several immediate open questions that we will leave to future work. First, by proposing distribution assumptions that are likely to persist in practice and proving that the algorithm converges to an optimal solution under these assumptions. Second, by extending the convergence evidence beyond linear predictors. While obtaining absolute convergence guarantees seems unattainable at the moment, it may be possible to establish oracle-based convergence guarantees."}, {"heading": "A Proofs", "text": "Proof of Lemma 1: Let the distribution across instances be evenly concentrated over the vectors of the standard base, e1,.., ed. Let w \u0445 have any vector in {\u00b1 1} d. Then, with probability 1 / 4 about the choice of w (1) 0, w (2) 0, we have that the characters of < w (1) 0, ei >, < w (2) 0, ei > will match each other, but not with < w, ei >. It is easy to see that the i'te coordinate of w (1) and w (2) will never be updated. Therefore, no matter how many iterations we will perform, the solution will be wrong on ei. Consequently, the probability of an error is lower than the probability of being limited by the random variable 1d = 1 Zi, the zi variables are i.i.d. Bernoulli variables with [Zi = 1-1 / ff] is."}, {"heading": "B Experimental Results", "text": "We will demonstrate the performance of our algorithm in two controlled setups using a perceptron-based algorithm. In the first setup, we will test our algorithm on synthetic data generated by randomly scanning instances from the Ball in Rd unit, with different probabilities for random label flips. In the second setup, we will test our performance on a binary classification task based on the MNIST dataset, again with random label flips with different probabilities. We will show that our adjustment of the perceptron algorithm results in resilience for high noise probabilities, as opposed to the Vanilla Perceptron algorithm, which is unable to convert even small amounts of Noise.B.1 Linear classification on synthetic data to test the performance of the proposed perceptron-like algorithm."}], "references": [{"title": "Two-view feature generation model for semisupervised learning", "author": ["Rie Kubota Ando", "Tong Zhang"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Training connectionist networks with queries and selective sampling", "author": ["Les E Atlas", "David A Cohn", "Richard E Ladner", "Mohamed A El-Sharkawi", "Robert J Marks", "ME Aggoune", "DC Park"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1989}, {"title": "The power of localization for efficiently learning linear separators with noise", "author": ["Pranjal Awasthi", "Maria Florina Balcan", "Philip M Long"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Decontamination of training samples for supervised pattern recognition methods. In Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)", "author": ["Ricardo Barandela", "Eduardo Gasca"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Training deep neural-networks based on unreliable labels", "author": ["Alan Joseph Bekker", "Jacob Goldberger"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Combining labeled and unlabeled data with cotraining", "author": ["Avrim Blum", "Tom Mitchell"], "venue": "In Proceedings of the eleventh annual conference on Computational learning theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Label-noise robust logistic regression and its applications", "author": ["Jakramate Bootkrajang", "Ata Kab\u00e1n"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Boosting in the presence of label noise", "author": ["Jakramate Bootkrajang", "Ata Kab\u00e1n"], "venue": "arXiv preprint arXiv:1309.6818,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "The balanced accuracy and its posterior distribution", "author": ["Kay Henning Brodersen", "Cheng Soon Ong", "Klaas Enno Stephan", "Joachim M Buhmann"], "venue": "In Pattern recognition (ICPR),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Identifying mislabeled training data", "author": ["Carla E. Brodley", "Mark A. Friedl"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Improving generalization with active learning", "author": ["David Cohn", "Les Atlas", "Richard Ladner"], "venue": "Machine learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1994}, {"title": "On the robustness of convnets to training on noisy labels", "author": ["David Flatow", "Daniel Penner"], "venue": "penner_report.pdf,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Classification in the presence of label noise: a survey", "author": ["Beno\u0131\u0302t Fr\u00e9nay", "Michel Verleysen"], "venue": "IEEE transactions on neural networks and learning systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Selective sampling using the query by committee algorithm", "author": ["Yoav Freund", "H Sebastian Seung", "Eli Shamir", "Naftali Tishby"], "venue": "Machine learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Training deep neural networks using a noise adaptation layer", "author": ["Jacob Goldberger", "Ehud Ben-Reuven"], "venue": "Under review for ICLR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2017}, {"title": "Entropy regularization. Semi-supervised learning, pages 151\u2013168", "author": ["Yves Grandvalet", "Yoshua Bengio"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Semi-supervised learning by entropy minimization", "author": ["Yves Grandvalet", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["Gary B Huang", "Manu Ramesh", "Tamara Berg", "Erik Learned-Miller"], "venue": "Technical report,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Quality management on amazon mechanical turk", "author": ["Panagiotis G Ipeirotis", "Foster Provost", "Jing Wang"], "venue": "In Proceedings of the ACM SIGKDD workshop on human computation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Probabilistic learning from mislabelled data for multimedia content recognition", "author": ["Pravin Kakar", "Alex Yong-Sang Chia"], "venue": "In Multimedia and Expo (ICME),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Efficient noise-tolerant learning from statistical queries", "author": ["Michael Kearns"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Neural network ensembles, cross validation, and active learning", "author": ["Anders Krogh", "Jesper Vedelsby"], "venue": "Advances in neural information processing systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1995}, {"title": "Design of robust neural network classifiers", "author": ["Jan Larsen", "L Nonboe", "Mads Hintz-Madsen", "Lars Kai Hansen"], "venue": "In Acoustics, Speech and Signal Processing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Age and gender classification using convolutional neural networks", "author": ["Gil Levi", "Tal Hassner"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Evaluation of face recognition apis and libraries", "author": ["Philip Masek", "Magnus Thulin"], "venue": "Master\u2019s thesis, University of Gothenburg,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "An empirical comparison of three boosting algorithms on real data sets with artificial class noise", "author": ["Ross A McDonald", "David J Hand", "Idris A Eckley"], "venue": "In International Workshop on Multiple Classifier Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2003}, {"title": "Learning from binary labels with instance-dependent corruption", "author": ["Aditya Krishna Menon", "Brendan van Rooyen", "Nagarajan Natarajan"], "venue": "arXiv preprint arXiv:1605.00751,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Learning to label aerial images from noisy data", "author": ["Volodymyr Mnih", "Geoffrey E Hinton"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Learning with noisy labels", "author": ["Nagarajan Natarajan", "Inderjit S Dhillon", "Pradeep K Ravikumar", "Ambuj Tewari"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Analyzing the effectiveness and applicability of co-training", "author": ["Kamal Nigam", "Rayid Ghani"], "venue": "In Proceedings of the ninth international conference on Information and knowledge management,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2000}, {"title": "Loss factorization, weakly supervised learning and label noise robustness", "author": ["Giorgio Patrini", "Frank Nielsen", "Richard Nock", "Marcello Carioni"], "venue": "arXiv preprint arXiv:1602.02450,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Making neural networks robust to label noise: a loss correction approach", "author": ["Giorgio Patrini", "Alessandro Rozza", "Aditya Menon", "Richard Nock", "Lizhen Qu"], "venue": "arXiv preprint arXiv:1609.03683,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Training deep neural networks on noisy labels with bootstrapping", "author": ["Scott Reed", "Honglak Lee", "Dragomir Anguelov", "Christian Szegedy", "Dumitru Erhan", "Andrew Rabinovich"], "venue": "arXiv preprint arXiv:1412.6596,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Active learning literature survey", "author": ["Burr Settles"], "venue": "University of Wisconsin, Madison,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Query by committee", "author": ["H Sebastian Seung", "Manfred Opper", "Haim Sompolinsky"], "venue": "In Proceedings of the fifth annual workshop on Computational learning theory,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1992}, {"title": "Training convolutional networks with noisy labels", "author": ["Sainbayar Sukhbaatar", "Joan Bruna", "Manohar Paluri", "Lubomir Bourdev", "Rob Fergus"], "venue": "arXiv preprint arXiv:1406.2080,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Learning from massive noisy labeled data for image classification", "author": ["Tong Xiao", "Tian Xia", "Yi Yang", "Chang Huang", "Xiaogang Wang"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Semi-supervised learning literature survey", "author": ["Xiaojin Zhu"], "venue": "Computer Sciences TR 1530,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2005}], "referenceMentions": [{"referenceID": 18, "context": "Unfortunately, this gives rise to a problem of abundant noisy labels - labels may often be corrupted [19], which might deteriorate the performance of neural-networks [12].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "Unfortunately, this gives rise to a problem of abundant noisy labels - labels may often be corrupted [19], which might deteriorate the performance of neural-networks [12].", "startOffset": 166, "endOffset": 170}, {"referenceID": 33, "context": "The idea of deciding \u201cwhen to update\u201d based on a disagreement between classifiers is closely related to approaches for active learning and selective sampling - a setup in which the learner does not have unlimited access to labeled examples, but rather has to query for each instance\u2019s label, provided at a given cost (see for example [34]).", "startOffset": 334, "endOffset": 338}, {"referenceID": 13, "context": "Specifically, the well known query-by-committee algorithm maintains a version space of hypotheses and at each iteration, decides whether to query the label of a given instance by sampling two hypotheses uniformly at random from the version space [14, 35].", "startOffset": 246, "endOffset": 254}, {"referenceID": 34, "context": "Specifically, the well known query-by-committee algorithm maintains a version space of hypotheses and at each iteration, decides whether to query the label of a given instance by sampling two hypotheses uniformly at random from the version space [14, 35].", "startOffset": 246, "endOffset": 254}, {"referenceID": 24, "context": "To find the gender for each image, we use an online service to match a gender to a given name (as is suggested by [25]), a method which is naturally prone to noisy labels (due to unisex names).", "startOffset": 114, "endOffset": 118}, {"referenceID": 12, "context": "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31].", "startOffset": 116, "endOffset": 120}, {"referenceID": 2, "context": "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31].", "startOffset": 301, "endOffset": 334}, {"referenceID": 6, "context": "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31].", "startOffset": 301, "endOffset": 334}, {"referenceID": 7, "context": "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31].", "startOffset": 301, "endOffset": 334}, {"referenceID": 20, "context": "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31].", "startOffset": 301, "endOffset": 334}, {"referenceID": 22, "context": "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31].", "startOffset": 301, "endOffset": 334}, {"referenceID": 25, "context": "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31].", "startOffset": 301, "endOffset": 334}, {"referenceID": 26, "context": "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31].", "startOffset": 301, "endOffset": 334}, {"referenceID": 28, "context": "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31].", "startOffset": 301, "endOffset": 334}, {"referenceID": 30, "context": "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31].", "startOffset": 301, "endOffset": 334}, {"referenceID": 0, "context": "Beyond these approaches, there are methods that assume a small clean data set and another large, noisy, or even unlabeled, data set [1, 6, 30, 38].", "startOffset": 132, "endOffset": 146}, {"referenceID": 5, "context": "Beyond these approaches, there are methods that assume a small clean data set and another large, noisy, or even unlabeled, data set [1, 6, 30, 38].", "startOffset": 132, "endOffset": 146}, {"referenceID": 29, "context": "Beyond these approaches, there are methods that assume a small clean data set and another large, noisy, or even unlabeled, data set [1, 6, 30, 38].", "startOffset": 132, "endOffset": 146}, {"referenceID": 37, "context": "Beyond these approaches, there are methods that assume a small clean data set and another large, noisy, or even unlabeled, data set [1, 6, 30, 38].", "startOffset": 132, "endOffset": 146}, {"referenceID": 32, "context": "[33] proposed to change the cross entropy loss function by adding a regularization term that takes into account the current prediction of the network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "This method is inspired by a technique called minimum entropy regularization, detailed in [16, 17].", "startOffset": 90, "endOffset": 98}, {"referenceID": 16, "context": "This method is inspired by a technique called minimum entropy regularization, detailed in [16, 17].", "startOffset": 90, "endOffset": 98}, {"referenceID": 11, "context": "It was also found to be effective by [12], which suggested a further improvement of this method by effectively increasing the weight of the regularization term during the training procedure.", "startOffset": 37, "endOffset": 41}, {"referenceID": 27, "context": "[28] suggested to use a probablilstic model that models the conditional probability of seeing a wrong label, where the correct label is a latent variable of the model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "While [28] assume that the probablity of label-flips between classes is known in advance, a follow-up work by [36] extends this method to a case were these probabilities are unknown.", "startOffset": 6, "endOffset": 10}, {"referenceID": 35, "context": "While [28] assume that the probablity of label-flips between classes is known in advance, a follow-up work by [36] extends this method to a case were these probabilities are unknown.", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "An improved method, that takes into account the fact that some instances might be more likely to have a wrong label, has been proposed recently in [15].", "startOffset": 147, "endOffset": 151}, {"referenceID": 14, "context": "It is worth noting that there are some other works that suggest methods that are very similar to [15, 36], with a slightly different objective or training method [5, 20], or otherwise suggest a complicated process which involves estimation of the class-dependent noise probabilities [32].", "startOffset": 97, "endOffset": 105}, {"referenceID": 35, "context": "It is worth noting that there are some other works that suggest methods that are very similar to [15, 36], with a slightly different objective or training method [5, 20], or otherwise suggest a complicated process which involves estimation of the class-dependent noise probabilities [32].", "startOffset": 97, "endOffset": 105}, {"referenceID": 4, "context": "It is worth noting that there are some other works that suggest methods that are very similar to [15, 36], with a slightly different objective or training method [5, 20], or otherwise suggest a complicated process which involves estimation of the class-dependent noise probabilities [32].", "startOffset": 162, "endOffset": 169}, {"referenceID": 19, "context": "It is worth noting that there are some other works that suggest methods that are very similar to [15, 36], with a slightly different objective or training method [5, 20], or otherwise suggest a complicated process which involves estimation of the class-dependent noise probabilities [32].", "startOffset": 162, "endOffset": 169}, {"referenceID": 31, "context": "It is worth noting that there are some other works that suggest methods that are very similar to [15, 36], with a slightly different objective or training method [5, 20], or otherwise suggest a complicated process which involves estimation of the class-dependent noise probabilities [32].", "startOffset": 283, "endOffset": 287}, {"referenceID": 36, "context": "Another method from the same family is the one described in [37], who suggests to differentiate between \u201cconfusing\u201d noise, where some features of the example make it hard to label, or otherwise a completely random label noise, where the mislabeling has no clear reason.", "startOffset": 60, "endOffset": 64}, {"referenceID": 3, "context": "From the family of preprocessing methods, we mention [4, 10], that try to eliminate instances that are suspected to be mislabeled.", "startOffset": 53, "endOffset": 60}, {"referenceID": 9, "context": "From the family of preprocessing methods, we mention [4, 10], that try to eliminate instances that are suspected to be mislabeled.", "startOffset": 53, "endOffset": 60}, {"referenceID": 32, "context": "In particular, from the family of modified loss function we chose the two variants of the regularized cross entropy loss suggested by [33] (soft and hard bootsrapping).", "startOffset": 134, "endOffset": 138}, {"referenceID": 14, "context": "From the family of adding a layer that models the noise, we chose to compare to one of the models suggested in [15] (which is very similar to the model proposed by [36]), because this model does not require any assumptions or complication of the training process.", "startOffset": 111, "endOffset": 115}, {"referenceID": 35, "context": "From the family of adding a layer that models the noise, we chose to compare to one of the models suggested in [15] (which is very similar to the model proposed by [36]), because this model does not require any assumptions or complication of the training process.", "startOffset": 164, "endOffset": 168}, {"referenceID": 13, "context": "In [14] a thorough analysis is provided for various base algorithms implementing the query-by-committe update rule, and particularly they analyze the perceptron base algorithm under some strong distributional assumptions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "In other works, an ensemble of neural networks is trained in an active learning setup to improve the generalization of neural networks [2, 11, 22].", "startOffset": 135, "endOffset": 146}, {"referenceID": 10, "context": "In other works, an ensemble of neural networks is trained in an active learning setup to improve the generalization of neural networks [2, 11, 22].", "startOffset": 135, "endOffset": 146}, {"referenceID": 21, "context": "In other works, an ensemble of neural networks is trained in an active learning setup to improve the generalization of neural networks [2, 11, 22].", "startOffset": 135, "endOffset": 146}, {"referenceID": 17, "context": "The images were taken from the Labeled Faces in the Wild (LFW) benchmark [18].", "startOffset": 73, "endOffset": 77}, {"referenceID": 24, "context": "Since the gender of each subject is not provided, we follow the method of [25] and use a service that determines a person\u2019s gender by their name (if it is recognized), along with a confidence level.", "startOffset": 74, "endOffset": 78}, {"referenceID": 23, "context": "We use a network architecture suggested by [24], using an available tensorflow implementation1.", "startOffset": 43, "endOffset": 47}, {"referenceID": 8, "context": "Since the amount of male and female subjects in the dataset is not balanced, we use an objective of maximizing the balanced accuracy [9] - the average accuracy obtained on either class.", "startOffset": 133, "endOffset": 136}, {"referenceID": 32, "context": "We inspect the balanced accuracy on our test data during the training process, comparing our method to a vanilla neural network training, as well as to soft and hard bootstrapping described in [33] and to the s-model described in [15], all of which are using the same network architecture.", "startOffset": 193, "endOffset": 197}, {"referenceID": 14, "context": "We inspect the balanced accuracy on our test data during the training process, comparing our method to a vanilla neural network training, as well as to soft and hard bootstrapping described in [33] and to the s-model described in [15], all of which are using the same network architecture.", "startOffset": 230, "endOffset": 234}, {"referenceID": 14, "context": "We use the initialization parameters for [15, 33] that were suggested in the original papers.", "startOffset": 41, "endOffset": 49}, {"referenceID": 32, "context": "We use the initialization parameters for [15, 33] that were suggested in the original papers.", "startOffset": 41, "endOffset": 49}, {"referenceID": 14, "context": "The second best algorithm is the s-model described in [15].", "startOffset": 54, "endOffset": 58}], "year": 2017, "abstractText": "Deep learning requires data. A useful approach to obtain data is to be creative and mine data from various sources, that were created for different purposes. Unfortunately, this approach often leads to noisy labels. In this paper, we propose a meta algorithm for tackling the noisy labels problem. The key idea is to decouple \u201cwhen to update\u201d from \u201chow to update\u201d. We demonstrate the effectiveness of our algorithm by mining data for gender classification by combining the Labeled Faces in the Wild (LFW) face recognition dataset with a textual genderizing service, which leads to a noisy dataset. While our approach is very simple to implement, it leads to state-of-the-art results. We analyze some convergence properties of the proposed algorithm.", "creator": "LaTeX with hyperref package"}}}