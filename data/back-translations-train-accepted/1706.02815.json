{"id": "1706.02815", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2017", "title": "From Bayesian Sparsity to Gated Recurrent Nets", "abstract": "The iterations of many first-order algorithms, when applied to minimizing common regularized regression functions, often resemble neural network layers with pre-specified weights. This observation has prompted the development of learning-based approaches that purport to replace these iterations with enhanced surrogates forged as DNN models from available training data. For example, important NP-hard sparse estimation problems have recently benefitted from this genre of upgrade, with simple feedforward or recurrent networks ousting proximal gradient-based iterations. Analogously, this paper demonstrates that more powerful Bayesian algorithms for promoting sparsity, which rely on complex multi-loop majorization-minimization techniques, mirror the structure of more sophisticated long short-term memory (LSTM) networks, or alternative gated feedback networks previously designed for sequence prediction. As part of this development, we examine the parallels between latent variable trajectories operating across multiple time-scales during optimization, and the activations within deep network structures designed to adaptively model such characteristic sequences. The resulting insights lead to a novel sparse estimation system that, when granted training data, can estimate optimal solutions efficiently in regimes where other algorithms fail, including practical direction-of-arrival (DOA) and 3D geometry recovery problems. The underlying principles we expose are also suggestive of a learning process for a richer class of multi-loop algorithms in other domains.", "histories": [["v1", "Fri, 9 Jun 2017 02:56:54 GMT  (844kb,D)", "http://arxiv.org/abs/1706.02815v1", "25 pages long version for our NIPS 2017 submission"], ["v2", "Wed, 2 Aug 2017 17:03:43 GMT  (844kb,D)", "http://arxiv.org/abs/1706.02815v2", null]], "COMMENTS": "25 pages long version for our NIPS 2017 submission", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hao he", "bo xin", "david wipf"], "accepted": true, "id": "1706.02815"}, "pdf": {"name": "1706.02815.pdf", "metadata": {"source": "CRF", "title": "From Bayesian Sparsity to Gated Recurrent Nets", "authors": ["Hao He", "Bo Xin"], "emails": ["hehaodele@pku.edu.cn", "jimxinbo@gmail.com", "davidwipf@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Many practical iterative algorithms for minimizing an energy function Ly (x), parameterized by a certain vector y, adopt the updating approach (t + 1) = f (Ax) + By), (1) where the iteration rate is through the layers (indexed by t) of a deep neural network (DNN), and f is a point-by-point nonlinear operator. Then, when we ask whether we have access to an ensemble game of these iterations, it is similar to being led through the layers (indexed by t) of a deep neural network (DNN) (23, 35, 39, 2). It naturally begs the question of whether we have access to an ensemble of pairs in which x, x, x, x, x, minx, Ly (x) can be a suitably structured Ly (x) to produce a minimum of Ly (x)."}, {"heading": "2 Connecting SBL and LSTM Networks", "text": "This section first covers the basic SBL model, followed by an algorithmic characterization of how the correlation structure can be handled during a sparse estimate. Later, we derive specialized SBL update rules that reveal a close connection with LSTM cells. [1] Also, note that a more recent interesting modification of approximate message delivery [34] (or its unfolded, trainable, deep analog converging to the same solution [9]) can handle certain specialized forms of dictionary correlation; however, the approach does not work with the types of strong arbitrary / unrestricted correlation that we are looking at in this work."}, {"heading": "2.1 Original SBL Model", "text": "Considering an observed vector y-Rn and a dictionary of features \u0435-Rn-m, SBL assumes that the Gaussian probability model and a parameterized zero mean Gaussian are given for the unknown coefficients x-Rm of p (y-x), p (x-x), p (x-ig) and p (x-ig) by the unknown hyperparameters [38]. Since both probabilities and predictions are Gaussian, the posterior p (x-y; g) Gaussian with the mean x-value is also designated as a satisfying x-factor and p as a fixed variance factor [38]. Since both probabilities and predictions are gaussian, the posterior p (x-y; g) is also Gaussian, with the mean x-value being a satisfying x-value."}, {"heading": "2.2 Iterative Reweighted `1 Implementation", "text": "Although not originally derived, SBL can only be implemented with a modified form of iterative re-evaluation (= 1 standard optimization that discloses its power to generate economical estimates). Generally, if we use the \"0 standard of (2) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x) x (2) x (2) x (2) x) x (2) x (2) x) x (2) x (2) x (2) x (2) x (2) x) x (2) x (x) x) x (1) x (x) x (1) x (1) x."}, {"heading": "2.3 Revised SBL Iterations", "text": "The iterative rebalancing of SBL-1 formulas as we have been through (7) can no longer be implemented through the simple, recurring structures that have been used in the past, e.g. (1) to develop an optimal IHT profile or other parity-enhancing iterations. Instead, the additional latent dependencies that result from w, x, and \u03b3 that allow SBL to store or update incrementally learned representations (to create an optimal savings profile) require a more complex network architecture to be updated. Although there are probably several ways in which such an architecture could be developed, in this section we deduce specialized SBL iterations that are mapped directly to one of the most common RNN structures, namely LSTM networks, so that the notation we assume was deliberately chosen to facilitate a later association with LSTM cells."}, {"heading": "2.4 Correspondences with LSTM Components", "text": "In this section, we will find out how the SBL iterations presented in Section 2.3 effectively have the same structure as a canonical LSTM cell (the only differences are the shape of the nonlinearity and the exact details of the gate subnets). To facilitate this goal, Figure 1 includes a canonical LSTM network structure commented on with SBL-derived quantities. We will now go through these correspondences. First, the exogenous input to the network is the observation vector y, which does not change from time step to time step, similar to the strategy used by feedback networks to obtain step-by-step refined representations. [46] The output in the time step t is a gate that serves as a current estimate of the SBL hyperparameters. In contrast, we treat x (t) as an internal LSTM memory cell or the latent cell states.3 This cript reflects the emphasis we put on the learning process while the SL is tucked away."}, {"heading": "3 The Dynamics of SBL Iterations", "text": "Although SBL iterations, as we have shown, can be shaped into an LSTM structure, there are indications that the full potential of this association may be currently undercooked. At this point, we will empirically examine the trajectories of SBL iterations generated by the rules derived from Section 2.3. This process will expose certain characteristic dynamics that point to a richer class of recurring network structures inspired by sequence prediction tasks [15], which will be analyzed later in Section 4 and empirically tested in Section 5."}, {"heading": "3.1 Large Timescale Differences", "text": "We will first present a synthetic experiment that highlights the different time scales on which latent variables SBL = can nonetheless fluctuate over the course of a typical optimization path. (The experimental design is as follows: First, we create a dictionary \u03a6 via\u03a6 = \u03a6 BD, (13) In addition, we can quickly remember that the following elements were drawn from N (0, 1); B \u00b2 R100 \u00b7 100 is a block diagonal matrix of 20, 5 \u00d7 5 blocks, each with unit diagonals and off diagonals set to 0.9; and D \u00b2 R100 \u00b7 100 is a completely diagonal matrix that recalculates each column of the final order to have a unit 2 norm, and finally multiplies it by a random character pattern. This process ensures that we will include 20 clusters of 5 adjacents, each of which will be introduced with strong correlations so that we generate a Rxax."}, {"heading": "3.2 The Potential Value of an Adaptive Updating Schedule", "text": "Although the previous experiment was designed to reveal the different scales of subsets of latent variables, it did not provide any indication of how these different scales can actually affect the final estimation accuracy. Suppose, for example, that we could accelerate the convergence of x for any fixed value of w, would this improve overall performance? This simulation goes directly into this problem. We start with a similar experimental design as in Section 3.1, although we reduce the dimensions for visualization purposes. In short, we select \"R10 \u00d7 20,\" formed of 10 clusters of size 2 each, and \"K\" x, \"0 = 3. Figure 3 (a) indicates the optimal support pattern, with\" 1 \"indicating the localization of a true non-zero and otherwise a zero. Without losing generality, we have also rearranged the dictionary columns so that the first three columns serve as non-zero."}, {"heading": "4 Extension to Gated Feedback Networks", "text": "The discrepancy in the convergence rates described in Section 3.1 occurs in part because the gate and cell updates do not fully resolve the internal loop weighting '1 optimization necessary to calculate a globally optimal x (t + 1) Givenw (t) (see Section 2.3). Variation in the number of internal loop iterations, which means additional versions of (8) \u2212 (11) withw (t) fixed, is a heuristic for the normalization of the various trajectory frequencies, but this requires additional computational effort, and prior knowledge is necessary to manage the iteration for efficiency or final estimate quality, the latter sensitivity being exposed in Section 3.2."}, {"heading": "4.1 Clockwork Networks and Fixed Inner-Loop Iterations", "text": "In fact, it is the case that it will be able to put itself at the top, in the way that it is able to put itself at the top, \"he said.\" We have to be able to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said."}, {"heading": "4.2 Automatically Scheduling Inner-Loops via Gated Feedback", "text": "The gated feedback RNN (GF-RNN) [15] has recently been developed to update the CW-RNN with an additional set of gated connections that de facto allow the network to learn its own clock speeds. In short, the GF-RNN includes stacked LSTM layers (or slightly simpler gated recurrent unit (GRU) layers [14], which can communicate bilaterally via additional, data-dependent gates that can be opened and closed on different timescales. In the context of SBL, this means that we no longer need to burden a specialized LSTM structure with the burden of coordinating trajectory dynamics. Instead, we can stack layers that, at least from a conceptual point of view, are designed to reflect the different dynamics of unequal variable sets such as w (t) or x (t)."}, {"heading": "4.3 Network Design and Training Protocol", "text": "We stack two gated recurrent layers that are loosely designed to mimic the relatively fast SBL adaptation to the basic correlation structure and slower resolution of the final support patterns and coefficient estimates. These layers are built from an LSTM base architecture (or sometimes a GRU for comparison purposes).For the final output layer, we use a multi-level classification gap to predict supp [x], the well-known \"NP-hard\" part of the sparse estimate (determining final coefficient amplitudes requires only the smallest squares).Full network details are moved to Appendix A, including special modifications to process complex data according to the requirements of DOA applications.For a given dictionary, a separate network needs to be trained using SGD, to which we add a unique additional dimension of randomness via a stochastic online data generation strategy. Specifically, we generate a non-vector in each sample initially using a mini-amplifier."}, {"heading": "5 Experiments", "text": "This section presents experiments with synthetic data and two applications."}, {"heading": "5.1 Evaluations via Synthetic Correlated Dictionaries", "text": "To reproduce experiments from [2], we generate correlated synthetic characteristics via \u03a6 = \u2211 n = 1 i2uiv > i, where ui-Rn and vi-Rm iid are drawn from a unit of Gaussian distribution, and each column of \u03a6 is subsequently reduced to unit '2 norm. Basic truth samples x * have d unequal elements that are randomly drawn from U [\u2212 0.5, 0.5] without the interval [\u2212 0.1, 0.1]. We use n = 20, m = 100 and vary d, with larger values generating a much harder combinatorial estimation problem (exhausted search is not feasible here). All algorithms are presented with y and attempt to estimate supp [x]. We rate with rigorous accuracy, meaning that the percentage of studies with exact support recovery and loose accuracy that quantifies the percentage of true positives among the top n \"conjectures.\""}, {"heading": "5.2 Practical Application I: Direction-of-Arrival (DOA) Estimation", "text": "The DOA estimate is a fundamental problem in sonar / radar processing [32]. Given a number of n omni-directional sensors with d-signal waves acting on them, the goal is to estimate the angle direction of the wave sources in relation to the sensors. In certain array geometries and known propagation media, the estimate of these angles can be directly assigned to the solution (2) in the complex domain. In this scenario, each column [n] represents the output of the sensor array (a point in Cn) from a hypothetical source of unity strength at an angular location and can be calculated using the wave prediction formula [32]. The entire dictionary can be constructed by concatenating columns linked to angles that form a certain distance of interest, e.g. each 1 x across a half circle, and will be highly correlated."}, {"heading": "5.3 Practical Application II: 3D Geometry Recovery via Photometric Stereo", "text": "Photometric stereo is another area of application, where the approximate solution (2) has recently produced state-of-the-art results using SBL [27]. The goal here is to restore the 3D surface standards of a given scene using images taken by a single camera using different lighting conditions. Assuming that these images can be roughly broken down into a diffuse Lambertian component and exhibit sparse corruptions such as shadows and spectral lights, surface norms can then be restored at each pixel using (2) to isolate these sparse factors, followed by a final square post-processing step [27]. In this context, the construction is carried out using the known camera and illumination geometry, and y represents intensity measurements for a particular pixel across images projected onto the zero space of a special transposed illumination matrix (see annex for full details)."}, {"heading": "6 Conclusion", "text": "We have shown that gated recurrent networks, which are carefully structured to reflect the multi-scale optimization paths of multiloop SBL iterations, can lead to a significant increase in both accuracy and efficiency. Note that simpler, first-rate gradient descent-style algorithms can be ineffective when applied to economy-enhancing energy functions with a combinatorial number of poor local optimizations and highly concave or non-differentiable surfaces in the vicinity of minima. Furthermore, implementing gentler approaches such as SBL with gradient descent would be impractical as any gradient calculation would be prohibitively expensive. Therefore, recent learning-to-learn approaches such as [1] based on gradient descent are difficult to apply in the current environment."}, {"heading": "7 Appendix A: Modeling and Training Details", "text": "We first describe the basic gated feedback RNN structure, followed by our special model architecture including extensions for handling complex data. We conclude with training details and experimental settings."}, {"heading": "7.1 Gated Feedback RNN Structure", "text": "The gated feedback RNN cell [15] is a key component of our model. Detailed calculation flows for a gated feedback LSTM cell (GFLSTM), which represents a certain specialization used in all our experiments, follow asc (t) j = f (t) j c (t) j c (t \u2212 1) j + i (t) j c (t) jh (t) j (t) j (t) j (t) j (t) j (t) j + U ijh (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j) i (t) i) i (t) i (t) d) j (t) i (t) i (t) i) d) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t) j (t (t) j (t) j (t) j (t) j (t) j (t) j (t) j (i) j (t) j (t)."}, {"heading": "7.2 Proposed Model Architecture and Extensions", "text": "Basic Model: Although our model consists of RNN cells, as soon as we fix the number of deployment steps, it essentially becomes a forward network. As shown in Figure 5, the input is transferred to the lowest RNN cell in each unrolled step. After the model generates its results in each unrolled step, all of these outputs are concatenated and fed into a fully connected layer to produce the final prediction. As we choose the supp [x] = {i: x \u00b2 i 6 = 0}, we consider the problem as a multi-step classification task and apply a softmax layer on top of the fully connected layer. We formalize this process as [q (t), H (H)] = frnn (H \u2212 1), y; prevrnn) p = fpred (1), q (2), q (D), q (T); prefd)."}, {"heading": "7.3 Training Details", "text": "In our experiments, models are implemented using Torch7 and experiments are carried out using a single NVIDIA Tesla K40M GPU card.Training Hyperparameters: To ensure consistency with the concept of the epoch of [2], our models are trained by 600000 / 250 = 2400 batches with batch size equal to 250. Typically, with 400 epochs (or 800 epochs in some extreme cases) of RMSprop optimization, we approach a satisfactory level of performance, with an initial standard learning rate of 0.002, which is taken into account by 0.25 every 50 epochs after the first 250 epochs of training. Model Hyperparameters: In terms of the model architecture, there are the following hyperparameters: number of RNN hidden units h, number of stacked RNN layers r, and number of RNN cell steps T."}, {"heading": "8 Appendix B: Experimental Details for Direction-of-Arrival (DOA) Estimation", "text": "This appendix contains background information, followed by details on experiments and training."}, {"heading": "8.1 Background", "text": "The estimate of the direction of arrival (DOA) for sonar and radar applications can be formulated by observing modular (t) = d \u00b2 k = 1sk (t) f (\u03b8 k) + (t), (26), where y (t) \u00b2 Cn is the measured sonar / radar signal at present t, f: R \u00b2 Cn and d is the number of source waveforms whose orders of magnitude s (t) = [s1 (t),..., sd (t)]] > Cd and angle positions are the same. Then, the problem can be rewritten so that the alternative observation modular (t) = m \u00b2 p \u00b2 i = 1xi (t) inspi + (t) = pp (t) + (t) are conceived."}, {"heading": "8.2 Experimental Design", "text": "In our experiment, we start with some natural assumptions. First, we consider the narrow-band, wide-field-dependent problem, which means that the incoming waves are approximately planar and each source originates from a single point. Furthermore, we assume that our sensors are arranged with a linear, uniformly static array geometry, i.e. with a uniform linear array (ULA) and a known propagation medium. Then, the measurement vector y (t), which the sensors receive at a given time, is given by (26), whereby the non-linear function Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sense-Sendently-Sendently-Sendently-Sendently-Sendently-Sendently-Sendently-Sendently-Sendently-Sendently-Sendently-Sendently-Sendently-Sendently-Sendently-Sendently-Sendently-Sendently-Sendently-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Sen.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send.-Send--Send.-Send.-Send.-Send.-Sen"}, {"heading": "9 Appendix C: Experimental Details for 3D Geometry Recovery via Photometric Stereo", "text": "This appendix describes our photometric stereo experiments in more detail, including the visualization of fault areas."}, {"heading": "9.1 Background", "text": "Photometric stereo is a useful method for restoring high-resolution surface norms from a 3D scene using 2D images taken in different light conditions. A proposed model for the observation process at a single pixel is iso = \u03c1Ln + e, (30) where the r-measurements o-Rr, n-R3 denote the true 3D surface standard, lines of L-Rr-3 define the illumination directions, \u03c1 is the diffuse albedo, which acts here as a scalar multiplier, and e represents a collection of shadows, spectral highlights or other corrupulating influences [28, 44]. If e were not present, then the surface norm can be clearly determined from a simple, smallest square. However, a more robust alternative involves solving min n, e-ions, e-ions or other corrupulating influences [28, 44]."}, {"heading": "9.2 Experimental Design", "text": "We test algorithms separately on two objects, \"Bunny\" and \"Caesar\" of [27]. Initial illumination conditions are generated whose directions are randomly selected by a hemisphere with the object in the center. Then, 32-bit HDR grayscale images of the object are displayed with foreground masks and randomly selected parameters, 0.64 for Bunny and 0.8 for Caesar. The resulting image resolution for Bunny isTable 2: attributes of our models used in the production of Figure 4 (c) results. Model Hidden Unit Size # Parameter Training Time (sec. / epoch) S-Acc GRU-small 320 1296740 98,314 0.1588LSTM-small 272 1213220 119,605 0.3017 GFGRU-small 220 1285340 170,282 0.280,282 0.4651GFLSTM-small 200 1209300 172,013 0.4691 GRU-big 86546040,024"}, {"heading": "10 Appendix D: Additional Experiments and Self-Comparisons", "text": "We provide further evaluation details for generic, sparse recovery problems, followed by a series of ablation studies."}, {"heading": "10.1 Additional Details for Sparse Vector Recovery Evaluation", "text": "Table 2 lists all the important attributes of our self-comparison models from Figure 4 (c) in the main part. With regard to the evaluation of generic problems, we define strict accuracy (s-acc) and loose accuracy (l-acc) viaSgt = {j: x-acc = 6 = 0}, Spred (d) = {j: pj is one of the d largest outputs} (33) s-acc = 1N \u2211 N i = 1 I [Sigt = Sipred (d)], l-acc = 1N \u2211 N i = 1 | Sign-acc (n) | d, (34) where N is the number of samples."}, {"heading": "10.2 Ablation Study for Generic Sparse Estimation Problem", "text": "In Table 3, we list the ablation results of GFLSTM models with different hyperparameters in case d n = 0.4. Capacity enhancements generally favor performance, especially when capacity is relatively low. However, the effectiveness and efficiency of changing hidden size, LSTM layers, or number of roll-off steps vary. Stacking too many LSTM layers is the least efficient way to increase model capacity, taking into account the trade-off between training time and performance improvement. In terms of roll-off steps, insufficient steps (say under 10) do not impair the models \"ability, while excessive roll-off is a waste of computing power, and hidden size is a fairly effective way to control model capacity."}, {"heading": "11 Appendix E: Technical Proofs", "text": "The proof of Proposition 1It was shown in [41] that the use of (t + 1) i = [p) i = [p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i) p \"p\" p (p) i (p) p) i (p) (p) i (p) (p) p) i (p) (p) i) (p) i (p) (p) i) (p) (p) i) (p) (p) p) i (p) (p) i) (p) i) (p) (p) i) (p) p) i (p) (p) i) (p) i) (p) (p) i) (p) i) (p) (p) i) (p) i) (p) (p) i) (p) (p) i) (p) (p) i) (p) p (p) i) (p) (p) i) p (p) i) (p) (p) (p) i) (p (p) i) (p (p) i) (p (p) i) (p (p) i) (p (p) i) (p (p) (p) (p) (p) (p) p (p) i) (p (p (p) i) (p (p) (p) i) (p (p (p) (p) (p) i) (p) (p (p) i) (p (p) i) (p (p) (p) i) (p (p (p) (p) i) (p (p) i) (p (p (p"}], "references": [{"title": "Learning to learn by gradient descent by gradient descent", "author": ["M. Andrychowicz", "M. Denil", "S. Gomez", "M.W. Hoffman", "D. Pfau", "T. Schaul", "B. Shillingford", "N. de Freitas"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Maximal sparsity with deep networks", "author": ["W. Gao B. Xin", "Y. Wang", "D. Wipf"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Electromagnetic brain mapping", "author": ["S. Baillet", "J.C. Mosher", "R.M. Leahy"], "venue": "IEEE Signal Processing Magazine, pages 14\u201330, Nov.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems", "author": ["A. Beck", "M. Teboulle"], "venue": "IEEE Trans. Image Processing, 18(11),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM J. Imaging Sciences, 2(1),", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Iterative hard thresholding for compressed sensing", "author": ["T. Blumensath", "M.E. Davies"], "venue": "Applied and Computational Harmonic Analysis, 27(3),", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Normalized iterative hard thresholding: Guaranteed stability and performance", "author": ["T. Blumensath", "M.E. Davies"], "venue": "IEEE J. Selected Topics Signal Processing, 4(2),", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Distance transformations in arbitrary dimensions", "author": ["G. Borgefors"], "venue": "Computer Vision, Graphics, and Image Processing, 27(3):321\u2013345,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1984}, {"title": "AMP-inspired deep networks for sparse linear inverse problems", "author": ["M. Borgerding", "P. Schniter", "S. Rangan"], "venue": "arXiv preprint arXiv:1612.01183,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2017}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information", "author": ["E. Cand\u00e8s", "J. Romberg", "T. Tao"], "venue": "IEEE Trans. Information Theory, 52(2):489\u2013 509, Feb.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Decoding by linear programming", "author": ["E. Cand\u00e8s", "T. Tao"], "venue": "IEEE Trans. Information Theory, 51(12),", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Enhancing sparsity by reweighted `1 minimization", "author": ["E. Cand\u00e8s", "M. Wakin", "S. Boyd"], "venue": "J. Fourier Anal. Appl., 14(5):877\u2013905,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Gated feedback recurrent neural networks", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "International Conference on Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Sparse channel estimation via matching pursuit with application to equalization", "author": ["S.F. Cotter", "B.D. Rao"], "venue": "IEEE Trans. on Communications, 50(3),", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Root sparse Bayesian learning for off-grid DOA estimation", "author": ["J. Dai", "X. Bao", "W. Xu", "C. Chang"], "venue": "IEEE Signal Processing Letters, 24(1),", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "J. Royal Statistical Society, Series B (Methodological), 39(1):1\u201338,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1977}, {"title": "Variable selection via nonconcave penalized likelihood and its oracle properties", "author": ["J. Fan", "R. Li"], "venue": "J. American Statistical Assoc., 96,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Adaptive sparseness using Jeffreys prior", "author": ["M.A.T. Figueiredo"], "venue": "NIPS,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Recurrent nets that time and count", "author": ["F.A. Gers", "J. Schmidhuber"], "venue": "International Joint Conference on Neural Networks,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "Multi snapshot sparse Bayesian learning for DOA", "author": ["P. Gerstoft", "C.F. Mecklenbrauker", "A. Xenaki", "S. Nannuru"], "venue": "IEEE Signal Processing Letters, 23(20),", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning fast approximations of sparse coding", "author": ["K. Gregor", "Y. LeCun"], "venue": "ICML,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8),", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1997}, {"title": "A tutorial on MM algorithms", "author": ["D.R. Hunter", "K. Lange"], "venue": "American Statistician, 58(1),", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Robust photometric stereo using sparse regression", "author": ["S. Ikehata", "D.P. Wipf", "Y. Matsushita", "K. Aizawa"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Photometric stereo using sparse Bayesian regression for general diffuse surfaces", "author": ["S. Ikehata", "D.P. Wipf", "Y. Matsushita", "K. Aizawa"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, 36(9):1816\u20131831,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Photometric stereo using sparse Bayesian regression for general diffuse surfaces", "author": ["S. Ikehata", "D.P. Wipf", "Y. Matsushita", "K. Aizawa"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, 36(9):1816\u20131831,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "A clockwork RNN", "author": ["J. Koutnik", "K. Greff", "F. Gomez", "J. Schmidhuber"], "venue": "International Conference on Machine Learning,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Bayesian interpolation", "author": ["D.J.C. MacKay"], "venue": "Neural Computation, 4(3):415\u2013447,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1992}, {"title": "Sparse signal reconstruction perspective for source localization with sensor arrays", "author": ["D.M. Malioutov", "M. \u00c7etin", "A.S. Willsky"], "venue": "IEEE Trans. Signal Processing, 53(8),", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Statistical and Adaptive Signal Processing", "author": ["D.G. Manolakis", "V.K. Ingle", "S.M. Kogon"], "venue": "McGrall-Hill, Boston,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2000}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G. Hinton"], "venue": "International Conference on Machine Learning,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Vector approximate message passing", "author": ["S. Rangan", "P. Schniter", "A.K. Fletcher"], "venue": "arXiv preprint arXiv:1610.03082,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning efficient sparse and low rank models", "author": ["P. Sprechmann", "A.M. Bronstein", "G. Sapiro"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, 37(9),", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "A proof of convergence of the concave-convex procedure using Zangwill\u2019s theory", "author": ["B.K. Sriperumbudu", "G.R.G. Lanckriet"], "venue": "Neural computation, 24,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "J. of the Royal Statistical Society,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1996}, {"title": "Sparse Bayesian learning and the relevance vector machine", "author": ["M.E. Tipping"], "venue": "Journal of Machine Learning Research, 1,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning deep `0 encoders", "author": ["Z. Wang", "Q. Ling", "T. Huang"], "venue": "arXiv preprint arXiv:1509.00153v2,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Sparse estimation with structured dictionaries", "author": ["D.P. Wipf"], "venue": "Advances in Nerual Information Processing 24,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Iterative reweighted `1 and `2 methods for finding sparse solutions", "author": ["D.P. Wipf", "S. Nagarajan"], "venue": "Journal of Selected Topics in Signal Processing (Special Issue on Compressive Sensing), 4(2), April", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}, {"title": "Latent variable Bayesian models for promoting sparsity", "author": ["D.P. Wipf", "B.D. Rao", "S. Nagarajan"], "venue": "IEEE Trans. Information Theory, 57(9), Sept.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Photometric method for determining surface orientation from multiple images", "author": ["R.J. Woodham"], "venue": "Optical Engineering, 19(1),", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1980}, {"title": "Robust photometric stereo via low-rank matrix completion and recovery", "author": ["L. Wu", "A. Ganesh", "B. Shi", "Y. Matsushita", "Y. Wang", "Y. Ma"], "venue": "Asian Conference on Computer Vision,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "Off-grid direction of arrival estimation using sparse Bayesian inference", "author": ["Z. Yang", "L. Xie", "C. Zhang"], "venue": "IEEE Trans. Signal Processing, 61(1):38\u201343,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "Feedback networks", "author": ["A.R. Zamir", "T.L. Wu", "L. Sun", "W. Shen", "J. Malik", "S. Savarese"], "venue": "arXiv preprint arXiv:1612.09508,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 22, "context": "When we treat By as a bias or exogenous input, then the progression of these iterations through time resembles activations passing through the layers (indexed by t) of a deep neural network (DNN) [23, 35, 39, 2].", "startOffset": 196, "endOffset": 211}, {"referenceID": 34, "context": "When we treat By as a bias or exogenous input, then the progression of these iterations through time resembles activations passing through the layers (indexed by t) of a deep neural network (DNN) [23, 35, 39, 2].", "startOffset": 196, "endOffset": 211}, {"referenceID": 38, "context": "When we treat By as a bias or exogenous input, then the progression of these iterations through time resembles activations passing through the layers (indexed by t) of a deep neural network (DNN) [23, 35, 39, 2].", "startOffset": 196, "endOffset": 211}, {"referenceID": 1, "context": "When we treat By as a bias or exogenous input, then the progression of these iterations through time resembles activations passing through the layers (indexed by t) of a deep neural network (DNN) [23, 35, 39, 2].", "startOffset": 196, "endOffset": 211}, {"referenceID": 2, "context": "applications [3, 12, 16, 20, 26, 31], solving (2) is NP-hard, and therefore efficient approximations are sought.", "startOffset": 13, "endOffset": 36}, {"referenceID": 11, "context": "applications [3, 12, 16, 20, 26, 31], solving (2) is NP-hard, and therefore efficient approximations are sought.", "startOffset": 13, "endOffset": 36}, {"referenceID": 15, "context": "applications [3, 12, 16, 20, 26, 31], solving (2) is NP-hard, and therefore efficient approximations are sought.", "startOffset": 13, "endOffset": 36}, {"referenceID": 19, "context": "applications [3, 12, 16, 20, 26, 31], solving (2) is NP-hard, and therefore efficient approximations are sought.", "startOffset": 13, "endOffset": 36}, {"referenceID": 25, "context": "applications [3, 12, 16, 20, 26, 31], solving (2) is NP-hard, and therefore efficient approximations are sought.", "startOffset": 13, "endOffset": 36}, {"referenceID": 30, "context": "applications [3, 12, 16, 20, 26, 31], solving (2) is NP-hard, and therefore efficient approximations are sought.", "startOffset": 13, "endOffset": 36}, {"referenceID": 4, "context": "Popular examples with varying degrees of computational overhead include convex relaxations such as `1-norm regularization [5, 11, 37] and many flavors of iterative hard-thresholding (IHT) [6, 7].", "startOffset": 122, "endOffset": 133}, {"referenceID": 10, "context": "Popular examples with varying degrees of computational overhead include convex relaxations such as `1-norm regularization [5, 11, 37] and many flavors of iterative hard-thresholding (IHT) [6, 7].", "startOffset": 122, "endOffset": 133}, {"referenceID": 36, "context": "Popular examples with varying degrees of computational overhead include convex relaxations such as `1-norm regularization [5, 11, 37] and many flavors of iterative hard-thresholding (IHT) [6, 7].", "startOffset": 122, "endOffset": 133}, {"referenceID": 5, "context": "Popular examples with varying degrees of computational overhead include convex relaxations such as `1-norm regularization [5, 11, 37] and many flavors of iterative hard-thresholding (IHT) [6, 7].", "startOffset": 188, "endOffset": 194}, {"referenceID": 6, "context": "Popular examples with varying degrees of computational overhead include convex relaxations such as `1-norm regularization [5, 11, 37] and many flavors of iterative hard-thresholding (IHT) [6, 7].", "startOffset": 188, "endOffset": 194}, {"referenceID": 5, "context": "However, the Achilles\u2019 heel of all these approaches is that they will generally not converge to good approximate minimizers of (2) if \u03a6 has columns with a high degree of correlation [6, 11], which is unfortunately often the case in practice [40].", "startOffset": 182, "endOffset": 189}, {"referenceID": 10, "context": "However, the Achilles\u2019 heel of all these approaches is that they will generally not converge to good approximate minimizers of (2) if \u03a6 has columns with a high degree of correlation [6, 11], which is unfortunately often the case in practice [40].", "startOffset": 182, "endOffset": 189}, {"referenceID": 39, "context": "However, the Achilles\u2019 heel of all these approaches is that they will generally not converge to good approximate minimizers of (2) if \u03a6 has columns with a high degree of correlation [6, 11], which is unfortunately often the case in practice [40].", "startOffset": 241, "endOffset": 245}, {"referenceID": 1, "context": "To mitigate the effects of such correlations, we could leverage the aforementioned correspondence with common DNN structures to learn something like a correlation-invariant algorithm or update rules [2], although in this scenario our starting point would be an algorithmic format with known deficiencies.", "startOffset": 199, "endOffset": 202}, {"referenceID": 37, "context": "One important example is sparse Bayesian learning (SBL) [38], which has been shown to solve (2) using a principled, multiloop majorization-minimization approach [25] even in cases where \u03a6 displays strong correlations [40].", "startOffset": 56, "endOffset": 60}, {"referenceID": 24, "context": "One important example is sparse Bayesian learning (SBL) [38], which has been shown to solve (2) using a principled, multiloop majorization-minimization approach [25] even in cases where \u03a6 displays strong correlations [40].", "startOffset": 161, "endOffset": 165}, {"referenceID": 39, "context": "One important example is sparse Bayesian learning (SBL) [38], which has been shown to solve (2) using a principled, multiloop majorization-minimization approach [25] even in cases where \u03a6 displays strong correlations [40].", "startOffset": 217, "endOffset": 221}, {"referenceID": 23, "context": "1 Herein we demonstrate that, when judiciously unfolded, SBL iterations can be formed into variants of long short-term memory (LSTM) cells, one of the more popular recurrent deep neural network architectures [24], or gated extensions thereof [15].", "startOffset": 208, "endOffset": 212}, {"referenceID": 14, "context": "1 Herein we demonstrate that, when judiciously unfolded, SBL iterations can be formed into variants of long short-term memory (LSTM) cells, one of the more popular recurrent deep neural network architectures [24], or gated extensions thereof [15].", "startOffset": 242, "endOffset": 246}, {"referenceID": 22, "context": "This association significantly broadens recent work associating elementary, one-step iterative sparsity algorithms like (1) with simple recurrent or feedforward deep network architectures [23, 35, 39, 2].", "startOffset": 188, "endOffset": 203}, {"referenceID": 34, "context": "This association significantly broadens recent work associating elementary, one-step iterative sparsity algorithms like (1) with simple recurrent or feedforward deep network architectures [23, 35, 39, 2].", "startOffset": 188, "endOffset": 203}, {"referenceID": 38, "context": "This association significantly broadens recent work associating elementary, one-step iterative sparsity algorithms like (1) with simple recurrent or feedforward deep network architectures [23, 35, 39, 2].", "startOffset": 188, "endOffset": 203}, {"referenceID": 1, "context": "This association significantly broadens recent work associating elementary, one-step iterative sparsity algorithms like (1) with simple recurrent or feedforward deep network architectures [23, 35, 39, 2].", "startOffset": 188, "endOffset": 203}, {"referenceID": 31, "context": "\u2022 We achieve state-of-the-art performance on several empirical tasks, including direction-ofarrival (DOA) estimation [32] and 3D geometry recovery via photometric stereo [43].", "startOffset": 117, "endOffset": 121}, {"referenceID": 42, "context": "\u2022 We achieve state-of-the-art performance on several empirical tasks, including direction-ofarrival (DOA) estimation [32] and 3D geometry recovery via photometric stereo [43].", "startOffset": 170, "endOffset": 174}, {"referenceID": 0, "context": "\u2022 Although learning-to-learn style approaches [1, 23, 35, 39] have been commonly applied to relatively simple gradient descent optimization templates, this is the first successful attempt we are aware of to learn a complex, multi-loop, majorization-minimization algorithm [25].", "startOffset": 46, "endOffset": 61}, {"referenceID": 22, "context": "\u2022 Although learning-to-learn style approaches [1, 23, 35, 39] have been commonly applied to relatively simple gradient descent optimization templates, this is the first successful attempt we are aware of to learn a complex, multi-loop, majorization-minimization algorithm [25].", "startOffset": 46, "endOffset": 61}, {"referenceID": 34, "context": "\u2022 Although learning-to-learn style approaches [1, 23, 35, 39] have been commonly applied to relatively simple gradient descent optimization templates, this is the first successful attempt we are aware of to learn a complex, multi-loop, majorization-minimization algorithm [25].", "startOffset": 46, "endOffset": 61}, {"referenceID": 38, "context": "\u2022 Although learning-to-learn style approaches [1, 23, 35, 39] have been commonly applied to relatively simple gradient descent optimization templates, this is the first successful attempt we are aware of to learn a complex, multi-loop, majorization-minimization algorithm [25].", "startOffset": 46, "endOffset": 61}, {"referenceID": 24, "context": "\u2022 Although learning-to-learn style approaches [1, 23, 35, 39] have been commonly applied to relatively simple gradient descent optimization templates, this is the first successful attempt we are aware of to learn a complex, multi-loop, majorization-minimization algorithm [25].", "startOffset": 272, "endOffset": 276}, {"referenceID": 33, "context": "Note also that a recent interesting modification of approximate message passing [34] (or its unfolded, trainable deep analog that converges to the same solution [9]), can handle certain specialized forms of dictionary correlation; however, the approach does not work with the types of strong arbitrary/unconstrained correlation we consider in this work.", "startOffset": 80, "endOffset": 84}, {"referenceID": 8, "context": "Note also that a recent interesting modification of approximate message passing [34] (or its unfolded, trainable deep analog that converges to the same solution [9]), can handle certain specialized forms of dictionary correlation; however, the approach does not work with the types of strong arbitrary/unconstrained correlation we consider in this work.", "startOffset": 161, "endOffset": 164}, {"referenceID": 37, "context": "where \u03bb > 0 is a fixed variance factor and \u03b3 denotes a vector of unknown hyperparamters [38].", "startOffset": 88, "endOffset": 92}, {"referenceID": 29, "context": "For this purpose we marginalize over x and then maximize the resulting type-II likelihood function with respect to \u03b3 [30].", "startOffset": 117, "endOffset": 121}, {"referenceID": 37, "context": "Conveniently, the resulting convolution-of-Gaussians integral is available in closed-form [38] such that we can equivalently minimize the negative loglikelihood L(\u03b3) = \u2212 log \u222b p(y|x)p(x;\u03b3)dx \u2261 y>\u03a3\u22121 y y + log |\u03a3y|.", "startOffset": 90, "endOffset": 94}, {"referenceID": 40, "context": "In general, if we replace the `0 norm from (2) with any smooth approximation g(|x|), where g is a concave, non-decreasing function and | \u00b7 | applies elementwise, then cost function descent2 can be guaranteed using iterations of the form [41]", "startOffset": 237, "endOffset": 241}, {"referenceID": 24, "context": "(6) This process can be viewed as a multi-loop, majorization-minimization algorithm [25] (a generalization of the EM algorithm [18]), whereby the inner-loop involves computing x by minimizing a first-order, upper-bounding approximation \u2016y\u2212\u03a6x\u20162 + \u03bb \u2211 i w (t) i |xi|, while the outer-loop updates the bound/majorizer itself as parameterized by the weights w.", "startOffset": 84, "endOffset": 88}, {"referenceID": 17, "context": "(6) This process can be viewed as a multi-loop, majorization-minimization algorithm [25] (a generalization of the EM algorithm [18]), whereby the inner-loop involves computing x by minimizing a first-order, upper-bounding approximation \u2016y\u2212\u03a6x\u20162 + \u03bb \u2211 i w (t) i |xi|, while the outer-loop updates the bound/majorizer itself as parameterized by the weights w.", "startOffset": 127, "endOffset": 131}, {"referenceID": 36, "context": "Obviously, if g(u) = u, then w = 1 for all t, and (6) reduces to the Lasso objective for `1 norm regularized sparse regression [37], and only a single iteration is required.", "startOffset": 127, "endOffset": 131}, {"referenceID": 12, "context": "However, one popular non-trivial instantiation of this approach assumes g(u) = \u2211 i log (ui + ) with > 0 a user-defined parameter [13].", "startOffset": 129, "endOffset": 133}, {"referenceID": 18, "context": "This prevents the overshrinkage of large coefficients, a well-known criticism of `1 norm penalties [19].", "startOffset": 99, "endOffset": 103}, {"referenceID": 35, "context": "Or global convergence to some stationary point with mild additional assumptions [36].", "startOffset": 80, "endOffset": 84}, {"referenceID": 39, "context": "However, such cases can generally only occur when we are in the neighborhood of ideal, maximally sparse solutions by definition [40], when different weights are actually desirable even among correlated columns for resolving the final sparse estimates.", "startOffset": 128, "endOffset": 132}, {"referenceID": 32, "context": ", it acts just like a rectilinear (ReLU) unit [33].", "startOffset": 46, "endOffset": 50}, {"referenceID": 4, "context": "However, for additional flexibility, \u03b1 and \u03b2 could be selected to implement various forms of momentum, ultimately leading to cell updates akin to the popular FISTA [5] or monotonic FISTA [4] algorithms.", "startOffset": 164, "endOffset": 167}, {"referenceID": 3, "context": "However, for additional flexibility, \u03b1 and \u03b2 could be selected to implement various forms of momentum, ultimately leading to cell updates akin to the popular FISTA [5] or monotonic FISTA [4] algorithms.", "startOffset": 187, "endOffset": 190}, {"referenceID": 45, "context": "This is much like the strategy used by feedback networks for obtaining incrementally refined representations [46].", "startOffset": 109, "endOffset": 113}, {"referenceID": 20, "context": "If we allow for peephole connections [21], it is possible to reverse these roles; however, for simplicity and the most direct mapping to LSTM cells we do not pursue this alternative here.", "startOffset": 37, "endOffset": 41}, {"referenceID": 22, "context": "But certainly there is potential in replacing such iterations with learned LSTM-like surrogates, at least when provided with access to sufficient training data as in prior attempts to learn sparse estimation algorithms [23, 39, 2].", "startOffset": 219, "endOffset": 230}, {"referenceID": 38, "context": "But certainly there is potential in replacing such iterations with learned LSTM-like surrogates, at least when provided with access to sufficient training data as in prior attempts to learn sparse estimation algorithms [23, 39, 2].", "startOffset": 219, "endOffset": 230}, {"referenceID": 1, "context": "But certainly there is potential in replacing such iterations with learned LSTM-like surrogates, at least when provided with access to sufficient training data as in prior attempts to learn sparse estimation algorithms [23, 39, 2].", "startOffset": 219, "endOffset": 230}, {"referenceID": 14, "context": "This process will unmask certain characteristic dynamics suggestive of a richer class of recurrent network structures, inspired by sequence prediction tasks [15], to be analyzed later in Section 4 and empirically tested in Section 5.", "startOffset": 157, "endOffset": 161}, {"referenceID": 20, "context": "Forw the result of Proposition 1 suggests that these weights can be computed as the solution of a simple regularized regression problem, which can easily be replaced with a small network analogous to that used in [21]; similarly for \u03bd.", "startOffset": 213, "endOffset": 217}, {"referenceID": 28, "context": "Interestingly, in the context of sequence prediction, the clockwork RNN (CW-RNN) has been proposed to cope with temporal dependencies engaged across multiple scales [29].", "startOffset": 165, "endOffset": 169}, {"referenceID": 28, "context": "In the context of sequence prediction, the clockwork recurrent neural network (CW-RNN) has been proposed to cope with temporal dependencies engaged across multiple scales [29].", "startOffset": 171, "endOffset": 175}, {"referenceID": 14, "context": "The gated feedback RNN (GF-RNN) [15] was recently developed to update the CW-RNN with an additional set of gated connections that, in effect, allow the network to learn its own clock rates.", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "In brief, the GF-RNN involves stacked LSTM layers (or somewhat simpler gated recurrent unit (GRU) layers [14]), that are permitted to communicate bilaterally via additional, data-dependent gates that can open and close on different time-scales.", "startOffset": 105, "endOffset": 109}, {"referenceID": 7, "context": "Plot (d) shows Chamfer distance-based errors [8] from the direction-of-arrival (DOA) experiment.", "startOffset": 45, "endOffset": 48}, {"referenceID": 1, "context": "To reproduce experiments from [2], we generate correlated synthetic features via \u03a6 = \u2211n i=1 1 i2uiv > i , where ui \u2208 R and vi \u2208 R are drawn iid from a unit Gaussian distribution, and each column of \u03a6 is subsequently rescaled to unit `2 norm.", "startOffset": 30, "endOffset": 33}, {"referenceID": 37, "context": "Figures 4(a) and 4(b) evaluate our model, averaged across 10 trials, against an array of optimizationbased approaches: SBL [38], `1 norm minimization [5], and IHT [6]; and existing learning-based DNN models: an ISTA-inspired network [23], an IHT-inspired network [39], and the best maximal sparsity net (MaxSparseNet) from [2] (detailed settings in Appendix A).", "startOffset": 123, "endOffset": 127}, {"referenceID": 4, "context": "Figures 4(a) and 4(b) evaluate our model, averaged across 10 trials, against an array of optimizationbased approaches: SBL [38], `1 norm minimization [5], and IHT [6]; and existing learning-based DNN models: an ISTA-inspired network [23], an IHT-inspired network [39], and the best maximal sparsity net (MaxSparseNet) from [2] (detailed settings in Appendix A).", "startOffset": 150, "endOffset": 153}, {"referenceID": 5, "context": "Figures 4(a) and 4(b) evaluate our model, averaged across 10 trials, against an array of optimizationbased approaches: SBL [38], `1 norm minimization [5], and IHT [6]; and existing learning-based DNN models: an ISTA-inspired network [23], an IHT-inspired network [39], and the best maximal sparsity net (MaxSparseNet) from [2] (detailed settings in Appendix A).", "startOffset": 163, "endOffset": 166}, {"referenceID": 22, "context": "Figures 4(a) and 4(b) evaluate our model, averaged across 10 trials, against an array of optimizationbased approaches: SBL [38], `1 norm minimization [5], and IHT [6]; and existing learning-based DNN models: an ISTA-inspired network [23], an IHT-inspired network [39], and the best maximal sparsity net (MaxSparseNet) from [2] (detailed settings in Appendix A).", "startOffset": 233, "endOffset": 237}, {"referenceID": 38, "context": "Figures 4(a) and 4(b) evaluate our model, averaged across 10 trials, against an array of optimizationbased approaches: SBL [38], `1 norm minimization [5], and IHT [6]; and existing learning-based DNN models: an ISTA-inspired network [23], an IHT-inspired network [39], and the best maximal sparsity net (MaxSparseNet) from [2] (detailed settings in Appendix A).", "startOffset": 263, "endOffset": 267}, {"referenceID": 1, "context": "Figures 4(a) and 4(b) evaluate our model, averaged across 10 trials, against an array of optimizationbased approaches: SBL [38], `1 norm minimization [5], and IHT [6]; and existing learning-based DNN models: an ISTA-inspired network [23], an IHT-inspired network [39], and the best maximal sparsity net (MaxSparseNet) from [2] (detailed settings in Appendix A).", "startOffset": 323, "endOffset": 326}, {"referenceID": 31, "context": "DOA estimation is a fundamental problem in sonar/radar processing [32].", "startOffset": 66, "endOffset": 70}, {"referenceID": 31, "context": "In this scenario, each column of \u03a6 represents the sensor array output (a point in C) from a hypothetical source with unit strength at angular location \u03b8i, and can be computed using wave progagation formula [32].", "startOffset": 206, "endOffset": 210}, {"referenceID": 16, "context": "Recently SBL-based algorithms have produced state-of-the-art results solving the DOA problem [17, 22, 45], and we compare our approach against SBL here.", "startOffset": 93, "endOffset": 105}, {"referenceID": 21, "context": "Recently SBL-based algorithms have produced state-of-the-art results solving the DOA problem [17, 22, 45], and we compare our approach against SBL here.", "startOffset": 93, "endOffset": 105}, {"referenceID": 44, "context": "Recently SBL-based algorithms have produced state-of-the-art results solving the DOA problem [17, 22, 45], and we compare our approach against SBL here.", "startOffset": 93, "endOffset": 105}, {"referenceID": 7, "context": "Because the nonzero positions in x\u2217 now have physical meaning, we apply the Chamfer distance [8] as the error metric, which quantifies how close we are to true source locations (lower is better).", "startOffset": 93, "endOffset": 96}, {"referenceID": 26, "context": "Photometric stereo represents another application domain whereby approximately solving (2) using SBL has recently produced state-of-the-art results [27].", "startOffset": 148, "endOffset": 152}, {"referenceID": 26, "context": "Under the assumption that these images can be approximately decomposed into a diffuse Lambertian component and sparse corruptions such as shadows and specular highlights, then surface normals at each pixel can be recovered using (2) to isolate these sparse factors followed by a final least squares post-processing step [27].", "startOffset": 320, "endOffset": 324}, {"referenceID": 1, "context": "We compare our GFLSTM model against both SBL and the MaxSparseNet [2] (both of which outperform other existing methods).", "startOffset": 66, "endOffset": 69}, {"referenceID": 26, "context": "Tests are performed using the 32-bit HDR gray-scale images of objects \u2018Bunny\u2019 (256\u00d7 256) and \u2018Caesar\u2019 (300\u00d7 400) as in [27].", "startOffset": 119, "endOffset": 123}, {"referenceID": 1, "context": "For (very) weakly-supervised training data, we apply the same approach as before, only we use nonzero magnitudes drawn from a Gaussian, with mean and variance loosely tuned to the photometric stereo data, consistent with [2].", "startOffset": 221, "endOffset": 224}, {"referenceID": 0, "context": "Therefore, recent learning-to-learn approaches such as [1] that rely on gradient descent are difficult to apply in the present setting.", "startOffset": 55, "endOffset": 58}, {"referenceID": 14, "context": "The gated feedback RNN cell [15] is a key component of our model.", "startOffset": 28, "endOffset": 32}, {"referenceID": 1, "context": "Training Hyperparameters: To provide consistency with the concept of epoch from [2], our models are trained by 600000/250 = 2400 batches with batch size equal to 250.", "startOffset": 80, "endOffset": 83}, {"referenceID": 22, "context": "A Useful Training Heuristic: When training with a fixed-sized dataset, as existing learning approaches to sparse estimation do [23, 39, 2], there is always the risk of overfitting.", "startOffset": 127, "endOffset": 138}, {"referenceID": 38, "context": "A Useful Training Heuristic: When training with a fixed-sized dataset, as existing learning approaches to sparse estimation do [23, 39, 2], there is always the risk of overfitting.", "startOffset": 127, "endOffset": 138}, {"referenceID": 1, "context": "A Useful Training Heuristic: When training with a fixed-sized dataset, as existing learning approaches to sparse estimation do [23, 39, 2], there is always the risk of overfitting.", "startOffset": 127, "endOffset": 138}, {"referenceID": 31, "context": "\u03b8 \u2217 d] > [32].", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": "Metric: We apply the symmetric Chamfer distance[8] to evaluate the estimation quality with respect to the ground truth source directions.", "startOffset": 47, "endOffset": 50}, {"referenceID": 27, "context": "o = \u03c1Ln+ e, (30) where the r measurments are denoted o \u2208 R, n \u2208 R denotes the true 3D surface normal, rows of L \u2208 Rr\u00d73 define lighting directions, \u03c1 is the diffuse albedo, acting here as a scalar multiplier, and e represents an aggregations of shadows, specular highlights, or other corrupting influences [28, 44].", "startOffset": 305, "endOffset": 313}, {"referenceID": 43, "context": "o = \u03c1Ln+ e, (30) where the r measurments are denoted o \u2208 R, n \u2208 R denotes the true 3D surface normal, rows of L \u2208 Rr\u00d73 define lighting directions, \u03c1 is the diffuse albedo, acting here as a scalar multiplier, and e represents an aggregations of shadows, specular highlights, or other corrupting influences [28, 44].", "startOffset": 305, "endOffset": 313}, {"referenceID": 27, "context": "where \u00f1 is the surface normal rescaled by \u03c1, which is equivalent to computing [28]", "startOffset": 78, "endOffset": 82}, {"referenceID": 26, "context": "We test algorithms separately on two objects, \u2018Bunny\u2019 and \u2018Caesar\u2019 from [27].", "startOffset": 72, "endOffset": 76}, {"referenceID": 1, "context": "We adopt the basic pipeline from [2] to accomplish this.", "startOffset": 33, "endOffset": 36}, {"referenceID": 40, "context": "It has been demonstrated in [41] that using", "startOffset": 28, "endOffset": 32}, {"referenceID": 9, "context": "which is always realizable for any \u03b3\u0303 \u2208 R+ given the concavity of h(\u03b3) [10].", "startOffset": 71, "endOffset": 75}, {"referenceID": 41, "context": "This bound holds for all u \u2208 R, with equality when u = \u0393\u03a6> ( \u03bbI + \u03a6\u0393\u03a6> )\u22121 y [42].", "startOffset": 77, "endOffset": 81}, {"referenceID": 24, "context": "One attractive feature of this formulation is that \u03b3 can be optimized jointly with at least one set of variational parameters (in this case u), as opposed to most majorization-minimization strategies [25] that fix the upper bound before minimizing the original variables (in this case \u03b3).", "startOffset": 200, "endOffset": 204}], "year": 2017, "abstractText": "The iterations of many first-order algorithms, when applied to minimizing common regularized regression functions, often resemble neural network layers with prespecified weights. This observation has prompted the development of learningbased approaches that purport to replace these iterations with enhanced surrogates forged as DNN models from available training data. For example, important NPhard sparse estimation problems have recently benefitted from this genre of upgrade, with simple feedforward or recurrent networks ousting proximal gradient-based iterations. Analogously, this paper demonstrates that more powerful Bayesian algorithms for promoting sparsity, which rely on complex multi-loop majorizationminimization techniques, mirror the structure of more sophisticated long short-term memory (LSTM) networks, or alternative gated feedback networks previously designed for sequence prediction. As part of this development, we examine the parallels between latent variable trajectories operating across multiple time-scales during optimization, and the activations within deep network structures designed to adaptively model such characteristic sequences. The resulting insights lead to a novel sparse estimation system that, when granted training data, can estimate optimal solutions efficiently in regimes where other algorithms fail, including practical direction-of-arrival (DOA) and 3D geometry recovery problems. The underlying principles we expose are also suggestive of a learning process for a richer class of multi-loop algorithms in other domains.", "creator": "LaTeX with hyperref package"}}}