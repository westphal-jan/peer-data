{"id": "1706.03175", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2017", "title": "Recovery Guarantees for One-hidden-layer Neural Networks", "abstract": "In this paper, we consider regression problems with one-hidden-layer neural networks (1NNs). We distill some properties of activation functions that lead to $\\mathit{local~strong~convexity}$ in the neighborhood of the ground-truth parameters for the 1NN squared-loss objective. Most popular nonlinear activation functions satisfy the distilled properties, including rectified linear units (ReLUs), leaky ReLUs, squared ReLUs and sigmoids. For activation functions that are also smooth, we show $\\mathit{local~linear~convergence}$ guarantees of gradient descent under a resampling rule. For homogeneous activations, we show tensor methods are able to initialize the parameters to fall into the local strong convexity region. As a result, tensor initialization followed by gradient descent is guaranteed to recover the ground truth with sample complexity $ d \\cdot \\log(1/\\epsilon) \\cdot \\mathrm{poly}(k,\\lambda )$ and computational complexity $n\\cdot d \\cdot \\mathrm{poly}(k,\\lambda) $ for smooth homogeneous activations with high probability, where $d$ is the dimension of the input, $k$ ($k\\leq d$) is the number of hidden nodes, $\\lambda$ is a conditioning property of the ground-truth parameter matrix between the input layer and the hidden layer, $\\epsilon$ is the targeted precision and $n$ is the number of samples. To the best of our knowledge, this is the first work that provides recovery guarantees for 1NNs with both sample complexity and computational complexity $\\mathit{linear}$ in the input dimension and $\\mathit{logarithmic}$ in the precision.", "histories": [["v1", "Sat, 10 Jun 2017 02:56:39 GMT  (125kb,D)", "http://arxiv.org/abs/1706.03175v1", "ICML 2017"]], "COMMENTS": "ICML 2017", "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["kai zhong", "zhao song", "prateek jain 0002", "peter l bartlett", "inderjit s dhillon"], "accepted": true, "id": "1706.03175"}, "pdf": {"name": "1706.03175.pdf", "metadata": {"source": "CRF", "title": "Recovery Guarantees for One-hidden-layer Neural Networks\u2217", "authors": ["Kai Zhong", "Zhao Song", "Peter L. Bartlett", "Inderjit S. Dhillon"], "emails": ["zhongkai@ices.utexas.edu", "zhaos@utexas.edu", "prajain@microsoft.com", "bartlett@cs.berkeley.edu", "inderjit@cs.utexas.edu"], "sections": [{"heading": null, "text": "A preliminary version of this paper appears in Proceedings of the Thirty-fourth International Conference on Machine Learning (ICML 2017). \u00b2 Partly supported by NSF grants CCF-1320746, IIS-1546452 and CCF-1564000, and partly during an internship in Microsoft research, India. \u00b2 Partly supported by the UTCS TAship (CS361 Spring 17 Introduction to Computer Security). \u00a7 Partly supported by the Australian Research Council through an Australian Laureate Fellowship (FL110100281) and the Australian Research Council Centre of Excellence for Mathematical and Statistical Frontiers (ACEMS), and NSF grants IIS-1619362. \u00b6 Partly supported by NSF grants CCF-1320746, IIS-1546452 and CCF-1564000.ar Xiv: 170 6.03 175v 1 [cs.L G] 10 Ju"}, {"heading": "Contents", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Related Work 4", "text": "2.1 Expressiveness.............................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3 Problem Formulation 6", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Positive Definiteness of Hessian 7", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 Tensor Methods for Initialization 9", "text": "..................................................................................................................................."}, {"heading": "6 Global Convergence 11", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 Numerical Experiments 12", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 Conclusion 13", "text": "References 14"}, {"heading": "A Notation 19", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B Preliminaries 19", "text": "B.1 Useful facts................................................................................................................."}, {"heading": "C Properties of Activation Functions 25", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D Local Positive Definiteness of Hessian 26", "text": "D.1 Main results for the positive definition of the Hessian. D.3. Error of the Hessian......... 26D.1.1. Limitation of the spectrum of the Hessian at ground level..... 26. D.1.2. Local linear convergence of the gradient slope...... 26D.1.1. Limitation of the spectrum of the Hessian at ground level..... 26. D.1.2. Local linear convergence of the gradient slope....... eigenvalues of the Hessian for the orthogonal case. 27D.2 Positive definition of the Hessian population at ground level............ 33. D.2.3 Upper limit of the Hessian eigenvalues for the non-orthogonal case. 29. D.2.2 Lower limit of the Hessian eigenvalues for the non-orthogonal case."}, {"heading": "E Tensor Methods 55", "text": "E.1. Tensor. Initialization Algorithm. Reduced Third Order. 71. Error. 71. Error. First. 57. E.3. Error. 55. E.2. Main Result for Tensor. Tensor.. 59. 3.1. Error. For the Second Order. Moment in Different Cases. 57. E.3. Error. 59. E.3.2 Error Bound for the Second Order. Weight. Matrix.... 59. 3.1 Error Bound for the Second Order. 63. Moment. 63. E.3.3. Subspace Estimation Using Power Method."}, {"heading": "1 Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2 Related Work", "text": "The recent empirical success of the NNs has strengthened their theoretical analyses [FZK + 16, Bal16, BMBY16, SBL16, APVZ14, AGMR17, GKKT17]. In this paper we divide them into three main directions."}, {"heading": "2.1 Expressive Power", "text": "Although single-layer neural networks with a sufficient number of hidden nodes cannot approach continuous function [Hor91], flat networks cannot in practice provide the same performance as deep networks. Theoretically, several recent studies show that the depth of NNs plays an essential role in the expressiveness of neural networks [DFS16]. As shown in [CSS16, CS16, Tel16], functions that can be implemented by a deep network of polynomic size require exponential size to be implemented by a flat network. [RPK + 16, PLR + 16, MPCB14, AGMR17] design some measurements of expression force that exhibit an exponential dependence on the depth of the network. However, the increase in the expressiveness of NNs or its depth also increases the difficulty of the learning process to achieve a sufficiently good model. In this paper, we focus on N1NN ensuring and a limited number of samples."}, {"heading": "2.2 Achievability of Global Optima", "text": "It is widely believed that the formation of deep models using linear methods works so well either because the fault surface has no local minimums or because they exist that they must be close to global minima. In particular [CHM + 15] shows that they analyze the loss surface of a specific neural network through spin-glass theory and show that it has exponentially many local optima, the loss of which is small and close to that of a global optimum."}, {"heading": "2.3 Generalization Bound / Recovery Guarantees", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "3 Problem Formulation", "text": "In view of a sentence of n samplesS = > (x1, y1), (x2, y2), (xn, xi), (xn, xi), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (2), \"(2),\" (2), \"(2),\" (2), \"(2),\", \"(2,\", \"(2),\", \"(2),\", \"(2),\", \"(2),\" (2, \",\" (2), \"(2),\", \"(2,\" (2), \"(2),\", \"(2),\" (), \",\", \"(2,\" (2), \",\" (2), \"(),\", \",\", \"(2,\" (2), \"(),\", \"(2,\"), \"(2,\"), \",\" (2, \"(),\", \",\" (2, \"),\" (2, \"),\", \"(2,\"), \",\" (), (2, \"(),\"), (2, (), (), (), (2, \",\", (), (2, \", (), (), (), (2, (2,\", (), (), (2, (), (), (2, \",\", (2, \", (), (2,\"), (2, \"), (, (,\"), (, (, (2, \"), (2, (),"}, {"heading": "4 Positive Definiteness of Hessian", "text": "In this section, we examine the Hessian empirical risk in the vicinity of the ground-based truth. We consider the case when v \u00b2 \u00b2 is already known. Note that with homogenous activations, we can assume that v \u00b2 i \u00b2 -1 \u00b2 -1 \u00b2 -1 \u00b2 -1 \u00b2 -1 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 \u00b2 -2 \u00b2 -2 \u00b2 \u00b2 -2 \u00b2 -2 \u00b2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 \u00b2 \u00b2 -2 \u00b2 -2 \u00b2 \u00b2 \u00b2 -2 \u00b2 -2 \u00b2 \u00b2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 \u00b2 \u00b2 \u00b2 -2 \u00b2 -2 \u00b2 \u00b2 \u00b2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 \u00b2 -2 \u00b2 \u00b2 \u00b2 \u00b2 -2 \u00b2 -2 \u00b2 \u00b2 -2 \u00b2 \u00b2 \u00b2 -2 \u00b2 -2 \u00b2 \u00b2 \u00b2 -2 \u00b2 \u00b2 \u00b2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 -2 \u00b2 \u00b2 -2 \u00b2 -2 \u00b2 \u00b2 \u00b2 -2 \u00b2 -2 \u00b2 \u00b2 -2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 -2 \u00b2 \u00b2 -2 \u00b2 -2 \u00b2 \u00b2 -2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 -2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 -2 \u00b2 \u00b2 -2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 -2 \u00b2 \u00b2 \u00b2 \u00b2 -2 \u00b2 -2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 -2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 -2 \u00b2 \u00b2 -2 \u00b2 \u00b2 \u00b2 \u00b2 -2 \u00b2 \u00b2 \u00b2 -2 \u00b2 \u00b2 -2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 -2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2"}, {"heading": "5 Tensor Methods for Initialization", "text": "In this section we show that tensor methods can restore the parameters W \u0445 with a certain precision and restore exactly v \u0445 for homogeneous activations. It is known that most tensor problems are NP-hard [H\u00e5s90, HL13] or even difficult to approximate [SWZ17b]. However, by making some assumptions, the tensor decomposition method becomes efficient [AGH + 14, WTSA15, WA16, SWZ16]. Here we use noiseless assumption and Gaussian input to show a detectable and efficient tensor method."}, {"heading": "5.1 Preliminary", "text": "Let us define a special external product for the simplification of notation. If v-Rd is a vector and I am the identity matrix, then v-r matrix (then v-r matrix as M = 1 is siviv > i and I is the identity matrix, then M-r-i = 1 is siviv > i and I is the identity matrix, then M-r-i = 1 si d-j j j = 1 6 l = 1 Al, i, j where A1, i, j vi vi ej ej vi vi vi vi vi vi vi ej, A3, i, j = ej vi vi vi vi vi vi vi vi, i j j vi vi vi vi, i j j j j vi vi vi vi vi, i vi vi vi vi vi vi vi vi vi."}, {"heading": "5.2 Algorithm", "text": "As shown in the previous section, we can use the non-orthogonal decomposition method [KCL15] to decompose the corresponding estimated tensor P 3 and obtain an approximation of the parameters.The precision of the obtained parameters depends on the estimation error of P3, which requires that the non-orthogonal decomposition method (d3 / 2) samples requires an error. Also, the time complexity for tensor decomposition on a d \u00d7 d tensor is (d3).In this paper, we reduce the cubic dependence of the sample / computational complexity in dimension [JSA15] to linear dependence."}, {"heading": "5.3 Theoretical Analysis", "text": "s leave the activation function a homogeneous satisfactory assumption 5.3. For each 0 < < 1 and t \u2265 1 there is an algorithm (algorithm 1) that takes time and prints a matrix W (0), Rd \u00b7 k and a vector v (0), Rk such that with a probability of at least 1 \u2212 d \u2212 (t), and W (0) \u2212 W \u00b2 F \u2264 \u00b7 poly (k), W \u00b7 F and v (0) i = v \u00b2 i."}, {"heading": "6 Global Convergence", "text": "If you combine the positive determination of Hessian in section 4 with the tensor initialization methods in section 5, you get the general, globally converging algorithm algorithm algorithm 2 and its guarantee clause 6.1.Theorem 6.1 (Guarantees for Global Convergence). Let S specify a series of i.i.d. samples from distribution D (defined in (1)) and the activation function be homogeneous, which satisfies property 3.1, 3.2, 3.3 (a) and assumption 5.3. Then, for each t \u2265 1 and each > 0, if | S | \u2265 d log (1 /) \u00b7 poly (log d, t, k, \u03bb), T \u2265 log (1 /) \u00b7 poly (k, \u03bd, \u04452p1 / \u03c1) and 0 < \u03b7 \u2264 1 / (kv2max\u043c 2p 1), then an algorithm follows (method Learning1NN in algorithm 2)."}, {"heading": "7 Numerical Experiments", "text": "In this section, we use synthetic data to verify our theoretical results. We generate data points {xi, yi} i = 1,2, \u00b7 \u00b7, n from the distribution D (defined in Eq. (1)). We set W * = 1 >, where U * Rd \u00b7 k and V * Rk \u00b7 k are orthogonal matrices generated from QR decomposition of Gaussian matrices, is a diagonal matrix whose diagonal elements are 1, 1 + 1 \u2212 1k \u2212 1, 1 \u00b7 k \u2212 1, \u00b7 \u00b7 \u00b7,. In this experiment, we set \u00b2 n = 2 and k = 5. We set v * i to be randomly selected by {\u2212 1} with equal chance. We use square ReLU * (z) = max {z, 0} 2, which is a smooth homogeneous function. For non-orthogonal tensorting methods, we use the code we provide."}, {"heading": "8 Conclusion", "text": "As shown in Theorem 6.1, tensor initialization followed by gradient descent will provide a globally converging algorithm with linear time / sample complexity in dimension, logarithmic precision and polynomial in other factors for smooth homogeneous activation functions. Our distilled properties for activation functions encompass a wide range of nonlinear functions and hopefully provide an intuition to understand the role of nonlinear activations in optimization. Deeper neural networks and convergence for SGD will be considered in the future."}, {"heading": "A Notation", "text": "For each positive integer n that we use to double the number of people recorded, we must specify the number of people recorded. (D) We must adjust the number of people recorded. (D) We must adjust the number of people recorded. (D) We must adjust the number of people recorded. (D) We must adjust the number of people recorded. (D) We must adjust the number of people recorded. (D) We must adjust the number of people recorded. (D) We must adjust the number of people recorded. (D) We must adjust the number of people recorded. (D) We must adjust the number of people recorded. (D) We must adjust the number of people recorded. (D) We must adjust the number of people recorded. (D) We must adjust the number of people recorded. (D) We must adjust the number of people recorded. (D) We must adjust the number of people recorded. (D) We must adjust the number of people recorded. (D) We must adjust the number of people recorded. (D) We must adjust the number of people recorded."}, {"heading": "B Preliminaries", "text": "In this section we present some lemons and side effects that are used in the proofs."}, {"heading": "B.1 Useful Facts", "text": "s specify a fixed d-dimensional vector, then for all C dimensions 1 and n-1, we havePr x-N (0, Id) [< x, z > 2 \u2264 5C-Z 2 log n] \u2265 1 \u2212 1 / (ndC).Proof. This results from Proposition 1.1 in [HKZ12].Fact B.2. For all C-1 and n-1, we havePr x-N (0, Id) [1 x-V-2 \u2264 5Cd log n].Proof. This follows Proposition 1.1 in [HKZ12].Fact B.3. Given a complete column range matrix N (0, Id) [w2, \u00b7 wk]."}, {"heading": "B.2 Matrix Bernstein", "text": "In many proofs, we have to determine the difference between some population matrices / tensors and their empirical versions. Typically, the classical amber matrix requires the norm of the random matrix (e.g.: theorem 6.1 in [Tro12]) or the random matrix the subexponential property (theorem 6.2 in [Tro12]). In our cases, however, most of the random matrices do not meet these conditions. So we derive the following lemmats, which can deal with random matrices that are almost certainly not limited or follow the subexponential distribution, but are likely to be limited.Lemma B.7 (amber matrix for unlimited cases (A modified version of the bounded case, theorem 6.1 in [Tro12]). Let us b denotes a distribution over Rd1 \u00b7 d2."}, {"heading": "C Properties of Activation Functions", "text": "Theorem 3.5. ReLU? p (z) = max {z, 0}, leaking ReLU? p (z) = max {z, 0.01z}, squared ReLU? p (z) = max {z, 0} 2 and all non-linear, non-decreasing smooth functions with limited symmetric effect (z), such as the Sigmoid function? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p?? p?? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p p p p p p p p? p p p p p p? p p p p p p p p p? p? p p? p? p? p p p p p p? p? p? p p? p? p p? p? p? p p p p? p p p? p? p p p p? p? p p p p p? p? p? p? p p p p? p p p p? p? p p p p p? p? p p? p p p? p p p? p p p p? p p p p p? p p? p p? p p? p p p p? p p p? p p p p? p p p? p p? p p p p? p? p p p p p p? p p? p p? p p p p p p? p p p? p? p p p p? p p p p p p? p? p p p p? p? p p? p p p? p? p p p p p? p p p p p p? p p p p p p? p p? p p p p? p p p p p p p p p? p p p p p p"}, {"heading": "D Local Positive Definiteness of Hessian", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D.1 Main Results for Positive Definiteness of Hessian", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D.1.1 Bounding the Spectrum of the Hessian near the Ground Truth", "text": "V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-"}, {"heading": "D.1.2 Local Linear Convergence of Gradient Descent", "text": "There is no linear convergence between our knowledge, so there is no linear convergence for general non-smooth goals, the ReLU.Theorem D.2 (linear convergence of gradients, formal version of theorem 4.4).Let us consider the current convergence of theorem 4.4).Let us consider the current convergence of theorem 4.4).W c (linear convergence of theorem 4.4).W c (linear convergence of theorem 4.4).W c (linear convergence of theorem 4.4).W c (linear convergence of theorem 4.4).W"}, {"heading": "D.2 Positive Definiteness of Population Hessian at the Ground Truth", "text": "The aim of this section is to prove Lemma D.3.Lemma D.3 (Positive definition of the population of Hesse by the basic truth). If \u03c6 (z) satisfies the properties 3.1,3.2 and 3.3, we have the following property for the second derivative of the function fD (W) at W \u0445, \u0442 (v2min\u03c1 (\u03c3k) / (\u0445 2\u03bb))) I \u04412fD (W \u0445) O (kv2max\u03c3 2p 1) I.Proof. The proof follows directly on Lemma D.6 (section D.2.2) and Lemma D.7 (section D.2.3)."}, {"heading": "D.2.1 Lower Bound on the Eigenvalues of Hessian for the Orthogonal Case", "text": "The main idea is to calculate the LHS of Eq (8) and then reformulate the equation and find a lower limit, which is i-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K)."}, {"heading": "D.2.2 Lower Bound on the Eigenvalues of Hessian for Non-orthogonal Case", "text": "The main idea is to reduce the problem to a k-by-k problem = > then reduce the eigenvalues using the orthogonal weight matrices. If we (z) meet the properties 3.1,3,2 and 3.3, we have the following property for the second derivative of the function fD (W) at the W limit (v2mina) / (v2mina))) I-2fD (W-2fD). If we leave a Rdk denote vector [a > 1 a > 2 \u00b7 a > k] >, let b > Rdk denote vector [b > 2 b > k > k] > k > k > k) > and let c > Rdk denote vector [c > 1 c > k > i) > i denote vector [c > k > i) >. We can calculate the smallest eigenvalue of the Hessian K limit."}, {"heading": "D.2.3 Upper Bound on the Eigenvalues of Hessian for Non-orthogonal Case", "text": "Lemma D.7 (Top Boundary). If the \u00b2 p \u00b2 p \u00b2 p p \u00b2 p p p p \u00b2 p p p \u00b2 p p p \u00b2 p p \u00b2 p p \u00b2 p p \u00b2 p p \u00b2 p p p \u00b2 p p \u00b2 p p \u00b2 p p \u00b2 p p \u00b2 p p \u00b2 p \u00b2 p p \u00b2 p p \u00b2 p \u00b2 p p \u00b2 p p \u00b2 p p \u00b2 p p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2) p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2) p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2"}, {"heading": "D.3 Error Bound of Hessians near the Ground Truth for Smooth Activations", "text": "The aim of this section is to prove Lemma D.8Lemma D.8 (margin of error of Hesse near ground level for smooth activation). Let \u03c6 (z) satisfy Property 3.1, Property 3.2 and Property 3.3 (a). Let W specify a series of i.i.d. samples from the distribution defined in (1). Then we have for all t \u2265 1 and 0 < < 1 / 2, if | S | 2 dB \u00b7 Poly (log d, t), then with probability at least 1 \u2212 1 / dB (t), as well as for each t \u00b2 S (W) \u2212 2fD (W). v2maxk.p 1 (\u03c3 p 1 + 2 W \u2212 W).Proof. This follows by combining Lemma D.10 and Lemma D.11 directly."}, {"heading": "D.3.1 Second-order Smoothness near the Ground Truth for Smooth Activations", "text": "The objective of this section is to lemmas D.10.Fact D.9. Let us use the i th column of W (max), W (max), W (max), W (max), W (max), W (max), W (max), W (max), W (w), W (w), W (max), W (w), W (max), W (w), W (w), W (w), W (w), W (w), W (w), W (max), W (w), W (w), W (max), W (w), W (w), W (w), W (w), W (w), W (w), W (w)."}, {"heading": "D.3.2 Empirical and Population Difference for Smooth Activations", "text": "s inequality requires the spectral norm of each random matrix, which almost certainly needs to be limited. Details can be assumed in Lemma B.7 and Corollary B.8.Lemma D.11 (Empirical Distribution and Population Difference for Smooth Activations). Let us (z) satisfy the properties 3.1,2 and 3.3 (a). Let W \u2212 W \u2212 W and Corollary B.8.Lemma D.11 (Empirical and Population Difference for Smooth Activations), x-x-x (z)."}, {"heading": "D.5 Positive Definiteness for a Small Region", "text": "\u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2"}, {"heading": "E Tensor Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "E.1 Tensor Initialization Algorithm", "text": "Note: The eigenvalues of P2 and P + 2 are not necessarily negative. However, only k of eigenvalues is large enough, so we can first select the top k eigenvectors / eigenvalues of C + P + 2 and C + I \u2212 P \u00b2 2, where C is large enough that C + 2 P2 is large enough. Then, we select the top k eigenvectors with the largest eigenvalues of the order of magnitude executed in TopK in algorithm 3. For the outputs of TopK, k2 are the numbers of the selected largest eigenvalues of C + P \u00b7 2 and C \u00b7 P \u00b7 I \u2212 P \u00b2."}, {"heading": "E.2 Main Result for Tensor Methods", "text": "The goal of this section is to prove theorem 5.6.Theorem 5.6. Leave the activation function as a homogeneous, satisfactory Assumption 5.3. For all 0 < 1 and 2, if you have a matrix W (0), k) and a vector v (0), so that there is an algorithm (algorithm 1) that gives the algorithms 2 (1) and 2 (1) time and outputs of a matrix W (0), k (0) and a vector v (0), so that, with Algorithm 4 Recovery of the Ground Truth Parameters of the Neural Network, i.e., w (0) and v (1) Recovery W (V)."}, {"heading": "E.3 Error Bound for the Subspace Spanned by the Weight Matrix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "E.3.1 Error Bound for the Second-order Moment in Different Cases", "text": "1. Let M2 be defined as in Definition 5.1. Let M2 be defined as in Definition 5.1. Let M2 be defined as in Definition 5.1 as in Definition 2.2. (1) Let M2 be defined as in Definition 2.1 as in Definition 2.2. (1) Let M2 be defined as in Definition 2.1 as in Definition 2.2. (1) Let M2 be defined as in Definition 2.2. (1) Let M2 as in Definition 2.1. (1) Let M2 as in Definition 3.1. (1) Let M2 as in Definition 2.2. (1) Let M2 as in Definition 2.1. (1) Let M2 as in Definition 2.1. (1) Let M2 as in Definition 2.2. (1) Let M2 as in Definition. (1) Let M2 as in Definition 3.1. (1) Let M2 as in Definition 2.1. (1) Let M2 as in Definition 2.1. (1) Let M2 as in Definition. (1) Let M2 as in Definition. (1) Let M2 as in Definition. (1) Let M2 as in Definition. (1) Let M2 as in Definition 2.2. (1) Let M2 as in Definition. (1) Let M2 as in Definition 2.2. (1) Let M2 as in Definition. (1. (1) Let M2 as in Definition. (1.) Let M2 as in Definition. (1. (1) Let M2 as in Definition. (1.) Let M2 as in Definition. (1. (1.)"}, {"heading": "E.3.2 Error Bound for the Second-order Moment", "text": "The aim of this section is to prove Lemma E.5, which shows that we can approximate the moments of the second order to a certain precision by using linear sample complexity in Lemma E.5 (estimation of the moment of the second order). Let us define P2 and j2 in Definition 5.4. Suppose the activation function satisfies the one from the distribution D (defined in (1). Let P 2 be the empirical version of P2 using the data set S, i.e. P 2 = ES [P2]. Suppose the activation function satisfies property 3.1 and assumption 5.3. Suppose that the 0 < < 1 and t \u2265 1 and m0 = mini [k] [mj2, i | 2 (w \u00b2) 2 (j2 \u2212 j2) and the probability jj2 (j2 \u2212 j2) that we have the probability."}, {"heading": "E.3.3 Subspace Estimation Using Power Method", "text": "Let us have a large positive number so that C > 2% P2% P2% P2% P2% P2% P2% P2% P2% P2% P2% P2% P2% P2% P2% P2% P2% P2% P2% P2% P2 (P2). Let us let C be a large positive number so that C > 2% P2% P2% P2% P2% P2% P2%. Then after T = O (log (1 /))), the output of algorithm 3 \u2212 P2% P2% P2%. Let C be a large positive number so that C > 2% P2% P2% P2% P2% P2% P2% P2% P2% P2%. Then after T = O (log (1 /)))."}, {"heading": "E.4 Error Bound for the Reduced Third-order Moment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "E.4.1 Error Bound for the Reduced Third-order Moment in Different Cases", "text": "Lemma E.8. Let M3 be defined as in Definition 5.1. Let M3 be defined as in Definition 5.1. Let M3 be defined as in Definition 5.1. Let M3 be the empirical version of M3, i.e., let M3 as in Definition (x, y) as in Definition (x, x, x) as in Definition (x, x) as in Definition (x, x) as in Definition (x, x) as in Definition (x, x) as in Definition (x, x) as in Definition (x). Let V as in Definition (x, x) as in Definition (x, x) as in Definition (x) as in (x) as in (x)."}, {"heading": "E.4.2 Final Error Bound for the Reduced Third-order Moment", "text": "s define the orthogonal column span of W \u00b2: = P3 (V, V, V, V, V), where P3 is defined in definition 5.4 using \u03b1. Let R \u00b2 3 be the empirical version of R3 using Dataset S, with each sample of S i.i.d. from the distribution D (defined in (1)). Suppose the activation function is property 3.1 and the assumption 5.3. Let R \u00b2 3 be the empirical version of R3 using Dataset S, with each sample of S i.i.i.d. being based on the distribution D (defined in (1)). Suppose the activation function is property 3.1 and the assumption 5.3."}, {"heading": "E.5 Error Bound for the Magnitude and Sign of the Weight Vectors", "text": "The lemmas in this section together with lemmas E.5 provide guarantees for algorithm 4. In particular, lemmas E.12 shows with linear sample complexity in d that we can approximate the 1storder moment with a certain precision. And lemmas E.13 and lemmas E.14 represent the error limits of linear systems equivalent under certain disturbances (28)."}, {"heading": "E.5.1 Robustness for Solving Linear Systems", "text": "In view of two matrices A, A and A and two vectors b, b and Rd. Let us leave z = argminz and z = argminz and z = argminz and z = argminz and Rk. (A + A) z \u2212 (b + b). Let us leave z = argminz and z = argminz and z = argminz and Rk. (A + A) z \u2212 (A) 2 (A) + \u03c3 \u2212 2 k (A)."}, {"heading": "E.5.2 Error Bound for the First-order Moment", "text": "Lemma E.12 (Error bound for the first-order moment). Let Q1 be defined as follows: \"..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "E.5.3 Linear System for the First-order Moment", "text": "The following Lemma returns estimation errors for the first linear system in Eq. (28).Lemma E.13 (solution of the linear system for the moment of the first order).Let U-Rd-k be the orthogonal column span of W-K. Let V-Rd-k specify an orthogonal matrix that satisfies V-V > \u2212 UU >. Let Q1 be the empirical version of Q1 as in Eq. (25) and Q-1 so that Q1 \u2212 Q-1 represents the vector satisfactorily."}, {"heading": "E.5.4 Linear System for the Second-order Moment", "text": "The following Lemma returns estimation errors for the second linear system in Eq. (28).Lemma E.14 (solution of the linear system for the moment of the second order).Let U-Rd \u00b7 k have the orthogonal column span of W-V = > H-V = > W-V > (solution of the linear system for the moment of the second order).Let U-Rd \u00b7 k have the orthogonal column span of W-V = > W-V > W-V = > W-V > (solution of the linear system for the moment of the second order).Let Q2 as in Eq. (26) and Q-2 have the estimate of Q2 such that Q2 \u2212 Q-2 is the vector satisfaction of the first and second columns."}, {"heading": "F Acknowledgments", "text": "The authors thank Surbhi Goel, Adam Klivans, Qi Lei, Eric Price, David P. Woodruff, Peilin Zhong, Hongyang Zhang and Jiong Zhang for their useful discussions."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>In this paper, we consider regression problems with one-hidden-layer neural networks (1NNs).<lb>We distill some properties of activation functions that lead to local strong convexity in the<lb>neighborhood of the ground-truth parameters for the 1NN squared-loss objective. Most popular<lb>nonlinear activation functions satisfy the distilled properties, including rectified linear units<lb>(ReLUs), leaky ReLUs, squared ReLUs and sigmoids. For activation functions that are also<lb>smooth, we show local linear convergence guarantees of gradient descent under a resampling rule.<lb>For homogeneous activations, we show tensor methods are able to initialize the parameters to<lb>fall into the local strong convexity region. As a result, tensor initialization followed by gradient<lb>descent is guaranteed to recover the ground truth with sample complexity d \u00b7 log(1/ ) \u00b7poly(k, \u03bb)<lb>and computational complexity n \u00b7 d \u00b7 poly(k, \u03bb) for smooth homogeneous activations with high<lb>probability, where d is the dimension of the input, k (k \u2264 d) is the number of hidden nodes, \u03bb<lb>is a conditioning property of the ground-truth parameter matrix between the input layer and<lb>the hidden layer, is the targeted precision and n is the number of samples. To the best of our<lb>knowledge, this is the first work that provides recovery guarantees for 1NNs with both sample<lb>complexity and computational complexity linear in the input dimension and logarithmic in the<lb>precision. \u2217A preliminary version of this paper appears in Proceedings of the Thirty-fourth International Conference on<lb>Machine Learning (ICML 2017).<lb>\u2020Supported in part by NSF grants CCF-1320746, IIS-1546452 and CCF-1564000, and part of the work was done<lb>while interning in Microsoft research, India.<lb>\u2021Supported in part by UTCS TAship (CS361 Spring 17 Introduction to Computer Security).<lb>Supported in part by Australian Research Council through an Australian Laureate Fellowship (FL110100281) and<lb>through the Australian Research Council Centre of Excellence for Mathematical and Statistical Frontiers (ACEMS),<lb>and NSF grants IIS-1619362.<lb>\u00b6Supported in part by NSF grants CCF-1320746, IIS-1546452 and CCF-1564000.<lb>ar<lb>X<lb>iv<lb>:1<lb>70<lb>6.<lb>03<lb>17<lb>5v<lb>1<lb>[<lb>cs<lb>.L<lb>G<lb>]<lb>1<lb>0<lb>Ju<lb>n<lb>20<lb>17", "creator": "LaTeX with hyperref package"}}}