{"id": "1706.03301", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2017", "title": "Neural Networks and Rational Functions", "abstract": "Neural networks and rational functions efficiently approximate each other. In more detail, it is shown here that for any ReLU network, there exists a rational function of degree $O(\\text{polylog}(1/\\epsilon))$ which is $\\epsilon$-close, and similarly for any rational function there exists a ReLU network of size $O(\\text{polylog}(1/\\epsilon))$ which is $\\epsilon$-close. By contrast, polynomials need degree $\\Omega(\\text{poly}(1/\\epsilon))$ to approximate even a single ReLU. When converting a ReLU network to a rational function as above, the hidden constants depend exponentially on the number of layers, which is shown to be tight; in other words, a compositional representation can be beneficial even for rational functions.", "histories": [["v1", "Sun, 11 Jun 2017 03:07:42 GMT  (731kb,D)", "http://arxiv.org/abs/1706.03301v1", "To appear, ICML 2017"]], "COMMENTS": "To appear, ICML 2017", "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["matus telgarsky"], "accepted": true, "id": "1706.03301"}, "pdf": {"name": "1706.03301.pdf", "metadata": {"source": "META", "title": "Neural networks and rational functions", "authors": ["Matus Telgarsky"], "emails": ["<mjt@illinois.edu>."], "sections": [{"heading": "1. Overview", "text": "The aim of this paper is to characterize neural networks more precisely by finding a functional class that is well approximated not only by neural networks, but also by neural networks. The functional class examined here is the class of rational functions: functions that are represented as the ratio of two polynomials, the denominator being a strictly positive polynomial. For the sake of simplicity, neural networks are always used as ReLU activation \u03c3r (x): = max {0, x}; for checking neural networks and their terminology, the reader is referred to Section 1.4. For the sake of brevity, a network with ReLU activation is simply called a ReLU network."}, {"heading": "1.1. Main results", "text": "The main theorems state that ReLU networks and rational functions converge in sense1University of Illinois, Urbana-Champaign; work completed while attending the Simons Institute. \u2212 Answer to: Your friends < mjt @ illinois.edu >.Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, 2017. \u2212 Answer to the question whether function 1 / 4 \u2212 0.25 0.00 0.50 0.75 1.0001234 spikerat poly netFigure 1. Rational, polynomial, and ReLU network is appropriate to \"spike,\" a function that requires 1 / 4 \u2212 0.25 0.50 0.00 0.00 0.00 0.50 0.50 0.00 -- a representation with the other that the approximation of one class to the other requires a representation whose size is polynomial in ln (1 /) rather than polynomial in 1 / 1. Theorem 1.1."}, {"heading": "1.2. Auxiliary results", "text": "First, it should be emphasized that theorem 1.1 with polynomials is impossible: namely, while it is true that ReLU networks can efficiently approximate polynomials (Jarotsky, 2016; Safran & Shamir, 2016; Liang & Srikant, 2017) in order to approximate a single ReLU or equivalent absolute value function (Petrushev & Popov, 1987, Chapter 4, page 73), another interesting point is the depth required when converting a rational function into a ReLU network. Theorem 1.1 is impossible if the depth is o (ln (1 /)): Specifically, it is impossible to approximate the rational function of degree 1 (Petrushev & Popov, 1987, Chapter 4, page 73)."}, {"heading": "1.3. Related work", "text": "The results of this work follow a long series of papers on the representational power of neural networks and related functions. Undoubtedly, the ability of ReLU networks to adapt to continuous functions has been proven many times, but it seems that the earliest reference refers to Lebesgue (Newman, 1964, page 1), although of course results of this type are usually much more contemporary (Cybenko, 1989). More recently, it has been shown that certain functional classes only allow approximate representations with many layers (Telgarsky, 2015), followed by evidence showing the possibility of a Depth-3 function requiring exponentially many nodes when rewritten with 2 layers (Eldan & Shamir, 2016). There are also a variety of other results that give the ability of ReLU networks to approximate different functional classes (Cohen et al, 2016; Poggio et al, 2017)."}, {"heading": "1.4. Further notation", "text": "Here is a brief description of the type of neural networks used in this thesis. Neural networks represent the calculation as a directed graph in which nodes consume the results of their parents, apply a calculation to them, and pass on the resulting value. In the present thesis, nodes take the results of their parents z and compute \u03c3r (a > z + b), where a is a vector, b is a scalar, and \u03c3r (x): = max {0, x}; another popular nonlinear choice is the sigmoid x 7 \u2192 (1 + exp (\u2212 x) \u2212 1. The graphs in this thesis are acyclic and connected to a single node that lacks children that are referred to as univariate output, but the literature contains many variations on all these choices.As already mentioned, a rational function f: Rd \u2192 R is the ratio of two polynomials."}, {"heading": "2. Approximating ReLU networks with rational functions", "text": "In this section the proofs of Part 2 of Theorem 1.1, Theorem 1.2, Lemma 1.3 and Conclusion 1.4 are developed."}, {"heading": "2.1. Newman polynomials", "text": "The starting point is a groundbreaking result in the theory of rational functions (Zolotarev, 1877; Newman, 1964): There is a rational function of degree O (ln (1 /) 2 that can approximate the absolute value function along [\u2212 1, + 1] to accuracy > 0, which in turn gives the way to approximate the ReLU, since\u03c3r (x) = max {0, x} = x + | x | 2. (2.1) The construction here uses the Newman polynomials (New-man, 1964): You give it an integer r, defineNr (x): = r \u2212 1 = 1 (x + exp (\u2212 i / \u221a r))). The Newman polynomials N5, N9 and N13 are shown in Figure 3. Typical polynomials in the approximation theory (e.g. the Chebyshev polynomials) can be represented as very active oscillations."}, {"heading": "2.2. Proof of Lemma 1.3", "text": "Now that a single ReLU can easily be converted into a rational function, the next task is to replace each ReLU in a ReLU network with a rational function and to calculate the approximation error. This is exactly the statement of Lemma 1.3. The proof for Lemma 1.3 is an induction on layers, with all the details banished to the appendix. However, the key calculation is as follows: Let R (x) specify a rational approximation to layer i + 1 and let H (x) designate the polyvalent figure calculated by layer i and let HR (x) recognize the figure obtained by having each individual node in H fixed by R. Fix any node in layer i + 1, and let x 7 (a > H (x) + b) designate its output as a function of input."}, {"heading": "2.3. Proof of part 2 of Theorem 1.1", "text": "The first step, via term 1.3, is to replace each \u03c3r with a rational low-degree function (this last bit using Newman polynomials); the second step is to inductively collapse the network into a single rational function; the reason for the dependence on the number of nodes m is that the summation of rational functions, in contrast to polynomials, results in an increase in degree: p1 (x) q1 (x) + p1 (x) q2 (x) = p1 (x) q2 (x) + p2 (x) q1 (x) q2 (x) q2 (x)."}, {"heading": "2.4. Proof of Theorem 1.2", "text": "The last interesting bit is to show that the dependence on ml in part 2 of theorem 1.1 (where m is the number of nodes and l is the number of layers) is narrow. Remember the \"triangular function\" \u2206 (x): = 2x x [0, 1 / 2], 2 (1 \u2212 x) x (1 / 2, 1], 0 otherwise. The k-fold composition \u2206 k is a piecemeal affine function with 2k \u2212 1 regularly tense peaks (Telgarsky, 2015). This function has proved to be inapproximable by flat networks of subexponential size and may now also prove to be a hard case for rational approximation.Let's consider the horizontal line by y = 1 / 2. The \u0441k function crosses this line 2k times. Let's now consider a rational function f (x) = p (x) / q (x).The set of points where f (x) = 1 / 2 points correspond to those of 2k = 2k (2k) is greater than the number (2k = 2p) by 0.2p (2k)."}, {"heading": "3. Approximating rational functions with ReLU networks", "text": "In this section, the proof for Part 1 of Theorem 1.1 and the density result in Sentence 1.5 are developed."}, {"heading": "3.1. Proving part 1 of Theorem 1.1", "text": "The first step is the approximation of the polynomials to the ReLU networks, and the second is the approximation to the division operation.The representation of the polynomials will be based on constructions based on Yarotsky (2016).The starting point is the following approximation of the quadrature function. Lemma 3.1 (Yarotsky, 2016).Let any kind of polynomial [1] be given. There is f: x \u2192 [0], represented as a ReLU network with O (1 /))) nodes and layers, so that supx [0,1]. (f) \u2212 x2 (0) \u2212 x2 and f (0).Yarotsky's proof is beautiful and deserves mention.The approximation of x2 is the function fk, defined as asfk (x)."}, {"heading": "3.2. Proof of Proposition 1.5", "text": "It remains to be shown that flat networks struggle to approach the reciprocal map x 7 \u2192 1 / x, using the same pattern as various proofs in (Telgarsky, 2016), followed by more recent work (Yarotsky, 2016; Safran & Shamir, 2016): the idea is to first limit the number of affine parts in ReLU networks to a certain size, and then point out that each linear segment must make significant errors in a curved function, namely 1 / x.The evidence is fairly crude and thus relegated to the appendices."}, {"heading": "4. Summary of figures", "text": "During this work, a number of numbers were presented to show not only the amazing approximation properties of rational functions, but also the higher accuracy achieved by both ReLU networks and rational functions compared to polynomials. Of course, this is only a qualitative demonstration, but still gives some intuition. In all these demonstrations, rational functions and polynomials have grade 9, unless otherwise marked. ReLU networks have two hidden layers, each with 3 nodes. This is not exactly apples (e.g. the rational function has twice as many parameters as the polynomial), but still reasonable, as most of the approximation literature fixes polynomial and rational degrees in comparisons. Figure 1 shows the ability of all three classes to approximate a truncated reciprocal function. Both rational functions and ReLU networks have the ability to form \"switching nodes,\" which allow them to approximate different functions at different intervals with low complexity."}, {"heading": "5. Open problems", "text": "There are many next steps for this and related outcomes: 1. Can rational functions or another approximate class be used to bind the generalization properties of neural networks more tightly? Remarkably, the VC dimension of sigmoid networks uses conversion to polynomials (Anthony & Bartlett, 1999); 2. Can rational functions or another approximate class be used to design algorithms for forming neural networks? It does not seem easy to design reasonable algorithms to minimize versus rational functions; if this is fundamental and also contrary to neural networks, it indicates an algorithmic benefit of neural networks; 3. Can rational functions or another approximate class provide a sufficiently refined complexity estimate of neural networks that can then be transformed into a neural network regulatory scheme?"}, {"heading": "Acknowledgements", "text": "The author thanks Adam Klivans and Suvrit Sra for stimulating conversations. Adam Klivans and the author thank Almare Gelato Italiano in downtown Berkeley for further stimulating conversations, but now on the topic of health and exercise. Finally, the author thanks the University of Illinois, Urbana-Champaign, and the Simons Institute in Berkeley for financial support for this work."}, {"heading": "A. Deferred material from Section 2", "text": "first step consists of some missing details about newman polynomials.A.1. newman polynomialsdefine the newman polynomial (newman, 1964) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Newman # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Newman # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "B. Deferred material from Section 3", "text": "B.1. As proof of part 1 of theorem 1.1To begin with, Yarotsky (f / 2) (f / 2) slightly adjusted the lemmas to limit the range to [0, 1]. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}], "references": [{"title": "Neural Network Learning: Theoretical Foundations", "author": ["Anthony", "Martin", "Bartlett", "Peter L"], "venue": null, "citeRegEx": "Anthony et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Anthony et al\\.", "year": 1999}, {"title": "Log depth circuits for division and related problems", "author": ["Beame", "Paul", "Cook", "Stephen A", "Hoover", "H. James"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Beame et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Beame et al\\.", "year": 1986}, {"title": "On the expressive power of deep learning: A tensor analysis", "author": ["Cohen", "Nadav", "Sharir", "Or", "Shashua", "Amnon"], "venue": "COLT", "citeRegEx": "Cohen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2016}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["Cybenko", "George"], "venue": "Mathematics of Control, Signals and Systems,", "citeRegEx": "Cybenko and George.,? \\Q1989\\E", "shortCiteRegEx": "Cybenko and George.", "year": 1989}, {"title": "The power of depth for feedforward neural networks", "author": ["Eldan", "Ronen", "Shamir", "Ohad"], "venue": "In COLT,", "citeRegEx": "Eldan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Eldan et al\\.", "year": 2016}, {"title": "Reliably learning the relu in polynomial time", "author": ["Goel", "Surbhi", "Kanade", "Varun", "Klivans", "Adam", "Thaler", "Justin"], "venue": "In COLT,", "citeRegEx": "Goel et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Goel et al\\.", "year": 2017}, {"title": "Why deep neural networks for function approximation", "author": ["Liang", "Shiyu", "R. Srikant"], "venue": "In ICLR,", "citeRegEx": "Liang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2017}, {"title": "Constructive approximation : advanced problems", "author": ["G.G. Lorentz", "Golitschek", "Manfred von", "Makovoz", "Yuly"], "venue": "Michigan Math. J., 11(1):11\u201314,", "citeRegEx": "Lorentz et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Lorentz et al\\.", "year": 1996}, {"title": "Approximating threshold circuits by rational functions", "author": ["Paturi", "Ramamohan", "Saks", "Michael E"], "venue": "Inf. Comput.,", "citeRegEx": "Paturi et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Paturi et al\\.", "year": 1994}, {"title": "Rational approximation of real functions. Encyclopedia of mathematics and its applications", "author": ["Petrushev", "P.P. Penco Petrov", "Popov", "Vasil A"], "venue": null, "citeRegEx": "Petrushev et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Petrushev et al\\.", "year": 1987}, {"title": "Why and when can deep \u2013 but not shallow \u2013 networks avoid the curse of dimensionality: a review", "author": ["Poggio", "Tomaso", "Mhaskar", "Hrushikesh", "Rosasco", "Lorenzo", "Miranda", "Brando", "Liao", "Qianli"], "venue": null, "citeRegEx": "Poggio et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Poggio et al\\.", "year": 2017}, {"title": "Depth separation in relu networks for approximating smooth non-linear functions. 2016", "author": ["Safran", "Itay", "Shamir", "Ohad"], "venue": null, "citeRegEx": "Safran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Safran et al\\.", "year": 2016}, {"title": "Representation benefits of deep feedforward networks", "author": ["Telgarsky", "Matus"], "venue": null, "citeRegEx": "Telgarsky and Matus.,? \\Q2015\\E", "shortCiteRegEx": "Telgarsky and Matus.", "year": 2015}, {"title": "Benefits of depth in neural networks", "author": ["Telgarsky", "Matus"], "venue": "In COLT,", "citeRegEx": "Telgarsky and Matus.,? \\Q2016\\E", "shortCiteRegEx": "Telgarsky and Matus.", "year": 2016}, {"title": "Splines, rational functions and neural networks", "author": ["Williamson", "Robert C", "Bartlett", "Peter L"], "venue": "In NIPS,", "citeRegEx": "Williamson et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Williamson et al\\.", "year": 1991}, {"title": "Error bounds for approximations with deep relu networks. 2016", "author": ["Yarotsky", "Dmitry"], "venue": null, "citeRegEx": "Yarotsky and Dmitry.,? \\Q2016\\E", "shortCiteRegEx": "Yarotsky and Dmitry.", "year": 2016}, {"title": "denote the triangle function from (Telgarsky", "author": ["R R"], "venue": null, "citeRegEx": "R\u2192,? \\Q2015\\E", "shortCiteRegEx": "R\u2192", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "There are also a variety of other result giving the ability of ReLU networks to approximate various function classes (Cohen et al., 2016; Poggio et al., 2017).", "startOffset": 117, "endOffset": 158}, {"referenceID": 10, "context": "There are also a variety of other result giving the ability of ReLU networks to approximate various function classes (Cohen et al., 2016; Poggio et al., 2017).", "startOffset": 117, "endOffset": 158}, {"referenceID": 7, "context": "Rational functions are extensively studied in the classical approximation theory literature (Lorentz et al., 1996; Petrushev & Popov, 1987).", "startOffset": 92, "endOffset": 139}, {"referenceID": 1, "context": ") An ICML reviewer revealed prior work which was embarrassingly overlooked by the author: it has been known, since decades ago (Beame et al., 1986), that neural networks using threshold nonlinearities (i.", "startOffset": 127, "endOffset": 147}, {"referenceID": 5, "context": "Together this suggests that not only the connections between rational functions and neural networks are tight (and somewhat known/unsurprising), but also that threshold networks and ReLU networks have perhaps more similarities than what is suggested by the differing VC dimension bounds, approximation results, and algorithmic results (Goel et al., 2017).", "startOffset": 335, "endOffset": 354}, {"referenceID": 7, "context": "Following conventions in the approximation theory literature (Lorentz et al., 1996), the denominator polynomial will always be strictly positive.", "startOffset": 61, "endOffset": 83}, {"referenceID": 1, "context": "This leads to the second trick (which was also used by Beame et al. (1986)!).", "startOffset": 55, "endOffset": 75}], "year": 2017, "abstractText": "Neural networks and rational functions efficiently approximate each other. In more detail, it is shown here that for any ReLU network, there exists a rational function of degreeO(poly log(1/ )) which is -close, and similarly for any rational function there exists a ReLU network of size O(poly log(1/ )) which is -close. By contrast, polynomials need degree \u03a9(poly(1/ )) to approximate even a single ReLU. When converting a ReLU network to a rational function as above, the hidden constants depend exponentially on the number of layers, which is shown to be tight; in other words, a compositional representation can be beneficial even for rational functions. 1. Overview Significant effort has been invested in characterizing the functions that can be efficiently approximated by neural networks. The goal of the present work is to characterize neural networks more finely by finding a class of functions which is not only well-approximated by neural networks, but also well-approximates neural networks. The function class investigated here is the class of rational functions: functions represented as the ratio of two polynomials, where the denominator is a strictly positive polynomial. For simplicity, the neural networks are taken to always use ReLU activation \u03c3r(x) := max{0, x}; for a review of neural networks and their terminology, the reader is directed to Section 1.4. For the sake of brevity, a network with ReLU activations is simply called a ReLU network. 1.1. Main results The main theorem here states that ReLU networks and rational functions approximate each other well in the sense University of Illinois, Urbana-Champaign; work completed while visiting the Simons Institute. Correspondence to: your friend <mjt@illinois.edu>. Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s). \u22121.00 \u22120.75 \u22120.50 \u22120.25 0.00 0.25 0.50 0.75 1.00 0 1 2 3 4 spike rat poly net Figure 1. Rational, polynomial, and ReLU network fit to \u201cspike\u201d, a function which is 1/x along [1/4, 1] and 0 elsewhere. that -approximating one class with the other requires a representation whose size is polynomial in ln(1 / ), rather than being polynomial in 1/ . Theorem 1.1. 1. Let \u2208 (0, 1] and nonnegative integer k be given. Let p : [0, 1] \u2192 [\u22121,+1] and q : [0, 1] \u2192 [2\u2212k, 1] be polynomials of degree \u2264 r, each with\u2264 smonomials. Then there exists a function f : [0, 1] \u2192 R, representable as a ReLU network of size (number of nodes) O ( k ln(1 / ) + min { srk ln(sr / ), sdk ln(dsr / ) }) ,", "creator": "LaTeX with hyperref package"}}}