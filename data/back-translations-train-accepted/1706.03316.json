{"id": "1706.03316", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2017", "title": "Collect at Once, Use Effectively: Making Non-interactive Locally Private Learning Possible", "abstract": "Non-interactive Local Differential Privacy (LDP) requires data analysts to collect data from users through noisy channel at once. In this paper, we extend the frontiers of Non-interactive LDP learning and estimation from several aspects. For learning with smooth generalized linear losses, we propose an approximate stochastic gradient oracle estimated from non-interactive LDP channel, using Chebyshev expansion. Combined with inexact gradient methods, we obtain an efficient algorithm with quasi-polynomial sample complexity bound. For the high-dimensional world, we discover that under $\\ell_2$-norm assumption on data points, high-dimensional sparse linear regression and mean estimation can be achieved with logarithmic dependence on dimension, using random projection and approximate recovery. We also extend our methods to Kernel Ridge Regression. Our work is the first one that makes learning and estimation possible for a broad range of learning tasks under non-interactive LDP model.", "histories": [["v1", "Sun, 11 Jun 2017 07:04:50 GMT  (29kb)", "http://arxiv.org/abs/1706.03316v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["kai zheng 0007", "wenlong mou", "liwei wang 0001"], "accepted": true, "id": "1706.03316"}, "pdf": {"name": "1706.03316.pdf", "metadata": {"source": "CRF", "title": "Making Non-interactive Locally Private Learning Possible", "authors": ["Kai Zheng", "Wenlong Mou", "Liwei Wang"], "emails": ["zhengk92@pku.edu.cn", "mouwenlong@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 170 6.03 316v 1 [cs.L G] 1"}, {"heading": "1 Introduction", "text": "In fact, it is the case that most of them are able to move into another world, in which they are able to move into another world, in which they are able to move into another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they are"}, {"heading": "2 Preliminaries", "text": "Some notations: [p] = {1, 2, \u00b7 \u00b7, p}. Vectors are written in bold symbol, e.g. x, w. x represents the univariate number which has no relation to x. For a vector x = [x1, x2, \u00b7 \u00b7, xd] T, xk represents the power of each element. B2 (r) = {x | x x x x x \u00b2 2 6 r}. If you define S + as the semipositive matrix space, ProjS + (\u00b7) means the projection of a matrix according to the Frobenius norm (i.e. the elimination of all negative eigenvalues). For a univariate function f (x), f (k) (x) it represents its k-th derivative and defines that f (k) cannot be found in the supplementary form."}, {"heading": "2.1 Local Differential Privacy", "text": "Here we adopt the LDP definition, which in [1].Definition 1. A mechanism Q: V \u2192 Z is referred to as (1) -local differential private or (2) -LDP, if for any v, v. \"V\".V and any (measurable) subset S \".V.\" There are corresponding basic results for LDP: Lemma 1 (Gaussian mechanism).If V = {v. \"Rd.\" V. \".V\".V \".V\".V \".V.\".V. \".V.\".V \".V\".V \".V\".V \".V\".V \".V\".V \"is\".V \".V\" (1) -LDP \".V\".V \".V\".V \".V.\" (.V. \".V.\".V. \".V.\".V. \".V.\".V. \".V.\".V \".V\".V \".V\".V \".V\".V \".V.\".V. \".V.\" (.V \".V.\".V \".V.\".V. \".V\".V. \".V.\".V. \".V.\".V. \").\" (1).V \".V.\".V. \".V.\" (1). \".V.\".V. \".V.\".V. \".V.\".V. \"(.V.\".V. \".V.\".V. \".V.\". \").\" (1). \".V.\".V. \".V.\"). \"(1).\".V. \".V.\".V. \"(.V.\".V. \".V.\".V. \".V.\"). \"(.V.\".V. \".V.\".V. \").V.\" (.V. \").V.\".V. \"(.V.\").V. \".V.\" (.V. \").V.\".V. \"(.V.\".V. \").V.\" (.V. \").V.\" (.V.).V. \"(.V.\").V. \".V.\" (.V.).V. \"(.V.).V.\" (.V.).V.).V. \"(.V.).V."}, {"heading": "3 High Dimensional and Non-parametric Learning via Random Pro-", "text": "In this section we will look at three non-interactive LDP learning problems: Mean Estimation and Linear Regression in High-dimensions and Kernel Ridge Regression. By means of random1Note, one can also use the advanced composition mechanism [20] with a sophisticated analysis, but the main dependence on n and d will remain almost the same. Projection techniques are able to obtain a logarithmic dependence on d in high-dimensional settings and also obtain good guarantees for the kernel version. The first problem is considered in statistical settings, since we have to assume a sparse mean. The latter two problems are considered ERM problems, which can easily be transferred to population risk through uniform convergence."}, {"heading": "3.1 High-dimensional Mean Estimation", "text": "In this section, we propose a non-interactive LDP estimation problem that causes us to make a high probability estimate. (By adopting \"2-finite data points\" and \"1-finite population,\" we can obtain error rates with logarithmic dependence on d.) Our results stand in sharp contrast to the lower limit for \"2-finite general mean estimate\" under standard DP [2], and the lower limit for \"1-finite mean estimate under local DP [7]. It is easy to see that our method extends to the medium estimate problem for arbitrary low complexity constraints in high dimensions. We specify our results in the\" 1 \"setting to keep the arguments clear. Our problem assumes a statistical estimation setting like this:\" 2-finite mean estimate \"D supports an unknown distribution."}, {"heading": "3.2 Sparse Linear Regression", "text": "In this section, we will consider empirical losses of the sparse linear regression, i.e., we have the ability to transfer the data into space. (D) We want to obtain a vectorwpriv-C within a non-interactive LDP model, so that the empirical excess of risk L (w; D) \u2212 L (w; D) becomes a polynomial dependence of log d and 1n. As in the case of the high-dimensional mean estimate, we will introduce large noise."}, {"heading": "3.3 Infinite Dimension: Kernel Ridge Regression", "text": "This leads to new difficulties for LDP learning, since we are not able to process the noise in the Hilbert space. (In this section, we will take the kernel regression as an example to show how we can deal with similar problems.) Note: Our technique also fits similar problems. (F) Note: The shift of the Fourier functions (RFF), which refers to the shift of the Hilbert space as H, and the corresponding function board as H, and the corresponding function card as H, as H. Let the Hilbert space be H according to the random Fourier function board, and its function card as H, where dp is the RFF projection dimension."}, {"heading": "4 Learning Smooth Generalized Linear Model", "text": "In this section, we consider learning as a smoothly generalized linear model in non-interactive LDP setting. (...) It is essentially difficult to obtain an unbiased estimator of the course. (...) We will first solve this problem with reasonable assumptions. (...) Our definition could be shown in conjunction with exponential family GLM, which is commonly used in machine learning. (...) We also illustrate our algorithms and guarantees with logistic regression.Definition 2. (...) We say that a univariate function h (...) is absolutely smooth if for each r > 0, f (...) fulfills the following characteristics: There are functions (...)."}, {"heading": "4.1 Example: Learning Logistic Regression", "text": "Either from the point of view of the exponential, generalized linear model of the family or the specific loss function, it is not difficult to see that logistic loss belongs to SGLL. E.g. in logistic regression, (w; x, y) = log (1 + e \u2212 yw Tx) = \u2212 (y 2w Tx) + (1 2w Tx + ln (1 + e \u2212 w Tx))). Therefore, we leave h1 (x) = x 2, h2 (x) = x 2 + ln (1 + e \u2212 x). As we know, logistic loss for some parameters is \u03b2 convex and \u03b2-smooth and the absolutely smooth property of linear function is obvious, therefore, if we prove f (x) = ln (1 + e \u2212 x) absolutely smooth, then logistic loss fulfills the definition of SGLL."}, {"heading": "5 Conclusions", "text": "In this paper, we consider efficient algorithms for common learning and estimation problems within the framework of a non-interactive LDP model. In particular, we propose efficient algorithms for sparse linear regression and meaning estimation problems and demonstrate the polynomial dependence on excess risk or square error over log d and 1n, which is precisely to be expected in the high-dimensional case. We also extend our methods to non-parametric cases and show good limits on kernel ridge regression. For more difficult generalized problems to optimize linear losses, we use private Chebyshev approximations to estimate gradients of objective loss, combined with existing methods of gradient derivation, to achieve final outcomes. Sample complexity of our mechanism is quasi-polynomic in relation to 1\u03b1, where \u03b1 is the desired population surplus risk. An interesting open problem is whether our theoretical guarantees are optimal."}, {"heading": "A Appendix", "text": "In this case it is so that we find the following numbers with the probability 2 / 3: The probability 1 / 3: The probability 1 / 2: The probability 1 / 2: The probability 2: The probability 1: 2: The probability 1: 2: The probability 1: 2: The probability 1: 2: The probability 2: 3: The probability 2: 3: The probability 2: 3: The probability 2: 3: The probability 1: 2: 3: The probability 1: 2: 3: The probability 1: 2: 3: The truth 2: The truth 2: 3: The truth 2: The truth 2: The truth 1: 3: The truth 2: The truth 2: The truth 1: The truth 2: The truth 2: The truth 2: The truth 1: The truth 1: The truth 1: The truth 1: The truth 1: The truth 1: The truth 1: The truth 1: The truth 1: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The truth 2: The"}], "references": [{"title": "Local, private, efficient protocols for succinct histograms", "author": ["Raef Bassily", "Adam Smith"], "venue": "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Private empirical risk minimization: Efficient algorithms and tight error bounds", "author": ["Raef Bassily", "Adam Smith", "Abhradeep Thakurta"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Convex optimization: Algorithms and complexity", "author": ["S\u00e9bastien Bubeck"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Privacy-preserving logistic regression", "author": ["K. Chaudhuri", "C. Monteleoni"], "venue": "In Conference on Neural Information Processing Systems, British Columbia,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Differentially private empirical risk minimization", "author": ["K. Chaudhuri", "C. Monteleoni", "A.D. Sarwate"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Dimensionality reduction with subgaussian matrices: a unified theory", "author": ["Sjoerd Dirksen"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Minimax optimal procedures for locally private estimation", "author": ["John Duchi", "Martin Wainwright", "Michael Jordan"], "venue": "arXiv preprint arXiv:1604.02390,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Local privacy and minimax bounds: Sharp rates for probability estimation", "author": ["John Duchi", "Martin J Wainwright", "Michael I Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Local privacy and statistical minimax rates", "author": ["John C Duchi", "Michael I Jordan", "Martin J Wainwright"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Stochastic intermediate gradient method for convex problems with stochastic inexact oracle", "author": ["Pavel Dvurechensky", "Alexander Gasnikov"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["C. Dwork", "F. McSherry", "K. Nissim", "A. Smith"], "venue": "In Theory of cryptography,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "The algorithmic foundations of differential privacy", "author": ["Cynthia Dwork", "Aaron Roth"], "venue": "Foundations and Trends R  \u00a9 in Theoretical Computer Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Statistical query algorithms for mean vector estimation and stochastic convex optimization", "author": ["Vitaly Feldman", "Crist\u00f3bal Guzm\u00e1n", "Santosh Vempala"], "venue": "In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "The optimal mechanism in differential privacy", "author": ["Quan Geng", "Pramod Viswanath"], "venue": "In Information Theory (ISIT),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "A simple and practical algorithm for differentially private data release", "author": ["M. Hardt", "K. Ligett", "F. Mcsherry"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "A multiplicative weights mechanism for privacy-preserving data analysis", "author": ["M. Hardt", "G.N. Rothblum"], "venue": "In IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Loss minimization and parameter estimation with heavy tails", "author": ["Daniel Hsu", "Sivan Sabato"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Discrete distribution estimation under local privacy", "author": ["Peter Kairouz", "Keith Bonawitz", "Daniel Ramage"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Extremal mechanisms for local differential privacy", "author": ["Peter Kairouz", "Sewoong Oh", "Pramod Viswanath"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "The composition theorem for differential privacy", "author": ["Peter Kairouz", "Sewoong Oh", "Pramod Viswanath"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Secure multi-party differential privacy", "author": ["Peter Kairouz", "Sewoong Oh", "Pramod Viswanath"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "What can we learn privately", "author": ["S.P. Kasiviswanathan", "H.K. Lee", "K. Nissim", "S. Raskhodnikova", "A. Smith"], "venue": "In IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Efficient private empirical risk minimization for high-dimensional learning", "author": ["Shiva Prasad Kasiviswanathan", "Hongxia Jin"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "What can we learn privately", "author": ["Shiva Prasad Kasiviswanathan", "Homin K Lee", "Kobbi Nissim", "Sofya Raskhodnikova", "Adam Smith"], "venue": "SIAM Journal on Computing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Efficient noise-tolerant learning from statistical queries", "author": ["Michael Kearns"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1998}, {"title": "Private convex empirical risk minimization and high-dimensional regression", "author": ["Daniel Kifer", "Adam Smith", "Abhradeep Thakurta"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Differentially private m-estimators", "author": ["J. Lei"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Some coefficient estimates for polynomials on the unit interval", "author": ["MA Qazi", "QI Rahman"], "venue": "Serdica Mathematical Journal,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Random features for large-scale kernel machines", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "In NIPS,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Learning in a large function space: Privacy-preserving mechanisms for svm learning", "author": ["B. Rubinstein", "P.L. Bartlett", "L. Huang", "N. Taft"], "venue": "Journal of Privacy and Confidentiality,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Privacy-preserving statistical estimation with optimal convergence rates", "author": ["A. Smith"], "venue": "In ACM Symposium on Theory of Computing,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Differentially private model selection via stability arguments and the robustness of the lasso", "author": ["Adam Smith", "Abhradeep Thakurta"], "venue": "J Mach Learn Res Proc Track,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Nearly optimal private lasso", "author": ["Kunal Talwar", "Abhradeep Thakurta", "Li Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Faster algorithms for privately releasing marginals", "author": ["J. Thaler", "J. Ullman", "S. Vadhan"], "venue": "In International Colloquium on Automata, Languages, and Programming,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Is gauss quadrature better than clenshaw\u2013curtis", "author": ["Lloyd N Trefethen"], "venue": "SIAM review,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "An introduction to matrix concentration inequalities", "author": ["Joel A Tropp"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "A note on sums of independent random matrices after ahlswede-winter", "author": ["Roman Vershynin"], "venue": "Lecture notes,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Estimation in high dimensions: a geometric perspective", "author": ["Roman Vershynin"], "venue": "In Sampling theory, a renaissance,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Privacy for free: Posterior sampling and stochastic gradient monte carlo", "author": ["Y. Wang", "S.E. Fienberg", "A.J. Smola"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Differentially private data releasing for smooth queries", "author": ["Z. Wang", "C. Jin", "K. Fan", "J. Zhang", "J. Huang", "Y. Zhong", "L. Wang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Randomized response: A survey technique for eliminating evasive answer bias", "author": ["Stanley L Warner"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1965}, {"title": "Efficient private erm for smooth objectives", "author": ["Jiaqi Zhang", "Kai Zheng", "Wenlong Mou", "Liwei Wang"], "venue": "arXiv preprint arXiv:1703.09947,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2017}, {"title": "Functional mechanism: regression analysis under differential privacy", "author": ["Jun Zhang", "Zhenjie Zhang", "Xiaokui Xiao", "Yin Yang", "Marianne Winslett"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": ",[11], provide a solid foundation and rigorous standard for private data analysis.", "startOffset": 1, "endOffset": 5}, {"referenceID": 15, "context": "Since then, there has been extensive literature studying the fundamental trade-offs between differential privacy and accuracy for query answering [16, 15, 34, 40], machine learning [4, 5, 30, 39], and statistical inference [27, 31].", "startOffset": 146, "endOffset": 162}, {"referenceID": 14, "context": "Since then, there has been extensive literature studying the fundamental trade-offs between differential privacy and accuracy for query answering [16, 15, 34, 40], machine learning [4, 5, 30, 39], and statistical inference [27, 31].", "startOffset": 146, "endOffset": 162}, {"referenceID": 33, "context": "Since then, there has been extensive literature studying the fundamental trade-offs between differential privacy and accuracy for query answering [16, 15, 34, 40], machine learning [4, 5, 30, 39], and statistical inference [27, 31].", "startOffset": 146, "endOffset": 162}, {"referenceID": 39, "context": "Since then, there has been extensive literature studying the fundamental trade-offs between differential privacy and accuracy for query answering [16, 15, 34, 40], machine learning [4, 5, 30, 39], and statistical inference [27, 31].", "startOffset": 146, "endOffset": 162}, {"referenceID": 3, "context": "Since then, there has been extensive literature studying the fundamental trade-offs between differential privacy and accuracy for query answering [16, 15, 34, 40], machine learning [4, 5, 30, 39], and statistical inference [27, 31].", "startOffset": 181, "endOffset": 195}, {"referenceID": 4, "context": "Since then, there has been extensive literature studying the fundamental trade-offs between differential privacy and accuracy for query answering [16, 15, 34, 40], machine learning [4, 5, 30, 39], and statistical inference [27, 31].", "startOffset": 181, "endOffset": 195}, {"referenceID": 29, "context": "Since then, there has been extensive literature studying the fundamental trade-offs between differential privacy and accuracy for query answering [16, 15, 34, 40], machine learning [4, 5, 30, 39], and statistical inference [27, 31].", "startOffset": 181, "endOffset": 195}, {"referenceID": 38, "context": "Since then, there has been extensive literature studying the fundamental trade-offs between differential privacy and accuracy for query answering [16, 15, 34, 40], machine learning [4, 5, 30, 39], and statistical inference [27, 31].", "startOffset": 181, "endOffset": 195}, {"referenceID": 26, "context": "Since then, there has been extensive literature studying the fundamental trade-offs between differential privacy and accuracy for query answering [16, 15, 34, 40], machine learning [4, 5, 30, 39], and statistical inference [27, 31].", "startOffset": 223, "endOffset": 231}, {"referenceID": 30, "context": "Since then, there has been extensive literature studying the fundamental trade-offs between differential privacy and accuracy for query answering [16, 15, 34, 40], machine learning [4, 5, 30, 39], and statistical inference [27, 31].", "startOffset": 223, "endOffset": 231}, {"referenceID": 11, "context": "For more details on DP results, please refer to the excellent monograph written by Dwork and Roth [12].", "startOffset": 98, "endOffset": 102}, {"referenceID": 40, "context": "Borrowing ideas from classical wisdom on collecting sensitive survey data [41], Local Differential Privacy (LDP) [22, 9] was proposed as a stronger notion of privacy to resolve this problem.", "startOffset": 74, "endOffset": 78}, {"referenceID": 21, "context": "Borrowing ideas from classical wisdom on collecting sensitive survey data [41], Local Differential Privacy (LDP) [22, 9] was proposed as a stronger notion of privacy to resolve this problem.", "startOffset": 113, "endOffset": 120}, {"referenceID": 8, "context": "Borrowing ideas from classical wisdom on collecting sensitive survey data [41], Local Differential Privacy (LDP) [22, 9] was proposed as a stronger notion of privacy to resolve this problem.", "startOffset": 113, "endOffset": 120}, {"referenceID": 7, "context": "Therefore, this line of research has attracted lots of attention [8, 9, 19, 1, 18].", "startOffset": 65, "endOffset": 82}, {"referenceID": 8, "context": "Therefore, this line of research has attracted lots of attention [8, 9, 19, 1, 18].", "startOffset": 65, "endOffset": 82}, {"referenceID": 18, "context": "Therefore, this line of research has attracted lots of attention [8, 9, 19, 1, 18].", "startOffset": 65, "endOffset": 82}, {"referenceID": 0, "context": "Therefore, this line of research has attracted lots of attention [8, 9, 19, 1, 18].", "startOffset": 65, "endOffset": 82}, {"referenceID": 17, "context": "Therefore, this line of research has attracted lots of attention [8, 9, 19, 1, 18].", "startOffset": 65, "endOffset": 82}, {"referenceID": 24, "context": "In the interactive world, LDP is promised with connection to Statistical Query (SQ) model [25], from its very beginning [24].", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "In the interactive world, LDP is promised with connection to Statistical Query (SQ) model [25], from its very beginning [24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 12, "context": "SQ algorithms for a wide range of convex ERM problems were proposed by [13], implying good risk bounds for LDP.", "startOffset": 71, "endOffset": 75}, {"referenceID": 8, "context": "[9] established matching upper and lower bounds for convex risk minimization problems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Existing works primarily focus on basic estimation problems such as means and discrete densities [8, 7, 1], or some function calculations [21].", "startOffset": 97, "endOffset": 106}, {"referenceID": 6, "context": "Existing works primarily focus on basic estimation problems such as means and discrete densities [8, 7, 1], or some function calculations [21].", "startOffset": 97, "endOffset": 106}, {"referenceID": 0, "context": "Existing works primarily focus on basic estimation problems such as means and discrete densities [8, 7, 1], or some function calculations [21].", "startOffset": 97, "endOffset": 106}, {"referenceID": 20, "context": "Existing works primarily focus on basic estimation problems such as means and discrete densities [8, 7, 1], or some function calculations [21].", "startOffset": 138, "endOffset": 142}, {"referenceID": 32, "context": "In classical differential privacy literature, this has been be addressed using different techniques, guarantee error bounds logarithmically dependent on dimension [33, 32].", "startOffset": 163, "endOffset": 171}, {"referenceID": 31, "context": "In classical differential privacy literature, this has been be addressed using different techniques, guarantee error bounds logarithmically dependent on dimension [33, 32].", "startOffset": 163, "endOffset": 171}, {"referenceID": 6, "context": "However, lower bounds have been shown in local privacy model even for high-dimensional 1-sparse mean estimation, ruling out any good guarantees [7].", "startOffset": 144, "endOffset": 147}, {"referenceID": 13, "context": "One can also consider optimal mechanisms in terms of privacy parameters like [14], which is of independent interests.", "startOffset": 77, "endOffset": 81}, {"referenceID": 6, "context": "This is in sharp contrast with information-theoretic lower bounds for 1-sparse mean estimation for l\u221e bounded data [7].", "startOffset": 115, "endOffset": 118}, {"referenceID": 9, "context": "SIGM algorithm in [10] is exploited to find the minimizer with inexact gradients.", "startOffset": 18, "endOffset": 22}, {"referenceID": 40, "context": "Other Related Work: Local privacy dates back to [41], who uses random responses to protect privacy in surveys.", "startOffset": 48, "endOffset": 52}, {"referenceID": 7, "context": "In recent LDP literature, both [8] and [18] studied density estimation methods and their theoretical behaviors in LDP model.", "startOffset": 31, "endOffset": 34}, {"referenceID": 17, "context": "In recent LDP literature, both [8] and [18] studied density estimation methods and their theoretical behaviors in LDP model.", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "Rather than statistical setting in above two work, [1] considered how to produce frequent items and corresponding frequencies of a dataset in local model.", "startOffset": 51, "endOffset": 54}, {"referenceID": 18, "context": "Besides, [19] investigated optimality of LDP mechanisms based on information theoretical measures for statistical discrimination.", "startOffset": 9, "endOffset": 13}, {"referenceID": 33, "context": "[34] employed polynomials for marginal queries.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[40] leveraged trigonometric polynomials to answer smooth queries.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[43] also used polynomial approximations and get basic convergence results in standard DP model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Besides, the random projection and recovery has also been used in DP learning [23] and local DP histogram estimation [1].", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "Besides, the random projection and recovery has also been used in DP learning [23] and local DP histogram estimation [1].", "startOffset": 117, "endOffset": 120}, {"referenceID": 25, "context": "[26] and [32] considered the convergence of private LASSO estimator under RSC and incoherence assumptions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[26] and [32] considered the convergence of private LASSO estimator under RSC and incoherence assumptions.", "startOffset": 9, "endOffset": 13}, {"referenceID": 32, "context": "[33] considered constrained ERM of sparse linear regression, and obtained \u00d5(log d/n2/3) rate using private Frank-Wolfe.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "By stronger assumption of l2 bounded data, [23] gave a general framework for high dimensional empirical risk minimization (ERM) problem.", "startOffset": 43, "endOffset": 47}, {"referenceID": 3, "context": "Objective and output perturbation are used to get low excess risks [4, 5].", "startOffset": 67, "endOffset": 73}, {"referenceID": 4, "context": "Objective and output perturbation are used to get low excess risks [4, 5].", "startOffset": 67, "endOffset": 73}, {"referenceID": 1, "context": "Both [2] and [42] considered concrete private algorithms to solve ERM.", "startOffset": 5, "endOffset": 8}, {"referenceID": 41, "context": "Both [2] and [42] considered concrete private algorithms to solve ERM.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "1 Local Differential Privacy Here we adopt the LDP definition given in [1].", "startOffset": 71, "endOffset": 74}, {"referenceID": 11, "context": "A mechanism Q : V \u2192 Z is said to be (\u01eb, \u03b4)-local differential private or (\u01eb, \u03b4)-LDP, if for any v,v\u2032 \u2208 V, and any (measurable) subset S \u2282 Z, there is Pr[Q(v) \u2208 S] 6 e Pr[Q(v\u2032) \u2208 S] + \u03b4 Just the same with basic results in DP [12], there are corresponding basic results for LDP: Lemma 1 (Gaussian Mechanism).", "startOffset": 224, "endOffset": 228}, {"referenceID": 19, "context": "Using random Note one can also use the advanced composition mechanism [20] with a refined analysis, but the main dependence over n and d will remain nearly the same.", "startOffset": 70, "endOffset": 74}, {"referenceID": 1, "context": "Our results are in sharp contrast with the lower bound for l2-bounded general mean estimation under standard DP [2], as well as the lower bound for l\u221e-bounded 1-sparse mean estimation under local DP [7].", "startOffset": 112, "endOffset": 115}, {"referenceID": 6, "context": "Our results are in sharp contrast with the lower bound for l2-bounded general mean estimation under standard DP [2], as well as the lower bound for l\u221e-bounded 1-sparse mean estimation under local DP [7].", "startOffset": 199, "endOffset": 202}, {"referenceID": 37, "context": "This locally private estimation procedure can be viewed as a variant of noisy compressed sensing, where l2 recovery rate is fundamentally controlled by the Gaussian Mean Width of constraint set [38].", "startOffset": 194, "endOffset": 198}, {"referenceID": 16, "context": "To tackle this problem, we employ Median-of-Mean estimator to get exponential tails [17].", "startOffset": 84, "endOffset": 88}, {"referenceID": 16, "context": "The tail properties are guaranteed in the following lemma: Lemma 4 (Proposition 9 in [17]).", "startOffset": 85, "endOffset": 89}, {"referenceID": 37, "context": "The primary tool we are using are General M\u2217 bound in [38].", "startOffset": 54, "endOffset": 58}, {"referenceID": 37, "context": "2 in [38], High Probability Version).", "startOffset": 5, "endOffset": 9}, {"referenceID": 32, "context": "Note [33] assume data is in L\u221e ball, while both [23] and ours assume data is in L2 ball.", "startOffset": 5, "endOffset": 9}, {"referenceID": 22, "context": "Note [33] assume data is in L\u221e ball, while both [23] and ours assume data is in L2 ball.", "startOffset": 48, "endOffset": 52}, {"referenceID": 6, "context": "However, in LDP model, [7] show it was impossible to obtain polynomial dependences over log d for l0 mean estimation problem if data is in L\u221e ball.", "startOffset": 23, "endOffset": 26}, {"referenceID": 28, "context": "In this subsection, we take kernel ridge regression as an example to show how to use Random Fourier Features (RFF) [29] to deal with such cases caused by shift-invariant kernels (i.", "startOffset": 115, "endOffset": 119}, {"referenceID": 29, "context": "Borrow the key result in [30] (restated in lemma 7 below), which used RFF to design private mechanims for SVM in DP model, it becomes easy to prove guarantees for kernel ridge regression in our setting (see Corollary 2).", "startOffset": 25, "endOffset": 29}, {"referenceID": 29, "context": "Lemma 7 ([30]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 2, "context": "This is a common assumption in stochastic optimization literature, such as [3].", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "[10] For an objective function f(w), a (\u03b3, \u03b2, \u03c3) stochastic oracle returns a turple", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Based on above (\u03b3, \u03b2, \u03c3) stochastic oracle, and the algorithm proposed in SIGM paper [10] (omitted here, due to the limitation of space), our complete learning algorithm is given in Algorithm 6.", "startOffset": 85, "endOffset": 89}, {"referenceID": 9, "context": "Before proving our sample complexity, we state the basic convergence result of SIGM algorithm: Lemma 10 ([10]).", "startOffset": 105, "endOffset": 109}, {"referenceID": 0, "context": "References [1] Raef Bassily and Adam Smith.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Raef Bassily, Adam Smith, and Abhradeep Thakurta.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] S\u00e9bastien Bubeck et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] K.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] K.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Sjoerd Dirksen.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] John Duchi, Martin Wainwright, and Michael Jordan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] John Duchi, Martin J Wainwright, and Michael I Jordan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] John C Duchi, Michael I Jordan, and Martin J Wainwright.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Pavel Dvurechensky and Alexander Gasnikov.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Cynthia Dwork and Aaron Roth.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Vitaly Feldman, Crist\u00f3bal Guzm\u00e1n, and Santosh Vempala.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Quan Geng and Pramod Viswanath.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Daniel Hsu and Sivan Sabato.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Peter Kairouz, Keith Bonawitz, and Daniel Ramage.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Peter Kairouz, Sewoong Oh, and Pramod Viswanath.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Peter Kairouz, Sewoong Oh, and Pramod Viswanath.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Peter Kairouz, Sewoong Oh, and Pramod Viswanath.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Shiva Prasad Kasiviswanathan and Hongxia Jin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Michael Kearns.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Daniel Kifer, Adam Smith, and Abhradeep Thakurta.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] MA Qazi and QI Rahman.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] Ali Rahimi, Benjamin Recht, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] Adam Smith and Abhradeep Thakurta.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] Kunal Talwar, Abhradeep Thakurta, and Li Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35] Lloyd N Trefethen.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36] Joel A Tropp et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] Roman Vershynin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[38] Roman Vershynin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[39] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[40] Z.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[41] Stanley L Warner.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[42] Jiaqi Zhang, Kai Zheng, Wenlong Mou, and Liwei Wang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[43] Jun Zhang, Zhenjie Zhang, Xiaokui Xiao, Yin Yang, and Marianne Winslett.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "1 in [36]), we have 1 n \u2225 \u2225X\u0304Tr \u2225", "startOffset": 5, "endOffset": 9}, {"referenceID": 36, "context": "Lemma 11 ([37]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "additive error [6], we know with high probability, there is | \u3008w,x\u3009 \u2212 \u3008 \u03a6Tw,\u03a6Tx \u3009 | 6 O ( \u221a", "startOffset": 15, "endOffset": 18}, {"referenceID": 29, "context": "On the other hand, nearly borrow the proof of Lemma 17 in [30] and property of RRF , we have L\u0124(g \u2217)\u2212 LH(f) 6 \u00d5 (\u221a d dp )", "startOffset": 58, "endOffset": 62}, {"referenceID": 34, "context": "\u2225 T 6 \u03bc1(k; r)\u03bc2(k; r) k, according to the results in [35], we have", "startOffset": 54, "endOffset": 58}, {"referenceID": 27, "context": "According to the formula of Tm(x) given in [28] and well-known Stirling\u2019s approximation, after some translation, we have |bmk| 6 max \u03b8\u2208(0, 1 2 ) O (\u221a m \u00b7 [ (1\u2212 \u03b8)1\u2212\u03b8 \u03b8\u03b8(1\u2212 2\u03b8)1\u22122\u03b8 ]m)", "startOffset": 43, "endOffset": 47}, {"referenceID": 34, "context": "Besides, from the absolutely smooth property of hi(x)(i \u2208 {1, 2}) and the convergence results in [35], we have am 6 O ( 1 m2 ) , thus ck = \u2211p m=k ambmk 6 O (2 p).", "startOffset": 97, "endOffset": 101}], "year": 2017, "abstractText": "Non-interactive Local Differential Privacy (LDP) requires data analysts to collect data from users through noisy channel at once. In this paper, we extend the frontiers of Non-interactive LDP learning and estimation from several aspects. For learning with smooth generalized linear losses, we propose an approximate stochastic gradient oracle estimated from non-interactive LDP channel using Chebyshev expansion, which is combined with inexact gradient methods to obtain an efficient algorithm with quasi-polynomial sample complexity bound. For the highdimensional world, we discover that under l2-norm assumption on data points, high-dimensional sparse linear regression and mean estimation can be achieved with logarithmic dependence on dimension, using random projection and approximate recovery. We also extend our methods to Kernel Ridge Regression. Our work is the first one that makes learning and estimation possible for a broad range of learning tasks under non-interactive LDP model.", "creator": "LaTeX with hyperref package"}}}