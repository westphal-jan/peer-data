{"id": "1706.03369", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2017", "title": "On the Sampling Problem for Kernel Quadrature", "abstract": "The standard Kernel Quadrature method for numerical integration with random point sets (also called Bayesian Monte Carlo) is known to converge in root mean square error at a rate determined by the ratio $s/d$, where $s$ and $d$ encode the smoothness and dimension of the integrand. However, an empirical investigation reveals that the rate constant $C$ is highly sensitive to the distribution of the random points. In contrast to standard Monte Carlo integration, for which optimal importance sampling is well-understood, the sampling distribution that minimises $C$ for Kernel Quadrature does not admit a closed form. This paper argues that the practical choice of sampling distribution is an important open problem. One solution is considered; a novel automatic approach based on adaptive tempering and sequential Monte Carlo. Empirical results demonstrate a dramatic reduction in integration error of up to 4 orders of magnitude can be achieved with the proposed method.", "histories": [["v1", "Sun, 11 Jun 2017 16:08:17 GMT  (4491kb,D)", "http://arxiv.org/abs/1706.03369v1", "To appear at Thirty-fourth International Conference on Machine Learning (ICML 2017)"]], "COMMENTS": "To appear at Thirty-fourth International Conference on Machine Learning (ICML 2017)", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.NA stat.CO", "authors": ["fran\u00e7ois-xavier briol", "chris j oates", "jon cockayne", "wilson ye chen", "mark a girolami"], "accepted": true, "id": "1706.03369"}, "pdf": {"name": "1706.03369.pdf", "metadata": {"source": "CRF", "title": "On the Sampling Problem for Kernel Quadrature", "authors": ["Fran\u00e7ois-Xavier Briol", "Chris J. Oates", "Jon Cockayne", "Wilson Ye Chen", "Mark Girolami"], "emails": ["<f-x.briol@warwick.ac.uk>."], "sections": [{"heading": "1. INTRODUCTION", "text": "The question that arises is whether it is a \"real\" or a \"false\" approach. (...) The question that arises is whether it is a \"false\" approach. (...) The question that arises is whether it is a \"false\" approach. (...) The question that arises is whether it is a \"false\" approach. (...) The question that arises is whether it is a \"false\" approach. (...) The question that arises is. (...) The question that arises is. (...) The question that arises is. (...) The question that arises is whether it is a \"false\" approach. (...) (...) The question that arises. (...) The question that arises. (...) The question that arises. (...) The question that arises. (...) The question that arises. (...) The question that arises. (...) The answer that arises. (.... (...). (. (...) The answer. (.... (.) The answer. (. (.) The answer. (. (.) The answer. (. (...) The answer. (. (.) The answer. (... (.) The answer.......... (.) The answer. (.. (. (.) The answer. (.... (.) The answer.. (..........) The answer. The answer. (... The answer. (................. The answer... The answer... The answer. The answer.) The answer.... The answer. The answer... The answer.. The answer.. The answer.... The answer. (... The answer. The answer.... The answer......... The question. The answer. The answer.. The question.. The question that. The question that arises.. The question that arises.. (... The question.. The question that arises. (... The question that arises.. (... The question that. The question that arises. (... The question that.... The question that.. The question. The question..."}, {"heading": "2. BACKGROUND", "text": "This section provides an overview of KQ (paragraphs 2.1 and 2.2), empirical (paragraph 2.3) and theoretical (paragraph 2.4) results for the selection of sample distribution and discusses kernel learning for KQ (paragraph 2.5)."}, {"heading": "2.1. Overview of Kernel Quadrature", "text": "It is assumed that K \u2212 1 almost certainly exists; for non-degenerated kernels, this corresponds to the above definition of KQ = K \u2212 1z. It is assumed that K \u2212 1 almost certainly exists; for non-degenerated kernels, which means that there are no atoms. From the above definition of KQ = K \u2212 1z. It is assumed that K \u2212 1 k (x, xj).Defining zj = X k (\u00b7, xj) dm leads to the estimate in Eqn. 2 with weights w = K \u2212 1z. pairs for which the zj closed form in Table 1 of Briol et al (2015b).Computation of these weights incurs a computational cost of no more than O (n3) and can be justified."}, {"heading": "2.2. Over-Reliance on the Kernel", "text": "In Osborne et al. (2012a); Huszar and Duvenaud (2012); Gunter et al. (2014); Briol et al. (2015a), the selection of xn was approached as a greedy optimization problem, with the maximum integration error en (w; {xj} nj = 1) given the location of the previous {xj} n \u2212 1j = 1. This approach has shown considerable success in use. However, the error criterion en is highly dependent on the choice of the kernel k and the sequential optimization approach is prone to kernel misspecifications. Especially if the intrinsic length scale of k is \"too small,\" then the {xj} nj = 1 is all clusters around the mode, leading to a poor integral estimation (see Figure 5 in the appendix). Related work on the partial selection, such as the lever point number (Bach, 2013), cannot also be specified in a robust way."}, {"heading": "2.3. Sensitivity to the Sampling Distribution", "text": "s look at the toy problem with the state space X = R, the target distribution E = N (0, 1), a single test function f (x) = 1 + sin (2\u03c0x), and the kernel k (x, x) = exp (\u2212 (x \u2212 x) 2. Let us look at a range of sample distributions of the form E = N (0, 2) for E (0, 3) for this problem. Figure 1 shows that the M-M-M-M-M-M-M-M-M-M-M = 1 (1), M-M-M-M (f) -D (f) -D (f)) 2 is an empirical estimate for the RMSE, where the M-M-K-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T."}, {"heading": "2.4. Established Results", "text": "Here we recall the most important convergence results to date on KQ and discuss how they relate to sampling distribution decisions (2016). To reduce the level of detail, we make several assumptions at the outset: domain X will be either Rd itself or a compact subset of Rd that meets an \"inner cone condition,\" meaning that there is an angle in which there is an \"inner cone condition,\" and a radius r > 0 of one that exists for each x-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X."}, {"heading": "2.5. Goals", "text": "Our first goal was to formalize the sampling problem for KQ; this is now complete. Our second goal was to develop a novel automatic approach to selecting KQ, called SMC-KQ; the full details are in paragraph 3.Note also that the integrand f will generally belong to an infinity of Hilbert spaces, while for KQ a single kernel k must be selected. This choice will affect the performance of the KQ estimator; in fig. 2, for example, the problem of paragraph 2.3 was reconsidered based on a class of k (x, x \u2032) = exp (\u2212 (x \u2212 x \u2032) 2 / '2) parameterized by' (0, \u221e)."}, {"heading": "3. METHODS", "text": "In this section, the methods SMC-KQ and SMC-KQ-KL are presented. Our aim is to explain in detail the main components (SMC, temp, crit) of Alg. 1. To this end, sections 3.1 and 3.2 set our SMC sampler to moderate distributions, while section 3.3 provides heuristics for choosing the temperature schedule. Paragraph 3.4 extends the approach to kernel learning, and paragraph 3.5 proposes a new criterion to determine when a desired error tolerance is reached."}, {"heading": "3.1. Thermodynamic Ansatz", "text": "To begin with, we consider f, k and n as fixed points. The following problem is central to our proposed SMC-KQ method: An optimal distribution (in the sense of Eqn 3) can be achieved by distributing the formal distribution (1). (4) It is assumed that all aspects exist (i.e., it can be normalized). The motivation for this approach parameter is from sec. (2), where there is a reference distribution that needs to be specified and which should be uninformative in practice. (3) It is assumed that all exist (i.e. it can be normalized). (4) The motivation for this approach parameter comes from sec. (2), where there is a distribution category. (0) and (2) can be cast in this form. (1) and (2)."}, {"heading": "3.4. Kernel Learning", "text": "In Sec. 2.5 we showed the benefits of kernel learning for KQ."}, {"heading": "4. RESULTS", "text": "Here we compared SMC-KQ (and SMC-KQ-KL) with the corresponding default approaches KQ (and KQ-KL), which are based on B-KQ. In paragraph 4.1 an assessment is made in which the true value of integrals is constructively known, whereas in paragraph 4.2 the methods for solving a parameter estimation problem with differential equations were used."}, {"heading": "4.1. Simulation Study", "text": "To continue our illustration from section 2, we investigated the performance of SMC-KQ and SMC-KQ-KL with respect to the integration of f (x) = 1 + sin (2\u03c0x) against the distribution, where the reference distribution was assumed to be \u043f0 = N (0, 82). All experiments used SMC with N = 300 particles, random walk metropolis transitions (Alg. 3), the re-sample threshold \u03c1 = 0.95 and a maximum grid size \u2206 = 0.1. The dependence of the subsequent results on the choice of \u03b50 was examined in Figure 10 in the appendix. Figure 3 (above) reports on the results of SMC-KQ against KQ, for fixed length scale '= 1. Corresponding results for SMC-KQ-KL against KQ-KL are shown in the chart below. It was observed that SMC-KQ (or Appendix) reported on the results of SMC-KQ."}, {"heading": "4.2. Inference for Differential Equations", "text": "Consider the model given by dx / dt = f (t | \u03b8) with solution x (t | \u03b8) with solution Q = 2.5 for each major simulation system depending on unknown parameters. Suppose we can obtain observations using the following noise model (probability): y (ti) = x (ti | \u03b8) + ei at times 0 = t1 <. < tn, where we assume that ei \u00b2 N (0, \u03c32) for known \u03c3 > 0. Our goal is to estimate x (T | \u03b8) for a fixed (potentially large) T > 0. To do this, we use a Bayean approach and specify a previous p (\u03b8), then we obtain samples from the rear kernel \u03c0 (\u03b8): = p (p | y) with the help of MCMC. The rear predictive mean is then defined as: x (T | \u00b7)) = x (T | junction)) = x (T | junction) with x, and this can be etched with the help of an irical mean."}, {"heading": "5. DISCUSSION", "text": "In this paper, we formalized the optimal sample problem for KQ. A general, practical solution based on novel SMC methods was proposed. Initial empirical results show performance gains compared to KQ's standard approach to the method. A more difficult example based on differential equation parameter estimates was used to illustrate the potential of SMC-KQ for Bayean calculation in combination with Stein's methodology. Our methods were general, but require user-specific choice of an initial distribution.0 For compact state spaces X, however, we recommend taking \"0\" to be uniform. For non-compact spaces, however, there is a degree of flexibility and standard solutions, such as broad Gaussian distributions, necessarily require user input. However, choosing \"0\" is easier than choosing \"oneself, since\" 0 \"is not required to be optimal in order to use our example of KQ's relative improved performance (KQ)."}, {"heading": "ACKNOWLEDGEMENTS", "text": "FXB was supported by the EPSRC Scholarship [EP / L016710 / 1], CJO & MG by the Lloyds Register Foundation Programme on Data-Centric Engineering, WYC by the ARC Centre of Excellence in Mathematical and Statistical Frontiers, MG by the EPSRC Scholarships [EP / J016934 / 3, EP / K034154 / 1, EP / P020720 / 1], an EPSRC Established Career Fellowship, the EU Scholarship [EU / 259348], a Royal Society Wolfson Research Merit Award, FXB, CJO, JC & MG by the SAMSI Working Group on Probabilistic Numerics."}, {"heading": "A. Appendix", "text": "This appendix supplements the paper \"On the sampling problem for kernel quadrature.\" Section A.1 discusses the potential lack of robustness of greedy optimization methods that motivated the development of SMC-KQ. Section A.2 and A.3 discuss some of the theoretical aspects of KQ, while Section A.4 and A.5 present additional numerical experiments and implementation details. Finally, Section A.6 provides detailed pseudo-codes for all algorithms used in this paper."}, {"heading": "A.1. Lack of Robustness of Optimisation Methods", "text": "In order to demonstrate the non-robustness of erroneously specified cores, which is a feature of optimization-based methods, we considered integration using the initial state x1 for functions that can be approximated by the kernel k (x, x \u2032) = exp (\u2212 (x \u2212 x \u2032) 2 / '2). An initial state x1 was specified at source and then for n = 2, 3,.. the state xn was chosen to be the error criterion en (w; {xj} nj = 1) given the location of {xj} nj = 1. This is known as \"sequential Bayesian squaring\" (SBQ; Huszar and Duvenaud, 2012; Gunter et al., 2014; Briol et al., 2015a). The kernel length scale was set to' = 0.01 and we consider (as a thought experiment, since it does not factor into our selection of points) a more regular integral size, such as is in principle (SQ = 5)."}, {"heading": "A.2. Additional Definitions", "text": "Space L2 is defined in such a way that the living integral space finally existes.For a multiindex \u03b1 = (\u03b11,...) define | = \u03b11 + \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u03b1d. The (standard) Sobolev space of order s \u00b2 N is with the designation Hs (\u0432) = {f: X \u2192 R s.t. (\u0445 x1) \u03b11... (\u0445 xd) \u03b1df * L2 (\u0445) Vacuum Control System (ss \u00b2 N) Vacuum Control System (ss \u00b2) Vacuum Control System (ss) Vacuum Control System (ss) Vacuum Control System (ss) Vacuum Control System (ss) Vacuum Control System (ss) Vacuum Control System (ss) Vacuum Control System (s) Vacuum Control System (s) Vacuum Control System (s) Vacuum Control System (s) Vacuum Control System (s) Vacuum Control System (s) Vacuum Control System (s)"}, {"heading": "A.3. Theoretical Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.3.1. PROOF OF THEOREM 1", "text": "Proof. Thm. 11.13 in Wendland (2004) shows that there are constants 0 < ck < p > p, h0 > 0, so that | f (x) \u2212 f (x) \u2212 f (7) for all x-X, provided hn < h0, where Eqn = sup x-X min i = 1,..., n-x \u2212 xi \u00b2 2. Under the hypotheses we can assume that the deterministic states x1,.., xm hm < h0. Then Eqn. 7 applies to all n > m, where the xm + 1,.., xn are independent of the actual states. Consequently, the deterministic states x1,..., xm, hm x-x-x-x (x) < f (x) -f (x) | cchsn."}, {"heading": "A.3.2. PROOF OF THEOREM 2", "text": "The proof: The Cauchy-Schwarz result for the kernel means that the embedding (Smola et al., 2007) is at least (f) \u2212 (8) equal. \u2212 The result for the kernel means that the embedding (Smola et al., 2007) is at least (f) \u2212 (f) equal. \u2212 The result for the kernel means that the embedding (f) is equal. \u2212 The result for the kernel means that the embedding (f) is equal. \u2212 The result for the kernel means that the embedding (f) is equal. \u2212 The result for the kernel means that the embedding (f) is equal. \u2212 The result for the kernel means that the embedding (f) is equal. \u2212 The result for the kernel means that the embedding (f) is equal. \u2212 The result for the kernel means that the embedding (f) is equal."}, {"heading": "A.3.3. \u03a0B FOR THE EXAMPLE OF FIGURE 1", "text": "In this section, we consider the margin we have for the example in Figure 1. < < p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p = p"}, {"heading": "A.3.4. ADDITIONAL THEORETICAL MATERIAL", "text": "As mentioned in the main text, the worst-case error en ({xj} nj = 1) can be calculated in a closed form: en ({xj) nj = 1) 2 = 2 = 2 = 2 = 2 > KwHere we have defined a result that does not refer to KQ itself, but takes into account the importance of sampling methods for integrating functions in a Hilbert space. Plaskota et al. (2009) provides the following elementary evidence for their result: Theorem 3. The assumptions of Sec. 2.4 are taken into account to hold."}, {"heading": "A.5. Experimental Results", "text": "A.5.1. IMPLEMENTATION OF SIMULATION STUDYDenote by N (x | \u00b5, \u03a3) the p.d.f. of the multivariate Gaussian distribution with mean \u00b5 and covariance \u03a3. Furthermore, we denote the diagonal covariance matrix with the diagonal element \u03c32. Then the elementary manipulation of Gaussian density leads to: k (x, y): = exp (\u2212, j = 1 (xj \u2212 yj) 2 l2) = (\u0432l) d\u03c6 (x | y, l / \u221a 2); lk (x, y): = 2 \u0445 d = 1 (xj \u2212 yj) 2l3 k (x, y)]: = (p (xj \u2212 yj) 2 l2): = (p (p, p) 2 l2 l2): = (p (x, p) 2 lk (\u00b7, x)]: (p, p) 2 (x)"}, {"heading": "A.5.2. DEPENDENCE ON PARAMETERS FOR THE SIMULATION STUDY", "text": "For the current illustration with f (x) = 1 + sin (x), \u0442 = N (0, 1), \u0445 \u2032 = N (0, \u03c32) and k (x, x \") = exp (\u2212 (x \u2212 x\") 2 / '2), we have investigated how the RMSE of KQ depends on the choice of both \u03c3 and \". Here, we go beyond the results shown in Fig. 2, which considered fixed n, to now consider the simultaneous choice of both \u03c3\" for different n. Notice that in these numerical experiments the core matrix inverse K \u2212 1 has been replaced by the regulated inverse (K + \u03bbI) \u2212 1, which introduces a small \"nugget\" term for stabilization. The results shown in Fig. 8 show two principles that guided the methodological development in this work: \u2022 Length scales of \"which are\" too small \"to learn from n samples do not allow good approximations > 0 and then lead to the same problem from a high MSE-Q perspective."}, {"heading": "A.5.3. ADDITIONAL RESULTS FOR THE SIMULATION STUDY", "text": "To understand whether the termination criterion of Sec. 3.5 was appropriate (and, by extension, the validity of the convexity approach in Sec. 3.2), in Fig. 9 we presented histograms for both estimated and actual optimal (inverse) temperature parameters t *. The results supported the use of the criterion, in the form for testing described above. In Fig. 10 we report the dependence of performance on the choice of first distribution. There was relatively little impact on the RMSE achieved by the method for this wide range of first distribution, which supports the purported robustness of the method. We are also testing the method for more complex integrals in Fig. 11: f x = 1 + sin (4\u03c0x) and f (x). These are more challenging for KQ compared to the illustrations in the main text, as they are more difficult to interpolate due to their higher periodicity."}, {"heading": "A.6. Algorithms and Implementation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.6.1. SMC SAMPLER", "text": "In Alg. 2, the standard SMC scheme is shown. Collection occurs when the effective sample size, i.e. the actual q function, sinks below a fraction of the total number of N particles. In this thesis, we used \u03c1 = 0.95, which is a common default.Algorithm 2 Sequential q function Monte Carlo Iteration SMC ({(wj, xj) Nj = 1, ti \u2212 1, \u03c1) Input {(wj, xj)} Nj = 1 (Particle approximation to the next inverse temperature) Input ti \u2212 1 (previous inverse temperature) Input ti \u2212 1 (previous inverse temperature) Input \u03c1 (re-sample threshold) w \u00b2 w \u00b2 wj \u00b2 / x."}, {"heading": "A.6.2. CHOICE OF TEMPERATURE SCHEDULE", "text": "Following Zhou et al. (2016), we used an adaptive temperature scheme construction. This was based on the conditional effective sample size of the SMC particle set, which was estimated as follows: Algorithm 4 Conditional Effective Sample Sizing Function CESS ({(wj, xj)} Nj = 1, t) input {(wj, xj) Nj = 1 (particle approx. amoun. i \u2212 1) input t (candidate next invert temperature) zj (est'd. cond. ESS) [\u03c0 (xj) / \u03c00 (xj)] ti \u2212 1 (\u0445j) E \u2190 N (\u2211 Nj = 1 wjzj) 2 / (\u2211 N = 1 wjz 2 j) return E (est'd. cond. ESS) The specific construction for the temperature scheme is detailed in Alg. 5 below and uses a sequential least quares programming algorithm: Algorithm {w5'S (temperature adjustment function)."}, {"heading": "A.6.3. TERMINATION CRITERION", "text": "For SMC-KQ, we have calculated an upper limit for the Worst Case Error in the Hilbert Unit Ball. This was calculated as follows: Algorithm 6: Termination criterion function crit (n), k, {xj} Nj = 1) Input k (target disn.) Input k (kernel) Input k (kernel) Input {xj) Nj = 1 (Collection of States) R2 (n) 0 e0 (x, x) Input K (dx) (in 'l error) for m = 1,.., M do x (x) j Unif ({xj) Nj = 1) (n) Zj (x) X (x), x), x (j), j), j (x), j), j (x), j (c), j (x), j (c), j (c), c (c), c (c), c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c"}, {"heading": "A.6.4. KERNEL LEARNING", "text": "A generic approach to selecting kernel parameters is the maximum limit probability method."}], "references": [{"title": "Sharp analysis of low-rank kernel matrix approximations", "author": ["F. Bach"], "venue": "In Proc. I. Conf. Learn. Theory,", "citeRegEx": "Bach.,? \\Q2013\\E", "shortCiteRegEx": "Bach.", "year": 2013}, {"title": "On the equivalence between kernel quadrature rules and random features", "author": ["F. Bach"], "venue": null, "citeRegEx": "Bach.,? \\Q2015\\E", "shortCiteRegEx": "Bach.", "year": 2015}, {"title": "On approximate computation of integrals", "author": ["N.S. Bakhvalov"], "venue": "Vestnik MGU, Ser. Math. Mech. Astron. Phys. Chem.,", "citeRegEx": "Bakhvalov.,? \\Q1959\\E", "shortCiteRegEx": "Bakhvalov.", "year": 1959}, {"title": "Reproducing kernel Hilbert spaces in probability and statistics", "author": ["A. Berlinet", "C. Thomas-Agnan"], "venue": "Springer Science & Business Media,", "citeRegEx": "Berlinet and Thomas.Agnan.,? \\Q2011\\E", "shortCiteRegEx": "Berlinet and Thomas.Agnan.", "year": 2011}, {"title": "Frank-Wolfe Bayesian quadrature: Probabilistic integration with theoretical guarantees", "author": ["F-X. Briol", "C.J. Oates", "M. Girolami", "M.A. Osborne"], "venue": "In Adv. Neur. Inf. Proc. Sys.,", "citeRegEx": "Briol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Briol et al\\.", "year": 2015}, {"title": "Probabilistic integration: A role for statisticians in numerical analysis", "author": ["F-X. Briol", "C.J. Oates", "M. Girolami", "M.A. Osborne", "D. Sejdinovic"], "venue": null, "citeRegEx": "Briol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Briol et al\\.", "year": 2015}, {"title": "A sequential particle filter method for static models", "author": ["N. Chopin"], "venue": "Biometrika, 89(3):539\u2013552,", "citeRegEx": "Chopin.,? \\Q2002\\E", "shortCiteRegEx": "Chopin.", "year": 2002}, {"title": "Optimal weighted leastsquares methods", "author": ["A. Cohen", "G. Migliorati"], "venue": null, "citeRegEx": "Cohen and Migliorati.,? \\Q2016\\E", "shortCiteRegEx": "Cohen and Migliorati.", "year": 2016}, {"title": "Sequential monte carlo samplers", "author": ["P. Del Moral", "A. Doucet", "A. Jasra"], "venue": "J. R. Stat. Soc. Ser. B. Stat. Methodol.,", "citeRegEx": "Moral et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Moral et al\\.", "year": 2006}, {"title": "Bayesian Numerical Analysis, volume IV of Statistical Decision Theory and Related Topics, pages 163\u2013175", "author": ["P. Diaconis"], "venue": null, "citeRegEx": "Diaconis.,? \\Q1988\\E", "shortCiteRegEx": "Diaconis.", "year": 1988}, {"title": "Digital nets and sequences: Discrepancy Theory and Quasi\u2013Monte Carlo Integration", "author": ["J. Dick", "F. Pillichshammer"], "venue": null, "citeRegEx": "Dick and Pillichshammer.,? \\Q2010\\E", "shortCiteRegEx": "Dick and Pillichshammer.", "year": 2010}, {"title": "High-dimensional integration: The quasi-Monte Carlo way", "author": ["J. Dick", "F.Y. Kuo", "I.H. Sloan"], "venue": "Acta Numerica,", "citeRegEx": "Dick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dick et al\\.", "year": 2013}, {"title": "Fast approximation of matrix coherence and statistical leverage", "author": ["P. Drineas", "M. Magdon-Ismail", "M.W. Mahoney", "D.P. Woodruff"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Drineas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2012}, {"title": "Parameter multi-domain \u2018hp", "author": ["J.L. Eftang", "B. Stamm"], "venue": "empirical interpolation. I. J. Numer. Methods in Eng.,", "citeRegEx": "Eftang and Stamm.,? \\Q2012\\E", "shortCiteRegEx": "Eftang and Stamm.", "year": 2012}, {"title": "Sequential quasi Monte Carlo", "author": ["M. Gerber", "N. Chopin"], "venue": "J. R. Statist. Soc. B,", "citeRegEx": "Gerber and Chopin.,? \\Q2015\\E", "shortCiteRegEx": "Gerber and Chopin.", "year": 2015}, {"title": "Sampling for inference in probabilistic models with fast Bayesian quadrature", "author": ["T. Gunter", "R. Garnett", "M. Osborne", "P. Hennig", "S. Roberts"], "venue": "In Adv. Neur. Inf. Proc. Sys.,", "citeRegEx": "Gunter et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gunter et al\\.", "year": 2014}, {"title": "Coherence motivated sampling and convergence analysis of least squares polynomial Chaos regression", "author": ["J. Hampton", "A. Doostan"], "venue": "Comput. Methods Appl. Mech. Engrg.,", "citeRegEx": "Hampton and Doostan.,? \\Q2015\\E", "shortCiteRegEx": "Hampton and Doostan.", "year": 2015}, {"title": "Optimal importance sampling for the approximation of integrals", "author": ["A. Hinrichs"], "venue": "J. Complexity,", "citeRegEx": "Hinrichs.,? \\Q2010\\E", "shortCiteRegEx": "Hinrichs.", "year": 2010}, {"title": "Optimally-weighted herding is Bayesian quadrature", "author": ["F. Huszar", "D. Duvenaud"], "venue": "In Uncert. Artif. Intell.,", "citeRegEx": "Huszar and Duvenaud.,? \\Q2012\\E", "shortCiteRegEx": "Huszar and Duvenaud.", "year": 2012}, {"title": "Convergence guarantees for kernel-based quadrature rules in misspecified settings", "author": ["M. Kanagawa", "B. Sriperumbudur", "K. Fukumizu"], "venue": "In Adv. Neur. Inf. Proc. Sys.,", "citeRegEx": "Kanagawa et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kanagawa et al\\.", "year": 2016}, {"title": "Active uncertainty calibration in bayesian ode solvers", "author": ["H. Kersting", "P. Hennig"], "venue": "In Proc. Conf. Uncert. Artif. Intell.,", "citeRegEx": "Kersting and Hennig.,? \\Q2016\\E", "shortCiteRegEx": "Kersting and Hennig.", "year": 2016}, {"title": "Eigenvalues of compact operators with applications to integral operators", "author": ["H. K\u00f6nig"], "venue": "Linear Algebra Appl.,", "citeRegEx": "K\u00f6nig.,? \\Q1986\\E", "shortCiteRegEx": "K\u00f6nig.", "year": 1986}, {"title": "The empirical interpolation method", "author": ["S. Kristoffersen"], "venue": "Master\u2019s thesis, Department of Mathematical Sciences, Norwegian University of Science and Technology,", "citeRegEx": "Kristoffersen.,? \\Q2013\\E", "shortCiteRegEx": "Kristoffersen.", "year": 2013}, {"title": "Black-Box Importance Sampling", "author": ["Q. Liu", "J.D. Lee"], "venue": "I. Conf. Artif. Intell. Stat.,", "citeRegEx": "Liu and Lee.,? \\Q2017\\E", "shortCiteRegEx": "Liu and Lee.", "year": 2017}, {"title": "Active Area Search via Bayesian Quadrature", "author": ["Y. Ma", "R. Garnett", "J. Schneider"], "venue": "I. Conf Artif. Intell. Stat.,", "citeRegEx": "Ma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2014}, {"title": "Randomized algorithms for matrices and data", "author": ["M.W. Mahoney"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "Mahoney.,? \\Q2011\\E", "shortCiteRegEx": "Mahoney.", "year": 2011}, {"title": "Convergence Rates for a Class of Estimators", "author": ["C.J. Oates", "J. Cockayne", "F-X. Briol", "M. Girolami"], "venue": "Based on Stein\u2019s Identity", "citeRegEx": "Oates et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oates et al\\.", "year": 2016}, {"title": "Control Functionals for Monte Carlo Integration", "author": ["C.J. Oates", "M. Girolami", "N. Chopin"], "venue": "J. R. Stat. Soc. Ser. B. Stat. Methodol.,", "citeRegEx": "Oates et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Oates et al\\.", "year": 2017}, {"title": "Bayes-Hermite quadrature", "author": ["A. O\u2019Hagan"], "venue": "J. Statist. Plann. Inference,", "citeRegEx": "O.Hagan.,? \\Q1991\\E", "shortCiteRegEx": "O.Hagan.", "year": 1991}, {"title": "Active learning of model evidence using Bayesian quadrature", "author": ["M.A. Osborne", "D. Duvenaud", "R. Garnett", "C.E. Rasmussen", "S. Roberts", "Z. Ghahramani"], "venue": "In Adv. Neur. Inf. Proc. Sys.,", "citeRegEx": "Osborne et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Osborne et al\\.", "year": 2012}, {"title": "Bayesian quadrature for ratios", "author": ["M.A. Osborne", "R. Garnett", "S. Roberts", "C. Hart", "S. Aigrain", "N. Gibson"], "venue": "In Proc. I. Conf. Artif. Intell. Stat.,", "citeRegEx": "Osborne et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Osborne et al\\.", "year": 2012}, {"title": "OASIS: Adaptive Column Sampling for Kernel Matrix Approximation", "author": ["R. Patel", "T.A. Goldstein", "E.L. Dyer", "A.Mirhoseini", "R.G. Baraniuk"], "venue": null, "citeRegEx": "Patel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Patel et al\\.", "year": 2015}, {"title": "Alternating Optimisation and Quadrature for Robust Reinforcement Learning", "author": ["S. Paul", "K. Ciosek", "M.A. Osborne", "S. Whiteson"], "venue": null, "citeRegEx": "Paul et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Paul et al\\.", "year": 2016}, {"title": "New averaging technique for approximating weighted integrals", "author": ["L. Plaskota", "G.W. Wasilkowski", "Y. Zhao"], "venue": "J. Complexity,", "citeRegEx": "Plaskota et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Plaskota et al\\.", "year": 2009}, {"title": "Bayesian Quadrature in Nonlinear Filtering", "author": ["J. Pr\u00fcher", "M. \u0160imandl"], "venue": "In 12th I. Conf. Inform. Control Autom. Robot.,", "citeRegEx": "Pr\u00fcher and \u0160imandl.,? \\Q2015\\E", "shortCiteRegEx": "Pr\u00fcher and \u0160imandl.", "year": 2015}, {"title": "Bayesian Monte Carlo", "author": ["C.E. Rasmussen", "Z. Ghahramani"], "venue": "In Adv. Neur. Inf. Proc. Sys.,", "citeRegEx": "Rasmussen and Ghahramani.,? \\Q2002\\E", "shortCiteRegEx": "Rasmussen and Ghahramani.", "year": 2002}, {"title": "Monte Carlo statistical methods", "author": ["C. Robert", "G. Casella"], "venue": "Springer Science & Business Media,", "citeRegEx": "Robert and Casella.,? \\Q2013\\E", "shortCiteRegEx": "Robert and Casella.", "year": 2013}, {"title": "An introduction to ordinary differential equations", "author": ["J.C. Robinson"], "venue": null, "citeRegEx": "Robinson.,? \\Q2004\\E", "shortCiteRegEx": "Robinson.", "year": 2004}, {"title": "On the relation between Gaussian process quadratures and sigma-point methods", "author": ["S. S\u00e4rkk\u00e4", "J. Hartikainen", "L. Svensson", "F. Sandblom"], "venue": null, "citeRegEx": "S\u00e4rkk\u00e4 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "S\u00e4rkk\u00e4 et al\\.", "year": 2015}, {"title": "Data spectroscopy: Eigenspaces of convolution operators and clustering", "author": ["T. Shi", "M. Belkin", "B. Yu"], "venue": "Ann. Statist.,", "citeRegEx": "Shi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2009}, {"title": "Trace Ideals and Their Applications", "author": ["B. Simon"], "venue": null, "citeRegEx": "Simon.,? \\Q1979\\E", "shortCiteRegEx": "Simon.", "year": 1979}, {"title": "A Hilbert space embedding for distributions", "author": ["A. Smola", "A. Gretton", "L. Song", "B. Sch\u00f6lkopf"], "venue": "In Algorithmic Learn. Theor.,", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Numerical cubature on scattered data by radial basis functions", "author": ["A. Sommariva", "M. Vianello"], "venue": null, "citeRegEx": "Sommariva and Vianello.,? \\Q2006\\E", "shortCiteRegEx": "Sommariva and Vianello.", "year": 2006}, {"title": "Special Functions: An Introduction to the Classical Functions of Mathematical Physics", "author": ["N.M. Temme"], "venue": null, "citeRegEx": "Temme.,? \\Q1996\\E", "shortCiteRegEx": "Temme.", "year": 1996}, {"title": "Scattered Data Approximation", "author": ["H. Wendland"], "venue": null, "citeRegEx": "Wendland.,? \\Q2004\\E", "shortCiteRegEx": "Wendland.", "year": 2004}, {"title": "Towards Automatic Model Comparison: An Adaptive Sequential Monte Carlo Approach", "author": ["Y. Zhou", "A.M. Johansen", "J.A.D. Aston"], "venue": "J. Comput. Graph. Statist.,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 28, "context": "The KQ estimator is identical to the posterior mean in Bayesian Monte Carlo (O\u2019Hagan, 1991; Rasmussen and Ghahramani, 2002), and its relationship with classical numerical quadrature rules has been studied (Diaconis, 1988; S\u00e4rkk\u00e4 et al.", "startOffset": 76, "endOffset": 123}, {"referenceID": 35, "context": "The KQ estimator is identical to the posterior mean in Bayesian Monte Carlo (O\u2019Hagan, 1991; Rasmussen and Ghahramani, 2002), and its relationship with classical numerical quadrature rules has been studied (Diaconis, 1988; S\u00e4rkk\u00e4 et al.", "startOffset": 76, "endOffset": 123}, {"referenceID": 9, "context": "The KQ estimator is identical to the posterior mean in Bayesian Monte Carlo (O\u2019Hagan, 1991; Rasmussen and Ghahramani, 2002), and its relationship with classical numerical quadrature rules has been studied (Diaconis, 1988; S\u00e4rkk\u00e4 et al., 2015).", "startOffset": 205, "endOffset": 242}, {"referenceID": 38, "context": "The KQ estimator is identical to the posterior mean in Bayesian Monte Carlo (O\u2019Hagan, 1991; Rasmussen and Ghahramani, 2002), and its relationship with classical numerical quadrature rules has been studied (Diaconis, 1988; S\u00e4rkk\u00e4 et al., 2015).", "startOffset": 205, "endOffset": 242}, {"referenceID": 4, "context": "Under regularity conditions, Briol et al. (2015b) established the following RMSE bound for KQ: \u221a E[\u03a0\u0302(f)\u2212\u03a0(f)]2 \u2264 C(f ; \u03a0 \u2032) ns/d\u2212 , (s > d/2)", "startOffset": 29, "endOffset": 50}, {"referenceID": 2, "context": "An information-theoretic lower bound on the RMSE is O(n\u2212s/d\u22121/2) (Bakhvalov, 1959).", "startOffset": 65, "endOffset": 82}, {"referenceID": 27, "context": "In particular, the approach (i) provides practical guidance for selection of \u03a0\u2032 for KQ, (ii) offers robustness to kernel misspecification, and (iii) extends recent work on computing posterior expectations with kernels obtained using Stein\u2019s method (Oates et al., 2017).", "startOffset": 248, "endOffset": 268}, {"referenceID": 4, "context": "Pairs (\u03a0, k) for which the zj have closed form are reported in Table 1 of Briol et al. (2015b). Computation of these weights incurs a computational cost of at most O(n) and can be justified when either (i) evaluation of f forms the computational bottleneck, or (ii) the gain in estimator precision (as a function in n) dominates this cost (i.", "startOffset": 74, "endOffset": 95}, {"referenceID": 5, "context": "Notable contributions on KQ include Diaconis (1988); O\u2019Hagan (1991); Rasmussen and Ghahramani (2002) who introduced the method and Huszar and Duvenaud (2012); Osborne et al.", "startOffset": 36, "endOffset": 52}, {"referenceID": 5, "context": "Notable contributions on KQ include Diaconis (1988); O\u2019Hagan (1991); Rasmussen and Ghahramani (2002) who introduced the method and Huszar and Duvenaud (2012); Osborne et al.", "startOffset": 36, "endOffset": 68}, {"referenceID": 5, "context": "Notable contributions on KQ include Diaconis (1988); O\u2019Hagan (1991); Rasmussen and Ghahramani (2002) who introduced the method and Huszar and Duvenaud (2012); Osborne et al.", "startOffset": 36, "endOffset": 101}, {"referenceID": 5, "context": "Notable contributions on KQ include Diaconis (1988); O\u2019Hagan (1991); Rasmussen and Ghahramani (2002) who introduced the method and Huszar and Duvenaud (2012); Osborne et al.", "startOffset": 36, "endOffset": 158}, {"referenceID": 5, "context": "Notable contributions on KQ include Diaconis (1988); O\u2019Hagan (1991); Rasmussen and Ghahramani (2002) who introduced the method and Huszar and Duvenaud (2012); Osborne et al. (2012a;b); Gunter et al. (2014); Bach (2015); Briol et al.", "startOffset": 36, "endOffset": 206}, {"referenceID": 0, "context": "(2014); Bach (2015); Briol et al.", "startOffset": 8, "endOffset": 20}, {"referenceID": 0, "context": "(2014); Bach (2015); Briol et al. (2015a;b); S\u00e4rkk\u00e4 et al. (2015); Kanagawa et al.", "startOffset": 8, "endOffset": 66}, {"referenceID": 0, "context": "(2014); Bach (2015); Briol et al. (2015a;b); S\u00e4rkk\u00e4 et al. (2015); Kanagawa et al. (2016); Liu and Lee (2017) who provided consequent methodological extensions.", "startOffset": 8, "endOffset": 90}, {"referenceID": 0, "context": "(2014); Bach (2015); Briol et al. (2015a;b); S\u00e4rkk\u00e4 et al. (2015); Kanagawa et al. (2016); Liu and Lee (2017) who provided consequent methodological extensions.", "startOffset": 8, "endOffset": 110}, {"referenceID": 20, "context": "solvers (Kersting and Hennig, 2016), reinforcement learning (Paul et al.", "startOffset": 8, "endOffset": 35}, {"referenceID": 32, "context": "solvers (Kersting and Hennig, 2016), reinforcement learning (Paul et al., 2016), filtering (Pr\u00fcher and \u0160imandl, 2015) and design of experiments (Ma et al.", "startOffset": 60, "endOffset": 79}, {"referenceID": 34, "context": ", 2016), filtering (Pr\u00fcher and \u0160imandl, 2015) and design of experiments (Ma et al.", "startOffset": 19, "endOffset": 45}, {"referenceID": 24, "context": ", 2016), filtering (Pr\u00fcher and \u0160imandl, 2015) and design of experiments (Ma et al., 2014).", "startOffset": 72, "endOffset": 89}, {"referenceID": 3, "context": "LetH denote the Hilbert space characterised by the reproducing kernel k, and denote its norm as \u2016 \u00b7 \u2016H (Berlinet and Thomas-Agnan, 2011).", "startOffset": 103, "endOffset": 136}, {"referenceID": 10, "context": "These characterisations connect KQ to (a) non-parametric regression, (b) probabilistic integration and (c) quasi-Monte Carlo (QMC) methods (Dick and Pillichshammer, 2010).", "startOffset": 139, "endOffset": 170}, {"referenceID": 42, "context": "The scattered data approximation literature (Sommariva and Vianello, 2006) and the numerical analysis literature (where KQ is known as the \u2018empirical interpolation method\u2019; Eftang and Stamm, 2012; Kristoffersen, 2013) can also be connected to KQ.", "startOffset": 44, "endOffset": 74}, {"referenceID": 13, "context": "The scattered data approximation literature (Sommariva and Vianello, 2006) and the numerical analysis literature (where KQ is known as the \u2018empirical interpolation method\u2019; Eftang and Stamm, 2012; Kristoffersen, 2013) can also be connected to KQ.", "startOffset": 113, "endOffset": 217}, {"referenceID": 22, "context": "The scattered data approximation literature (Sommariva and Vianello, 2006) and the numerical analysis literature (where KQ is known as the \u2018empirical interpolation method\u2019; Eftang and Stamm, 2012; Kristoffersen, 2013) can also be connected to KQ.", "startOffset": 113, "endOffset": 217}, {"referenceID": 0, "context": "However, our search of all of these literatures did not yield guidance on the optimal selection of the sampling distribution \u03a0\u2032 (with the exception of Bach (2015) reported in Sec.", "startOffset": 151, "endOffset": 163}, {"referenceID": 0, "context": "Related work on sub-sample selection, such as leverage scores (Bach, 2013), can also be non-robust to mis-specified kernels.", "startOffset": 62, "endOffset": 74}, {"referenceID": 23, "context": "In Osborne et al. (2012a); Huszar and Duvenaud (2012); Gunter et al.", "startOffset": 3, "endOffset": 26}, {"referenceID": 13, "context": "(2012a); Huszar and Duvenaud (2012); Gunter et al.", "startOffset": 9, "endOffset": 36}, {"referenceID": 11, "context": "(2012a); Huszar and Duvenaud (2012); Gunter et al. (2014); Briol et al.", "startOffset": 37, "endOffset": 58}, {"referenceID": 2, "context": "(2014); Briol et al. (2015a), the selection ofxn was approached as a greedy optimisation problem, wherein the maximal integration error en(w; {xj}j=1) was minimised, given the location of the previous {xj} j=1 .", "startOffset": 8, "endOffset": 29}, {"referenceID": 25, "context": "The same intuition is used for column sampling and to construct leverage scores (Mahoney, 2011; Drineas et al., 2012).", "startOffset": 80, "endOffset": 117}, {"referenceID": 12, "context": "The same intuition is used for column sampling and to construct leverage scores (Mahoney, 2011; Drineas et al., 2012).", "startOffset": 80, "endOffset": 117}, {"referenceID": 40, "context": "Assume that \u222b X k(x,x)\u03a0(dx) < \u221e, so that \u03a3 is self-adjoint, positive semi-definite and trace-class (Simon, 1979).", "startOffset": 99, "endOffset": 112}, {"referenceID": 21, "context": "Then, from an extension of Mercer\u2019s theorem (K\u00f6nig, 1986) we have a decomposition k(x,x\u2032) = \u2211\u221e m=1 \u03bcmem(x)em(x \u2032), where \u03bcm and em(x) are the eigenvalues and eigenfunctions of \u03a3.", "startOffset": 44, "endOffset": 57}, {"referenceID": 21, "context": "Then, from an extension of Mercer\u2019s theorem (K\u00f6nig, 1986) we have a decomposition k(x,x\u2032) = \u2211\u221e m=1 \u03bcmem(x)em(x \u2032), where \u03bcm and em(x) are the eigenvalues and eigenfunctions of \u03a3. Further assume thatH is dense in L2(\u03a0). The first result is adapted and extended from Thm. 1 in Oates et al. (2016). Theorem 1.", "startOffset": 45, "endOffset": 295}, {"referenceID": 4, "context": "1 of Briol et al. (2015b) for samples from \u03a0 (see the Appendix) and was extended to MCMC samples in Oates et al.", "startOffset": 5, "endOffset": 26}, {"referenceID": 4, "context": "1 of Briol et al. (2015b) for samples from \u03a0 (see the Appendix) and was extended to MCMC samples in Oates et al. (2016). An extension to Figure 2.", "startOffset": 5, "endOffset": 120}, {"referenceID": 17, "context": "the case of a mis-specified kernel was considered in Kanagawa et al. (2016). However a limitation of this direction of research is that it does not address the question of how to select \u03a0\u2032.", "startOffset": 53, "endOffset": 76}, {"referenceID": 0, "context": "The second result that we present is a consequence of the recent work of Bach (2015), who considered a particular choice of \u03a0\u2032 = \u03a0B, depending on a fixed \u03bb > 0, via the density \u03c0B(x;\u03bb) \u221d \u2211\u221e m=1 \u03bcm \u03bcm+\u03bb em(x).", "startOffset": 73, "endOffset": 85}, {"referenceID": 0, "context": "The second result that we present is a consequence of the recent work of Bach (2015), who considered a particular choice of \u03a0\u2032 = \u03a0B, depending on a fixed \u03bb > 0, via the density \u03c0B(x;\u03bb) \u221d \u2211\u221e m=1 \u03bcm \u03bcm+\u03bb em(x). The following is adapted from Prop. 1 in Bach (2015): Theorem 2.", "startOffset": 73, "endOffset": 262}, {"referenceID": 0, "context": "Some remarks are in order: (i) Bach (2015, Prop. 3) showed that, for \u03a0B, integration error scales at an optimal rate in n up to logarithmic terms and, after n samples, is of size \u221a \u03bcn. (ii) The distribution \u03a0B is obtained from minimising an upper bound on the integration error, rather than the error itself. It is unclear to us how well \u03a0B approximates an optimal sampling distribution for KQ. (iii) In general \u03a0B is hard to compute. For the specific case X = [0, 1], H equal to Hs(\u03a0) and \u03a0 uniform, the distribution \u03a0B is also uniform (and hence independent of n; see Sec. 4.4 of Bach (2015)).", "startOffset": 31, "endOffset": 594}, {"referenceID": 0, "context": "Some remarks are in order: (i) Bach (2015, Prop. 3) showed that, for \u03a0B, integration error scales at an optimal rate in n up to logarithmic terms and, after n samples, is of size \u221a \u03bcn. (ii) The distribution \u03a0B is obtained from minimising an upper bound on the integration error, rather than the error itself. It is unclear to us how well \u03a0B approximates an optimal sampling distribution for KQ. (iii) In general \u03a0B is hard to compute. For the specific case X = [0, 1], H equal to Hs(\u03a0) and \u03a0 uniform, the distribution \u03a0B is also uniform (and hence independent of n; see Sec. 4.4 of Bach (2015)). However, even for the simple example of Sec. 2.3, \u03a0B does not appear to have a closed form (details in Appendix). An approximation scheme was proposed in Sec. 4.2 of Bach (2015) but the error of this scheme was not studied.", "startOffset": 31, "endOffset": 774}, {"referenceID": 15, "context": "considered in Hampton and Doostan (2015); Cohen and Migliorati (2016).", "startOffset": 14, "endOffset": 41}, {"referenceID": 7, "context": "considered in Hampton and Doostan (2015); Cohen and Migliorati (2016).", "startOffset": 42, "endOffset": 70}, {"referenceID": 6, "context": "To realise such an algorithm, this paper exploited SMC methods (Chopin, 2002; Del Moral et al., 2006).", "startOffset": 63, "endOffset": 101}, {"referenceID": 45, "context": "The specific schedule used in this work was determined based on the conditional effective sample size of the current particle population, as proposed in the recent work of Zhou et al. (2016). Full details are presented in Algs.", "startOffset": 172, "endOffset": 191}, {"referenceID": 31, "context": "More sophisticated alternatives that also involve the kernel k, such as leverage scores, were not considered, since in general these (i) introduce a vulnerability to mis-specified kernels and (ii) require manipulation of a N \u00d7 N kernel matrix (Patel et al., 2015).", "startOffset": 243, "endOffset": 263}, {"referenceID": 26, "context": "To implement KQ under an unknown normalisation constant for \u03a0, we followed Oates et al. (2017) and made use of a Gaussian kernel that was adapted with Stein\u2019s method (see the Appendix for details).", "startOffset": 75, "endOffset": 95}, {"referenceID": 26, "context": "To implement KQ under an unknown normalisation constant for \u03a0, we followed Oates et al. (2017) and made use of a Gaussian kernel that was adapted with Stein\u2019s method (see the Appendix for details). The reference distribution \u03a00 was an wide uniform prior on the hypercube [0, 10]. Brute force computation was used to obtain a benchmark value for the integral. For the SMC algorithm, an independent lognormal transition kernel was used at each iteration with parameters automatically tuned to the current set of particles. Results in Fig. 4 demonstrate that SMC-KQ outperforms KQ for these integration problems. These results improve upon those reported in Oates et al. (2016) for a similar integration problem based on parameter estimation for differential equations.", "startOffset": 75, "endOffset": 675}, {"referenceID": 11, "context": "Two extensions of this research are identified: First, the curse of dimension that is intrinsic to standard Sobolev spaces can be alleviated by demanding \u2018dominating mixed smoothness\u2019; our methods are compatible with these (essentially tensor product) kernels (Dick et al., 2013).", "startOffset": 260, "endOffset": 279}, {"referenceID": 14, "context": "Second, the use of sequential QMC (Gerber and Chopin, 2015) can be considered, motivated by further orders of magnitude reduction in numerical error observed for deterministic point sets (see Fig.", "startOffset": 34, "endOffset": 59}, {"referenceID": 18, "context": "This is known as \u2018sequential Bayesian quadrature\u2019 (SBQ; Huszar and Duvenaud, 2012; Gunter et al., 2014; Briol et al., 2015a).", "startOffset": 50, "endOffset": 124}, {"referenceID": 15, "context": "This is known as \u2018sequential Bayesian quadrature\u2019 (SBQ; Huszar and Duvenaud, 2012; Gunter et al., 2014; Briol et al., 2015a).", "startOffset": 50, "endOffset": 124}, {"referenceID": 44, "context": "13 in Wendland (2004) we have that there exist constants 0 < ck <\u221e, h0 > 0 such that |f\u0302(x)\u2212 f(x)| \u2264 ckhn\u2016f\u2016H (7)", "startOffset": 6, "endOffset": 22}, {"referenceID": 26, "context": "1 in Oates et al. (2016) establishes that, under the present hypotheses on X and \u03a0\u2032, there exists 0 < c\u03a0\u2032, <\u221e such that E[h n ] \u2264 c\u03a0\u2032, m\u22122s/d+ for all > 0, where c\u03a0\u2032, is independent of n.", "startOffset": 5, "endOffset": 25}, {"referenceID": 41, "context": "The Cauchy-Schwarz result for kernel mean embeddings (Smola et al., 2007) gives", "startOffset": 53, "endOffset": 73}, {"referenceID": 0, "context": "1 of Bach (2015) established that when x1, .", "startOffset": 5, "endOffset": 17}, {"referenceID": 39, "context": "1 in Shi et al. (2009)).", "startOffset": 5, "endOffset": 23}, {"referenceID": 43, "context": "8 in Temme (1996), p.", "startOffset": 5, "endOffset": 18}, {"referenceID": 0, "context": "The approximation method in Bach (2015) was also used to obtain the numerical approximation to \u03a0B shown in Fig.", "startOffset": 28, "endOffset": 40}, {"referenceID": 32, "context": "The following is due to Plaskota et al. (2009); Hinrichs (2010) and we provide an elementary proof of their result: Theorem 3.", "startOffset": 24, "endOffset": 47}, {"referenceID": 17, "context": "(2009); Hinrichs (2010) and we provide an elementary proof of their result: Theorem 3.", "startOffset": 8, "endOffset": 24}, {"referenceID": 36, "context": "4 in Robert and Casella (2013). For the second case, where F is the unit ball inH, we start by establishing a (tight) upper bound for the supremum of f over f \u2208 F : |f(x)| = \u2223\u2223\u3008f, k(\u00b7,x)\u3009H\u2223\u2223 \u2264 \u2016f\u2016H\u2016k(\u00b7,x)\u2016H = \u2016f\u2016H \u221a \u3008k(\u00b7,x), k(\u00b7,x)\u3009H = \u2016f\u2016H \u221a k(x,x)", "startOffset": 5, "endOffset": 31}, {"referenceID": 14, "context": "This suggests that a SQMC approach (Gerber and Chopin, 2015) could provide further improvement and this suggested for future work.", "startOffset": 35, "endOffset": 60}, {"referenceID": 26, "context": "Following Oates et al. (2017) we considered the Stein operator", "startOffset": 10, "endOffset": 30}, {"referenceID": 45, "context": "Following Zhou et al. (2016) we employed an adaptive temperature schedule construction.", "startOffset": 10, "endOffset": 29}], "year": 2017, "abstractText": "The standard Kernel Quadrature method for numerical integration with random point sets (also called Bayesian Monte Carlo) is known to converge in root mean square error at a rate determined by the ratio s/d, where s and d encode the smoothness and dimension of the integrand. However, an empirical investigation reveals that the rate constant C is highly sensitive to the distribution of the random points. In contrast to standard Monte Carlo integration, for which optimal importance sampling is wellunderstood, the sampling distribution that minimises C for Kernel Quadrature does not admit a closed form. This paper argues that the practical choice of sampling distribution is an important open problem. One solution is considered; a novel automatic approach based on adaptive tempering and sequential Monte Carlo. Empirical results demonstrate a dramatic reduction in integration error of up to 4 orders of magnitude can be achieved with the proposed method.", "creator": "LaTeX with hyperref package"}}}