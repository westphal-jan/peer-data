{"id": "1706.03469", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Data-Efficient Policy Evaluation Through Behavior Policy Search", "abstract": "We consider the task of evaluating a policy for a Markov decision process (MDP). The standard unbiased technique for evaluating a policy is to deploy the policy and observe its performance. We show that the data collected from deploying a different policy, commonly called the behavior policy, can be used to produce unbiased estimates with lower mean squared error than this standard technique. We derive an analytic expression for the optimal behavior policy --- the behavior policy that minimizes the mean squared error of the resulting estimates. Because this expression depends on terms that are unknown in practice, we propose a novel policy evaluation sub-problem, behavior policy search: searching for a behavior policy that reduces mean squared error. We present a behavior policy search algorithm and empirically demonstrate its effectiveness in lowering the mean squared error of policy performance estimates.", "histories": [["v1", "Mon, 12 Jun 2017 05:19:47 GMT  (2216kb,D)", "http://arxiv.org/abs/1706.03469v1", "Accepted to ICML 2017; Extended version; 15 pages"]], "COMMENTS": "Accepted to ICML 2017; Extended version; 15 pages", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["josiah p hanna", "philip s thomas", "peter stone", "scott niekum"], "accepted": true, "id": "1706.03469"}, "pdf": {"name": "1706.03469.pdf", "metadata": {"source": "META", "title": "Data-Efficient Policy Evaluation Through Behavior Policy Search", "authors": ["Josiah P. Hanna", "Philip S. Thomas", "Peter Stone", "Scott Niekum"], "emails": ["<jphanna@cs.utexas.edu>."], "sections": [{"heading": "1. Introduction", "text": "Many sequential decision-making issues, including diabetes treatment (Bast\u03c0ani P., 2014), digital marketing (Theocharous et al., 2015) and robot control (Lillicrap et al., 2015), are modelled as Markov decision-making processes (MDPs) and solved using reinforcement learning (RL) algorithms. An important problem in applying RL to real-world problems is policy evaluation. The goal of policy evaluation is to estimate the expected return (sum of rewards) produced by a policy. We refer to this policy as the valuation policy, \u03c0e. The standard approach to policy evaluation is to re-use the results and evaluate the resulting returns. While this approach is unbiased to Monte Carlo estimators, it can exhibit high variability. 1The University of Texas at Austin, Texas, Texas, USA 2The University of Massachusetts, Amherst, Massachusetts, USA Carnegon Mellie University, Pennsylvania, USA."}, {"heading": "2. Preliminaries", "text": "In this section, policy assessment issues, Monte Carlo and Advantage Sum policy assessment methods, and the importance of sampling for non-policy assessment.ar Xiv: 170 6.03 469v 1 [cs.A I] 1 2Ju n20 17"}, {"heading": "2.1. Background", "text": "We use the notational standard MDPNv1 (Thomas, 2015), and for the sake of simplicity we assume that Di, A and R are finite.1 Let us let H: = (S0, A0, R0, S1,..., SL, AL, RL) be a path and g (H): = \u2211 L t = 0 \u03b3tRt be the discounted return of the path H. Let us let B: = E [g (H) | H \u0445 \u03c0] be the expected discounted return if the stochastic policy of S0 is used from the original state distribution. In this thesis we look at parameterized strategies, B: where the distribution of measures is determined by the vector progress. We assume that the transitions and reward functions are unknown and L is finite. We get a valuation policy, E: for which we would like to evaluate the policy."}, {"heading": "2.2. Monte-Carlo Estimates", "text": "The most commonly used method of policy assessment is the Monte-Carlo (MC) estimator. \u03c1 (\u03c0e) for iteration i is the average return: MC (Tue): = 1i + 1 i-J = 0 L-T = 0 \u03b3tRt = 1 i + 1 i-J = 0 g (Hj). This estimator is impartial and highly consistent in the face of mild assumptions. 2 However, this method can exhibit large variations.1The methods and theoretical results discussed in this paper are applicable to both finite and infinite S, A and R, as well as to partially observable Markov decisions. 2A highly consistent estimator of \u03c1 (\u03c0e) means that:"}, {"heading": "2.3. Advantage Sum Estimates", "text": "Like the Monte Carlo estimator, the ASE estimator selects \u03b8i = \u03b8e for all i, but introduces a control variant to reduce the variance without introducing distortion; this control variant requires the provision of an approximate model of the MDP. Let's specify the reward function of this model as r (s, a). Let's specify q [st, at) = E [\u2211 L t \"= t\" r \"(st,\" at \") and v [st) = E [q\" e \"(st, at), i.e. the action value function and state value function of \u03c0e in this approximate model. Then, the estimator of the benefit sum is given by: AS (Di): = 1i + 1 i\" j \"= 0 L\" p \"= 0\" t \"(Rt \u2212 q\" E \")."}, {"heading": "2.4. Importance Sampling", "text": "Importance Sample is a method for reweighting returns from a behavioral policy, \u03b8, so that they are unbiased returns from valuation policy. In RL, the reweighted IS yield of a trajectory, H, is sampled from a behavioral policy: IS (H, \u03b8): = g (H) L-t = 0 \u03c0e (St | At) \u03c0\u03b8 (St | At).The IS non-policy estimate is then a Monte Carlo estimate of E [IS (H, \u03b8) | H-\u0441\u0441\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u0442\u043e\u0441\u0442\u043e\u0441\u043e: IS (Di): = 1i + 1 i \u0445 j = 0 IS (Hj, \u0445j).In RL, the importance sample allows the use of non-political data as if it were at the political level. In this case, the deviation of the IS estimate is often much worse than the deviation of the on-policy MC estimates, because behavioural policy allows the use of non-political data as if it were at the political level."}, {"heading": "3. Behavior Policy Search", "text": "In this section, we will first describe the theoretical potential for reducing variance with appropriately selected behavioral policies. Generally, this policy will be unknown. Therefore, we propose a subproblem for policy assessment - the search problem for behavioral policies - solutions that adjust behavioral policies to provide lower mean square error estimates. To our knowledge, we are the first to propose an adjustment of behavioral policies for policy assessment."}, {"heading": "3.1. The Optimal Behavior Policy", "text": "Appropriately selected behavioral policies can reduce deviation to zero. While this fact is generally known for its importance sampling, here we show that this policy exists for each MDP and evaluation policy under two restrictive assumptions: All returns are positive and the domain is deterministic. In the following section, we describe how an initial policy can be adapted to the optimal behavioral policy, even if these conditions do not hold. Let w\u03c0 (H): = = = 0 \u03c0 (At | St). Let us consider a behavioral policy in such a way that for each trajectory H: \u03c1 (E) = IS (H, \u03c0? b) = g (H) w\u03c0e (H) w\u03c0? b (H). Reordering the terms of these expressions results in: w: b (H) = g (H) w\u03c0e (H) w\u03c0e) = IS = IS (H) = IS = IS = IS (H) = IS = IS (H) = E), so that the optimal policy can be chosen so that the probability of adherence to a policy is H (b)."}, {"heading": "3.2. The Behavior Policy Search Problem", "text": "Since the optimal behavioral policy cannot be determined analytically, we propose instead the problem of behavioral policy search (BPS) to find the problem that lowers the MSE of the estimates of \u03c1 (\u03c0e). A BPS problem is defined by the input factors: 1. An evaluation policy \u03c0e with political parameters \u03c0e. 2. A non-political evaluation of the OPE algorithm, OPE (H, \u03b8) that takes a course, H \u0445 \u03c0\u03b8, or alternatively a series of trajectories, and provides an estimate of the impact (\u03c0e).A BPS solution is a policy that emerges that non-political estimates with OPE have lower MSE values than on-policy estimates. Methods for this problem are BPS algorithms. Recall We have a formalized policy evaluation within a step-by-step setting in which a trajectory is generated for policy evaluation."}, {"heading": "4. Behavior Policy Gradient Theorem", "text": "We now introduce our primary contributions: an analytical expression for the gradient of the mean square error of the IS estimator and a stochastic gradient descendant algorithm that adapts to the MSE between the IS estimate and the behavior (\u03c0e). Nevertheless, our algorithm - the behavior of the political gradient (BPG) - starts with political estimates and adapts the behavior policy with gradient descendants on the MSE in relation to the state of the MSE in relation to the political parameters. The gradient of the MSE in relation to the political parameters is given by the following theorem: Theorem 1. None MSE [IS (H, \u03b8)] = E [\u2212 IS (H, \u03b8) 2L-Descendance t = 0."}, {"heading": "4.1. Control Variate Extension", "text": "In cases where an approximate model is available, we can further reduce the variance by adjusting the behavior policies of the doubly robust estimator (Jiang & Li, 2016; Thomas & Brunskill, 2016). However, based on a similar intuition to the Advantage Sum estimator (Section 2.3), the estimator Doubly Robust (DR) uses the value functions of an approximate model as a control variant to adjust the variance of import ampling.3 We show here that we can adjust the behavior policies to reduce the mean square error of the DR estimates. We call this new method DR-BPG for doubly robust behavioral variant a control variant to adjust the variance of import ampling.3 We show here that we can adjust the behavior policies to reduce the mean square error of the DR estimates. We call the new method DR-BPG for doubly robust behavioral variant a control variant."}, {"heading": "4.2. Connection to REINFORCE", "text": "BPG is closely related to existing work in the field of policy gradient RL (c.f., (Sutton et al., 2000) and we draw links between such a method and BPG to illustrate how BPG changes the distribution of trajectories. REINFORCE (Williams, 1992) attempts to maximize the number of trajectories by gradient ascent to \u03c1 (\u03c0\u03b8) using the following unbiased gradient of trajectories (\u03c0\u03b8): \u2202. This update increases the likelihood of actions leading to high return paths. BPG can be interpreted as REINFORCE, where the return of a trajectory is the square of its meaning amplified trajectory. BPG increases the likelihood of actions leading to high return paths."}, {"heading": "5. Empirical Study", "text": "In this section, we present an empirical study on the reduction of variance through behavioural search. We design our experiments to answer the following questions: \u2022 Can the behavioural search with BPG reduce the MSE policy assessment compared to policy estimates in tabular and continuous areas? \u2022 Does the adjustment of the behavioural policy of the Doubly Ro-Bust estimator with DR-BPG reduce the MSE of the Advantage Sum estimator? \u2022 Does the scarcity of measures leading to high rewards affect the performance gap between BPG and Monte Carlo estimates?"}, {"heading": "5.1. Experimental Set-up", "text": "We address our first experimental question by evaluating BPG in three areas. We briefly describe each domain here; full details can be found in Appendix C. The first domain is a 4x4 Gridworld. We obtain two evaluation guidelines by applying REINFORCE to this task, starting from a policy that randomly selects actions. We then select an evaluation guideline, \u03c01, from the early stages of learning - an improved but far from convergent policy - and one after learning converges, \u03c02. We conduct all experiments once with \u03c0e: = \u03c01 and a second time with \u03c0e: = \u03c02. Our second and third tasks are the continuous control of Cartpole Swing Up and Acrobot tasks that are implemented within the RLLAB (Duan et al., 2016). The evaluation guideline in each area is a neural network that maps the state to the mean of a school distributions optimized partially by confidence regions (the optimization of policies in 2015)."}, {"heading": "5.2. Main Results", "text": "In the first phase of the experiment, we agreed that the BPG and BPG measures we have taken need to be significantly reduced (Figure 1a). However, compared to a Monte Carlo estimate of 100 trajectories from the first step, it is only a marginal improvisation. At the end of each study, we used the final behavioural policy to collect 100 more trajectories and estimate the behaviour (Figure 1a). Compared to a Monte Carlo estimate of 100 trajectories from the first step, the BPG and BPG measures for risk reduction (Figure 1a) are lower. Compared to a Monte Carlo estimate of 100 trajectories from the first step, the MSE measure is 85.48% lower with this improved behavioural policy. In the second step, the MSE measure is 31.02% lower. This result shows that we can find behavioural policies that are significantly lower than MSE. To understand the differences in the performance between these two instances, we need to understand the distribution of policy assessments."}, {"heading": "5.3. Control Variate Extensions", "text": "In this section, we evaluate the combination of model-based control varies with the behavioral search. Specifically, we compare the AS estimator with Doubly Robust BPG (DR-BPG). In these experiments, we use a 10x10 stochastic grid world. The added stochasticity increases the difficulty to build an accurate model from trajectories. Since these methods require a model, we construct this model in two ways. The first method uses all trajectories in D to build the model and then uses the same set to build the trajectory (\u03c0e) with ASE or DR. The second method uses trajectories from the first 10 iterations to build the model and then fixes the model for the remaining iterations. For DR-BPG, the behavioral search begins with iteration 10 and the second condition. We call the first method \"update\" and the second method \"BDR variation\" fixed. \"The update method validates the theoretical guarantees of these methods, but learns a more accurate model."}, {"heading": "5.4. Rareness of Event", "text": "Our final experiment aims to understand how the gap between on- and off-policy variance is influenced by the probability of rare events. We test this intuition by varying the probability of a rare, high-magnitude event and observing how this change affects the performance gap between on- and off-policy evaluation. In this experiment, we use a variant of deterministic gridworld, in which the UP action in the initial state (the upper left corner) causes a transition to the final state with a reward of + 50."}, {"heading": "6. Related Work", "text": "While adaptive significance sampling has been studied in the estimation literature, we focus here on adaptive import samples for MDPs and Markov Reward Processes (i.e., an MDP with a fixed policy); existing work on adaptive IS in RL has taken into account the change in transition probabilities to reduce the variance in policy assessment (Desai & Glynn, 2001; Frank et al., 2008) or the variance in policy gradient estimates (Ciosek & Whiteson, 2017); as the transition probabilities in RL are typically unknown, adaptation of behavioural policy is a more general approach to adaptive IS. Ciosek and Whiteson also adjust the distribution of trajectories with gradient deviation policies (Ciosek & Whiteson, 2017)."}, {"heading": "7. Discussion and Future Work", "text": "One open question is the characterization of attitudes where policy adjustment is much better than policy estimates. To answer this question, our Gridworld experiment has shown that BPG, if it shows little deviation, can only offer marginal improvements. BPG increases the likelihood of observing rare events of a high magnitude. If the assessment policy never sees such events, there is little use for using BPG. However, in anticipation and with an appropriately selected step size, BPG will never reduce the data efficiency of the policy assessment. It is also necessary that the assessment policy contributes to the deviation in returns. If all deviations are due to the environment, it seems unlikely that BPG will offer a major improvement."}, {"heading": "8. Conclusion", "text": "We introduced the behavioral search problem to improve the estimation of \u03c1 (\u03c0e) for an assessment policy \u03c0e. We present a solution - Behavior Policy Gradient - to this problem, which adapts the behavioral policy with stochastic gradient drop to the estimator's variance for the importance samples. Experiments show that BPG lowers the mean square error of \u03c1 (\u03c0e) estimates compared to political estimates. We also show that BPG can further reduce the MSE of estimates in conjunction with a model-based control variant method."}, {"heading": "9. Acknowledgements", "text": "We thank Daniel Brown and the anonymous critics for their useful comments on the work and its presentation, which was conducted at the Personal Autonomous Robotics Lab (PeARL) and the Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory of the University of Texas at Austin. PeARL research is partially funded by NSF (IIS-1638107, IIS-1617639), LARG research is partially funded by NSF (CNS-1330072, CNS-1305287, IIS-1637736, IIS-1651089), ONR (21C184-01), AFOSR (FA9550-14-1-0087), Raytheon, Toyota, AT & T and Lockheed Martin. Josiah Hanna is supported by an NSF Graduate Research Fellowship. Peter Stone is a member of the Board of Directors of Cogitai, Inc."}, {"heading": "A. Proof of Theorem 1", "text": "In Appendix A we give the complete derivation of our primary theoretical contribution - the Importance Sampling Method (IS) Gradient of Variance. We also present the Gradient of Variance for the Double Robust (DR) Estimator. We first derive an analytical expression for the Gradient of Variance of an Arbitrary, Unbiased Policy Estimator (OPE). Importance Sampling is such an extraordinary policy evaluator. From our general derivation we derive the Gradient of Variance of the IS Estimator and then extend it to the DR Estimator. A.1. Grade of Variance of an Unbiased Policy Evaluation Methodology. We first present a problem from which we derive the variance of the IS Estimator and then extend it to the DR Estimator."}, {"heading": "B. BPG\u2019s Off-Policy Estimates are Unbiased", "text": "This appendix proves that the estimate of the BPG is an unbiased estimate of the iteration. the difficulty is that the estimate of the iteration n of the BPG depends on all iterations for i = 1. n and each iteration is not independent of the others. Nevertheless, we prove here that the estimate of the iteration n generates an unbiased estimate of the iteration. To illustrate the dependence of the iteration on the iterations, we will show that E [IS (Hn, \u03b8n) | \u03b80 = \u03b8e] is an unbiased estimate of the iteration, in which the IS estimate is conditioned on the iterations 0 = \u0445e. In order to make the dependence of the iteration on the iterations \u2212 1 explicit, we will write f (Hi \u2212 1), where the iteration of Hi \u2212 1 \u00b7 effi \u2212 1 is effi \u2212 1."}, {"heading": "C. Supplemental Experiment Description", "text": "This appendix contains experimental details in addition to the details contained in Section 5 of the paper but still far from following our learning processes.Gridworld: This domain is a 4x4 Gridworld with a terminal state with reward 10 at (3, 3), a state with reward 1 at (1, 3), and all other states having reward \u2212 1. The basis for action contains the four cardinal instructions and actions that move the agent in his intended direction (unless he moves into a wall that does not produce movement).The agent begins in (0,0), \u03b3 = 1, and L = 100. All policies use softmax action selection with temperature 1, where the probability of an action in a state is given by: \u03c0 s. We obtain two evaluation policies by applying REINFORCE to this task, starting from a policy that selects actions uniformly at random."}], "references": [{"title": "Model-free intelligent diabetes management using machine learning", "author": ["Bastani", "Meysam"], "venue": "PhD thesis, Masters thesis, Department of Computing Science, University of Alberta,", "citeRegEx": "Bastani and Meysam.,? \\Q2014\\E", "shortCiteRegEx": "Bastani and Meysam.", "year": 2014}, {"title": "Gradient convergence in gradient methods with erros", "author": ["Bertsekas", "Dimitri P", "Tsitsiklis", "John N"], "venue": null, "citeRegEx": "Bertsekas et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bertsekas et al\\.", "year": 2000}, {"title": "OFFER: Offenvironment reinforcement learning", "author": ["Ciosek", "Kamil", "Whiteson", "Shimon"], "venue": "In Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Ciosek et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ciosek et al\\.", "year": 2017}, {"title": "Simulation in optimization and optimization in simulation: A Markov chain perspective on adaptive Monte Carlo algorithms", "author": ["Desai", "Paritosh Y", "Glynn", "Peter W"], "venue": "In Proceedings of the 33rd conference on Winter simulation,", "citeRegEx": "Desai et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Desai et al\\.", "year": 2001}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Duan", "Yan", "Chen", "Xi", "Houthooft", "Rein", "Schulman", "John", "Abbeel", "Pieter"], "venue": "Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Reinforcement learning in the presence of rare events", "author": ["Frank", "Jordan", "Mannor", "Shie", "Precup", "Doina"], "venue": "In Proceedings of the 25th International Conference on Machine learning,", "citeRegEx": "Frank et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Frank et al\\.", "year": 2008}, {"title": "Monte Carlo methods, methuen & co", "author": ["JM Hammersley", "Handscomb", "DC"], "venue": "Ltd., London, pp", "citeRegEx": "Hammersley et al\\.,? \\Q1964\\E", "shortCiteRegEx": "Hammersley et al\\.", "year": 1964}, {"title": "Doubly robust off-policy evaluation for reinforcement learning", "author": ["Jiang", "Nan", "Li", "Lihong"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "Jiang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2016}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Kearns", "Michael", "Singh", "Satinder"], "venue": "Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2002}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "CoRR, abs/1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["D. Precup", "R.S. Sutton", "S. Singh"], "venue": "In Proceedings of the 17th International Conference on Machine Learning,", "citeRegEx": "Precup et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Precup et al\\.", "year": 2000}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Moritz", "Philipp", "Jordan", "Michael", "Abbeel", "Pieter"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning ( ICML),", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Large Sample Methods in Statistics: An Introduction with Applications", "author": ["P.K. Sen", "J.M. Singer"], "venue": null, "citeRegEx": "Sen and Singer,? \\Q1993\\E", "shortCiteRegEx": "Sen and Singer", "year": 1993}, {"title": "Reinforcement learning in finite mdps: PAC analysis", "author": ["Strehl", "Alexander L", "Li", "Lihong", "Littman", "Michael L"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Strehl et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2009}, {"title": "Reinforcement Learning: An Introduction", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Sutton", "Richard S", "McAllester", "David", "Singh", "Satinder", "Mansour", "Yishay"], "venue": "In Proceedings of the 13th Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutton et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2000}, {"title": "A notation for Markov decision processes", "author": ["Thomas", "Philip S"], "venue": "ArXiv, arXiv:1512.09075v1,", "citeRegEx": "Thomas and S.,? \\Q2015\\E", "shortCiteRegEx": "Thomas and S.", "year": 2015}, {"title": "Data-efficient off-policy policy evaluation for reinforcement learning", "author": ["Thomas", "Philip S", "Brunskill", "Emma"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "Thomas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Thomas et al\\.", "year": 2016}, {"title": "High confidence off-policy evaluation", "author": ["Thomas", "Philip S", "Theocharous", "Georgios", "Ghavamzadeh", "Mohammad"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Thomas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Thomas et al\\.", "year": 2015}, {"title": "True online TD (\u03bb)", "author": ["van Seijen", "Harm", "Sutton", "Richard S"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Seijen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Seijen et al\\.", "year": 2014}, {"title": "Variance reduction in Monte-Carlo tree search", "author": ["J. Veness", "M. Lanctot", "M. Bowling"], "venue": "In Proceedings of the 24th Conference on Neural Information Processing Systems,", "citeRegEx": "Veness et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Veness et al\\.", "year": 2011}, {"title": "Learning a value analysis tool for agent evaluation", "author": ["M. White", "M. Bowling"], "venue": "In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "White and Bowling,? \\Q1976\\E", "shortCiteRegEx": "White and Bowling", "year": 1976}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Optimal unbiased estimators for evaluating agent performance", "author": ["M. Zinkevich", "M. Bowling", "N. Bard", "M. Kan", "D. Billings"], "venue": "In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Zinkevich et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 9, "context": ", 2015), and robot control (Lillicrap et al., 2015), are modeled as Markov decision processes (MDPs) and solved using reinforcement learning (RL) algorithms.", "startOffset": 27, "endOffset": 51}, {"referenceID": 23, "context": "Previous work has addressed variance reduction for on-policy returns (Zinkevich et al., 2006; White & Bowling, 2009; Veness et al., 2011).", "startOffset": 69, "endOffset": 137}, {"referenceID": 20, "context": "Previous work has addressed variance reduction for on-policy returns (Zinkevich et al., 2006; White & Bowling, 2009; Veness et al., 2011).", "startOffset": 69, "endOffset": 137}, {"referenceID": 18, "context": "If \u03c0b is not chosen carefully, IS often has high variance (Thomas et al., 2015).", "startOffset": 58, "endOffset": 79}, {"referenceID": 5, "context": "While IS-based variance reduction has been explored in RL, this prior work has required knowledge of the environment\u2019s transition probabilities and remains onpolicy (Desai & Glynn, 2001; Frank et al., 2008; Ciosek & Whiteson, 2017).", "startOffset": 165, "endOffset": 231}, {"referenceID": 10, "context": ", (Thomas & Brunskill, 2016; Precup et al., 2000)).", "startOffset": 2, "endOffset": 49}, {"referenceID": 15, "context": ", (Sutton et al., 2000)) and we draw connections between one such method and BPG to illustrate how BPG changes the distribution of trajectories.", "startOffset": 2, "endOffset": 23}, {"referenceID": 4, "context": "Our second and third tasks are the continuous control Cartpole Swing Up and Acrobot tasks implemented within RLLAB (Duan et al., 2016).", "startOffset": 115, "endOffset": 134}, {"referenceID": 11, "context": "Policies are partially optimized with trust-region policy optimization (Schulman et al., 2015) applied to a randomly initialized policy.", "startOffset": 71, "endOffset": 94}, {"referenceID": 5, "context": "Existing work on adaptive IS in RL has considered changing the transition probabilities to lower the variance of policy evaluation (Desai & Glynn, 2001; Frank et al., 2008) or lower the variance of policy gradient estimates (Ciosek & Whiteson, 2017).", "startOffset": 131, "endOffset": 172}, {"referenceID": 23, "context": "In addition to the control variate technique used by the Advantage Sum estimator (Zinkevich et al., 2006; White & Bowling, 2009), Veness et al.", "startOffset": 81, "endOffset": 128}, {"referenceID": 13, "context": "When the goal is to minimize MSE it is often permissible to use biased methods such as temporal difference learning (van Seijen & Sutton, 2014), model-based policy evaluation (Kearns & Singh, 2002; Strehl et al., 2009), or variants of weighted importance sampling (Precup et al.", "startOffset": 175, "endOffset": 218}, {"referenceID": 10, "context": ", 2009), or variants of weighted importance sampling (Precup et al., 2000).", "startOffset": 53, "endOffset": 74}, {"referenceID": 18, "context": ", 2006; White & Bowling, 2009), Veness et al. consider using common random numbers and antithetic variates to reduce the variance of roll-outs in Monte Carlo Tree Search (MCTS) (2011). These techniques require a model of the environment (as is typical for MCTS) and do not appear to be applicable to the general RL policy evaluation problem.", "startOffset": 32, "endOffset": 184}], "year": 2017, "abstractText": "We consider the task of evaluating a policy for a Markov decision process (MDP). The standard unbiased technique for evaluating a policy is to deploy the policy and observe its performance. We show that the data collected from deploying a different policy, commonly called the behavior policy, can be used to produce unbiased estimates with lower mean squared error than this standard technique. We derive an analytic expression for the optimal behavior policy\u2014the behavior policy that minimizes the mean squared error of the resulting estimates. Because this expression depends on terms that are unknown in practice, we propose a novel policy evaluation sub-problem, behavior policy search: searching for a behavior policy that reduces mean squared error. We present a behavior policy search algorithm and empirically demonstrate its effectiveness in lowering the mean squared error of policy performance estimates.", "creator": "LaTeX with hyperref package"}}}