{"id": "1706.03691", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2017", "title": "Certified Defenses for Data Poisoning Attacks", "abstract": "Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our bound comes paired with a candidate attack that nearly realizes the bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12% to 23% test error by adding only 3% poisoned data.", "histories": [["v1", "Fri, 9 Jun 2017 16:26:49 GMT  (864kb,D)", "http://arxiv.org/abs/1706.03691v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["jacob steinhardt", "pang wei koh", "percy liang"], "accepted": true, "id": "1706.03691"}, "pdf": {"name": "1706.03691.pdf", "metadata": {"source": "CRF", "title": "Certified Defenses for Data Poisoning Attacks", "authors": ["Jacob Steinhardt", "Pang Wei Koh"], "emails": ["jsteinha@stanford.edu", "pangwei@cs.stanford.edu", "pliang@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "In machine learning, however, the most important component of all - the training data - is directly from the outside world. For a system trained on user data, an attacker can simply inject harmful data by creating a user account. Such data poisoning attacks require us to rethink what it means for a system to be safe. The focus of current work is on data poisoning attacks, first by Biggio et al. (2012) and later by a number of others (Xiao et al., 2012; 2015b; Newell et al., 2014; Mei and Zhu, 2015b; Burkard and Lagesse, 2017; Koh and Liang, 2017) This body has shown that data poisoning attacks can worsen the accuracy of classification."}, {"heading": "2 Problem Setting", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "2.1 Data Sanitization Defenses", "text": "A defender naively tied to the complete (clean + poisoned) data Dc-Dp = data is doomed to failure, since even a single poisoned point can in some cases arbitrarily alter the model (Liu and Zhu, 2016; Park et al., 2017). In this essay, we consider data cleanup as a defense (Cretu et al., 2008), which examines the complete data and tries to remove the poisoned points, for example by deleting outliers. Formally, the defender constructs a workable sentence F-X-Y and trains only on points in F: \"def = argmin.L.\" (Dc-Dp). \"F\".F \".F.\" where the outliers (x, y).S. \"\" (1). In the face of such a defense F., \"we would like to calculate an upper limit for the worst possible test loss (choice of Dp) - in symbols, maxDp.\" L. \""}, {"heading": "3 Attack, Defense, and Duality", "text": "To make progress, we look at three approaches: (i) we move from the test loss to the training loss to the clean data, which is related to each other by standard concentration arguments, as long as we adequately regulate the model (see Appendix B for details); (ii) we look at the training loss to the complete (clean + poisoned) data, which limits the loss of the clean data upwards due to the non-negativity of the loss. (ii) We have: L (Appendix B for details) (i). (i) we look at the training loss to the complete (clean + poisoned) data, which limits the loss of the clean data upwards due to the non-negativity of the loss. (ii) While (ii) could be a source of boredom, it is tight when the poisoned data fits well (as we show)."}, {"heading": "3.1 Fixed Defenses: Computing the Minimax Loss via Online Learning", "text": "We now focus on calculating the minimax loss M (4) when F is independent of Dp (Fixed Defenses). In the process of calculating M (Max), we will also produce candidate attacks (Max). Our algorithm is based on no-regrets online learning, which models a game between a learner and nature and therefore represents a natural adaptation to our data poisoning. To simplify the representation, we assume that it is a \"2-ball of the radius.\" Our algorithm shown in Algorithm 1 is very simple: in each iteration it switches between finding the worst target (x (t), y (t)) in relation to the current model."}, {"heading": "3.2 Data-Dependent Defenses: Upper and Lower Bounds", "text": "We now turn our attention to data-dependent defenders where the viable set F depends on the data Dc-Dp (and can therefore be influenced by the attacker), for example, consider the disk defense (see (2), which uses the empirical (poisoned) mean instead of the true mean: Fslab (Dp) def = {(x, y): | < x-\u00b5 (Dp), \u00b5 (Dp) \u2212 \u00b5 (Dp) \u2212 y (Dp) > | sy}, (7) where the upper set (Dp) is the empirical mean over Dc-Dp; the notation F (Dp) tracks the dependence of the viable set on Dp. Similar to Section 3.1, we analyze the minimax losses M, which we can bind as in (5)."}, {"heading": "4 Experiments I: Oracle Defenses", "text": "One advantage of our framework is that we get a tool that can be easily run on new datasets and defense systems to learn about the robustness of defense and gain insights into potential attacks. We first examine two image datasets: MNIST-1-7, and the Dogfish datasets used by Koh and Liang (2017). For Dogfish, which is a binary classification task, we used the same neural network characteristics as in Koh and Liang (2017), so that each of the n = 1800 training images is represented by a 2048-dimensional vector. For this and subsequent experiments, our loss is the hinge loss (i.e., we train an SVM).We consider the combined oracle slab and defense from Section 2.1: F = Fgorithy that we lose. \""}, {"heading": "4.1 Text Data: Handling Integrity Constraints", "text": "In this case, the only difference is that the QP of Section 4 has the additional condition that it becomes an integral square program (IQP), which can be uncompromisingly expensive."}, {"heading": "5 Experiments II: Data-Dependent Defenses", "text": "Previously, we saw that they were invulnerable, provided we had an oracle defender who knew the true class means. Instead, if we consider a data-dependent defender who uses the empirical (poisoned) means, how much can this change the vulnerability of these data sets? In this section, we will see that the answer is quite promising. However, as described in Section 3.2, we can still use our framework to reach upper and lower limits even in this data-dependent case, although the limits will not necessarily match. The main difficulty lies in calculating U-shaped (\u03b8), which involves a potentially irrevocable maximization (see (8). However, for 2-class SVMs there is a comprehensible semi-defined programming algorithm; the full details are in Appendix D, but the rough idea is the following: We can show that the optimal distribution of the data set in (8) to a maximum of 4 points is supported (a support vector and not a vector in each class)."}, {"heading": "6 Related Work", "text": "Due to their increased use in security-critical environments such as malware detection, there has been an explosion in work on the security of machine learning systems; see Barreno et al. (2010), Biggio et al. (2014a), Papernot et al. (2016b), and Gardiner and Nagaraja (2016) for some recent surveys. Our paper refers to the long line of work on data intoxication testing; beyond linear classifiers, others have studied LASSO (Xiao et al., 2015a), clustering (Biggio et al., 2013; 2014c), PCA (Rubinstein et al., 2009), topic modeling (Mei and Zhu, 2015a), collaborative filtering (Li et al., 2016), neural networks (Yang et al., al., 2017), and other models (Mozaffari-Kermani et al., 2015; Vuurens et al., 2016, Wang), vulnerabilities have been discovered in security systems (2016, Wang)."}, {"heading": "7 Discussion", "text": "In this paper, we have presented a tool for investigating data poisoning that goes beyond empirical validation by providing certificates for a large family of attacks. After applying this framework to binary SVMs, we can consider a number of extensions: for example, to other loss functions or to the classification of multiclass. We can also consider defense mechanisms that go beyond the sphere and disk considered here - for example, the cleanup of text data using a language model. In all of these cases, the Minimax loss M gives us a natural starting point for investigating both attacks and attacks. Separately, the bound L (BA) / M was useful because M admits the natural Minimax formulation (5), but the worst case L (BA) can be expressed directly as a two-stage optimization problem (Mei and Zhu, 2015b), which is generally insoluble but allows a number of attacks (Bard)."}, {"heading": "A Proof of Proposition 1", "text": "Proposition 1 follows the usual duality arguments that we reproduce here. First, remember the definition of remorse: for a sequence of loss functions ft (\u03b8), t = 1,.., T, and an algorithm with iterated \u03b8 (1),.., \u03b8 (T), remorse is defined as regret (T) def = T \u2211 t = 1 ft (t) \u2212 min 270 (t). (10) In our particular case, we take ft (\u03b8) = 1nL (\u03b8; Dc) + '(\u03b8; x (t + 1), y (t + 1))))) Henceft (\u03b8 (t)) = 1 n L (\u03b8); Dc) +' (TB) + '(t) (t); x (t + 1), y (t), y (T), y (t), grey (T), c), c (t), c (t), c (t), c (t), c), c)."}, {"heading": "B Defending Against Overfitting Attacks", "text": "In paragraph 3, we asserted that it was possible to defend ourselves against excessive adjustment of defence with appropriate regulation. In this paragraph, we substantiate this assertion. The key is the classical theory of uniform convergence, which allows us to say that, with probability 1 - \u03b4, the following uniform limit applies: \"1N\" (x, y) - \"Dc\" (\u03b8; x, y) - \"p\" [\"(\u03b8; x, y) -\" E \"(N), (14) -\" E \"-\" whereby E is a mistake that is roughly equal \"log\" (1 / \u03b4) n. \"-\" - \"-\" - \"- - -\" - - - - - - - \"- - - -\" - - - - \"- - - - - - -\" - - - - - - - - \"- - - - - -\" - - - - - - \"- - - -\" - - - - - \"- - - -\" - - - - - - - \"- - - - - - -\" - - - - - - - - \"- - - -\" - - - - - - \"- - - - - - - -\" - - - - - - - \"- - - - - - -\" - - - - - - \"- - - - - - -\" - - - - \"- - - -\" - - \"- - - -\" - - - - - - \"- -\" - - - - \"-\" - - - - \"-\" - - - \"-\" - \"- - - -\" - - \"- - - - - - -\" - - \"- - -\" - - \"- - -\" - \"- - -\" - \"-\" - - \"- - - - - -\" - \"- - - -\" - \"- - - -\" - \"- - - - -\" - - - - \"-\" - - \"-\" - \"- - - - -\" - \"- - - - -\" - - \"- - - - -\" - \"- - - -\" - \"- - - - -\" - - - - - - \"-\" - - - - - - - - - - - \"-\" - \"-\" - - \"- -\" - -"}, {"heading": "C Regret Bound for Adaptive RDA", "text": "Our optimization algorithm (algorithm 1) is similar in the spirit of Regularized Dual Averaging (Xiao, 2010), but the known limits of regret for RDA do not apply directly because the regulator was chosen adaptively to ensure the limitation of standards that applies in this case. In fact, in this case, a slightly different analysis is required, which is closer to that of Steinhardt et al. (2014) for a sparse linear regression. While the details would take us beyond the scope of this paper, we note the regret tied here: Theorem 2. After T-steps of updating algorithm 1, the regret of algorithm 1 as regression (T) \u2264 22\u03b7 + T = 1 \u0433g (t) \u043222 2\u0441t. (15) We make two observations: First, since it is necessary that it be in the order of 1: T, we can ensure an average regret O (1 / \u0445T)."}, {"heading": "D Semidefinite Program for U\u0303(\u03b8)", "text": "At this point, we will explain the semi-definitive program for U-points discussed in section 5. < b > points (both): U-points (both points): U-points (both points): U-points (both points): U-points (both points): U-points (both points): U-points (both points): U-points (both points): U-points (both points): U-points (both points): U-points (both points). (16) Our goal is to solve the maximization in the special case that F is defined by the data-pendent sphere and the plate defense (both points): U-points (both points; x, y) = max (1 \u2212 y < x, 0) is the sharpening loss. First, we argue that the optimal points without loss of universality are supported at most points (xa, 1), (xa, 1), (xa), (xa), (xa), (xa), (points (xa), (points (xa): (xa, xa), (xa), (points (xa): (xa), (xa), (xa), (xa), (xa), (xa), (xa), (xa), (xa): (a), (xa)."}], "references": [{"title": "The power of localization for efficiently learning linear separators with noise", "author": ["P. Awasthi", "M.F. Balcan", "P.M. Long"], "venue": "In Symposium on Theory of Computing (STOC),", "citeRegEx": "Awasthi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2014}, {"title": "Practical Bilevel Optimization: Algorithms and Applications", "author": ["J.F. Bard"], "venue": null, "citeRegEx": "Bard.,? \\Q1999\\E", "shortCiteRegEx": "Bard.", "year": 1999}, {"title": "The security of machine learning", "author": ["M. Barreno", "B. Nelson", "A.D. Joseph", "J.D. Tygar"], "venue": "Machine Learning,", "citeRegEx": "Barreno et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Barreno et al\\.", "year": 2010}, {"title": "Vulnerability of deep reinforcement learning to policy induction attacks", "author": ["V. Behzadan", "A. Munir"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Behzadan and Munir.,? \\Q2015\\E", "shortCiteRegEx": "Behzadan and Munir.", "year": 2015}, {"title": "Poisoning attacks against support vector machines", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Biggio et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2012}, {"title": "Is data clustering in adversarial settings secure", "author": ["B. Biggio", "I. Pillai", "S.R. Bul\u00f2", "D. Ariu", "M. Pelillo", "F. Roli"], "venue": "In Workshop on Artificial Intelligence and Security AISec),", "citeRegEx": "Biggio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2013}, {"title": "Security evaluation of pattern classifiers under attack. Knowledge and Data Engineering", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "IEEE Transactions on,", "citeRegEx": "Biggio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2014}, {"title": "Poisoning behavioral malware clustering", "author": ["B. Biggio", "K. Rieck", "D. Ariu", "C. Wressnegger", "I. Corona", "G. Giacinto", "F. Roli"], "venue": "In Workshop on Artificial Intelligence and Security (AISec),", "citeRegEx": "Biggio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2014}, {"title": "Poisoning completelinkage hierarchical clustering", "author": ["B. Biggio", "B.S. Rota", "P. Ignazio", "M. Michele", "M.E. Zemene", "P. Marcello", "R. Fabio"], "venue": "In Workshop on Structural, Syntactic, and Statistical Pattern Recognition,", "citeRegEx": "Biggio et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2011}, {"title": "Static prediction games for adversarial learning problems", "author": ["M. Br\u00fcckner", "C. Kanzow", "T. Scheffer"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Br\u00fcckner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Br\u00fcckner et al\\.", "year": 2012}, {"title": "Analysis of causative attacks against SVMs learning from data streams", "author": ["C. Burkard", "B. Lagesse"], "venue": "In International Workshop on Security And Privacy Analytics,", "citeRegEx": "Burkard and Lagesse.,? \\Q2017\\E", "shortCiteRegEx": "Burkard and Lagesse.", "year": 2017}, {"title": "Hidden voice commands", "author": ["N. Carlini", "P. Mishra", "T. Vaidya", "Y. Zhang", "M. Sherr", "C. Shields", "D. Wagner", "W. Zhou"], "venue": "In USENIX Security,", "citeRegEx": "Carlini et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Carlini et al\\.", "year": 2016}, {"title": "Learning from untrusted data", "author": ["M. Charikar", "J. Steinhardt", "G. Valiant"], "venue": "In Symposium on Theory of Computing (STOC),", "citeRegEx": "Charikar et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Charikar et al\\.", "year": 2017}, {"title": "Robust high dimensional sparse regression and matching pursuit", "author": ["Y. Chen", "C. Caramanis", "S. Mannor"], "venue": "arXiv,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Robust estimators in high dimensions without the computational intractability", "author": ["I. Diakonikolas", "G. Kamath", "D. Kane", "J. Li", "A. Moitra", "A. Stewart"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Diakonikolas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Diakonikolas et al\\.", "year": 2016}, {"title": "CVXPY: A Python-embedded modeling language for convex optimization", "author": ["S. Diamond", "S. Boyd"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Diamond and Boyd.,? \\Q2016\\E", "shortCiteRegEx": "Diamond and Boyd.", "year": 2016}, {"title": "On the security of machine learning in malware c&c detection: A survey", "author": ["J. Gardiner", "S. Nagaraja"], "venue": "ACM Computing Surveys (CSUR),", "citeRegEx": "Gardiner and Nagaraja.,? \\Q2016\\E", "shortCiteRegEx": "Gardiner and Nagaraja.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Goodfellow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Adversarial attacks on neural network policies. arXiv, 2017", "author": ["S. Huang", "N. Papernot", "I. Goodfellow", "Y. Duan", "P. Abbeel"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Huang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2009}, {"title": "Learning halfspaces with malicious noise", "author": ["A.R. Klivans", "P.M. Long", "R.A. Servedio"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Klivans et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Klivans et al\\.", "year": 2009}, {"title": "Understanding black-box predictions via influence functions", "author": ["P.W. Koh", "P. Liang"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Koh and Liang.,? \\Q2017\\E", "shortCiteRegEx": "Koh and Liang.", "year": 2017}, {"title": "Adversarial examples in the physical world", "author": ["A. Kurakin", "I. Goodfellow", "S. Bengio"], "venue": null, "citeRegEx": "Kurakin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kurakin et al\\.", "year": 2016}, {"title": "Agnostic estimation of mean and covariance", "author": ["K.A. Lai", "A.B. Rao", "S. Vempala"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Lai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2016}, {"title": "Curie: A method for protecting SVM classifier from poisoning", "author": ["R. Laishram", "V.V. Phoha"], "venue": "attack. arXiv,", "citeRegEx": "Laishram and Phoha.,? \\Q2016\\E", "shortCiteRegEx": "Laishram and Phoha.", "year": 2016}, {"title": "Practical evasion of a learning-based classifier: A case study", "author": ["P. Laskov", "N. \u0160rndic"], "venue": "In Symposium on Security and Privacy,", "citeRegEx": "Laskov and \u0160rndic\u0300.,? \\Q2014\\E", "shortCiteRegEx": "Laskov and \u0160rndic\u0300.", "year": 2014}, {"title": "Data poisoning attacks on factorization-based collaborative filtering", "author": ["B. Li", "Y. Wang", "A. Singh", "Y. Vorobeychik"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Tactics of adversarial attack on deep reinforcement learning agents", "author": ["Y. Lin", "Z. Hong", "Y. Liao", "M. Shih", "M. Liu", "M. Sun"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2017}, {"title": "The teaching dimension of linear learners", "author": ["J. Liu", "X. Zhu"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Liu and Zhu.,? \\Q2016\\E", "shortCiteRegEx": "Liu and Zhu.", "year": 2016}, {"title": "YALMIP: A toolbox for modeling and optimization in MATLAB", "author": ["J. L\u00f6fberg"], "venue": "CACSD,", "citeRegEx": "L\u00f6fberg.,? \\Q2004\\E", "shortCiteRegEx": "L\u00f6fberg.", "year": 2004}, {"title": "Learning word vectors for sentiment analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "The security of latent Dirichlet allocation", "author": ["S. Mei", "X. Zhu"], "venue": "In Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Mei and Zhu.,? \\Q2015\\E", "shortCiteRegEx": "Mei and Zhu.", "year": 2015}, {"title": "Spam filtering with naive Bayes \u2013 which naive Bayes", "author": ["V. Metsis", "I. Androutsopoulos", "G. Paliouras"], "venue": "In CEAS,", "citeRegEx": "Metsis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Metsis et al\\.", "year": 2006}, {"title": "Systematic poisoning attacks on and defenses for machine learning in healthcare", "author": ["M. Mozaffari-Kermani", "S. Sur-Kolay", "A. Raghunathan", "N.K. Jha"], "venue": "IEEE Journal of Biomedical and Health Informatics,", "citeRegEx": "Mozaffari.Kermani et al\\.,? \\Q1893\\E", "shortCiteRegEx": "Mozaffari.Kermani et al\\.", "year": 1893}, {"title": "On the practicality of integrity attacks on document-level sentiment analysis", "author": ["A. Newell", "R. Potharaju", "L. Xiang", "C. Nita-Rotaru"], "venue": "In Workshop on Artificial Intelligence and Security (AISec),", "citeRegEx": "Newell et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Newell et al\\.", "year": 2014}, {"title": "Paragraph: Thwarting signature learning by training maliciously", "author": ["J. Newsome", "B. Karp", "D. Song"], "venue": "In International Workshop on Recent Advances in Intrusion Detection,", "citeRegEx": "Newsome et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Newsome et al\\.", "year": 2006}, {"title": "Exact recoverability from dense corrupted observations via `1-minimization", "author": ["N.H. Nguyen", "T.D. Tran"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Nguyen and Tran.,? \\Q2017\\E", "shortCiteRegEx": "Nguyen and Tran.", "year": 2017}, {"title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow"], "venue": null, "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "Towards the science of security and privacy in machine learning. arXiv, 2016b", "author": ["N. Papernot", "P. McDaniel", "A. Sinha", "M. Wellman"], "venue": null, "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "Resilient linear classification: an approach to deal with attacks on training data", "author": ["S. Park", "J. Weimer", "I. Lee"], "venue": "In International Conference on Cyber-Physical Systems,", "citeRegEx": "Park et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Park et al\\.", "year": 2017}, {"title": "Antidote: Understanding and defending against poisoning of anomaly detectors", "author": ["B. Rubinstein", "B. Nelson", "L. Huang", "A.D. Joseph", "S. Lau", "S. Rao", "N. Taft", "J. Tygar"], "venue": "In ACM SIGCOMM Conference on Internet measurement conference,", "citeRegEx": "Rubinstein et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rubinstein et al\\.", "year": 2009}, {"title": "The statistics of streaming sparse regression", "author": ["J. Steinhardt", "S. Wager", "P. Liang"], "venue": null, "citeRegEx": "Steinhardt et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Steinhardt et al\\.", "year": 2014}, {"title": "Avoiding imposters and delinquents: Adversarial crowdsourcing and peer prediction", "author": ["J. Steinhardt", "G. Valiant", "M. Charikar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Steinhardt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Steinhardt et al\\.", "year": 2016}, {"title": "Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones", "author": ["J.F. Sturm"], "venue": "Optimization Methods and Software,", "citeRegEx": "Sturm.,? \\Q1999\\E", "shortCiteRegEx": "Sturm.", "year": 1999}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Stealing machine learning models via prediction APIs", "author": ["F. Tram\u00e8r", "F. Zhang", "A. Juels", "M.K. Reiter", "T. Ristenpart"], "venue": "In USENIX Security,", "citeRegEx": "Tram\u00e8r et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tram\u00e8r et al\\.", "year": 2016}, {"title": "How much spam can you take? An analysis of crowdsourcing results to increase accuracy", "author": ["J. Vuurens", "A.P. de Vries", "C. Eickhoff"], "venue": "ACM SIGIR Workshop on Crowdsourcing for Information Retrieval,", "citeRegEx": "Vuurens et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vuurens et al\\.", "year": 2011}, {"title": "Combating Attacks and Abuse in Large Online Communities", "author": ["G. Wang"], "venue": "PhD thesis, University of California Santa Barbara,", "citeRegEx": "Wang.,? \\Q2016\\E", "shortCiteRegEx": "Wang.", "year": 2016}, {"title": "Is feature selection secure against training data poisoning", "author": ["H. Xiao", "B. Biggio", "G. Brown", "G. Fumera", "C. Eckert", "F. Roli"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Xiao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2015}, {"title": "Support vector machines under adversarial label contamination", "author": ["H. Xiao", "B. Biggio", "B. Nelson", "C. Eckert", "F. Roli"], "venue": null, "citeRegEx": "Xiao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2015}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "author": ["L. Xiao"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Xiao.,? \\Q2010\\E", "shortCiteRegEx": "Xiao.", "year": 2010}, {"title": "Generative poisoning attack method against neural networks", "author": ["C. Yang", "Q. Wu", "H. Li", "Y. Chen"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2017}, {"title": "Modeling adversarial learning as nested Stackelberg games", "author": ["Y. Zhou", "M. Kantarcioglu"], "venue": "In Pacific-Asia Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Zhou and Kantarcioglu.,? \\Q2016\\E", "shortCiteRegEx": "Zhou and Kantarcioglu.", "year": 2016}, {"title": "Let `(\u03b8;x, y) be any margin-based loss: `(\u03b8;x, y) = \u03c6(y\u3008\u03b8, x\u3009), where \u03c6 is 1-Lipschitz. Then the bound (14) holds with probability 1 \u2212 \u03b4, for E(N", "author": ["N . More precisely", "Kakade"], "venue": "Kakade et al", "citeRegEx": "precisely and Kakade,? \\Q2009\\E", "shortCiteRegEx": "precisely and Kakade", "year": 2009}], "referenceMentions": [{"referenceID": 34, "context": "(2012) and later by a number of others (Xiao et al., 2012; 2015b; Newell et al., 2014; Mei and Zhu, 2015b; Burkard and Lagesse, 2017; Koh and Liang, 2017).", "startOffset": 39, "endOffset": 154}, {"referenceID": 10, "context": "(2012) and later by a number of others (Xiao et al., 2012; 2015b; Newell et al., 2014; Mei and Zhu, 2015b; Burkard and Lagesse, 2017; Koh and Liang, 2017).", "startOffset": 39, "endOffset": 154}, {"referenceID": 21, "context": "(2012) and later by a number of others (Xiao et al., 2012; 2015b; Newell et al., 2014; Mei and Zhu, 2015b; Burkard and Lagesse, 2017; Koh and Liang, 2017).", "startOffset": 39, "endOffset": 154}, {"referenceID": 24, "context": "Moreover, while some defenses have been proposed against specific attacks (Laishram and Phoha, 2016), few have been stress-tested against a determined attacker.", "startOffset": 74, "endOffset": 100}, {"referenceID": 4, "context": "The focus of the present work is on data poisoning attacks against classification algorithms, first studied by Biggio et al. (2012) and later by a number of others (Xiao et al.", "startOffset": 111, "endOffset": 132}, {"referenceID": 4, "context": "While previous work showed successful attacks on the MNIST-1-7 (Biggio et al., 2012) and Dogfish (Koh and Liang, \u2217Equal contribution.", "startOffset": 63, "endOffset": 84}, {"referenceID": 2, "context": "We consider the causative attack model (Barreno et al., 2010), which consists of a game between two players: the defender (who seeks to learn a model \u03b8), and the attacker (who wants the learner to learn a bad model).", "startOffset": 39, "endOffset": 61}, {"referenceID": 45, "context": ", 2014a); moreover, a determined attacker can often reverse-engineer necessary system details (Tram\u00e8r et al., 2016).", "startOffset": 94, "endOffset": 115}, {"referenceID": 2, "context": "Attacks that attempt to increase the overall test loss L(\u03b8\u0302), known as indiscriminate availability attacks (Barreno et al., 2010), can be thought of as a denial-of-service attack.", "startOffset": 107, "endOffset": 129}, {"referenceID": 28, "context": "A defender who trains na\u00efvely on the full (clean + poisoned) data Dc \u222a Dp is doomed to failure, as even a single poisoned point can in some cases arbitrarily change the model (Liu and Zhu, 2016; Park et al., 2017).", "startOffset": 175, "endOffset": 213}, {"referenceID": 39, "context": "A defender who trains na\u00efvely on the full (clean + poisoned) data Dc \u222a Dp is doomed to failure, as even a single poisoned point can in some cases arbitrarily change the model (Liu and Zhu, 2016; Park et al., 2017).", "startOffset": 175, "endOffset": 213}, {"referenceID": 34, "context": "One example for text classification is letting F be documents that contain only licensed words (Newell et al., 2014).", "startOffset": 95, "endOffset": 116}, {"referenceID": 50, "context": "averaging (Xiao, 2010), which is an alternative to gradient descent that often converges faster for norm-constrained optimization problems.", "startOffset": 10, "endOffset": 22}, {"referenceID": 16, "context": "We first study two image datasets: MNIST-1-7, and the Dogfish dataset used by Koh and Liang (2017). For MNIST-1-7, following Biggio et al.", "startOffset": 78, "endOffset": 99}, {"referenceID": 4, "context": "For MNIST-1-7, following Biggio et al. (2012), we considered binary classification between the digits", "startOffset": 25, "endOffset": 46}, {"referenceID": 21, "context": "For Dogfish, which is a binary classification task, we used the same neural net features as in Koh and Liang (2017), so that each of the n = 1800 training images is represented by a 2048-dimensional vector.", "startOffset": 95, "endOffset": 116}, {"referenceID": 15, "context": "(9) The results of Algorithm 1 are given in Figures 2a and 2b; here and elsewhere, we used a combination of CVXPY (Diamond and Boyd, 2016), YALMIP (L\u00f6fberg, 2004), SeDuMi (Sturm, 1999), and Gurobi (Gurobi Optimization, Inc.", "startOffset": 114, "endOffset": 138}, {"referenceID": 29, "context": "(9) The results of Algorithm 1 are given in Figures 2a and 2b; here and elsewhere, we used a combination of CVXPY (Diamond and Boyd, 2016), YALMIP (L\u00f6fberg, 2004), SeDuMi (Sturm, 1999), and Gurobi (Gurobi Optimization, Inc.", "startOffset": 147, "endOffset": 162}, {"referenceID": 43, "context": "(9) The results of Algorithm 1 are given in Figures 2a and 2b; here and elsewhere, we used a combination of CVXPY (Diamond and Boyd, 2016), YALMIP (L\u00f6fberg, 2004), SeDuMi (Sturm, 1999), and Gurobi (Gurobi Optimization, Inc.", "startOffset": 171, "endOffset": 184}, {"referenceID": 4, "context": "We compare our attack to two baselines in Figure 2c \u2014 the gradient descent method employed by Biggio et al. (2012) and Mei and Zhu (2015b), and a simple baseline that inserts copies of points from Dc with the opposite label (subject to the flipped points lying in F ).", "startOffset": 94, "endOffset": 115}, {"referenceID": 4, "context": "We compare our attack to two baselines in Figure 2c \u2014 the gradient descent method employed by Biggio et al. (2012) and Mei and Zhu (2015b), and a simple baseline that inserts copies of points from Dc with the opposite label (subject to the flipped points lying in F ).", "startOffset": 94, "endOffset": 139}, {"referenceID": 34, "context": "Beyond the the sphere and slab constraints, a valid attack on text data must satisfy additional integrity constraints (Newell et al., 2014) \u2014 if we encode input text t by a vector x = \u03c6(t) of indicator features, then each entry of \u03c6(t) is a non-negative integer.", "startOffset": 118, "endOffset": 139}, {"referenceID": 31, "context": "\u2217Though Mei and Zhu (2015b) state that their cost is convex, they communicated to us that this is incorrect.", "startOffset": 8, "endOffset": 28}, {"referenceID": 32, "context": "We ran both the upper bound relaxation and the IQP solver on two text datasets, the Enron spam corpus (Metsis et al., 2006) and the IMDB sentiment corpus (Maas et al.", "startOffset": 102, "endOffset": 123}, {"referenceID": 30, "context": ", 2006) and the IMDB sentiment corpus (Maas et al., 2011).", "startOffset": 38, "endOffset": 57}, {"referenceID": 2, "context": "Due to their increased use in security-critical settings such as malware detection, there has been an explosion of work on the security of machine learning systems; see Barreno et al. (2010), Biggio et al.", "startOffset": 169, "endOffset": 191}, {"referenceID": 2, "context": "Due to their increased use in security-critical settings such as malware detection, there has been an explosion of work on the security of machine learning systems; see Barreno et al. (2010), Biggio et al. (2014a), Papernot et al.", "startOffset": 169, "endOffset": 214}, {"referenceID": 2, "context": "Due to their increased use in security-critical settings such as malware detection, there has been an explosion of work on the security of machine learning systems; see Barreno et al. (2010), Biggio et al. (2014a), Papernot et al. (2016b), and Gardiner and Nagaraja (2016) for some recent surveys.", "startOffset": 169, "endOffset": 239}, {"referenceID": 2, "context": "Due to their increased use in security-critical settings such as malware detection, there has been an explosion of work on the security of machine learning systems; see Barreno et al. (2010), Biggio et al. (2014a), Papernot et al. (2016b), and Gardiner and Nagaraja (2016) for some recent surveys.", "startOffset": 169, "endOffset": 273}, {"referenceID": 5, "context": ", 2015a), clustering (Biggio et al., 2013; 2014c), PCA (Rubinstein et al.", "startOffset": 21, "endOffset": 49}, {"referenceID": 40, "context": ", 2013; 2014c), PCA (Rubinstein et al., 2009), topic modeling (Mei and Zhu, 2015a), collaborative filtering (Li et al.", "startOffset": 20, "endOffset": 45}, {"referenceID": 26, "context": ", 2009), topic modeling (Mei and Zhu, 2015a), collaborative filtering (Li et al., 2016), neural networks (Yang et al.", "startOffset": 70, "endOffset": 87}, {"referenceID": 51, "context": ", 2016), neural networks (Yang et al., 2017), and other models (Mozaffari-Kermani et al.", "startOffset": 25, "endOffset": 44}, {"referenceID": 46, "context": ", 2017), and other models (Mozaffari-Kermani et al., 2015; Vuurens et al., 2011; Wang, 2016).", "startOffset": 26, "endOffset": 92}, {"referenceID": 47, "context": ", 2017), and other models (Mozaffari-Kermani et al., 2015; Vuurens et al., 2011; Wang, 2016).", "startOffset": 26, "endOffset": 92}, {"referenceID": 35, "context": "There have also been a number of demonstrated vulnerabilities in deployed systems (Newsome et al., 2006; Laskov and \u0160rndic\u0300, 2014; Biggio et al., 2014b).", "startOffset": 82, "endOffset": 152}, {"referenceID": 25, "context": "There have also been a number of demonstrated vulnerabilities in deployed systems (Newsome et al., 2006; Laskov and \u0160rndic\u0300, 2014; Biggio et al., 2014b).", "startOffset": 82, "endOffset": 152}, {"referenceID": 44, "context": "A striking recent security vulnerability discovered in machine learning systems is adversarial test images that can fool image classifiers despite being imperceptible from normal images (Szegedy et al., 2014; Goodfellow et al., 2015; Carlini et al., 2016; Kurakin et al., 2016; Papernot et al., 2016a).", "startOffset": 186, "endOffset": 301}, {"referenceID": 18, "context": "A striking recent security vulnerability discovered in machine learning systems is adversarial test images that can fool image classifiers despite being imperceptible from normal images (Szegedy et al., 2014; Goodfellow et al., 2015; Carlini et al., 2016; Kurakin et al., 2016; Papernot et al., 2016a).", "startOffset": 186, "endOffset": 301}, {"referenceID": 11, "context": "A striking recent security vulnerability discovered in machine learning systems is adversarial test images that can fool image classifiers despite being imperceptible from normal images (Szegedy et al., 2014; Goodfellow et al., 2015; Carlini et al., 2016; Kurakin et al., 2016; Papernot et al., 2016a).", "startOffset": 186, "endOffset": 301}, {"referenceID": 22, "context": "A striking recent security vulnerability discovered in machine learning systems is adversarial test images that can fool image classifiers despite being imperceptible from normal images (Szegedy et al., 2014; Goodfellow et al., 2015; Carlini et al., 2016; Kurakin et al., 2016; Papernot et al., 2016a).", "startOffset": 186, "endOffset": 301}, {"referenceID": 18, "context": "Adversarial training (Goodfellow et al., 2015) and generative adversarial networks (Goodfellow et al.", "startOffset": 21, "endOffset": 46}, {"referenceID": 17, "context": ", 2015) and generative adversarial networks (Goodfellow et al., 2014) are similarly focused on test time; they both improve test performance by altering the training objective.", "startOffset": 44, "endOffset": 69}, {"referenceID": 27, "context": "Recent adversarial attacks on reinforcement learners (Huang et al., 2017; Behzadan and Munir, 2017; Lin et al., 2017) blend train and test vulnerabilities.", "startOffset": 53, "endOffset": 117}, {"referenceID": 14, "context": "Robust algorithms have been exhibited for mean and covariance estimation and clustering (Diakonikolas et al., 2016; Lai et al., 2016; Charikar et al., 2017), classification (Klivans et al.", "startOffset": 88, "endOffset": 156}, {"referenceID": 23, "context": "Robust algorithms have been exhibited for mean and covariance estimation and clustering (Diakonikolas et al., 2016; Lai et al., 2016; Charikar et al., 2017), classification (Klivans et al.", "startOffset": 88, "endOffset": 156}, {"referenceID": 12, "context": "Robust algorithms have been exhibited for mean and covariance estimation and clustering (Diakonikolas et al., 2016; Lai et al., 2016; Charikar et al., 2017), classification (Klivans et al.", "startOffset": 88, "endOffset": 156}, {"referenceID": 20, "context": ", 2017), classification (Klivans et al., 2009; Awasthi et al., 2014), regression (Nasrabadi et al.", "startOffset": 24, "endOffset": 68}, {"referenceID": 0, "context": ", 2017), classification (Klivans et al., 2009; Awasthi et al., 2014), regression (Nasrabadi et al.", "startOffset": 24, "endOffset": 68}, {"referenceID": 13, "context": ", 2014), regression (Nasrabadi et al., 2011; Nguyen and Tran, 2013; Chen et al., 2013; Bhatia et al., 2015) and crowdsourced data aggregation (Steinhardt et al.", "startOffset": 20, "endOffset": 107}, {"referenceID": 42, "context": ", 2015) and crowdsourced data aggregation (Steinhardt et al., 2016).", "startOffset": 42, "endOffset": 67}, {"referenceID": 1, "context": "Separately, the bound L(\u03b8\u0302) / M was useful because M admits the natural minimax formulation (5), but the worst-case L(\u03b8\u0302) can be expressed directly as a bilevel optimization problem (Mei and Zhu, 2015b), which is intractable in general but admits a number of heuristics (Bard, 1999).", "startOffset": 270, "endOffset": 282}, {"referenceID": 9, "context": "Bilevel optimization has been considered in the related setting of Stackelberg games (Br\u00fcckner and Scheffer, 2011; Br\u00fcckner et al., 2012; Zhou and Kantarcioglu, 2016), and is natural to apply here as well.", "startOffset": 85, "endOffset": 166}, {"referenceID": 52, "context": "Bilevel optimization has been considered in the related setting of Stackelberg games (Br\u00fcckner and Scheffer, 2011; Br\u00fcckner et al., 2012; Zhou and Kantarcioglu, 2016), and is natural to apply here as well.", "startOffset": 85, "endOffset": 166}], "year": 2017, "abstractText": "Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our bound comes paired with a candidate attack that nearly realizes the bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12% to 23% test error by adding only 3% poisoned data.", "creator": "LaTeX with hyperref package"}}}