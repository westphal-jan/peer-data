{"id": "1706.03762", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.", "histories": [["v1", "Mon, 12 Jun 2017 17:57:34 GMT  (1102kb,D)", "http://arxiv.org/abs/1706.03762v1", "15 pages, 5 figure"], ["v2", "Mon, 19 Jun 2017 16:49:45 GMT  (1125kb,D)", "http://arxiv.org/abs/1706.03762v2", "15 pages, 5 figure"], ["v3", "Tue, 20 Jun 2017 05:20:02 GMT  (1125kb,D)", "http://arxiv.org/abs/1706.03762v3", "15 pages, 5 figure"], ["v4", "Fri, 30 Jun 2017 17:29:30 GMT  (1124kb,D)", "http://arxiv.org/abs/1706.03762v4", "15 pages, 5 figures"]], "COMMENTS": "15 pages, 5 figure", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["ashish vaswani", "noam shazeer", "niki parmar", "jakob uszkoreit", "llion jones", "aidan n gomez", "lukasz kaiser", "illia polosukhin"], "accepted": true, "id": "1706.03762"}, "pdf": {"name": "1706.03762.pdf", "metadata": {"source": "CRF", "title": "Attention Is All You Need", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Aidan N. Gomez"], "emails": ["avaswani@google.com", "noam@google.com", "nikip@google.com", "usz@google.com", "llion@google.com", "aidan@cs.toronto.edu", "lukaszkaiser@google.com", "illia.polosukhin@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "However, recursive neural networks, long-term short-term memory [12] and gated recurrent [7] neural networks are firmly established as state-of-the-art approaches in sequence modeling and transduction problems such as speech modeling and machine translation [31, 2, 5]. Numerous efforts have since pushed the boundaries of recurring speech models and encoder decoder architectures further [34, 22, 14]. Recurring models typically take into account the calculation along the symbol positions of input and output sequences. By aligning positions to steps in computing time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht \u2212 1 and the input for position. This inherently sequential nature precedes parallels within training examples that become critical at longer sequence lengths, while memory limitations are limited by examples."}, {"heading": "2 Background", "text": "The goal of reducing the sequential calculation is also the basis of the Extended Neural GPU [21], ByteNet [16], and ConvS2S [9], which all use Convolutionary Neural Networks as a basic building block and calculate hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from any two input or output positions increases in distance between positions, linear for ConvS2S, and logarithmic for ByteNet. This makes it difficult to learn dependencies between distant positions [11]. In the transformer, this is reduced to a constant number of operations, but at the expense of a reduced effective resolution based on weighted average positions, an effect that we counteract with the multi-head attention described in Section 3.2.2."}, {"heading": "3 Model Architecture", "text": "Most competitive models for the transduction of neural sequences have an encoder-decoder structure [5, 2, 31], whereby the encoder generates an input sequence of symbol representations (x1,..., xn) to a sequence of continuous representations z = (z1,..., zn). At z, the decoder then generates an output sequence (y1,..., ym) of symbols one element at a time. At each step, the model is auto-regressive [10], with the previously generated symbols used as an additional input in the creation of the next. The transformer follows this overall architecture with stacked self-attention and point-to-point fully connected layers for both the encoder and the decoder, as shown in the left or right half of Figure 1."}, {"heading": "3.1 Encoder and Decoder Stacks", "text": "Encoder: The encoder consists of a stack of N = 6 identical layers. Each layer has two sublayers. The first layer is a multi-head self-attention mechanism, and the second is a simple, positively fully connected feed network. We use a residual connection around each of the two sublayers, followed by the normalization of the layer [1]. That is, the output of each sublayer is LayerNorm (x + sublayer (x)), where sublayer (x) is the function implemented by the sublayer itself. To facilitate these residual connections, all the sublayers in the model, as well as the embedding of the layers, generate outputs of the dimension dmodel = 512. Decoder: The decoder also consists of a stack of N = 6 identical layers. In addition to the two sublayers in each encoder layer, the decoder adds a third sublayer that executes multihead attention over the output of the same layers, again depending on the level."}, {"heading": "3.2 Attention", "text": "An attention function can be described as mapping a query and a set of key-value pairs to an output where the query, key, values, and output are vectors; the output is calculated as a weighted sum of values, with the weight assigned to each value being calculated by a query compatibility function with the corresponding key."}, {"heading": "3.2.1 Scaled Dot-Product Attention", "text": "The input consists of queries and keys of dimension dk and values of dimension dv. In practice, we calculate the attention function on a series of queries simultaneously, packed into a matrix Q. The keys and values are also packed into matrices K and V. We calculate the output matrix as follows: Attention (Q, K, V) = softmax (QKT \u221a dk) V (1) The two most commonly used attention functions are additive attention [2], and point-product (multiplicative) attention (multiplicative) is generated by using this attention. Point-product attention is identical to our algorithm, with the exception of the scaling factor of size dk. Additional attention compiles the compatibility by generating a greater attention result (multiplicative), while the attention in two layers of hidden values is more efficient in practice."}, {"heading": "3.2.2 Multi-Head Attention", "text": "Instead of performing a single attention function with dk-dimensional keys, values and queries, we found it advantageous to project the queries, keys and values h-times with different learned linear projections onto dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values, we then perform the attention function in parallel, resulting in dv-dimensional output values, which are concatenated and produce the final values, as shown in Figure 2Multi-Head Attention, where the model can jointly perceive information from different representation subspaces at different positions. Averaging prevents this with a single attention head. MultiHead (Q, K, V) = Concat (Head1,..., Headh) for the headi = Attention (QW Q i, KW K i, V i) Where the projections are parameter matrices QRi, Total Model, Vdk, Vdk-Vdi and Dimensional Wall respectively."}, {"heading": "3.2.3 Applications of Attention in our Model", "text": "The transformer uses multihead attention in three different ways: \u2022 In the encoder decoder attention layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. Thus, any position in the encoder can be served across all positions in the input sequence. This mimics the typical encoder decoder attention mechanisms in sequence-to-sequence models such as [34, 2, 9]. \u2022 The encoder contains self-attention layers. In a self-attention layer, all keys, values and queries come from the same place, in this case the output of the previous layer in the encoder. Each position in the encoder can take care of all positions in the previous layer of the encoder. \u2022 Similarly, self-attention layers allow any position in the decoder all positions in the decoder up to that position. We must prevent the information from flowing uncoded in the decoder."}, {"heading": "3.3 Position-wise Feed-Forward Networks", "text": "In addition to the attention sub-planes, each of the layers in our encoder and decoder contains a fully connected feed network that is applied at each position separately and identically. It consists of two linear transformations with a ReLU activation in between. FFN (x) = max (0, xW1 + b1) W2 + b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer. The input and output dimension is dmodel, and the inner layer is dimension 2048."}, {"heading": "3.4 Embeddings and Softmax", "text": "Similar to other models of sequence transduction, we use learned embedding to convert the input and output tokens into vectors of the dimension model. We also use the usual learned linear transformation and softmax function to convert the decoder output into predicted probabilities of the next token. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation. In the embedding layers, we multiply these weights by \u221a dmodel."}, {"heading": "3.5 Positional Encoding", "text": "Since our model contains no repetition and no folding so that the model can use the sequence of the sequence, we must inject some information about the relative or absolute position of the symbols in the sequence. To this end, we add \"position encodings\" to the input encodings at the bottom of the encoder and decoder stack. The position encodings have the same dimension as the embeddings, so that the two can be summed up. There are many possibilities of position encodings, learned and fixed [9]. In this work, we use sine and cosine functions of different frequencies: PE (pos, 2i) = sin (pos / 10000 2i / dmodel 1) PE (pos, 2i + 1) = cos (pos / 10000 2i / dmodel) 2i-1, where pos is the position and i the dimension. In other words, each dimension of the position coding corresponds to a sinusoid. The waveform of a fixed form 2i / dmodel is 1-1 PE (pos, 2i + 1) = 1 (pos, 2i + 1) = 1 (pos / 10000 2i + 1) = 1 (pos / 10000 2i 2i / dmodel), where pos is the position and i the dimension of the dimension of a sine-1-1-1-1."}, {"heading": "4 Why Self-Attention", "text": "In this section, various aspects of introspection are compared with the recessive and conventional layers used for mapping a variable sequence of symbols (x1,... xn) in order to achieve another sequence of equal length (z1, zn), and the amount of billing, which can be compared with the minimum number of sequences."}, {"heading": "5 Training", "text": "This section describes the training program for our models."}, {"heading": "5.1 Training Data and Batching", "text": "We trained with the standard WMT 2014 English-German dataset, which consists of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared vocabulary of about 37,000 tokens. For English-French, we used the significantly larger WMT2014 English-French dataset, which consists of 36M sentences, and divided tokens into a 32,000-word piece of vocabulary [34]. Sentence pairs were compiled by approximate sequence length. Each training batch contained one set of sentence pairs, which contained about 25,000 source tokens and 25,000 target tokens."}, {"heading": "5.2 Hardware and Schedule", "text": "We trained our models on a machine with 8 NVIDIA P100 GPUs. For our base models, which used the hyperparameters described in the entire essay, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our large models (as described in Table 3), the step time was 1.0 seconds, and the models were trained for 300,000 steps or 3.5 days."}, {"heading": "5.3 Optimizer", "text": "We used the Adam Optimizer [18] with \u03b21 = 0.9, \u03b22 = 0.98 and = 10 \u2212 9. We varied the learning rate during the training according to the formula: lrate = d \u2212 0.5 model \u00b7 min (step _ num \u2212 0.5, step _ num \u00b7 warmup _ steps \u2212 1.5) (3) This corresponds to a linear increase in the learning rate for the first warmup _ steps training steps and a proportional decrease in this learning rate according to the inverted square root of the step number. We used warmup _ steps = 4000."}, {"heading": "5.4 Regularization", "text": "During the training, we apply three types of regulation: Residual dropout We apply dropout [30] to the performance of each base layer before it is added and normalized to the input of the base layer. In addition, we apply dropout to the sums of embedding and position encodings in both the encoder and decoder stack. For the base model, we use a rate of Pdrop = 0.1.Attention Dropout Query on key attentions that are structurally similar to the hidden to hidden weights in a feed network, albeit across positions. Softmax activations that elicit attention weights can then be seen as an analogy of hidden activations of the layer. A natural option is to extend dropout [30] to attention by performing attention interruptions as, Attention (Q, K, V) = Dropout (softmax (QKT) = insufficient activity)."}, {"heading": "6 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Machine Translation", "text": "In the WMT 2014 English-German translation task, our large transformer (big) model in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, resulting in a new, state-of-the-art BLEU score of 28.4. Configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model outperforms all previously released individual models and ensembles at a fraction of the training cost of one of the previous best models. In the WMT 2014 English-French translation task, our large model achieves a BLEU score of 41.0, exceeding all previously released individual models at less than 1 / 4 of the training cost of the previous state-of-the-art model. The transformer (big) model, which was trained for English-French, used drop-out rate Pdrop = 0.1, instead of 0.3. In the base models, we used a single point comparison, with the 34 points for the final 5 points."}, {"heading": "6.2 Model Variations", "text": "In order to assess the importance of the various components of the transformer, we varied our basic model in different ways, measuring the change in performance in the translation from English to English on the Latest 2013 development kit. We used beam search as described in the previous section, but no averaging of control points. We present these results in Table 3. In Table 3 lines (A), we vary the number of attention heads and the key and value dimensions, keeping the amount of calculation constant as described in Section 3.2.2. While the attention of a single head is 0.9 BLEU worse than the best setting, the quality also falls off with too many headings.In Table 3 lines (B), we find that reducing the attention key size dk harms the model quality, suggesting that determining compatibility is not easy and that a more sophisticated compatibility function than a dot product can be advantageous."}, {"heading": "6.3 English Constituency Parsing", "text": "To assess whether the transformer can generalize to other tasks, we conducted experiments to analyze English constituencies, a task that poses specific challenges: The output is subject to severe structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to achieve state-of-the-art results in small data regimes [33]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) part of Penn Treebank [23], about 40K training sets. We also trained it in a semi-supervised setting, using the larger confidence and BerkleyParser company with about 17M sets. [We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100 to determine the output of residual models.] We used a vocabulary of only 16K and a low vocabulary of 32K for one."}, {"heading": "7 Conclusion", "text": "In this thesis, we introduced the Transformer, the first fully attention-based sequence transduction model that replaces the recurring layers most commonly used in encoder decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurring or winding layers. We also provide an indication of the broader applicability of our models through experiments on English constituencies. We are excited about the future of attention-based models and plan to apply them to other tasks. In the first task, our best model even outperforms all previously reported ensembles. We also provide an indication of the broader applicability of our models through experiments on English constituencies. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems that affect other input and output modalities as text, and to investigate local, limited attention mechanisms in order to deal efficiently with video input models and output models, such as audio."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Massive exploration of neural machine translation", "author": ["Denny Britz", "Anna Goldie", "Minh-Thang Luong", "Quoc V. Le"], "venue": "architectures. CoRR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2017}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "arXiv preprint arXiv:1601.06733,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "translation. CoRR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Xception: Deep learning with depthwise separable convolutions", "author": ["Francois Chollet"], "venue": "arXiv preprint arXiv:1610.02357,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1412.3555,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith"], "venue": "In Proc. of NAACL,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Convolutional sequence to sequence learning", "author": ["Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N. Dauphin"], "venue": "arXiv preprint arXiv:1705.03122v2,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Gradient flow in recurrent nets: the difficulty of learning", "author": ["Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi", "J\u00fcrgen Schmidhuber"], "venue": "long-term dependencies,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Self-training PCFG grammars with latent annotations across languages", "author": ["Zhongqiang Huang", "Mary Harper"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Neural GPUs learn algorithms", "author": ["\u0141ukasz Kaiser", "Ilya Sutskever"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Neural machine translation in linear time", "author": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "Aaron van den Oord", "Alex Graves", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1610.10099v2,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2017}, {"title": "Structured attention networks", "author": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In ICLR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Factorization tricks for LSTM networks", "author": ["Oleksii Kuchaiev", "Boris Ginsburg"], "venue": "arXiv preprint arXiv:1703.10722,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}, {"title": "A structured self-attentive sentence embedding", "author": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1703.03130,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "Can active memory replace attention", "author": ["Samy Bengio \u0141ukasz Kaiser"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1508.04025,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1993}, {"title": "Effective self-training for parsing", "author": ["David McClosky", "Eugene Charniak", "Mark Johnson"], "venue": "In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "A decomposable attention model", "author": ["Ankur Parikh", "Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Jakob Uszkoreit"], "venue": "In Empirical Methods in Natural Language Processing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "A deep reinforced model for abstractive summarization", "author": ["Romain Paulus", "Caiming Xiong", "Richard Socher"], "venue": "arXiv preprint arXiv:1705.04304,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2017}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "arXiv preprint arXiv:1508.07909,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "author": ["Noam Shazeer", "Azalia Mirhoseini", "Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "venue": "arXiv preprint arXiv:1701.06538,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2017}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1929}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Rethinking the inception architecture for computer", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "venue": "vision. CoRR,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Vinyals", "Kaiser", "Koo", "Petrov", "Sutskever", "Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey"], "venue": "arXiv preprint arXiv:1609.08144,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Deep recurrent models with fast-forward connections for neural machine", "author": ["Jie Zhou", "Ying Cao", "Xuguang Wang", "Peng Li", "Wei Xu"], "venue": "translation. CoRR,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": "Recurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [31, 2, 5].", "startOffset": 50, "endOffset": 54}, {"referenceID": 5, "context": "Recurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [31, 2, 5].", "startOffset": 75, "endOffset": 78}, {"referenceID": 29, "context": "Recurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [31, 2, 5].", "startOffset": 267, "endOffset": 277}, {"referenceID": 0, "context": "Recurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [31, 2, 5].", "startOffset": 267, "endOffset": 277}, {"referenceID": 3, "context": "Recurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [31, 2, 5].", "startOffset": 267, "endOffset": 277}, {"referenceID": 32, "context": "Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [34, 22, 14].", "startOffset": 124, "endOffset": 136}, {"referenceID": 20, "context": "Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [34, 22, 14].", "startOffset": 124, "endOffset": 136}, {"referenceID": 12, "context": "Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [34, 22, 14].", "startOffset": 124, "endOffset": 136}, {"referenceID": 17, "context": "Recent work has achieved significant improvements in computational efficiency through factorization tricks [19] and conditional \u2217Equal contribution.", "startOffset": 107, "endOffset": 111}, {"referenceID": 27, "context": "computation [29], while also improving model performance in case of the latter.", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 17].", "startOffset": 224, "endOffset": 231}, {"referenceID": 15, "context": "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 17].", "startOffset": 224, "endOffset": 231}, {"referenceID": 23, "context": "In all but a few cases [25], however, such attention mechanisms are used in conjunction with a recurrent network.", "startOffset": 23, "endOffset": 27}, {"referenceID": 19, "context": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [21], ByteNet [16] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [21], ByteNet [16] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.", "startOffset": 111, "endOffset": 115}, {"referenceID": 7, "context": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [21], ByteNet [16] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.", "startOffset": 128, "endOffset": 131}, {"referenceID": 9, "context": "This makes it more difficult to learn dependencies between distant positions [11].", "startOffset": 77, "endOffset": 81}, {"referenceID": 2, "context": "Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 25, 26, 20].", "startOffset": 198, "endOffset": 213}, {"referenceID": 23, "context": "Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 25, 26, 20].", "startOffset": 198, "endOffset": 213}, {"referenceID": 24, "context": "Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 25, 26, 20].", "startOffset": 198, "endOffset": 213}, {"referenceID": 18, "context": "Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 25, 26, 20].", "startOffset": 198, "endOffset": 213}, {"referenceID": 13, "context": "In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [15, 16] and [9].", "startOffset": 132, "endOffset": 140}, {"referenceID": 14, "context": "In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [15, 16] and [9].", "startOffset": 132, "endOffset": 140}, {"referenceID": 7, "context": "In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [15, 16] and [9].", "startOffset": 145, "endOffset": 148}, {"referenceID": 3, "context": "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 31].", "startOffset": 87, "endOffset": 97}, {"referenceID": 0, "context": "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 31].", "startOffset": 87, "endOffset": 97}, {"referenceID": 29, "context": "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 31].", "startOffset": 87, "endOffset": 97}, {"referenceID": 8, "context": "At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": "The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention.", "startOffset": 70, "endOffset": 73}, {"referenceID": 1, "context": "While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3].", "startOffset": 160, "endOffset": 163}, {"referenceID": 32, "context": "This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [34, 2, 9].", "startOffset": 100, "endOffset": 110}, {"referenceID": 0, "context": "This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [34, 2, 9].", "startOffset": 100, "endOffset": 110}, {"referenceID": 7, "context": "This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [34, 2, 9].", "startOffset": 100, "endOffset": 110}, {"referenceID": 7, "context": "There are many choices of positional encodings, learned and fixed [9].", "startOffset": 66, "endOffset": 69}, {"referenceID": 9, "context": "The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 32, "context": "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [34] and byte-pair [28] representations.", "startOffset": 308, "endOffset": 312}, {"referenceID": 26, "context": "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [34] and byte-pair [28] representations.", "startOffset": 327, "endOffset": 331}, {"referenceID": 14, "context": "Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [16], increasing the length of the longest paths between any two positions in the network.", "startOffset": 142, "endOffset": 146}, {"referenceID": 4, "context": "Separable convolutions [6], however, decrease the complexity considerably, to O(k \u00b7 n \u00b7 d + n \u00b7 d).", "startOffset": 23, "endOffset": 26}, {"referenceID": 1, "context": "Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens.", "startOffset": 48, "endOffset": 51}, {"referenceID": 32, "context": "2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [34].", "startOffset": 108, "endOffset": 112}, {"referenceID": 16, "context": "3 Optimizer We used the Adam optimizer [18] with \u03b21 = 0.", "startOffset": 39, "endOffset": 43}, {"referenceID": 28, "context": "Residual Dropout We apply dropout [30] to the output of each sub-layer, before it is added to the sub-layer input and normalized.", "startOffset": 34, "endOffset": 38}, {"referenceID": 28, "context": "A natural possibility is to extend dropout [30] to attention.", "startOffset": 43, "endOffset": 47}, {"referenceID": 30, "context": "1 [32].", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": "Model BLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [16] 23.", "startOffset": 65, "endOffset": 69}, {"referenceID": 33, "context": "75 Deep-Att + PosUnk [35] 39.", "startOffset": 21, "endOffset": 25}, {"referenceID": 32, "context": "0 \u00b7 10 GNMT + RL [34] 24.", "startOffset": 17, "endOffset": 21}, {"referenceID": 7, "context": "4 \u00b7 10 ConvS2S [9] 25.", "startOffset": 15, "endOffset": 18}, {"referenceID": 27, "context": "5 \u00b7 10 MoE [29] 26.", "startOffset": 11, "endOffset": 15}, {"referenceID": 33, "context": "2 \u00b7 10 Deep-Att + PosUnk Ensemble [35] 40.", "startOffset": 34, "endOffset": 38}, {"referenceID": 32, "context": "0 \u00b7 10 GNMT + RL Ensemble [34] 26.", "startOffset": 26, "endOffset": 30}, {"referenceID": 7, "context": "1 \u00b7 10 ConvS2S Ensemble [9] 26.", "startOffset": 24, "endOffset": 27}, {"referenceID": 32, "context": "6 [34].", "startOffset": 2, "endOffset": 6}, {"referenceID": 32, "context": "We set the maximum output length during inference to input length + 50, but terminate early when possible [34].", "startOffset": 106, "endOffset": 110}, {"referenceID": 31, "context": "Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [33].", "startOffset": 121, "endOffset": 125}, {"referenceID": 21, "context": "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [23], about 40K training sentences.", "startOffset": 114, "endOffset": 118}, {"referenceID": 31, "context": "(2014) [33] WSJ only, discriminative 88.", "startOffset": 7, "endOffset": 11}, {"referenceID": 25, "context": "(2006) [27] WSJ only, discriminative 90.", "startOffset": 7, "endOffset": 11}, {"referenceID": 6, "context": "(2016) [8] WSJ only, discriminative 91.", "startOffset": 7, "endOffset": 10}, {"referenceID": 11, "context": "3 Huang & Harper (2009) [13] semi-supervised 91.", "startOffset": 24, "endOffset": 28}, {"referenceID": 22, "context": "(2006) [24] semi-supervised 92.", "startOffset": 7, "endOffset": 11}, {"referenceID": 31, "context": "(2014) [33] semi-supervised 92.", "startOffset": 7, "endOffset": 11}, {"referenceID": 6, "context": "(2016) [8] generative 93.", "startOffset": 7, "endOffset": 10}, {"referenceID": 31, "context": "[33].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].", "startOffset": 231, "endOffset": 234}, {"referenceID": 31, "context": "In contrast to RNN sequence-to-sequence models [33], the Transformer outperforms the BerkeleyParser [27] even when training only on the WSJ training set of 40K sentences.", "startOffset": 47, "endOffset": 51}, {"referenceID": 25, "context": "In contrast to RNN sequence-to-sequence models [33], the Transformer outperforms the BerkeleyParser [27] even when training only on the WSJ training set of 40K sentences.", "startOffset": 100, "endOffset": 104}], "year": 2017, "abstractText": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.", "creator": "LaTeX with hyperref package"}}}