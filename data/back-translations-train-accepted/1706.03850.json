{"id": "1706.03850", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Adversarial Feature Matching for Text Generation", "abstract": "The Generative Adversarial Network (GAN) has achieved great success in generating realistic (real-valued) synthetic data. However, convergence issues and difficulties dealing with discrete data hinder the applicability of GAN to text. We propose a framework for generating realistic text via adversarial training. We employ a long short-term memory network as generator, and a convolutional network as discriminator. Instead of using the standard objective of GAN, we propose matching the high-dimensional latent feature distributions of real and synthetic sentences, via a kernelized discrepancy metric. This eases adversarial training by alleviating the mode-collapsing problem. Our experiments show superior performance in quantitative evaluation, and demonstrate that our model can generate realistic-looking sentences.", "histories": [["v1", "Mon, 12 Jun 2017 20:55:51 GMT  (1244kb,D)", "http://arxiv.org/abs/1706.03850v1", "Accepted by ICML 2017"], ["v2", "Sat, 29 Jul 2017 05:50:13 GMT  (2106kb,D)", "http://arxiv.org/abs/1706.03850v2", "Accepted by ICML 2017"]], "COMMENTS": "Accepted by ICML 2017", "reviews": [], "SUBJECTS": "stat.ML cs.CL cs.LG", "authors": ["yizhe zhang", "zhe gan", "kai fan", "zhi chen", "ricardo henao", "dinghan shen", "lawrence carin"], "accepted": true, "id": "1706.03850"}, "pdf": {"name": "1706.03850.pdf", "metadata": {"source": "META", "title": "Adversarial Feature Matching for Text Generation", "authors": ["Yizhe Zhang", "Zhe Gan", "Kai Fan", "Zhi Chen", "Ricardo Henao", "Lawrence Carin"], "emails": ["<yizhe.zhang@duke.edu>."], "sections": [{"heading": "1. Introduction", "text": "The general idea is to estimate a distribution of sentences from a corpus, which are then used to evaluate the model. This task is important because it makes it possible to generate responses that maintain the semantic and syntactic properties of the real world sentences while potentially diverging from other examples used to estimate the model. Thus, for example, it is desirable to generate responses that are more diverse and less ingenious (Li et al., 2016). A simple approach is to explore a latent space to represent (fixed length) sentences using an encoded framework (autoencoder) based on recurrent Neural Networks. (Cho et al., 2014; Sutskever et al.), then generate synthetic sentences by decoding at the University of Durham, NC, 27708."}, {"heading": "2. Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Generative Adversarial Networks", "text": "GAN (Goodfellow et al., 2014) aims to obtain the equilibrium of the following optimization objectives LGAN = Ex \u0445 px logD (x) + Ez \u0445 pz log [1 \u2212 D (G (z))], (1) where LGAN is maximized and minimized from empirical distribution px (\u00b7) w.r.t. G (\u00b7). Note that the first term of (1) does not depend on G (\u00b7). If the discriminator is optimal, the solution of this adversarial game corresponds to minimizing the Jenson-Shannon divergence (JSD) (Arjovsky & Bottou, 2017), when the signal from the real data distribution pz (\u00b7), which is considered optimal, is minimizing the Jenson-Shannon divergence (JSD) (JSD) divergence (JSD) (Arjovsky & Bottou, 2017) between the real data distribution px and the data px ()."}, {"heading": "2.2. TextGAN", "text": "In view of a set of Corpus S, instead of directly optimizing the target of standard GAN in (1), we adopt an approach similar to the feature matching scheme of Salimans et al (2016). Specifically, we consider the objective LD = LGAN = LGAN set points (2) LMMD2 (2) LG = LMD2 (3) LGAN = Es set points (s) + Ez set points (1) Lrecon = LMD2 (2), where LD and LG are iteratively maximized w.r.t D () and minimized w.r.t G (), respectively LGAN is the default object of GAN in (1). Lrecon is the euclidean distance between the reconstructed latent code, z and the original code, z, drawn from the previous distribution pz."}, {"heading": "2.3. Alternative (data efficient) objectives", "text": "One limitation of the proposed approach is that the dimensionality of the characters f and f may be much greater than the size of the subset of data (minibatch) used during learning, so the empirical distribution may not be sufficiently representative. To mitigate this problem, we consider two strategies. Consolidating f and f into a lowerdimensional attribute space generally requires the size of the minibatch proportional to the number of dimensions (Ramdas et al., 2014). To alleviate this problem, we consider f and f in a lowerdimensional attribute test, using a compression network with fully connected layers also learned from D (\u00b7). This is useful because the discriminator will still encourage the most difficult features to be abstracted from the original features f and f. This approach offers significant compression savings, such as compressing the MD (4) scale with O dimensions."}, {"heading": "2.4. Model specification", "text": "In fact, it is such that it is a matter of a kind and manner in which it concerns the conceptualization of the individual concepts, in which the individual concepts of the individual concepts are interwoven with each other. (...) The conceptualization of the individual concepts and concepts of the individual concepts is very different in the manner and manner in which the individual concepts of the individual concepts are interwoven with each other in the individual conceptual levels. (...) The conceptualization of the individual concepts and concepts of the individual concepts and concepts of the individual concepts, of the individual concepts of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual, of the individual concepts, of the individual, of the individual concepts, of the individual, of the individual concepts, of the individual concepts, of the individual concepts, of the individual, of the individual concepts of the individual concepts, of the individual concepts, of the individual, of the individual concepts of the individual, of the individual concepts, of the individual concepts, of the individual, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual, of the individual concepts, of the individual concepts, of the individual concepts, of the individual, of the individual concepts, of the individual concepts, of the individual concepts, of the individual, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual, of the individual concepts, of the individual concepts, of the individual, of the individual concepts, of the individual concepts, of the individual concepts, of the individual, of the individual, of the individual concepts, of the individual, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual, of the individual concepts,"}, {"heading": "2.5. Training Techniques", "text": "In order to train the generator G (\u00b7), which contains discrete variables, the direct application of gradient estimation can be difficult (Yu et al., 2017). Scorpio function-based approaches, such as the REINFORCE algorithm (Williams, 1992), achieve an unbiased gradient estimate for discrete variables using Monte Carlo estimation. However, in our experiments we found that the variance of gradient estimation is very large, which is in line with Maddison et al. (2017). Here, we consider a Soft-Argmax operator (Zhang et al., 2016), similar to the Gumbel Softmax (Gumbel & Lieblein, 1954; Jang et al.) when performing the learning, as an approximation of (7): yt \u2212 1 = Wesoftmax (Vht \u2212 1 L)."}, {"heading": "3. Related Work", "text": "In fact, it is so that most of them are able to survive themselves if they do not abide by the rules. (...) In fact, it is so that they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) \"(...)\" (...) (...) () (() () () () () () () () () () () () () () () ()) () () ()) () () () ()) () () () ()) () () ()) () ()) () () () ()) () () ()) () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () (() () () (() () () (() () () () (() (() (() (() () () (() ((() (() () () (() (() () (() () () ((() () ((() () ((() (() () () (() (() () (((() () (((()) () ((("}, {"heading": "4. Experiments", "text": "We are the only ones dealing with the question of whether the model can also generate sentences that integrate both scientific and informal spellings. We randomly select 0.5 million sentences from arXiv's abstracts and 0.5 million sentences from arXiv to construct a series of teaching and validation sets, i.e. 1 million sentences for each one we randomly select, for a total of 50,000 sentences from BookCorpus and 0.5 million sentences from arXiv. We train the way in which the LSTM typically includes more parameters and is more difficult than the CNN sentences from both corpus, for a total of 50,000 sentences. We train the Sentencesi and discriminator / encoder iterative."}, {"heading": "5. Conclusion", "text": "We have introduced a novel approach to text generation using hostile training, called TextGAN, and discussed several techniques to specify and train such a model.We have shown that the proposed model performs better than similar approaches, can generate realistic sentences, and that the learned latent representation space can encode plausible sentences \"smoothly.\" We evaluate the proposed methods quantitatively using basic models and existing methods.The results suggest a superior performance of TextGAN. In future work, we will try to apply conditional GAN models (Mirza & Osindero, 2014) to untangle the latent representations for different writing styles.This would allow a smooth lexical and grammatical transition between different writing styles.It would also be interesting to generate text by conditioning observed images (Pu et al., 2016).In addition, we plan to create an additional sequence of refinement in Schmidt's first set (after generating an initial sequence of refinement in M)."}, {"heading": "Acknowledgments", "text": "This research was supported by ARO, DARPA, DOE, NGA, ONR and NSF."}], "references": [{"title": "Towards principled methods for training generative adversarial networks", "author": ["Arjovsky", "Martin", "Bottou", "L\u00e9on"], "venue": "In ICLR,", "citeRegEx": "Arjovsky et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Arjovsky et al\\.", "year": 2017}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio", "Samy", "Vinyals", "Oriol", "Jaitly", "Navdeep", "Shazeer", "Noam"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Generating sentences from a continuous space", "author": ["Bowman", "Samuel R", "Vilnis", "Luke", "Vinyals", "Oriol", "Dai", "Andrew M", "Jozefowicz", "Rafal", "Bengio", "Samy"], "venue": "In CoNLL,", "citeRegEx": "Bowman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Chen", "Xi", "Duan", "Yan", "Houthooft", "Rein", "Schulman", "John", "Sutskever", "Ilya", "Abbeel", "Pieter"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "In JMLR,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Adversarial feature learning", "author": ["Donahue", "Jeff", "Kr\u00e4henb\u00fchl", "Philipp", "Darrell", "Trevor"], "venue": "In ICLR,", "citeRegEx": "Donahue et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2017}, {"title": "Training generative neural networks via maximum mean discrepancy optimization", "author": ["Dziugaite", "Gintare Karolina", "Roy", "Daniel M", "Ghahramani", "Zoubin"], "venue": null, "citeRegEx": "Dziugaite et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dziugaite et al\\.", "year": 2015}, {"title": "Unsupervised learning of sentence representations using convolutional neural networks", "author": ["Gan", "Zhe", "Pu", "Yunchen", "Henao", "Ricardo", "Li", "Chunyuan", "He", "Xiaodong", "Carin", "Lawrence"], "venue": "arXiv preprint arXiv:1611.07897,", "citeRegEx": "Gan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gan et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Graves", "Alex", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "A kernel two-sample test", "author": ["Gretton", "Arthur", "Borgwardt", "Karsten M", "Rasch", "Malte J", "Sch\u00f6lkopf", "Bernhard", "Smola", "Alexander"], "venue": null, "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Statistical theory of extreme values and some practical applications: a series of lectures", "author": ["Gumbel", "Emil Julius", "Lieblein", "Julius"], "venue": null, "citeRegEx": "Gumbel et al\\.,? \\Q1954\\E", "shortCiteRegEx": "Gumbel et al\\.", "year": 1954}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "In Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["B. Hu", "Z. Lu", "H. Li", "Q. Chen"], "venue": "In NIPS,", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "How (not) to train your generative model: Scheduled sampling, likelihood, adversary", "author": ["Husz\u00e1r", "Ferenc"], "venue": null, "citeRegEx": "Husz\u00e1r and Ferenc.,? \\Q2015\\E", "shortCiteRegEx": "Husz\u00e1r and Ferenc.", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In ICML,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Categorical reparameterization with gumbel-softmax", "author": ["Jang", "Eric", "Gu", "Shixiang", "Poole", "Ben"], "venue": "In ICLR,", "citeRegEx": "Jang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Jang et al\\.", "year": 2017}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["R. Johnson", "T. Zhang"], "venue": "In NAACL HLT,", "citeRegEx": "Johnson and Zhang,? \\Q2015\\E", "shortCiteRegEx": "Johnson and Zhang", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "In ACL,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "In EMNLP,", "citeRegEx": "Kim,? \\Q2014\\E", "shortCiteRegEx": "Kim", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Professor forcing: A new algorithm for training recurrent networks", "author": ["Lamb", "Alex M", "GOYAL", "Anirudh Goyal ALIAS PARTH", "Zhang", "Ying", "Saizheng", "Courville", "Aaron C", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Lamb et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lamb et al\\.", "year": 2016}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["Larsen", "Anders Boesen Lindbo", "S\u00f8nderby", "S\u00f8ren Kaae", "Larochelle", "Hugo", "Winther", "Ole"], "venue": null, "citeRegEx": "Larsen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Larsen et al\\.", "year": 2016}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Li", "Jiwei", "Monroe", "Will", "Ritter", "Alan", "Galley", "Michel", "Gao", "Jianfeng", "Jurafsky", "Dan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Adversarial learning for neural dialogue generation", "author": ["Li", "Jiwei", "Monroe", "Will", "Shi", "Tianlin", "Ritter", "Alan", "Jurafsky", "Dan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Generative moment matching networks", "author": ["Li", "Yujia", "Swersky", "Kevin", "Zemel", "Richard S"], "venue": "In ICML,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A kernelized stein discrepancy for goodness-of-fit tests", "author": ["Liu", "Qiang", "Lee", "Jason D", "Jordan", "Michael I"], "venue": "In ICML,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Visualizing data using t-sne", "author": ["Maaten", "Laurens van der", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "The concrete distribution: A continuous relaxation of discrete random variables", "author": ["Maddison", "Chris J", "Mnih", "Andriy", "Teh", "Yee Whye"], "venue": "In ICLR,", "citeRegEx": "Maddison et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Maddison et al\\.", "year": 2017}, {"title": "Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks", "author": ["Mescheder", "Lars", "Nowozin", "Sebastian", "Geiger", "Andreas"], "venue": "In ICML,", "citeRegEx": "Mescheder et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mescheder et al\\.", "year": 2017}, {"title": "Unrolled generative adversarial networks", "author": ["Metz", "Luke", "Poole", "Ben", "Pfau", "David", "Sohl-Dickstein", "Jascha"], "venue": "In ICLR,", "citeRegEx": "Metz et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Metz et al\\.", "year": 2017}, {"title": "Conditional generative adversarial nets", "author": ["Mirza", "Mehdi", "Osindero", "Simon"], "venue": null, "citeRegEx": "Mirza et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mirza et al\\.", "year": 2014}, {"title": "f-gan: Training generative neural samplers using variational divergence minimization", "author": ["Nowozin", "Sebastian", "Cseke", "Botond", "Tomioka", "Ryota"], "venue": "In NIPS,", "citeRegEx": "Nowozin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nowozin et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni", "Kishore", "Roukos", "Salim", "Ward", "Todd", "Zhu", "Wei-Jing"], "venue": "In ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Variational autoencoder for deep learning of images, labels and captions", "author": ["Pu", "Yunchen", "Gan", "Zhe", "Henao", "Ricardo", "Yuan", "Xin", "Li", "Chunyuan", "Stevens", "Andrew", "Carin", "Lawrence"], "venue": null, "citeRegEx": "Pu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pu et al\\.", "year": 2016}, {"title": "On the high-dimensional power of linear-time kernel two-sample testing under mean-difference alternatives", "author": ["Ramdas", "Aaditya", "Reddi", "Sashank J", "Poczos", "Barnabas", "Singh", "Aarti", "Wasserman", "Larry"], "venue": null, "citeRegEx": "Ramdas et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ramdas et al\\.", "year": 2014}, {"title": "Improved techniques for training gans", "author": ["Salimans", "Tim", "Goodfellow", "Ian", "Zaremba", "Wojciech", "Cheung", "Vicki", "Radford", "Alec", "Chen", "Xi"], "venue": "In NIPS,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q. Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A note on the evaluation of generative models", "author": ["Theis", "Lucas", "Oord", "A\u00e4ron van den", "Bethge", "Matthias"], "venue": "In ICLR,", "citeRegEx": "Theis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2016}, {"title": "Learning to draw samples: With application to amortized mle for generative adversarial learning", "author": ["Wang", "Dilin", "Liu", "Qiang"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Seqgan: sequence generative adversarial nets with policy gradient", "author": ["Yu", "Lantao", "Zhang", "Weinan", "Wang", "Jun", "Yong"], "venue": "In AAAI,", "citeRegEx": "Yu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2017}, {"title": "Generating text via adversarial training", "author": ["Zhang", "Yizhe", "Gan", "Zhe", "Carin", "Lawrence"], "venue": "In NIPS Workshop on Adversarial Training,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Energy-based generative adversarial network", "author": ["Zhao", "Junbo", "Mathieu", "Michael", "LeCun", "Yann"], "venue": "In ICLR,", "citeRegEx": "Zhao et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2017}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Y. Zhu", "R. Kiros", "R. Zemel", "R. Salakhutdinov", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "In ICCV,", "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 25, "context": "For instance, in the context of dialog generation, it is desirable to generate answers that are more diverse and less generic (Li et al., 2016).", "startOffset": 126, "endOffset": 143}, {"referenceID": 4, "context": "One simple approach consists of first learning a latent space to represent (fixed-length) sentences using an encoderdecoder (autoencoder) framework based on Recurrent Neural Networks (RNNs) (Cho et al., 2014; Sutskever et al., 2014), then generate synthetic sentences by decoding ran-", "startOffset": 190, "endOffset": 232}, {"referenceID": 39, "context": "One simple approach consists of first learning a latent space to represent (fixed-length) sentences using an encoderdecoder (autoencoder) framework based on Recurrent Neural Networks (RNNs) (Cho et al., 2014; Sutskever et al., 2014), then generate synthetic sentences by decoding ran-", "startOffset": 190, "endOffset": 232}, {"referenceID": 2, "context": "The reason for this is that, when mapping sentences to their latent representations using an autoencoder, the mappings usually cover a small but structured region of the latent space, which corresponds to a manifold embedding (Bowman et al., 2016).", "startOffset": 226, "endOffset": 247}, {"referenceID": 2, "context": "The reason for this is that, when mapping sentences to their latent representations using an autoencoder, the mappings usually cover a small but structured region of the latent space, which corresponds to a manifold embedding (Bowman et al., 2016). In practice, most regions of the latent space do not necessarily map (decode) to realistic sentences. Consequently, randomly sampling latent representations often yields nonsensical sentences. Recent work by Bowman et al. (2016) has attempted to generate more diverse sentences via RNN-based variational autoencoders.", "startOffset": 227, "endOffset": 478}, {"referenceID": 1, "context": "Bengio et al. (2015) coined this phenomenon exposure bias.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Bengio et al. (2015) coined this phenomenon exposure bias. Toward addressing this problem, Bengio et al. (2015) proposed the scheduled sampling approach.", "startOffset": 0, "endOffset": 112}, {"referenceID": 1, "context": "Bengio et al. (2015) coined this phenomenon exposure bias. Toward addressing this problem, Bengio et al. (2015) proposed the scheduled sampling approach. However, Husz\u00e1r (2015) showed that scheduled sampling is a fundamentally inconsistent training strategy, in that it produces largely unstable results in practice.", "startOffset": 0, "endOffset": 177}, {"referenceID": 9, "context": "The Generative Adversarial Network (GAN) (Goodfellow et al., 2014) is an appealing and natural answer to the above issues.", "startOffset": 41, "endOffset": 66}, {"referenceID": 23, "context": "Recent work (Lamb et al., 2016) has incorporated an additional discriminator to train a sequence-to-sequence language model that better preserves ar X iv :1 70 6.", "startOffset": 12, "endOffset": 31}, {"referenceID": 32, "context": ", mode collapsing (Metz et al., 2017), and (ii) the generator\u2019s contribution to the learning signal is insubstantial when the discriminator is close to its local optimum, i.", "startOffset": 18, "endOffset": 37}, {"referenceID": 39, "context": "For instance, by borrowing ideas from reinforcement learning, Yu et al. (2017); Li et al.", "startOffset": 62, "endOffset": 79}, {"referenceID": 25, "context": "(2017); Li et al. (2017) treat the sentence generation as a sequential decision making process.", "startOffset": 8, "endOffset": 25}, {"referenceID": 20, "context": "Specifically, the Long ShortTerm Memory (LSTM) (Hochreiter & Schmidhuber, 1997) RNN is used as generator, and the Convolutional Neural Network (CNN) (Kim, 2014) is used as discriminator.", "startOffset": 149, "endOffset": 160}, {"referenceID": 9, "context": "GAN (Goodfellow et al., 2014) aims to obtain the equilibrium of the following optimization objective", "startOffset": 4, "endOffset": 29}, {"referenceID": 9, "context": "When the discriminator is optimal, solving this adversarial game is equivalent to minimizing the Jenson-Shannon Divergence (JSD) (Arjovsky & Bottou, 2017) between the real data distribution px(\u00b7) and the synthetic data distribution px\u0303(\u00b7) , p(G(z)) , where z \u223c pz(\u00b7) (Goodfellow et al., 2014).", "startOffset": 267, "endOffset": 292}, {"referenceID": 45, "context": "This problem also exists in the recently proposed energy-based GAN (EBGAN) (Zhao et al., 2017), as the distance metric implied by EBGAN is the Total Variance Distance (TVD), which has the same issue w.", "startOffset": 75, "endOffset": 94}, {"referenceID": 0, "context": "JSD, as shown by Arjovsky et al. (2017).", "startOffset": 17, "endOffset": 40}, {"referenceID": 38, "context": "Given a sentence corpus S , instead of directly optimizing the objective from standard GAN in (1), we adopt an approach that is similar to the feature matching scheme of Salimans et al. (2016). Specifically, we consider the objective", "startOffset": 170, "endOffset": 193}, {"referenceID": 11, "context": "LMMD2 represents the Maximum Mean Discrepancy (MMD) (Gretton et al., 2012) between the empirical distribution of sentence embeddings f\u0303 and f , for synthetic and real data,", "startOffset": 52, "endOffset": 74}, {"referenceID": 11, "context": "The kernel can be written as an inner product overH: k(x, x\u2032) = \u3008k(x, \u00b7), k(x\u2032, \u00b7)\u3009H, and \u03c6(x) , k(x, \u00b7) \u2208 H is denoted as the feature mapping (Gretton et al., 2012).", "startOffset": 143, "endOffset": 165}, {"referenceID": 11, "context": "With a universal kernel like the Gaussian kernel, k(x, y) = exp(\u2212 ||x\u2212y|| 2 2\u03c3 ), with bandwidth \u03c3, minimizing the MMD objective will match moments of all orders (Gretton et al., 2012).", "startOffset": 162, "endOffset": 184}, {"referenceID": 32, "context": "The original GAN objective has been shown to be prone to mode collapsing, especially when the so-called logD alternative for the generator loss is used (Metz et al., 2017), i.", "startOffset": 152, "endOffset": 171}, {"referenceID": 0, "context": "In fact, if the kernel function is universal, the MMD metric will be no worse than TVD in terms of vanishing gradients (Arjovsky et al., 2017).", "startOffset": 119, "endOffset": 142}, {"referenceID": 0, "context": "However, if the bandwidth of the kernel is too small, much smaller than the average distance between data points, the vanishing gradient problem remains (Arjovsky et al., 2017).", "startOffset": 153, "endOffset": 176}, {"referenceID": 10, "context": "As shown in Gretton et al. (2012), the MMD is a proper metric when the kernel is universal.", "startOffset": 12, "endOffset": 34}, {"referenceID": 37, "context": "In fact, a reliable Gaussian kernel MMD twosample test generally requires the size of the minibatch to be proportional to the number of dimensions (Ramdas et al., 2014).", "startOffset": 147, "endOffset": 168}, {"referenceID": 38, "context": "By setting \u03a3\u0303 = \u03a3 = I, (5) reduces to the first-moment feature matching technique from Salimans et al. (2016). Note that this loss L G is an upper bound of the JSD (omitting constant, proved in the Supplementary Material) between two multivariate Gaussian distribution N (\u03bc,\u03a3) and This", "startOffset": 87, "endOffset": 110}, {"referenceID": 19, "context": "CNN discriminator We use the CNN architecture in Kim (2014); Collobert et al.", "startOffset": 49, "endOffset": 60}, {"referenceID": 5, "context": "CNN discriminator We use the CNN architecture in Kim (2014); Collobert et al. (2011) for sentence encoding.", "startOffset": 61, "endOffset": 85}, {"referenceID": 5, "context": "Following Collobert et al. (2011), we induce a latent feature map c = \u03b3(X\u2217Wc+b) \u2208 RT\u2212h+1, where \u03b3(\u00b7) is a nonlinear activation function (we use the hyperbolic tangent, tanh), b \u2208 RT\u2212h+1 is a bias vector,", "startOffset": 10, "endOffset": 34}, {"referenceID": 5, "context": "We then apply a max-over-time pooling operation (Collobert et al., 2011) to the feature map and take its maximum value, i.", "startOffset": 48, "endOffset": 72}, {"referenceID": 19, "context": "There are other CNN architectures in the literature (Kalchbrenner et al., 2014; Hu et al., 2014; Johnson & Zhang, 2015).", "startOffset": 52, "endOffset": 119}, {"referenceID": 14, "context": "There are other CNN architectures in the literature (Kalchbrenner et al., 2014; Hu et al., 2014; Johnson & Zhang, 2015).", "startOffset": 52, "endOffset": 119}, {"referenceID": 13, "context": ", 2014; Hu et al., 2014; Johnson & Zhang, 2015). We adopt the CNN model of Kim (2014); Collobert et al.", "startOffset": 8, "endOffset": 86}, {"referenceID": 5, "context": "We adopt the CNN model of Kim (2014); Collobert et al. (2011) due to its simplicity and excellent performance on sentence classification tasks.", "startOffset": 38, "endOffset": 62}, {"referenceID": 43, "context": "Soft-argmax approximation To train the generator G(\u00b7), which contains discrete variables, direct application of the gradient estimation may be difficult (Yu et al., 2017).", "startOffset": 153, "endOffset": 170}, {"referenceID": 44, "context": "Here we consider a soft-argmax operator (Zhang et al., 2016), similar to the Gumbel-softmax (Gumbel & Lieblein, 1954; Jang et al.", "startOffset": 40, "endOffset": 60}, {"referenceID": 17, "context": ", 2016), similar to the Gumbel-softmax (Gumbel & Lieblein, 1954; Jang et al., 2017), when performing learning, as an approximation to (7):", "startOffset": 39, "endOffset": 83}, {"referenceID": 29, "context": "However, in our experiments, we found that the variance of the gradient estimation is very large, which is consistent with Maddison et al. (2017). Here we consider a soft-argmax operator (Zhang et al.", "startOffset": 123, "endOffset": 146}, {"referenceID": 9, "context": "Pre-training Previous literature (Goodfellow et al., 2014; Salimans et al., 2016) has discussed the fundamental difficulty of training GANs using gradient-based methods.", "startOffset": 33, "endOffset": 81}, {"referenceID": 38, "context": "Pre-training Previous literature (Goodfellow et al., 2014; Salimans et al., 2016) has discussed the fundamental difficulty of training GANs using gradient-based methods.", "startOffset": 33, "endOffset": 81}, {"referenceID": 38, "context": "In general, gradient descent optimization schemes may fail to converge to the equilibrium by moving along the orbit trajectory among saddle points (Salimans et al., 2016).", "startOffset": 147, "endOffset": 170}, {"referenceID": 8, "context": "Toward this end, we initialize the LSTM parameters of the generator by pre-training a standard CNN-LSTM autoencoder (Gan et al., 2016).", "startOffset": 116, "endOffset": 134}, {"referenceID": 38, "context": "We also utilized other training techniques to stabilize training, such as soft-labeling (Salimans et al., 2016).", "startOffset": 88, "endOffset": 111}, {"referenceID": 7, "context": "Generative Moment Matching Networks (GMMNs) (Dziugaite et al., 2015; Li et al., 2015) are closely related to our approach.", "startOffset": 44, "endOffset": 85}, {"referenceID": 27, "context": "Generative Moment Matching Networks (GMMNs) (Dziugaite et al., 2015; Li et al., 2015) are closely related to our approach.", "startOffset": 44, "endOffset": 85}, {"referenceID": 27, "context": "However, these methods either directly match the empirical distribution in the data domain, or extract features using a pre-trained autoencoder (Li et al., 2015).", "startOffset": 144, "endOffset": 161}, {"referenceID": 37, "context": "Note that the minibatch size required to obtain reasonable statistical power grows linearly with the number of dimension (Ramdas et al., 2014), and the computational cost of MMD grows quadratically with the size of data points.", "startOffset": 121, "endOffset": 142}, {"referenceID": 25, "context": "A two-step method, where a feature encoder is generated first as in Li et al. (2015) helps alleviate the problems above.", "startOffset": 68, "endOffset": 85}, {"referenceID": 25, "context": "A two-step method, where a feature encoder is generated first as in Li et al. (2015) helps alleviate the problems above. However, in Li et al. (2015) the feature encoder is fixed once pre-trained, limiting the potential to adjust features during the training phase.", "startOffset": 68, "endOffset": 150}, {"referenceID": 25, "context": "A two-step method, where a feature encoder is generated first as in Li et al. (2015) helps alleviate the problems above. However, in Li et al. (2015) the feature encoder is fixed once pre-trained, limiting the potential to adjust features during the training phase. Alternatively, our approach matches the real and synthetic data on a sentence feature space, where features are dynamically and adversarially adapted to focus on the most challenging features for the generator to mimic. In addition, features are designed to maintain both discrimination and reconstruction ability, instead of merely focusing on reconstruction as in Li et al. (2015).", "startOffset": 68, "endOffset": 649}, {"referenceID": 45, "context": "Recent work considered combining autoencoders or variational autoencoders (Kingma & Welling, 2014) with GAN (Zhao et al., 2017; Larsen et al., 2016; Makhzani et al., 2015; Mescheder et al., 2017; Wang & Liu, 2016).", "startOffset": 108, "endOffset": 213}, {"referenceID": 24, "context": "Recent work considered combining autoencoders or variational autoencoders (Kingma & Welling, 2014) with GAN (Zhao et al., 2017; Larsen et al., 2016; Makhzani et al., 2015; Mescheder et al., 2017; Wang & Liu, 2016).", "startOffset": 108, "endOffset": 213}, {"referenceID": 31, "context": "Recent work considered combining autoencoders or variational autoencoders (Kingma & Welling, 2014) with GAN (Zhao et al., 2017; Larsen et al., 2016; Makhzani et al., 2015; Mescheder et al., 2017; Wang & Liu, 2016).", "startOffset": 108, "endOffset": 213}, {"referenceID": 5, "context": "Donahue et al. (2017) learned a reverse mapping from data space to latent space.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "Chen et al. (2016) maximized the mutual information between the generated data and the latent codes by leveraging a network-adapted variational proposal distribution.", "startOffset": 0, "endOffset": 19}, {"referenceID": 28, "context": "Aside from MMD, kernel-based discrepancy metrics such as kernelized Stein discrepancy (Liu et al., 2016; Wang & Liu, 2016) have been shown to be computationally tractable, while maintaining statistical power.", "startOffset": 86, "endOffset": 122}, {"referenceID": 0, "context": "Wasserstein GAN (Arjovsky et al., 2017) considers an Earth-Mover (EM) distance of the real data and synthetic data distribution, instead of the JSD as in standard GAN (Goodfellow et al.", "startOffset": 16, "endOffset": 39}, {"referenceID": 9, "context": ", 2017) considers an Earth-Mover (EM) distance of the real data and synthetic data distribution, instead of the JSD as in standard GAN (Goodfellow et al., 2014) or TVD as in Zhao et al.", "startOffset": 135, "endOffset": 160}, {"referenceID": 0, "context": "Wasserstein GAN (Arjovsky et al., 2017) considers an Earth-Mover (EM) distance of the real data and synthetic data distribution, instead of the JSD as in standard GAN (Goodfellow et al., 2014) or TVD as in Zhao et al. (2017). The EM metric yields stable gradients, thus avoiding the collapsing mode and vanishing gradient problem of the latter two.", "startOffset": 17, "endOffset": 225}, {"referenceID": 0, "context": "Wasserstein GAN (Arjovsky et al., 2017) considers an Earth-Mover (EM) distance of the real data and synthetic data distribution, instead of the JSD as in standard GAN (Goodfellow et al., 2014) or TVD as in Zhao et al. (2017). The EM metric yields stable gradients, thus avoiding the collapsing mode and vanishing gradient problem of the latter two. We note that our approach is equivalent to minimizing a MMD loss over the data domain, however, with a NNbased embedded Gaussian kernel. As shown in Arjovsky et al. (2017), MMD is a proper metric when the kernel is universal.", "startOffset": 17, "endOffset": 521}, {"referenceID": 46, "context": "Data and Experimental Setup Our model is trained using a combination of two datasets: (i) the BookCorpus dataset (Zhu et al., 2015), which consists of 70 million sentences from over 7000 books; and (ii) the ArXiv dataset, which consists of 5 million sentences from abstracts of papers from various subjects, obtained from the arXiv website.", "startOffset": 113, "endOffset": 131}, {"referenceID": 35, "context": "The validation performance is evaluated by loss of generator and corpus-level BLEU score (Papineni et al., 2002), described below.", "startOffset": 89, "endOffset": 112}, {"referenceID": 25, "context": "We use a mixture of 5 isotropic Gaussian (RBF) kernels with different bandwidths \u03c3 as in Li et al. (2015). Bandwidth parameters are selected to be close to the median distance (in our case around 20) of feature vectors encoded from real sentences.", "startOffset": 89, "endOffset": 106}, {"referenceID": 39, "context": "Gradients are clipped if the norm of the parameter vector exceeds 5 (Sutskever et al., 2014).", "startOffset": 68, "endOffset": 92}, {"referenceID": 38, "context": ", ||Ef\u2212Ef\u0303 ||, as in Salimans et al. (2016). Further details of the experimental design are provided in the the Supplementary Material.", "startOffset": 21, "endOffset": 44}, {"referenceID": 35, "context": "Quantitative comparison We evaluate the generatedsentence quality using the BLEU score (Papineni et al., 2002) and Kernel Density Estimation (KDE), as in Goodfellow et al.", "startOffset": 87, "endOffset": 110}, {"referenceID": 43, "context": "We also compare with seqGAN (Yu et al., 2017).", "startOffset": 28, "endOffset": 45}, {"referenceID": 8, "context": ", 2002) and Kernel Density Estimation (KDE), as in Goodfellow et al. (2014); Nowozin et al.", "startOffset": 51, "endOffset": 76}, {"referenceID": 8, "context": ", 2002) and Kernel Density Estimation (KDE), as in Goodfellow et al. (2014); Nowozin et al. (2016). For comparison, we consider textGAN with 4 different loss objectives: Mean Matching (MM) as in Salimans et al.", "startOffset": 51, "endOffset": 99}, {"referenceID": 8, "context": ", 2002) and Kernel Density Estimation (KDE), as in Goodfellow et al. (2014); Nowozin et al. (2016). For comparison, we consider textGAN with 4 different loss objectives: Mean Matching (MM) as in Salimans et al. (2016), Covariance Matching (CM) as in (5), MMD and MMD with compressed network (MMD-L), by mapping the original 900-dimensional features to 200-dimensional, as described in Section 2.", "startOffset": 51, "endOffset": 218}, {"referenceID": 2, "context": "We finally consider a Variational Autoencoder (VAE) implemented as in Bowman et al. (2016). To train the VAE model, we use annealing to gradually increase the KL divergence between the prior and approximated posterior.", "startOffset": 70, "endOffset": 91}, {"referenceID": 40, "context": "Despite the fact that the KDE approach, as a log-likelihood estimator tends to have high variance (Theis et al., 2016), the KDE score tracks well with our BLEU score evaluation.", "startOffset": 98, "endOffset": 118}, {"referenceID": 42, "context": "For BLEU score evaluation, we follow the strategy in Yu et al. (2017) of using the entire test set as the reference.", "startOffset": 53, "endOffset": 70}, {"referenceID": 43, "context": "One promising direction is to leverage reinforcement learning strategies as in Yu et al. (2017), where the updating for LSTM can be more effectively steered.", "startOffset": 79, "endOffset": 96}, {"referenceID": 2, "context": "Latent feature space trajectories Following Bowman et al. (2016), we further empirically evaluate whether the latent variable space can \u201cdensely\u201d encode sentences.", "startOffset": 44, "endOffset": 65}, {"referenceID": 36, "context": "It would be also interesting to generate text by conditioning on observed images (Pu et al., 2016).", "startOffset": 81, "endOffset": 98}], "year": 2017, "abstractText": "The Generative Adversarial Network (GAN) has achieved great success in generating realistic (realvalued) synthetic data. However, convergence issues and difficulties dealing with discrete data hinder the applicability of GAN to text. We propose a framework for generating realistic text via adversarial training. We employ a long shortterm memory network as generator, and a convolutional network as discriminator. Instead of using the standard objective of GAN, we propose matching the high-dimensional latent feature distributions of real and synthetic sentences, via a kernelized discrepancy metric. This eases adversarial training by alleviating the mode-collapsing problem. Our experiments show superior performance in quantitative evaluation, and demonstrate that our model can generate realistic-looking sentences.", "creator": "LaTeX with hyperref package"}}}