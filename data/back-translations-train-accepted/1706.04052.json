{"id": "1706.04052", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Beyond Monte Carlo Tree Search: Playing Go with Deep Alternative Neural Network and Long-Term Evaluation", "abstract": "Monte Carlo tree search (MCTS) is extremely popular in computer Go which determines each action by enormous simulations in a broad and deep search tree. However, human experts select most actions by pattern analysis and careful evaluation rather than brute search of millions of future nteractions. In this paper, we propose a computer Go system that follows experts way of thinking and playing. Our system consists of two parts. The first part is a novel deep alternative neural network (DANN) used to generate candidates of next move. Compared with existing deep convolutional neural network (DCNN), DANN inserts recurrent layer after each convolutional layer and stacks them in an alternative manner. We show such setting can preserve more contexts of local features and its evolutions which are beneficial for move prediction. The second part is a long-term evaluation (LTE) module used to provide a reliable evaluation of candidates rather than a single probability from move predictor. This is consistent with human experts nature of playing since they can foresee tens of steps to give an accurate estimation of candidates. In our system, for each candidate, LTE calculates a cumulative reward after several future interactions when local variations are settled. Combining criteria from the two parts, our system determines the optimal choice of next move. For more comprehensive experiments, we introduce a new professional Go dataset (PGD), consisting of 253233 professional records. Experiments on GoGoD and PGD datasets show the DANN can substantially improve performance of move prediction over pure DCNN. When combining LTE, our system outperforms most relevant approaches and open engines based on MCTS.", "histories": [["v1", "Tue, 13 Jun 2017 13:30:04 GMT  (9523kb,D)", "http://arxiv.org/abs/1706.04052v1", "AAAI 2017"]], "COMMENTS": "AAAI 2017", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE", "authors": ["jinzhuo wang", "wenmin wang", "ronggang wang", "wen gao 0001"], "accepted": true, "id": "1706.04052"}, "pdf": {"name": "1706.04052.pdf", "metadata": {"source": "CRF", "title": "Beyond Monte Carlo Tree Search: Playing Go with Deep Alternative Neural Network and Long-Term Evaluation", "authors": ["Jinzhuo Wang", "Wenmin Wang", "Ronggang Wang", "Wen Gao"], "emails": ["jzwang@pku.edu.cn,", "wangwm@ece.pku.edu.cn,", "rgwang@ece.pku.edu.cn,", "wgao@pku.edu.cn"], "sections": [{"heading": "Introduction", "text": "In fact, most of them are able to surpass themselves, both in terms of the way they move, in terms of the way they move, in terms of the way they move, in terms of the way they move, in terms of the way they move, in terms of the way they move."}, {"heading": "Related Work", "text": "It is a best-first search method based on random explorations of the search space, and is gradually getting better at accurately estimating the values of optimal movements (Bouzy and Helmstetter 2004) (Coulom 2006). Such programs have led to strong amateur performance, but there still remains a considerable gap between top professionals and the strongest computer programs. The majority of recent advances are due to increased quantity and quality of the previous knowledge used to distort the search toward more promising states, and this knowledge is widely believed to be the biggest bottleneck toward further advancement. The first successful current Go program (Kocsis and Szepesva \u0301 ri 2006) is based on MCTS. Its basic algorithms have been expanded in MoGo (Gelly and Silver 2007)."}, {"heading": "The Proposed Computer Go System", "text": "The proposed system is illustrated in Figure 1). Faced with a situation, we first obtain a probability distribution of the legal points, which we learn from a model-conscious predictive tool based on monitored policy lessons from existing professional experience. Then, we analyze these candidates with great confidence through a long-term evaluation, in order to aim for an expectation reward after several future steps once the local situation has been clarified. The measure with the highest combination of two criteria is our final choice."}, {"heading": "Deep Alternative Neural Network", "text": "Below we describe the structure of THEN including its key component (alternative layer) and the general architecture, and then discuss its relationships and advantages over popular DCNNs.Alternatively, the key component of THEN is the alternative layer (AL), which consists of a standard convolutional layer followed by a conventional layer followed by a designed recursive layer. Specifically, the convolution operation is first performed to extract characteristics from local neighborhoods on characteristic maps, then a recursive layer is applied to the output and iterative procedures for T-times. This procedure guides each unit through discrete time steps and aggregated larger receptive fields (RFs)."}, {"heading": "Long-Term Evaluation of Candidates", "text": "There is only a limited way to evaluate these candidates in a long-term perspective, as they can only predict the immediate next step, which limits the information received from the lower levels (Tian and Zhu 2016). Furthermore, many situations in intense combat or capture are far beyond fair evaluation and need to be accurately assessed when local variations are handled. We aim to avoid short-sighted movements. There is some work such as (Littman 1994) that sees games as a sequential decision process of a targeted agent interacting with visual environment. We expand this idea to evaluate candidates in a similar way. We calculate the cumulative rewards of each candidate with multiple future interactions. We achieve a definitive score and determine the optimal action model and internal state. Figure 3 shows our model structure, which an agent builds around a blind search space such as MCTS."}, {"heading": "Setup", "text": "The first data set we used is GoGoD (2015 winter version). The data set consists of 82, 609 historical and modern games. We limited our experiments to a subset of games that met the following criteria: 19 \u00d7 19 board games, modern (played after 1950), \"standard\" komi (komi), and no handicap stones. We did not differentiate between rule sets (most games followed Chinese or Japanese rules). Our criteria produced a training set of about 70,000 games. We used unpopular KGS datasets because it consists of more games by lower Dan players. The average level is about 5 dan.In addition, we have collected a new professional go dataset consisting of 253, 233 professional datasets that exceed GoGoGoGoD and KGS in quantity and playing strength."}, {"heading": "Move Prediction", "text": "Then we compare our best THEN model with DCNN-based methods. Finally, we examine the effects of long-term evaluation and report on the overall performance on motion forecast. There are two crucial configurations for THEN model. The first is the AL setting including its sequence and number. The second is the deployment time T in recursive layers. Comparative details are reported in Table 2, where B 6C 2FC represents a baseline of similar configuration with THEN, but the use of standard conversion layers instead of ALs. The first column of the left table in Table 2 has only one AL layer and the accuracy comparison shows the benefits of inserting AL in advance. We attribute it to context mining of lower properties. The fourth column of the left table in Table 2 shows the performance increases as the number of AL increases confirming the effectiveness of inserting recurrent layers."}, {"heading": "Playing Strength", "text": "Finally, we evaluate the overall gameplay strength of our system by playing against several publicly available benchmark programs, all of which were played with the strongest settings available and a fixed number of rollouts per turn. We used GnuGo 3.8 Level 10, MoGo (Gelly and Silver 2007), Pachi 11.99 (genjo-devel) with the sample files, and Fuego 1.1 during our experiments. For each setting, 3 groups of 100 games were played. We report on the average win rate and standard deviation calculated from the group averages. All game experiments mentioned in this paper used Komi 7.5 and Chinese rules. Consider (Look further if the opponent thinks) in Pachi and Fuego. As Table 3 shows, our system outperforms most MCTS-based Go programs, and the win rate of our approach is higher than previous work except (Tian and Zhu 2016)."}, {"heading": "Conclusion", "text": "In this work, we have proposed a computer-go system based on a novel deep alternative neural network (THEN) and long-term evaluation (LTE), as well as a new dataset consisting of approximately 25k professional datasets. On two datasets, we have shown that THEN can predict the next step of go professionals with an accuracy that significantly exceeds previous methods of deep Convolutionary Neural Network (DCNN). LTE strategy can further improve the quality of candidate selection by combining the impact of future interaction rather than immediate reward. Without a crude simulation of possible interactions in a large and deep search space, our system is capable of outperforming most MCTS-based open source go programs."}, {"heading": "Acknowledgement", "text": "This work was supported by the Shenzhen Peacock Plan (20130408-183003656)."}], "references": [{"title": "and Gailly", "author": ["P. Baudi\u0161"], "venue": "J.-l.", "citeRegEx": "Baudi\u0161 and Gailly 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Helmstetter", "author": ["B. Bouzy"], "venue": "B.", "citeRegEx": "Bouzy and Helmstetter 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "P", "author": ["C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "Cowling"], "venue": "I.; Rohlfshagen, P.; Tavener, S.; Perez, D.; Samothrakis, S.; and Colton, S.", "citeRegEx": "Browne et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Storkey", "author": ["C. Clark"], "venue": "A.", "citeRegEx": "Clark and Storkey 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Kavukcuoglu Collobert", "R. Farabet 2011] Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Fuego- an open-source framework for board games and go engine based on monte carlo tree search", "author": ["Enzenberger"], "venue": "Computational Intelligence and AI in Games, IEEE Transactions on 2(4):259\u2013270", "citeRegEx": "Enzenberger,? \\Q2010\\E", "shortCiteRegEx": "Enzenberger", "year": 2010}, {"title": "and Silver", "author": ["S. Gelly"], "venue": "D.", "citeRegEx": "Gelly and Silver 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "and Silver", "author": ["S. Gelly"], "venue": "D.", "citeRegEx": "Gelly and Silver 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and M\u00fcller", "author": ["Huang", "S.-C."], "venue": "M.", "citeRegEx": "Huang and M\u00fcller 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Szepesv\u00e1ri", "author": ["L. Kocsis"], "venue": "C.", "citeRegEx": "Kocsis and Szepesv\u00e1ri 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "G", "author": ["A. Krizhevsky", "I. Sutskever", "Hinton"], "venue": "E.", "citeRegEx": "Krizhevsky. Sutskever. and Hinton 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "M", "author": ["Littman"], "venue": "L.", "citeRegEx": "Littman 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "C", "author": ["Maddison"], "venue": "J.; Huang, A.; Sutskever, I.; and Silver, D.", "citeRegEx": "Maddison et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "D", "author": ["Richards, N.", "Moriarty"], "venue": "E.; and Miikkulainen, R.", "citeRegEx": "Richards. Moriarty. and Miikkulainen 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Current frontiers in computer go", "author": ["Rimmel"], "venue": "Computational Intelligence and AI in Games, IEEE Transactions on 2(4):229\u2013238", "citeRegEx": "Rimmel,? \\Q2010\\E", "shortCiteRegEx": "Rimmel", "year": 2010}, {"title": "T", "author": ["N.N. Schraudolph", "P. Dayan", "Sejnowski"], "venue": "J.", "citeRegEx": "Schraudolph. Dayan. and Sejnowski 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "C", "author": ["D. Silver", "A. Huang", "Maddison"], "venue": "J.; Guez, A.; Sifre, L.; Van Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.; Panneershelvam, V.; Lanctot, M.; et al.", "citeRegEx": "Silver et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Zisserman", "author": ["K. Simonyan"], "venue": "A.", "citeRegEx": "Simonyan and Zisserman 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Nair", "author": ["I. Sutskever"], "venue": "V.", "citeRegEx": "Sutskever and Nair 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "S", "author": ["R.S. Sutton", "D.A. McAllester", "Singh"], "venue": "P.; Mansour, Y.; et al.", "citeRegEx": "Sutton et al. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Going deeper with convolutions", "author": ["Szegedy"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Szegedy", "year": 2015}, {"title": "and Zhu", "author": ["Y. Tian"], "venue": "Y.", "citeRegEx": "Tian and Zhu 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "P", "author": ["Werbos"], "venue": "J.", "citeRegEx": "Werbos 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "Solving deep memory pomdps with recurrent policy gradients", "author": ["Wierstra"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "Wierstra,? \\Q2007\\E", "shortCiteRegEx": "Wierstra", "year": 2007}, {"title": "R", "author": ["Williams"], "venue": "J.", "citeRegEx": "Williams 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "and M\u00fcller", "author": ["C. Xiao"], "venue": "M.", "citeRegEx": "Xiao and M\u00fcller 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [], "year": 2017, "abstractText": "Monte Carlo tree search (MCTS) is extremely popular in computer Go which determines each action by enormous simulations in a broad and deep search tree. However, human experts select most actions by pattern analysis and careful evaluation rather than brute search of millions of future interactions. In this paper, we propose a computer Go system that follows experts way of thinking and playing. Our system consists of two parts. The first part is a novel deep alternative neural network (DANN) used to generate candidates of next move. Compared with existing deep convolutional neural network (DCNN), DANN inserts recurrent layer after each convolutional layer and stacks them in an alternative manner. We show such setting can preserve more contexts of local features and its evolutions which are beneficial for move prediction. The second part is a long-term evaluation (LTE) module used to provide a reliable evaluation of candidates rather than a single probability from move predictor. This is consistent with human experts nature of playing since they can foresee tens of steps to give an accurate estimation of candidates. In our system, for each candidate, LTE calculates a cumulative reward after several future interactions when local variations are settled. Combining criteria from the two parts, our system determines the optimal choice of next move. For more comprehensive experiments, we introduce a new professional Go dataset (PGD), consisting of 253, 233 professional records. Experiments on GoGoD and PGD datasets show the DANN can substantially improve performance of move prediction over pure DCNN. When combining LTE, our system outperforms most relevant approaches and open engines based on", "creator": "LaTeX with hyperref package"}}}