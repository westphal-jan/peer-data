{"id": "1706.04097", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Provable Alternating Gradient Descent for Non-negative Matrix Factorization with Strong Correlations", "abstract": "Non-negative matrix factorization is a basic tool for decomposing data into the feature and weight matrices under non-negativity constraints, and in practice is often solved in the alternating minimization framework. However, it is unclear whether such algorithms can recover the ground-truth feature matrix when the weights for different features are highly correlated, which is common in applications. This paper proposes a simple and natural alternating gradient descent based algorithm, and shows that with a mild initialization it provably recovers the ground-truth in the presence of strong correlations. In most interesting cases, the correlation can be in the same order as the highest possible. Our analysis also reveals its several favorable features including robustness to noise. We complement our theoretical results with empirical studies on semi-synthetic datasets, demonstrating its advantage over several popular methods in recovering the ground-truth.", "histories": [["v1", "Tue, 13 Jun 2017 14:39:59 GMT  (1519kb,D)", "http://arxiv.org/abs/1706.04097v1", "Accepted to the International Conference on Machine Learning (ICML), 2017"]], "COMMENTS": "Accepted to the International Conference on Machine Learning (ICML), 2017", "reviews": [], "SUBJECTS": "cs.LG cs.DS cs.NA stat.ML", "authors": ["yuanzhi li", "yingyu liang"], "accepted": true, "id": "1706.04097"}, "pdf": {"name": "1706.04097.pdf", "metadata": {"source": "META", "title": "Provable Alternating Gradient Descent for Non-negative Matrix Factorization with Strong Correlations", "authors": ["Yuanzhi Li", "Yingyu Liang"], "emails": ["<yuanzhil@cs.princeton.edu>,", "<yingyul@cs.princeton.edu>."], "sections": [{"heading": "1. Introduction", "text": "In fact, it is not the case that it would act in a way in which people live in a country in which they are able to live and live in a world, to move in a world in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in a country, in which they are"}, {"heading": "1.1. Contributions", "text": "In this thesis we start from a generative model of the data points, since the ground truth attribute matrix A \u0445 is given. In each round we get y = A \u0445 x, 1, where x is usually sampled from an unknown distribution \u00b5 and the goal is to restore the ground truth attribute matrix A \u0445. We give an algorithm called AND, which starts from a mild initialization matrix and demonstrably converges to A \u0445 in polynomial time. We also justify the convergence by a sequence of experiments. Our algorithm shows the following favorable properties."}, {"heading": "1.1.1. SIMPLE GRADIENT DESCENT ALGORITHM", "text": "The AND algorithm runs in stages and maintains a working matrix A (t) at each stage. At the t-th iteration at one stage, after we have received a sample y, it executes the following: (Decode) z = \u03c6\u03b1 ((A (0) \u2020 y), (Update) A (t + 1) = A (t) + \u03b7 (yz > \u2212 A (t) zz >), where alpha is a threshold parameter, \u03c6\u03b1 (x) = {x, if x \u2265 \u03b1, 0 otherwise, (A (0) \u2020 is the Moore-Penrose pesudo-inverse of A (0), and \u03b7 is the refresh step aimed at restoring the appropriate weight for the data point, and the refresh step uses the decoded weight to refresh the function matrix. The final working matrix at one stage is used as A (0) at the next stage. See algorithm 1 for the details.At a high level, our feature matrix can be thought of updating."}, {"heading": "1.1.2. HANDLING STRONG CORRELATIONS", "text": "The most remarkable property of AND is that it has been proven to handle a highly correlated distribution \u00b5 to the weight x, which means that the coordinates of x can have very strong correlations to each other, which is important because such a correlated x is, of course, evident in practice. If, for example, a document contains the topic \"machine learning,\" it is more likely to include the topic \"computer science\" than \"geography\" (Lead & Lafferty, 2006).Most of the previous theoretical approaches to analyzing the alternation between decryption and coding, such as (Awasthi & Risteski, 2015; Li et al., 2016; Arora et al., 2015), require that the coordinates of x be dependent in pairs or almost pair independent (which E\u00b5 [xixj] \u2248 E\u00b5 [xj] E\u00b5 [xj])). In this paper, we show that even if the coordinates are highly correlated, the AND algorithm can restore an equilibrium."}, {"heading": "1.1.3. PSEUDO-INVERSE DECODING", "text": "One of the features of our algorithm is the use of Moore-Penrose pesudo-invers in decoding. Inverse decoding has also been used in (Li et al., 2016; Arora et al., 2015; 2016). However, their algorithms must carefully find a reversal so that certain standards are minimized, which is not as efficient as the vanilla Moore-Penrose pesudo-invers. It has also been observed in (Arora et al., 2016) that Moore-Penrose pesudo-invers work equally well in practice, but the experiment was only performed when A = A. In this paper we show that Moore-Penrose pesudo-invers also work well when A 6 = A, both theoretically and empirically. 1.1.4 THRESHOLDING AT DIFFERENT \u03b1Thresholding at a value \u03b1 > 0 is a common trick used in many algorithms."}, {"heading": "1.1.5. ROBUSTNESS TO NOISE", "text": "The algorithm can tolerate a general family of noise with limited moments; we present in the main part the result for a simplified case of Gaussian noise and return the general result in the appendix. The algorithm can restore the ground truth matrix up to a small blow-up factor times the sound level in each example if the basic truth has a good state number. This robustness is also supported by our experiments."}, {"heading": "2. Related Work", "text": "Non-negative matrix factorization has a rich empirical history, starting with the practical algorithms of (Lee & Seung, 1997; 1999; 2001) It has been widely used in applications, and various methods for NMF exist, e.g. (Kim & Park, 2008; Lee & Seung, 2001; Cichocki et al., 2007; Ding et al., 2013; 2014) However, they do not have detectable recovery guarantees. Theoretical analysis (Arora et al., 2012b) provided a fixed parameter tractable algorithm for NMF using algebraic equations. They also provided appropriate hardness results: namely, they show that there is no algorithm running in time (mW) o (D), unless there is a subexponential runtime algorithm for 3-SAT. (Arora et al, 2012b) also investigated NMF under the assumptions of separability."}, {"heading": "3. Problem and Definitions", "text": "The goal is to restore the basic truth in which the data is generated from a matrix. (A) The maximum (minimum) basic truth characteristic matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix-matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matx matrix matrix matrix matrix matrix matrix matrix matrix matx matrix matrix matrix matrix matx matx matx matx matx matx matrix matrix matrix matx matrix matrix"}, {"heading": "4. Algorithm", "text": "The algorithm is formally described in algorithm 1. It runs in s-levels and in the j-level it uses the same threshold \u03b1j and the same matrix A (0) for decoding, where A (0) is either the input initialization matrix or the working matrix obtained at the end of the last stage. Each stage consists of T-iterations, and each iteration decodes a data point and uses the decoded result to update the working matrix. Instead of a data point, a series of data points can be used, and our analysis still holds. By running in stages, we save most of the cost of the calculation (A (0) \u2020, as our results show that only polylogarithmic stages are needed. In the simple case where x-levels are required, the algorithm can use the same threshold \u03b1 = 1 / 4 for all stages (see theorem 1), while for the general decreasing thresholds, see the levels needed for practice (see theorem 4)."}, {"heading": "5. Result for A Simplified Case", "text": "Theorem 1 (main, binary) For the generative model (5,1), there is \"=\" (1) in such a way that for each (r, k, m, \u03bb) GCC x and each > 0, algorithm AND with T = poly (D, 1), {\u03b1i} si = 1 = {14} s i = 1 for s = polylog (D, 1) and each > 0, algorithm AND with T = poly (D, 1) and an \"initialization matrix A0\" output a matrix A so that there is a diagonal matrix \u044512I with surface usage pole (D, 1)."}, {"heading": "5.1. Intuition", "text": "Intuition is based on the assumption that we have the \"correct decoding,\" i.e., we do not assume that A = 6 is correct if we have magic for each y (t), our decoding z (t) = \u03c6\u03b1j (A \u2020 y (t)). Here and in this subsection, A is an abbreviation for A (0), the gradient is A (t + 1) = A (t) = A (t) \u2212 A (t) \u2212 A (t) x (t) x (t) x (t) x (t)))) > Subtraction A on both sides, we become (t + 1) \u2212 A (t) \u2212 A (t) \u2212 A (t) (t) (t) x (t)) > Since x (t)) > this property is obtained on both sides, we (t) > are positively semidefined as long as E (t) (t) (t) (t) (t) >)."}, {"heading": "5.2. Proof Sketch", "text": "For the sake of simplicity, we focus only on one level and the expected update. The expected update of A (t + 1) is given by A (t + 1) = A (t) + \u03b7 (E) \u2212 A (t) \u00b7 E [zz >]). Let's write A (0) = A (n) \u00b7 0 + E0), where \"0\" is diagonal and \"E0\" is not diagonal. Then, the decoding of z \u2212 A (0) \u2020 y) = (n) (\"0 + E0) \u2212 1x) is written. Leave\" E \"the diagonal part and the diagonal part of (n) \u2212 1 \u2212 1.The key problem for the decoding is that\" z \"will be close to\" x in the following sense. Lemma 2 (decoding, informal) is \"E\" small and \"E\" is small. \"Then with a correct threshold of.\""}, {"heading": "6. More General Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Result for General x", "text": "This sub-section considers the general case in which x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "6.2. Robustness to Noise", "text": "We will now consider the case when the data is generated by y = A * x *, where \"A\" is noise. To demonstrate this, we will only focus on the case when xi \u00b2 0, 1 \"and\" A \"is random Gaussian noise. 5 A more general theorem can be found in the appendix. Definition 4 (,\" \"A\") initialization. The output matrix A0 fulfills for some \",\" A \"[0, 1), 1. A0 = A\" (\"A\") + N, \"for some diagonal and off-diagonal matrix E.2,\" \"E\" 2, \"\" I \"2,\" \"N\" 2. \"Theorem 5 (noise, binary). There is any xi \u00b2\" 0, 1. \"There is such an analysis that for each (r, k, m\") -GCC, each > 0, \"algorithm with\" D = 4y. \""}, {"heading": "7. Experiments", "text": "To demonstrate the advantage of AND, we supplement the theoretical analysis with empirical studies of semi-synthetic datasets in which we have soil truth characteristics and thus can verify convergence. We then provide support for the benefit of using decreasing thresholds and test their robustness to noise. However, in the appendix we test its soil truth and economy of x, and provide qualitative results in some real-world applications. Our work focuses on the convergence of soil truth characteristics. realworld datasets in general do not have ground-truth. So we construct semi-synthetic datasets in topic modeling: first take the word-topic matrix learned by some topic modeling method as the ground-truth A, and then draw x from some specific distribution.For fair comparison, we use one not learned by any algorithm evaluated here."}, {"heading": "7.1. Convergence to the Ground-Truth", "text": "Figure 1 shows the convergence rate of the algorithms on the three datasets. AND converges at a linear rate on all three datasets (note that the y-axis is in log scale). HALS converges on the datasets DIR and CTM, but convergence proceeds at a slower rate. Also, the error oscillates on CTM. Furthermore, it does not converge on NEG, where the Ground Truth Matrix has negative entries. ANLS converges on DIR and CTM at a very slow rate due to the non-negative smallest square calculation in each iteration. 9 All other Algo-9We also note that even the threshold of HALS and ALNS, which are designed for non-negative attribute matrices, is removed, they do not converge with the basic truth, indicating that they have no recovery guarantees."}, {"heading": "7.2. The Threshold Schemes", "text": "Figure 2 (a) shows the results of using different threshold schemes on DIR, while Figure 2 (b) shows that on CTM. Using a constant threshold for all iterations, the error decreases only for the first few steps and then stops sinking. This is consistent with our analysis and stands in stark contrast to the case with decreasing thresholds."}, {"heading": "7.3. Robustness to Noise", "text": "Figure 2 (c) shows the performance of AND on the NOISE dataset with different noise levels \u03b3. The error decreases in the first steps, but then stabilizes around a constant noise level as predicted by our analysis, showing that it can restore the basic truth with good accuracy, even if the data contains a significant amount of noise."}, {"heading": "Acknowledgements", "text": "This work was partially supported by NSF grants CCF1527371, DMS-1317308, Simons Investigator Award, Simons Collaboration Grant and ONR-N00014-16-1-2329."}, {"heading": "A. Complete Proofs", "text": "For the sake of simplicity, we focus only on one level and the expected update. The expected update of A (t) is given by A (t) (t + 1) = A (t) + \u03b7 (E [yz >] \u2212 A (t) E [zz >]). Let us write A = A * (t), where A + E0 is diagonal and E0 is not diagonal. \u2212 The first step of our analysis is a key dilemma for decoding. It states that under suitable conditions, z will be close to the erection in the following sense: E [xxx >], E [zx >], E [mmz >]."}, {"heading": "A.1. Decoding", "text": "A.1. xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-xi-"}, {"heading": "A.2. Update", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.2.1. GENERAL UPDATE LEMMA", "text": "Lemma 10 (Update). Let's assume the updated rule is diagonal and Et is not diagonal and Et is not diagonal for all. (Let's assume we have an updated rule given by both sides. (Let's assume we have an updated rule.) Let's accept. (Let's accept.) Let's accept. (Let's accept.) Let's accept. (Let's accept.). (Let's accept.) Let's accept. (Let's accept.) Let's accept. (Let's accept.) Let's accept. (Let's accept.) Let's accept. (Let's accept.). (Let's accept. (Let's accept.) Let's accept. (Let's accept.) Let's accept. (Let's accept.) Let's accept. (Let's accept. (Let's accept.). (Let's accept. (Let's accept.). (Let's accept. (Let's accept.). (Let's accept. (Let's accept.). (Let's accept. (Let's accept.). (Let's accept. (Let's accept.). (Let's accept."}, {"heading": "A.3. Proof of the Main Theorems", "text": "For the sake of simplicity, we focus only on the expected update. The on-line version can directly assume that the variance of the update is polynomially limited and accordingly sets a polynomially small \u03b7. The expected update of A (t + 1) = A (t + 1) + (E [yz >] \u2212 A (t) E [zz >]) Let us select A = 14, concentrate on one level and A = A (n) write (n). Then the decoding of A (t) = 2 x (n) is given."}, {"heading": "A.4. Expectation Lemmas", "text": "In this section we assume that x follows (r, k, m, \u03bb) -GCC. Then we show the following terms."}, {"heading": "A.4.1. LEMMAS WITH ONLY GCC", "text": "E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-"}, {"heading": "A.5. Robustness", "text": "In this subsection, we show that our algorithm is also robust with respect to noise. To demonstrate the idea, we will provide evidence for the case in which we [0], [1], [2], [2], [3], [3], [4], [4], [4], [4], [5], [6], [6], [6], [6], [6], [6], [6], [6], [6], [7], [7], [7], [7], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8,], [8,], [8,], [8,], [8,] 8, [8,], [8,], [8,] 8, [8], [8,], [8,], [8,], [8,], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], 8, [8], [8], [8], [8], 8, [8], [8], 8, 8, [8], 8, 8, [8], 8, 8, 8, 8, 8, 8], 8, 8, 8, 8, [8, [8], 8, 8], 8, 8, [8, [8], [8], 8, 8, [8], 8, [8], 8], 8, [8], 8, 8, [, [8], 8], [8], 8, [8], 8, [8], [, 8], [8], [, [8], [8]."}, {"heading": "Proof of Lemma 24.", "text": "\u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2"}, {"heading": "B. Additional Experiments", "text": "The first set of experiments in Section B.1 evaluates the performance of our algorithm at low initialization, because for our theoretical analysis, a warm start is critical to convergence. It turns out that our algorithm is not very sensitive to the warm start; even if there is a lot of noise during initialization, it still delivers reasonable results. This allows it to be used in a wide range of applications where a strong warm start is difficult to attain.The second set of experiments in Section B.2 evaluates the performance of the algorithm when the weight x is very scarce. Note that our current limits are somewhat dependent on the \"1 standard of x. We believe that this is only because we want to make our statement as general as possible by making assumptions only about the first two moments of x. If, for example, we also assume that x has beautiful third moments, then our limit can be greatly improved."}, {"heading": "B.1. Robustness to Initializations", "text": "In all the experiments in the main text, the initialization matrix A0 is set to A0 = A * (I + U), where I am the identity matrix and U is a matrix whose entries i.i.d. are samples from the uniform distribution to [\u2212 0.05, 0.05]. Note that this is a very weak initialization, because [A0] i = (1 + Ui, i) [A *] i + \u2211 j 6 = i Uj, i [A *] j and the size of the noise component \u2211 j 6 = i Uj, i [A *] j can be larger than the signal part (1 + Ui, i) [A *] i.rn] i. Here we examine even worse initializations: A0 = A * (I + U) + N, where I have the identity matrix, U is a matrix whose entries i.i.i.i.d. samples from the uniform distribution to [\u2212 0.05, rn] i.r.r.l for one i.r.l, i.r.l for one i.r.l."}, {"heading": "B.2. Robustness to Sparsity", "text": "In particular, we construct a 100 \u00b7 5000 matrix X, in which each column is pulled from a dirichlet before D (\u03b1) to d = 100 dimension, where \u03b1 = (\u03b1 / d, \u03b1 / d,...) for a scalar \u03b1. Then the dataset Y = A \u0445 X. We varied the \u03b1 parameter of the previous one to control the expected support spareness, and executed the algorithm on the generated data. Figure 4 shows the results. At a size of \u03b1 to 20, the algorithm still converges exponentially with the basic truth. If \u03b1 = 80 means that the weight vectors (columns in X) have almost full support, the algorithm still delivers good results and stabilizes to a small relative error at the end. This demonstrates that the algorithm is not sensitive to the support spareness of the data."}, {"heading": "B.3. Qualitative Results on Some Real World Applications", "text": "We applied our algorithm to two popular applications with real-world data to demonstrate the applicability of the method to real-world scenarios. Note that the assessments here are qualitative, as the guarantee for our algorithm is convergence with the basic truth, while in practice there is no predefined basic truth for these data sets. Quantitative studies with other criteria that can be calculated in practice are reserved for future work."}, {"heading": "B.3.1. TOPIC MODELING", "text": "Our algorithm is initialized with 10 random documents from the dataset, and the hyperparameters such as the learning rate come from the experiments in the main text. Note that better initialization is possible while we keep things simple here to demonstrate the efficiency of the method. Table 1 shows the results of the NMF method and the LDA method in the sklearn package, 10 and the result of our AND method. It shows that our method actually leads to reasonable topics, with a quality comparable to well-implemented popular methods that are adapted to this task. 10http: / / scikit-learn.org / B.3.2. Implementation of IMAGE DECOMPOSITION Here, our method is used to calculate 6 components on the Olivetti faces of the datasets, which is a standard dataset for image decomposition."}], "references": [{"title": "Two svds suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation", "author": ["A. Anandkumar", "S. Kakade", "D. Foster", "Y. Liu", "D. Hsu"], "venue": "Technical report,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Learning latent bayesian networks and topic models under expansion constraints", "author": ["A. Anandkumar", "D. Hsu", "A. Javanmard", "S. Kakade"], "venue": "In ICML,", "citeRegEx": "Anandkumar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2013}, {"title": "Learning topic models \u2013 going beyond svd", "author": ["S. Arora", "R. Ge", "A. Moitra"], "venue": "In FOCS,", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["S. Arora", "R. Ge", "Y. Halpern", "D. Mimno", "A. Moitra", "D. Sontag", "Y. Wu", "M. Zhu"], "venue": "In ICML,", "citeRegEx": "Arora et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2013}, {"title": "Simple, efficient, and neural algorithms for sparse coding", "author": ["S. Arora", "R. Ge", "T. Ma", "A. Moitra"], "venue": "In COLT,", "citeRegEx": "Arora et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2015}, {"title": "Computing a nonnegative matrix factorization\u2013 provably", "author": ["Arora", "Sanjeev", "Ge", "Rong", "Kannan", "Ravindran", "Moitra", "Ankur"], "venue": "In STOC, pp. 145\u2013162", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Provable algorithms for inference in topic models", "author": ["Arora", "Sanjeev", "Ge", "Rong", "Koehler", "Frederic", "Ma", "Tengyu", "Moitra", "Ankur"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Arora et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2016}, {"title": "On some provably correct cases of variational inference for topic models", "author": ["Awasthi", "Pranjal", "Risteski", "Andrej"], "venue": "In NIPS, pp. 2089\u20132097,", "citeRegEx": "Awasthi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2015}, {"title": "Non-negative matrix factorization under heavy noise", "author": ["Bhattacharyya", "Chiranjib", "Goyal", "Navin", "Kannan", "Ravindran", "Pani", "Jagdeep"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "Bhattacharyya et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bhattacharyya et al\\.", "year": 2016}, {"title": "Correlated topic models", "author": ["Blei", "David", "Lafferty", "John"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Blei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2006}, {"title": "Probabilistic topic models", "author": ["Blei", "David M"], "venue": "Communications of the ACM,", "citeRegEx": "Blei and M.,? \\Q2012\\E", "shortCiteRegEx": "Blei and M.", "year": 2012}, {"title": "Hierarchical als algorithms for nonnegative matrix and 3d tensor factorization", "author": ["Cichocki", "Andrzej", "Zdunek", "Rafal", "Amari", "Shun-ichi"], "venue": "In International Conference on Independent Component Analysis and Signal Separation,", "citeRegEx": "Cichocki et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cichocki et al\\.", "year": 2007}, {"title": "Topic discovery through data dependent and random projections", "author": ["W. Ding", "M.H. Rohban", "P. Ishwar", "V. Saligrama"], "venue": "arXiv preprint arXiv:1303.3664,", "citeRegEx": "Ding et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2013}, {"title": "Efficient distributed topic modeling with provable guarantees", "author": ["W. Ding", "M.H. Rohban", "P. Ishwar", "V. Saligrama"], "venue": "In AISTAT, pp", "citeRegEx": "Ding et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2014}, {"title": "Online learning for latent dirichlet allocation. In advances in neural information processing", "author": ["Hoffman", "Matthew", "Bach", "Francis R", "Blei", "David M"], "venue": null, "citeRegEx": "Hoffman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2010}, {"title": "Nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method", "author": ["Kim", "Hyunsoo", "Park", "Haesun"], "venue": "SIAM journal on matrix analysis and applications,", "citeRegEx": "Kim et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2008}, {"title": "Unsupervised learning by convex and conic coding", "author": ["Lee", "Daniel D", "Seung", "H Sebastian"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1997}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["Lee", "Daniel D", "Seung", "H Sebastian"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1999}, {"title": "Algorithms for non-negative matrix factorization", "author": ["Lee", "Daniel D", "Seung", "H Sebastian"], "venue": "In NIPS, pp", "citeRegEx": "Lee et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2001}, {"title": "Recovery guarantee of non-negative matrix factorization via alternating updates", "author": ["Li", "Yuanzhi", "Liang", "Yingyu", "Risteski", "Andrej"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Overlapping community detection at scale: a nonnegative matrix factorization approach", "author": ["Yang", "Jaewon", "Leskovec", "Jure"], "venue": "In Proceedings of the sixth ACM international conference on Web search and data mining,", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "(Anandkumar et al., 2012)), which undermine their applicability in practice.", "startOffset": 0, "endOffset": 25}, {"referenceID": 19, "context": "This problem is poorly understood, with only a few provable guarantees known (Awasthi & Risteski, 2015; Li et al., 2016).", "startOffset": 77, "endOffset": 120}, {"referenceID": 19, "context": "Most of the previous theoretical approaches for analyzing alternating between decoding and encoding, such as (Awasthi & Risteski, 2015; Li et al., 2016; Arora et al., 2015), require the coordinates of x to be pairwiseindependent, or almost pairwise-independent (meaning E\u03bc[xixj ] \u2248 E\u03bc[xi]E\u03bc[xj ]).", "startOffset": 109, "endOffset": 172}, {"referenceID": 4, "context": "Most of the previous theoretical approaches for analyzing alternating between decoding and encoding, such as (Awasthi & Risteski, 2015; Li et al., 2016; Arora et al., 2015), require the coordinates of x to be pairwiseindependent, or almost pairwise-independent (meaning E\u03bc[xixj ] \u2248 E\u03bc[xi]E\u03bc[xj ]).", "startOffset": 109, "endOffset": 172}, {"referenceID": 19, "context": "Inverse decoding was also used in (Li et al., 2016; Arora et al., 2015; 2016).", "startOffset": 34, "endOffset": 77}, {"referenceID": 4, "context": "Inverse decoding was also used in (Li et al., 2016; Arora et al., 2015; 2016).", "startOffset": 34, "endOffset": 77}, {"referenceID": 6, "context": "It was also observed in (Arora et al., 2016) that Moore-Penrose", "startOffset": 24, "endOffset": 44}, {"referenceID": 11, "context": ", (Kim & Park, 2008; Lee & Seung, 2001; Cichocki et al., 2007; Ding et al., 2013; 2014).", "startOffset": 2, "endOffset": 87}, {"referenceID": 12, "context": ", (Kim & Park, 2008; Lee & Seung, 2001; Cichocki et al., 2007; Ding et al., 2013; 2014).", "startOffset": 2, "endOffset": 87}, {"referenceID": 8, "context": ", 2012b) also studied NMF under separability assumptions about the features, and (Bhattacharyya et al., 2016) studied NMF under related assumptions.", "startOffset": 81, "endOffset": 109}, {"referenceID": 19, "context": "The most related work is (Li et al., 2016), which analyzed an alternating minimization type algorithm.", "startOffset": 25, "endOffset": 42}, {"referenceID": 1, "context": "Recently, there is a line of theoretical work analyzing tensor decomposition (Arora et al., 2012a; 2013; Anandkumar et al., 2013) or combinatorial methods (Awasthi & Risteski, 2015).", "startOffset": 77, "endOffset": 129}, {"referenceID": 19, "context": "Since A and A\u2217 are in the same subspace, inspired by (Li et al., 2016) we can write A\u2217 as A(\u03a3 + E) for a diagonal matrix \u03a3 and an off-diagonal matrix E, and thus the decoding becomes z = \u03c6\u03b1(\u03a3x + Ex).", "startOffset": 53, "endOffset": 70}, {"referenceID": 3, "context": "In particular, we used the matrix with 100 topics computed by the algorithm in (Arora et al., 2013) on the NIPS papers dataset (about 1500 documents, average length about 1000).", "startOffset": 79, "endOffset": 99}, {"referenceID": 14, "context": "We compare the algorithm AND to the following popular methods: Alternating Non-negative Least Square (ANLS (Kim & Park, 2008)), multiplicative update (MU (Lee & Seung, 2001)), LDA (online version (Hoffman et al., 2010)),7 and Hierarchical Alternating Least Square (HALS (Cichocki et al.", "startOffset": 196, "endOffset": 218}, {"referenceID": 11, "context": ", 2010)),7 and Hierarchical Alternating Least Square (HALS (Cichocki et al., 2007)).", "startOffset": 59, "endOffset": 82}], "year": 2017, "abstractText": "Non-negative matrix factorization is a basic tool for decomposing data into the feature and weight matrices under non-negativity constraints, and in practice is often solved in the alternating minimization framework. However, it is unclear whether such algorithms can recover the groundtruth feature matrix when the weights for different features are highly correlated, which is common in applications. This paper proposes a simple and natural alternating gradient descent based algorithm, and shows that with a mild initialization it provably recovers the ground-truth in the presence of strong correlations. In most interesting cases, the correlation can be in the same order as the highest possible. Our analysis also reveals its several favorable features including robustness to noise. We complement our theoretical results with empirical studies on semi-synthetic datasets, demonstrating its advantage over several popular methods in recovering the groundtruth.", "creator": "LaTeX with hyperref package"}}}