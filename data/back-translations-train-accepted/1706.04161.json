{"id": "1706.04161", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Lost Relatives of the Gumbel Trick", "abstract": "The Gumbel trick is a method to sample from a discrete probability distribution, or to estimate its normalizing partition function. The method relies on repeatedly applying a random perturbation to the distribution in a particular way, each time solving for the most likely configuration. We derive an entire family of related methods, of which the Gumbel trick is one member, and show that the new methods have superior properties in several settings with minimal additional computational cost. In particular, for the Gumbel trick to yield computational benefits for discrete graphical models, Gumbel perturbations on all configurations are typically replaced with so-called low-rank perturbations. We show how a subfamily of our new methods adapts to this setting, proving new upper and lower bounds on the log partition function and deriving a family of sequential samplers for the Gibbs distribution. Finally, we balance the discussion by showing how the simpler analytical form of the Gumbel trick enables additional theoretical results.", "histories": [["v1", "Tue, 13 Jun 2017 17:01:54 GMT  (144kb,D)", "http://arxiv.org/abs/1706.04161v1", "34th International Conference on Machine Learning (ICML 2017)"]], "COMMENTS": "34th International Conference on Machine Learning (ICML 2017)", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["matej balog", "nilesh tripuraneni", "zoubin ghahramani", "adrian weller"], "accepted": true, "id": "1706.04161"}, "pdf": {"name": "1706.04161.pdf", "metadata": {"source": "META", "title": "Lost Relatives of the Gumbel Trick", "authors": ["Matej Balog", "Nilesh Tripuraneni", "Zoubin Ghahramani", "Adrian Weller"], "emails": ["<first.last@gmail.com>."], "sections": [{"heading": "1. Introduction", "text": "This paper deals with the fundamental problem of a discrete probability distribution and the evaluation of its normalizing constant. A probability distribution p on a discrete sample space X is easy to implement in terms of its potential functionality. In this context, the distribution of Gibbs on X is associated with the potential functioning.The challenges of such a discrete probability distribution and assessment of the partition function are fundamental problems with ubiq-1University of Cambridge, UK 2MPI-IS, Tu-bingen, Germany 3UC Distribution on X, USA 4Uber AI Labs, 5Alan Turing Institute, UK. Correspondence to: Matej Balog < first.last @ gmail.com Code: https."}, {"heading": "2. Relatives of the Gumbel Trick", "text": "In this section we will look at the Gumbel trick and describe the mechanism by which it can be generalized to a whole family of tricks. We will show how these tricks can be considered equivalent to the mean of standard Gumbel perturbations in different spaces, instantiate several examples and compare the properties of the various tricks. Notation During this work, X should be a finite sample space of size N: = | X |. Let p: X \u2192 [0, \u221e) be an unnormalized mass function over X and let Z: = [0] be its normalizing partition function. Write p (x): = p (x): = p (x) / Z for the normalized version of p, and? (x): = ln p (x) for the log-unnormalized probability, i.e. the potential function. We will write Exp (\u03bb) for the exponential distribution with the rate (inverse mean) and Gumbel (\u00b5) for the log-unnormalized probability, i.e. the potential function."}, {"heading": "2.1. The Gumbel Trick", "text": "Analogous to the connection between the Gumbel trick and the Poisson process established by Maddison (2016), we introduce the Gumbel trick for discrete probability distributions with a simple and elegant construction using competing exponential clocks. Let's look at independent clocks that are started simultaneously so that the probability of the j-th clock after a random time Tj-Exp (2001) is initially proportional to its rate. These properties are also used in the survival analysis (Cox & Oakes, 1984). Let's look at competing exponential clocks (Tx) x-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-Z-Z-X-Z-Z-Z-Z-Z-Z-Z-X-Z-Z-Z-Z-Z-Z-X-Z-Z-Z-Z-Z-X-Z-Z-Z-X-Z-Z-Z-X-Z-Z-Z-X-Z-Z-Z-X-Z-Z-Z-X-Z-Z-Z-Z-X-Z-Z-Z-X-Z-Z-Z-Z-Z-X-Z-Z-Z-Z-X-Z-Z-Z-Z-X-Z-Z-Z-Z-X-Z-Z-Z-Z-Z-X-Z-Z-Z-Z-Z-X-Z-Z-Z-Z-Z-Z-Z-Z-X-Z-Z-Z-Z-Z-X-Z-Z-Z-Z-Z-Z-Z-Z-Z-X-Z-Z-Z-Z-Z-Z-Z-Z-Z-X-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-"}, {"heading": "2.2. Constructing New Tricks", "text": "Given the equality in distribution (1), we can treat the problem of estimating the partition function Z as a parameter estimation problem for the exponential distribution (1). Applying the function g (x) = \u2212 lnx \u2212 c, as in the Gumbel trick, to obtain a Gumbel (\u2212 c + lnZ) random variable involves calculating its mean to obtain an unbiased estimator of lnZ, is only one way to derive information from it about Z.We consider the application of different functions g to (1), in particular the functions g, which transform the exponential distribution into another distribution with a known mean. Since the original exponential distribution has the rate Z, the transformed distribution will have a mean f (Z), where f will generally no longer be the logarithmic function. Since we are often interested in estimating different transformations f (Z) of Z, this provides us with a collection of unbiased estimators from which we can choose."}, {"heading": "2.3. Comparing Tricks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.3.1. ASYMPTOTIC EFFICIENCY", "text": "The Delta Method (Casella & Berger, 2002) is a simple method for assessing the asymptotic variance of estimators achieved by a differentiated transformation of an estimator with known variance. In the last column of Table 1, asymptotic variances of corresponding tricks are listed when unbiased estimators of f (Z) are led by the function f \u2212 1 to yield (biased but consistent and not negative) estimators of Z itself. It is interesting to examine the constants that multiply Z2 in some of the obtained asymptotic variance expressions for the various tricks. Furthermore, the Gurland ratio (Gurland, 1956) shows that this constant is at least 1 for the Weibull and Fre'chet tricks, which is exactly the value achieved by the exponential trick (which corresponds to \u03b1 = 1)."}, {"heading": "2.3.2. MEAN SQUARED ERROR (MSE)", "text": "For estimator Y, their MSE (Y) = var (Y) + bias (Y) 2 is a commonly used benchmark. If the Gumbel or exponential tricks are used to estimate either Z or lnZ, the estimator's biases, variances and MSEs can be analytically calculated using standard methods (Appendix A). For example, the unbiased estimator of lnZ from the Gumbel trick can be transformed into a consistent, non-negative estimator of Z by exponentiation: Y = Exp (1M \u2211 M = 1Xm), where X1,. XM i.d., Gumbel (\u2212 c + lnZ) using Equation (1 '). The bias and variance of Y can be calculated using the independence and moment-generating functions of the Xm's, see Appenditure A for details.Perhaps all estimator properties depend only on the true value of Z and not on the structure ()."}, {"heading": "2.4. Bayesian Perspective", "text": "A Bayesian approach opens up two possibilities when constructing estimators of Z or its transformations f (Z): 1. A choice of the previous distribution p0 (Z), which encodes previous beliefs about the value of Z before observations; 2. A choice of how to summarize the posterior distribution pM (Z | X1,..., XM) taking into account the M samples; and taking the previous p0 (Z) and Z \u2212 1, the posterior: pM (Z | X1,.., XM), ZM \u2212 1e \u2212 Z \u00b2 M m = 1 Xm. Detecting the density of a gamma (M, \u2211 M m = 1Xm) results in the random variable: pM (Z | X1,., XM)."}, {"heading": "3. Low-rank Perturbations", "text": "One way to use disturbances and MAP to achieve computational savings is to replace independent disturbances in the potential of each configuration with an approximation, such as in discrete graphical models in which the scan space X has a product space structure X = X1 \u00b7 \u00b7 \u00b7 \u00b7 Xn, where Xi has the state space of the i-th variable. Definition 3 (Hazan & Jaakkola, 2012): The sum-irregular disturbance value MAP is the random variable U: = max x-x-x {\u03c6 (x) + n-th variable. Definition 3 (Hazan & Jaakkola, 2012): The sum-irregular disturbance value MAP is the upper variable U: = max x-x-x-x (x) + n-th variable i (xi)} in which {\u03b3i (xi) | application Xi, 1 \u2264 i} \u2264 \u2264-n} n-.ibel-X-x-x-1-rubber-X-x-x-x-definition includes \u2212 x."}, {"heading": "3.1. Upper Bounds on the Partition Function", "text": "For all (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0)) (0) (0) (0) (0) (0) (0) (0)) (0) (0) (0) (1) (1) (1) (1) (1) (1) (1)) (1) (1) (1) (1) (1) (1) (1)) (1) (1) (1) (1) (1)) (1) (1)) (1) (1) (1) (1)) (1) (1) (1) (1) (1) (1) (1)"}, {"heading": "3.2. Clamping", "text": "Consider the subtotal error MAP values, where the values of the first j \u2212 1 variable are set and only the rest is disturbed: Uj (x1,.., xj \u2212 1): = max xj,..., xn \u03c6 (x) + n \u2211 i = j \u03b3i (xi). The following random problem in which the Uj is involved serves three purposes: (I.) it represents the introduction step for Proposition 4, (II.) it shows that brackets never violate the estimation of the partition function with Fre \ufffd chet and Weibull tricks, and (III.) it is used to show that a sequential sampler, which is \u2212 \u2212 \u2212 n \u2212 approximate in Section 3.3 below, is Lemma 7 (parentheses of Lemma). For all j {1,., n} and (x1,.,.) the variables of Urell (x1),."}, {"heading": "3.3. Sequential Sampling", "text": "Hazan et al. (2013) derived a sequential sampling method for the Gibbs distribution by taking advantage of the U (0) Gumbel trick above lnZ. In the same sense, onecan derived sequential sampling methods from the tricks of Fre \ufffd chet and Weibull, which led to the following algorithm. Algorithm 1 Sequential sampling method for the Gibbs distribution input: \u03b1 (\u2212 1, 0), potential function enhancement to X output: a sample x from the Gibbs distribution e\u03c6 (x) 1: for j = 1 to n do 2: for xj Xj do3: pj (xj) \u2190 e \u2212 c (1 + \u03b1) 1 / \u03b1 E\u03b3 [e \u2212 Uj + 1 (x1,..., xj)] \u2212 1 / 2 (xj) deviation from the quality (xj) when the deviation from the quantity (xj) and the deviation from the quantity (xj) is not (xj) to the quantity (xj)."}, {"heading": "3.4. Lower Bounds on the Partition Function", "text": "Similar to the Gumbel trick (Hazan et al., 2013), lower limits can be derived to lnZ by disrupting an arbitrary subset S of variables. Suggestion 9. Let X = X1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Xn be a product space and a potential function on X. Let \u03b1 (\u2212 1, 0). For each subset S {1,.., n} of variables x1,.., xn we have lnZ \u2265 c + ln (1 + \u03b1) \u03b1 \u2212 1 \u03b1lnE [e \u2212 \u03b1maxx (x) + \u03b3S (xS)}], where xS: = {xi: i \u03baS} and \u03b3S (xS). Gumbel (\u2212 c) independent for each setting of xS. By averaging n of such lower limits, the singleton sets S = {i} together, we obtain a lower limit to lnZ containing the average turbulence (\u2212 c)."}, {"heading": "4. Advantages of the Gumbel Trick", "text": "We have seen how the Gumbel trick can be embedded in a continuous family of tricks, consisting of Fre? chet, exponential and Weibull tricks. We have shown that the new tricks can lead to more efficient estimators of the partition function in the full perturbation setting (section 2) and in the low perturbation setting to sequential samplers and new limits on lnZ, which can also be more efficient, as we examine in Section 5.3. To balance the discussion of the benefits of various tricks, in this section we briefly highlight the advantages of the Gumbel trick, which stems from its simpler analytical form. First, we see that the function g (x) = \u2212 lnx \u2212 c has the property that the variance of the resulting estimator (from lnZ) does not depend on the value of the Z; the function g is a variance that stabilizes the transformation for the exponential distribution."}, {"heading": "5. Experiments", "text": "We conducted experiments with the following objectives: 1. To show that the higher efficiency of the exponential trick in the complete perturbation environment is useful in practice, we compared it with the Gumbel trick in the A * sampling (Maddison et al., 2014) (Section 5.1) and in the large-scale discrete sampling environment of Chen & Ghahramani (2016) (Section 5.2); 2. To show that non-zero values of \u03b1 can also lead to better estimations of lnZ in the low-level perturbation environment, we compare the boundaries of Fre'chet and Weibull trick U (\u03b1) with the Gumbel trick, which binds U (0) to a common discrete graphic model with varying coupling strengths; see Section 5.3."}, {"heading": "5.1. A* Sampling", "text": "A * sampling (Maddison et al., 2014) is a continuous distribution sampling algorithm that disrupts the lonon-normalized density \u03c6 by a continuous generalization of the Gumbel trick, the so-called Gumbel process, and uses a variant of the A * search to find the location of the maximum of the disturbed \u03c6. Returning the location yields an exact sample from the original distribution, as in the discrete Gumbel trick. Furthermore, the corresponding maximum value has also verified the Gumbel (\u2212 c + lnZ) distribution (Maddison et al., 2014). Our analysis in Section 2.3 shows that the exponential trick produces an estimator with a lower MSE than the Gumbel trick; we have briefly verified this in the Robust Bayesian Regression experiment by Maddison et al. (2014)."}, {"heading": "5.2. Scalable Partition Function Estimation", "text": "Chen & Ghahramani (2016) considered the sample from a discrete distribution of the form p (x) lnA (l) lnA (f) 0 (x) S = 1 fs (x) if the number of factors S is large in relation to the size of the sample space. Calculation i.i.d. Gumbel perturbations \u03b3 (x) for each x) X is then relatively cheap compared to estimating all potentials \u03c6 (x) = f0 (x) + \u2211 S = 1 ln fs (x). Chen & Ghahramani (2016) observed that any (disturbed) potential can be estimated by subsampling the factors, and potentials that seem unlikely to yield the MAP value can be subtracted from the search. The authors formulated the problem as a multi-faceted bandit problem with a finite reward of the basic totality and derived safe algorithms for efficiently determining the maximum perturbed potential with a probable guarantee."}, {"heading": "6. Discussion", "text": "By evaluating the evaluation of partition functions as a parameter estimation problem for exponential distribution, we derived a family of methods of which the Gumbel trick is a special case. These methods can be considered equivalent to (1) disruptive models with different distributions or (2) average standard Gumbel disturbances in different areas, resulting in implementations at a low additional cost. We demonstrated that the new exponential trick in full-fledged disturbance adjustment provides an estimator with lower MSE or instead allows to use up to 40% fewer samples than the Gumbel trick estimator to perform the same MSE. In the subordinate disturbance adjustment, we used our Fre'chet, exponential and Weibull tricks to derive new limits for lnZ and sequential samplers for Gibbs distribution, showing that they can also behave better than the corresponding Gumbel trick results, depending on the optimum size of the problem, however, as determined by the trick."}, {"heading": "Acknowledgements", "text": "The authors thank Tamir Hazan for helpful discussions and Mark Rowland, Maria Lomeli and the anonymous critics for helpful comments. AW appreciates the support provided by the Alan Turing Institute under the EPSRC funding programme EP / N510129 / 1 and the Leverhulme Trust through the CFI."}, {"heading": "APPENDIX: Lost Relatives of the Gumbel Trick", "text": "Here we provide evidence for the results stated in the main text, along with additional supporting citations required for this evidence."}, {"heading": "A. Comparison of Gumbel and Exponential tricks", "text": "In Section 2.3.1 we analyzed the asymptotic efficiency of different estimators of Z by measuring their asymptotic variance. (As all our estimators in the full-rank perturbation setting are constant, their bias is 0 in the limit of infinite data, and so this asymptotic variance equals the asymptotic MSE.) In most cases we cannot obtain these analytically and there we can fall back on an empirical evaluation for which estimators from Gumbel and Exponential Tricks can be analytically treated."}, {"heading": "B. Sum-unary perturbations", "text": "Remember that the sum-less perturbations refer to the setting in which the individual variable potentials are distorted with Gumbel noise, and the distorted potential of a configuration summarizes the perturbations from all variables (see definition 3 in the main text). Using the sum-less perturbations, we can derive a family U (alpha) of the upper limits of a configuration (proposition 4) and construct sequential samplers for the Gibbs distribution from it (algorithm 1). Here, we provide evidence for the related results given in sections 3.1 and 3.2.Notation. We will write the results for x\u03b2 x, in which x, \u03b2 or R increases the clarity of our exposure. Lemma 13 (Weibull and Fre) chet tricks. For all finite propositions Y and any function h, we havepow \u2212 Y pow \u2212 EW = EW."}, {"heading": "C. Averaged unary perturbations", "text": "In the main text, we have performed the following two lower limits on the log separation function: (1, 0), (2), (2), (2), (3), (3), (3), (3), (4), (4), (4), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5, (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5, (5), (5), (5), (5, (5), (5),"}, {"heading": "D. Technical results", "text": "In this section, we write the following explanation for the Gumbel-Trick (max.): (max.): (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max.), (max., (max.), (max.), (max.), (max.), (max. max., max. (max.), max. (max.), max. max. (max.), max. max. (max.), max. max. (max.), max. max. (max.), max. (max.), max. max., max. max., max., max. max., max. max., max., max. max., max., max. max., max. max. max., max., max. max., max. max., max. max., max. max., max., max. max. max., max. max., max.), max., max., max., max. max. max., max. max., max., max. max., max. max., max., max. max., max. max. max., max., max., max. max., max., max. max., max., max., max., max., max., max., max., max., max., max., max max max max max max., max max. max. max max., max, max max max max., max., max max max max max max max., max max max max max max max, max max max max., max max max max max"}], "references": [{"title": "Marginal inference in MRFs using Frank-Wolfe", "author": ["D. Belanger", "D. Sheldon", "A. McCallum"], "venue": "In NIPS Workshop on Greedy Optimization, Frank-Wolfe and Friends,", "citeRegEx": "Belanger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Belanger et al\\.", "year": 2013}, {"title": "Local Perturband-MAP for Structured Prediction", "author": ["G. Bertasius", "Q. Liu", "L. Torresani", "J. Shi"], "venue": "In AISTATS,", "citeRegEx": "Bertasius et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bertasius et al\\.", "year": 2017}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "IEEE Transactions on pattern analysis and machine intelligence,", "citeRegEx": "Boykov et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Boykov et al\\.", "year": 2001}, {"title": "Scalable discrete sampling as a multi-armed bandit problem", "author": ["Y. Chen", "Z. Ghahramani"], "venue": "In ICML,", "citeRegEx": "Chen and Ghahramani,? \\Q2016\\E", "shortCiteRegEx": "Chen and Ghahramani", "year": 2016}, {"title": "Analysis of survival data, volume 21", "author": ["D. Cox", "D. Oakes"], "venue": "CRC Press,", "citeRegEx": "Cox and Oakes,? \\Q1984\\E", "shortCiteRegEx": "Cox and Oakes", "year": 1984}, {"title": "Global optimization for first order Markov random fields with submodular priors", "author": ["J. Darbon"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "Darbon,? \\Q2009\\E", "shortCiteRegEx": "Darbon", "year": 2009}, {"title": "Taming the curse of dimensionality: Discrete integration by hashing and optimization", "author": ["S. Ermon", "A. Sabharwal", "B. Selman"], "venue": "In ICML,", "citeRegEx": "Ermon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ermon et al\\.", "year": 2013}, {"title": "An inequality satisfied by the Gamma function", "author": ["J. Gurland"], "venue": "Scandinavian Actuarial Journal,", "citeRegEx": "Gurland,? \\Q1956\\E", "shortCiteRegEx": "Gurland", "year": 1956}, {"title": "On the partition function and random maximum a-posteriori perturbations", "author": ["T. Hazan", "T. Jaakkola"], "venue": "In ICML,", "citeRegEx": "Hazan and Jaakkola,? \\Q2012\\E", "shortCiteRegEx": "Hazan and Jaakkola", "year": 2012}, {"title": "On sampling from the Gibbs distribution with random maximum a-posteriori perturbations", "author": ["T. Hazan", "S. Maji", "T. Jaakkola"], "venue": "In NIPS", "citeRegEx": "Hazan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2013}, {"title": "High dimensional inference with random maximum aposteriori perturbations", "author": ["T. Hazan", "F. Orabona", "A. Sarwate", "S. Maji", "T. Jaakkola"], "venue": null, "citeRegEx": "Hazan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2016}, {"title": "Exact sampling with integer linear programs and random perturbations", "author": ["C. Kim", "A. Sabharwal", "S. Ermon"], "venue": "In AAAI,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Convergent tree-reweighted message passing for energy minimization", "author": ["V. Kolmogorov"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Kolmogorov,? \\Q2006\\E", "shortCiteRegEx": "Kolmogorov", "year": 2006}, {"title": "Barrier Frank-Wolfe for Marginal Inference", "author": ["Krishnan", "Rahul G", "Lacoste-Julien", "Simon", "Sontag", "David"], "venue": "In NIPS", "citeRegEx": "Krishnan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Krishnan et al\\.", "year": 2015}, {"title": "Graphical models. Oxford statistical science series", "author": ["S. Lauritzen"], "venue": "Autre", "citeRegEx": "Lauritzen,? \\Q1996\\E", "shortCiteRegEx": "Lauritzen", "year": 1996}, {"title": "A Poisson process model for Monte Carlo", "author": ["C. Maddison"], "venue": null, "citeRegEx": "Maddison,? \\Q2014\\E", "shortCiteRegEx": "Maddison", "year": 2014}, {"title": "Active boundary annotation using random MAP perturbations", "author": ["S. Maji", "T. Hazan", "T. Jaakkola"], "venue": "In AISTATS,", "citeRegEx": "Maji et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Maji et al\\.", "year": 2014}, {"title": "libDAI: A free and open source C++ library for discrete approximate inference in graphical models", "author": ["J. Mooij"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mooij,? \\Q2010\\E", "shortCiteRegEx": "Mooij", "year": 2010}, {"title": "On measure concentration of random maximum a-posteriori perturbations", "author": ["F. Orabona", "T. Hazan", "A. Sarwate", "T. Jaakkola"], "venue": "In ICML,", "citeRegEx": "Orabona et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Orabona et al\\.", "year": 2014}, {"title": "Gaussian sampling by local perturbations", "author": ["G. Papandreou", "A. Yuille"], "venue": "In NIPS", "citeRegEx": "Papandreou and Yuille,? \\Q2010\\E", "shortCiteRegEx": "Papandreou and Yuille", "year": 2010}, {"title": "Perturb-and-MAP random fields: Using discrete optimization to learn and sample from energy models", "author": ["G. Papandreou", "A. Yuille"], "venue": "In Proc. IEEE Int. Conf. on Computer Vision (ICCV),", "citeRegEx": "Papandreou and Yuille,? \\Q2011\\E", "shortCiteRegEx": "Papandreou and Yuille", "year": 2011}, {"title": "Randomized optimum models for structured prediction", "author": ["D. Tarlow", "R. Adams", "R. Zemel"], "venue": "In AISTATS,", "citeRegEx": "Tarlow et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tarlow et al\\.", "year": 2012}, {"title": "Graphical Models, Exponential Families, and Variational Inference", "author": ["M. Wainwright", "M. Jordan"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "Clamping improves TRW and mean field approximations", "author": ["A. Weller", "J. Domke"], "venue": "In AISTATS,", "citeRegEx": "Weller and Domke,? \\Q2016\\E", "shortCiteRegEx": "Weller and Domke", "year": 2016}, {"title": "Approximating the Bethe partition function", "author": ["A. Weller", "T. Jebara"], "venue": "In UAI,", "citeRegEx": "Weller and Jebara,? \\Q2014\\E", "shortCiteRegEx": "Weller and Jebara", "year": 2014}, {"title": "Clamping variables and approximate inference", "author": ["A. Weller", "T. Jebara"], "venue": "In NIPS,", "citeRegEx": "Weller and Jebara,? \\Q2014\\E", "shortCiteRegEx": "Weller and Jebara", "year": 2014}, {"title": "Variable clamping for optimization-based inference", "author": ["J. Zhao", "J. Djolonga", "S. Tschiatschek", "A. Krause"], "venue": "In NIPS Workshop on Advances in Approximate Bayesian Inference,", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}, {"title": "P[x = x\u2217] be the probability mass function of x\u2217. The Gumbel trick lower bound on the log partition function lnZ due to Hazan et al", "author": ["\u223c Gumbel(\u2212c"], "venue": null, "citeRegEx": "Gumbel.\u2212c..,? \\Q2013\\E", "shortCiteRegEx": "Gumbel.\u2212c..", "year": 2013}, {"title": "lnZ \u2212 L(0) \u2265 0 (i.e. that L(0) is a lower bound on lnZ), this is a stronger result showing that the size of the gap is an upper bound on the KL divergence between the average-unary perturbation MAP distribution qavg and the Gibbs distribution", "author": ["Hazan"], "venue": null, "citeRegEx": "Hazan,? \\Q2013\\E", "shortCiteRegEx": "Hazan", "year": 2013}], "referenceMentions": [{"referenceID": 12, "context": "This MAP problem is NP-hard in general; however, substantial research effort has led to the development of solvers which can efficiently compute or estimate the MAP solution on many problems that occur in practice (e.g., Boykov et al., 2001; Kolmogorov, 2006; Darbon, 2009).", "startOffset": 214, "endOffset": 273}, {"referenceID": 5, "context": "This MAP problem is NP-hard in general; however, substantial research effort has led to the development of solvers which can efficiently compute or estimate the MAP solution on many problems that occur in practice (e.g., Boykov et al., 2001; Kolmogorov, 2006; Darbon, 2009).", "startOffset": 214, "endOffset": 273}, {"referenceID": 2, "context": ", Boykov et al., 2001; Kolmogorov, 2006; Darbon, 2009). Evaluating the partition function is a harder problem, containing for instance #P-hard counting problems. The general aim of perturb-and-MAP methods is to reduce the problem of partition function evaluation, or the problem of sampling from the Gibbs distribution, to repeated instances of the MAP problem (where each instance is on a different random perturbation of the original model). The Gumbel trick (Papandreou & Yuille, 2011) relies on adding Gumbel-distributed noise to each configuration\u2019s potential \u03c6(x). We derive a wider family of perturb-andMAP methods that can be seen as perturbing the model in different ways \u2013 in particular using the Weibull and Fr\u00e9chet distributions alongside the Gumbel. We show that the new methods can be implemented with essentially no additional computational cost by simply averaging existing Gumbel MAP perturbations in different spaces, and that they can lead to more accurate estimators of the partition function. Evaluating or perturbing each configuration\u2019s potential with i.i.d. Gumbel noise can be computationally expensive. One way to mitigate this is to cleverly prune computation in regions where the maximum perturbed potential is unlikely to be found (Maddison et al., 2014; Chen & Ghahramani, 2016). Another approach exploits the product structure of the sample space in discrete graphical models, replacing i.i.d. Gumbel noise with a \u201clow-rank\u201d approximation. Hazan & Jaakkola (2012); Hazan et al.", "startOffset": 2, "endOffset": 1495}, {"referenceID": 2, "context": ", Boykov et al., 2001; Kolmogorov, 2006; Darbon, 2009). Evaluating the partition function is a harder problem, containing for instance #P-hard counting problems. The general aim of perturb-and-MAP methods is to reduce the problem of partition function evaluation, or the problem of sampling from the Gibbs distribution, to repeated instances of the MAP problem (where each instance is on a different random perturbation of the original model). The Gumbel trick (Papandreou & Yuille, 2011) relies on adding Gumbel-distributed noise to each configuration\u2019s potential \u03c6(x). We derive a wider family of perturb-andMAP methods that can be seen as perturbing the model in different ways \u2013 in particular using the Weibull and Fr\u00e9chet distributions alongside the Gumbel. We show that the new methods can be implemented with essentially no additional computational cost by simply averaging existing Gumbel MAP perturbations in different spaces, and that they can lead to more accurate estimators of the partition function. Evaluating or perturbing each configuration\u2019s potential with i.i.d. Gumbel noise can be computationally expensive. One way to mitigate this is to cleverly prune computation in regions where the maximum perturbed potential is unlikely to be found (Maddison et al., 2014; Chen & Ghahramani, 2016). Another approach exploits the product structure of the sample space in discrete graphical models, replacing i.i.d. Gumbel noise with a \u201clow-rank\u201d approximation. Hazan & Jaakkola (2012); Hazan et al. (2013) showed that from such an approximation, upper and lower bounds on the partition function and a sequential sampler for the Gibbs distribution can still be recovered.", "startOffset": 2, "endOffset": 1516}, {"referenceID": 1, "context": "Further recent applications of perturb-and-MAP include structured prediction in computer vision (Bertasius et al., 2017) and turning the discrete sampling problem into an optimization task that can be cast as a multi-armed bandit problem (Chen & Ghahramani, 2016), see Section 5.", "startOffset": 96, "endOffset": 120}, {"referenceID": 0, "context": "The Frank-Wolfe method may be applied by iteratively updating marginals using a constrained MAP solver and line search (Belanger et al., 2013; Krishnan et al., 2015).", "startOffset": 119, "endOffset": 165}, {"referenceID": 13, "context": "The Frank-Wolfe method may be applied by iteratively updating marginals using a constrained MAP solver and line search (Belanger et al., 2013; Krishnan et al., 2015).", "startOffset": 119, "endOffset": 165}, {"referenceID": 11, "context": "Tarlow et al. (2012) extended this perturb-and-MAP approach to sampling, in particular by considering more general structured prediction problems.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "Tarlow et al. (2012) extended this perturb-and-MAP approach to sampling, in particular by considering more general structured prediction problems. Hazan & Jaakkola (2012) pointed out that MAP perturbations are useful not only for sampling the Gibbs distribution (considering the argmax of the perturbed model), but also for bounding and approximating the partition function (by considering the value of the max).", "startOffset": 0, "endOffset": 171}, {"referenceID": 6, "context": "Afterwards, Hazan et al. (2013) derived new lower bounds on the partition function and proposed a new sampler for the Gibbs distribution that samples variables of a discrete graphical model sequentially, using expected values of lowrank MAP perturbations to construct the conditional probabilities.", "startOffset": 12, "endOffset": 32}, {"referenceID": 6, "context": "Afterwards, Hazan et al. (2013) derived new lower bounds on the partition function and proposed a new sampler for the Gibbs distribution that samples variables of a discrete graphical model sequentially, using expected values of lowrank MAP perturbations to construct the conditional probabilities. Due to the low-rank approximation, this algorithm has the option to reject a sample. Orabona et al. (2014) and Hazan et al.", "startOffset": 12, "endOffset": 406}, {"referenceID": 6, "context": "Afterwards, Hazan et al. (2013) derived new lower bounds on the partition function and proposed a new sampler for the Gibbs distribution that samples variables of a discrete graphical model sequentially, using expected values of lowrank MAP perturbations to construct the conditional probabilities. Due to the low-rank approximation, this algorithm has the option to reject a sample. Orabona et al. (2014) and Hazan et al. (2016) subsequently derived measure concentration results for the Gumbel distribution that can be used to control the rejection probability.", "startOffset": 12, "endOffset": 430}, {"referenceID": 6, "context": "Afterwards, Hazan et al. (2013) derived new lower bounds on the partition function and proposed a new sampler for the Gibbs distribution that samples variables of a discrete graphical model sequentially, using expected values of lowrank MAP perturbations to construct the conditional probabilities. Due to the low-rank approximation, this algorithm has the option to reject a sample. Orabona et al. (2014) and Hazan et al. (2016) subsequently derived measure concentration results for the Gumbel distribution that can be used to control the rejection probability. Maji et al. (2014) derived an uncertainty measure from random MAP perturbations, using it within a Bayesian active learning framework for interactive image boundary annotation.", "startOffset": 12, "endOffset": 583}, {"referenceID": 6, "context": "Afterwards, Hazan et al. (2013) derived new lower bounds on the partition function and proposed a new sampler for the Gibbs distribution that samples variables of a discrete graphical model sequentially, using expected values of lowrank MAP perturbations to construct the conditional probabilities. Due to the low-rank approximation, this algorithm has the option to reject a sample. Orabona et al. (2014) and Hazan et al. (2016) subsequently derived measure concentration results for the Gumbel distribution that can be used to control the rejection probability. Maji et al. (2014) derived an uncertainty measure from random MAP perturbations, using it within a Bayesian active learning framework for interactive image boundary annotation. Perturb-and-MAP was famously generalized to continuous spaces by Maddison et al. (2014), replacing the Gumbel distribution with a Gumbel process and calling the resulting algorithm A* sampling.", "startOffset": 12, "endOffset": 829}, {"referenceID": 6, "context": "Afterwards, Hazan et al. (2013) derived new lower bounds on the partition function and proposed a new sampler for the Gibbs distribution that samples variables of a discrete graphical model sequentially, using expected values of lowrank MAP perturbations to construct the conditional probabilities. Due to the low-rank approximation, this algorithm has the option to reject a sample. Orabona et al. (2014) and Hazan et al. (2016) subsequently derived measure concentration results for the Gumbel distribution that can be used to control the rejection probability. Maji et al. (2014) derived an uncertainty measure from random MAP perturbations, using it within a Bayesian active learning framework for interactive image boundary annotation. Perturb-and-MAP was famously generalized to continuous spaces by Maddison et al. (2014), replacing the Gumbel distribution with a Gumbel process and calling the resulting algorithm A* sampling. Maddison (2016) cast this work into a unified framework together with adaptive rejection sampling techniques, based on the notion of exponential races.", "startOffset": 12, "endOffset": 951}, {"referenceID": 6, "context": "Afterwards, Hazan et al. (2013) derived new lower bounds on the partition function and proposed a new sampler for the Gibbs distribution that samples variables of a discrete graphical model sequentially, using expected values of lowrank MAP perturbations to construct the conditional probabilities. Due to the low-rank approximation, this algorithm has the option to reject a sample. Orabona et al. (2014) and Hazan et al. (2016) subsequently derived measure concentration results for the Gumbel distribution that can be used to control the rejection probability. Maji et al. (2014) derived an uncertainty measure from random MAP perturbations, using it within a Bayesian active learning framework for interactive image boundary annotation. Perturb-and-MAP was famously generalized to continuous spaces by Maddison et al. (2014), replacing the Gumbel distribution with a Gumbel process and calling the resulting algorithm A* sampling. Maddison (2016) cast this work into a unified framework together with adaptive rejection sampling techniques, based on the notion of exponential races. This recent view generally brings together perturband-MAP and accept-reject samplers, exploiting the connection between the Gumbel distribution and competing exponential clocks that we also discuss in Section 2.1. Inspired by A* sampling, Kim et al. (2016) proposed an exact sampler for discrete graphical models based on lazilyinstantiated random perturbations, which uses linear programming relaxations to prune the optimization space.", "startOffset": 12, "endOffset": 1344}, {"referenceID": 0, "context": "The Frank-Wolfe method may be applied by iteratively updating marginals using a constrained MAP solver and line search (Belanger et al., 2013; Krishnan et al., 2015). Weller & Jebara (2014a) instead use just one MAP call over a discretized mesh of marginals to approximate the Bethe partition function, which itself is an estimate (which often performs well) of the true partition function.", "startOffset": 120, "endOffset": 191}, {"referenceID": 15, "context": "The Gumbel Trick Similarly to the connection between the Gumbel trick and the Poisson process established by Maddison (2016), we introduce the Gumbel trick for discrete probability distributions using a simple and elegant construction via competing exponential clocks.", "startOffset": 109, "endOffset": 125}, {"referenceID": 7, "context": "For example, it can be shown using Gurland\u2019s ratio (Gurland, 1956) that this constant is at least 1 for the Weibull and Fr\u00e9chet tricks, which is precisely the value achieved by the Exponential trick (which corresponds to \u03b1 = 1).", "startOffset": 51, "endOffset": 66}, {"referenceID": 9, "context": "Unary perturbations provide the upper bound lnZ \u2264 E[U ] on the log partition function (Hazan & Jaakkola, 2012), can be used to construct a sequential sampler for the Gibbs distribution (Hazan et al., 2013), and, if the perturbations are scaled down by a factor of n, a lower bound on lnZ can also be recovered (Hazan et al.", "startOffset": 185, "endOffset": 205}, {"referenceID": 9, "context": ", 2013), and, if the perturbations are scaled down by a factor of n, a lower bound on lnZ can also be recovered (Hazan et al., 2013).", "startOffset": 112, "endOffset": 132}, {"referenceID": 9, "context": "Corollary 9 of Hazan et al. (2016) can be used to show that var(e\u2212\u03b1U ) is finite for \u03b1 > \u2212 1 2 \u221a n , and so then the estimation is well-behaved.", "startOffset": 15, "endOffset": 35}, {"referenceID": 9, "context": "Corollary 9 of Hazan et al. (2016) can be used to show that var(e\u2212\u03b1U ) is finite for \u03b1 > \u2212 1 2 \u221a n , and so then the estimation is well-behaved. A natural question is how these new bounds relate to the Gumbel trick upper bound lnZ \u2264 E[U ] by Hazan & Jaakkola (2012). The following result aims to answers this: Proposition 5.", "startOffset": 15, "endOffset": 266}, {"referenceID": 9, "context": "This was shown previously in restricted settings (Hazan et al., 2013; Zhao et al., 2016).", "startOffset": 49, "endOffset": 88}, {"referenceID": 26, "context": "This was shown previously in restricted settings (Hazan et al., 2013; Zhao et al., 2016).", "startOffset": 49, "endOffset": 88}, {"referenceID": 26, "context": "Similar results showing that clamping improves partition function estimation have been obtained for the mean field and TRW approximations (Weller & Domke, 2016), and in certain settings for the Bethe approximation (Weller & Jebara, 2014b) and LFIELD (Zhao et al., 2016).", "startOffset": 250, "endOffset": 269}, {"referenceID": 9, "context": "Sequential Sampling Hazan et al. (2013) derived a sequential sampling procedure for the Gibbs distribution by exploiting the U(0) Gumbel trick upper bound on lnZ.", "startOffset": 20, "endOffset": 40}, {"referenceID": 9, "context": "As for the Gumbel sequential sampler of Hazan et al. (2013), the expected number of restarts (and hence the running time) only depend on the quality of the upper bound (U(\u03b1) \u2212 lnZ), and not on the ordering of variables.", "startOffset": 40, "endOffset": 60}, {"referenceID": 9, "context": "Lower Bounds on the Partition Function Similarly as in the Gumbel trick case (Hazan et al., 2013), one can derive lower bounds on lnZ by perturbing an arbitrary subset S of variables.", "startOffset": 77, "endOffset": 97}, {"referenceID": 9, "context": "Again, L(0) := E[L] can be defined by continuity, where E[L] \u2264 lnZ is the Gumbel trick lower bound by Hazan et al. (2013).", "startOffset": 102, "endOffset": 122}, {"referenceID": 16, "context": "Second, exploiting the fact that the logarithm function leads to additive perturbations, Maji et al. (2014) showed that the entropy of x\u2217, the configuration with maximum potential after sum-unary perturbation in the sense of Definition 3, can be bounded as H(x\u2217) \u2264 B(p) := \u2211n i=1 E\u03b3i [\u03b3i(xi )].", "startOffset": 89, "endOffset": 108}, {"referenceID": 9, "context": "While we knew from Hazan et al. (2013) that lnZ \u2212 L(0) \u2265 0, this is a stronger result showing that the size of the gap is an upper bound on the KL divergence between the approximate sampling distribution of x\u2217\u2217 and the Gibbs distribution p.", "startOffset": 19, "endOffset": 39}, {"referenceID": 18, "context": "Fourth, viewed as a function of the Gumbel perturbations \u03b3, the random variable U has a bounded gradient, allowing earlier measure concentration results (Orabona et al., 2014; Hazan et al., 2016).", "startOffset": 153, "endOffset": 195}, {"referenceID": 10, "context": "Fourth, viewed as a function of the Gumbel perturbations \u03b3, the random variable U has a bounded gradient, allowing earlier measure concentration results (Orabona et al., 2014; Hazan et al., 2016).", "startOffset": 153, "endOffset": 195}, {"referenceID": 9, "context": ", 2014; Hazan et al., 2016). Proving similar measure concentration results for the expectations E[e\u2212\u03b1U ] appearing in U(\u03b1) for \u03b1 6= 0 may be more challenging. 5. Experiments We conducted experiments with the following aims: 1. To show that the higher efficiency of the Exponential trick in the full-rank perturbation setting is useful in practice, we compared it to the Gumbel trick in A* sampling (Maddison et al., 2014) (Section 5.1) and in the large-scale discrete sampling setting of Chen & Ghahramani (2016) (Section 5.", "startOffset": 8, "endOffset": 513}, {"referenceID": 15, "context": "A* Sampling A* sampling (Maddison et al., 2014) is a sampling algorithm for continuous distributions that perturbs the logunnormalized density \u03c6 with a continuous generalization of the Gumbel trick, called the Gumbel process, and uses a variant of A* search to find the location of the maximum of the perturbed \u03c6. Returning the location yields an exact sample from the original distribution, as in the discrete Gumbel trick. Moreover, the corresponding maximum value also has the Gumbel(\u2212c + lnZ) distribution (Maddison et al., 2014). Our analysis in Section 2.3 tells us that the Exponential trick yields an estimator with lower MSE than the Gumbel trick; we briefly verified this on the Robust Bayesian Regression experiment of Maddison et al. (2014). We constructed estimators of lnZ from the Gumbel and Exponential tricks (debiased version, see Section 2.", "startOffset": 25, "endOffset": 753}, {"referenceID": 17, "context": "We replaced the expectations in U(\u03b1)\u2019s with sample averages of size M = 100, using libDAI (Mooij, 2010) to solve the MAP problems yielding these samples.", "startOffset": 90, "endOffset": 103}, {"referenceID": 27, "context": "Low-rank Perturbation Bounds on lnZ Hazan & Jaakkola (2012) evaluated tightness of the Gumbel trick upper bound U(0) \u2265 lnZ on 10\u00d7 10 binary spin glass models.", "startOffset": 36, "endOffset": 60}, {"referenceID": 9, "context": "Sequential samplers for the Gibbs distribution The family of sequential samplers for the Gibbs distribution presented in the main text as Algorithm 1 has the same overall structure as the sequential sampler derived by Hazan et al. (2013) from the Gumbel trick upper bound U(0), and hence correctness can be argued similarly.", "startOffset": 218, "endOffset": 238}, {"referenceID": 27, "context": "The following results links together the errors acquired when using summed unary perturbations to upper bound the log partition function lnZ \u2264 U(0) using the Gumbel trick upper bound by Hazan & Jaakkola (2012), to approximately sample from the Gibbs distribution by using qsum instead, and to upper bound the entropy of the approximate distribution qsum using the bound due to Maji et al.", "startOffset": 186, "endOffset": 210}, {"referenceID": 16, "context": "The following results links together the errors acquired when using summed unary perturbations to upper bound the log partition function lnZ \u2264 U(0) using the Gumbel trick upper bound by Hazan & Jaakkola (2012), to approximately sample from the Gibbs distribution by using qsum instead, and to upper bound the entropy of the approximate distribution qsum using the bound due to Maji et al. (2014). Proposition 11.", "startOffset": 377, "endOffset": 396}, {"referenceID": 9, "context": "The Gumbel trick lower bound on the log partition function lnZ due to Hazan et al. (2013) is:", "startOffset": 70, "endOffset": 90}, {"referenceID": 16, "context": "To this end, we first need an entropy bound for qavg analogous to Theorem 1 of (Maji et al., 2014).", "startOffset": 79, "endOffset": 98}, {"referenceID": 16, "context": "Theorem 1 of (Maji et al., 2014) and this Theorem 15 differ in three aspects: (1) the former is an upper bound and the latter is a lower bound, (2) the former sums the expectations while the latter averages them, and (3) the distributions qsum and qavg of x\u2217 in the two theorems are different.", "startOffset": 13, "endOffset": 32}, {"referenceID": 10, "context": "The second revision of (Hazan et al., 2016) computes the derivative of U\u03c6(0) \u2212 \u2211 x\u2208X qsum(x)\u03c6(x), which is similar to our L\u03c6(0)\u2212 \u2211 x\u2208X qavg(x)\u03c6(x), by differentiating under the expectation.", "startOffset": 23, "endOffset": 43}, {"referenceID": 14, "context": "This proof proceeded in the same way as the proof of Maji et al. (2014) for the upper bound, except that establishing the minimizing configuration of the infimum is a non-trivial step that is actually required in this case.", "startOffset": 53, "endOffset": 72}, {"referenceID": 9, "context": "While we knew from Hazan et al. (2013) that lnZ \u2212 L(0) \u2265 0 (i.", "startOffset": 19, "endOffset": 39}], "year": 2017, "abstractText": "The Gumbel trick is a method to sample from a discrete probability distribution, or to estimate its normalizing partition function. The method relies on repeatedly applying a random perturbation to the distribution in a particular way, each time solving for the most likely configuration. We derive an entire family of related methods, of which the Gumbel trick is one member, and show that the new methods have superior properties in several settings with minimal additional computational cost. In particular, for the Gumbel trick to yield computational benefits for discrete graphical models, Gumbel perturbations on all configurations are typically replaced with socalled low-rank perturbations. We show how a subfamily of our new methods adapts to this setting, proving new upper and lower bounds on the log partition function and deriving a family of sequential samplers for the Gibbs distribution. Finally, we balance the discussion by showing how the simpler analytical form of the Gumbel trick enables additional theoretical results.", "creator": "LaTeX with hyperref package"}}}