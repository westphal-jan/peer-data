{"id": "1706.04208", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Hybrid Reward Architecture for Reinforcement Learning", "abstract": "One of the main challenges in reinforcement learning (RL) is generalisation. In typical deep RL methods this is achieved by approximating the optimal value function with a low-dimensional representation using a deep network. While this approach works well in many domains, in domains where the optimal value function cannot easily be reduced to a low-dimensional representation, learning can be very slow and unstable. This paper contributes towards tackling such challenging domains, by proposing a new method, called Hybrid Reward Architecture (HRA). HRA takes as input a decomposed reward function and learns a separate value function for each component reward function. Because each component typically only depends on a subset of all features, the overall value function is much smoother and can be easier approximated by a low-dimensional representation, enabling more effective learning. We demonstrate HRA on a toy-problem and the Atari game Ms. Pac-Man, where HRA achieves above-human performance.", "histories": [["v1", "Tue, 13 Jun 2017 18:05:48 GMT  (1018kb,D)", "http://arxiv.org/abs/1706.04208v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["harm van seijen", "mehdi fatemi", "joshua romoff", "romain laroche", "tavian barnes", "jeffrey tsang"], "accepted": true, "id": "1706.04208"}, "pdf": {"name": "1706.04208.pdf", "metadata": {"source": "CRF", "title": "Hybrid Reward Architecture for Reinforcement Learning", "authors": ["Harm van Seijen", "Mehdi Fatemi", "Jeffrey Tsang"], "emails": ["harm.vanseijen@microsoft.com", "mehdi.fatemi@microsoft.com", "joshua.romoff@mail.mcgill.ca", "romain.laroche@microsoft.com", "tavian.barnes@microsoft.com", "tsang.jeffrey@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that you are able to hide without being able to play by the rules, and that you are able to play by the rules."}, {"heading": "2 Related Work", "text": "Our HRA methodology builds on the Horde architecture (Sutton et al., 2011), but the Horde architecture consists of a large number of \"demons\" learning in parallel through non-political learning. Each demon trains a separate general value function (GVF) based on its own policy and pseudo-reward function. A pseudo-reward can be any function-based signal that encodes useful information. The Horde architecture focuses on building general knowledge about the world, encoded through a large number of GVFs. HRA focuses on training individual components of the environmental reward function to achieve a smoother value function in order to efficiently learn a tax policy.Learning related to multiple reward functions is also a topic of multi-purpose learning (Roijers et al., 2013)."}, {"heading": "3 Model", "text": "Consider a Markov Decision Process (MDP) that models an active ingredient that interacts with an environment in discrete time steps. It has a state set S, action set A, ambient reward functionRenv: S \u00b7 A \u2192 R and a transition probability function P: S \u00b7 A \u00b7 S \u2192 [0, 1]. The active ingredient observes the next state st + 1, which is derived from the transition probability function P (st, at) and a reward rt = Renv (st, at). The behavior is defined by a policy \u03c0: S \u00b7 A \u2192 [0, 1], which represents the selection probabilities over actions. The goal of an active ingredient is to find a policy that maximizes the expected yield, that is the discounted sum of rewards: Gt: Gt: = Intention i = 0 ctui rt + i, where the contraction factor conversely [0, 1] improves the importance of immediate rewards over the expected function by controlling the corresponding measure Q-E."}, {"heading": "3.1 Hybrid Reward Architecture", "text": "Since a Q value function is highly dimensional, it is typically equated with a deep network with parameters Q = Q cement functions: Q (s, a; \u03b8). DQN estimates the optimal Q value function by minimizing the order of loss functions: Li (\u03b8i) = Es, a, r, s \"(HRQ, s\"). (yDQNi \u2212 Q \"s reward scheme (s, a). (yDQNi \u2212 s\" reward scheme \"), (3) where these parameters are a target network frozen for a number of iterations, while the online network Q (s, a) is updatededededed.Let the reward function of the environment be Renv. We propose to regulate the target function of the deep network by dividing the reward function into n reward functions weighted by wi: s (a)."}, {"heading": "3.2 Improving Performance further by using high-level domain knowledge.", "text": "However, one of the strengths of HRA is that it can easily exploit more domain knowledge when it is available. Domain knowledge can be used in one of the following ways: 3. Using pseudo-reward functions. Instead of updating a leader of HRA using an environmental reward component, it can be updated using a pseudo-reward. In this scenario, a number of GVFs are trained in parallel using pseudo-rewards. Each leader of HRA uses (one) appropriate GVF (s), which can often lead to more efficient learning. The first two types of domain knowledge can be used with any method, not just HRA. However, as HRA can apply this knowledge to each leader individually, it can exploit domain knowledge on a much larger scale. We show this empirically in Section 4.1."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Fruit Collection task", "text": "This year is the highest in the history of the country."}, {"heading": "5 Discussion", "text": "One of the strengths of HRA is that it can exploit domain knowledge to a much greater extent than single-head methods. This is clearly evident from the task of fruit collection: while removing irrelevant features results in a big performance improvement for HRA, DQN has not experienced effective learning under the same network architecture. Furthermore, splitting the pixel image into multiple binary channels only leads to a small improvement in the performance of A3C over direct learning from pixels. This shows that the reason why modern deep RL problems with Ms. Pac-Man are not related to learning from pixels; the underlying problem is that the optimal value function for Ms. Pac-Man cannot simply be assigned to a low-dimensional representation. HRA solves Ms. Pac-Man by learning nearly 1800 general value functions, resulting in an exponential breakdown of problem size: while the input state, which corresponds to the channels of each order of magnitude, is in 10F."}, {"heading": "A Ms. Pac-Man - experimental details", "text": "The second domain is the Atari 2600. The second domain is the Atari 2600. It both starts by controlling pellets while avoiding ghosts (contact with a woman causes Pac-Man to lose a life).There is a new level where you can flip the ghost over for a short duration so that it can be eaten for extra points. Bonus fruits can be eaten for additional points, twice per level. Once all pellets have been eaten, a new level is started. There are a total of 4 different cards (see Figure 7 and Table 2) and 7 different fruit types, each with a different score (see Table 3).Ms. Pac-Man is set as one of the tough games from the ALE benchmark. When comparing performance, it is important to realize that there are two different evaluation methods ALE games used in the literature (see Table 4)."}], "references": [{"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The option-critic architecture", "author": ["P. Bacon", "J. Harb", "D. Precup"], "venue": "In Proceedings of the Thirthy-first AAAI Conference On Artificial Intelligence (AAAI),", "citeRegEx": "Bacon et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bacon et al\\.", "year": 2017}, {"title": "Recent advances in hierarchical reinforcement learning", "author": ["A.G. Barto", "S. Mahadevan"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "Barto and Mahadevan,? \\Q2003\\E", "shortCiteRegEx": "Barto and Mahadevan", "year": 2003}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Cortex and mind: Unifying cognition", "author": ["J.M. Fuster"], "venue": "Oxford university press,", "citeRegEx": "Fuster,? \\Q2003\\E", "shortCiteRegEx": "Fuster", "year": 2003}, {"title": "Learning and memory: From brain to behavior", "author": ["M.A. Gluck", "E. Mercado", "C.E. Myers"], "venue": "Palgrave Macmillan,", "citeRegEx": "Gluck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gluck et al\\.", "year": 2013}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In Proceedings of the IEEE international conference on computer vision,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["T.D. Kulkarni", "K.R. Narasimhan", "A. Saeedi", "J.B. Tenenbaum"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "Kumaran", "H. King D", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518:529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T. Harley", "T.P. Lillicrap", "D. Silver", "K. Kavukcuoglu"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["A. Nair", "P. Srinivasan", "S. Blackwell", "C. Alcicek", "R. Fearon", "Maria", "A. De", "V. Panneershelvam", "M. Suleyman", "C. Beattie", "S. Petersen", "S. Legg", "V. Mnih", "K. Kavukcuoglu", "D. Silver"], "venue": "Deep Learning Workshop,", "citeRegEx": "Nair et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2015}, {"title": "A survey of multi-objective sequential decision-making", "author": ["D.M. Roijers", "P. Vamplew", "S. Whiteson", "R. Dazeley"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Roijers et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Roijers et al\\.", "year": 2013}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Schaul et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2016}, {"title": "Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010)", "author": ["J. Schmidhuber"], "venue": "In IEEE Transactions on Autonomous Mental Development", "citeRegEx": "Schmidhuber,? \\Q2010\\E", "shortCiteRegEx": "Schmidhuber", "year": 2010}, {"title": "Intrinsically motivated reinforcement learning: A promising framework for developmental robotics", "author": ["A. Stout", "G. Konidaris", "A.G. Barto"], "venue": "In The AAAI Spring Symposium on Developmental Robotics,", "citeRegEx": "Stout et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Stout et al\\.", "year": 2005}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "Precup", "Doina"], "venue": "In Proceedings of 10th International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Sutton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2011}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["R.S. Sutton", "D. Precup", "S.P. Singh"], "venue": "Artificial Intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Algorithms for reinforcement learning", "author": ["C. Szepesv\u00e1ri"], "venue": null, "citeRegEx": "Szepesv\u00e1ri,? \\Q2009\\E", "shortCiteRegEx": "Szepesv\u00e1ri", "year": 2009}, {"title": "Learning values across many orders of magnitude", "author": ["H. van Hasselt", "A. Guez", "M. Hessel", "V. Mnih", "D. Silver"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Deep reinforcement learning with double q-learning", "author": ["H. van Hasselt", "A. Guez", "D. Silver"], "venue": "In AAAI, pp. 2094\u20132100,", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "A theoretical and empirical analysis of expected sarsa", "author": ["H. van Seijen", "H. van Hasselt", "S. Whiteson", "M. Wiering"], "venue": "In IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL),", "citeRegEx": "Seijen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Seijen et al\\.", "year": 2009}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["A. Vezhnevets", "V. Mnih", "S. Osindero", "A. Graves", "O. Vinyals", "J. Agapiou", "K. Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Vezhnevets et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vezhnevets et al\\.", "year": 2016}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Z. Wang", "T. Schaul", "M. Hessel", "H. van Hasselt", "M. Lanctot", "N. Freitas"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "The best method with fixed start evaluation is STRAW with 6,673 points (Vezhnevets et al., 2016); the best with random start evaluation is the dueling network architecture with 2,251 points (Wang et al., 2016)", "author": ["Mnih"], "venue": "The human baseline,", "citeRegEx": "Mnih,? \\Q2015\\E", "shortCiteRegEx": "Mnih", "year": 2015}, {"title": "A3C baselines Since we use low-level features for the HRA architecture, we implement A3C and evaluate it both on the pixel-based environment and on the low-level features. The implementation is performed in a way to reproduce the results", "author": ["Mnih"], "venue": "Mnih et al", "citeRegEx": "Mnih,? \\Q2015\\E", "shortCiteRegEx": "Mnih", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "In reinforcement learning (RL) (Sutton & Barto, 1998; Szepesv\u00e1ri, 2009), the goal is to find a behaviour policy that maximises the return\u2014the discounted sum of rewards received over time\u2014in a data-driven way.", "startOffset": 31, "endOffset": 71}, {"referenceID": 8, "context": "Mnih et al. (2015) achieved a big breakthrough in this area: by combining standard RL techniques with deep neural networks, they outperformed humans on a large number of Atari 2600 games, by learning a policy from pixels.", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "Intrinsic motivation (Stout et al., 2005; Schmidhuber, 2010) uses the above observation to improve learning in sparse-reward domains.", "startOffset": 21, "endOffset": 60}, {"referenceID": 13, "context": "Intrinsic motivation (Stout et al., 2005; Schmidhuber, 2010) uses the above observation to improve learning in sparse-reward domains.", "startOffset": 21, "endOffset": 60}, {"referenceID": 16, "context": "Similar to the Horde architecture (Sutton et al., 2011), all these agents can learn in parallel on the same sample sequence by using off-policy learning.", "startOffset": 34, "endOffset": 55}, {"referenceID": 3, "context": "Pac-Man, a hard game from the ALE benchmark set (Bellemare et al., 2013).", "startOffset": 48, "endOffset": 72}, {"referenceID": 16, "context": "Our HRA method builds upon the Horde architecture (Sutton et al., 2011).", "startOffset": 50, "endOffset": 71}, {"referenceID": 11, "context": "Learning with respect to multiple reward functions is also a topic of multi-objective learning (Roijers et al., 2013).", "startOffset": 95, "endOffset": 117}, {"referenceID": 17, "context": "This work is also related to options (Sutton et al., 1999; Bacon et al., 2017), and more generally hierarchical learning (Barto & Mahadevan, 2003; Kulkarni et al.", "startOffset": 37, "endOffset": 78}, {"referenceID": 1, "context": "This work is also related to options (Sutton et al., 1999; Bacon et al., 2017), and more generally hierarchical learning (Barto & Mahadevan, 2003; Kulkarni et al.", "startOffset": 37, "endOffset": 78}, {"referenceID": 7, "context": ", 2017), and more generally hierarchical learning (Barto & Mahadevan, 2003; Kulkarni et al., 2016).", "startOffset": 50, "endOffset": 98}, {"referenceID": 9, "context": "Hence, to test the effect of our preprocessing, we implement the A3C method (Mnih et al., 2016) and run it with our preprocessing.", "startOffset": 76, "endOffset": 95}, {"referenceID": 0, "context": "It is inspired by upper confidence bounds (Auer et al., 2002).", "startOffset": 42, "endOffset": 61}, {"referenceID": 4, "context": "For our final experiment, we implement a special head inspired by executive-memory literature (Fuster, 2003; Gluck et al., 2013).", "startOffset": 94, "endOffset": 128}, {"referenceID": 5, "context": "For our final experiment, we implement a special head inspired by executive-memory literature (Fuster, 2003; Gluck et al., 2013).", "startOffset": 94, "endOffset": 128}, {"referenceID": 22, "context": "The best reported fixed start score comes from STRAW (Vezhnevets et al., 2016); the best reported random start score comes from the Dueling network architecture (Wang et al.", "startOffset": 53, "endOffset": 78}, {"referenceID": 23, "context": ", 2016); the best reported random start score comes from the Dueling network architecture (Wang et al., 2016).", "startOffset": 90, "endOffset": 109}, {"referenceID": 8, "context": "The human fixed start score comes from Mnih et al. (2015); the human random start score comes from Nair et al.", "startOffset": 39, "endOffset": 58}, {"referenceID": 8, "context": "The human fixed start score comes from Mnih et al. (2015); the human random start score comes from Nair et al. (2015). We train A3C for 800 million frames.", "startOffset": 39, "endOffset": 118}], "year": 2017, "abstractText": "One of the main challenges in reinforcement learning (RL) is generalisation. In typical deep RL methods this is achieved by approximating the optimal value function with a low-dimensional representation using a deep network. While this approach works well in many domains, in domains where the optimal value function cannot easily be reduced to a low-dimensional representation, learning can be very slow and unstable. This paper contributes towards tackling such challenging domains, by proposing a new method, called Hybrid Reward Architecture (HRA). HRA takes as input a decomposed reward function and learns a separate value function for each component reward function. Because each component typically only depends on a subset of all features, the overall value function is much smoother and can be easier approximated by a low-dimensional representation, enabling more effective learning. We demonstrate HRA on a toy-problem and the Atari game Ms. Pac-Man, where HRA achieves above-human performance.", "creator": "LaTeX with hyperref package"}}}