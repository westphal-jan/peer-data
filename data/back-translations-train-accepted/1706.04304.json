{"id": "1706.04304", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "Dueling Bandits with Weak Regret", "abstract": "We consider online content recommendation with implicit feedback through pairwise comparisons, formalized as the so-called dueling bandit problem. We study the dueling bandit problem in the Condorcet winner setting, and consider two notions of regret: the more well-studied strong regret, which is 0 only when both arms pulled are the Condorcet winner; and the less well-studied weak regret, which is 0 if either arm pulled is the Condorcet winner. We propose a new algorithm for this problem, Winner Stays (WS), with variations for each kind of regret: WS for weak regret (WS-W) has expected cumulative weak regret that is $O(N^2)$, and $O(N\\log(N))$ if arms have a total order; WS for strong regret (WS-S) has expected cumulative strong regret of $O(N^2 + N \\log(T))$, and $O(N\\log(N)+N\\log(T))$ if arms have a total order. WS-W is the first dueling bandit algorithm with weak regret that is constant in time. WS is simple to compute, even for problems with many arms, and we demonstrate through numerical experiments on simulated and real data that WS has significantly smaller regret than existing algorithms in both the weak- and strong-regret settings.", "histories": [["v1", "Wed, 14 Jun 2017 03:44:32 GMT  (1612kb,D)", "http://arxiv.org/abs/1706.04304v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bangrui chen", "peter i frazier"], "accepted": true, "id": "1706.04304"}, "pdf": {"name": "1706.04304.pdf", "metadata": {"source": "META", "title": "Dueling Bandits with Weak Regret", "authors": ["Bangrui Chen", "Peter I. Frazier"], "emails": ["<pf98@cornell.edu>."], "sections": [{"heading": "1. Introduction", "text": "This year, the time has come for us to be able to set out in search of new paths that will lead us into the future."}, {"heading": "2. Related Work", "text": "Most work on dueling bandits focuses on strong regret. Yue et al. (2012) shows that the worst possible expected cumulative strong regret for some time is T for each algorithm. (N log (T)) Algorithms have been proposed that reach this lower limit under the Condorcet winning assumption in the final horizon setting: Interleaved Filter (IF) (Yue et al., 2012) and Beat the Mean (Yue & Joachims, 2011). Relative Upper Confidence Bound (RUCB) (Zoghi et al., 2014) also reaches this lower limit in the horizonless setting. (RMED) Relative Minimum Empirical Divergence (RMED et al., 2015) is the first algorithm to tie a regret to this lower limit. Zoghi et al al al al al al al al al al al al al. (2015) suggested two algorithms, Copeland Confiable and Scalable."}, {"heading": "3. Problem Formulation", "text": "Each time t = 1, 2,.. the system selects two items and displays them to the user, i.e. the system performs a duel between two arms. The user then gives binary feedback indicating his preferred item and determines which arm wins the duel. This binary feedback is random and conditionally independent of all past interactions with regard to the pair of arms shown. We also let pi, j indicate the probability that the user gives feedback indicating a preference for arm i when arms i and j. If the user prefers arm i over arm j, we assume pi, j > 0,5. We also assume symmetry: pi, j = 1 \u2212 pj, i.e. we assume that arm 1 is a condorcet winner, i.e. that p1, i > 0.5 for i = 2, \u00b7, n. In some results we also consider the setting in which arms are a total order, which we have < that means that Items is 0.5."}, {"heading": "4. Winner Stays", "text": "We now propose an algorithm called Winner Stays (WS), with two variants: WS-W for weak regret and WS-S for strong regret. Section 4.1 introduces WS-W and illustrates its dynamics. Section 4.2 proves that the expected cumulative weak regret of WS-W under the Condorcet winning setting O (N2) and O (N log (N log (N))) is below the total job setting. Section 4.3 introduces WS-S and proves that its expected cumulative strong regret is O (N2 + N log (T) under the Condorcet winning setting and O (N log (T) + N log (N)) under the total job setting, both of which have an optimal dependence on T. Section 4.4 extends our theoretical results to service-based bandits."}, {"heading": "4.1. Winner Stays with Weak Regret (WS-W)", "text": "It is not the first and the second, which it is in the second and third rounds of the ES- and the third round of the ES- and the third, which it is in the third and fourth rounds of the ES- and the third round of the ES- and the third round of the ES- and the third round of the ES- and the third round of the ES- and the second round of the ES- and the third round of the ES- and the third round of the ES- and the third round of the ES- and the third round of the ES- and the third round of the ES- and the third round of the two ES- and the third round of the ES- and the third round of the ES- and the two ES- and the third row of the two ES- and the third round of the ES- and the two ES- and the third round of the ES- and the two ES- and the third row of the two ES- and the third row of the two ES- and the two arms of the two W- and the two WS- and the two WS- and the two WS- and the third arms, the two WS- and the two WS- and the two and the two WS- and the third arms."}, {"heading": "4.2. Analysis of WS-W", "text": "In this section, we analyze the weak regret of the WS-W. After submitting definitions and preliminary results, we must prove that WS-W expected the cumulative weak regret surrounded by O (N2) when we include the probative force of all lemmas in the complement. We define t, \"the beginning of the round, and Z (\" \u2212 1) as the unique time and arm of such C (t \"\u2212 1) = (N \u2212 1). We define.\" + 1 for all i = Z (\"\u2212 1).We define t,\" the beginning of the iteration k in the round, \"as the first time we draw the kth unique pair in the round.\""}, {"heading": "4.3. Winner Stays with Strong Regret (WS-S)", "text": "In this section, we define a version of WS for strong regret, WS-S, which uses WS-W as a subroutine. < WS-S is defined in Algorithm 2p, and therefore log indeed consists of an exploration phase and an exploitation phase. The length of the exploitation phase increases exponentially with the number of phases. Changing the parameter \u03b2 balances the lengths of these phases and thus the balances between the exploration phase and the exploitation phase. Choose our theoretical results under instruction \u03b2.Algorithm 2 WS-S Input: \u03b2 > 1, Weapons 1, \u00b7 N for \"= 1, \u00b7 2 \u00b7 doExploration phase: Run the\" th round of WS-W. Exploitation phase: Let Z (\") be the index of the best arm at the end of the\" th round. For the next b\u03b2'c periods, we draw Z (\") and ignore the cumulative round. We now have the strong cumulative."}, {"heading": "4.4. Extension to Utility-Based Regret", "text": "Suppose the user has a ui utility connected to each arm i. Without loss of generality, we assume u1 > u2 > \u00b7 \u00b7 > uN, and as with the overall job creation, we demand that pi, j > 0.5, if i < j. Typically, the pi, j would come via a generative model from the utilities of arms i and j. We give an example in our numerical experiments. Then, the utility-based weak regret for a period r (t) = u1 \u2212 max {uit, ujt}, which is the difference in the utility between the best arm overall and the best arm the user can select from the ones offered. The utility-based strong regret for a period is r (t) = u1 \u2212 uit + ujt \u00b2. To get this utility, the strongest regret for the period (W) must be drawn."}, {"heading": "5. Numerical Experiments", "text": "In this section, we evaluate WS among both weak and strong regret settings, taking into account both their original (binary) and benefit-based versions. In the weak regret setting, we compare WS-W with RUCB and QSA. In the strong regret setting, we compare WS-S with 7 benchmarks, including RUCB and Relative Minimum Empirical Divergence (RMED) by Komiyama et al. (2015)."}, {"heading": "5.1. Weak Regret", "text": "We are now comparing WS-W with QSA and RUCB using simulated data and the academic Yelp dataset (Yelp, 2012)."}, {"heading": "5.1.1. SIMULATED DATA", "text": "In this example, we compare WS-W with RUCB and QSA for a problem with 50 arms and binary weak regret. Each arm is a 20-dimensional vector evenly generated from the unit circle. We assume pi, j = 0.8 for all i < j. The results are summarized in Figure 2a. RUCB has approximately linear regret over the time horizon shown. This is common in dueling bandit literature, where many algorithms require 104 comparisons before they reach a log (T) cumulative regret for 50 arms. WS-W finds the optimal arm after x 500 comparisons and has a regret consistent with our theoretically established constant expected cumulative weak regret."}, {"heading": "5.1.2. YELP ACADEMIC DATASET", "text": "In this example, we compare WS-W with RUCB and QSA using the academic Yelp dataset (Yelp, 2012) and usage-based vulnerabilities. We select 100 Las Vegas restaurants as our arms. Associated with each arm (restaurant), i is a 20-dimensional feature vector Ai, which is calculated using doc2vec (Rehurek & Sojka, 2010) from its ratings. We select 49 users who have rated at least 20 of these 100 restaurants. For each user, we model their use for restaurant i as ui = Ai \u00b7 \u03b8, which is a 20-dimensional vector of preferences. We then exclude this for each user based on linear regression.To model pi, j, we then use the probit model."}, {"heading": "5.2. Strong Regret", "text": "In this section, we compare WS-S with binary and utility-based strong regret with 7 literature benchmarks. We use the sushi and MSLR datasets previously used by Komiyama et al. (2016) and Zoghi et al. (2015) to evaluate dueling bandit algorithms. Sushi dataset (Komiyama et al., 2016) contains 16 arms that correspond to the types of sushi, with paired preferences derived from data on sushi preferences of 5000 users in Kamishima (2003). We specify preference matrices (pi, j) for both datasets in the supplement. In usage-based regret, we define ui = 2 (1 \u2212 p1, i).WS has a user-defined parameter \u03b2 /.In our comparison problems, we consider 1.1 and less references for both groups."}, {"heading": "6. Conclusion", "text": "We propose a new algorithm, WS, with variants designed for the Settings of Weak Regret (WS-W) and Strong Regret (WS-S). We prove that WS has consistently weak regret and optimally strong regret in T. In numerical experiments, WS outperforms all benchmarks taken into account on both simulated and real data sets."}, {"heading": "Acknowledgements", "text": "The authors were partially supported by NSF CAREER CMMI-1254298, NSF CMMI-1536895, NSF IIS-1247696, NSF DMR-1120296, AFOSR FA9550-12-1-0200, AFOSR FA9550-15-1-0038 and AFOSR FA9550-16-1-0046."}, {"heading": "A. Gambler\u2019s Ruin Lemma", "text": "In our analysis of WS-W, we will use the results of a special case of the player's ruin problem (Karlin, 1968), which was performed as follows: Suppose a player initially has millions of dollars. In each of these rounds, he loses one dollar with the probability q 6 = 12 and wins 1 dollar with the probability 1 \u2212 q. he stops playing if he has either m + 1 dollars or no money left. We have the following result with a proof on page 73 of Karlin (1968).Lemma 6 (Gambler's Ruin Lemma).In the player's ruin problem: (1) the probability that the player reaches m + 1 dollars before reaching 0 dollars is qm = (1 \u2212 qq) m \u2212 1 (1 \u2212 qq) m \u2212 1; (2) the expected number of steps before the player's ruin problem is m1 \u2212 2q \u2212 1 \u2212 qm \u2212 1 (1 \u2212 qq) m \u2212 1; (2) the expected number of steps before the player's ruin problem is 2q \u2212 1 \u2212 qm \u2212 1 \u2212 qm."}, {"heading": "B. Proof of Lemma 1", "text": "Proof. Suppose we compare arm i \u2212 \u2212 \u2212 arm j in this iteration with i > j and arm i is the incumbent. Then we know that C (t ', k \u2212 1, i) = (N \u2212 1) (' \u2212 1) + k \u2212 1 and C (t ', k \u2212 1, j) = \u2212 \"+ 1. We play these two arms until C (t', k + T ', k \u2212 1, i) = (N \u2212 1) (' \u2212 1) + k or C (t ', k + T', k \u2212 1, j) = (N \u2212 1) (n \u2212 1) + k. Since the probability of winning arm i over arm j pi pi, j over this period is pi, j, we know that the dynamic of this iteration is the same as that of Gambler's Ruin problem. Denote E = C (t ', k \u2212 1, i) \u2212 C (t', k \u2212 1 \u2212 j) + kk."}, {"heading": "C. Proof of Lemma 2", "text": "In this section, we prove Lemma 2 from the main work. This section is structured as follows: In Section C.1, we provide two limits on the likelihood of losing and winning the incumbent: In Section C.2, we consider a version of the problem in which better and worse incumbents have constant (but different) odds of winning and provide a cap on the number of worse incumbents in a round before a better incumbent, starting with a round; in Section C.4, we demonstrate a similar limit for the expected number of incumbents before a better incumbent, starting with a worse number of incumbents before a worse incumbent before a better incumbent, beginning with a round; in Section C.4, we demonstrate a similar limit for the expected number of incumbents before a better incumbent, beginning with the beginning of a round; in Section C.5, we complete the proof of Lemma 2.Through this section, we conduct a one to a correspondence between n and (, \"k) defined by n (1. =)."}, {"heading": "D. Proof of Lemma 3", "text": "Proof. It is easy to see that in the last iteration, which has a worse incumbent, the better arm is always arm 1. Therefore, in this proof, we only consider C (t, 1). At the end of the \"th round,\" ifC (t) = W (t) + 1 \u2212 1 with probability p > 12 and W (t + 1) = W (t) \u2212 1 with probability 1 \u2212 p = \u2212. \"Let us consider a simple random path W (t) so that W (t) + 1 with probability p > 12 and W (t + 1) = W (t) \u2212 1 with probability 1 \u2212 p. If we refer to p (t) as P (t) with probability, W (t) = \u2212\") with \"> 0, then it is easy to calculate that p \u00b2 = (1 \u2212 p) with probability p (t) and p (t) with probability 1, 1)."}, {"heading": "E. Proof of Lemma 4", "text": "Proof: To show the first claimed equation we have: E [B (', k) T', kD (', k) T = E [B (', k) T ', k | D (') = 1] P (') = 1). (8) The first term E [B (', k) T ', k | D (', k) T '(', k) = 1 can be limited by using it as E [B (', k) T', k | D (') (') = E [E [B (', k) T', k '(', k) ', with A (', k) T ', k', round. (') We concentrate on the inner term E [B (', k) T (', k) T', k | D (', k)."}, {"heading": "F. Proof of Lemma 5", "text": "Proof: For the first inequality, we know E [N \u2212 1 \u2211 k = 1 B (', k) T' (', k) T' (', k) T', k | D (') = 0] D (', k)]. (9) Furthermore, we know that E [B '(', k) T, k | D (') = 0] = E [T', k | B (', k) = 0, D (') = 0] P (B (', k) = 0 | D (') = 0 | D (') \u2264 N' 2p \u2212 1 P (', k) = 0 [D (') = 0), with the last equation following the application of Lemma 1 and the iterated conditional expectation. From this we know (9) = N \u2212 1 \u0445 k = 1 N '2p \u2212 1 P (', k) = 0 \u2212 D (') (', k) = 0 [D] (', k = 1, '(p), = (p), (1), \u2264 (p)."}, {"heading": "G. Proof of Theorem 2", "text": "In this section, we prove that the cumulative expected weak regret of WS-W in the Condorcet winning setting is limited by O (N2). First, we want to give an example to illustrate why our algorithm will not have an O (N log (N)) regret in the Condorcet winning setting. (In the Condorcet winning setting, Lemma 2 is no longer true. (Here is a counterexample to illustrate why Lemma 2 is no longer true.) Suppose we have a total of N = 3k + 1 arms in the Condorcet winning setting and three types of other arms: k Type-A weapons, k Type-B weapons and k Type-C weapons. (Suppose we prefer Type-B weapons in the overall setting), Type-B weapons in the overall setting as Type-C weapons, and Type-C arms as Type-A weapons in the overall setting. (Suppose we prefer Type-C weapons in the overall setting), Type-C weapons as Type-A weapons in the overall setting. (Suppose we prefer Type-B weapons in the overall setting), Type-B weapons in the overall setting as Type-C weapons and Type-C weapons in the overall setting. (Suppose we prefer Type-C weapons in the overall setting), Type-B weapons in the overall setting, Type-C weapons in the overall setting. (Suppose we prefer Type-B weapons in the overall setting), Type-B weapons in the overall setting as Type-C weapons and Type-C weapons in the overall setting."}, {"heading": "H. Preference Matrices", "text": "In the sushi experiment, the user preference matrix is indicated by Figure 4.In the MSLR experiment, the ranking preference matrix is indicated by the following information: 0.5 0.535 0.613 0.757 0.7650.465 0.5 0.580 0.727 0.738 0.387 0.420 0.5 0.659 0.669 0.243 0.276 0.341 0.5 0.510 0.235 0.262 0.331 0.490 0.5"}, {"heading": "I. Condorcet Winner Experiment", "text": "This is common in the literature of dueling bandits, where even works that take into account more general attitudes theoretically test their methods for problems consistent with the adoption of the overall order (Komiyama et al., 2016; Urvoy et al., 2013).In this section, we will consider an additional example that has a Condorcet winner but does not have an overall order among weapons, which has a cyclic verse and is similar to the cyclic example in Komiyama et al. (2015).The preference matrix is: 0.5 0.6 0.6 0.6 0.6 0.6 0.4 0.4 0.5 0.6 0.4 0.4 0.4 0.4 0.6 0.6 0.4 0.4 0.4 0.5 In the above example, Arm 1 beats Arm 3, Arm 3 beats Arm 4 and Arm 4 beats Arm 2.Again, we consider both binary strong regret and use-based strong regret as the same time as the other two user-defined results."}, {"heading": "J. Sensitivity Analysis", "text": "In this section, we perform a sensitivity analysis of \u03b2 in WSS using the MSLR dataset. In this analysis, we select \u03b2 = 1.01, 1.05, 1.1, 1.2, 1.5 and compare them with RMED and RUCB. The result is summarized in Figure 6.Based on Figure 6, WS-S with \u03b2 = 1.05, 1.1, 1.2 outperforms RMED and RUCB. If \u03b2 = 1.01, we spend too much time exploring and do not use enough. Likewise, WS-S with \u03b2 = 1.5 can explore over exploits and not enough. In both cases, WS-S performs worse than RMED and RUCB. However, as long as \u03b2 is within a reasonable range, WS-S can outperform existing state-of-art algorithms."}], "references": [{"title": "Reducing dueling bandits to cardinal bandits", "author": ["Ailon", "Nir", "Karnin", "Zohar Shay", "Joachims", "Thorsten"], "venue": "In ICML,", "citeRegEx": "Ailon et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2014}, {"title": "Change of time and change of measure, volume 21", "author": ["Barndorff-Nielsen", "Ole E", "Shiryaev", "Albert"], "venue": "World Scientific Publishing Co Inc,", "citeRegEx": "Barndorff.Nielsen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barndorff.Nielsen et al\\.", "year": 2015}, {"title": "Preference-based rank elicitation using statistical models: The case of mallows", "author": ["Busa-Fekete", "R\u00f3bert", "H\u00fcllermeier", "Eyke", "Sz\u00f6r\u00e9nyi", "Bal\u00e1zs"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Busa.Fekete et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Busa.Fekete et al\\.", "year": 2014}, {"title": "Fidelity, soundness, and efficiency of interleaved comparison methods", "author": ["Hofmann", "Katja", "Whiteson", "Shimon", "Rijke", "Maarten De"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "Hofmann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hofmann et al\\.", "year": 2013}, {"title": "Active ranking using pairwise comparisons", "author": ["Jamieson", "Kevin G", "Nowak", "Robert"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Jamieson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jamieson et al\\.", "year": 2011}, {"title": "Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search", "author": ["Joachims", "Thorsten", "Granka", "Laura", "Pan", "Bing", "Hembrooke", "Helene", "Radlinski", "Filip", "Gay", "Geri"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "Joachims et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Joachims et al\\.", "year": 2007}, {"title": "Nantonac collaborative filtering: recommendation based on order responses", "author": ["Kamishima", "Toshihiro"], "venue": "In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Kamishima and Toshihiro.,? \\Q2003\\E", "shortCiteRegEx": "Kamishima and Toshihiro.", "year": 2003}, {"title": "A First Course In Stochastic Processes", "author": ["Karlin", "Samuel"], "venue": null, "citeRegEx": "Karlin and Samuel.,? \\Q1968\\E", "shortCiteRegEx": "Karlin and Samuel.", "year": 1968}, {"title": "Regret lower bound and optimal algorithm in dueling bandit problem", "author": ["Komiyama", "Junpei", "Honda", "Junya", "Kashima", "Hisashi", "Nakagawa", "Hiroshi"], "venue": "In COLT,", "citeRegEx": "Komiyama et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Komiyama et al\\.", "year": 2015}, {"title": "Copeland dueling bandit problem: Regret lower bound, optimal algorithm, and computationally efficient algorithm", "author": ["Komiyama", "Junpei", "Honda", "Junya", "Nakagawa", "Hiroshi"], "venue": "arXiv preprint arXiv:1605.01677,", "citeRegEx": "Komiyama et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Komiyama et al\\.", "year": 2016}, {"title": "Bayes-optimal entropy pursuit for active choice-based preference learning", "author": ["Pallone", "Stephen N", "Frazier", "Peter I", "Henderson", "Shane G"], "venue": "arXiv preprint arXiv:1702.07694,", "citeRegEx": "Pallone et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Pallone et al\\.", "year": 2017}, {"title": "How does clickthrough data reflect retrieval quality", "author": ["Radlinski", "Filip", "Kurup", "Madhu", "Joachims", "Thorsten"], "venue": "In Proceedings of the 17th ACM conference on Information and knowledge management,", "citeRegEx": "Radlinski et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Radlinski et al\\.", "year": 2008}, {"title": "Software framework for topic modelling with large corpora", "author": ["Rehurek", "Radim", "Sojka", "Petr"], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. Citeseer,", "citeRegEx": "Rehurek et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rehurek et al\\.", "year": 2010}, {"title": "On the number of success in independent trials", "author": ["Y.H. Wang"], "venue": "Statistica Sinica,", "citeRegEx": "Wang,? \\Q1993\\E", "shortCiteRegEx": "Wang", "year": 1993}, {"title": "Interactively optimizing information retrieval systems as a dueling bandits problem", "author": ["Yue", "Yisong", "Joachims", "Thorsten"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Yue et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2009}, {"title": "Beat the mean bandit", "author": ["Yue", "Yisong", "Joachims", "Thorsten"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Yue et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2011}, {"title": "The k-armed dueling bandits problem", "author": ["Yue", "Yisong", "Broder", "Josef", "Kleinberg", "Robert", "Joachims", "Thorsten"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Yue et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2012}, {"title": "Relative upper confidence bound for the k-armed dueling bandit problem", "author": ["Zoghi", "Masrour", "Whiteson", "Shimon", "Munos", "Remi", "Rijke", "Maarten de"], "venue": "In JMLR Workshop and Conference Proceedings,", "citeRegEx": "Zoghi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zoghi et al\\.", "year": 2014}, {"title": "Copeland dueling bandits", "author": ["Zoghi", "Masrour", "Karnin", "Zohar S", "Whiteson", "Shimon", "De Rijke", "Maarten"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zoghi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zoghi et al\\.", "year": 2015}, {"title": "section, we consider an additional example that has a Condorcet winner but does not have a total order among arms. The example has a cyclic struture, and is similar to the cyclic example", "author": ["Komiyama"], "venue": null, "citeRegEx": "Komiyama,? \\Q2015\\E", "shortCiteRegEx": "Komiyama", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "Implicit pairwise comparisons avoid the inaccuracy of user ratings (Joachims et al., 2007) and the difficulty of engaging users in providing explicit feedback.", "startOffset": 67, "endOffset": 90}, {"referenceID": 3, "context": "Strong regret has been more widely studied, as discussed below, and has application to choosing ranking algorithms for search (Hofmann et al., 2013).", "startOffset": 126, "endOffset": 148}, {"referenceID": 11, "context": "To perform a duel, query results from two rankers are interleaved (Radlinski et al., 2008), and the ranking algorithm that provided the first result chosen by the user is declared the winner of the duel.", "startOffset": 66, "endOffset": 90}, {"referenceID": 12, "context": "Weak regret was proposed by Yue et al. (2012) and arises in content recommendation when arms correspond to items, and the user incurs no regret whenever his most preferred item is made available.", "startOffset": 28, "endOffset": 46}, {"referenceID": 12, "context": "Weak regret was proposed by Yue et al. (2012) and arises in content recommendation when arms correspond to items, and the user incurs no regret whenever his most preferred item is made available. Examples include in-app restaurant recommendations provided by food delivery services like Grubhub and UberEATS, in which implicit feedback may be inferred from selections, and the user only incurs regret if her most preferred restaurant is not recommended. Examples also include recommendation of online broadcasters on platforms such as Twitch, in which implicit feedback may again be inferred from selections, and the user is fully satisfied as long as her favored broadcaster is listed. Despite its applicability, Yue et al. (2012) is the only paper of which we are aware that studies weak regret, and it does not provide algorithms specifically designed for this setting.", "startOffset": 28, "endOffset": 732}, {"referenceID": 16, "context": "Algorithms have been proposed that reach this lower bound under the Condorcet winner assumption in the finite-horizon setting: Interleaved Filter (IF) (Yue et al., 2012) and Beat the Mean (BTM) (Yue & Joachims, 2011).", "startOffset": 151, "endOffset": 169}, {"referenceID": 17, "context": "Relative Upper Confidence Bound (RUCB) (Zoghi et al., 2014) also reaches this lower bound in the horizonless setting.", "startOffset": 39, "endOffset": 59}, {"referenceID": 8, "context": "Relative Minimum Empirical Divergence (RMED) (Komiyama et al., 2015) is the first algorithm to have a regret bound that matches this lower bound.", "startOffset": 45, "endOffset": 68}, {"referenceID": 10, "context": "(Pallone et al., 2017) studies adaptive preference learning across arms using pairwise preferences.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "Yue et al. (2012) shows that the worst-case expected cumulative strong regret up to time T for any algorithm is \u03a9(N log(T )).", "startOffset": 0, "endOffset": 18}, {"referenceID": 7, "context": "Relative Minimum Empirical Divergence (RMED) (Komiyama et al., 2015) is the first algorithm to have a regret bound that matches this lower bound. Zoghi et al. (2015) proposed two algorithms, Copeland Confidence Bound (CCB) and Scalable Copeland Bandits (SCB), which achieve an optimal regret bound without assuming existence of a Condorcet winner.", "startOffset": 46, "endOffset": 166}, {"referenceID": 7, "context": "Relative Minimum Empirical Divergence (RMED) (Komiyama et al., 2015) is the first algorithm to have a regret bound that matches this lower bound. Zoghi et al. (2015) proposed two algorithms, Copeland Confidence Bound (CCB) and Scalable Copeland Bandits (SCB), which achieve an optimal regret bound without assuming existence of a Condorcet winner. While weak regret was proposed in Yue et al. (2012), it has not been widely studied to our knowledge, and despite its applicability we are unaware of papers that provide algorithms designed for it specifically.", "startOffset": 46, "endOffset": 400}, {"referenceID": 7, "context": "Relative Minimum Empirical Divergence (RMED) (Komiyama et al., 2015) is the first algorithm to have a regret bound that matches this lower bound. Zoghi et al. (2015) proposed two algorithms, Copeland Confidence Bound (CCB) and Scalable Copeland Bandits (SCB), which achieve an optimal regret bound without assuming existence of a Condorcet winner. While weak regret was proposed in Yue et al. (2012), it has not been widely studied to our knowledge, and despite its applicability we are unaware of papers that provide algorithms designed for it specifically. While one can apply algorithms designed for the strong regret setting to weak regret, and use the fact that strong dominates weak regret to obtain weak regret bounds ofO(N log(T )), these are looser than the constant-in-T bounds that we show. Active learning using pairwise comparisons is also closely related to our work. Jamieson & Nowak (2011) considers an active learning problem that is similar to our problem in that the primary goal is to sort arms based on the user\u2019s preferences, using adaptive pairwise comparisons.", "startOffset": 46, "endOffset": 906}, {"referenceID": 2, "context": "Busa-Fekete et al. (2013) and Busa-Fekete et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 2, "context": "Busa-Fekete et al. (2013) and Busa-Fekete et al. (2014) consider topk element selection using adaptive pairwise comparisons.", "startOffset": 0, "endOffset": 56}, {"referenceID": 0, "context": "Extension to Utility-Based Regret We now briefly discuss utility-based extensions of weak and strong regret for the total order setting, following utilitybased bandits studied in Ailon et al. (2014). Our regret bounds also apply here, with a small modification.", "startOffset": 179, "endOffset": 199}, {"referenceID": 8, "context": "In the strong regret setting, we compare WS-S with 7 benchmarks including RUCB and Relative Minimum Empirical Divergence (RMED) by Komiyama et al. (2015). We also include an experiment violating the total order assumption in Section 11 in the supplement.", "startOffset": 131, "endOffset": 154}, {"referenceID": 8, "context": "We use the sushi and MSLR datasets, which were previously used by Komiyama et al. (2016) and Zoghi et al.", "startOffset": 66, "endOffset": 89}, {"referenceID": 8, "context": "We use the sushi and MSLR datasets, which were previously used by Komiyama et al. (2016) and Zoghi et al. (2015) respectively to evaluate dueling bandit algorithms.", "startOffset": 66, "endOffset": 113}, {"referenceID": 9, "context": "The sushi dataset (Komiyama et al., 2016) contains 16 arms corresponding to types of sushi, with pairwise preferences inferred from data on sushi preferences from 5000 users in Kamishima (2003).", "startOffset": 18, "endOffset": 41}, {"referenceID": 8, "context": "The sushi dataset (Komiyama et al., 2016) contains 16 arms corresponding to types of sushi, with pairwise preferences inferred from data on sushi preferences from 5000 users in Kamishima (2003). The MSLR dataset has 5 arms, corresponding to ranking algorithms, with pairwise preferences provided in Zoghi et al.", "startOffset": 19, "endOffset": 194}, {"referenceID": 8, "context": "The sushi dataset (Komiyama et al., 2016) contains 16 arms corresponding to types of sushi, with pairwise preferences inferred from data on sushi preferences from 5000 users in Kamishima (2003). The MSLR dataset has 5 arms, corresponding to ranking algorithms, with pairwise preferences provided in Zoghi et al. (2015). We give preference matrices (pi,j) for both datasets in the supplement.", "startOffset": 19, "endOffset": 319}, {"referenceID": 13, "context": "W follows a Poisson distribution with parameter \u2211\u221e i=1 ( 1\u2212p p )i = 1\u2212p 2p\u22121 (Theorem 4, Wang (1993)).", "startOffset": 89, "endOffset": 101}, {"referenceID": 9, "context": "This is common in the dueling bandits literature, where even work that considers more general settings theoretically test their methods on problems that satisfy the total order assumption (Komiyama et al., 2016; Urvoy et al., 2013).", "startOffset": 188, "endOffset": 231}, {"referenceID": 8, "context": "This is common in the dueling bandits literature, where even work that considers more general settings theoretically test their methods on problems that satisfy the total order assumption (Komiyama et al., 2016; Urvoy et al., 2013). In this section, we consider an additional example that has a Condorcet winner but does not have a total order among arms. The example has a cyclic struture, and is similar to the cyclic example in Komiyama et al. (2015). The preference matrix is: \uf8ef\uf8ef\uf8f0 0.", "startOffset": 189, "endOffset": 454}], "year": 2017, "abstractText": "We consider online content recommendation with implicit feedback through pairwise comparisons, formalized as the so-called dueling bandit problem. We study the dueling bandit problem in the Condorcet winner setting, and consider two notions of regret: the more well-studied strong regret, which is 0 only when both arms pulled are the Condorcet winner; and the less well-studied weak regret, which is 0 if either arm pulled is the Condorcet winner. We propose a new algorithm for this problem, Winner Stays (WS), with variations for each kind of regret: WS for weak regret (WS-W) has expected cumulative weak regret that is O(N), and O(N log(N)) if arms have a total order; WS for strong regret (WS-S) has expected cumulative strong regret of O(N + N log(T )), and O(N log(N) +N log(T )) if arms have a total order. WS-W is the first dueling bandit algorithm with weak regret that is constant in time. WS is simple to compute, even for problems with many arms, and we demonstrate through numerical experiments on simulated and real data that WS has significantly smaller regret than existing algorithms in both the weakand strong-regret settings.", "creator": "LaTeX with hyperref package"}}}