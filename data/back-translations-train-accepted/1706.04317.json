{"id": "1706.04317", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics", "abstract": "The recent adaptation of deep neural network-based methods to reinforcement learning and planning domains has yielded remarkable progress on individual tasks. Nonetheless, progress on task-to-task transfer remains limited. In pursuit of efficient and robust generalization, we introduce the Schema Network, an object-oriented generative physics simulator capable of disentangling multiple causes of events and reasoning backward through causes to achieve goals. The richly structured architecture of the Schema Network can learn the dynamics of an environment directly from data. We compare Schema Networks with Asynchronous Advantage Actor-Critic and Progressive Networks on a suite of Breakout variations, reporting results on training efficiency and zero-shot generalization, consistently demonstrating faster, more robust learning and better transfer. We argue that generalizing from limited data and learning causal relationships are essential abilities on the path toward generally intelligent systems.", "histories": [["v1", "Wed, 14 Jun 2017 05:11:08 GMT  (1013kb,D)", "https://arxiv.org/abs/1706.04317v1", null], ["v2", "Thu, 17 Aug 2017 23:37:54 GMT  (1013kb,D)", "http://arxiv.org/abs/1706.04317v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ken kansky", "tom silver", "david a m\u00e9ly", "mohamed eldawy", "miguel l\u00e1zaro-gredilla", "xinghua lou", "nimrod dorfman", "szymon sidor", "d scott phoenix", "dileep george"], "accepted": true, "id": "1706.04317"}, "pdf": {"name": "1706.04317.pdf", "metadata": {"source": "META", "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics", "authors": ["Ken Kansky", "Tom Silver", "David A. M\u00e9ly", "Mohamed Eldawy", "Miguel L\u00e1zaro-Gredilla", "Xinghua Lou", "Nimrod Dorfman", "Szymon Sidor", "Scott Phoenix", "Dileep George"], "emails": ["<ken@vicarious.com>,", "<tom@vicarious.com>."], "sections": [{"heading": "1. Introduction", "text": "In fact, it is the case that you are able to be in a position without being able to play by the rules, and that you are able to play by the rules."}, {"heading": "2. Related Work", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "3. Schema Networks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. MDPs and Notation", "text": "The traditional formalism for the Reinforcement Learning problem is the Markov Decision Process (MDP): An MDP M is a quintuple (S, A, T, R, \u03b3), where S is a series of states, A is a series of actions, T (s (t + 1) | s (t), a (t) is the probability of a transition from a state s (t) to a state s (t + 1) \u0445S after an act a (t), A (r (t + 1) | s (t), a (t)) is the probability of receiving a reward r (t + 1) or E (R) after performing an act a (t), while in a state s (t), and vice versa [0, 1] is the rate at which future rewards are discounted exponentially."}, {"heading": "3.2. Model Definition", "text": "A schema network is a structured generative model of an MDP. First, we describe the architecture of the model informally. An image input is divided into a list of entities that can be regarded as instances of objects in the sense of OO-MDPs (Diuk et al., 2008). All entities share the same set of attributes. We refer to a specific attribute of a specific entity as an entity attribute, which is presented as a binary variable to indicate the presence of this attribute for an entity. An entity state is an assignment of states to all attributes of the entity, and the complete model state is the set of all entity states. A grounded schema is a binary variable associated with a particular entity attribute whose value depends on the present values of a set of binary entity attributes."}, {"heading": "3.3. Construction of Entities and Attributes", "text": "In practice, we assume that a vision system is responsible for recognizing and tracking objects in an image. It is therefore largely up to the vision system to determine what constitutes a unit. Essentially, any traceable image attribute could be a unit that typically includes objects, their boundaries, and their surfaces. Recent work has shown that a possible method for unattended construction of objects using autoencoders (Garnelo et al., 2016) could be most relevant for predicting whether it can be controlled by an action. Depending on the task, schema networks could learn to think flexibly at different levels of representation. For example, the use of surface objects may be most relevant for predicting collisions, while the use of one unit per object could be most relevant for predicting whether it can be controlled by an action. The experiments in this paper use surface units that are described further in Section 5.1, these attributes can be provided by the surface-oriented system, or by the attributes:"}, {"heading": "3.4. Connections to Existing Models", "text": "Schema networks are closely related to object-oriented MDPs (OO-MDPs) (Diuk et al., 2008) and Relational MDPs (R-MDPs) (Guestrin et al., 2003a). However, neither OO-MDPs nor R-MDPs define a transition function with an explicit OR of possible causes, and traditionally, transition functions have not been learned in these models. In contrast, schema networks provide an explicit OR to think about multiple causalities, which allows regression planning. Furthermore, the structure of schema networks can be learned efficiently. Schema networks are also related to the recently proposed Interaction Network (IN) (Battaglia et al., 2016) and Neural Physics Engine (NPE) (Chang et al., 2016)."}, {"heading": "4. Learning and Planning in Schema Networks", "text": "In this section, we describe how schema networks are trained (i.e., learned their structure) by interacting with an environment and how they can be used for planning. Planning is not only necessary during the test period to maximize reward, but can also be used to improve exploration during the training process."}, {"heading": "4.1. Training Procedure", "text": "Faced with a series of actions, rewards, and images, we represent any possible action and reward with a binary variable, converting each image into a series of entity states. The number of entities may vary between adjacent frames, taking into account objects that appear or shift out of view. Faced with a set of entity states, we prepare the entity states for a representation more convenient for learning. For N entities observed over T periods, we would like \u03b1 (t \u2212 1) i, j to predict t \u2212 1 (for 1 \u2264 i and 2 \u2264 t \u2264 T) based on the attribute values of the ith entity and its spatial neighbors at a time. Then, the attribute values of E (t \u2212 1) i and its neighbors can be represented as a line vector of length MR, with M predicting the number of attributes of the attribute-associated entity, and R \u2212 1 the number of the fixed positions of the respective entities determined by one."}, {"heading": "4.2. Schema Learning", "text": "In this context, it should be noted that the problem is a complex problem that occurs in a discrete space of parameterizations (the schemas), and that the learning problem is to apply a greedy algorithm that solves a sequence of LP relaxations. See Jaakkola et al. (2010) for further work on the application of LP relaxations to structural learning. X and y, defined above, find a mappingy = fW (X) in which all variables involved are binary and follow operations of Boolean logic. In addition, it corresponds to oring and overlays the negation. W: 0, 1} D: L is a binary matrix in which each column represents a schema."}, {"heading": "4.3. Planning as Probabilistic Inference", "text": "rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rf"}, {"heading": "5. Experiments", "text": "We compared the performance of Schema Networks, A3C, and PN's (Progressive Networks) to several variations of the Breakout game. The variations chosen all share similar dynamics, but the layouts change, requiring different strategies to achieve high scores. A variety of concepts must be learned to accurately predict object movements and rewards. For example, if we predict why rewards occur, the model must untangle possible causes to determine that reward depends on the color of a building block, but regardless of the speed and position of the ball where it was hit. While these causal relationships are easy to restore for humans, we do not yet have an existing approach for learning a generative model that can restore all these dynamics without supervision and effectively transfer them. Scheme Networks rely on input entity states instead of raw images, and we have provided the same information for A3C and PN's by including the three channels of each of the 34 additional walls containing one of the four, each of which were part of the three walls."}, {"heading": "5.1. Transfer Learning", "text": "This experiment examines how effectively schema networks and PNs are able to learn a new breakout variant after pre-training, which examines how well the two models can transfer existing knowledge to a new task. Figure 3a shows the learning rates during the 100k mini-breakout training. In a second experiment, we prepared for Large Breakout for 100k frames and the advanced training on the middle-wall variant, which is shown in Figure 1b. Figure 3b shows that PNs take considerable time to learn in this new environment, while schema networks do not learn anything new because the dynamics are the same."}, {"heading": "5.2. Zero-Shot Generalization", "text": "Many breakout variations can be constructed, all with the same dynamics. If one model learns the dynamics of one variation correctly, the others could be played perfectly by planning with the learned model. Instead of comparing transfer with additional training using PNs, we can compare the zero-shot generalization in these variations by training A3C only on standard breakout. Fig. 1b-e shows some of these variations with the following modifications from the training environment: \u2022 Offset Paddle (Fig. 1d): The paddle is moved up a few pixels. \u2022 Middle Wall (Fig. 1b): A wall is placed in the center of the screen, forcing the agent to hit the stones. \u2022 Random Target (Fig. 1e): A group of bricks is destroyed when the ball hits one of them, and then reappears in a new random position. \u2022 Juggle (Fig.)"}, {"heading": "5.3. Testing for Learned Causes", "text": "To better assess whether these models really know the causes of rewards, we developed another zero-shot generalization experiment. We trained both Scheme Networks and A3C on a mini breakout variant, in which the color of a block determines whether a positive or negative reward is received when it is destroyed. Six colors of blocks offer + 1 reward, and two colors offer -1 reward. Negative blocks occurred 33% of the time during training in a random position. Then, during the test, the blocks were split into two halves, with all positively colored blocks on one half and negatively colored blocks on the other side (see Fig. 1c). If the causes of the rewards were learned correctly, the agent should prefer, if possible, to aim at the positive half. As Table 1 shows, Scheme Networks learned correctly from random arrangements caused by stone colors that show rewards reaching the average, while others aim for positive half, while the positive half prefers none."}, {"heading": "6. Discussion and Conclusion", "text": "The fact that Schema Networks are able to reap rewards more efficiently than state-of-the-art model-free methods such as A3C is all the more remarkable because high values are a by-product of learning a precise game model. Schema Networks \"success stems in part from the holistic representation of the state. Our results suggest that providing deep RL models such as A3C with such representation as input can improve both training efficiency and generalization, a finding supported by recent experiments (Usunier et al., 2016; Garnelo et al., 2016; Chang et al., 2016; Battaglia et al., 2016) that have causal effects on causal network models.The environments considered in this dynamic are diverse, but also simplified in relation to the real world."}, {"heading": "Acknowledgements", "text": "Special thanks to Eric Purdy and Ramki Gummadi for useful insights and discussions during the preparation of this work."}, {"heading": "A. Breakout playing visualizations", "text": "See https: / / vimeopro.com / user45297729 / schema-networks for visualizations of schema networks that offer different variants of post-workout breakouts based on breakout.Figure 4 shows typical gameplay for one variant."}, {"heading": "B. LP-based Greedy Schema Learning", "text": "The details of LP-based learning in Section 4.2 are given here.Algorithm 1 LP-based greedy schema learning Input: Input vectors {xn}, for which fW (xn) = 0 (the current schema network predicts 0), and the corresponding output scalars yn 1: Find a cluster of input vectors that can be solved with a single (relaxed) schema while maintaining perfect precision (no false alarms).Select an input sample and put it into the set \"solved,\" then solve the LPmin w [0,1] D: yn = 1 (1 \u2212 xn) ws.t. (1 \u2212 xn) w > 1: yn = 0 (1 \u2212 xn) w = 0 (1 \u2212 xn) n: n (1 \u2212 xn) w = 0. Simplify the scheme you just found by making it as economical as possible."}], "references": [{"title": "Cognitive psychology and its implications", "author": ["Anderson", "John R"], "venue": "WH Freeman/Times Books/Henry Holt & Co,", "citeRegEx": "Anderson and R.,? \\Q1990\\E", "shortCiteRegEx": "Anderson and R.", "year": 1990}, {"title": "Planning by probabilistic inference", "author": ["Attias", "Hagai"], "venue": "In AISTATS,", "citeRegEx": "Attias and Hagai.,? \\Q2003\\E", "shortCiteRegEx": "Attias and Hagai.", "year": 2003}, {"title": "Interaction networks for learning about objects, relations and physics", "author": ["Battaglia", "Peter", "Pascanu", "Razvan", "Lai", "Matthew", "Rezende", "Danilo Jimenez"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Battaglia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Battaglia et al\\.", "year": 2016}, {"title": "A compositional object-based approach to learning physical dynamics", "author": ["Chang", "Michael B", "Ullman", "Tomer", "Torralba", "Antonio", "Tenenbaum", "Joshua B"], "venue": "arXiv preprint arXiv:1612.00341,", "citeRegEx": "Chang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2016}, {"title": "Variational planning for graph-based mdps", "author": ["Cheng", "Qiang", "Liu", "Chen", "Feng", "Ihler", "Alexander T"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Cheng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2013}, {"title": "An object-oriented representation for efficient reinforcement learning", "author": ["Diuk", "Carlos", "Cohen", "Andre", "Littman", "Michael L"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Diuk et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Diuk et al\\.", "year": 2008}, {"title": "Made-up minds: a constructivist approach to artificial intelligence", "author": ["Drescher", "Gary L"], "venue": "MIT press,", "citeRegEx": "Drescher and L.,? \\Q1991\\E", "shortCiteRegEx": "Drescher and L.", "year": 1991}, {"title": "Towards deep symbolic reinforcement learning", "author": ["Garnelo", "Marta", "Arulkumaran", "Kai", "Shanahan", "Murray"], "venue": "arXiv preprint arXiv:1609.05518,", "citeRegEx": "Garnelo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garnelo et al\\.", "year": 2016}, {"title": "Generalizing plans to new environments in relational mdps", "author": ["Guestrin", "Carlos", "Koller", "Daphne", "Gearhart", "Chris", "Kanodia", "Neal"], "venue": "In Proceedings of the 18th international joint conference on Artificial intelligence,", "citeRegEx": "Guestrin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2003}, {"title": "Efficient solution algorithms for factored mdps", "author": ["Guestrin", "Carlos", "Koller", "Daphne", "Parr", "Ronald", "Venkataraman", "Shobha"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Guestrin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2003}, {"title": "Learning bayesian network structure using lp relaxations", "author": ["Jaakkola", "Tommi S", "Sontag", "David", "Globerson", "Amir", "Meila", "Marina"], "venue": "In AISTATS, pp", "citeRegEx": "Jaakkola et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaakkola et al\\.", "year": 2010}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["Jaderberg", "Max", "Mnih", "Volodymyr", "Czarnecki", "Wojciech Marian", "Schaul", "Tom", "Leibo", "Joel Z", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1611.05397,", "citeRegEx": "Jaderberg et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2016}, {"title": "Learning in graphical models, volume 89", "author": ["Jordan", "Michael Irwin"], "venue": "Springer Science & Business Media,", "citeRegEx": "Jordan and Irwin.,? \\Q1998\\E", "shortCiteRegEx": "Jordan and Irwin.", "year": 1998}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["Koller", "Daphne", "Friedman", "Nir"], "venue": "MIT press,", "citeRegEx": "Koller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2009}, {"title": "A physics-based model prior for objectoriented mdps", "author": ["Scholz", "Jonathan", "Levihn", "Martin", "Isbell", "Charles", "Wingate", "David"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Scholz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Scholz et al\\.", "year": 2014}, {"title": "The predictron: End-to-end learning and planning", "author": ["Silver", "David", "van Hasselt", "Hado", "Hessel", "Matteo", "Schaul", "Tom", "Guez", "Arthur", "Harley", "Tim", "Dulac-Arnold", "Gabriel", "Reichert", "Rabinowitz", "Neil", "Barreto", "Andre"], "venue": "arXiv preprint arXiv:1612.08810,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Value iteration networks", "author": ["Tamar", "Aviv", "Levine", "Sergey", "Abbeel", "WU Pieter", "YI", "Thomas", "Garrett"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tamar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tamar et al\\.", "year": 2016}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["Taylor", "Matthew E", "Stone", "Peter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Taylor et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2009}, {"title": "Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks", "author": ["Usunier", "Nicolas", "Synnaeve", "Gabriel", "Lin", "Zeming", "Chintala", "Soumith"], "venue": "arXiv preprint arXiv:1609.02993,", "citeRegEx": "Usunier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Usunier et al\\.", "year": 2016}, {"title": "Deep reinforcement learning with double q-learning", "author": ["Van Hasselt", "Hado", "Guez", "Arthur", "Silver", "David"], "venue": "In AAAI,", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Embed to control: A locally linear latent dynamics model for control from raw images", "author": ["Watter", "Manuel", "Springenberg", "Jost", "Boedecker", "Joschka", "Riedmiller", "Martin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Watter et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Watter et al\\.", "year": 2015}, {"title": "Psychology: Themes and Variations", "author": ["W. Weiten"], "venue": "PSY 113 General Psychology Series. Cengage Learning,", "citeRegEx": "Weiten,? \\Q2012\\E", "shortCiteRegEx": "Weiten", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "Recent work has suggested how to overcome this deficiency by utilizing object-based representations (Diuk et al., 2008; Usunier et al., 2016).", "startOffset": 100, "endOffset": 141}, {"referenceID": 18, "context": "Recent work has suggested how to overcome this deficiency by utilizing object-based representations (Diuk et al., 2008; Usunier et al., 2016).", "startOffset": 100, "endOffset": 141}, {"referenceID": 21, "context": "well-acknowledged Gestalt principle, which states that the ability to perceive objects as a bounded figure in front of an unbounded background is fundamental to all perception (Weiten, 2012).", "startOffset": 176, "endOffset": 190}, {"referenceID": 2, "context": "Battaglia et al. (2016) and Chang et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "Battaglia et al. (2016) and Chang et al. (2016) go further, learning relations to predict object interactions.", "startOffset": 0, "endOffset": 48}, {"referenceID": 11, "context": "Model-free Deep RL models like A3C are unable to substantially generalize beyond their training experience (Jaderberg et al., 2016; Rusu et al., 2016).", "startOffset": 107, "endOffset": 150}, {"referenceID": 2, "context": "The Interaction Network (Battaglia et al., 2016) (IN) and the Neural Physics Engine (NPE) (Chang et al.", "startOffset": 24, "endOffset": 48}, {"referenceID": 3, "context": ", 2016) (IN) and the Neural Physics Engine (NPE) (Chang et al., 2016) use object-level and pairwise relational representations to learn models of intuitive physics.", "startOffset": 49, "endOffset": 69}, {"referenceID": 20, "context": "Progress in model-based Deep RL has thus far been limited, though methods like Embed to Control (Watter et al., 2015), Value Iteration Networks (Tamar et al.", "startOffset": 96, "endOffset": 117}, {"referenceID": 16, "context": ", 2015), Value Iteration Networks (Tamar et al., 2016), and the Predictron (Silver et al.", "startOffset": 34, "endOffset": 54}, {"referenceID": 5, "context": "Schema Networks build upon the ideas of the ObjectOriented Markov Decision Process (OO-MDP) introduced by Diuk et al. (2008) (see also Scholz et al.", "startOffset": 106, "endOffset": 125}, {"referenceID": 5, "context": "Schema Networks build upon the ideas of the ObjectOriented Markov Decision Process (OO-MDP) introduced by Diuk et al. (2008) (see also Scholz et al. (2014)).", "startOffset": 106, "endOffset": 156}, {"referenceID": 5, "context": "Schema Networks build upon the ideas of the ObjectOriented Markov Decision Process (OO-MDP) introduced by Diuk et al. (2008) (see also Scholz et al. (2014)). Related frameworks include relational and first-order logical MDPs (Guestrin et al., 2003a). These formalisms, which harken back to classical AI\u2019s roots in symbolic reasoning, are designed to enable robust generalization. Recent work by Garnelo et al. (2016) on \u201cdeep symbolic reinforcement learning\u201d makes this connection explicit, marrying firstorder logic with deep RL.", "startOffset": 106, "endOffset": 417}, {"referenceID": 5, "context": "An image input is parsed into a list of entities, which may be thought of as instances of objects in the sense of OO-MDPs (Diuk et al., 2008).", "startOffset": 122, "endOffset": 141}, {"referenceID": 7, "context": "Recent work has demonstrated one possible method for unsupervised entity construction using autoencoders (Garnelo et al., 2016).", "startOffset": 105, "endOffset": 127}, {"referenceID": 5, "context": "Schema Networks are closely related to Object-Oriented MDPs (OO-MDPs) (Diuk et al., 2008) and Relational MDPs (R-MDPs) (Guestrin et al.", "startOffset": 70, "endOffset": 89}, {"referenceID": 2, "context": "Schema Networks are also related to the recently proposed Interaction Network (IN) (Battaglia et al., 2016) and Neural Physics Engine (NPE) (Chang et al.", "startOffset": 83, "endOffset": 107}, {"referenceID": 3, "context": ", 2016) and Neural Physics Engine (NPE) (Chang et al., 2016).", "startOffset": 40, "endOffset": 60}, {"referenceID": 10, "context": "See Jaakkola et al. (2010) for further work on applying LP relaxations to structure learning.", "startOffset": 4, "endOffset": 27}, {"referenceID": 4, "context": "Cheng et al. (2013) use variational inference for planning but resort to MPBP to optimize the variational free energy functional.", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "This finding corroborates recent attempts (Usunier et al., 2016; Garnelo et al., 2016; Chang et al., 2016; Battaglia et al., 2016) to incorporate object and relational structure into neural network-based models.", "startOffset": 42, "endOffset": 130}, {"referenceID": 7, "context": "This finding corroborates recent attempts (Usunier et al., 2016; Garnelo et al., 2016; Chang et al., 2016; Battaglia et al., 2016) to incorporate object and relational structure into neural network-based models.", "startOffset": 42, "endOffset": 130}, {"referenceID": 3, "context": "This finding corroborates recent attempts (Usunier et al., 2016; Garnelo et al., 2016; Chang et al., 2016; Battaglia et al., 2016) to incorporate object and relational structure into neural network-based models.", "startOffset": 42, "endOffset": 130}, {"referenceID": 2, "context": "This finding corroborates recent attempts (Usunier et al., 2016; Garnelo et al., 2016; Chang et al., 2016; Battaglia et al., 2016) to incorporate object and relational structure into neural network-based models.", "startOffset": 42, "endOffset": 130}], "year": 2017, "abstractText": "The recent adaptation of deep neural networkbased methods to reinforcement learning and planning domains has yielded remarkable progress on individual tasks. Nonetheless, progress on task-to-task transfer remains limited. In pursuit of efficient and robust generalization, we introduce the Schema Network, an objectoriented generative physics simulator capable of disentangling multiple causes of events and reasoning backward through causes to achieve goals. The richly structured architecture of the Schema Network can learn the dynamics of an environment directly from data. We compare Schema Networks with Asynchronous Advantage Actor-Critic and Progressive Networks on a suite of Breakout variations, reporting results on training efficiency and zero-shot generalization, consistently demonstrating faster, more robust learning and better transfer. We argue that generalizing from limited data and learning causal relationships are essential abilities on the path toward generally intelligent systems.", "creator": "LaTeX with hyperref package"}}}