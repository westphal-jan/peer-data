{"id": "1706.04599", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "On Calibration of Modern Neural Networks", "abstract": "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.", "histories": [["v1", "Wed, 14 Jun 2017 17:33:50 GMT  (1825kb,D)", "http://arxiv.org/abs/1706.04599v1", "Accepted to ICML 2017"], ["v2", "Thu, 3 Aug 2017 13:29:46 GMT  (1271kb,D)", "http://arxiv.org/abs/1706.04599v2", "ICML 2017"]], "COMMENTS": "Accepted to ICML 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chuan guo", "geoff pleiss", "yu sun", "kilian q weinberger"], "accepted": true, "id": "1706.04599"}, "pdf": {"name": "1706.04599.pdf", "metadata": {"source": "META", "title": "On Calibration of Modern Neural Networks", "authors": ["Chuan Guo", "Geoff Pleiss", "Yu Sun", "Kilian Q. Weinberger"], "emails": ["<cg563@cornell.edu>,", "<geoff@cs.cornell.edu>,", "<ys646@cornell.edu>."], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2. Definitions", "text": "This year, it will be able to retaliate and retaliate."}, {"heading": "3. Observing Miscalibration", "text": "This year, as never before in the history of a country in which it is a country in which it is a country in which it is a country in which it is a country, a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country"}, {"heading": "4. Calibration Methods", "text": "In this section, we first review existing calibration methods and introduce new variants of our own. All methods are post-processing steps that generate (calibrated) probabilities. Each method requires a hold-out validation set, which in practice can be the same set used for setting hyperparameters. We assume that training, validation and test sets come from the same distribution."}, {"heading": "4.1. Calibrating Binary Models", "text": "For simplicity, in this subsection, we only assume the constants for the positive class1. Faced with a sample from the validation set (xi, yi), we have access to p-i - the predicted probability of yi = 1, as well as zi, R - the non-probability of the network, or logit. The predicted probability p-i is derived from zi with a sigmoid function; i.e. p-i = s-i (zi). Our goal is to produce a calibrated probability based on yi, p-i and zi.Histogram binning (Zadrozny & Elkan, 2001) is a simple non-parametric calibration method. In a nutshell, all uncalibrated predictions p-i are divided into mutually exclusive bBB1."}, {"heading": "4.2. Extension to Multiclass Models", "text": "In this case, the network logits zi (k) i) where y (j) i (j) i, p (i) i, p (i) i, p (i) i, p (i) i, p (i) i, p (i) i, p (i) i, p (i) i, p (i) i (k).The goal is to generate a calibrated confidence q (i) and (possibly new) class prediction q (i)."}, {"heading": "4.3. Other Related Works", "text": "Kuleshov & Ermon (2015) investigate how to generate calibrated probabilities when the output space is a structured object. Lakshminarayanan et al. (2016) use network groups to obtain uncertainty estimates. Pereyra et al. (2017) penalize exaggerated predictions as a form of regulation. Hendrycks & Gimpel (2017) use confidence3To emphasize the link with previous work, we define temperature scaling in terms of 1T rather than a multiplicative scale. Scores to determine whether the samples are outside of distribution. Bayesian neural networks (Denker & Lecun, 1990; MacKay, 1992) provide a probability distribution of the results as an alternative way to represent uncertainty. Gal & Ghai poses a problem where reliability between the 2016 and 2017 models is not available."}, {"heading": "5. Results", "text": "The fact is that most of them are able to outdo themselves in the way they do it: in the way they do it, in the way they do it, in the way they do it, in the way they do it, in the way they do it, in the way they do it, in the way they do it, in the way they do it, in the way they do it."}, {"heading": "6. Conclusion", "text": "Modern neural networks exhibit a curious phenomenon: probability errors and miscalibrations get worse even when classification errors are reduced. We have shown that recent advances in the architecture and formation of neural networks - model capacity, normalization, and regulation - have a strong impact on network calibration. It remains to be understood why these trends affect calibration while improving accuracy. Nevertheless, simple techniques can effectively address the phenomenon of miscalibration in neural networks. Temperature scaling is the simplest, fastest, and simplest method, and surprisingly often the most effective."}, {"heading": "7. Acknowledgments", "text": "The authors are supported in part by the grants III-1618134, III1526012 and IIS-1149882 of the National Science Foundation, as well as the Bill and Melinda Gates Foundation and the Office of Naval Research."}, {"heading": "S1. Further Information on Calibration Metrics", "text": "We can connect the ECE-Metrik with our exact false calibration definition, which is redefined here: E-P [VP] \u2212 FP (a) = P (P) = P (a, b). With the RiemannStieltjes-Integral we have the kumulative distribution function of P (p), so that FP (b) \u2212 FP (a) = P (P) = P (a, b). With the RiemannStieltjes-Integral we have E-P (Y) = P (Y) \u2212 p (P) = P (P) \u2212 p (P) = 1 0).P (Y) = P (Y).dFP (P).p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p..p.p.p.p..p..p.p.p.p..p.p.p.p..p.p.p..p.p.p.p.p.p..p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p..p.p.p.p.p.p.p.p.. p.p.p.p.p.p.p.p.p.. p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p"}, {"heading": "S2. Further Information on Temperature Scaling", "text": "Here we derive the temperature scaling model using the entropy maximization principle with a suitable q \u03b2ation.Claim 1. Given n samples' logit vectors z1,.., zn and class names y1,.., yn, temperature scaling is the unique solution q to the following entropy maximization problem: max q \u2212 n \u2211 i = 1 K \u2211 k = 1 q (zi) (k), subject to q (zi) (k) 0 \u0432 i, k \u00b2 k = 1 q (zi) (k) (k) (k) (k) = 1 \u0445 i = 1 z (yi) i = n graphic i = 1 K (k). \u2212 \u2212 \u2212 \u2212 \u2212 k) q (zi (zi) (k).The first two constraints ensure that q is a probability distribution, while the last constraint limits the extent of distributions. Intuitively, the constraint determines that the average true class logit is equal to the average weighted gi."}, {"heading": "S3. Additional Tables", "text": "S1, S2, and S3 show the MCE, test error, and NLL for all experimental settings outlined in Section 5.Supplementary Materials: On Calibration of Modern Neural Networks0 100 200 400 500 0.511.522.533.5EpochE ntr o p y / N LL / TEntropy vs. NLL on CIFAR \u2212 100Entropy & NLL after CalibrationEntropy before CalibrationNLL before CalibrationOptimal T SelectedFigure S1."}, {"heading": "S4. Additional Reliability Diagrams", "text": "We include the following points: \"It is important that people are able to solve their problems.\" \"It is important that people are able to get a grip on their problems.\" \"It is important to solve the problems.\" \"It is important that the problems are solved.\" \"It is important that the problems are solved.\" \"It is important that the problems are solved.\" \"It is important that the problems are solved.\" \"It is important that the problems are solved.\" \"It is important that the problems are solved.\" \"It is important that the problems are solved.\" \"It is important that the problems are solved.\" \"It is important that the problems are solved,\" that the problems are solved, \"but not solved,\" but that the problems are solved. \""}], "references": [{"title": "End to end learning for self-driving cars", "author": ["Bojarski", "Mariusz", "Del Testa", "Davide", "Dworakowski", "Daniel", "Firner", "Bernhard", "Flepp", "Beat", "Goyal", "Prasoon", "Jackel", "Lawrence D", "Monfort", "Mathew", "Muller", "Urs", "Zhang", "Jiakai"], "venue": "arXiv preprint arXiv:1604.07316,", "citeRegEx": "Bojarski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bojarski et al\\.", "year": 2016}, {"title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission", "author": ["Caruana", "Rich", "Lou", "Yin", "Gehrke", "Johannes", "Koch", "Paul", "Sturm", "Marc", "Elhadad", "Noemie"], "venue": "In KDD,", "citeRegEx": "Caruana et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Caruana et al\\.", "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Collobert", "Ronan", "Kavukcuoglu", "Koray", "Farabet", "Cl\u00e9ment"], "venue": "In BigLearn Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Are humans good intuitive statisticians after all? rethinking some conclusions from the literature on judgment under uncertainty", "author": ["Cosmides", "Leda", "Tooby", "John"], "venue": null, "citeRegEx": "Cosmides et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Cosmides et al\\.", "year": 1996}, {"title": "The comparison and evaluation of forecasters", "author": ["DeGroot", "Morris H", "Fienberg", "Stephen E"], "venue": "The statistician,", "citeRegEx": "DeGroot et al\\.,? \\Q1983\\E", "shortCiteRegEx": "DeGroot et al\\.", "year": 1983}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In CVPR, pp", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Transforming neural-net output levels to probability distributions", "author": ["Denker", "John S", "Lecun", "Yann"], "venue": "In NIPS, pp", "citeRegEx": "Denker et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Denker et al\\.", "year": 1990}, {"title": "The elements of statistical learning, volume 1. Springer series in statistics", "author": ["Friedman", "Jerome", "Hastie", "Trevor", "Tibshirani", "Robert"], "venue": null, "citeRegEx": "Friedman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2001}, {"title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning", "author": ["Gal", "Yarin", "Ghahramani", "Zoubin"], "venue": "In ICML,", "citeRegEx": "Gal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2016}, {"title": "Fast r-cnn", "author": ["Girshick", "Ross"], "venue": "In ICCV, pp. 1440\u20131448,", "citeRegEx": "Girshick and Ross.,? \\Q2015\\E", "shortCiteRegEx": "Girshick and Ross.", "year": 2015}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Hannun", "Awni", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan", "Diamos", "Greg", "Elsen", "Erich", "Prenger", "Ryan", "Satheesh", "Sanjeev", "Sengupta", "Shubho", "Coates", "Adam"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In CVPR,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks", "author": ["Hendrycks", "Dan", "Gimpel", "Kevin"], "venue": "In ICLR,", "citeRegEx": "Hendrycks et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Hendrycks et al\\.", "year": 2017}, {"title": "Distilling the knowledge in a neural network", "author": ["Hinton", "Geoffrey", "Vinyals", "Oriol", "Dean", "Jeff"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Deep networks with stochastic depth", "author": ["Huang", "Gao", "Sun", "Yu", "Liu", "Zhuang", "Sedra", "Daniel", "Weinberger", "Kilian"], "venue": "In ECCV,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Densely connected convolutional networks", "author": ["Huang", "Gao", "Liu", "Zhuang", "Weinberger", "Kilian Q", "van der Maaten", "Laurens"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2017}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": null, "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Iyyer", "Mohit", "Manjunatha", "Varun", "Boyd-Graber", "Jordan", "Daum\u00e9 III", "Hal"], "venue": "In ACL,", "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Information theory and statistical mechanics", "author": ["Jaynes", "Edwin T"], "venue": "Physical review,", "citeRegEx": "Jaynes and T.,? \\Q1957\\E", "shortCiteRegEx": "Jaynes and T.", "year": 1957}, {"title": "Calibrating predictive model estimates to support personalized medicine", "author": ["Jiang", "Xiaoqian", "Osl", "Melanie", "Kim", "Jihoon", "OhnoMachado", "Lucila"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "Jiang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2012}, {"title": "Modelling uncertainty in deep learning for camera relocalization", "author": ["Kendall", "Alex", "Cipolla", "Roberto"], "venue": null, "citeRegEx": "Kendall et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kendall et al\\.", "year": 2016}, {"title": "What uncertainties do we need in bayesian deep learning for computer vision", "author": ["Kendall", "Alex", "Gal", "Yarin"], "venue": "arXiv preprint arXiv:1703.04977,", "citeRegEx": "Kendall et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kendall et al\\.", "year": 2017}, {"title": "3d object representations for fine-grained categorization", "author": ["Krause", "Jonathan", "Stark", "Michael", "Deng", "Jia", "Fei-Fei", "Li"], "venue": "In IEEE Workshop on 3D Representation and Recognition (3dRR), Sydney, Australia,", "citeRegEx": "Krause et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2013}, {"title": "Learning multiple layers of features from tiny", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Reliable confidence estimation via online learning", "author": ["Kuleshov", "Volodymyr", "Ermon", "Stefano"], "venue": "arXiv preprint arXiv:1607.03594,", "citeRegEx": "Kuleshov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kuleshov et al\\.", "year": 2016}, {"title": "Calibrated structured prediction", "author": ["Kuleshov", "Volodymyr", "Liang", "Percy"], "venue": "In NIPS, pp", "citeRegEx": "Kuleshov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kuleshov et al\\.", "year": 2015}, {"title": "Simple and scalable predictive uncertainty estimation using deep ensembles", "author": ["Lakshminarayanan", "Balaji", "Pritzel", "Alexander", "Blundell", "Charles"], "venue": "arXiv preprint arXiv:1612.01474,", "citeRegEx": "Lakshminarayanan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lakshminarayanan et al\\.", "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "A practical bayesian framework for backpropagation networks", "author": ["MacKay", "David JC"], "venue": "Neural computation,", "citeRegEx": "MacKay and JC.,? \\Q1992\\E", "shortCiteRegEx": "MacKay and JC.", "year": 1992}, {"title": "Obtaining well calibrated probabilities using bayesian binning", "author": ["Naeini", "Mahdi Pakdaman", "Cooper", "Gregory F", "Hauskrecht", "Milos"], "venue": "In AAAI, pp", "citeRegEx": "Naeini et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Naeini et al\\.", "year": 2015}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In Deep Learning and Unsupervised Feature Learning Workshop,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Predicting good probabilities with supervised learning", "author": ["Niculescu-Mizil", "Alexandru", "Caruana", "Rich"], "venue": "In ICML, pp", "citeRegEx": "Niculescu.Mizil et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Niculescu.Mizil et al\\.", "year": 2005}, {"title": "Regularizing neural networks by penalizing confident output distributions", "author": ["Pereyra", "Gabriel", "Tucker", "George", "Chorowski", "Jan", "Kaiser", "\u0141ukasz", "Hinton", "Geoffrey"], "venue": "arXiv preprint arXiv:1701.06548,", "citeRegEx": "Pereyra et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Pereyra et al\\.", "year": 2017}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "author": ["Platt", "John"], "venue": "Advances in large margin classifiers,", "citeRegEx": "Platt and John,? \\Q1999\\E", "shortCiteRegEx": "Platt and John", "year": 1999}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In ICLR,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Socher", "Richard", "Perelygin", "Alex", "Wu", "Jean", "Chuang", "Jason", "Manning", "Christopher D", "Ng", "Andrew", "Potts", "Christopher"], "venue": "In EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Improved semantic representations from treestructured long short-term memory", "author": ["Tai", "Kai Sheng", "Socher", "Richard", "Manning", "Christopher D"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Achieving human parity in conversational speech recognition", "author": ["Xiong", "Wayne", "Droppo", "Jasha", "Huang", "Xuedong", "Seide", "Frank", "Seltzer", "Mike", "Stolcke", "Andreas", "Yu", "Dong", "Zweig", "Geoffrey"], "venue": "arXiv preprint arXiv:1610.05256,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers", "author": ["Zadrozny", "Bianca", "Elkan", "Charles"], "venue": "In ICML, pp", "citeRegEx": "Zadrozny et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Zadrozny et al\\.", "year": 2001}, {"title": "Transforming classifier scores into accurate multiclass probability estimates", "author": ["Zadrozny", "Bianca", "Elkan", "Charles"], "venue": "In KDD, pp", "citeRegEx": "Zadrozny et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Zadrozny et al\\.", "year": 2002}, {"title": "Wide residual networks", "author": ["Zagoruyko", "Sergey", "Komodakis", "Nikos"], "venue": "In BMVC,", "citeRegEx": "Zagoruyko et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko et al\\.", "year": 2016}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["Zhang", "Chiyuan", "Bengio", "Samy", "Hardt", "Moritz", "Recht", "Benjamin", "Vinyals", "Oriol"], "venue": "In ICLR,", "citeRegEx": "Zhang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 11, "context": "Recent advances in deep learning have dramatically improved neural network accuracy (Simonyan & Zisserman, 2015; Srivastava et al., 2015; He et al., 2016; Huang et al., 2016; 2017).", "startOffset": 84, "endOffset": 180}, {"referenceID": 14, "context": "Recent advances in deep learning have dramatically improved neural network accuracy (Simonyan & Zisserman, 2015; Srivastava et al., 2015; He et al., 2016; Huang et al., 2016; 2017).", "startOffset": 84, "endOffset": 180}, {"referenceID": 10, "context": "As a result, neural networks are now entrusted with making complex decisions in applications, such as object detection (Girshick, 2015), speech recognition (Hannun et al., 2014), and medical diagnosis (Caruana et al.", "startOffset": 156, "endOffset": 177}, {"referenceID": 1, "context": ", 2014), and medical diagnosis (Caruana et al., 2015).", "startOffset": 31, "endOffset": 53}, {"referenceID": 0, "context": "As an example, consider a self-driving car that uses a neural network to detect pedestrians and other obstructions (Bojarski et al., 2016).", "startOffset": 115, "endOffset": 138}, {"referenceID": 19, "context": "Alternatively, in automated health care, control should be passed on to human doctors when the confidence of a disease diagnosis network is low (Jiang et al., 2012).", "startOffset": 144, "endOffset": 164}, {"referenceID": 10, "context": "guage model in speech recognition (Hannun et al., 2014; Xiong et al., 2016), or with camera information for object detection (Kendall & Cipolla, 2016).", "startOffset": 34, "endOffset": 75}, {"referenceID": 38, "context": "guage model in speech recognition (Hannun et al., 2014; Xiong et al., 2016), or with camera information for object detection (Kendall & Cipolla, 2016).", "startOffset": 34, "endOffset": 75}, {"referenceID": 27, "context": "This is visualized in Figure 1, which compares a 5-layer LeNet (left) (LeCun et al., 1998) with a 110-layer ResNet (right) (He et al.", "startOffset": 70, "endOffset": 90}, {"referenceID": 11, "context": ", 1998) with a 110-layer ResNet (right) (He et al., 2016) on the CIFAR-100 dataset.", "startOffset": 40, "endOffset": 57}, {"referenceID": 10, "context": "guage model in speech recognition (Hannun et al., 2014; Xiong et al., 2016), or with camera information for object detection (Kendall & Cipolla, 2016). In 2005, Niculescu-Mizil & Caruana (2005) showed that neural networks typically produce well-calibrated probabilities on binary classification tasks.", "startOffset": 35, "endOffset": 194}, {"referenceID": 29, "context": "Expected Calibration Error (Naeini et al., 2015) \u2013 or ECE \u2013 approximates (2) by partitioning predictions into M equally-spaced bins (similar to the reliability diagrams) and", "startOffset": 27, "endOffset": 48}, {"referenceID": 29, "context": "The Maximum Calibration Error (Naeini et al., 2015) \u2013 or MCE \u2013 estimates an upper bound of this deviation.", "startOffset": 30, "endOffset": 51}, {"referenceID": 7, "context": "Negative log likelihood is a standard measure of a probabilistic model\u2019s quality (Friedman et al., 2001).", "startOffset": 81, "endOffset": 104}, {"referenceID": 7, "context": "It is a standard result (Friedman et al., 2001) that, in expectation, NLL is minimized if and only if \u03c0\u0302(Y |X) recovers the ground truth conditional distribution \u03c0(Y |X).", "startOffset": 24, "endOffset": 47}, {"referenceID": 11, "context": "It is now common to see networks with hundreds, if not thousands of layers (He et al., 2016; Huang et al., 2016) and hundreds of convolutional filters per layer (Zagoruyko & Komodakis, 2016).", "startOffset": 75, "endOffset": 112}, {"referenceID": 14, "context": "It is now common to see networks with hundreds, if not thousands of layers (He et al., 2016; Huang et al., 2016) and hundreds of convolutional filters per layer (Zagoruyko & Komodakis, 2016).", "startOffset": 75, "endOffset": 112}, {"referenceID": 42, "context": "Recent work shows that very deep or wide models are able to generalize better than smaller ones, while exhibiting the capacity to easily fit the training set (Zhang et al., 2017).", "startOffset": 158, "endOffset": 178}, {"referenceID": 11, "context": "Recent research suggests that these normalization techniques have enabled the development of very deep architectures, such as ResNets (He et al., 2016) and DenseNets (Huang et al.", "startOffset": 134, "endOffset": 151}, {"referenceID": 15, "context": ", 2016) and DenseNets (Huang et al., 2017).", "startOffset": 22, "endOffset": 42}, {"referenceID": 11, "context": "The top performing ImageNet models of 2015 all use an order of magnitude less weight decay than models of previous years (He et al., 2016; Simonyan & Zisserman, 2015).", "startOffset": 121, "endOffset": 166}, {"referenceID": 42, "context": "Zhang et al. (2017) observe that deep neural networks seemingly violate the common understanding of learning theory that large models with little regularization will not generalize well.", "startOffset": 0, "endOffset": 20}, {"referenceID": 29, "context": "Bayesian Binning into Quantiles (BBQ) (Naeini et al., 2015) is a extension of histogram binning using Bayesian 1 This is in contrast with the setting in Section 2, in which the model produces both a class prediction and confidence.", "startOffset": 38, "endOffset": 59}, {"referenceID": 13, "context": "Temperature scaling is commonly used in settings such as knowledge distillation (Hinton et al., 2015) and statistical mechanics (Jaynes, 1957).", "startOffset": 80, "endOffset": 101}, {"referenceID": 26, "context": "Lakshminarayanan et al. (2016) use ensembles of networks to obtain uncertainty estimates.", "startOffset": 0, "endOffset": 31}, {"referenceID": 26, "context": "Lakshminarayanan et al. (2016) use ensembles of networks to obtain uncertainty estimates. Pereyra et al. (2017) penalize overconfident predictions as a form of regularization.", "startOffset": 0, "endOffset": 112}, {"referenceID": 26, "context": "Lakshminarayanan et al. (2016) use ensembles of networks to obtain uncertainty estimates. Pereyra et al. (2017) penalize overconfident predictions as a form of regularization. Hendrycks & Gimpel (2017) use confidence To highlight the connection with prior works we define temperature scaling in terms of 1 T instead of a multiplicative scalar.", "startOffset": 0, "endOffset": 202}, {"referenceID": 36, "context": "Gal & Ghahramani (2016) draw a connection between Dropout (Srivastava et al., 2014) and model uncertainty, claiming that sampling models with dropped nodes is a way to estimate the probability distribution over all possible models for a given sample.", "startOffset": 58, "endOffset": 83}, {"referenceID": 36, "context": "Gal & Ghahramani (2016) draw a connection between Dropout (Srivastava et al., 2014) and model uncertainty, claiming that sampling models with dropped nodes is a way to estimate the probability distribution over all possible models for a given sample. Kendall & Gal (2017) combine this approach with a model that outputs a predictive mean and variance for each data point.", "startOffset": 59, "endOffset": 272}, {"referenceID": 22, "context": "Stanford Cars (Krause et al., 2013): 196 classes of cars by make, model, and year.", "startOffset": 14, "endOffset": 35}, {"referenceID": 5, "context": "ImageNet 2012 (Deng et al., 2009): Natural scene images from 1000 classes.", "startOffset": 14, "endOffset": 33}, {"referenceID": 30, "context": "Street View House Numbers (SVHN) (Netzer et al., 2011): 32 \u00d7 32 colored images of cropped out house numbers from Google Street View.", "startOffset": 33, "endOffset": 54}, {"referenceID": 11, "context": "We train state-of-the-art convolutional networks: ResNets (He et al., 2016), ResNets with stochastic depth (SD) (Huang et al.", "startOffset": 58, "endOffset": 75}, {"referenceID": 14, "context": ", 2016), ResNets with stochastic depth (SD) (Huang et al., 2016), Wide ResNets (Zagoruyko & Komodakis, 2016), and DenseNets (Huang et al.", "startOffset": 44, "endOffset": 64}, {"referenceID": 15, "context": ", 2016), Wide ResNets (Zagoruyko & Komodakis, 2016), and DenseNets (Huang et al., 2017).", "startOffset": 67, "endOffset": 87}, {"referenceID": 35, "context": "Stanford Sentiment Treebank (SST) (Socher et al., 2013): Movie reviews, represented as sentence parse trees that are annotated by sentiment.", "startOffset": 34, "endOffset": 55}, {"referenceID": 37, "context": "As described in (Tai et al., 2015), the training/validation/test sets contain 6920/872/1821 documents for binary, and 544/1101/2210 for fine-grained.", "startOffset": 16, "endOffset": 34}, {"referenceID": 17, "context": "On 20 News and Reuters, we train Deep Averaging Networks (DANs) (Iyyer et al., 2015) with 3 feed-forward layers and Batch Normalization.", "startOffset": 64, "endOffset": 84}, {"referenceID": 37, "context": "On SST, we train TreeLSTMs (Long Short Term Memory) (Tai et al., 2015) using the default settings in the authors\u2019 code.", "startOffset": 52, "endOffset": 70}, {"referenceID": 2, "context": "In Torch7 (Collobert et al., 2011), for example, we implement temperature scaling by inserting a nn.", "startOffset": 10, "endOffset": 34}], "year": 2017, "abstractText": "Confidence calibration \u2013 the problem of predicting probability estimates representative of the true correctness likelihood \u2013 is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-ofthe-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling \u2013 a singleparameter variant of Platt Scaling \u2013 is surprisingly effective at calibrating predictions.", "creator": "LaTeX with hyperref package"}}}