{"id": "1706.04690", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP", "abstract": "Online sparse linear regression is an online problem where an algorithm repeatedly chooses a subset of coordinates to observe in an adversarially chosen feature vector, makes a real-valued prediction, receives the true label, and incurs the squared loss. The goal is to design an online learning algorithm with sublinear regret to the best sparse linear predictor in hindsight. Without any assumptions, this problem is known to be computationally intractable. In this paper, we make the assumption that data matrix satisfies restricted isometry property, and show that this assumption leads to computationally efficient algorithms with sublinear regret for two variants of the problem. In the first variant, the true label is generated according to a sparse linear model with additive Gaussian noise. In the second, the true label is chosen adversarially.", "histories": [["v1", "Wed, 14 Jun 2017 23:03:54 GMT  (25kb)", "http://arxiv.org/abs/1706.04690v1", "Appearing in 34th International Conference on Machine Learning (ICML), 2017"]], "COMMENTS": "Appearing in 34th International Conference on Machine Learning (ICML), 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["satyen kale", "zohar s karnin", "tengyuan liang", "d\u00e1vid p\u00e1l"], "accepted": true, "id": "1706.04690"}, "pdf": {"name": "1706.04690.pdf", "metadata": {"source": "CRF", "title": "Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP", "authors": ["Satyen Kale"], "emails": ["satyenkale@google.com", "zkarnin@gmail.com", "Tengyuan.Liang@chicagobooth.edu", "dpal@yahoo-inc.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 6.04 690v 1 [cs.L G] 1Online sparse linear regression is an online problem in which an algorithm repeatedly selects a subset of coordinates which it observes in a hostile chosen characteristic vector, makes a real prediction, receives the true label and suffers the square loss. The aim is to design an online learning algorithm which subsequently regrets the best sparse linear predictor. Without assumptions it is known that this problem is mathematically insoluble. In this paper we assume that the data matrix fulfils limited isometric properties and show that this assumption leads to computationally efficient algorithms with sublinear regret for two variants of the problem. In the first variant, the true label is generated on the basis of a sparse linear model with additive Gaussian noise. In the second, the true label is reversed."}, {"heading": "1 Introduction", "text": "In modern, sequential prediction problems, the dimensions are typically high, and the construction of features can be even a mathematically intensive task. (For this reason, there is only a limited number of features for each new data variant in sequential prediction.) An example of this situation is the medical diagnosis of a disease in which each feature is the result of a medical test on the patient. As it is undesirable to submit a patient to a battery of medical tests, we would like to design adaptive diagnostic procedures based on few, highly informative tests. (OSLR) is a sequential prediction problem in which an algorithm allows to see a small subset of coordinates of each function. The problem is parameterized by 3 positive integers: d, the dimensions of features of vectors, functionality, functionality, functionality, functionality, functionality, functionality, functionality of the functionality of the functionality of the functionality, functionality of the functionality of the functionality of the function, the functionality of the functionality of the functionality of the function of the function, the functionality of the functionality of the functionality of the function of the vectors, the functionality of the functionality of the vectors of the vectors, the functionality of the functionality of the vectors, the functionality of the functionality of the functionality of the functionality of the vectors, the functionality of the functionality of the functionality of the functionality of the vectors, the functionality of the functionality of the functionality of the functionality of the functionality of the vectors, the functionality of the functionality of the functionality of the vectors, the functionality of the functionality of the functionality of the functionality of the functionality of the functionality of the vectors, the functionality of the functionality of the functionality of the functionality of the functionality of the sequence of the functionality of the functionality of the functionality of the functionality of the sequence of the functionality of the"}, {"heading": "1.1 Summary of Results", "text": "In fact, it is not so that it is a real problem, but a real problem, which is to be observed in both cases in the real world. (...) The real situation of people in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world,"}, {"heading": "1.2 Related work", "text": "In fact, most of us are able to survive on our own if we do not put ourselves in a position to survive on our own."}, {"heading": "1.3 Notation and Preliminaries", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "1.4 Proper Online Sparse Linear Regression", "text": "We present a variant of Online Sparse Regression (OSLR), which we call real Online Sparse Linear Regression (POSLR). The adjective \"correct\" means that the algorithm must output a weight vector in each round and its prediction is calculated by taking an internal product with the feature vector. We assume that there is an underlying sequence (x1, y1), (x2, y2),.. (xT, yT) of the examples described in Rd \u00b7 R. In each round t = 1, 2,.., T, the algorithm behaves according to the following protocol: 1. Select a vector wt. \u2212 kRd so that it is 0 \u2264 k.2."}, {"heading": "2 Realizable Model", "text": "In this section, we will design an algorithm for POSLR for the realizable model. In this setting, we assume that there is a vector w * * * Rd that causes the label sequence y1, y2,... yT to be generated according to the linear model yt = < xt, w * > + \u03b7t, (1), where \u03b71, \u03b72,..., \u03b7T are independent random variables of N (0, \u03c32). We assume that the standard deviation \u03c3, or an upper limit thereof, is given as input to the algorithm. We assume that the vector (\u041a1, \u04452,...) applies to all."}, {"heading": "2.1 Algorithm", "text": "The algorithm maintains an unbiased estimate X of the MatrixXt. (The rows of X-t are vectors x-T 1, x-T 2,.., x-T t, the unbiased estimates of xT1, x-T 2,..., x-t to construct the estimates, in each round w, we consider the linear predictions of the algorithm w by selecting the linear predictions of [d] the magnitude k0. (The estimate isx-t = dk0 \u00b7 xt (St). (2) To calculate the predictions of the algorithm, we consider the linear predictions of [d] the magnitude k0 s.t. (D-T)."}, {"heading": "2.2 Main Result", "text": "The main result in this section is a logarithmic regret under the following assumptions: 2 \u00b7 The feature vectors have the property that the matrix Xt fulfills the RIP condition with (15, 3k), with t0 = O (k log (d) log (T)))) \u2022 The underlying POSLR online prediction problem has a savings budget of k and observation budget k0. \u2022 The model is feasible as in Equation (1) with i.i.d unbiased Gaussian noise with standard deviation \u03c3 = O (1).Theorem 2. For any other prediction > 0, with a probability of at least 1 \u2212 3, algorithm 1 satisfactoryRegretT = O (k2 log (d / k0) 3 log (T) 3 log (T)).The theorem claims that an O (logT) condition is efficiently achievable in the realizable setting."}, {"heading": "2.3 Sketch of Proof", "text": "The main building block in the evidence collection theorem 2 is given in term 3. It proves that the sequence of solutions w \u00b2 does not follow the optimal answer w \u00b2 on which the signal yt \u00b2 is based. Specifically, ignoring all second-order conditions shows that the difference between our answer < xt, wt > and the (almost) optimal answer < xt, w \u00b2 s > is limited to the sparse approximation wt \u00b2 t \u00b2. Now that a careful calculation of the difference in losses has led to a regret, the difference between our answers < xt, wt > and the (almost) optimal answer < xt, w \u00b2 s > is limited to the sparse approximation of w \u00b2 t. Given this fact, a careful calculation of the difference in losses leads to a regret-bound w.r.t. w \u00b2 In particular, an elementary analysis of the loss expression leads to equality."}, {"heading": "3 Agnostic Setting", "text": "In this section we focus on the agnostic constellation in which we do not put a distribution assumption on the sequence. (In this context, there is no \"true\" sparse model, but the learner - with limited access to features - competes with the best condition-sparse model defined using complete information. (xt, yt) Tt = 1.As before, we assume that xt and yt are limited. (2016) We have shown that reaching a sublinear condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the condition for the sequence of the condition for the condition for the sequence of the condition for the sequence of the sequence of the"}, {"heading": "3.1 Algorithm", "text": "The algorithms in the agnostic environments are of different nature, starting from the stochastic setting. Our algorithm is motivated by literature aimed at maximizing the sub-modular set function. [1] Although the problem is solved in the way the experts use it as submodular algorithms, algorithmization is limited to submodular maximization. [2] Boutsidis et al. [3] has the sparse linear regression as the maximization of the weak supermodular function. We will introduce an algorithm that incorporates various ideas from the referenced literature to attack online austerity with limited features."}, {"heading": "3.2 Main Result", "text": "In this section we will show that Algorithm 2 is a direct sequence of Lemma 5 in the appendix."}, {"heading": "4 Conclusions and Future Work", "text": "In this paper, we have given computationally efficient algorithms for the online problem of sparse linear regression assuming that the design matrices of feature vectors fulfill RIP-like characteristics. Since the problem is difficult without assumptions, our work is the first to show that assumptions similar to those used for sparse recovery in batch setting also provide traceability in the online setting. Are there still several open questions in this work line and will be the basis for future work? Is it possible to improve the regret tied to agnostic setting? Can we lower the limit to regret in different settings? Is it possible to loosen the RIP assumption on the design matrices and have even more efficient algorithms? Some obvious weaknesses of the RIP assumption that we have made do not result in traceability. For example, by simply assuming that the final matrix XIP RT satisfies lower levels of matrix satisfaction and does not sufficiently adapt to each Xrix medium size."}, {"heading": "A Proofs for Realizable Setting", "text": "That is, the permutation is determined by the order of magnitude of entries outside S. We divide Sc into subsets of size k to which this permutation applies: Define Sj, for j, for j, for j, for j, for j, for j, for j, for j, for j, for j, for j, for j, for j, for j, for j, for j, for j, for j, for j, for j, for j, for j, for j, for j, for j, for j, for j."}, {"heading": "B Proofs for Agnostic Setting", "text": "We start with an auxiliary sample for Lemma 10, which deals with the question of whether it is a problem that is a complete problem. (...) We start with an auxiliary sample for Lemma 10. (...) We start with a sample for the nominal strength of the nominal size. (...) We start with a sample for the nominal size. (...) We start with a sample for the nominal size. (...) We start with a sample for the nominal size. (...) We start with a sample for the nominal size. (...) We start with a sample for the nominal size. (...) We start with a sample for the nominal size. (...) We start with a sample for the nominal size. (...) We start with nominal size. (...) We start with nominal size. (...) We start with nominal size. (...) We start with nominal size. (...) We start with nominal size. (...) We start with a matrix whose nominal value. (...) We start with a sample for the nominal size. (... nominal size. (...) We start with a nominal size. (... We. (...) We start with a nominal size. (...). (... We."}], "references": [{"title": "Budgeted prediction with expert advice", "author": ["Kareem Amin", "Satyen Kale", "Gerald Tesauro", "Deepak S. Turaga"], "venue": "In AAAI,", "citeRegEx": "Amin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Amin et al\\.", "year": 2015}, {"title": "Linear and conic programming estimators in high dimensional errors-in-variables models", "author": ["Alexandre Belloni", "Mathieu Rosenbaum", "Alexandre B. Tsybakov"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Belloni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Belloni et al\\.", "year": 2016}, {"title": "Simultaneous analysis of Lasso and Dantzig selector", "author": ["Peter J Bickel", "Ya\u2019acov Ritov", "Alexandre B Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Greedy minimization of weakly supermodular set functions", "author": ["Christos Boutsidis", "Edo Liberty", "Maxim Sviridenko"], "venue": "arXiv preprint arXiv:1502.06528,", "citeRegEx": "Boutsidis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2015}, {"title": "The Dantzig selector: statistical estimation when p is much larger than n", "author": ["Emmanuel Candes", "Terence Tao"], "venue": "The Annals of Statistics,", "citeRegEx": "Candes and Tao.,? \\Q2007\\E", "shortCiteRegEx": "Candes and Tao.", "year": 2007}, {"title": "Decoding by linear programming", "author": ["Emmanuel J Candes", "Terence Tao"], "venue": "IEEE transactions on information theory,", "citeRegEx": "Candes and Tao.,? \\Q2005\\E", "shortCiteRegEx": "Candes and Tao.", "year": 2005}, {"title": "Prediction, learning, and games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Efficient learning with partially observed attributes", "author": ["Nicol\u00f2 Cesa-Bianchi", "Shai Shalev-Shwartz", "Ohad Shamir"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2011}, {"title": "Variable selection is hard", "author": ["Dean Foster", "Howard Karloff", "Justin Thaler"], "venue": "In COLT,", "citeRegEx": "Foster et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2015}, {"title": "Online sparse linear regression", "author": ["Dean Foster", "Satyen Kale", "Howard Karloff"], "venue": "In COLT,", "citeRegEx": "Foster et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2016}, {"title": "Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization", "author": ["Elad Hazan", "Satyen Kale"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hazan and Kale.,? \\Q2014\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2014}, {"title": "Linear regression with limited observation", "author": ["Elad Hazan", "Tomer Koren"], "venue": "In ICML,", "citeRegEx": "Hazan and Koren.,? \\Q2012\\E", "shortCiteRegEx": "Hazan and Koren.", "year": 2012}, {"title": "Model selection for high-dimensional regression under the generalized irrepresentability condition", "author": ["Adel Javanmard", "Andrea Montanari"], "venue": "In NIPS,", "citeRegEx": "Javanmard and Montanari.,? \\Q2013\\E", "shortCiteRegEx": "Javanmard and Montanari.", "year": 2013}, {"title": "Open problem: Efficient online sparse regression", "author": ["Satyen Kale"], "venue": "In COLT, pages 1299\u20131301,", "citeRegEx": "Kale.,? \\Q2014\\E", "shortCiteRegEx": "Kale.", "year": 2014}, {"title": "Attribute efficient linear regression with distribution-dependent sampling", "author": ["Doron Kukliansky", "Ohad Shamir"], "venue": "In ICML,", "citeRegEx": "Kukliansky and Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Kukliansky and Shamir.", "year": 2015}, {"title": "Learning without concentration", "author": ["Shahar Mendelson"], "venue": "In COLT, pages", "citeRegEx": "Mendelson.,? \\Q2014\\E", "shortCiteRegEx": "Mendelson.", "year": 2014}, {"title": "Sparse approximate solutions to linear systems", "author": ["Balas Kausik Natarajan"], "venue": "SIAM journal on computing,", "citeRegEx": "Natarajan.,? \\Q1995\\E", "shortCiteRegEx": "Natarajan.", "year": 1995}, {"title": "Sparse recovery under matrix uncertainty", "author": ["Mathieu Rosenbaum", "Alexandre B. Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "Rosenbaum and Tsybakov.,? \\Q2010\\E", "shortCiteRegEx": "Rosenbaum and Tsybakov.", "year": 2010}, {"title": "An online algorithm for maximizing submodular functions", "author": ["Matthew J. Streeter", "Daniel Golovin"], "venue": "In NIPS,", "citeRegEx": "Streeter and Golovin.,? \\Q2008\\E", "shortCiteRegEx": "Streeter and Golovin.", "year": 2008}, {"title": "On model selection consistency of lasso", "author": ["Peng Zhao", "Bin Yu"], "venue": "Journal of Machine learning research,", "citeRegEx": "Zhao and Yu.,? \\Q2006\\E", "shortCiteRegEx": "Zhao and Yu.", "year": 2006}, {"title": "Online learning with costly features and labels", "author": ["Navid Zolghadr", "G\u00e1bor Bart\u00f3k", "Russell Greiner", "Andr\u00e1s Gy\u00f6rgy", "Csaba Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "Zolghadr et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zolghadr et al\\.", "year": 2013}, {"title": "X\u0304w\u2016 is weakly supermodular. Its proof can be found in [Boutsidis et al., 2015], yet for completeness we provide it here as well", "author": ["\u2016y"], "venue": null, "citeRegEx": "\u2212,? \\Q2015\\E", "shortCiteRegEx": "\u2212", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "One example of this situation, from [Cesa-Bianchi et al., 2011], is medical diagnosis of a disease, in which each feature is the result of a medical test on the patient.", "startOffset": 36, "endOffset": 63}, {"referenceID": 5, "context": "In this paper, we answer this open question by providing efficient algorithms with sublinear regret under the assumption that the matrix of feature vectors satisfies the restricted isometry property (RIP) [Candes and Tao, 2005].", "startOffset": 205, "endOffset": 227}, {"referenceID": 2, "context": "RIP and related Restricted Eigenvalue Condition [Bickel et al., 2009] have been widely used as a standard assumption for theoretical analysis in the compressive sensing and sparse regression literature, in the offline case.", "startOffset": 48, "endOffset": 69}, {"referenceID": 5, "context": ", yT as inputs, the offline problem of finding a k-sparsew that minimizes the error \u2211T t=1(yt\u2212\u3008xt, w\u3009)2 does not admit a polynomial time algorithm under standard complexity assumptions Foster et al. [2015]. This hardness persists even under the assumption that there exists a k-sparse w such that yt = \u3008xt, w\u2217\u3009 for all t.", "startOffset": 185, "endOffset": 206}, {"referenceID": 5, "context": ", yT as inputs, the offline problem of finding a k-sparsew that minimizes the error \u2211T t=1(yt\u2212\u3008xt, w\u3009)2 does not admit a polynomial time algorithm under standard complexity assumptions Foster et al. [2015]. This hardness persists even under the assumption that there exists a k-sparse w such that yt = \u3008xt, w\u2217\u3009 for all t. Furthermore, the computational hardness is present even when the solution is required to be only \u00d5(k)-sparse solution and has to minimize the error only approximately; see Foster et al. [2015] for details.", "startOffset": 185, "endOffset": 515}, {"referenceID": 5, "context": ", yT as inputs, the offline problem of finding a k-sparsew that minimizes the error \u2211T t=1(yt\u2212\u3008xt, w\u3009)2 does not admit a polynomial time algorithm under standard complexity assumptions Foster et al. [2015]. This hardness persists even under the assumption that there exists a k-sparse w such that yt = \u3008xt, w\u2217\u3009 for all t. Furthermore, the computational hardness is present even when the solution is required to be only \u00d5(k)-sparse solution and has to minimize the error only approximately; see Foster et al. [2015] for details. The hardness result was extended to online sparse regression by Foster et al. [2016]. They showed that for all \u03b4 > 0 there exists no polynomial-time algorithm with regret O(T ) unless NP \u2286 BPP .", "startOffset": 185, "endOffset": 613}, {"referenceID": 5, "context": ", yT as inputs, the offline problem of finding a k-sparsew that minimizes the error \u2211T t=1(yt\u2212\u3008xt, w\u3009)2 does not admit a polynomial time algorithm under standard complexity assumptions Foster et al. [2015]. This hardness persists even under the assumption that there exists a k-sparse w such that yt = \u3008xt, w\u2217\u3009 for all t. Furthermore, the computational hardness is present even when the solution is required to be only \u00d5(k)-sparse solution and has to minimize the error only approximately; see Foster et al. [2015] for details. The hardness result was extended to online sparse regression by Foster et al. [2016]. They showed that for all \u03b4 > 0 there exists no polynomial-time algorithm with regret O(T ) unless NP \u2286 BPP . Foster et al. [2016] posed the open question of what additional assumptions can be made on the data to make the problem tractable.", "startOffset": 185, "endOffset": 744}, {"referenceID": 3, "context": "In this paper, we answer this open question by providing efficient algorithms with sublinear regret under the assumption that the matrix of feature vectors satisfies the restricted isometry property (RIP) [Candes and Tao, 2005]. It has been shown that if RIP holds and there exists a sparse linear predictor w such that yt = \u3008xt, w\u2217\u3009 + \u03b7t where \u03b7t is independent noise, the offline sparse linear regression problem admits computationally efficient algorithms, e.g., Candes and Tao [2007]. RIP and related Restricted Eigenvalue Condition [Bickel et al.", "startOffset": 206, "endOffset": 488}, {"referenceID": 4, "context": "The solution of this problem cannot be obtained by a simple application of say, the Dantzig selector [Candes and Tao, 2007] since we do not observe the data matrix X , but rather a subsample of its entries.", "startOffset": 101, "endOffset": 123}, {"referenceID": 10, "context": "This bound has optimal dependence on T , since even in the full information setting where all features are observed there is a lower bound of \u03a9(logT ) [Hazan and Kale, 2014].", "startOffset": 151, "endOffset": 173}, {"referenceID": 3, "context": "The analysis in [Boutsidis et al., 2015] shows that the RIP assumption implies that the set function defined as the minimum loss achievable by a linear regressor restricted to the set in question satisfies a property called weak supermodularity.", "startOffset": 16, "endOffset": 40}, {"referenceID": 3, "context": "The analysis in [Boutsidis et al., 2015] shows that the RIP assumption implies that the set function defined as the minimum loss achievable by a linear regressor restricted to the set in question satisfies a property called weak supermodularity. Weak supermodularity is a relaxation of standard supermodularity that is still strong enough to show performance bounds for the standard greedy feature selection algorithm for solving the sparse regression problem. We then employ a technique developed by Streeter and Golovin [2008] to construct an online learning algorithm that mimics the greedy feature selection algorithm.", "startOffset": 17, "endOffset": 529}, {"referenceID": 3, "context": "2 Related work A related setting is attribute-efficient learning [Cesa-Bianchi et al., 2011, Hazan and Koren, 2012, Kukliansky and Shamir, 2015]. This is a batch learning problem in which the examples are generated i.i.d., and the goal is to simply output a linear regressor using only a limited number of features per example with bounded excess risk compared to the optimal linear regressor, when given full access to the features at test time. Since the goal is not prediction but simply computing the optimal linear regressor, efficient algorithms exist and have been developed by the aforementioned papers. Without any assumptions, only inefficient algorithms for the online sparse linear regression problem are known Zolghadr et al. [2013], Foster et al.", "startOffset": 66, "endOffset": 746}, {"referenceID": 3, "context": "2 Related work A related setting is attribute-efficient learning [Cesa-Bianchi et al., 2011, Hazan and Koren, 2012, Kukliansky and Shamir, 2015]. This is a batch learning problem in which the examples are generated i.i.d., and the goal is to simply output a linear regressor using only a limited number of features per example with bounded excess risk compared to the optimal linear regressor, when given full access to the features at test time. Since the goal is not prediction but simply computing the optimal linear regressor, efficient algorithms exist and have been developed by the aforementioned papers. Without any assumptions, only inefficient algorithms for the online sparse linear regression problem are known Zolghadr et al. [2013], Foster et al. [2016]. Kale [2014] posed the open question of whether it is possible to design an efficient algorithm for the problem with a sublinear regret bound.", "startOffset": 66, "endOffset": 768}, {"referenceID": 3, "context": "2 Related work A related setting is attribute-efficient learning [Cesa-Bianchi et al., 2011, Hazan and Koren, 2012, Kukliansky and Shamir, 2015]. This is a batch learning problem in which the examples are generated i.i.d., and the goal is to simply output a linear regressor using only a limited number of features per example with bounded excess risk compared to the optimal linear regressor, when given full access to the features at test time. Since the goal is not prediction but simply computing the optimal linear regressor, efficient algorithms exist and have been developed by the aforementioned papers. Without any assumptions, only inefficient algorithms for the online sparse linear regression problem are known Zolghadr et al. [2013], Foster et al. [2016]. Kale [2014] posed the open question of whether it is possible to design an efficient algorithm for the problem with a sublinear regret bound.", "startOffset": 66, "endOffset": 781}, {"referenceID": 3, "context": "2 Related work A related setting is attribute-efficient learning [Cesa-Bianchi et al., 2011, Hazan and Koren, 2012, Kukliansky and Shamir, 2015]. This is a batch learning problem in which the examples are generated i.i.d., and the goal is to simply output a linear regressor using only a limited number of features per example with bounded excess risk compared to the optimal linear regressor, when given full access to the features at test time. Since the goal is not prediction but simply computing the optimal linear regressor, efficient algorithms exist and have been developed by the aforementioned papers. Without any assumptions, only inefficient algorithms for the online sparse linear regression problem are known Zolghadr et al. [2013], Foster et al. [2016]. Kale [2014] posed the open question of whether it is possible to design an efficient algorithm for the problem with a sublinear regret bound. This question was answered in the negative by Foster et al. [2016], who showed that efficiency can only be obtained under additional assumptions on the data.", "startOffset": 66, "endOffset": 978}, {"referenceID": 2, "context": "In the realizable setting, the linear program at the heart of the algorithm is motivated from Dantzig selection Candes and Tao [2007] and error-in-variable regression Rosenbaum and Tsybakov [2010], Belloni et al.", "startOffset": 112, "endOffset": 134}, {"referenceID": 2, "context": "In the realizable setting, the linear program at the heart of the algorithm is motivated from Dantzig selection Candes and Tao [2007] and error-in-variable regression Rosenbaum and Tsybakov [2010], Belloni et al.", "startOffset": 112, "endOffset": 197}, {"referenceID": 1, "context": "In the realizable setting, the linear program at the heart of the algorithm is motivated from Dantzig selection Candes and Tao [2007] and error-in-variable regression Rosenbaum and Tsybakov [2010], Belloni et al. [2016]. The problem of finding the best sparse linear predictor when only a sample of the entries in the data matrix is available is also discussed by Belloni et al.", "startOffset": 198, "endOffset": 220}, {"referenceID": 1, "context": "In the realizable setting, the linear program at the heart of the algorithm is motivated from Dantzig selection Candes and Tao [2007] and error-in-variable regression Rosenbaum and Tsybakov [2010], Belloni et al. [2016]. The problem of finding the best sparse linear predictor when only a sample of the entries in the data matrix is available is also discussed by Belloni et al. [2016] (see also the references therein).", "startOffset": 198, "endOffset": 386}, {"referenceID": 1, "context": "In the realizable setting, the linear program at the heart of the algorithm is motivated from Dantzig selection Candes and Tao [2007] and error-in-variable regression Rosenbaum and Tsybakov [2010], Belloni et al. [2016]. The problem of finding the best sparse linear predictor when only a sample of the entries in the data matrix is available is also discussed by Belloni et al. [2016] (see also the references therein). In fact, these papers solve a more general problem where we observe a matrix Z rather than X that is an unbiased estimator of X . While we can use their results in a black-box manner, they are tailored for the setting where the variance of each Zij is constant and it is difficult to obtain the exact dependence on this variance in their bounds. In our setting, this variance can be linear in the dimension of the feature vectors, and hence we wish to control the dependence on the variance in the bounds. Thus, we use an algorithm that is similar to the one in Belloni et al. [2016], and provide an analysis for it (in the appendix).", "startOffset": 198, "endOffset": 1005}, {"referenceID": 4, "context": "The following definition will play a key role: Definition 1 (Restricted Isometry Property Candes and Tao [2007]).", "startOffset": 90, "endOffset": 112}, {"referenceID": 4, "context": "The following definition will play a key role: Definition 1 (Restricted Isometry Property Candes and Tao [2007]). Let \u01eb \u2208 (0, 1) and k \u2265 0. We say that a matrix X \u2208 R satisfies restricted isometry property (RIP) with parameters (\u01eb, k) if for any w \u2208 R with \u2016w\u20160 \u2264 k we have (1\u2212 \u01eb) \u2016w\u20162 \u2264 1 \u221a n \u2016Xw\u20162 \u2264 (1 + \u01eb) \u2016w\u20162 . One can show that RIP holds with overwhelming probability if n = \u03a9(\u01ebk log(ed/k)) and each row of the matrix is sampled independently from an isotropic sub-Gaussian distribution. In the realizable setting, the sub-Gaussian assumption can be relaxed to incorporate heavy tail distribution via the \u201csmall ball\u201d analysis introduced in Mendelson [2014], since we only require one-sided lower isometry property.", "startOffset": 90, "endOffset": 665}, {"referenceID": 8, "context": "Once again, without any regularity condition on the design matrix, Foster et al. [2016] have shown that achieving a sub-linear regret O(T ) is in general computationally hard, for any constant \u03b4 > 0 unless NP \u2286 BPP.", "startOffset": 67, "endOffset": 88}, {"referenceID": 3, "context": "Our algorithm is motivated from literature on maximization of sub-modular set function [Natarajan, 1995, Streeter and Golovin, 2008, Boutsidis et al., 2015]. Though the problem being NP-hard, greedy algorithm on sub-modular maximization provides provable good approximation ratio. Specifically, Streeter and Golovin [2008] considered online optimization of super/sub-modular set functions using expert algorithm as sub-routine.", "startOffset": 133, "endOffset": 323}, {"referenceID": 3, "context": "Our algorithm is motivated from literature on maximization of sub-modular set function [Natarajan, 1995, Streeter and Golovin, 2008, Boutsidis et al., 2015]. Though the problem being NP-hard, greedy algorithm on sub-modular maximization provides provable good approximation ratio. Specifically, Streeter and Golovin [2008] considered online optimization of super/sub-modular set functions using expert algorithm as sub-routine. Natarajan [1995], Boutsidis et al.", "startOffset": 133, "endOffset": 445}, {"referenceID": 3, "context": "Our algorithm is motivated from literature on maximization of sub-modular set function [Natarajan, 1995, Streeter and Golovin, 2008, Boutsidis et al., 2015]. Though the problem being NP-hard, greedy algorithm on sub-modular maximization provides provable good approximation ratio. Specifically, Streeter and Golovin [2008] considered online optimization of super/sub-modular set functions using expert algorithm as sub-routine. Natarajan [1995], Boutsidis et al. [2015] cast the sparse linear regression as maximization of weakly supermodular function.", "startOffset": 133, "endOffset": 470}, {"referenceID": 3, "context": "The definition is slightly stronger than that in Boutsidis et al. [2015]. We will show that sparse linear regression can be viewed as weakly supermodular minimization in Definition 7 once the design matrix has bounded restricted condition number.", "startOffset": 49, "endOffset": 73}, {"referenceID": 18, "context": "For the latter problem, we develop an online greedy algorithm along the lines of [Streeter and Golovin, 2008].", "startOffset": 81, "endOffset": 109}, {"referenceID": 0, "context": "We employ k1 = O\u2217(k) budgeted experts algorithms [Amin et al., 2015], denoted BEXP, with budget parameter k0 k1 .", "startOffset": 49, "endOffset": 68}, {"referenceID": 0, "context": "The precise characteristics of BEXP are given in Theorem 8 (adapted from Theorem 2 in [Amin et al., 2015]).", "startOffset": 86, "endOffset": 105}, {"referenceID": 3, "context": "This lemma is a direct consequence of Lemma 5 in [Boutsidis et al., 2015].", "startOffset": 49, "endOffset": 73}, {"referenceID": 6, "context": "8 in Cesa-Bianchi and Lugosi [2006]: Lemma 13.", "startOffset": 5, "endOffset": 36}, {"referenceID": 8, "context": "For example, simply assuming that the final matrix XT satisfies RIP rather than every intermediate matrix Xt for large enough t is not sufficient; a simple tweak to the lower bound construction of Foster et al. [2016] shows this.", "startOffset": 197, "endOffset": 218}], "year": 2017, "abstractText": "Online sparse linear regression is an online problem where an algorithm repeatedly chooses a subset of coordinates to observe in an adversarially chosen feature vector, makes a real-valued prediction, receives the true label, and incurs the squared loss. The goal is to design an online learning algorithm with sublinear regret to the best sparse linear predictor in hindsight. Without any assumptions, this problem is known to be computationally intractable. In this paper, we make the assumption that data matrix satisfies restricted isometry property, and show that this assumption leads to computationally efficient algorithms with sublinear regret for two variants of the problem. In the first variant, the true label is generated according to a sparse linear model with additive Gaussian noise. In the second, the true label is chosen adversarially.", "creator": "LaTeX with hyperref package"}}}