{"id": "1706.04732", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "Revenue Optimization with Approximate Bid Predictions", "abstract": "In the context of advertising auctions, finding good reserve prices is a notoriously challenging learning problem. This is due to the heterogeneity of ad opportunity types and the non-convexity of the objective function. In this work, we show how to reduce reserve price optimization to the standard setting of prediction under squared loss, a well understood problem in the learning community. We further bound the gap between the expected bid and revenue in terms of the average loss of the predictor. This is the first result that formally relates the revenue gained to the quality of a standard machine learned model.", "histories": [["v1", "Thu, 15 Jun 2017 04:05:56 GMT  (64kb,D)", "http://arxiv.org/abs/1706.04732v1", "Submitted to NIPS 2017"]], "COMMENTS": "Submitted to NIPS 2017", "reviews": [], "SUBJECTS": "cs.LG cs.GT", "authors": ["andr\\'es mu\\~noz medina", "sergei vassilvitskii"], "accepted": true, "id": "1706.04732"}, "pdf": {"name": "1706.04732.pdf", "metadata": {"source": "CRF", "title": "Revenue Optimization with Approximate Bid Predictions", "authors": ["Andr\u00e9s Mu\u00f1oz Medina", "Sergei Vassilvitskii"], "emails": ["sergeiv}@google.com"], "sections": [{"heading": "1 Introduction", "text": "In practice, there are two challenges that make this problem much more complicated. First, the distribution of value is never directly known; rather, the auctioneer can only observe the samples taken from it. Second, the items for sale are heterogeneous, and there are literally trillions of different types of items being sold. Therefore, it is likely that a particular type of item has never been observed before, and no information about its value is known. A standard approach to machine learning that addresses the heterogeneity of items is parameterized every impression by a feature vector, with the underlying assumption that bees are observed from auctions with similar characteristics."}, {"heading": "1.1 Related Work", "text": "The main contribution in this direction is the prediction that a near-optimal auction can only be designed with the knowledge of a fixed number of Myerson [1981] who has introduced an optimal auction design. Further work by Chawla et al. [2007] and Hartline and Roughgarden [2009], among others, refines its results to increasingly complex sales, taking into account multiple items, different demand functions and weaker assumptions about the shape of the value allocations. Most of the classical literature on revenue optimization focuses on the design of optimal auctions when the bidding distribution of buyers is known. Recent work has looked at the calculation and information challenges in terms of the shape of optimal auctions from the data. A long series of works [Cole and Roughgarden, 2015, Devanur et al., 2016, Dhangwatnotai et al., 2015, Morgenstern and Roughgarden, 2015] analyzes the example complexity of designing optimal auctions."}, {"heading": "2 Setup", "text": "Let D be a distribution over X \u00b7 [0, 1]. Let h: X \u2192 [0, 1] be a bidding prediction function and denote the square loss resulting from h: E [(h (x) \u2212 b) 2] = \u03b72.We assume h is given, and do not make assumptions about the structure of h or how it is achieved; it can be learned, for example, from other data. Let S = ((((x1, b1),.., (xm, bm)). Let D take a series of m i.i.d. samples from D and denote SX = (x1,., xm) its projection on X. Let S = (x1, b1) give a price forecast on Rev (p, b) = 1b = the revenues generated by the bidder."}, {"heading": "2.1 Generalization Error", "text": "Understanding the difference between the empirical performance of an algorithm and its expected performance, also known as a generalization error, is a central tenet of learning theory. At a high level, the generalization error is a function of the size of the learning group: larger groups lead to smaller generalization errors; and the inherent complexity of the learning algorithm: simple rules such as linear classifiers generalize better than more complex ones. In this paper we describe the complexity of a class G of functions based on its growth function. The growth function corresponds to the maximum number of binary labels that can be achieved by G over all possible samples SX. It is closely related to the VC dimension when the range of functions in G relates to values in {0, 1} and the pseudo-dimension [Morgenstern and Roughgarden, 2015, Mohri et mirial]."}, {"heading": "3 Warmup", "text": "To better understand the problem, we begin with the introduction of a simple mechanism for transforming the hypothesis function h into a reserve price function r with guarantees for its achievable repetitions. Lemma 1. Let r: X \u2192 [0, 1] be defined by r (x): = max (h) \u2212 \u03b72 / 3, 0). Function r then fulfills S (r) \u2264 1 / 2 + 2\u03b72 / 3. Evidence. According to the definition of S and r we have: S (r) = E [b \u2212 r (x) 1b \u2265 r (x)] = E [b \u2212 r (x)] + E [r (x) < r (x) < r (x) < r (x)]] \u2264 E [b \u2212 h (x) + E (c) \u2212 r (c) \u2212 a) \u2212 r (c)."}, {"heading": "4 Results Overview", "text": "In principle, to maximize revenue, we need to find a class of functions G with low complexity, but this contains a function that roughly minimizes empirical separation; the difficulty stems from the fact that the Rev revenue function is not continuous and highly non-concave - a small price change p can lead to very large revenue changes. This is the main reason why simply using the predictor h (x) as a substitute for a reserve function is a poor choice, even if its average error is low. For example, a function h is just as likely to be many times too high to have a very small error when predicting a multiple, but to have 0 revenue in half the cases.A solution at the other end of the spectrum would simply remember the optimal prices from the sample S by setting r (xi) = bi. While this leads to optimal empirical revenue, a function class G containing r would be satisfactory."}, {"heading": "4.1 Algorithm Description", "text": "In this section we give an overview of the algorithm h h = 1 h = 1 h = 1 h = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 h = 1 h = 1 h = 1 h = 1 = 1 h = 1 h = 1 = 1 h = 1 h = 1 = 1 h = 1 h = 1 h = 1 h = 1 h = 1 = 1 h = 1 h = 1 h = 1 = 1 h = 1 = 1 h = 1 h = 1 = 1 h = 1 = 1 h = 1 = 1 = 1 h = 1 = 1 = 1 h = 1 = 1 = 1 h = 1 = 1 = 1 = 1 h = 1 = 1 = 1 = 1 = 1 h = 1 = 1 = 1 = 1 = 1 h = 1 = 1 = 1 = 1 = 1 = 1 = 1 h = 1 = 1 = 1 = 1 = 1 = 1 h = 1 = 1 = 1 = 1 = 1 = 1 = 1 h = 1 = 1 = 1 = 1 = 1 = 1 = 1 h = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 h = 1 = 1 = 1 = 1 = 1 = 1 h = 1 = 1 = 1 = 1 = 1 = 1 h = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 h = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 h = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 h = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 ="}, {"heading": "5 Bounding Separation", "text": "This limit refers to the variance of the supply distribution and the maximum revenues that can be obtained. It formally shows what leads a distribution to a certain value, then the setting of a minimum price does not lead to a separation. On the other hand, we consider the same income distribution, where F (x) = 1 \u2212 1 / x. Here, each variance of the offer price leads to revenues of 1, that is, the distribution has an unrestricted mass to a value of 1, so it is not too surprising that more revenues cannot be extracted. We show that after setting the optimal reserve price, the separation can be limited by a function of the variance of the distribution."}, {"heading": "5.1 Approximating Maximum Revenue", "text": "Goldberg et al. [2001] have shown in their groundbreaking work that an auctioneer who determines the optimal monopoly reserve, given a value distribution F to [1, M] with the mean B, would at least adjust the revenue from theorem 4 to refine this approximation ratio depending on the deviation from theorem 5. For each distribution F with the mean B and the deviation \u03c32, the maximum turnover from monopoly reserves, R, is satisfactory: BR \u2264 4.78 + 2 protocol (1 + \u03c3 2 B2) Proof. Let it happen = BR. Note that we begin by dividing both sides of the explanation of 4 by R 2: \u03c32 B2 + \u03b12 \u2212 2e\u03b1 \u2212 2e\u03b1 \u2212 1 \u2212 2e\u03b1 - 1e\u03b1 \u2212 1e."}, {"heading": "5.2 Partition of X", "text": "The sequence 2 suggests grouping the points so that the variance of the bids in each cluster is minimized. In view of a division {C1,.., Ck} of X, we denote them by mj = | SX-Cj |, B-J = 1mj-Cj-i: xi-Cj bi, s = 1 mj-i: xi-Cj (bi-B-J) 2. Let us also leave rj = argmaxp > 0 p-0 p | {bi > p-xi-Cj} | and R-J = rj-i | {bi > rj-xi-Cj} |. Let us leave r (x) = 1 rj1x-Cj then S- (3B-m2) 1 / 3 (k-J = 1mj-J) 2 / 3."}, {"heading": "6 Clustering Algorithm", "text": "With regard to Lemma 3 and since the quantity B = quantity B = quantity B = quantity B = quantity B = quantity C = quantity C = quantity C = quantity C = quantity C = quantity C = quantity C = quantity C = quantity C = quantity C = quantity C = quantity C = quantity C = quantity B = quantity B = quantity C = quantity B (quantity C) = quantity B (quantity C). However, to assign a new point X to a cluster, we would need access the offer b (quantity X), which is unknown in the evaluation period. Instead, we show how we can use the predictions of h (quantity C) to define an almost optimal clustering of X."}, {"heading": "7 Experiments", "text": "We compare the performance of our algorithms with the following basic parameters: 1. The offset algorithms we have presented in Section 3 are both unable to use the theoretical offset function; 2. The DC algorithms introduced by Mohri and Medina represent the state of the art in collecting revenue and optimize empirical revenue; 2. The DC algorithms introduced by Mohri and Medina are all able to optimize empirical revenue and optimize empirical revenue; 3. The tax revenue appropriations are getting even better; 3. We start by conducting experiments on synthetic data to demonstrate the respective algorithms; and we generate feature vectors xi-R10 with the coordinates derived from a mix of lognormal distributions using the means of 1 = 0, 2 = 1, 2 = 0.5 and mixing parameters p = 0.5."}, {"heading": "8 Conclusion", "text": "We provided a simple, scalable reduction of the problem of revenue optimization with ancillary information to the well-studied problem of minimizing the square loss. Our reduction provides the first polynomial time range with a quantifiable limit on the revenue achieved. In analyzing our algorithm, we also provided the first variance, the lower limit dependent on the revenue generated by setting optimal monopoly prices. Finally, we provided extensive empirical evidence of the advantages of RIC-h over the current state of the art."}, {"heading": "9 Acknowledgements", "text": "We thank Eric Balkanski, Renato Paes Leme and Martin Pa \ufffd l for helpful discussions and comments on earlier drafts of this work and the anonymous reviewer who pointed out the simple algorithm in Section 3."}, {"heading": "A Additional proofs", "text": "Lemma 2. The expected bid and the second moments of each distribution F are given by: B = 1 0 x (q) dq and s2 = 1 0 x (q) 2dq. Proof. We show the result only for the mean as proof for the second moment. It is well known that for a positive random variable the mean can be expressed as: B = 1 0 x (q) dx = 1 x (x) 0 dqdx = 1 x (x) 0 dqdx = 2 dqdx.where D = (x, q) | x > 0 and q \u2264 G (x)}. Let us leave D = (x, q) | 0 \u2264 q \u2264 1 and x \u2264 x (q)}. It is immediate that D \u00b2 D \u00b2 as q \u2264 G (x) implies by definition that x \u2264 x (q) and D \u00b2 (q) that we can have the complete expression (x)."}, {"heading": "B Dynamic Program", "text": "iiii. ii. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i."}, {"heading": "C Lower Bounds", "text": "Lemma 6. For each > 0 there is a distribution F, so that B \u2212 R \u2265 (3B) 1 / 3\u03c32 / 3 \u2212. Proof. Leave R = 1 and consider a distribution G given by G (x) = 1 for x < 1 and G (x) = 1 for x [R, M]. Then we have that the optimal turnover is 1 and the mean of this distribution isB = 0 G (x) = 1 + log.On the other hand, the second moment of the distribution is given by 2 x M 0 xG (x) = 1 + 2 (1 \u2212 1 M).Therefore, the variance of the distribution results from: \u03c32 = 2 (1 \u2212 1 M \u2212 logM \u2212 log2M2) Based on the fact that 1M = e \u2212 logM and using the Taylor expansion of this term, we see that the variance is approximately 3M 3 + o (log3M) as M \u2192 1. Since B \u2212 R = logM exists, it follows that for each 0 > M = 1 / 3\u043c (R)."}, {"heading": "D Complexity Bounds", "text": "Theorem 3. The growth function of the class G (h, k) can be limited as follows: (G (h, k), m) \u2264 m 2kkkk. Proof. Let S \u2032 = ((x1, z1),.., (xm, zm))) designate a sample. Let G = {(character (g (x1) \u2212 z1),.., character (g (xm) \u2212 zm))) | g \u00b2 G (h, k)}. We will move on to the cardinality of G. Note that a partition t \u00b2 Tk can divide the set of predictions h (x1),..., h (xm) into at most m \u00b2 k \u2212 1 different ways. In fact, this is immediate, since a k partition of [0, 1] by k \u2212 1 points t1,., tk \u2212 1 and each ti can be placed at least m \u00b2 different places."}], "references": [{"title": "Regret minimization for reserve prices in secondprice auctions", "author": ["Nicol\u00f2 Cesa-Bianchi", "Claudio Gentile", "Yishay Mansour"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2015}, {"title": "Algorithmic pricing via virtual valuations", "author": ["Shuchi Chawla", "Jason D. Hartline", "Robert D. Kleinberg"], "venue": "In Proceedings 8th ACM Conference on Electronic Commerce (EC-2007),", "citeRegEx": "Chawla et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Chawla et al\\.", "year": 2007}, {"title": "The sample complexity of revenue maximization", "author": ["Richard Cole", "Tim Roughgarden"], "venue": "CoRR, abs/1502.00963,", "citeRegEx": "Cole and Roughgarden.,? \\Q2015\\E", "shortCiteRegEx": "Cole and Roughgarden.", "year": 2015}, {"title": "Bid landscape forecasting in online ad exchange marketplace", "author": ["Ying Cui", "Ruofei Zhang", "Wei Li", "Jianchang Mao"], "venue": "In Proceedings of SIGKDD,", "citeRegEx": "Cui et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2011}, {"title": "The sample complexity of auctions with side information", "author": ["Nikhil R. Devanur", "Zhiyi Huang", "Christos-Alexandros Psomas"], "venue": "In Proceedings of STOC,", "citeRegEx": "Devanur et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Devanur et al\\.", "year": 2016}, {"title": "Revenue maximization with a single sample", "author": ["Peerapong Dhangwatnotai", "Tim Roughgarden", "Qiqi Yan"], "venue": "Games and Economic Behavior,", "citeRegEx": "Dhangwatnotai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dhangwatnotai et al\\.", "year": 2015}, {"title": "Competitive auctions and digital goods", "author": ["Andrew V. Goldberg", "Jason D. Hartline", "Andrew Wright"], "venue": "In Proceedings of the Twelfth Annual Symposium on Discrete Algorithms, January", "citeRegEx": "Goldberg et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2001}, {"title": "Simple versus optimal mechanisms", "author": ["Jason D. Hartline", "Tim Roughgarden"], "venue": "In Proceedings 10th ACM Conference on Electronic Commerce (EC-2009),", "citeRegEx": "Hartline and Roughgarden.,? \\Q2009\\E", "shortCiteRegEx": "Hartline and Roughgarden.", "year": 2009}, {"title": "The value of knowing a demand curve: Bounds on regret for online posted-price auctions", "author": ["Robert D. Kleinberg", "Frank Thomson Leighton"], "venue": "In Proceedings of FOCS,", "citeRegEx": "Kleinberg and Leighton.,? \\Q2003\\E", "shortCiteRegEx": "Kleinberg and Leighton.", "year": 2003}, {"title": "A field guide to personalized reserve prices", "author": ["Renato Paes Leme", "Martin P\u00e1l", "Sergei Vassilvitskii"], "venue": "In Proceedings of the 25th International Conference on World Wide Web, WWW 2016,", "citeRegEx": "Leme et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Leme et al\\.", "year": 2016}, {"title": "Learning theory and algorithms for revenue optimization in second-price auctions with reserve", "author": ["Mehryar Mohri", "Andres Mu\u00f1oz Medina"], "venue": "In Proceedings of ICML,", "citeRegEx": "Mohri and Medina.,? \\Q2014\\E", "shortCiteRegEx": "Mohri and Medina.", "year": 2014}, {"title": "Foundations of Machine Learning", "author": ["Mehryar Mohri", "Afshin Rostamizadeh", "Ameet Talwalkar"], "venue": "ISBN 026201825X,", "citeRegEx": "Mohri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohri et al\\.", "year": 2012}, {"title": "On the pseudo-dimension of nearly optimal auctions", "author": ["Jamie Morgenstern", "Tim Roughgarden"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Morgenstern and Roughgarden.,? \\Q2015\\E", "shortCiteRegEx": "Morgenstern and Roughgarden.", "year": 2015}, {"title": "Learning simple auctions", "author": ["Jamie Morgenstern", "Tim Roughgarden"], "venue": "In Proceedings ofCOLT,", "citeRegEx": "Morgenstern and Roughgarden.,? \\Q2016\\E", "shortCiteRegEx": "Morgenstern and Roughgarden.", "year": 2016}, {"title": "Optimal auction design", "author": ["R. Myerson"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Myerson.,? \\Q1981\\E", "shortCiteRegEx": "Myerson.", "year": 1981}, {"title": "Minimizing regret with multiple reserves", "author": ["Tim Roughgarden", "Joshua R. Wang"], "venue": "In Proceedings of the 2016 ACM Conference on Economics and Computation, EC \u201916,", "citeRegEx": "Roughgarden and Wang.,? \\Q2016\\E", "shortCiteRegEx": "Roughgarden and Wang.", "year": 2016}, {"title": "Objective variables for probabilistic revenue maximization in second-price auctions with reserve", "author": ["Maja R. Rudolph", "Joseph G. Ellis", "David M. Blei"], "venue": "In Proceedings of WWW", "citeRegEx": "Rudolph et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rudolph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "The celebrated work by Myerson [1981] shows how to optimally set reserves in second price auctions, provided the value distribution of each bidder is known.", "startOffset": 23, "endOffset": 38}, {"referenceID": 1, "context": "1 Related Work Optimizing revenue in auctions has been a rich area of study, beginning with the seminal work of Myerson [1981] who introduced optimal auction design.", "startOffset": 112, "endOffset": 127}, {"referenceID": 0, "context": "Follow up work by Chawla et al. [2007] and Hartline and Roughgarden [2009], among others, refined his results to increasingly more complex settings, taking into account multiple items, diverse demand functions, and weaker assumptions on the shape of the value distributions.", "startOffset": 18, "endOffset": 39}, {"referenceID": 0, "context": "Follow up work by Chawla et al. [2007] and Hartline and Roughgarden [2009], among others, refined his results to increasingly more complex settings, taking into account multiple items, diverse demand functions, and weaker assumptions on the shape of the value distributions.", "startOffset": 18, "endOffset": 75}, {"referenceID": 0, "context": "Follow up work by Chawla et al. [2007] and Hartline and Roughgarden [2009], among others, refined his results to increasingly more complex settings, taking into account multiple items, diverse demand functions, and weaker assumptions on the shape of the value distributions. Most of the classical literature on revenue optimization focuses on the design of optimal auctions when the bidding distribution of buyers is known. More recent work has considered the computational and information theoretic challenges in learning optimal auctions from data. A long line of work [Cole and Roughgarden, 2015, Devanur et al., 2016, Dhangwatnotai et al., 2015, Morgenstern and Roughgarden, 2015, 2016] analyzes the sample complexity of designing optimal auctions. The main contribution of this direction is to show that under fairly general bidding scenarios, a near-optimal auction can be designed knowing only a polynomial number of samples from bidders\u2019 valuations. Other authors, [Leme et al., 2016, Roughgarden and Wang, 2016] have focused on the computational complexity of finding optimal reserve prices from samples, showing that even for simple mechanisms the problem is often NP-hard to solve directly. Another well studied approach to data-driven revenue optimization is that of online learning. Here, auctions occur one at a time, and the learning algorithm must compute prices as a function of the history of the algorithm. These algorithms generally make no distributional assumptions and measure their performance in terms of regret: the difference between the algorithm\u2019s performance and the performance of the best fixed reserve price in hindsight. Kleinberg and Leighton [2003] developed an online revenue optimization algorithm for posted-price auctions that achieves low regret.", "startOffset": 18, "endOffset": 1685}, {"referenceID": 0, "context": "Their work was later extended to second-price auctions by Cesa-Bianchi et al. [2015]. A natural approach in both of these settings is to attempt to predict an optimal reserve price, equivalently the highest bid submitted by any of the buyers.", "startOffset": 58, "endOffset": 85}, {"referenceID": 0, "context": "Their work was later extended to second-price auctions by Cesa-Bianchi et al. [2015]. A natural approach in both of these settings is to attempt to predict an optimal reserve price, equivalently the highest bid submitted by any of the buyers. While the problem of learning this reserve price is well understood for the simplistic model of buyers with i.i.d. valuations [Cesa-Bianchi et al., 2015, Devanur et al., 2016, Kleinberg and Leighton, 2003], the problem becomes much more challenging in practice, when the valuations of a buyer also depend on features associated with the ad opportunity (for instance user demographics, and publisher information). This problem is not nearly as well understood as its i.i.d. counterpart. Mohri and Medina [2014] provide learning guarantees and an algorithm based on DC programming to optimize revenue in second-price auctions with reserve.", "startOffset": 58, "endOffset": 753}, {"referenceID": 0, "context": "Their work was later extended to second-price auctions by Cesa-Bianchi et al. [2015]. A natural approach in both of these settings is to attempt to predict an optimal reserve price, equivalently the highest bid submitted by any of the buyers. While the problem of learning this reserve price is well understood for the simplistic model of buyers with i.i.d. valuations [Cesa-Bianchi et al., 2015, Devanur et al., 2016, Kleinberg and Leighton, 2003], the problem becomes much more challenging in practice, when the valuations of a buyer also depend on features associated with the ad opportunity (for instance user demographics, and publisher information). This problem is not nearly as well understood as its i.i.d. counterpart. Mohri and Medina [2014] provide learning guarantees and an algorithm based on DC programming to optimize revenue in second-price auctions with reserve. The proposed algorithm, however, does not easily scale to large auction datasets as each iteration involves solving a convex optimization problem. A smoother version of this algorithm is given by Rudolph et al. [2016]. However, being a highly non-convex problem, neither algorithm provides a guarantee on the revenue attainable by the algorithm\u2019s output.", "startOffset": 58, "endOffset": 1099}, {"referenceID": 0, "context": "Their work was later extended to second-price auctions by Cesa-Bianchi et al. [2015]. A natural approach in both of these settings is to attempt to predict an optimal reserve price, equivalently the highest bid submitted by any of the buyers. While the problem of learning this reserve price is well understood for the simplistic model of buyers with i.i.d. valuations [Cesa-Bianchi et al., 2015, Devanur et al., 2016, Kleinberg and Leighton, 2003], the problem becomes much more challenging in practice, when the valuations of a buyer also depend on features associated with the ad opportunity (for instance user demographics, and publisher information). This problem is not nearly as well understood as its i.i.d. counterpart. Mohri and Medina [2014] provide learning guarantees and an algorithm based on DC programming to optimize revenue in second-price auctions with reserve. The proposed algorithm, however, does not easily scale to large auction datasets as each iteration involves solving a convex optimization problem. A smoother version of this algorithm is given by Rudolph et al. [2016]. However, being a highly non-convex problem, neither algorithm provides a guarantee on the revenue attainable by the algorithm\u2019s output. Devanur et al. [2016] give sample complexity bounds on the design of optimal auctions with side information.", "startOffset": 58, "endOffset": 1258}, {"referenceID": 0, "context": "Their work was later extended to second-price auctions by Cesa-Bianchi et al. [2015]. A natural approach in both of these settings is to attempt to predict an optimal reserve price, equivalently the highest bid submitted by any of the buyers. While the problem of learning this reserve price is well understood for the simplistic model of buyers with i.i.d. valuations [Cesa-Bianchi et al., 2015, Devanur et al., 2016, Kleinberg and Leighton, 2003], the problem becomes much more challenging in practice, when the valuations of a buyer also depend on features associated with the ad opportunity (for instance user demographics, and publisher information). This problem is not nearly as well understood as its i.i.d. counterpart. Mohri and Medina [2014] provide learning guarantees and an algorithm based on DC programming to optimize revenue in second-price auctions with reserve. The proposed algorithm, however, does not easily scale to large auction datasets as each iteration involves solving a convex optimization problem. A smoother version of this algorithm is given by Rudolph et al. [2016]. However, being a highly non-convex problem, neither algorithm provides a guarantee on the revenue attainable by the algorithm\u2019s output. Devanur et al. [2016] give sample complexity bounds on the design of optimal auctions with side information. However, the authors consider only cases where this side information is given by \u03c3 \u2208 [0, 1], thus limiting the applicability of these results\u2014online advertising auctions are normally parameterized by a large set of features. Finally, Cui et al. [2011] proposes partitioning data into clusters and solving the i.", "startOffset": 58, "endOffset": 1597}, {"referenceID": 0, "context": "Their work was later extended to second-price auctions by Cesa-Bianchi et al. [2015]. A natural approach in both of these settings is to attempt to predict an optimal reserve price, equivalently the highest bid submitted by any of the buyers. While the problem of learning this reserve price is well understood for the simplistic model of buyers with i.i.d. valuations [Cesa-Bianchi et al., 2015, Devanur et al., 2016, Kleinberg and Leighton, 2003], the problem becomes much more challenging in practice, when the valuations of a buyer also depend on features associated with the ad opportunity (for instance user demographics, and publisher information). This problem is not nearly as well understood as its i.i.d. counterpart. Mohri and Medina [2014] provide learning guarantees and an algorithm based on DC programming to optimize revenue in second-price auctions with reserve. The proposed algorithm, however, does not easily scale to large auction datasets as each iteration involves solving a convex optimization problem. A smoother version of this algorithm is given by Rudolph et al. [2016]. However, being a highly non-convex problem, neither algorithm provides a guarantee on the revenue attainable by the algorithm\u2019s output. Devanur et al. [2016] give sample complexity bounds on the design of optimal auctions with side information. However, the authors consider only cases where this side information is given by \u03c3 \u2208 [0, 1], thus limiting the applicability of these results\u2014online advertising auctions are normally parameterized by a large set of features. Finally, Cui et al. [2011] proposes partitioning data into clusters and solving the i.i.d. problem for each cluster. The crucial choice of a partition, however, is heuristic and thus provides no guarantees on the achievable revenue. Our results. We show that given a predictor of the bid with squared loss of \u03b7, we can construct a reserve function r that extracts all but g(\u03b7) revenue, for a simple increasing function g. (See Theorem 2 for the exact statement.) To the best of our knowledge, this is the first result that ties the revenue one can achieve directly to the quality of a standard prediction task. Our algorithm for computing r is scalable, practical, and efficient. Along the way we show what kinds of distributions are amenable to revenue optimization via reserve prices. We prove that when bids are drawn i.i.d. from a distribution F , the ratio between the mean bid and the revenue extracted with the optimum monopoly reserve scales as O(logVar(F ))\u2014Theorem 5. This result refines the log h bound derived by Goldberg et al. [2001], and formalizes the intuition that reserve prices are more successful for low variance distributions.", "startOffset": 58, "endOffset": 2618}, {"referenceID": 10, "context": "The following theorem is an adaptation of Theorem 1 of Mohri and Medina [2014] to our particular setup.", "startOffset": 55, "endOffset": 79}, {"referenceID": 12, "context": "The proof is similar to that in Morgenstern and Roughgarden [2015]; we include it in Appendix D for completness.", "startOffset": 32, "endOffset": 67}, {"referenceID": 6, "context": "1 Approximating Maximum Revenue In their seminal work Goldberg et al. [2001] showed that when faced with a bidder drawing values distribution F on [1,M ] with mean B, an auctioneer setting the optimum monopoly reserve would recover at least \u03a9(B/ logM) revenue.", "startOffset": 54, "endOffset": 77}, {"referenceID": 10, "context": "Following the suggestions in [Mohri and Medina, 2014] we chose \u03b3 \u2208 {0.", "startOffset": 29, "endOffset": 53}, {"referenceID": 10, "context": "It is not surprising to see that in the linear scenario, the DC algorithm of [Mohri and Medina, 2014] and the offset algorithm outperform RIC-h under low noise conditions.", "startOffset": 77, "endOffset": 101}, {"referenceID": 10, "context": "The DC algorithm introduced by Mohri and Medina [2014], which represents the state of the art in learning a revenue optimal reserve price and optimizes the empirical \u03b3-Lipschitz approximation to the revenue function.", "startOffset": 31, "endOffset": 55}], "year": 2017, "abstractText": "In the context of advertising auctions, finding good reserve prices is a notoriously challenging learning problem. This is due to the heterogeneity of ad opportunity types and the non-convexity of the objective function. In this work, we show how to reduce reserve price optimization to the standard setting of prediction under squared loss, a well understood problem in the learning community. We further bound the gap between the expected bid and revenue in terms of the average loss of the predictor. This is the first result that formally relates the revenue gained to the quality of a standard machine learned model.", "creator": "LaTeX with hyperref package"}}}