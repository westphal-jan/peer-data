{"id": "1706.04892", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "Second-Order Kernel Online Convex Optimization with Adaptive Sketching", "abstract": "Kernel online convex optimization (KOCO) is a framework combining the expressiveness of non-parametric kernel models with the regret guarantees of online learning. First-order KOCO methods such as functional gradient descent require only $\\mathcal{O}(t)$ time and space per iteration, and, when the only information on the losses is their convexity, achieve a minimax optimal $\\mathcal{O}(\\sqrt{T})$ regret. Nonetheless, many common losses in kernel problems, such as squared loss, logistic loss, and squared hinge loss posses stronger curvature that can be exploited. In this case, second-order KOCO methods achieve $\\mathcal{O}(\\log(\\text{Det}(\\boldsymbol{K})))$ regret, which we show scales as $\\mathcal{O}(d_{\\text{eff}}\\log T)$, where $d_{\\text{eff}}$ is the effective dimension of the problem and is usually much smaller than $\\mathcal{O}(\\sqrt{T})$. The main drawback of second-order methods is their much higher $\\mathcal{O}(t^2)$ space and time complexity. In this paper, we introduce kernel online Newton step (KONS), a new second-order KOCO method that also achieves $\\mathcal{O}(d_{\\text{eff}}\\log T)$ regret. To address the computational complexity of second-order methods, we introduce a new matrix sketching algorithm for the kernel matrix $\\boldsymbol{K}_t$, and show that for a chosen parameter $\\gamma \\leq 1$ our Sketched-KONS reduces the space and time complexity by a factor of $\\gamma^2$ to $\\mathcal{O}(t^2\\gamma^2)$ space and time per iteration, while incurring only $1/\\gamma$ times more regret.", "histories": [["v1", "Thu, 15 Jun 2017 14:33:08 GMT  (54kb)", "http://arxiv.org/abs/1706.04892v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["daniele calandriello", "alessandro lazaric", "michal valko"], "accepted": true, "id": "1706.04892"}, "pdf": {"name": "1706.04892.pdf", "metadata": {"source": "META", "title": "Second-Order Kernel Online Convex Optimization with Adaptive Sketching", "authors": ["Daniele Calandriello", "Alessandro Lazaric", "Michal Valko"], "emails": ["<daniele.calandriello@inria.fr>."], "sections": [{"heading": null, "text": "ar Xiv: 170 6.04 892v 1 [stat.ML] 1 5Ju n20 17framework combines the expressivity of non-parametric kernel models with the regret guarantees of online learning. First-order KOCO methods such as functional gradient descent require only O (t) time and space per iteration, and if the only information about the losses is their connotation, they achieve a minimized optimum O (Det (K)) regret. Nevertheless, many frequent losses in kernel problems such as square loss, logistic loss and square hinge loss show stronger curvatures that can be exploited. In this case, second-order KOCO methods achieve an O (log (Det (K))) regret that we show as O (deff logT), where Deff is the effective dimension of the problem and the squared hinge loss in the newsletter is much smaller than the main KO (the second order)."}, {"heading": "1. Introduction", "text": "s goal is to1SequeL Team, INRIA Lille - Northern Europe. Correspondence to: Daniele Calandriello < daniele.calandriello @ inria.fr >.Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. (s) Copyright 2017 by the author (s).minimize the regret, defined as the difference between the losses of the predictions. (s)"}, {"heading": "2. Background", "text": "In this section we present linear algebra and the RKHS notation, and formally we present the OCO problem in an RKHS (Schoolhead & Smola, 2001). We use the uppercase letters A for matrices, lowercase letters a for vectors, lowercase letters a for scalars. We refer to them as [A] ij and [a] i the (i, j) element of a matrix and i-th element of a vector, respectively the identity matrix of the dimension T and byDiag (a).T, the diagonal matrix with the vector RT. We use eT, i to indicate the vector of the dimension T indicator for the element i. If the dimension of I and ei is clear, we point to the identity of the T."}, {"heading": "3. Kernelized Online Newton Step", "text": "The question is whether this can be a way of reducing the indebtedness. (2016) The forecast then reduces the indebtedness of the indebtedness. (2016) The forecast reduces the indebtedness of the indebtedness. (2016) The indebtedness of the indebtedness reduces the indebtedness. (2016) The indebtedness of the indebtedness reduces the indebtedness. (2016) The indebtedness of the indebtedness is decreasing. (2016) The indebtedness of the indebtedness is decreasing. (2016) The indebtedness of the indebtedness is decreasing. (2016) The indebtedness is decreasing. (2016) The indebtedness is decreasing. (2016) The indebtedness is decreasing. (2016) The indebtedness is decreasing. (2016) The indebtedness is decreasing. (2016) The indebtedness is decreasing. (2016) The indebtedness is increasing. (2016) The indebtedness is increasing. (2016) The indebtedness is increasing. (2016) The indebtedness is increasing. (2016) The indebtedness is increasing. (2016)."}, {"heading": "4. Kernel Online Row Sampling", "text": "Although KONS achieves low repentance, it can be considered a collection of the K matrix. (t2) It requires O (t2) space and O (t3) time, which quickly becomes unfeasible as t grows. (2016) To improve space and time efficiency, we replace Kt with a precise, low approximation K (Alg. 2). There are two main obstacles to overcome in adapting ORS: From an algorithmic perspective, we must find a calculable estimator for RLS, while from an analytical perspective, we must prove that our space and time complexity do not match the dimension of RLS (such as Cohen et al. 2016), which can be potentially inefficient."}, {"heading": "5. Sketched ONS", "text": "c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c"}, {"heading": "6. Discussion", "text": "Regret does not mean that the time we need for the calculation is too short. (t) Note: The time we need for the calculation can also be converted incrementally to O (t) time. (t) Note: The time we need for the calculation can also be converted incrementally to O (t) time. (t) Note: The time we need for the calculation is also incrementally to O (t) time. (t) Note: The time we need for the calculation is incrementally to O (t) time. (t) Note: The time we need for the calculation can also be converted incrementally to O (t) time. (t) Note: The time we need for the calculation is incrementally to O (t) time. (t) Note: The time we need for the calculation is incrementally to O (t) time."}, {"heading": "A. Preliminary results", "text": "We start with a generic linear algebra identity used in our work. Proposition 2: For each X-Rn \u00b7 m matrix and \u03b1 > 0, XXT (XXT + \u03b1I) \u2212 1 = X (XTX + \u03b1I) \u2212 1XTand (XXT + \u03b1I) \u2212 1 = 1\u03b1 \u03b1I (XXT + \u03b1I) \u2212 1 = 1\u03b1 (XXT \u2212 XXT + \u03b1I) (XXT + \u03b1I) \u2212 1 = 1\u03b1 (XXT + \u03b1I) \u2212 1 = X (XTX + \u03b1I) \u2212 1XT). Proposition 3: For each matrix or linear operator X \u2212 x, if a selection matrix S (XXT + \u03b1I) \u2212 1 / 2: 1 / 2 (XSSTXSSTXTXT) and the difference (XSSTXTXT)."}, {"heading": "B. Proofs for Section 3", "text": "The detection of em. 1. We start by applying the definition of ut + 1 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 gt = \u2212 bt = = property (which can always be done because, for \u03b1 > 0, At is invertible, ut + 1 = wt \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 gt = A \u2212 1t (Atwt \u2212 gt). Let us now focus on the last term and use the definition of At, Atwt \u2212 gt = At \u2212 1wt + reiggTtwt \u2212 gt = At \u2212 1ut \u2212 At \u2212 1rt + (certaintg T twt \u2212 1 / certaint). Let us consider the definition of At \u2212 1rt and use the assumption g t 6 = 0, At \u2212 1rt = h (fortuntAt \u2212 1A \u2212 1 t \u2212 1t \u2212 1t). Let us consider the definition of T twt."}, {"heading": "C. Proofs for Section 4", "text": "The proof for Thm. 2: We derive the proof for a generic K with its induced processes = 1 (xt) and Ct. (SKETCHED-KONS (Alg. 3) applies this proof to the reproduced world. Our goal is to prove that Alg. 2 produces exact and small dictionaries in all time steps. (T) Formally, a dictionary does not consider the reproduced world. (T) - 1 / 2Kt. (I \u2212 SsSTs). (I \u2212 SSTs). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T)."}, {"heading": "D. Proofs for Section 5", "text": "With this evidence, we become the linear algebra identity of Prop. 2. We begin with the refor mulation of b-mutti = [b-mutti] i. In particular, \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 1 (RTt \u2212 1RTT - 1) \u2212 \u2212 \u2212 (1RT1T) (RT1T) (RT1T) (RT1T) (RT1T) (RT1T) (RT1T) \u2212 \u2212 \u2212 \u2212 \u2212 \u2212. (RT1T) (RT1T) (RT1T) (RT1T) (RT1T) (RT1T)."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>Kernel online convex optimization (KOCO) is a<lb>framework combining the expressiveness of non-<lb>parametric kernel models with the regret guaran-<lb>tees of online learning. First-order KOCO meth-<lb>ods such as functional gradient descent require<lb>onlyO(t) time and space per iteration, and, when<lb>the only information on the losses is their con-<lb>vexity, achieve a minimax optimal O(<lb>\u221a<lb>T ) re-<lb>gret. Nonetheless, many common losses in ker-<lb>nel problems, such as squared loss, logistic loss,<lb>and squared hinge loss posses stronger curvature<lb>that can be exploited. In this case, second-order<lb>KOCOmethods achieveO(log(Det(K))) regret,<lb>which we show scales as<lb>O(deff logT ), where<lb>deff is the effective dimension of the problem and<lb>is usually much smaller than O(<lb>\u221a<lb>T ). The main<lb>drawback of second-order methods is their much<lb>higher O(t2) space and time complexity. In this<lb>paper, we introduce kernel online Newton step<lb>(KONS), a new second-order KOCOmethod that<lb>also<lb>achievesO(deff logT ) regret. To address the<lb>computational complexity of second-order meth-<lb>ods, we introduce a new matrix sketching algo-<lb>rithm for the kernel matrixKt, and show that for<lb>a chosen parameter \u03b3 \u2264 1 our Sketched-KONS<lb>reduces the space and time complexity by a fac-<lb>tor of \u03b3 toO(t2\u03b32) space and time per iteration,<lb>while incurring only 1/\u03b3 times more regret.", "creator": "LaTeX with hyperref package"}}}