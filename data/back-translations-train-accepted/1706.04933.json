{"id": "1706.04933", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "Multi-objective Bandits: Optimizing the Generalized Gini Index", "abstract": "We study the multi-armed bandit (MAB) problem where the agent receives a vectorial feedback that encodes many possibly competing objectives to be optimized. The goal of the agent is to find a policy, which can optimize these objectives simultaneously in a fair way. This multi-objective online optimization problem is formalized by using the Generalized Gini Index (GGI) aggregation function. We propose an online gradient descent algorithm which exploits the convexity of the GGI aggregation function, and controls the exploration in a careful way achieving a distribution-free regret $\\tilde{\\bigO} (T^{-1/2} )$ with high probability. We test our algorithm on synthetic data as well as on an electric battery control problem where the goal is to trade off the use of the different cells of a battery in order to balance their respective degradation rates.", "histories": [["v1", "Thu, 15 Jun 2017 15:43:21 GMT  (455kb,D)", "http://arxiv.org/abs/1706.04933v1", "13 pages, 3 figures, draft version of ICML'17 paper"]], "COMMENTS": "13 pages, 3 figures, draft version of ICML'17 paper", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["r\u00f3bert busa-fekete", "bal\u00e1zs sz\u00f6r\u00e9nyi", "paul weng", "shie mannor"], "accepted": true, "id": "1706.04933"}, "pdf": {"name": "1706.04933.pdf", "metadata": {"source": "META", "title": "Multi-objective Bandits: Optimizing the Generalized Gini Index", "authors": ["R\u00f3bert Busa-Fekete", "Bal\u00e1zs Sz\u00f6r\u00e9nyi", "Paul Weng", "Shie Mannor"], "emails": ["<paul@weng.fr>."], "sections": [{"heading": "1. Introduction", "text": "The Multi-Armed Bandit Dilemma (MAB) problem (or Bandit Problem) refers to an iterative regret decision problem in the United States in which an agent repeatedly selects amongK options that metaphorically correspond to pulling an ofK arm of a bandit machine. In each round, the agent receives a random reward or cost depending on the selected arm. The agent's goal is to optimize an evaluation method, such as the error rate (expected percentage of times a sub-optimal arm is played) or cumulative regret (difference between the sum of payments received and the (expected) payouts that could be achieved by selecting the best arm in each round). In the stochastic multi-bandit setup, payouts are assumed to obey fixed distributions that cannot change with the arms but do not change with time. In order to achieve the desired goal, the agent must engage the classic exploration."}, {"heading": "2. Formal setup", "text": "The multi-armed or K-armed bandit problem is assigned by real-valued random variables X1,..., XK or by K-weapons (which we simply use the numbers 1,.., K). In each time step t, the online learner selects one and receives a random sample of the corresponding distributions. These samples, which are called costs, are assumed to be independent of all previous actions and costs.1 The target of the learner can be defined in different ways, e.g. minimizing the sum of costs over time (Lai & Robbins, 1985; Examiner et al., 2002a). In the multi-objective multi-armed bandit (MO-MAB), the costs are not scalar real values, but real vectors. More specifically, a D-objective K-armed bandit problem (D-armed bandit problem, 1985; Examiner et al., 2002a). In the multi-objective bandit problem, it is absolutely not the multiobjective (MAB), but the multiobjective bandit problem."}, {"heading": "3. Multi-objective optimization", "text": "In order to complete the MO-MAB setting, we must introduce the concept of arm optimality. First, we introduce the Pareto dominance relationship, which is defined as follows: v, v. RD: v. \"D = 1,.., D, vd.\" O \"D: a set of D-dimension vectors. (3) The Pareto front of O, denotedO.\" D: a set of vectors such as: v. \"O.\" D: v. \"V.\" V. \"V.\" V. \"V.\" V. \"V.\" V. \"V.\" V. \"V.\" V. \"(3) In the case of multi-objective optimization, it is usually necessary to calculate the Pareto front or to search for a specific element of the Pareto front. V. V..V. V. V. V. V..V. V. (and even impossible to implement, depending on the size of the solution room. V.\" V. \"V.\" V. \"V.\" V. \"V.\" V. \"V. V. V. V. V. V. V. V. V."}, {"heading": "4. Generalized Gini Index", "text": "The general Gini index (GGI) (Weymark, 1981) is defined as 2A multivariate function f: RD \u2192 R is therefore considered monotonous (not decreasing) if for all fixed x, x. \"(note:\" X \"means that f (x) \u2264 f (x\"). (note: \"X\") follows for a cost vector x = (x1 \"). (note: xD):\" RD (x) = D. \"(note:\" X \"). (note:\" X \"). (note:\" X \"). (note:\" X \"). (note:\" X. \"). (note:\" D \"). (note:\" X. \") (note: (note:\" X. \"). (note:\" X. \") (note: (D.). (D). (note:\" D). (note: \"X.\" (\"). (note:\" X. \") (note: (D). (D). (D). (note: (\" X. \"). (note:\" D). (D). (note: \"(X). (D). (D). (note:\")."}, {"heading": "5. Optimal policy", "text": "In the individual objective case, the arms are compared on the basis of their averages, which trigger an overall order over the arms. In the ultra objective setting, we use the GGI criterion to compare the arms. We can calculate the GGI value of each arm k as Gw (\u00b5k), if its vector meaning \u00b5k is known. Then, an optimal arm k * minimizes the GGI value, i.e. k * argmin k * Gw (\u00b5k).However, in this work, we also consider mixed strategies, which can be defined as A = {\u03b1 RK | \u2211 K = 1 \u03b1k = 1 \u0445 0 \u03b1}, because they can achieve lower GGI values than any fixed arm (see Figure 1).A policy parameterized by \u03b1 selects arm k with probability. An optimal mixed policy can then be defined as follows:"}, {"heading": "6. Regret", "text": "After we have played T-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-"}, {"heading": "7. Learning algorithm based on OCO", "text": "In this section, we propose an online learning algorithm called MO-OGDE to optimize the regret defined in the previous section. Our method exploits the convexity of the GGI operator and formalizes the problem of policy search as a convex optimization problem on the Internet, which is solved by the Online Gradient Descent (OGD) (Zinkevich, 2003) algorithm with projection onto a gradually expanding truncated probability simplex. Subsequently, we will provide a regret analysis of our method. Due to the space limitation, the evidence is moved to the appendix. Algorithm 1 MO-OGDE (\u03b4) 1: Pull each arm once 2: Set\u03b1 (K + 1) = (1 / K, \u00b7 \u00b7, 1 / K) 3: for rounds t = K + 1, K + 2,."}, {"heading": "7.1. MO-OGDE", "text": "Our objective function, which must be minimized, can be considered a function of \u03b1, i.e., f (\u03b2) = Gw (\u00b5\u03b1), with the matrix\u00b5 = (\u00b51,.., \u00b5K) containing the mean of the arm distributions as its columns. Note that the convexity of GGI implies the convexity of f (\u03b1). Since we are playing with mixed strategies, the domain of our optimization problem can be calculated in terms of the K-dimensional probability simplex x x x x x x x x x x x x x x x x x x. The gradients of f (\u03b1), the gradients of f (\u03b1) and the gradients of G."}, {"heading": "7.2. Regret analysis", "text": "The technical difficulty in optimizing GGI in an online manner is that we generally do an estimate (Tk (t)) (\u03b1) \u2212 f (\u03b1) (= Gw (\u00b5) \u03b1) \u2212 Gw (\u00b5\u03b1)) \u2212 1 / 2, which, unless all arms are recorded a linear number of times, results in regret of the same magnitude, which is typically too large. Nevertheless, an optimal policy approach determines a convex combination of several arms in the shape of p (\u03b1), which is the optimal cost vector in terms of GGI, given the arm distribution at hand. Let us specify K = {k) k > 0}. Furthermore, weapons in [K]\\ None with an individual GGI lower than weapons in K do not necessarily participate in the optimal combination."}, {"heading": "7.3. Regret vs. pseudo-regret", "text": "Next, we have limited the difference of regret defined in (9) and the pseudo-regret defined in (10) upwards. To achieve this goal, we first have the difference of Tk (t) and \u2211 t = 1 \u03b1 (\u03c4) k. Claim 1. For each t = 1, 2,.. and each k = 1,... K applies that P [kt = k] = \u03b1k, that Tk (t) \u2212 \u03a3 t = 1 \u03b1 (\u03c4) k = 1 [1 (k.A. = k) \u2212 \u03b1 (\u0432) k] is a martyr's death. Proof: As P [kt = k] = \u03b1k, so also applies that Tk (t) \u2212 \u2211 t = 1 \u03b1 (\u03c4) k = 1 [1 (k.A. = k) \u2212 p. In addition, | 1 (k.A.) is the difference between the regret (k.A.) and the pseudo-regret (k.A.) a martyr's death. (k.A.) The difference between the pseudo-regret (k.A.) and the pseudo-regret (k.A.) is (k.A.)."}, {"heading": "8. Experiments", "text": "In order to test our algorithm, we performed two experiments: In the first, we generated synthetic data from multi-objective bandit instances with known parameters. In this way, we were able to calculate the pseudo-remorse (10) and thus investigate the empirical performance of the algorithms. In the second experiment, we run our algorithm on a complex multi-objective online optimization problem, namely a problem of battery control. Before presenting these experiments, we present another algorithm that will serve as a basis."}, {"heading": "8.1. A baseline method", "text": "In the previous section, we have introduced a gradient-based approach, in which the mean estimates are used to approximate the gradient of the objective function. Nevertheless, the optimal policy using the mean estimates can be directly approximated by solving the linear program according to (8). We use the same investigation as MO-OGDE, see line 6 of Algorithm 1. Specifically, the learner solves the following linear program in each time step t: minimize D \u2211 d = 1 w \u00b2 d \u00b2 d \u00b2 d \u00b2 j = 1 bj, d \u00b2 j = rd + bj, d \u00b2 K \u00b2 k = 1 p \u00b2 k, j \u00b2 j, d \u00b2 T1 = 1 \u03b1 \u00b2 p \u00b2 t / K bj, d \u00b2 0 \u00b2 j, d \u00b2 [D] Note that the solution of the learning program lies above that in relation to \u03b1 \u00b2 s, as opposed to \u03b2 \u00b2 K. We refer to this algorithm as MO-LP. Note that this approach is cost-efficient, as each program is always a means-linear solution."}, {"heading": "8.2. Synthetic Experiments", "text": "We created random multi-objective bandit instances in which each component of the multivariate cost distribution has independently random Bernoulli distributions; the number of armsK was set to {5, 20} and the dimension of the cost distribution taken from D {5, 10}; the weight vector w of GGI was set to wd = 1 / 2d \u2212 1. Since the parameters of the bandit instance are known, the regret defined in section 6 can be calculated; we executed the MOOGDE and MO-LP algorithms with 100 repetitions; the multi-objective bandit instance was regenerated after each run. the regrets of the two algorithms averaged over the repetitions are shown in Figure 2 & < the results show some general trends, while the instance of the bandit instance is slightly decreased after each run."}, {"heading": "8.3. Battery control task", "text": "Since the performance profile of battery cells, components of an electric battery, can vary due to small physical and production differences, efficient balancing of these cells is required to achieve better performance and longer battery life. We model this problem as MO-MAB, where the arms exhibit different cell control strategies and the goal is to balance multiple goals: charge state (SOC), temperature, and aging. More specifically, the learner views this problem as a GGI optimization problem. Results (averaging over 100 runs) are presented in Figure 3, where we evaluated MO-OGDE vs. MO-LP. The dashed green lines represent the regret of playing fixed GGI optimization problems, even though both an MDE policy and an MO policy are much more effective than an MDE policy."}, {"heading": "9. Related work", "text": "The single-objective MAB problem has been studied intensively in particular in recent years (Bubeck & Cesa-Bianchi, 2012), yet there is only a very limited amount of work related to the multi-objective setup. Drugan & Nowe \"(2013), to the best of our knowledge, is the first to consider the multi-objective multi-arm problem in a repentance optimization framework with a stochastic assumption. Their work consists in extending the UCB algorithm (Auer et al., 2002a) to handle multi-dimensional feedback vectors with the goal of determining all arms on the Pareto front. (2014) They investigated a sequential decision problem with vectorial feedback. In their setup, the agent is allowed to choose from a finite series of actions and then observes the vectorial feedback for each action, so it is a complete information setup unlike our setup."}, {"heading": "10. Conclusion and future work", "text": "Contrary to most of the approaches previously proposed in the MOMAB, we do not seek the Pareto front, but seek a fair solution, which is important, for example, when each target corresponds to the benefit of another agent. To encode fairness, we use the Generalized Gini Index (GGI), a well-known criterion developed in economics. To optimize this criterion, we proposed a gradient-based algorithm that exploits the convexity of the GGI. We evaluated our algorithm in two areas and achieved promising experimental results. In the literature (Ga'bor et al., 1998; Roijers et al., 2013) several multi-objective learning algorithms for amplification were proposed (Ga'bor et al., 1998; Roijers et al., 2013)."}, {"heading": "Acknowledgements", "text": "The authors thank Vikram Bhattacharjee and Orkun Karabasoglu for providing the battery model. This research was partially supported by the Seventh Framework Programme of the European Communities (FP7 / 2007-2013) under Funding Agreement 306638 (SUPREL)."}, {"heading": "A. Lemma 1", "text": "Let us first of all note that we have not partially (partially) proved that not partially (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially)) (partially) (partially) (partially) (partially)) (partially) (partially) (partially)) (partially) (partially) (partially)) (partially) (partially) (partially) (partially)) (partially) (partially) (partially) (partially) (partially)) (partially) (partially) (partially) (partially) (partially)) (partially) (partially)) (partially) (partially) (partially) (partially) (partially)) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially)) (partially) (partially) (partially) (partially) (partially) (partially) (partially) (partially)))) (partially) (partially) (partially) (partially) (partially) (partially) (partially)"}, {"heading": "B. O\u0303(T\u22121/2) convergence along the trajectory", "text": "Proposition 2: Probability at least 1 \u2212 2 (DT + 1) K = K (K = K = K = K = K = K = K (K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K (K = K = K). Proof: Since the Gini Index is L-Lipschitz (with L = K = K), we simply have to bind the difference By Tt = 1 (T) (T = 1 / K) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T)) (T)) (T))). K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K, so that the K (T) vectors are not independent."}, {"heading": "C. Proof of Proposition 3", "text": "To defuse the situation, we define the following technical problems as useful: Lemma 2nd letters e (f), argmina (f), argmina (f), argmina (f), argmina (f), argmina (f), and argmina (t), argmina (k), argmina (f), argmina (f), and argmina (t), and argmina (t), and argmina (t), and argmina (t), and argmina (t), and argmina (t), and argmina (t), and argmina (t). (f), and argmina (t), and argmina (t), and argmina (t), and argmina (t), and argmina (t), and argmina (t), and argmina (t), and argmina (t), and argmina (t), and argmina (t), and argmina (t), and argmina (t), and argmina (t), and argmina (t (t), and argmina (t), and argmina (t), and argmina (t (t), and argmina (t), and argmina (t (t), and argmina (t), and argmina (t (t), and argmina (t), and argmina (t). (t (t), and argmina (t), and argmina (t (t). (t), and argmina (t). (t (t). (t). (t)...... (...... (.................. (....................................."}, {"heading": "D. The proof of g\u2217 > 0", "text": "Lemon 3: The number of those who refer to the individual components is equal. The number of those who refer to the individual components is equal. (1) The number of those who refer to the individual components is equal. (2) The set of those who refer to the individual components is equal. (3) The set of those who refer to the individual components is equal. (4) The set of those who refer to the individual components is equal. (4) The set of components is equal. (4) The set of components is equal. (4) The set of components is equal. (4) The set of components is equal. (4) The set of components is equal. (4) The set of components is equal. (3) The set of components is equal. (4) The set of components is equal. (4) The set of components is equal. (4) The set of components is equal. (4) The set of components is equal. (4) The set of components is equal. (4) The set of components is equal."}, {"heading": "E. Proof of Theorem 1", "text": "Theorem 1: Probability at least 1: 1 \u2212 KD2 VVVVVVVV2: f (1 T = 1 T = 1 \u03b1 (t)) \u2212 f (T) \u2264 2L \u221a 6D ln3 (8DKT 2 / \u03b4) T, for each sufficiently large T, where L is the lipshitz constant of Gw (x). Proof: Firstly, we determine the following sum, which appears in the last term of Proposition 3, upwards: T \u2211 t = 1 1 \u221a 1 (t) = \u03c41 + \u221a 2 | ext (T) | a0T \u04451 + 1 \u221a 1 + 1 \u221a 2 | ext (T) | a0 [\u221a 1 + \u04451 \u041a1 \u041a1 + \u041a1 + \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 \u041a1 (T) | a0 (T) \u04321 \u041a1 + 1 \u221a 2 | a0 (T) V1 V1 VVVVVVVV2 2 2 2 2 2 2 V2 2 2 2 2 2 V2 2 2 2 2 V2 2 2 2 2 V2 2 2 2 2 V2 2 2 2 2 V2 2 2 2 2 V2 2 2 2 2 V2 2 2 V2 2 2 2 2 V2 2 2 2 V2 2 2 2 2 V2 2 V2 2 2 V2 2 2 2 V2 2 2 2 V2 2 2 2 V2 2 2 2 2 V2 2 2 2 2 V2 2 2 2 2 2 2 V2 2 2 2 2 2 2 2 2 2 V2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 V2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2"}, {"heading": "F. Regret vs. pseudo regret", "text": "Corollary 2: \"with probability at least 1 \u2212\" (T) \u2212 \"(T) \u2212\" (T) \u2212 \"(T)\" (T) \"(T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T (T) (T) (T) (T) (T (T) (T) (T) (T) (T (T) (T) (T (T) (T) (T) (T) (T) (T (T) (T) (T) (T (T) (T) (T (T) (T) (T) (T) (T) (T) (T) (T) (T (T) (T (T) (T) (T) (T) (T (T) (T) (T) (T) (T) (T (T) (T) (T) (T) (T) (T) (T (T) (T) (T) (T) (T) (T (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T)"}, {"heading": "G. Battery control task", "text": "This year, the time has come for an agreement to be reached, and it will only take a few days."}], "references": [{"title": "Blackwell approachability and no-regret learning are equivalent", "author": ["Abernethy", "Jacob D", "Bartlett", "Peter L", "Hazan", "Elad"], "venue": "In COLT 2011 - The 24th Annual Conference on Learning Theory, June 9-11,", "citeRegEx": "Abernethy et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2011}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Sequential decision making with vector outcomes", "author": ["Azar", "Yossi", "Feige", "Uriel", "Feldman", "Michal", "Tennenholtz", "Moshe"], "venue": "In ITCS, pp", "citeRegEx": "Azar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2014}, {"title": "Convex Optimization", "author": ["Boyd", "Stephen", "Vandenberghe", "Lieven"], "venue": null, "citeRegEx": "Boyd et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2004}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi", "year": 2012}, {"title": "When is a weighted average of ordered sample elements a maximum likelihood estimator of the location parameter", "author": ["Buczolich", "Zolt\u00e1n", "Sz\u00e9kely", "G\u00e1bor J"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Buczolich et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Buczolich et al\\.", "year": 1989}, {"title": "The measurement of inequality of incomes", "author": ["H. Dalton"], "venue": "Economic J.,", "citeRegEx": "Dalton,? \\Q1920\\E", "shortCiteRegEx": "Dalton", "year": 1920}, {"title": "Review on methods of state-of-charge estimation with viewpoint to the modern LiFePO4/Li4Ti5O12 lithium-ion systems", "author": ["J. Dambrowski"], "venue": "In International Telecommunication Energy Conference,", "citeRegEx": "Dambrowski,? \\Q2013\\E", "shortCiteRegEx": "Dambrowski", "year": 2013}, {"title": "Designing multi-objective multi-armed bandits algorithms: A study", "author": ["M.M. Drugan", "A. Now\u00e9"], "venue": "In IJCNN, pp", "citeRegEx": "Drugan and Now\u00e9,? \\Q2013\\E", "shortCiteRegEx": "Drugan and Now\u00e9", "year": 2013}, {"title": "Multi-criteria reinforcement learning", "author": ["G\u00e1bor", "Zolt\u00e1n", "Kalm\u00e1r", "Zsolt", "Szepesv\u00e1ri", "Csaba"], "venue": "In ICML, pp", "citeRegEx": "G\u00e1bor et al\\.,? \\Q1998\\E", "shortCiteRegEx": "G\u00e1bor et al\\.", "year": 1998}, {"title": "Dynamic lithium-ion battery model for system simulation", "author": ["Gao", "Lijun", "Chen", "Shenyi", "Dougal", "Roger A"], "venue": "IEEE Trans. on Components and Packaging Technologies,", "citeRegEx": "Gao et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2002}, {"title": "Introduction to Online", "author": ["Hazan", "Elad"], "venue": "Convex Optimization. NOW,", "citeRegEx": "Hazan and Elad.,? \\Q2016\\E", "shortCiteRegEx": "Hazan and Elad.", "year": 2016}, {"title": "Battery performance models in ADVISOR", "author": ["V.H. Johnson"], "venue": "Journal of Power Sources,", "citeRegEx": "Johnson,? \\Q2002\\E", "shortCiteRegEx": "Johnson", "year": 2002}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "Robbins", "Herbert"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Lai et al\\.", "year": 1985}, {"title": "Stochastic convex optimization with multiple objectives", "author": ["Mahdavi", "Mehrdad", "Yang", "Tianbao", "Jin", "Rong"], "venue": "In NIPS, pp", "citeRegEx": "Mahdavi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mahdavi et al\\.", "year": 2013}, {"title": "Online learning with sample path constraints", "author": ["Mannor", "Shie", "Tsitsiklis", "John N", "Yu", "Jia Yuan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mannor et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2009}, {"title": "Approachability in unknown games: Online learning meets multi-objective optimization", "author": ["Mannor", "Shie", "Perchet", "Vianney", "Stoltz", "Gilles"], "venue": "In Proceedings of The 27th Conference on Learning Theory, COLT", "citeRegEx": "Mannor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2014}, {"title": "On solving linear programs with the ordered weighted averaging objective", "author": ["W. Ogryczak", "T. Sliwinski"], "venue": "Eur. J. Operational Research,", "citeRegEx": "Ogryczak and Sliwinski,? \\Q2003\\E", "shortCiteRegEx": "Ogryczak and Sliwinski", "year": 2003}, {"title": "On minimizing ordered weighted regrets in multiobjective Markov decision processes", "author": ["W. Ogryczak", "P. Perny", "P. Weng"], "venue": "In ADT, Lecture Notes in Artificial Intelligence,", "citeRegEx": "Ogryczak et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ogryczak et al\\.", "year": 2011}, {"title": "A compromise programming approach to multiobjective Markov decision processes", "author": ["W. Ogryczak", "P. Perny", "P. Weng"], "venue": "International Journal of Information Technology & Decision Making,", "citeRegEx": "Ogryczak et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ogryczak et al\\.", "year": 2013}, {"title": "Bandit solutions provide unified ethical models for randomized clinical trials and comparative effectiveness research", "author": ["W.H. Press"], "venue": "PNAS, 106(52):22398\u201322392,", "citeRegEx": "Press,? \\Q2009\\E", "shortCiteRegEx": "Press", "year": 2009}, {"title": "A survey of multi-objective sequential decision-making", "author": ["Roijers", "Diederik M", "Vamplew", "Peter", "Whiteson", "Shimon", "Dazeley", "Richard"], "venue": "J. Artif. Intell. Res.,", "citeRegEx": "Roijers et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Roijers et al\\.", "year": 2013}, {"title": "Online learning and online convex optimization", "author": ["Shalev-Shwartz", "Shai"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz and Shai.,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz and Shai.", "year": 2012}, {"title": "Contextual bandits with similarity information", "author": ["A. Slivkins"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Slivkins,? \\Q2014\\E", "shortCiteRegEx": "Slivkins", "year": 2014}, {"title": "An interactive weighted Tchebycheff procedure for multiple objective programming", "author": ["R.E. Steuer", "Choo", "E.-U"], "venue": "Mathematical Programming,", "citeRegEx": "Steuer et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Steuer et al\\.", "year": 1983}, {"title": "Research on LiMn2O4 battery\u2019s state of charge estimation with the consideration of degradation", "author": ["Tao", "Gao"], "venue": "Technical report, Tsinghua University,", "citeRegEx": "Tao and Gao.,? \\Q2012\\E", "shortCiteRegEx": "Tao and Gao.", "year": 2012}, {"title": "Generalized gini inequality indices", "author": ["Weymark", "John A"], "venue": "Mathematical Social Sciences,", "citeRegEx": "Weymark and A.,? \\Q1981\\E", "shortCiteRegEx": "Weymark and A.", "year": 1981}, {"title": "On ordered weighted averaging aggregation operators in multi-criteria decision making", "author": ["R.R. Yager"], "venue": "IEEE Trans. on Syst., Man and Cyb.,", "citeRegEx": "Yager,? \\Q1988\\E", "shortCiteRegEx": "Yager", "year": 1988}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Zinkevich", "Martin"], "venue": "In ICML,", "citeRegEx": "Zinkevich and Martin.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich and Martin.", "year": 2003}], "referenceMentions": [{"referenceID": 24, "context": "The bandit setup has become the standard modeling framework for many practical applications, such as online advertisement (Slivkins, 2014) or medical treatment design (Press, 2009) to name a few.", "startOffset": 122, "endOffset": 138}, {"referenceID": 21, "context": "The bandit setup has become the standard modeling framework for many practical applications, such as online advertisement (Slivkins, 2014) or medical treatment design (Press, 2009) to name a few.", "startOffset": 167, "endOffset": 180}, {"referenceID": 22, "context": "Besides, there are several studies published recently that consider multi-objective sequential decision problem under uncertainty (Drugan & Now\u00e9, 2013; Roijers et al., 2013; Mahdavi et al., 2013).", "startOffset": 130, "endOffset": 195}, {"referenceID": 15, "context": "Besides, there are several studies published recently that consider multi-objective sequential decision problem under uncertainty (Drugan & Now\u00e9, 2013; Roijers et al., 2013; Mahdavi et al., 2013).", "startOffset": 130, "endOffset": 195}, {"referenceID": 28, "context": "Different aggregation function can be used depending on the problem at hand, such as sum, weighted sum, min, max, (augmented) weighted Chebyshev norm (Steuer & Choo, 1983), Ordered Weighted Averages (OWA) (Yager, 1988) or Ordered Weighted Regret (OWR) (Ogryczak et al.", "startOffset": 205, "endOffset": 218}, {"referenceID": 19, "context": "Different aggregation function can be used depending on the problem at hand, such as sum, weighted sum, min, max, (augmented) weighted Chebyshev norm (Steuer & Choo, 1983), Ordered Weighted Averages (OWA) (Yager, 1988) or Ordered Weighted Regret (OWR) (Ogryczak et al., 2011) and its weighted version (Ogryczak et al.", "startOffset": 252, "endOffset": 275}, {"referenceID": 20, "context": ", 2011) and its weighted version (Ogryczak et al., 2013).", "startOffset": 33, "endOffset": 56}, {"referenceID": 7, "context": "This means that GGI is strictly decreasing with Pigou-Dalton transfers and all the components of w\u2032 are positive. Based on formulation (5), Ogryczak & Sliwinski (2003) showed that the GGI value of a vector x can be obtained by solving a linear program.", "startOffset": 54, "endOffset": 168}, {"referenceID": 15, "context": "In the online convex optimization setup with multiple objectives (Mahdavi et al., 2013), the learner\u2019s forecast x is evaluated in terms of multiple convex loss functions f (t) 0 (x), f (t) 1 (x), .", "startOffset": 65, "endOffset": 87}, {"referenceID": 17, "context": "In the approachability problem (Mannor et al., 2014; 2009; Abernethy et al., 2011), there are two players, say A and B.", "startOffset": 31, "endOffset": 82}, {"referenceID": 0, "context": "In the approachability problem (Mannor et al., 2014; 2009; Abernethy et al., 2011), there are two players, say A and B.", "startOffset": 31, "endOffset": 82}, {"referenceID": 10, "context": "Several multi-objective reinforcement learning algorithm have been proposed in the literature (G\u00e1bor et al., 1998; Roijers et al., 2013).", "startOffset": 94, "endOffset": 136}, {"referenceID": 22, "context": "Several multi-objective reinforcement learning algorithm have been proposed in the literature (G\u00e1bor et al., 1998; Roijers et al., 2013).", "startOffset": 94, "endOffset": 136}, {"referenceID": 0, "context": "Their work consists of extending the UCB algorithm (Auer et al., 2002a) so as to be able to handle multi-dimensional feedback vectors with the goal of determining all arms on the Pareto front. Azar et al. (2014) investigated a sequential decision making problem with vectorial feedback.", "startOffset": 52, "endOffset": 212}, {"referenceID": 13, "context": "The battery is modeled using the internal resistance (Rint) model (Johnson, 2002).", "startOffset": 66, "endOffset": 81}, {"referenceID": 8, "context": "The estimation of SOC is based on the Ampere Hour Counting method (Dambrowski, 2013).", "startOffset": 66, "endOffset": 84}, {"referenceID": 11, "context": "The variation of temperature in the system is determined according to the dissipative heat loss due to the internal resistance and thermal convection (Gao et al., 2002).", "startOffset": 150, "endOffset": 168}], "year": 2017, "abstractText": "We study the multi-armed bandit (MAB) problem where the agent receives a vectorial feedback that encodes many possibly competing objectives to be optimized. The goal of the agent is to find a policy, which can optimize these objectives simultaneously in a fair way. This multi-objective online optimization problem is formalized by using the Generalized Gini Index (GGI) aggregation function. We propose an online gradient descent algorithm which exploits the convexity of the GGI aggregation function, and controls the exploration in a careful way achieving a distribution-free regret \u00d5(T\u22121/2) with high probability. We test our algorithm on synthetic data as well as on an electric battery control problem where the goal is to trade off the use of the different cells of a battery in order to balance their respective degradation rates.", "creator": "LaTeX with hyperref package"}}}