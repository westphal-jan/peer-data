{"id": "1706.04972", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Device Placement Optimization with Reinforcement Learning", "abstract": "The past few years have witnessed a growth in size and computational requirements for training and inference with neural networks. Currently, a common approach to address these requirements is to use a heterogeneous distributed environment with a mixture of hardware devices such as CPUs and GPUs. Importantly, the decision of placing parts of the neural models on devices is often made by human experts based on simple heuristics and intuitions. In this paper, we propose a method which learns to optimize device placement for TensorFlow computational graphs. Key to our method is the use of a sequence-to-sequence model to predict which subsets of operations in a TensorFlow graph should run on which of the available devices. The execution time of the predicted placements is then used as the reward signal to optimize the parameters of the sequence-to-sequence model. Our main result is that on Inception-V3 for ImageNet classification, and on RNN LSTM, for language modeling and neural machine translation, our model finds non-trivial device placements that outperform hand-crafted heuristics and traditional algorithmic methods.", "histories": [["v1", "Tue, 13 Jun 2017 16:26:40 GMT  (1613kb)", "http://arxiv.org/abs/1706.04972v1", "To appear at ICML 2017"], ["v2", "Sun, 25 Jun 2017 23:55:21 GMT  (1613kb)", "http://arxiv.org/abs/1706.04972v2", "To appear at ICML 2017"]], "COMMENTS": "To appear at ICML 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["azalia mirhoseini", "hieu pham", "quoc v le", "benoit steiner", "rasmus larsen", "yuefeng zhou", "naveen kumar", "mohammad norouzi", "samy bengio", "jeff dean"], "accepted": true, "id": "1706.04972"}, "pdf": {"name": "1706.04972.pdf", "metadata": {"source": "META", "title": "Device Placement Optimization with Reinforcement Learning", "authors": ["Azalia Mirhoseini", "Hieu Pham", "Benoit Steiner", "Rasmus Larsen", "Yuefeng Zhou", "Naveen Kumar", "Mohammad Norouzi", "Samy Bengio", "Jeff Dean"], "emails": ["lia@google.com>,", "<hyhieu@google.com>."], "sections": [{"heading": null, "text": "ar Xiv: 170 6.04 972v 1 [cs.L G] 13 Jun 2017size and computerational requirements for training and inference with neural networks. Currently, a common approach to addressing these requirements is to use a heterogeneous distributed environment with a mix of hardware devices such as CPUs and GPUs. Importantly, the decision to place parts of neural models on devices is often made by human experts based on simple heuristics and intuition. In this paper, we propose a method by which we learn to optimize device placement for TensorFlow computer graphics. Crucial to our method is the use of a sequence-to-equation model to predict which subsets of operations should be performed in a tensorflow diagram on which of the available devices. The execution time of the predicted placements will then be used as a reward signal for optimizing the parameters of the sequence sequence sequence sequence work to be applied to the main result of the Nalgorithm for classification and the Nalgorithm that is used to classify the Nalgorithm."}, {"heading": "1. Introduction", "text": "In recent years, neural networks have proven to be a general and effective tool for many practical problems, such as image classification (Krizhevsky et al., 2012; Szegedy et al., 2015; Er et al., 2016), speech recognition (Hinton et al., 2012; Graves & Jaitly, 2014; Hannun et al., 2014; Chan et al., 2015), machine translation (Sutskever et al., 2014; Cho et al., 2014; Equal Post 1Google 2Members of the Google Brain Residency Program (g.co / brainresidency) 3Google et al. Equivalent to: Azalia Mirhoseini et al., azalia @ google.com < hyhieu @ google.com >.Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017."}, {"heading": "2. Related Work", "text": "Our work is closely related to the idea of using neural networks and strengthening learning for combinatorial optimizations (Vinyals et al., 2015; Bello et al., 2016).The space of possible placements for a computer graph is discrete, and we model placements using a sequenceto-sequence approach that is trained with political processes. However, experiments in the early work were only concerned with toy datasets, while this work applies the framework to a large-scale practical application with noisy rewards. Reinforcement Learning has also been applied to optimize system performance. For example, Mao et al. (2016) suggests training a resource management algorithm with political gradients to optimize the expected value of a handmade reward, as opposed to this work where we directly optimize the runtime of the configurations, thus relieving the need to design intermedia cost modeling."}, {"heading": "3. Method", "text": "A placement P = {p1, p2,..., pM} is the assignment of an operation oi-G to a device pi, where pi-1,..., D}. Let r (P) specify the time it takes to perform a full execution of G under the placement P. The goal of optimizing the placement of devices is to find P so that the execution time r (P) is minimized."}, {"heading": "3.1. Training with Policy Gradients", "text": "While we try to minimize the execution time r (P), the direct optimization of r (P) leads to two important problems: First, at the beginning of the training process, due to the poor placements that are sampled, the measurements of the r (P) phenomenon can be loud, resulting in inappropriate learning signals. Second, as the RL model gradually approaches, the sampled placements become more similar to each other, resulting in small differences between the corresponding runtimes, resulting in less distinguishable learning signals. Empirically, we find that the square root of the runtime, R (P) = \u221a r (P), makes the learning process more robust, resulting in small differences between the corresponding runtimes, resulting in less distinguishable learning signals. Accordingly, we propose a stochastic pol-Figure 2. Device Placement Model Architecture (P | G; Celsius) to minimize the objective J model that we call the EP model."}, {"heading": "3.2. Architecture Details", "text": "We use a sequence-to-sequence model (Sutskever et al., 2014) with LSTM (Hochreiter & Schmidhuber, 1997) and a content-based attention mechanism (Bahdanau et al., 2015) to predict the placings. Figure 2 shows the overall architecture of our model, which can be divided into two parts: encoder RNN and decoder RNN. The input to the encoder RNN is the sequence of the operations of the input graph. We store the operations by concatenating their information. Specifically, for each input graph G we first capture the types of its operations. The type of an operation describes the underlying calculation, such as MatMul or Conve2d. For each type we store a tunable embedding vector. We then capture the size of the list of output locations for each operation and link them to a specified size, the encoding of the output code."}, {"heading": "3.3. Co-locating Operations", "text": "A key challenge in applying our method to TensorFlow computation diagrams is that these diagrams generally have thousands of operations (see Table 1). Modelling such a large number of operations using sequence-to-sequence models is difficult due to disappearing and exploding gradient problems (Pascanu et al., 2013) and large storage capacities. We propose to reduce the number of objects to be placed on different devices by manually forcing multiple operations to be on the same device. In practice, this is made easier by the colocate function of TensorFlow. We use multiple heuristics to form co-location groups. Specifically, we rely on TensorFlow's standard co-location groups, such as the joint localization of the results of each operation with their gradients. We continue to apply simple heuristics to merge further operations into co-location groups."}, {"heading": "3.4. Distributed Training", "text": "We accelerate the training process of our model with asynchronously distributed training as shown in Figure 3. Our framework consists of several controllers, each of which executes the current policy defined by the attention-oriented sequence-to-sequence model as described in Section 3.2. All controllers interact with a single common parameter server. We note that the parameter server only holds the parameters of the controllers, not the parameters of the input graph, as it can potentially create a latency bottleneck on the parameter server to transmit these parameters. Each controller in our framework interacts with the K workers, with K being the number of samples in equation. The training process has two alternating phases. In the first phase, each worker receives a signal indicating that he is waiting for placements in his controller while each controller receives a signal."}, {"heading": "4. Experiments", "text": "In the following experiments we apply our proposed method to assign calculations to devices on three major neural networks of the deep learning literature: Recurrent Neural Language Model (RNNLM) (Zaremba et al., 2014; Jozefowicz et al., 2016), Attentional Neural Machine Translation (Bahdanau et al., 2015) and InceptionV3 (Szegedy et al., 2016). We compare the RL placements with strong existing baselines described in Section 4.2."}, {"heading": "4.1. Experiment Setup", "text": "We evaluate our approach to three established, in-depth learning models: \u2022 Recurrent Neural Network Language Model (RNNLM) with multiple LSTM laymen (Zaremba et al., 2014; Jozefowicz et al., 2016). The network structure of this model represents an enormous potential for parallel execution, as each LSTM cell can be started similar to that of RNLM. Its large number of hidden states due to the source and target targets requires modelling. Both Sutskever et al. (2014) andWu et al. (2016) suggest placing each LSTM layer, the attention layer and the soft layer."}, {"heading": "4.2. Baselines", "text": "This placement executes the entire neural network on a single CPU. This placement executes the entire neural network on a single CPU. If an operation does not have a GPU implementation, it is placed on CPU.Scotch. We also estimate the computational cost of each operation as well as the amount of data flowing along each edge of the neural network model, and feed it to the Scottish static mapper (Pellegrini, 2009). We also annotate the architectural graph (see Section 2) with the computing and communication capacities of the underlying devices. MinCut. We use the same Scotch optimizer, but eliminate the CPU from the list of available devices fed into the optimizer. Similar to the single GPU placement, if an operation does not have a GPU layer, the GPU implementation is designed for the entire GPU layer, but eliminates the CPU from the list of available devices fed into the optimizer. Similar to the single GPU placement, if an operation does not have a GPU layer, the GPU implementation is designed for the entire GPU layer. For the processing of some large CUs on each PU, it remains unexecutable due to the single placement of the single GPU, although it is the singlebar."}, {"heading": "4.3. Single-Step Runtime Efficiency", "text": "In Table 2, we present the transit times per step of the placements found by our method and by the baselines. We observe that our model is either on par with or better than other methods of placements. Although we do not receive any information other than the transit times of the placements and the number of devices available, our model learns subtle trade-offs between increasing performance through parallelism and the costs caused by interdevice communications.RNNLM. Our method recognizes that it is possible to fit the entire RNLM graph into a GPU, and decides to do so in order to save the interdevice communication latences.The resulting placement is more than twice as fast as the best published human-designed baseline.Neural MT finds a non-trivial placement (see Figure 4), which results in an acceleration of up to 20.6% for 4 GPUs."}, {"heading": "4.4. End-to-End Runtime Efficiency", "text": "In fact, most of them will be able to establish themselves in the region without being able to establish themselves."}, {"heading": "4.5. Analysis of Found Placements", "text": "To understand the reasons for RL-based placements, we analyze their profile information and compare it with those of expert-designed placements.Neuronal MT. First, we compare the computational loads per device from RL-based placement and expert placement for the NMT model. Figure 8 shows such a performance profiling. RL-based placement balances the workload significantly better than expert-designed placement. Interestingly, if we do not consider the time for backpropagation, expert-designed placement makes sense because the workload is more balanced (though still less balanced than ours).The imbalance is much more significant when the backpropagation time.Inception-V3. However, in Inception-V3, RL-based placement does not attempt to balance the calculations between GPUs as shown in Figure 9-top."}, {"heading": "5. Conclusion", "text": "In this paper, we present an adaptive method for optimizing device placement for neural networks. Key to our approach is the use of a sequence-to-sequence model to suggest device placement in light of operations in a neural network.The model is designed to optimize the execution time of the neural network.In addition to execution time, the number of devices available is the only additional information about the hardware configuration that we feed into our model.Our results show that the proposed approach learns the properties of the environment, including the complex trade-off between computation and communication in hardware. In a number of tasks such as image classification, language modeling, and machine translation, our method outperforms the placements carefully designed by human experts and highly optimized algorithmic solvers."}, {"heading": "Acknowledgements", "text": "We would like to thank Martin Abadi, Stephan Gouws and the Google Brain team for their help with the project."}], "references": [{"title": "Tensorflow: A system for large-scale machine learning", "author": ["Tucker", "Paul", "Vasudevan", "Vijay", "Warden", "Pete", "Wicke", "Martin", "Yu", "Yuan", "Zheng", "Xiaoqiang"], "venue": "arXiv preprint arXiv:1605.08695,", "citeRegEx": "Tucker et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tucker et al\\.", "year": 2016}, {"title": "Deep voice: Real-time neural text-tospeech", "author": ["Arik", "Sercan O", "Chrzanowski", "Mike", "Coates", "Adam", "Diamos", "Gregory", "Gibiansky", "Andrew", "Kang", "Yongguo", "Li", "Xian", "Miller", "John", "Raiman", "Jonathan", "Sengupta", "Shubho"], "venue": "arXiv preprint arXiv:1702.07825,", "citeRegEx": "Arik et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Arik et al\\.", "year": 2017}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A fast multilevel implementation of recursive spectral bisection for partitioning unstructured problems", "author": ["S.T. Barnard", "H.D. Simon"], "venue": "Concurrency: practice and Experience,", "citeRegEx": "Barnard and Simon,? \\Q1994\\E", "shortCiteRegEx": "Barnard and Simon", "year": 1994}, {"title": "Neural combinatorial optimization with reinforcement learning", "author": ["Bello", "Irwan", "Pham", "Hieu", "Le", "Quoc V", "Norouzi", "Mohammad", "Bengio", "Samy"], "venue": "arXiv preprint arXiv:1611.09940,", "citeRegEx": "Bello et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bello et al\\.", "year": 2016}, {"title": "Listen, attend and spell", "author": ["Chan", "William", "Jaitly", "Navdeep", "Le", "Quoc V", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1508.01211,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Improvement of the efficiency of genetic algorithms for scalable parallel graph partitioning in a multi-level framework", "author": ["C. Chevalier", "F. Pellegrini"], "venue": "EuroPar, Dresden,", "citeRegEx": "Chevalier and Pellegrini,? \\Q2006\\E", "shortCiteRegEx": "Chevalier and Pellegrini", "year": 2006}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Holger et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Holger et al\\.", "year": 2014}, {"title": "Dermatologist-level classification of skin cancer", "author": ["Esteva", "Andre", "Kuprel", "Brett", "Novoa", "Rob", "Ko", "Justin", "Swetter", "Susan", "Blau", "Helen M", "Thrun", "Sebastian"], "venue": null, "citeRegEx": "Esteva et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Esteva et al\\.", "year": 2016}, {"title": "A lineartime heuristic for improving network partitions", "author": ["Fiduccia", "Charles M", "Mattheyses", "Robert M"], "venue": "In Papers on Twenty-five years of electronic design automation,", "citeRegEx": "Fiduccia et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Fiduccia et al\\.", "year": 1988}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Alex", "Jaitly", "Navdeep"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "New spectral methods for ratio cut partitioning and clustering", "author": ["Hagen", "Lars", "Kahng", "Andrew B"], "venue": "IEEE transactions on computer-aided design of integrated circuits and systems,", "citeRegEx": "Hagen et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Hagen et al\\.", "year": 1992}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Hannun", "Awni", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan", "Diamos", "Greg", "Elsen", "Erich", "Prenger", "Ryan", "Satheesh", "Sanjeev", "Sengupta", "Shubho", "Coates", "Adam"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "A multilevel algorithm for partitioning graphs", "author": ["B. Hendrickson", "R. Leland"], "venue": "Technical Report SAND931301,", "citeRegEx": "Hendrickson and Leland,? \\Q1993\\E", "shortCiteRegEx": "Hendrickson and Leland", "year": 1993}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "Jurgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Optimization by simulated annealing: an experimental evaluation; part i, graph partitioning", "author": ["Johnson", "David S", "Aragon", "Cecilia R", "McGeoch", "Lyle A", "Schevon", "Catherine"], "venue": "Operations research,", "citeRegEx": "Johnson et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 1989}, {"title": "Exploring the limits of language modeling", "author": ["Jozefowicz", "Rafal", "Vinyals", "Oriol", "Schuster", "Mike", "Shazeer", "Noam", "Wu", "Yonghui"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "A fast and high quality multilevel scheme for partitioning irregular graphs", "author": ["G. Karypis", "V. Kumar"], "venue": "Technical Report 95-035,", "citeRegEx": "Karypis and Kumar,? \\Q1995\\E", "shortCiteRegEx": "Karypis and Kumar", "year": 1995}, {"title": "Metis\u2013unstructured graph partitioning and sparse matrix ordering system, version", "author": ["Karypis", "George", "Kumar", "Vipin"], "venue": null, "citeRegEx": "Karypis et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Karypis et al\\.", "year": 1995}, {"title": "An efficient heuristic procedure for partitioning graphs", "author": ["Kernighan", "Brian W", "Lin", "Shen"], "venue": "The Bell system technical journal,", "citeRegEx": "Kernighan et al\\.,? \\Q1970\\E", "shortCiteRegEx": "Kernighan et al\\.", "year": 1970}, {"title": "Achieving budgetoptimality with adaptive schemes in crowdsourcing", "author": ["Khetan", "Ashish", "Oh", "Sewoong"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Khetan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Khetan et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Optimization by simulated annealing", "author": ["Kirkpatrick", "Scott", "Vecchi", "Mario P"], "venue": "Science, 220(4598):671\u2013680,", "citeRegEx": "Kirkpatrick et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Kirkpatrick et al\\.", "year": 1983}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Resource management with deep reinforcement learning", "author": ["Mao", "Hongzi", "Alizadeh", "Mohammad", "Menache", "Ishai", "Kandula", "Srikanth"], "venue": "In Proceedings of the 15th ACM Workshop on Hot Topics in Networks,", "citeRegEx": "Mao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2016}, {"title": "Wavenet: A generative model for raw audio", "author": ["Oord", "Aaron van den", "Dieleman", "Sander", "Zen", "Heiga", "Simonyan", "Karen", "Vinyals", "Oriol", "Graves", "Alex", "Kalchbrenner", "Nal", "Senior", "Andrew", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1609.03499,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "A parallelisable multi-level banded diffusion scheme for computing balanced partitions with smooth boundaries", "author": ["F. Pellegrini"], "venue": "EuroPar, Rennes,", "citeRegEx": "Pellegrini,? \\Q2007\\E", "shortCiteRegEx": "Pellegrini", "year": 2007}, {"title": "Distillating knowledge about scotch", "author": ["F. Pellegrini"], "venue": null, "citeRegEx": "Pellegrini,? \\Q2009\\E", "shortCiteRegEx": "Pellegrini", "year": 2009}, {"title": "Experimental analysis of the dual recursive bipartitioning algorithm for static mapping", "author": ["F. Pellegrini", "J. Roman"], "venue": "Research Report,", "citeRegEx": "Pellegrini and Roman,? \\Q1996\\E", "shortCiteRegEx": "Pellegrini and Roman", "year": 1996}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Szegedy", "Christian", "Vanhoucke", "Vincent", "Ioffe", "Sergey", "Shlens", "Jon", "Wojna", "Zbigniew"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Simple statistical gradient following algorithms for connectionnist reinforcement learning", "author": ["Williams", "Ronald"], "venue": "In Machine Learning,", "citeRegEx": "Williams and Ronald.,? \\Q1992\\E", "shortCiteRegEx": "Williams and Ronald.", "year": 1992}, {"title": "Recurrent neural network regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 24, "context": "Over the past few years, neural networks have proven to be a general and effective tool for many practical problems, such as image classification (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2016), speech recognition (Hinton et al.", "startOffset": 146, "endOffset": 210}, {"referenceID": 32, "context": "Over the past few years, neural networks have proven to be a general and effective tool for many practical problems, such as image classification (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2016), speech recognition (Hinton et al.", "startOffset": 146, "endOffset": 210}, {"referenceID": 13, "context": "Over the past few years, neural networks have proven to be a general and effective tool for many practical problems, such as image classification (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2016), speech recognition (Hinton et al.", "startOffset": 146, "endOffset": 210}, {"referenceID": 12, "context": ", 2016), speech recognition (Hinton et al., 2012; Graves & Jaitly, 2014; Hannun et al., 2014; Chan et al., 2015), machine translation (Sutskever et al.", "startOffset": 28, "endOffset": 112}, {"referenceID": 5, "context": ", 2016), speech recognition (Hinton et al., 2012; Graves & Jaitly, 2014; Hannun et al., 2014; Chan et al., 2015), machine translation (Sutskever et al.", "startOffset": 28, "endOffset": 112}, {"referenceID": 26, "context": ", 2016) and speech synthesis (Oord et al., 2016; Arik et al., 2017; Wang et al., 2017).", "startOffset": 29, "endOffset": 86}, {"referenceID": 1, "context": ", 2016) and speech synthesis (Oord et al., 2016; Arik et al., 2017; Wang et al., 2017).", "startOffset": 29, "endOffset": 86}, {"referenceID": 31, "context": "For example, in a neural translation network, each layer, including all LSTM layers, the attention layer, and the softmax layer, is computed by a GPU (Sutskever et al., 2014; Wu et al., 2016).", "startOffset": 150, "endOffset": 191}, {"referenceID": 33, "context": "Although such decisions can be made by machine learning practitioners, they can be challenging, especially when the network has many branches (Szegedy et al., 2016), or when the minibatches get larger.", "startOffset": 142, "endOffset": 164}, {"referenceID": 29, "context": "Existing algorithmic solvers (Pellegrini, 2009; Karypis & Kumar, 1995b), on the other hand, are not flexible enough to work with a dynamic environment with many interferences.", "startOffset": 29, "endOffset": 71}, {"referenceID": 33, "context": "Our main result is that our method finds non-trivial placements onmultiple devices for Inception-V3 (Szegedy et al., 2016), Recurrent Neural Language Model (Zaremba et al.", "startOffset": 100, "endOffset": 122}, {"referenceID": 36, "context": ", 2016), Recurrent Neural Language Model (Zaremba et al., 2014; Jozefowicz et al., 2016) and Neural Machine Translation (Sutskever et al.", "startOffset": 41, "endOffset": 88}, {"referenceID": 17, "context": ", 2016), Recurrent Neural Language Model (Zaremba et al., 2014; Jozefowicz et al., 2016) and Neural Machine Translation (Sutskever et al.", "startOffset": 41, "endOffset": 88}, {"referenceID": 31, "context": ", 2016) and Neural Machine Translation (Sutskever et al., 2014; Wu et al., 2016).", "startOffset": 39, "endOffset": 80}, {"referenceID": 29, "context": "Single-step measurements show that Scotch (Pellegrini, 2009) yields disappointing results on all three benchmarks, suggesting that their graph-based heuristics are not flexible enough for them.", "startOffset": 42, "endOffset": 60}, {"referenceID": 4, "context": "Our work is closely related to the idea of using neural networks and reinforcement learning for combinatorial optimization (Vinyals et al., 2015; Bello et al., 2016).", "startOffset": 123, "endOffset": 165}, {"referenceID": 25, "context": "For example, Mao et al. (2016) propose to train a resource management algorithm with policy gradients.", "startOffset": 13, "endOffset": 31}, {"referenceID": 22, "context": "Early work such as Kernighan & Lin (1970); Kirkpatrick et al. (1983); Fiduccia & Mattheyses (1988); Johnson et al.", "startOffset": 43, "endOffset": 69}, {"referenceID": 22, "context": "Early work such as Kernighan & Lin (1970); Kirkpatrick et al. (1983); Fiduccia & Mattheyses (1988); Johnson et al.", "startOffset": 43, "endOffset": 99}, {"referenceID": 16, "context": "(1983); Fiduccia & Mattheyses (1988); Johnson et al. (1989) employ several iterative refinement procedures that start from a partition and continue to explore similar partitions to improve.", "startOffset": 38, "endOffset": 60}, {"referenceID": 16, "context": "(1983); Fiduccia & Mattheyses (1988); Johnson et al. (1989) employ several iterative refinement procedures that start from a partition and continue to explore similar partitions to improve. Alternative methods such as Hagen & Kahng (1992); Karypis & Kumar (1995b) perform spectral analyses on matrix representations of graphs to partition them.", "startOffset": 38, "endOffset": 239}, {"referenceID": 16, "context": "(1983); Fiduccia & Mattheyses (1988); Johnson et al. (1989) employ several iterative refinement procedures that start from a partition and continue to explore similar partitions to improve. Alternative methods such as Hagen & Kahng (1992); Karypis & Kumar (1995b) perform spectral analyses on matrix representations of graphs to partition them.", "startOffset": 38, "endOffset": 264}, {"referenceID": 29, "context": "A well-known graph partitioning algorithm with an open source software library is the Scotch optimizer (Pellegrini, 2009), which we use as a baseline in our experiments.", "startOffset": 103, "endOffset": 121}, {"referenceID": 28, "context": "Scotch relies on a collection of graph partitioning techniques such as k-way Fiduccia-Mattheyses (Fiduccia & Mattheyses, 1988), multilevel method (Barnard & Simon, 1994; Hendrickson & Leland, 1993; Karypis & Kumar, 1995a), band method (Chevalier & Pellegrini, 2006), diffusion method (Pellegrini, 2007), and dual recursive bipartitioning mapping (Pellegrini & Roman, 1996)).", "startOffset": 284, "endOffset": 302}, {"referenceID": 31, "context": "We use a sequence-to-sequence model (Sutskever et al., 2014) with LSTM (Hochreiter & Schmidhuber, 1997) and a content-based attention mechanism (Bahdanau et al.", "startOffset": 36, "endOffset": 60}, {"referenceID": 2, "context": ", 2014) with LSTM (Hochreiter & Schmidhuber, 1997) and a content-based attention mechanism (Bahdanau et al., 2015) to predict the placements.", "startOffset": 91, "endOffset": 114}, {"referenceID": 2, "context": "The decoder is an attentional LSTM (Bahdanau et al., 2015) with a fixed number of time steps that is equal to the number of operations in a graph G.", "startOffset": 35, "endOffset": 58}, {"referenceID": 27, "context": "Modeling such a large number of operations with sequence-to-sequence models is difficult due to vanishing and exploding gradient issues (Pascanu et al., 2013) and large memory footprints.", "startOffset": 136, "endOffset": 158}, {"referenceID": 36, "context": "In the following experiments, we apply our proposed method to assign computations to devices on three important neural networks in the deep learning literature: Recurrent Neural Language Model (RNNLM) (Zaremba et al., 2014; Jozefowicz et al., 2016), Attentional Neural Machine Translation (Bahdanau et al.", "startOffset": 201, "endOffset": 248}, {"referenceID": 17, "context": "In the following experiments, we apply our proposed method to assign computations to devices on three important neural networks in the deep learning literature: Recurrent Neural Language Model (RNNLM) (Zaremba et al., 2014; Jozefowicz et al., 2016), Attentional Neural Machine Translation (Bahdanau et al.", "startOffset": 201, "endOffset": 248}, {"referenceID": 2, "context": ", 2016), Attentional Neural Machine Translation (Bahdanau et al., 2015), and InceptionV3 (Szegedy et al.", "startOffset": 48, "endOffset": 71}, {"referenceID": 33, "context": ", 2015), and InceptionV3 (Szegedy et al., 2016).", "startOffset": 25, "endOffset": 47}, {"referenceID": 36, "context": "\u2022 Recurrent Neural Network Language Model (RNNLM) with multiple LSTM layers (Zaremba et al., 2014; Jozefowicz et al., 2016).", "startOffset": 76, "endOffset": 123}, {"referenceID": 17, "context": "\u2022 Recurrent Neural Network Language Model (RNNLM) with multiple LSTM layers (Zaremba et al., 2014; Jozefowicz et al., 2016).", "startOffset": 76, "endOffset": 123}, {"referenceID": 2, "context": "\u2022 Neural Machine Translation with attention mechanism (NMT) (Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 60, "endOffset": 100}, {"referenceID": 31, "context": "Both Sutskever et al. (2014) andWu et al.", "startOffset": 5, "endOffset": 29}, {"referenceID": 31, "context": "Both Sutskever et al. (2014) andWu et al. (2016) propose to place each LSTM layer, the attention layer, and the softmax layer, each on a separate device.", "startOffset": 5, "endOffset": 49}, {"referenceID": 33, "context": "\u2022 Inception-V3 (Szegedy et al., 2016) is a widely-used architecture for image recognition and visual feature extraction (Khetan & Oh, 2016; Esteva et al.", "startOffset": 15, "endOffset": 37}, {"referenceID": 8, "context": ", 2016) is a widely-used architecture for image recognition and visual feature extraction (Khetan & Oh, 2016; Esteva et al., 2016).", "startOffset": 90, "endOffset": 130}, {"referenceID": 32, "context": "For Inception-V3, each step is executed on a batch of images, each of size 299 \u00d7 299 \u00d7 3, which is the widely-used setting for the ImageNet Challenge (Szegedy et al., 2015).", "startOffset": 150, "endOffset": 172}, {"referenceID": 29, "context": "We estimate the computational costs of each operation as well as the amount of data that flows along each edge of the neural network model, and feed them to the Scotch static mapper (Pellegrini, 2009).", "startOffset": 182, "endOffset": 200}, {"referenceID": 33, "context": "Each GPU runs a replica of the model and processes a batch of size 32 (Szegedy et al., 2016).", "startOffset": 70, "endOffset": 92}], "year": 2017, "abstractText": "The past few years have witnessed a growth in size and computational requirements for training and inference with neural networks. Currently, a common approach to address these requirements is to use a heterogeneous distributed environment with a mixture of hardware devices such as CPUs and GPUs. Importantly, the decision of placing parts of the neural models on devices is often made by human experts based on simple heuristics and intuitions. In this paper, we propose a method which learns to optimize device placement for TensorFlow computational graphs. Key to our method is the use of a sequence-tosequence model to predict which subsets of operations in a TensorFlow graph should run on which of the available devices. The execution time of the predicted placements is then used as the reward signal to optimize the parameters of the sequence-to-sequence model. Our main result is that on Inception-V3 for ImageNet classification, and on RNN LSTM, for language modeling and neural machine translation, our model finds non-trivial device placements that outperform hand-crafted heuristics and traditional algorithmic methods.", "creator": "LaTeX with hyperref package"}}}