{"id": "1706.05064", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning", "abstract": "As a step towards developing zero-shot task generalization capabilities in reinforcement learning (RL), we introduce a new RL problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instructions and to longer sequences of instructions. For generalization over unseen instructions, we propose a new objective which encourages learning correspondences between similar subtasks by making analogies. For generalization over sequential instructions, we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. To deal with delayed reward, we propose a new neural architecture in the meta controller that learns when to update the subtask, which makes learning more efficient. Experimental results on a stochastic 3D domain show that the proposed ideas are crucial for generalization to longer instructions as well as unseen instructions.", "histories": [["v1", "Thu, 15 Jun 2017 20:04:35 GMT  (3869kb,D)", "http://arxiv.org/abs/1706.05064v1", "ICML 2017"]], "COMMENTS": "ICML 2017", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["junhyuk oh", "satinder p singh", "honglak lee", "pushmeet kohli"], "accepted": true, "id": "1706.05064"}, "pdf": {"name": "1706.05064.pdf", "metadata": {"source": "META", "title": "Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning", "authors": ["Junhyuk Oh", "Satinder Singh", "Honglak Lee", "Pushmeet Kohli"], "emails": ["<junhyuk@umich.edu>."], "sections": [{"heading": "1. Introduction", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "2. Related Work", "text": "There are a number of hierarchical RL approaches designed to deal with sequential tasks, typically taking the form of a meta-controller and a series of substage controllers for subtasks (Sutton et al., 1999; Dietterich, 2000; Ghavamzadeh and Mahadevan, 2003; Konidaris et al., 2012; Konidaris and Barto, 2007), but much of the work to date has been fixed (e.g. Taxi Domain, 2000). In other words, the optimal sequence of subtasks is fixed during evaluation (e.g., picking up a passenger followed by navigation in the taxi domain), making it difficult to assess the agent's ability to perform previously learned tasks."}, {"heading": "3. Learning a Parameterized Skill", "text": "In this paper, a parameterized skill is defined as a policy of multiple tasks corresponding to multiple tasks defined by categorical input task parameters, e.g. [Pick up, X]. Formally, we define a parameterized skill as a figure of O \u00b7 G \u2192 A \u00b7 B, where O is a set of observations, G is a set of task parameters, A is a set of primitive actions, and B = {0, 1} indicates whether the task is completed or not. A task space is defined by the cartesian product of task parameters: G = G (1)... \u00d7 G (n), where G (i) is a set of i-th parameters (e.g. G = {Visit, Pick up}). In the light of an observation xt, the observation xt x is defined at time and task parameters g = [g (n) is finally next."}, {"heading": "3.1. Learning to Generalize by Analogy-Making", "text": "Only a subset of tasks (G \u00b2 G) are available during the training, so to generalize invisible tasks during the evaluation, we propose a knowledge of the relationship between different task parameters when learning task embedding (G). To this end, we can suggest an analogy object (inspired by Reed et al. (2015). The main idea is to learn similarities between tasks. \u2212 For example, if target objects and \"Visit / Pick up\" actions are independent (i.e., each action can be applied to each target), we can enforce the analogy [Visit, X]: [Visit, Y]: [Pick up, X \u2212 Gick up, Y] for all X and Y in the embedding space, which means that the difference between \"Visit\" and \"Pick up\" is consistent, independent of target objects and vice versa. This allows the agent to generalize invisible combinations of actions and objectives."}, {"heading": "3.2. Training", "text": "The parameterized skill is trained through a series of tasks (G \u00b2 G) using the Actor-Critic Method with Generalized Benefit Assessment (Schulman et al., 2016). We also found that pre-training by political distillation (Rusu et al., 2016; Parisotto et al., 2016) yields slightly better results than discussed in Tessler et al. (2017). Throughout the training, the parameterized skill is also used to predict whether the current state is terminated by a binary classification target or not, and the analogous target is applied separately to the task. Full details of the learning goals are described in the appendix."}, {"heading": "3.3. Experiments", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to move, to fight, to fight, to move, to move, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "4. Learning to Execute Instructions using Parameterized Skill", "text": "We now look at the problem of instruction execution, where the agent is given a sequence of simple natural language instructions, as shown in Figure 1. We are assuming an already trained parameterized skill, as described in Section 3. So the most important remaining problem is how to use the parameterized skills to execute instructions. Although the requirement that instructions must be executed sequentially facilitates the problem (as, for example, conditional instructions), the agent still has to make complex decisions because it should deviate from instructions for dealing with unexpected events (e.g. battery shortage) and remember what it has done to deal with loop instructions, as in Section 1. In order to address the above challenges, our hierarchical RL architecture (see Figure 3) consists of two modules: meta controller and parameterized skills. Specifically, a meta controller reads the instructions and passes task parameters to a parameterized skill, the meta signal back to the final controller."}, {"heading": "4.1. Meta Controller Architecture", "text": "As illustrated in Figure 4, the meta-controller is a mapping O \u00b7 M \u00b7 G \u00b7 B \u2192 G, where M is a list of instructions. Intuitively, the meta-controller decides subtask parameter gt \u0394G based on observation xt \u0441O, the list of instructionsM \u0394M, the previously selected subtask gt \u2212 1 and its termination signal (b \u0445 \u03b2\u03c6). In contrast to the recent hierarchical deep RL approaches, where the meta-controller can only update its subtask (or option) if the previous one is terminated or only after a fixed number of steps, our meta-controller can update the subtask at any time and takes the termination signal as an additional input. This gives the meta-controller more flexibility and allows to interrupt ongoing tasks before completion. To track the agent's progress in executing the command, the meta-controller maintains its internal state by executing a context vector (determined by section 1.1 and a sub-task)."}, {"heading": "4.1.1. CONTEXT", "text": "Considering the rt \u2212 1 set retrieved in the previous time step from the statements (described in Section 4.1.2), the previously selected sub-task gt \u2212 1 and the completion of the sub-task bt \u0445 \u03b2\u03c6 (bt | st, gt \u2212 1), the meta-controller calculates the context vector (ht) as follows: ht = LSTM (st, ht \u2212 1) st = f (xt, rt \u2212 1, gt \u2212 1, bt), where f is a neural network. Intuitively gt \u2212 1 and bt give information about which sub-task has been solved by the parameterized skill and whether it has been completed or not. Therefore st is a summary of the current observation and the ongoing subtask. Considered takes into account the history of st by the LSTM used by the sub-task updater."}, {"heading": "4.1.2. SUBTASK UPDATER", "text": "The subtask updater constructs a memory structure from the list of statements, retrieves a statement by holding a pointer to memory, and computes the subtask parameters. Instructions as a list of records M = (m1, m2,..., mK), where each record consists of a list of words, mi = (w1,..., w | mi |), the subtask updater constructs memory blocks M = (m1, m2,..., mK), where each column is an E-dimensional embedding of a record. The subtask updater maintains a statement pointer (pt, RK) that is not negative and adds up to 1, which statement the meta controller executes. Memory construction and retrieval can be written as: Memory: M = [rt], which is a baht task (m1), w (m2), w (mK)."}, {"heading": "4.2. Learning to Operate at a Large Time-Scale", "text": "Although the meta controllers can learn optimal policy by updating the subtask at each time step, this decision is invalid at each time step, since the subtask termination is unable to define the time level of the meta controller, as in many current hierarchical deep RL approaches (see Section 2), this approach would result in a largely open loopmeta controller that is unable to interrupt ongoing subtask tasks before termination, which is necessary to deal with unexpected events that are not specified. To solve this dilemma, we suggest learning the meta controller timescale by introducing an internal binary decision indicating whether or not to update the subtask updater, as illustrated in Figure 5."}, {"heading": "4.3. Training", "text": "Since the meta-controller also learns a sub-task that is embedded in the evaluation (gt \u2212 1) and has to do with invisible sub-tasks, an analogous target is also used. Details of the lens function are given in the appendix."}, {"heading": "4.4. Experiments", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "5. Conclusion", "text": "In this work, we explored a kind of zero-shot task generalization in the RL with a new problem where the agent is obliged to execute and generalize instructions; we proposed an analogous goal that allows generalization over invisible parameterized tasks in different scenarios; we also proposed a novel method to learn the timeframe of the meta-controller, which proved to be more efficient and flexible than alternative approaches to interrupting subtasks and dealing with delayed sequential decision problems; our empirical results in a stochastic 3D domain showed that our architecture can be well generalized to longer statements as well as invisible statements; although our hierarchical RL architecture was demonstrated in the simple environment where the statements should be executed sequentially, we believe that our key ideas are not limited to this setting, but can be extended to richer forms of statements."}, {"heading": "Acknowledgement", "text": "This work has been supported by the NSF grant IIS-1526059. Any opinions, findings, conclusions or recommendations expressed herein are those of the authors and do not necessarily reflect the views of the sponsor."}, {"heading": "A. Experimental Setting for Parameterized Tasks", "text": "The episode ends after 50 steps for \"independent\" and \"object-dependent\" cases and 70 steps for \"inter / extrapolation\" cases. The agent receives a positive reward (+ 1) if he successfully completes the specified task and receives a time penalty of \u2212 0.1 for each step. Details of each generalization are described below. The semantics of the tasks are consistent in all types of target objects. \u2022 The training set of the tasks is shown in Table 3. Examples of analogies used in this experiment: \u2022 Visit, X: [Visit, Y]: [Transform, X: [Transform, Y]: [Visit, X]."}, {"heading": "B. Experimental Setting for Instruction Execution", "text": "As a result, most people are able to determine for themselves what they want and what they want."}, {"heading": "D. Details of Learning Objectives", "text": "(CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) () (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP) (CFSP (CFSP) (CFSP) (CFSP) (CFSP (CFSP) (CFSP) (CFSP) (CFSP) (CFSP (CFSP) (CFSP) (CFSP) (CFSP) ("}, {"heading": "E. Architectures and Hyperparameters", "text": "Background: Combining condition variables in a neural network (e.g., combining tasks for embedding in the Convolutionary Network in parameterized skill) has a form of multiplicative interaction instead of concatenating such variables as those of (Memisevic and Hinton, 2010; Oh et al., 2015), and also applies to parameter prediction approaches where the parameters of the neural network are produced by condition variables (e.g., example, class embedding), which has shown to be effective in achieving zero-shot and one-shot generalization in image generalization problems (Lei Ba et al, 2015)."}], "references": [{"title": "Programmable reinforcement learning agents", "author": ["D. Andre", "S.J. Russell"], "venue": "In NIPS,", "citeRegEx": "Andre and Russell.,? \\Q2000\\E", "shortCiteRegEx": "Andre and Russell.", "year": 2000}, {"title": "State abstraction for programmable reinforcement learning agents", "author": ["D. Andre", "S.J. Russell"], "venue": "In AAAI/IAAI,", "citeRegEx": "Andre and Russell.,? \\Q2002\\E", "shortCiteRegEx": "Andre and Russell.", "year": 2002}, {"title": "Modular multitask reinforcement learning with policy", "author": ["J. Andreas", "D. Klein", "S. Levine"], "venue": "sketches. CoRR,", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "The option-critic architecture", "author": ["P.-L. Bacon", "J. Harb", "D. Precup"], "venue": "In AAAI,", "citeRegEx": "Bacon et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bacon et al\\.", "year": 2017}, {"title": "Learning feed-forward one-shot learners", "author": ["L. Bertinetto", "J.F. Henriques", "J. Valmadre", "P.H. Torr", "A. Vedaldi"], "venue": "arXiv preprint arXiv:1606.05233,", "citeRegEx": "Bertinetto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bertinetto et al\\.", "year": 2016}, {"title": "Reinforcement learning for mapping instructions to actions", "author": ["S.R.K. Branavan", "H. Chen", "L.S. Zettlemoyer", "R. Barzilay"], "venue": "ACL/IJCNLP,", "citeRegEx": "Branavan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Branavan et al\\.", "year": 2009}, {"title": "Learning to interpret natural language navigation instructions from observations", "author": ["D.L. Chen", "R.J. Mooney"], "venue": "In AAAI,", "citeRegEx": "Chen and Mooney.,? \\Q2011\\E", "shortCiteRegEx": "Chen and Mooney.", "year": 2011}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["J. Chung", "S. Ahn", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Chung et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2017}, {"title": "Learning parameterized skills", "author": ["B.C. da Silva", "G. Konidaris", "A.G. Barto"], "venue": "In ICML,", "citeRegEx": "Silva et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2012}, {"title": "Learning modular neural network policies for multi-task and multi-robot transfer", "author": ["C. Devin", "A. Gupta", "T. Darrell", "P. Abbeel", "S. Levine"], "venue": null, "citeRegEx": "Devin et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Devin et al\\.", "year": 2017}, {"title": "Hierarchical reinforcement learning with the maxq value function decomposition", "author": ["T.G. Dietterich"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich.", "year": 2000}, {"title": "Stochastic neural networks for hierarchical reinforcement learning", "author": ["C. Florensa", "Y. Duan", "P. Abbeel"], "venue": "In ICLR,", "citeRegEx": "Florensa et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Florensa et al\\.", "year": 2017}, {"title": "Hierarchical policy gradient algorithms", "author": ["M. Ghavamzadeh", "S. Mahadevan"], "venue": "In ICML,", "citeRegEx": "Ghavamzadeh and Mahadevan.,? \\Q2003\\E", "shortCiteRegEx": "Ghavamzadeh and Mahadevan.", "year": 2003}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "In CVPR,", "citeRegEx": "Hadsell et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hadsell et al\\.", "year": 2006}, {"title": "Learning and transfer of modulated locomotor controllers", "author": ["N. Heess", "G. Wayne", "Y. Tassa", "T.P. Lillicrap", "M.A. Riedmiller", "D. Silver"], "venue": "arXiv preprint arXiv:1610.05182,", "citeRegEx": "Heess et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Using task features for zero-shot knowledge transfer in lifelong learning", "author": ["D. Isele", "M. Rostami", "E. Eaton"], "venue": "In IJCAI,", "citeRegEx": "Isele et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Isele et al\\.", "year": 2016}, {"title": "Actor-critic algorithms", "author": ["V.R. Konda", "J.N. Tsitsiklis"], "venue": "In NIPS,", "citeRegEx": "Konda and Tsitsiklis.,? \\Q1999\\E", "shortCiteRegEx": "Konda and Tsitsiklis.", "year": 1999}, {"title": "Building portable options: Skill transfer in reinforcement learning", "author": ["G. Konidaris", "A.G. Barto"], "venue": "In IJCAI,", "citeRegEx": "Konidaris and Barto.,? \\Q2007\\E", "shortCiteRegEx": "Konidaris and Barto.", "year": 2007}, {"title": "Transfer in reinforcement learning via shared features", "author": ["G. Konidaris", "I. Scheidwasser", "A.G. Barto"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Konidaris et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Konidaris et al\\.", "year": 2012}, {"title": "A clockwork rnn", "author": ["J. Koutnik", "K. Greff", "F. Gomez", "J. Schmidhuber"], "venue": "In ICML,", "citeRegEx": "Koutnik et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koutnik et al\\.", "year": 2014}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["T.D. Kulkarni", "K.R. Narasimhan", "A. Saeedi", "J.B. Tenenbaum"], "venue": "arXiv preprint arXiv:1604.06057,", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Predicting deep zero-shot convolutional neural networks using textual descriptions", "author": ["J. Lei Ba", "K. Swersky", "S. Fidler"], "venue": "In ICCV,", "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Walk the talk: Connecting language, knowledge, and action in route instructions", "author": ["M. MacMahon", "B. Stankiewicz", "B. Kuipers"], "venue": "In AAAI,", "citeRegEx": "MacMahon et al\\.,? \\Q2006\\E", "shortCiteRegEx": "MacMahon et al\\.", "year": 2006}, {"title": "Autonomous discovery of temporal abstractions from interaction with an environment", "author": ["A. McGovern", "A.G. Barto"], "venue": "PhD thesis, University of Massachusetts,", "citeRegEx": "McGovern and Barto.,? \\Q2002\\E", "shortCiteRegEx": "McGovern and Barto.", "year": 2002}, {"title": "Listen, attend, and walk: Neural mapping of navigational instructions to action sequences", "author": ["H. Mei", "M. Bansal", "M.R. Walter"], "venue": "arXiv preprint arXiv:1506.04089,", "citeRegEx": "Mei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mei et al\\.", "year": 2015}, {"title": "Learning to represent spatial transformations with factored higher-order boltzmann machines", "author": ["R. Memisevic", "G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Memisevic and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Memisevic and Hinton.", "year": 2010}, {"title": "Exploiting similarities among languages for machine translation", "author": ["T. Mikolov", "Q.V. Le", "I. Sutskever"], "venue": "arXiv preprint arXiv:1309.4168,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Actionconditional video prediction using deep networks in atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S. Singh"], "venue": "In NIPS,", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Memorybased control of active perception and action in minecraft", "author": ["J. Oh", "V. Chockalingam", "S. Singh", "H. Lee"], "venue": "In ICML,", "citeRegEx": "Oh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2016}, {"title": "Actor-mimic: Deep multitask and transfer reinforcement learning", "author": ["E. Parisotto", "J.L. Ba", "R. Salakhutdinov"], "venue": "In ICLR,", "citeRegEx": "Parisotto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Parisotto et al\\.", "year": 2016}, {"title": "Reinforcement learning with hierarchies of machines", "author": ["R. Parr", "S.J. Russell"], "venue": "In NIPS,", "citeRegEx": "Parr and Russell.,? \\Q1997\\E", "shortCiteRegEx": "Parr and Russell.", "year": 1997}, {"title": "Deep visual analogy-making", "author": ["S.E. Reed", "Y. Zhang", "H. Lee"], "venue": "In NIPS,", "citeRegEx": "Reed et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2015}, {"title": "Policy distillation", "author": ["A.A. Rusu", "S.G. Colmenarejo", "C. Gulcehre", "G. Desjardins", "J. Kirkpatrick", "R. Pascanu", "V. Mnih", "K. Kavukcuoglu", "R. Hadsell"], "venue": "In ICLR,", "citeRegEx": "Rusu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Universal value function approximators", "author": ["T. Schaul", "D. Horgan", "K. Gregor", "D. Silver"], "venue": "In ICML,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["J. Schulman", "P. Moritz", "S. Levine", "M. Jordan", "P. Abbeel"], "venue": "In ICLR,", "citeRegEx": "Schulman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2016}, {"title": "The efficient learning of multiple task sequences", "author": ["S.P. Singh"], "venue": "In NIPS,", "citeRegEx": "Singh.,? \\Q1991\\E", "shortCiteRegEx": "Singh.", "year": 1991}, {"title": "Transfer of learning by composing solutions of elemental sequential tasks", "author": ["S.P. Singh"], "venue": "Machine Learning,", "citeRegEx": "Singh.,? \\Q1992\\E", "shortCiteRegEx": "Singh.", "year": 1992}, {"title": "Mazebase: A sandbox for learning from games", "author": ["S. Sukhbaatar", "A. Szlam", "G. Synnaeve", "S. Chintala", "R. Fergus"], "venue": "arXiv preprint arXiv:1511.07401,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["R.S. Sutton", "D. Precup", "S. Singh"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Understanding natural language commands for robotic navigation and mobile manipulation", "author": ["S. Tellex", "T. Kollar", "S. Dickerson", "M.R. Walter", "A.G. Banerjee", "S.J. Teller", "N. Roy"], "venue": "In AAAI,", "citeRegEx": "Tellex et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tellex et al\\.", "year": 2011}, {"title": "Asking for help using inverse semantics", "author": ["S. Tellex", "R.A. Knepper", "A. Li", "D. Rus", "N. Roy"], "venue": "In RSS,", "citeRegEx": "Tellex et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tellex et al\\.", "year": 2014}, {"title": "A deep hierarchical approach to lifelong learning in minecraft", "author": ["C. Tessler", "S. Givony", "T. Zahavy", "D.J. Mankowitz", "S. Mannor"], "venue": "In AAAI,", "citeRegEx": "Tessler et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Tessler et al\\.", "year": 2017}, {"title": "Reinforcement learning neural turing machines", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "Zaremba and Sutskever.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2015}], "referenceMentions": [{"referenceID": 29, "context": "We developed a 3D visual environment using Minecraft based on Oh et al. (2016) where the agent can interact with many objects.", "startOffset": 62, "endOffset": 79}, {"referenceID": 40, "context": "Typically these have the form of a meta controller and a set of lower-level controllers for subtasks (Sutton et al., 1999; Dietterich, 2000; Parr and Russell, 1997; Ghavamzadeh and Mahadevan, 2003; Konidaris et al., 2012; Konidaris and Barto, 2007).", "startOffset": 101, "endOffset": 248}, {"referenceID": 10, "context": "Typically these have the form of a meta controller and a set of lower-level controllers for subtasks (Sutton et al., 1999; Dietterich, 2000; Parr and Russell, 1997; Ghavamzadeh and Mahadevan, 2003; Konidaris et al., 2012; Konidaris and Barto, 2007).", "startOffset": 101, "endOffset": 248}, {"referenceID": 32, "context": "Typically these have the form of a meta controller and a set of lower-level controllers for subtasks (Sutton et al., 1999; Dietterich, 2000; Parr and Russell, 1997; Ghavamzadeh and Mahadevan, 2003; Konidaris et al., 2012; Konidaris and Barto, 2007).", "startOffset": 101, "endOffset": 248}, {"referenceID": 12, "context": "Typically these have the form of a meta controller and a set of lower-level controllers for subtasks (Sutton et al., 1999; Dietterich, 2000; Parr and Russell, 1997; Ghavamzadeh and Mahadevan, 2003; Konidaris et al., 2012; Konidaris and Barto, 2007).", "startOffset": 101, "endOffset": 248}, {"referenceID": 20, "context": "Typically these have the form of a meta controller and a set of lower-level controllers for subtasks (Sutton et al., 1999; Dietterich, 2000; Parr and Russell, 1997; Ghavamzadeh and Mahadevan, 2003; Konidaris et al., 2012; Konidaris and Barto, 2007).", "startOffset": 101, "endOffset": 248}, {"referenceID": 19, "context": "Typically these have the form of a meta controller and a set of lower-level controllers for subtasks (Sutton et al., 1999; Dietterich, 2000; Parr and Russell, 1997; Ghavamzadeh and Mahadevan, 2003; Konidaris et al., 2012; Konidaris and Barto, 2007).", "startOffset": 101, "endOffset": 248}, {"referenceID": 10, "context": ", Taxi domain (Dietterich, 2000)).", "startOffset": 14, "endOffset": 32}, {"referenceID": 37, "context": "This makes it hard to evaluate the agent\u2019s ability to compose pre-learned policies to solve previously unseen sequential tasks in a zero-shot fashion unless we re-train the agent on the new tasks in a transfer learning setting (Singh, 1991; 1992; McGovern and Barto, 2002).", "startOffset": 227, "endOffset": 272}, {"referenceID": 25, "context": "This makes it hard to evaluate the agent\u2019s ability to compose pre-learned policies to solve previously unseen sequential tasks in a zero-shot fashion unless we re-train the agent on the new tasks in a transfer learning setting (Singh, 1991; 1992; McGovern and Barto, 2002).", "startOffset": 227, "endOffset": 272}, {"referenceID": 0, "context": "Our work is also closely related to Programmable HAMs (PHAMs) (Andre and Russell, 2000; 2002) in that a PHAM is designed to execute a given program.", "startOffset": 62, "endOffset": 93}, {"referenceID": 21, "context": "Kulkarni et al. (2016) proposed hierarchical Deep Q-Learning and demonstrated improved exploration in a challenging Atari game.", "startOffset": 0, "endOffset": 23}, {"referenceID": 21, "context": "Kulkarni et al. (2016) proposed hierarchical Deep Q-Learning and demonstrated improved exploration in a challenging Atari game. Tessler et al. (2017) proposed a similar architecture, but the highlevel controller is allowed to choose primitive actions directly.", "startOffset": 0, "endOffset": 150}, {"referenceID": 3, "context": "Bacon et al. (2017) proposed the option-critic architecture, which learns options without pseudo reward and demonstrated that it can learn distinct options in Atari", "startOffset": 0, "endOffset": 20}, {"referenceID": 14, "context": "Heess et al. (2016) formulated the actions of the meta controller as continuous variables that are used to modulate the behavior of the low-level controller.", "startOffset": 0, "endOffset": 20}, {"referenceID": 11, "context": "Florensa et al. (2017) trained a stochastic neural network with mutual information regularization to discover skills.", "startOffset": 0, "endOffset": 23}, {"referenceID": 7, "context": "For example, da Silva et al. (2012) introduced parameterized skills that map sets of task descriptions to policies.", "startOffset": 16, "endOffset": 36}, {"referenceID": 7, "context": "For example, da Silva et al. (2012) introduced parameterized skills that map sets of task descriptions to policies. Isele et al. (2016) achieved zero-shot task generalization through dictionary learning with sparsity constraints.", "startOffset": 16, "endOffset": 136}, {"referenceID": 7, "context": "For example, da Silva et al. (2012) introduced parameterized skills that map sets of task descriptions to policies. Isele et al. (2016) achieved zero-shot task generalization through dictionary learning with sparsity constraints. Schaul et al. (2015) proposed universal value function approximators (UVFAs) that learn value functions for state and goal pairs.", "startOffset": 16, "endOffset": 251}, {"referenceID": 7, "context": "For example, da Silva et al. (2012) introduced parameterized skills that map sets of task descriptions to policies. Isele et al. (2016) achieved zero-shot task generalization through dictionary learning with sparsity constraints. Schaul et al. (2015) proposed universal value function approximators (UVFAs) that learn value functions for state and goal pairs. Devin et al. (2017) proposed composing sub-networks that are shared across tasks and robots in order to achieve generalization to unseen configurations of them.", "startOffset": 16, "endOffset": 380}, {"referenceID": 2, "context": "Andreas et al. (2016) proposed a framework to learn the underlying subtasks from a policy sketch which specifies a sequence of subtasks, and the agent can generalize over new sequences of them in principle.", "startOffset": 0, "endOffset": 22}, {"referenceID": 22, "context": "(2011; 2014) for robotics and MacMahon et al. (2006); Chen and Mooney (2011); Mei et al.", "startOffset": 30, "endOffset": 53}, {"referenceID": 5, "context": "(2006); Chen and Mooney (2011); Mei et al.", "startOffset": 8, "endOffset": 31}, {"referenceID": 5, "context": "(2006); Chen and Mooney (2011); Mei et al. (2015) for a simulated environment.", "startOffset": 8, "endOffset": 50}, {"referenceID": 5, "context": "Although Branavan et al. (2009) also tackle a similar problem, the agent is given a single instruction at a time, while our agent needs to learn how to align instructions and state given a full list of instructions.", "startOffset": 9, "endOffset": 32}, {"referenceID": 40, "context": "Policy: \u03c0\u03c6(at|xt, g) Termination: \u03b2\u03c6(bt|xt, g), where \u03c0\u03c6 is the policy optimized for the task g, and \u03b2\u03c6 is a termination function (Sutton et al., 1999) which is the probability that the state is terminal at time t for the given task g.", "startOffset": 130, "endOffset": 151}, {"referenceID": 33, "context": "To this end, we propose an analogy-making objective inspired by Reed et al. (2015). The main idea is to learn correspondences between tasks.", "startOffset": 64, "endOffset": 83}, {"referenceID": 14, "context": "We incorporate these constraints into the following objectives based on contrastive loss (Hadsell et al., 2006):", "startOffset": 89, "endOffset": 111}, {"referenceID": 27, "context": ", parallelogram structure in the embedding space; see Mikolov et al. (2013); Reed et al.", "startOffset": 54, "endOffset": 76}, {"referenceID": 27, "context": ", parallelogram structure in the embedding space; see Mikolov et al. (2013); Reed et al. (2015)), while the other constraints prevent trivial solutions.", "startOffset": 54, "endOffset": 96}, {"referenceID": 36, "context": "Training The parameterized skill is trained on a set of tasks (G\u2032 \u2282 G) through the actor-critic method with generalized advantage estimation (Schulman et al., 2016).", "startOffset": 141, "endOffset": 164}, {"referenceID": 34, "context": "We also found that pre-training through policy distillation (Rusu et al., 2016; Parisotto et al., 2016) gives slightly better results as discussed in Tessler et al.", "startOffset": 60, "endOffset": 103}, {"referenceID": 31, "context": "We also found that pre-training through policy distillation (Rusu et al., 2016; Parisotto et al., 2016) gives slightly better results as discussed in Tessler et al.", "startOffset": 60, "endOffset": 103}, {"referenceID": 31, "context": ", 2016; Parisotto et al., 2016) gives slightly better results as discussed in Tessler et al. (2017). Throughout training, the parameterized skill is also made to predict whether the current state is terminal or not through a binary classification objective, and the analogy-making objective is applied to the task embedding separately.", "startOffset": 8, "endOffset": 100}, {"referenceID": 29, "context": "We developed a 3D visual environment using Minecraft based on Oh et al. (2016) as shown in Figure 1.", "startOffset": 62, "endOffset": 79}, {"referenceID": 16, "context": "The network architecture of the parameterized skill consists of 4 convolution layers and one LSTM (Hochreiter and Schmidhuber, 1997) layer.", "startOffset": 98, "endOffset": 132}, {"referenceID": 16, "context": "The network architecture of the parameterized skill consists of 4 convolution layers and one LSTM (Hochreiter and Schmidhuber, 1997) layer. We conducted curriculum training by changing the size of the world, the density of object and walls according to the agent\u2019s success rate. We implemented actor-critic method with 16 CPU threads based on Sukhbaatar et al. (2015). The parameters are updated after 8 episodes for each thread.", "startOffset": 99, "endOffset": 368}, {"referenceID": 44, "context": "Since instructions should be executed sequentially, we use a location-based memory addressing mechanism (Zaremba and Sutskever, 2015; Graves et al., 2014) to manage the instruction pointer.", "startOffset": 104, "endOffset": 154}, {"referenceID": 13, "context": "Since instructions should be executed sequentially, we use a location-based memory addressing mechanism (Zaremba and Sutskever, 2015; Graves et al., 2014) to manage the instruction pointer.", "startOffset": 104, "endOffset": 154}, {"referenceID": 40, "context": "Instead, having temporallyextended actions can be useful for dealing with delayed reward by operating at a larger time-scale (Sutton et al., 1999).", "startOffset": 125, "endOffset": 146}, {"referenceID": 21, "context": "The idea of learning the time-scale of a recurrent neural network is closely related to hierarchical RNN approaches (Koutnik et al., 2014; Chung et al., 2017) where different groups of recurrent hidden units operate at different time-scales to capture both long-term and short-term temporal information.", "startOffset": 116, "endOffset": 158}, {"referenceID": 7, "context": "The idea of learning the time-scale of a recurrent neural network is closely related to hierarchical RNN approaches (Koutnik et al., 2014; Chung et al., 2017) where different groups of recurrent hidden units operate at different time-scales to capture both long-term and short-term temporal information.", "startOffset": 116, "endOffset": 158}, {"referenceID": 22, "context": "This approach is similar to recent hierarchical deep RL methods (Kulkarni et al., 2016; Tessler et al., 2017).", "startOffset": 64, "endOffset": 109}, {"referenceID": 43, "context": "This approach is similar to recent hierarchical deep RL methods (Kulkarni et al., 2016; Tessler et al., 2017).", "startOffset": 64, "endOffset": 109}], "year": 2017, "abstractText": "As a step towards developing zero-shot task generalization capabilities in reinforcement learning (RL), we introduce a new RL problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instructions and to longer sequences of instructions. For generalization over unseen instructions, we propose a new objective which encourages learning correspondences between similar subtasks by making analogies. For generalization over sequential instructions, we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. To deal with delayed reward, we propose a new neural architecture in the meta controller that learns when to update the subtask, which makes learning more efficient. Experimental results on a stochastic 3D domain show that the proposed ideas are crucial for generalization to longer instructions as well as unseen instructions.", "creator": "LaTeX with hyperref package"}}}