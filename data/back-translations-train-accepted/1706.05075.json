{"id": "1706.05075", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2017", "title": "Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme", "abstract": "Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What's more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.", "histories": [["v1", "Wed, 7 Jun 2017 03:14:23 GMT  (965kb,D)", "http://arxiv.org/abs/1706.05075v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["suncong zheng", "feng wang", "hongyun bao", "yuexing hao", "peng zhou", "bo xu"], "accepted": true, "id": "1706.05075"}, "pdf": {"name": "1706.05075.pdf", "metadata": {"source": "CRF", "title": "Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme", "authors": ["Suncong Zheng", "Feng Wang", "Hongyun Bao", "Yuexing Hao", "Peng", "Bo Xu"], "emails": ["peng.zhou@ia.ac.cn", "xubo@ia.ac.cn"], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Related Works", "text": "Entities and Relationships Extraction is an important step in constructing a knowledge base that can be used for many NLP tasks. Two main frameworks have been widely used to solve the problem of entity extraction and its relationships, one is the Pipelined Method and the other is the Common Learning Method.The Pipelined Method treats this task as two separate tasks, i.e., called Entity Recognition (Nadeau and Sekine, 2007) and Relation Classification (RC) (Rink, 2010).The classic NER models are linear statistical models, such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Passos et al., 2014; Luo et al., 2015).Recently, several neural network architectures (Chiu and Nichols, 2015; Huang et al., JP., 2015; Lample et al., 201yal., Versatile (CRF) have been successfully applied to the working models."}, {"heading": "3 Method", "text": "We propose a novel labeling scheme and an end-to-end model with distorted objective function to jointly extract entities and their relationships. In this section, we first present how to change the extraction problem to a labeling problem based on our labeling method, and then explain the model we use to extract the results."}, {"heading": "3.1 The Tagging Scheme", "text": "Figure 2 is an example of how the results are tagged. Each word is given a label that helps to extract the results. Tag \"O\" stands for the \"Other\" tag, which means that the corresponding word is independent of the extracted results. In addition to \"O,\" the other tags consist of three parts: the word position in the unit, the relationship type, and the relationship type role. We use the \"BIES\" characters (Begin, Inside, End, Single) to represent the position information of a word in the unit. \"1\" means that the word belongs to the first unit in the group of three, while \"2\" belongs to the second unit that stands behind the relationship type. \""}, {"heading": "3.2 From Tag Sequence To Extracted Results", "text": "From the tag sequence in Figure 2, we know that \"Trump\" and \"USA\" share the same relationship type \"Country-President,\" \"Apple Inc\" and \"Steven Paul Jobs\" share the same relationship type \"Company-Founder.\" We combine units with the same relationship type to a triplet to obtain the end result. Accordingly, \"Trump\" and \"United States\" can be combined to form a triplet whose relationship type is \"Country-President.\" Since the relationship type of \"Trump\" \"\" 2 \"and\" United States \"\" is \"1,\" the end result is {United States, Country-President, Trump}. The same applies to {Apple Inc, Company-Founder, Steven Paul Jobs}. If a sentence contains two or more triplets with the same relationship type, we combine all two units to form a triplet based on the closest principle. If, for example, the relationship type \"Country-President\" United States \"in Figure 2,\" then there are jobs in the relationship between \"We\" and the United States Inc., \"then we combine all two units to form a relationship based on the closest principle."}, {"heading": "3.3 The End-to-end Model", "text": "In this thesis, we examine an end-toend model to generate the tag sequence, as shown in Figure 3. It contains a bi-directional LSTM layer to encode the input sentence and an LSTM-based decoding layer with biased loss. Biased loss can increase the relevance of entity tags. Bi-LSTM encoding layer, in which the Bi-LSTM encoding layer has demonstrated the effectiveness of capturing the semantic information of each word, contains the relevance of entity tags. The word \"embedding\" layer converts the word with 1-hot representation to an embedding vector. Hence, a sequence of words can be represented as W = wt."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experimental setting", "text": "To evaluate the performance of our methods, we use the NYT 2 public datasets produced by remote monitoring method (Ren et al., 2017). A large amount of training data can be obtained using remote control methods without manual labeling. While the test set is labeled manually to ensure its quality, the training data contains a total of 353k triplets, and the test set contains three, 880 triplets. Also, the size of the relation we use is not standard Precision (Prec), Recall (Rec) and F1 Score to evaluate the results. Unlike classical methods, our method can extract triplets without knowing the information of the entity types. In other words, we have not used the label of the entity type types to train the model, so we do not need to consider the entity types in the evaluation."}, {"heading": "4.2 Experimental Results", "text": "We can see that our method, LSTM-LSTM bias, outperforms all other methods in the F1 score and achieves a 3% improvement in F1 over the best method CoType (Ren et al., 2017). It also shows the effectiveness of our proposed method. Furthermore, we can see from Table 1 that the common extraction methods are better than pipelines and the tagging methods are better than most of the common extraction methods. It also confirms the validity of our tagging scheme for the task of joint extraction of entities and relations. Compared to traditional methods, the precision of the end-to-end models is significantly improved. But only LSTM-LSTM bias can be better to balance precision and memory."}, {"heading": "5 Analysis and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Error Analysis", "text": "In this paper, we focus on extracting triplets consisting of two entities and one relationship. Table 1 has shown the predicted results of the task. It treats a triplet as correct only if the relationship type and the head offsets of two corresponding entities are both correct. To find out the factors influencing the results of end-to-end models, we analyze the performance in predicting each element in the triplets, as shown in Table 2. E1 and E2 represent the performance in predicting each entity. If the head offset of the first entity is correct, then the instance of E1 is correct, the same as E2. Regardless of the relationship type, if the head offsets of two correct entities are both correct, the instance of (E1, E2) is correct. As shown in Table 2, (E1, E2), the instance of E1 has a higher precision on its two and not (E2) results."}, {"heading": "5.2 Analysis of Biased Loss", "text": "In contrast to LSTM-CRF and LSTM-LSTM, our approach tends to use relational labels to improve the connections between entities. To further analyze the effect of the bias objective function, we visualize the ratio of predicted individual entities for each end-to-end method as Figure 4. The individual entities refer to those that cannot find their corresponding entities. Figure 4 shows whether it is E1 or E2, our method can obtain a relatively low ratio of each entity. It means that our method can effectively match two entities compared to LSTM-CRF and LSTM-LSTM that pay little attention to the relational tag. In addition, we also change the bias parameter \u03b1 from 1 to 20, and the predicted results are shown in Figure 5. If \u03b1 is too large, it will affect the accuracy of the prediction, and if \u03b1 is too small, the memory will decrease. If \u03b1 = STM-LSTM-LSTM-LSTM can match the best values, then the memory will decrease."}, {"heading": "5.3 Case Study", "text": "In this section, we look at the prediction results of end-to-end methods and then select several representative examples to illustrate the advantages and disadvantages of the methods, as shown in Table 3. Each example contains three rows, the first row being the gold standard, the second and third row being the extracted results of the models LSTM-LSTM-LSTM bias or LSTM-LSTM bias, respectively. S1 represents the situation that the distance between the two interconnected units is far apart, which is more difficult to detect their relationships. Compared to LSTM-LSTM bias, LSTM lenses use functions that enhance the relevance between units. Therefore, in this example, LSTM-LSTMBias can extract two related roles, while LSTMLSTM can only be considered one unit of \"Florida\" and the unit of \"STM\" as a unit of \"STM.\""}, {"heading": "6 Conclusion", "text": "In this paper, we propose a novel tagging scheme and examine the end-to-end models for collectively extracting entities and relationships. Experimental results show the effectiveness of our proposed method. However, it still has shortcomings in identifying the overlapping relationships. In the future work, we will replace the Softmax function at the output level with several classifiers, so that a word can have multiple tags. In this way, a word can appear in several triple results, which can solve the problem of overlapping relationships. Although our model can enhance the effect of entity tags, the connection between two corresponding entities needs to be refined in the next work."}, {"heading": "Acknowledgments", "text": "We thank Xiang Ren for details of the records and helpful discussions. This work is also supported by the National High Technology Research and Development Program of China (863 Program) (Grant No. 2015AA015402), the National Natural Science Foundation of China (No. 61602479) and the NSFC Project 61501463."}], "references": [{"title": "Open information extraction from the web", "author": ["Michele Banko", "Michael J Cafarella", "Stephen Soderland", "Matthew Broadhead", "Oren Etzioni."], "venue": "IJCAI. volume 7, pages 2670\u20132676.", "citeRegEx": "Banko et al\\.,? 2007", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["Jason PC Chiu", "Eric Nichols."], "venue": "Processings of Transactions of the Association for Computational Linguistics.", "citeRegEx": "Chiu and Nichols.,? 2015", "shortCiteRegEx": "Chiu and Nichols.", "year": 2015}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["C\u0131cero Nogueira et al. dos Santos."], "venue": "Proceedings of the 53th ACL international conference. volume 1, pages 626\u2013634.", "citeRegEx": "Santos.,? 2015", "shortCiteRegEx": "Santos.", "year": 2015}, {"title": "Improved relation extraction with feature-rich compositional embedding models", "author": ["Matthew R Gormley", "Mo Yu", "Mark Dredze."], "venue": "Proceedings of the EMNLP.", "citeRegEx": "Gormley et al\\.,? 2015", "shortCiteRegEx": "Gormley et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Knowledgebased weak supervision for information extraction of overlapping relations", "author": ["Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Compu-", "citeRegEx": "Hoffmann et al\\.,? 2011", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Zhiheng Huang", "Wei Xu", "Kai Yu."], "venue": "arXiv preprint arXiv:1508.01991 .", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP. volume 3, page 413.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations", "author": ["Nanda Kambhatla."], "venue": "Proceedings of the 43th ACL international conference. page 22.", "citeRegEx": "Kambhatla.,? 2004", "shortCiteRegEx": "Kambhatla.", "year": 2004}, {"title": "Investigating lstms for joint extraction of opinion entities and relations", "author": ["Arzoo Katiyar", "Claire Cardie."], "venue": "Proceedings of the 54th ACL international conference.", "citeRegEx": "Katiyar and Cardie.,? 2016", "shortCiteRegEx": "Katiyar and Cardie.", "year": 2016}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando Pereira"], "venue": "In Proceedings of the eighteenth international conference on machine learning, ICML", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Neural architectures for named entity recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."], "venue": "Proceedings of the NAACL international conference.", "citeRegEx": "Lample et al\\.,? 2016", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Incremental joint extraction of entity mentions and relations", "author": ["Qi Li", "Heng Ji."], "venue": "Proceedings of the 52rd Annual Meeting of the Association for Computational Linguistics. pages 402\u2013412.", "citeRegEx": "Li and Ji.,? 2014", "shortCiteRegEx": "Li and Ji.", "year": 2014}, {"title": "Joint entity recognition and disambiguation", "author": ["Gang Luo", "Xiaojiang Huang", "Chin-Yew Lin", "Zaiqing Nie."], "venue": "Conference on Empirical Methods in Natural Language Processing. pages 879\u2013888.", "citeRegEx": "Luo et al\\.,? 2015", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL. Association for Computational Linguistics,", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "End-to-end relation extraction using lstms on sequences and tree structures", "author": ["Makoto Miwa", "Mohit Bansal."], "venue": "Proceedings of the 54rd Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Miwa and Bansal.,? 2016", "shortCiteRegEx": "Miwa and Bansal.", "year": 2016}, {"title": "Modeling joint entity and relation extraction with table representation", "author": ["Makoto Miwa", "Yutaka Sasaki."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. pages 1858\u20131869.", "citeRegEx": "Miwa and Sasaki.,? 2014", "shortCiteRegEx": "Miwa and Sasaki.", "year": 2014}, {"title": "A survey of named entity recognition and classification", "author": ["David Nadeau", "Satoshi Sekine."], "venue": "Lingvisticae Investigationes 30(1):3\u201326.", "citeRegEx": "Nadeau and Sekine.,? 2007", "shortCiteRegEx": "Nadeau and Sekine.", "year": 2007}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["Alexandre Passos", "Vineet Kumar", "Andrew McCallum."], "venue": "International Conference on Computational Linguistics. pages 78\u201386.", "citeRegEx": "Passos et al\\.,? 2014", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Cotype: Joint extraction of typed entities and relations with knowledge bases", "author": ["Xiang Ren", "Zeqiu Wu", "Wenqi He", "Meng Qu", "Clare R Voss", "Heng Ji", "Tarek F Abdelzaher", "Jiawei Han."], "venue": "Proceedings of the 26th WWW international conference.", "citeRegEx": "Ren et al\\.,? 2017", "shortCiteRegEx": "Ren et al\\.", "year": 2017}, {"title": "Utd: Classifying semantic relations by combining lexical and semantic resources", "author": ["Bryan et al. Rink."], "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation. pages 256\u2013259.", "citeRegEx": "Rink.,? 2010", "shortCiteRegEx": "Rink.", "year": 2010}, {"title": "Joint inference of entities, relations, and coreference", "author": ["Sameer Singh", "Sebastian Riedel", "Brian Martin", "Jiaping Zheng", "Andrew McCallum."], "venue": "Proceedings of the 2013 workshop on Automated knowledge base construction. ACM, pages 1\u20136.", "citeRegEx": "Singh et al\\.,? 2013", "shortCiteRegEx": "Singh et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Line: Large-scale information network embedding", "author": ["Jian Tang", "Meng Qu", "Mingzhe Wang", "Ming Zhang", "Jun Yan", "Qiaozhu Mei."], "venue": "Proceedings of the 24th International Conference on World Wide Web. ACM, pages 1067\u20131077.", "citeRegEx": "Tang et al\\.,? 2015", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Supertagging with lstms", "author": ["Ashish Vaswani", "Yonatan Bisk", "Kenji Sagae", "Ryan Musa."], "venue": "Proceedings of the NAACL international conference. pages 232\u2013237.", "citeRegEx": "Vaswani et al\\.,? 2016", "shortCiteRegEx": "Vaswani et al\\.", "year": 2016}, {"title": "Semantic relation classification via convolutional neural networks with simple negative sampling", "author": ["Kun et al. Xu."], "venue": "Proceedings of the EMNLP.", "citeRegEx": "Xu.,? 2015a", "shortCiteRegEx": "Xu.", "year": 2015}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Yan et al. Xu."], "venue": "Proceedings of EMNLP international conference.", "citeRegEx": "Xu.,? 2015b", "shortCiteRegEx": "Xu.", "year": 2015}, {"title": "Joint inference for fine-grained opinion extraction", "author": ["Bishan Yang", "Claire Cardie."], "venue": "Proceedings of the 51rd Annual Meeting of the Association for Computational Linguistics. pages 1640\u20131649.", "citeRegEx": "Yang and Cardie.,? 2013", "shortCiteRegEx": "Yang and Cardie.", "year": 2013}, {"title": "Jointly identifying entities and extracting relations in encyclopedia text via a graphical model approach", "author": ["Xiaofeng Yu", "Wai Lam."], "venue": "Proceedings of the 21th COLING international conference. pages 1399\u20131407.", "citeRegEx": "Yu and Lam.,? 2010", "shortCiteRegEx": "Yu and Lam.", "year": 2010}, {"title": "Relation classification via convolutional deep neural network", "author": ["Daojian et al. Zeng."], "venue": "Proceedings of the 25th COLING international conference. pages 2335\u20132344.", "citeRegEx": "Zeng.,? 2014", "shortCiteRegEx": "Zeng.", "year": 2014}, {"title": "Neural models for sequence chunking", "author": ["Feifei Zhai", "Saloni Potdar", "Bing Xiang", "Bowen Zhou."], "venue": "Proceedings of the AAAI international conference.", "citeRegEx": "Zhai et al\\.,? 2017", "shortCiteRegEx": "Zhai et al\\.", "year": 2017}, {"title": "A neural network framework for relation extraction: Learning entity semantic and relation pattern", "author": ["Suncong Zheng", "Jiaming Xu", "Peng Zhou", "Hongyun Bao", "Zhenyu Qi", "Bo Xu."], "venue": "KnowledgeBased Systems 114:12\u201323.", "citeRegEx": "Zheng et al\\.,? 2016", "shortCiteRegEx": "Zheng et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Different from open information extraction (Open IE) (Banko et al., 2007) whose relation words are extracted from the given sentence, in this task, relation words are extracted from a predefined relation set which may not appear in the given sentence.", "startOffset": 53, "endOffset": 73}, {"referenceID": 18, "context": ", extracting the entities (Nadeau and Sekine, 2007) first and then recognizing their relations (Rink, 2010).", "startOffset": 26, "endOffset": 51}, {"referenceID": 21, "context": ", extracting the entities (Nadeau and Sekine, 2007) first and then recognizing their relations (Rink, 2010).", "startOffset": 95, "endOffset": 107}, {"referenceID": 12, "context": "The results of entity recognition may affect the performance of relation classification and lead to erroneous delivery (Li and Ji, 2014).", "startOffset": 119, "endOffset": 136}, {"referenceID": 12, "context": "However, most existing joint methods are feature-based structured systems (Li and Ji, 2014; Miwa and Sasaki, 2014; Yu and Lam, 2010; Ren et al., 2017).", "startOffset": 74, "endOffset": 150}, {"referenceID": 17, "context": "However, most existing joint methods are feature-based structured systems (Li and Ji, 2014; Miwa and Sasaki, 2014; Yu and Lam, 2010; Ren et al., 2017).", "startOffset": 74, "endOffset": 150}, {"referenceID": 29, "context": "However, most existing joint methods are feature-based structured systems (Li and Ji, 2014; Miwa and Sasaki, 2014; Yu and Lam, 2010; Ren et al., 2017).", "startOffset": 74, "endOffset": 150}, {"referenceID": 20, "context": "However, most existing joint methods are feature-based structured systems (Li and Ji, 2014; Miwa and Sasaki, 2014; Yu and Lam, 2010; Ren et al., 2017).", "startOffset": 74, "endOffset": 150}, {"referenceID": 16, "context": "In order to reduce the manual work in feature extraction, recently, (Miwa and Bansal, 2016) presents a neural network-based method for the end-to-end entities and relations extraction.", "startOffset": 68, "endOffset": 91}, {"referenceID": 4, "context": "Recently, end-to-end models based on LSTM (Hochreiter and Schmidhuber, 1997) have been successfully applied to various tagging tasks: Named Entity Recognition (Lample et al.", "startOffset": 42, "endOffset": 76}, {"referenceID": 11, "context": "Recently, end-to-end models based on LSTM (Hochreiter and Schmidhuber, 1997) have been successfully applied to various tagging tasks: Named Entity Recognition (Lample et al., 2016), CCG Supertagging (Vaswani et al.", "startOffset": 159, "endOffset": 180}, {"referenceID": 25, "context": ", 2016), CCG Supertagging (Vaswani et al., 2016), Chunking (Zhai et al.", "startOffset": 26, "endOffset": 48}, {"referenceID": 31, "context": ", 2016), Chunking (Zhai et al., 2017) et al.", "startOffset": 18, "endOffset": 37}, {"referenceID": 20, "context": "Therefore, we conduct experiments on a public dataset1 which is produced by distant supervision method (Ren et al., 2017) to validate our approach.", "startOffset": 103, "endOffset": 121}, {"referenceID": 18, "context": ", named entity recognition (NER) (Nadeau and Sekine, 2007) and relation classification (RC) (Rink, 2010).", "startOffset": 33, "endOffset": 58}, {"referenceID": 21, "context": ", named entity recognition (NER) (Nadeau and Sekine, 2007) and relation classification (RC) (Rink, 2010).", "startOffset": 92, "endOffset": 104}, {"referenceID": 19, "context": "Classical NER models are linear statistical models, such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Passos et al., 2014; Luo et al., 2015).", "startOffset": 123, "endOffset": 162}, {"referenceID": 13, "context": "Classical NER models are linear statistical models, such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Passos et al., 2014; Luo et al., 2015).", "startOffset": 123, "endOffset": 162}, {"referenceID": 1, "context": "Recently, several neural network architectures (Chiu and Nichols, 2015; Huang et al., 2015; Lample et al., 2016) have been successfully applied to NER, which is regarded as a sequential token tagging task.", "startOffset": 47, "endOffset": 112}, {"referenceID": 6, "context": "Recently, several neural network architectures (Chiu and Nichols, 2015; Huang et al., 2015; Lample et al., 2016) have been successfully applied to NER, which is regarded as a sequential token tagging task.", "startOffset": 47, "endOffset": 112}, {"referenceID": 11, "context": "Recently, several neural network architectures (Chiu and Nichols, 2015; Huang et al., 2015; Lample et al., 2016) have been successfully applied to NER, which is regarded as a sequential token tagging task.", "startOffset": 47, "endOffset": 112}, {"referenceID": 21, "context": "Existing methods for relation classification can also be divided into handcrafted feature based methods (Rink, 2010; Kambhatla, 2004) and neural network based methods (Xu, 2015a; Zheng et al.", "startOffset": 104, "endOffset": 133}, {"referenceID": 8, "context": "Existing methods for relation classification can also be divided into handcrafted feature based methods (Rink, 2010; Kambhatla, 2004) and neural network based methods (Xu, 2015a; Zheng et al.", "startOffset": 104, "endOffset": 133}, {"referenceID": 26, "context": "Existing methods for relation classification can also be divided into handcrafted feature based methods (Rink, 2010; Kambhatla, 2004) and neural network based methods (Xu, 2015a; Zheng et al., 2016; Zeng, 2014; Xu, 2015b; dos Santos, 2015).", "startOffset": 167, "endOffset": 239}, {"referenceID": 32, "context": "Existing methods for relation classification can also be divided into handcrafted feature based methods (Rink, 2010; Kambhatla, 2004) and neural network based methods (Xu, 2015a; Zheng et al., 2016; Zeng, 2014; Xu, 2015b; dos Santos, 2015).", "startOffset": 167, "endOffset": 239}, {"referenceID": 30, "context": "Existing methods for relation classification can also be divided into handcrafted feature based methods (Rink, 2010; Kambhatla, 2004) and neural network based methods (Xu, 2015a; Zheng et al., 2016; Zeng, 2014; Xu, 2015b; dos Santos, 2015).", "startOffset": 167, "endOffset": 239}, {"referenceID": 27, "context": "Existing methods for relation classification can also be divided into handcrafted feature based methods (Rink, 2010; Kambhatla, 2004) and neural network based methods (Xu, 2015a; Zheng et al., 2016; Zeng, 2014; Xu, 2015b; dos Santos, 2015).", "startOffset": 167, "endOffset": 239}, {"referenceID": 20, "context": "Most of the joint methods are feature-based structured systems (Ren et al., 2017; Yang and Cardie, 2013; Singh et al., 2013; Miwa and Sasaki, 2014; Li and Ji, 2014).", "startOffset": 63, "endOffset": 164}, {"referenceID": 28, "context": "Most of the joint methods are feature-based structured systems (Ren et al., 2017; Yang and Cardie, 2013; Singh et al., 2013; Miwa and Sasaki, 2014; Li and Ji, 2014).", "startOffset": 63, "endOffset": 164}, {"referenceID": 22, "context": "Most of the joint methods are feature-based structured systems (Ren et al., 2017; Yang and Cardie, 2013; Singh et al., 2013; Miwa and Sasaki, 2014; Li and Ji, 2014).", "startOffset": 63, "endOffset": 164}, {"referenceID": 17, "context": "Most of the joint methods are feature-based structured systems (Ren et al., 2017; Yang and Cardie, 2013; Singh et al., 2013; Miwa and Sasaki, 2014; Li and Ji, 2014).", "startOffset": 63, "endOffset": 164}, {"referenceID": 12, "context": "Most of the joint methods are feature-based structured systems (Ren et al., 2017; Yang and Cardie, 2013; Singh et al., 2013; Miwa and Sasaki, 2014; Li and Ji, 2014).", "startOffset": 63, "endOffset": 164}, {"referenceID": 16, "context": "Recently, (Miwa and Bansal, 2016) uses a LSTMbased model to extract entities and relations, which can reduce the manual work.", "startOffset": 10, "endOffset": 33}, {"referenceID": 7, "context": "It is widely used in machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) and sequence tagging tasks (Lample et al.", "startOffset": 41, "endOffset": 97}, {"referenceID": 23, "context": "It is widely used in machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) and sequence tagging tasks (Lample et al.", "startOffset": 41, "endOffset": 97}, {"referenceID": 11, "context": ", 2014) and sequence tagging tasks (Lample et al., 2016; Vaswani et al., 2016).", "startOffset": 35, "endOffset": 78}, {"referenceID": 25, "context": ", 2014) and sequence tagging tasks (Lample et al., 2016; Vaswani et al., 2016).", "startOffset": 35, "endOffset": 78}, {"referenceID": 11, "context": "For examples, (Lample et al., 2016) use a CRF layers to decode the tag sequence, while", "startOffset": 14, "endOffset": 35}, {"referenceID": 25, "context": "(Vaswani et al., 2016; Katiyar and Cardie, 2016) apply LSTM layer to produce the tag sequence.", "startOffset": 0, "endOffset": 48}, {"referenceID": 9, "context": "(Vaswani et al., 2016; Katiyar and Cardie, 2016) apply LSTM layer to produce the tag sequence.", "startOffset": 0, "endOffset": 48}, {"referenceID": 20, "context": "Dataset To evaluate the performance of our methods, we use the public dataset NYT 2 which is produced by distant supervision method (Ren et al., 2017).", "startOffset": 132, "endOffset": 150}, {"referenceID": 20, "context": "Besides, the groundtruth relation mentions are given and \u201cNone\u201d label is excluded as (Ren et al., 2017; Li and Ji, 2014; Miwa and Bansal, 2016) did.", "startOffset": 85, "endOffset": 143}, {"referenceID": 12, "context": "Besides, the groundtruth relation mentions are given and \u201cNone\u201d label is excluded as (Ren et al., 2017; Li and Ji, 2014; Miwa and Bansal, 2016) did.", "startOffset": 85, "endOffset": 143}, {"referenceID": 16, "context": "Besides, the groundtruth relation mentions are given and \u201cNone\u201d label is excluded as (Ren et al., 2017; Li and Ji, 2014; Miwa and Bansal, 2016) did.", "startOffset": 85, "endOffset": 143}, {"referenceID": 20, "context": "We create a validation set by randomly sampling 10% data from test set and use the remaining data as evaluation based on (Ren et al., 2017)\u2019s suggestion.", "startOffset": 121, "endOffset": 139}, {"referenceID": 14, "context": "The word embeddings used in the encoding part are initialed by running word2vec3 (Mikolov et al., 2013) on NYT training corpus.", "startOffset": 81, "endOffset": 103}, {"referenceID": 20, "context": "Details of the data can be found in Ren\u2019s(Ren et al., 2017) paper.", "startOffset": 41, "endOffset": 59}, {"referenceID": 20, "context": "For the pipelined methods, we follow (Ren et al., 2017)\u2019s settings: The NER results are obtained by CoType (Ren et al.", "startOffset": 37, "endOffset": 55}, {"referenceID": 20, "context": ", 2017)\u2019s settings: The NER results are obtained by CoType (Ren et al., 2017) then several classical relation classification methods are applied to detect the relations.", "startOffset": 59, "endOffset": 77}, {"referenceID": 15, "context": "These methods are: (1) DS-logistic (Mintz et al., 2009) is a distant supervised and feature based method, which combines the advantages of supervised IE and unsupervised IE features; (2) LINE (Tang et al.", "startOffset": 35, "endOffset": 55}, {"referenceID": 24, "context": ", 2009) is a distant supervised and feature based method, which combines the advantages of supervised IE and unsupervised IE features; (2) LINE (Tang et al., 2015) is a network embedding method, which is suitable for arbitrary types of information networks; (3) FCM (Gormley et al.", "startOffset": 144, "endOffset": 163}, {"referenceID": 3, "context": ", 2015) is a network embedding method, which is suitable for arbitrary types of information networks; (3) FCM (Gormley et al., 2015) is a compositional model that combines lexicalized linguistic context and word embeddings for relation extraction.", "startOffset": 110, "endOffset": 132}, {"referenceID": 12, "context": "The jointly extracting methods used in this paper are listed as follows: (4) DS-Joint (Li and Ji, 2014) is a supervised method, which jointly extracts entities and relations using structured perceptron on human-annotated dataset; (5) MultiR (Hoffmann et al.", "startOffset": 86, "endOffset": 103}, {"referenceID": 5, "context": "The jointly extracting methods used in this paper are listed as follows: (4) DS-Joint (Li and Ji, 2014) is a supervised method, which jointly extracts entities and relations using structured perceptron on human-annotated dataset; (5) MultiR (Hoffmann et al., 2011) is a typical distant supervised method based on multi-instance learning algorithms to combat the noisy training data; (6) CoType (Ren et al.", "startOffset": 241, "endOffset": 264}, {"referenceID": 20, "context": ", 2011) is a typical distant supervised method based on multi-instance learning algorithms to combat the noisy training data; (6) CoType (Ren et al., 2017) is a domain independent framework by jointly embedding entity mentions, relation mentions, text features and type labels into meaningful representations.", "startOffset": 137, "endOffset": 155}, {"referenceID": 11, "context": "In addition, we also compare our method with two classical end-to-end tagging models: LSTMCRF (Lample et al., 2016) and LSTM-LSTM (Vaswani et al.", "startOffset": 94, "endOffset": 115}, {"referenceID": 25, "context": ", 2016) and LSTM-LSTM (Vaswani et al., 2016).", "startOffset": 22, "endOffset": 44}, {"referenceID": 20, "context": "It can be seen that our method, LSTM-LSTM-Bias, outperforms all other methods in F1 score and achieves a 3% improvement in F1 over the best method CoType (Ren et al., 2017).", "startOffset": 154, "endOffset": 172}, {"referenceID": 10, "context": "Because, LSTM is capable of learning long-term dependencies and CRF (Lafferty et al., 2001) is good at capturing the joint probability of the entire sequence of labels.", "startOffset": 68, "endOffset": 91}], "year": 2017, "abstractText": "Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem. Then, based on our tagging scheme, we study different end-toend models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What\u2019s more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.", "creator": "LaTeX with hyperref package"}}}