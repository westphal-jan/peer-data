{"id": "1706.05125", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2017", "title": "Deal or No Deal? End-to-End Learning for Negotiation Dialogues", "abstract": "Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other's reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available (", "histories": [["v1", "Fri, 16 Jun 2017 01:26:09 GMT  (174kb)", "http://arxiv.org/abs/1706.05125v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["mike lewis", "denis yarats", "yann n dauphin", "devi parikh", "dhruv batra"], "accepted": true, "id": "1706.05125"}, "pdf": {"name": "1706.05125.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Mike Lewis", "Denis Yarats", "Yann N. Dauphin", "Devi Parikh", "Dhruv Batra"], "emails": ["mikelewis@fb.com", "denisy@fb.com", "ynd@fb.com", "parikh@gatech.edu", "dbatra@gatech.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 6.05 125v 1 [cs.A I] 1 6Ju n20 17Cooperative settings where agents with different goals try to agree on common decisions. Negotiations require complex communication and thinking skills, but success is easy to measure, which makes this an interesting task for AI. We are collecting a large dataset of human negotiations on a multi-layered negotiation task, where agents who fail to observe each other's reward functions must reach an agreement (or deal) on dialogue in natural language. For the first time, we show that it is possible to train end-to-end negotiation models that need to learn both linguistic and argumentative skills without annotated dialogue states. We also introduce dialogue rollouts where the model plans ahead by simulating possible full continuations of the conversation, and find that this technique dramatically improves performance."}, {"heading": "1 Introduction", "text": "We need to get in touch with others who have different objectives in order to achieve their objectives."}, {"heading": "2 Data Collection", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Overview", "text": "To provide complete training for negotiators, we first develop a new negotiating task and curate a data set of human-human dialogues for this task. This task and the data set follow our proposed general framework for studying semi-cooperative dialogue. First, each agent is shown an input specifying a space of possible actions and a reward function that will evaluate the outcome of the negotiations. Agents then take turns sending messages in natural language or selecting that a final decision has been made. When an agent selects this agreement, both agents independently issue what they believe was the agreed decision. If conflicting decisions are made, neither agent will receive a reward."}, {"heading": "2.2 Task", "text": "Our task is an example of multi-issue negotiation (Fershtman, 1990) and is based on DeVault et al. (2015). Two agents are each shown the same set of items and instructed to divide them so that each item is assigned to an agent. Each agent is assigned a different randomly generated value function, giving a non-negative value for each item. The value functions are so limited that: (1) the total value for a user of all items is 10; (2) each item has a non-zero value for at least one user; and (3) some items have a non-zero value for both users. These limitations force that it is not possible for both agents to obtain a maximum score, and that no item is worthless for both agents, so that the negotiation is competitive. After 10 rounds, we allow the agents to complete negotiations without agreement, which is worth 0 points for both users.We use 3 item types (in total, 7 and 1), showing between Balls and Balls."}, {"heading": "2.3 Data Collection", "text": "Workers received $0.15 per dialogue, with a bonus of $0.05 for maximum values. We used only workers from the United States with a 95% approval rating and at least 5000 previous HITs. Our data collection interface was adapted to that of Das et al. (2016). We collected a total of 5,808 dialogs based on 2,236 unique scenarios (in which one scenario represents the available items and values for the two users). We provided a test set of 252 scenarios (526 dialogs)."}, {"heading": "3 Likelihood Model", "text": "We propose a simple but effective basic model for the interlocutor, in which a sequence-to-sequence model is trained to generate the complete dialogue based on the input of an agent."}, {"heading": "3.1 Data Representation", "text": "Each dialog is converted into two training examples that show the entire conversation from the perspective of each agent. Examples differ in terms of input targets, output choices, and whether utterances have been read or writed.Training examples include an input target g that specifies the available elements and their values, a dialog x, and an output decision o that specifies which elements each agent will receive. Specifically, we present g as a list of six integers that correspond to the number and value of each of the three item types. Dialog x is a list of symbols x0.. T that contain the rotations of each agent associated with symbols that indicate whether a move has been written by the agent or his partner, and that end in a special symbol that indicates that an agent has made an agreement. Output o consists of six integers that describe how many of the three item types are assigned to each agent. See Figure 2."}, {"heading": "3.2 Supervised Learning", "text": "We train a sequence-to-sequence network to generate a perspective of the dialog depending on the input targets of the agent (Figure 3a). The model uses 4 recurring neural networks implemented as GRUs (Cho et al., 2014): GRUw, GRUg, GRU \u2212 o, and GRU \u2190 \u2212 o. The input targets of agent g are encoded with GRUg. We refer to the final hidden state as h g. The model then predicts each token xt from left to right due to the previous tokens and hg. At each time step GRUw takes the previous hidden state ht \u2212 1, previous tokens xt \u2212 1 (embedded in a matrix E), and the input coding hg. Conditioning on the input at each time step helps the model to learn dependencies between language and goals.ht = GRUw = GRUw (Exh, 1) \u2212."}, {"heading": "3.3 Decoding", "text": "During decoding, the model must generate an output token xt based on the dialog history x0.. t \u2212 1 and the input targets g by encoding a series of tokens that the other agent has issued until its turn (Figure 3b). The dialog ends when each agent issues a special end-of-turn token, and the model then outputs a series of choices. We select each element independently, but ensure consistency by checking that the solution is in a workable set O: o \u0445 = argmax o-of-dialog tokens (oi | x0.. T, g) (12) In our task, a solution is feasible if each element is assigned exactly to an agent. The space of solutions is small enough to be tradable."}, {"heading": "4 Goal-based Training", "text": "Instead, we explore pre-training with supervised learning, and then fine-tune against the assessment benchmarks using reinforcement. Similar two-step learning strategies have been used before (e.g. Li et al. (2016); Das et al. (2017). During reinforcement, Agent A tries to improve his parameters from conversations with another agent. While the other Agent B might be a human, in our experiments we used our tightly supervised model, which was trained to imitate humans. The second model is fixed, as we found that updating the parameters of both agents has led to divergence of human language. In fact, Agent A learns to improve by simulating conversations with the help of a substitute agent."}, {"heading": "5 Goal-based Decoding", "text": "For example, an agent may choose to accept an offer or submit a counter-offer. Instead, we examine decoding by maximizing the expected reward. We achieve this by using pTB as a future model for full dialogue and then deterministically calculating the reward. Rewards for an expression are averaged using random samples to calculate the expected future reward (Figure 4). We use a two-step process: First, we generate c candidate expressions U = u0.. c, which represent possible complete turns the broker could make by calculating the expected future reward (Figure 4)."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Training Details", "text": "We implement our models with PyTorch. All hyperparameters are selected on the basis of a development dataset, the input marks are embedded in a 64-dimensional space, while the dialog marks are embedded with 256-dimensional embedding (without pre-training). The input marks have a hidden level of size 64 and the dialog GRUw is of size 128. The output marks GRU \u2212 \u2192 o and GRU \u2190 \u2212 o both have a hidden state of size 256, the size of Hs is also 256. During the supervised training, we optimize the model with stochastic gradient departure with a minibatch size of 16, an initial learning rate of 1.0, Nesterov's dynamics with \u00b5 = 0.1 (Nesterov, 1983), and clipping gradients whose L2 standard exceeds 0.5. We train the model for 30 epochs and select the snapshot of the model with the best validability."}, {"heading": "6.2 Comparison Systems", "text": "We compare the performance of the following groups: LIKELIHOOD uses supervised training and decoding (\u00a7 3), RL is refined with goal-based self-play (\u00a7 4), ROLLOUTS uses supervised training combined with goal-based decoding by means of rollouts (\u00a7 5), and RL + ROLLOUTS uses rollouts with a basic model that is trained with reinforcement learning."}, {"heading": "6.3 Intrinsic Evaluation", "text": "The results are presented in Table 3 and show that the simple LIKELIHOOD model elicits the most human responses and that the alternative training and decoding strategies cause a deviation from human speech. However, note that this deviation does not necessarily correspond to lower quality language - it can also indicate different strategic decisions about what to say. Results in Section 6.4 show that all models could talk to humans."}, {"heading": "6.4 End-to-End Evaluation", "text": "We measure end-to-end performance in dialogues with both the probability-based agent and people based on pre-set scenarios; people are told to interact with other people just as they were during the collection of our data set (and few seemed to realize that they were talking to machines); we measure the following statistics: Result: The average score for each agent (who could be a human or a model), out of 10. Match: The percentage of dialogues in which both agents agreed to the same decision. Pareto-optimism: The percentage of Pareto-optimal solutions for agreed agreements (one solution is Pareto-optimal if the score of both agents can be improved without lowering the score of the other). Lower scores indicate inefficient negotiation outcomes. Results are shown in Table 1. First, we see that the RL and ROLLOUTS models achieve significantly better results when negotiating with the HOOL model, in particular."}, {"heading": "7 Analysis", "text": "In this section we will explore the strengths and weaknesses of our models. Target-based models negotiate harder. The RL + ROLLOUTS model has much longer dialogues with people than LIKELIHOOD, which is reflected in the lower agreement rates. People prefer this course over an uncompromising opponent. One factor that is not well captured by the simulated learning processes is that people do not enter into a deal that is reflected in the lower agreement rates."}, {"heading": "8 Related Work", "text": "Most work on goal-oriented dialogue systems assumes that state representations are anno-tated in the training data (Williams and Young, 2007; Henderson et al., 2014; Wen et al., 2016). Using state annotations allows for a cleaner separation of reasoning and natural linguistic aspects of dialogues, but our end-to-end approach makes data collection cheaper and allows tasks where it is unclear how to annotate the state. Bordes and Weston (2016) explore goal-oriented dialogues with a monitored model - we show improvements over monitored learning with goal-based training and decoding. Recently, er et al. (2017) use task-specific rules to integrate task setting and dialogue history into a more structured state representation than ours.Reinforcement learning (RL) has been used in many dialogue settings. RL has been widely used to improve dialogue managers managing transitions between dialogue states."}, {"heading": "9 Conclusion", "text": "We have compiled a large dataset of human-to-human negotiations, which contains a variety of interesting tactics. We have shown that it is possible to train dialogue agents end-to-end, but that their ability can be substantially improved by training and deciphering them to maximize their goals rather than the likelihood. There is still much potential for future work, especially in exploring other thought strategies and improving the diversity of expression without deviating from human language. We will also examine other negotiating tasks to see if models can learn to share negotiation strategies across disciplines."}, {"heading": "Acknowledgments", "text": "We would like to thank Luke Zettlemoyer and the anonymous EMNLP reviewers for their insightful comments, as well as the staff at Mechanical Turk who helped us collect data."}], "references": [{"title": "Modelling Strategic Conversation: The STAC project", "author": ["Nicholas Asher", "Alex Lascarides", "Oliver Lemon", "Markus Guhe", "Verena Rieser", "Philippe Muller", "Stergos Afantenos", "Farah Benamara", "Laure Vieu", "Pascal Denis"], "venue": "Proceedings of SemDial", "citeRegEx": "Asher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Asher et al\\.", "year": 2012}, {"title": "Evaluating Practical Negotiating Agents: Results and Analysis of the 2011", "author": ["Tim Baarslag", "Katsuhide Fujita", "Enrico H Gerding", "Koen Hindriks", "Takayuki Ito", "Nicholas R Jennings", "Catholijn Jonker", "Sarit Kraus", "Raz Lin", "Valentin Robu"], "venue": null, "citeRegEx": "Baarslag et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Baarslag et al\\.", "year": 2013}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "KyunghyunCho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning End-to-End Goal-oriented Dialog", "author": ["Antoine Bordes", "Jason Weston."], "venue": "arXiv preprint arXiv:1605.07683 .", "citeRegEx": "Bordes and Weston.,? 2016", "shortCiteRegEx": "Bordes and Weston.", "year": 2016}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.1259 .", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Strategic Dialogue Management via Deep Reinforcement Learning", "author": ["Heriberto Cuay\u00e1huitl", "Simon Keizer", "Oliver Lemon."], "venue": "arXiv preprint arXiv:1511.08099 .", "citeRegEx": "Cuay\u00e1huitl et al\\.,? 2015", "shortCiteRegEx": "Cuay\u00e1huitl et al\\.", "year": 2015}, {"title": "Visual Dialog", "author": ["Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "Jos\u00e9 MF Moura", "Devi Parikh", "Dhruv Batra."], "venue": "arXiv preprint arXiv:1611.08669 .", "citeRegEx": "Das et al\\.,? 2016", "shortCiteRegEx": "Das et al\\.", "year": 2016}, {"title": "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning", "author": ["Abhishek Das", "Satwik Kottur", "Jos\u00e9 MF Moura", "Stefan Lee", "Dhruv Batra."], "venue": "arXiv preprint arXiv:1703.06585 .", "citeRegEx": "Das et al\\.,? 2017", "shortCiteRegEx": "Das et al\\.", "year": 2017}, {"title": "Toward Natural Turn-taking in a Virtual Human Negotiation Agent", "author": ["David DeVault", "Johnathan Mell", "Jonathan Gratch."], "venue": "AAAI Spring Symposium on Turn-taking and Coordination in HumanMachine Interaction. AAAI Press, Stanford, CA.", "citeRegEx": "DeVault et al\\.,? 2015", "shortCiteRegEx": "DeVault et al\\.", "year": 2015}, {"title": "Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems", "author": ["Jesse Dodge", "Andreea Gane", "Xiang Zhang", "Antoine Bordes", "Sumit Chopra", "Alexander H. Miller", "Arthur Szlam", "Jason Weston."], "venue": "ICLR abs/1511.06931.", "citeRegEx": "Dodge et al\\.,? 2016", "shortCiteRegEx": "Dodge et al\\.", "year": 2016}, {"title": "Policy Networks with Two-stage Training for Dialogue Systems", "author": ["Mehdi Fatemi", "Layla El Asri", "Hannes Schulz", "Jing He", "Kaheer Suleman."], "venue": "arXiv preprint arXiv:1606.03152 .", "citeRegEx": "Fatemi et al\\.,? 2016", "shortCiteRegEx": "Fatemi et al\\.", "year": 2016}, {"title": "The Importance of the Agenda in Bargaining", "author": ["Chaim Fershtman."], "venue": "Games and Economic Behavior 2(3):224\u2013238.", "citeRegEx": "Fershtman.,? 1990", "shortCiteRegEx": "Fershtman.", "year": 1990}, {"title": "POMDPbased Dialogue Manager Adaptation to Extended Domains", "author": ["Milica Ga\u0161ic", "Catherine Breslin", "Matthew Henderson", "Dongho Kim", "Martin Szummer", "Blaise Thomson", "Pirros Tsiakoulis", "Steve Young."], "venue": "Proceedings of SIGDIAL.", "citeRegEx": "Ga\u0161ic et al\\.,? 2013", "shortCiteRegEx": "Ga\u0161ic et al\\.", "year": 2013}, {"title": "Learning symmetric collaborative dialogue agents with dynamic knowledge graph embeddings", "author": ["H. He", "A. Balakrishnan", "M. Eric", "P. Liang."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "He et al\\.,? 2017", "shortCiteRegEx": "He et al\\.", "year": 2017}, {"title": "The Second Dialog State Tracking Challenge", "author": ["Matthew Henderson", "Blaise Thomson", "Jason Williams."], "venue": "15th Annual Meeting of the Special Interest Group on Discourse and Dialogue. volume 263.", "citeRegEx": "Henderson et al\\.,? 2014", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Evaluating Persuasion Strategies and Deep Reinforcement Learning methods for Negotiation", "author": ["Simon Keizer", "Markus Guhe", "Heriberto Cuay\u00e1huitl", "Ioannis Efstathiou", "Klaus-Peter Engelbrecht", "Mihai Dobre", "Alexandra Lascarides", "Oliver Lemon"], "venue": null, "citeRegEx": "Keizer et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Keizer et al\\.", "year": 2017}, {"title": "Bandit based Monte-Carlo Planning", "author": ["Levente Kocsis", "Csaba Szepesv\u00e1ri."], "venue": "European conference on machine learning. Springer, pages 282\u2013293.", "citeRegEx": "Kocsis and Szepesv\u00e1ri.,? 2006", "shortCiteRegEx": "Kocsis and Szepesv\u00e1ri.", "year": 2006}, {"title": "A Diversity-promoting Objective Function for Neural Conversation Models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint arXiv:1510.03055 .", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Deep Reinforcement Learning for Dialogue Generation", "author": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Michel Galley", "Jianfeng Gao", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1606.01541 .", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V. Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau."], "venue": "In", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Learning Like a Child: Fast Novel Visual Concept Learning From Sentence Descriptions of Images", "author": ["Junhua Mao", "Xu Wei", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan L. Yuille."], "venue": "The IEEE International Conference on Computer Vision (ICCV).", "citeRegEx": "Mao et al\\.,? 2015", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Emergence of Grounded Compositional Language in Multi-Agent Populations", "author": ["Igor Mordatch", "Pieter Abbeel."], "venue": "arXiv preprint arXiv:1703.04908 .", "citeRegEx": "Mordatch and Abbeel.,? 2017", "shortCiteRegEx": "Mordatch and Abbeel.", "year": 2017}, {"title": "The Bargaining Problem", "author": ["John F Nash Jr."], "venue": "Econometrica: Journal of the Econometric Society pages 155\u2013162.", "citeRegEx": "Jr.,? 1950", "shortCiteRegEx": "Jr.", "year": 1950}, {"title": "A Method of Solving a Convex Programming Problem with Convergence Rate O (1/k2)", "author": ["Yurii Nesterov."], "venue": "Soviet Mathematics Doklady. volume 27, pages 372\u2013376.", "citeRegEx": "Nesterov.,? 1983", "shortCiteRegEx": "Nesterov.", "year": 1983}, {"title": "Sampleefficient Batch Reinforcement Learning for Dialogue Management Optimization", "author": ["Olivier Pietquin", "Matthieu Geist", "Senthilkumar Chandramohan", "Herv\u00e9 Frezza-Buet."], "venue": "ACM Trans. Speech Lang. Process. 7(3):7:1\u20137:21.", "citeRegEx": "Pietquin et al\\.,? 2011", "shortCiteRegEx": "Pietquin et al\\.", "year": 2011}, {"title": "Reinforcement Learning for Adaptive Dialogue Systems: A Datadriven Methodology for Dialogue Management and Natural Language Generation", "author": ["Verena Rieser", "Oliver Lemon."], "venue": "Springer Science & Business Media.", "citeRegEx": "Rieser and Lemon.,? 2011", "shortCiteRegEx": "Rieser and Lemon.", "year": 2011}, {"title": "Data-driven Response Generation in Social Media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 583\u2013593.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "NegoChat: A Chat-based Negotiation Agent", "author": ["Avi Rosenfeld", "Inon Zuckerman", "Erel Segal-Halevi", "Osnat Drein", "Sarit Kraus."], "venue": "Proceedings of the 2014 International Conference on Autonomous Agents andMulti-agent Systems. International Foun-", "citeRegEx": "Rosenfeld et al\\.,? 2014", "shortCiteRegEx": "Rosenfeld et al\\.", "year": 2014}, {"title": "Mastering the Game of Go with Deep Neural Networks", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System", "author": ["Satinder Singh", "Diane Litman", "Michael Kearns", "Marilyn Walker."], "venue": "Journal of Artificial Intelligence Research 16:105\u2013133.", "citeRegEx": "Singh et al\\.,? 2002", "shortCiteRegEx": "Singh et al\\.", "year": 2002}, {"title": "Development of lying to conceal a transgression: Children\u2019s control of expressive behaviour during verbal deception", "author": ["Victoria Talwar", "Kang Lee."], "venue": "International Journal of Behavioral Development 26(5):436\u2013444.", "citeRegEx": "Talwar and Lee.,? 2002", "shortCiteRegEx": "Talwar and Lee.", "year": 2002}, {"title": "Multi-party, Multiissue, Multi-strategy Negotiation for Multi-modal Virtual Agents", "author": ["David Traum", "Stacy C. Marsella", "Jonathan Gratch", "Jina Lee", "Arno Hartholt."], "venue": "Proceedings of the 8th International Conference on Intelligent Virtual Agents.", "citeRegEx": "Traum et al\\.,? 2008", "shortCiteRegEx": "Traum et al\\.", "year": 2008}, {"title": "A Neural Conversational Model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "arXiv preprint arXiv:1506.05869 .", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "A Networkbased End-to-End Trainable Task-oriented Dialogue System", "author": ["Tsung-Hsien Wen", "David Vandyke", "Nikola Mrksic", "Milica Gasic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "Steve Young."], "venue": "arXiv preprint arXiv:1604.04562 .", "citeRegEx": "Wen et al\\.,? 2016", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Partially Observable Markov Decision Processes for Spoken Dialog Systems", "author": ["Jason D Williams", "Steve Young."], "venue": "Computer Speech & Language 21(2):393\u2013422.", "citeRegEx": "Williams and Young.,? 2007", "shortCiteRegEx": "Williams and Young.", "year": 2007}, {"title": "Simple Statistical Gradientfollowing Algorithms for Connectionist Reinforcement Learning", "author": ["Ronald J Williams."], "venue": "Machine learning 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}], "referenceMentions": [{"referenceID": 31, "context": "Such dialogues contain both cooperative and adversarial elements, and require agents to understand, plan, and generate utterances to achieve their goals (Traum et al., 2008; Asher et al., 2012).", "startOffset": 153, "endOffset": 193}, {"referenceID": 0, "context": "Such dialogues contain both cooperative and adversarial elements, and require agents to understand, plan, and generate utterances to achieve their goals (Traum et al., 2008; Asher et al., 2012).", "startOffset": 153, "endOffset": 193}, {"referenceID": 30, "context": "Deceit is a complex skill that requires hypothesising the other agent\u2019s beliefs, and is learnt relatively late in child development (Talwar and Lee, 2002).", "startOffset": 132, "endOffset": 154}, {"referenceID": 11, "context": "Our task is an instance of multi issue bargaining (Fershtman, 1990), and is based on DeVault et al.", "startOffset": 50, "endOffset": 67}, {"referenceID": 8, "context": "Our task is an instance of multi issue bargaining (Fershtman, 1990), and is based on DeVault et al. (2015). Two agents are both shown the same collection of items, and instructed to divide them so that each item assigned to one agent.", "startOffset": 85, "endOffset": 107}, {"referenceID": 6, "context": "Our data collection interface was adapted from that of Das et al. (2016).", "startOffset": 55, "endOffset": 73}, {"referenceID": 4, "context": "The model uses 4 recurrent neural networks, implemented as GRUs (Cho et al., 2014): GRUw, GRUg, GRU\u2212\u2192o , and GRU\u2190\u2212o .", "startOffset": 64, "endOffset": 82}, {"referenceID": 20, "context": "The token at each time step is predicted with a softmax, which uses weight tying with the embedding matrix E (Mao et al., 2015):", "startOffset": 109, "endOffset": 127}, {"referenceID": 2, "context": "The classifiers share bidirectional GRUo and attention mechanism (Bahdanau et al., 2014) over the dialogue, and additionally conditions on the input goals.", "startOffset": 65, "endOffset": 88}, {"referenceID": 32, "context": "Unlike the Neural Conversational Model (Vinyals and Le, 2015), our approach shares all parameters for reading and generating tokens.", "startOffset": 39, "endOffset": 61}, {"referenceID": 15, "context": "Li et al. (2016); Das et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 6, "context": "(2016); Das et al. (2017)).", "startOffset": 8, "endOffset": 26}, {"referenceID": 35, "context": "t\u22121,g)[R(xt)] (14) The gradient of L \u03b8 is calculated as in REINFORCE (Williams, 1992):", "startOffset": 69, "endOffset": 85}, {"referenceID": 23, "context": "1 (Nesterov, 1983), and clipping gradients whose L2 norm exceeds 0.", "startOffset": 2, "endOffset": 18}, {"referenceID": 34, "context": "tated in the training data (Williams and Young, 2007; Henderson et al., 2014; Wen et al., 2016).", "startOffset": 27, "endOffset": 95}, {"referenceID": 14, "context": "tated in the training data (Williams and Young, 2007; Henderson et al., 2014; Wen et al., 2016).", "startOffset": 27, "endOffset": 95}, {"referenceID": 33, "context": "tated in the training data (Williams and Young, 2007; Henderson et al., 2014; Wen et al., 2016).", "startOffset": 27, "endOffset": 95}, {"referenceID": 3, "context": "Bordes and Weston (2016) explore end-toend goal orientated dialogue with a supervised model\u2014we show improvements over supervised learning with goal-based training and decoding.", "startOffset": 0, "endOffset": 25}, {"referenceID": 3, "context": "Bordes and Weston (2016) explore end-toend goal orientated dialogue with a supervised model\u2014we show improvements over supervised learning with goal-based training and decoding. Recently, He et al. (2017) use task-specific rules to combine the task input and dialogue history into a more structured state representation than ours.", "startOffset": 0, "endOffset": 204}, {"referenceID": 29, "context": "RL has been widely used to improve dialogue managers, which manage transitions between dialogue states (Singh et al., 2002; Pietquin et al., 2011; Rieser and Lemon, 2011; Ga\u0161ic et al., 2013; Fatemi et al., 2016).", "startOffset": 103, "endOffset": 211}, {"referenceID": 24, "context": "RL has been widely used to improve dialogue managers, which manage transitions between dialogue states (Singh et al., 2002; Pietquin et al., 2011; Rieser and Lemon, 2011; Ga\u0161ic et al., 2013; Fatemi et al., 2016).", "startOffset": 103, "endOffset": 211}, {"referenceID": 25, "context": "RL has been widely used to improve dialogue managers, which manage transitions between dialogue states (Singh et al., 2002; Pietquin et al., 2011; Rieser and Lemon, 2011; Ga\u0161ic et al., 2013; Fatemi et al., 2016).", "startOffset": 103, "endOffset": 211}, {"referenceID": 12, "context": "RL has been widely used to improve dialogue managers, which manage transitions between dialogue states (Singh et al., 2002; Pietquin et al., 2011; Rieser and Lemon, 2011; Ga\u0161ic et al., 2013; Fatemi et al., 2016).", "startOffset": 103, "endOffset": 211}, {"referenceID": 10, "context": "RL has been widely used to improve dialogue managers, which manage transitions between dialogue states (Singh et al., 2002; Pietquin et al., 2011; Rieser and Lemon, 2011; Ga\u0161ic et al., 2013; Fatemi et al., 2016).", "startOffset": 103, "endOffset": 211}, {"referenceID": 7, "context": "RL has also been used to allow agents to invent new languages (Das et al., 2017; Mordatch and Abbeel, 2017).", "startOffset": 62, "endOffset": 107}, {"referenceID": 21, "context": "RL has also been used to allow agents to invent new languages (Das et al., 2017; Mordatch and Abbeel, 2017).", "startOffset": 62, "endOffset": 107}, {"referenceID": 8, "context": ", 2013; Fatemi et al., 2016). In contrast, our end-toend approach has no explicit dialogue manager. Li et al. (2016) improve metrics such as diversity for non-goal-orientated dialogue using RL, which would make an interesting extension to our work.", "startOffset": 8, "endOffset": 117}, {"referenceID": 6, "context": "Das et al. (2017) use reinforcement learning to improve cooperative bot-bot dialogues.", "startOffset": 0, "endOffset": 18}, {"referenceID": 26, "context": "Work on learning end-to-end dialogues has concentrated on \u2018chat\u2019 settings, without explicit goals (Ritter et al., 2011; Vinyals and Le, 2015; Li et al., 2015).", "startOffset": 98, "endOffset": 158}, {"referenceID": 32, "context": "Work on learning end-to-end dialogues has concentrated on \u2018chat\u2019 settings, without explicit goals (Ritter et al., 2011; Vinyals and Le, 2015; Li et al., 2015).", "startOffset": 98, "endOffset": 158}, {"referenceID": 17, "context": "Work on learning end-to-end dialogues has concentrated on \u2018chat\u2019 settings, without explicit goals (Ritter et al., 2011; Vinyals and Le, 2015; Li et al., 2015).", "startOffset": 98, "endOffset": 158}, {"referenceID": 19, "context": "Such models are notoriously hard to evaluate (Liu et al., 2016), because the huge diversity of reasonable responses, whereas our task has a clear objective.", "startOffset": 45, "endOffset": 63}, {"referenceID": 9, "context": "Our end-to-end approach would also be much more straightforward to integrate into a generalpurpose dialogue agent than one that relied on annotated dialogue states (Dodge et al., 2016).", "startOffset": 164, "endOffset": 184}, {"referenceID": 1, "context": "There has also been computational work on modelling negotiations (Baarslag et al., 2013)\u2014our work differs in that agents communicate in unrestricted natural language, rather than pre-specified symbolic actions, and our focus on improving performance relative to humans rather than other automated systems.", "startOffset": 65, "endOffset": 88}, {"referenceID": 27, "context": "The only automated natural language negotiations systems we are aware of have first mapped language to domainspecific logical forms, and then focused on choosing the next dialogue act (Rosenfeld et al., 2014; Cuay\u00e1huitl et al., 2015; Keizer et al., 2017).", "startOffset": 184, "endOffset": 254}, {"referenceID": 5, "context": "The only automated natural language negotiations systems we are aware of have first mapped language to domainspecific logical forms, and then focused on choosing the next dialogue act (Rosenfeld et al., 2014; Cuay\u00e1huitl et al., 2015; Keizer et al., 2017).", "startOffset": 184, "endOffset": 254}, {"referenceID": 15, "context": "The only automated natural language negotiations systems we are aware of have first mapped language to domainspecific logical forms, and then focused on choosing the next dialogue act (Rosenfeld et al., 2014; Cuay\u00e1huitl et al., 2015; Keizer et al., 2017).", "startOffset": 184, "endOffset": 254}, {"referenceID": 18, "context": "Nash Jr (1950). There has also been computational work on modelling negotiations (Baarslag et al.", "startOffset": 5, "endOffset": 15}, {"referenceID": 1, "context": "There has also been computational work on modelling negotiations (Baarslag et al., 2013)\u2014our work differs in that agents communicate in unrestricted natural language, rather than pre-specified symbolic actions, and our focus on improving performance relative to humans rather than other automated systems. Our task is based on that of DeVault et al. (2015), who study natural language negotiations for pedagogical purposes\u2014their version includes speech rather than textual dialogue, and embodied agents, which would make interesting extensions to our work.", "startOffset": 66, "endOffset": 357}, {"referenceID": 28, "context": "Our use of a combination of supervised and reinforcement learning for training, and stochastic rollouts for decoding, builds on strategies used in game playing agents such as AlphaGo (Silver et al., 2016).", "startOffset": 183, "endOffset": 204}, {"referenceID": 16, "context": "Our use of rollouts could be extended by choosing the other agent\u2019s responses based on sampling, using Monte Carlo Tree Search (MCTS) (Kocsis and Szepesv\u00e1ri, 2006).", "startOffset": 134, "endOffset": 163}, {"referenceID": 28, "context": "However, our setting has a higher branching factor than in domains where MCTS has been successfully applied, such as Go (Silver et al., 2016)\u2014future work should explore scaling tree search to dialogue modelling.", "startOffset": 120, "endOffset": 141}], "year": 2017, "abstractText": "Much of human dialogue occurs in semicooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other\u2019s reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available.", "creator": "LaTeX with hyperref package"}}}