{"id": "1706.05394", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2017", "title": "A Closer Look at Memorization in Deep Networks", "abstract": "We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.", "histories": [["v1", "Fri, 16 Jun 2017 18:11:09 GMT  (8495kb,D)", "http://arxiv.org/abs/1706.05394v1", "Appears in Proceedings of the 34th International Conference on Machine Learning (ICML 2017), Devansh Arpit, Stanis{\\l}aw Jastrz\\k{e}bski, Nicolas Ballas, and David Krueger contributed equally to this work"], ["v2", "Sat, 1 Jul 2017 14:26:51 GMT  (8495kb,D)", "http://arxiv.org/abs/1706.05394v2", "Appears in Proceedings of the 34th International Conference on Machine Learning (ICML 2017), Devansh Arpit, Stanis{\\l}aw Jastrz\\k{e}bski, Nicolas Ballas, and David Krueger contributed equally to this work"]], "COMMENTS": "Appears in Proceedings of the 34th International Conference on Machine Learning (ICML 2017), Devansh Arpit, Stanis{\\l}aw Jastrz\\k{e}bski, Nicolas Ballas, and David Krueger contributed equally to this work", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["devansh arpit", "stanislaw k jastrzebski", "nicolas ballas", "david krueger", "emmanuel bengio", "maxinder s kanwal", "tegan maharaj", "asja fischer", "aaron c courville", "yoshua bengio", "simon lacoste-julien"], "accepted": true, "id": "1706.05394"}, "pdf": {"name": "1706.05394.pdf", "metadata": {"source": "META", "title": "A Closer Look at Memorization in Deep Networks", "authors": ["Devansh Arpit", "Stanis\u0142aw Jastrz\u0119bski", "Nicolas Ballas", "David Krueger", "Emmanuel Bengio", "Maxinder S. Kanwal", "Tegan Maharaj", "Asja Fischer", "Aaron Courville", "Yoshua Bengio", "Simon Lacoste-Julien"], "emails": ["<david.krueger@umontreal.ca>."], "sections": [{"heading": "1. Introduction", "text": "In fact, we are in a position to go in search of a solution that is able to trump itself, in the way that it is able to trump itself, in the way that it is, in the way that it is, in the way that it is, in the way that it is, in the way that it is and in the way that it is, in the way that it is and in the way that it is, in the way that it is and in the way that it is and in the way that it is and in the way that it is and in the way that it is and in the way that it is."}, {"heading": "2. Experiment Details", "text": "We conduct experiments with MNIST (LeCun et al., 1998) and CIFAR10 (Krizhevsky et al.) datasets. We examine two classes of models: two-layered multi-layer perceptrons (MLPs) with linear rectifier units (ReLUs) on MNIST and convolutionary neural networks (CNNs) on CIFAR10. Unless otherwise stated, the MLPs have 4096 hidden units per layer and are trained for 1000 epochs with SGD and learning rate 0.01. CNNs are a small Alexnet-style CNN2 (as in Zhang et al. (2017)) and are replaced with 2Input (2.2) \u2192 Crop (200.5.5) \u2192 Conv (BN \u2192 ReLU \u2192 MaxPooling (3.3) \u2192 Conv (200.5.5) \u2192 ReLU \u2192 MaxPool-SGD with dynamics = 0.9 and a learning rate of 0.0% each, with one half of each)."}, {"heading": "3. Qualitative Differences of DNNs Trained on Random vs. Real Data", "text": "Zhang et al. (2017) have shown empirically that DNNs are capable of adapting random data, which implicitly requires a high degree of memorization. In this section, we examine whether DNNs apply a similar memorization strategy when trained on real data. In particular, our experiments highlight some qualitative differences between DNNs trained on real data and random data, which supports the fact that DNNs do not use brute force memorization to adapt real data sets."}, {"heading": "3.1. Easy Examples as Evidence of Patterns in Real Data", "text": "We show that such \"simple examples\" (as well as corresponding \"hard examples\") are common in real data sets, but not in random data sets. Specifically, for each setting (real data, randX, randY) we train an MLP for a single epoch, based on 100 different random initializations and mixing of data. We find that in real data many examples are consistently (in) correctly classified by a single epoch, suggesting that different examples are much easier or harder in this sense. In the case of noise data, the difference between the examples is much smaller, suggesting that these examples (more) fit independently. Results are well modeled in Figure 1. For random differences in difficulty, this is not the case for random binomial noise."}, {"heading": "3.2. Loss-Sensitivity in Real vs. Random Data", "text": "To further investigate the difference between real and completely random inputs, we propose a proxy measure of memory over course. Since we cannot quantify how much each training sample x is stored, we instead measure the impact of each sample on the average loss. That is, we measure the standard of the loss gradient in relation to a previous example x according to t SGD updates. Lt is the loss after t updates; then the sensitivity measurement is set to bygtx = 0 Lt / \u2202 x-1. The parameters are updated to all future Lt indirectly by adapting the subsequent updates to different training examples. We denote the average to gtx according to T steps such as g-x and refer to it as loss sensitivity."}, {"heading": "3.3. Capacity and Effective Capacity", "text": "In this section, we examine the influence of capacity and effective capacity on learning data sets with different amounts of random input data or random labels."}, {"heading": "3.3.1. EFFECTS OF CAPACITY AND DATASET SIZE ON VALIDATION PERFORMANCES", "text": "In a first experiment, we investigated how the total model capacity affects the validation performance of data sets with varying levels of noise. At MNIST, we found that optimal validation performance requires a higher capacity model in the presence of noise examples (see Figure 5). This trend was consistent with noise inputs on CIFAR10, but we did not see a correlation between capacity and validation performance on random labels on CIFAR10. This result contradicts the intuitions of traditional learning theory, which suggests that capacity should be limited to force the learning of (only) the most regular patterns. Since DNNs can perfectly match the training set in any case, we assume that this higher capacity allows the network to adjust noise examples in a way that does not affect learning of the real data."}, {"heading": "3.3.2. EFFECTS OF CAPACITY AND DATASET SIZE ON TRAINING TIME", "text": "Our next experiment measures the time to convergence, i.e. how many eras it takes to achieve 100% training accuracy. Reducing the capacity or increasing the data set slows down training for both real and noisedata3. However, the effect for data sets containing noise is stronger, as our experiments in this section show (see Figure 6). The effective capacity of a DNN can be increased by increasing the representation capacity (e.g. by adding more hidden units) or training for longer. Therefore, increasing the number of hidden units reduces the number of training siterations required to fit to the data to a certain limit. We observe that the increasing representation capacity for real data is decreasing more sharply, indicating that this limit is lower, and a smaller representation capacity is sufficient for real data."}, {"heading": "4. DNNs Learn Patterns First", "text": "To achieve this goal, we build on the intuition that the number of different decision regions into which an entrance space is divided reflects the complexity of the learned hypothesis (Sokolic et al., 2016). This notion is similar in spirit to the degree to which a function can scatter random labels: a higher density of decision boundaries in the data space allows more samples to be scattered. Therefore, we estimate the complexity by measuring how densely points on data diversity exist around the decision boundaries of the model. Intuitively, a smaller fraction of the points near a decision boundary suggests that the learned hypothesis is easier when we randomly select points from the data distribution."}, {"heading": "4.1. Critical Sample Ratio (CSR)", "text": "Here we introduce the concept of a critical sample, which we use to estimate the density of the decision limits as described above. A critical sample of a model is a data point x, so that there is a point x near x, where the model predicts a different label than that of x. Specifically, we consider a critical sample of a classification network when there is a critical output vector f (x) = (f1 (x),., fk (x), when the model predicts a certain input sample x (Rn) from data diversity. We call x a critical sample when there is a critical point x, so that, arg max i fi (x) 6 = arg max j fj (x) (1).t. x \u2212 x \u2212 x, where there is a fixed box size. Note that, unlike with adversarial examples (Szegedy et al al., 2013; Goodfellow et al., 2014), the above definition depends only on the maxi (argex)."}, {"heading": "4.2. Critical Samples Throughout Training", "text": "We now show that the number of critical samples for a deep network (specifically a CNN) is gradually increasing compared to real data. To do this, we measure the number of critical samples in the validation set5, during the entire training 6. The results are shown in Figure 9. A5 We also measure the number of critical samples in the training sets. As we train our models with log losses, the training points are pushed away from the decision limit even after the network has learned to classify them correctly, resulting in an initial increase and then a decrease in the number of critical samples in the training sets. 6We use a box size of 0.3, which is small enough to be imperceptible in a 0-255 pixel scale. Different values for r have been tested, but not changed qualitatively higher number of critical samples for models trained on sound data compared to real data."}, {"heading": "5. Effect of Regularization on Learning", "text": "Zhang et al. (2017) argue that explicit regulation is not the most important explanation for good generalization performance, but that SGD-based optimization is largely responsible for it. Our results expand their claim and suggest that explicit regulation can significantly limit the speed of noise data storage without significantly affecting learning on real data. We compare the performance of CNNs trained on CIFAR10 and randY with the following regulators: drop-out (with drop-out rates in the range 0-0.9), input-drop-out (range 0-0.9), input-gaussian noise (with standard deviations in the range 0-0.7 and drop-out), hidden Gaussian noise (range 0-0.3), weight drop-out (range 0-1) and additionally drop-out with adversarial training (with weight factors in the range 0.2-0.7 and drop-out)."}, {"heading": "6. Related Work", "text": "In fact it is so that most of them are able to survive themselves if they do not follow the rules. (...) In fact it is so that they are able to survive themselves. (...) It is not so that they survive themselves. (...) It is as if they survive themselves. (...) It is as if they survive themselves. (...) It is as if they survive themselves. (...) It is as if they survive themselves. (...) It is as if they survive themselves. (...). \"(...).\" (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.). (.). (.).). (.). (.). (.). (.). (.). (.). (.). (.). (.).). (. (.). (.). (. (.).). (.). (. (.).). (. (.).). (.). (. (.).). (. (.).). (.). (. (.). (.). (.).). (.). (. (.).). (.). (. (.). (.). (.).). (. (.).). (.).). (.). (.). (.).). (.). (.). (.). (.).). (.). (.). (.). (.). (.).).).). ()."}, {"heading": "7. Conclusion", "text": "Our empirical study shows qualitative differences in DNN optimization of noise versus real data, all of which support the claim that DNNs trained with SGD variants first use patterns, not brute storage, to match real data. However, since DNNs have a proven ability to adapt noise, it is unclear why they find generalizable solutions to real data; we believe that deep learning priorities, including distributed and hierarchical representations, are likely to play an important role. Our analysis suggests that storage and generalization in DNNs depend on network architecture and optimization processes, but also on the data itself. We hope to stimulate future research on how data set properties influence the behavior of deep learning algorithms, and suggest a data-dependent understanding of DNN capacity as a research goal."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Akram Erraqabi and Jason Jo for their helpful discussions. SJ was supported by the scholarship no. DI 2014 / 016644 of the Polish Ministry of Science and Higher Education. DA was supported by IVADO, CIFAR and NSERC. EB was financially supported by Samsung Advanced Institute of Technology (SAIT). MSK and SJ were supported by MILA during this work. We appreciate the computing resources provided by ComputeCanada and CalculQuebec. Experiments were conducted with Theano (Theano Development Team, 2016) and Keras (Chollet et al., 2015)."}], "references": [{"title": "The effects of adding noise during backpropagation training on a generalization performance", "author": ["An", "Guozhong"], "venue": "Neural computation,", "citeRegEx": "An and Guozhong.,? \\Q1996\\E", "shortCiteRegEx": "An and Guozhong.", "year": 1996}, {"title": "Local rademacher complexities", "author": ["Bartlett", "Peter L", "Bousquet", "Olivier", "Mendelson", "Shahar"], "venue": "The Annals of Statistics,", "citeRegEx": "Bartlett et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2005}, {"title": "Learning deep architectures for ai", "author": ["Bengio", "Yoshua"], "venue": "Foundations and trends\u00ae in Machine Learning,", "citeRegEx": "Bengio and Yoshua,? \\Q2009\\E", "shortCiteRegEx": "Bengio and Yoshua", "year": 2009}, {"title": "Training with noise is equivalent to tikhonov regularization", "author": ["Bishop", "Chris M"], "venue": "Neural computation,", "citeRegEx": "Bishop and M.,? \\Q1995\\E", "shortCiteRegEx": "Bishop and M.", "year": 1995}, {"title": "Unsupervised Learning by Predicting Noise", "author": ["P. Bojanowski", "A. Joulin"], "venue": "ArXiv e-prints,", "citeRegEx": "Bojanowski and Joulin,? \\Q2017\\E", "shortCiteRegEx": "Bojanowski and Joulin", "year": 2017}, {"title": "Online learning and stochastic approximations", "author": ["Bottou", "L\u00e9on"], "venue": "On-line learning in neural networks,", "citeRegEx": "Bottou and L\u00e9on.,? \\Q1998\\E", "shortCiteRegEx": "Bottou and L\u00e9on.", "year": 1998}, {"title": "Entropy-sgd: Biasing gradient descent into wide valleys", "author": ["Chaudhari", "Pratik", "Choromanska", "Anna", "Soatto", "Stefano", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1611.01838,", "citeRegEx": "Chaudhari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chaudhari et al\\.", "year": 2016}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["Cybenko", "George"], "venue": "Mathematics of Control, Signals, and Systems (MCSS),", "citeRegEx": "Cybenko and George.,? \\Q1989\\E", "shortCiteRegEx": "Cybenko and George.", "year": 1989}, {"title": "Discriminatory analysis-nonparametric discrimination: consistency properties", "author": ["Fix", "Evelyn", "Hodges Jr.", "Joseph L"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Fix et al\\.,? \\Q1951\\E", "shortCiteRegEx": "Fix et al\\.", "year": 1951}, {"title": "Variabilita e mutabilita", "author": ["Gini", "Corrado"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Gini and Corrado.,? \\Q1913\\E", "shortCiteRegEx": "Gini and Corrado.", "year": 1913}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Hardt", "Moritz", "Recht", "Benjamin", "Singer", "Yoram"], "venue": "arXiv preprint arXiv:1509.01240,", "citeRegEx": "Hardt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2015}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Hornik", "Kurt", "Stinchcombe", "Maxwell", "White", "Halbert"], "venue": "Neural networks,", "citeRegEx": "Hornik et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "An empirical analysis of deep network loss surfaces", "author": ["Im", "Daniel Jiwoong", "Tao", "Michael", "Branson", "Kristin"], "venue": "arXiv preprint arXiv:1612.04010,", "citeRegEx": "Im et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Im et al\\.", "year": 2016}, {"title": "On large-batch training for deep learning: Generalization gap and sharp minima", "author": ["Keskar", "Nitish Shirish", "Mudigere", "Dheevatsa", "Nocedal", "Jorge", "Smelyanskiy", "Mikhail", "Tang", "Ping Tak Peter"], "venue": "arXiv preprint arXiv:1609.04836,", "citeRegEx": "Keskar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Keskar et al\\.", "year": 2016}, {"title": "Understanding blackbox predictions via influence functions", "author": ["Koh", "Pang Wei", "Liang", "Percy"], "venue": "arXiv preprint arXiv:1703.04730,", "citeRegEx": "Koh et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Koh et al\\.", "year": 2017}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher JC"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Why does deep and cheap learning work so well", "author": ["Lin", "Henry W", "Tegmark", "Max"], "venue": "arXiv preprint arXiv:1608.08225,", "citeRegEx": "Lin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Gradient-based hyperparameter optimization through reversible learning", "author": ["Maclaurin", "Dougal", "Duvenaud", "David K", "Adams", "Ryan P"], "venue": "In ICML, pp", "citeRegEx": "Maclaurin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maclaurin et al\\.", "year": 2015}, {"title": "Distributional smoothing with virtual adversarial training", "author": ["Miyato", "Takeru", "Maeda", "Shin-ichi", "Koyama", "Masanori", "Nakae", "Ken", "Ishii", "Shin"], "venue": "stat, 1050:25,", "citeRegEx": "Miyato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2015}, {"title": "Kernel analysis of deep networks", "author": ["Montavon", "Gr\u00e9goire", "Braun", "Mikio L", "M\u00fcller", "KlausRobert"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Montavon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Montavon et al\\.", "year": 2011}, {"title": "In search of the real inductive bias: On the role of implicit regularization in deep learning", "author": ["Neyshabur", "Behnam", "Tomioka", "Ryota", "Srebro", "Nathan"], "venue": "arXiv preprint arXiv:1412.6614,", "citeRegEx": "Neyshabur et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neyshabur et al\\.", "year": 2014}, {"title": "On the expressive power of deep neural networks", "author": ["Raghu", "Maithra", "Poole", "Ben", "Kleinberg", "Jon", "Ganguli", "Surya", "Sohl-Dickstein", "Jascha"], "venue": "arXiv preprint arXiv:1606.05336,", "citeRegEx": "Raghu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Raghu et al\\.", "year": 2016}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Overtraining, regularization and searching for a minimum, with application to neural networks", "author": ["J. Sjoberg", "J. Sjoeberg", "J. Sj\u00f6berg", "L. Ljung"], "venue": "International Journal of Control,", "citeRegEx": "Sjoberg et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Sjoberg et al\\.", "year": 1995}, {"title": "Robust large margin deep neural networks", "author": ["Sokolic", "Jure", "Giryes", "Raja", "Sapiro", "Guillermo", "Rodrigues", "Miguel RD"], "venue": "arXiv preprint arXiv:1605.08254,", "citeRegEx": "Sokolic et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sokolic et al\\.", "year": 2016}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian J", "Fergus", "Rob"], "venue": "CoRR, abs/1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Statistical learning theory, volume 1", "author": ["Vapnik", "Vladimir Naumovich", "Vlamimir"], "venue": null, "citeRegEx": "Vapnik et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik et al\\.", "year": 1998}, {"title": "The general inefficiency of batch training for gradient descent learning", "author": ["Wilson", "D Randall", "Martinez", "Tony R"], "venue": "Neural Networks,", "citeRegEx": "Wilson et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2003}, {"title": "On early stopping in gradient descent learning", "author": ["Yao", "Yuan", "Rosasco", "Lorenzo", "Caponnetto", "Andrea"], "venue": "Constructive Approximation,", "citeRegEx": "Yao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2007}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["Zhang", "Chiyuan", "Bengio", "Samy", "Hardt", "Moritz", "Recht", "Benjamin", "Vinyals", "Oriol"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Zhang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 12, "context": "On the contrary, deep nets are known to be universal approximators, capable of representing arbitrarily complex functions given sufficient capacity (Cybenko, 1989; Hornik et al., 1989).", "startOffset": 148, "endOffset": 184}, {"referenceID": 16, "context": ",\u201ctrain the LeNet architecture (LeCun et al., 1998) for 100 epochs using stochastic gradient descent (SGD) with a learning rate of 0.", "startOffset": 31, "endOffset": 51}, {"referenceID": 30, "context": "However, the experiments of Zhang et al. (2017) suggest that this is not the case.", "startOffset": 28, "endOffset": 48}, {"referenceID": 30, "context": "By demonstrating the ability of DNNs to \u201cmemorize\u201d random noise, Zhang et al. (2017) also raise the question whether deep networks use similar memorization tactics on real datasets.", "startOffset": 65, "endOffset": 85}, {"referenceID": 30, "context": "By demonstrating the ability of DNNs to \u201cmemorize\u201d random noise, Zhang et al. (2017) also raise the question whether deep networks use similar memorization tactics on real datasets. Intuitively, a brute-force memorization approach to fitting data does not capitalize on patterns shared between training examples or features; the content of what is memorized is irrelevant. A paradigmatic example of a memorization algorithm is k-nearest neighbors (Fix & Hodges Jr, 1951). Like Zhang et al. (2017), we do not formally define memorization; rather, we investigate this intuitive notion of memorization by training DNNs to fit random data.", "startOffset": 65, "endOffset": 497}, {"referenceID": 30, "context": "Thus, our analysis builds on the work of Zhang et al. (2017) and further investigates the role of memorization in DNNs.", "startOffset": 41, "endOffset": 61}, {"referenceID": 16, "context": "We perform experiments on MNIST (LeCun et al., 1998) and CIFAR10 (Krizhevsky et al.", "startOffset": 32, "endOffset": 52}, {"referenceID": 16, "context": "We perform experiments on MNIST (LeCun et al., 1998) and CIFAR10 (Krizhevsky et al.) datasets. We investigate two classes of models: 2-layer multi-layer perceptrons (MLPs) with rectifier linear units (ReLUs) on MNIST and convolutional neural networks (CNNs) on CIFAR10. If not stated otherwise, the MLPs have 4096 hidden units per layer and are trained for 1000 epochs with SGD and learning rate 0.01. The CNNs are a small Alexnet-style CNN2 (as in Zhang et al. (2017)), and are trained using", "startOffset": 33, "endOffset": 469}, {"referenceID": 30, "context": "Following Zhang et al. (2017), in many of our experiments we replace either (some portion of) the labels (with random labels), or the inputs (with i.", "startOffset": 10, "endOffset": 30}, {"referenceID": 18, "context": "We compute g x by unrolling t SGD steps and applying backpropagation over the unrolled computation graph, as done by Maclaurin et al. (2015). Unlike Maclaurin et al.", "startOffset": 117, "endOffset": 141}, {"referenceID": 18, "context": "We compute g x by unrolling t SGD steps and applying backpropagation over the unrolled computation graph, as done by Maclaurin et al. (2015). Unlike Maclaurin et al. (2015), we only use this procedure to compute g x, and do not modify the training procedure in any way.", "startOffset": 117, "endOffset": 173}, {"referenceID": 30, "context": "Our experiments demonstrate that time-toconvergence is not only longer on noise data (as noted by Zhang et al. (2017)), but also, increases substantially as a function of dataset size, relative to real data.", "startOffset": 98, "endOffset": 118}, {"referenceID": 25, "context": "To achieve this goal, we build on the intuition that the number of different decision regions into which an input space is partitioned reflects the complexity of the learned hypothesis (Sokolic et al., 2016).", "startOffset": 185, "endOffset": 207}, {"referenceID": 26, "context": "Note that, unlike with adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014) the above definition depends only on the predicted label arg maxi fi(x) of x, and not the true label, which is not meaningfully defined for random data.", "startOffset": 44, "endOffset": 91}, {"referenceID": 10, "context": "Note that, unlike with adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014) the above definition depends only on the predicted label arg maxi fi(x) of x, and not the true label, which is not meaningfully defined for random data.", "startOffset": 44, "endOffset": 91}, {"referenceID": 10, "context": "To perform this search, we propose using Langevin dynamics applied to the fast gradient sign method (FGSM, Goodfellow et al. (2014)) as shown in algorithm 14.", "startOffset": 107, "endOffset": 132}, {"referenceID": 30, "context": "Zhang et al. (2017) argue that explicit regularizations are not the main explanation of good generalization performance, rather SGD based optimization is largely responsible for it.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": ", see (Vapnik & Vapnik, 1998; Bartlett et al., 2005)).", "startOffset": 6, "endOffset": 52}, {"referenceID": 29, "context": "Our work builds on the experiments and challenges the interpretations of Zhang et al. (2017). We make heavy use of their methodology of studying DNN training in the context of noise datasets.", "startOffset": 73, "endOffset": 93}, {"referenceID": 29, "context": "Our work builds on the experiments and challenges the interpretations of Zhang et al. (2017). We make heavy use of their methodology of studying DNN training in the context of noise datasets. Zhang et al. (2017) show that DNNs can perfectly fit noise and thus that their generalization ability cannot be explained through traditional statistical learning theory (e.", "startOffset": 73, "endOffset": 212}, {"referenceID": 30, "context": "Zhang et al. (2017) argue that explicit and implicit regularizers (including SGD) might not explain or limit shattering of random data.", "startOffset": 0, "endOffset": 20}, {"referenceID": 11, "context": "A number of arguments support the idea that SGD-based learning imparts a regularization effect, especially with a small batch size (Wilson & Martinez, 2003) or a small number of epochs (Hardt et al., 2015).", "startOffset": 185, "endOffset": 205}, {"referenceID": 29, "context": "More generally, the efficacy of early stopping shows that SGD first learns simpler models (Yao et al., 2007).", "startOffset": 90, "endOffset": 108}, {"referenceID": 11, "context": "A number of arguments support the idea that SGD-based learning imparts a regularization effect, especially with a small batch size (Wilson & Martinez, 2003) or a small number of epochs (Hardt et al., 2015). Previous work also suggests that SGD prioritizes the learning of simple hypothesis first. Sjoberg et al. (1995) showed that, for linear models, SGD first learns models with small ` parameter norm.", "startOffset": 186, "endOffset": 319}, {"referenceID": 20, "context": "Montavon et al. (2011) use kernel methods to analyze the complexity of deep learning architectures, and find that network priors (e.", "startOffset": 0, "endOffset": 23}, {"referenceID": 20, "context": "Montavon et al. (2011) use kernel methods to analyze the complexity of deep learning architectures, and find that network priors (e.g. implemented by the network structure of a CNN or MLP) control the speed of learning at each layer. Neyshabur et al. (2014) note that the number of parameters does not control the effective capacity of a DNN, and that the reason for DNNs\u2019 generalization is unknown.", "startOffset": 0, "endOffset": 258}, {"referenceID": 22, "context": "Multiple techniques for analyzing the training of DNNs have been proposed before, including looking at generalization error, trajectory length evolution (Raghu et al., 2016), analyzing Jacobians associated to different layers (Wang; Saxe et al.", "startOffset": 153, "endOffset": 173}, {"referenceID": 23, "context": ", 2016), analyzing Jacobians associated to different layers (Wang; Saxe et al., 2013), or the shape of the loss minima found by SGD (Im et al.", "startOffset": 60, "endOffset": 85}, {"referenceID": 13, "context": ", 2013), or the shape of the loss minima found by SGD (Im et al., 2016; Chaudhari et al., 2016; Keskar et al., 2016).", "startOffset": 54, "endOffset": 116}, {"referenceID": 6, "context": ", 2013), or the shape of the loss minima found by SGD (Im et al., 2016; Chaudhari et al., 2016; Keskar et al., 2016).", "startOffset": 54, "endOffset": 116}, {"referenceID": 14, "context": ", 2013), or the shape of the loss minima found by SGD (Im et al., 2016; Chaudhari et al., 2016; Keskar et al., 2016).", "startOffset": 54, "endOffset": 116}, {"referenceID": 26, "context": "Critical samples are closely related to adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014); the differences are: 1) critical samples are indifferent to any ground truth, only noting changes in a model\u2019s prediction, 2) critical samples refer to real data-points; adversarial examples refer to the adversarial points near real data-points.", "startOffset": 61, "endOffset": 108}, {"referenceID": 10, "context": "Critical samples are closely related to adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014); the differences are: 1) critical samples are indifferent to any ground truth, only noting changes in a model\u2019s prediction, 2) critical samples refer to real data-points; adversarial examples refer to the adversarial points near real data-points.", "startOffset": 61, "endOffset": 108}, {"referenceID": 19, "context": "More similarly to critical samples, virtual adversarial training (VAT) (Miyato et al., 2015) looks at changes in the predictive distribution.", "startOffset": 71, "endOffset": 92}], "year": 2017, "abstractText": "We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.", "creator": "LaTeX with hyperref package"}}}