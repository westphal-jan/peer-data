{"id": "1706.05507", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2017", "title": "Variants of RMSProp and Adagrad with Logarithmic Regret Bounds", "abstract": "Adaptive gradient methods have become recently very popular, in particular as they have been shown to be useful in the training of deep neural networks. In this paper we have analyzed RMSProp, originally proposed for the training of deep neural networks, in the context of online convex optimization and show $\\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and SC-RMSProp for which we show logarithmic regret bounds for strongly convex functions. Finally, we demonstrate in the experiments that these new variants outperform other adaptive gradient techniques or stochastic gradient descent in the optimization of strongly convex functions as well as in training of deep neural networks.", "histories": [["v1", "Sat, 17 Jun 2017 09:48:55 GMT  (9014kb,D)", "http://arxiv.org/abs/1706.05507v1", "ICML 2017, 16 pages, 23 figures"]], "COMMENTS": "ICML 2017, 16 pages, 23 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.NE stat.ML", "authors": ["mahesh chandra mukkamala", "matthias hein"], "accepted": true, "id": "1706.05507"}, "pdf": {"name": "1706.05507.pdf", "metadata": {"source": "META", "title": "Variants of RMSProp and Adagrad with Logarithmic Regret Bounds", "authors": ["Mahesh Chandra Mukkamala", "Matthias Hein"], "emails": ["<mmahesh.chandra873@gmail.com>."], "sections": [{"heading": null, "text": "It's not just the way we move, but also the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move, the way we move. \""}], "references": [{"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "In Proceedings of COMPSTAT\u20192010,", "citeRegEx": "Bottou,? \\Q2010\\E", "shortCiteRegEx": "Bottou", "year": 2010}, {"title": "Learning step size controllers for robust neural network training", "author": ["C. Daniel", "J. Taylor", "S. Nowozin"], "venue": "In AAAI,", "citeRegEx": "Daniel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Daniel et al\\.", "year": 2016}, {"title": "Equilibrated adaptive learning rates for non-convex optimization", "author": ["Y. Dauphin", "H. de Vries", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Dauphin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Introduction to online convex optimization", "author": ["E. Hazan"], "venue": "Foundations and Trends in Optimization,", "citeRegEx": "Hazan,? \\Q2016\\E", "shortCiteRegEx": "Hazan", "year": 2016}, {"title": "Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization", "author": ["E. Hazan", "S. Kale"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hazan and Kale,? \\Q2014\\E", "shortCiteRegEx": "Hazan and Kale", "year": 2014}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Agarwal", "S. Kale"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Lecture 6d - a separate, adaptive learning rate for each connection", "author": ["G. Hinton", "N. Srivastava", "K. Swersky"], "venue": "Slides of Lecture Neural Networks for Machine Learning,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Karpathy and Fei.Fei,? \\Q2016\\E", "shortCiteRegEx": "Karpathy and Fei.Fei", "year": 2016}, {"title": "Adam: a method for stochastic optimization", "author": ["D.P. Kingma", "J.L. Bai"], "venue": "ICLR,", "citeRegEx": "Kingma and Bai,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Bai", "year": 2015}, {"title": "Improving stochastic gradient descent with feedback", "author": ["Koushik", "Jayanth", "Hayashi", "Hiroaki"], "venue": "arXiv preprint arXiv:1611.01505,", "citeRegEx": "Koushik et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Koushik et al\\.", "year": 2016}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report, University of Toronto,", "citeRegEx": "Krizhevsky,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky", "year": 2009}, {"title": "A survey of algorithms and analysis for adaptive online learning", "author": ["McMahan", "H Brendan"], "venue": "arXiv preprint arXiv:1403.3465,", "citeRegEx": "McMahan and Brendan.,? \\Q2014\\E", "shortCiteRegEx": "McMahan and Brendan.", "year": 2014}, {"title": "An overview of gradient descent optimization", "author": ["S. Ruder"], "venue": "algorithms. preprint,", "citeRegEx": "Ruder,? \\Q2016\\E", "shortCiteRegEx": "Ruder", "year": 2016}, {"title": "Unit tests for stochastic optimization", "author": ["T. Schaul", "I. Antonoglou", "D. Silver"], "venue": "In ICLR,", "citeRegEx": "Schaul et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2014}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber", "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning", "author": ["C. Szegedy", "S. Ioffe", "V. Vanhoucke"], "venue": "In ICLR Workshop,", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Rethinking the inception architecture for computer vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": "In CVPR,", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "preprint, arXiv:1212.5701v1,", "citeRegEx": "Zeiler,? \\Q2012\\E", "shortCiteRegEx": "Zeiler", "year": 2012}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In ICML,", "citeRegEx": "Zinkevich,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich", "year": 2003}], "referenceMentions": [{"referenceID": 3, "context": "Introduction There has recently been a lot of work on adaptive gradient algorithms such as Adagrad (Duchi et al., 2011), RMSProp (Hinton et al.", "startOffset": 99, "endOffset": 119}, {"referenceID": 9, "context": ", 2011), RMSProp (Hinton et al., 2012), ADADELTA (Zeiler, 2012), and Adam (Kingma & Bai, 2015).", "startOffset": 17, "endOffset": 38}, {"referenceID": 21, "context": ", 2012), ADADELTA (Zeiler, 2012), and Adam (Kingma & Bai, 2015).", "startOffset": 18, "endOffset": 32}, {"referenceID": 3, "context": "The original analysis of Adagrad (Duchi et al., 2011) was limited to the case of all convex functions for which it obtained a datadependent regret bound of order O( \u221a T ) which is known to be optimal (Hazan, 2016) for this class.", "startOffset": 33, "endOffset": 53}, {"referenceID": 4, "context": ", 2011) was limited to the case of all convex functions for which it obtained a datadependent regret bound of order O( \u221a T ) which is known to be optimal (Hazan, 2016) for this class.", "startOffset": 154, "endOffset": 167}, {"referenceID": 6, "context": "It has been shown in (Hazan et al., 2007) that one can achieve much better logarithmic regret bounds for the class of strongly convex functions.", "startOffset": 21, "endOffset": 41}, {"referenceID": 6, "context": "It is known that such bounds can be much better in practice than data independent bounds (Hazan et al., 2007),(McMahan, 2014).", "startOffset": 89, "endOffset": 109}, {"referenceID": 15, "context": "Interestingly, SC-Adagrad has been discussed in (Ruder, 2016), where it is said that \u201cit does not to work\u201d.", "startOffset": 48, "endOffset": 61}, {"referenceID": 22, "context": "One of the first methods which achieves the optimal regret bound of O( \u221a T ) for convex problems is online projected gradient descent (Zinkevich, 2003), defined as \u03b8t+1 = PC(\u03b8t \u2212 \u03b1tgt) (3) where \u03b1t = \u03b1 t is the step-size scheme and gt is a (sub)gradient of ft at \u03b8t.", "startOffset": 134, "endOffset": 151}, {"referenceID": 6, "context": "With \u03b1t = \u03b1t , online projected gradient descent method achieves the optimal O(log(T )) regret bound for strongly-convex problems (Hazan et al., 2007).", "startOffset": 130, "endOffset": 150}, {"referenceID": 3, "context": "If the adversarial is allowed to choose from the set of all possible convex functions on C \u2282 R, then Adagrad achieves the regret bound of orderO( \u221a T ) as shown in (Duchi et al., 2011).", "startOffset": 164, "endOffset": 184}, {"referenceID": 4, "context": "(Hazan, 2016).", "startOffset": 0, "endOffset": 13}, {"referenceID": 3, "context": "For better comparison to our results for RMSProp, we recall the result from (Duchi et al., 2011) in our notation.", "startOffset": 76, "endOffset": 96}, {"referenceID": 3, "context": "1 (Duchi et al., 2011) Let Assumptions A1, A2 hold and let \u03b8t be the sequence generated by Adagrad in Algorithm 1, where gt \u2208 \u2202ft(\u03b8t) and ft : C \u2192 R is an arbitrary convex function, then for stepsize \u03b1 > 0 the regret is upper bounded as R(T ) \u2264 D 2 \u221e 2\u03b1 d \u2211", "startOffset": 2, "endOffset": 22}, {"referenceID": 6, "context": "Strongly convex Adagrad (SC-Adagrad) The modification SC-Adagrad of Adagrad which we propose in the following can be motivated by the observation that the online projected gradient descent (Hazan et al., 2007) uses stepsizes of order \u03b1 = O( 1 T ) in order to achieve the logarithmic regret bound for strongly convex functions.", "startOffset": 189, "endOffset": 209}, {"referenceID": 15, "context": "This is probably why in (Ruder, 2016), the modification of Adagrad where one \u201cdrops the square-root\u201d did not work.", "startOffset": 24, "endOffset": 37}, {"referenceID": 6, "context": "1 [Lemma 12 (Hazan et al., 2007)] Let A,B be positive definite matrices, let A B 0 then A\u22121 \u2022 (A\u2212B) \u2264 log ( |A| |B| ) (6)", "startOffset": 12, "endOffset": 32}, {"referenceID": 6, "context": "1 also see for Lemma 12 of (Hazan et al., 2007).", "startOffset": 27, "endOffset": 47}, {"referenceID": 16, "context": "RMSProp and SC-RMSProp RMSProp is one of the most popular adaptive gradient algorithms used for the training of deep neural networks (Schaul et al., 2014; Dauphin et al., 2015; Daniel et al., 2016; Schmidhuber, 2015).", "startOffset": 133, "endOffset": 216}, {"referenceID": 2, "context": "RMSProp and SC-RMSProp RMSProp is one of the most popular adaptive gradient algorithms used for the training of deep neural networks (Schaul et al., 2014; Dauphin et al., 2015; Daniel et al., 2016; Schmidhuber, 2015).", "startOffset": 133, "endOffset": 216}, {"referenceID": 1, "context": "RMSProp and SC-RMSProp RMSProp is one of the most popular adaptive gradient algorithms used for the training of deep neural networks (Schaul et al., 2014; Dauphin et al., 2015; Daniel et al., 2016; Schmidhuber, 2015).", "startOffset": 133, "endOffset": 216}, {"referenceID": 17, "context": "RMSProp and SC-RMSProp RMSProp is one of the most popular adaptive gradient algorithms used for the training of deep neural networks (Schaul et al., 2014; Dauphin et al., 2015; Daniel et al., 2016; Schmidhuber, 2015).", "startOffset": 133, "endOffset": 216}, {"referenceID": 16, "context": "Note that RMSProp outperformed other adaptive methods like Adagrad order Adadelta as well as SGD with momentum in a large number of tests in (Schaul et al., 2014).", "startOffset": 141, "endOffset": 162}, {"referenceID": 1, "context": "It has been argued that if the changes in the parameter update are approximately Gaussian distributed, then the matrix At can be seen as a preconditioner which approximates the diagonal of the Hessian (Daniel et al., 2016).", "startOffset": 201, "endOffset": 222}, {"referenceID": 9, "context": "This is in contrast to the original suggestion of (Hinton et al., 2012) to choose \u03b2t = 0.", "startOffset": 50, "endOffset": 71}, {"referenceID": 9, "context": "On the other hand RMSProp was originally developed by (Hinton et al., 2012) for usage in deep learning.", "startOffset": 54, "endOffset": 75}, {"referenceID": 13, "context": "We refer to (Krizhevsky, 2009) for more details on the CIFAR datasets.", "startOffset": 12, "endOffset": 30}, {"referenceID": 0, "context": "Algorithms: We compare 1) Stochastic Gradient Descent (SGD) (Bottou, 2010) with O(1/t) decaying step-size for the strongly convex problems and for non-convex problems we use a constant learning rate, 2) Adam (Kingma & Bai, 2015) , is used with step size decay of \u03b1t = \u03b1 t for strongly convex problems and for non-convex problems we use a constant step-size.", "startOffset": 60, "endOffset": 74}, {"referenceID": 9, "context": "4) RMSProp as proposed in (Hinton et al., 2012) is used for both strongly convex problems and non-convex problems with \u03b2t = 0.", "startOffset": 26, "endOffset": 47}, {"referenceID": 9, "context": "In order that the parameter range is similar to the original RMSProp ((Hinton et al., 2012)) we fix as \u03b3 = 0.", "startOffset": 70, "endOffset": 91}, {"referenceID": 9, "context": "We show that the conditions for convergence of RMSProp for the convex case are different than what is used by (Hinton et al., 2012) and that this leads to better performance in practice.", "startOffset": 110, "endOffset": 131}], "year": 2017, "abstractText": "Adaptive gradient methods have become recently very popular, in particular as they have been shown to be useful in the training of deep neural networks. In this paper we have analyzed RMSProp, originally proposed for the training of deep neural networks, in the context of online convex optimization and show \u221a T -type regret bounds. Moreover, we propose two variants SC-Adagrad and SC-RMSProp for which we show logarithmic regret bounds for strongly convex functions. Finally, we demonstrate in the experiments that these new variants outperform other adaptive gradient techniques or stochastic gradient descent in the optimization of strongly convex functions as well as in training of deep neural networks.", "creator": "LaTeX with hyperref package"}}}