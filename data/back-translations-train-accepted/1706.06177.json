{"id": "1706.06177", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2017", "title": "Topic Modeling for Classification of Clinical Reports", "abstract": "Electronic health records (EHRs) contain important clinical information about patients. Efficient and effective use of this information could supplement or even replace manual chart review as a means of studying and improving the quality and safety of healthcare delivery. However, some of these clinical data are in the form of free text and require pre-processing before use in automated systems. A common free text data source is radiology reports, typically dictated by radiologists to explain their interpretations. We sought to demonstrate machine learning classification of computed tomography (CT) imaging reports into binary outcomes, i.e. positive and negative for fracture, using regular text classification and classifiers based on topic modeling. Topic modeling provides interpretable themes (topic distributions) in reports, a representation that is more compact than the commonly used bag-of-words representation and can be processed faster than raw text in subsequent automated processes. We demonstrate new classifiers based on this topic modeling representation of the reports. Aggregate topic classifier (ATC) and confidence-based topic classifier (CTC) use a single topic that is determined from the training dataset based on different measures to classify the reports on the test dataset. Alternatively, similarity-based topic classifier (STC) measures the similarity between the reports' topic distributions to determine the predicted class. Our proposed topic modeling-based classifier systems are shown to be competitive with existing text classification techniques and provides an efficient and interpretable representation.", "histories": [["v1", "Mon, 19 Jun 2017 21:04:22 GMT  (1266kb,D)", "http://arxiv.org/abs/1706.06177v1", "18 pages"]], "COMMENTS": "18 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["efsun sarioglu kayi", "kabir yadav", "james m chamberlain", "hyeong-ah choi"], "accepted": true, "id": "1706.06177"}, "pdf": {"name": "1706.06177.pdf", "metadata": {"source": "CRF", "title": "Topic Modeling for Classification of Clinical Reports", "authors": ["Efsun Sarioglu Kayi", "Kabir Yadav", "James M. Chamberlain", "Hyeong-Ah Choi"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Large amounts of clinically important medical data are now stored in Electronic Health Records (EHRs). In addition to simple performance measurements, more advanced applications can include decision-making tools, such as matching past patient patterns to recommend the need for a specific medical test or therapy, which can improve effectiveness and efficiency by helping physicians avoid unnecessary or potentially harmful tests or therapies. However, some of this data is in the form of free text, and needs to be processed and coded for better retrieval and analysis by automated or semi-automated systems.Topic modeling is an uncontrolled technique that can automatically identify topics from a given set of documents and find topic distributions for each of them. Presenting reports according to their topic distributions is more compact and can therefore be processed faster than raw text in subsequent automated processing. Biomedical concepts can also be presented well as nouns [1] and, when compared to other parts of the language, tend to be more specific to them."}, {"heading": "2 Material and Methods", "text": "Before we get to the results and findings of this research, this section provides the technical background for conducting this research: topic modeling and text classification. For topic modeling, we will review the historical progress in this area by explaining the main models used in Section 2.1 and how they differ from each other. Afterwards, the two popular classification techniques, SVM and Decision Tree, will be explained in Section 2.2."}, {"heading": "2.1 Topic Modeling", "text": "Topic modeling is an unattended learning algorithm that can automatically detect themes in a document collection. To this end, several techniques can be used, including Latent Semantic Analysis (LSA) [5], Probabilistic Latent Semantic Analysis (PLSA) [6], and Latent Dirichlet Allocation (LDA) [7]. LSA is a method of representing the hidden semantic structure of a term-document matrix in which lines are documents and columns are words / tokens [5] based on Singular Value Decomposition (SVD). One limitation of LSA is that each word is represented as a single point with the same meaning; therefore, polysems of words cannot be distinguished in this representation. Also, the final output of the LSA, which consists of axes in the Euclidean area, is not interpretable or descriptive."}, {"heading": "2.2 Text Classification", "text": "Text classification is a controlled machine learning algorithm in which the category of each document is learned from a pre-labeled set of documents. Decision trees and support vector machines (SVM) are two such classification algorithms. In a decision tree, internal nodes are the selected terms from the vocabulary, the branches are the criteria for the weight of terms, and the leaves represent the classes. SVM, on the other hand, tries to find a decision boundary between classes that is furthest away from any point in the training data set [9]. In this study, decision tree and SVM are selected as classification techniques: the decision tree is preferred because of its explicit rules-based output, which can be easily evaluated in terms of validity, and SVM performs well in text classification tasks [10, 11]."}, {"heading": "3 Calculation", "text": "Our proposed text classification techniques can be used in different areas, but our main objective in this study was to use such techniques to effectively classify clinical reports. Therefore, radiological reports from different emergency medical departments have been used to evaluate the performance of the proposed classifiers, which are computed tomography (CT) imaging reports performed on head trauma and are further explained in Section 3.1. the pre-processing they undergo before a classification is performed is explained in Section 3.3, and the measures used to evaluate the performance of these classifiers are explained in Section 3.2. Finally, after explaining the raw text classification of these clinical reports in Section 3.4, the proposed topic-based classifiers are explained in Section 3.5."}, {"heading": "3.1 Dataset", "text": "This study used prospective patient CT data previously collected to derive traumatic orbital fracture risk [12] and a rule for the clinical prediction of traumatic brain injury in children [13]. Staff radiologists dictated each CT report and the outcome of interest (either acute orbital fracture or findings consistent with traumatic brain injury) was extracted from a trained data abstractor. Of the 3,705 orbital CT reports, 3,242 were negative and 463 positive, of the 2,126 pediatric head CT reports, 1,973 were negative and 153 positive."}, {"heading": "3.2 Evaluation", "text": "This section explains the measures used to evaluate the classification algorithms. Once a classifier is built, its performance is evaluated using a separate data set. To prevent overmatch, only a subset of the data set, the so-called training data set, was used to train the classifier, and its effectiveness was then measured in the remaining invisible documents of the test set. To effectively measure the success of the classifier, training and test data sets were prepared with different proportions: 75%, 66%, 50%, 34% and 25%. These training and test data sets were randomized and layered to ensure that each subset is a good representation of the original data set in terms of class distribution. The orbital data set has a positive class ratio of 12.5%, and the pediatric data set has a positive class ratio of 7.2%. To summarize the classification performance, precision, memory and possible F-score measurement cases, assessments will be made."}, {"heading": "3.3 Preprocessing", "text": "Text data must be converted to a suitable format for automated processing, a common method of this is sack-of-words (BoW) representation, in which each document becomes a vector of its words / tokens. Entries in this matrix can be binary, indicating or weighting the existence or absence of a word in a document depending on how often a word exists in a document. In this study, the use of term weights resulted in slightly better classification results than other options. Frequent words were also removed from the vocabulary to limit its size. Furthermore, these common words typically do not add much information; most were stopwords, such as it, bin, das, von, at and. Other pre-processing tasks such as stemming were also investigated, but did not have a significant impact on classification performance."}, {"heading": "3.4 Raw Text Classification of Clinical Reports", "text": "The raw text of clinical reports was classified using conventional classification techniques, as shown in Figure 3. After pre-processing, the raw text files and associated results were combined and classified using SVM and Decision Tree in Weka. Weka is a collection of Java-written machine learning algorithms for data mining tasks [14]."}, {"heading": "3.5 Topic Modeling-based Classification of Clinical Reports", "text": "As described in Section 2.1, we chose LDA to generate thetopic models of clinical reports because it is a generative probabilistic system for documents and robust against overfitting. To conduct the experiments, the Stanford Topic Modeling Toolbox (TMT) [15] was used, an open source software that provides opportunities to train and derive theme models for text data."}, {"heading": "3.5.1 Topic Vector Classifier", "text": "In the Topic Vector Classifier, a theme model of all reports was created, and the topic distribution of each report was used to represent it in the form of topic vectors, which could be considered an alternative representation to bagof words (BoW), where terms are replaced by topics, and entries for each report show the likelihood of a specific topic for that report. Presentation as topic vectors is more compact than BoW, since the vocabulary for a text collection typically has thousands of entries, while a topic model is typically constructed with a maximum of hundreds of topics. These topic vectors were then classified using conventional classification algorithms, such as SVM and decision tree (see Algorithm 1).Algorithm 1 Topic Vector Classifierlearn the Topic Model for the documents merge the documents in Topic Vector Representation with their classes of Train decision tree and SVM based on documents presented as topic vectors."}, {"heading": "3.5.2 Confidence-based Topic Classifier (CTC)", "text": "In this classifier, after learning the topic model, a single topic is selected that has the highest reliability [16] for a class. Reliability (4) of a topic X for a class Y is calculated as supporting (5) the topic and the class together divided by supporting the topic itself. On the basis of this topic, the predictions for the test data set are made, as shown in Algorithm 2.Algorithm 2 Confidence-based Topic Classifier (CTC), the subject model for the documents in the training data set learn to merge the documents in the topic vector representation with their classes. Calculate the conf (T \u21d2 C) for each topic T and class C in the training data set find the topic with the highest reliability for the positive class a threshold for the selected topic t for all documents in the test dataset. The topic distribution of the document returns its value v for the selected topic t, if v > th predicts the positive end."}, {"heading": "3.5.3 Similarity-based Topic Classifier (STC)", "text": "In this classifier, the topic model was learned from the training data sets and the average of the topic distributions for each class was calculated (see algorithm 2). To calculate the similarity, the cosine measure was used. At two vectors x and y, the cosine of the angle between them can be calculated as in Equation 6. Its value ranges from 0 to 1 and the more similar the vector, the higher the cosine value. In this case, one vector represents the average topic distribution for a certain class and another vector the topic distribution of a test documentation.Algorithm 3 Similarity-based Topic Classifier (STC) learns the topic model for the documents in the training data set by merging the documents with their classes in the topic vector representation."}, {"heading": "3.5.4 Aggregate Topic Classifier (ATC)", "text": "In this approach, a representative topic vector was compiled for each class by averaging the corresponding topic distributions in the training dataset, and then a discriminatory topic was selected so that the difference between positive and negative representative vectors, as illustrated in Algorithm 4, is maximum. Subsequently, the reports in the test datasets were classified by analyzing the values of that topic, and a threshold for determining the predicted class was chosen. This threshold could be automatically selected based on the class distributions if the dataset is distorted or cross-validation methods can be applied to select a threshold that provides the best classification performance in a validation dataset. This approach is called Aggregate Topic Classifier (ATC), because training labels were used in an aggregate manner rather than individually."}, {"heading": "4 Results", "text": "The main objective of this study is to analyze and optimize the classification of clinical texts. As a starting point, raw texts of clinical reports were classified by well-known conventional classification algorithms. Alternatively, corpora theme modeling was used as a compact representation of clinical reports, and classifiers were built using this representation in various ways.The classification results Algorithm 4 Aggregates Topic Classifier (ATC) learn the theme model for the documents in the training dataset, the documents in the topic vector representation merge with their classes to calculate the average of the theme distributions for each class, the difference between the class average being a maximum threshold th for the selected topic t. For all documents in the test dataset, see that the topic distribution of the document finds its value v for the selected topic when v > th predicts a positive end when the use of the proposed topic model-based classifiers is presented in accordance with the evaluation techniques explained in Section 3.2."}, {"heading": "4.1 Raw Text Classification Results", "text": "The raw text of clinical reports was pre-processed and classified using Decision Tree (DT) and SVM and graphically displayed in Figures 5 and 6 for the orbital and pediatric datasets respectively. SVM consistently performs better than Decision Tree (DT) in different training and testing conditions and for both datasets."}, {"heading": "4.2 Topic Modeling-based Classification Results", "text": "One of the advantages of switching from the use of the entire vocabulary to the use of topics discussed in Section 2.1 is the dimensioning in relation to this transformation. The number of topics is generally lower than the number of persons tested. The figures are reflected in the total number of persons tested. The number of persons tested is usually lower than the number of persons tested. The number of persons tested is higher than the number of persons tested. The number of persons tested is higher than the number of persons tested."}, {"heading": "5 Discussion", "text": "Unlike the standard topic modeling techniques, studies were conducted to further improve the capabilities of standard topic classification. In [17] Wallach expanded the LDA algorithm to handle n-grams. Griffiths et al. combined LDA with POS tagging to have both content and functional words [2]. However, these studies led to a more complex topic modeling algorithm, which mainly serves to make the properties of top modeling techniques comparable to Natural Language Processing (NLP), which can slow the topic down. Other NLP-based classification techniques, e.g. [18, 19] an effective classification of clinical reports, but they are computationally expensive and require adjustment by medical experts. As such, we wanted to build a quick and efficient solution using topic modeling without increasing algorithmic complexity or time to generate topic models."}, {"heading": "6 Conclusion", "text": "In this study, subject modeling of clinical reports using different classification techniques was used, and automated clinical outcomes were compared with conventional machine learning methods. Compared to bag presentation, subject vector classification was performed with the added benefit of dimension reduction and interpretability. Several monitored classifiers were built on the basis of the subject model of the documents in the training dataset. In the trust-based topic classifier (CTC), the subject with the greatest confidence for the positive class was used to classify reports in the test dataset. Alternatively, using a similar subject classifier (STC) to classify a document, its topic distribution was compared in similarity to the average subject distributions of each class. Finally, in Aggregated Topic Classifiers (ATC), a single discriminatory topic was selected and used to classify the reports in the test datasets. Under these subject models, subject classifiers of the world-wide subject classifier (SVS) were selected and substituted with most of the best classifier ATS classifiers among the test datasets."}], "references": [{"title": "R", "author": ["Y. Huang", "H.J. Lowe", "D. Klein"], "venue": "J. Cucina, Improved Identification of Noun Phrases in Clinical Radiology Reports Using a High-Performance Statistical Natural Language Parser Augmented with the UMLS Specialist Lexicon., J Am Med Inform Assoc 12 (3) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Integrating Topics and Syntax", "author": ["T.L. Griffiths", "M. Steyvers", "D.M. Blei", "J.B. Tenenbaum"], "venue": "in: NIPS", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Clinical Report Classification Using Natural Language Processing and Topic Modeling", "author": ["E. Sarioglu", "K. Yadav", "H.-A. Choi"], "venue": "11th International Conference on Machine Learning and Applications (ICMLA) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Topic Modeling Based Classification of Clinical Reports", "author": ["E. Sarioglu", "K. Yadav", "H.-A. Choi"], "venue": "in: 51st Annual Meeting of the Association for Computational Linguistics Proceedings of the Student Research Workshop", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Indexing by Latent Semantic Analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American Society for Information Science 41 (6) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1990}, {"title": "Probabilistic Latent Semantic Analysis", "author": ["T. Hofmann"], "venue": "in: UAI", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res. 3 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Unsupervised Learning by Probabilistic Latent Semantic Analysis", "author": ["T. Hofmann"], "venue": "Mach. Learn. 42 (1-2) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines", "author": ["J.C. Platt"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Text Categorization with Suport Vector Machines: Learning with Many Relevant Features", "author": ["T. Joachims"], "venue": "in: Proceedings of the 10th European Conference on Machine Learning", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "A Re-examination of Text Categorization Methods", "author": ["Y. Yang", "X. Liu"], "venue": "in: Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "S", "author": ["K. Yadav", "E. Cowan", "J.S. Haukoos", "Z. Ashwell", "V. Nguyen", "P. Gennis"], "venue": "P. Wall, Derivation of a Clinical Risk Score for Traumatic Orbital Fracture., J Trauma Acute Care Surg 73 (5) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Identification of Children at Very Low Risk of Clinically-Important Brain Injuries After Head Trauma: A Prospective Cohort", "author": ["N. Kuppermann"], "venue": "Study., Lancet", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "The WEKA Data Mining Software: An Update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "SIGKDD Explor. Newsl. 11 (1) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Mining Association Rules Between Sets of Items in Large Databases", "author": ["R. Agrawal", "T. Imieli\u0144ski", "A. Swami"], "venue": "in: Proceedings of the 1993 ACM SIG- MOD International Conference on Management of Data", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "Topic Modeling: Beyond Bag-of-Words", "author": ["H.M. Wallach"], "venue": "in: Proceedings of the 23rd International Conference on Machine Learning", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Automated Outcome Classification of Emergency Department Computed Tomography Imaging Reports", "author": ["K. Yadav", "E. Sarioglu", "M. Smith", "H.-A. Choi"], "venue": "Academic Emergency Medicine 20 (8) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "An Efficient Feature Selection Using Hidden Topic in Text Categorization", "author": ["Z. Zhang", "X.-H. Phan", "S. Horiguchi"], "venue": "in: Proceedings of the 22nd International Conference on Advanced Information Networking and Applications - Workshops", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Improving Text Classification Accuracy Using Topic Modeling over an Additional Corpus", "author": ["S. Banerjee"], "venue": "in: Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Clinical Case-based Retrieval Using Latent Topic Analysis", "author": ["C.W. Arnold", "S.M. El-Saden", "A.A.T. Bui", "R. Taira"], "venue": "AMIA Annu Symp Proc 2010 ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Also, biomedical concepts can be well represented as nouns [1] and compared to other parts of speech, they tend to specialize better into topics [2].", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "Also, biomedical concepts can be well represented as nouns [1] and compared to other parts of speech, they tend to specialize better into topics [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 2, "context": "A preliminary version of this work has been reported in [3, 4].", "startOffset": 56, "endOffset": 62}, {"referenceID": 3, "context": "A preliminary version of this work has been reported in [3, 4].", "startOffset": 56, "endOffset": 62}, {"referenceID": 2, "context": "In [3], the performance of topic vector classification with conventional classifiers was analyzed and in [4], aggregate topic classifier (ATC) were introduced using a single dataset.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "In [3], the performance of topic vector classification with conventional classifiers was analyzed and in [4], aggregate topic classifier (ATC) were introduced using a single dataset.", "startOffset": 105, "endOffset": 108}, {"referenceID": 4, "context": "Several techniques can be used for this purpose including Latent Semantic Analysis (LSA) [5], Probabilistic Latent Semantic Analysis (PLSA) [6], and Latent Dirichlet Allocation (LDA) [7].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "Several techniques can be used for this purpose including Latent Semantic Analysis (LSA) [5], Probabilistic Latent Semantic Analysis (PLSA) [6], and Latent Dirichlet Allocation (LDA) [7].", "startOffset": 140, "endOffset": 143}, {"referenceID": 6, "context": "Several techniques can be used for this purpose including Latent Semantic Analysis (LSA) [5], Probabilistic Latent Semantic Analysis (PLSA) [6], and Latent Dirichlet Allocation (LDA) [7].", "startOffset": 183, "endOffset": 186}, {"referenceID": 4, "context": "LSA is a way of representing hidden semantic structure of a term-document matrix in which rows are documents and columns are words/tokens [5] based on Singular Value Decomposition (SVD).", "startOffset": 138, "endOffset": 141}, {"referenceID": 7, "context": "Also, the final output of LSA, which consists of axes in Euclidean space, is not interpretable or descriptive [8].", "startOffset": 110, "endOffset": 113}, {"referenceID": 5, "context": "PLSA is considered to be a probabilistic version of LSA where an unobserved class variable is associated with each occurrence of a word in a particular document [6].", "startOffset": 161, "endOffset": 164}, {"referenceID": 6, "context": "PLSA solves the polysemy problem; however it is not considered a fully generative model of documents which can lead to overfitting [7].", "startOffset": 131, "endOffset": 134}, {"referenceID": 6, "context": "LDA, first defined by Blei et al [7], defines a topic as a distribution over a fixed vocabulary, where each document can exhibit topics with different proportions.", "startOffset": 33, "endOffset": 36}, {"referenceID": 6, "context": "LDA performs better than PLSA for small datasets because it avoids overfitting and it also supports polysemy [7].", "startOffset": 109, "endOffset": 112}, {"referenceID": 8, "context": ", N where xt \u2208 R and yt \u2208 {1,\u22121}, it tries to find a separating hyperplane with the maximum margin [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 9, "context": "In this study, decision tree and SVM are chosen as classification techniques: Decision tree is preferred due to its explicit rule based output that can be easily evaluated for content validity and SVM performs well in text classification tasks [10, 11].", "startOffset": 244, "endOffset": 252}, {"referenceID": 10, "context": "In this study, decision tree and SVM are chosen as classification techniques: Decision tree is preferred due to its explicit rule based output that can be easily evaluated for content validity and SVM performs well in text classification tasks [10, 11].", "startOffset": 244, "endOffset": 252}, {"referenceID": 11, "context": "1 Dataset This study used prospectively collected patient CT report data previously collected for derivation of a traumatic orbital fracture clinical risk score [12] and a pediatric traumatic brain injury clinical prediction rule [13].", "startOffset": 161, "endOffset": 165}, {"referenceID": 12, "context": "1 Dataset This study used prospectively collected patient CT report data previously collected for derivation of a traumatic orbital fracture clinical risk score [12] and a pediatric traumatic brain injury clinical prediction rule [13].", "startOffset": 230, "endOffset": 234}, {"referenceID": 13, "context": "Weka is a collection of machine learning algorithms for data mining tasks written in Java [14].", "startOffset": 90, "endOffset": 94}, {"referenceID": 14, "context": "2 Confidence-based Topic Classifier (CTC) In this classifier, after the topic model is learned, a single topic is chosen that has the biggest confidence [16] for a class.", "startOffset": 153, "endOffset": 157}, {"referenceID": 15, "context": "In [17], Wallach extended the LDA algorithm to handle n-grams.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "combined LDA with POS tagging to have both content and functional words [2].", "startOffset": 72, "endOffset": 75}, {"referenceID": 16, "context": ", [18, 19], an be effective in classifying clinical reports as well, however they are computationally expensive and they may require customization by medical experts.", "startOffset": 2, "endOffset": 10}, {"referenceID": 17, "context": "Zhang et al [20] used topic modeling as a keyword selection mechanism by selecting the top words from topics based on their entropy.", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "In another similar study, Banerjee [22] uses topics as additional features to BoW features for the purpose of classification.", "startOffset": 35, "endOffset": 39}, {"referenceID": 19, "context": "[23] shows an information retrieval system where patients can be queried and compared based on their topic distributions.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Electronic health records (EHRs) contain important clinical information about patients. Efficient and effective use of this information could supplement or even replace manual chart review as a means of studying and improving the quality and safety of healthcare delivery. However, some of these clinical data are in the form of free text and require pre-processing before use in automated systems. A common free text data source is radiology reports, typically dictated by radiologists to explain their interpretations. We sought to demonstrate machine learning classification of computed tomography (CT) imaging reports into binary outcomes, i.e. positive and negative for fracture, using regular text classification and classifiers based on topic modeling. Topic modeling provides interpretable themes (topic distributions) in reports, a representation that is more compact than the commonly used bag-of-words representation and can be processed faster than raw text in subsequent automated processes. We demonstrate new classifiers based on this topic modeling representation of the reports. Aggregate topic classifier (ATC) and confidence-based topic classifier (CTC) use a single topic that is determined from the training dataset based on different measures to classify the reports on the test dataset. Alternatively, similarity-based topic classifier (STC) measures the similarity between the reports\u2019 topic distributions to determine the predicted class. Our proposed topic modeling-based classifier systems are shown to be competitive with existing text classification techniques and provides an efficient and interpretable representation.", "creator": "LaTeX with hyperref package"}}}