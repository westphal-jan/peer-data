{"id": "1706.06197", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2017", "title": "meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting", "abstract": "We propose a simple yet effective technique for neural network learning. The forward propagation is computed as usual. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-$k$ elements (in terms of magnitude) are kept. As a result, only $k$ rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction ($k$ divided by the vector dimension) in the computational cost. Surprisingly, experimental results demonstrate that we can update only 1--4\\% of the weights at each back propagation pass. This does not result in a larger number of training iterations. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given.", "histories": [["v1", "Mon, 19 Jun 2017 22:36:33 GMT  (153kb)", "http://arxiv.org/abs/1706.06197v1", "ICML 2017"], ["v2", "Wed, 5 Jul 2017 01:34:50 GMT  (158kb)", "http://arxiv.org/abs/1706.06197v2", "Accepted by The 34th International Conference on Machine Learning (ICML 2017)"], ["v3", "Mon, 30 Oct 2017 09:48:41 GMT  (158kb)", "http://arxiv.org/abs/1706.06197v3", "Accepted by The 34th International Conference on Machine Learning (ICML 2017)"], ["v4", "Tue, 31 Oct 2017 02:04:52 GMT  (158kb)", "http://arxiv.org/abs/1706.06197v4", "Accepted by The 34th International Conference on Machine Learning (ICML 2017)"]], "COMMENTS": "ICML 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL cs.CV", "authors": ["xu sun", "xuancheng ren", "shuming ma", "houfeng wang"], "accepted": true, "id": "1706.06197"}, "pdf": {"name": "1706.06197.pdf", "metadata": {"source": "META", "title": "meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting", "authors": ["Xu Sun", "Xuancheng Ren", "Shuming Ma", "Houfeng Wang"], "emails": ["<xusun@pku.edu.cn>."], "sections": [{"heading": null, "text": "ar Xiv: 170 6.06 197v 1 [cs.L G] June 19, 2017Learning neural networks. Forward propagation is calculated as usual. In backward propagation, only a small subset of the complete gradient is calculated to update the model parameters, the gradient vectors are sparsified so that only the uppermost k elements (in terms of magnitude) are preserved. As a result, only k rows or columns (depending on the layout) of the weight matrix are modified, resulting in a linear reduction (k divided by the vector dimension) of the calculation costs. Surprisingly, experimental results show that we can only update 1-4% of the weights for each backward propagation pass. This does not result in a greater number of training siterations. More interesting is that the accuracy of the resulting models is actually improved rather than deteriorated, and a detailed analysis is given."}, {"heading": "1. Introduction", "text": "It is not uncommon for a neural network to have a huge number of model parameters. In this study, we propose a minimal effort to reactivate the distribution method we call meProp. The idea is that we calculate only a very small but critical portion of the gradient information and update only the corresponding minimal portion of the parameters in each learning step, resulting in economical gradients, so that only the most relevant parameters are updated."}, {"heading": "2. Proposed Method", "text": "We propose a simple but effective method for learning neural networks: Forward propagation is calculated as usual. During backward propagation, only a small subset of the full gradient is calculated to update the model parameters. Gradient vectors are \"quantified\" so that only the top k components are preserved in terms of magnitude. We first present the proposed method and then describe the implementation details."}, {"heading": "2.1. meProp", "text": "For the sake of simplicity, let us take as an example an arithmetic unit with a linear transformation and a nonlinear transformation: y = Wx (1) z = \u03c3 (y) (2), where W \u00b2 Rn \u00b7 m, x \u00b2 Rm, y \u00b2 Rn, z \u00b2 Rn, m is the dimension of the input vector, n is the dimension of the output vector, and it is a nonlinear function (e.g., relu, tanh, and sigmoid). During backpropagation, we must consider the gradient of the parameters matrix W and the input vector x: The dimension of the output vectors is a nonlinear function (e.g., tanh, and sigmoid)."}, {"heading": "2.2. Implementation", "text": "We have encoded two neural network models, including an LSTM model for part-of-speech tagging (POS), a feedback NN model (MLP) for transition-based dependency parsing, and MNIST image recognition. We use optimizers with automatically adaptive learning rates, including Adam (Kingma & Ba, 2014) and AdaGrad (Duchi et al., 2011). In our implementation, we do not modify optimizers, although there are many zero elements in the gradations.Most CPU experiments are conducted on the framework itself encoded in C #. This framework builds a dynamic computation curve for each sample, making it suitable for data of varying lengths.A typical training method consists of four parts: computational graph construction, forward propagation, backpropagation, and parameter updating."}, {"heading": "2.2.1. WHERE TO APPLY MEPROP", "text": "The proposed method aims to reduce the complexity of the back propagation by reducing the elements in the computationally intensive operations. In our preliminary observations, we only apply meProp to the back propagation from the output of the multiplication or matrix vector multiplication to its in-puts. In other elementary operations (e.g. activation functions), the original back propagation must be maintained, as these operations are already fast enough compared to matrix matrix or matrix vector multiplication operations. If there are several hidden layers, the top-k sparsification must be applied to each hidden layer, because the thin gradient will again be dense from one layer to the other. In meProp, the gradients are sparsified with a top-k operation."}, {"heading": "3. Related Work", "text": "Riedmiller and Braun (1993) proposed a direct adaptive method for rapid learning, which makes a local adjustment of the weight update to the behavior of the error function. Tollenaere (1990) also proposed an adaptive acceleration strategy for reverse propagation. Dropout (Srivastava et al., 2014) was proposed to improve training speed and reduce the risk of overadaptation. Sparse encoding is a class of unsupervised methods for learning over-complete databases to present data efficiently (Olshausen & Field, 1996). Ranzato et al. (2006) proposed a sparse autoencoder model for learning sparse over-complete features. The proposed method differs significantly from previous studies on reverse propagation, suspensions and sparse encoding. The sampled output loss methods (Jean et al., 2015) are based on the softmax layer (output layer) and not limited to our sampling method only."}, {"heading": "4. Experiments", "text": "To demonstrate that the proposed method is universal, we conduct experiments on different models (LSTM / MLP), different training methods (Adam / AdaGrad), and multiple tasks. Part-of-Speech Tagging (POS tag): We use the standard benchmark data set from previous work (Collins, 2002) derived from Penn Treebank Corpus, and use Sections 0-18 of the Wall Street Journal (WSJ) for training (38,219 examples) and Sections 22-24 for testing (5,462 examples). The evaluation metric is word accuracy. A popular model for this task is the LSTM model (Hochreiter & Schmidhuber, 1997), 1, which acts as our baseline.Transition-based Dependency Parsing (Parsing): After previous work, we use the English Penn TreeBank (PTB et al., 1993)."}, {"heading": "4.1. Experimental Settings", "text": "For POS tags, the input dimension is 1 (word) \u00b7 50 (dim per word) + 7 (features) \u00b7 20 (dimensions per feature) = 190, and the output dimension is 45. For parsing, the input dimension is 48 (features) \u00b7 50 (dimensions per feature) = 2400, and the output dimension is 25. For MNIST, the input dimension is 28 (pixels per line) \u00b7 28 (pixels per column) \u00b7 1 (dimensions per pixel) = 784, and the output dimension is 10\u00ba. As discussed in section 2, the optimal k of the top layer for the output layer is different."}, {"heading": "4.2. Experimental Results", "text": "In this experiment, the LSTM is based on a hidden layer, and the MLP is based on two hidden layers (experiments on more hidden layers will be presented later).We conduct experiments using various optimization methods, including AdaGrad and Adam. Since meProp is applied to the linear transformations (which involve large computing costs), we report the linear transformation time as backprop time.It does not include the nonlinear activations, which usually have less than 2% computational cost.The total time of the backpropagation, which includes nonlinear activations, is reported as total backprop time.Based on the amount of development and previous work, we set the mini-batch size to 1 (set), 10,000 (transition examples), and 10 (images) for POS tag, parsing, and MNIST, each having optimal computing time."}, {"heading": "4.3. Varying Backprop Ratio", "text": "In Figure 4 (left) we vary the k of top-k meProp to compare the test accuracy on different ratios of meProp Backprop. For example, if k = 5, this means that the backprop ratio is 5 / 500 = 1%. The optimizer is Adam. As we can see, meProp consistently achieves a better accuracy than the baseline. In fact, the best test accuracy of meProp, 98.15% (+ 0.33), is better than that given in Table 1."}, {"heading": "4.4. Top-k vs. Random", "text": "Figure 4 (center) shows the results of top-k meProp vs. random meProp. Random meProp means that random elements (instead of top-k) are selected for backpropagation. As we can see, the top-k version works better than the random version. It suggests that top-k elements contain the most important information about the gradients."}, {"heading": "4.5. Varying Hidden Dimension", "text": "We have another question: Does the top-k meProp work well simply because the original model does not require the large dimension of the hidden layers? For example, meProp (topk = 5) works well simply because the LSTM works well with the hidden dimension of 5, and there is no need to use the hidden dimension of 500. To investigate this, we conduct experiments to use the same hidden dimension as k and the results are in Table 3. However, as we can see, the results of the small hidden dimensions are much worse than those of meProp.In addition, Figure 4 (right) shows more detailed curves by varying the value of k. In the figure, different k results in a different backprop ratio for meProp and a different hidden dimension ratio for LSTM / MLP. As we can see, the answer to this question is negative: meProp is not based on redundant hidden layer elements."}, {"heading": "4.6. Adding Dropout", "text": "Since we have observed that meProp can reduce deep learning overadjustment, it is a natural question whether meProp reduces the same type of overadjustment risk as dropout.Therefore, we use development data to find an appropriate value for the dropout rate in these tasks, and then add meProp to check if further improvements are possible.Table 4 shows the results. As we see, meProp can achieve a further improvement over dropout.Specifically, meProp shows an improvement of 0.46 UAS in parsing, and the results indicate that the type of overadjustment that meProp reduces is likely to differ from that of the dropout.Therefore, a model should be able to use both meProp and dropout to reduce overadjustment."}, {"heading": "4.7. Adding More Hidden Layers", "text": "Another question is whether meProp relies on flat models with only a few hidden layers or not. To answer this question, we also conduct experiments with more hidden layers, from 2 hidden layers to 5 hidden layers. We find that setting the drop-out rate to 0.1 works well for most cases with different number of layers. To simplify the comparison, we set the same drop rate to 0.1 in this experiment. Table 5 shows that adding the number of hidden layers does not harm meProp's performance."}, {"heading": "4.8. Speedup on GPU", "text": "For the implementation of meProp on the GPU, the simplest solution is to treat the entire GPU as a \"great training example,\" in which the GPU is based on the averaged values of all GPUs."}, {"heading": "4.9. Related Systems on the Tasks", "text": "The POS tagging task is a well-known benchmark task with accuracy reports ranging from 97.2% to 97.4% (Toutanova et al., 2003; Sun, 2014; Shen et al., 2007; Tsuruoka et al., 2011; Collobert et al., 2011; Huang et al., 2015).Our method reaches 97.31% (Table 4).For the transition-based parsing task, MaltParser (Nivre et al., 2007) typically has a UAS score of 91.4 to 91.5 (Zhang & Clark, 2008; Nivre et al., 2007; Huang & Sagae, 2010).As one of the most popular transition-based parsers (Nivre et al., 2007), Chen and Manning (2014) reach 92.0 UAS using neural networks. Our method reaches 91.99 UAS (Table 4).For the MLP-based approaches, 98-99% accuracy can also be achieved, frequently around 1998 UAS-2.0 techniques."}, {"heading": "5. Conclusions", "text": "Deep learning backpropagation attempts to modify all parameters in each stochastic update, which is inefficient and can even lead to overadjustment due to unnecessary modification of many weakly relevant parameters. We propose a method of backpropagation with minimum effort (meProp), where we calculate only a very small but critical portion of the gradient and modify only the corresponding small portion of the parameters with each update, resulting in very different gradients in order to modify only highly relevant parameters for the given training specimence. meProp is independent of the optimization method. experiments show that meProp can reduce the calculation costs of backpropagation by one to two orders of magnitude by updating only 1-4% parameters and still improving model accuracy in most cases."}, {"heading": "Acknowledgements", "text": "The authors thank the anonymous reviewers for their insightful comments and suggestions on this paper. This work was supported in part by the National Natural Science Foundation of China (No. 61673028), the National High Technology Research and Development Program of China (863 Program, No. 2015AA015404) and an Okawa Research Grant (2016)."}], "references": [{"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Danqi", "Manning", "Christopher D"], "venue": "In Proceedings of EMNLP\u201914,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Deep, big, simple neural nets for handwritten digit recognition", "author": ["Ciresan", "Dan C", "Meier", "Ueli", "Gambardella", "Luca Maria", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Ciresan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2010}, {"title": "Multi-column deep neural networks for image classification", "author": ["Ciresan", "Dan C", "Meier", "Ueli", "Schmidhuber", "J\u00fcrgen"], "venue": "In Proceedings of CVPR\u201912,", "citeRegEx": "Ciresan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2012}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Collins", "Michael"], "venue": "In Proceedings of EMNLP\u201902,", "citeRegEx": "Collins and Michael.,? \\Q2002\\E", "shortCiteRegEx": "Collins and Michael.", "year": 2002}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel P"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Communication quantization for dataparallel training of deep neural networks", "author": ["Dryden", "Nikoli", "Moon", "Tim", "Jacobs", "Sam Ade", "Essen", "Brian Van"], "venue": "In Proceedings of the 2nd Workshop on Machine Learning in HPC Environments,", "citeRegEx": "Dryden et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dryden et al\\.", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John C", "Hazan", "Elad", "Singer", "Yoram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Dynamic programming for linear-time incremental parsing", "author": ["Huang", "Liang", "Sagae", "Kenji"], "venue": "In Proceedings of ACL\u201910,", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Huang", "Zhiheng", "Xu", "Wei", "Yu", "Kai"], "venue": "CoRR, abs/1508.01991,", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Jarrett", "Kevin", "Kavukcuoglu", "Koray", "Ranzato", "Marc\u2019Aurelio", "LeCun", "Yann"], "venue": "In Proceeding of ICCV\u201909,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean", "S\u00e9bastien", "Cho", "KyungHyun", "Memisevic", "Roland", "Bengio", "Yoshua"], "venue": "In Proceedings of ACL/IJCNLP\u201915,", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Marcus", "Mitchell P", "Marcinkiewicz", "Mary Ann", "Santorini", "Beatrice"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Maltparser: A language-independent system for data-driven dependency parsing", "author": ["Nivre", "Joakim", "Hall", "Johan", "Nilsson", "Jens", "Chanev", "Atanas", "Eryigit", "G\u00fclsen", "K\u00fcbler", "Sandra", "Marinov", "Svetoslav", "Marsi", "Erwin"], "venue": "Natural Language Engineering,", "citeRegEx": "Nivre et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "Natural image statistics and efficient coding", "author": ["Olshausen", "Bruno A", "Field", "David J"], "venue": "Network: Computation in Neural Systems,", "citeRegEx": "Olshausen et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Olshausen et al\\.", "year": 1996}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["Ranzato", "Marc\u2019Aurelio", "Poultney", "Christopher S", "Chopra", "Sumit", "LeCun", "Yann"], "venue": "In NIPS\u201906,", "citeRegEx": "Ranzato et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2006}, {"title": "A direct adaptive method for faster backpropagation learning: The rprop algorithm", "author": ["Riedmiller", "Martin", "Braun", "Heinrich"], "venue": "In Proceedings of IEEE International Conference on Neural Networks", "citeRegEx": "Riedmiller et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Riedmiller et al\\.", "year": 1993}, {"title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts", "author": ["Shazeer", "Noam", "Mirhoseini", "Azalia", "Maziarz", "Krzysztof", "Davis", "Andy", "Le", "Quoc V", "Hinton", "Geoffrey E", "Dean", "Jeff"], "venue": "layer. CoRR,", "citeRegEx": "Shazeer et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Shazeer et al\\.", "year": 2017}, {"title": "Guided learning for bidirectional sequence classification", "author": ["Shen", "Libin", "Satta", "Giorgio", "Joshi", "Aravind K"], "venue": "In Proceedings of ACL\u201907,", "citeRegEx": "Shen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2007}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["Simard", "Patrice Y", "Steinkraus", "Dave", "Platt", "John C"], "venue": "In Proceedings of ICDR\u201903,", "citeRegEx": "Simard et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2003}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Structure regularization for structured prediction", "author": ["Sun", "Xu"], "venue": "In NIPS\u201914,", "citeRegEx": "Sun and Xu.,? \\Q2014\\E", "shortCiteRegEx": "Sun and Xu.", "year": 2014}, {"title": "Supersab: fast adaptive back propagation with good scaling properties", "author": ["Tollenaere", "Tom"], "venue": "Neural networks,", "citeRegEx": "Tollenaere and Tom.,? \\Q1990\\E", "shortCiteRegEx": "Tollenaere and Tom.", "year": 1990}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Toutanova", "Kristina", "Klein", "Dan", "Manning", "Christopher D", "Singer", "Yoram"], "venue": "In Proceedings of HLT-NAACL\u201903,", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Learning with lookahead: Can history-based models rival globally optimized models", "author": ["Tsuruoka", "Yoshimasa", "Miyao", "Yusuke", "Kazama", "Jun\u2019ichi"], "venue": "In Proceedings of CoNLL\u201911,", "citeRegEx": "Tsuruoka et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tsuruoka et al\\.", "year": 2011}, {"title": "A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing", "author": ["Zhang", "Yue", "Clark", "Stephen"], "venue": "In Proceedings of EMNLP\u201908,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 6, "context": "We use the optimizers with automatically adaptive learning rates, including Adam (Kingma & Ba, 2014) and AdaGrad (Duchi et al., 2011).", "startOffset": 113, "endOffset": 133}, {"referenceID": 17, "context": "Ranzato et al. (2006) proposed a sparse autoencoder model for learning sparse over-complete features.", "startOffset": 0, "endOffset": 22}, {"referenceID": 11, "context": "The sampled-output-loss methods (Jean et al., 2015) are limited to the softmax layer (output layer) and are only based on random sampling, while our method does not have those limitations.", "startOffset": 32, "endOffset": 51}, {"referenceID": 19, "context": "The sparsely-gated mixture-ofexperts (Shazeer et al., 2017) only sparsifies the mixtureof-experts gated layer and it is limited to the specific set-", "startOffset": 37, "endOffset": 59}, {"referenceID": 5, "context": "There are also prior studies focusing on reducing the communication cost in distributed systems (Seide et al., 2014; Dryden et al., 2016), by quantizing each value of the gradient from 32-bit float to only 1bit.", "startOffset": 96, "endOffset": 137}, {"referenceID": 14, "context": "Transition-based Dependency Parsing (Parsing): Following prior work, we use English Penn TreeBank (PTB) (Marcus et al., 1993) for evaluation.", "startOffset": 104, "endOffset": 125}, {"referenceID": 14, "context": "Transition-based Dependency Parsing (Parsing): Following prior work, we use English Penn TreeBank (PTB) (Marcus et al., 1993) for evaluation. We follow the standard split of the corpus and use sections 2-21 as the training set (39,832 sentences, 1,900,056 transition examples), section 22 as the development set (1,700 sentences, 80,234 transition examples) and section 23 as the final test set (2,416 sentences, 113,368 transition examples). The evaluation metric is unlabeled attachment score (UAS). We implement a parser using MLP following Chen and Manning (2014), which is used as our baseline.", "startOffset": 105, "endOffset": 568}, {"referenceID": 13, "context": "MNIST Image Recognition (MNIST): We use the MNIST handwritten digit dataset (LeCun et al., 1998) for evaluation.", "startOffset": 76, "endOffset": 96}, {"referenceID": 5, "context": "Furthermore, we test the GPU speedup on MLP with the large hidden dimension (Dryden et al., 2016).", "startOffset": 76, "endOffset": 97}, {"referenceID": 25, "context": "4% (Toutanova et al., 2003; Sun, 2014; Shen et al., 2007; Tsuruoka et al., 2011; Collobert et al., 2011; Huang et al., 2015).", "startOffset": 3, "endOffset": 124}, {"referenceID": 20, "context": "4% (Toutanova et al., 2003; Sun, 2014; Shen et al., 2007; Tsuruoka et al., 2011; Collobert et al., 2011; Huang et al., 2015).", "startOffset": 3, "endOffset": 124}, {"referenceID": 26, "context": "4% (Toutanova et al., 2003; Sun, 2014; Shen et al., 2007; Tsuruoka et al., 2011; Collobert et al., 2011; Huang et al., 2015).", "startOffset": 3, "endOffset": 124}, {"referenceID": 4, "context": "4% (Toutanova et al., 2003; Sun, 2014; Shen et al., 2007; Tsuruoka et al., 2011; Collobert et al., 2011; Huang et al., 2015).", "startOffset": 3, "endOffset": 124}, {"referenceID": 9, "context": "4% (Toutanova et al., 2003; Sun, 2014; Shen et al., 2007; Tsuruoka et al., 2011; Collobert et al., 2011; Huang et al., 2015).", "startOffset": 3, "endOffset": 124}, {"referenceID": 15, "context": "5 (Zhang & Clark, 2008; Nivre et al., 2007; Huang & Sagae, 2010).", "startOffset": 2, "endOffset": 64}, {"referenceID": 15, "context": "As one of the most popular transition-based parsers, MaltParser (Nivre et al., 2007) has 91.", "startOffset": 64, "endOffset": 84}, {"referenceID": 15, "context": "5 (Zhang & Clark, 2008; Nivre et al., 2007; Huang & Sagae, 2010). As one of the most popular transition-based parsers, MaltParser (Nivre et al., 2007) has 91.5 UAS. Chen and Manning (2014) achieves 92.", "startOffset": 24, "endOffset": 189}, {"referenceID": 13, "context": "3% (LeCun et al., 1998; Simard et al., 2003; Ciresan et al., 2010).", "startOffset": 3, "endOffset": 66}, {"referenceID": 21, "context": "3% (LeCun et al., 1998; Simard et al., 2003; Ciresan et al., 2010).", "startOffset": 3, "endOffset": 66}, {"referenceID": 1, "context": "3% (LeCun et al., 1998; Simard et al., 2003; Ciresan et al., 2010).", "startOffset": 3, "endOffset": 66}, {"referenceID": 10, "context": "With the help from convolutional layers and other techniques, the accuracy can be improved to over 99% (Jarrett et al., 2009; Ciresan et al., 2012).", "startOffset": 103, "endOffset": 147}, {"referenceID": 2, "context": "With the help from convolutional layers and other techniques, the accuracy can be improved to over 99% (Jarrett et al., 2009; Ciresan et al., 2012).", "startOffset": 103, "endOffset": 147}], "year": 2017, "abstractText": "We propose a simple yet effective technique for neural network learning. The forward propagation is computed as usual. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-k elements (in terms of magnitude) are kept. As a result, only k rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction (k divided by the vector dimension) in the computational cost. Surprisingly, experimental results demonstrate that we can update only 1\u20134% of the weights at each back propagation pass. This does not result in a larger number of training iterations. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given.", "creator": "LaTeX with hyperref package"}}}