{"id": "1706.06529", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2017", "title": "A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI", "abstract": "Two popular classes of methods for approximate inference are Markov chain Monte Carlo (MCMC) and variational inference. MCMC tends to be accurate if run for a long enough time, while variational inference tends to give better approximations at shorter time horizons. However, the amount of time needed for MCMC to exceed the performance of variational methods can be quite high, motivating more fine-grained tradeoffs. This paper derives a distribution over variational parameters, designed to minimize a bound on the divergence between the resulting marginal distribution and the target, and gives an example of how to sample from this distribution in a way that interpolates between the behavior of existing methods based on Langevin dynamics and stochastic gradient variational inference (SGVI).", "histories": [["v1", "Tue, 20 Jun 2017 16:06:50 GMT  (14701kb,D)", "http://arxiv.org/abs/1706.06529v1", "International Conference on Machine Learning (ICML) 2017"]], "COMMENTS": "International Conference on Machine Learning (ICML) 2017", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["justin domke"], "accepted": true, "id": "1706.06529"}, "pdf": {"name": "1706.06529.pdf", "metadata": {"source": "META", "title": "A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI", "authors": ["Justin Domke"], "emails": ["<domke@cs.umass.edu>."], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "1.1. Notation", "text": "This paper uses three notation conventions that are not universal: First, the conditional probability q (z | w) is alternatively written as qw (z) when appropriate; second, A'B indicates that A and B have the same expected value or (if A is a constant) that B is an unbiased estimator of A. Finally, uppercase and lowercase letters indicate which terms are random variables in a KL divergence. KL (q (Z | w) kp (Z | w)))) = Eq (Z | w) log (q (Z | w) / p (Z | w))) is thus the divergence between q (z | w) and p (z | w) for a fixed w, while KL (q (Z | W) kp (Z | W)) = Eq (W, Z) log (q (Z | W) / p (Z | W))) is a default conditional divergence."}, {"heading": "2. Information Theoretic Results", "text": "This section derives some results from an information theory point of view, without special consideration of the form of the target distribution or the way in which samples could be taken from it. Evidence for this is given in the appendix."}, {"heading": "2.1. Preliminaries", "text": "Let p (z) be the target distribution. To make it easier, z is treated here as a continuous variable, although the results in this section are correct if they are discrete. There is a fixed set of conditional distributions q (z | w), which can also be written as qw (z). In principle, you could know how to set q (w) in such a way that the resulting boundary distribution over z is close to p (z) as measured by the sound divergence, Dtrue = KL (q (Z) kp (Z))))) = Eq (Z) log q (Z) p (Z) p (Z) is difficult to control directly, as the boundary distribution q (z) cannot typically be evaluated in a closed form. A boundary results from the conditional KL divergence, as in the following Lemma.Lemma.Lemma 1. The deviation from q (z) to p (z) isKL (Z) kp."}, {"heading": "2.2. Divergence Bound and its Minimizer", "text": "This thesis is based on a convex combination of D0 and D1, parameterized by some 2 [0, 1], namely D0 = (1) D0 + D1. (7) Since D0 and D1 are upper limits, this also applies to D. The following theorem indicates the distribution q (w) to minimize D. Theorem 3. For fixed values of and p (w | z) the distribution q (w) results, the D isq (w) = exp s (w) A) (8) A = logZwexp s (w) s (w) = log p (w) KL (q (Z | w) kp (Z | w))) 1 1 KL (q (Z | w) kp (Z))))."}, {"heading": "Moreover, at q\u21e4, the objective value is D\u21e4 = A.", "text": "There is an upper limit for CLDivergence, A need not be positive."}, {"heading": "2.4. Latent variables in variational inference", "text": "The connection of p (w | z) to p (z) to define D1 is strongly related to additional random variables (Agakov & Barber, 2004), which have been investigated in variational conclusions to increase the representational power of q, e.g. by including latent stochastic transition operators (Salimans et al., 2015), random Gaussian process maps (Tran et al., 2016), or hierarchical variables (Ranganath et al., 2016). Imagine doing variation inferences with q (z, w), where w would be helpful now, and the goal would be to define \u2713 in such a way that q (z) p (z) p (z) p (z) p (z) p (z).Since w must be integrated, the entropy of z under q is typically intractable, but can be limited by an augmentation of p with a certain distribution p (w | z) and then optimize the common Lw and Lz over gmentation."}, {"heading": "3. Bayesian Inference", "text": "This section considers algorithms as a sample from the distribution defined by Equation 13. Probabilistic inference can be used in different environments, but for concreteness, the rest of this paper focuses on Bayesian inference. To fix the notation, we assume a set of N inputs xi with corresponding outputs yi. (In a generative environment xi would be empty.) If z is then a vector of parameters, the posterior distribution over z is up to a constant protocol p (z) = log p0 (z) + NXi = 1log p (yi | xi, z) + C, (14) where p0 (z) is the previous one and p (yi | xi, z) is the probability for the i-th date. The goal of probable inference is to evaluate expectations regarding p (z)."}, {"heading": "3.1. Langevin Dynamics", "text": "The objective of the MCMC methods is to obtain samples from the target distribution p (z). Langevin dynamics sample is obtained by an extremely simple process of repeating gradient steps of log (z) with injected Gaussian noise. Specifically, the iterate isz + 2 r log p (z) + p ', (15) where from a standard multivariate Gaussian distribution is sampled and is a step variable that can decay over time. If Langevin dynamics is used as a suggestion for a MetropolisHastings sampler, it can be shown that the correct acceptance ratio isexp s (z) s (z) s (z) k2 8 krs (z0) k2 (z z0) \u00b7 (rs (z) + namisHastings sampler, it is the correct acceptance ratio isexp s (z) s (z) = log p (z) s adjusted factor, and z0 is the suggested step from this Eq to 15."}, {"heading": "3.2. Stochastic Gradient Variational Inference", "text": "The aim of varying conclusions is to maximize L (w) = Eqw (Z) [log p (Z) log qw (Z)], (18) equivalent to minimizing the KL divergence between qw (z) and p (z). If it were possible to accurately calculate L (Ghahramani & Beal, 2000), this could generally not be maximized by a simple gradient iteration similar to w + 2 rwL (w). (19) While in some cases specific p and q can be used to precisely update L (Ghahramani & Beal, 2000), it is not possible to accurately evaluate the expectation of Z in L (Ranganath et al., 2014; Salimans & Knowles, 2014) to calculate the gradient as rL = Eqw (Z) [(log qw (Z) log qw (Z) log qw (Z))))) and to calculate it by taking samples from qw (this result may be Paqw)."}, {"heading": "3.2.1. REPARAMETERIZATION TRICK", "text": "rE \"s rf\u00fc ide rsrteeeirsrVo rf\u00fc ide rsrteeeirrsrVo rf\u00fc die rsrteeeirVo rf\u00fc die rsrteeeirsrVo rf\u00fc die rsrteeeirsrVo rf\u00fc die rsrteeeirsrVo rf\u00fc die rsrteeeirsrVo rf\u00fc die rteeeirsrVo rf\u00fc die rteeeirsrrrVo rf\u00fc die rteeeirsrrrVo rf\u00fc die rteeeirsrrsrVo rf\u00fc die rteeeirsrrrrsrVo rf\u00fc die rteeeirrrsrsrVo rf\u00fc.\" rdw"}, {"heading": "4. Hybrid Dynamics", "text": "An algorithm that interpolates between the methods of the previous section results from the application of the Langevin dynamics to the distribution over w, defined by Thm. 3 If the adjacent distribution p (w | z) is selected, as in paragraph 2.3, with some r (w) depending on it, then it defines L (w) = log r (w) + Eqw (Z) [log p (z) + (1) log q (z | w)]], (24) this is the time the form of s (w), which is derived in 5, lowering the constant runtime of log rz. Applying Langevin Dynamics2 to it results in iterationw w w + 2 rL (w) + p (25), which in turn is a step size that can decrease over time and is therefore sampled from a standard Gaussian distribution."}, {"heading": "5. Specifics For Gaussians", "text": "The following experiments use the simplest common variation distribution for qw (Z), namely a fully factored Gaussian distribution. To make w unlimited, allow w = (\u00b5), where \u00b5 is a vector of intermediate components and the standard deviation of the i-th dimension i = 10. To derive a vector r from this distribution, simply take zr, w = \u00b5 + r, where there is an elemental product. Entropy of qw (Z) is up to the constant factors H (w) = P i-i ln 10."}, {"heading": "It remains to set the base density. These experiments used", "text": "The value u was calculated to numerically optimize the divergence jumped by a standard one-dimensional Gaussian p (z). Since D = A can be calculated at the solution (theorem 3), for each given and u, D by symbolically integrating the divergence from \u00b5 and then numerically from the divergence, the values used in these experiments are lower, with linear interpolation for each other.0 0.1 0.2 0.3 0.4 0.5 0.6 0.8 0.9 1.0 u -.33 -.472 -.631 -.792 -.953 -1.11 -1.49 -1.74 -102For some step variables 0. The dynamics Langevin would immediately be w = 2 (we)."}, {"heading": "6. Experiments", "text": "There does not seem to be a single standard performance metric to evaluate approximate inference algorithms. Bayesian conclusions sometimes use accuracy or probability of test sets, but these metrics have high variance due to the dataset and mix the evaluation of the inference method with the evaluation of the model on which it is run. (An inference method with low coverage of p (z) may have higher accuracy than a perfect sampler based on model specifications or dataset specifications.) For a finer-grained measure of inference performance, this paper uses the maximum Mean Discrepancy (MMD) metric (Gretton et al., 2006; 2012). MMD is essentially the empirical difference between the averages of two samples in any feature space. We first draw a sample from p (z) by comparing a traditional MCMC algorithm (Stan, 2016) based on a variant of hamite to bitum."}, {"heading": "6.1. Toy Distributions", "text": "For a first demonstration, the algorithm was applied to a series of two-dimensional distributions of toys with different values. Some results are shown in Fig. 1, more in the appendix. While the algorithm indicates the expected trade-offs between speed and accuracy for different (Fig. 2), the results are quite uninteresting, as all curves cross near the same point in time / MMD where = 0 (variation inference) and = 1 (MCMC) meet. Therefore, either variation conclusions or MCMC are nearly optimal for a given period of time and therefore do not extend the \"Pareto boundary\" too much for these problems, although one might prefer an intermediate stage as the transition horizon is not known in advance."}, {"heading": "6.2. Bayesian Logistic Regression", "text": "Next, the algorithm was applied to binary logistic regression with multiple standard datasets, using a minibatch size of 25, a different random vector r for each element in the minibatch, and a standard multivariate laplace distribution as the previous one. Simply selecting a single value or schedule for the step is unsatisfactory as the best schedule for a given problem, the length of time, and the value of varies. To make a fair comparison, the best value of was selected retrospectively each time (averaging performance over repetitions before this choice). It is likely that decaying step sizes would work better than if they were not optimal even after 106 iterations, and then the best value of was chosen each time (with average performance over repetitions before this choice)."}, {"heading": "7. Discussion", "text": "This paper contains two contributions: First, it derives a distribution of variation parameters to minimize a boundary between marginal divergence and a target distribution; second, it is used to derive an algorithm that interpolates between Langevin dynamics and stochastic variation conclusions; and Bayesian logistic regression experiments show that the intermediate values of this algorithm are useful in the sense that there are several orders of magnitude of time horizons where an intermediate algorithm works better than either Langevin dynamics or variational inferences; and many improvements to stochastic Langevin dynamics and variational inferences have been proposed that go beyond the algorithms used here."}, {"heading": "Ghahramani, Zoubin and Beal, Matthew J. Propagation", "text": "Algorithms for variable Bajesian learning. In NIPS, 2000. 3.2"}, {"heading": "Grenander, Ulf and Miller, Michael. Representations of", "text": "Knowledge in complex systems. J. R. Statist. Soc., 56 (4): 549-603, 1994. 1"}, {"heading": "Gretton, Arthur, Borgwardt, Karsten M., Rasch, Malte J.,", "text": "Scho Stuelkopf, Bernhard, and Smola, Alexander J. A Core Method for the Two-Sample Problem. In NIPS, 2006. 6"}, {"heading": "Gretton, Arthur, Borgwardt, Karsten M., Rasch, Malte J.,", "text": "Scho Stulkopf, Bernhard, and Smola, Alexander J. A Core Test with Two Samples. Journal of Machine Learning Research, 13: 723-773, 2012. 6"}, {"heading": "Hoffman, Matthew D. and Gelman, Andrew. The no-u-turn", "text": "Sampler: Adaptable orbit length setting in the hamiltonian Montcarlo. Journal of Machine Learning Research, 15 (1): 1593-1623, 2014. 6"}, {"heading": "Hoffman, Matthew D., Blei, David M., Wang, Chong,", "text": "and Paisley, John William. Stochastic Conclusion of Variations. Journal of Machine Learning Research, 14 (1): 1303-1347, 2013. 7"}, {"heading": "Kingma, Diederik P and Welling, Max. Stochastic gradient", "text": "vb and the variable auto encoder. In ICLR, 2014. 3.2.1"}, {"heading": "Kingma, Diederik P., Salimans, Tim, and Welling, Max.", "text": "In NIPS, 2015. 3.2.1Kucukelbir, Alp, Tran, Dustin, Ranganath, Rajesh, Gelman, Andrew, and Lead, David M. Automatic Differentiation Variation Conclusion. Journal of Machine Learning Research, 18 (14): 1-45, 2017. 3.2.1Kushner, Harold and Yin, George. Stochastic Approximation and Recursive Algorithms and Applications. Springer-Verlag, 2003. 3.2.1"}, {"heading": "Li, Chunyuan, Chen, Changyou, Carlson, David E., and", "text": "Carin, Lawrence. Preconditioned stochastic gradient Langevin dynamics for deep neural networks. In AAAI, 2016. 7"}, {"heading": "Mandt, Stephan and Blei, David M. Smoothed gradients", "text": "for stochastic conclusions of variation. In NIPS, 2014. 7"}, {"heading": "Mandt, Stephan, Hoffman, Matthew D., and Blei, David M.", "text": "A Variation Analysis of Stochastic Gradient Algorithms. In ICML, 2016. 3.1Neal, Radford. MCMC using Hamiltonian dynamics, Band Handbook of Markov Chain Monte Carlo, pp. 113-162. Chapman & Hall / CRC Press, 2010. 3.1."}, {"heading": "Paisley, John William, Blei, David M., and Jordan,", "text": "Michael I. Variative Bayesian conclusion with stochastic search. In ICML, 2012. 3.2Patterson, Sam and Teh, Yee Whye. Stochastic gradient of Langevin dynamics on probability simplification. In NIPS, 2013. 7"}, {"heading": "Rahimi, Ali and Recht, Benjamin. Random features for", "text": "Mainframe with kernel. In NIPS, 2007. 6"}, {"heading": "Ranganath, Rajesh, Gerrish, Sean, and Blei, David M.", "text": "Black Box Variation Conclusion. In AISTATS, 2014. 3.2Ranganath, Rajesh, Tran, Dustin, and Lead, David M. Hierarchical Variation Models. In ICML, 2016. 2.4Rezende, Danilo Jimenez and Mohammed, Shakir. Conclusion of Variation with Normalizing Rivers. In ICML, 2015. 7"}, {"heading": "Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra,", "text": "Daan. Stochastic Repropagation and Conclusion of Variations in Deep Latent Gaussian Models. In International Conference on Machine Learning, 2014. 3.2.1Robert, Christian and Casella, George. Monte Carlo Statistical Methods. Springer-Verlag, 2nd edition, 2004. 1"}, {"heading": "Roeder, Geoffrey, Wu, Yuhuai, and Duvenaud, David.", "text": "Capturing the Landing: A Simple Gradient Reduced in Variance for Advance. NIPS Workshop in Advances in Approximate Bayesian Inference, 2016. 3.2.1Sakaya, Joseph and Klami, Arto. Re-use of Gradient Calculations in Automatic Conclusions of Variation. Technical Report, 2016. NIPS Workshop on Approximate Bayesian Inference. 7Salimans, Tim and Knowles, David A. On the Use of Control Variants with Stochastic Approximation for Variation Bays and their Connection to Stochastic Linear Regression. In ICLR, 2014. 3.2"}, {"heading": "Salimans, Tim, Kingma, Diedrik, and Welling, Max.", "text": "Markov chain monte carlo and variational inference: Bridging the gap. In ICML, 2015. 2.4, 7Team, Stan Development. Stan Modeling Language Users Guide and Reference Manual, 2.14.0. edition, 2016. 6Teh, Yee Whye, Thiery, Alexandre H., and Vollmer, Sebastian J. Consistency and fluctuations for stochastic gradient-Langevin dynamics. J. Mach. Learn. Res., 17 (1): 193-225, 2016. 3.1"}, {"heading": "Titsias, Michalis K. and La\u0301zaro-Gredilla, Miguel. Doubly", "text": "stochastic variation bays for non-conjugated inference. In ICML, 2014. 3.2.1"}, {"heading": "Tran, Dustin, Ranganath, Rajesh, and Blei, David M. The", "text": "In the ICLR process 2016."}, {"heading": "Welling, Max and Teh, Yee Whye. Bayesian learning via", "text": "stochastic gradient-Langevin dynamics. In ICML, 2011. 1, 3.1, 7"}, {"heading": "8. Appendix", "text": "This appendix contains additional diagrams and evidence for the results of Section 2.Lemma 6. The deviation from q (z) to p (z) is KL (q (Z) kp (Z))) = KL (q (Z | W) kp (Z)))) | {z} D0 Iq [W, Z], (27) where D0 = Eq (W, Z) log (q (Z | W) / p (Z))) is a conditional divergence and Iq under q.Proof. Define the common distribution p (w, z) = q (w) p (z). Subsequently, the KL divergence (Cover & Thomas, 2006, Thm. 2.5.3) states that KL (q (Z, W) kp (W)) = KL (w) = p (1) p (W | Z) + kZ (Z) kp (Z) kp (Z)."}, {"heading": "Moreover, at q\u21e4, the objective value is D\u21e4 = A.", "text": "The first is that the derivatives of D0 and D1 relating to q (w) q (= q) q (= q) q (= q) q (= q) q (= q) q (= q) q (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) p (w) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) z (z) z) z (z) z) z (z) z (z) z) (z) z (z) z) (z) z (z) z) (z) (z) z) (z) (z) p (z) p (z) z (z) p (z) p (z) p (z) p (z) p (z (z) p (z) p (z) p (z (z) p (z) p (z (z) p (z) p (z (z) p (z) p (z (z) p (z (z) p (z) p (z (z) p (z (z) p (z) p (z (z) p (z) p (z) p (z (z) p (z (z) p (z) p (z (z) p (z) p (z) p (z) p (f) p (z) p (z) p (f) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p) p (z) p (z) p) p (z) p (z) p (z) p (z) p) p (z) p) p) p (z) p) p (z) p (z) p (z) p) p) p ("}], "references": [{"title": "An auxiliary variational method", "author": ["Agakov", "Felix V", "Barber", "David"], "venue": "In NIPS,", "citeRegEx": "Agakov et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Agakov et al\\.", "year": 2004}, {"title": "Bayesian posterior sampling via stochastic gradient fisher scoring", "author": ["Ahn", "Sungjin", "Balan", "Anoop Korattikara", "Welling", "Max"], "venue": "In ICML,", "citeRegEx": "Ahn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2012}, {"title": "Variance reduction in stochastic gradient langevin dynamics", "author": ["Sinead A", "P\u00f3czos", "Barnab\u00e1s", "Smola", "Alexander J", "Xing", "Eric P"], "venue": "In NIPS,", "citeRegEx": "A. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "A. et al\\.", "year": 2016}, {"title": "Propagation algorithms for variational bayesian learning", "author": ["Ghahramani", "Zoubin", "Beal", "Matthew J"], "venue": "In NIPS,", "citeRegEx": "Ghahramani et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ghahramani et al\\.", "year": 2000}, {"title": "Representations of knowledge in complex systems", "author": ["Grenander", "Ulf", "Miller", "Michael"], "venue": "J. R. Statist. Soc.,", "citeRegEx": "Grenander et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Grenander et al\\.", "year": 1994}, {"title": "A kernel method for the two-sample-problem", "author": ["Gretton", "Arthur", "Borgwardt", "Karsten M", "Rasch", "Malte J", "Sch\u00f6lkopf", "Bernhard", "Smola", "Alexander J"], "venue": "In NIPS,", "citeRegEx": "Gretton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2006}, {"title": "A kernel two-sample test", "author": ["Gretton", "Arthur", "Borgwardt", "Karsten M", "Rasch", "Malte J", "Sch\u00f6lkopf", "Bernhard", "Smola", "Alexander J"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo", "author": ["Hoffman", "Matthew D", "Gelman", "Andrew"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hoffman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2014}, {"title": "Stochastic variational inference", "author": ["Hoffman", "Matthew D", "Blei", "David M", "Wang", "Chong", "Paisley", "John William"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Stochastic gradient vb and the variational auto-encoder", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Variational dropout and the local reparameterization trick", "author": ["Kingma", "Diederik P", "Salimans", "Tim", "Welling", "Max"], "venue": "In NIPS,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Automatic differentiation variational inference", "author": ["Kucukelbir", "Alp", "Tran", "Dustin", "Ranganath", "Rajesh", "Gelman", "Andrew", "Blei", "David M"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kucukelbir et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kucukelbir et al\\.", "year": 2017}, {"title": "Stochastic Approximation and Recursive Algorithms and Applications", "author": ["Kushner", "Harold", "Yin", "George"], "venue": null, "citeRegEx": "Kushner et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kushner et al\\.", "year": 2003}, {"title": "Preconditioned stochastic gradient langevin dynamics for deep neural networks", "author": ["Li", "Chunyuan", "Chen", "Changyou", "Carlson", "David E", "Carin", "Lawrence"], "venue": "In AAAI,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Smoothed gradients for stochastic variational inference", "author": ["Mandt", "Stephan", "Blei", "David M"], "venue": "In NIPS,", "citeRegEx": "Mandt et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mandt et al\\.", "year": 2014}, {"title": "A variational analysis of stochastic gradient algorithms", "author": ["Mandt", "Stephan", "Hoffman", "Matthew D", "Blei", "David M"], "venue": "In ICML,", "citeRegEx": "Mandt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mandt et al\\.", "year": 2016}, {"title": "MCMC using Hamiltonian dynamics, volume Handbook of Markov Chain Monte Carlo", "author": ["Neal", "Radford"], "venue": null, "citeRegEx": "Neal and Radford.,? \\Q2010\\E", "shortCiteRegEx": "Neal and Radford.", "year": 2010}, {"title": "Variational bayesian inference with stochastic search", "author": ["Paisley", "John William", "Blei", "David M", "Jordan", "Michael I"], "venue": "In ICML,", "citeRegEx": "Paisley et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Paisley et al\\.", "year": 2012}, {"title": "Stochastic gradient riemannian langevin dynamics on the probability simplex", "author": ["Patterson", "Sam", "Teh", "Yee Whye"], "venue": "In NIPS,", "citeRegEx": "Patterson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Patterson et al\\.", "year": 2013}, {"title": "Random features for large-scale kernel machines", "author": ["Rahimi", "Ali", "Recht", "Benjamin"], "venue": "In NIPS,", "citeRegEx": "Rahimi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi et al\\.", "year": 2007}, {"title": "Black box variational inference", "author": ["Ranganath", "Rajesh", "Gerrish", "Sean", "Blei", "David M"], "venue": "In AISTATS,", "citeRegEx": "Ranganath et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2014}, {"title": "Hierarchical variational models", "author": ["Ranganath", "Rajesh", "Tran", "Dustin", "Blei", "David M"], "venue": "In ICML,", "citeRegEx": "Ranganath et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2016}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo Jimenez", "Mohammed", "Shakir"], "venue": "In ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and variational inference in deep latent gaussian models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Sticking the landing: A simple reduced-variance gradient for advi", "author": ["Roeder", "Geoffrey", "Wu", "Yuhuai", "Duvenaud", "David"], "venue": "NIPS workshop in Advances in Approximate Bayesian Inference,", "citeRegEx": "Roeder et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Roeder et al\\.", "year": 2016}, {"title": "Re-using gradient computations in automatic variational inference", "author": ["Sakaya", "Joseph", "Klami", "Arto"], "venue": "Technical report,", "citeRegEx": "Sakaya et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sakaya et al\\.", "year": 2016}, {"title": "On using control variates with stochastic approximation for variational bayes and its connection to stochastic linear regression", "author": ["Salimans", "Tim", "Knowles", "David A"], "venue": "In ICLR,", "citeRegEx": "Salimans et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2014}, {"title": "Markov chain monte carlo and variational inference: Bridging the gap", "author": ["Salimans", "Tim", "Kingma", "Diedrik", "Welling", "Max"], "venue": "In ICML,", "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "Consistency and fluctuations for stochastic gradient langevin dynamics", "author": ["Teh", "Yee Whye", "Thiery", "Alexandre H", "Vollmer", "Sebastian J"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Teh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2016}, {"title": "Doubly stochastic variational bayes for non-conjugate inference", "author": ["Titsias", "Michalis K", "L\u00e1zaro-Gredilla", "Miguel"], "venue": "In ICML,", "citeRegEx": "Titsias et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Titsias et al\\.", "year": 2014}, {"title": "The variational gaussian process", "author": ["Tran", "Dustin", "Ranganath", "Rajesh", "Blei", "David M"], "venue": "In ICLR,", "citeRegEx": "Tran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2016}, {"title": "Bayesian learning via stochastic gradient langevin dynamics", "author": ["Welling", "Max", "Teh", "Yee Whye"], "venue": "In ICML,", "citeRegEx": "Welling et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 27, "context": "by including latent stochastic transition operators (Salimans et al., 2015), random Gaussian process mappings (Tran et al.", "startOffset": 52, "endOffset": 75}, {"referenceID": 30, "context": ", 2015), random Gaussian process mappings (Tran et al., 2016), or hierarchical variables (Ranganath et al.", "startOffset": 42, "endOffset": 61}, {"referenceID": 21, "context": ", 2016), or hierarchical variables (Ranganath et al., 2016).", "startOffset": 35, "endOffset": 59}, {"referenceID": 15, "context": "(See (Mandt et al., 2016) for an analysis.", "startOffset": 5, "endOffset": 25}, {"referenceID": 28, "context": ") In the limit of a small step-size, the variance of the noise due to stochastic estimation of the gradient of log p(z) will be of order \u270f while the variance of the injected noise is of order \u270f, meaning the latter dominates (Welling & Teh, 2011; Teh et al., 2016).", "startOffset": 224, "endOffset": 263}, {"referenceID": 20, "context": "One line of approach to this (Ranganath et al., 2014; Salimans & Knowles, 2014) is to write the gradient as rL = Eqw(Z)[(log qw(Z) log p(Z))r log qw(Z)], and estimate this by drawing samples from qw(z).", "startOffset": 29, "endOffset": 79}, {"referenceID": 17, "context": "Firstly, one can use control variates, based on either Taylor expansion (Paisley et al., 2012) or the fact that the expected value of rw log qw(Z) is zero.", "startOffset": 72, "endOffset": 94}, {"referenceID": 23, "context": "REPARAMETERIZATION TRICK Another line of approach to variational inference is based on the \u201creparameterization trick\u201d (Kingma & Welling, 2014; Rezende et al., 2014; Titsias & L\u00e1zaro-Gredilla, 2014), known in the stochastic approximation literature as a \u201cpathwise\u201d derivative (Kushner & Yin, 2003, Sec.", "startOffset": 118, "endOffset": 197}, {"referenceID": 24, "context": "If H(w) cannot be integrated in closed-form an alternative estimator based on L(w) = ER[log p(zR,w) log qw(zR,w)] (22) is possible, and in some circumstances this can even have lower variance (Roeder et al., 2016).", "startOffset": 192, "endOffset": 213}, {"referenceID": 11, "context": "Alternatively, this can all be done efficiently by automatic differentiation, (Kucukelbir et al., 2017) the approach used here.", "startOffset": 78, "endOffset": 103}, {"referenceID": 10, "context": "However, variance can often be reduced by the \u201clocal reparameterization trick\u201d (Kingma et al., 2015) in which a different random vector ri is drawn for each datum in the minibatch.", "startOffset": 79, "endOffset": 100}, {"referenceID": 5, "context": ") For a more fine-grained measure of inference performance, this paper uses the Maximum Mean Discrepancy (MMD) measure (Gretton et al., 2006; 2012).", "startOffset": 119, "endOffset": 147}], "year": 2017, "abstractText": "Two popular classes of methods for approximate inference are Markov chain Monte Carlo (MCMC) and variational inference. MCMC tends to be accurate if run for a long enough time, while variational inference tends to give better approximations at shorter time horizons. However, the amount of time needed for MCMC to exceed the performance of variational methods can be quite high, motivating more fine-grained tradeoffs. This paper derives a distribution over variational parameters, designed to minimize a bound on the divergence between the resulting marginal distribution and the target, and gives an example of how to sample from this distribution in a way that interpolates between the behavior of existing methods based on Langevin dynamics and stochastic gradient variational inference (SGVI).", "creator": "LaTeX with hyperref package"}}}