{"id": "1706.06873", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2017", "title": "MEC: Memory-efficient Convolution for Deep Neural Network", "abstract": "Convolution is a critical component in modern deep neural networks, thus several algorithms for convolution have been developed. Direct convolution is simple but suffers from poor performance. As an alternative, multiple indirect methods have been proposed including im2col-based convolution, FFT-based convolution, or Winograd-based algorithm. However, all these indirect methods have high memory-overhead, which creates performance degradation and offers a poor trade-off between performance and memory consumption. In this work, we propose a memory-efficient convolution or MEC with compact lowering, which reduces memory-overhead substantially and accelerates convolution process. MEC lowers the input matrix in a simple yet efficient/compact way (i.e., much less memory-overhead), and then executes multiple small matrix multiplications in parallel to get convolution completed. Additionally, the reduced memory footprint improves memory sub-system efficiency, improving performance. Our experimental results show that MEC reduces memory consumption significantly with good speedup on both mobile and server platforms, compared with other indirect convolution algorithms.", "histories": [["v1", "Wed, 21 Jun 2017 13:00:39 GMT  (264kb,D)", "http://arxiv.org/abs/1706.06873v1", "ICML2017"]], "COMMENTS": "ICML2017", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["minsik cho", "daniel brand"], "accepted": true, "id": "1706.06873"}, "pdf": {"name": "1706.06873.pdf", "metadata": {"source": "CRF", "title": "MEC: Memory-efficient Convolution for Deep Neural Network", "authors": ["Minsik Cho", "Daniel Brand"], "emails": ["<minsikcho@us.ibm.com>."], "sections": [{"heading": "1. Introduction", "text": "In these layers, the convolutional layer is one of the most important but the slowest and most memory-intensive forms of consumption in advanced / modern Convolutionary DNN (Abuzaid et al., 2015; Chen et al., 2016; Cong & 1IBM T. Watson Research Center, NY, USA). Correspondence on: Minsik Cho < minsikcho @ us.ibm.com >.Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by author: Xiao, 2014; Denton et al, 2014; Park et al., 2014; Vasilache et al., 2014."}, {"heading": "2. Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Notations", "text": "The notation used in this paper is listed in Table 1. For integers we use lower case letters, for tensors and matrices we use upper case letters. The array can be interpreted as consisting of p sections, each of which is subdivided into q subsections and each has r elements. The same array can also be interpreted as a p \u00b7 qr matrix, or as a pq \u00b7 r matrix, etc. We interpret a tensor specifically as a matrix if it requires matrix operations, otherwise (i.e. for data movements) we retain the tensor form. If we work with a math library, such as cuBLAS (cuBLAS), which requires an order of column sizes, then we still use the same matrix main form, but interpret all matrices as transposed. We use the notation to write a b element to a sub-i matrix: most of the matrices can be represented as A-b matrix."}, {"heading": "2.2. Previous Work", "text": "Due to the importance of DNN, several techniques for efficient convolution calculation have been proposed (Chetlur et al., 2014; Perkins et al., 2016).The most relevant for our work are im2col-based convolution, FFT (Fast Fourier Transform) -based convolution (Highlander & Rodriguez, 2016; Mathieu et al., 2013; Vasilache et al., 2014), and Winograd-based convolution (Lavin, 2015).MEC provides the same functionality with reduced memory requirements. \u2022 im2col-based convolution transforms / lowers the input matrix into a toeplitz matrix with redundancy (a.k.a, lowered matrix) such conversion can be performed as fast matrix multiplication that can take advantage of interactivity."}, {"heading": "3. Algorithm", "text": "In this section, we propose our folding algorithm, MEC, with detailed examples. The main goal of MEC is to reduce the memory overhead during folding, which can be beneficial for any Convolutionary DNN in three ways: \u2022 MEC can enable training or inferring a larger model for a given storage capacity. \u2022 MEC can enable larger mini-batch sizes to speed up the turn-around / per-era latency during training. \u2022 MEC can speed up calculation by improving the efficiency of storage subsystems (e.g. more cache hits). Unlike the widely used im2col-based folding (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, 2014), MEC performs compact / LAS-friendly reductions so that memory overhead can be minimized without compromising performance / accuracy."}, {"heading": "3.1. Motivation", "text": "In this section we review im2col-based convolution and its advantages and disadvantages with Fig. 1, the direct convolution in (a) and im2col-based convolution using BLAS in (b). In direct convolution, an element of the output matrix O is generated by a dot product between the kernel K and a sub-matrix of the input I. The sub-matrices are obtained by pushing K over I in both dimensions. Each subsequent sub-matrix is achieved by shifting the distance or SW. For example, Fig. 1 (a) shows two sub-matrices in grey and dotted boxes respectively. The 3 \u00d7 3 kernels are processed to generate the corresponding output values in grey and dotted boxes (i.e., 3 and 4), respectively dotted boxes."}, {"heading": "3.2. MEC Overview", "text": "The main reason why the im2col-based algorithm has so much memory overhead is that there is a significant level of redundancy in the lowered matrix when sh or sw is small and K. And the overhead gets even worse when it is relatively smaller than I do, which is therefore common in the state-of-the-art DNN architecture (er et al., 2015; Simonyan & Zisserman, 2014; Szegedy et al., 2014). To reduce memory overhead, it is crucial to reduce the amount of redundancy in the lowered matrix and keep the compatibility pattern BLAS-compatible (otherwise, the bad compilation itself can slow down the entire evolution).MEC exceeds such challenges by reducing multiple columns at once, rather than each one at a time."}, {"heading": "3.3. MEC Algorithm", "text": "In this section, we present the full MEC by extending Algorithm 1 to Algorithm 2 to edit the channels (ic and kc) and mini-batches (in), and discuss the implementation details in the context of deep learning (mainly via image format output). However, due to the compact lowering in MEC, it is mathematically advantageous that I am able to use myself in the way shown in Table 2, because it eliminates vertical redundant pixels and re-covers them in a contiguous memory space. (I, K, s)."}, {"heading": "3.4. Analysis", "text": "In this section, we analyze memory storage in the MEC using im2col-based folding. The size of the lowered matrix, L in the MEC, is: inowihkwkc (3) Compared to the lowered matrix of im2col (see Equation (2)), approximately a factor of kh results. For a more accurate comparison, let us make their difference R.R = inkc (ohowkhkw \u2212 owihkw) = inkcowkw (ohkh \u2212 ih) = inkcowkw (ih \u2212 khsh + kh \u2212 ih) = inkcowkw (ih \u2212 kh) (kh \u2212 1) (4). Since ih > kh, MEC always reduces the memory requirement as long as kh > sh (i.e. there is overlap between the core instances) exists. Note that in the case of kh \u2264 sh, no redundant information has to be eliminated."}, {"heading": "4. Experimental Results", "text": "We implemented MEC for CPU / GPU in C + + with multithreaded OpenBLAS, OpenMP and cuBLAS (cuBLAS) with single 32-bit precision. We also implemented a fully parallel im2col-based convolution on CPU / GPU (Jia, 2014) with the same libraries. We compared MEC with other open source conversion packages in C + + to make a fair point-by-point comparison and accurately capture the memory overhead and performance. We uploaded an open source FFT-based conversion (cuFFT, Theano-FFT) for GPU. We adopted an open source winograde-based convolution (Falcon, 2016) and optimized it to reduce the memory overhead for CPU, and further modified / optimized it for GPU following (Lavin, 2015; et al al)."}, {"heading": "5. Conclusion", "text": "In this paper, we introduced MEC, a memory-efficient folding algorithm for deep learning. We proposed a novel matrix lowering scheme to improve memory efficiency for MEC, which also improves computing efficiency due to the reduced storage footprint. We can clearly observe from extensive experiments that MEC requires the least amount of memory, but in most cases provides high performance on both mobile and server platforms, positioning MEC as an attractive folding engine across multiple platforms. MEC is well suited for ForDNN-based applications in memory-constrained environments such as Mobile / IoT, while increasing DNN's learning capacity on high-end server systems. In this appendix, we outline Wino.gpu optimizations in detail. Our Wino.gpu blocks are all hand-tuned / fully unrolled F (2 x, 3 x), which can fit into the cache in applications."}], "references": [{"title": "Caffe con troll: Shallow ideas to speed up deep learning", "author": ["Abuzaid", "Firas", "Hadjis", "Stefan", "Zhang", "Ce", "R\u00e9", "Christopher"], "venue": "CoRR, abs/1504.04343,", "citeRegEx": "Abuzaid et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abuzaid et al\\.", "year": 2015}, {"title": "High Performance Convolutional Neural Networks for Document Processing", "author": ["Chellapilla", "Kumar", "Puri", "Sidd", "Simard", "Patrice"], "venue": "In Tenth International Workshop on Frontiers in Handwriting Recognition,", "citeRegEx": "Chellapilla et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chellapilla et al\\.", "year": 2006}, {"title": "Compressing neural networks with the hashing", "author": ["Chen", "Wenlin", "Wilson", "James T", "Tyree", "Stephen", "Weinberger", "Kilian Q", "Yixin"], "venue": "trick. CoRR,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks", "author": ["Chen", "Yu-Hsin", "Krishna", "Tushar", "Emer", "Joel", "Sze", "Vivienne"], "venue": "In IEEE International Solid-State Circuits Conference,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["Chetlur", "Sharan", "Woolley", "Cliff", "Vandermersch", "Philippe", "Cohen", "Jonathan", "Tran", "John", "Catanzaro", "Bryan", "Shelhamer", "Evan"], "venue": "CoRR, abs/1410.0759,", "citeRegEx": "Chetlur et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chetlur et al\\.", "year": 2014}, {"title": "Memory bounded deep convolutional networks", "author": ["Collins", "Maxwell D", "Kohli", "Pushmeet"], "venue": "CoRR, abs/1412.1442,", "citeRegEx": "Collins et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2014}, {"title": "Minimizing computation in convolutional neural networks", "author": ["Cong", "Jason", "Xiao", "Bingjun"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "Cong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cong et al\\.", "year": 2014}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Denton", "Emily", "Zaremba", "Wojciech", "Bruna", "Joan", "LeCun", "Yann", "Fergus", "Rob"], "venue": "CoRR, abs/1404.0736,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Gong", "Yunchao", "Liu", "Yang", "Ming", "Bourdev", "Lubomir D"], "venue": "CoRR, abs/1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "coding. CoRR,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In arXiv prepring arXiv:1506.01497,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Very efficient training of convolutional neural networks using fast fourier transform and overlap-and-add", "author": ["Highlander", "Tyler", "Rodriguez", "Andres"], "venue": null, "citeRegEx": "Highlander et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Highlander et al\\.", "year": 2016}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Jaderberg", "Max", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "CoRR, abs/1405.3866,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Learning Semantic Image Representations at a Large Scale", "author": ["Jia", "Yangqing"], "venue": "PhD thesis, EECS Department,", "citeRegEx": "Jia and Yangqing.,? \\Q2014\\E", "shortCiteRegEx": "Jia and Yangqing.", "year": 2014}, {"title": "Gemmbased level 3 blas: High-performance model implementations and performance evaluation benchmark", "author": ["K\u00e5gstr\u00f6m", "Bo", "Ling", "Per", "van Loan", "Charles"], "venue": "ACM Trans. Math. Softw.,", "citeRegEx": "K\u00e5gstr\u00f6m et al\\.,? \\Q1998\\E", "shortCiteRegEx": "K\u00e5gstr\u00f6m et al\\.", "year": 1998}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Kim", "Yong-Deok", "Park", "Eunhyeok", "Yoo", "Sungjoo", "Choi", "Taelim", "Yang", "Lu", "Shin", "Dongjun"], "venue": "CoRR, abs/1511.06530,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Deepx: A software accelerator for low-power deep learning inference on mobile devices", "author": ["N.D. Lane", "S. Bhattacharya", "P. Georgiev", "C. Forlivesi", "L. Jiao", "L. Qendro", "F. Kawsar"], "venue": null, "citeRegEx": "Lane et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lane et al\\.", "year": 2016}, {"title": "An early resource characterization of deep learning on wearables, smartphones and internet-of-things devices", "author": ["Lane", "Nicholas D", "Bhattacharya", "Sourav", "Georgiev", "Petko", "Forlivesi", "Claudio", "Kawsar", "Fahim"], "venue": "In Proceedings of the 2015 International Workshop on Internet", "citeRegEx": "Lane et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lane et al\\.", "year": 2015}, {"title": "Fast algorithms for convolutional neural networks", "author": ["Lavin", "Andrew"], "venue": "CoRR, abs/1509.09308,", "citeRegEx": "Lavin and Andrew.,? \\Q2015\\E", "shortCiteRegEx": "Lavin and Andrew.", "year": 2015}, {"title": "Speedingup convolutional neural networks using fine-tuned cpdecomposition", "author": ["Lebedev", "Vadim", "Ganin", "Yaroslav", "Rakhuba", "Maksim", "Oseledets", "Ivan V", "Lempitsky", "Victor S"], "venue": "CoRR, abs/1412.6553,", "citeRegEx": "Lebedev et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lebedev et al\\.", "year": 2014}, {"title": "Fast training of convolutional networks through ffts", "author": ["Mathieu", "Micha\u00ebl", "Henaff", "Mikael", "LeCun", "Yann"], "venue": "CoRR, abs/1312.5851,", "citeRegEx": "Mathieu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2013}, {"title": "Holistic sparsecnn: Forging the trident of accuracy, speed, and size", "author": ["Park", "Jongsoo", "Li", "Sheng R", "Wen", "Wei", "Hai", "Chen", "Yiran", "Dubey", "Pradeep"], "venue": "CoRR, abs/1608.01409,", "citeRegEx": "Park et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Park et al\\.", "year": 2016}, {"title": "cltorch: a hardware-agnostic backend for the torch deep neural network library, based on opencl", "author": ["Perkins", "Hugh"], "venue": null, "citeRegEx": "Perkins and Hugh.,? \\Q2016\\E", "shortCiteRegEx": "Perkins and Hugh.", "year": 2016}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": "CoRR, abs/1312.6229,", "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan and Zisserman,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2014}, {"title": "Fast convolutional nets with fbfft: A GPU performance evaluation", "author": ["Vasilache", "Nicolas", "Johnson", "Jeff", "Mathieu", "Micha\u00ebl", "Chintala", "Soumith", "Piantino", "Serkan", "LeCun", "Yann"], "venue": "CoRR, abs/1412.7580,", "citeRegEx": "Vasilache et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vasilache et al\\.", "year": 2014}, {"title": "Accelerating convolutional neural networks for mobile applications", "author": ["Wang", "Peisong", "Cheng", "Jian"], "venue": "In Proceedings of the 2016 ACM on Multimedia Conference,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Arithmetic complexity of computations", "author": ["Winograd", "Shmuel"], "venue": null, "citeRegEx": "Winograd and Shmuel.,? \\Q1980\\E", "shortCiteRegEx": "Winograd and Shmuel.", "year": 1980}], "referenceMentions": [{"referenceID": 1, "context": "To address the performance issues in convolutional layers, efficient/approximation algorithms have been proposed (Chellapilla et al., 2006; Denton et al., 2014; Jaderberg et al., 2014; Jia, 2014; Vasilache et al., 2014), tailed implementations for limited cases have been actively investigated (Lavin, 2015), and industrial-strength libraries are offered (Chetlur et al.", "startOffset": 113, "endOffset": 219}, {"referenceID": 7, "context": "To address the performance issues in convolutional layers, efficient/approximation algorithms have been proposed (Chellapilla et al., 2006; Denton et al., 2014; Jaderberg et al., 2014; Jia, 2014; Vasilache et al., 2014), tailed implementations for limited cases have been actively investigated (Lavin, 2015), and industrial-strength libraries are offered (Chetlur et al.", "startOffset": 113, "endOffset": 219}, {"referenceID": 12, "context": "To address the performance issues in convolutional layers, efficient/approximation algorithms have been proposed (Chellapilla et al., 2006; Denton et al., 2014; Jaderberg et al., 2014; Jia, 2014; Vasilache et al., 2014), tailed implementations for limited cases have been actively investigated (Lavin, 2015), and industrial-strength libraries are offered (Chetlur et al.", "startOffset": 113, "endOffset": 219}, {"referenceID": 26, "context": "To address the performance issues in convolutional layers, efficient/approximation algorithms have been proposed (Chellapilla et al., 2006; Denton et al., 2014; Jaderberg et al., 2014; Jia, 2014; Vasilache et al., 2014), tailed implementations for limited cases have been actively investigated (Lavin, 2015), and industrial-strength libraries are offered (Chetlur et al.", "startOffset": 113, "endOffset": 219}, {"referenceID": 4, "context": ", 2014), tailed implementations for limited cases have been actively investigated (Lavin, 2015), and industrial-strength libraries are offered (Chetlur et al., 2014).", "startOffset": 143, "endOffset": 165}, {"referenceID": 2, "context": ", mobile/IOT devices) (Chen et al., 2015; Collins & Kohli, 2014; Gong et al., 2014; Kim et al., 2015; Lebedev et al., 2014; Wang & Cheng, 2016) so as to minimize response delay (e.", "startOffset": 22, "endOffset": 143}, {"referenceID": 8, "context": ", mobile/IOT devices) (Chen et al., 2015; Collins & Kohli, 2014; Gong et al., 2014; Kim et al., 2015; Lebedev et al., 2014; Wang & Cheng, 2016) so as to minimize response delay (e.", "startOffset": 22, "endOffset": 143}, {"referenceID": 15, "context": ", mobile/IOT devices) (Chen et al., 2015; Collins & Kohli, 2014; Gong et al., 2014; Kim et al., 2015; Lebedev et al., 2014; Wang & Cheng, 2016) so as to minimize response delay (e.", "startOffset": 22, "endOffset": 143}, {"referenceID": 20, "context": ", mobile/IOT devices) (Chen et al., 2015; Collins & Kohli, 2014; Gong et al., 2014; Kim et al., 2015; Lebedev et al., 2014; Wang & Cheng, 2016) so as to minimize response delay (e.", "startOffset": 22, "endOffset": 143}, {"referenceID": 9, "context": ", better user experience) and network overhead (Han et al., 2015; Lane et al., 2016; 2015).", "startOffset": 47, "endOffset": 90}, {"referenceID": 17, "context": ", better user experience) and network overhead (Han et al., 2015; Lane et al., 2016; 2015).", "startOffset": 47, "endOffset": 90}, {"referenceID": 4, "context": "Previous Work Due to the importance of DNN, several techniques for efficient convolution computation have been proposed (Chetlur et al., 2014; Perkins, 2016).", "startOffset": 120, "endOffset": 157}, {"referenceID": 21, "context": "The most relevant to our work is im2col-based convolution, FFT (Fast Fourier Transform)-based convolution (Highlander & Rodriguez, 2016; Mathieu et al., 2013; Vasilache et al., 2014), and Winograd-based convolution (Lavin, 2015).", "startOffset": 106, "endOffset": 182}, {"referenceID": 26, "context": "The most relevant to our work is im2col-based convolution, FFT (Fast Fourier Transform)-based convolution (Highlander & Rodriguez, 2016; Mathieu et al., 2013; Vasilache et al., 2014), and Winograd-based convolution (Lavin, 2015).", "startOffset": 106, "endOffset": 182}, {"referenceID": 1, "context": "a, lowered matrix) such that convolution can be performed as fast matrix-matrix multiplication, which can take advantage of highly optimized linear algebra packages including BLAS (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, 2014).", "startOffset": 180, "endOffset": 239}, {"referenceID": 4, "context": "a, lowered matrix) such that convolution can be performed as fast matrix-matrix multiplication, which can take advantage of highly optimized linear algebra packages including BLAS (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, 2014).", "startOffset": 180, "endOffset": 239}, {"referenceID": 4, "context": ", 3x3) than input matrices (Chetlur et al., 2014; He et al., 2015; Perkins, 2016; Simonyan & Zisserman, 2014).", "startOffset": 27, "endOffset": 109}, {"referenceID": 10, "context": ", 3x3) than input matrices (Chetlur et al., 2014; He et al., 2015; Perkins, 2016; Simonyan & Zisserman, 2014).", "startOffset": 27, "endOffset": 109}, {"referenceID": 7, "context": "In contrast to the above schemes, which do not degrade accuracy, various approximation strategies have been proposed including low-rank/monochromatic approximation (Denton et al., 2014; Jaderberg et al., 2014), vector quantization (Gong et al.", "startOffset": 164, "endOffset": 209}, {"referenceID": 12, "context": "In contrast to the above schemes, which do not degrade accuracy, various approximation strategies have been proposed including low-rank/monochromatic approximation (Denton et al., 2014; Jaderberg et al., 2014), vector quantization (Gong et al.", "startOffset": 164, "endOffset": 209}, {"referenceID": 8, "context": ", 2014), vector quantization (Gong et al., 2014), fine-tuning (Lebedev et al.", "startOffset": 29, "endOffset": 48}, {"referenceID": 20, "context": ", 2014), fine-tuning (Lebedev et al., 2014), and DCT (Discrete Cosine Transform)/hashing (Lebedev et al.", "startOffset": 21, "endOffset": 43}, {"referenceID": 20, "context": ", 2014), and DCT (Discrete Cosine Transform)/hashing (Lebedev et al., 2014).", "startOffset": 53, "endOffset": 75}, {"referenceID": 1, "context": "In contrast to the widely-adopted im2col-based convolution (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, 2014), MEC performs compact/BLAS-friendly lowering such that memory-overhead can be minimized without degrading performance/accuracy.", "startOffset": 59, "endOffset": 118}, {"referenceID": 4, "context": "In contrast to the widely-adopted im2col-based convolution (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, 2014), MEC performs compact/BLAS-friendly lowering such that memory-overhead can be minimized without degrading performance/accuracy.", "startOffset": 59, "endOffset": 118}, {"referenceID": 1, "context": "im2col) and gemm in BLAS (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, 2014) by off-loading the geometry-specific specializations in convolution to a plain matrix, which is depicted in Fig.", "startOffset": 25, "endOffset": 84}, {"referenceID": 4, "context": "im2col) and gemm in BLAS (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, 2014) by off-loading the geometry-specific specializations in convolution to a plain matrix, which is depicted in Fig.", "startOffset": 25, "endOffset": 84}, {"referenceID": 4, "context": "im2col-based convolution is generic enough to be used in any DNN on both mobile/IoT and high-end platforms (Chetlur et al., 2014; Lane et al., 2015).", "startOffset": 107, "endOffset": 148}, {"referenceID": 18, "context": "im2col-based convolution is generic enough to be used in any DNN on both mobile/IoT and high-end platforms (Chetlur et al., 2014; Lane et al., 2015).", "startOffset": 107, "endOffset": 148}, {"referenceID": 10, "context": "And, the overhead becomes even worse when K is relatively smaller than I which occurs frequently in the state-of-the-art DNN architectures (He et al., 2015; Perkins, 2016; Simonyan & Zisserman, 2014; Szegedy et al., 2014).", "startOffset": 139, "endOffset": 221}, {"referenceID": 10, "context": "For thorough comparison, we built a comprehensive benchmark set consisting of 12 unique convolution layers, cv1-cv12 from various public DNNs (He et al., 2015; Krizhevsky et al., 2012; Sermanet et al., 2013; Simonyan & Zisserman, 2014; Szegedy et al., 2014) as in Table 2.", "startOffset": 142, "endOffset": 257}, {"referenceID": 16, "context": "For thorough comparison, we built a comprehensive benchmark set consisting of 12 unique convolution layers, cv1-cv12 from various public DNNs (He et al., 2015; Krizhevsky et al., 2012; Sermanet et al., 2013; Simonyan & Zisserman, 2014; Szegedy et al., 2014) as in Table 2.", "startOffset": 142, "endOffset": 257}, {"referenceID": 24, "context": "For thorough comparison, we built a comprehensive benchmark set consisting of 12 unique convolution layers, cv1-cv12 from various public DNNs (He et al., 2015; Krizhevsky et al., 2012; Sermanet et al., 2013; Simonyan & Zisserman, 2014; Szegedy et al., 2014) as in Table 2.", "startOffset": 142, "endOffset": 257}, {"referenceID": 10, "context": "cpu to ResNet-101 in (He et al., 2015) and estimated the weighted impact on memory-overhead and runtime on Mobile as in Table 3, which shows that MEC.", "startOffset": 21, "endOffset": 38}, {"referenceID": 10, "context": "ResNet-101 (He et al., 2015) on Mobile.", "startOffset": 11, "endOffset": 28}], "year": 2017, "abstractText": "Convolution is a critical component in modern deep neural networks, thus several algorithms for convolution have been developed. Direct convolution is simple but suffers from poor performance. As an alternative, multiple indirect methods have been proposed including im2colbased convolution, FFT-based convolution, or Winograd-based algorithm. However, all these indirect methods have high memory-overhead, which creates performance degradation and offers a poor trade-off between performance and memory consumption. In this work, we propose a memory-efficient convolution or MEC with compact lowering, which reduces memoryoverhead substantially and accelerates convolution process. MEC lowers the input matrix in a simple yet efficient/compact way (i.e., much less memory-overhead), and then executes multiple small matrix multiplications in parallel to get convolution completed. Additionally, the reduced memory footprint improves memory subsystem efficiency, improving performance. Our experimental results show that MEC reduces memory consumption significantly with good speedup on both mobile and server platforms, compared with other indirect convolution algorithms.", "creator": "LaTeX with hyperref package"}}}