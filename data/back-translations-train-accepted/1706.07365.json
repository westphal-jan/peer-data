{"id": "1706.07365", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2017", "title": "Pixels to Graphs by Associative Embedding", "abstract": "Graphs are a useful abstraction of image content. Not only can graphs represent details about individual objects in a scene but they can capture the interactions between pairs of objects. We present a method for training a convolutional neural network such that it takes in an input image and produces a full graph. This is done end-to-end in a single stage with the use of associative embeddings. The network learns to simultaneously identify all of the elements that make up a graph and piece them together. We benchmark on the Visual Genome dataset, and report a Recall@50 of 9.7% compared to the prior state-of-the-art at 3.4%, a nearly threefold improvement on the challenging task of scene graph generation.", "histories": [["v1", "Thu, 22 Jun 2017 15:20:25 GMT  (4886kb,D)", "http://arxiv.org/abs/1706.07365v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["alejandro newell", "jia deng"], "accepted": true, "id": "1706.07365"}, "pdf": {"name": "1706.07365.pdf", "metadata": {"source": "CRF", "title": "Pixels to Graphs by Associative Embedding", "authors": ["Alejandro Newell", "Jia Deng"], "emails": ["jiadeng}@umich.edu", "Recall@50"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of them will be able to move into a different world, in which they are able to move, in which they move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they"}, {"heading": "2 Related Work", "text": "There are many ways to define the task of identifying objects and the relationships between them, including localizing reference expressions [11], recognizing interactions between people and objects [3], or the more general tasks of visual recognition (VRD) [18] and generating scenarios [17]. The goal is to accurately determine the relationships between objects and capture them in the image with precise boundaries. Visual recognition has attracted much more recent attention [18, 27, 17, 23]. The open and challenging nature of the task lends itself to a variety of approaches and solutions."}, {"heading": "3 Pixels\u2192 Graph", "text": "Our goal is to construct a graph from a series of pixels. Specifically, we want to construct a graph based on the space of those pixels, which means that in addition to the vertices of the graph, we want to know their exact positions. In this case, a vertex can refer to any object in the scene, including people, cars, clothes, and buildings. Relationships between these objects are then captured by the edges of the graph. These relationships can include verbs (eating, riding), spatial relations (to the left of, behind), and comparisons (smaller than, the same color as). Formally, we consider a directed graph G = (V, E). A given vertex vi-V is grounded in a place (xi, yi) and defined by its class and boundary boxes. Each edge e-E takes the form ei = (vs, vt, ri), which defines a relationship from type ri to vt."}, {"heading": "3.1 Detecting graph elements", "text": "In fact, you will be able to go to a place where you can go to a place where you can go to a place where you can go to another world."}, {"heading": "3.2 Connecting elements with associative embeddings", "text": "In fact, it is so that it is a matter of a way in which one sees oneself in a position to surpass oneself, and in which it is a matter of a way in which one sees oneself in a position to surpass oneself, in which one sees oneself surpassing oneself, in which one outweighs oneself, in which one outweighs oneself, in which one outweighs oneself, in which one outweighs oneself, in which one outweighs oneself, in which one outweighs oneself, in which one outweighs oneself, in which one outweighs oneself, in which one outweighs oneself, in which one outweighs oneself, in which one outweighs oneself, in which one outweighs oneself, in which one outweighs oneself, in which one outweighs oneself, in which one outweighs oneself outweighs oneself, in which one outweighs oneself outweighs oneself, in which one outweighs oneself outweighs oneself, in which one eself outweighs oneself outweighs oneself, in which one eself outweighs oneself, in which one eself outweighs oneself, in which one eself outweighs oneself, in which one eself outweighs oneself, in which one eself outweighs oneself, in which one eself outweighs oneself, in love, in which one eself outweighs oneself, in which one eself outweighs oneself, in which one eself outweighs oneself, in love, in which one eself outweighs oneself, in which one eself outweighs oneself, in which one eself outweighs oneself, in love, in which one eself, in which one eself outweighs oneself, in love, in which one eself outwardly outwardly outwardly outwardly outwardly, in love, in love, in which one eself, in love, in which one eswardly outwardly outwardly outwardly outwardly, in love, in"}, {"heading": "3.3 Support for overlapping detections", "text": "In fact, it is the case that most of us are able to surpass ourselves, and that they will do so, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" It is not as if we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in a phase where we are in which we are in a phase where we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which are in which we are in which are in which we are in which are"}, {"heading": "4 Implementation details", "text": "We train a stacked hourglass architecture [21] in TensorFlow [1]. Input to the network is a 512x512 image, with a resolution of 64x64. The output feature length f is 256. We make a slight modification of the original hourglass design: doubling the number of features to 512 at the two lowest resolutions of the hourglass. All losses - classification, bounding box regression, associative embedding - are weighted equally over the course of the training. We use so = 3 and sr = 6, which are sufficient to fully accommodate the detection annotations for all but a small fraction of the cases. Incorporating previous detections, a previous set of object identifiers are provided, either as floor truth annotations or as suggestions for an independent system."}, {"heading": "5 Experiments", "text": "In this context, it should be noted that this is a very complex matter."}, {"heading": "6 Conclusion", "text": "The properties of a graph that allow it to capture so much information about the semantic content of an image come with added complexity for any system that wants to predict it. We show how to monitor a network in such a way that all considerations about a graph can be abstracted into a single network. Using associative embedding and disordered output places gives the network the flexibility it needs to train in this task. Our results on the Visual Genome clearly demonstrate the effectiveness of our approach."}, {"heading": "7 Acknowledgements", "text": "This publication is based on a paper supported by the Office of Sponsored Research (OSR) of the King Abdullah University of Science and Technology (KAUST) under Award No. OSR-2015-CRG42639."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u00edn Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Learning to generalize to new compositions in image understanding", "author": ["Yuval Atzmon", "Jonathan Berant", "Vahid Kezami", "Amir Globerson", "Gal Chechik"], "venue": "arXiv preprint arXiv:1608.07639,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Learning to detect human-object interactions", "author": ["Yu-Wei Chao", "Yunfan Liu", "Xieyang Liu", "Huayi Zeng", "Jia Deng"], "venue": "arXiv preprint arXiv:1702.05448,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2017}, {"title": "Detecting visual relationships with deep relational networks", "author": ["Bo Dai", "Yuqi Zhang", "Dahua Lin"], "venue": "arXiv preprint arXiv:1704.03114,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2017}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Tomas Mikolov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Learning globally-consistent local distance functions for shape-based image retrieval and classification", "author": ["Andrea Frome", "Yoram Singer", "Fei Sha", "Jitendra Malik"], "venue": "IEEE 11th International Conference on Computer Vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Improving image-sentence embeddings using large weakly annotated photo collections", "author": ["Yunchao Gong", "Liwei Wang", "Micah Hodosh", "Julia Hockenmaier", "Svetlana Lazebnik"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["Raia Hadsell", "Sumit Chopra", "Yann LeCun"], "venue": "In Computer vision and pattern recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Modeling relationships in referential expressions with compositional modular networks", "author": ["Ronghang Hu", "Marcus Rohrbach", "Jacob Andreas", "Trevor Darrell", "Kate Saenko"], "venue": "arXiv preprint arXiv:1611.09978,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Image retrieval using scene graphs", "author": ["Justin Johnson", "Ranjay Krishna", "Michael Stark", "Li-Jia Li", "David Shamma", "Michael Bernstein", "Li Fei- Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations. 2016", "author": ["Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma", "Michael Bernstein", "Li Fei-Fei"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Vip-cnn: A visual phrase reasoning convolutional neural network for visual relationship detection", "author": ["Yikang Li", "Wanli Ouyang", "Xiaogang Wang"], "venue": "arXiv preprint arXiv:1702.07191,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2017}, {"title": "Deep variation-structured reinforcement learning for visual relationship and attribute detection", "author": ["Xiaodan Liang", "Lisa Lee", "Eric P Xing"], "venue": "arXiv preprint arXiv:1703.03054,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2017}, {"title": "On support relations and semantic scene graphs", "author": ["Wentong Liao", "Michael Ying Yang", "Hanno Ackermann", "Bodo Rosenhahn"], "venue": "arXiv preprint arXiv:1609.05834,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Visual relationship detection with language priors", "author": ["Cewu Lu", "Ranjay Krishna", "Michael Bernstein", "Li Fei-Fei"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Beyond holistic object recognition: Enriching image understanding with part states", "author": ["Cewu Lu", "Hao Su", "Yongyi Lu", "Li Yi", "Chikeung Tang", "Leonidas Guibas"], "venue": "arXiv preprint arXiv:1612.07310,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Associative embedding: End-to-end learning for joint detection and grouping", "author": ["Alejandro Newell", "Jia Deng"], "venue": "arXiv preprint arXiv:1611.05424,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Stacked hourglass networks for human pose estimation", "author": ["Alejandro Newell", "Kaiyu Yang", "Jia Deng"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Phrase localization and visual relationship detection with comprehensive linguistic cues", "author": ["Bryan A Plummer", "Arun Mallya", "Christopher M Cervantes", "Julia Hockenmaier", "Svetlana Lazebnik"], "venue": "arXiv preprint arXiv:1611.06641,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Discovering objects and their relations from entangled scene representations", "author": ["David Raposo", "Adam Santoro", "David Barrett", "Razvan Pascanu", "Timothy Lillicrap", "Peter Battaglia"], "venue": "arXiv preprint arXiv:1702.05068,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2017}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing", "author": ["Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["Kilian Q Weinberger", "John Blitzer", "Lawrence K Saul"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Scene graph generation by iterative message passing", "author": ["Danfei Xu", "Yuke Zhu", "Christopher B Choy", "Li Fei-Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2017}, {"title": "Visual translation embedding network for visual relation detection", "author": ["Hanwang Zhang", "Zawlin Kyaw", "Shih-Fu Chang", "Tat-Seng Chua"], "venue": "arXiv preprint arXiv:1702.08319,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2017}, {"title": "Towards context-aware interaction recognition", "author": ["Bohan Zhuang", "Lingqiao Liu", "Chunhua Shen", "Ian Reid"], "venue": "arXiv preprint arXiv:1703.06246,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2017}], "referenceMentions": [{"referenceID": 6, "context": "Recent years have seen rapid progress in the classification and localization of objects [7, 24, 10].", "startOffset": 88, "endOffset": 99}, {"referenceID": 22, "context": "Recent years have seen rapid progress in the classification and localization of objects [7, 24, 10].", "startOffset": 88, "endOffset": 99}, {"referenceID": 16, "context": "[18] This breakdown often restricts the visual features used in later steps and limits reasoning over the full graph and over the full contents of the image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Detecting vertices and edges therefore reduces to the problem of scoring candidate detections as in established object detection approaches [24].", "startOffset": 140, "endOffset": 144}, {"referenceID": 18, "context": "The network needs a way for detections to refer to each other, and for that we draw inspiration from associative embeddings [20], a method originally proposed for joint detection and grouping in the context of multiperson pose estimation.", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "We apply our method to the task of generating a semantic graph of objects and relations and test on the Visual Genome dataset [14].", "startOffset": 126, "endOffset": 130}, {"referenceID": 9, "context": "This includes localization from referential expressions [11], detection of human-object interactions [3], or the more general tasks of visual relationship detection (VRD) [18] and scene graph generation [12].", "startOffset": 56, "endOffset": 60}, {"referenceID": 2, "context": "This includes localization from referential expressions [11], detection of human-object interactions [3], or the more general tasks of visual relationship detection (VRD) [18] and scene graph generation [12].", "startOffset": 101, "endOffset": 104}, {"referenceID": 16, "context": "This includes localization from referential expressions [11], detection of human-object interactions [3], or the more general tasks of visual relationship detection (VRD) [18] and scene graph generation [12].", "startOffset": 171, "endOffset": 175}, {"referenceID": 10, "context": "This includes localization from referential expressions [11], detection of human-object interactions [3], or the more general tasks of visual relationship detection (VRD) [18] and scene graph generation [12].", "startOffset": 203, "endOffset": 207}, {"referenceID": 16, "context": "Visual relationship detection has drawn much recent attention [18, 28, 27, 2, 17, 19, 22, 23].", "startOffset": 62, "endOffset": 93}, {"referenceID": 26, "context": "Visual relationship detection has drawn much recent attention [18, 28, 27, 2, 17, 19, 22, 23].", "startOffset": 62, "endOffset": 93}, {"referenceID": 25, "context": "Visual relationship detection has drawn much recent attention [18, 28, 27, 2, 17, 19, 22, 23].", "startOffset": 62, "endOffset": 93}, {"referenceID": 1, "context": "Visual relationship detection has drawn much recent attention [18, 28, 27, 2, 17, 19, 22, 23].", "startOffset": 62, "endOffset": 93}, {"referenceID": 15, "context": "Visual relationship detection has drawn much recent attention [18, 28, 27, 2, 17, 19, 22, 23].", "startOffset": 62, "endOffset": 93}, {"referenceID": 17, "context": "Visual relationship detection has drawn much recent attention [18, 28, 27, 2, 17, 19, 22, 23].", "startOffset": 62, "endOffset": 93}, {"referenceID": 20, "context": "Visual relationship detection has drawn much recent attention [18, 28, 27, 2, 17, 19, 22, 23].", "startOffset": 62, "endOffset": 93}, {"referenceID": 21, "context": "Visual relationship detection has drawn much recent attention [18, 28, 27, 2, 17, 19, 22, 23].", "startOffset": 62, "endOffset": 93}, {"referenceID": 16, "context": "For example: incorporating vision and language when reasoning over a pair of objects [18]; using message-passing RNNs to process a set of proposed object boxes [26]; predicting over triplets of bounding boxes that corresponding to proposals for a subject, phrase, and object [15]; using reinforcement learning to sequentially evaluate on pairs of object proposals and determine their relationships [16]; comparing the visual features and relative spatial positions of pairs of boxes [4];", "startOffset": 85, "endOffset": 89}, {"referenceID": 24, "context": "For example: incorporating vision and language when reasoning over a pair of objects [18]; using message-passing RNNs to process a set of proposed object boxes [26]; predicting over triplets of bounding boxes that corresponding to proposals for a subject, phrase, and object [15]; using reinforcement learning to sequentially evaluate on pairs of object proposals and determine their relationships [16]; comparing the visual features and relative spatial positions of pairs of boxes [4];", "startOffset": 160, "endOffset": 164}, {"referenceID": 13, "context": "For example: incorporating vision and language when reasoning over a pair of objects [18]; using message-passing RNNs to process a set of proposed object boxes [26]; predicting over triplets of bounding boxes that corresponding to proposals for a subject, phrase, and object [15]; using reinforcement learning to sequentially evaluate on pairs of object proposals and determine their relationships [16]; comparing the visual features and relative spatial positions of pairs of boxes [4];", "startOffset": 275, "endOffset": 279}, {"referenceID": 14, "context": "For example: incorporating vision and language when reasoning over a pair of objects [18]; using message-passing RNNs to process a set of proposed object boxes [26]; predicting over triplets of bounding boxes that corresponding to proposals for a subject, phrase, and object [15]; using reinforcement learning to sequentially evaluate on pairs of object proposals and determine their relationships [16]; comparing the visual features and relative spatial positions of pairs of boxes [4];", "startOffset": 398, "endOffset": 402}, {"referenceID": 3, "context": "For example: incorporating vision and language when reasoning over a pair of objects [18]; using message-passing RNNs to process a set of proposed object boxes [26]; predicting over triplets of bounding boxes that corresponding to proposals for a subject, phrase, and object [15]; using reinforcement learning to sequentially evaluate on pairs of object proposals and determine their relationships [16]; comparing the visual features and relative spatial positions of pairs of boxes [4];", "startOffset": 483, "endOffset": 486}, {"referenceID": 25, "context": "learning to project proposed objects into a vector space such that the difference between two object vectors is informative of the relationship between them [27].", "startOffset": 157, "endOffset": 161}, {"referenceID": 22, "context": "Most of these approaches rely on generated bounding boxes from a Region Proposal Network (RPN) [24].", "startOffset": 95, "endOffset": 99}, {"referenceID": 5, "context": "For example, to measure the similarity between pairs of images [6, 25], or to map visual and text features to a shared vector space [5, 8, 13].", "startOffset": 63, "endOffset": 70}, {"referenceID": 23, "context": "For example, to measure the similarity between pairs of images [6, 25], or to map visual and text features to a shared vector space [5, 8, 13].", "startOffset": 63, "endOffset": 70}, {"referenceID": 4, "context": "For example, to measure the similarity between pairs of images [6, 25], or to map visual and text features to a shared vector space [5, 8, 13].", "startOffset": 132, "endOffset": 142}, {"referenceID": 7, "context": "For example, to measure the similarity between pairs of images [6, 25], or to map visual and text features to a shared vector space [5, 8, 13].", "startOffset": 132, "endOffset": 142}, {"referenceID": 11, "context": "For example, to measure the similarity between pairs of images [6, 25], or to map visual and text features to a shared vector space [5, 8, 13].", "startOffset": 132, "endOffset": 142}, {"referenceID": 18, "context": "Recent work uses vector embeddings to group together body joints for multiperson pose estimation [20].", "startOffset": 97, "endOffset": 101}, {"referenceID": 18, "context": "More specifically, in [20] a network is trained to detect body joints of the various people in an image.", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "We use a stacked hourglass network [21] to process an image and produce the output feature tensor.", "startOffset": 35, "endOffset": 39}, {"referenceID": 22, "context": "And to predict bounding box information we use anchor boxes and regress offsets based on the approach in Faster-RCNN [24].", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "To train the network to produce a coherent set of embeddings we build off of the loss penalty used in [20].", "startOffset": 102, "endOffset": 106}, {"referenceID": 18, "context": "To \u201cpush apart\u201d embeddings across different vertices we use a slightly different penalty from what is described in [20], instead applying a margin-based penalty similar to [9]:", "startOffset": 115, "endOffset": 119}, {"referenceID": 8, "context": "To \u201cpush apart\u201d embeddings across different vertices we use a slightly different penalty from what is described in [20], instead applying a margin-based penalty similar to [9]:", "startOffset": 172, "endOffset": 175}, {"referenceID": 18, "context": "Convergence of the network improves greatly after increasing the dimension d of tags up from 1 as used in [20].", "startOffset": 106, "endOffset": 110}, {"referenceID": 19, "context": "We train a stacked hourglass architecture [21] in TensorFlow [1].", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": "We train a stacked hourglass architecture [21] in TensorFlow [1].", "startOffset": 61, "endOffset": 64}, {"referenceID": 16, "context": "[18] \u2013 \u2013 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] \u2013 \u2013 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Dataset: We evaluate the performance of our method on the Visual Genome dataset [14].", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "[26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Following [26], we report performance on three problem settings: SGGen: Detect and classify all objects and determine the relationships between them.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "For the full scene graph task (SGGen) these detections are provided by a Region Proposal Network (RPN) [24].", "startOffset": 103, "endOffset": 107}], "year": 2017, "abstractText": "Graphs are a useful abstraction of image content. Not only can graphs represent details about individual objects in a scene but they can capture the interactions between pairs of objects. We present a method for training a convolutional neural network such that it takes in an input image and produces a full graph. This is done end-to-end in a single stage with the use of associative embeddings. The network learns to simultaneously identify all of the elements that make up a graph and piece them together. We benchmark on the Visual Genome dataset, and report a Recall@50 of 9.7% compared to the prior state-of-the-art at 3.4%, a nearly threefold improvement on the challenging task of scene graph generation.", "creator": "LaTeX with hyperref package"}}}