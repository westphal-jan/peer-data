{"id": "1706.08502", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2017", "title": "Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog", "abstract": "A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, all learned without any human supervision!", "histories": [["v1", "Mon, 26 Jun 2017 17:47:46 GMT  (2434kb,D)", "http://arxiv.org/abs/1706.08502v1", "9 pages, 7 figures, 2 tables"], ["v2", "Thu, 3 Aug 2017 03:37:00 GMT  (2436kb,D)", "http://arxiv.org/abs/1706.08502v2", "9 pages, 7 figures, 2 tables, accepted at EMNLP 2017 as short paper"], ["v3", "Sun, 20 Aug 2017 04:41:15 GMT  (2434kb,D)", "http://arxiv.org/abs/1706.08502v3", "9 pages, 7 figures, 2 tables, accepted at EMNLP 2017 as short paper"]], "COMMENTS": "9 pages, 7 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["satwik kottur", "jos\u00e9 m f moura", "stefan lee", "dhruv batra"], "accepted": true, "id": "1706.08502"}, "pdf": {"name": "1706.08502.pdf", "metadata": {"source": "CRF", "title": "Natural Language Does Not Emerge \u2018Naturally\u2019 in Multi-Agent Dialog", "authors": ["Satwik Kottur", "Stefan Lee", "Dhruv Batra"], "emails": [], "sections": [{"heading": null, "text": "In a number of recent papers, techniques for the holistic learning of communication protocols between cooperative multi-agent populations have been proposed, while simultaneously identifying in the protocols developed by the agents the emergence of a grounded human interpretable language, all learned without human supervision! In this paper, using a task-and-tell reference game between two agents, we present a sequence of \"negative\" results culminating in a \"positive\" - and show that while most languages invented by agents are effective (i.e. achieving near-perfect task rewards), they are definitely not interpretable or compositional. In essence, we note that natural language does not emerge \"naturally,\" despite the appearance of a simple occurrence of natural languages that can be derived from recent literature. We discuss how it is possible to elicit the invented languages to become more human and compositional by increasing the constraints on how two agents can communicate."}, {"heading": "1 Introduction", "text": "In fact, most of them are people who are able to determine for themselves how they want to behave."}, {"heading": "3 Modeling Q-BOT and A-BOT", "text": "We formalize Q-BOT and A-BOT as agents operating in a partially observable world and optimize their strategies through deep affirmation learning.A-BOT Each agent observes his own input (task G for Q-BOT and object instance I for A-BOT) and the output of the other agent as a stochastic environment. At the beginning of the round, Q-BOT observes the history and this new utterance as state stA = [I, q1, a1] and acts by pronouncing some tokens qt from his vocabulary VQ. A-BOT observes the history and this new utterance as state stA = [I, q1, a1]., qt \u2212 1, qt] and emits a response from VA. In the last round, Q-BOT takes a final action by predicting a pair of attribute values."}, {"heading": "4 The Road to Compositionality", "text": "This section describes our most important observation - that while agents always successfully invent an lan-1github.com / pytorch / pytorch 2github.com / batra-mlp-lab / lang-emergeguage to solve the game with near-perfect accuracy, the invented languages are clearly not compositional, interpretable or \"natural\" (e.g. ABOT ignores Q-BOT's pronouncements and simply encodes each object with a unique symbol when the vocabulary is sufficiently large).In this section, we present a series of settings that are becoming increasingly restrictive to persuade agents to adopt a compositional language, providing analysis of the languages learned and \"cheating strategies\" developed along the way. Tab. 2 summarizes the results for all settings. Optimal strategies (achieving near-perfect training rewards) were found in all experiments."}, {"heading": "4.1 Overcomplete Vocabularies", "text": "We start with the simplest setting, in which both ABOT and Q-BOT are given arbitrarily large vocabulary. We note that if | VA | is larger than the number of instances (64), the learned policy usually ignores what Q-BOT asks, and instead lets A-BOT round the instance with pairs of symbols that are unique to an instance, e.g. both pairs of tokens (a1, a2) = (14, 31), (40, 1) transmit (red, triangular, filled), as in Fig. 3. Note that this means that no \"dialogue\" is necessary and that each agent has a code book that maps symbols to object references. Essentially, this setting has collapsed to an analog Lewis Signaling Game (LS) in which A-BOT signals its full world status, and Q-BOT simply reports the target attributes. Further examples to illustrate this result, as expected."}, {"heading": "4.2 Attribute-Value Vocabulary", "text": "Since our world has 3 attributes (shape / color / style) and 4 + 4 + 4 = 12 possible settings of their states, one might think that the intuitive choice of | VQ | = 3 and | VA | = 12 will be enough to circumvent the enumeration strategy of the previous experiment. Surprisingly, however, we find that the new language learned in this environment is not only clearly non-compositional, but also very difficult to interpret! Some observations result from Fig. 4, which shows sample dialogs for these settings. We observe that Q-BOT only uses the first round to convey the task to A-BOT by encoding tasks in a series agnostic manner, such as: (style, shape, style) \u2192 X, (color, shape), (shape, color) \u2192 Y, and (color, style), (style), symbolic at first, (style, color, style)."}, {"heading": "4.3 Memoryless A-BOT, Minimal Vocabulary", "text": "The key problem with the previous setting is that A-BOT's statements are different things based on Q's (a1 = 1 is different from a2 = 1). Essentially, the communication protocol is over-parameterized and we need to limit it further. In other words, we reset the state vector SA at any time so that A-BOT can no longer distinguish rounds from each other."}, {"heading": "5 Evolution of Language", "text": "Although compositional language is one of the optimal strategies, actors tend to learn other equally useful forms of communication, so that compositional language does not emerge without an explicit need. Perhaps, even in situations where compositivity is emerging, it is more interesting to analyze the process of creation than the learned language itself. Therefore, we present such a study, which explicitly determines when each symbol was grounded by the actors in the training timeline, along with its effects on performance in the task and talk game."}, {"heading": "5.1 Dialog Trees", "text": "When two agents - Q-BOT and A-BOT - communicate with each other, they can be seen as traversing a dialog tree whose sub-tree is shown in Fig. 6. Simply put, a dialog tree is a list of all possible dialogs presented in the form of a tree, with the levels of the tree corresponding to the round of interaction. To refine this, we consider a sub-dialog tree for (shape, color) task shown in Fig. 6 for the setting in Fig. 4.3. For QBOT's first symbol q1 = Y, A-BOT | = 4 has plausible answers presented as a 4-way fork. In general, the dialog tree for Task & Talk as a whole | VQ | 2 | VA | 2 sheets and is 4 levels deep. We use the dialog between agents to get off and land in one of these sheets. Dialogue trees offer an interesting alternative view of our learning problem."}, {"heading": "5.2 Evolution Timeline", "text": "In order to gain further insights into the languages learned, we create a language development plan, which is shown in Fig. 7. Specifically, at regular intervals during political learning, we construct dialogue trees. At some point in the learning process, the nodes in the tree become \"pure\" (all (instance, task) in the Fig., at which point we can say that the agents have learned this dialogue subsequence. Fig. 7 shows a timeline of concepts learned in different nodes of the trees during the training. We describe the procedure to determine when a particular \"concept\" was grounded by the agents in their language."}, {"heading": "6 Conclusion", "text": "Finally, we presented a sequence of \"negative\" results, culminating in a \"positive\" one - showing that while most invented languages are effective (i.e. achieving near-perfect rewards), they are decidedly non-interpretable or compositional. Our goal is simply to improve the understanding and interpretability of invented languages in multi-agent dialogue, place current work in context, and provide fertile directions for future work."}], "references": [{"title": "Learning End-to-End Goal-Oriented Dialog", "author": ["Antoine Bordes", "Jason Weston."], "venue": "arXiv preprint arXiv:1605.07683 .", "citeRegEx": "Bordes and Weston.,? 2016", "shortCiteRegEx": "Bordes and Weston.", "year": 2016}, {"title": "Strategic information transmission", "author": ["Vincent Crawford", "Joel Sobel."], "venue": "Econometrica 50(6):1431\u201351.", "citeRegEx": "Crawford and Sobel.,? 1982", "shortCiteRegEx": "Crawford and Sobel.", "year": 1982}, {"title": "Visual Dialog", "author": ["Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "Jos\u00e9 M.F. Moura", "Devi Parikh", "Dhruv Batra."], "venue": "CVPR.", "citeRegEx": "Das et al\\.,? 2017a", "shortCiteRegEx": "Das et al\\.", "year": 2017}, {"title": "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning", "author": ["Abhishek Das", "Satwik Kottur", "Jos\u00e9 M.F. Moura", "Stefan Lee", "Dhruv Batra."], "venue": "arXiv preprint arXiv:1703.06585 .", "citeRegEx": "Das et al\\.,? 2017b", "shortCiteRegEx": "Das et al\\.", "year": 2017}, {"title": "Guesswhat?! visual object discovery through multi-modal dialogue", "author": ["Harm de Vries", "Florian Strub", "Sarath Chandar", "Olivier Pietquin", "Hugo Larochelle", "Aaron C. Courville."], "venue": "CVPR.", "citeRegEx": "Vries et al\\.,? 2017", "shortCiteRegEx": "Vries et al\\.", "year": 2017}, {"title": "Learning to communicate with deep multi-agent reinforcement learning", "author": ["Jakob Foerster", "Yannis M Assael", "Nando de Freitas", "Shimon Whiteson."], "venue": "NIPS.", "citeRegEx": "Foerster et al\\.,? 2016", "shortCiteRegEx": "Foerster et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "AISTATS.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Emergence of language with multi-agent games: Learning to communicate with sequences of symbols", "author": ["Serhii Havrylov", "Ivan Titov."], "venue": "ICLR Workshop.", "citeRegEx": "Havrylov and Titov.,? 2017", "shortCiteRegEx": "Havrylov and Titov.", "year": 2017}, {"title": "Learning to play guess who? and inventing a grounded language as a consequence", "author": ["Emilio Jorge", "Mikael K\u00e5geb\u00e4ck", "Emil Gustavsson."], "venue": "NIPS workshop on deep reinforcement learning.", "citeRegEx": "Jorge et al\\.,? 2016", "shortCiteRegEx": "Jorge et al\\.", "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "ICLR.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Multi-agent cooperation and the emergence of (natural) language", "author": ["Angeliki Lazaridou", "Alexander Peysakhovich", "Marco Baroni."], "venue": "ICLR.", "citeRegEx": "Lazaridou et al\\.,? 2017", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2017}, {"title": "An ISU dialogue system exhibiting reinforcement learning of dialogue policies: generic slot-filling in the TALK in-car system", "author": ["Oliver Lemon", "Kallirroi Georgila", "James Henderson", "Matthew Stuttle."], "venue": "EACL.", "citeRegEx": "Lemon et al\\.,? 2006", "shortCiteRegEx": "Lemon et al\\.", "year": 2006}, {"title": "Convention: A philosophical study", "author": ["David Lewis."], "venue": "John Wiley & Sons.", "citeRegEx": "Lewis.,? 2008", "shortCiteRegEx": "Lewis.", "year": 2008}, {"title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems", "author": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau."], "venue": "SIGDIAL.", "citeRegEx": "Lowe et al\\.,? 2015", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Emergence of grounded compositional language in multi-agent populations", "author": ["Igor Mordatch", "Pieter Abbeel."], "venue": "arXiv preprint arXiv:1703.04908 . Nasrin Mostafazadeh, Chris Brockett, Bill Dolan, Michel Galley, Jianfeng Gao, Georgios P. Spithourakis,", "citeRegEx": "Mordatch and Abbeel.,? 2017", "shortCiteRegEx": "Mordatch and Abbeel.", "year": 2017}, {"title": "Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation", "author": ["Lucy Vanderwende"], "venue": null, "citeRegEx": "Vanderwende.,? \\Q2017\\E", "shortCiteRegEx": "Vanderwende.", "year": 2017}, {"title": "The evolution of syntactic communication", "author": ["Martin A. Nowak", "Joshua B. Plotkin", "Vincent A.A. Jansen."], "venue": "Nature 404(6777):495\u2013498.", "citeRegEx": "Nowak et al\\.,? 2000", "shortCiteRegEx": "Nowak et al\\.", "year": 2000}, {"title": "Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models", "author": ["Iulian V. Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "AAAI.", "citeRegEx": "Serban et al\\.,? 2016a", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1605.06069 .", "citeRegEx": "Serban et al\\.,? 2016b", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Learning multiagent communication with backpropagation", "author": ["Sainbayar Sukhbaatar", "Rob Fergus"], "venue": "In NIPS", "citeRegEx": "Sukhbaatar and Fergus,? \\Q2016\\E", "shortCiteRegEx": "Sukhbaatar and Fergus", "year": 2016}, {"title": "Learning language games through interaction", "author": ["Sida I Wang", "Percy Liang", "Christopher D Manning."], "venue": "Association for Computational Linguistics (ACL) .", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Dialog-based language learning", "author": ["Jason Weston."], "venue": "arXiv preprint arXiv:1604.06045 .", "citeRegEx": "Weston.,? 2016", "shortCiteRegEx": "Weston.", "year": 2016}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Procedures as a representation for data in a computer program for understanding natural language", "author": ["Terry Winograd."], "venue": "Technical report, DTIC Document.", "citeRegEx": "Winograd.,? 1971", "shortCiteRegEx": "Winograd.", "year": 1971}], "referenceMentions": [{"referenceID": 11, "context": "While historically such agents have been based on slot filling (Lemon et al., 2006), the dominant paradigm today is neural dialog models (Bordes and Weston, 2016; Weston, 2016; Serban et al.", "startOffset": 63, "endOffset": 83}, {"referenceID": 13, "context": "Specifically, a typical pipeline is to collect a large dataset of human-human dialog (Lowe et al., 2015; Das et al., 2017a; de Vries et al., 2017; Mostafazadeh et al., 2017), inject a machine in the middle of a di-", "startOffset": 85, "endOffset": 173}, {"referenceID": 2, "context": "Specifically, a typical pipeline is to collect a large dataset of human-human dialog (Lowe et al., 2015; Das et al., 2017a; de Vries et al., 2017; Mostafazadeh et al., 2017), inject a machine in the middle of a di-", "startOffset": 85, "endOffset": 173}, {"referenceID": 23, "context": "An alternative paradigm that has a long history (Winograd, 1971) and is witnessing a recent resurgence (Wang et al.", "startOffset": 48, "endOffset": 64}, {"referenceID": 2, "context": "for our investigation is the recent work of Das et al. (2017b), who proposed a cooperative reference game between two agents, where communication is necessary to accomplish the goal due to an information asymmetry.", "startOffset": 44, "endOffset": 63}, {"referenceID": 2, "context": "over Das et al. (2017b) is an exhaustive study of the conditions that must be present before compositional grounded language emerges, and subtle but important differences in execution \u2013 tabular QLearning (which does not scale) vs.", "startOffset": 5, "endOffset": 24}, {"referenceID": 12, "context": "Such a setting has been widely studies in economics and game theory as the classic Lewis Signaling (LS) game (Lewis, 2008).", "startOffset": 109, "endOffset": 122}, {"referenceID": 22, "context": "To train these agents, we update policy parameters \u03b8Q and \u03b8A using the popular REINFORCE (Williams, 1992) policy gradient algorithm.", "startOffset": 89, "endOffset": 105}, {"referenceID": 6, "context": "All modules within an agent are initialized using the Xavier method (Glorot and Bengio, 2010).", "startOffset": 68, "endOffset": 93}, {"referenceID": 9, "context": "We use 1000 episodes of two-round dialogs to compute policy gradients, and perform updates according to Adam optimizer (Kingma and Ba, 2015), with a learning rate of 0.", "startOffset": 119, "endOffset": 140}, {"referenceID": 16, "context": "It seems clear that like in human communication (Nowak et al., 2000), a limited vocabulary that", "startOffset": 48, "endOffset": 68}, {"referenceID": 1, "context": "talk\u2019 games (Crawford and Sobel, 1982).", "startOffset": 12, "endOffset": 38}, {"referenceID": 2, "context": "This is similar to learned languages reported in recent works and is most closely related to Das et al. (2017b), who solve this problem by taking away Q-BOT\u2019s state rather than A-BOT\u2019s memory.", "startOffset": 93, "endOffset": 112}], "year": 2017, "abstractText": "A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, all learned without any human supervision! In this paper, using a Task and Tell reference game between two agents as a testbed, we present a sequence of \u2018negative\u2019 results culminating in a \u2018positive\u2019 one \u2013 showing that while most agent-invented languages are effective (i.e. achieve nearperfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge \u2018naturally\u2019, despite the semblance of ease of natural-languageemergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.", "creator": "LaTeX with hyperref package"}}}