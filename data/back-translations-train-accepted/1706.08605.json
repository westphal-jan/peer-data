{"id": "1706.08605", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2017", "title": "Developing Bug-Free Machine Learning Systems With Formal Mathematics", "abstract": "Noisy data, non-convex objectives, model misspecification, and numerical instability can all cause undesired behaviors in machine learning systems. As a result, detecting actual implementation errors can be extremely difficult. We demonstrate a methodology in which developers use an interactive proof assistant to both implement their system and to state a formal theorem defining what it means for their system to be correct. The process of proving this theorem interactively in the proof assistant exposes all implementation errors since any error in the program would cause the proof to fail. As a case study, we implement a new system, Certigrad, for optimizing over stochastic computation graphs, and we generate a formal (i.e. machine-checkable) proof that the gradients sampled by the system are unbiased estimates of the true mathematical gradients. We train a variational autoencoder using Certigrad and find the performance comparable to training the same model in TensorFlow.", "histories": [["v1", "Mon, 26 Jun 2017 21:30:02 GMT  (93kb,D)", "http://arxiv.org/abs/1706.08605v1", "To appear at the Thirty-fourth International Conference on Machine Learning (ICML) 2017"]], "COMMENTS": "To appear at the Thirty-fourth International Conference on Machine Learning (ICML) 2017", "reviews": [], "SUBJECTS": "cs.SE cs.AI", "authors": ["daniel selsam", "percy liang", "david l dill"], "accepted": true, "id": "1706.08605"}, "pdf": {"name": "1706.08605.pdf", "metadata": {"source": "META", "title": "Developing Bug-Free Machine Learning Systems With Formal Mathematics", "authors": ["Daniel Selsam", "Percy Liang", "David L. Dill"], "emails": ["<dselsam@stanford.edu>."], "sections": [{"heading": "1. Introduction", "text": "In fact, it is so that it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about and a way in which it is about a way in which it is about and a way in which it is about a way in which it is about and a way in which it is about a way in which it is about and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about and a way in which it is about a way in which it is about a way in which it is about a way in which it is about"}, {"heading": "2. Motivation", "text": "In the development of machine learning systems, many program optimizations involve extensive algebraic derivations to represent mathematical expressions in closed form. Suppose, for example, that you want to efficiently calculate the following quantity: \u2022 x N (x; \u00b5, Diag (\u03c32) logN (x; 0, In \u00b7 n). (1) You expand the density functions, grind the algebra by hand, and finally derive the following closed form expression: \u2212 1 2 [n \u2211 i = 1 (\u03c32i \u2212 \u00b52i) + n log 2\u03c0] (2) You implement a method for calculating this quantity and insert it as part of a larger program, but when you perform your first experiment, your diagrams are not as encouraging as you had hoped. Finally, after ruling out many other possible explanations, you decide to take a closer look at this method. You implement a method for calculating this quantity and insert it as part of a larger program, compare it with your method on a two-sided and two-sided basis."}, {"heading": "3. Background: The Lean Theorem Prover", "text": "In order to explain and motivate the relevant features of Lean, we will apply our methodology to a toy: we can write standard programs in the programs of the softplus function."}, {"heading": "4. Case Study: Certified Stochastic Computation Graphs", "text": "Stochastic calculation graphs are directional acyclic graphs in which each node represents a specific calculation process that can be deterministic or stochastical (Schulman et al., 2015). The loss function of a graph is defined as the expected value of the sum of the sheet nodes relative to the stochastic decisions. Figure 2 shows the stochastic calculation graph for a simple variation coder. Using our methodology, we developed a system, Certigrad, that allows the user to construct any stochastic calculation graph from the primitives we provide. The main purpose of the system is to take a program that describes a stochastic calculation graph and execute a randomized algorithm (stochastic baking propagation) that generates demonstrably unbiased samples of the gradients of the loss function in relation to the parameters in anticipation."}, {"heading": "4.1. Overview of Certigrad", "text": "In fact, most of them will be able to move to a different world in which they are able to flee than to a different world in which they live."}, {"heading": "4.2. Informal specification", "text": "Suppose g is a stochastic calculation graph with n nodes and (to simplify the notation) that only one parameter \u03b8 is required. Then, g, \u03b8 jointly define a distribution of the values to the n nodes (X1,.., Xn). Let the cost (g, X1: n) be the function that summarizes the values of the leaf nodes. Our primary goal is to write a (stochastic) back propagation algorithm that is so bprop that for each graph g, e.g. GDP [bprop (g, \u03b8, X1: n)] = scratches (e.g.) [cost (g, X1: n)])) (3) While this equation may be sufficient to communicate the specification to a person with a mathematical background, more precision is needed to communicate it to a computer. The next step is to formalize the background mathematics so that real numbers (so that we can believe that we can construct a computer so that they can form an equation)."}, {"heading": "4.3. Designing the mathematics libraries", "text": "So we do not have to deal with fundamental questions and can simply assume that we can assume some kind of real numbers without having to construct them (e.g. by Riemann humming). Note that we have to choose with great care, since even a single false axiom (perhaps caused by a single missing precondition) can in principle allow the testing of false theories without having to construct them (e.g. by Riemann humming).5 However, there are many premises that arise in mathematical theories that are almost always met, and so we would have the property that all formal proofs can be trusted without inspection."}, {"heading": "E {n : N} (d : Dist n) (f : Rn\u2192 R) : R SCG n : Type", "text": "The mathematical semantics of all three constructors is simple: E (example (pdf, prog)) f = \u0442 (\u03bb x, pdf x * f x) E (det xs) f = f xs E (composed d1 d2) f = E d1 (\u03bb x, (E (d2 x) f)))), as well as procedural semantics: run (example (pdf, prog) rng = prog rng run (det xs) rng = (xs, rng) run (composed d1 d2) rng = let (x, rng '): = run d1 rng in run (d2 x) rng. We consider a stochastic program to be correct if we can prove the relevant theorems about its functional denotation, and we try it by running an RNG to its prog denotation."}, {"heading": "4.4. Formal specification", "text": "First, we design types for all other objects and functions that occur in the informal description. First, we need a type SCG n to represent stochastic calculation graphs on n nodes, and a function SCG.to _ dist to distribute an SCG n and a scalar parameter \u03b8 over n real numbers (Dist n). We also need a function cost diagram and the values on each of its nodes, and sum up the values on the sheet nodes. Figure 4 returns the complete types of all objects that occur in the specification. Now, we can write down a type-correct analog of the informal specification, which is represented in Equation 3: def bprop _ spec (bprop: \u0432 {n}, SCG n \u2192 R \u2192 Rn \u2192 R): Prop: = (n: N) (g: SCG n), E (SCG.prop _ spec (bprop: n}, probp: n,} s functions SCG-xx (and n)."}, {"heading": "4.5. Interactive proof", "text": "Whereas conventional wisdom is that you would write your program before trying to prove it right, the interactive proof process provides so much helpful information about what the system needs to do that immediately after drafting the specification, we started with the proof. We divided the proof into two steps. First, we implemented the simplest possible function that met the specification (which only calculated the gradient for a single parameter at a time and did not write any memoirs at all) and proved that this was correct. Second, we implemented a more powerful version (which computed the gradient for multiple parameters simultaneously with memoirs) and proved that it was equivalent to the first. For the first step, we started with a placeholder implementation that immediately returned zero and let the interactive proof process guide the implementation. Whenever the proof seemed to require an induction on a particular data structure, we expanded the program to recur on that data structure; whenever the proof was needed to show a certain value, the program was a two-way."}, {"heading": "4.6. Optimizations", "text": "In the development of machine learning models, one often starts with an easy-to-understand model that induces a gradient estimator with unacceptably high variance and executes informal mathematics by hand to derive a new model that has the same objective function but induces a better gradient estimator. In our approach, the user can write both models and apply the process of interactive detection to confirm that they induce the same objective function. Common transformations can be written once and proven correct, so that the user only has to write the first model and the second can be automatically derived and proved to be equivalent. Within the framework of Certigrad, we have written an optimization program that integrates the KL divergence of multivariate isotropic Gaussian distribution and we have proven once and for all that the optimization is solid (we have also verified an optimization that repairs a model so that random variables do not depend on parameters (and not on two shifts), and that one variable differs by two)."}, {"heading": "4.7. Verifying backpropagation for specific models", "text": "Although we have proven that bprop meets its formal specification (bprop _ spec), we cannot be sure that it calculates the correct gradients for a given model unless we prove that the model meets the requirements of the specification. Although some of the requirements are technically impossible to decide, most machine learning models will in practice meet them all for simple reasons. We have written a (heuristic) tactics program to prove that certain models meet all the requirements, and use it to verify that bprop calculates the correct gradients for the AEVB model derived from \u00a7 4.6."}, {"heading": "4.8. Running the system", "text": "We have proven that our system is correct in an idealized mathematical context with infinitely accurate real numbers. In order to actually execute the system, we need to replace all real numbers in the program with floating-point numbers. Although this invalidates the specification technically and in some cases can cause numerical instability, this error class is well understood (Higham, 2002), could also be excluded in principle (Harrison, 2006; Boldo et al., 2015; Ramananandro et al., 2016) and is conceptually different from the algorithmic and mathematical errors our methodology is designed to eliminate. To improve performance, we also replace all tensors with an optimized tensor library (Own). This approximation could introduce errors into our system if the proprietary methods, for whatever reason, are not functionally equivalent to those we formally think about; of course, developers could achieve even greater safety by also verifying their optimized tensor code."}, {"heading": "4.9. Experiments", "text": "As an experiment, we trained an AEVB model with a 2-layer encoding network and a 2-layer decoding network on MNIST using the ADAM optimization method (Kingma & Ba, 2014) and compared both the expected loss and runtime of our system at each epoch with the same model and optimization method in TensorFlow, both running on 2 CPU cores."}, {"heading": "5. Discussion", "text": "Our primary motivation is to develop error-free machine learning systems, but our approach can offer significant benefits even when systems are built that do not have to be perfect. Perhaps the biggest burden that software developers have to bear is to fully understand how and why their system works, and we have found that by formally specifying the system requirements, we have been able to transfer much of that burden to the computer. Not only have we been able to synthesize some fragments of the system (\u00a7 4.5), we have been able to achieve an extremely high level of confidence that our system was error-free without having to think about how all parts of the system fit together. In our approach, the computer - not the human - is responsible for all local characteristics that the developer determines that the entire system is correct."}, {"heading": "Acknowledgments", "text": "We thank Jacob Steinhardt, Alexander Ratner, Cristina White, William Hamilton, Nathaniel Thomas and Vatsal Sharan for their valuable feedback on early designs. We also thank Leonardo de Moura, Tatsu Hashimoto and Joseph Helfer for helpful discussions. This work was supported by the Future of Life Institute 2016-158712."}], "references": [{"title": "TensorFlow: Large-scale machine learning", "author": ["Fernanda", "Vinyals", "Oriol", "Warden", "Pete", "Wattenberg", "Martin", "Wicke", "Yu", "Yuan", "Zheng", "Xiaoqiang"], "venue": "on heterogeneous systems,", "citeRegEx": "Fernanda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fernanda et al\\.", "year": 2015}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "In Python for Scientific Computing Conference,", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Verified compilation of floatingpoint computations", "author": ["Boldo", "Sylvie", "Jourdan", "Jacques-Henri", "Leroy", "Xavier", "Melquiond", "Guillaume"], "venue": "Journal of Automated Reasoning,", "citeRegEx": "Boldo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Boldo et al\\.", "year": 2015}, {"title": "Using crash hoare logic for certifying the fscq file system", "author": ["Chen", "Haogang", "Ziegler", "Daniel", "Chajed", "Tej", "Chlipala", "Adam", "Kaashoek", "M Frans", "Zeldovich", "Nickolai"], "venue": "In Proceedings of the 25th Symposium on Operating Systems Principles,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "The bedrock structured programming system: Combining generative metaprogramming and hoare logic in an extensible program verifier", "author": ["Chlipala", "Adam"], "venue": "In ACM SIGPLAN Notices,", "citeRegEx": "Chlipala and Adam.,? \\Q2013\\E", "shortCiteRegEx": "Chlipala and Adam.", "year": 2013}, {"title": "The calculus of constructions", "author": ["Coquand", "Thierry", "Huet", "G\u00e9rard"], "venue": "Information and computation,", "citeRegEx": "Coquand et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Coquand et al\\.", "year": 1988}, {"title": "The Lean theorem prover (system description)", "author": ["de Moura", "Leonardo", "Kong", "Soonho", "Avigad", "Jeremy", "Van Doorn", "Floris", "von Raumer", "Jakob"], "venue": "In Automated Deduction-CADE-25,", "citeRegEx": "Moura et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Moura et al\\.", "year": 2015}, {"title": "Formal proof\u2013the four-color theorem", "author": ["Gonthier", "Georges"], "venue": "Notices of the AMS,", "citeRegEx": "Gonthier and Georges.,? \\Q2008\\E", "shortCiteRegEx": "Gonthier and Georges.", "year": 2008}, {"title": "Edinburgh lcf: a mechanised logic of computation", "author": ["Gordon", "Michael JC"], "venue": null, "citeRegEx": "Gordon and JC.,? \\Q1979\\E", "shortCiteRegEx": "Gordon and JC.", "year": 1979}, {"title": "Introduction to hol a theorem proving environment for higher order logic", "author": ["Gordon", "Michael JC", "Melham", "Tom F"], "venue": null, "citeRegEx": "Gordon et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Gordon et al\\.", "year": 1993}, {"title": "A formal proof of the kepler conjecture", "author": ["Hales", "Thomas", "Adams", "Mark", "Bauer", "Gertrud", "Dang", "Dat Tat", "Harrison", "John", "Hoang", "Truong Le", "Kaliszyk", "Cezary", "Magron", "Victor", "McLaughlin", "Sean", "Nguyen", "Thang Tat"], "venue": "arXiv preprint arXiv:1501.02155,", "citeRegEx": "Hales et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hales et al\\.", "year": 2015}, {"title": "Hol light: A tutorial introduction", "author": ["Harrison", "John"], "venue": "In International Conference on Formal Methods in ComputerAided Design,", "citeRegEx": "Harrison and John.,? \\Q1996\\E", "shortCiteRegEx": "Harrison and John.", "year": 1996}, {"title": "Floating-point verification using theorem proving", "author": ["Harrison", "John"], "venue": "In International School on Formal Methods for the Design of Computer, Communication and Software Systems,", "citeRegEx": "Harrison and John.,? \\Q2006\\E", "shortCiteRegEx": "Harrison and John.", "year": 2006}, {"title": "Accuracy and stability of numerical algorithms", "author": ["Higham", "Nicholas J"], "venue": null, "citeRegEx": "Higham and J.,? \\Q2002\\E", "shortCiteRegEx": "Higham and J.", "year": 2002}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Formal verification of a realistic compiler", "author": ["Leroy", "Xavier"], "venue": "Communications of the ACM,", "citeRegEx": "Leroy and Xavier.,? \\Q2009\\E", "shortCiteRegEx": "Leroy and Xavier.", "year": 2009}, {"title": "Isabelle/HOL: a proof assistant for higherorder logic, volume 2283", "author": ["Nipkow", "Tobias", "Paulson", "Lawrence C", "Wenzel", "Markus"], "venue": null, "citeRegEx": "Nipkow et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Nipkow et al\\.", "year": 2002}, {"title": "Pvs: A prototype verification system", "author": ["Owre", "Sam", "Rushby", "John M", "Shankar", "Natarajan"], "venue": "In Automated Deduction\u2014CADE-11,", "citeRegEx": "Owre et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Owre et al\\.", "year": 1992}, {"title": "An overview of the mizar project", "author": ["Rudnicki", "Piotr"], "venue": "In Proceedings of the 1992 Workshop on Types for Proofs and Programs,", "citeRegEx": "Rudnicki and Piotr.,? \\Q1992\\E", "shortCiteRegEx": "Rudnicki and Piotr.", "year": 1992}, {"title": "Gradient estimation using stochastic computation graphs", "author": ["Schulman", "John", "Heess", "Nicolas", "Weber", "Theophane", "Abbeel", "Pieter"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Theano (Bergstra et al., 2010) has been under development for almost a decade and yet there is a recent GitHub issue (https://github.", "startOffset": 7, "endOffset": 30}, {"referenceID": 16, "context": "Our approach makes use of a tool called an interactive proof assistant (Gordon, 1979; Gordon & Melham, 1993; Harrison, 1996; Nipkow et al., 2002; Owre et al., 1992; Coq Development Team, 2015-2016; de Moura et al., 2015), which consists of (a) a programming language, (b) a language to state mathematical theorems, and (c) a set of tools for constructing formal proofs of such theorems.", "startOffset": 71, "endOffset": 220}, {"referenceID": 17, "context": "Our approach makes use of a tool called an interactive proof assistant (Gordon, 1979; Gordon & Melham, 1993; Harrison, 1996; Nipkow et al., 2002; Owre et al., 1992; Coq Development Team, 2015-2016; de Moura et al., 2015), which consists of (a) a programming language, (b) a language to state mathematical theorems, and (c) a set of tools for constructing formal proofs of such theorems.", "startOffset": 71, "endOffset": 220}, {"referenceID": 3, "context": "Proving correctness of machine learning systems requires building on the tools and insights from two distinct fields: program verification (Leroy, 2009; Klein et al., 2009; Chlipala, 2013; Chen et al., 2015), which has aimed to prove properties of computer programs, and formal mathematics (Rudnicki, 1992; Gonthier, 2008; Gonthier et al.", "startOffset": 139, "endOffset": 207}, {"referenceID": 10, "context": ", 2015), which has aimed to prove properties of computer programs, and formal mathematics (Rudnicki, 1992; Gonthier, 2008; Gonthier et al., 2013; Hales et al., 2015), which has aimed to formally represent and generate machine-checkable proofs of mathematical theorems.", "startOffset": 90, "endOffset": 165}, {"referenceID": 19, "context": "ticality of our approach, we implemented a new machine learning system, Certigrad, for optimizing over stochastic computation graphs (Schulman et al., 2015).", "startOffset": 133, "endOffset": 156}, {"referenceID": 1, "context": ", 2015) and Theano (Bergstra et al., 2010) by allowing nodes to represent random variables and by defining the loss function for a graph to be the expected value of the sum of the leaf nodes over the stochastic choices.", "startOffset": 19, "endOffset": 42}, {"referenceID": 16, "context": "Our development makes use of certain features that are unique to Lean, but most of what we present is equally applicable to Coq, and to a lesser extent, other interactive theorem provers such as Isabelle/HOL (Nipkow et al., 2002).", "startOffset": 208, "endOffset": 229}, {"referenceID": 19, "context": "Stochastic computation graphs are directed acyclic graphs in which each node represents a specific computational operation that may be deterministic or stochastic (Schulman et al., 2015).", "startOffset": 163, "endOffset": 186}, {"referenceID": 10, "context": "Whereas in traditional formal mathematics the goal is to construct mathematics from first principles (Gonthier et al., 2013; Hales et al., 2015), we need not concern ourselves with foundational issues and can simply assume that standard mathematical properties hold.", "startOffset": 101, "endOffset": 144}, {"referenceID": 19, "context": "We also needed to assume a generalization of the differentiability requirement mentioned in Schulman et al. (2015), that a subset of the nodes determined by the structure of the graph must be differentiable no matter the result of any stochastic choices (GradsExist g \u03b8).", "startOffset": 92, "endOffset": 115}, {"referenceID": 2, "context": "Although doing so technically invalidates the specification and can introduce numerical instability in some cases, this class of errors is well understood (Higham, 2002), could be ruled out as well in principle (Harrison, 2006; Boldo et al., 2015; Ramananandro et al., 2016) and is conceptually distinct from the algorithmic and mathematical errors that our methodology is designed to eliminate.", "startOffset": 211, "endOffset": 274}], "year": 2017, "abstractText": "Noisy data, non-convex objectives, model misspecification, and numerical instability can all cause undesired behaviors in machine learning systems. As a result, detecting actual implementation errors can be extremely difficult. We demonstrate a methodology in which developers use an interactive proof assistant to both implement their system and to state a formal theorem defining what it means for their system to be correct. The process of proving this theorem interactively in the proof assistant exposes all implementation errors since any error in the program would cause the proof to fail. As a case study, we implement a new system, Certigrad, for optimizing over stochastic computation graphs, and we generate a formal (i.e. machine-checkable) proof that the gradients sampled by the system are unbiased estimates of the true mathematical gradients. We train a variational autoencoder using Certigrad and find the performance comparable to training the same model in TensorFlow.", "creator": "LaTeX with hyperref package"}}}