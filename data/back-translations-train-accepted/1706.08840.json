{"id": "1706.08840", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2017", "title": "Gradient Episodic Memory for Continual Learning", "abstract": "One major obstacle towards artificial intelligence is the poor ability of models to quickly solve new problems, without forgetting previously acquired knowledge. To better understand this issue, we study the problem of learning over a continuum of data, where the model observes, once and one by one, examples concerning an ordered sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model to learn over continuums of data, called Gradient of Episodic Memory (GEM), which alleviates forgetting while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.", "histories": [["v1", "Mon, 26 Jun 2017 14:53:34 GMT  (2190kb,D)", "https://arxiv.org/abs/1706.08840v1", null], ["v2", "Wed, 28 Jun 2017 16:19:12 GMT  (2190kb,D)", "http://arxiv.org/abs/1706.08840v2", null], ["v3", "Wed, 12 Jul 2017 17:14:59 GMT  (89kb,D)", "http://arxiv.org/abs/1706.08840v3", null], ["v4", "Wed, 2 Aug 2017 20:37:51 GMT  (1852kb)", "http://arxiv.org/abs/1706.08840v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["david lopez-paz", "marc'aurelio ranzato"], "accepted": true, "id": "1706.08840"}, "pdf": {"name": "1706.08840.pdf", "metadata": {"source": "CRF", "title": "Gradient Episodic Memory for Continual Learning", "authors": ["David Lopez-Paz"], "emails": ["dlp@fb.com", "ranzato@fb.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 6.08 840v 4 [cs.L G] 2A ug2 01"}, {"heading": "1 Introduction", "text": "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii"}, {"heading": "2 A Framework for Continual Learning", "text": "The central object of study in this paper is the data continuum of eq. (1), in which each triplet (xi, ti, yi) is formed by a feature vector xi-Xti, a task descriptor ti-T, and a target vector yi-Yti. For convenience, we assume that the continuum is local iid, that is, each triplet (xi, ti, yi) can be queried at any time to predict the target vector associated with a test pair (x, Y). Such a test pair may be part of a task we have observed in the past. (1) For example, our goal is to learn a predictor f: X-T \u2192 Y that can be queried at any time to predict the target vector associated with a test pair (x, Y)."}, {"heading": "2.1 Training Protocol and Evaluation Metrics", "text": "Most of the mentioned persons are able to abide by the rules that they have applied in their respective country. (...) Most of the mentioned persons are able to abide by the rules. (...) Most of the mentioned persons are able to abide by the rules. (...) Most of the mentioned persons are able to abide by the rules. (...) Most of the mentioned persons are able to abide by the rules. (...) Most of the mentioned persons are able to abide by the rules. (...) Most of the mentioned persons are able to abide by the rules. (...) Most of them are able to abide by the rules. (...)"}, {"heading": "3 Gradient of Episodic Memory (GEM)", "text": "In this section, we suggest that memory is populated with examples from the last task, although we could each apply better strategies. (GEM) The main feature of GEM is an episodic memory Mt, which stores a subset of the observed examples from task t. (For simplicity reasons, however, we assume that holistic task descriptors, and use them to index episodic memory tasks. In practice, the learner has a total budget of M memory locations, in which the number of total tasks is known, we can focus on minimizing negative backward transfers (catastrophic formation) through the efficient use of episodic memory tasks. In practice, the learner has a total budget of M memory locations. If the number of total tasks T is known, we can allocate m = M / T memory for each task. Conversely, if the number of total tasks T is unknown, we can gradually reduce the value of M memory locations, while we assume the last task is populated (i)."}, {"heading": "4 Experiments", "text": "In this section, we describe a variety of experiments that evaluate the performance of GEM in a continuous learning environment."}, {"heading": "4.1 Datasets", "text": "We consider the following data sets: \u2022 MNIST permutations [Kirkpatrick et al., 2017], a variant of the MNIST dataset of handwritten digits [LeCun et al., 1998], where each task is transformed by a fixed permutation of pixels. \u2022 Incremental CIFAR100 [Rebuffi et al., 2017], a variant of the CIFAR object recognition dataset with 100 classes [Krizhevsky, 2009], where each task introduces a new set of classes. For a total number of T tasks, each new task involves examples from a disjoint subset of 100 / T classes."}, {"heading": "4.2 Architectures", "text": "For MNIST tasks, we use fully connected neural networks with two hidden layers of 100 ReLU units. For CIFAR100 tasks, we use a smaller version of ResNet18 [He et al., 2015], with three times fewer feature maps at all levels. In addition, the CIFAR100 network has a definitive linear classifier per task. This is a simple way to use the task descriptor to adjust the power distribution to the subset of classes for each task."}, {"heading": "4.3 Methods", "text": "We compare GEM with several alternatives: 1. a single predictor that is trained across all tasks; 2. one independent predictor per task. Each independent predictor has the same architecture as \"single,\" but with T times less hidden units than \"single.\" Each new independent predictor can be randomly initialized or be a clone of the last trained predictor (decided by grid search).3. a multimodal predictor that has the same architecture of \"single,\" but has its own entry layer per task (only for MNIST records).4. EWC [Kirkpatrick et al., 2017] where loss is regulated to avoid catastrophic forgetfulness. 5. iCARL [Rebuffi et al., 2017] a class incremental learner that classifies the use of a nearest sample algorithm, and prevents catastrophic forgetfulness by using an episodic reminder based on an episodic learning algorithm."}, {"heading": "4.4 Results", "text": "Figure 1 (left) summarizes the average accuracy (ACC, Eq.2), feedback (BWT, Eq.3) and forward transmission (FWT, Eq.4) for all data sets and methods.We provide the full evaluation matrices R in Appendix B. Overall, GEM performs similarly or better than the multimodal model (which is very well suited for MNIST tasks).GEM minimizes backward transmission while showing negligible or positive forward transmission. Figure 1 (right) shows the development of the test accuracy of the first task throughout the data continuum.GEM exhibits minimal forgetability and positive backward transmission in the CIFAR100 experiment. Overall, GEM performs significantly better than other continuous learning methods such as EWC, while output is slightly higher than computing power."}, {"heading": "4.4.1 Importance of memory, number of passes, and order of tasks", "text": "Table 2 shows the final ACC in the CIFAR-100 experiment for both GEM and iCARL as a function of their episodic memory size. Also, in Table 2, the final ACC of the GEM is an increasing function of the episodic memory size, eliminating the need to set this hyperparameter carefully. GEM outperforms iCARL in a wide range of memory sizes. Table 3 examines the role of memory five times (in random order) taking into account the MNIST rotation dataset, as we perform more than one pass through the data, exacerbating the catastrophic problem of forgetfulness. For example, in the last column of Table 3 (with the exception of the result in the first row) each model is presented with the examples of a task (in random order) before moving on to the next task. Table 3 shows that memory-less methods (such as \"single\" and \"multimodal\") have a lower negative WT, resulting in a lower ACC."}, {"heading": "5 Related work", "text": "Continuous learning [Ring, 1994], also referred to as lifelong learning [Thrun, 1994, Thrun and Pratt, 2012, Thrun, 1998, 1996], considers learning through a sequence of tasks, where the learner considers both knowledge of past tasks and the use of this knowledge for the rapid acquisition of new skills. This learning environment has enjoyed several implementations [Carlson et al., 2010, Ruvolo and Eaton, 2013, Ring, 1997], and theoretical investigations [Baxter, 2000, Balcan et al., 2015, Pentina and Urner, 2016], although the latter are limited to linear models. In this work, we have re-examined continuous learning but proposed to focus on the more realistic setting, where examples are seen only once, and the learner is also provided with (potentially structured) task descriptions. Within this framework, we have introduced a new set of metrics, a training and testing protocol."}, {"heading": "6 Conclusion", "text": "We formalized the continuous learning scenario. First, we defined training and evaluation protocols to assess the quality of models in terms of their accuracy and ability to transfer knowledge between tasks forwards and backwards. Second, we introduced GEM, a simple model that uses episodic memory to avoid forgetting and promote positive feedback. Our experiments show the competitive performance of GEM over the state of the art. In its current form, our model has two major limitations. First, GEM does not use structured task descriptors that can be used to achieve positive forward transfer. Second, we did not study advanced methods to learn how to populate memory (such as the construction of task bodies [Lucic et al., 2017]), which is the key to prevent forgetting more efficiently."}, {"heading": "Acknowledgements", "text": "We thank M. Baroni, L. Bottou, M. Nickel, Y. Olivier and A. Szlam for their suggestions and insights."}, {"heading": "A Supplementary Material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Hyper-parameter Selection", "text": "In the following, we report on the hyperparameter values used in the network search."}, {"heading": "Single Predictor:", "text": "Learning rate: [1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001]"}, {"heading": "Independent Predictors:", "text": "Learning rate: [1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001] Fine tuning: [no, yes]"}, {"heading": "Multi-Modal Predictor:", "text": "Learning rate: [1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001]"}, {"heading": "EWC:", "text": "Learning Rate: [1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001] Regularization: [1, 3, 10, 30, 100, 300, 1000, 3000, 10000, 30000] iCARL: Learning Rate: [10.0, 3.0, 1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001] Regularization: [1, 3, 10, 30, 100, 300, 1000, 3000, 10000, 30000] # MNIST [0.1, 0.3, 1, 3, 10, 30] # CIFARmemory _ size: [100, 1000] # for MNIST [200, 1280, 2560, 5120] # for CIFAR"}, {"heading": "GEM:", "text": "Learning rate: [1.0, 0.3, 0.1, 0.03] Inner learning rate: [0.1, 1.0, 10] # MNIST Inner learning rate: [1.0, 3.0, 10.0] # CIFAR soft _ targets: [0.25, 0.5, 0.75, 1] memory _ size: [200, 1280, 5120] The best values we have found are: MNIST PERMUTATION Single: Learning rate = 0.03 Independent: Learning rate = 0.03, Finetune = yes Multimodal: Learning rate = 0.1 EWC: Learning rate = 0.1, Regulation = 3 GEM: Learning rate = 0.3, Memory size = 5120, Target = 0.5, Inner learning rate = 1.0MNIST ROTATION Single: Learning rate = 0.003 Independent: Learning rate = 0.1, Finetune = yes Multimodal: Learning rate = yes Multimodal: Learning rate = yes Multi-Modal: Learning rate = 0.1 EWC: Learning rate = 0.01, Regulation = 1000 GEM: 1.120 = 1.120 = 0.1, ARE = 0.1, Memory size = 0.1, ARE = 0.1, ARE = 0.1"}, {"heading": "B Full experiments", "text": "In this section we report on the evaluation matrices R for each model and each dataset. The first line of each matrix (above the line) is the baseline accuracy b, accuracy before each training. The input (i, j) of the matrix R is the accuracy of the j-th task shortly after the training of the i-th task."}, {"heading": "B.1 MNIST permutations", "text": "Singing the national anthem, singing the national anthem, singing the national anthem, singing the national anthem, singing the national anthem, singing the national anthem, singing the national anthem and singing the national anthem."}, {"heading": "Final Accuracy: 0.5928", "text": "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"}, {"heading": "Final Accuracy: 0.4313", "text": "0,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,0,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,0,000,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,000,000,0,000,000,000,0,0,0,0,0,0,000,000,000,000,0,000,0,0,0,0,0,0,0,0,0,0,0,0,0,000,000,000,0,000,0,0,0,0,0,000,0,0,0,0,0,0,0,0,0,0,0,"}, {"heading": "Final Accuracy: 0.7804", "text": "686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868686868"}, {"heading": "Final Accuracy: 0.6148", "text": "0,000,0,0,000,0,0,000,0,000,0,0,000,0,000,000,000,000,000,000,000,000,000,000,000,0,000,000,0,000,0,0,000,0,0,000,0,000,0,0,000,0,000,0,0,000,0,000,0,0,000,0,000,0,0,000,0,0,0,0,000,0,0,000,0,0,0,000,0,000,0,000,0,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,0,000,000,000,0,0,000,000,000,0,000,000,000,000,0,0,000,000,0,0,000,000,0,000,0,0,0,000,0,0,0,000,0,000,0,0,000,0,0,0,0,000,0,0,0,0,0,0,0,000,0,0,0,0,0,0,0,0,000,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"}, {"heading": "Final Accuracy: 0.8108", "text": "Backward: -0.0039 Forward: 0.0070"}, {"heading": "B.2 MNIST rotations", "text": "Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, Health, More, Health, Health, Health, Health, Health, Health,..."}, {"heading": "Final Accuracy: 0.5331", "text": "0,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,0,0,0,000,000,000,0,000,000,000,000,000,0,000,000,000,0,000,0,0,0,000,0,000,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"}, {"heading": "Final Accuracy: 0.5592", "text": "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"}, {"heading": "Final Accuracy: 0.7634", "text": "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"}, {"heading": "Final Accuracy: 0.5510", "text": "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"}, {"heading": "Final Accuracy: 0.8051", "text": "Backward: -0.0387 Forward: 0.6412"}, {"heading": "B.3 CIFAR-100 class-incremental", "text": "0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20"}, {"heading": "Final Accuracy: 0.4666", "text": "2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,0,0,2000 0,0,0,2000 0,0,0,2000 0,0,2000 0,0,0,2000 0,2000 0,3500 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,3500 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,"}, {"heading": "Final Accuracy: 0.3701", "text": "0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2560 0,2000 0,2560 0,2560 0,2220 0,2000 0,2220 0,2220 0,2220 0,2220 0,2220 0,2220 0,2220 0,2220 0,2240 0,2440 0,2740 0,1380 0,2160 0,2160 0,2160 0,2000 0,2340 0,2040 0,40 0,40 0,40 0,40 0,30 0,30 0,1840 0,00 0,1460 0,1460 0,1880 0,3500 0,3340 0,1980 0,2200 0,2200 0,2160 0,2160 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 20 20 20 20 20 20 20 20 0,20 0,20 20 0,20 0,20 20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 20 20 20 20 20 20 20 20 20 20 0,20 0,20 20 20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20"}, {"heading": "Final Accuracy: 0.4800", "text": "0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,4300 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,4300 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,400 0,2000 0,400 0,2000 0,400 0,2000 0,400 0,2000 0,400 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,0,2000 0,2000 0,2000 0,0,2000 0,2000 0,2000 0,2000 0,0,2000 0,2000 0,2000 0,2000 0,0,2000 0,2000 0,2000 0,0,2000 0,2000 0,0,2000 0,0,0,2000 0,0,2000 0,2000 0,2000 0,0,0,0,0,2000 0,2000 0,0,2000 0,2000 0,2000 0,2000 0,2000 0,0,0,2000 0,2000 0,2000 0,0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,0,2000 0,2000 0,2000 0,2000 0,2000 0,0,0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,0,2000 0,2000 0,0,0,0,0,2000 0,0,0,0,0,2000 0,2000 0,0,0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,2000 0,0,2000 0,0,0,2000 0,0,2000 0,0,2000 0,2000 0,2000 0,"}, {"heading": "Final Accuracy: 0.5075", "text": "0,2000 0,2000 0,1460 0,2180 0,2180 0,2180 0,2180 0,2180 0,2180 0,2180 0,2180 0,2180 0,2180 0,2180 0,2180 0,2180 0,2180 0,2100 0,2100 0,2100 0,50,2000 0,0,0,2000 0,0,0,2000 0,580 0,1720 0,2260 0,1920 0,3000 0,2260 0,1260 0,2220 0,2180 0,2180 0,2180 0,2180 0,2180 0,2180 0,2180 0,2180 0,2180 0,2200 0,2200 0,2200 0,2200 0,2200 0,2200 0,2220 0,2220 0,2040 0,240 0,2220 0,2240 0,2240 0,2240 0,2440 0,290 0,170,170.20 0,1760 0,2260 0,2260 0,2260 0,2240 0,1840 0,1840 0,1840 0,1840 0,1840 0,20 0,2020 0,2020 0,2200 0,2200 0,2200 0,2220 0,2220 0,200,2040 0,240 0,2220 0,22,20 0,240 0,22,20 0,22,20 0,2040 0,22,20 0,22,20 0,22,20 0,290 0,22,20 0,290 0,22,20 0,290 0,290 0,290 0,290 0,170,170,20 0,17,20 0,170,170,1760 0,1740 0,2260 0,2260 0,2240 0,2240 0,1840 0,1840 0,1840 0,1840 0,1840 0,20 0,20,40 0,2040 0,2040 0,2040 0,20,20 0,20,20 0,20,20 0,20,20 0,20,20 0,20,20 0,20,20 0,20,20 0,20,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20,20 0,20 0,20 2020 0,20 0,20 2020 0,20 0,20 0,20 0,20 0,20 2020 2020 0,20 0,20 0,20 0,20 2020 2020 2020 0,20 0,20 0,20 0,20 0,20 0,20 2020 2020 2020 2020 2020 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,40 0,40 0,40 0,40 0,40 0,40 0,40 0,40 0,40 0,40 0,40 0,40 0,40 0,40 0,"}, {"heading": "Final Accuracy: 0.6537", "text": "Backward: 0.0149 Next: 0.0075"}], "references": [{"title": "Catastrophic Forgetting in Gradient-Based Neural Networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Learning without forgetting", "author": ["D. Hoiem"], "venue": null, "citeRegEx": "Li and Hoiem.,? \\Q2016\\E", "shortCiteRegEx": "Li and Hoiem.", "year": 2016}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "Curriculum learning of multiple tasks. CVPR,", "citeRegEx": "Pan and Yang.,? \\Q2009\\E", "shortCiteRegEx": "Pan and Yang.", "year": 2009}, {"title": "Connectionist models of recognition memory: Constraints imposed by learning", "author": ["R. Ratcliff"], "venue": null, "citeRegEx": "Ratcliff.,? \\Q2017\\E", "shortCiteRegEx": "Ratcliff.", "year": 2017}, {"title": "ELLA: An Efficient Lifelong Learning Algorithm", "author": ["R. Hadsell"], "venue": null, "citeRegEx": "Hadsell.,? \\Q2013\\E", "shortCiteRegEx": "Hadsell.", "year": 2013}, {"title": "Is learning the n-th thing any easier than learning the first", "author": ["S. Thrun"], "venue": "Conference on Intelligent Robots and Systems,", "citeRegEx": "Thrun.,? \\Q1994\\E", "shortCiteRegEx": "Thrun.", "year": 1994}, {"title": "Improved multitask learning through synaptic intelligence", "author": ["F. Zenke", "B. Poole", "S. Ganguli"], "venue": null, "citeRegEx": "Zenke et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zenke et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "On the CIFAR100 tasks, we use a smaller version of ResNet18 [He et al., 2015], with three times less feature maps across all layers.", "startOffset": 60, "endOffset": 77}], "year": 2017, "abstractText": "One major obstacle towards artificial intelligence is the poor ability of models to quickly solve new problems, without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient of Episodic Memory (GEM), which alleviates forgetting while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of MNIST and CIFAR-100 demonstrate the strong performance of GEM when compared to the state-of-the-art.", "creator": "LaTeX with hyperref package"}}}