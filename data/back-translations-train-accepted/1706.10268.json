{"id": "1706.10268", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2017", "title": "SafetyNets: Verifiable Execution of Deep Neural Networks on an Untrusted Cloud", "abstract": "Inference using deep neural networks is often outsourced to the cloud since it is a computationally demanding task. However, this raises a fundamental issue of trust. How can a client be sure that the cloud has performed inference correctly? A lazy cloud provider might use a simpler but less accurate model to reduce its own computational load, or worse, maliciously modify the inference results sent to the client. We propose SafetyNets, a framework that enables an untrusted server (the cloud) to provide a client with a short mathematical proof of the correctness of inference tasks that they perform on behalf of the client. Specifically, SafetyNets develops and implements a specialized interactive proof (IP) protocol for verifiable execution of a class of deep neural networks, i.e., those that can be represented as arithmetic circuits. Our empirical results on three- and four-layer deep neural networks demonstrate the run-time costs of SafetyNets for both the client and server are low. SafetyNets detects any incorrect computations of the neural network by the untrusted server with high probability, while achieving state-of-the-art accuracy on the MNIST digit recognition (99.4%) and TIMIT speech recognition tasks (75.22%).", "histories": [["v1", "Fri, 30 Jun 2017 16:47:16 GMT  (101kb,D)", "http://arxiv.org/abs/1706.10268v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["zahra ghodsi", "tianyu gu", "siddharth garg"], "accepted": true, "id": "1706.10268"}, "pdf": {"name": "1706.10268.pdf", "metadata": {"source": "CRF", "title": "SafetyNets: Verifiable Execution of Deep Neural Networks on an Untrusted Cloud", "authors": ["Zahra Ghodsi", "Tianyu Gu", "Siddharth Garg"], "emails": ["sg175}@nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "Recent advances in deep learning have shown that multi-layered neural networks can achieve high performance on a wide range of machine learning tasks. However, training and performing neural network inferences is very expensive in computational terms. Therefore, several commercial vendors have begun to \"offer machine learning as a service\" (MLaaS) solutions that allow customers to outsource their invoices, both the training and the inference to which they include their data, to transfer to the cloud. Specifically, how can a client perform inferences on a deep cloud using an untrusted cloud, while the cloud has strong assurance that the cloud has performed inferences, there are compelling reasons for clients to beware of a third party cloud."}, {"heading": "2 Background", "text": "In this section, we first look at the necessary background of IP systems, and then describe the limited class of neural networks (which can be represented as arithmetic circuits) that SafetyNets manages."}, {"heading": "2.1 Interactive Proof Systems", "text": "In fact, it is a reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary,"}, {"heading": "2.2 Neural Networks as Arithmetic Circuits", "text": "As already mentioned, SafetyNets applies to neural networks, which can be expressed as arithmetic circuits. This requirement imposes the following constraints on neural network layers. Quadratic activations The activation functions in SafetyNets must be polynomials with integer coefficients (or, more precisely, coefficients in the Fp field), the simplest of which is the quadratic activation function, whose output is simply the square of their input. However, other frequently used activation functions such as ReLU, sigmoid or softmax activations are excluded except in the final output layer. Preliminary work has shown that neural networks with quadratic activations have the same representational power as networks with threshold activations and can be efficiently trained [6, 12]. Summary pooling pooling pooling pooling layers are often used to reduce network size and prevent translation invariance."}, {"heading": "2.3 Mathematical Model", "text": "A neural network of the L layer with the limitations discussed above can be modeled without loss of generality as follows: The input to the network is x-Fn0 \u00b7 bp, where n0 is the dimension of each input and b is the batch size. Level i [1, L] output has ni-output neurons2 and is specified and distorted bi-1-Fnip using a weight matrix wi-1-Fni-1p. Level i [1, L], yi-Fni-bp output is: yi = \u03c3quad (wi \u2212 1, yi \u2212 1 + bi \u2212 11 T)."}, {"heading": "3 SafetyNets", "text": "This year it has come to the point where it will be able to retaliate, to retaliate, \"he says.\" It's the way it is, \"he says.\" It's the way it is, \"he says."}, {"heading": "3.1 Sum-check for Matrix Multiplication", "text": "Since zi = wi.yi (remember that we assumed zero distortion for clarity), we can verify a claim about the multilinear expansion of zi evaluated at randomly selected points qi and ri by expressing Z-i (qi, ri) as [17]: Z-i (qi, ri) = \u2211 j-j {0,1} log (ni) W-i (qi, j).Y-i (j, ri) (6) Note that Equation 6 has the same form as the sum check problem in Equation 1. Consequently, the sum check protocol described in Section 2.1 can be used to verify this assertion. At the end of the sum check rounds, the examiner will have assertions about W-i which he checks locally, and Y-i which is verified by the sum check protocol for quadratic activations as described in Section 3.2."}, {"heading": "3.2 Sum-check for Quadratic Activation", "text": "In this step, we verify an assertion about the output of the square activation layer i, Y-i (si, ri) by writing it in relation to the input of the activation layer as follows: Y-i (si, ri) = \u2211 j-j-log (ni), k-i {0,1} log (b) I-i (si, j) I-i (ri, k) Z-2 i-1 (j, k), (7) where I-i (.,.) is the multilinear extension of the identity matrix. Equation 7 can also be verified by the sum check protocol and yields an assertion about Z-i-1, i.e. the inputs on the activation layer. This assertion is verified in turn by the protocol described in Section 3.1."}, {"heading": "3.3 Implementation", "text": "The fact that SafetyNets works only on elements in a finite field Fp, while we determine the prime numbers 1 = 12, presents a practical challenge. That is, how do we convert floating-point inputs and weights from the training into field elements and how do we select the size of the field p? Let us use w \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i.\" We do the same for inputs with a scaling factor \u03b1, \"i.e. x\" b\u03b1x \"e.\" Then, in order to ensure that all values on the network scale are isotropically multiplied, we need bi \"b\u03b12\" i \"- 1\" \u03b2 \"(2i \u2212 1 + 1) b.\" While larger alpha and lower values, in particular, we need to quantify the values in the network as well."}, {"heading": "4 Empirical Evaluation", "text": "In this section, we present empirical evidence to support our claim that SafetyNets enables low-cost verifiable execution of deep neural networks on untrusted clouds without compromising classification accuracy."}, {"heading": "4.1 Setup", "text": "We evaluated SafetyNets for three classification tasks. (1) Handwritten digit recognition on MNIST dataset 01, using 50,000 training sessions, 10,000 validations and 10,000 test images. (2) A more sophisticated version of digit recognition, MNIST back margin, an artificial dataset created by inserting a random background into the MNIST image. (3) Speech recognition on the TIMIT dataset, divided into a training set of 462 speakers, a validation set of 144 speakers, and a test set of 24 speakers. Raw audio samples are pre-processed as described in [3]. Each example includes its preceding and subsequent 7 frames, resulting in an 1845-dimensional input."}, {"heading": "4.2 Classification Accuracy of SafetyNets", "text": "Figure 2 compares training and testing errors of CNN-2-Quad / FcNN-3-Quad versus CNN-2-ReLU. Furthermore, we observe that networks with square activations seem to converge faster than networks with square activations, possibly because their gradients are larger than the gradient clipping. Next, we use the scaling and rounding strategy proposed in Section 3.3 to convert weights and inputs to integers. Table 1 shows the effects of the scaling factors \u03b1 and \u03b2 on the classification errors and maximum values observed in the network during the inference for MNIST-Back edge."}, {"heading": "5 Conclusion", "text": "In this paper, we introduced SafetyNets, a new framework that allows a client to verify the accuracy of deep neural networks running on an untrusted cloud. Building on the rich literature on interactive evidence systems for verifying universal and specialized calculations, we have designed and implemented a special IP protocol that is tailored to a specific class of deep neural networks, i.e. those that can be presented as arithmetic circuits. We have shown that the introduction of these limitations does not affect the accuracy of networks to real classification tasks such as digit and speech recognition, while allowing a client to demonstrably outsource inferences to the cloud at a low cost. In our future work, we will apply SafetyNets to deeper networks, extending them to integrity and privacy. There are VC techniques [16] that guarantee both, but typically come with higher costs."}], "references": [{"title": "Computational complexity: a modern approach", "author": ["S. Arora", "B. Barak"], "venue": "Cambridge University Press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Do deep nets really need to be deep? In Advances in neural information processing systems", "author": ["J. Ba", "R. Caruana"], "venue": "pages 2654\u20132662", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "A. Ng", "H. Lee"], "venue": "Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 215\u2013223", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Verifying computations with streaming interactive proofs", "author": ["G. Cormode", "J. Thaler", "K. Yi"], "venue": "Proceedings of the VLDB Endowment, pages 25\u201336", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Globally optimal training of generalized polynomial neural networks with nonlinear spectral methods", "author": ["A. Gautier", "Q.N. Nguyen", "M. Hein"], "venue": "pages 1687\u20131695", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Non-interactive verifiable computing: Outsourcing computation to untrusted workers", "author": ["R. Gennaro", "C. Gentry", "B. Parno"], "venue": "Annual Cryptology Conference, pages 465\u2013482", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy", "author": ["R. Gilad-Bachrach", "N. Dowlin", "K. Laine", "K. Lauter", "M. Naehrig", "J. Wernsing"], "venue": "pages 201\u2013210", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Delegating computation: interactive proofs for muggles", "author": ["S. Goldwasser", "Y.T. Kalai", "G.N. Rothblum"], "venue": "Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 113\u2013122", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1302.4389", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Speaker-independent phone recognition using hidden markov models", "author": ["K.-F. Lee", "H.-W. Hon"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, 37(11):1641\u20131648", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1989}, {"title": "On the computational efficiency of training neural networks", "author": ["R. Livni", "S. Shalev-Shwartz", "O. Shamir"], "venue": "Advances in Neural Information Processing Systems, pages 855\u2013863", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Algebraic methods for interactive proof systems", "author": ["C. Lund", "L. Fortnow", "H. Karloff", "N. Nisan"], "venue": "Journal of the ACM (JACM), pages 859\u2013868", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1992}, {"title": "Distributed execution with remote audit", "author": ["F. Monrose", "P. Wyckoff", "A.D. Rubin"], "venue": "NDSS, 99:3\u20135", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1999}, {"title": "Towards the science of security and privacy in machine learning", "author": ["N. Papernot", "P. McDaniel", "A. Sinha", "M. Wellman"], "venue": "arXiv preprint arXiv:1611.03814", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Pinocchio: Nearly practical verifiable computation", "author": ["B. Parno", "J. Howell", "C. Gentry", "M. Raykova"], "venue": "Security and Privacy (SP), 2013 IEEE Symposium on, pages 238\u2013252. IEEE", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Time-optimal interactive proofs for circuit evaluation", "author": ["J. Thaler"], "venue": "Advances in Cryptology\u2013 CRYPTO 2013, pages 71\u201389", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "A hybrid architecture for interactive verifiable computation", "author": ["V. Vu", "S. Setty", "A.J. Blumberg", "M. Walfish"], "venue": "Security and Privacy (SP), 2013 IEEE Symposium on, pages 223\u2013237", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Verifying computations without reexecuting them", "author": ["M. Walfish", "A.J. Blumberg"], "venue": "Communications of the ACM, 58(2):74\u201384", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "arXiv preprint arXiv:1301.3557", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Convexified convolutional neural networks", "author": ["Y. Zhang", "P. Liang", "M.J. Wainwright"], "venue": "arXiv preprint arXiv:1609.01000", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "While promising, the MLaaS model (and outsourced computing, in general) raises immediate security concerns, specifically relating to the integrity (or correctness) of computations performed by the cloud and the privacy of the client\u2019s data [15].", "startOffset": 240, "endOffset": 244}, {"referenceID": 17, "context": "The security risks posed by cloud computing have spurred theoretical advances in the area of verifiable computing (VC) [19].", "startOffset": 119, "endOffset": 123}, {"referenceID": 5, "context": "Trusted platform modules [7], for instance, require the client to place trust on the hardware manufacturer, and assume that the hardware is tamper-proof.", "startOffset": 25, "endOffset": 28}, {"referenceID": 12, "context": "Audits based on the server\u2019s execution time [14] require precise knowledge of the server\u2019s hardware configuration and assume, for instance, that the server is not over-clocked.", "startOffset": 44, "endOffset": 48}, {"referenceID": 3, "context": "The work in this paper leverages powerful VC techniques referred to as interactive proof (IP) systems [5, 9, 17, 18].", "startOffset": 102, "endOffset": 116}, {"referenceID": 7, "context": "The work in this paper leverages powerful VC techniques referred to as interactive proof (IP) systems [5, 9, 17, 18].", "startOffset": 102, "endOffset": 116}, {"referenceID": 15, "context": "The work in this paper leverages powerful VC techniques referred to as interactive proof (IP) systems [5, 9, 17, 18].", "startOffset": 102, "endOffset": 116}, {"referenceID": 16, "context": "The work in this paper leverages powerful VC techniques referred to as interactive proof (IP) systems [5, 9, 17, 18].", "startOffset": 102, "endOffset": 116}, {"referenceID": 17, "context": "A major criticism of IP systems (and, indeed, all existing VC techniques) when used for verifying general-purpose computations is that the prover\u2019s overheads are large, often orders of magnitude more than just computing f(x) [19].", "startOffset": 225, "endOffset": 229}, {"referenceID": 15, "context": "Recently, however, Thaler [17] showed that certain types of computations admit IP protocols with highly efficient verifiers and provers, which lays the foundations for the specialized IP protocols for deep neural networks that we develop in this paper.", "startOffset": 26, "endOffset": 30}, {"referenceID": 3, "context": "Existing IP systems proposed in literature [5, 9, 17, 18] use, at their heart, a protocol referred to as the sum-check protocol [13] that we describe here in some detail, and then discuss its applicability in verifying general-purpose computations expressed as arithmetic circuits.", "startOffset": 43, "endOffset": 57}, {"referenceID": 7, "context": "Existing IP systems proposed in literature [5, 9, 17, 18] use, at their heart, a protocol referred to as the sum-check protocol [13] that we describe here in some detail, and then discuss its applicability in verifying general-purpose computations expressed as arithmetic circuits.", "startOffset": 43, "endOffset": 57}, {"referenceID": 15, "context": "Existing IP systems proposed in literature [5, 9, 17, 18] use, at their heart, a protocol referred to as the sum-check protocol [13] that we describe here in some detail, and then discuss its applicability in verifying general-purpose computations expressed as arithmetic circuits.", "startOffset": 43, "endOffset": 57}, {"referenceID": 16, "context": "Existing IP systems proposed in literature [5, 9, 17, 18] use, at their heart, a protocol referred to as the sum-check protocol [13] that we describe here in some detail, and then discuss its applicability in verifying general-purpose computations expressed as arithmetic circuits.", "startOffset": 43, "endOffset": 57}, {"referenceID": 11, "context": "Existing IP systems proposed in literature [5, 9, 17, 18] use, at their heart, a protocol referred to as the sum-check protocol [13] that we describe here in some detail, and then discuss its applicability in verifying general-purpose computations expressed as arithmetic circuits.", "startOffset": 128, "endOffset": 132}, {"referenceID": 0, "context": "[2] V rejects an incorrect claim by P with probability greater than (1\u2212 ) where = nd p is referred to as the soundness error.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] demonstrated how sum-check can be used to verify the execution of arithmetic circuits using an IP protocol now referred to as GKR.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "While we refer the reader to [9] for further details of GKR, one important aspect of the protocol bears mention.", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "Prior work has shown that neural networks with quadratic activations have the same representation power as networks those with threshold activations and can be efficiently trained [6, 12].", "startOffset": 180, "endOffset": 187}, {"referenceID": 10, "context": "Prior work has shown that neural networks with quadratic activations have the same representation power as networks those with threshold activations and can be efficiently trained [6, 12].", "startOffset": 180, "endOffset": 187}, {"referenceID": 8, "context": "However, techniques such as max pooling [10] and stochastic pooling [20] are not supported since max and divisions operations are not easily represented as arithmetic circuits.", "startOffset": 40, "endOffset": 44}, {"referenceID": 18, "context": "However, techniques such as max pooling [10] and stochastic pooling [20] are not supported since max and divisions operations are not easily represented as arithmetic circuits.", "startOffset": 68, "endOffset": 72}, {"referenceID": 6, "context": "We note that the restrictions above are shared by a recently proposed technique, CryptoNets [8], that seeks to perform neural network based inference on encrypted inputs so as to guarantee data privacy.", "startOffset": 92, "endOffset": 95}, {"referenceID": 16, "context": "We note that as in prior work [18], SafetyNets amortizes the prover and verifier costs over batches of inputs.", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "Thaler [17] for verifying \u201cregular\" arithmetic circuits, that themselves specialize and refine prior work [5].", "startOffset": 7, "endOffset": 11}, {"referenceID": 3, "context": "Thaler [17] for verifying \u201cregular\" arithmetic circuits, that themselves specialize and refine prior work [5].", "startOffset": 106, "endOffset": 109}, {"referenceID": 16, "context": "[18], this check can be replaced by a check on whether the multilinear extension of zL\u22121 is correctly computed at a randomly picked point in the field, with minimal impact on the soundness error.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "That is, the verifier picks two vectors, qL\u22121 \u2208 FL p and rL\u22121 \u2208 F p at random, evaluates Z\u0303L\u22121(qL\u22121, rL\u22121), and checks whether it was correctly computed using a specialized sum-check protocol for matrix multiplication due to Thaler [17] (described in Section 3.", "startOffset": 232, "endOffset": 236}, {"referenceID": 15, "context": "yi (recall we assumed zero biases for clarity), we can check an assertion about the multilinear extension of zi evaluated at randomly picked points qi and ri by expressing Z\u0303i(qi, ri) as [17]: Z\u0303i(qi, ri) = \u2211", "startOffset": 187, "endOffset": 191}, {"referenceID": 6, "context": "Similar empirical observations were made by the CryptoNets work [8].", "startOffset": 64, "endOffset": 67}, {"referenceID": 3, "context": "As in prior works [5, 17, 18], we restrict our choice of p to Mersenne primes since they afford efficient modular arithmetic implementations, and specifically to the primes p = 2 \u2212 1 and p = 2 \u2212 1.", "startOffset": 18, "endOffset": 29}, {"referenceID": 15, "context": "As in prior works [5, 17, 18], we restrict our choice of p to Mersenne primes since they afford efficient modular arithmetic implementations, and specifically to the primes p = 2 \u2212 1 and p = 2 \u2212 1.", "startOffset": 18, "endOffset": 29}, {"referenceID": 16, "context": "As in prior works [5, 17, 18], we restrict our choice of p to Mersenne primes since they afford efficient modular arithmetic implementations, and specifically to the primes p = 2 \u2212 1 and p = 2 \u2212 1.", "startOffset": 18, "endOffset": 29}, {"referenceID": 2, "context": "ZCA whitening is applied to the raw dataset before training and testing [4].", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "The raw audio samples are pre-processed as described by [3].", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "During testing, all labels are mapped to 39 classes [11] for evaluation.", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": "Neural Networks For the two MNIST tasks, we used a convolutional neural network same as [21] with 2 convolutional layers with 5 \u00d7 5 filters, a stride of 1 and a mapcount of 16 and 32 for the first and second layer respectively.", "startOffset": 88, "endOffset": 92}, {"referenceID": 1, "context": "For TIMIT, we use a four layer network described by [3] with 3 hidden, fully connected layers with 2000 neurons and quadratic activations.", "startOffset": 52, "endOffset": 55}, {"referenceID": 15, "context": "Our implementation of SafetyNets uses Thaler\u2019s code for the IP protocol for matrix multiplication [17] and our own implementation of the IP for quadratic activations.", "startOffset": 98, "endOffset": 102}, {"referenceID": 8, "context": "We note that SafetyNets does not support techniques such as Maxout [10] that have", "startOffset": 67, "endOffset": 71}, {"referenceID": 1, "context": "[3] report an error of 18.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "A recent and closely related technique, CryptoNets [8], uses homomorphic encryption to provide privacy, but not integrity, for neural networks executing in the cloud.", "startOffset": 51, "endOffset": 54}, {"referenceID": 14, "context": "There are VC techniques [16] that guarantee both, but typically come at higher costs.", "startOffset": 24, "endOffset": 28}], "year": 2017, "abstractText": "Inference using deep neural networks is often outsourced to the cloud since it is a computationally demanding task. However, this raises a fundamental issue of trust. How can a client be sure that the cloud has performed inference correctly? A lazy cloud provider might use a simpler but less accurate model to reduce its own computational load, or worse, maliciously modify the inference results sent to the client. We propose SafetyNets, a framework that enables an untrusted server (the cloud) to provide a client with a short mathematical proof of the correctness of inference tasks that they perform on behalf of the client. Specifically, SafetyNets develops and implements a specialized interactive proof (IP) protocol for verifiable execution of a class of deep neural networks, i.e., those that can be represented as arithmetic circuits. Our empirical results on threeand four-layer deep neural networks demonstrate the run-time costs of SafetyNets for both the client and server are low. SafetyNets detects any incorrect computations of the neural network by the untrusted server with high probability, while achieving state-of-the-art accuracy on the MNIST digit recognition (99.4%) and TIMIT speech recognition tasks (75.22%).", "creator": "LaTeX with hyperref package"}}}