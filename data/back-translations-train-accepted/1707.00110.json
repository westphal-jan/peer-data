{"id": "1707.00110", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2017", "title": "Efficient Attention using a Fixed-Size Memory Representation", "abstract": "The standard content-based attention mechanism typically used in sequence-to-sequence models is computationally expensive as it requires the comparison of large encoder and decoder states at each time step. In this work, we propose an alternative attention mechanism based on a fixed size memory representation that is more efficient. Our technique predicts a compact set of K attention contexts during encoding and lets the decoder compute an efficient lookup that does not need to consult the memory. We show that our approach performs on-par with the standard attention mechanism while yielding inference speedups of 20% for real-world translation tasks and more for tasks with longer sequences. By visualizing attention scores we demonstrate that our models learn distinct, meaningful alignments.", "histories": [["v1", "Sat, 1 Jul 2017 08:16:24 GMT  (3265kb,D)", "http://arxiv.org/abs/1707.00110v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["denny britz", "melody y guan", "minh-thang luong"], "accepted": true, "id": "1707.00110"}, "pdf": {"name": "1707.00110.pdf", "metadata": {"source": "CRF", "title": "Efficient Attention using a Fixed-Size Memory Representation", "authors": ["Denny Britz", "Melody Y. Guan"], "emails": ["dennybritz@google.com", "melodyguan@google.com", "thangluong@google.com"], "sections": [{"heading": "1 Introduction", "text": "Sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) have achieved state-of-the-art in a variety of tasks, including Neural Machine Translation (NMT) (Bahdanau et al., 2014; Wu et al., 2016), Text Summary (Rush et al., 2015; Nallapati et al., 2016), Speech Recognition (Chan et al., 2015; Chorowski and Jaitly, 2016), Captioning (Xu et al., 2015) and Dialogical Modelling (Vinyals et al., 2015; Li et al.).The most popular approaches are based on an encoder decoder architecture consisting of two recursive neural networks (RNNNs) and an attention mechanism targeting source code tokens (Bahdanau et al., 2014; Luong et al., 2015)."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Sequence-to-Sequence Model with Attention", "text": "Our models are based on an encoder-decoder architecture with attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). An encoder function takes as input a sequence of source marks x = (x1,..., xm) and produces a sequence of states s = (s1,..., sm). The decoder is an RNN that predicts the probability of a target sequence y = (y1,..., yT | s). The probability of each target mark yi = (s1,..., | V |} is calculated on the basis of the recursive state in the decoder RNN, hi, the previous words y < i, and a context vector ci. The context vector ci, also called attention vector, is calculated as a weighted average of source states."}, {"heading": "3 Memory-Based Attention Model", "text": "During encoding, we calculate an attention matrix C, RK, D, where K is the number of attention vectors and a hyperparameter of our method, and D is the dimensionality of the highest encoding state. This matrix is calculated by predicting a score vector in each encoding time step. C is then a linear combination of the encoding states, weighted by \u03b1t: Ck = 0 \u03b1tkst (4) \u03b1t = softmax (W\u03b1st), (5) whereas W\u03b1 is a parameter matrix in RK \u00d7 D. The computational time complexity for this operation is O (KD | S |). One can think of C as a compact fixed-length memory memory memory that is the decoding attention of the decoder. An exception is the dot attention of Luong et al."}, {"heading": "3.1 Model Interpretations", "text": "Our memory-based attention model can be understood intuitively in two ways: We can interpret it as a \"prediction\" of the totality of attention contexts generated by a standard attention mechanism during encoding. To see this, let's assume we set K \u2248 | T |. In this case, we predict all | T | attention contexts during the encoding phase and learn to choose the right one during decoding. This is cheaper than calculating contexts one by one based on the decoder and encoder content. In fact, we could force this goal by first training a regular attention model and adding a regulatory term to force the memory matrix C to be close to the T \u00b7 D vectors computed by the standard attention. We could first train a regular attention model and then add a regulatory term to force the memory matrix C to be close to the T \u00b7 D vectors computed by the standard attention. We leave future work to choose such an object based on the matrix. Alternatively, we can compute our mechanism by first interpreting the location of the memory matrix, placing a K on the basis of the series D, and then a K on the basis of the prematrix."}, {"heading": "3.2 Position Encodings (PE)", "text": "In the formulation above, the predictions of the attention contexts are symmetrical, which means that Ci is not forced to differ from Cj 6 = i. While we hope that the model would learn to generate different attention contexts, we now present an extension that pushes the model in this direction. We add position codes to the score matrix that force the first few context vectors C1, C2... to focus on the beginning of the sequence and the last vectors..., CK \u2212 1, CK to focus on the end (thus encouraging vectors in the middle to focus on the middle). To obtain ls, we multiply the score vector \u03b1 withposition codes: CPE = | S | 1 = 0 \u03b1PEhs (8) \u03b1PEs = softmax (W\u03b1hs \u0441ls) (9)."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Toy Copying Experiment", "text": "To investigate the trade-off between speed and performance, we compared our technique with standard models with and without a sequence copy task of different lengths as in Graves et al. (2014). We generated 4 training data sets with 100,000 examples and a validation data set with 1,000 examples. Vocabulary size was 20. For each data set, the sequences had lengths randomly chosen between 0 and L, and were unique for each data set."}, {"heading": "4.1.1 Training Setup", "text": "All models are implemented with TensorFlow based on the seq2seq implementation by Britz et al. (2017) 3 and trained on a single machine with an Nvidia K40m GPU. We use a 2-layer 256 unit, a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) encoder, a 2-layer 256 unit LSTM decoder, and 256-dimensional embedding. For the attention base, we use the standard parameterized attention (Bahdanau et al., 2014). When entering each cell, a drop of 0.2 (0.8 retain the probability) is applied, and we optimize with Adam (Kingma and Ba, 2014) at a learning rate of 0.0001 and the batch size 128. We train for a maximum of 200,000 steps (see Figure 3 for example learning curves). BLEU values are enabled using tokenized data from the scroll sheet using the http / deq4 / deactivated in the Mosdeq4 multicenter code."}, {"heading": "4.1.2 Results", "text": "This is a study on the exchange between computing time and representational power. Results with and without position encryption are almost identical to the data. We can see that the performance increases to a point that depends on the data length. Both suggest themselves on the baseline. Results with and without position encryption are almost identical to the data."}, {"heading": "4.2 Machine Translation", "text": "Next, we will examine whether the memory-based attention mechanism is capable of handling complex data sets from the real world. To this end, we will use 4 large WMT '175 machine translation datasets for the following language pairs: English-Czech (en-cs, 52M examples), English-German (en-de, 5.9M examples), English-Finnish (en-fi, 2.6M examples) and English-Turkish (en-tr, 207.373 examples). We will use the newly available pre-5statmt.org / wmt17 / translation-task.htmlprocessed datasets for the WMT' 17 task."}, {"heading": "4.2.1 Training Setup", "text": "We use a similar setup to the Toy Copy task, but we use 512 RNN and embedding units, train with 8 distributed workers with one GPU each, and train for a maximum of 1 million steps. During the training, we store checkpoints every 30 minutes and select the best ones based on the validation BLEU score."}, {"heading": "4.2.2 Results", "text": "Table 2 compares our approach with and without position encodings and with different values for hyperparameter K with base models with regular attention mechanisms. Learning curves are shown in Figure 4. We see that our memory model with sufficiently high K, despite its simpler nature, performs on par with or slightly better than the attention-based base model. Generally, models with K = 64 perform better than corresponding models with K = 32, suggesting that the use of a larger number of attention vectors can capture a more comprehensive understanding of the source sequences. Position encodings also seem to consistently improve model performance.Table 3 shows that our model leads to a faster decoding time even for a complex dataset with a large6http: / / data.statmt.org / wmt17 / Translation-task / preprocessedTask / preprocessedvocabulary of 16k."}, {"heading": "5 Visualizing Attention", "text": "A useful feature of the standard attention mechanism is that it produces a meaningful alignment between source and target sequences. Often, the attention mechanism learns to gradually focus on the next source symbol while decoding the target. These visualizations can be an important tool when debugging and evaluating seq2seq models, and are often used for unknown token substitutes, which raises the question of whether our proposed attention mechanism also learns to generate meaningful alignments. As the number of attention contexts is generally smaller than the sequence length, it is not immediately obvious what each context would focus on. Our hope was that the model would learn to focus on multiple alignments at the same time, within the same attention vector. For example, if the source sequence is 40-long and we have K = 10 attention contexts, we would hope that C1 would roughly focus attention on the tokens from 1 to 4 to C8, that the tokens are distributed between these cases, C2 to 5."}, {"heading": "6 Related Work", "text": "Our papers build on previous work to make seq2seq models more computationally efficient. Luong et al. (2015) introduce various attention mechanisms that are mathematically simpler and work just as well or better than the original presented in Bahdanau et al. (2014). However, these typically still require O (D2) computational complexity or lack the flexibility to view the complete source sequence. Efficient location-based attention (Xu et al., 2015) has also been explored in the image recognition domain. Wu et al. (2016) presents several improvements to the standard seq2seq architecture that allow for a more efficient calculation of GPUs, e.g. only presence on the bottom layer. Kalchbrenner et al. (2016) propose a linear time architecture based on stacked evolutionary neural networks. Gehring et al. (2016) also propose the use of linear attention-based cognitive encoders to generate linear attention (2016) and linear attention-based encognitive encomonts (2016)."}, {"heading": "7 Conclusion", "text": "In this paper, we propose a novel memory-based attention mechanism that leads to a linear computation time of O (KD (| S | + | T |) when decoding in seq2seq models. Through a series of experiments, we show that our technique leads to consistent conclusions when sequences become longer, and complex data distributions such as those in Neural Machine Translation can be adapted. We show that our attention mechanism, despite being limited to a fixed representation after coding, learns meaningful alignments. We encourage future work that examines the optimal values of K for different language tasks and investigate whether it is possible to predict K based on the present task or not. We also encourage the evaluation of our models on other tasks that have to deal with long sequences but have concise representations, such as summarizing and answering questions, as well as further exploring their effects on memory and training speed."}], "references": [{"title": "Morphological inflection generation with hard monotonic attention", "author": ["Roee Aharoni", "Yoav Goldberg."], "venue": "CoRR abs/1611.01487. http://arxiv.org/abs/1611.01487.", "citeRegEx": "Aharoni and Goldberg.,? 2016", "shortCiteRegEx": "Aharoni and Goldberg.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Massive Exploration of Neural Machine Translation Architectures", "author": ["Denny Britz", "Anna Goldie", "Thang Luong", "Quoc Le."], "venue": "CoRR abs/1703.03906. http://arxiv.org/abs/1703.03906.", "citeRegEx": "Britz et al\\.,? 2017", "shortCiteRegEx": "Britz et al\\.", "year": 2017}, {"title": "A taxonomy of human translation styles", "author": ["Michael Carl", "Barbara Dragsted", "Arnt Lykke Jakobsen."], "venue": "Translation Journal 16(2).", "citeRegEx": "Carl et al\\.,? 2011", "shortCiteRegEx": "Carl et al\\.", "year": 2011}, {"title": "Listen, attend and spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc V. Le", "Oriol Vinyals."], "venue": "CoRR abs/1508.01211. http://arxiv.org/abs/1508.01211.", "citeRegEx": "Chan et al\\.,? 2015", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Towards better decoding and language model integration in sequence to sequence models", "author": ["Jan Chorowski", "Navdeep Jaitly."], "venue": "CoRR abs/1612.02695. http://arxiv.org/abs/1612.02695.", "citeRegEx": "Chorowski and Jaitly.,? 2016", "shortCiteRegEx": "Chorowski and Jaitly.", "year": 2016}, {"title": "A cheap linear attention mechanism with fast lookups and fixed-size representations", "author": ["Alexandre de Br\u00e9bisson", "Pascal Vincent."], "venue": "CoRR abs/1609.05866. http://arxiv.org/abs/1609.05866.", "citeRegEx": "Br\u00e9bisson and Vincent.,? 2016", "shortCiteRegEx": "Br\u00e9bisson and Vincent.", "year": 2016}, {"title": "A convolutional encoder model for neural machine translation", "author": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin."], "venue": "CoRR abs/1611.02344. http://arxiv.org/abs/1611.02344.", "citeRegEx": "Gehring et al\\.,? 2016", "shortCiteRegEx": "Gehring et al\\.", "year": 2016}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka."], "venue": "CoRR abs/1410.5401. http://arxiv.org/abs/1410.5401.", "citeRegEx": "Graves et al\\.,? 2014", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Neural machine translation in linear time", "author": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "A\u00e4ron van den Oord", "Alex Graves", "Koray Kavukcuoglu."], "venue": "CoRR abs/1610.10099. http://arxiv.org/abs/1610.10099.", "citeRegEx": "Kalchbrenner et al\\.,? 2016", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6980. http://arxiv.org/abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "CoRR abs/1510.03055. http://arxiv.org/abs/1510.03055.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "CoRR abs/1508.04025. http://arxiv.org/abs/1508.04025.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Sequence-to-sequence rnns for text summarization", "author": ["Ramesh Nallapati", "Bing Xiang", "Bowen Zhou."], "venue": "CoRR abs/1602.06023. http://arxiv.org/abs/1602.06023.", "citeRegEx": "Nallapati et al\\.,? 2016", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "Online and linear-time attention by enforcing monotonic alignments", "author": ["Colin Raffel", "Thang Luong", "Peter J. Liu", "Ron J. Weiss", "Douglas Eck."], "venue": "CoRR abs/1704.00784. http://arxiv.org/abs/1704.00784.", "citeRegEx": "Raffel et al\\.,? 2017", "shortCiteRegEx": "Raffel et al\\.", "year": 2017}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston."], "venue": "CoRR abs/1509.00685. http://arxiv.org/abs/1509.00685.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "ACL.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Weakly supervised memory networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus."], "venue": "CoRR abs/1503.08895. http://arxiv.org/abs/1503.08895.", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc V. Le."], "venue": "CoRR abs/1506.05869. http://arxiv.org/abs/1506.05869.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Sequence-to-sequence learning as beamsearch optimization", "author": ["Sam Wiseman", "Alexander M. Rush."], "venue": "CoRR abs/1606.02960. http://arxiv.org/abs/1606.02960.", "citeRegEx": "Wiseman and Rush.,? 2016", "shortCiteRegEx": "Wiseman and Rush.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio."], "venue": "CoRR abs/1502.03044.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "Sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) have achieved state of the art results across a wide variety of tasks, including Neural Machine Translation (NMT) (Bahdanau et al.", "startOffset": 28, "endOffset": 70}, {"referenceID": 5, "context": "Sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) have achieved state of the art results across a wide variety of tasks, including Neural Machine Translation (NMT) (Bahdanau et al.", "startOffset": 28, "endOffset": 70}, {"referenceID": 1, "context": ", 2014) have achieved state of the art results across a wide variety of tasks, including Neural Machine Translation (NMT) (Bahdanau et al., 2014; Wu et al., 2016), text summarization (Rush et al.", "startOffset": 122, "endOffset": 162}, {"referenceID": 17, "context": ", 2016), text summarization (Rush et al., 2015; Nallapati et al., 2016), speech recognition (Chan et al.", "startOffset": 28, "endOffset": 71}, {"referenceID": 15, "context": ", 2016), text summarization (Rush et al., 2015; Nallapati et al., 2016), speech recognition (Chan et al.", "startOffset": 28, "endOffset": 71}, {"referenceID": 4, "context": ", 2016), speech recognition (Chan et al., 2015; Chorowski and Jaitly, 2016), image captioning (Xu et al.", "startOffset": 28, "endOffset": 75}, {"referenceID": 6, "context": ", 2016), speech recognition (Chan et al., 2015; Chorowski and Jaitly, 2016), image captioning (Xu et al.", "startOffset": 28, "endOffset": 75}, {"referenceID": 23, "context": ", 2015; Chorowski and Jaitly, 2016), image captioning (Xu et al., 2015), and conversational modeling (Vinyals and Le, 2015; Li et al.", "startOffset": 54, "endOffset": 71}, {"referenceID": 21, "context": ", 2015), and conversational modeling (Vinyals and Le, 2015; Li et al., 2015).", "startOffset": 37, "endOffset": 76}, {"referenceID": 13, "context": ", 2015), and conversational modeling (Vinyals and Le, 2015; Li et al., 2015).", "startOffset": 37, "endOffset": 76}, {"referenceID": 1, "context": "The most popular approaches are based on an encoder-decoder architecture consisting of two recurrent neural networks (RNNs) and an attention mechanism that aligns target to source tokens (Bahdanau et al., 2014; Luong et al., 2015).", "startOffset": 187, "endOffset": 230}, {"referenceID": 14, "context": "The most popular approaches are based on an encoder-decoder architecture consisting of two recurrent neural networks (RNNs) and an attention mechanism that aligns target to source tokens (Bahdanau et al., 2014; Luong et al., 2015).", "startOffset": 187, "endOffset": 230}, {"referenceID": 1, "context": "Our models are based on an encoder-decoder architecture with attention mechanism (Bahdanau et al., 2014; Luong et al., 2015).", "startOffset": 81, "endOffset": 124}, {"referenceID": 14, "context": "Our models are based on an encoder-decoder architecture with attention mechanism (Bahdanau et al., 2014; Luong et al., 2015).", "startOffset": 81, "endOffset": 124}, {"referenceID": 3, "context": "Eye-tracking and keystroke logging data from human translators show that translators generally do not reread previously translated source text words when producing target text (Carl et al., 2011).", "startOffset": 176, "endOffset": 195}, {"referenceID": 1, "context": "Variants of fatt used in Bahdanau et al. (2014) and Luong et al.", "startOffset": 25, "endOffset": 48}, {"referenceID": 1, "context": "Variants of fatt used in Bahdanau et al. (2014) and Luong et al. (2015) are:", "startOffset": 25, "endOffset": 72}, {"referenceID": 14, "context": "An exception is the dot-attention from Luong et al. (2015), which is O(D|S||T |), which we discuss further in Section 3.", "startOffset": 39, "endOffset": 59}, {"referenceID": 14, "context": "Standard location-based attention mechanism, by contrast, predicts a location in the source sequence to focus on (Luong et al., 2015; Xu et al., 2015).", "startOffset": 113, "endOffset": 150}, {"referenceID": 23, "context": "Standard location-based attention mechanism, by contrast, predicts a location in the source sequence to focus on (Luong et al., 2015; Xu et al., 2015).", "startOffset": 113, "endOffset": 150}, {"referenceID": 19, "context": "adapting a formula from (Sukhbaatar et al., 2015).", "startOffset": 24, "endOffset": 49}, {"referenceID": 9, "context": "To investigate the trade-off between speed and performance, we compare our technique to standard models with and without attention on a Sequence Copy Task of varying length like in Graves et al. (2014). We generated 4 training datasets of 100,000 examples and a validation dataset of 1,000 examples.", "startOffset": 181, "endOffset": 202}, {"referenceID": 10, "context": "We use a 2-layer 256-unit, a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) encoder, a 2-layer 256-unit LSTM decoder, and 256-dimensional embeddings.", "startOffset": 48, "endOffset": 82}, {"referenceID": 1, "context": "For the attention baseline, we use the standard parametrized attention (Bahdanau et al., 2014).", "startOffset": 71, "endOffset": 94}, {"referenceID": 12, "context": "8 keep probability) is applied to the input of each cell and we optimize using Adam (Kingma and Ba, 2014) at a learning rate of 0.", "startOffset": 84, "endOffset": 105}, {"referenceID": 1, "context": "All models are implemented using TensorFlow based on the seq2seq implementation of Britz et al. (2017)3 and trained on a single machine with a Nvidia K40m GPU.", "startOffset": 83, "endOffset": 103}, {"referenceID": 22, "context": "size of 10 (Wiseman and Rush, 2016).", "startOffset": 11, "endOffset": 35}, {"referenceID": 18, "context": "We learn shared vocabularies of 16,000 subword units using the BPE algorithm (Sennrich et al., 2016).", "startOffset": 77, "endOffset": 100}, {"referenceID": 23, "context": "Efficient location-based attention (Xu et al., 2015) has also been explored in the image recognition domain.", "startOffset": 35, "endOffset": 52}, {"referenceID": 9, "context": "Luong et al. (2015) introduce various attention mechanisms that are computationally simpler and perform as well or better than the original one presented in Bahdanau et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "(2015) introduce various attention mechanisms that are computationally simpler and perform as well or better than the original one presented in Bahdanau et al. (2014). However, these typically still require O(D2) computation complexity, or lack the flexibility to look at the full source sequence.", "startOffset": 144, "endOffset": 167}, {"referenceID": 0, "context": "(2015) introduce various attention mechanisms that are computationally simpler and perform as well or better than the original one presented in Bahdanau et al. (2014). However, these typically still require O(D2) computation complexity, or lack the flexibility to look at the full source sequence. Efficient location-based attention (Xu et al., 2015) has also been explored in the image recognition domain. Wu et al. (2016) presents several enhancements to the standard seq2seq architecture that allow more efficient computation on GPUs, such as only attending on the bottom layer.", "startOffset": 144, "endOffset": 424}, {"referenceID": 0, "context": "(2015) introduce various attention mechanisms that are computationally simpler and perform as well or better than the original one presented in Bahdanau et al. (2014). However, these typically still require O(D2) computation complexity, or lack the flexibility to look at the full source sequence. Efficient location-based attention (Xu et al., 2015) has also been explored in the image recognition domain. Wu et al. (2016) presents several enhancements to the standard seq2seq architecture that allow more efficient computation on GPUs, such as only attending on the bottom layer. Kalchbrenner et al. (2016) propose a linear time architecture based on stacked convolutional neural networks.", "startOffset": 144, "endOffset": 609}, {"referenceID": 0, "context": "(2015) introduce various attention mechanisms that are computationally simpler and perform as well or better than the original one presented in Bahdanau et al. (2014). However, these typically still require O(D2) computation complexity, or lack the flexibility to look at the full source sequence. Efficient location-based attention (Xu et al., 2015) has also been explored in the image recognition domain. Wu et al. (2016) presents several enhancements to the standard seq2seq architecture that allow more efficient computation on GPUs, such as only attending on the bottom layer. Kalchbrenner et al. (2016) propose a linear time architecture based on stacked convolutional neural networks. Gehring et al. (2016) also propose the use of convolutional encoders to speed up NMT.", "startOffset": 144, "endOffset": 714}, {"referenceID": 0, "context": "(2015) introduce various attention mechanisms that are computationally simpler and perform as well or better than the original one presented in Bahdanau et al. (2014). However, these typically still require O(D2) computation complexity, or lack the flexibility to look at the full source sequence. Efficient location-based attention (Xu et al., 2015) has also been explored in the image recognition domain. Wu et al. (2016) presents several enhancements to the standard seq2seq architecture that allow more efficient computation on GPUs, such as only attending on the bottom layer. Kalchbrenner et al. (2016) propose a linear time architecture based on stacked convolutional neural networks. Gehring et al. (2016) also propose the use of convolutional encoders to speed up NMT. de Br\u00e9bisson and Vincent (2016) propose a linear attention mechanism based on covariance matrices applied to information retrieval.", "startOffset": 144, "endOffset": 810}, {"referenceID": 0, "context": "(2015) introduce various attention mechanisms that are computationally simpler and perform as well or better than the original one presented in Bahdanau et al. (2014). However, these typically still require O(D2) computation complexity, or lack the flexibility to look at the full source sequence. Efficient location-based attention (Xu et al., 2015) has also been explored in the image recognition domain. Wu et al. (2016) presents several enhancements to the standard seq2seq architecture that allow more efficient computation on GPUs, such as only attending on the bottom layer. Kalchbrenner et al. (2016) propose a linear time architecture based on stacked convolutional neural networks. Gehring et al. (2016) also propose the use of convolutional encoders to speed up NMT. de Br\u00e9bisson and Vincent (2016) propose a linear attention mechanism based on covariance matrices applied to information retrieval. Raffel et al. (2017) enable online linear time attention calculation by enforcing that the alignment between input and output sequence elements be monotonic.", "startOffset": 144, "endOffset": 931}, {"referenceID": 0, "context": "Previously, monotonic attention was proposed for morphological inflection generation by Aharoni and Goldberg (2016).", "startOffset": 88, "endOffset": 116}], "year": 2017, "abstractText": "The standard content-based attention mechanism typically used in sequence-to-sequence models is computationally expensive as it requires the comparison of large encoder and decoder states at each time step. In this work, we propose an alternative attention mechanism based on a fixed size memory representation that is more efficient. Our technique predicts a compact set of K attention contexts during encoding and lets the decoder compute an efficient lookup that does not need to consult the memory. We show that our approach performs on-par with the standard attention mechanism while yielding inference speedups of 20% for real-world translation tasks and more for tasks with longer sequences. By visualizing attention scores we demonstrate that our models learn distinct, meaningful alignments.", "creator": "LaTeX with hyperref package"}}}