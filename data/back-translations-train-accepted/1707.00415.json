{"id": "1707.00415", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jul-2017", "title": "Dual Supervised Learning", "abstract": "Many supervised learning tasks are emerged in dual forms, e.g., English-to-French translation vs. French-to-English translation, speech recognition vs. text to speech, and image classification vs. image generation. Two dual tasks have intrinsic connections with each other due to the probabilistic correlation between their models. This connection is, however, not effectively utilized today, since people usually train the models of two dual tasks separately and independently. In this work, we propose training the models of two dual tasks simultaneously, and explicitly exploiting the probabilistic correlation between them to regularize the training process. For ease of reference, we call the proposed approach \\emph{dual supervised learning}. We demonstrate that dual supervised learning can improve the practical performances of both tasks, for various applications including machine translation, image processing, and sentiment analysis.", "histories": [["v1", "Mon, 3 Jul 2017 06:19:31 GMT  (1152kb,D)", "http://arxiv.org/abs/1707.00415v1", "ICML 2017"]], "COMMENTS": "ICML 2017", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yingce xia", "tao qin", "wei chen", "jiang bian", "nenghai yu", "tie-yan liu"], "accepted": true, "id": "1707.00415"}, "pdf": {"name": "1707.00415.pdf", "metadata": {"source": "META", "title": "Dual Supervised Learning", "authors": ["Yingce Xia", "Tao Qin", "Wei Chen", "Jiang Bian", "Nenghai Yu", "Tie-Yan Liu"], "emails": ["<taoqin@microsoft.com>."], "sections": [{"heading": "1. Introduction", "text": "It is about the question to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is"}, {"heading": "2. Framework", "text": "In this section, we will formulate the problem of dual supervised learning (DSL), describe an algorithm for DSL, and discuss its connections with existing tutorials. In our experiments, we selected the most frequently cited models using either open source code or sufficient implementation details to ensure that we can reproduce the results of previous work. All of our experiments are performed on a single Telsa K40m GPU."}, {"heading": "2.1. Problem Formulation", "text": "To take advantage of duality, we formulate a new learning scheme that includes two tasks: a dual task that takes a sample from room X as input and maps into room Y, and a dual task takes a sample from room Y as input and maps into room X. Suppose we have n pairs of training {(xi, yi) ni = 1 i.i.d. Sampling from room X \u00b7 Y according to an unknown distribution P. Our goal is to show the bidirectional relationship between the two inputs x and y. To be specific, we perform the following two tasks: (1) the primary learning task aims to find a function f: X 7 \u2192 Y so that the prediction of f for x is similar to its real counterpart y; (2) the dual learning task aims to find a function g: Y 7 \u2192 X so that the prediction of g for y is similar to its real counterpart x. The dissimilarity is punished by a loss function."}, {"heading": "2.2. Algorithm Description", "text": "In practical applications of artificial intelligence, the basic marginal distributions are usually not available. As an alternative, we use the empirical marginal distributions P-x and P-y to fulfill the constraint in equation. (2) To solve the DSL problem, we introduce Lagrange multipliers and add the equality constraint of the probable duality to the objective functions. First, we convert the probable duality constraint into the following regulation term (including the empirical marginal distributions): \"Duality = (log P-x) + logP-y (y-x) \u2212 logP-y (x-y; investyx)))). (3) We then learn the models of the two tasks by minimizing the weighted combination between the original loss functions and the aforementioned regulation term. The algorithm will be flexible in algorithm 1.In personal practice D and the optimizers 1.1 (depending on the assignment 1 and 1)."}, {"heading": "2.3. Discussions", "text": "The duality between tasks was used to enable learning from unlabeled data (He et al., 2016a). As an early attempt to exploit duality, this work actually uses the outer connection between dual tasks, which helps to form a closed feedback loop and allows unattended learning. For example, in the application of machine translation, the primary tasks / models are first translated into a French sentence y \u2032 by an unlabeled English sentence x; then, the dual task / model translates y \u2032 back to an English sentence x \u2032; finally, both the primary and dual models are input by minimiz algorithm 1 Dual Supervise Learning Algorithm: Marginal Distributions P \u00b2 (xi) and P \u00b2 (yi) for any i [n]; Lagrange parameters for the DSxy and the Optimizers Opt2 and Opt2; repeata Get Minibatch of {xj-Paj} (xj = 1); yxy;"}, {"heading": "3. Application to Machine Translation", "text": "First, we apply our dual supervised learning algorithm to machine translation and examine whether it can improve translation qualities by using the probable duality of dual translation tasks. Following, we conduct experiments on three pairs of dual tasks 2: English \u2194 French (En \u2194 Fr), English \u2194 Germany (En \u2194 De) and English \u2194 Chinese (En \u2194 Zh)."}, {"heading": "3.1. Settings", "text": "We use the same datasets as in (Jean et al., 2015) to conduct experiments on En-Fr and En-D, respectively (WMT, 2014). As part of WMT \"14, the training data consists of 12M sets for En-Fr and 4.5M for En-D, respectively (WMT, 2014). We combine the knowledge acquired and the knowledge acquired to improve the validation and use of nestest2014 as test sets. For the dual tasks of En-Fr, we use 10M set pairs that we have received from a commercial company as training data. We lease the validation of NIST2006 as validation and NIST2008 as test sets3 as a test. Note that during the training of all three pairs of dual tasks, we drop all sets with more than 50 words.Marginal distributions P (x) and P (x) We use the language modeling based on LMSTM."}, {"heading": "3.2. Results", "text": "Table 1 shows the BLEU results of the dual task definition by replacing the \"UNK\" method with the applied DSL methodology. \"We compared this with different DSL methods.\" Note that in this table, we use (MT08) and (MT12) to denote the results performed on NIST2008 and NIST2012. Although we can find that on all these three pairs of symmetrical tasks DSL can improve the performance of both dual tasks simultaneously. In order to better understand the effects of applying the probable duality restriction as regularization, we calculate the \"duality on the test set of DSL compared to RNNSearch.\" Particularly after applying DSL to En \u2192 Fr, the \"duality decreases from 1545.68 to 1468.28, which also indicates that the two models become more coherent in terms of probable duality. (Jean et al, 2015) An effective suggested translation technique that we can achieve by comparing the corresponding translation technique with the UNK."}, {"heading": "4. Application to Images Processing", "text": "In the field of image processing, image classification (image \u2192 label) and image generation (label \u2192 image) are dual. In this section, we apply our dual supervised learning framework to these two tasks and conduct experimental studies based on a public data set, CIFAR10 (Krizhevsky & Hinton, 2009), with 10 image classes. In our experiments, we apply a popular method, ResNet5, for image classification and a newer method, Pixel CNN + + 6, for image generation. Let X designate the image space and Y designate the category space associated with CIFAR10."}, {"heading": "4.1. Settings", "text": "Marginal Distributions In our experiments, we simply use the uniform distribution to specify the Marginal Distribution P-72 (y) of the 10-class labels < 72 (2017), which means that the Marginal Distribution of each class is equal to 0.1. Image distribution P-x is usually defined as an error that occurs only on the previous pixels xj with index j < i}, where all pixels of the image are serialized and xi is the value of the i-th pixel of a m-pixel image. Note that the model xi is only on the previous pixels xj with index j < i}, using the PixelCNN + + + model, which is so far the best algorithm to model the image distribution. Models For the task of image classification, we choose 32-layer ResNet (referred to as ResNet-32) and 110-layer ResNet (referred to as ResNet-i training), which we use as the best algorithm to model so far to model the image distribution."}, {"heading": "4.2. Results on Image Classification", "text": "Table 4 compares the error rates of two image classification models, i.e. DSL vs. Baseline, on the test set. This table shows that DSL with ResNet-32 or ResNet-110 achieves better accuracy than the baseline method. Interestingly, we observe from Table 4 that DSL leads to a higher relative performance improvement of the ResNet-110 compared to the ResNet-32. We suspect a possible reason for this, that due to the limited training data, adequate regulation may be more beneficial to the 110-layer ResNet with higher model complexity, and duality-oriented regularization actually plays this role and leads to a higher relative improvement."}, {"heading": "4.3. Results on Image Generation", "text": "Our further experimental results show that DSL, based on ResNet-110, can reduce the bpd test from 2.94 (baseline) to 2.93 (DSL), which is a new state of the art on CIFAR-10. In fact, it is quite difficult to improve bpd by 0.01, but this seems like a slight change. We also find that there is no significant improvement on test bpd based on ResNet-32. An intuitive explanation is that since ResNet-110 is stronger than ResNet-32 in modeling the conditional probability P (y | x), it can better support the task of image generation by limiting / regulating the probable duality. as stated in (Theis et al., 2015), bpd is not the only evaluation rule of image generation P (y | x). Therefore, we continue to perform a qualitative analysis by creating images generated by dual supervised learning with images generated by the basic model for each category."}, {"heading": "5. Application to Sentiment Analysis", "text": "Finally, we apply the dual supervised learning system to the field of sentiment analysis, in which the primary task, the sentiment classification (Maas et al., 2011; Dai & Le, 2015), is to predict the sentiment polarity label of a given sentence; and the dual task, though not entirely obvious but real, is sentence generation based on sentiment polarity."}, {"heading": "5.1. Experimental Setup", "text": "In this dataset, each set is associated with either a positive or negative sentiment label. We sample a subset of 3750 sets of training data as a validation set for hyperparameter tuning and use the remaining training data for model training.Marginal Distributions We simply use the uniform distribution to determine the boundary distribution P (y) of polarity designations, meaning that the boundary distribution of positive or negative class equality is 0.5. On the other hand, we use the benefits of LSTM-based language modeling to determine the boundary distribution P (x) of a set x. The test perplexities (Bengio et al., 2003) of the obtained language model is 58.74.Model Implementation We use the widely used LSTM (Dai & Le.)"}, {"heading": "5.2. Results", "text": "Table 5 compares the performance of DSL with the basic method in terms of both the error rates of sentiment classification and the helplessness of sentence generation. We have two observations from the table: First, DSL can reduce the classification error by 0.90 without changing the LSTM-based model structure. Second, DSL slightly improves the helplessness of sentence generation, but the improvement is not very significant, because the sentiment label can only provide a maximum of 1-bit information, so the perplexity difference between the language model (i.e. the boundary distribution P (x)) and CLM (i.e. the conditional distribution P (x | y)) is not so great that the sentiment label limits the improvement through DSL generation. Qualitative analysis on sentence generation, as shown above, is a good DSL model."}, {"heading": "5.3. Discussions", "text": "In previous experiments, we started DSL training with well-trained primary and dual models. We conducted some further experiments to verify whether warm start is a must for DSL. (1) We train DSL with a warm start set generator and a cold start set classifier (randomly initialized). In this case, DSL reaches a classification error of 9.44%, which is better than the baseline classifier in Ta-ble 5. (2) We train DSL with a warm start classifier and a cold start set generator. The perplexity of the generator after DSL training reached 58.79, which is better than the baseline generator. (3) We train DSL from both cold start models. The final classification error is 9.50% and the perplexity of the generator is 58.82, both of which are better than the baseline. These results show that the success of DSL does not necessarily require warm start models, although they can speed up the training of DSL."}, {"heading": "6. Conclusions and Future Work", "text": "We have introduced a probable concept of duality that can serve as a data-dependent regulator to better guide education. Empirical studies have validated the effectiveness of dual supervised learning. There are several directions to explore in the future. Firstly, we will test dual supervised learning on more dual tasks, such as speech recognition and speech synthesis. Secondly, we will enrich theoretical studies to better understand dual supervised learning. Thirdly, it is interesting to combine dual supervised learning with unsupervised dual learning (He et al., 2016) to use unmarked data to further improve the two dual tasks. Fourth, we will combine dual supervised learning with dual inference (Appia et al., 2017) to use structural duality to improve both dual learning and dual practice."}, {"heading": "A. Theoretical Analysis", "text": "As we know, the final goal of dual learning is to give correct predictions for the invisible test data. That is, we want to minimize the (expected) risk of dual learning, which is defined as: \"It is a (double) error (f (x), y) +\" 2 (g), x), x, \"x\" (f), x, \"x,\" x, \"x,\" \"x,\" x, \"x,\" x, \"x,\" \"x,\" \"x,\" \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" \"x,\" \"x,\" \"x,\" \"x,\" \"x,\" \"x,\" \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\", \"x,\" x, \",\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \",\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x, x,\" x, \"x,\" x, \"x,\" x, x, x, \"x,\" x, \"x, x,\" x, \"x,\" x, \"x, x, x,\" x, \"x,\" x, \"x,\" x, \"x,\", \"x,\" x, \"x,\" x, \",\" x, \"x,\" x, \"x,\", \"x,\" x, \"x,\", \"x,\" x, \",\" x, \""}, {"heading": "B. Details about the Language Models for Marginal Distributions", "text": "We use the LSTM language models (Sundermeyer et al., 2012; Mikolov et al., 2010) to characterize the boundary distribution of a sentence x, which is defined as \u00dc Tx i = 1 P (xi | x < i), where xi is the i-th word in x, Tx denotes the number of words in x and the index < i {1, 2, \u00b7 \u00b7 \u00b7, i \u2212 1}. The embedding dimension and the hidden node are both 1024. We apply 0.5 dropouts to the embedding and the last hidden layer before Softmax. The validation difficulties of the language models are illustrated in Table 7, where the validation sentences are equal. For the boundary distributions for sentences in the sentiment classification, we re-select the LSTM language model as for machine translation applications. The two differences are: (i) the vocabulary size is 10000; (ii) the embedding dimension is the 500."}], "references": [{"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Chrzanowski", "Mike", "Coates", "Adam", "Diamos", "Greg"], "venue": "In 33rd International Conference on Machine Learning,", "citeRegEx": "Chrzanowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chrzanowski et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Bartlett", "Peter L", "Mendelson", "Shahar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2002}, {"title": "A neural probabilistic language model", "author": ["Bengio", "Yoshua", "Ducharme", "R\u00e9jean", "Vincent", "Pascal", "Jauvin", "Christian"], "venue": "Journal of machine learning research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Semi-supervised sequence learning", "author": ["Dai", "Andrew M", "Le", "Quoc V"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "(icassp), 2013 ieee international conference on,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Dual learning for machine translation", "author": ["He", "Di", "Xia", "Yingce", "Qin", "Tao", "Wang", "Liwei", "Yu", "Nenghai", "Liu", "Tie-Yan", "Ma", "Wei-Ying"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Support vector machines", "author": ["Hearst", "Marti A", "Dumais", "Susan T", "Osuna", "Edgar", "Platt", "John", "Scholkopf", "Bernhard"], "venue": "IEEE Intelligent Systems and their Applications,", "citeRegEx": "Hearst et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Hearst et al\\.", "year": 1998}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean", "S\u00e9bastien", "Cho", "Kyunghyun", "Memisevic", "Roland", "Bengio", "Yoshua"], "venue": "In ACL,", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tomas", "Karafi\u00e1t", "Martin", "Burget", "Lukas", "Cernock\u1ef3", "Jan", "Khudanpur", "Sanjeev"], "venue": "In Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Foundations of machine learning", "author": ["Mohri", "Mehryar", "Rostamizadeh", "Afshin", "Talwalkar", "Ameet"], "venue": "MIT press,", "citeRegEx": "Mohri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohri et al\\.", "year": 2012}, {"title": "Wavenet: A generative model for raw audio", "author": ["Oord", "Aaron van den", "Dieleman", "Sander", "Zen", "Heiga", "Simonyan", "Karen", "Vinyals", "Oriol", "Graves", "Alex", "Kalchbrenner", "Nal", "Senior", "Andrew", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1609.03499,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni", "Kishore", "Roukos", "Salim", "Ward", "Todd", "Zhu", "WeiJing"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "ICML (3),", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Pixelcnn++: A pixelcnn implementation with discretized logistic mixture likelihood and other modifications", "author": ["Salimans", "Tim", "Karpathy", "Andrej", "Chen", "Xi", "P. Kingma", "Diederik", "Bulatov", "Yaroslav"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Salimans et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2017}, {"title": "Minimum risk training for neural machine translation", "author": ["Shen", "Shiqi", "Cheng", "Yong", "He", "Zhongjun", "Wei", "Wu", "Hua", "Sun", "Maosong", "Liu", "Yang"], "venue": null, "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Lstm neural networks for language modeling", "author": ["Sundermeyer", "Martin", "Schl\u00fcter", "Ralf", "Ney", "Hermann"], "venue": "In Interspeech, pp", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "A note on the evaluation of generative models", "author": ["Theis", "Lucas", "Oord", "A\u00e4ron van den", "Bethge", "Matthias"], "venue": "arXiv preprint arXiv:1511.01844,", "citeRegEx": "Theis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2015}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Tibshirani", "Robert"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "citeRegEx": "Tibshirani and Robert.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani and Robert.", "year": 1996}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["van den Oord", "Aaron", "Kalchbrenner", "Nal", "Espeholt", "Lasse", "Vinyals", "Oriol", "Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Pixel recurrent neural networks", "author": ["van den Oord", "Aaron", "Kalchbrenner", "Nal", "Kavukcuoglu", "Koray"], "venue": "In 33rd International Conference on Machine Learning,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Dual inference for machine learning", "author": ["Xia", "Yingce", "Bian", "Jiang", "Qin", "Tao", "Yu", "Nenghai", "Liu", "TieYan"], "venue": "In The 26th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Xia et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Xia et al\\.", "year": 2017}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": ", 2016b;a), speech recognition (Graves et al., 2013; Amodei et al., 2016), and speech generation/synthesis (Oord et al.", "startOffset": 31, "endOffset": 73}, {"referenceID": 15, "context": ", 2016), and speech generation/synthesis (Oord et al., 2016).", "startOffset": 41, "endOffset": 60}, {"referenceID": 18, "context": ", 2016b) as our baseline for image classification, and PixelCNN++(Salimans et al., 2017) as our baseline for image generation.", "startOffset": 65, "endOffset": 88}, {"referenceID": 9, "context": "While `duality can be regarded as a regularization term, it is data dependent, which makes DSL different from Lasso (Tibshirani, 1996) or SVM (Hearst et al., 1998), where the regularization term is data-independent.", "startOffset": 142, "endOffset": 163}, {"referenceID": 10, "context": "Datasets We employ the same datasets as used in (Jean et al., 2015) to conduct experiments on En\u2194Fr and En\u2194De.", "startOffset": 48, "endOffset": 67}, {"referenceID": 20, "context": "Marginal Distributions P\u0302 (x) and P\u0302 (y) We use the LSTMbased language modeling approach (Sundermeyer et al., 2012; Mikolov et al., 2010) to characterize the marginal distribution of a sentence x, defined as \u220fTx i=1 P (xi|x<i), where xi is the ith word in x, Tx denotes the number of words in x, and the index < i indicates {1, 2, \u00b7 \u00b7 \u00b7 , i \u2212 1}.", "startOffset": 89, "endOffset": 137}, {"referenceID": 13, "context": "Marginal Distributions P\u0302 (x) and P\u0302 (y) We use the LSTMbased language modeling approach (Sundermeyer et al., 2012; Mikolov et al., 2010) to characterize the marginal distribution of a sentence x, defined as \u220fTx i=1 P (xi|x<i), where xi is the ith word in x, Tx denotes the number of words in x, and the index < i indicates {1, 2, \u00b7 \u00b7 \u00b7 , i \u2212 1}.", "startOffset": 89, "endOffset": 137}, {"referenceID": 1, "context": "Model We apply the GRU as the recurrent module to implement the sequence-to-sequence model, which is the same as (Bahdanau et al., 2015; Jean et al., 2015).", "startOffset": 113, "endOffset": 155}, {"referenceID": 10, "context": "Model We apply the GRU as the recurrent module to implement the sequence-to-sequence model, which is the same as (Bahdanau et al., 2015; Jean et al., 2015).", "startOffset": 113, "endOffset": 155}, {"referenceID": 1, "context": "Following the common practice, we denote the baseline algorithm proposed in (Bahdanau et al., 2015; Jean et al., 2015) as RNNSearch.", "startOffset": 76, "endOffset": 118}, {"referenceID": 10, "context": "Following the common practice, we denote the baseline algorithm proposed in (Bahdanau et al., 2015; Jean et al., 2015) as RNNSearch.", "startOffset": 76, "endOffset": 118}, {"referenceID": 16, "context": "com/nyu-dl/dl4mt-tutorial Evaluation Metrics The translation qualities are measured by tokenized case-sensitive BLEU (Papineni et al., 2002) scores, which is implemented by (multi bleu, 2015).", "startOffset": 117, "endOffset": 140}, {"referenceID": 10, "context": ", the \u03b8xy and \u03b8yx) by using two warm-start models, which is generated by following the same process as (Jean et al., 2015).", "startOffset": 103, "endOffset": 122}, {"referenceID": 17, "context": "0 during the training for En\u2194Fr, En\u2194De, and En\u2194Zh, respectively (Pascanu et al., 2013).", "startOffset": 64, "endOffset": 86}, {"referenceID": 10, "context": "(Jean et al., 2015) proposed an effective post-process technique, which can achieve better translation performance by replacing the \u201cUNK\u201d with the corresponding word-level translations.", "startOffset": 0, "endOffset": 19}, {"referenceID": 10, "context": "[1] (Jean et al., 2015); [2] (Shen et al.", "startOffset": 4, "endOffset": 23}, {"referenceID": 19, "context": ", 2015); [2] (Shen et al., 2016)", "startOffset": 13, "endOffset": 32}, {"referenceID": 18, "context": "We use bits per dimension (briefly, bpd) (Salimans et al., 2017), to assess the performance of image generation.", "startOffset": 41, "endOffset": 64}, {"referenceID": 18, "context": "94, which is the same as reported in (Salimans et al., 2017).", "startOffset": 37, "endOffset": 60}, {"referenceID": 18, "context": ", 2016b) and (Salimans et al., 2017).", "startOffset": 13, "endOffset": 36}, {"referenceID": 21, "context": "As pointed out in (Theis et al., 2015), bpd is not the only evaluation rule of image generation.", "startOffset": 18, "endOffset": 38}, {"referenceID": 3, "context": "The test perplexities (Bengio et al., 2003) of the obtained language model is 58.", "startOffset": 22, "endOffset": 43}, {"referenceID": 25, "context": "Fourth, we will combine dual supervised learning with dual inference (Xia et al., 2017) so as to leverage structural duality to enhance both the training and inference procedures.", "startOffset": 69, "endOffset": 87}, {"referenceID": 14, "context": "Theorem 1 ((Mohri et al., 2012)).", "startOffset": 11, "endOffset": 31}, {"referenceID": 20, "context": "We use the LSTM language models (Sundermeyer et al., 2012; Mikolov et al., 2010) to characterize the marginal distribution of a sentence x, defined as \u220fTx i=1 P (xi|x<i), where xi is the i-th word in x, Tx denotes the number of words in x, and the index < i indicates {1, 2, \u00b7 \u00b7 \u00b7 , i \u2212 1}.", "startOffset": 32, "endOffset": 80}, {"referenceID": 13, "context": "We use the LSTM language models (Sundermeyer et al., 2012; Mikolov et al., 2010) to characterize the marginal distribution of a sentence x, defined as \u220fTx i=1 P (xi|x<i), where xi is the i-th word in x, Tx denotes the number of words in x, and the index < i indicates {1, 2, \u00b7 \u00b7 \u00b7 , i \u2212 1}.", "startOffset": 32, "endOffset": 80}], "year": 2017, "abstractText": "Many supervised learning tasks are emerged in dual forms, e.g., English-to-French translation vs. French-to-English translation, speech recognition vs. text to speech, and image classification vs. image generation. Two dual tasks have intrinsic connections with each other due to the probabilistic correlation between their models. This connection is, however, not effectively utilized today, since people usually train the models of two dual tasks separately and independently. In this work, we propose training the models of two dual tasks simultaneously, and explicitly exploiting the probabilistic correlation between them to regularize the training process. For ease of reference, we call the proposed approach dual supervised learning. We demonstrate that dual supervised learning can improve the practical performances of both tasks, for various applications including machine translation, image processing, and sentiment analysis.", "creator": "LaTeX with hyperref package"}}}