{"id": "1707.00896", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2017", "title": "Multilingual Hierarchical Attention Networks for Document Classification", "abstract": "Hierarchical attention networks have recently achieved remarkable performance for document classification in a given language. However, when multilingual document collections are considered, training such models separately for each language entails linear parameter growth and lack of cross-language transfer. Learning a single multilingual model with fewer parameters is therefore a challenging but potentially beneficial objective. To this end, we propose multilingual hierarchical attention networks for learning document structures, with shared encoders and/or attention mechanisms across languages, using multi-task learning and an aligned semantic space as input. We evaluate the proposed models on multilingual document classification with disjoint label sets, on a large dataset which we provide, with 600k news documents in 8 languages, and 5k labels. The multilingual models outperform strong monolingual ones in low-resource as well as full-resource settings, and use fewer parameters, thus confirming their computational efficiency and the utility of cross-language transfer.", "histories": [["v1", "Tue, 4 Jul 2017 10:28:04 GMT  (2326kb,D)", "http://arxiv.org/abs/1707.00896v1", null], ["v2", "Sun, 9 Jul 2017 10:37:52 GMT  (2334kb,D)", "http://arxiv.org/abs/1707.00896v2", null], ["v3", "Wed, 6 Sep 2017 15:06:16 GMT  (2337kb,D)", "http://arxiv.org/abs/1707.00896v3", null], ["v4", "Fri, 15 Sep 2017 10:47:26 GMT  (2337kb,D)", "http://arxiv.org/abs/1707.00896v4", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["nikolaos pappas", "andrei popescu-belis"], "accepted": true, "id": "1707.00896"}, "pdf": {"name": "1707.00896.pdf", "metadata": {"source": "CRF", "title": "Multilingual Hierarchical Attention Networks for Document Classification", "authors": ["Nikolaos Pappas", "Andrei Popescu-Belis"], "emails": ["nikolaos.pappas@idiap.ch", "andrei.popescu-belis@idiap.ch"], "sections": [{"heading": "1 Introduction", "text": "In recent years, it has been shown that the number of unemployed has increased many times over the previous months, and the number of unemployed has increased many times over. (...) In recent years, the number of unemployed has doubled over the previous months. (...) In the first six months of this year, the number of unemployed has doubled over the previous months. (...) In the first six months of this year, the number of unemployed has multiplied over the previous months. (...) In the first six months of this year, the number of unemployed has tripled over the previous months. (...) In the first six months of this year, the number of unemployed has multiplied over the previous months. (...) In the first six months of this year, the number of unemployed has tripled compared to the number of unemployed. (...) In the first six months of this year, the number of unemployed has tripled over the first six months of this year."}, {"heading": "2 Related Work", "text": "Research on learning multilingual word representation is based on early work on word embedding (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014), with the goal of learning a coordinated word embedding space for multiple languages using bilingual dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016), parallel sentences (Gouws et al., 2015), or comparable documents such as Wikipedia pages (Yih et al., 2011; Al-Rfou et al., 2013). Bilingual embedding has been learned using language models that use neural languages (Klementiev et al., 2012; Zou et al., 2013), including auto-coders (Chandar et al., 2014). Despite advances at the word level, the document level remains relatively less explored: the approaches of Hermann and Blunsom."}, {"heading": "3 Background: Hierarchical Attention Networks for Document Classification", "text": "We adopt the hierarchical attention networks proposed by Yang et al. (2016) for the presentation of documents, as shown in Figure 2. We consider a data set D = {(xi, yi), i = 1,.., N} from N documents xi with the caption yi-0, 1} k. Each document xi = {w11, w12,..., wKT} is represented by the sequence of the d-dimensional embedding of its words in sentences, where T is the maximum number of words in a sentence and K is the maximum number of sentences in a document. The network takes as input a document xi and issues a document vector ui. Specifically, it has two levels of abstraction, vs. sentence. The first consists of an encoder gw with the parameters Hw and an attention aw with the parameters Aw, while the second similarly includes an encoder and an attention (gs, Hs and as). The output ui is defined by the layer to yi."}, {"heading": "3.1 Encoder Layers", "text": "At the word level, the function gw encodes the order of the input words {wit | t = 1,.., T} for each sentence i of the document, labeled as: h (it) w = gw (wit), t [1, T], (1), and at the sentence level, the function gs encodes the order of the sentence vectors {si | i = 1,..., S}, labeled as h (i) s. The gw and gs functions can be any forward or recurring network with the parameters Hw or Hs. We consider the following networks: a fully connected network, labeled as density, a gated recurrent unit network (Cho et al., 2014), labeled as GRU ~ hidirectional GRU, as GRbidirectional GRU, as GRbified U, as a hidden U version (MW = 1)."}, {"heading": "3.2 Attention Layers", "text": "A typical method of getting a representation for a given word sequence at each level is to take the last hidden state vector output by the encoder. However, it is difficult to encode all the relevant input information required in a fixed-length vector. This problem is solved by introducing an attention mechanism at each level (noted \u03b1w and \u03b1s) that estimates the meaning of each hidden state vector for the representation of the sentence or meaning of the document. Sentence vector si-Rdw, where dw is the dimension of the word encoder, is obtained as follows: 1T T-p = 1 \u03b1 (it) w h (it) w = 1 T-p = 1 exp (v > ituw) \u2211 j exp (v > ijuw) h (it) w (it) w (it) w (it) w) w (it) w (it) w (it) w) is a fully connected neural network with Wamew parameters."}, {"heading": "3.3 Classification Layers", "text": "The output of such a network is typically fed to a Softmax layer for classification, with a loss based on the cross entropy between gold and predicted labels (Tang et al., 2015) or a loss based on the negative protocol probability of correct labels (Yang et al., 2016). However, Softmax emphasizes the probability of the most likely label, the Mayory, LSTM (Hochreiter and Schmidhuber, 1997), and is able to capture temporal information forwards or backwards (Yang et al., 2016). It is not ideal for multi-label classification. Instead, it is more appropriate to have independent predictions for each label. Therefore, we replace the Softmax with a sigmoid function, and for each document represented by the ui vector, the probability of k labels is modeled as follows: y = i = p (y | ui) = 11 + e (Wcubc + labels), which are one."}, {"heading": "4 Multilingual Hierarchical Attention Networks: MHANs", "text": "When multilingual data is available, the above-mentioned network can be trained separately for each language, but the required parameters grow linearly with the number of languages. Furthermore, they do not exploit common knowledge between languages or transfer it from one to the other. We propose here a hierarchical attention network with common components between languages, hence with sublinear parameter growth that allows knowledge transfer between languages. We now consider M languages as L = {Ll | l = 1,..., M} and a multilingual set of thematic documents D = {(x (l) i, y (l) i) | i = 1,..., Nl} as defined above."}, {"heading": "4.1 Sharing Components across Languages", "text": "To enable multilingual learning, we propose three different ways of dividing components between networks in a multifunctional learning environment shown in Figure 3, namely: (a) dividing the parameters of word and sentence encoders, hence \u03b8enc = {Hw, W (l) w, Hs, W (l) s, W (l) s, W (l) c}; (b) dividing the parameters of word and sentence encoding models, hence \u03b8att = {H (l) w, Ww, H (l) s, W (l) c}; and (c) dividing the two previous parameter sets, hence successatt = {Hw, Ww, Hs, Ws, W (l) c}. If successmono = {H (l) w, W (l) w, H (l) s, W (l) s, W (l) s) s, emigs, W (l) s, c} are the parameters of multilingual monolingual dense models with gamers (r), then we have:"}, {"heading": "4.2 Training over Disjoint Label Sets", "text": "For the training, we replace the monolingual training objective (equation 6) with a common multilingual objective that allows the sharing of components, i.e. a subset of parameters for each language \u03b81, \u03b82,..., \u03b8M, across different language networks: L (\u03b81,..., \u03b8M) = \u2212 1Z M \u2211 l \u03b3l Ne \u2211 i H (y (l) i, y (l) i) (8), where Z = M \u00d7 Ne is the size of the epoch and the meaning to be given for the goal of each language. The common objective L can be minimized in relation to the parameters \u03b81, \u03b82,..., using SGD as before. However, if we continue to train on examples from different languages, it is difficult to learn a common space that works well across languages, because each language update applies only to a subset of parameters and the model may differ from other languages."}, {"heading": "5 A New Corpus for Multilingual Document Classification: DW", "text": "Multilingual document classification records are usually limited in size, have multilingual target categories, and assign documents to only one category. However, classification is often necessary in cases where the categories are not strictly aligned and can be multiple per document. This is the case, for example, with online news agencies that track multilingual messages written and commented on by different journalists, a process that is costly and time-consuming. In previous studies, two sets of data were used for multilingual document classification: Reuters RCV1 / RCV2 (6,000 documents, 2 languages and 4 labels) and TED Talk transcripts (12,078 documents, 12 languages and 15 labels) introduced by Hermann and Blunsom (2014). The former is tailored to evaluate word embedding that is aligned across languages rather than complex multilingual document models. The latter are twice as large and cover more languages than records in a multilingual environment."}, {"heading": "6 Evaluation", "text": "We evaluate our multilingual models for resource-rich and resource-poor scenarios of multilingual document classification via disjoint label sets on the Deutsche Welle corpus."}, {"heading": "6.1 Settings", "text": "The corpus is divided per language into 80% for training, 10% for validation and 10% for testing. We evaluate both types of labels (Yg, Ys) on a full-resource scenario and only the general topics (Yg) on a low-resource scenario. We report on the micro-averaged F1 values for each test set, as in previous work (Hermann and Blunsom, 2014).Model configuration. For all models, we use the pre-formed 40-dimensional multilingual embeddings trained on the Leipzig corpus. (2016) We have zero pad documents up to a maximum of 30 words per set and 30 sentences per document. Hyper parameters have been selected on the validation sets. We have the following settings: 100-dimensional encoders and attention embeddings (at each level), relative activation layers for all interlayers, batch-size hierarchy, hierarchy-size."}, {"heading": "6.2 Results", "text": "In this year it has come to the point where we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we in which we are in which we in which we are in which we in which we in which we are in which we in which we in which we in which we are in which we in which we in which we are in which we in which we in which we in which we in which we are in which we in which we in which we in which we are in which we in which we in which we in which we in which we in which we in which we are in which we in which we in which we in which we in which we are in which we in which we in which we in which we in which we in which we in which we in which we in which we are in which we in which we in which we in which we in which we in which we in which we are in which we in which we in which we in which we in which we in which we in which we in which we in which we are in which we in which we in which we in which we are in which we in which we in which we in which we in which we in which we in which we are in which we in which we in which we in which we in which we in which we in which we are in which we in which we in which we in which we in which we in which we are in which we in which we in which we in which we in which we are in which we in which we in which we in which we in which we in which we in which we in which we in which"}, {"heading": "6.3 Qualitative Analysis", "text": "We analyze the performance of the model across the full range of labels to determine what type of labels it performs better than the monolingual model, and provide some qualitative examples. Figure 5 shows the cumulative true positive (TP) difference between the monolingual and multilingual models on the Arabic, German, Portuguese and Russian test kits, sorted by frequency of labels. We can observe that the cumulative TP difference of the multilingual model steadily increases as the multilingual model performs better than the monolingual one: Russia (21), Berlin (19), Iraq (14), Elections (13) and NATO (13), while for the opposite direction, the top 5 labels where the multilingual model performs better than the monolingual one: Russia (13), Iraq (13) and NATO (13) were probably multilingual (97), (73), football (25) and football (47)."}, {"heading": "7 Conclusion", "text": "We proposed multilingual hierarchical attention networks for classifying documents and showed that they can benefit from both full-resource scenarios and low-resource scenarios by using fewer parameters than monolingual networks. In the first scenario, it was most advantageous to share only the attention mechanisms, while in the second scenario, the coders are used in conjunction with the attention mechanisms. These results confirm the benefits of language transfer, which is also an important component of human language learning (Odlin, 1989; Ringbom, 2007). Furthermore, our study extends the applicability of multilingual classification of documents, as our framework is not limited to common labels. There are several future guidelines for this study. In its current form, our models cannot be generalized to languages without precedent, as Firat et al. (2016b) has attempted for neural MT. This could be achieved by an independent classification layer in label size, such as the Shoo-iao et."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning. Sofia, Bulgaria.", "citeRegEx": "Al.Rfou et al\\.,? 2013", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Massively multilingual word embeddings", "author": ["Waleed Ammar", "George Mulcaire", "Yulia Tsvetkov", "Guillaume Lample", "Chris Dyer", "Noah A. Smith."], "venue": "CoRR abs/1602.01925.", "citeRegEx": "Ammar et al\\.,? 2016", "shortCiteRegEx": "Ammar et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 5th International Conference on Learning Representations. San Diego, CA, USA.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "On-line learning and stochastic approximations", "author": ["L\u00e9on Bottou."], "venue": "David Saad, editor, On-line Learning in Neural Networks, Cambridge University Press, pages 9\u201342.", "citeRegEx": "Bottou.,? 1998", "shortCiteRegEx": "Bottou.", "year": 1998}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Sarath Chandar", "Stanislas Lauly", "Hugo Larochelle", "Mitesh Khapra", "Balaraman Ravindran", "Vikas C. Raykar", "Amrita Saha."], "venue": "Advances in Neural Information Processing Sys-", "citeRegEx": "Chandar et al\\.,? 2014", "shortCiteRegEx": "Chandar et al\\.", "year": 2014}, {"title": "End-to-end learning of LDA by mirrordescent back propagation over a deep architecture", "author": ["Jianshu Chen", "Ji He", "Yelong Shen", "Lin Xiao", "Xiaodong He", "Jianfeng Gao", "Xinying Song", "Li Deng."], "venue": "Advances in Neural Information Processing Sys-", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning where to attend with deep architectures for image tracking", "author": ["Misha Denil", "Loris Bazzani", "Hugo Larochelle", "Nando de Freitas."], "venue": "Neural Computation 24(8):2151\u20132184.", "citeRegEx": "Denil et al\\.,? 2012", "shortCiteRegEx": "Denil et al\\.", "year": 2012}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Manaal Faruqui", "Chris Dyer."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. Gothenburg, Sweden, pages", "citeRegEx": "Faruqui and Dyer.,? 2014", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2014}, {"title": "Jointly learning to embed and predict with multiple languages", "author": ["Daniel C. Ferreira", "Andr\u00e9 F.T. Martins", "Mariana S.C. Almeida."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-", "citeRegEx": "Ferreira et al\\.,? 2016", "shortCiteRegEx": "Ferreira et al\\.", "year": 2016}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Firat et al\\.,? 2016a", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Zero-resource translation with multi-lingual neural machine translation", "author": ["Orhan Firat", "Baskaran Sankaran", "Yaser Al-Onaizan", "Fatos T. Yarman Vural", "Kyunghyun Cho."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Firat et al\\.,? 2016b", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "BilBOWA: Fast bilingual distributed representations without word alignments", "author": ["Stephan Gouws", "Yoshua Bengio", "Gregory S. Corrado."], "venue": "Proceedings of the 32nd International Conference on Machine Learning. Lille, France, pages 748\u2013756.", "citeRegEx": "Gouws et al\\.,? 2015", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Multilingual models for compositional distributed semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Baltimore, Maryland, pages 58\u201368.", "citeRegEx": "Hermann and Blunsom.,? 2014", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Proceedings of the 28th International Conference on Neural In-", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, MIT Press, volume 9 (8), pages 1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Neural discourse structure for text categorization", "author": ["Yangfeng Ji", "Noah Smith."], "venue": "CoRR abs/1702.01829. http://arxiv.org/abs/1702.01829.", "citeRegEx": "Ji and Smith.,? 2017", "shortCiteRegEx": "Ji and Smith.", "year": 2017}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["Rie Johnson", "Tong Zhang."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human", "citeRegEx": "Johnson and Zhang.,? 2015", "shortCiteRegEx": "Johnson and Zhang.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Doha, Qatar, pages 1746\u20131751.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Lei Ba."], "venue": "Proceedings of the International Conference on Learning Representations. Banff, Canada.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Alexandre Klementiev", "Ivan Titov", "Binod Bhattarai."], "venue": "Proceedings of the International Conference on Computational Linguistics. Bombay, India.", "citeRegEx": "Klementiev et al\\.,? 2012", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher."], "venue": "Proceedings of the 33rd In-", "citeRegEx": "Kumar et al\\.,? 2015", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["Siwei Lai", "Liheng Xu", "Kang Liu", "Jun Zhao."], "venue": "Proceedings of the 29th AAAI Conference on Artificial Intelligence. Austin, Texas, pages 2267\u20132273.", "citeRegEx": "Lai et al\\.,? 2015", "shortCiteRegEx": "Lai et al\\.", "year": 2015}, {"title": "Learning to combine foveal glimpses with a third-order Boltzmann machine", "author": ["Hugo Larochelle", "Geoffrey Hinton."], "venue": "Proceedings of the 23rd International Conference on Neural Information Processing Systems. Vancouver, Canada, pages 1243\u20131251.", "citeRegEx": "Larochelle and Hinton.,? 2010", "shortCiteRegEx": "Larochelle and Hinton.", "year": 2010}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V. Le", "Tomas Mikolov."], "venue": "Proceedings of the 31st International Conference on Machine Learning. Beijing, China, pages 1188\u2013 1196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["Rui Lin", "Shujie Liu", "Muyun Yang", "Mu Li", "Ming Zhou", "Sheng Li."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal, pages", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal, pages", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of the International Conference on Learning Representations. Scottsdale, AZ, USA.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "All-in text: Learning document, label, and word representations jointly", "author": ["Jinseok Nam", "Eneldo Loza Menc\u0131\u0301a", "Johannes F\u00fcrnkranz"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence", "citeRegEx": "Nam et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nam et al\\.", "year": 2016}, {"title": "Language transfer: Crosslinguistic influence in language learning", "author": ["Terence Odlin."], "venue": "Cambridge Applied Linguistics, Cambridge University Press.", "citeRegEx": "Odlin.,? 1989", "shortCiteRegEx": "Odlin.", "year": 1989}, {"title": "Explaining the stars: Weighted multiple-instance learning for aspect-based sentiment analysis", "author": ["Nikolaos Pappas", "Andrei Popescu-Belis."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Doha, Qatar, pages", "citeRegEx": "Pappas and Popescu.Belis.,? 2014", "shortCiteRegEx": "Pappas and Popescu.Belis.", "year": 2014}, {"title": "Explicit document modeling through weighted multiple-instance learning", "author": ["Nikolaos Pappas", "Andrei Popescu-Belis."], "venue": "Journal of Artificial Intelligence Research pages 591\u2013626. https://doi.org/doi:10.1613/jair.5240.", "citeRegEx": "Pappas and Popescu.Belis.,? 2017", "shortCiteRegEx": "Pappas and Popescu.Belis.", "year": 2017}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Doha, Qatar, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Less is more: Zero-shot learning from online textual documents with noise suppression", "author": ["Ruizhi Qiao", "Lingqiao Liu", "Chunhua Shen", "Anton van den Hengel."], "venue": "Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recogni-", "citeRegEx": "Qiao et al\\.,? 2016", "shortCiteRegEx": "Qiao et al\\.", "year": 2016}, {"title": "Cross-linguistic Similarity in Foreign Language Learning", "author": ["Hakan Ringbom."], "venue": "Second language acquisition. Multilingual Matters.", "citeRegEx": "Ringbom.,? 2007", "shortCiteRegEx": "Ringbom.", "year": 2007}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal, pages 379\u2013389.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu."], "venue": "Empirical Methods on Natural Language Processing. Lisbon, Spain, pages 1422\u20131432.", "citeRegEx": "Tang et al\\.,? 2015", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Uppsala, Sweden, pages 384\u2013", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Learning a parametric embedding by preserving local structure", "author": ["Laurens van der Maaten."], "venue": "Proceedings of the 12th International Conference on Artificial Intelligence and Statistics. Clearwater Beach, FL, USA, pages 384\u2013391.", "citeRegEx": "Maaten.,? 2009", "shortCiteRegEx": "Maaten.", "year": 2009}, {"title": "Hierarchical attention networks for document classification", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Multi-level structured models for documentlevel sentiment classification", "author": ["Ainur Yessenalina", "Yisong Yue", "Claire Cardie."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Compu-", "citeRegEx": "Yessenalina et al\\.,? 2010", "shortCiteRegEx": "Yessenalina et al\\.", "year": 2010}, {"title": "Learning discriminative projections for text similarity measures", "author": ["Wen-tau Yih", "Kristina Toutanova", "John C. Platt", "Christopher Meek."], "venue": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning. Portland, OR, USA,", "citeRegEx": "Yih et al\\.,? 2011", "shortCiteRegEx": "Yih et al\\.", "year": 2011}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "Advances in Neural Information Processing Systems 28. Montreal, Canada, pages 649\u2013 657.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Seattle, Washing-", "citeRegEx": "Zou et al\\.,? 2013", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 36, "context": "Learning word sequence representations has become increasingly useful for a variety of NLP tasks such as document classification (Tang et al., 2015; Yang et al., 2016), neural machine translation (NMT) (Cho et al.", "startOffset": 129, "endOffset": 167}, {"referenceID": 39, "context": "Learning word sequence representations has become increasingly useful for a variety of NLP tasks such as document classification (Tang et al., 2015; Yang et al., 2016), neural machine translation (NMT) (Cho et al.", "startOffset": 129, "endOffset": 167}, {"referenceID": 6, "context": ", 2016), neural machine translation (NMT) (Cho et al., 2014; Luong et al., 2015), question answering (Chen et al.", "startOffset": 42, "endOffset": 80}, {"referenceID": 26, "context": ", 2016), neural machine translation (NMT) (Cho et al., 2014; Luong et al., 2015), question answering (Chen et al.", "startOffset": 42, "endOffset": 80}, {"referenceID": 5, "context": ", 2015), question answering (Chen et al., 2015; Kumar et al., 2015) and summarization (Rush et al.", "startOffset": 28, "endOffset": 67}, {"referenceID": 21, "context": ", 2015), question answering (Chen et al., 2015; Kumar et al., 2015) and summarization (Rush et al.", "startOffset": 28, "endOffset": 67}, {"referenceID": 35, "context": ", 2015) and summarization (Rush et al., 2015).", "startOffset": 26, "endOffset": 45}, {"referenceID": 10, "context": "NMT (Firat et al., 2016a).", "startOffset": 4, "endOffset": 25}, {"referenceID": 34, "context": "Secondly, the models should be capable of cross-language transfer, which is an important component in human language learning (Ringbom, 2007).", "startOffset": 126, "endOffset": 141}, {"referenceID": 20, "context": "Previous studies in document classification attempted to overcome these issues by employing multilingual word embeddings which allow direct comparisons and groupings across languages (Klementiev et al., 2012; Hermann and Blunsom, 2014; Ferreira et al., 2016).", "startOffset": 183, "endOffset": 258}, {"referenceID": 13, "context": "Previous studies in document classification attempted to overcome these issues by employing multilingual word embeddings which allow direct comparisons and groupings across languages (Klementiev et al., 2012; Hermann and Blunsom, 2014; Ferreira et al., 2016).", "startOffset": 183, "endOffset": 258}, {"referenceID": 9, "context": "Previous studies in document classification attempted to overcome these issues by employing multilingual word embeddings which allow direct comparisons and groupings across languages (Klementiev et al., 2012; Hermann and Blunsom, 2014; Ferreira et al., 2016).", "startOffset": 183, "endOffset": 258}, {"referenceID": 36, "context": "Moreover, despite recent advances in monolingual document modeling (Tang et al., 2015; Yang et al., 2016), multilingual models are still based on shallow networks.", "startOffset": 67, "endOffset": 105}, {"referenceID": 39, "context": "Moreover, despite recent advances in monolingual document modeling (Tang et al., 2015; Yang et al., 2016), multilingual models are still based on shallow networks.", "startOffset": 67, "endOffset": 105}, {"referenceID": 39, "context": "In this paper, we propose Multilingual Hierarchical Attention Networks to learn shared document structures across languages for document classification with disjoint label sets, as opposed to training hierarchical attention networks (HANs) in a language-specific manner (Yang et al., 2016).", "startOffset": 270, "endOffset": 289}, {"referenceID": 8, "context": "The goal is to learn an aligned word embedding space for multiple languages by leveraging bilingual dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016), parallel sentences (Gouws et al.", "startOffset": 113, "endOffset": 157}, {"referenceID": 1, "context": "The goal is to learn an aligned word embedding space for multiple languages by leveraging bilingual dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016), parallel sentences (Gouws et al.", "startOffset": 113, "endOffset": 157}, {"referenceID": 12, "context": ", 2016), parallel sentences (Gouws et al., 2015) or comparable documents such as Wikipedia pages (Yih et al.", "startOffset": 28, "endOffset": 48}, {"referenceID": 41, "context": ", 2015) or comparable documents such as Wikipedia pages (Yih et al., 2011; Al-Rfou et al., 2013).", "startOffset": 56, "endOffset": 96}, {"referenceID": 0, "context": ", 2015) or comparable documents such as Wikipedia pages (Yih et al., 2011; Al-Rfou et al., 2013).", "startOffset": 56, "endOffset": 96}, {"referenceID": 20, "context": "Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al.", "startOffset": 84, "endOffset": 127}, {"referenceID": 43, "context": "Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al.", "startOffset": 84, "endOffset": 127}, {"referenceID": 4, "context": ", 2013), including auto-encoders (Chandar et al., 2014).", "startOffset": 33, "endOffset": 55}, {"referenceID": 24, "context": "Early work on neural document classification was based on shallow feed-forward networks, which required unsupervised pre-training (Le and Mikolov, 2014).", "startOffset": 130, "endOffset": 152}, {"referenceID": 0, "context": ", 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored: the approaches proposed by Hermann and Blunsom (2014) or Ferreira et al.", "startOffset": 8, "endOffset": 354}, {"referenceID": 0, "context": ", 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored: the approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages.", "startOffset": 8, "endOffset": 380}, {"referenceID": 0, "context": ", 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored: the approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages. These sets are costly to produce, and are often not available in real world settings; here, we remove this constraint. Moreover, we develop new, deeper multilingual document models with hierarchical structure based on prior art at the word level. Early work on neural document classification was based on shallow feed-forward networks, which required unsupervised pre-training (Le and Mikolov, 2014). Later studies focused on neural networks with hierarchical structure. Kim (2014) proposed a convolutional neural network (CNN) for sentence classification.", "startOffset": 8, "endOffset": 981}, {"referenceID": 0, "context": ", 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored: the approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages. These sets are costly to produce, and are often not available in real world settings; here, we remove this constraint. Moreover, we develop new, deeper multilingual document models with hierarchical structure based on prior art at the word level. Early work on neural document classification was based on shallow feed-forward networks, which required unsupervised pre-training (Le and Mikolov, 2014). Later studies focused on neural networks with hierarchical structure. Kim (2014) proposed a convolutional neural network (CNN) for sentence classification. Johnson and Zhang (2015) proposed a CNN for high-dimensional data classification, while Zhang et al.", "startOffset": 8, "endOffset": 1081}, {"referenceID": 0, "context": ", 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored: the approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages. These sets are costly to produce, and are often not available in real world settings; here, we remove this constraint. Moreover, we develop new, deeper multilingual document models with hierarchical structure based on prior art at the word level. Early work on neural document classification was based on shallow feed-forward networks, which required unsupervised pre-training (Le and Mikolov, 2014). Later studies focused on neural networks with hierarchical structure. Kim (2014) proposed a convolutional neural network (CNN) for sentence classification. Johnson and Zhang (2015) proposed a CNN for high-dimensional data classification, while Zhang et al. (2015) adopted a character-level CNN for text classification.", "startOffset": 8, "endOffset": 1164}, {"referenceID": 0, "context": ", 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored: the approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages. These sets are costly to produce, and are often not available in real world settings; here, we remove this constraint. Moreover, we develop new, deeper multilingual document models with hierarchical structure based on prior art at the word level. Early work on neural document classification was based on shallow feed-forward networks, which required unsupervised pre-training (Le and Mikolov, 2014). Later studies focused on neural networks with hierarchical structure. Kim (2014) proposed a convolutional neural network (CNN) for sentence classification. Johnson and Zhang (2015) proposed a CNN for high-dimensional data classification, while Zhang et al. (2015) adopted a character-level CNN for text classification. Lai et al. (2015) proposed a recurrent CNN to capture sequential information, which outperformed simpler CNNs.", "startOffset": 8, "endOffset": 1237}, {"referenceID": 0, "context": ", 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored: the approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages. These sets are costly to produce, and are often not available in real world settings; here, we remove this constraint. Moreover, we develop new, deeper multilingual document models with hierarchical structure based on prior art at the word level. Early work on neural document classification was based on shallow feed-forward networks, which required unsupervised pre-training (Le and Mikolov, 2014). Later studies focused on neural networks with hierarchical structure. Kim (2014) proposed a convolutional neural network (CNN) for sentence classification. Johnson and Zhang (2015) proposed a CNN for high-dimensional data classification, while Zhang et al. (2015) adopted a character-level CNN for text classification. Lai et al. (2015) proposed a recurrent CNN to capture sequential information, which outperformed simpler CNNs. Lin et al. (2015) and Tang et al.", "startOffset": 8, "endOffset": 1348}, {"referenceID": 0, "context": ", 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored: the approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages. These sets are costly to produce, and are often not available in real world settings; here, we remove this constraint. Moreover, we develop new, deeper multilingual document models with hierarchical structure based on prior art at the word level. Early work on neural document classification was based on shallow feed-forward networks, which required unsupervised pre-training (Le and Mikolov, 2014). Later studies focused on neural networks with hierarchical structure. Kim (2014) proposed a convolutional neural network (CNN) for sentence classification. Johnson and Zhang (2015) proposed a CNN for high-dimensional data classification, while Zhang et al. (2015) adopted a character-level CNN for text classification. Lai et al. (2015) proposed a recurrent CNN to capture sequential information, which outperformed simpler CNNs. Lin et al. (2015) and Tang et al. (2015) proposed hierarchical recurrent NNs and showed that they were superior to CNN-based models.", "startOffset": 8, "endOffset": 1371}, {"referenceID": 0, "context": ", 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored: the approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages. These sets are costly to produce, and are often not available in real world settings; here, we remove this constraint. Moreover, we develop new, deeper multilingual document models with hierarchical structure based on prior art at the word level. Early work on neural document classification was based on shallow feed-forward networks, which required unsupervised pre-training (Le and Mikolov, 2014). Later studies focused on neural networks with hierarchical structure. Kim (2014) proposed a convolutional neural network (CNN) for sentence classification. Johnson and Zhang (2015) proposed a CNN for high-dimensional data classification, while Zhang et al. (2015) adopted a character-level CNN for text classification. Lai et al. (2015) proposed a recurrent CNN to capture sequential information, which outperformed simpler CNNs. Lin et al. (2015) and Tang et al. (2015) proposed hierarchical recurrent NNs and showed that they were superior to CNN-based models. Recently, Yang et al. (2016) demonstrated that a hierarchical attention network with bi-directional gated encoders outperforms traditional and neu-", "startOffset": 8, "endOffset": 1492}, {"referenceID": 23, "context": "Early examples of attention mechanisms within neural models appeared in computer vision (Larochelle and Hinton, 2010; Denil et al., 2012).", "startOffset": 88, "endOffset": 137}, {"referenceID": 7, "context": "Early examples of attention mechanisms within neural models appeared in computer vision (Larochelle and Hinton, 2010; Denil et al., 2012).", "startOffset": 88, "endOffset": 137}, {"referenceID": 31, "context": "(2016) and more recently (Pappas and Popescu-Belis, 2017; Ji and Smith, 2017).", "startOffset": 25, "endOffset": 77}, {"referenceID": 16, "context": "(2016) and more recently (Pappas and Popescu-Belis, 2017; Ji and Smith, 2017).", "startOffset": 25, "endOffset": 77}, {"referenceID": 29, "context": "learn the importance or saliency of sentences with respect to the classification objective, included Yessenalina et al. (2010); Pappas and PopescuBelis (2014); Yang et al.", "startOffset": 101, "endOffset": 127}, {"referenceID": 29, "context": "learn the importance or saliency of sentences with respect to the classification objective, included Yessenalina et al. (2010); Pappas and PopescuBelis (2014); Yang et al.", "startOffset": 101, "endOffset": 159}, {"referenceID": 29, "context": "(2010); Pappas and PopescuBelis (2014); Yang et al. (2016) and more recently (Pappas and Popescu-Belis, 2017; Ji and Smith, 2017).", "startOffset": 40, "endOffset": 59}, {"referenceID": 2, "context": "In NMT, Bahdanau et al. (2015) proposed an attention-based encoder-decoder network, while Luong et al.", "startOffset": 8, "endOffset": 31}, {"referenceID": 2, "context": "In NMT, Bahdanau et al. (2015) proposed an attention-based encoder-decoder network, while Luong et al. (2015) proposed a local and ensemble attention model for NMT.", "startOffset": 8, "endOffset": 110}, {"referenceID": 2, "context": "In NMT, Bahdanau et al. (2015) proposed an attention-based encoder-decoder network, while Luong et al. (2015) proposed a local and ensemble attention model for NMT. Firat et al. (2016a) proposed a single encoder-decoder model with shared attention across language pairs for multi-way, multilingual NMT.", "startOffset": 8, "endOffset": 186}, {"referenceID": 2, "context": "In NMT, Bahdanau et al. (2015) proposed an attention-based encoder-decoder network, while Luong et al. (2015) proposed a local and ensemble attention model for NMT. Firat et al. (2016a) proposed a single encoder-decoder model with shared attention across language pairs for multi-way, multilingual NMT. Hermann et al. (2015) developed attention-based document readers for question answering.", "startOffset": 8, "endOffset": 325}, {"referenceID": 2, "context": "In NMT, Bahdanau et al. (2015) proposed an attention-based encoder-decoder network, while Luong et al. (2015) proposed a local and ensemble attention model for NMT. Firat et al. (2016a) proposed a single encoder-decoder model with shared attention across language pairs for multi-way, multilingual NMT. Hermann et al. (2015) developed attention-based document readers for question answering. Chen et al. (2015) proposed a recurrent attention model over an external memory.", "startOffset": 8, "endOffset": 411}, {"referenceID": 2, "context": "In NMT, Bahdanau et al. (2015) proposed an attention-based encoder-decoder network, while Luong et al. (2015) proposed a local and ensemble attention model for NMT. Firat et al. (2016a) proposed a single encoder-decoder model with shared attention across language pairs for multi-way, multilingual NMT. Hermann et al. (2015) developed attention-based document readers for question answering. Chen et al. (2015) proposed a recurrent attention model over an external memory. Similarly, Kumar et al. (2015) introduced a dynamic memory network for question answering and other tasks.", "startOffset": 8, "endOffset": 504}, {"referenceID": 39, "context": "We adopt the hierarchical attention networks for document representation proposed by Yang et al. (2016), as displayed in Figure 2.", "startOffset": 85, "endOffset": 104}, {"referenceID": 6, "context": "We consider the following networks: a fully-connected one, noted as Dense, a Gated Recurrent Unit network (Cho et al., 2014) noted as GRU, and a bi-directional GRU, noted as biGRU.", "startOffset": 106, "endOffset": 124}, {"referenceID": 36, "context": "The output of such a network is typically fed to a softmax layer for classification, with a loss based on the cross-entropy between gold and predicted labels (Tang et al., 2015) or a loss based on the negative log-likelihood of the correct labels (Yang et al.", "startOffset": 158, "endOffset": 177}, {"referenceID": 39, "context": ", 2015) or a loss based on the negative log-likelihood of the correct labels (Yang et al., 2016).", "startOffset": 77, "endOffset": 96}, {"referenceID": 15, "context": "ory, LSTM (Hochreiter and Schmidhuber, 1997) and is able to capture temporal information forward or backward in time.", "startOffset": 10, "endOffset": 44}, {"referenceID": 3, "context": "The above objective is differentiable and can be minimized with stochastic gradient descent (Bottou, 1998) or vari-", "startOffset": 92, "endOffset": 106}, {"referenceID": 19, "context": "ants such as Adam (Kingma and Ba, 2014), to maximize classification performance.", "startOffset": 18, "endOffset": 39}, {"referenceID": 1, "context": "e with multilingual word embeddings that have aligned meanings across languages (Ammar et al., 2016).", "startOffset": 80, "endOffset": 100}, {"referenceID": 10, "context": "To address this, we employ a training strategy similar to Firat et al. (2016a), who sampled parallel sentences for multi-way machine translation from different language pairs in a cyclic fashion.", "startOffset": 58, "endOffset": 79}, {"referenceID": 13, "context": "4 labels) and TED talk transcripts (12,078 documents, 12 languages and 15 labels) introduced by Hermann and Blunsom (2014). The former is tailored for evaluating word embeddings aligned across languages, rather than complex multilingual document models.", "startOffset": 96, "endOffset": 123}, {"referenceID": 39, "context": "6 times longer than in Yang et al.\u2019s (2016) monolingual dataset (436 vs.", "startOffset": 23, "endOffset": 44}, {"referenceID": 13, "context": "We report the microaveraged F1 scores for each test set, as in previous work (Hermann and Blunsom, 2014).", "startOffset": 77, "endOffset": 104}, {"referenceID": 1, "context": "For all models, we use the pre-trained 40-dimensional multilingual embeddings trained on the Leipzig corpus from Ammar et al. (2016). We zero-pad documents up to a maximum of 30 words per sentence and 30 sentences per document.", "startOffset": 113, "endOffset": 133}, {"referenceID": 20, "context": "\u2022 NN : A neural network which feeds the average vector of the input words directly to a classification layer, as the common baseline for multilingual document classification by Klementiev et al. (2012).", "startOffset": 177, "endOffset": 202}, {"referenceID": 39, "context": "This model is the one proposed by Yang et al. (2016) adapted to our task.", "startOffset": 34, "endOffset": 53}, {"referenceID": 29, "context": "These results confirm the merits of language transfer, which is also an important component of human language learning (Odlin, 1989; Ringbom, 2007).", "startOffset": 119, "endOffset": 147}, {"referenceID": 34, "context": "These results confirm the merits of language transfer, which is also an important component of human language learning (Odlin, 1989; Ringbom, 2007).", "startOffset": 119, "endOffset": 147}, {"referenceID": 33, "context": "This could be achieved by a label-size independent classification layer as in zero-shot classification (Qiao et al., 2016; Nam et al., 2016).", "startOffset": 103, "endOffset": 140}, {"referenceID": 28, "context": "This could be achieved by a label-size independent classification layer as in zero-shot classification (Qiao et al., 2016; Nam et al., 2016).", "startOffset": 103, "endOffset": 140}, {"referenceID": 10, "context": "In their current form, our models cannot generalize to languages without any example, as attempted by Firat et al. (2016b) for neural MT.", "startOffset": 102, "endOffset": 123}], "year": 2017, "abstractText": "Hierarchical attention networks have recently achieved remarkable performance for document classification in a given language. However, when multilingual document collections are considered, training such models separately for each language entails linear parameter growth and lack of cross-language transfer. Learning a single multilingual model with fewer parameters is therefore a challenging but potentially beneficial objective. To this end, we propose multilingual hierarchical attention networks for learning document structures, with shared encoders and/or attention mechanisms across languages, using multi-task learning and an aligned semantic space as input. We evaluate the proposed models on multilingual document classification with disjoint label sets, on a large dataset which we provide, with 600k news documents in 8 languages, and 5k labels. The multilingual models outperform strong monolingual ones in lowresource as well as full-resource settings, and use fewer parameters, thus confirming their computational efficiency and the utility of cross-language transfer.", "creator": "LaTeX with hyperref package"}}}