{"id": "1707.00995", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2017", "title": "An empirical study on the effectiveness of images in Multimodal Neural Machine Translation", "abstract": "In state-of-the-art Neural Machine Translation (NMT), an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions that they describe. In this paper, we compare several attention mechanism on the multimodal translation task (English, image to German) and evaluate the ability of the model to make use of images to improve translation. We surpass state-of-the-art scores on the Multi30k data set, we nevertheless identify and report different misbehavior of the machine while translating.", "histories": [["v1", "Tue, 4 Jul 2017 13:57:04 GMT  (1158kb,D)", "http://arxiv.org/abs/1707.00995v1", "Accepted to EMNLP 2017"]], "COMMENTS": "Accepted to EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jean-benoit delbrouck", "st\\'ephane dupont"], "accepted": true, "id": "1707.00995"}, "pdf": {"name": "1707.00995.pdf", "metadata": {"source": "CRF", "title": "An empirical study on the effectiveness of images in Multimodal Neural Machine Translation", "authors": ["Jean-Benoit Delbrouck"], "emails": ["stephane.dupont}@umons.ac.be"], "sections": [{"heading": "1 Introduction", "text": "In machine translation, neural networks have attracted a lot of research attention. Recently, the attention-based encoder decoder frameworks (Sutskever et al., 2014; Bahdanau et al., 2014) have been widely adopted. In this approach, Recurrent Neural Networks (RNNs) show source sequences of words on target sequences. The attention mechanism is learned to focus on different parts of the input sentence while it is decoded. Attention mechanisms have also proven successful with other modalities such as images, where they are able to visit the important parts of an image, for example when generating text headlines (Xu et al., 2015). For such applications, Convolutional Neural Networks (CNNs) such as Deep Residual al al al al al al al (er et al., 2016) we have shown that the most important parts of an image work best."}, {"heading": "2 Neural Machine Translation", "text": "In this section, we explain the architecture of neural machine translation by Bahdanau et al. (2014), which was implemented as an attention-based encoder decoder framework with recurring neural networks (\u00a7 2.1), and then explain the conditional GRU layer (\u00a7 2.2) - the gating mechanism we have chosen for our RNN - and how the model can be ported to a multimodal version (\u00a7 2.3)."}, {"heading": "2.1 Text-based NMT", "text": "Considering a source set X = (x1, x2,., xM), the neural network is directly modelled the conditional probability p (Y | X) of its translation Y = (y1, y2,.., yN). The network consists of an encoder and a decoder with an attention mechanism. The encoder calculates a representation C for each source set and a decoder generates a target word at a time and by breaking down the following conditional probability: log p (Y | X) = 1 protocol p (yt | y < t, C) (1) Each source word xi and the target word yi are a column index of the matrix EX and EY. The encoder is a bi-directional RNN with Gated Recurrent Unit (GRU)."}, {"heading": "2.2 Conditional GRU", "text": "The conditional GRU 2 consists of two stacked GRU activations called REC1 and REC2 and an intermediate attention mechanism (referred to as ATT in the footnote). At each time step t, REC1 first computes a hidden state suggestion st based on the previous hidden state st \u2212 1 and the previously issued word yt \u2212 1: z \u2032 t = \u03c3 (W \u2032 zEY [yt \u2212 1] + U \u2032 zst \u2212 1) r \u2032 t = \u03c3 (W \u2032 rEY [yt \u2212 1] + U \u2032 rst \u2212 1) s \u2032 t = tanh (W \u2032 EY [yt \u2212 1] + r \u2032 t (U \u2032 st \u2212 1) s \u2032 t = (1 \u2212 z \u2032 t) s \u2032 t + z \u2032 t st \u2212 1 (6). Then the attention mechanism computes the source sentence based on the annotation sequence C [yt \u2212 1] + r \u2032 t (U \u2032 s \u2212 1 \u2032 t) s \u2032 t = (1 \u2032 t) s \u2212 1 \u2032 t \u2032 t = (1 \u2032 t) s \u2032 t + z \u2032 t st \u2212 1 \u2032 t st \u2212 1 (1 \u2032 t)."}, {"heading": "2.3 Multimodal NMT", "text": "Recently, Calixto et al. (2017) proposed a doubly attentive decoder (referred to in the author's essay as the \"MNMT\" model), which can be considered an extension of the attention-based NMT model proposed in the previous section. Given the sequence of the second a modality notes I = (a1, a2,.., aL), we also calculate a new context vector based on the same interim state hidden suggestion s \u2032 t: it = f \u2032 att (I, s \u2032 t) (9) This new time-dependent context vector is an additional input to a modified version of REC2, which now calculates the final hidden state st using the interim state hidden suggestion s \u2032 s \u2032 t and both time-dependent context vectors ct and it: zt = \u03c3 (Wzct + Wzit \u2032 t) is a new context vector (i.e. t) rt = Wrct \u2032 s \u2032 t (Writ + s \u2032 t = Urt)."}, {"heading": "3 Attention-based Models", "text": "We evaluate three models of the image attention mechanism for Equation 7. What they have in common is that at each step t of the decryption phase, all approaches first use the annotation sequence I as input to derive a time-dependent context vector that contains relevant information in the image to facilitate prediction of the current target word yt. Although these models differ in the way the time-dependent context vector is derived, they share the same subsequent steps. For each mechanism, we propose two hand-picked illustrations that show where the attention is placed in an image."}, {"heading": "3.1 Soft attention", "text": "Soft attention was first used for syntactical constituency analysis by Vinyals et al. (2015), but has since been widely used for translation tasks. It should be noted that it is slightly different from Bahdanau et al. (2014), where its attention takes the previous hidden state of the decoder as input instead of the current (intermediate) state, as in Equation 7. This mechanism has also been successfully investigated for the task of generating image descriptions (Xu et al., 2015), in which a model generates the description of an image in natural language. It has also been used in multimodal translation (Calixto et al., 2017), for which it represents a state of art.The idea of the soft attention model is to take into account all annotations when deriving the context vector from it. It consists of a single advanced network used to calculate an expected alignment et between modality annotations and the target word, which are sent to the current modality at the time and the interaction.1"}, {"heading": "3.2 Hard Stochastic attention", "text": "This model is a stochastic and sample-based process in which we make a difficult decision at any time to make just one comment, corresponding to a spatial position in the image. Hard attention has been used so far in the context of object recognition (Mnih et al., 2014; Ba et al., 2015) and later extended to image description generation (Xu et al., 2015). In the context of multimodal NMT, we can follow Xu et al. (2015) because both our models involve the same process of image. The mechanism fatt is now a function that returns a sampled mean latent variable p, i based on a multimodal distribution parameterized p, i based on the quantification multinoulli ({\u03b11,..., L}) (15) in which the variable p is returned."}, {"heading": "3.3 Local Attention", "text": "In this section, we propose a local attention mechanism that focuses only on a small subset of the image annotations. Local attention has been used for text-based translations (Luong et al., 2015) and is inspired by the selective attention model of Gregor et al. (2015) for image generation, whose approach allows the model to select an image area with varying position and zoom. Local attention instead uses the same \"zoom\" for all target positions and still performs well. This model can be seen as a trade-off between the soft and hard attention models. It selects a spot in the annotation sequence (a spatial localization) and focuses selectively on a small context window around it. Although an image cannot be seen as a temporal sequence, we still hope that the model finds points of interest and selects the useful information around it. This approach has the advantage of being differentiable, while the stocking attention requires more complicated techniques such as amplification and image enhancement."}, {"heading": "4 Image attention optimization", "text": "Three optimizations can be added to the attention mechanism with regard to the image modality, all of which lead to a better use of the image by the model and improve the translation results as a whole. At each decoding step t, we calculate according to the previous decoder state st \u2212 1: \u03b2t = \u03c3 (Wst \u2212 1 + b\u03b2) (24) It is then used to calculate the time-dependent image context vector: it = \u03b2t L \u2211 l = 1 \u03b1t, lal (25) Xu et al. (2015) empirically states that it places more emphasis on the objects in the image descriptions generated with their model. We also double the output size of the comprehensible parameters Ua, Wa and vT in Equation 12 when it comes to calculating the expected annotations on the image annotation sequence."}, {"heading": "5 Experiments", "text": "For these experiments on multimodal machine translation, we used the Multi30K dataset (Elliott et al., 2016), an enhanced version of the Flickr30K entities. For each image, one of the English descriptions was selected and manually translated into German by a professional translator. 29,000 and 1,014 triples respectively were used as training and development data."}, {"heading": "5.1 Training and model details", "text": "All of our models are based on the Nematus frame (Sennrich et al., 2017).The encoder is a bidirectional RNN with GRU, a 1024D single-layer forward and a 1024D single-layer reverse RNN. Word embeddings for source and target language are of 620D and are formed together with the model. Word embeddings and other non-recurring matrices are initialized by sampling from a Gaussian N (0, 0.012), recurring matrices are random orthogonal and bias vectors are all initialized to zero. To create the image annotations used by our decoder, we used a ResNet-50 pre-trained on ImageNet and extracted the features of size 14 x 1024 on its res4f layer (Er et al., 2016)."}, {"heading": "5.2 Quantitative results", "text": "With improvements of + 1.51 BLEU and -2.2 TER for both precision-oriented metrics, the model shows a strong similarity of the n-gram of our candidate translations in reference terms. METEOR's more retrieval-oriented metrics are roughly the same in our models, which is expected because all attention mechanisms are common in each time step t, i.e. taking into account the attention weights of the previous time step t \u2212 1 to calculate the new hidden intergovernmental proposal and thus the new context vector. Again, the greatest improvement is given by the hard stochastic attention mechanism (+ 0.4 METEOR): because it is modeled as a decision procedure in accordance with the previous decisions, this may reinforce the idea of memory. We also mention interesting improvements in the use of the grounded mechanism, especially for soft attention, although this compared image part probably lends more attention to the soft attention of the more complex places compared to the previous choice in 2016."}, {"heading": "5.3 Qualitative results", "text": "In fact, the words \"dog\" (dog), \"forest\" (dog) and \"away\" (hott) are found in the objects that benefit from a high gating scalar. In fact, the attention mechanism has learned to recognize the objects within a scene (dog), that is, to decode the word \"dog\" (dog), the word \"dog\" (dog). \""}, {"heading": "6 Conclusion and future work", "text": "We have tried different attention mechanisms and image modality optimizations and found overall improvements and encouraging results in the Flickr30K entities dataset. Although we have identified some deficiencies in the current attention mechanisms, we can almost certainly conclude that images are a helpful resource for the machine in a translation task. We look forward to trying more abundant and appropriate functions for multimodal translation (i.e. more dense subtitling functions), and another interesting approach would be to use visually grounded word embeddings to capture visual notions of semantic kinship."}, {"heading": "7 Acknowledgements", "text": "This work was partially supported by the IGLU Chist Era project with a contribution from the Belgian Fonds de la Recherche Scientique (FNRS), Contract No. R.50.11.15.F, and by the FSO project VCYCLE with a contribution from the Belgian Walloon Region, Contract No 1510501."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["Jimmy Ba", "Volodymyr Mnih", "Koray Kavukcuoglu."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Ba et al\\.,? 2015", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Does multimodality help human and machine for translation and image captioning", "author": ["Ozan Caglayan", "Walid Aransa", "Yaxing Wang", "Marc Masana", "Mercedes Garc\u0131\u0301a-Mart\u0131\u0301nez", "Fethi Bougares", "Lo\u0131\u0308c Barrault", "Joost van de Weijer"], "venue": null, "citeRegEx": "Caglayan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Caglayan et al\\.", "year": 2016}, {"title": "Doubly-attentive decoder for multi-modal neural machine translation", "author": ["Iacer Calixto", "Qun Liu", "Nick Campbell."], "venue": "CoRR abs/1702.01287. http://arxiv.org/abs/1702.01287.", "citeRegEx": "Calixto et al\\.,? 2017", "shortCiteRegEx": "Calixto et al\\.", "year": 2017}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "\u00c7alar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Multimodal compact bilinear pooling for multimodal neural machine translation", "author": ["Jean-Benoit Delbrouck", "Stephane Dupont."], "venue": "arXiv preprint arXiv:1703.08084 https://arxiv.org/pdf/1703.08084.pdf.", "citeRegEx": "Delbrouck and Dupont.,? 2017", "shortCiteRegEx": "Delbrouck and Dupont.", "year": 2017}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Michael Denkowski", "Alon Lavie."], "venue": "Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.", "citeRegEx": "Denkowski and Lavie.,? 2014", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "Multi30k: Multilingual english-german image descriptions pages 70\u201374", "author": ["D. Elliott", "S. Frank", "K. Sima\u2019an", "L. Specia"], "venue": null, "citeRegEx": "Elliott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2016}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal", "Zoubin Ghahramani."], "venue": "Advances in Neural Information Processing Systems 29 (NIPS).", "citeRegEx": "Gal and Ghahramani.,? 2016", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Rezende", "Daan Wierstra."], "venue": "Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learn-", "citeRegEx": "Gregor et al\\.,? 2015", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Attention-based multimodal neural machine translation", "author": ["Po-Yao Huang", "Frederick Liu", "Sz-Rung Shiang", "Jean Oh", "Chris Dyer."], "venue": "Proceedings of the First Conference on Machine Translation, Berlin, Germany.", "citeRegEx": "Huang et al\\.,? 2016", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "koray kavukcuoglu"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Asso-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "Nematus: a Toolkit for Neural Machine", "author": ["Rico Sennrich", "Orhan Firat", "Kyunghyun Cho", "Alexandra Birch", "Barry Haddow", "Julian Hitschler", "Marcin Junczys-Dowmunt", "Samuel L\u201daubli", "Antonio Valerio Miceli Barone", "Jozef Mokry", "Maria Nadejde"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2017}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."], "venue": "In Proceedings of Association for Machine Translation in the Americas. pages 223\u2013231.", "citeRegEx": "Snover et al\\.,? 2006", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "A shared task on multimodal machine translation and crosslingual image description", "author": ["Lucia Specia", "Stella Frank", "Khalil Sima\u2019an", "Desmond Elliott"], "venue": "In Proceedings of the First Conference on Machine Translation", "citeRegEx": "Specia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141 ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."], "venue": "David Blei and Francis Bach, editors,", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "CoRR abs/1212.5701. http://arxiv.org/abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 21, "context": "Recently, the attention-based encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2014) has been largely adopted.", "startOffset": 56, "endOffset": 103}, {"referenceID": 1, "context": "Recently, the attention-based encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2014) has been largely adopted.", "startOffset": 56, "endOffset": 103}, {"referenceID": 23, "context": "Attention mechanisms have shown to work with other modalities too, like images, where their are able to learn to attend the salient parts of an image, for instance when generating text captions (Xu et al., 2015).", "startOffset": 194, "endOffset": 211}, {"referenceID": 11, "context": "For such applications, Convolutional Neural Networks (CNNs) such as Deep Residual (He et al., 2016) have shown to work best to represent images.", "startOffset": 82, "endOffset": 99}, {"referenceID": 20, "context": "To investigate the effectiveness of information obtained from images, a multimodal machine translation shared task (Specia et al., 2016) has been addressed to the MT community1.", "startOffset": 115, "endOffset": 136}, {"referenceID": 11, "context": "The best results of NMT model were those of Huang et al. (2016) who used LSTM fed with global visual features or multiple regional visual features followed by rescoring.", "startOffset": 44, "endOffset": 64}, {"referenceID": 3, "context": "Recently, Calixto et al. (2017) proposed a doubly-attentive decoder that outperformed this baseline with less data and without rescoring.", "startOffset": 10, "endOffset": 32}, {"referenceID": 1, "context": "In this section, we detail the neural machine translation architecture by Bahdanau et al. (2014), implemented as an attention-based encoder-decoder framework with recurrent neural networks (\u00a72.", "startOffset": 74, "endOffset": 97}, {"referenceID": 16, "context": "We use a deep output layer (Pascanu et al., 2014) to compute a vocabulary-sized vector :", "startOffset": 27, "endOffset": 49}, {"referenceID": 3, "context": "Recently, Calixto et al. (2017) proposed a doubly attentive decoder (referred as the \u201dMNMT\u201d model in the author\u2019s paper) which can be seen as an expansion of the attention-based NMT model proposed in the previous section.", "startOffset": 10, "endOffset": 32}, {"referenceID": 23, "context": "This mechanism has also been successfully investigated for the task of image description generation (Xu et al., 2015) where a model generates an image\u2019s description in natural language.", "startOffset": 100, "endOffset": 117}, {"referenceID": 3, "context": "It has been used in multimodal translation as well (Calixto et al., 2017), for which it constitutes a state-of-the-art.", "startOffset": 51, "endOffset": 73}, {"referenceID": 20, "context": "Soft attention has firstly been used for syntactic constituency parsing by Vinyals et al. (2015) but has been widely used for translation tasks ever since.", "startOffset": 75, "endOffset": 97}, {"referenceID": 1, "context": "One should note that it slightly differs from Bahdanau et al. (2014) where their attention takes as input the previous decoder hidden state instead of the current (intermediate) one as shown in equation 7.", "startOffset": 46, "endOffset": 69}, {"referenceID": 14, "context": "Hard attention has previously been used in the context of object recognition (Mnih et al., 2014; Ba et al., 2015) and later extended to image description generation (Xu et al.", "startOffset": 77, "endOffset": 113}, {"referenceID": 0, "context": "Hard attention has previously been used in the context of object recognition (Mnih et al., 2014; Ba et al., 2015) and later extended to image description generation (Xu et al.", "startOffset": 77, "endOffset": 113}, {"referenceID": 23, "context": ", 2015) and later extended to image description generation (Xu et al., 2015).", "startOffset": 59, "endOffset": 76}, {"referenceID": 0, "context": ", 2014; Ba et al., 2015) and later extended to image description generation (Xu et al., 2015). In the context of multimodal NMT, we can follow Xu et al. (2015) because both our models involve the same process on images.", "startOffset": 8, "endOffset": 160}, {"referenceID": 13, "context": "Local Attention has been used for text-based translation (Luong et al., 2015) and is inspired by the selective attention model of Gregor et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 10, "context": ", 2015) and is inspired by the selective attention model of Gregor et al. (2015) for image generation.", "startOffset": 60, "endOffset": 81}, {"referenceID": 6, "context": "Lastly, we use a grounding attention inspired by Delbrouck and Dupont (2017). The mechanism merge each spatial location ai in the annotation sequence I with the initial decoder state s0 obtained in equation 5 with non-linearity :", "startOffset": 49, "endOffset": 77}, {"referenceID": 8, "context": "For this experiments on Multimodal Machine Translation, we used the Multi30K dataset (Elliott et al., 2016) which is an extended version of the Flickr30K Entities.", "startOffset": 85, "endOffset": 107}, {"referenceID": 17, "context": "All our models are build on top of the nematus framework (Sennrich et al., 2017).", "startOffset": 57, "endOffset": 80}, {"referenceID": 11, "context": "To create the image annotations used by our decoder, we used a ResNet-50 pre-trained on ImageNet and extracted the features of size 14 \u00d7 14 \u00d7 1024 at its res4f layer (He et al., 2016).", "startOffset": 166, "endOffset": 183}, {"referenceID": 9, "context": "We apply dropout using one same mask in all time steps (Gal and Ghahramani, 2016).", "startOffset": 55, "endOffset": 81}, {"referenceID": 18, "context": "We use the byte pair encoding algorithm on the train set to convert space-separated tokens into subwords (Sennrich et al., 2016), reducing our vocabulary size to 9226 and 14957 words for English and German respectively.", "startOffset": 105, "endOffset": 128}, {"referenceID": 24, "context": "All variants of our attention model were trained with ADADELTA (Zeiler, 2012), with minibatches of size 80 for our monomodal (text-only) NMT model and 40 for our multimodal NMT.", "startOffset": 63, "endOffset": 77}, {"referenceID": 15, "context": "We use the metrics BLEU4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al.", "startOffset": 25, "endOffset": 48}, {"referenceID": 7, "context": ", 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al.", "startOffset": 16, "endOffset": 43}, {"referenceID": 19, "context": ", 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006) to evaluate the quality of our models\u2019 translations.", "startOffset": 52, "endOffset": 73}, {"referenceID": 3, "context": "We notice a nice overall progress over Calixto et al. (2017) multimodal baseline, especially when using the stochastic attention.", "startOffset": 39, "endOffset": 61}, {"referenceID": 2, "context": "BLEU\u2191 METEOR\u2191 TER\u2193 Monomodal (text only) Caglayan et al. (2016) 32.", "startOffset": 41, "endOffset": 64}, {"referenceID": 2, "context": "BLEU\u2191 METEOR\u2191 TER\u2193 Monomodal (text only) Caglayan et al. (2016) 32.50 49.2 Calixto et al. (2017) 33.", "startOffset": 41, "endOffset": 97}, {"referenceID": 2, "context": "Multimodal Caglayan et al. (2016) 27.", "startOffset": 11, "endOffset": 34}, {"referenceID": 2, "context": "Multimodal Caglayan et al. (2016) 27.82 45.0 Huang et al. (2016) 36.", "startOffset": 11, "endOffset": 65}, {"referenceID": 2, "context": "Multimodal Caglayan et al. (2016) 27.82 45.0 Huang et al. (2016) 36.50 54.1 Calixto et al. (2017) 36.", "startOffset": 11, "endOffset": 98}, {"referenceID": 3, "context": "We pick Calixto et al. (2017) scores as baseline and report our results accordingly (green for improvement and red for deterioration).", "startOffset": 8, "endOffset": 30}, {"referenceID": 2, "context": "Note that even though our baseline NMT model is basically the same as Calixto et al. (2017), our experiments results are slightly better.", "startOffset": 70, "endOffset": 92}, {"referenceID": 2, "context": "We also compared our results to Caglayan et al. (2016) because our multimodal models are nearly identical with the major exception of the gating scalar (cfr.", "startOffset": 32, "endOffset": 55}, {"referenceID": 2, "context": "Without this scalar, the translation scores undergo a massive drop (as seen in Caglayan et al. (2016)) which means that the attention mechanisms don\u2019t really understand the more complex relationships between objects, what is really happening in the scene.", "startOffset": 79, "endOffset": 102}], "year": 2017, "abstractText": "In state-of-the-art Neural Machine Translation (NMT), an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions that they describe. In this paper, we compare several attention mechanism on the multimodal translation task (English, image \u2192 German) and evaluate the ability of the model to make use of images to improve translation. We surpass state-of-the-art scores on the Multi30k data set, we nevertheless identify and report different misbehavior of the machine while translating.", "creator": "LaTeX with hyperref package"}}}