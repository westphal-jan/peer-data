{"id": "1707.01067", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2017", "title": "ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games", "abstract": "In this paper, we propose ELF, an Extensive, Lightweight and Flexible platform for fundamental reinforcement learning research. Using ELF, we implement a highly customizable real-time strategy (RTS) engine with three game environments (Mini-RTS, Capture the Flag and Tower Defense). Mini-RTS, as a miniature version of StarCraft, captures key game dynamics and runs at 40K frame-per-second (FPS) per core on a Macbook Pro notebook. When coupled with modern reinforcement learning methods, the system can train a full-game bot against built-in AIs end-to-end in one day with 6 CPUs and 1 GPU. In addition, our platform is flexible in terms of environment-agent communication topologies, choices of RL methods, changes in game parameters, and can host existing C/C++-based game environments like Arcade Learning Environment. Using ELF, we thoroughly explore training parameters and show that a network with Leaky ReLU and Batch Normalization coupled with long-horizon training and progressive curriculum beats the rule-based built-in AI more than $70\\%$ of the time in the full game of Mini-RTS. Strong performance is also achieved on the other two games. In game replays, we show our agents learn interesting strategies. ELF, along with its RL platform, will be open-sourced.", "histories": [["v1", "Tue, 4 Jul 2017 16:48:56 GMT  (2309kb,D)", "http://arxiv.org/abs/1707.01067v1", "Submission to NIPS 2017"]], "COMMENTS": "Submission to NIPS 2017", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yuandong tian", "qucheng gong", "wenling shang", "yuxin wu", "larry zitnick"], "accepted": true, "id": "1707.01067"}, "pdf": {"name": "1707.01067.pdf", "metadata": {"source": "CRF", "title": "ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games", "authors": ["Yuandong Tian", "Qucheng Gong", "Wenling Shang", "Yuxin Wu", "Larry Zitnick"], "emails": ["yuandong@fb.com", "qucheng@fb.com", "wendy.shang@oculus.com", "yuxinwu@fb.com", "zitnick@fb.com"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Architecture", "text": "This year is the highest in the history of the country."}, {"heading": "3 Real-time strategy Games", "text": "Real-time strategy games (RTS) are considered to be one of the next great AI challenges after Chess and Go [25]. In RTS games, players gather resources together, build units (facilities, troops, etc.) and explore the environment in the fog of war (i.e., regions out of the sight of units are invisible) to raid / defend the enemy until a player wins. RTS games are known for their exponential and changing action space (e.g. 510 possible actions for 10 units, each with 5 choices), subtle game situations and long-delayed rewards. Typically, professional players undertake 200-300 actions per minute, and the game lasts 20-30 minutes. Very few existing RTS engines can be used directly for research. Commercial RTS games (e.g. StarCraft I / II) have sophisticated dynamics, interactions and graphics. Game strategies for RTS games have long been proven to be complex. In addition, open source RTS games are not designed for internal sources such as internal interfaces, and they can not be easily developed by RTS researchers."}, {"heading": "3.1 Our approach", "text": "Many popular RTS games and their variants (e.g. StarCraft, DoTA, Leagues of Legends, Tower Defense) share the same structure: A few units are controlled by one player to move, attack, collect, or perform special spells to influence their own or an enemy's army. Our command hierarchy allows you to create a new game by (1) issuing available commands (2), and (3) issuing commands like each unit triggered by specific scenarios. To do this, we offer simple but effective tools. Explorers can create these variables either by adding commands in C + +, or by writing game scripts (e.g. Lua). All derived games share the mechanism of hierarchical commands, replay, etc. AIs can also be extended. We offer the following three games: Mini-RTS, Capture the Flag and Tower Defense (Fig. 3) sharing the following gameplay characteristics:"}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Benchmarking ELF", "text": "We run ELF on a single server with a different number of CPU cores to test the efficiency of parallelism. Figure 4 (a) shows the results when running Mini-RTS. We can see that ELF scales well with the number of CPU cores used to run the environments. We also embed the Atari emulator [4] into our platform and check the speed difference between an ALE with only one thread and an ALE per core (Figure 4 (b)). While a single-thread engine delivers about 5.8K FPS on Pong, our parallel ALE runs at a comparable speed (5.1K FPS per core) with up to 16 cores, while OpenAI Gym (with Python thon threads) runs 3x slower (1.7K FPS per core) with 16 cores 1024 threads and degrades with more cores. The number of threads is important for training, since they could be the same as the number of Pythons we observe at 1024."}, {"heading": "4.2 Baselines on Real-time Strategy Games", "text": "We focus on 1-vs-1 complete games between trained AIs and built-in AIs. Built-in AIs have access to complete information (e.g., the number of enemy tanks), while trained AIs know partial information in the fog of war, i.e., game environments within the range of their own units. There are exceptions: in Mini-RTS, the location of the enemy base is known so the trained AI can attack; in Capture the Flag, the flag position is known to all; Tower Defense is a game of complete information.Details of the built-in AI. For Mini-RTS, there are two rules-based AIs: SIMPLE collects, builds five tanks and then attacks the enemy base. HIT N RUN harasses frequently, builds and attacks. For Capture the Flag, we have a built-in AI. For Tower Defense (TD), no AI is required."}, {"heading": "4.2.1 A3C baseline", "text": "Next, we will describe our basics and their variants. While we refer to these basics, we are the first to show trained AIs for Real-Time Strategy (RTS) with partial information. For all games, we will randomize the starting situations for various experiences and use A3C to play the full game. We will run all experiments 5 times and report on standard deviations. We will use simple conventional networks with two heads, one for actions and the other for values. Input functions are spatially structured (20 by 20) and the abstractions of the current multi-channel game environment. At each location, the way they are taken will be quantified and written to find the appropriate channels. For Mini-RTS, we will add an additional constant channel filled with the player's current resources."}, {"heading": "4.2.2 Curriculum Training", "text": "All AIs shown in Tbl. 3 and Tbl. 4 are trained with curriculum training. In Mini-RTS, we let the built-in AI play the first ticks, with k \u0445 uniform (0, 1000), then we switch to the AI to be trained. This reduces (1) initially the difficulty of the game and (2) there are various situations for training to avoid local minima. During training, the help of the built-in AIs is gradually reduced until no help is given. All reported win rates are achieved by the trained agents alone. We make the comparison with and without curriculum training in Tbl. 6. It is clear that performance is improved with curriculum training. Similarly, when fine-tuning models that are pre-trained with a type of opponent, towards a mix of opponents (e.g. 50% SIMPLE + 50% HIT N RUN)."}, {"heading": "4.2.3 Monte-Carlo Tree Search", "text": "Monte-Carlo Tree Search (MCTS) can be used for planning when complete information about the game is known, including the complete state s without war fog and the precise forward model s \u2032 = s \u2032 (s, a). Based on the current state of play, MCTS builds a game tree that tends towards paths with a high win rate. Leaves are extended with all candidate moves and the win rate is calculated by random self-play until the end of the game. We use 8 threads with 100 rollouts each. We use parallelization [9], in which each thread independently expands and combines a tree to obtain the most visited action. As shown in Tbl. 7, MCTS achieves a comparable win rate for models trained with RL. Note that the win rates of the two methods are not directly comparable, as RL methods have no knowledge of game dynamics and their state knowledge is reduced by the limits introduced by the war fog."}, {"heading": "5 Conclusion and Future Work", "text": "In this article, we propose ELF, a research-oriented platform for simultaneous game simulation that offers a comprehensive set of game options, a lightweight game simulator and a flexible environment. On ELF, we build an RTS game engine and three initial environments (Mini-RTS, Capture the Flag and Tower Defense) that run 40KFPS per core on a MacBook Pro laptop. As a result, a full-game bot in these games can be trained continuously on a single machine in one day and exhibit interesting learned behaviors.ELF opens up many opportunities for future research. This lightweight and flexible platform allows us to efficiently explore RTS methods for games, including forward-looking modeling, hierarchical RL planning under uncertainty, RL space with a complicated action and research platform, so we can compete with a more affordable library."}, {"heading": "6 Appendix: Detailed descriptions of RTS engine and games", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Overview", "text": "So on ELF we build three different environments, Mini-RTS, Capture the Flag and Tower Defense. Tbl. 8 shows their characteristics."}, {"heading": "6.2 Hierarchical Commands", "text": "The command level in our RTS engine is hierarchical (Fig. 8). A high-level command can execute other commands at the same time during execution, which can then be executed and potentially trigger other commands as well. A command can also execute subsequent commands for future ticks. There are two types of commands, permanent and immediate. Permanent commands (e.g. Move, Attack) take many ticks to completion (e.g. Enemy Down), while instant commands take effect within one tick."}, {"heading": "6.3 Units", "text": "Tbl. 9 shows available units for Mini-RTS that capture all the basic dynamics of RTS games: gathering, building facilities, building different types of troops, defending against enemy attacks and / or enemy invasion bases. For troops, there are melee units with high hit points, high attack points but low movement speed and agile units with low hit point, long attack range but high movement speed. Tbl. 10 shows available units for Capture the Flag. Note that our framework is extensive and adding more units is easy."}, {"heading": "6.4 Game dynamics for Capture the Flag", "text": "The flag appears in the middle of the card. The athlete can carry a flag or fight against each other. When carrying a flag, an athlete has a reduced movement speed. After death, he drops the flag if he carries one, and after a certain time appears automatically at the base. As soon as a flag is brought to the base of a player, the player receives a point and the flag is returned to the middle of the card. The first player who scores 5 points wins."}, {"heading": "6.5 Game dynamics for Tower Defense", "text": "In Tower Defense, the player defends his base in the upper left corner. Every 200 ticks spawn more and more enemy attackers in the lower right corner of the map and travel through a maze to the player's base. The player can build towers along the way to prevent the enemy from reaching the target. For 5 enemies killed, the player can build a new tower. The player loses when 10 enemies reach his base, and wins if he survives 10 waves of attacks."}, {"heading": "6.6 Game Balance", "text": "We test the game balance of Mini-RTS and Capture the Flag. We use the same AI to fight each other. In Mini-RTS the win rate for player 0 is 50.0 (\u00b1 3.0) and in Capture the Flag the win rate for player 0 is 49.9 (\u00b1 1.1)."}, {"heading": "6.7 Replay", "text": "We offer the serialization of replay and state snapshot of arbitrary ticks, which is more flexible than many commercial games."}, {"heading": "7 Detailed explanation of the experiments", "text": "Tbl. 11 shows the discrete action space for Mini-RTS and Capture the Flag that was used in the experiments."}, {"heading": "7.1 Randomness", "text": "All games based on the RTS engine are deterministic, but modern RL methods require diverse experiences to explore the game statespace more efficiently. When we train AIs for Mini-RTS, we add randomly by placing resources and bases, and adding units and buildings randomly to start with a low probability. In Capture the Flag, all athletes have a random starting position, and the flag appears in a random place with equal distances to the bases of both players."}, {"heading": "7.2 Rule based AIs for Mini-RTS", "text": "Once he has 5 melee attackers, all 5 attackers attack the enemy base. Hit & Run AI This AI builds up 3 workers and asks them to collect resources, then builds a barrack if the resource allows, and then starts building up range attackers. If he has 2 range attackers, the range attackers move to the enemy base and attack enemy troops within range. In enemy counterattacks, the range attackers strike and run away."}, {"heading": "7.3 Rule based AIs for Capture the Flag", "text": "Simple AI This AI tries to get flag when the flag is unoccupied. If one of the athletes gets the flag, he will escort the flag back to the base while other athletes defend the attack of the opponent. If an opposing athlete carries the flag, all athletes will attack the flag bearer."}], "references": [{"title": "Reinforcement learning through asynchronous advantage actor-critic on a gpu", "author": ["Mohammad Babaeizadeh", "Iuri Frosio", "Stephen Tyree", "Jason Clemons", "Jan Kautz"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2017}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Marc G. Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"], "venue": "CoRR, abs/1207.4708,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Playing SNES in the retro learning environment", "author": ["Nadav Bhonker", "Shai Rozenberg", "Itay Hubara"], "venue": "CoRR, abs/1611.02205,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Samothrakis, and Simon 9  Colton. A survey of monte carlo tree search methods", "author": ["Cameron B Browne", "Edward Powley", "Daniel Whitehouse", "Simon M Lucas", "Peter I Cowling", "Philipp Rohlfshagen", "Stephen Tavener", "Diego Perez", "Spyridon"], "venue": "IEEE Transactions on Computational Intelligence and AI in games,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "On the development of a free rts game engine", "author": ["Michael Buro", "Timothy Furtak"], "venue": "In Game- OnNA Conference,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Parallel monte-carlo tree search", "author": ["Guillaume MJ-B Chaslot", "Mark HM Winands", "H Jaap van Den Herik"], "venue": "In International Conference on Computers and Games,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Spring: https://springrts.com", "author": ["Stefan Johansson", "Robin Westberg"], "venue": "URL https: //springrts.com/", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "The malmo platform for artificial intelligence experimentation", "author": ["Matthew Johnson", "Katja Hofmann", "Tim Hutton", "David Bignell"], "venue": "In International joint conference on artificial intelligence (IJCAI),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Vizdoom: A doom-based ai research platform for visual reinforcement learning", "author": ["Micha\u0142 Kempka", "Marek Wydmuch", "Grzegorz Runc", "Jakub Toczek", "Wojciech Ja\u015bkowski"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Playing fps games with deep reinforcement learning", "author": ["Guillaume Lample", "Devendra Singh Chaplot"], "venue": "arXiv preprint arXiv:1609.05521,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Andrew L Maas", "Awni Y Hannun", "Andrew Y Ng"], "venue": "In Proc. ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Andrew L Maas", "Awni Y Hannun", "Andrew Y Ng"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Learning to navigate in complex environments", "author": ["Piotr Mirowski", "Razvan Pascanu", "Fabio Viola", "Hubert Soyer", "Andrew J. Ballard", "Andrea Banino", "Misha Denil", "Ross Goroshin", "Laurent Sifre", "Koray Kavukcuoglu", "Dharshan Kumaran", "Raia Hadsell"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["Arun Nair", "Praveen Srinivasan", "Sam Blackwell", "Cagdas Alcicek", "Rory Fearon", "Alessandro De Maria", "Vedavyas Panneershelvam", "Mustafa Suleyman", "Charles Beattie", "Stig Petersen", "Shane Legg", "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver"], "venue": "CoRR, abs/1507.04296,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "The combinatorial multi-armed bandit problem and its application to realtime strategy games", "author": ["Santiago Ontan\u00f3n"], "venue": "In Proceedings of the Ninth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Multiagent bidirectionally-coordinated nets for learning to play starcraft combat", "author": ["Peng Peng", "Quan Yuan", "Ying Wen", "Yaodong Yang", "Zhenkun Tang", "Haitao Long", "Jun Wang"], "venue": "games. CoRR,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2017}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Pieter Abbeel", "Michael I Jordan", "Philipp Moritz"], "venue": "In ICML,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Mazebase: A sandbox for learning from games", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Gabriel Synnaeve", "Soumith Chintala", "Rob Fergus"], "venue": "CoRR, abs/1511.07401,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S Sutton", "David A McAllester", "Satinder P Singh", "Yishay Mansour"], "venue": "In NIPS,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1999}, {"title": "Torchcraft: a library for machine learning research on real-time strategy", "author": ["Gabriel Synnaeve", "Nantas Nardelli", "Alex Auvolat", "Soumith Chintala", "Timoth\u00e9e Lacroix", "Zeming Lin", "Florian Richoux", "Nicolas Usunier"], "venue": "games. CoRR,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Better computer go player with neural network and long-term prediction", "author": ["Yuandong Tian", "Yan Zhu"], "venue": "arXiv preprint arXiv:1511.06410,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Episodic exploration for deep deterministic policies: An application to starcraft micromanagement", "author": ["Nicolas Usunier", "Gabriel Synnaeve", "Zeming Lin", "Soumith Chintala"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2017}], "referenceMentions": [{"referenceID": 1, "context": "In addition, our platform is flexible in terms of environment-agent communication topologies, choices of RL methods, changes in game parameters, and can host existing C/C++-based game environments like ALE [4].", "startOffset": 206, "endOffset": 209}, {"referenceID": 11, "context": "Using ELF, we thoroughly explore training parameters and show that a network with Leaky ReLU [16] and Batch Normalization [10] coupled with long-horizon training and progressive curriculum beats the rule-based built-in AI more than 70% of the time in the full game of Mini-RTS.", "startOffset": 93, "endOffset": 97}, {"referenceID": 6, "context": "Using ELF, we thoroughly explore training parameters and show that a network with Leaky ReLU [16] and Batch Normalization [10] coupled with long-horizon training and progressive curriculum beats the rule-based built-in AI more than 70% of the time in the full game of Mini-RTS.", "startOffset": 122, "endOffset": 126}, {"referenceID": 1, "context": "how to train intelligent agents to behave properly from sparse rewards [4, 6, 5, 13, 27].", "startOffset": 71, "endOffset": 88}, {"referenceID": 2, "context": "how to train intelligent agents to behave properly from sparse rewards [4, 6, 5, 13, 27].", "startOffset": 71, "endOffset": 88}, {"referenceID": 9, "context": "how to train intelligent agents to behave properly from sparse rewards [4, 6, 5, 13, 27].", "startOffset": 71, "endOffset": 88}, {"referenceID": 20, "context": "how to train intelligent agents to behave properly from sparse rewards [4, 6, 5, 13, 27].", "startOffset": 71, "endOffset": 88}, {"referenceID": 10, "context": "For example, changing environment parameters [33], as well as using internal data [14, 18] have been shown to lead to a substantial acceleration in training.", "startOffset": 82, "endOffset": 90}, {"referenceID": 13, "context": "For example, changing environment parameters [33], as well as using internal data [14, 18] have been shown to lead to a substantial acceleration in training.", "startOffset": 82, "endOffset": 90}, {"referenceID": 1, "context": "Old games and their wrappers [4, 6, 5, 13]) are substantially faster, but are less realistic with limited customizability.", "startOffset": 29, "endOffset": 42}, {"referenceID": 2, "context": "Old games and their wrappers [4, 6, 5, 13]) are substantially faster, but are less realistic with limited customizability.", "startOffset": 29, "endOffset": 42}, {"referenceID": 9, "context": "Old games and their wrappers [4, 6, 5, 13]) are substantially faster, but are less realistic with limited customizability.", "startOffset": 29, "endOffset": 42}, {"referenceID": 20, "context": ", MazeBase [27], \u03bcRTS [21]) are efficient and highly customizable, but are not very extensive in their capabilities.", "startOffset": 11, "endOffset": 15}, {"referenceID": 16, "context": ", MazeBase [27], \u03bcRTS [21]) are efficient and highly customizable, but are not very extensive in their capabilities.", "startOffset": 22, "endOffset": 26}, {"referenceID": 24, "context": "Previously, most research on RTS games focused only on lower-level scenarios such as tactical battles [32, 23].", "startOffset": 102, "endOffset": 110}, {"referenceID": 17, "context": "Previously, most research on RTS games focused only on lower-level scenarios such as tactical battles [32, 23].", "startOffset": 102, "endOffset": 110}, {"referenceID": 1, "context": ", ALE [4]), board games (e.", "startOffset": 6, "endOffset": 9}, {"referenceID": 23, "context": "Chess and Go [30]), and physics engines, by writing a simple adaptor.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "We use the Asynchronous Advantagous Actor-Critic (A3C) model [19] and explore extensive design choices including frameskip, temporal horizon, network structure, curriculum training, etc.", "startOffset": 61, "endOffset": 65}, {"referenceID": 11, "context": "We show that a network with Leaky ReLU [16] and Batch Normalization [10] coupled with long-horizon training and progressive curriculum beats the rule-based built-in AI more than 70% of the time in full-game Mini-RTS.", "startOffset": 39, "endOffset": 43}, {"referenceID": 6, "context": "We show that a network with Leaky ReLU [16] and Batch Normalization [10] coupled with long-horizon training and progressive curriculum beats the rule-based built-in AI more than 70% of the time in full-game Mini-RTS.", "startOffset": 68, "endOffset": 72}, {"referenceID": 14, "context": "Modern reinforcement learning methods often require heavy parallelism to obtain diverse experiences [19, 20].", "startOffset": 100, "endOffset": 108}, {"referenceID": 15, "context": "Modern reinforcement learning methods often require heavy parallelism to obtain diverse experiences [19, 20].", "startOffset": 100, "endOffset": 108}, {"referenceID": 2, "context": "Most existing RL environments (OpenAI Gym [6] and Universe [31], RLE [5], Atari [4], Doom [13]) provide Python interfaces which wrap only single game instances.", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "Most existing RL environments (OpenAI Gym [6] and Universe [31], RLE [5], Atari [4], Doom [13]) provide Python interfaces which wrap only single game instances.", "startOffset": 80, "endOffset": 83}, {"referenceID": 9, "context": "Most existing RL environments (OpenAI Gym [6] and Universe [31], RLE [5], Atari [4], Doom [13]) provide Python interfaces which wrap only single game instances.", "startOffset": 90, "endOffset": 94}, {"referenceID": 14, "context": "g, vanilla A3C [19]), in which each agent follows and updates its own copy of the model.", "startOffset": 15, "endOffset": 19}, {"referenceID": 0, "context": ", BatchA3C [33] or GA3C [1]), where the model can perform batched forward prediction to better utilize GPUs.", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": ", Monte-Carlo Tree Search (MCTS) [7, 30, 25]) and Self-Play, in which at one time step, a single environment might emit multiple states processed by multiple models, or one-to-many.", "startOffset": 33, "endOffset": 44}, {"referenceID": 23, "context": ", Monte-Carlo Tree Search (MCTS) [7, 30, 25]) and Self-Play, in which at one time step, a single environment might emit multiple states processed by multiple models, or one-to-many.", "startOffset": 33, "endOffset": 44}, {"referenceID": 19, "context": ", Monte-Carlo Tree Search (MCTS) [7, 30, 25]) and Self-Play, in which at one time step, a single environment might emit multiple states processed by multiple models, or one-to-many.", "startOffset": 33, "endOffset": 44}, {"referenceID": 1, "context": ", ALE [4]), board games (e.", "startOffset": 6, "endOffset": 9}, {"referenceID": 23, "context": "Chess and Go [30]), and a customized RTS engine, with a simple adaptor.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": ", A3C [19], Policy Gradient [28], Q-learning, Trust Region Policy Optimization [24], etc) are implemented, mostly with very few lines of Python codes.", "startOffset": 6, "endOffset": 10}, {"referenceID": 21, "context": ", A3C [19], Policy Gradient [28], Q-learning, Trust Region Policy Optimization [24], etc) are implemented, mostly with very few lines of Python codes.", "startOffset": 28, "endOffset": 32}, {"referenceID": 18, "context": ", A3C [19], Policy Gradient [28], Q-learning, Trust Region Policy Optimization [24], etc) are implemented, mostly with very few lines of Python codes.", "startOffset": 79, "endOffset": 83}, {"referenceID": 19, "context": "Real-time strategy (RTS) games are considered to be one of the next grand AI challenges after Chess and Go [25].", "startOffset": 107, "endOffset": 111}, {"referenceID": 7, "context": "Open-source RTS games like Spring [11], OpenRA [22] and Warzone 2100 [26] focus on complex graphics and effects, convenient user interface, stable network play, flexible map editors and plug-and-play mods (i.", "startOffset": 34, "endOffset": 38}, {"referenceID": 4, "context": "ORTS [8], BattleCode [2] and RoboCup Simulation League [15] are designed for coding competitions and focused on rule-based AIs.", "startOffset": 5, "endOffset": 8}, {"referenceID": 16, "context": ", \u03bcRTS [21], MazeBase [27]) are fast and simple, often coming with various baselines,", "startOffset": 7, "endOffset": 11}, {"referenceID": 20, "context": ", \u03bcRTS [21], MazeBase [27]) are fast and simple, often coming with various baselines,", "startOffset": 22, "endOffset": 26}, {"referenceID": 1, "context": "Platform ALE [4] RLE [5] Universe [31] Malmo [12] Frame per second 6000 530 60 120 Platform DeepMind Lab [3] VizDoom [13] TorchCraft [29] Mini-RTS Frame per second 287(C)/866(G) \u223c 7,000 2,000 (frameskip=50) 40,000 Table 2: Frame rate comparison.", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "Platform ALE [4] RLE [5] Universe [31] Malmo [12] Frame per second 6000 530 60 120 Platform DeepMind Lab [3] VizDoom [13] TorchCraft [29] Mini-RTS Frame per second 287(C)/866(G) \u223c 7,000 2,000 (frameskip=50) 40,000 Table 2: Frame rate comparison.", "startOffset": 21, "endOffset": 24}, {"referenceID": 8, "context": "Platform ALE [4] RLE [5] Universe [31] Malmo [12] Frame per second 6000 530 60 120 Platform DeepMind Lab [3] VizDoom [13] TorchCraft [29] Mini-RTS Frame per second 287(C)/866(G) \u223c 7,000 2,000 (frameskip=50) 40,000 Table 2: Frame rate comparison.", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "Platform ALE [4] RLE [5] Universe [31] Malmo [12] Frame per second 6000 530 60 120 Platform DeepMind Lab [3] VizDoom [13] TorchCraft [29] Mini-RTS Frame per second 287(C)/866(G) \u223c 7,000 2,000 (frameskip=50) 40,000 Table 2: Frame rate comparison.", "startOffset": 117, "endOffset": 121}, {"referenceID": 22, "context": "Platform ALE [4] RLE [5] Universe [31] Malmo [12] Frame per second 6000 530 60 120 Platform DeepMind Lab [3] VizDoom [13] TorchCraft [29] Mini-RTS Frame per second 287(C)/866(G) \u223c 7,000 2,000 (frameskip=50) 40,000 Table 2: Frame rate comparison.", "startOffset": 133, "endOffset": 137}, {"referenceID": 22, "context": "Recently, TorchCraft [29] provides APIs for StarCraft I to access its internal game states.", "startOffset": 21, "endOffset": 25}, {"referenceID": 1, "context": "We also embed Atari emulator [4] into our platform and check the speed difference between a single-threaded ALE and paralleled ALE per core (Fig.", "startOffset": 29, "endOffset": 32}, {"referenceID": 14, "context": "For all games, we randomize the initial game states for more diverse experience and use A3C [19] to train AIs to play the full game.", "startOffset": 92, "endOffset": 96}, {"referenceID": 6, "context": "Since the input is sparse and heterogeneous, we experiment on CNN architectures with Batch Normalization [10] and Leaky ReLU [17].", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "Since the input is sparse and heterogeneous, we experiment on CNN architectures with Batch Normalization [10] and Leaky ReLU [17].", "startOffset": 125, "endOffset": 129}, {"referenceID": 14, "context": "While Vanilla A3C [19] uses T = 5 for Atari games, the reward in Mini-RTS is more delayed (\u223c 80 actions before a reward).", "startOffset": 18, "endOffset": 22}, {"referenceID": 5, "context": "We use root parallelization [9] in which each thread independently expands a tree, and are combined to get the most visited action.", "startOffset": 28, "endOffset": 31}], "year": 2017, "abstractText": "In this paper, we propose ELF, an Extensive, Lightweight and Flexible platform for fundamental reinforcement learning research. Using ELF, we implement a highly customizable real-time strategy (RTS) engine with three game environments (Mini-RTS, Capture the Flag and Tower Defense). Mini-RTS, as a miniature version of StarCraft, captures key game dynamics and runs at 40K frame-persecond (FPS) per core on a Macbook Pro notebook. When coupled with modern reinforcement learning methods, the system can train a full-game bot against builtin AIs end-to-end in one day with 6 CPUs and 1 GPU. In addition, our platform is flexible in terms of environment-agent communication topologies, choices of RL methods, changes in game parameters, and can host existing C/C++-based game environments like ALE [4]. Using ELF, we thoroughly explore training parameters and show that a network with Leaky ReLU [16] and Batch Normalization [10] coupled with long-horizon training and progressive curriculum beats the rule-based built-in AI more than 70% of the time in the full game of Mini-RTS. Strong performance is also achieved on the other two games. In game replays, we show our agents learn interesting strategies. ELF, along with its RL platform, will be open-sourced.", "creator": "LaTeX with hyperref package"}}}