{"id": "1707.01075", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2017", "title": "Improving Slot Filling Performance with Attentive Neural Networks on Dependency Structures", "abstract": "Slot Filling (SF) aims to extract the values of certain types of attributes (or slots, such as person:cities\\_of\\_residence) for a given entity from a large collection of source documents. In this paper we propose an effective DNN architecture for SF with the following new strategies: (1). Take a regularized dependency graph instead of a raw sentence as input to DNN, to compress the wide contexts between query and candidate filler; (2). Incorporate two attention mechanisms: local attention learned from query and candidate filler, and global attention learned from external knowledge bases, to guide the model to better select indicative contexts to determine slot type. Experiments show that this framework outperforms state-of-the-art on both relation extraction (16\\% absolute F-score gain) and slot filling validation for each individual system (up to 8.5\\% absolute F-score gain).", "histories": [["v1", "Tue, 4 Jul 2017 17:18:50 GMT  (409kb,D)", "http://arxiv.org/abs/1707.01075v1", "EMNLP'2017"]], "COMMENTS": "EMNLP'2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lifu huang", "avirup sil", "heng ji", "radu florian"], "accepted": true, "id": "1707.01075"}, "pdf": {"name": "1707.01075.pdf", "metadata": {"source": "CRF", "title": "Improving Slot Filling Performance with Attentive Neural Networks on Dependency Structures", "authors": ["Lifu Huang", "Avirup Sil"], "emails": ["huangl7@rpi.edu", "avi@us.ibm.com", "jih@rpi.edu", "raduf@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "The goal of Slot Filling (SF) is to extract pre-defined types of attributes or multiple classifications for each type of set (e.g. per: 2016 residence types) for a particular query entity from a large collection of documents. The slot filler (attribute value) can be an entity, time expression or value (e.g. per: fees).The TAC-KBP slot filling task (Ji et al., 2011a; Surdeanu and Ji, 2014) defined 41 slot types, including 25 types for person and 16 types for organization.A critical component of slot filling is the relation extraction, namely the relation between a query entity and candidate slot filler in one of the 41 types or none. Most previous studies have treated SF in the same way as within the set relationship tasks in ACE 1 or SemEval (Hendrickx et al, 2009) They created training data based on remote crowd or outsourcing."}, {"heading": "2 Architecture Overview", "text": "Figure 2 illustrates the pipeline of an SF system. Faced with a query and a source corpus, the system retrieves related documents, identifies candidate fillers (including entities, time, values, and titles), extracts the relationship between the query and each candidate filler that occurs in the same sentence, and finally determines the filler for each slot. Candidate extraction plays a critical role in such an SF pipeline. In this work, we focus on the relationship extraction component and design a neural architecture. Faced with a query, candidate filler, and a sentence, we first construct a regulated dependency graph and take all < governor, dependent > word pairs as input into Convolutional Neural Networks (CNN). In addition, we design two attention mechanisms: (1) Local attention that uses the concatenation of query and candidate filter vectors to measure the relationship of each input rams as the filter width and the filter type we set."}, {"heading": "3 Regularized Dependency Graph based CNN", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Regularized Dependency Graph", "text": "Dependence analysis based features, especially the shortest dependence path between two entities, have proven effective in extracting key information to identify the relationship between two entities (Bunescu and Mooney, 2005; Zhao and Grishman, 2005; GuoDong et al., 2005; Jiang et al., 2015) Several recent studies have also investigated how a dependence path can be transformed into a sequence and applied neural networks are applied to the relation classification sequence (Liu et al., 2015; Xu et al., 2015).However, the shortest dependence path between query and candidate filler is not always sufficient to deduce the slot type for relation classification (Liu et al., 2015; Cai et al., 2016; Xu et al., 2015).However, for SF, the shortest dependence path between query type and candidate type, it is not sufficient to apply the query path between two candidate types."}, {"heading": "3.2 Graph based CNN", "text": "Previous work (Adel et al., 2016) divided an input set into three parts based on the positions of the query and the candidate filler, and created a feature vector for each part using a common CNN. To compress the broad contexts rather than taking the raw sentence as input directly, we split the regulated dependency graph into three parts: query-related subgraph, candidate-filler-related subgraph, and the dependency path between the query and filler. Each subgraph is taken as input to a CNN, as shown in Figure 2. We now describe the details of each part as the following. Input Layer: Each subgraph or path-related dependency graph is represented as a set of dependent word pairs G = {< g1 >, d1 >, < g2, d2 >, d2 >, d2 >, gn, dn >}. Here, a di-dependent vector and each word is represented accordingly."}, {"heading": "4 Attention Strategies for SF", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Local Attention", "text": "The basic idea of the attention mechanism is to assign a weight to each position of a lower layer when calculating the representations for an upper layer so that the model can pay attention to specific regions (Bahdanau et al., 2014). In SF, the indicative words are the most meaningful information the model should pay attention to. Wang et al. (2016) applied the entities \"attention directly to determine the most influential parts of the input set. By the same intuition, we apply the attention from the query and candidate filler instead of input to the folding output to prevent information from disappearing during the folding process (Yin et al., 2016). Figure 4 illustrates our approach to incorporating local attention. We first associate the vector of the q query and the candidate filler f using pre-trained embeddings v = [vq, vf], R2d, [matt, several words] in the single [matt] words."}, {"heading": "4.2 Global Attention", "text": "Considering E1 in section 1, the most discriminating word is not only related to the query and filler, but also to the way in which it is used. (2016) It aims to identify the query and filler in terms of context. (In most cases, the training data is not balanced and some relation types cannot be associated with high-quality vectors.) Thus, we have developed two methods to generate read slot types in terms of representations. (2016) It is about capturing slot types with limited slot types. (2016) It is about generating pre-read slot types in terms of representations. (First) We compose pre-read lexical word embedded type types in terms of each slot type name name name name name name name name types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types. (2016)"}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Data", "text": "For the model training, Angeli et al. (2014) have created some high-quality clean annotations for SF based on crowd sourcing2. In addition, Adel et al. (2016) have automatically generated a larger amount of noisy training data based on remote monitoring, including approximately 1,725,891 positive training instances for 41 slot types. We have manually evaluated the accuracy of the identification of filler candidates and their slot type annotations and extracted a portion of their noisy annotations and combined them with the clean annotations. Ultimately, we receive 23,993 positive and 3,000 negative training instances for all slot types. We evaluate our approach in two settings: (1) Relation extraction for all slot types taking into account the limitations of query and candidate fillers. We use a script3 to generate a test set (4892 instances) from KBP 2012 / 2013 slot fill data with manual evaluation."}, {"heading": "5.2 Relation Extraction", "text": "In addition, we also design several variants to demonstrate the effectiveness of each component in2http: / / nlp.stanford.edu / software / mimlre-2014-07-17data.tar.gz3http: / / cistern.cis.lmu.de.our approach. Table 2 presents the detailed approaches and features used by these methods.We report results with Macro F1 and Micro F1. Macro F1 is calculated from the average precision and retrieval of all types, while Micro F1 is calculated from general precision and retrieval, which is more useful when the size of each category varies. Table 3 shows the comparison results on relation extraction. We can see that by incorporating the shortest dependence path or the regulated dependence curve into neural networks, the model can achieve more than 13% micro F score by incorporating the previously widely used methods through the state of the art."}, {"heading": "5.3 Slot Filling Validation", "text": "In the task TAC-KBP 2013 Slot Filling Validation (SFV) (Ji et al., 2011b), there are 100 queries. First, we retrieve the sentences from the source corpus (approximately 2,099,319 documents) and identify the query and candidate filler based on the offsets generated by each answer. Then, we apply our approach to recalculate the slot type. Figure 6 shows the F values based on our approach and the original system. In a multi-pass system, we select one for comparison. We can see that our approach consistently improves the performance of almost all SF systems in an absolute profit range of [-0.18%, 8.48%]. Analyzing each system run, we find that our approach can provide more returns to SF systems that have lower precision. Previous studies (Tamang and Ji, 2011; Rodriguez et al., 2015; Zhi et al., 2015; Visathan et al.)."}, {"heading": "5.4 Detailed Analysis", "text": "Significance Test: Table 3 shows the results of several variants of our approach. To demonstrate the difference between the results of these approaches, some are not random, we sample 10 subsets (each containing 500 cases) from the test records and perform paired t-test between each of these two approaches over these 10 records to check if the average difference in their performance is significantly different or not. Table 6 shows the distribution of training data and the F score of each type. We can see that for some slot types, such as pro: date of birth and pro: age, the total types of training data for each slot type. Table 4 shows the distribution of training data and the F-score of each type. We can see that for some slot types, such as pro: date of birth and pro: age, the total types of their candidates are easy to learn and distinguish from other slot types."}, {"heading": "6 Related Work", "text": "A major challenge for SF is the lack of labeled data to generalize a wide range of features and patterns, especially for slot types located in the long tail of fairly distorted distribution of slot fills (Ji et al., 2011a). Previous work has largely focused on offsetting the need for data by creating patterns (Sun et al., 2011; Roth et al., 2014b), automatic annotation by remote supervision (Surdeanu et al., 2011; Roth et al., 2014a; Adel et al., 2016) and the creation of trigger lists for unattended dependency graphs (Yu and Ji, 2016). Some work (Rodriguez et al., 2015; 4http: / www.geonames.org / Zhi et al., 2015; Viswanathan et al., 2015; Rajani and Mooney al."}, {"heading": "7 Conclusions and Future Work", "text": "In this paper, we discussed the unique challenges of slot filling compared to traditional relation extraction tasks. We designed a regulated dependency diagram based on neural architecture for slot filling. By incorporating local and global attention mechanisms, this approach can better capture indicative contexts. Experiments on relation extraction and slot filling validation demonstrate the effectiveness of our neural architecture. In the future, we will combine additional rules, patterns and constraints from the slot filling task with DNN techniques to further improve slot filling."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Slot Filling (SF) aims to extract the values of certain types of attributes (or slots, such as person:cities of residence) for a given entity from a large collection of source documents. In this paper we propose an effective DNN architecture for SF with the following new strategies: (1). Take a regularized dependency graph instead of a raw sentence as input to DNN, to compress the wide contexts between query and candidate filler; (2). Incorporate two attention mechanisms: local attention learned from query and candidate filler, and global attention learned from external knowledge bases, to guide the model to better select indicative contexts to determine slot type. Experiments show that this framework outperforms state-of-the-art on both relation extraction (16% absolute F-score gain) and slot filling validation for each individual system (up to 8.5% absolute Fscore gain).", "creator": "LaTeX with hyperref package"}}}