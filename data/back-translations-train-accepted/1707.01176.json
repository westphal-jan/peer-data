{"id": "1707.01176", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2017", "title": "Charmanteau: Character Embedding Models For Portmanteau Creation", "abstract": "Portmanteaus are a word formation phenomenon where two words are combined to form a new word. We propose character-level neural sequence-to-sequence (S2S) methods for the task of portmanteau generation that are end-to-end-trainable, language independent, and do not explicitly use additional phonetic information. We propose a noisy-channel-style model, which allows for the incorporation of unsupervised word lists, improving performance over a standard source-to-target model. This model is made possible by an exhaustive candidate generation strategy specifically enabled by the features of the portmanteau task. Experiments find our approach superior to a state-of-the-art FST-based baseline with respect to ground truth accuracy and human evaluation.", "histories": [["v1", "Tue, 4 Jul 2017 23:11:52 GMT  (262kb,D)", "https://arxiv.org/abs/1707.01176v1", "Accepted for publication in EMNLP 2017"], ["v2", "Mon, 24 Jul 2017 16:27:55 GMT  (262kb,D)", "http://arxiv.org/abs/1707.01176v2", "Accepted for publication in EMNLP 2017"]], "COMMENTS": "Accepted for publication in EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["varun gangal", "harsh jhamtani", "graham neubig", "eduard h hovy", "eric nyberg"], "accepted": true, "id": "1707.01176"}, "pdf": {"name": "1707.01176.pdf", "metadata": {"source": "CRF", "title": "CharManteau: Character Embedding Models For Portmanteau Creation", "authors": ["Varun Gangal", "Harsh Jhamtani", "Graham Neubig", "Eduard Hovy", "Eric Nyberg"], "emails": ["vgangal@cs.cmu.edu", "jharsh@cs.cmu.edu", "gneubig@cs.cmu.edu", "hovy@cs.cmu.edu", "ehn@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Portmanteaus (or lexical mixtures Algeo (1977)) are novel words formed from parts of several root words to indicate a new concept that otherwise cannot be expressed precisely. Portmanteaus have become more common in modern social media, news reports and advertising. \u2022 A popular example is Brexit (UK + Exit). Petri (2012). These are not only found in English, but also in many other languages such as Bahasa Indonesia Dardjowidjojo (1979), Modern Hebrew BatEl (1996); Berman (1989) and Spanish Pin eros (2004). Their short length makes them ideal for headlines and brand names (Gabler, 2015). Unlike more defined morphological phenomena such as inflection and derivation, Portmanteau Generation * designates equivalent success programs using a set of rules. For example, Shaw et al. (2014) we note that the composition of Portmanteau words depends on several important factors."}, {"heading": "2 Proposed Models", "text": "This section describes our neural models."}, {"heading": "2.1 Forward Architecture", "text": "Under our first proposed architecture, the input sequence is x = concat (x (1), \",\" x (2), \"while the output sequence is portmanteau y. The model learns the distribution P (y | x).The network architecture we use is an attentive S2S model (Bahdanau et al., 2014). We use a bi-directional encoder that is known to work well for S2S problems with similar token order, which is true in our case. Let's \u2212 \u2212 \u2212 LSTM and \u2190 \u2212 LSTM represent the forward and backward encoder; eenc () and edec () represent the character sequence used by encoders and decoders. The following equations describe the model: h \u2212 enc 0 = \u2212 enc = \u2212 enc (enc), h = enc = (enc), c = (enc), c (enc)."}, {"heading": "2.2 Backward Architecture", "text": "The second proposed model uses Bayes \"rule to reverse the probabilities P (y | x) = P (x | y) P (y) P (x) P (x) = argmaxy P (x | y) P (y). Thus, we have an inverted model of probability P (x | y) that the given root words were generated from the portmanteau and a sign language model P (y), which is a probability distribution across all strings y (A), where A is the alphabet of the language. This type of factoring of probability is also known as a noisy channel model, which has recently proven effective for neuronal MT (Hoang et al. (2017), Yu et al. (2016). Such a model offers two advantages1. The reverse direction model (or alignment model) gives a higher probability to those vocabularies from which the root words can be easily distinguished."}, {"heading": "3 Making Predictions", "text": "Given these models, we have to make predictions about what we will do with two methods: Greedy Decoding: In most models of the neural sequence, we greedily decode the next character based on the probability distribution of the next character in the current time step. We call this decoding strategy GREEDY. Exhausting Generation: Many portmanteaus have been observed as concatenating a prefix of the first word and a suffix of the second word. Therefore, we generate all candidate results that follow this rule. Afterwards, we evaluate these candidates with the decoder and issue the one with the maximum score. We call this decoding strategy SCORE.Given the small size of our training data, we expect Ensembling (Breiman, 1996) to reduce the variance of the model and improve performance. In this essay, we group our models by training several models on 80% partial samples of the training data and capturing probability values on average across the whole ensemble."}, {"heading": "4 Dataset", "text": "The existing dataset of Deri and Knight (2015) contains 401 Portmanteau examples from Wikipedia. We call this dataset DWiki. Apart from being small for detailed analysis, DWiki is distorted by the fact that it comes from only one source. We manually collect DLarge, a dataset of 1624 different English portmantefrom the following sources: \u2022 Urban Dictionary2 \u2022 Wikipedia \u2022 Wiktionary \u2022 BCU's Neologism Lists from '94 to' 12. Of course, DWiki and DLarge. We define DBlind = DLarge \u2212 DWiki as a dataset of 1223 examples, not from Wikipedia. We observed that 84.7% of the words in DLarge can be generated by concatenating the first word with a suffix of the second. 2Not all neologisms are portmanteaus, so we manually select those that are suitable for our dataset."}, {"heading": "5 Experiments", "text": "In this section we show results comparing different configurations of our model with the base model FST of Deri and Knight (2015) (BASELINE), which are evaluated on the basis of exact matches and average Levenshtein Editdistance (Distance) w.r.t Ground Truth."}, {"heading": "5.1 Objective Evaluation Results", "text": "In Experiment 1, we follow the same setup as Deri and Knight (2015). DWiki is divided into 10 folds. Each folded model uses 8 folds for training, 1 for validation and 1 for testing, and then the average (10-fold cross validation method) performance indicators for the test fold are evaluated. Table 1 shows the results of Experiment 1 for different model configurations. We get the BASELINE numbers from Deri and Knight (2015). Our best model receives 48.75% matches and 1.12 distances, compared to 45.39% matches and 1.59 distances using BASELINE. For Experiment 2, we try to compare our best approaches from Experiment 1 to BASELINE on a large, pre-stored dataset. Each model is trained on DWiki and tested for DBlind. BASELINE has been trained similarly, so it shows a fair comparison. Table 2 shows the results from sequence 3. Our best model gets a distance of 1.96 to 2.32."}, {"heading": "5.1.1 Performance on Uncovered Examples", "text": "This applies to 229 of 1223 examples in DBlind. We compare the FORWARD approach together with a GREEDY decoding strategy with the BASELINE approach for these examples. Both FORWARD + GREEDY and the BASELINE get 0 matches in these examples. The distance for these examples is 4.52 for BASELINE and 4.09 for FORWARD + GREEDY. This means that one of our approaches (FORWARD + GREEDY) even exceeds BASELINE in these examples."}, {"heading": "5.2 Significance Tests", "text": "Since our data set is still relatively small (1223 examples), it is crucial to verify whether BACKWARD is actually statistically significantly better than BASELINE in terms of matches. To achieve this, we use a paired Bootstrap 4 comparison (Koehn, 2004) between BACKWARD and BASELINE in terms of matches. BACKWARD is better (receives more matches) than BASELINE in 99.9% (p = 0.999) of subsets. Likewise, BACKWARD has a smaller distance than BASELINE, with a margin of 0.2 in 99.5% (p = 0.995) of subsets."}, {"heading": "5.3 Subjective Evaluation and Analysis", "text": "In order to compare the faults of our model with the baseline, we designed and performed a human assessment task on AMT.5 In the survey, we show the results of human annotators from our system and baseline. We ask them to judge which alternative is better overall based on the following criteria: 1. It is a good abbreviation for two original words 2. It sounds better. We requested annotations on a scale of 1-4. To avoid bias, we have shifted the order of two portmanteaus between our system and the baseline. We limit annotators to being from Anglophone countries, have a HIT approval rate > 80% and pay $0.40 per HIT (5 questions per HIT).As shown in Table 4, the output from our system was better described by humans than in the initial 12% for different models."}, {"heading": "6 Related Work", "text": "O \u00a4zbal and Strapparava (2012) generate new words to describe a product based on its category and characteristics. However, their method is limited to handmade rules compared to our data-driven ap-4We averages above M = 1000 randomly selected subsets of DBlind, each of which is size N = 611 (\u2248 1223 / 2). 5We avoid comparisons with the truth because commentators may tend to lean towards the truth due to their existing popularity. They also focus on brand names. Hiranandani et al. (2017) have proposed an approach to recommending brand names based on brand / product description, but consider only a limited number of features such as memorability and readability. Smith et al. (2014) develop an approach to creating portmanteaus that requires user-defined weights for attributes to sound good."}, {"heading": "7 Conclusion", "text": "Our experiments demonstrate the effectiveness of the proposed system in predicting portmanteau using stem words. We conclude that embedding characters in the English vocabulary before training helps the model. By human evaluation, we show that the predictions of our model are superior to the baseline. In addition, we have published our data set and code6 to promote further research on the Portmanteau phenomenon. We also publish an online demo 7 in which our trained model for Portmanteau suggestions can be queried. An obvious extension of our work is to try out similar models in several languages."}, {"heading": "Acknowledgements", "text": "We thank Dongyeop Kang, David Mortensen, Qinlan Shen and anonymous reviewers for their valuable comments. This research was sup-6https: / / github.com / vgtomahawk / Charmanteau-CamReady7http: / / tinyurl.com / y9x6mvyported partly funded by DARPA grant FA8750-12-20342 under the DEFT program."}], "references": [{"title": "Blends, a structural and systemic view", "author": ["John Algeo."], "venue": "American speech 52(1/2):47\u201364.", "citeRegEx": "Algeo.,? 1977", "shortCiteRegEx": "Algeo.", "year": 1977}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Selecting the best of the worst: the grammar of Hebrew blends", "author": ["Outi Bat-El."], "venue": "Phonology 13(03):283\u2013328.", "citeRegEx": "Bat.El.,? 1996", "shortCiteRegEx": "Bat.El.", "year": 1996}, {"title": "The role of blends in Modern Hebrew word-formation", "author": ["Ruth Berman."], "venue": "Studia linguistica et orientalia memoriae Haim Blanc dedicata. Wiesbaden: Harrassowitz pages 45\u201361.", "citeRegEx": "Berman.,? 1989", "shortCiteRegEx": "Berman.", "year": 1989}, {"title": "Bagging predictors", "author": ["Leo Breiman."], "venue": "Machine learning 24(2):123\u2013140.", "citeRegEx": "Breiman.,? 1996", "shortCiteRegEx": "Breiman.", "year": 1996}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv:1603.06147 .", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Acronymic Patterns in Indonesian", "author": ["Soenjono Dardjowidjojo."], "venue": "Pacific Linguistics Series C 45:143\u2013 160.", "citeRegEx": "Dardjowidjojo.,? 1979", "shortCiteRegEx": "Dardjowidjojo.", "year": 1979}, {"title": "How to make a frenemy: Multitape FSTs for portmanteau generation", "author": ["Aliya Deri", "Kevin Knight."], "venue": "Proceedings of NAACL-HLT . pages 206\u2013 210.", "citeRegEx": "Deri and Knight.,? 2015", "shortCiteRegEx": "Deri and Knight.", "year": 2015}, {"title": "Morphological Inflection Generation using Character Sequence to Sequence Learning", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Graham Neubig", "Chris Dyer."], "venue": "Proceedings of NAACL-HLT . pages 634\u2013 643.", "citeRegEx": "Faruqui et al\\.,? 2016", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "The Weird Science of Naming New Products", "author": ["Neal Gabler."], "venue": "New York Times - http://tinyurl. com/lmlq7ex .", "citeRegEx": "Gabler.,? 2015", "shortCiteRegEx": "Gabler.", "year": 2015}, {"title": "Generating appealing brand names", "author": ["Gaurush Hiranandani", "Pranav Maneriker", "Harsh Jhamtani."], "venue": "arXiv preprint arXiv:1706.09335 .", "citeRegEx": "Hiranandani et al\\.,? 2017", "shortCiteRegEx": "Hiranandani et al\\.", "year": 2017}, {"title": "Decoding as Continuous Optimization in Neural Machine Translation", "author": ["Cong Duy Vu Hoang", "Gholamreza Haffari", "Trevor Cohn."], "venue": "arXiv:1701.02854 .", "citeRegEx": "Hoang et al\\.,? 2017", "shortCiteRegEx": "Hoang et al\\.", "year": 2017}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "EMNLP. pages 388\u2013395.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black."], "venue": "arXiv:1511.04586 .", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "A computational approach to the automation of creative naming", "author": ["G\u00f6zde \u00d6zbal", "Carlo Strapparava."], "venue": "Proceedings of ACL. Association for Computational Linguistics, pages 703\u2013711.", "citeRegEx": "\u00d6zbal and Strapparava.,? 2012", "shortCiteRegEx": "\u00d6zbal and Strapparava.", "year": 2012}, {"title": "Say No to Portmanteaus", "author": ["Alexandra Petri."], "venue": "Washington Post - http://tinyurl.com/kvmep2t .", "citeRegEx": "Petri.,? 2012", "shortCiteRegEx": "Petri.", "year": 2012}, {"title": "The creation of portmanteaus in the extragrammatical morphology of Spanish", "author": ["Carlos-Eduardo Pi\u00f1eros."], "venue": "Probus 16(2):203\u2013240.", "citeRegEx": "Pi\u00f1eros.,? 2004", "shortCiteRegEx": "Pi\u00f1eros.", "year": 2004}, {"title": "Emergent faithfulness to morphological and semantic heads in lexical blends", "author": ["Katherine E Shaw", "Andrew M White", "Elliott Moreton", "Fabian Monrose."], "venue": "Proceedings of the Annual Meetings on Phonology. volume 1.", "citeRegEx": "Shaw et al\\.,? 2014", "shortCiteRegEx": "Shaw et al\\.", "year": 2014}, {"title": "Nehovah: A neologism creator nomen ipsum", "author": ["Michael R Smith", "Ryan S Hintze", "Dan Ventura."], "venue": "Proceedings of the International Conference on Computational Creativity. pages 173\u2013181.", "citeRegEx": "Smith et al\\.,? 2014", "shortCiteRegEx": "Smith et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "The CMU pronunciation dictionary, release 0.6", "author": ["R Weide"], "venue": null, "citeRegEx": "Weide.,? \\Q1998\\E", "shortCiteRegEx": "Weide.", "year": 1998}, {"title": "The Neural Noisy Channel", "author": ["Lei Yu", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Tomas Kocisky."], "venue": "arXiv:1611.02554 .", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Multi-source neural translation", "author": ["Barret Zoph", "Kevin Knight."], "venue": "arXiv:1601.00710 .", "citeRegEx": "Zoph and Knight.,? 2016", "shortCiteRegEx": "Zoph and Knight.", "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "Their short length makes them ideal for headlines and brandnames (Gabler, 2015).", "startOffset": 65, "endOffset": 79}, {"referenceID": 0, "context": "Portmanteaus (or lexical blends Algeo (1977)) are novel words formed from parts of multiple root words in order to refer to a new concept which can\u2019t otherwise be expressed concisely.", "startOffset": 32, "endOffset": 45}, {"referenceID": 0, "context": "Portmanteaus (or lexical blends Algeo (1977)) are novel words formed from parts of multiple root words in order to refer to a new concept which can\u2019t otherwise be expressed concisely. Portmanteaus have become frequent in modern-day social media, news reports and advertising, one popular example being Brexit (Britain + Exit). Petri (2012). These are found not only in English but many other languages such as Bahasa Indonesia Dardjowidjojo (1979), Modern Hebrew BatEl (1996); Berman (1989) and Spanish Pi\u00f1eros (2004).", "startOffset": 32, "endOffset": 340}, {"referenceID": 0, "context": "Portmanteaus (or lexical blends Algeo (1977)) are novel words formed from parts of multiple root words in order to refer to a new concept which can\u2019t otherwise be expressed concisely. Portmanteaus have become frequent in modern-day social media, news reports and advertising, one popular example being Brexit (Britain + Exit). Petri (2012). These are found not only in English but many other languages such as Bahasa Indonesia Dardjowidjojo (1979), Modern Hebrew BatEl (1996); Berman (1989) and Spanish Pi\u00f1eros (2004).", "startOffset": 32, "endOffset": 448}, {"referenceID": 0, "context": "Portmanteaus (or lexical blends Algeo (1977)) are novel words formed from parts of multiple root words in order to refer to a new concept which can\u2019t otherwise be expressed concisely. Portmanteaus have become frequent in modern-day social media, news reports and advertising, one popular example being Brexit (Britain + Exit). Petri (2012). These are found not only in English but many other languages such as Bahasa Indonesia Dardjowidjojo (1979), Modern Hebrew BatEl (1996); Berman (1989) and Spanish Pi\u00f1eros (2004).", "startOffset": 32, "endOffset": 476}, {"referenceID": 0, "context": "Portmanteaus (or lexical blends Algeo (1977)) are novel words formed from parts of multiple root words in order to refer to a new concept which can\u2019t otherwise be expressed concisely. Portmanteaus have become frequent in modern-day social media, news reports and advertising, one popular example being Brexit (Britain + Exit). Petri (2012). These are found not only in English but many other languages such as Bahasa Indonesia Dardjowidjojo (1979), Modern Hebrew BatEl (1996); Berman (1989) and Spanish Pi\u00f1eros (2004).", "startOffset": 32, "endOffset": 491}, {"referenceID": 0, "context": "Portmanteaus (or lexical blends Algeo (1977)) are novel words formed from parts of multiple root words in order to refer to a new concept which can\u2019t otherwise be expressed concisely. Portmanteaus have become frequent in modern-day social media, news reports and advertising, one popular example being Brexit (Britain + Exit). Petri (2012). These are found not only in English but many other languages such as Bahasa Indonesia Dardjowidjojo (1979), Modern Hebrew BatEl (1996); Berman (1989) and Spanish Pi\u00f1eros (2004). Their short length makes them ideal for headlines and brandnames (Gabler, 2015).", "startOffset": 32, "endOffset": 518}, {"referenceID": 16, "context": "For instance, Shaw et al. (2014) state that the composition of the portmanteau from its root words depends on several factors, two important ones being maintaining prosody and retaining character segments from the root words, especially the head.", "startOffset": 14, "endOffset": 33}, {"referenceID": 7, "context": "An existing work by Deri and Knight (2015) aims to solve the problem of predicting portmanteau using a multi-tape FST model, which is datadriven, unlike prior approaches.", "startOffset": 20, "endOffset": 43}, {"referenceID": 8, "context": "Prior works, such as Faruqui et al. (2016), have demonstrated the efficacy of neural approaches for morphological tasks such as inflection.", "startOffset": 21, "endOffset": 43}, {"referenceID": 7, "context": "In experiments (\u00a75), our model performs better than the baseline Deri and Knight (2015) on both objective and subjective measures, demonstrating that such methods can be used effectively in a morphological task.", "startOffset": 65, "endOffset": 88}, {"referenceID": 1, "context": "The network architecture we use is an attentional S2S model (Bahdanau et al., 2014).", "startOffset": 60, "endOffset": 83}, {"referenceID": 11, "context": "This way of factorizing the probability is also known as a noisy channel model, which has recently also been shown to be effective for neural MT (Hoang et al. (2017), Yu et al.", "startOffset": 146, "endOffset": 166}, {"referenceID": 11, "context": "This way of factorizing the probability is also known as a noisy channel model, which has recently also been shown to be effective for neural MT (Hoang et al. (2017), Yu et al. (2016)).", "startOffset": 146, "endOffset": 184}, {"referenceID": 20, "context": "1 Specifically in our experiments, 134K words from the CMU dictionary (Weide, 1998).", "startOffset": 70, "endOffset": 83}, {"referenceID": 4, "context": "Given that our training data is small in size, we expect ensembling (Breiman, 1996) to help reduce model variance and improve performance.", "startOffset": 68, "endOffset": 83}, {"referenceID": 7, "context": "The existing dataset by Deri and Knight (2015) contains 401 portmanteau examples from Wikipedia.", "startOffset": 24, "endOffset": 47}, {"referenceID": 7, "context": "In this section, we show results comparing various configurations of our model to the baseline FST model of Deri and Knight (2015) (BASELINE).", "startOffset": 108, "endOffset": 131}, {"referenceID": 7, "context": "In Experiment 1, we follow the same setup as Deri and Knight (2015). DWiki is split into 10 folds.", "startOffset": 45, "endOffset": 68}, {"referenceID": 7, "context": "In Experiment 1, we follow the same setup as Deri and Knight (2015). DWiki is split into 10 folds. Each fold model uses 8 folds for training, 1 for validation, and 1 for test. The average (10 fold crossvalidation style approach) performance metrics on the test fold are then evaluated. Table 1 shows the results of Experiment 1 for various model configurations. We get the BASELINE numbers from Deri and Knight (2015). Our best model obtains 48.", "startOffset": 45, "endOffset": 418}, {"referenceID": 7, "context": "For BASELINE (Deri and Knight, 2015), we use their trained model from http://leps.", "startOffset": 13, "endOffset": 36}, {"referenceID": 12, "context": "In order to do this, we use a paired bootstrap4 comparison (Koehn, 2004) between BACKWARD and BASELINE in terms of Matches.", "startOffset": 59, "endOffset": 72}, {"referenceID": 19, "context": "Recently, neural approaches have been used for S2S problems (Sutskever et al., 2014) such as MT.", "startOffset": 60, "endOffset": 84}, {"referenceID": 9, "context": "Hiranandani et al. (2017) have proposed an approach to recommend brand names based on brand/product description.", "startOffset": 0, "endOffset": 26}, {"referenceID": 9, "context": "Hiranandani et al. (2017) have proposed an approach to recommend brand names based on brand/product description. However, they consider only a limited number of features like memorability and readability. Smith et al. (2014) devise an approach to generate portmanteaus, which requires user-defined weights for attributes like sounding good.", "startOffset": 0, "endOffset": 225}, {"referenceID": 9, "context": "Hiranandani et al. (2017) have proposed an approach to recommend brand names based on brand/product description. However, they consider only a limited number of features like memorability and readability. Smith et al. (2014) devise an approach to generate portmanteaus, which requires user-defined weights for attributes like sounding good. Generating a portmanteau from two root words can be viewed as a S2S problem. Recently, neural approaches have been used for S2S problems (Sutskever et al., 2014) such as MT. Ling et al. (2015) and Chung et al.", "startOffset": 0, "endOffset": 534}, {"referenceID": 5, "context": "(2015) and Chung et al. (2016) have shown that character-level neural sequence models work as well as word-level ones for language modelling and MT.", "startOffset": 11, "endOffset": 31}, {"referenceID": 5, "context": "(2015) and Chung et al. (2016) have shown that character-level neural sequence models work as well as word-level ones for language modelling and MT. Zoph and Knight (2016) propose S2S models for multi-source MT, which have multi-sequence inputs, similar to our case.", "startOffset": 11, "endOffset": 172}], "year": 2017, "abstractText": "Portmanteaus are a word formation phenomenon where two words are combined to form a new word. We propose character-level neural sequence-tosequence (S2S) methods for the task of portmanteau generation that are end-toend-trainable, language independent, and do not explicitly use additional phonetic information. We propose a noisy-channelstyle model, which allows for the incorporation of unsupervised word lists, improving performance over a standard sourceto-target model. This model is made possible by an exhaustive candidate generation strategy specifically enabled by the features of the portmanteau task. Experiments find our approach superior to a state-of-the-art FST-based baseline with respect to ground truth accuracy and human evaluation.", "creator": "LaTeX with hyperref package"}}}