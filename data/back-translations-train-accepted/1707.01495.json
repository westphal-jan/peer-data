{"id": "1707.01495", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jul-2017", "title": "Hindsight Experience Replay", "abstract": "Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum.", "histories": [["v1", "Wed, 5 Jul 2017 17:55:53 GMT  (1023kb,D)", "http://arxiv.org/abs/1707.01495v1", null], ["v2", "Mon, 10 Jul 2017 18:35:33 GMT  (1023kb,D)", "http://arxiv.org/abs/1707.01495v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE cs.RO", "authors": ["marcin", "rychowicz", "filip wolski", "alex ray", "jonas schneider", "rachel fong", "peter welinder", "bob mcgrew", "josh tobin", "pieter abbeel", "wojciech zaremba"], "accepted": true, "id": "1707.01495"}, "pdf": {"name": "1707.01495.pdf", "metadata": {"source": "CRF", "title": "Hindsight Experience Replay", "authors": ["Marcin Andrychowicz", "Filip Wolski", "Alex Ray", "Jonas Schneider", "Rachel Fong", "Peter Welinder", "Bob McGrew", "Josh Tobin", "Pieter Abbeel", "Wojciech Zaremba"], "emails": ["marcin@openai.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to play by the rules that they have established over the past few years, and that they are able to play by the rules, \"he said.\" But it's not as if they have been able to play by the rules, \"he said.\" But it's not as if they have been able to play by the rules. \""}, {"heading": "2 Background", "text": "In this section we present the formalism of reinforcement learning used in the work, as well as RL algorithms we use in our experiments."}, {"heading": "2.1 Reinforcement Learning", "text": "To simplify the representation, we assume that the environment is fully observable. An environment is described by a set of states S, a set of measures A, a distribution of initial states p (s0), a reward function r: S \u00b7 A \u2192 R, transition probabilities p (st + 1 | st, at), and a discount factor \u03b3 [0, 1]. A deterministic policy is an assignment of states to measures: \u03c0: S \u2192 A. Each episode begins with a sample of an initial state s0. At any time, the actor produces an action based on the current state: at = \u03c0 (st). Then he receives the reward rt = r (st, at) and the new state of the environment is sampled from the distribution p (\u00b7 | st, at)."}, {"heading": "2.2 Deep Q-Networks (DQN)", "text": "Deep Q-Networks (DQN) (Mnih et al., 2015) is a model-free RL algorithm for discrete action spaces. Here, we outline it informally only, see Mnih et al. (2015) for more details. In DQN, we maintain a neural network Q that resembles Q *. A greedy policy w.r.t. Q is defined as \u03c0Q (s) = argmaxa AQ (s, a). A -greedy policy w.r.t. Q is a policy that most likely takes a random action (uniformly sampled from A) and the action \u03c0Q (s) with probability 1 \u2212. During the training, we generate episodes with -greedy politics w.r.t. The current approximation of the action value function Q. The transition stages (st, at, rt, st + 1) that occur during the training are stored in the so-called replay buffer. The generation of new episodes with the neural network function is broken with the rule of the Q."}, {"heading": "2.3 Deep Deterministic Policy Gradients (DDPG)", "text": "Deep Deterministic Policy Gradients (DDPG) (Lillicrap et al., 2015) is a modelless RL algorithm for continuous action. Here we outline it informally only, see Lillicrap et al. (2015) for more details. In DDPG we maintain two neural networks: a target policy (also referred to as an actor) \u03c0: S \u2192 A and an action value function approximator (referred to as a critic) Q: S \u00b7 A \u2192 R. The task of the critic is to approximate the action value function of the actor Q\u03c0. Episodes are generated using a behavior policy that is a loud version of the target policy, e.g. \u03c0b (s) = \u03c0 (s) + N (0, 1). The critic is trained in a similar way to the Q function in the DQN, but the goals yt are calculated based on actions executed by the actor, i.e. yt = rt (Q + 1)."}, {"heading": "2.4 Universal Value Function Approximators (UVFA)", "text": "The Universal Value Function Approximators (UVFA) (Schaul et al., 2015a) are an extension of the DQN to the setup in which there is more than one goal that we can try to achieve. Let G be the space of possible goals. Each target g-G corresponds to a reward function rg: S \u00b7 A \u2192 R. Each episode begins with the sampling of a state-target pair from a distribution p (s0, g). The target remains fixed for the entire episode. At each time step, the agent receives as input not only the current state, but also the current target p: S \u00b7 G \u2192 A and receives the reward rt = rg (st, at). The Q function now depends not only on a state-action pair, but also on a target Q\u03c0 (st, at, g) = E [Rt | st, at, g]. Schaul et al. (2015a) show that in this setup it is possible to train an approximator to the Q function directly from the Rt | t."}, {"heading": "3 Hindsight Experience Replay", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 A motivating example", "text": "Consider an overlapping environment with the state space S = {0, 1} n and the action space A = {0, 1,.., n \u2212 1} for an integer n in which the execution of the i-th action reverses the i-th bit of the state. For each episode, we will uniformly map an initial state as well as a target state and the policy will receive a reward of \u2212 1 as long as it is not in the target state, i.e. RG (s, a) = [s 6 = g]. Standard RL algorithms will inevitably fail in this environment because they will never receive any reward other than \u2212 1. Notice that they use techniques to improve exploration (Houthooft et al., 2016), count-based exploration (Ostrovski et al., 2017) or bootstrapped DQN (Osband et al., 2016) does not help here because the real problem does not consist in the diversity of the states visited."}, {"heading": "3.2 Multi-goal RL", "text": "We are interested in training agents who learn to achieve several different goals. We follow the approach of Universal Value Function Approximators (Schaul et al., 2015a), i.e. we train strategies and value functions that take as input not only a state s, but also a goal g. Furthermore, we show that training an agent to perform multiple tasks can be easier than training just one task (see Sec. 4.3 for details) and therefore our approach can be applicable even if there is only one task we want to perform for the agent (a similar situation has recently been observed by Pinto and Gupta). We assume that any goal g corresponding to a predicate fg: S \u2192 0, 1} and that the objective of the agent is to satisfy fg (s)."}, {"heading": "3.3 Algorithm", "text": "The idea behind Hindsight Experience Replay (HER) is very simple: after experiencing some episodes s0, s1,.. sT we store in the repeat buffer each transition st \u2192 st + 1 not only with the original target that is used for this episode, but also with a subset of other targets. Note that the target that is being pursued affects the actions of the agent, but not the environmental dynamics and therefore we can repeat each track with an arbitrary target, provided that we use an off-policy RL algorithm such as DQN (Mnih et al., 2015), DDPG (Lillicrap et al., 2015), NAF (Gu et al., 2016) or SDQN al. A choice that must be made to use HER, additional targets that are used for the replay."}, {"heading": "4 Experiments", "text": "The video of our experiments is available at the following address: https: / / goo.gl / SMrQnI.This section is structured as follows: In paragraph 4.1 we present multi-goal RL environments that we use both for the experiments and for our training procedures. In paragraph 4.2 we compare the performance of DDPG with and without HER. In paragraph 4.3 we check whether HER improves performance in the one-goal setup. In paragraph 4.4 we analyze the effects of the use of form reward functions. In paragraph 4.5 we compare different strategies for collecting additional targets for HER. In paragraph 4.6 we show the results of experiments on the physical robot."}, {"heading": "4.1 Environments", "text": "It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it is. (it.) It is. (it.) It is. (it. (it is.) It is. (it. (it.) It is. (it. (it.) It is. (it. (it.) It is. (it. (it.) It is. (it. (it.). (it is. (it. (it.) It is. (it. (it.) It is. (it. (it.). (it. (it.) It is. (it. (it. (it.) It is. (it. (it.) It is. (it. (it. (it.) It is. (it. (it. (it.) It is. (it. (it. (it.) It is. (it. (it.) It is. (it. (it. (it.)"}, {"heading": "4.2 Does HER improve performance?", "text": "To check whether HER improves performance, we evaluate DDPG with and without HER on all 3 tasks. In addition, we compare against DDPG with count-based exploration5 (Strehl and Littman, 2005; Kolter and Ng, 2009; Tang et al., 2016; Bellemare et al., 2016; Ostrovski et al., 2017). For HER, we store each transition into the repeat buffer twice: once with the goal of generating the episode and once with the goal of corresponding to the final state of the episode (we call this strategy definitive). In Sec. 4,5, we perform ablation studies of various strategies that S for selecting the targets for the repetition, here we include the best version of Sec. 4,5 in the plot for comparison. From Fig. 3, it is clear that DDPG is not able to solve any of the tasks without HER."}, {"heading": "4.4 How does HER interact with reward shaping?", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "4.6 Deployment on a physical robot", "text": "We developed a strategy for the simulator-trained pick-and-place task (version with the future strategy and k = 4 from paragraph 4.5) and used it on a physical hollow robot with no fine-tuning; the boxing position was predicted with a separately trained CNN using raw Holt Head camera images; for details see Appendix B. Initially, the strategy was successful in 2 out of 5 attempts; it was not robust to small errors in estimating the boxing position, as it was trained to be in perfect condition from the simulation; after retraining the policy with Gaussian sound (std = 1cm), the success rate increased to 5 / 5; the video showing some of the attempts is available at https: / / goo.gl / SMrQnI."}, {"heading": "5 Related work", "text": "The technique of experience review was introduced in Lin (1992) and became very popular after being used in the DQN agent playing Atari (Mnih et al., 2015). Prioritized experience review (Schaul et al., 2015b) is an improvement of experience review that prioritizes transitions in the repeat buffer to speed up training, it is orthogonal to our work and both approaches can easily be combined. Simultaneous learning of strategies for multiple tasks has been extensively researched in the context of policy seeking, e.g. Caruana (1998); Da Silva et al. (2012); Kober et al. (2012); Devin et al. (2016); Pinto and Gupta (2016). Learning outside the political value functions for multiple tasks has been studied by Foster and Dayan (2002) and Sutton et al. (2011) Our work is most strongly based on Schaul et al. (2015a), which considers a neural problem simultaneously, multiple tasks."}, {"heading": "6 Conclusions", "text": "We introduced a new technique called Hindsight Experience Replay, which enables the application of RL algorithms to problems with sparse and binary rewards. Our technique can be combined with any RL algorithm, and we have done this experimentally with DQN and DDPG. We showed that HER training policy enables the objects to be pushed, pushed and placed into predefined positions with a robotic arm, while the Vanilla RL algorithm does not solve these tasks. We also showed that the pick-and-place policy works well on the physical robot without fine-tuning. As far as we know, this is the first time that complicated behavior has been learned with only sparse binary rewards."}, {"heading": "Acknowledgments", "text": "We would like to thank Ankur Handa, Jonathan Ho, John Schulman, Matthias Plappert, Tim Salimans and Vikash Kumar for their feedback on previous versions of this manuscript, as well as Rein Houthooft and the entire OpenAI team for fruitful discussions."}, {"heading": "A Experiment details", "text": "In this section we provide more details about our experimental setups and hyperparameters that we are able to install uniformators. We have a network with a hidden layer of 256 neurons; the length of each episode was synonymous with the number of bits and the way they move in each area is very different; the way in which they move is very different; the way in which they move is very different; the way in which they move is very different; the way in which they move is very different; the way in which they move and move is very different; the way in which they move is very different; the way in which they move is very different; the object is randomly placed on the table, in which 30 cm x 20cm x 20cm for sliding square with the center directly under the gripper. (Both objects are 5cm wide) For sliding, the target state it is uniformed."}, {"heading": "B Deployment on the physical robot", "text": "We have trained a Convolutionary Neural Network (CNN) that predicts the position of the box based on the raw image of the hollow-head camera. CNN was trained only on images from the Mujoco renderer. Although the training images were not photorealistic, thanks to a high degree of randomization of textures, flash and other visual parameters in training, the trained network works well with real data. This approach, called domain randomization, is described in Tobin et al. (2017). At the beginning of each episode, we initialize a simulated environment using the box position predicted by CNN and the robot state coming from the physical robot. From that point on, we execute the guidelines in the simulator. After each step of time, we send the simulated robot swivel angles to the real one that is position-controlled and uses the simulated data as targets."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization", "author": ["B. Bakker", "J. Schmidhuber"], "venue": "Proc. of the 8-th Conf. on Intelligent Autonomous Systems, pages 438\u2013445.", "citeRegEx": "Bakker and Schmidhuber,? 2004", "shortCiteRegEx": "Bakker and Schmidhuber", "year": 2004}, {"title": "Unifying countbased exploration and intrinsic motivation", "author": ["M. Bellemare", "S. Srinivasan", "G. Ostrovski", "T. Schaul", "D. Saxton", "R. Munos"], "venue": "Advances in Neural Information Processing Systems, pages 1471\u20131479.", "citeRegEx": "Bellemare et al\\.,? 2016", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "Proceedings of the 26th annual international conference on machine learning, pages 41\u201348. ACM.", "citeRegEx": "Bengio et al\\.,? 2009", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Learning to learn, pages 95\u2013133. Springer.", "citeRegEx": "Caruana,? 1998", "shortCiteRegEx": "Caruana", "year": 1998}, {"title": "Path integral guided policy search", "author": ["Y. Chebotar", "M. Kalakrishnan", "A. Yahya", "A. Li", "S. Schaal", "S. Levine"], "venue": "arXiv preprint arXiv:1610.00529.", "citeRegEx": "Chebotar et al\\.,? 2016", "shortCiteRegEx": "Chebotar et al\\.", "year": 2016}, {"title": "Learning parameterized skills", "author": ["B. Da Silva", "G. Konidaris", "A. Barto"], "venue": "arXiv preprint arXiv:1206.6398.", "citeRegEx": "Silva et al\\.,? 2012", "shortCiteRegEx": "Silva et al\\.", "year": 2012}, {"title": "Learning modular neural network policies for multi-task and multi-robot transfer", "author": ["C. Devin", "A. Gupta", "T. Darrell", "P. Abbeel", "S. Levine"], "venue": "arXiv preprint arXiv:1609.07088.", "citeRegEx": "Devin et al\\.,? 2016", "shortCiteRegEx": "Devin et al\\.", "year": 2016}, {"title": "Learning and development in neural networks: The importance of starting small", "author": ["J.L. Elman"], "venue": "Cognition, 48(1):71\u201399.", "citeRegEx": "Elman,? 1993", "shortCiteRegEx": "Elman", "year": 1993}, {"title": "Structure in the space of value functions", "author": ["D. Foster", "P. Dayan"], "venue": "Machine Learning, 49(2):325\u2013346.", "citeRegEx": "Foster and Dayan,? 2002", "shortCiteRegEx": "Foster and Dayan", "year": 2002}, {"title": "Automated curriculum learning for neural networks", "author": ["A. Graves", "M.G. Bellemare", "J. Menick", "R. Munos", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1704.03003.", "citeRegEx": "Graves et al\\.,? 2017", "shortCiteRegEx": "Graves et al\\.", "year": 2017}, {"title": "Hybrid computing using a neural network with dynamic external memory", "author": ["A. Graves", "G. Wayne", "M. Reynolds", "T. Harley", "I. Danihelka", "A. Grabska-Barwi\u0144ska", "S.G. Colmenarejo", "E. Grefenstette", "T. Ramalho", "J Agapiou"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2016}, {"title": "Continuous deep q-learning with model-based acceleration", "author": ["S. Gu", "T. Lillicrap", "I. Sutskever", "S. Levine"], "venue": "arXiv preprint arXiv:1603.00748.", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Automatic goal generation for reinforcement learning agents", "author": ["D. Held", "X. Geng", "C. Florensa", "P. Abbeel"], "venue": "arXiv preprint arXiv:1705.06366.", "citeRegEx": "Held et al\\.,? 2017", "shortCiteRegEx": "Held et al\\.", "year": 2017}, {"title": "Vime: Variational information maximizing exploration", "author": ["R. Houthooft", "X. Chen", "Y. Duan", "J. Schulman", "F. De Turck", "P. Abbeel"], "venue": "Advances in Neural Information Processing Systems, pages 1109\u2013 1117.", "citeRegEx": "Houthooft et al\\.,? 2016", "shortCiteRegEx": "Houthooft et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba,? 2014", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Reinforcement learning to adjust parametrized motor primitives to new situations", "author": ["J. Kober", "A. Wilhelm", "E. Oztop", "J. Peters"], "venue": "Autonomous Robots, 33(4):361\u2013379.", "citeRegEx": "Kober et al\\.,? 2012", "shortCiteRegEx": "Kober et al\\.", "year": 2012}, {"title": "Near-bayesian exploration in polynomial time", "author": ["J.Z. Kolter", "A.Y. Ng"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, pages 513\u2013520. ACM.", "citeRegEx": "Kolter and Ng,? 2009", "shortCiteRegEx": "Kolter and Ng", "year": 2009}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "arXiv preprint arXiv:1504.00702.", "citeRegEx": "Levine et al\\.,? 2015", "shortCiteRegEx": "Levine et al\\.", "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "arXiv preprint arXiv:1509.02971.", "citeRegEx": "Lillicrap et al\\.,? 2015", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "author": ["Lin", "L.-J."], "venue": "Machine learning, 8(3-4):293\u2013321.", "citeRegEx": "Lin and L..J.,? 1992", "shortCiteRegEx": "Lin and L..J.", "year": 1992}, {"title": "Discrete sequential prediction of continuous actions for deep rl", "author": ["L. Metz", "J. Ibarz", "N. Jaitly", "J. Davidson"], "venue": "arXiv preprint arXiv:1705.05035.", "citeRegEx": "Metz et al\\.,? 2017", "shortCiteRegEx": "Metz et al\\.", "year": 2017}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Autonomous inverted helicopter flight via reinforcement learning", "author": ["A. Ng", "A. Coates", "M. Diel", "V. Ganapathi", "J. Schulte", "B. Tse", "E. Berger", "E. Liang"], "venue": "Experimental Robotics IX, pages 363\u2013372.", "citeRegEx": "Ng et al\\.,? 2006", "shortCiteRegEx": "Ng et al\\.", "year": 2006}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["A.Y. Ng", "D. Harada", "S. Russell"], "venue": "ICML, volume 99, pages 278\u2013287.", "citeRegEx": "Ng et al\\.,? 1999", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "Deep exploration via bootstrapped dqn", "author": ["I. Osband", "C. Blundell", "A. Pritzel", "B. Van Roy"], "venue": "Advances In Neural Information Processing Systems, pages 4026\u20134034.", "citeRegEx": "Osband et al\\.,? 2016", "shortCiteRegEx": "Osband et al\\.", "year": 2016}, {"title": "Count-based exploration with neural density models", "author": ["G. Ostrovski", "M.G. Bellemare", "Oord", "A. v. d.", "R. Munos"], "venue": "arXiv preprint arXiv:1703.01310.", "citeRegEx": "Ostrovski et al\\.,? 2017", "shortCiteRegEx": "Ostrovski et al\\.", "year": 2017}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["J. Peters", "S. Schaal"], "venue": "Neural networks, 21(4):682\u2013697.", "citeRegEx": "Peters and Schaal,? 2008", "shortCiteRegEx": "Peters and Schaal", "year": 2008}, {"title": "Learning to push by grasping: Using multiple tasks for effective learning", "author": ["L. Pinto", "A. Gupta"], "venue": "arXiv preprint arXiv:1609.09025.", "citeRegEx": "Pinto and Gupta,? 2016", "shortCiteRegEx": "Pinto and Gupta", "year": 2016}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM Journal on Control and Optimization, 30(4):838\u2013855.", "citeRegEx": "Polyak and Juditsky,? 1992", "shortCiteRegEx": "Polyak and Juditsky", "year": 1992}, {"title": "Data-efficient deep reinforcement learning for dexterous manipulation", "author": ["I. Popov", "N. Heess", "T. Lillicrap", "R. Hafner", "G. Barth-Maron", "M. Vecerik", "T. Lampe", "Y. Tassa", "T. Erez", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1704.03073.", "citeRegEx": "Popov et al\\.,? 2017", "shortCiteRegEx": "Popov et al\\.", "year": 2017}, {"title": "Universal value function approximators", "author": ["T. Schaul", "D. Horgan", "K. Gregor", "D. Silver"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1312\u20131320.", "citeRegEx": "Schaul et al\\.,? 2015a", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "arXiv preprint arXiv:1511.05952.", "citeRegEx": "Schaul et al\\.,? 2015b", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M Lanctot"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "A theoretical analysis of model-based interval estimation", "author": ["A.L. Strehl", "M.L. Littman"], "venue": "Proceedings of the 22nd international conference on Machine learning, pages 856\u2013863. ACM.", "citeRegEx": "Strehl and Littman,? 2005", "shortCiteRegEx": "Strehl and Littman", "year": 2005}, {"title": "Intrinsic motivation and automatic curricula via asymmetric self-play", "author": ["S. Sukhbaatar", "I. Kostrikov", "A. Szlam", "R. Fergus"], "venue": "arXiv preprint arXiv:1703.05407.", "citeRegEx": "Sukhbaatar et al\\.,? 2017", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2017}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup"], "venue": "The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2, pages 761\u2013768. International Foundation for Autonomous Agents and Multiagent Systems.", "citeRegEx": "Sutton et al\\.,? 2011", "shortCiteRegEx": "Sutton et al\\.", "year": 2011}, {"title": " exploration: A study of count-based exploration for deep reinforcement learning", "author": ["H. Tang", "R. Houthooft", "D. Foote", "A. Stooke", "X. Chen", "Y. Duan", "J. Schulman", "F. De Turck", "P. Abbeel"], "venue": "arXiv preprint arXiv:1611.04717.", "citeRegEx": "Tang et al\\.,? 2016", "shortCiteRegEx": "Tang et al\\.", "year": 2016}, {"title": "Domain randomization for transferring deep neural networks from simulation to the real world", "author": ["J. Tobin", "R. Fong", "A. Ray", "J. Schneider", "W. Zaremba", "P. Abbeel"], "venue": "arXiv preprint arXiv:1703.06907.", "citeRegEx": "Tobin et al\\.,? 2017", "shortCiteRegEx": "Tobin et al\\.", "year": 2017}, {"title": "Mujoco: A physics engine for model-based control", "author": ["E. Todorov", "T. Erez", "Y. Tassa"], "venue": "Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 5026\u20135033. IEEE.", "citeRegEx": "Todorov et al\\.,? 2012", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Feudal networks for hierarchical reinforcement learning", "author": ["A.S. Vezhnevets", "S. Osindero", "T. Schaul", "N. Heess", "M. Jaderberg", "D. Silver", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1703.01161.", "citeRegEx": "Vezhnevets et al\\.,? 2017", "shortCiteRegEx": "Vezhnevets et al\\.", "year": 2017}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint arXiv:1410.4615.", "citeRegEx": "Zaremba and Sutskever,? 2014", "shortCiteRegEx": "Zaremba and Sutskever", "year": 2014}, {"title": "raw image from the fetch head camera. The CNN was trained using only images coming from the Mujoco renderer. Despite the fact that training images were not photorealistic, the trained network performs well on real world data thanks to a high degree of randomization of textures, lightning and other visual parameters in training", "author": ["Tobin"], "venue": null, "citeRegEx": "Tobin,? \\Q2017\\E", "shortCiteRegEx": "Tobin", "year": 2017}], "referenceMentions": [{"referenceID": 22, "context": "This includes simulated environments, such as playing Atari games (Mnih et al., 2015), and defeating the best human player at the game of Go (Silver et al.", "startOffset": 66, "endOffset": 85}, {"referenceID": 33, "context": ", 2015), and defeating the best human player at the game of Go (Silver et al., 2016), as well as robotic tasks such as helicopter control (Ng et al.", "startOffset": 63, "endOffset": 84}, {"referenceID": 23, "context": ", 2016), as well as robotic tasks such as helicopter control (Ng et al., 2006), hitting a baseball (Peters and Schaal, 2008), screwing a cap onto a bottle (Levine et al.", "startOffset": 61, "endOffset": 78}, {"referenceID": 27, "context": ", 2006), hitting a baseball (Peters and Schaal, 2008), screwing a cap onto a bottle (Levine et al.", "startOffset": 28, "endOffset": 53}, {"referenceID": 18, "context": ", 2006), hitting a baseball (Peters and Schaal, 2008), screwing a cap onto a bottle (Levine et al., 2015), or door opening (Chebotar et al.", "startOffset": 84, "endOffset": 105}, {"referenceID": 5, "context": ", 2015), or door opening (Chebotar et al., 2016).", "startOffset": 25, "endOffset": 48}, {"referenceID": 24, "context": "However, a common challenge, especially for robotics, is the need to engineer a reward function that not only reflects the task at hand but is also carefully shaped (Ng et al., 1999) to guide the policy optimization.", "startOffset": 165, "endOffset": 182}, {"referenceID": 5, "context": ", 2015), or door opening (Chebotar et al., 2016). However, a common challenge, especially for robotics, is the need to engineer a reward function that not only reflects the task at hand but is also carefully shaped (Ng et al., 1999) to guide the policy optimization. For example, Popov et al. (2017) use a cost function consisting of five relatively complicated terms which need to be carefully weighted in order to train a policy for stacking a brick on top of another one.", "startOffset": 26, "endOffset": 300}, {"referenceID": 31, "context": "Our approach is based on training universal policies (Schaul et al., 2015a) which take as input not only the current state, but also a goal state.", "startOffset": 53, "endOffset": 75}, {"referenceID": 22, "context": "Deep Q-Networks (DQN) (Mnih et al., 2015) is a model-free RL algorithm for discrete action spaces.", "startOffset": 22, "endOffset": 41}, {"referenceID": 22, "context": "Deep Q-Networks (DQN) (Mnih et al., 2015) is a model-free RL algorithm for discrete action spaces. Here we sketch it only informally, see Mnih et al. (2015) for more details.", "startOffset": 23, "endOffset": 157}, {"referenceID": 29, "context": "(2015)) or to use a polyak-averaged2 (Polyak and Juditsky, 1992) version of the main network instead (Lillicrap et al.", "startOffset": 37, "endOffset": 64}, {"referenceID": 19, "context": "(2015)) or to use a polyak-averaged2 (Polyak and Juditsky, 1992) version of the main network instead (Lillicrap et al., 2015).", "startOffset": 101, "endOffset": 125}, {"referenceID": 21, "context": "Mnih et al. (2015)) or to use a polyak-averaged2 (Polyak and Juditsky, 1992) version of the main network instead (Lillicrap et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 19, "context": "Deep Deterministic Policy Gradients (DDPG) (Lillicrap et al., 2015) is a model-free RL algorithm for continuous action spaces.", "startOffset": 43, "endOffset": 67}, {"referenceID": 19, "context": "Deep Deterministic Policy Gradients (DDPG) (Lillicrap et al., 2015) is a model-free RL algorithm for continuous action spaces. Here we sketch it only informally, see Lillicrap et al. (2015) for more details.", "startOffset": 44, "endOffset": 190}, {"referenceID": 31, "context": "Universal Value Function Approximators (UVFA) (Schaul et al., 2015a) is an extension of DQN to the setup where there is more than one goal we may try to achieve.", "startOffset": 46, "endOffset": 68}, {"referenceID": 31, "context": "Universal Value Function Approximators (UVFA) (Schaul et al., 2015a) is an extension of DQN to the setup where there is more than one goal we may try to achieve. Let G be the space of possible goals. Every goal g \u2208 G corresponds to some reward function rg : S \u00d7A \u2192 R. Every episode starts with sampling a state-goal pair from some distribution p(s0, g). The goal stays fixed for the whole episode. At every timestep the agent gets as input not only the current state but also the current goal \u03c0 : S \u00d7 G \u2192 A and gets the reward rt = rg(st, at). The Q-function now depends not only on a state-action pair but also on a goal Q(st, at, g) = E[Rt|st, at, g]. Schaul et al. (2015a) show that in this setup it is possible to train an approximator to the Q-function using direct bootstrapping from the Bellman equation (just like in case of DQN) and that a greedy policy derived from it can generalize to previously unseen state-action pairs.", "startOffset": 47, "endOffset": 676}, {"referenceID": 14, "context": "VIME (Houthooft et al., 2016), count-based exploration (Ostrovski et al.", "startOffset": 5, "endOffset": 29}, {"referenceID": 26, "context": ", 2016), count-based exploration (Ostrovski et al., 2017) or bootstrapped DQN (Osband et al.", "startOffset": 33, "endOffset": 57}, {"referenceID": 25, "context": ", 2017) or bootstrapped DQN (Osband et al., 2016)) does not help here because the real problem is not in lack of diversity of states being visited, rather it is simply impractical to explore such a large state space.", "startOffset": 28, "endOffset": 49}, {"referenceID": 31, "context": "We follow the approach from Universal Value Function Approximators (Schaul et al., 2015a), i.", "startOffset": 67, "endOffset": 89}, {"referenceID": 28, "context": "3 for details) and therefore our approach may be applicable even if there is only one task we would like the agent to perform (a similar situation was recently observed by Pinto and Gupta (2016)).", "startOffset": 172, "endOffset": 195}, {"referenceID": 22, "context": "Notice that the goal being pursued influences the agent\u2019s actions but not the environment dynamics and therefore we can replay each trajectory with an arbitrary goal assuming that we use an off-policy RL algorithm like DQN (Mnih et al., 2015), DDPG (Lillicrap et al.", "startOffset": 223, "endOffset": 242}, {"referenceID": 19, "context": ", 2015), DDPG (Lillicrap et al., 2015), NAF (Gu et al.", "startOffset": 14, "endOffset": 38}, {"referenceID": 12, "context": ", 2015), NAF (Gu et al., 2016) or SDQN (Metz et al.", "startOffset": 13, "endOffset": 30}, {"referenceID": 21, "context": ", 2016) or SDQN (Metz et al., 2017).", "startOffset": 16, "endOffset": 35}, {"referenceID": 39, "context": "The robot is simulated using the MuJoCo (Todorov et al., 2012) physics engine.", "startOffset": 40, "endOffset": 62}, {"referenceID": 19, "context": "Training is performed using the DDPG algorithm (Lillicrap et al., 2015) with Adam (Kingma and Ba, 2014) as the optimizer.", "startOffset": 47, "endOffset": 71}, {"referenceID": 15, "context": ", 2015) with Adam (Kingma and Ba, 2014) as the optimizer.", "startOffset": 18, "endOffset": 39}, {"referenceID": 34, "context": "Moreover, we compare against DDPG with count-based exploration5 (Strehl and Littman, 2005; Kolter and Ng, 2009; Tang et al., 2016; Bellemare et al., 2016; Ostrovski et al., 2017).", "startOffset": 64, "endOffset": 178}, {"referenceID": 17, "context": "Moreover, we compare against DDPG with count-based exploration5 (Strehl and Littman, 2005; Kolter and Ng, 2009; Tang et al., 2016; Bellemare et al., 2016; Ostrovski et al., 2017).", "startOffset": 64, "endOffset": 178}, {"referenceID": 37, "context": "Moreover, we compare against DDPG with count-based exploration5 (Strehl and Littman, 2005; Kolter and Ng, 2009; Tang et al., 2016; Bellemare et al., 2016; Ostrovski et al., 2017).", "startOffset": 64, "endOffset": 178}, {"referenceID": 2, "context": "Moreover, we compare against DDPG with count-based exploration5 (Strehl and Littman, 2005; Kolter and Ng, 2009; Tang et al., 2016; Bellemare et al., 2016; Ostrovski et al., 2017).", "startOffset": 64, "endOffset": 178}, {"referenceID": 26, "context": "Moreover, we compare against DDPG with count-based exploration5 (Strehl and Littman, 2005; Kolter and Ng, 2009; Tang et al., 2016; Bellemare et al., 2016; Ostrovski et al., 2017).", "startOffset": 64, "endOffset": 178}, {"referenceID": 24, "context": "Of course for every problem there exists a reward which makes it easy (Ng et al., 1999) but designing such shaped rewards requires a lot of domain knowledge and may in some cases not be much easier than directly scripting the policy.", "startOffset": 70, "endOffset": 87}, {"referenceID": 28, "context": "Popov et al. (2017)).", "startOffset": 0, "endOffset": 20}, {"referenceID": 22, "context": "The technique of experience replay has been introduced in Lin (1992) and became very popular after it was used in the DQN agent playing Atari (Mnih et al., 2015).", "startOffset": 142, "endOffset": 161}, {"referenceID": 32, "context": "Prioritized experience replay (Schaul et al., 2015b) is an improvement to experience replay which prioritizes transitions in the replay buffer in order to speed up training.", "startOffset": 30, "endOffset": 52}, {"referenceID": 8, "context": "Our approach may be seen as a form of implicit curriculum learning (Elman, 1993; Bengio et al., 2009).", "startOffset": 67, "endOffset": 101}, {"referenceID": 3, "context": "Our approach may be seen as a form of implicit curriculum learning (Elman, 1993; Bengio et al., 2009).", "startOffset": 67, "endOffset": 101}, {"referenceID": 2, "context": "Caruana (1998); Da Silva et al.", "startOffset": 0, "endOffset": 15}, {"referenceID": 2, "context": "Caruana (1998); Da Silva et al. (2012); Kober et al.", "startOffset": 0, "endOffset": 39}, {"referenceID": 2, "context": "Caruana (1998); Da Silva et al. (2012); Kober et al. (2012); Devin et al.", "startOffset": 0, "endOffset": 60}, {"referenceID": 2, "context": "Caruana (1998); Da Silva et al. (2012); Kober et al. (2012); Devin et al. (2016); Pinto and Gupta (2016).", "startOffset": 0, "endOffset": 81}, {"referenceID": 2, "context": "Caruana (1998); Da Silva et al. (2012); Kober et al. (2012); Devin et al. (2016); Pinto and Gupta (2016). Learning off-policy value functions for multiple tasks was investigated by Foster and Dayan (2002) and Sutton et al.", "startOffset": 0, "endOffset": 105}, {"referenceID": 2, "context": "Caruana (1998); Da Silva et al. (2012); Kober et al. (2012); Devin et al. (2016); Pinto and Gupta (2016). Learning off-policy value functions for multiple tasks was investigated by Foster and Dayan (2002) and Sutton et al.", "startOffset": 0, "endOffset": 205}, {"referenceID": 2, "context": "Caruana (1998); Da Silva et al. (2012); Kober et al. (2012); Devin et al. (2016); Pinto and Gupta (2016). Learning off-policy value functions for multiple tasks was investigated by Foster and Dayan (2002) and Sutton et al. (2011). Our work is most heavily based on Schaul et al.", "startOffset": 0, "endOffset": 230}, {"referenceID": 2, "context": "Caruana (1998); Da Silva et al. (2012); Kober et al. (2012); Devin et al. (2016); Pinto and Gupta (2016). Learning off-policy value functions for multiple tasks was investigated by Foster and Dayan (2002) and Sutton et al. (2011). Our work is most heavily based on Schaul et al. (2015a) who considers training a single neural network approximating multiple value functions.", "startOffset": 0, "endOffset": 287}, {"referenceID": 1, "context": "Bakker and Schmidhuber (2004); Vezhnevets et al.", "startOffset": 0, "endOffset": 30}, {"referenceID": 1, "context": "Bakker and Schmidhuber (2004); Vezhnevets et al. (2017). Our approach may be seen as a form of implicit curriculum learning (Elman, 1993; Bengio et al.", "startOffset": 0, "endOffset": 56}, {"referenceID": 1, "context": "Bakker and Schmidhuber (2004); Vezhnevets et al. (2017). Our approach may be seen as a form of implicit curriculum learning (Elman, 1993; Bengio et al., 2009). While curriculum is now often used for training neural networks (e.g. Zaremba and Sutskever (2014); Graves et al.", "startOffset": 0, "endOffset": 259}, {"referenceID": 1, "context": "Bakker and Schmidhuber (2004); Vezhnevets et al. (2017). Our approach may be seen as a form of implicit curriculum learning (Elman, 1993; Bengio et al., 2009). While curriculum is now often used for training neural networks (e.g. Zaremba and Sutskever (2014); Graves et al. (2016)), the curriculum is almost always hand-crafted.", "startOffset": 0, "endOffset": 281}, {"referenceID": 1, "context": "Bakker and Schmidhuber (2004); Vezhnevets et al. (2017). Our approach may be seen as a form of implicit curriculum learning (Elman, 1993; Bengio et al., 2009). While curriculum is now often used for training neural networks (e.g. Zaremba and Sutskever (2014); Graves et al. (2016)), the curriculum is almost always hand-crafted. The problem of automatic curriculum generation has been approached only recently. Graves et al. (2017) consider a setup where there is a fixed discrete set of tasks and empirically evaluate different strategies for automatic curriculum generation in this settings.", "startOffset": 0, "endOffset": 432}, {"referenceID": 1, "context": "Bakker and Schmidhuber (2004); Vezhnevets et al. (2017). Our approach may be seen as a form of implicit curriculum learning (Elman, 1993; Bengio et al., 2009). While curriculum is now often used for training neural networks (e.g. Zaremba and Sutskever (2014); Graves et al. (2016)), the curriculum is almost always hand-crafted. The problem of automatic curriculum generation has been approached only recently. Graves et al. (2017) consider a setup where there is a fixed discrete set of tasks and empirically evaluate different strategies for automatic curriculum generation in this settings. Another approach investigated by Sukhbaatar et al. (2017) and Held et al.", "startOffset": 0, "endOffset": 652}, {"referenceID": 1, "context": "Bakker and Schmidhuber (2004); Vezhnevets et al. (2017). Our approach may be seen as a form of implicit curriculum learning (Elman, 1993; Bengio et al., 2009). While curriculum is now often used for training neural networks (e.g. Zaremba and Sutskever (2014); Graves et al. (2016)), the curriculum is almost always hand-crafted. The problem of automatic curriculum generation has been approached only recently. Graves et al. (2017) consider a setup where there is a fixed discrete set of tasks and empirically evaluate different strategies for automatic curriculum generation in this settings. Another approach investigated by Sukhbaatar et al. (2017) and Held et al. (2017) uses self-play between the policy and a task-setter in order to automatically generate goal states which are on the border of what the current policy can achieve.", "startOffset": 0, "endOffset": 675}], "year": 2017, "abstractText": "Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task. The video presenting our experiments is available at https://goo.gl/SMrQnI.", "creator": "LaTeX with hyperref package"}}}