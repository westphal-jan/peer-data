{"id": "1707.01555", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jul-2017", "title": "A Deep Network with Visual Text Composition Behavior", "abstract": "While natural languages are compositional, how state-of-the-art neural models achieve compositionality is still unclear. We propose a deep network, which not only achieves competitive accuracy for text classification, but also exhibits compositional behavior. That is, while creating hierarchical representations of a piece of text, such as a sentence, the lower layers of the network distribute their layer-specific attention weights to individual words. In contrast, the higher layers compose meaningful phrases and clauses, whose lengths increase as the networks get deeper until fully composing the sentence.", "histories": [["v1", "Wed, 5 Jul 2017 19:37:23 GMT  (677kb)", "http://arxiv.org/abs/1707.01555v1", "accepted to ACL2017"]], "COMMENTS": "accepted to ACL2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["hongyu guo"], "accepted": true, "id": "1707.01555"}, "pdf": {"name": "1707.01555.pdf", "metadata": {"source": "CRF", "title": "A Deep Network with Visual Text Composition Behavior", "authors": ["Hongyu Guo"], "emails": ["hongyu.guo@nrc-cnrc.gc.ca"], "sections": [{"heading": null, "text": "ar Xiv: 170 7.01 555v 1 [cs.C L] 5J ul2 017tional How modern neural models achieve compositivity is still unclear. We propose a deep network that not only achieves competitive accuracy in text classification, but also exhibits compositional behavior. In other words, while hierarchical representations of a text, such as a sentence, are created, the lower layers of the network distribute their layer-specific attention weights to individual words. In contrast, the higher layers assemble meaningful phrases and sentences, the length of which increases as the networks deepen until the sentence is fully composed."}, {"heading": "1 Introduction", "text": "Deep neural networks use task-specific architectures to develop hierarchical representations of the entrance, in which higher representations are derived from lower-level characteristics (Conneau et al., 2016). Such hierarchical representations have visually demonstrated compositivity in image processing, i.e., pixels combine to form shapes and then contours (Farabet et al., 2013; Zeiler and Fergus, 2014). Natural languages are also compositional, i.e. words combine to form phrases and then sentences. However, unlike the vision, it is still unclear how deep neural models in the NLP, which operate mainly on distributed word embedding, achieve compositionality (Li et al., 2015, 2016). We propose a network of Attention Gated Transformation (AGT) in which the characteristic generation of individual layers is not replaced by a layer-characteristic-trivial mechanism (Bahdanau et al, 2014)."}, {"heading": "2 Attention Gated Transformation Network", "text": "Our AGT network was inspired by Highway Networks (Srivastava et al., 2015a, b), where each layer is equipped with a transformation gate."}, {"heading": "2.1 Transform Gate for Information Flow", "text": "Consider a forward-facing neural network with multiple layers. Each layer l typically applies a nonlinear transformation f (e.g. tanh, parameterized by W f l) to generate its output yl. Here, l = 0 indicates the first layer and y0 is equal to the given input text x, namely y0 = x: yl = f (yl \u2212 1, W f l) Tl + yl \u2212 1 (1 \u2212 Tl) (1) While in a highway network (the left column of Figure 1) an additional nonlinear transformation function Tl is added to the l (l > 0) layer: yl = f (yl \u2212 1, W f l) Tl + yl \u2212 1 (1 \u2212 Tl) (2), where the function Tl expresses the representation yl of the f-shape, the l-shape is generated by transformation."}, {"heading": "2.2 Attention Gated Transformation", "text": "In fact, the choice of words used in individual countries is only a word that is derived from a spoken word that is derived from a spoken word. (...) It is as if it is a spoken word that is derived from a spoken word. (...) It is not as if it becomes a spoken word. (...) It is as if it becomes a spoken word. (...) It is as if it becomes a spoken word. (...) It is as if it becomes a spoken word. (...) It is as if it becomes a spoken word. (...) It is as if it becomes a spoken word. (...) It is as if it becomes a spoken word. (...) It is as if it becomes a spoken word. (...) It is as if it becomes a spoken word."}, {"heading": "3 Experimental Studies", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Main Results", "text": "The reasons for this development are manifold: they can be found primarily in the USA, but also in the USA, where they are widely spread in the USA and in other countries. (...) In the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA and in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in"}, {"heading": "3.2 Further Observations", "text": "As discussed at the end of section 2.2, the intuitive use of new words at different levels allowed the networks to explore different word combinations of the given text more effectively than the use of all words only at the lowest level of the networks. Empirically, we observed that if only s + 0 was used in the AGT network, namely removing s + i for i > 0, the accuracy of the tested words dropped from 50.5% to 48.5%. In other words, converting a linear combination of the characteristics of the wordbag was insufficient to achieve sufficient accuracy for the classification task. For example, if two more selection vectors s + i were added, namely removing s + i for i > 2, the AGT could improve its accuracy to 49.0%. Furthermore, we observed that the AGT networks tended to select informative words at the lower levels, which could be caused by the recursive form of equation 8."}, {"heading": "4 Conclusion and Future Work", "text": "We have presented a novel deep network that not only achieves very competitive accuracy in text classification, but also exhibits interesting textual compositional behavior that could shed light on how neural models work in NLP tasks. In the future, we plan to apply the AGT networks to the gradual generation of nature text (Guo, 2015; Hu et al., 2017)."}], "references": [{"title": "Generating text with deep rein", "author": ["Hongyu Guo"], "venue": null, "citeRegEx": "2471", "shortCiteRegEx": "2471", "year": 2015}, {"title": "Convolutional neural networks", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "J. Mach. Learn. Res. 15(1).", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Highway networks", "author": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "CoRR abs/1505.00387.", "citeRegEx": "Srivastava et al\\.,? 2015a", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Training very deep networks", "author": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "Proceedings of the 28th International Conference on Neural Information Processing Systems. NIPS\u201915, pages 2377\u20132385.", "citeRegEx": "Srivastava et al\\.,? 2015b", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "CoRR abs/1503.00075.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "CoRR abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D. Zeiler", "Rob Fergus."], "venue": "Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I. pages 818\u2013833.", "citeRegEx": "Zeiler and Fergus.,? 2014", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}, {"title": "An empirical study on the effect of negation words on sentiment", "author": ["Xiaodan Zhu", "Hongyu Guo", "Saif Mohammad", "Svetlana Kiritchenko."], "venue": "ACL (1). pages 304\u2013313.", "citeRegEx": "Zhu et al\\.,? 2014", "shortCiteRegEx": "Zhu et al\\.", "year": 2014}, {"title": "Long short-term memory over recursive structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo."], "venue": "ICML. pages 1604\u20131612.", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": ", pixels combine to form shapes and then contours (Farabet et al., 2013; Zeiler and Fergus, 2014).", "startOffset": 50, "endOffset": 97}, {"referenceID": 4, "context": "The left column of Figure 1 reflects the highway networks as proposed by (Srivastava et al., 2015b).", "startOffset": 73, "endOffset": 99}, {"referenceID": 1, "context": "use the same splits for training, dev, and test data as in (Kim, 2014) to predict the finegrained 5-class sentiment categories of the sentences.", "startOffset": 59, "endOffset": 70}, {"referenceID": 1, "context": "For comparison purposes, following (Kim, 2014; Kalchbrenner et al., 2014; Lei et al., 2015), we trained the models using both phrases and sentences, but only evaluate sentences at test time.", "startOffset": 35, "endOffset": 91}, {"referenceID": 6, "context": "For optimization, we used Adadelta (Zeiler, 2012), with learning rate of 0.", "startOffset": 35, "endOffset": 49}, {"referenceID": 2, "context": "0005, mini-batch of 50, transform gate bias of 1, and dropout (Srivastava et al., 2014) rate of 0.", "startOffset": 62, "endOffset": 87}, {"referenceID": 5, "context": "0%) (Tai et al., 2015; Zhu et al., 2015) and high-order CNN approaches (51.", "startOffset": 4, "endOffset": 40}, {"referenceID": 9, "context": "0%) (Tai et al., 2015; Zhu et al., 2015) and high-order CNN approaches (51.", "startOffset": 4, "endOffset": 40}, {"referenceID": 8, "context": "These are important words for sentiment classification (Zhu et al., 2014).", "startOffset": 55, "endOffset": 73}, {"referenceID": 3, "context": "In addition, like the transform gate in the Highway networks (Srivastava et al., 2015a) and the forget gate in the LSTM (Gers et al.", "startOffset": 61, "endOffset": 87}], "year": 2017, "abstractText": "While natural languages are compositional, how state-of-the-art neural models achieve compositionality is still unclear. We propose a deep network, which not only achieves competitive accuracy for text classification, but also exhibits compositional behavior. That is, while creating hierarchical representations of a piece of text, such as a sentence, the lower layers of the network distribute their layer-specific attention weights to individual words. In contrast, the higher layers compose meaningful phrases and clauses, whose lengths increase as the networks get deeper until fully composing the sentence.", "creator": "LaTeX with hyperref package"}}}