{"id": "1707.02026", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jul-2017", "title": "A Nested Attention Neural Hybrid Model for Grammatical Error Correction", "abstract": "Grammatical error correction (GEC) systems strive to correct both global errors in word order and usage, and local errors in spelling and inflection. Further developing upon recent work on neural machine translation, we propose a new hybrid neural model with nested attention layers for GEC. Experiments show that the new model can effectively correct errors of both types by incorporating word and character-level information,and that the model significantly outperforms previous neural models for GEC as measured on the standard CoNLL-14 benchmark dataset. Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective in correcting local errors that involve small edits in orthography.", "histories": [["v1", "Fri, 7 Jul 2017 03:10:32 GMT  (702kb,D)", "https://arxiv.org/abs/1707.02026v1", null], ["v2", "Mon, 10 Jul 2017 02:56:49 GMT  (600kb,D)", "http://arxiv.org/abs/1707.02026v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jianshu ji", "qinlong wang", "kristina toutanova", "yongen gong", "steven truong", "jianfeng gao"], "accepted": true, "id": "1707.02026"}, "pdf": {"name": "1707.02026.pdf", "metadata": {"source": "CRF", "title": "A Nested Attention Neural Hybrid Model for Grammatical Error Correction", "authors": ["Jianshu Ji", "Qinlong Wang", "Kristina Toutanova", "Yongen Gong", "Steven Truong", "Jianfeng Gao"], "emails": ["jianshuj@microsoft.com", "qinlwang@microsoft.com", "yongeg@microsoft.com", "stevetr@microsoft.com", "jfgao@microsoft.com", "kristout@google.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 Related Work", "text": "The CoNLL-14 joint task overview by Ng et al. (2014) provides a comparative evaluation of the approaches. Two notable advances after the joint task have been made in the areas of combining classifiers and phrase designation MT (Rozovskaya and Roth, 2016). The latter work has achieved the highest performance to date in the task of 49.5 in F0.5 in the evaluation of F0.5 in the CoNLL-14 test series. This method integrates discriminatory training in terms of task-specific evaluation function, a rich set of features and several large language models. Neural approaches to the task are less explored. We believe that the advances of Junczys-Dowmunt and Grundkiewicz are complementary."}, {"heading": "3 Nested Attention Hybrid Model", "text": "Our model is hybrid, using both word-level and character-level representations. It consists of a word-based sequence-to-sequence model as the backbone and additional character-level encoder, decoder, and attention components that focus on words that lie outside the vocabulary of the vocabulary."}, {"heading": "3.1 Word-based sequence-to-sequence model as backbone", "text": "The word-based backbone closely follows the basic neural sequence-to-sequence architecture proposed by Bahdanau et al. (2015) and applied to the grammatical error correction of Yuan and Briscoe (2016). For the sake of completeness, we give an outline here. It uses recursive neural networks to encode the input set and decode the output sets. The encoder generates a corresponding context-specific sequence of hidden state vectors e: e = (h1,., hT) The hidden state t is calculated as: ft = GRUencf (ft \u2212 1, xt), bt = GRUencb (bt + 1, xt), ht = ahnd; bt], where GRUencb stands for the functions subscribed to."}, {"heading": "3.2 Hybrid encoder and decoder with two nested levels of attention", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset and Evaluation", "text": "One data source is the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013), which is provided as a training set for the CoNLL-13 and CoNLL-14 divided tasks. From the original corpus of about 60K parallel sentences, we randomly selected close to 5K set pairs for use as validation sets, and 45K parallel sets for use in training. A second data source Training Validation Test # Sent pairs 2,608,679 4,771 1,312 is the Cambridge Learner Corpus (CLC) (Nicholls, 2003), from which we extracted a much larger set of parallel sets for use in training. Finally, we used additional training examples from the Lang-8 Corpus of Learner English v1.0 (Tajiri et al., 2012)."}, {"heading": "4.2 Baseline", "text": "We evaluate our model with attention compared to the strong baseline of a word-based neural sequence-to-sequence model, using post-processing to deal with words outside the vocabulary (Yuan and Briscoe, 2016); we refer to this model as a word replacement NMT + UNK. Like Yuan and Briscoe (2016), we use a traditional word alignment model (GIZA + +) to derive a word correction lexicon from the parallel training set. However, when decrypting, we do not use GIZA + + to find the corresponding source word for each tar-1http: / / www.comp.nus.edu.sg / \u02dc nlp / sw / m2scorer.tar.gzget OOV, but instead follow Cho et al. (2015), Section 3.3, to use the attention weights of the NMT system. The target OV of the word itself is then replaced by the most likely correction of the word there."}, {"heading": "4.3 Training Details and Results", "text": "To reproduce the Yuan and Briscoe model (2016), we selected the baseline word vocabulary by selecting the 30K most common words in the source and target to form the source and target vocabulary. In preliminary experiments for the hybrid models, we found that the selection of the vocabulary for the source and target was better based on the combined frequency (.003 in F0.5) and instead used this method for vocabulary selection and target vocabulary selection. However, there was no gain observed by using such a vocabulary selection method in the baseline. Although the source and target vocabularies in the hybrid models are the same as in the word-level model, the embedding parameters for the source and target word selection are not shared. Hyper parameters for the losses in our models are selected and are formed as follows with all models: \u03b2 = 0.5."}, {"heading": "4.4 Integrating a Web-scale Language Model", "text": "The value of large language models for grammatical error correction is well known, and such models have been used in classification and MT-based systems. To determine the potential of such models in word-based neural sequence systems, we are integrating a web scale, numbered language model provided for download by Junczys-Dowmunt and Grundkiewicz (2016). Candidates generated by neural models are reordered using the following linear interpolation of log probabilities: sy | x = logPNN (y | x) + \u03bb logPLM (y), a hyperparameter that balances the weights of the neural network model and the language model."}, {"heading": "5 Analysis", "text": "We analyze the effects of information at the subword level and the two nested attention levels more closely by looking at the performance of the models on different segments of the data. In particular, we analyze the performance of the models on sentences containing OOV source words versus those without OOV words, and corrections of orthographically similar or unequal word forms."}, {"heading": "5.1 Performance by Segment: OOV versus Non-OOV", "text": "We present a comparative performance analysis of models of the CoNLL-13 development set. First, we divide the set into two segments: OOV and NonOOV, based on whether there is at least one OOV word in the given source input. Table 5 shows that both hybrid architectures significantly outperform the word-level model in both segments of the data. The additional nested attention at the character level of our hybrid model brings a significant improvement over the basic hybrid model in the OOV segment and a low degradation in the non-OOV segment. We should note that in the future attention at the working character level can be added to non-OOV source words in the nested attention model, which could improve performance in this segment as well. Table 6 shows an example where the nested attention hybrid model successfully corrects a spelling problem that leads to an OOV error in the source of the base word, not simply observing the source of the OV during the specific source of the source word."}, {"heading": "5.2 Impact of Nested Attention on Different Error Types", "text": "To further analyze the impact of the extra attention at the character level introduced by our design, we continue to focus on the OOV segment in more detail.The concept of editing, which is also used by the official M2 score metric, is defined as a minimal pair of corresponding sub-strings in a source set and a correction. For example, in the sentence-fragment pair: \"Even if there is a risk of harming someone, people still prefer to keep their pets without a leash.\" The minimum edits are \"harms \u2192 harm\" and \"prefers.\" The F0.5 score is calculated with weighted precision and reminder of the system of editing to someone who is holding their pets without a leash. \"The one-year processing time is very high.\""}, {"heading": "6 Conclusions", "text": "We have introduced a novel hybrid neural model with two nested attention levels: word-level and character-level. The model addresses the unique challenges of the task of grammatical error correction and achieves the best results on the CoNLL-14 benchmark among fully neural systems. Our nested attention hybrid model deeply combines the strengths of the word-level and character-level in all components of an integral neural model: the encoder, the attention layers, and the decoder, enabling both global word-level and local character-level errors to be corrected in a consistent manner. The new architecture significantly improves the correction of confusion between rare or orthographically similar words compared to word-level sequences and non-nested hybrid models."}, {"heading": "Acknowledgements", "text": "We would like to thank the ACL reviewers for their insightful suggestions, Victoria Zayats for her help in reproducing the NMT system at word level, and Yu Shi, Daxin Jiang and Michael Zeng for the helpful discussions."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Correcting ESL errors using phrasal SMT techniques", "author": ["Chris Brockett", "William B Dolan", "Michael Gamon."], "venue": "Proceedings of the 21st International", "citeRegEx": "Brockett et al\\.,? 2006", "shortCiteRegEx": "Brockett et al\\.", "year": 2006}, {"title": "N-gram counts and language models from the Common Crawl", "author": ["Christian Buck", "Kenneth Heafield", "Bas Van Ooyen."], "venue": "LREC.", "citeRegEx": "Buck et al\\.,? 2014", "shortCiteRegEx": "Buck et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of ACL.", "citeRegEx": "Cho et al\\.,? 2015", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "Neural network translation models for grammatical error correction", "author": ["Shamil Chollampatt", "Kaveh Taghipour", "Hwee Tou Ng."], "venue": "Proceedings of IJCAI.", "citeRegEx": "Chollampatt et al\\.,? 2016", "shortCiteRegEx": "Chollampatt et al\\.", "year": 2016}, {"title": "Building a large annotated corpus of learner English: The NUS corpus of learner English", "author": ["Daniel Dahlmeier", "Hwee Tou Ng", "Siew Mei Wu."], "venue": "Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications.", "citeRegEx": "Dahlmeier et al\\.,? 2013", "shortCiteRegEx": "Dahlmeier et al\\.", "year": 2013}, {"title": "A large scale rankerbased system for search query spelling correction", "author": ["Jianfeng Gao", "Xiaolong(Shiao-Long) Li", "Daniel Micol", "Chris Quirk", "Xu Sun"], "venue": "In The 23rd International Conference on Computational Linguistics", "citeRegEx": "Gao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2010}, {"title": "Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit", "author": ["Richard HR Hahnloser", "Rahul Sarpeshkar", "Misha A Mahowald", "Rodney J Douglas", "H Sebastian Seung."], "venue": "Nature 405(6789):947\u2013951.", "citeRegEx": "Hahnloser et al\\.,? 2000", "shortCiteRegEx": "Hahnloser et al\\.", "year": 2000}, {"title": "Phrase-based machine translation is state-ofthe-art for automatic grammatical error correction", "author": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz."], "venue": "EMNLP.", "citeRegEx": "Junczys.Dowmunt and Grundkiewicz.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt and Grundkiewicz.", "year": 2016}, {"title": "Dragnn: A transitionbased framework for dynamically connected neural networks", "author": ["Lingpeng Kong", "Chris Alberti", "Daniel Andor", "Ivan Bogatyy", "David Weiss"], "venue": null, "citeRegEx": "Kong et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2017}, {"title": "Fully character-level neural machine translation without explicit segmentation", "author": ["Jason Lee", "Kyunghyun Cho", "Thomas Hofmann."], "venue": "TACL 5.", "citeRegEx": "Lee et al\\.,? 2017", "shortCiteRegEx": "Lee et al\\.", "year": 2017}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-", "citeRegEx": "Luong and Manning.,? 2016", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "The CoNLL-2014 shared task on grammatical error correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant."], "venue": "CoNLL Shared Task. pages 1\u201314.", "citeRegEx": "Ng et al\\.,? 2014", "shortCiteRegEx": "Ng et al\\.", "year": 2014}, {"title": "The Cambridge Learner Corpus: Error coding and analysis for lexicography and ELT", "author": ["Diane Nicholls."], "venue": "Proceedings of the Corpus Linguistics 2003 conference. volume 16, pages 572\u2013581.", "citeRegEx": "Nicholls.,? 2003", "shortCiteRegEx": "Nicholls.", "year": 2003}, {"title": "Grammatical error correction: Machine translation and classifiers", "author": ["Alla Rozovskaya", "Dan Roth."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pages 2205\u20132215.", "citeRegEx": "Rozovskaya and Roth.,? 2016", "shortCiteRegEx": "Rozovskaya and Roth.", "year": 2016}, {"title": "Reassessing the goals of grammatical error correction: Fluency instead of grammaticality", "author": ["Keisuke Sakaguchi", "Courtney Napoles", "Matt Post", "Joel Tetreault."], "venue": "Transactions of the Association for Computational Linguistics 4:169\u2013182.", "citeRegEx": "Sakaguchi et al\\.,? 2016", "shortCiteRegEx": "Sakaguchi et al\\.", "year": 2016}, {"title": "Sentence-level grammatical error identification as sequence-to-sequence correction", "author": ["Allen Schmaltz", "Yoon Kim", "Alexander M Rush", "Stuart M Shieber."], "venue": "arXiv preprint arXiv:1604.04677 .", "citeRegEx": "Schmaltz et al\\.,? 2016", "shortCiteRegEx": "Schmaltz et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Tense and aspect error correction for ESL learners using global context", "author": ["Toshikazu Tajiri", "Mamoru Komachi", "Yuji Matsumoto."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2.", "citeRegEx": "Tajiri et al\\.,? 2012", "shortCiteRegEx": "Tajiri et al\\.", "year": 2012}, {"title": "Neural language correction with character-based attention", "author": ["Ziang Xie", "Anand Avati", "Naveen Arivazhagan", "Dan Jurafsky", "Andrew Y. Ng."], "venue": "CoRR abs/1603.09727. http://arxiv.org/abs/1603.09727.", "citeRegEx": "Xie et al\\.,? 2016", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio."], "venue": "ICML. volume 14, pages 77\u201381.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Grammatical error correction using neural machine translation", "author": ["Zheng Yuan", "Ted Briscoe."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.", "citeRegEx": "Yuan and Briscoe.,? 2016", "shortCiteRegEx": "Yuan and Briscoe.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "One of the most successful approaches to grammatical error correction (GEC) is to cast the problem as (monolingual) machine translation (MT), where we translate from possibly ungrammatical English sentences to corrected ones (Brockett et al., 2006; Gao et al., 2010; Junczys-Dowmunt and Grundkiewicz, 2016).", "startOffset": 225, "endOffset": 306}, {"referenceID": 7, "context": "One of the most successful approaches to grammatical error correction (GEC) is to cast the problem as (monolingual) machine translation (MT), where we translate from possibly ungrammatical English sentences to corrected ones (Brockett et al., 2006; Gao et al., 2010; Junczys-Dowmunt and Grundkiewicz, 2016).", "startOffset": 225, "endOffset": 306}, {"referenceID": 9, "context": "One of the most successful approaches to grammatical error correction (GEC) is to cast the problem as (monolingual) machine translation (MT), where we translate from possibly ungrammatical English sentences to corrected ones (Brockett et al., 2006; Gao et al., 2010; Junczys-Dowmunt and Grundkiewicz, 2016).", "startOffset": 225, "endOffset": 306}, {"referenceID": 15, "context": "The approach has proven superior to systems based on local classifiers that can only fix focused errors in prepositions, determiners, or inflected forms (Rozovskaya and Roth, 2016).", "startOffset": 153, "endOffset": 180}, {"referenceID": 19, "context": "Recently, neural machine translation (NMT) systems have achieved substantial improvements in translation quality over phrase-based MT systems (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 142, "endOffset": 189}, {"referenceID": 0, "context": "Recently, neural machine translation (NMT) systems have achieved substantial improvements in translation quality over phrase-based MT systems (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 142, "endOffset": 189}, {"referenceID": 23, "context": "Thus, there is growing interest in applying neural systems to GEC (Yuan and Briscoe, 2016; Xie et al., 2016).", "startOffset": 66, "endOffset": 108}, {"referenceID": 21, "context": "Thus, there is growing interest in applying neural systems to GEC (Yuan and Briscoe, 2016; Xie et al., 2016).", "startOffset": 66, "endOffset": 108}, {"referenceID": 16, "context": "achieve native speaker fluency (Sakaguchi et al., 2016).", "startOffset": 31, "endOffset": 55}, {"referenceID": 23, "context": "Standard approaches in neural machine translation, also applied to grammatical error correction by Yuan and Briscoe (2016), address the large vocabulary problem by restricting the vocabulary to a limited number of high-frequency words and rear X iv :1 70 7.", "startOffset": 99, "endOffset": 123}, {"referenceID": 21, "context": "An alternative approach, proposed by Xie et al. (2016), applies a character-level sequence to sequence neural model.", "startOffset": 37, "endOffset": 55}, {"referenceID": 22, "context": "Our model differs substantially from the wordlevel S2S model of Yuan and Briscoe (2016) and the character-level S2S model of Xie et al.", "startOffset": 64, "endOffset": 88}, {"referenceID": 21, "context": "Our model differs substantially from the wordlevel S2S model of Yuan and Briscoe (2016) and the character-level S2S model of Xie et al. (2016)", "startOffset": 125, "endOffset": 143}, {"referenceID": 12, "context": "We extend the wordcharacter hybrid model of Luong and Manning (2016), which was originally developed for machine translation, by introducing a character attention layer.", "startOffset": 44, "endOffset": 69}, {"referenceID": 13, "context": "We validate the effectiveness of our model on the CoNLL-14 benchmark dataset (Ng et al., 2014).", "startOffset": 77, "endOffset": 94}, {"referenceID": 12, "context": "Results show that the proposed model outperforms all previous neural models for GEC, including the hybrid model of Luong and Manning (2016), which we apply to GEC for the first time.", "startOffset": 115, "endOffset": 140}, {"referenceID": 21, "context": "56 achieved by using a neural model and an external language model (Xie et al., 2016).", "startOffset": 67, "endOffset": 85}, {"referenceID": 15, "context": "Two notable advances after the shared task have been in the areas of combining classifiers and phrase-based MT (Rozovskaya and Roth, 2016) and adapting phrase-based MT to the GEC task (Junczys-Dowmunt and Grundkiewicz, 2016).", "startOffset": 111, "endOffset": 138}, {"referenceID": 9, "context": "Two notable advances after the shared task have been in the areas of combining classifiers and phrase-based MT (Rozovskaya and Roth, 2016) and adapting phrase-based MT to the GEC task (Junczys-Dowmunt and Grundkiewicz, 2016).", "startOffset": 184, "endOffset": 224}, {"referenceID": 12, "context": "The CoNLL-14 shared task overview paper of Ng et al. (2014) provides a comparative evaluation of approaches.", "startOffset": 43, "endOffset": 60}, {"referenceID": 9, "context": "Two notable advances after the shared task have been in the areas of combining classifiers and phrase-based MT (Rozovskaya and Roth, 2016) and adapting phrase-based MT to the GEC task (Junczys-Dowmunt and Grundkiewicz, 2016). The latter work has reported the highest performance to date on the task of 49.5 in F0.5 score on the CoNLL-14 test set. This method integrates discriminative training toward the task-specific evaluation function, a rich set of features, and multiple large language models. Neural approaches to the task are less explored. We believe that the advances from Junczys-Dowmunt and Grundkiewicz (2016) are complementary to the ones we propose for neural MT, and could be integrated with neural models to achieve even higher performance.", "startOffset": 185, "endOffset": 623}, {"referenceID": 21, "context": "Two prior works explored sequence to sequence neural models for GEC (Xie et al., 2016; Yuan and Briscoe, 2016), while Chollampatt et al.", "startOffset": 68, "endOffset": 110}, {"referenceID": 23, "context": "Two prior works explored sequence to sequence neural models for GEC (Xie et al., 2016; Yuan and Briscoe, 2016), while Chollampatt et al.", "startOffset": 68, "endOffset": 110}, {"referenceID": 17, "context": "Neural models were also applied to the related sub-task of grammatical error identification (Schmaltz et al., 2016).", "startOffset": 92, "endOffset": 115}, {"referenceID": 5, "context": ", 2016; Yuan and Briscoe, 2016), while Chollampatt et al. (2016) integrated neural features in a phrase-based system for the task.", "startOffset": 39, "endOffset": 65}, {"referenceID": 5, "context": ", 2016; Yuan and Briscoe, 2016), while Chollampatt et al. (2016) integrated neural features in a phrase-based system for the task. Neural models were also applied to the related sub-task of grammatical error identification (Schmaltz et al., 2016). Yuan and Briscoe (2016) demonstrated the promise of neural MT for GEC but did not adapt the basic sequence-to-sequence with attention to its unique challenges, falling back to traditional word-alignment models to address vocabulary coverage with a post-processing heuristic.", "startOffset": 39, "endOffset": 272}, {"referenceID": 5, "context": ", 2016; Yuan and Briscoe, 2016), while Chollampatt et al. (2016) integrated neural features in a phrase-based system for the task. Neural models were also applied to the related sub-task of grammatical error identification (Schmaltz et al., 2016). Yuan and Briscoe (2016) demonstrated the promise of neural MT for GEC but did not adapt the basic sequence-to-sequence with attention to its unique challenges, falling back to traditional word-alignment models to address vocabulary coverage with a post-processing heuristic. Xie et al. (2016) built a character-level sequence", "startOffset": 39, "endOffset": 541}, {"referenceID": 12, "context": "We build mostly on the hybrid model of Luong and Manning (2016) to expand its capability to correct rare words by fine-grained character-level attention.", "startOffset": 39, "endOffset": 64}, {"referenceID": 12, "context": "We build mostly on the hybrid model of Luong and Manning (2016) to expand its capability to correct rare words by fine-grained character-level attention. We directly compare our model to the one of Luong and Manning (2016) on the grammar correction task.", "startOffset": 39, "endOffset": 223}, {"referenceID": 18, "context": "open vocabulary (Sennrich et al., 2016), and more recently, fully character-level modeling (Lee et al.", "startOffset": 16, "endOffset": 39}, {"referenceID": 11, "context": ", 2016), and more recently, fully character-level modeling (Lee et al., 2017).", "startOffset": 59, "endOffset": 77}, {"referenceID": 0, "context": "The word-based backbone closely follows the basic neural sequence-to-sequence architecture with attention as proposed by Bahdanau et al. (2015) and applied to grammatical error correction by Yuan and Briscoe (2016).", "startOffset": 121, "endOffset": 144}, {"referenceID": 0, "context": "The word-based backbone closely follows the basic neural sequence-to-sequence architecture with attention as proposed by Bahdanau et al. (2015) and applied to grammatical error correction by Yuan and Briscoe (2016). For completeness, we give a sketch here.", "startOffset": 121, "endOffset": 215}, {"referenceID": 3, "context": "The hidden state ht at time t is computed as: ft = GRUencf (ft\u22121, xt) , bt = GRUencb(bt+1, xt), ht = [ft; bt], where GRUencf and GRUencb stand for gated recurrent unit functions as described in Cho et al. (2014). We use the symbol GRU with different subscripts to represent GRU functions using different sets of parameters (for example, we used the encf and encb subscripts to denote the parameters of the forward and backward word-level encoder units.", "startOffset": 194, "endOffset": 212}, {"referenceID": 8, "context": "ReLU indicates rectified linear units (Hahnloser et al., 2000).", "startOffset": 38, "endOffset": 62}, {"referenceID": 12, "context": "Our hybrid source encoder architecture is similar to the one proposed by Luong and Manning (2016).", "startOffset": 73, "endOffset": 98}, {"referenceID": 4, "context": "A post-processing UNK-replacement method is then used (Cho et al., 2015; Yuan and Briscoe, 2016) to replace these special tokens with target words.", "startOffset": 54, "endOffset": 96}, {"referenceID": 23, "context": "A post-processing UNK-replacement method is then used (Cho et al., 2015; Yuan and Briscoe, 2016) to replace these special tokens with target words.", "startOffset": 54, "endOffset": 96}, {"referenceID": 12, "context": "The hybrid model of (Luong and Manning, 2016) uses a jointly trained character-level decoder to generate target words corresponding to UNK tokens, and outperforms the traditional approach in the machine translation task.", "startOffset": 20, "endOffset": 45}, {"referenceID": 12, "context": "The architecture of Luong and Manning (2016)", "startOffset": 20, "endOffset": 45}, {"referenceID": 12, "context": "As in Luong and Manning (2016), the \u201cseparate path\u201d architecture is used to capture the relevant context and define the initial state for the character-level decoder:", "startOffset": 6, "endOffset": 31}, {"referenceID": 22, "context": "To be able to attend to the relevant source character sequence when generating the target character sequence, we use the concept of hard attention (Xu et al., 2015), but use an arg-max approximation for inference instead of sampling.", "startOffset": 147, "endOffset": 164}, {"referenceID": 10, "context": "A similar approach to represent discrete hidden structure in a variety of architectures is used in Kong et al. (2017).", "startOffset": 99, "endOffset": 118}, {"referenceID": 12, "context": "If the source word xzs is in the source vocabulary, the model is analogous to the one of Luong and Manning (2016) and does not use characterlevel attention: the source context is available only in aggregated form to initialize the state of the decoder.", "startOffset": 89, "endOffset": 114}, {"referenceID": 12, "context": "While the model can be naturally generalized to integrate characterlevel attention for known words, the original hybrid model proposed by Luong and Manning (2016) does not use any character-level information for known words.", "startOffset": 138, "endOffset": 163}, {"referenceID": 6, "context": "One data source is the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013), which is provided as a training set for the CoNLL-13 and CoNLL-14 shared tasks.", "startOffset": 61, "endOffset": 85}, {"referenceID": 14, "context": "is the Cambridge Learner Corpus (CLC) (Nicholls, 2003), from which we extracted a substantially larger set of parallel sentences.", "startOffset": 38, "endOffset": 54}, {"referenceID": 20, "context": "0 (Tajiri et al., 2012).", "startOffset": 2, "endOffset": 23}, {"referenceID": 13, "context": "We evaluate the performance of the models on the standard sets from the CoNLL-14 shared task (Ng et al., 2014).", "startOffset": 93, "endOffset": 110}, {"referenceID": 6, "context": "alyze model performance on the CoNLL-13 development set (Dahlmeier et al., 2013).", "startOffset": 56, "endOffset": 80}, {"referenceID": 23, "context": "We evaluate our model in comparison to the strong baseline of a word-based neural sequenceto-sequence model with attention, with postprocessing for handling out-of-vocabulary words (Yuan and Briscoe, 2016); we refer to this model as word NMT+UNK replacement.", "startOffset": 181, "endOffset": 205}, {"referenceID": 23, "context": "We evaluate our model in comparison to the strong baseline of a word-based neural sequenceto-sequence model with attention, with postprocessing for handling out-of-vocabulary words (Yuan and Briscoe, 2016); we refer to this model as word NMT+UNK replacement. Like Yuan and Briscoe (2016), we use a traditional wordalignment model (GIZA++) to derive a wordcorrection lexicon from the parallel training set.", "startOffset": 182, "endOffset": 288}, {"referenceID": 3, "context": "gz get OOV, but follow Cho et al. (2015), Section 3.", "startOffset": 23, "endOffset": 41}, {"referenceID": 23, "context": "To reproduce the model of Yuan and Briscoe (2016), we selected the word vocabulary for the baseline by choosing the 30K most frequent words in the source and target respectively to form the source and target vocabularies.", "startOffset": 26, "endOffset": 50}, {"referenceID": 12, "context": "In addition to the word-level baseline, we include the performance of a hybrid model with a single level of attention, which follows the work of Luong and Manning (2016) for machine translation, and is the first application of a hybrid word/character-level model to grammatical error correction.", "startOffset": 145, "endOffset": 170}, {"referenceID": 23, "context": "As shown in Table 3, our implementation of the word NMT+UNK replacement baseline approaches the performance of the one reported in Yuan and Briscoe (2016) (38.", "startOffset": 131, "endOffset": 155}, {"referenceID": 2, "context": "In particular, we use the modified Kneser-Ney 5-gram language model trained from Common Crawl (Buck et al., 2014), made available for download by Junczys-Dowmunt and Grundkiewicz (2016).", "startOffset": 94, "endOffset": 113}, {"referenceID": 2, "context": "In particular, we use the modified Kneser-Ney 5-gram language model trained from Common Crawl (Buck et al., 2014), made available for download by Junczys-Dowmunt and Grundkiewicz (2016).", "startOffset": 95, "endOffset": 186}, {"referenceID": 21, "context": "Character-based NMT + LM (Xie et al., 2016) 40.", "startOffset": 25, "endOffset": 43}, {"referenceID": 21, "context": "The table also lists the results reported by Xie et al. (2016), for their character-level neural model combined with a large word-level language model.", "startOffset": 45, "endOffset": 63}, {"referenceID": 21, "context": "in the prior work by more than 4 points, although we should note that Xie et al. (2016) used a smaller parallel data set for training.", "startOffset": 70, "endOffset": 88}], "year": 2017, "abstractText": "Grammatical error correction (GEC) systems strive to correct both global errors in word order and usage, and local errors in spelling and inflection. Further developing upon recent work on neural machine translation, we propose a new hybrid neural model with nested attention layers for GEC. Experiments show that the new model can effectively correct errors of both types by incorporating word and character-level information, and that the model significantly outperforms previous neural models for GEC as measured on the standard CoNLL14 benchmark dataset. Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective in correcting local errors that involve small edits in orthography.", "creator": "LaTeX with hyperref package"}}}