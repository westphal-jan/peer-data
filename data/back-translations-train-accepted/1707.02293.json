{"id": "1707.02293", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jul-2017", "title": "Bayesian Models of Data Streams with Hierarchical Power Priors", "abstract": "Making inferences from data streams is a pervasive problem in many modern data analysis applications. But it requires to address the problem of continuous model updating and adapt to changes or drifts in the underlying data generating distribution. In this paper, we approach these problems from a Bayesian perspective covering general conjugate exponential models. Our proposal makes use of non-conjugate hierarchical priors to explicitly model temporal changes of the model parameters. We also derive a novel variational inference scheme which overcomes the use of non-conjugate priors while maintaining the computational efficiency of variational methods over conjugate models. The approach is validated on three real data sets over three latent variable models.", "histories": [["v1", "Fri, 7 Jul 2017 09:44:15 GMT  (5603kb,D)", "http://arxiv.org/abs/1707.02293v1", "ICML 2017"]], "COMMENTS": "ICML 2017", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["andr\u00e9s r masegosa", "thomas d nielsen", "helge langseth", "dar\u00edo ramos-l\u00f3pez", "antonio salmer\u00f3n", "anders l madsen"], "accepted": true, "id": "1707.02293"}, "pdf": {"name": "1707.02293.pdf", "metadata": {"source": "CRF", "title": "Bayesian Models of Data Streams with Hierarchical Power Priors", "authors": ["Andr\u00e9s Masegosa", "Thomas D. Nielsen", "Helge Langseth", "Dar\u0131\u0301o Ramos-L\u00f3pez", "Antonio Salmer\u00f3n", "Anders L. Madsen"], "emails": ["<andresmasegosa@ual.es>."], "sections": [{"heading": "1. Introduction", "text": "Flexible and computationally efficient models for streaming data are needed in many machine learning applications, and in this paper we propose a new class of models for these situations. Specifically, we are interested in models that are suitable for areas where changes in the underlying generative process occur (Gama et al., 2014). We envision a situation where you get stacks of data at discrete times. As each new batch arrives, we want to derive information from the new data while retaining relevant information from historical observations. Our modelling is inspired by previous work on recursive estimation (O \u00bc zkan et al., 2013; Ka \u0301 rny, 2014), power priors (Ibrahim & Chen, 2000) and exponential for-1Department of Mathematics, Unversity of Almeri \u0301 a, Almeri \u0301 a, Spain 2Department of Computer and Information Science, Norwegian University of Science and Technology, Trondheim, Norway 3Department of Computer Science Aalborg, University PM4S, Denmark 2014S."}, {"heading": "2. Preliminaries", "text": "To simplify the presentation, we will first focus on the model structure shown in Figure 1 (a). This model includes the observed data x = xi = 1: N, global hidden variables (or parameters) \u03b2 = \u03b21: M, a set of local hidden variables z = z1: N, and a vector of fixed (hyper) parameters designated by \u03b1. Notice how the dynamics of the process are not included in the model of Figure 1 (a); the model is specified in the context of the data streams in Section 4, where we expand it to include explicit dynamics about the (global) parameters to capture the concept of drift. With the conditional distributions in the model belonging to the exponential family, we have that all distributions are of the following formln p (Y | pa)."}, {"heading": "3. Related Work", "text": "This is an unrealistic problem if one assumes that the VB data is repeatedly delegated. (Doucet et al., 2000; Yao et al., 2009) In the context of varying conclusions, there are two main approaches. Ghahramani & Attias (2000); Broderick et al. (2013) suggest a recursive update of the varying approximation. Bayes (SVB) streaming variation approach depends on Broderick et al., 2013) is the most well-known approach in this category. Alternatively, the follow-up problem could be considered a stochastic optimization problem. Stochastic variation conclusions (SVI) (Hoffman et al., 2013) and the closely related Bayes (PVB) population variations (McInerney et al., 2015) are prominent examples from this group. SVI assumes the existence of a fixed data set to be observed sequentially, and in particular that this data set has a known size."}, {"heading": "4. Hierarchical Power Priors", "text": "In this section, we expand the model in Figure 1 (a) to take into account the dynamics of the data stream to be modeled. Here, we assume that only the parameters \u03b2 in Figure 1 (a) are time variable, which we specify with the subscript t, i.e. \u00df. First, we briefly describe the approach on which the proposed model is based. Then, we present the hierarchical performance and explain a varying sequencing procedure for this model class."}, {"heading": "4.1. Power Priors as Implicit Transition Models", "text": "In order to extend the model in Figure 1 (a) to data streams, we can introduce a transition model p (\u03b2t-1) for explicitly modelling the evolution of parameters over time, which allows estimating the predictive density over time. (3) However, this approach poses two problems. Firstly, in non-stationary domains we may not have a single transition model or transition model. Secondly, if we try to position the model within the conjugated exponential family in order to be able to calculate the gradients of the L in a closed form, we must ensure that the distribution family is limited for its own conjugated distribution, i.e. highly limited..."}, {"heading": "4.2. The Hierarchical Power Prior Model", "text": "In the approach taken by O \ufffd zkan et al. (2013) (and more broadly by SVB-PP), the forgetfulness factor \u03c1t is user-defined. Instead, in this paper we take a (hierarchical) Bayesian approach and introduce a prior distribution via \u03c1t, which allows the distribution via \u03c1t (and hence the forgetfulness mechanism) to adapt to the data flow. As we will see below, in order to support a variable update scheme, we must restrict the previous distribution via \u03c1t, effectively limiting the selection of the previous distribution to 1. Thus, for example, the ESS of a beta distribution is equal to the sum of the components of \u03bbt and thus equal to the number of data samples seen so far plus the pseudo-examples of the previous one. Either an exponential distribution or a normal distribution with fixed variance, both of which should be truncated to the interval [0, 1]."}, {"heading": "4.3. Variational Updating", "text": "For updating the model distributions, we take a variable approach in which we try to maximize the evidence in Equation (2) for the time step (2). However, since the model in Figure 1 (b) cannot directly maximize a conjugated exponential distribution due to the introduction of p (c), we will instead derive a (double) lower limited L (b) and use this lower limit as a proxy for the updating rules for the variational posteriors. First, we cannot directly define the lower bound LHPP (2) by using the lower LHPP (2) in Equation (2) for the HPP model we observe."}, {"heading": "4.4. The Multiple Hierarchical Power Prior Model", "text": "The HPP model can be immediately expanded to include multiple power priors \u03c1 (i) t, one for each global parameter \u03b2i. In this model, the \u03c1 (i) t are independent in pairs. The latter ensures that the optimization of the L model can be performed as described above, since the variation distribution for each \u03c1 (i) t can be updated independently of the other variation distributions via \u03c1 (j) t, for j 6 = i. This advanced model allows local model substructures to have different oblivion mechanisms, thereby extending the expressivity of the model. We will refer to this extended model as a model of multiple hierarchical power priors (MHPP)."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Experimental Set-up", "text": "In this section we will evaluate the following methods: \u2022 Streaming Variational Bayes (SVB). \u2022 Four versions of Population Variational Bayes (PVB) 2: Population size M equal to the average size of each data stack or M equal to a fixed value (M = 1000 in Section 5.2 and M = 10,000 in Section 5.3). \u2022 Learning-rate \u03bd = 0.1 or \u03bd = 0.01. \u2022 Two versions of SVB-PP: \u03c1 = 0.9 or \u03c1 = 0.99. \u2022 Two versions of SVB-HPP: A single common \u03c1 (de-noted SVB-HPP) or separate \u03c1 (i) parameter (SVBMHPP). The underlying variation engine is the VMP algorithm (Winn & Bishop, 2005) for all models; VMP was termi-2We do not compare with SVI because SVI is a special case of PVB when M is equal to the total size of the stream."}, {"heading": "5.2. Evaluation using an Artificial Data Set", "text": "First, we illustrate the behavior of the different approaches in a controlled experimental environment: We produced an artificial data stream by generating 100 samples (i.e., | xt | = 100) from a binomial distribution in each time step. We artificially introduced a concept drift by modifying the parameter p of the binomial distribution: p = 0.2 for the first 30 time steps, then p = 0.5 for the next 30 time steps, and finally p = 0.8 for the last 40 time steps. The data stream was modeled using a beta binomial model. Parameter Estimation: Figure 3 shows the evolution of Eq [\u03b2t] for the various methods. We realize that SVB simply generates a running mean of the data \u2212 St \u2212 because it is unable to adapt to the concept drift. PVB results strongly depend on the learning rate, with the higher learning rate reflected in the more aggressive axis working better."}, {"heading": "5.3. Evaluation using Real Data Sets", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.3.1. DATA AND MODELS", "text": "For this evaluation, we consider three real datasets from different areas: Electricity Market (Harries, 1999): The dataset describes the electricity market of two Australian states. It contains 45,312 cases of 6 attributes, including a class designation that compares the change in electricity prices with a moving average over the past 24 hours. Each instance in the dataset represents 30 minutes of trading; during our analysis, we created batches that contain all the information associated with the month.The data are also analyzed using a Bayesian linear regression model. The binary class designation is assumed to follow a Gaussian distribution to fit within the conjugated model class. Similarly, the boundary densities of the predictive attributes are also assumed to be a Gaussian mixture. Regression coefficients are given Gaussian previous distributions, and the variance is given with a gamma beforehand. Note that the total distribution does not fall into the conditional conjugated family (Hoffman)."}, {"heading": "5.3.2. EVALUATION AND DISCUSSION", "text": "To evaluate the various methods we have discussed, we look at the test margin probability PP-VB (TMLVB). Specifically, each set of data is randomly divided into a train dataset, and a test dataset containing two-thirds and one-third of the data stack is relatively high in each case. However, then TMLLt is calculated as TMLt = 1, and this is considered the basic method by SVB. To improve readability, we only record the results of the best-performing method within each group of methods. The right side of Figure 5 shows for each method the difference between its TMLLt and the stream received from SVB. To improve readability, we show the results of the method within each group of methods. Figure 5 shows the evolution of Eq [over time] for SVB-HPP and SVB-MHPP."}, {"heading": "6. Conclusions and Future Work", "text": "Unlike existing solutions to this problem, which aim at modelling slow-changing processes, our proposal is capable of coping with both abrupt and gradual concept drift according to a Bayesian approach; the new model takes into account the dynamics of the data stream by assuming that only the global parameters evolve over time; we are introducing the so-called hierarchical performance protocols, which provide a prior measurement of the learning rate that allows it to adapt to the data stream; we have addressed the complexity of the underlying inference tasks by developing an approximate variation scheme that optimizes a novel lower limit of the probability function; and, as a future work, we aim to offer a solid approach to semantic characterization of the concept drift by verifying the E [\u03c1 (i) t] values provided by SVB-MHPP."}, {"heading": "Acknowledgements", "text": "This work was carried out partly within the framework of the AMIDST project. AMIDST received funding from the Seventh Framework Programme of the European Union for Research, Technological Development and Demonstration under Funding Agreement 619209. Furthermore, this research was partly financed by the Spanish Ministry of Economy and Competitiveness through the projects TIN2015-74368JIN, TIN2013-46638-C3-1-P, TIN2016-77902-C3-3-P and ERDF funds."}, {"heading": "A. Proof of Theorem 1 and Lemma 2", "text": "In the specification of L we have that Eq (1 + (1 + (1 \u2212)), that Eq (1 + (1 \u2212)), Eq (1 + (1 \u2212)), Eq (1 + (1 \u2212)), Eq (1 \u2212), Eq (1 + (1 \u2212)), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (\u2212), Eq (\u2212), Eq (\u2212), Eq (\u2212), Eq (\u2212), Eq (\u2212), Eq (\u2212), Eq (\u2212, \u2212, \u2212), Eq (\u2212, Eq, Eq, Eq (\u2212, Eq), Eq (\u2212, Eq), Eq (\u2212, \u2212), Eq (\u2212, \u2212), Eq (\u2212), Eq (\u2212), Eq (\u2212), Eq (\u2212), Eq (\u2212), Eq (\u2212), Eq (\u2212), Eq (\u2212), Eq (\u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212, Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212), Eq (1 \u2212"}, {"heading": "B. Experimental Evaluation", "text": "B.1. Probabilistic modelsWe provide a (simplified) graphical description of the probability models used in the experiments. We also detail the distribution assumptions of the parameters which are then used to define the variable approximation. ELECTRICITY MODELx1, t x2, t x3, tyt (\u00b5i, \u03b3i) \u0445 NormalGamma (1, 1, 0, 1e \u2212 10) \u0421.L G] 7J ul2 017ztDaytxt ytGPS MODELp \u00b2 Dirichlet (1,.."}], "references": [{"title": "Online inference for the infinite topic-cluster model: Storylines from streaming text", "author": ["Ahmed", "Amr", "Ho", "Qirong", "Teo", "Choon Hui", "Eisenstein", "Jacob", "Smola", "Alexander J", "Xing", "Eric P"], "venue": "In AISTATS, pp", "citeRegEx": "Ahmed et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ahmed et al\\.", "year": 2011}, {"title": "Streaming variational Bayes", "author": ["Broderick", "Tamara", "Boy", "Nicholas", "Wibisono", "Andre", "Wilson", "Ashia C", "Jordan", "Michael I"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Broderick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Broderick et al\\.", "year": 2013}, {"title": "Financial data analysis with PGMs using AMIDST", "author": ["Caba\u00f1as", "Rafael", "Mart\u0131\u0301nez", "Ana M", "Masegosa", "Andr\u00e9s R", "Ramos-L\u00f3pez", "Dar\u0131\u0301o", "Samer\u00f3n", "Antonio", "Nielsen", "Thomas D", "Langseth", "Helge", "Madsen", "Anders L"], "venue": "In Data Mining Workshops (ICDMW),", "citeRegEx": "Caba\u00f1as et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Caba\u00f1as et al\\.", "year": 2016}, {"title": "On sequential Monte Carlo sampling methods for Bayesian filtering", "author": ["Doucet", "Arnaud", "Godsill", "Simon", "Andrieu", "Christophe"], "venue": "Statistics and computing,", "citeRegEx": "Doucet et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Doucet et al\\.", "year": 2000}, {"title": "A survey on concept drift adaptation", "author": ["Gama", "Jo\u00e3o", "\u017dliobait\u0117", "Indr\u0117", "Bifet", "Albert", "Pechenizkiy", "Mykola", "Bouchachia", "Abdelhamid"], "venue": "ACM Computing Surveys,", "citeRegEx": "Gama et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gama et al\\.", "year": 2014}, {"title": "Online variational Bayesian learning", "author": ["Ghahramani", "Zoubin", "H. Attias"], "venue": "In Slides from talk presented at NIPS workshop on Online Learning,", "citeRegEx": "Ghahramani et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ghahramani et al\\.", "year": 2000}, {"title": "Splice-2 comparative evaluation: Electricity pricing. NSW-CSE-TR-9905", "author": ["Harries", "Michael"], "venue": "School of Computer Siene and Engineering,", "citeRegEx": "Harries and Michael.,? \\Q1999\\E", "shortCiteRegEx": "Harries and Michael.", "year": 1999}, {"title": "Learning Bayesian networks: The combination of knowledge and statistical data", "author": ["Heckerman", "David", "Geiger", "Dan", "Chickering", "David M"], "venue": "Machine learning,", "citeRegEx": "Heckerman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Heckerman et al\\.", "year": 1995}, {"title": "Stochastic variational inference", "author": ["Hoffman", "Matthew D", "Blei", "David M", "Wang", "Chong", "Paisley", "John"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "On-line variational Bayesian learning", "author": ["Honkela", "Antti", "Valpola", "Harri"], "venue": "In 4th International Symposium on Independent Component Analysis and Blind Signal Separation,", "citeRegEx": "Honkela et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Honkela et al\\.", "year": 2003}, {"title": "Power prior distributions for regression models", "author": ["Ibrahim", "Joseph G", "Chen", "Ming-Hui"], "venue": "Statistical Science, pp", "citeRegEx": "Ibrahim et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ibrahim et al\\.", "year": 2000}, {"title": "On optimality properties of the power prior", "author": ["Ibrahim", "Joseph G", "Chen", "Ming-Hui", "Sinha", "Debajyoti"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Ibrahim et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ibrahim et al\\.", "year": 2003}, {"title": "Approximate Bayesian recursive estimation", "author": ["K\u00e1rn\u1ef3", "Miroslav"], "venue": "Information Sciences,", "citeRegEx": "K\u00e1rn\u1ef3 and Miroslav.,? \\Q2014\\E", "shortCiteRegEx": "K\u00e1rn\u1ef3 and Miroslav.", "year": 2014}, {"title": "Propagation of probabilities, means, and variances in mixed graphical association models", "author": ["Lauritzen", "Steffen L"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Lauritzen and L.,? \\Q1992\\E", "shortCiteRegEx": "Lauritzen and L.", "year": 1992}, {"title": "d-VMP: Distributed variational message passing", "author": ["A.R. Masegosa", "A.M. Mart\u0131\u0301nez", "H. Langseth", "T.D. Nielsen", "A. Salmer\u00f3n", "D. Ramos-L\u00f3pez", "A.L. Madsen"], "venue": "In PGM\u20192016. JMLR: Workshop and Conference Proceedings,", "citeRegEx": "Masegosa et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Masegosa et al\\.", "year": 2016}, {"title": "Probabilistic graphical models on multi-core cpus using Java 8", "author": ["Masegosa", "Andres R", "Martinez", "Ana M", "Borchani", "Hanen"], "venue": "IEEE Computational Intelligence Magazine,", "citeRegEx": "Masegosa et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Masegosa et al\\.", "year": 2016}, {"title": "Amidst: a Java toolbox for scalable probabilistic machine learning", "author": ["Masegosa", "Andr\u00e9s R", "Mart\u0131\u0301nez", "Ana M", "Ramos-L\u00f3pez", "Dar\u0131\u0301o", "Caba\u00f1as", "Rafael", "Salmer\u00f3n", "Antonio", "Nielsen", "Thomas D", "Langseth", "Helge", "Madsen", "Anders L"], "venue": "arXiv preprint arXiv:1704.01427,", "citeRegEx": "Masegosa et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Masegosa et al\\.", "year": 2017}, {"title": "The population posterior and Bayesian modeling on streams", "author": ["McInerney", "James", "Ranganath", "Rajesh", "Blei", "David"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "McInerney et al\\.,? \\Q2015\\E", "shortCiteRegEx": "McInerney et al\\.", "year": 2015}, {"title": "Marginalized adaptive particle filtering for nonlinear models with unknown time-varying noise parameters", "author": ["\u00d6zkan", "Emre", "\u0160mdl", "V\u00e1clav", "Saha", "Saikat", "Lundquist", "Christian", "Gustafsson", "Fredrik"], "venue": null, "citeRegEx": "\u00d6zkan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "\u00d6zkan et al\\.", "year": 2013}, {"title": "Online model selection based on the variational Bayes", "author": ["Sato", "Masa-Aki"], "venue": "Neural Computation,", "citeRegEx": "Sato and Masa.Aki.,? \\Q2001\\E", "shortCiteRegEx": "Sato and Masa.Aki.", "year": 2001}, {"title": "Variational message passing", "author": ["Winn", "John M", "Bishop", "Christopher M"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Winn et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Winn et al\\.", "year": 2005}, {"title": "Efficient methods for topic model inference on streaming document collections", "author": ["Yao", "Limin", "Mimno", "David", "McCallum", "Andrew"], "venue": "In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Yao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2009}, {"title": "Understanding mobility based on gps data", "author": ["Zheng", "Yu", "Li", "Quannan", "Chen", "Yukun", "Xie", "Xing", "Ma", "Wei-Ying"], "venue": "In Proceedings of the 10th International Conference on Ubiquitous Computing,", "citeRegEx": "Zheng et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2008}, {"title": "Mining interesting locations and travel sequences from gps trajectories", "author": ["Zheng", "Yu", "Zhang", "Lizhu", "Xie", "Xing", "Ma", "Wei-Ying"], "venue": "In Proceedings of the 18th International Conference on World Wide Web,", "citeRegEx": "Zheng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2009}, {"title": "Geolife: A collaborative social networking service among user, location and trajectory", "author": ["Zheng", "Yu", "Xie", "Xing", "Ma", "Wei-Ying"], "venue": "IEEE Data Eng. Bull.,", "citeRegEx": "Zheng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 4, "context": "Specifically, we are interested in models suitable for domains that exhibit changes in the underlying generative process (Gama et al., 2014).", "startOffset": 121, "endOffset": 140}, {"referenceID": 18, "context": "Our modelling is inspired by previous works on Bayesian recursive estimation (\u00d6zkan et al., 2013; K\u00e1rn\u1ef3, 2014), power priors (Ibrahim & Chen, 2000) and exponential forDepartment of Mathematics, Unversity of Almer\u0131\u0301a, Almer\u0131\u0301a, Spain Department of Computer and Information Science, Norwegian University of Science and Technology, Trondheim, Norway Department of Computer Science, Aalborg University, Aalborg, Denmark Hugin Expert A/S, Aalborg, Denmark.", "startOffset": 77, "endOffset": 110}, {"referenceID": 16, "context": "com) (Masegosa et al., 2017; 2016b; Caba\u00f1as et al., 2016).", "startOffset": 5, "endOffset": 57}, {"referenceID": 2, "context": "com) (Masegosa et al., 2017; 2016b; Caba\u00f1as et al., 2016).", "startOffset": 5, "endOffset": 57}, {"referenceID": 8, "context": "Extending the notation of Hoffman et al. (2013), we have that", "startOffset": 26, "endOffset": 48}, {"referenceID": 0, "context": "Bayesian inference on streaming data has been widely studied (Ahmed et al., 2011; Doucet et al., 2000; Yao et al., 2009).", "startOffset": 61, "endOffset": 120}, {"referenceID": 3, "context": "Bayesian inference on streaming data has been widely studied (Ahmed et al., 2011; Doucet et al., 2000; Yao et al., 2009).", "startOffset": 61, "endOffset": 120}, {"referenceID": 21, "context": "Bayesian inference on streaming data has been widely studied (Ahmed et al., 2011; Doucet et al., 2000; Yao et al., 2009).", "startOffset": 61, "endOffset": 120}, {"referenceID": 1, "context": "The streaming variational Bayes (SVB) algorithm (Broderick et al., 2013) is the most known approach of this category.", "startOffset": 48, "endOffset": 72}, {"referenceID": 8, "context": "Stochastic variational inference (SVI) (Hoffman et al., 2013) and the closely related population variational Bayes (PVB) (McInerney et al.", "startOffset": 39, "endOffset": 61}, {"referenceID": 17, "context": ", 2013) and the closely related population variational Bayes (PVB) (McInerney et al., 2015) are prominent examples from this group.", "startOffset": 67, "endOffset": 91}, {"referenceID": 4, "context": ", with concept drift (Gama et al., 2014)) is not addressed by SVB, as it assumes data exchangeability.", "startOffset": 21, "endOffset": 40}, {"referenceID": 11, "context": "The so-called power prior approach (Ibrahim & Chen, 2000) is also based on an exponential forgetting mechanisms, and has nice theoretical properties (Ibrahim et al., 2003).", "startOffset": 149, "endOffset": 171}, {"referenceID": 0, "context": "Bayesian inference on streaming data has been widely studied (Ahmed et al., 2011; Doucet et al., 2000; Yao et al., 2009). In the context of variational inference, there are two main approaches. Ghahramani & Attias (2000); Broderick et al.", "startOffset": 62, "endOffset": 221}, {"referenceID": 0, "context": "Bayesian inference on streaming data has been widely studied (Ahmed et al., 2011; Doucet et al., 2000; Yao et al., 2009). In the context of variational inference, there are two main approaches. Ghahramani & Attias (2000); Broderick et al. (2013) propose recursive Bayesian updating of the variational approximation.", "startOffset": 62, "endOffset": 246}, {"referenceID": 0, "context": "Bayesian inference on streaming data has been widely studied (Ahmed et al., 2011; Doucet et al., 2000; Yao et al., 2009). In the context of variational inference, there are two main approaches. Ghahramani & Attias (2000); Broderick et al. (2013) propose recursive Bayesian updating of the variational approximation. The streaming variational Bayes (SVB) algorithm (Broderick et al., 2013) is the most known approach of this category. Alternatively, one could cast the inference problem as a stochastic optimization problem. Stochastic variational inference (SVI) (Hoffman et al., 2013) and the closely related population variational Bayes (PVB) (McInerney et al., 2015) are prominent examples from this group. SVI assumes the existence of a fixed data set observed in a sequential manner, and in particular that this data set has a known finite size. This is unrealistic when modeling data streams. PVB addresses this problem by using the frequentist notion of a population distribution, F, which is assumed to generate the data stream by repeatedly sampling M data points at the time. M parameterizes the size of the population, and helps control the variance of the population posterior. Unfortunately, M must be specified by the user. No clear rule exists regarding how to set it, and McInerney et al. (2015) show that its optimal value may differ from one data stream to another.", "startOffset": 62, "endOffset": 1312}, {"referenceID": 0, "context": "Bayesian inference on streaming data has been widely studied (Ahmed et al., 2011; Doucet et al., 2000; Yao et al., 2009). In the context of variational inference, there are two main approaches. Ghahramani & Attias (2000); Broderick et al. (2013) propose recursive Bayesian updating of the variational approximation. The streaming variational Bayes (SVB) algorithm (Broderick et al., 2013) is the most known approach of this category. Alternatively, one could cast the inference problem as a stochastic optimization problem. Stochastic variational inference (SVI) (Hoffman et al., 2013) and the closely related population variational Bayes (PVB) (McInerney et al., 2015) are prominent examples from this group. SVI assumes the existence of a fixed data set observed in a sequential manner, and in particular that this data set has a known finite size. This is unrealistic when modeling data streams. PVB addresses this problem by using the frequentist notion of a population distribution, F, which is assumed to generate the data stream by repeatedly sampling M data points at the time. M parameterizes the size of the population, and helps control the variance of the population posterior. Unfortunately, M must be specified by the user. No clear rule exists regarding how to set it, and McInerney et al. (2015) show that its optimal value may differ from one data stream to another. The problem of Bayesian modeling of non-stationary data streams (i.e., with concept drift (Gama et al., 2014)) is not addressed by SVB, as it assumes data exchangeability. An online variational inference method, which exponentially forgets the variational parameters associated with old data, was proposed by Honkela & Valpola (2003). The so-called power prior approach (Ibrahim & Chen, 2000) is also based on an exponential forgetting mechanisms, and has nice theoretical properties (Ibrahim et al.", "startOffset": 62, "endOffset": 1718}, {"referenceID": 18, "context": "A time series based modeling approach for concept drift using implicit transition models was pursued by \u00d6zkan et al. (2013); K\u00e1rn\u1ef3 (2014).", "startOffset": 104, "endOffset": 124}, {"referenceID": 18, "context": "A time series based modeling approach for concept drift using implicit transition models was pursued by \u00d6zkan et al. (2013); K\u00e1rn\u1ef3 (2014). Unfortunately, the implicit transition model depends on a hyper-parameter determining the forgetting-factor, which has to be manually set.", "startOffset": 104, "endOffset": 138}, {"referenceID": 18, "context": "Rather than explicitly modeling the evolution of the \u03b2t parameters as in Equation (3), we instead follow the approach of K\u00e1rn\u1ef3 (2014) and \u00d6zkan et al. (2013) who define the time evolution model implicitly by constraining the maximum KL divergence over consecutive parameter distributions.", "startOffset": 138, "endOffset": 158}, {"referenceID": 18, "context": "K\u00e1rn\u1ef3 (2014) and \u00d6zkan et al. (2013) seek to approximate p(\u03b2t|x1:t\u22121) by the distribution p\u0302(\u03b2t|x1:t\u22121) having maximum entropy under the constraint in (5); for continuous distributions the maximum entropy can be formulated relative to an uninformative prior density pu(\u03b2t), which corresponds to the Kullbach-Leibler divergence between the two distributions.", "startOffset": 17, "endOffset": 37}, {"referenceID": 1, "context": "In our streaming data setting we follow assumed density filtering (Lauritzen, 1992) and the SVB approach (Broderick et al., 2013) and employ the approximation p(\u03b2t\u22121|x1:t\u22121) \u2248 q(\u03b2t\u22121|\u03bbt\u22121), where q(\u03b2t\u22121|\u03bbt\u22121) is the variational distribution calculated in the previous time step.", "startOffset": 105, "endOffset": 129}, {"referenceID": 11, "context": "The perspective provided by Lemma 1 introduces a well known result of power priors, which is also applicable in the current context (see the discussion after Theorem 1 in (Ibrahim et al., 2003)): \u201cthe power prior is an optimal prior to use and in fact minimizes the convex combination of KL divergences between two extremes: one in which no historical data is used and the other in which the historical data and current data are given equal weight.", "startOffset": 171, "endOffset": 193}, {"referenceID": 18, "context": "\u201d As noted in (Olesen et al., 1992; \u00d6zkan et al., 2013), this schema works as a moving window with exponential forgetting of past data, where the effective number of samples or, more technically, the so-called equivalent sample size of the posterior (Heckerman et al.", "startOffset": 14, "endOffset": 55}, {"referenceID": 7, "context": ", 2013), this schema works as a moving window with exponential forgetting of past data, where the effective number of samples or, more technically, the so-called equivalent sample size of the posterior (Heckerman et al., 1995), converges to,", "startOffset": 202, "endOffset": 226}, {"referenceID": 18, "context": "In the approach taken by \u00d6zkan et al. (2013) (and, by extension, SVB-PP), the forgetting factor \u03c1t is user-defined.", "startOffset": 25, "endOffset": 45}, {"referenceID": 14, "context": "On the other hand, observe that the form of the natural gradient of \u03c9t have an intuitive semantic interpretation, which also extends to the coordinate ascent variational message passing framework (Winn & Bishop, 2005) as shown by Masegosa et al. (2016a). Specifically, using the constant \u03b3 as a threshold, we see that if the uninformed prior pu(\u03b2t) provides a better fit to the variational posterior at time t than the variational parameters \u03bbt from the previous time step (KL(q(\u03b2t|\u03bbt), pu(\u03b2t)) + \u221230 \u221220 \u221210 0 10 20 30 0.", "startOffset": 230, "endOffset": 254}, {"referenceID": 8, "context": "Note that the overall distribution does not fall inside the conditional conjugate exponential family (Hoffman et al., 2013), hence PVB cannot be applied here, because lower-bound\u2019s gradient cannot be computed in closed-form.", "startOffset": 101, "endOffset": 123}, {"referenceID": 22, "context": "GPS (Zheng et al., 2008; 2009; 2010): This data set contains 17 621 GPS trajectories (time-stamped x and y coordinates), totalling more than 4.", "startOffset": 4, "endOffset": 36}, {"referenceID": 1, "context": "These hyper-parameters are hard to fix, as their optimal values depend on data characteristics (see Broderick et al. (2013); McInerney et al.", "startOffset": 100, "endOffset": 124}, {"referenceID": 1, "context": "These hyper-parameters are hard to fix, as their optimal values depend on data characteristics (see Broderick et al. (2013); McInerney et al. (2015) for similar conclusions).", "startOffset": 100, "endOffset": 149}], "year": 2017, "abstractText": "Making inferences from data streams is a pervasive problem in many modern data analysis applications. But it requires to address the problem of continuous model updating, and adapt to changes or drifts in the underlying data generating distribution. In this paper, we approach these problems from a Bayesian perspective covering general conjugate exponential models. Our proposal makes use of non-conjugate hierarchical priors to explicitly model temporal changes of the model parameters. We also derive a novel variational inference scheme which overcomes the use of non-conjugate priors while maintaining the computational efficiency of variational methods over conjugate models. The approach is validated on three real data sets over three latent variable models.", "creator": "LaTeX with hyperref package"}}}