{"id": "1707.02377", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jul-2017", "title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.", "histories": [["v1", "Sat, 8 Jul 2017 00:57:01 GMT  (664kb,D)", "http://arxiv.org/abs/1707.02377v1", "5th International Conference on Learning Representations, 2017"]], "COMMENTS": "5th International Conference on Learning Representations, 2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["minmin chen"], "accepted": true, "id": "1707.02377"}, "pdf": {"name": "1707.02377.pdf", "metadata": {"source": "CRF", "title": "MENTS THROUGH CORRUPTION", "authors": ["Minmin Chen"], "emails": ["m.chen@criteo.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "2 RELATED WORKS AND NOTATIONS", "text": "In this section we will introduce W2Vec and Paragraph Vectors, which are two similar approaches."}, {"heading": "3 METHOD", "text": "In fact, it is a matter of a way in which people are able to determine themselves how they want to behave. (...) It is as if they were able to behave. (...) It is as if they were able to behave. (...) It is as if they were able to behave. (...) It is as if they were able to behave themselves. (...) It is as if they were able to behave themselves. (...) It is as if they were able to behave themselves. (...) It is as if they were able to behave themselves. (...) It is as if they were able to behave themselves. (...)"}, {"heading": "3.1 CORRUPTION AS DATA-DEPENDENT REGULARIZATION", "text": "We approach the probability for each instance f (w, c, x) in relation to each instance f (w, c, x) in relation to each instance f (w, c, c) in relation to each instance f (w, c) in relation to each instance f (w, c, c) in relation to each instance f (w, c) in relation to each instance f (w, c) in relation to each instance f (w, c) in relation to each instance f (w, c) in relation to each instance f (w, c) in relation to each instance f (f) in relation to each instance f (w, c) in relation to each instance f (f) in relation to each instance f (w, c) in relation to each instance f (f) in relation to each instance f (w, c) in relation to each instance."}, {"heading": "4 EXPERIMENTS", "text": "We evaluate Doc2VecC using a mood analysis task, a document classification task, a semantic relationship task, and multiple learning algorithms for document representation. All experiments can be reproduced using the code available at https: / / github.com / mchen24 / iclr2017."}, {"heading": "4.1 BASELINES", "text": "We compare using the following document presentation basics: Word Bags (BoW); Denoising Autoencoders (DEA) (Vincent et al., 2008), a representation derived from the reconstruction of original document x using a corrupt x encoder. SDAs have proven to be state of the art for mood analysis tasks (Glorot et al., 2011). We used Kullback-Liebler divergence as a reconstruction error and an affine encoder. To expand the algorithm to a large vocabulary, we only consider the non-zero elements of x in the reconstruction error and used negative samples for revision; Word2Vec (Mikolov et al., 2013a) + IDF, a representation generated by weighted averages of word vectors learned with the help of Word2Vec; Doc2Vec (Le and Mikolov, 2014); Mikolov-Vector (a dividable, based on a Kiros), which is a saturator model."}, {"heading": "4.2 SENTIMENT ANALYSIS", "text": "In fact, it is the case that most of us are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "4.3 WORD ANALOGY", "text": "In this experiment we will compare the word embeddings generated by Doc2VecC with the word embeddings generated by Word2Vec, or with the word analogies introduced by it. The questions will be answered by simple algebraic operations on the word embeddings generated by the various methods. Please refer to the five types of syntactic questions, with a total of 8,869 semantic and 10,675 syntactic questions. The questions will be answered by the simple albeddings."}, {"heading": "4.4 DOCUMENT CLASSIFICATION", "text": "In fact, most of them will be able to play by the rules they have established in recent years."}, {"heading": "4.5 SEMANTIC RELATEDNESS", "text": "In this context, it should be noted that this is not an isolated case, but a case which is an isolated case. In this case, it is an isolated case, which is not an isolated case, but a case which is an isolated case. In this case, it is an isolated case, which is not an isolated case, but a case which is an isolated case."}, {"heading": "5 CONCLUSION", "text": "Doc2VecC ensures per se that the document representation generated by average Word embedding captures the semantics of the document learned during learning. It also introduces data-dependent regulation that favors informative or rare words while simultaneously dampening the embedding of common and non-discriminatory words. Thus, each document can be efficiently represented as a simple average of the learned word embedding. Compared to several existing Doc2VecC document display learning algorithms, Doc2VecC performs better not only in testing the efficiency, but also in the meaningfulness of the generated representations.3Word representation was initialized with publicly available 300-dimensional glove vectors trained on 840 billion tokens of common crawl data.The dataset contains 11,038 books containing over one billion words."}], "references": [{"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of machine Learning research, 3(Jan):993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Marginalized denoising auto-encoders for nonlinear representations", "author": ["M. Chen", "K.Q. Weinberger", "F. Sha", "Y. Bengio"], "venue": "ICML, pages 1476\u20131484.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["M. Chen", "Z. Xu", "K. Weinberger", "F. Sha"], "venue": "arXiv preprint arXiv:1206.4683.", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Language modeling for information retrieval, volume 13", "author": ["B. Croft", "J. Lafferty"], "venue": "Springer Science & Business Media.", "citeRegEx": "Croft and Lafferty,? 2013", "shortCiteRegEx": "Croft and Lafferty", "year": 2013}, {"title": "Semi-supervised sequence learning", "author": ["A.M. Dai", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems, pages 3079\u20133087.", "citeRegEx": "Dai and Le,? 2015", "shortCiteRegEx": "Dai and Le", "year": 2015}, {"title": "Document embedding with paragraph vectors", "author": ["A.M. Dai", "C. Olah", "Q.V. Le"], "venue": "arXiv preprint arXiv:1507.07998.", "citeRegEx": "Dai et al\\.,? 2015", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American society for information science, 41(6):391.", "citeRegEx": "Deerwester et al\\.,? 1990", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Liblinear: A library for large linear classification", "author": ["Fan", "R.-E.", "Chang", "K.-W.", "Hsieh", "C.-J.", "Wang", "X.-R.", "Lin", "C.-J."], "venue": "JMLR, 9(Aug):1871\u20131874.", "citeRegEx": "Fan et al\\.,? 2008", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "ICML, pages 513\u2013520.", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Multi-step regression learning for compositional distributional semantics", "author": ["E. Grefenstette", "G. Dinu", "Zhang", "Y.-Z.", "M. Sadrzadeh", "M. Baroni"], "venue": "arXiv preprint arXiv:1301.6939.", "citeRegEx": "Grefenstette et al\\.,? 2013", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2013}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "ACL, pages 873\u2013882.", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A.M. Rush"], "venue": "arXiv preprint arXiv:1508.06615.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R.R. Salakhutdinov", "R. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "Advances in neural information processing systems, pages 3294\u20133302.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "From word embeddings to document distances", "author": ["M.J. Kusner", "Y. Sun", "N.I. Kolkin", "K.Q. Weinberger"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML 2015), pages 957\u2013966.", "citeRegEx": "Kusner et al\\.,? 2015", "shortCiteRegEx": "Kusner et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "ICML, volume 14, pages 1188\u20131196.", "citeRegEx": "Le and Mikolov,? 2014", "shortCiteRegEx": "Le and Mikolov", "year": 2014}, {"title": "Learning word vectors for sentiment analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "ACL, pages 142\u2013150.", "citeRegEx": "Maas et al\\.,? 2011", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Visualizing data using t-sne", "author": ["Maaten", "L. v. d.", "G. Hinton"], "venue": "Journal of Machine Learning Research, 9(Nov):2579\u20132605.", "citeRegEx": "Maaten et al\\.,? 2008", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment", "author": ["M. Marelli", "L. Bentivogli", "M. Baroni", "R. Bernardi", "S. Menini", "R. Zamparelli"], "venue": "SemEval-2014.", "citeRegEx": "Marelli et al\\.,? 2014", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews", "author": ["G. Mesnil", "T. Mikolov", "M. Ranzato", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.5335.", "citeRegEx": "Mesnil et al\\.,? 2014", "shortCiteRegEx": "Mesnil et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "J. Dean"], "venue": "Advances in neural information processing systems.", "citeRegEx": "Mikolov and Dean,? 2013", "shortCiteRegEx": "Mikolov and Dean", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "Interspeech, volume 2, page 3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "Yih", "W.-t.", "G. Zweig"], "venue": "HLT-NAACL, volume 13, pages 746\u2013751.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Composition in distributional models of semantics", "author": ["J. Mitchell", "M. Lapata"], "venue": "Cognitive science, 34(8):1388\u20131429.", "citeRegEx": "Mitchell and Lapata,? 2010", "shortCiteRegEx": "Mitchell and Lapata", "year": 2010}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["G. Salton", "C. Buckley"], "venue": "Information processing & management, 24(5):513\u2013523.", "citeRegEx": "Salton and Buckley,? 1988", "shortCiteRegEx": "Salton and Buckley", "year": 1988}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": "Transactions of the Association for Computational Linguistics, 2:207\u2013218.", "citeRegEx": "Socher et al\\.,? 2014", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "EMNLP, volume 1631, page 1642.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improved semantic representations from treestructured long short-term memory networks", "author": ["K.S. Tai", "R. Socher", "C.D. Manning"], "venue": "arXiv preprint arXiv:1503.00075.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Learning with marginalized corrupted features", "author": ["L. Van Der Maaten", "M. Chen", "S. Tyree", "K.Q. Weinberger"], "venue": "ICML (1), pages 410\u2013418.", "citeRegEx": "Maaten et al\\.,? 2013", "shortCiteRegEx": "Maaten et al\\.", "year": 2013}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "Manzagol", "P.-A."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 1096\u20131103. ACM.", "citeRegEx": "Vincent et al\\.,? 2008", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Dropout training as adaptive regularization", "author": ["S. Wager", "S. Wang", "P.S. Liang"], "venue": "Advances in neural information processing systems, pages 351\u2013359.", "citeRegEx": "Wager et al\\.,? 2013", "shortCiteRegEx": "Wager et al\\.", "year": 2013}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["S. Wang", "C.D. Manning"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 90\u201394. Association for Computational Linguistics.", "citeRegEx": "Wang and Manning,? 2012", "shortCiteRegEx": "Wang and Manning", "year": 2012}, {"title": "Compositional matrix-space models for sentiment analysis", "author": ["A. Yessenalina", "C. Cardie"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 172\u2013182. Association for Computational Linguistics.", "citeRegEx": "Yessenalina and Cardie,? 2011", "shortCiteRegEx": "Yessenalina and Cardie", "year": 2011}, {"title": "Estimating linear models for compositional distributional semantics", "author": ["F.M. Zanzotto", "I. Korkontzelos", "F. Fallucchi", "S. Manandhar"], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics, pages 1263\u20131271.", "citeRegEx": "Zanzotto et al\\.,? 2010", "shortCiteRegEx": "Zanzotto et al\\.", "year": 2010}, {"title": "Text understanding from scratch", "author": ["X. Zhang", "Y. LeCun"], "venue": "arXiv preprint arXiv:1502.01710.", "citeRegEx": "Zhang and LeCun,? 2015", "shortCiteRegEx": "Zhang and LeCun", "year": 2015}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Y. Zhu", "R. Kiros", "R. Zemel", "R. Salakhutdinov", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "arXiv preprint arXiv:1506.06724.", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 31, "context": "Despite its simplicity, BoW works surprisingly well for many tasks (Wang and Manning, 2012).", "startOffset": 67, "endOffset": 91}, {"referenceID": 19, "context": "The well celebrated Word2Vec (Mikolov et al., 2013a), by learning to predict the target word using its neighboring words, maps words of similar meanings to nearby points in the continuous vector space.", "startOffset": 29, "endOffset": 52}, {"referenceID": 14, "context": "Paragraph Vectors (Le and Mikolov, 2014) generalize the idea to learn vector representation for documents.", "startOffset": 18, "endOffset": 40}, {"referenceID": 0, "context": "It outperforms established document representations, such as BoW and Latent Dirichlet Allocation (Blei et al., 2003), on various text understanding tasks (Dai et al.", "startOffset": 97, "endOffset": 116}, {"referenceID": 5, "context": ", 2003), on various text understanding tasks (Dai et al., 2015).", "startOffset": 45, "endOffset": 63}, {"referenceID": 22, "context": "It is motivated by the observation that linear operations on the word embeddings learned by Word2Vec can sustain substantial amount of syntactic and semantic meanings of a phrase or a sentence (Mikolov et al., 2013b).", "startOffset": 193, "endOffset": 216}, {"referenceID": 20, "context": "For example, vec(\u201cRussia\u201d) + vec(\u201criver\u201d) is close to vec(\u201cVolga River\u201d) (Mikolov and Dean, 2013), and", "startOffset": 73, "endOffset": 97}, {"referenceID": 22, "context": "vec(\u201cking\u201d) - vec(\u201cman\u201d) + vec(\u201cwomen\u201d) is close to vec(\u201cqueen\u201d) (Mikolov et al., 2013b).", "startOffset": 65, "endOffset": 88}, {"referenceID": 26, "context": "In contrast to existing approaches which post-process learned word embeddings to form document representation (Socher et al., 2013; Mesnil et al., 2014), Doc2VecC enforces a meaningful document representation can be formed by averaging the word embeddings during learning.", "startOffset": 110, "endOffset": 152}, {"referenceID": 18, "context": "In contrast to existing approaches which post-process learned word embeddings to form document representation (Socher et al., 2013; Mesnil et al., 2014), Doc2VecC enforces a meaningful document representation can be formed by averaging the word embeddings during learning.", "startOffset": 110, "endOffset": 152}, {"referenceID": 24, "context": "Popular representations range from the simplest BoW and its term-frequency based variants (Salton and Buckley, 1988), language model based methods (Croft and Lafferty, 2013; Mikolov et al.", "startOffset": 90, "endOffset": 116}, {"referenceID": 3, "context": "Popular representations range from the simplest BoW and its term-frequency based variants (Salton and Buckley, 1988), language model based methods (Croft and Lafferty, 2013; Mikolov et al., 2010; Kim et al., 2015), topic models (Deerwester et al.", "startOffset": 147, "endOffset": 213}, {"referenceID": 21, "context": "Popular representations range from the simplest BoW and its term-frequency based variants (Salton and Buckley, 1988), language model based methods (Croft and Lafferty, 2013; Mikolov et al., 2010; Kim et al., 2015), topic models (Deerwester et al.", "startOffset": 147, "endOffset": 213}, {"referenceID": 11, "context": "Popular representations range from the simplest BoW and its term-frequency based variants (Salton and Buckley, 1988), language model based methods (Croft and Lafferty, 2013; Mikolov et al., 2010; Kim et al., 2015), topic models (Deerwester et al.", "startOffset": 147, "endOffset": 213}, {"referenceID": 6, "context": ", 2015), topic models (Deerwester et al., 1990; Blei et al., 2003), Denoising Autoencoders and its variants (Vincent et al.", "startOffset": 22, "endOffset": 66}, {"referenceID": 0, "context": ", 2015), topic models (Deerwester et al., 1990; Blei et al., 2003), Denoising Autoencoders and its variants (Vincent et al.", "startOffset": 22, "endOffset": 66}, {"referenceID": 29, "context": ", 2003), Denoising Autoencoders and its variants (Vincent et al., 2008; Chen et al., 2012), and distributed vector representations (Mesnil et al.", "startOffset": 49, "endOffset": 90}, {"referenceID": 2, "context": ", 2003), Denoising Autoencoders and its variants (Vincent et al., 2008; Chen et al., 2012), and distributed vector representations (Mesnil et al.", "startOffset": 49, "endOffset": 90}, {"referenceID": 18, "context": ", 2012), and distributed vector representations (Mesnil et al., 2014; Le and Mikolov, 2014; Kiros et al., 2015).", "startOffset": 48, "endOffset": 111}, {"referenceID": 14, "context": ", 2012), and distributed vector representations (Mesnil et al., 2014; Le and Mikolov, 2014; Kiros et al., 2015).", "startOffset": 48, "endOffset": 111}, {"referenceID": 12, "context": ", 2012), and distributed vector representations (Mesnil et al., 2014; Le and Mikolov, 2014; Kiros et al., 2015).", "startOffset": 48, "endOffset": 111}, {"referenceID": 34, "context": "Another prominent line of work includes learning task-specific document representation with deep neural networks, such as CNN (Zhang and LeCun, 2015) or LSTM based approaches (Tai et al.", "startOffset": 126, "endOffset": 149}, {"referenceID": 27, "context": "Another prominent line of work includes learning task-specific document representation with deep neural networks, such as CNN (Zhang and LeCun, 2015) or LSTM based approaches (Tai et al., 2015; Dai and Le, 2015).", "startOffset": 175, "endOffset": 211}, {"referenceID": 4, "context": "Another prominent line of work includes learning task-specific document representation with deep neural networks, such as CNN (Zhang and LeCun, 2015) or LSTM based approaches (Tai et al., 2015; Dai and Le, 2015).", "startOffset": 175, "endOffset": 211}, {"referenceID": 19, "context": "There are two well-know model architectures used for both methods, referred to as Continuous Bag-of-Words (CBoW) and Skipgram models (Mikolov et al., 2013a).", "startOffset": 133, "endOffset": 156}, {"referenceID": 20, "context": "Several works (Mikolov and Dean, 2013; Mikolov et al., 2013b) showcased that syntactic and semantic regularities of phrases and sentences are reasonably well preserved by adding or subtracting word embeddings learned through Word2Vec.", "startOffset": 14, "endOffset": 61}, {"referenceID": 22, "context": "Several works (Mikolov and Dean, 2013; Mikolov et al., 2013b) showcased that syntactic and semantic regularities of phrases and sentences are reasonably well preserved by adding or subtracting word embeddings learned through Word2Vec.", "startOffset": 14, "endOffset": 61}, {"referenceID": 10, "context": "Huang et al. (2012) also proposed the idea of using average of word embeddings to represent the global context of a document.", "startOffset": 0, "endOffset": 20}, {"referenceID": 19, "context": "Exactly computing the probability is impractical, instead we approximate it with negative sampling (Mikolov et al., 2013a).", "startOffset": 99, "endOffset": 122}, {"referenceID": 30, "context": "(4) with its Taylor expansion with respect to x\u0303 up to the second-order (Van Der Maaten et al., 2013; Wager et al., 2013; Chen et al., 2014).", "startOffset": 72, "endOffset": 140}, {"referenceID": 1, "context": "(4) with its Taylor expansion with respect to x\u0303 up to the second-order (Van Der Maaten et al., 2013; Wager et al., 2013; Chen et al., 2014).", "startOffset": 72, "endOffset": 140}, {"referenceID": 30, "context": "Similar effect was observed for dropout training for logistic regression model (Wager et al., 2013) and denoising autoencoders (Chen et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 1, "context": ", 2013) and denoising autoencoders (Chen et al., 2014).", "startOffset": 35, "endOffset": 54}, {"referenceID": 29, "context": "We compare against the following document representation baselines: bag-of-words (BoW); Denoising Autoencoders (DEA) (Vincent et al., 2008), a representation learned from reconstructing original document x using corrupted one x\u0303.", "startOffset": 117, "endOffset": 139}, {"referenceID": 8, "context": "SDAs have been shown to be the state-of-the-art for sentiment analysis tasks (Glorot et al., 2011).", "startOffset": 77, "endOffset": 98}, {"referenceID": 19, "context": "To scale up the algorithm to large vocabulary, we only take into account the non-zero elements of x in the reconstruction error and employed negative sampling for the remainings; Word2Vec (Mikolov et al., 2013a)+IDF, a representation generated through weighted average of word vectors learned using Word2Vec; Doc2Vec (Le and Mikolov, 2014); Skip-thought Vectors(Kiros et al.", "startOffset": 188, "endOffset": 211}, {"referenceID": 14, "context": ", 2013a)+IDF, a representation generated through weighted average of word vectors learned using Word2Vec; Doc2Vec (Le and Mikolov, 2014); Skip-thought Vectors(Kiros et al.", "startOffset": 114, "endOffset": 136}, {"referenceID": 12, "context": ", 2013a)+IDF, a representation generated through weighted average of word vectors learned using Word2Vec; Doc2Vec (Le and Mikolov, 2014); Skip-thought Vectors(Kiros et al., 2015), a generic, distributed sentence encoder that extends the Word2Vec skip-gram model to sentence level.", "startOffset": 158, "endOffset": 178}, {"referenceID": 21, "context": "We also include RNNLM (Mikolov et al., 2010), a recurrent neural network based language model in the comparison.", "startOffset": 22, "endOffset": 44}, {"referenceID": 27, "context": "In the semantic relatedness task, we further compare to LSTM-based methods (Tai et al., 2015) that have been reported on this dataset.", "startOffset": 75, "endOffset": 93}, {"referenceID": 15, "context": "It comes with predefined train/test split (Maas et al., 2011): 25,000 reviews are used for training, 25,000 for testing, and the rest as unlabeled data.", "startOffset": 42, "endOffset": 61}, {"referenceID": 18, "context": "We test the various representation learning algorithms under two settings: one follows the same protocol proposed in (Mesnil et al., 2014), where representation is learned using all the available data, including the test set; another one where the representation is learned using training and unlabeled set only.", "startOffset": 117, "endOffset": 138}, {"referenceID": 7, "context": "For both settings, a linear support vector machine (SVM) (Fan et al., 2008) is trained afterwards on the learned representation for classification.", "startOffset": 57, "endOffset": 75}, {"referenceID": 27, "context": "As noted in (Tai et al., 2015), the performance of LSTM based method (similarly, the gated RNN used in Skip-thought vectors) drops significantly with increasing paragraph length, as it is hard to preserve state over long sequences of words.", "startOffset": 12, "endOffset": 30}, {"referenceID": 20, "context": "Note that for all the numbers reported, we applied the trick of subsampling of frequent words introduced in (Mikolov and Dean, 2013) to counter the imbalance between frequent and rare words.", "startOffset": 108, "endOffset": 132}, {"referenceID": 19, "context": "In this experiment, we are going to quantatively compare the word embeddings generated by Doc2VecC to the ones generated by Word2Vec, or Paragraph Vectors on the word analogy task introduced by Mikolov et al. (2013a). The dataset contains five types of semantic questions, and nine types of syntactic questions, with a total of 8,869 semantic and 10,675 syntactic questions.", "startOffset": 194, "endOffset": 217}, {"referenceID": 18, "context": "We observe similar trends as in Mikolov et al. (2013a). Increasing embedding dimensionality as well as training data size improves performance of the word embeddings on this task.", "startOffset": 32, "endOffset": 55}, {"referenceID": 14, "context": "This also explains why the PV-DBOW Le and Mikolov (2014) model architecture proposed in the original work, which completely removes word embedding layers, performs comparable to the distributed memory version.", "startOffset": 35, "endOffset": 57}, {"referenceID": 17, "context": "We test Doc2VecC on the SemEval 2014 Task 1: semantic relatedness SICK dataset (Marelli et al., 2014).", "startOffset": 79, "endOffset": 101}, {"referenceID": 35, "context": "We compare Doc2VecC with several winning solutions of the competition as well as several more recent techniques reported on this dataset, including bi-directional LSTM and Tree-LSTM3 trained from scratch on this dataset, Skip-thought vectors learned a large book corpus 4 (Zhu et al., 2015) and produced sentence embeddings of 4,800 dimensions on this dataset.", "startOffset": 272, "endOffset": 290}, {"referenceID": 12, "context": "Contrary to the vocabulary expansion technique used in (Kiros et al., 2015) to handle out-of-vocabulary words, we extend the vocabulary of the learned model directly on the target dataset in the following way: we use the pre-trained word embedding as an initialization, and fine-tune the word and sentence representation on the SICK dataset.", "startOffset": 55, "endOffset": 75}, {"referenceID": 12, "context": "Given the sentence embeddings, we used the exact same training and testing protocol as in (Kiros et al., 2015) to score each pair of sentences: with two sentence embedding u1 and u2, we concatenate their component-wise product, u1 \u00b7u2 and their absolute difference, |u1 \u2212 u2| as the feature representation.", "startOffset": 90, "endOffset": 110}, {"referenceID": 25, "context": "Despite its simplicity, Doc2VecC significantly out-performs the winning solutions of the competition, which are heavily feature engineered toward this dataset and several baseline methods, noticeably the dependency-tree RNNs introduced in (Socher et al., 2014), which relies on expensive dependency parsers to compose sentence vectors from word embeddings.", "startOffset": 239, "endOffset": 260}, {"referenceID": 27, "context": "The first group of results are from the submission to the 2014 SemEval competition; the second group includes several baseline methods reported in (Tai et al., 2015); the third group are methods based on LSTM reported in (Tai et al.", "startOffset": 147, "endOffset": 165}, {"referenceID": 27, "context": ", 2015); the third group are methods based on LSTM reported in (Tai et al., 2015) as well as the skip-thought vectors (Kiros et al.", "startOffset": 63, "endOffset": 81}, {"referenceID": 12, "context": ", 2015) as well as the skip-thought vectors (Kiros et al., 2015).", "startOffset": 44, "endOffset": 64}], "year": 2017, "abstractText": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.", "creator": "LaTeX with hyperref package"}}}