{"id": "1707.02459", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jul-2017", "title": "Improving Multilingual Named Entity Recognition with Wikipedia Entity Type Mapping", "abstract": "The state-of-the-art named entity recognition (NER) systems are statistical machine learning models that have strong generalization capability (i.e., can recognize unseen entities that do not appear in training data) based on lexical and contextual information. However, such a model could still make mistakes if its features favor a wrong entity type. In this paper, we utilize Wikipedia as an open knowledge base to improve multilingual NER systems. Central to our approach is the construction of high-accuracy, high-coverage multilingual Wikipedia entity type mappings. These mappings are built from weakly annotated data and can be extended to new languages with no human annotation or language-dependent knowledge involved. Based on these mappings, we develop several approaches to improve an NER system. We evaluate the performance of the approaches via experiments on NER systems trained for 6 languages. Experimental results show that the proposed approaches are effective in improving the accuracy of such systems on unseen entities, especially when a system is applied to a new domain or it is trained with little training data (up to 18.3 F1 score improvement).", "histories": [["v1", "Sat, 8 Jul 2017 16:17:04 GMT  (25kb)", "http://arxiv.org/abs/1707.02459v1", "11 pages, Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016"]], "COMMENTS": "11 pages, Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["jian ni", "radu florian"], "accepted": true, "id": "1707.02459"}, "pdf": {"name": "1707.02459.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["raduf}@us.ibm.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 7.02 459v 1 [cs.C L] 8J ul (NER) systems are statistical machine learning models that have a strong generalization capability (i.e., can detect invisible units that do not occur in training data), based on lexical and contextual information. However, such a model can still make mistakes if its characteristics favor an incorrect entity type. In this essay, we use Wikipedia as an open knowledge base to improve multilingual NER systems. Central to our approach is the construction of high-precision, comprehensive multilingual Wikipedia entity type mappings. These mappings are based on poorly commented data and can be expanded to new languages without human annotations or linguistic knowledge being involved. Based on these mappings, we develop several approaches to improve an NER system. We evaluate the performance of approaches using experiments on NER systems that have been trained for 6 languages, in particular if the results of such experiments are designed to be effective on a specific set of proposed systems."}, {"heading": "1 Introduction", "text": "It is an important task of the NLP to automatically recognize entities in the text and categorize them into predefined information types such as persons, organizations, geopolitical entities, places, events, etc., because they are a fundamental component of many information extraction and knowledge finding applications, including their linkage, questioning, and data mining.The state-of-the-art systems are usually statistical machine learning models trained with human-annotated data. Popular models include Maximum Entropy Markov Models (MEMM) (McCallum et al., 2000), Conditional Random Fields (CRF). (Lafferty et al., 2001), and neural networks trained with Wikipedia annotated data. Wikipedia et al., 2011; Lample et al., 2016) Such models have strong generalizability to recognize invisible entities1 on xical and contextual information."}, {"heading": "2 English Wikipedia Entity Type Mapping", "text": "In this section we will focus on the English Wikipedia. We divide Wikipedia pages into two types: \u2022 Entity pages that describe an entity or an object, either a named entity such as \"Michael Jordan\" or a common entity such as \"basketball.\" \u2022 Non-entity pages that do not describe a specific entity, including disambiguity pages, editing pages, list pages, etc. We have developed our own English NER system (Florian et al., 2004). The system has 51 entity types, and the main motivation for using such a fine-grained entity type is to build cognitive question-answering applications on top of the NER system. An important check for a question-answer system is the ability to detect whether a specific answer matches the expected ENTENTY type derived from the question. The entity type system used in this essay is designed to include many of the common types covered by a TENTER, which are naturally addressed by ENTERT (ENTENTER), such as ENTENTY, ENTENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, ENTY, EN"}, {"heading": "2.1 Wikipedia Entity Type Classification", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1.1 Features", "text": "We build maximum entropy classifiers (Nigam et al., 1999) for Wikipedia entity classification. We use both structured information and unstructured information of a Wikipedia page as attributes. Each Wikipedia page has a unique title. The title of an entity page is usually the name of the entity, and can contain auxiliary information in parentheses to distinguish entities with the same name. We use both the entity name and auxiliary information in parentheses (if any) of a Wikipedia title as attributes, because any useful information could provide for entity type classification. For example, based on the word \"prize\" in the title \"Nobel Prize\" or the word \"awards\" in the title \"Academy Awards,\" one can conclude that the entity type is AWARD. Likewise, the auxiliary information could be \"company\" in the title \"Jordan (company),\" which indicates that the entity is an entity is an entity. \""}, {"heading": "2.1.2 Training and Test Data", "text": "We use an EL system (Sil and Florian, 2014) to generate training data for the classification of Wikipedia units as follows: If a named unit in our NER training data is associated with the unit Type T with a Wikipedia page, this page is labeled with Unit Type T. Similarly, we use the EL system to generate a set of test data by linking named units in our NER test data with Wikipedia pages. The English Wikipedia snapshot was dumped in April 2014, which contains about 4.6 million pages. By using this method, we create a training data set of 4,699 English Wikipedia pages and a test set of 415 English Wikipedia pages. Note that the automatically generated classification and test data are indispensable for incorrect classification, as we may mismap the page of the Wikipedia page."}, {"heading": "2.1.3 Classifier Performance", "text": "In order to assess the predictive power of different characteristic types, we train a number of classifiers that use only title characteristics, only infobox characteristics, only text characteristics and all characteristics. In Table 1, we show the F1 score of classifiers for different entity types. ALL is the overall performance, and PER (PERSON), ORG (ORGANISATION), GPE, TITL (TITLEWORK), FAC (FACILITY) are the five most common entity types in the test data. Table 1 shows that text characteristics are the most important characteristics for classification of Wikipedia pages, since the classifier trained only with text characteristics achieves a total score of F1 of 87.2, which is better than the classifier trained with either title or infobox characteristics alone. Nevertheless, both infobox and title characteristics provide additional information for the total score of 97.1 classification of all entries, and 0.1."}, {"heading": "2.1.4 Improvement via Self-Training", "text": "Self-training is a semi-supervised learning method that can be used in applications where there are only a small number of q training examples, but a large number of unmarked examples. Since our poorly annotated training data for classifiers cover only about 1% of all Wikipedia pages, we are motivated to use self-training to further improve classification accuracy. First, we apply a standard self-training approach. The classifier trained with the initial training data is used to decrypt (i.e., classify) all unlabeled Wikipedia pages to predict their entity types with confidence. We add the self-decrypted Wikipedia pages with high self-confidence values to the training data and train a new classifier. Through experiments, a threshold of 0.9 is used to sort highly self-confident self-decryption examples. The F1 score of the new classifier is improved to 91.1, as shown in Table 2.3 below examples of self-adding."}, {"heading": "2.2 Wikipedia Entity Type Mapping", "text": "We are constructing an English Wikipedia entity type mapping by applying the English Wikipedia entity type classifier to all English Wikipedia pages. Each entry in the mapping contains an entity name (which is extracted from the title of a Wikipedia page) and the associated entity type with confidence value (which is determined by the classifier). We refer to the English Wikipedia entity type mapping, which includes all pages by English WikiMapping. To create a high-precision mapping, you can only want to include entities with confidence values greater than or equal to a threshold t in the mapping, and we refer to such mapping by English WikiMapping (t). Note that a mapping with a higher t will have more accurate entity types for its entities, but it will include fewer entities. Therefore, there is a trade-off between the accuracy of the Wikipedia's coverage and the entities that can be matched by Wikipedia's coverage."}, {"heading": "3 Multilingual Wikipedia Entity Type Mapping", "text": "Based on the English Wikipedia entity type mapping, we want to build a high accuracy, high coverage Portuguese classification 190 entity type mappings for other languages with a minimum of human annotations and language-dependent knowledge involved. We use the interlingual links of Wikipedia, which are the links between the pages of an entity in different languages. A direct approach is the projection with the interlingual links between English and Portuguese Wikipediapages pages of Wikipedia pages that provide useful information for this task: for each Portuguese Wikipedia page that has an interlingual link to an English Wikipedia page mappings, we project the entity type page of the English Wikipedia (determined by the English entity type type of the Portuguese Wikipedia mappings), and we use Portuguese as an example. A direct approach is the projection with the interlingual type of Wikipedia type of the Portuguese Wikipedia type of Wikipedia Wikipedia Wikiportable type we use the Wikiportable Wikipedia type of Wikipedia, the Wikiportable Wikipedia page we use the English Wikiportable page Wikiportable Wikipedia mappings (Wikiportable type of Wikiportable Wikipedia)."}, {"heading": "4 Improving NER Systems", "text": "We have developed several approaches that use Wikipedia entity type mappings to improve the system. Let M be a Wikipedia entity type mapping. For an entity name x, let M (x) are the set of possible entity types for x defined by the mapping. If an entity name x is in the mapping, then M (x) includes at least one entity type, i.e. M (x) is the type of entity types in which the entity types x (x) are not included in the mapping, then M (x) is the empty sentence, and | M (x) the entity type of entity mapping is the first method to use a Wikipedia entity type mapping M as a decoding constraint for an NER system. Within this approach, mapping is used as a constraint on the decoding."}, {"heading": "5 Experiments", "text": "In this section, we evaluate the effectiveness of the proposed Wikipedia-based approaches based on experiments with NER systems trained for 6 languages: English, Portuguese, Japanese, Spanish, Dutch and German. For each language, we compare the basic NER system with the following approaches: \u2022 DC (i): the decoding constraint approach with mapping Language-Wiki mapping (0,9, i). \u2022 Joint: the common approach combining DC (3) and PP (2). \u2022 DF (i): the Dictionary Feature approach with mapping Language-Wiki mapping (0,9, i). To evaluate the generalization capability of a NER system, we calculate the F1 score for the invisible units (Unseen) as well as for all units (All) in a test data set."}, {"heading": "5.1 English", "text": "The English NER base system is a CRF model trained with 328K characters of human commented news articles. It uses standard NER features in literature, including N gram word characteristics, word type characteristics, prefix and suffix characteristics, Brown cluster characteristics, gazetteer characteristics, document-level cache characteristics, etc. We have two human commented test records: the first sentence, Test (News), consists of 40K characters of human commented news articles; and the second sentence, Test (Political), consists of 77K characters of human commented articles from Wikipedia. The results are shown in Table 3.For Test (News), which is in the same range as the training data, the base system scores 88.2 F1 points for all units and a relatively low F1 value of 78.7 for the invisible units (38% of all units are invisible units). The dictatorial DF (2) approach achieves the highest F1 base value for all units and F1 base value for all units."}, {"heading": "5.2 Portuguese", "text": "For Portuguese, we have used a semi-supervised learning approach to build the Baseline-NER system. The training dataset contains 31K characters of human commented news articles and 2M characters of poorly commented data. The poorly commented data is generated as follows: We have a large number of parallel sentences between English and Portuguese news articles. We apply the English NER system to English sentences and project the entity type tags onto the Portuguese sentences by adjusting between the English and Portuguese sentences. The Baseline-NER system consists of a MEMM model (CRF cannot handle such a large size of training data because our NER system has 51 entity types and the number of characteristics and training times of the CRF grow at least quadratically in the number of entity types). The test dataset consists of 34K characters of human commented Portuguese news articles. The results are presented in Table 4 of the baseline data, as the performance of the DC is trained with only a small amount of human commented 1."}, {"heading": "5.3 Japanese", "text": "For the Japanese, the NER base system is a MEMM model trained with 20K characters of human-annotated news articles and 2.1M characters of poorly annotated data, and the poorly annotated data was generated using steps similar to those used in the Portuguese NER system. The test data set consists of 22K characters of human-annotated Japanese news articles. Results are shown in Table 5. Even in this case, with limited resources, DC (2) achieves the best improvement among Wikipedia-based approaches, improving the baseline by 9.0 F1 values for all units and 18.3 F1 values for invisible units (59% of all units). The common approach improves the baseline by 4.8 F1 values for all units and 9.6 F1 values for invisible units."}, {"heading": "5.4 Spanish, Dutch and German", "text": "We also evaluate the Wikipedia-based approaches to Spanish, Dutch and German NER systems that have been trained with the CoNLL data sets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Accordingly, we have trained only 4 entity types in the CoNLL data: PER (person), ORG (organization), LOC (location), MISC (various names). Accordingly, we have trained a CoNLL-like Wikipedia entity classifier that produces the CoNLL entity types. Training data for the classifier is generated using the English CoNLL training data set and the AIDA-YAGO2 data set that provides the Wikipedia titles for the aforementioned entities in the English CoNLL data set (Hoffart et al., 2011)."}, {"heading": "5.5 Discussion", "text": "From the experimental results, the following observations can be derived: \u2022 Wikipedia-based approaches are more effective in improving the generalization capability of NER systems (i.e. improving the accuracy of invisible units), especially when a system is applied to a new area (3.6 F1 score improvement for political party articles / English NER) or when training data with little human annotation (18.3 F1 score improvement for Japanese NER systems). \u2022 In the resource-poor scenario, where a NER system is trained with little human annotation data (e.g. 20K-30K tokens of training data for Portuguese and Japanese systems), the encoding constraint approach, which uses high precision, leads to the best improvements among German systems during the decoding phase. \u2022 In the \"K\" approach, where both the \"Smart Resource\" and \"K\" approaches are well trained."}, {"heading": "6 Conclusion", "text": "In this paper, we have proposed and evaluated several approaches that use highly accurate and comprehensive entity type mappings from Wikipedia to improve multilingual NER systems. These mappings are based on poorly annotated data and can easily be expanded to new languages without involving human annotation or language-dependent knowledge.Experimental results show that Wikipedia-based approaches are effective in improving the generalization capability of NER systems. If a system is well trained, the dictionary feature approach achieves the best improvement over the base system; while a system is trained with little human-annotated training data, a more aggressive decoding constraint approach achieves the best improvement. Improvements are greater for invisible units, and the approaches are particularly useful when a system is applied to a new domain or trained with little training data."}, {"heading": "Acknowledgments", "text": "We thank Avirup Sil for helpful comments and the collection of Wikipedia data, as well as the anonymous reviewers for their suggestions."}], "references": [{"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Augmenting Wikipedia with named entity tags", "author": ["Dakka", "Cucerzan2008] Wisam Dakka", "Silviu Cucerzan"], "venue": "In Proceedings of the 3rd International Joint Conference on Natural Language Processing,", "citeRegEx": "Dakka et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dakka et al\\.", "year": 2008}, {"title": "A statistical model for multilingual entity detection and tracking", "author": ["Florian et al.2004] Radu Florian", "Hany Hassan", "Abe Ittycheriah", "Hongyan Jing", "Nanda Kambhatla", "Xiaqiang Luo", "Nicolas Nicolov", "Salim Roukos"], "venue": null, "citeRegEx": "Florian et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2004}, {"title": "Collective entity linking in web text: A graphbased method", "author": ["Han et al.2011] Xianpei Han", "Le Sun", "Jun Zhao"], "venue": "In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Han et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Han et al\\.", "year": 2011}, {"title": "Robust disambiguation of named entities in text", "author": ["Mohamed Amir Yosef", "Ilaria Bordino", "Hagen F\u00fcrstenau", "Manfred Pinkal", "Marc Spaniol", "Bilyana Taneva", "Stefan Thater", "Gerhard Weikum"], "venue": null, "citeRegEx": "Hoffart et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffart et al\\.", "year": 2011}, {"title": "ExploitingWikipedia as external knowledge for named entity recognition", "author": ["Kazama", "Torisawa2007] Jun\u2019ichi Kazama", "Kentaro Torisawa"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computa-", "citeRegEx": "Kazama et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kazama et al\\.", "year": 2007}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando C.N. Pereira"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Neural architectures for named entity recognition", "author": ["Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "venue": "In Proceedings of NAACL-HLT (NAACL 2016),", "citeRegEx": "Lample et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Maximum entropy Markovmodels for information extraction and segmentation", "author": ["Dayne Freitag", "Fernando C.N. Pereira"], "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning,", "citeRegEx": "McCallum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2000}, {"title": "Using maximum entropy for text classification", "author": ["Nigam et al.1999] Kamal Nigam", "John Lafferty", "Andrew McCallum"], "venue": "Workshop on Machine Learning for Information Filtering,", "citeRegEx": "Nigam et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Nigam et al\\.", "year": 1999}, {"title": "Learning multilingual named entity recognition fromWikipedia", "author": ["Nothman et al.2013] Joel Nothman", "Nicky Ringland", "Will Radford", "Tara Murphy", "James R. Curran"], "venue": "Journal of Artificial Intelligence,", "citeRegEx": "Nothman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nothman et al\\.", "year": 2013}, {"title": "Named entity recognition with document-specific KB tag gazetteers", "author": ["Radford et al.2015] Will Radford", "Xavier Carreras", "James Henderson"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Ratinov", "Roth2009] Lev Ratinov", "Dan Roth"], "venue": "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning", "citeRegEx": "Ratinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2009}, {"title": "Mining Wiki resources for multilingual named entity recognition", "author": ["Richman", "Schone2008] Alexander E. Richman", "Patrick Schone"], "venue": "In Proceedings of ACL-08: HLT,", "citeRegEx": "Richman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Richman et al\\.", "year": 2008}, {"title": "The IBM systems for English entity discovery and linking and Spanish entity linking at TAC 2014", "author": ["Sil", "Florian2014] Avirup Sil", "Radu Florian"], "venue": "In Text Analysis Conference (TAC),", "citeRegEx": "Sil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sil et al\\.", "year": 2014}, {"title": "Introduction to the CONLL-2003 shared task: Language-independent named entity recognition", "author": ["Tjong Kim Sang", "Fien DeMeulder"], "venue": "In Proceedings of the Seventh Conference on Natural Language Learning", "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Introduction to the CONLL-2002 shared task: Language-independent named entity recognition", "author": [], "venue": "In Proceedings of the Sixth Conference on Natural Language Learning - Volume 20,", "citeRegEx": "Sang.,? \\Q2002\\E", "shortCiteRegEx": "Sang.", "year": 2002}, {"title": "A proposal to automatically build and maintain gazetteers for named entity recognition by using Wikipedia", "author": ["Toral", "Muoz2006] Antonio Toral", "Rafael Muoz"], "venue": null, "citeRegEx": "Toral et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Toral et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 8, "context": "Popular models include maximum entropy Markov models (MEMM) (McCallum et al., 2000), conditional random fields (CRF) (Lafferty et al.", "startOffset": 60, "endOffset": 83}, {"referenceID": 6, "context": ", 2000), conditional random fields (CRF) (Lafferty et al., 2001) and neural networks (Collobert et al.", "startOffset": 41, "endOffset": 64}, {"referenceID": 0, "context": ", 2001) and neural networks (Collobert et al., 2011; Lample et al., 2016).", "startOffset": 28, "endOffset": 73}, {"referenceID": 7, "context": ", 2001) and neural networks (Collobert et al., 2011; Lample et al., 2016).", "startOffset": 28, "endOffset": 73}, {"referenceID": 11, "context": ", (Kazama and Torisawa, 2007; Ratinov and Roth, 2009; Radford et al., 2015).", "startOffset": 2, "endOffset": 75}, {"referenceID": 11, "context": ", (Kazama and Torisawa, 2007; Ratinov and Roth, 2009; Radford et al., 2015). Kazama and Torisawa (2007) try to find the Wikipedia entity for each candidate word se-", "startOffset": 54, "endOffset": 104}, {"referenceID": 11, "context": "Radford et al. (2015) assume that document-specific knowledge base (e.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": ", (Richman and Schone, 2008; Nothman et al., 2013).", "startOffset": 2, "endOffset": 50}, {"referenceID": 10, "context": ", (Richman and Schone, 2008; Nothman et al., 2013). The motivation is that annotating multilingual NER data by human is both expensive and time-consuming. Richman and Schone (2008) utilize the category information of Wikipedia to determine the entity type of an entity based on manually constructed rules (e.", "startOffset": 29, "endOffset": 181}, {"referenceID": 10, "context": ", (Richman and Schone, 2008; Nothman et al., 2013). The motivation is that annotating multilingual NER data by human is both expensive and time-consuming. Richman and Schone (2008) utilize the category information of Wikipedia to determine the entity type of an entity based on manually constructed rules (e.g., category phrase \u201cLiving People\u201d is mapped to entity type PERSON). Such a rule-based entity type mapping is limited both in accuracy and coverage, e.g., (Toral and Muoz, 2006). Nothman et al. (2013) train a Wikipedia entity type classifier using human-annotated Wikipedia pages.", "startOffset": 29, "endOffset": 510}, {"referenceID": 10, "context": "6 F1 score on the BBN data as in (Nothman et al., 2013)).", "startOffset": 33, "endOffset": 55}, {"referenceID": 10, "context": "We use weakly annotated data to train an English Wikipedia entity type classifier, as opposed to using humanannotated data as in (Dakka and Cucerzan, 2008; Nothman et al., 2013).", "startOffset": 129, "endOffset": 177}, {"referenceID": 2, "context": "We have developed an in-house English NER system (Florian et al., 2004).", "startOffset": 49, "endOffset": 71}, {"referenceID": 9, "context": "We build maximum entropy classifiers (Nigam et al., 1999) for Wikipedia entity type classification.", "startOffset": 37, "endOffset": 57}, {"referenceID": 3, "context": "Entity linking (EL) or entity disambiguation is the task of determining the identities of entities mentioned in text, by linking each entity to an entry (if exists) in an open knowledge base such asWikipedia (Han et al., 2011; Hoffart et al., 2011).", "startOffset": 208, "endOffset": 248}, {"referenceID": 4, "context": "Entity linking (EL) or entity disambiguation is the task of determining the identities of entities mentioned in text, by linking each entity to an entry (if exists) in an open knowledge base such asWikipedia (Han et al., 2011; Hoffart et al., 2011).", "startOffset": 208, "endOffset": 248}, {"referenceID": 11, "context": ", (Kazama and Torisawa, 2007; Ratinov and Roth, 2009; Radford et al., 2015).", "startOffset": 2, "endOffset": 75}, {"referenceID": 4, "context": "The training data for the classifier is generated by using the CoNLL English training data set and the AIDA-YAGO2 data set that provides the Wikipedia titles for the named entities in the CoNLL English data set (Hoffart et al., 2011).", "startOffset": 211, "endOffset": 233}], "year": 2017, "abstractText": "The state-of-the-art named entity recognition (NER) systems are statistical machine learning models that have strong generalization capability (i.e., can recognize unseen entities that do not appear in training data) based on lexical and contextual information. However, such a model could still make mistakes if its features favor a wrong entity type. In this paper, we utilize Wikipedia as an open knowledge base to improve multilingual NER systems. Central to our approach is the construction of high-accuracy, highcoverage multilingual Wikipedia entity type mappings. These mappings are built from weakly annotated data and can be extended to new languages with no human annotation or language-dependent knowledge involved. Based on these mappings, we develop several approaches to improve an NER system. We evaluate the performance of the approaches via experiments on NER systems trained for 6 languages. Experimental results show that the proposed approaches are effective in improving the accuracy of such systems on unseen entities, especially when a system is applied to a new domain or it is trained with little training data (up to 18.3 F1 score improvement).", "creator": "LaTeX with hyperref package"}}}