{"id": "1707.02483", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jul-2017", "title": "Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection", "abstract": "The state-of-the-art named entity recognition (NER) systems are supervised machine learning models that require large amounts of manually annotated data to achieve high accuracy. However, annotating NER data by human is expensive and time-consuming, and can be quite difficult for a new language. In this paper, we present two weakly supervised approaches for cross-lingual NER with no human annotation in a target language. The first approach is to create automatically labeled NER data for a target language via annotation projection on comparable corpora, where we develop a heuristic scheme that effectively selects good-quality projection-labeled data from noisy data. The second approach is to project distributed representations of words (word embeddings) from a target language to a source language, so that the source-language NER system can be applied to the target language without re-training. We also design two co-decoding schemes that effectively combine the outputs of the two projection-based approaches. We evaluate the performance of the proposed approaches on both in-house and open NER data for several target languages. The results show that the combined systems outperform three other weakly supervised approaches on the CoNLL data.", "histories": [["v1", "Sat, 8 Jul 2017 19:45:47 GMT  (574kb,D)", "http://arxiv.org/abs/1707.02483v1", "11 pages, The 55th Annual Meeting of the Association for Computational Linguistics (ACL), 2017"]], "COMMENTS": "11 pages, The 55th Annual Meeting of the Association for Computational Linguistics (ACL), 2017", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["jian ni", "georgiana dinu", "radu florian"], "accepted": true, "id": "1707.02483"}, "pdf": {"name": "1707.02483.pdf", "metadata": {"source": "CRF", "title": "Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection", "authors": ["Jian Ni"], "emails": ["raduf}@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that the two are two different species that have developed in different ways in recent years: in the USA, in Europe, in Europe, in Europe, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "2 NER Models", "text": "The NER task can be formulated as a sequence labeling problem: faced with a sequence of words x1,..., xn, we want xi, 1 \u2264 i \u2264 n for each word. In this section, we present three NER models that are used in the work."}, {"heading": "2.1 CRFs and MEMMs", "text": "Conditional random fields (CRFs) are a class of discriminatory probabilistic graphical models that provide powerful tools for labeling sequential data (Lafferty et al., 2001). CRFs learn a conditional probability model p\u03bb (l | x) from a set of labeled training data, where x = (x1,..., xn) is a random sequence of input words, l = (l1,..., ln) is the sequence of labeling variables (NER tags) for x, and l has certain Markov properties bound to x. Specifically, a general CRF with the sequence o assumes that labeling variable li is derived from a fixed number o of previous labeling variables li \u2212 1,..., li \u2212 o, with the following conditional distribution: p\u03bb (l | x) = e properties bound to x."}, {"heading": "2.2 Neural Networks", "text": "With the increasing popularity of distributed (vector) representations of words, neural network models have recently been used to handle many NLP tasks, including those of NLP (Collobert et al., 2011; Lample et al., 2016). We use a locally normalized model (conditional distribution is normalized per token as in MEMMs) and introduce context dependency by conditioning the previously assigned tags. We use a target word and its surrounding context as characteristics. We do not use other common characteristics such as gazetteer or character-level representations as such characteristics that may not be readily available or transferable to other languages. We have used two neural network architectures. The first (called NN1) uses the word embedding of a word as input. The second (called N2) adds a hidden word smoothing that forms an average layer of non-LP functions."}, {"heading": "3 Annotation Projection Approach", "text": "Existing annotation approaches require parallel corpora or translations between a source language and a target language with alignment information. In this paper, we are developing a heuristic, language-independent data selection scheme aimed at selecting high-quality projected data from all comparable corpora. We are using English as the source language. Suppose we have comparable sentence pairs (X, Y) between English and a target language in which X \u2032 English sentences x (1),..., x (N), Y includes target sentences y (1),..., y (N) and y (j) is marked with an alignment model at x (j), 1 \u2264 j \u2264 N). We are using a sentence pair (x, y) as an example to illustrate how the annotation process works, where x = (x1, x2,... xs) is an English sentence, and y = (y2, yt) is a target language."}, {"heading": "3.1 Data Selection Scheme", "text": "We first design a measurement to measure the quality of annotations of a projection-labeled sentence in the target language. We construct a frequency table that includes all entities in the projected target language sentences. For each entity e, T also contains the projected Q tags for e and the relative frequency (empirical probability) P (l | e) that entity e is labeled with day l. Table 1 shows a snapshot of the frequency table in which the target language is Portuguese. We use P (l | e) to measure the reliability of the label e with day l in the target language. Intuition is that if an entity e is labeled with a tag l with higher frequency than other tags in the projected data, it is more likely that the annotation is correct. For example, if the common accuracy of the source system and the alignment system is greater than the marker, then the correct 0.5."}, {"heading": "3.2 Accuracy Improvements", "text": "We evaluate the effectiveness of the data selection scheme using experiments in 4 target languages: Japanese, Korean, German, and Portuguese. We use comparable corpora between English and each target language (from 2M to 6M tokens) with alignment information. For each target language, we also have a set of manually annotated NER data (from 30K to 45K tokens) that serve as test data to evaluate the target language NER system. The source (English) of the NER system is a linear-chain CRF model that achieves an accuracy of 88.9 F1 score on an independent NER test set. Alignment systems between English and the target languages are maximum Entropy models (Ittycheriah and Roukos, 2005), with an accuracy of 69.4 / 62.0 / 76.1 / 88.0 F1 score on an independent Japanese / Korean / Korean / Korean / Korean / German / Portuguese / Portuguese / Portuguese / Portuguese / Portuguese / Portuguese / Portuguese Score-iore-iore-Score-Score-Score-Score-iScore-Score-Score-Score-i8.0 F1 Score on an independent Japanese / Korean / German / Portuguese / Portuguese / Portuguese / Portuguese / Portuguese / Portuguese / Portuguese / iS / iS-Score-Score-Score-Score-iS-Score-Score-iS-Score-Score-Score-iS-Score-Score-Score-iS-Score-Score-Score-iK-Score-Score-Score-Score-Score-iK-Score-iK-Score-Score-Score-Score-iK-Score-Score-Score-iK-Score-Score-Score-iK-Score-Score-Score-Score-iK-Score-Score-Score-iK-Score-Score-Score-Score-iK-Score-Score-Score-Score-Score-Score-iK-Score-iK-Score-Score-Score-Score-Score-iK-Score-Score-Score"}, {"heading": "4 Representation Projection Approach", "text": "In this paper, we propose a new approach to the direct transfer of NER models based on representational projections. As part of this approach, we train a single English NER system that uses only word embedding as input representations. We create mapping functions that can map words in any language into English, and simply use the English NER system for decoding. Specifically, by mapping all languages into English, we use a universal NER system and do not need to retrain the system when a new language is added."}, {"heading": "4.1 Monolingual Word Embeddings", "text": "We first build vector representations of words (word embeddings) for a language using monolingual data. We use a variant of the Continuous Bag-of-Words (CBOW) model word2vec (Mikolov et al., 2013a), which concatenates, rather than adds, the context words that surround a target word (similar to (Ling et al., 2015). In addition, we use weights w = 1dist (x, xc), which disintegrate with the removal of a context word xc into a target word x. Tests of word similarity benchmarks show that this variant results in small improvements over the standard CBOW model. We train 300-dimensional word embeddings for English. Subsequently (Mikolov et al., 2013b) we use larger dimensional embeddings for the target languages, namely 800. We train word2vec for 1 epoch for English / Spanish and less data for the rest of the 5 epochs we have for the languages."}, {"heading": "4.2 Cross-Lingual Representation Projection", "text": "Similar to (Mikolov et al., 2013b) in a target language f, we first learn a small dictionary from a phrase table, the word-to-word alignment between English and the target language f. The dictionary contains English and target-language word pairs with weights: (xi, yi, wi) i = 1,..., n, where xi is an English word, yi is a target-language word, and the weight wi = P (xi | yi) is the relative frequency of xi given word pairs with weights. Suppose we have monolingual word embedding for English and the target language f. Let ui-Rd1 be the vector representation for the English word xi, vi-Rd2 the vector representation for the target-language word yi."}, {"heading": "4.3 Direct NER Model Transfer", "text": "The source language (English) NER system is a neural network model (with architecture NN1 or NN2) that uses only word embedding functions (embedding of a word and its surrounding context) in the English vector space. Model transmission is achieved simply by projecting the target language word embedding into the English vector space and decoding it using the English NER system.Specifically, given the word embedding of a word sequence in a target language f, (v1,..., vt) we project it into the English vector space by using the linear mapping Mf \u2192 e: (Mf \u2192 ev1,..., Mf \u2192 evt).The English NER system is then applied to the projected input to produce NER tags. Words that are not in the target language vocabulary are projected into their English embedding if they are in the English vocabulary, or if they are located in a different vector of a K."}, {"heading": "5 Co-Decoding", "text": "Given two poorly supervised NER systems trained on different data using different modes (MEMM model for annotation projection and neural network model for representation projection), we would like to design a co-decoding scheme that can combine the results (views) of the two systems to produce output that is more accurate than the results of individual systems. Since both systems are statistical models and can generate trust values (probabilities), a natural co-decoding scheme is to compare the confidence values of the NER tags generated by the two systems with higher confidence values, and the confidence values of two poorly supervised systems may not be directly comparable, especially when comparing O tags with non-O tags (i.e., entity tags). We are looking at an Exclude-O verification system that we find to be a co-decoding scheme that is empirically more effective."}, {"heading": "6 Experiments", "text": "In this section, we evaluate the performance of the proposed approaches to linguistic NER, including the 2 projection-based approaches and the 2 coding schemes for combining them: (1) The annotation projection approach (AP) with heuristic data selection; (2) The representation projection approach (with two neural network architectures NN1 and NN2); (3) The confidence-based coding scheme of Exclude-O; (4) The based coding scheme."}, {"heading": "6.1 NER Data Sets", "text": "The first group consists of internal humanly annotated Newswire-NER data for four languages: Japanese, Korean, German, and Portuguese, labeled with over 50 entity types, and the main motivation for using such a fine-grained entity type is to create cognitive question-and-answer applications based on the NER systems.The entity type set is designed to cover many of the common entity types that naturally receive formulated questions.The sizes of the test data sets range from 30K to 45K tokens.The second group includes open humanly annotated Newswire-NER data for Spanish, Dutch, and German from the CoNLL-NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003).The CoNLL data includes 4 entity types: PER (People), ORG (Organizations), LOC (Sites), and 70K (Sizes of Development Sets)."}, {"heading": "6.2 Evaluation for In-House NER Data", "text": "In Table 3, we show the results of different approaches to the internal NER data. For the projection of annotations, the source language (English) NER system is a linear CRF model, which was designed with 328K tokens of human commented English Newswire data. Target language NER systems are second order MEMM models, which are based on 1.3M, 1.5M, 2.6M and 1.5M tokens of projection designated data for Japanese, Korean, German and Portuguese Newswire data (see Table 2). For the presentation, the source language (English) NER systems are based on neural network models with the architectures NN1 and NN2 (see Figure 1), both of which are built with 328K tokens of human commented English Newswire data. Results show that the viviviviviviviviviviviviviviviviviviviviation approach with the NNN1 and N2 architectures is the best vivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivivi"}, {"heading": "6.3 Evaluation for CoNLL NER Data", "text": "For the CoNLL data, the source, a linear CRF model is formed with the CoNLL training data and the target language, where the threshold parameters q and n are determined on the basis of CoNLL development. Compared to the data selection contained in the annotation of the note, the data is selected using the heuristic data selection scheme, where the threshold parameters q and n are determined on the basis of CoNLL development."}, {"heading": "7 Related Work", "text": "The traditional annotation projection approaches (Yarowsky et al., 2001; Zitouni et Florian, 2008; Ehrmann et al., 2011) project NER tags across language pairs using parallel corpora or translations. Wang and Manning (2014) proposed a variant of the annotation projection that projects expectations of tags and uses these as constraints to train a model based on generalized expectation criteria. Annotation projection was also applied to several other lingual NLP tasks, including word disambiguation (Diab and Resnik, 2002), part-of-speech (POS) tagging (Yarowsky et al., 2001) and annotation projection of lingual parsing (Rasooli and Collins, 2015).Wikipedia was used to generate weakly labeled multilingual NER training data. The basic idea is to use Wikipedia pages in entity-type manual attribution (Wikipedia based on either the rules of Wikipedia and the rules of attribution)."}, {"heading": "8 Conclusion", "text": "In this paper, we developed two weakly monitored approaches to linguistic NER based on effective annotation and representation projection, and developed two coding schemes that intelligently combine the two projection-based systems. Experimental results show that the combined systems outperform three state-of-the-art linguistic NER approaches and provide a strong foundation for building linguistic NER systems without human annotation in target languages."}], "references": [{"title": "Polyglot-ner: Massive multilingual named entity recognition", "author": ["Rami Al-Rfou", "Vivek Kulkarni", "Bryan Perozzi", "Steven Skiena."], "venue": "Proceedings of the 2015 SIAM International Conference on Data Mining.", "citeRegEx": "Al.Rfou et al\\.,? 2015", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "An unsupervised method for word sense tagging using parallel corpora", "author": ["Mona Diab", "Philip Resnik."], "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. Association for Com-", "citeRegEx": "Diab and Resnik.,? 2002", "shortCiteRegEx": "Diab and Resnik.", "year": 2002}, {"title": "Building a multilingual named entity-annotated corpus using annotation projection", "author": ["Maud Ehrmann", "Marco Turchi", "Ralf Steinberger."], "venue": "Proceedings of Recent Advances in Natural Language Processing. Association", "citeRegEx": "Ehrmann et al\\.,? 2011", "shortCiteRegEx": "Ehrmann et al\\.", "year": 2011}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Manaal Faruqui", "Chris Dyer."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computa-", "citeRegEx": "Faruqui and Dyer.,? 2014", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2014}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Stephan Gouws", "Yoshua Bengio", "Greg Corrado."], "venue": "Proceedings of the 32nd International Conference on Machine Learning. JMLR Workshop", "citeRegEx": "Gouws et al\\.,? 2015", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Simple task-specific bilingual word embeddings", "author": ["Stephan Gouws", "Anders S\u00f8gaard."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Gouws and S\u00f8gaard.,? 2015", "shortCiteRegEx": "Gouws and S\u00f8gaard.", "year": 2015}, {"title": "Cross-lingual dependency parsing based on distributed representations", "author": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Guo et al\\.,? 2015", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "A maximum entropy word aligner for arabic-english machine translation", "author": ["Abraham Ittycheriah", "Salim Roukos."], "venue": "Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing. pages", "citeRegEx": "Ittycheriah and Roukos.,? 2005", "shortCiteRegEx": "Ittycheriah and Roukos.", "year": 2005}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Alexandre Klementiev", "Ivan Titov", "Binod Bhattarai."], "venue": "Proceedings of COLING 2012. The COLING 2012 Organizing Committee, Mumbai, India, pages 1459\u20131474.", "citeRegEx": "Klementiev et al\\.,? 2012", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."], "venue": "Proceedings of the Eighteenth International Conference on Machine", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Neural architectures for named entity recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Lample et al\\.,? 2016", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics:", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Maximum entropy markov models for information extraction and segmentation", "author": ["Andrew McCallum", "Dayne Freitag", "Fernando C.N. Pereira."], "venue": "Proceedings of the Seventeenth International Conference on Machine Learning.", "citeRegEx": "McCallum et al\\.,? 2000", "shortCiteRegEx": "McCallum et al\\.", "year": 2000}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR abs/1301.3781. http://arxiv.org/abs/1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Tomas Mikolov", "Quoc V. Le", "Ilya Sutskever."], "venue": "CoRR abs/1309.4168. http://arxiv.org/abs/1309.4168.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A survey of named entity recognition and classification", "author": ["David Nadeau", "Satoshi Sekine."], "venue": "Linguisticae Investigationes 30(1):3\u201326. Publisher: John Benjamins Publishing Company. https://doi.org/10.1075/li.30.1.03nad.", "citeRegEx": "Nadeau and Sekine.,? 2007", "shortCiteRegEx": "Nadeau and Sekine.", "year": 2007}, {"title": "Improving multilingual named entity recognition with wikipedia entity type mapping", "author": ["Jian Ni", "Radu Florian."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for", "citeRegEx": "Ni and Florian.,? 2016", "shortCiteRegEx": "Ni and Florian.", "year": 2016}, {"title": "Computer-Intensive Methods for Testing Hypotheses: An Introduction", "author": ["Eric W. Noreen."], "venue": "John Wiley & Sons, Inc., New York, NY, USA.", "citeRegEx": "Noreen.,? 1989", "shortCiteRegEx": "Noreen.", "year": 1989}, {"title": "Learning multilingual named entity recognition from wikipedia", "author": ["Joel Nothman", "Nicky Ringland", "Will Radford", "Tara Murphy", "James R. Curran."], "venue": "Journal of Artificial Intelligence 194:151\u2013175. https://doi.org/10.1016/j.artint.2012.03.006.", "citeRegEx": "Nothman et al\\.,? 2013", "shortCiteRegEx": "Nothman et al\\.", "year": 2013}, {"title": "Density-driven cross-lingual transfer of dependency parsers", "author": ["Sadegh Mohammad Rasooli", "Michael Collins."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association", "citeRegEx": "Rasooli and Collins.,? 2015", "shortCiteRegEx": "Rasooli and Collins.", "year": 2015}, {"title": "Mining wiki resources for multilingual named entity recognition", "author": ["E. Alexander Richman", "Patrick Schone."], "venue": "Proceedings of ACL-08: HLT . Association for Computational Linguistics, pages 1\u20139. http://aclweb.org/anthology/P08-1001.", "citeRegEx": "Richman and Schone.,? 2008", "shortCiteRegEx": "Richman and Schone.", "year": 2008}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Oscar T\u00e4ckstr\u00f6m", "Ryan McDonald", "Jakob Uszkoreit."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? 2012", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Introduction to the conll-2002 shared task: Language-independent named entity recognition", "author": ["Erik F. Tjong Kim Sang."], "venue": "Proceedings of the Sixth Conference on Natural Language Learning Volume 20. Association for Computational Linguis-", "citeRegEx": "Sang.,? 2002", "shortCiteRegEx": "Sang.", "year": 2002}, {"title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "author": ["Erik F. Tjong Kim Sang", "Fien De Meulder."], "venue": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 - Vol-", "citeRegEx": "Sang and Meulder.,? 2003", "shortCiteRegEx": "Sang and Meulder.", "year": 2003}, {"title": "Cross-lingual named entity recognition via wikification", "author": ["Chen-Tse Tsai", "Stephen Mayhew", "Dan Roth."], "venue": "Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning. Association for Computational Linguistics, pages", "citeRegEx": "Tsai et al\\.,? 2016", "shortCiteRegEx": "Tsai et al\\.", "year": 2016}, {"title": "Cross-lingual wikification using multilingual embeddings", "author": ["Chen-Tse Tsai", "Dan Roth."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Asso-", "citeRegEx": "Tsai and Roth.,? 2016", "shortCiteRegEx": "Tsai and Roth.", "year": 2016}, {"title": "Bilingual word embeddings from non-parallel documentaligned data applied to bilingual lexicon induction", "author": ["Ivan Vuli\u0107", "Marie-Francine Moens."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Vuli\u0107 and Moens.,? 2015", "shortCiteRegEx": "Vuli\u0107 and Moens.", "year": 2015}, {"title": "Cross-lingual projected expectation regularization for weakly supervised learning", "author": ["Mengqiu Wang", "D. Christopher Manning."], "venue": "Transactions of the Association of Computational Linguistics 2:55\u201366. http://aclweb.org/anthology/Q14-1005.", "citeRegEx": "Wang and Manning.,? 2014", "shortCiteRegEx": "Wang and Manning.", "year": 2014}, {"title": "Inducing multilingual text analysis tools via robust projection across aligned corpora", "author": ["David Yarowsky", "Grace Ngai", "Richard Wicentowski."], "venue": "Proceedings of the First International Conference on Human Language Technology Re-", "citeRegEx": "Yarowsky et al\\.,? 2001", "shortCiteRegEx": "Yarowsky et al\\.", "year": 2001}, {"title": "Mention detection crossing the language barrier", "author": ["Imed Zitouni", "Radu Florian."], "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 600\u2013609.", "citeRegEx": "Zitouni and Florian.,? 2008", "shortCiteRegEx": "Zitouni and Florian.", "year": 2008}], "referenceMentions": [{"referenceID": 16, "context": "The state-of-the-art NER systems are supervised machine learning models (Nadeau and Sekine, 2007), including maximum entropy", "startOffset": 72, "endOffset": 97}, {"referenceID": 13, "context": "Markov models (MEMMs) (McCallum et al., 2000), conditional random fields (CRFs) (Lafferty et al.", "startOffset": 22, "endOffset": 45}, {"referenceID": 10, "context": ", 2000), conditional random fields (CRFs) (Lafferty et al., 2001) and neural networks (Collobert et al.", "startOffset": 42, "endOffset": 65}, {"referenceID": 1, "context": ", 2001) and neural networks (Collobert et al., 2011; Lample et al., 2016).", "startOffset": 28, "endOffset": 73}, {"referenceID": 11, "context": ", 2001) and neural networks (Collobert et al., 2011; Lample et al., 2016).", "startOffset": 28, "endOffset": 73}, {"referenceID": 29, "context": ", (Yarowsky et al., 2001; Zitouni and Florian, 2008; Ehrmann et al., 2011).", "startOffset": 2, "endOffset": 74}, {"referenceID": 30, "context": ", (Yarowsky et al., 2001; Zitouni and Florian, 2008; Ehrmann et al., 2011).", "startOffset": 2, "endOffset": 74}, {"referenceID": 3, "context": ", (Yarowsky et al., 2001; Zitouni and Florian, 2008; Ehrmann et al., 2011).", "startOffset": 2, "endOffset": 74}, {"referenceID": 21, "context": ", (Richman and Schone, 2008; Nothman et al., 2013; Al-Rfou et al., 2015).", "startOffset": 2, "endOffset": 72}, {"referenceID": 19, "context": ", (Richman and Schone, 2008; Nothman et al., 2013; Al-Rfou et al., 2015).", "startOffset": 2, "endOffset": 72}, {"referenceID": 0, "context": ", (Richman and Schone, 2008; Nothman et al., 2013; Al-Rfou et al., 2015).", "startOffset": 2, "endOffset": 72}, {"referenceID": 22, "context": ", (T\u00e4ckstr\u00f6m et al., 2012; Tsai et al., 2016).", "startOffset": 2, "endOffset": 45}, {"referenceID": 25, "context": ", (T\u00e4ckstr\u00f6m et al., 2012; Tsai et al., 2016).", "startOffset": 2, "endOffset": 45}, {"referenceID": 21, "context": "The results show that the combined systems outperform the state-of-the-art cross-lingual NER approaches proposed in T\u00e4ckstr\u00f6m et al. (2012), Nothman et al.", "startOffset": 116, "endOffset": 140}, {"referenceID": 19, "context": "(2012), Nothman et al. (2013) and Tsai et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 19, "context": "(2012), Nothman et al. (2013) and Tsai et al. (2016) on the CoNLL NER test data (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003).", "startOffset": 8, "endOffset": 53}, {"referenceID": 10, "context": "provide powerful tools for labeling sequential data (Lafferty et al., 2001).", "startOffset": 52, "endOffset": 75}, {"referenceID": 13, "context": "A related conditional probability model, called maximum entropy Markov model (MEMM) (McCallum et al., 2000), assumes that l is a Markov", "startOffset": 84, "endOffset": 107}, {"referenceID": 10, "context": "As a result, CRFs can better handle the label bias problem (Lafferty et al., 2001).", "startOffset": 59, "endOffset": 82}, {"referenceID": 1, "context": "tor) representations of words, neural network models have recently been applied to tackle many NLP tasks including NER (Collobert et al., 2011; Lample et al., 2016).", "startOffset": 119, "endOffset": 164}, {"referenceID": 11, "context": "tor) representations of words, neural network models have recently been applied to tackle many NLP tasks including NER (Collobert et al., 2011; Lample et al., 2016).", "startOffset": 119, "endOffset": 164}, {"referenceID": 8, "context": "The alignment systems between English and the target languages are maximum entropy models (Ittycheriah and Roukos, 2005), with an accu-", "startOffset": 90, "endOffset": 120}, {"referenceID": 18, "context": "Using a stratified shuffling test (Noreen, 1989), for a significance level of 0.", "startOffset": 34, "endOffset": 48}, {"referenceID": 14, "context": "We use a variant of the Continuous Bag-of-Words (CBOW) word2vec model (Mikolov et al., 2013a), which concatenates the context words surrounding a target word instead of adding them (similarly to (Ling et al.", "startOffset": 70, "endOffset": 93}, {"referenceID": 12, "context": ", 2013a), which concatenates the context words surrounding a target word instead of adding them (similarly to (Ling et al., 2015)).", "startOffset": 110, "endOffset": 129}, {"referenceID": 15, "context": "Following (Mikolov et al., 2013b), we use larger dimensional embeddings for the target languages, namely 800.", "startOffset": 10, "endOffset": 33}, {"referenceID": 15, "context": "We learn cross-lingual word embedding mappings, similarly to (Mikolov et al., 2013b).", "startOffset": 61, "endOffset": 84}, {"referenceID": 15, "context": "In (7) we generalize the formulation in (Mikolov et al., 2013b) by adding frequency weights to the word pairs, so that more frequent pairs are of higher importance.", "startOffset": 40, "endOffset": 63}, {"referenceID": 17, "context": "systems also use the multilingual Wikipedia entity type mappings developed in (Ni and Florian, 2016) to generate dictionary features and as decoding constraints, which improve the annotation projection approach by 3.", "startOffset": 78, "endOffset": 100}, {"referenceID": 21, "context": "In Table 5 we compare our top systems (confidence or rank-based co-decoding of AP and NN1, determined by the development data) with the best results of the cross-lingual NER approaches proposed in T\u00e4ckstr\u00f6m et al. (2012), Nothman et al.", "startOffset": 197, "endOffset": 221}, {"referenceID": 19, "context": "(2012), Nothman et al. (2013) and Tsai et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 19, "context": "(2012), Nothman et al. (2013) and Tsai et al. (2016) on the CoNLL test data.", "startOffset": 8, "endOffset": 53}, {"referenceID": 21, "context": "Spanish P R F1 T\u00e4ckstr\u00f6m et al. (2012) x x 59.", "startOffset": 15, "endOffset": 39}, {"referenceID": 19, "context": "3 Nothman et al. (2013) x x 61.", "startOffset": 2, "endOffset": 24}, {"referenceID": 19, "context": "3 Nothman et al. (2013) x x 61.0 Tsai et al. (2016) x x 60.", "startOffset": 2, "endOffset": 52}, {"referenceID": 21, "context": "Dutch P R F1 T\u00e4ckstr\u00f6m et al. (2012) x x 58.", "startOffset": 13, "endOffset": 37}, {"referenceID": 19, "context": "4 Nothman et al. (2013) x x 64.", "startOffset": 2, "endOffset": 24}, {"referenceID": 19, "context": "4 Nothman et al. (2013) x x 64.0 Tsai et al. (2016) x x 61.", "startOffset": 2, "endOffset": 52}, {"referenceID": 21, "context": "German P R F1 T\u00e4ckstr\u00f6m et al. (2012) x x 40.", "startOffset": 14, "endOffset": 38}, {"referenceID": 19, "context": "4 Nothman et al. (2013) x x 55.", "startOffset": 2, "endOffset": 24}, {"referenceID": 19, "context": "4 Nothman et al. (2013) x x 55.8 Tsai et al. (2016) x x 48.", "startOffset": 2, "endOffset": 52}, {"referenceID": 2, "context": "Annotation projection has also been applied to several other cross-lingual NLP tasks, including word sense disambiguation (Diab and Resnik, 2002), part-of-speech (POS) tagging (Yarowsky et al.", "startOffset": 122, "endOffset": 145}, {"referenceID": 29, "context": "Annotation projection has also been applied to several other cross-lingual NLP tasks, including word sense disambiguation (Diab and Resnik, 2002), part-of-speech (POS) tagging (Yarowsky et al., 2001) and dependency parsing (Rasooli and Collins, 2015).", "startOffset": 176, "endOffset": 199}, {"referenceID": 20, "context": ", 2001) and dependency parsing (Rasooli and Collins, 2015).", "startOffset": 31, "endOffset": 58}, {"referenceID": 21, "context": "The basic idea is to first categorize Wikipedia pages into entity types, either based on manually constructed rules that utilize the category information of Wikipedia (Richman and Schone, 2008) or Freebase attributes (Al-Rfou et al.", "startOffset": 167, "endOffset": 193}, {"referenceID": 0, "context": "The basic idea is to first categorize Wikipedia pages into entity types, either based on manually constructed rules that utilize the category information of Wikipedia (Richman and Schone, 2008) or Freebase attributes (Al-Rfou et al., 2015), or via a classifier trained with manually labeled Wikipedia pages (Nothman et al.", "startOffset": 217, "endOffset": 239}, {"referenceID": 19, "context": ", 2015), or via a classifier trained with manually labeled Wikipedia pages (Nothman et al., 2013).", "startOffset": 75, "endOffset": 97}, {"referenceID": 0, "context": "The basic idea is to first categorize Wikipedia pages into entity types, either based on manually constructed rules that utilize the category information of Wikipedia (Richman and Schone, 2008) or Freebase attributes (Al-Rfou et al., 2015), or via a classifier trained with manually labeled Wikipedia pages (Nothman et al., 2013). Heuristic rules are then developed in these works to automatically label the Wikipedia text with NER tags. Ni and Florian (2016) built high-accuracy, high-coverage multilingual Wikipedia entity type mappings using weakly labeled data and applied those mappings as decoding constrains or dictionary features to improve multilingual NER systems.", "startOffset": 218, "endOffset": 460}, {"referenceID": 22, "context": "For direct NER model transfer, T\u00e4ckstr\u00f6m et al. (2012) built cross-lingual word clusters using monolingual data in source/target languages and aligned parallel data between source and target", "startOffset": 31, "endOffset": 55}, {"referenceID": 26, "context": "(2016) applied the cross-lingual wikifier developed in (Tsai and Roth, 2016) and multilingual Wikipedia dump to generate language-", "startOffset": 55, "endOffset": 76}, {"referenceID": 25, "context": "Tsai et al. (2016) applied the cross-lingual wikifier developed in (Tsai and Roth, 2016) and multilingual Wikipedia dump to generate language-", "startOffset": 0, "endOffset": 19}, {"referenceID": 9, "context": "Similar approaches for cross-lingual model transfer have been applied to other NLP tasks such as document classification (Klementiev et al., 2012), dependency parsing (Guo et al.", "startOffset": 121, "endOffset": 146}, {"referenceID": 7, "context": ", 2012), dependency parsing (Guo et al., 2015) and POS tagging (Gouws and S\u00f8gaard, 2015).", "startOffset": 28, "endOffset": 46}, {"referenceID": 6, "context": ", 2015) and POS tagging (Gouws and S\u00f8gaard, 2015).", "startOffset": 24, "endOffset": 49}], "year": 2017, "abstractText": "The state-of-the-art named entity recognition (NER) systems are supervised machine learning models that require large amounts of manually annotated data to achieve high accuracy. However, annotating NER data by human is expensive and time-consuming, and can be quite difficult for a new language. In this paper, we present two weakly supervised approaches for cross-lingual NER with no human annotation in a target language. The first approach is to create automatically labeled NER data for a target language via annotation projection on comparable corpora, where we develop a heuristic scheme that effectively selects goodquality projection-labeled data from noisy data. The second approach is to project distributed representations of words (word embeddings) from a target language to a source language, so that the sourcelanguage NER system can be applied to the target language without re-training. We also design two co-decoding schemes that effectively combine the outputs of the two projection-based approaches. We evaluate the performance of the proposed approaches on both in-house and open NER data for several target languages. The results show that the combined systems outperform three other weakly supervised approaches on the CoNLL data.", "creator": "LaTeX with hyperref package"}}}