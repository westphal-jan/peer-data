{"id": "1707.02747", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jul-2017", "title": "Robust Imitation of Diverse Behaviors", "abstract": "Deep generative models have recently shown great promise in imitation learning for motor control. Given enough data, even supervised approaches can do one-shot imitation learning; however, they are vulnerable to cascading failures when the agent trajectory diverges from the demonstrations. Compared to purely supervised methods, Generative Adversarial Imitation Learning (GAIL) can learn more robust controllers from fewer demonstrations, but is inherently mode-seeking and more difficult to train. In this paper, we show how to combine the favourable aspects of these two approaches. The base of our model is a new type of variational autoencoder on demonstration trajectories that learns semantic policy embeddings. We show that these embeddings can be learned on a 9 DoF Jaco robot arm in reaching tasks, and then smoothly interpolated with a resulting smooth interpolation of reaching behavior. Leveraging these policy representations, we develop a new version of GAIL that (1) is much more robust than the purely-supervised controller, especially with few demonstrations, and (2) avoids mode collapse, capturing many diverse behaviors when GAIL on its own does not. We demonstrate our approach on learning diverse gaits from demonstration on a 2D biped and a 62 DoF 3D humanoid in the MuJoCo physics environment.", "histories": [["v1", "Mon, 10 Jul 2017 08:46:14 GMT  (2138kb,D)", "http://arxiv.org/abs/1707.02747v1", null], ["v2", "Fri, 14 Jul 2017 09:31:26 GMT  (1189kb,D)", "http://arxiv.org/abs/1707.02747v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ziyu wang", "josh merel", "scott reed", "greg wayne", "nando de freitas", "nicolas heess"], "accepted": true, "id": "1707.02747"}, "pdf": {"name": "1707.02747.pdf", "metadata": {"source": "CRF", "title": "Robust Imitation of Diverse Behaviors", "authors": ["Ziyu Wang", "Josh Merel", "Scott Reed", "Greg Wayne", "Nando de Freitas", "Nicolas Heess"], "emails": ["ziyu@google.com", "jsmerel@google.com", "reedscot@google.com", "gregwayne@google.com", "nandodefreitas@google.com", "heess@google.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of them are able to survive themselves if they do not see themselves able to survive themselves, and that they are not able to survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}, {"heading": "2 Background and Related Work", "text": "We begin our brief consideration with generative models. A canonical method of training generative models is to maximize the probability of data (max.). This corresponds to minimizing the differentiation between the distribution of data and the model: DKL (pdata (\u00b7) | p.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I."}, {"heading": "3 A Generative Modeling Approach to Imitating Diverse Behaviors", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Behavioral cloning with variational autoencoders suited for control", "text": "In this section, we follow a similar approach to Duan et al. [6], but opt for stochastic VAEs with a distribution q\u03c6 (z | x1: T) in order to better regulate latent space. In our UAE, an encoder maps a demonstration sequence to an embedding vector z. Given z, we decipher both the state and the course of action as shown in Figure 1. To train the model, we minimize the following loss: L (\u03b1, w, \u03c6; \u03c4i) = \u2212 Eq\u03c6 (z | xi1: Ti) [Ti \u0445 t = 1 log \u03c0\u03b1 (a t | xit, z) + log pw (xit + 1 | xit, z)] + DKL (z | xi1: Ti) | p (z)) Our encoder q uses a bidirectional LSTM model to generate the final embedding."}, {"heading": "3.2 Diverse generative adversarial imitation learning", "text": "As already mentioned, it is difficult to imitate the different behaviors of the state, but the conditions under which we move are very different. (1) We have found a solution to achieve a more robust policy. (2) We have found a solution that is generated by the UAE. (3) We have found a solution that addresses the different behaviors of the GAIL in relation to the different posterior QE (z).We have optimized discrimination by optimizing the following objectivemax method. (4) A related work [4) presents a conditional work. (3) We have a conditional work. (4) We have a conditional work. (4) We have a conditional work. (4) We have a conditional work. (4) We have a conditional work. We have a conditional work. (4) We have a conditional work."}, {"heading": "4 Experiments", "text": "We look at three bodies: a robotic arm with 9 DoF, a planar walker with 9 DoF, and a complex humanoid 62 DoF (56-actuated joint angles and a freely translating and rotating 3D root joint). While BC is sufficient for the task to be completed to obtain a functioning controller, our complete learning process is crucial for the other two problems. We analyze the resulting embedding spaces and show that they have a rich and meaningful structure that can be used for control. Finally, we show that the encoder can be used to capture the core of novel demonstration courses that can then be reproduced by the controller. All experiments are performed with the physics engine MuJoCo [35]. Details on simulation and experimental setup can be found in the appendix."}, {"heading": "4.1 Robotic arm reaching", "text": "The physical Jaco is a robotic arm developed by Kinova Robotics. To obtain demonstrations, we trained 60 independent strategies to reach random target locations 2 in the workspace, based on the same initial configuration. From each of the first 50 strategies, we generated 30 trajectories, which serve as training data for the UAE model (a total of 1500 training paths). The remaining 10 strategies were used to generate test data. The achievement task is relatively simple, so the UAE strategy is relatively robust with this amount of data. After training, the UAE encodes and reproduces the demonstrations shown in Figure 2. Representative examples can be found in the video in the supplementary material. To further investigate the nature of the embedding space, we code two trajectories. Next, we construct the embedding of the interpolation policy by linking the interpolation of space with the interpolation points."}, {"heading": "4.2 2D Walker", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "4.3 Complex humanoid", "text": "We are looking at a humanoid body of high dimensionality, which is a hard control problem. Although the construction of this body and its associated control policy is described in [18] and briefly summarized in the appendix (Section A.3) for completeness, we are creating training paths with existing controllers that can produce instances of one of six different motion styles (see Section A.3). Examples of such trajectories are shown in Fig. 5 and in the supplementary video. Training paths consist of 250 random trajectories from 6 different neural network controllers trained to capture 6 different motion styles from the CMU motion database. Each trajectory is 334 steps or 10 seconds long. We are using a second set of 5 controllers from which we are generating trajectories for evaluation (3 of these strategies were trained to capture the same motion styles as the strategies for generating training data)."}, {"heading": "5 Conclusions", "text": "We have proposed an approach to imitation learning that combines the advantageous properties of density modeling techniques with those of GAIL. The result is a model that learns from a moderate number of demonstration courses (1) a semantically well-structured embedding of behaviors, (2) a corresponding multi-task controller that makes it possible to robustly execute various behaviors from this embedding space, and (3) an encoder that can map new gradients into the embedding space, thus enabling a unique imitation. Our experimental results show that our approach can work on a variety of control problems and that it even scales to very challenging problems such as controlling a simulated humanoid with a large number of degrees of freedom."}, {"heading": "A Details of the experiments", "text": "We trained the random attainment of strategies with deep deterministic gradients (DDPG, [31, 16]) to arrive at random positions in the work area. Simulations were performed for 2.5 seconds or 50 steps. For more details on hyperdiscrimination parameters and network configuration, refer to Table 1.A.2 WalkerThe demonstration guidelines were trained to achieve different speeds. Target speeds were trained from a set of four different speeds (m / s) -1, 0, 1, 3. For each target speed in {\u2212 1, 0, 1, 3}, we trained 12 strategies to achieve three target speeds each -1, 0, and 1 depending on a context label. Finally, 12 strategies are trained to achieve three target speeds each -1, 0, and 3 depending on a context label. For each target speed group, a network search is performed using two parameters: the initial protocol sigma for politics and random speeds."}], "references": [{"title": "A survey of robot learning from demonstration", "author": ["B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robotics and Autonomous Systems, 57(5):469\u2013483,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Wasserstein GAN", "author": ["M. Arjovsky", "S. Chintala", "L. Bottou"], "venue": "Preprint arXiv:1701.07875,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2017}, {"title": "Model-based adversarial imitation learning", "author": ["N. Baram", "O. Anschel", "S. Mannor"], "venue": "Preprint arXiv:1612.02179,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "BEGAN: Boundary equilibrium generative adversarial networks", "author": ["D. Berthelot", "T. Schumm", "L. Metz"], "venue": "Preprint arXiv:1703.10717,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2017}, {"title": "Robot programming by demonstration", "author": ["A. Billard", "S. Calinon", "R. Dillmann", "S. Schaal"], "venue": "Springer handbook of robotics, pages 1371\u20131394.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "One-shot imitation learning", "author": ["Y. Duan", "M. Andrychowicz", "B. Stadie", "J. Ho", "J. Schneider", "I. Sutskever", "P. Abbeel", "W. Zaremba"], "venue": "Preprint arXiv:1703.07326,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "NIPS 2016 tutorial: Generative adversarial networks", "author": ["I. Goodfellow"], "venue": "Preprint arXiv:1701.00160,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS, pages 2672\u20132680,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Bidirectional LSTM networks for improved phoneme classification and recognition", "author": ["A. Graves", "S. Fern\u00e1ndez", "J. Schmidhuber"], "venue": "Artificial Neural Networks: Formal Models and Their Applications\u2013ICANN 2005, pages 753\u2013753,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Boundary-seeking generative adversarial networks", "author": ["R.D. Hjelm", "A.P. Jacob", "T. Che", "K. Cho", "Y. Bengio"], "venue": "Preprint arXiv:1702.08431,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "Generative adversarial imitation learning", "author": ["J. Ho", "S. Ermon"], "venue": "NIPS, pages 4565\u20134573,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Imitation learning: A survey of learning methods", "author": ["A. Hussein", "M.M. Gaber", "E. Elyan", "C. Jayne"], "venue": "ACM Computing Surveys, 50(2):21,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Auto-encoding variational bayes", "author": ["D. Kingma", "M. Welling"], "venue": "Preprint arXiv:1312.6114,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Imitating driver behavior with generative adversarial networks", "author": ["A. Kuefler", "J. Morton", "T. Wheeler", "M. Kochenderfer"], "venue": "Preprint arXiv:1701.06699,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2017}, {"title": "Continuous control with deep reinforcement learning", "author": ["T. Lillicrap", "J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "arXiv:1509.02971,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Least squares generative adversarial networks", "author": ["X. Mao", "Q. Li", "H. Xie", "R.Y.K. Lau", "Z. Wang", "S.P. Smolley"], "venue": "Preprint ArXiv:1611.04076,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning human behaviors from motion capture by adversarial imitation", "author": ["J. Merel", "Y. Tassa", "TB. Dhruva", "S. Srinivasan", "J. Lemmon", "Z. Wang", "G. Wayne", "N. Heess"], "venue": "Preprint arXiv:1707.02201,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}, {"title": "Conditional generative adversarial nets", "author": ["M. Mirza", "S. Osindero"], "venue": "Preprint arXiv:1411.1784,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Contact-aware nonlinear control of dynamic characters", "author": ["U. Muico", "Y. Lee", "J. Popovi\u0107", "Z. Popovi\u0107"], "venue": "SIGGRAPH,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "DeepLoco: Dynamic locomotion skills using hierarchical deep reinforcement learning", "author": ["X.B. Peng", "G. Berseth", "K. Yin", "M. van de Panne"], "venue": "In SIGGRAPH,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2017}, {"title": "Efficient training of artificial neural networks for autonomous navigation", "author": ["D.A. Pomerleau"], "venue": "Neural Computation, 3(1):88\u201397,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1991}, {"title": "Loss-sensitive generative adversarial networks on Lipschitz densities", "author": ["G.J. Qi"], "venue": "Preprint arXiv:1701.06264,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2017}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "ICML,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient reductions for imitation learning", "author": ["S. Ross", "A. Bagnell"], "venue": "AIStats,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["S. Ross", "G.J. Gordon", "D. Bagnell"], "venue": "AIStats,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Policy distillation", "author": ["A. Rusu", "S. Colmenarejo", "C. Gulcehre", "G. Desjardins", "J. Kirkpatrick", "R. Pascanu", "V. Mnih", "K. Kavukcuoglu", "R. Hadsell"], "venue": "Preprint arXiv:1511.06295,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Abbeel", "M.I. Jordan", "P. Moritz"], "venue": "ICML,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "IEEE Transactions on Signal Processing, 45(11):2673\u20132681,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1997}, {"title": "Synthesis of controllers for stylized planar bipedal walking", "author": ["D. Sharon", "M. van de Panne"], "venue": "In ICRA,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2005}, {"title": "Deterministic policy gradient algorithms", "author": ["D. Silver", "G. Lever", "N. Heess", "T. Degris", "D. Wierstra", "M. Riedmiller"], "venue": "ICML,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Simulating biped behaviors from human motion data", "author": ["K.W. Sok", "M. Kim", "J. Lee"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Third-person imitation learning", "author": ["B.C. Stadie", "P. Abbeel", "I. Sutskever"], "venue": "Preprint arXiv:1703.01703,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2017}, {"title": "A note on the evaluation of generative models", "author": ["L. Theis", "A. van den Oord", "M. Bethge"], "venue": "Preprint arXiv:1511.01844,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "MuJoCo: A physics engine for model-based control", "author": ["E. Todorov", "T. Erez", "Y. Tassa"], "venue": "IROS, pages 5026\u20135033,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "WaveNet: A generative model for raw audio", "author": ["A. van den Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": "Preprint arXiv:1609.03499,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Conditional image generation with pixelCNN decoders", "author": ["A. van den Oord", "N. Kalchbrenner", "L. Espeholt", "O. Vinyals", "A. Graves"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Visualizing data using t-SNE", "author": ["L. van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2008}, {"title": "MAGAN: Margin adaptation for generative adversarial networks", "author": ["R. Wang", "A. Cully", "H. Jin Chang", "Y. Demiris"], "venue": "Preprint arXiv:1704.03817,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 13, "context": "We first introduce a variational autoencoder (VAE) [14, 24] for supervised imitation, consisting of a bi-directional LSTM [12, 29, 9] encoder mapping demonstration sequences to embedding vectors, and two decoders.", "startOffset": 51, "endOffset": 59}, {"referenceID": 23, "context": "We first introduce a variational autoencoder (VAE) [14, 24] for supervised imitation, consisting of a bi-directional LSTM [12, 29, 9] encoder mapping demonstration sequences to embedding vectors, and two decoders.", "startOffset": 51, "endOffset": 59}, {"referenceID": 11, "context": "We first introduce a variational autoencoder (VAE) [14, 24] for supervised imitation, consisting of a bi-directional LSTM [12, 29, 9] encoder mapping demonstration sequences to embedding vectors, and two decoders.", "startOffset": 122, "endOffset": 133}, {"referenceID": 28, "context": "We first introduce a variational autoencoder (VAE) [14, 24] for supervised imitation, consisting of a bi-directional LSTM [12, 29, 9] encoder mapping demonstration sequences to embedding vectors, and two decoders.", "startOffset": 122, "endOffset": 133}, {"referenceID": 8, "context": "We first introduce a variational autoencoder (VAE) [14, 24] for supervised imitation, consisting of a bi-directional LSTM [12, 29, 9] encoder mapping demonstration sequences to embedding vectors, and two decoders.", "startOffset": 122, "endOffset": 133}, {"referenceID": 35, "context": "The second is a dynamics model mapping the embedding and previous state to the present state, while modelling correlations among states with a WaveNet [36].", "startOffset": 151, "endOffset": 155}, {"referenceID": 34, "context": "Experiments with a 9 DoF Jaco robot arm and a 9 DoF 2D biped walker, implemented in the MuJoCo physics engine [35], show that the VAE learns a structured semantic embedding space, which allows for smooth policy interpolation.", "startOffset": 110, "endOffset": 114}, {"referenceID": 5, "context": "[6]) are powerful models for one-shot imitation, they require large training datasets in order to work for non-trivial tasks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "These limitations of supervised learning for imitation, also known as behavioral cloning (BC) [22], are well known [25, 26].", "startOffset": 94, "endOffset": 98}, {"referenceID": 24, "context": "These limitations of supervised learning for imitation, also known as behavioral cloning (BC) [22], are well known [25, 26].", "startOffset": 115, "endOffset": 123}, {"referenceID": 25, "context": "These limitations of supervised learning for imitation, also known as behavioral cloning (BC) [22], are well known [25, 26].", "startOffset": 115, "endOffset": 123}, {"referenceID": 10, "context": "Recently, Ho and Ermon [11] showed a way to overcome the brittleness of supervised imitation using another type of deep generative model called Generative Adversarial Networks (GANs) [8].", "startOffset": 23, "endOffset": 27}, {"referenceID": 7, "context": "Recently, Ho and Ermon [11] showed a way to overcome the brittleness of supervised imitation using another type of deep generative model called Generative Adversarial Networks (GANs) [8].", "startOffset": 183, "endOffset": 186}, {"referenceID": 6, "context": "GAIL allows one to learn more robust policies with fewer demonstrations, but adversarial training introduces another difficulty called mode collapse [7].", "startOffset": 149, "endOffset": 152}, {"referenceID": 36, "context": "Auto-regressive models have been highly effective in both image and audio generation [37, 36].", "startOffset": 85, "endOffset": 93}, {"referenceID": 35, "context": "Auto-regressive models have been highly effective in both image and audio generation [37, 36].", "startOffset": 85, "endOffset": 93}, {"referenceID": 13, "context": "(1) For continuous latent variables, this bound can be optimized efficiently via the re-parameterization trick [14, 24].", "startOffset": 111, "endOffset": 119}, {"referenceID": 23, "context": "(1) For continuous latent variables, this bound can be optimized efficiently via the re-parameterization trick [14, 24].", "startOffset": 111, "endOffset": 119}, {"referenceID": 7, "context": "[8], have become very popular.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "GANs were noted for their ability to produce sharp image samples, unlike the blurrier samples from contemporary VAE models [8].", "startOffset": 123, "endOffset": 126}, {"referenceID": 6, "context": "However, unlike VAEs and autoregressive models trained via maximum likelihood, they suffer from the mode collapse problem [7].", "startOffset": 122, "endOffset": 125}, {"referenceID": 1, "context": "Recent work has focused on alleviating mode collapse in image modeling [2, 4, 17, 23, 39, 10], but so far these have not been demonstrated in the control domain.", "startOffset": 71, "endOffset": 93}, {"referenceID": 3, "context": "Recent work has focused on alleviating mode collapse in image modeling [2, 4, 17, 23, 39, 10], but so far these have not been demonstrated in the control domain.", "startOffset": 71, "endOffset": 93}, {"referenceID": 16, "context": "Recent work has focused on alleviating mode collapse in image modeling [2, 4, 17, 23, 39, 10], but so far these have not been demonstrated in the control domain.", "startOffset": 71, "endOffset": 93}, {"referenceID": 22, "context": "Recent work has focused on alleviating mode collapse in image modeling [2, 4, 17, 23, 39, 10], but so far these have not been demonstrated in the control domain.", "startOffset": 71, "endOffset": 93}, {"referenceID": 38, "context": "Recent work has focused on alleviating mode collapse in image modeling [2, 4, 17, 23, 39, 10], but so far these have not been demonstrated in the control domain.", "startOffset": 71, "endOffset": 93}, {"referenceID": 9, "context": "Recent work has focused on alleviating mode collapse in image modeling [2, 4, 17, 23, 39, 10], but so far these have not been demonstrated in the control domain.", "startOffset": 71, "endOffset": 93}, {"referenceID": 36, "context": "Like GANs, autoregressive models produce sharp and at times realistic image samples [37], but they tend to be slow to sample from and unlike VAEs do not immediately provide a latent vector representation of the data.", "startOffset": 84, "endOffset": 88}, {"referenceID": 21, "context": "When demonstrations are plentiful, BC is effective [22, 27, 6].", "startOffset": 51, "endOffset": 62}, {"referenceID": 26, "context": "When demonstrations are plentiful, BC is effective [22, 27, 6].", "startOffset": 51, "endOffset": 62}, {"referenceID": 5, "context": "When demonstrations are plentiful, BC is effective [22, 27, 6].", "startOffset": 51, "endOffset": 62}, {"referenceID": 24, "context": "Without abundant data, BC is known to be inadequate [25, 26, 11].", "startOffset": 52, "endOffset": 64}, {"referenceID": 25, "context": "Without abundant data, BC is known to be inadequate [25, 26, 11].", "startOffset": 52, "endOffset": 64}, {"referenceID": 10, "context": "Without abundant data, BC is known to be inadequate [25, 26, 11].", "startOffset": 52, "endOffset": 64}, {"referenceID": 10, "context": "GAIL [11] avoids some of the pitfalls of BC by allowing the agent to interact with the environment and learn from these interactions.", "startOffset": 5, "endOffset": 9}, {"referenceID": 27, "context": "In practice, trust region policy optimization (TRPO) is used to stabilize the learning process [28].", "startOffset": 95, "endOffset": 99}, {"referenceID": 14, "context": "GAIL has become a popular choice for imitation learning [15] and there already exist model-based [3] and third-person [33] extensions.", "startOffset": 56, "endOffset": 60}, {"referenceID": 2, "context": "GAIL has become a popular choice for imitation learning [15] and there already exist model-based [3] and third-person [33] extensions.", "startOffset": 97, "endOffset": 100}, {"referenceID": 32, "context": "GAIL has become a popular choice for imitation learning [15] and there already exist model-based [3] and third-person [33] extensions.", "startOffset": 118, "endOffset": 122}, {"referenceID": 4, "context": "We cannot cover this literature at the level of detail it deserves, and instead refer readers to recent authoritative surveys on the topic [5, 1, 13].", "startOffset": 139, "endOffset": 149}, {"referenceID": 0, "context": "We cannot cover this literature at the level of detail it deserves, and instead refer readers to recent authoritative surveys on the topic [5, 1, 13].", "startOffset": 139, "endOffset": 149}, {"referenceID": 12, "context": "We cannot cover this literature at the level of detail it deserves, and instead refer readers to recent authoritative surveys on the topic [5, 1, 13].", "startOffset": 139, "endOffset": 149}, {"referenceID": 10, "context": "Inspired by recent works, including [11, 33, 6], we focus on taking advantage of the dramatic recent advances in deep generative modelling to learn high-dimensional policies capable of learning a diverse set of behaviors from few demonstrations.", "startOffset": 36, "endOffset": 47}, {"referenceID": 32, "context": "Inspired by recent works, including [11, 33, 6], we focus on taking advantage of the dramatic recent advances in deep generative modelling to learn high-dimensional policies capable of learning a diverse set of behaviors from few demonstrations.", "startOffset": 36, "endOffset": 47}, {"referenceID": 5, "context": "Inspired by recent works, including [11, 33, 6], we focus on taking advantage of the dramatic recent advances in deep generative modelling to learn high-dimensional policies capable of learning a diverse set of behaviors from few demonstrations.", "startOffset": 36, "endOffset": 47}, {"referenceID": 29, "context": "In graphics, a significant effort has been devoted to the design physics controllers that take advantage of motion capture data, or key-frames and other inputs provided by animators [30, 32, 40, 20].", "startOffset": 182, "endOffset": 198}, {"referenceID": 31, "context": "In graphics, a significant effort has been devoted to the design physics controllers that take advantage of motion capture data, or key-frames and other inputs provided by animators [30, 32, 40, 20].", "startOffset": 182, "endOffset": 198}, {"referenceID": 19, "context": "In graphics, a significant effort has been devoted to the design physics controllers that take advantage of motion capture data, or key-frames and other inputs provided by animators [30, 32, 40, 20].", "startOffset": 182, "endOffset": 198}, {"referenceID": 20, "context": "Yet, as pointed out in a recent hierarchical control paper [21], the design of such controllers often requires significant human insight.", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "[6], but opt for stochastic VAEs as having a distribution q\u03c6(z|x1:T ) to better regularize the latent space.", "startOffset": 0, "endOffset": 3}, {"referenceID": 35, "context": "The state decoder is similar to a conditional WaveNet model [36].", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": "A related work [18] introduces a conditional GAIL objective to learn controllers for multiple behaviors from state trajectories, but the discriminator conditions on an annotated class label, as in conditional GANs [19].", "startOffset": 15, "endOffset": 19}, {"referenceID": 18, "context": "A related work [18] introduces a conditional GAIL objective to learn controllers for multiple behaviors from state trajectories, but the discriminator conditions on an annotated class label, as in conditional GANs [19].", "startOffset": 214, "endOffset": 218}, {"referenceID": 7, "context": "If we further assume an optimal discriminator [8], the cost optimized by the generator then becomes", "startOffset": 46, "endOffset": 49}, {"referenceID": 33, "context": "We know that GANs approximately optimize this divergence, and it is well documented that optimizing it leads to mode seeking behavior [34].", "startOffset": 134, "endOffset": 138}, {"referenceID": 27, "context": "Finally, the policy parameterized by \u03b8 is optimized with TRPO [28] while holding parameters \u03b1 fixed, as shown in Algorithm 1.", "startOffset": 62, "endOffset": 66}, {"referenceID": 34, "context": "All experiments are conducted with the MuJoCo physics engine [35].", "startOffset": 61, "endOffset": 65}, {"referenceID": 10, "context": "also [11]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 37, "context": "We embed training trajectories and perform dimensionality reduction with t-SNE [38].", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "The construction of this body and associated control policies is described in [18], and is briefly summarized in the appendix (section A.", "startOffset": 78, "endOffset": 82}], "year": 2017, "abstractText": "Deep generative models have recently shown great promise in imitation learning for motor control. Given enough data, even supervised approaches can do one-shot imitation learning; however, they are vulnerable to cascading failures when the agent trajectory diverges from the demonstrations. Compared to purely supervised methods, Generative Adversarial Imitation Learning (GAIL) can learn more robust controllers from fewer demonstrations, but is inherently mode-seeking and more difficult to train. In this paper, we show how to combine the favourable aspects of these two approaches. The base of our model is a new type of variational autoencoder on demonstration trajectories that learns semantic policy embeddings. We show that these embeddings can be learned on a 9 DoF Jaco robot arm in reaching tasks, and then smoothly interpolated with a resulting smooth interpolation of reaching behavior. Leveraging these policy representations, we develop a new version of GAIL that (1) is much more robust than the purely-supervised controller, especially with few demonstrations, and (2) avoids mode collapse, capturing many diverse behaviors when GAIL on its own does not. We demonstrate our approach on learning diverse gaits from demonstration on a 2D biped and a 62 DoF 3D humanoid in the MuJoCo physics environment.", "creator": "LaTeX with hyperref package"}}}