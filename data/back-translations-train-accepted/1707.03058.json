{"id": "1707.03058", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jul-2017", "title": "Improving Neural Parsing by Disentangling Model Combination and Reranking Effects", "abstract": "Recent work has proposed several generative neural models for constituency parsing that achieve state-of-the-art results. Since direct search in these generative models is difficult, they have primarily been used to rescore candidate outputs from base parsers in which decoding is more straightforward. We first present an algorithm for direct search in these generative models. We then demonstrate that the rescoring results are at least partly due to implicit model combination rather than reranking effects. Finally, we show that explicit model combination can improve performance even further, resulting in new state-of-the-art numbers on the PTB of 94.25 F1 when training only on gold data and 94.66 F1 when using external data.", "histories": [["v1", "Mon, 10 Jul 2017 20:47:33 GMT  (23kb)", "http://arxiv.org/abs/1707.03058v1", "ACL 2017. The first two authors contributed equally"]], "COMMENTS": "ACL 2017. The first two authors contributed equally", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["daniel fried", "mitchell stern", "dan klein"], "accepted": true, "id": "1707.03058"}, "pdf": {"name": "1707.03058.pdf", "metadata": {"source": "CRF", "title": "Improving Neural Parsing by Disentangling Model Combination and Reranking Effects", "authors": ["Daniel Fried", "Mitchell Stern", "Dan Klein"], "emails": ["dfried@cs.berkeley.edu", "mitchell@cs.berkeley.edu", "klein@cs.berkeley.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 7.03 058v 1 [cs.C L] 10 July 2 017ative neural models for parsing constituencies that achieve state-of-the-art results. As the direct search in these generative models is difficult, they were primarily used to revive candidate outputs from base savings models where decoding is easier. We first present an algorithm for direct search in these generative models. Subsequently, we show that the rescoring results are at least partially due to implicit model combinations and not to reranking effects. Finally, we show that explicit model combinations can further improve performance, leading to new state-of-the-art numbers on the PTB of 94.25 F1 when training on gold data only and 94.66 F1 when using external data."}, {"heading": "1 Introduction", "text": "In fact, it is so that we are able to outtrump ourselves. (In fact: In fact, it is so that we are able to outtrump ourselves.) In fact, it is so that we are able to outtrump ourselves. (In fact: In fact, it is so that we are able to outtrump ourselves.) In fact, it is so that we are able to outtrump ourselves. \"(in the world.).\" (In the world.). \"\" In the world. \"\"... \"\". \"\" \"\".. \"\" \".\" \"\" \".\" \".\" \"\". \"\". \"\". \"\". \"\" \".\" \"\" \"..\" \".\" \"\". \"\" \".\" \"\". \"\". \"\". \"\". \"\". \"\". \"\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\".. \".\".. \"...\". \"....\"..... \".....\". \".............\"........................................................................................................... \"\" \"\" \"\" \"............\" \"\" \"\"......... \"\" \"\" \"\" \"\".... \"\" \"\" \"\" \"\""}, {"heading": "2 Decoding in generative neural models", "text": "All the parsers we examine in this work (the discriminatory parser RD, and the two generative parsers RG and LM, see section 1) produce parse trees in the deepest, left-to-right orbit, using the same basic actions: NT (X), which opens a new constituent with the non-terminal symbol X; SHIFT / GEN (w), which adds a word; and REDUCE, which closes the current constituents. We refer to Dyer et al. (2016) for a complete description of these actions and the constraints necessary for them to ensure a valid parse tree; 1The primary difference between actions in the discriminatory and generative models is that the discriminatory model uses a SHIFT action, which is fixed to produce the next word in the sentence, the generative models use GEN (w) to define a distribution across all possible words in the lexicon w."}, {"heading": "2.1 Action-synchronous beam search", "text": "Previous work on discriminatory neural constituency scanners has shown the effectiveness of beam search with a small beam (Vinyals et al., 2015) or even greedy search, as in the case of RD (Dyer et al., 2016). The standard beam search method, which we call action synchronous, maintains a beam of partially completed K-analyses, which all have the same number of actions taken. At each stage, a pool of successors is constructed by expanding each candidate in the beam with each of its possible next actions. As the next beam, the K successors are most likely to be selected. Unfortunately, we find that action synchronous beam searches break down in both generative models we are examining in this work by not finding any parses that have a high score under the model. This is due to the probabilities of actions NT (X) for all terms X and X for the terms, which are almost greater than the probability of the GW for the next word to be generated."}, {"heading": "2.2 Word-synchronous beam search", "text": "To deal with this problem, we force partial-parse candidates to compete with each other on a word-by-word level, rather than exclusively at the level of individual actions. The word-synchronous beam search we use is very similar to the approximate decoding procedures developed for other generative models (Henderson, 2003; Titov and Henderson, 2010; Buys and Blunsom, 2015) and can be considered a simplified version of the process used in the generative top-down parsers of Roark (2001) and Charniak (2010). In word-synchronous search, we expand the beam space by identifying rays by tuples (| W |, | Aw |), where | W | is the number of words produced so far in the sentence, and | Aw | is the number of structural actions taken since the last word."}, {"heading": "3 Experiments", "text": "Using the above decoding methods, we try to separate the reranking effects from the combination effects of the model through a series of reranking experiments, using Sections 2-21 for training, Section 22 for development, and Section 23 for testing, our basic experiments are conducted at Penn Treebank (Marcus et al., 1993), using the pre-coached model published by Choe and Charniak (2016) for the generative LSTM model (LM), and we train discriminatory (RD) and generative (RG) models that follow Dyer et al. (2016) by using the same hyperparameter settings and pre-coached word embedding from Ling et al. (2015) for the discriminatory model. The automatically predicted part-of-speech tags that we use as input for RD are the same as those used by Cross and Huang (2016). In each experiment, we obtain a set of parameter parameters for each set, or we perform a series of candidates for each RD."}, {"heading": "3.1 Augmenting the candidate set", "text": "We first experiment with combining the candidate lists from multiple models, which allows us to look for potential model errors and combination effects. Let's look at the standard re-ranking approach B \u2192 A, where we search in B to find a set of candidates for each set, and select the candidate with the highest score from these under A. We then expand this approach by also looking directly in A to find candidates with high scores for each set, and combine this with the candidate list proposed by B by taking the Union, A, B, B, B. We then select the candidate with the highest score from this list under A. If A generally seeks candidates outside the candidate list of B, but reduces this score (i.e., if B, A, A, A, A, A, is inferior to B, A), this indicates a model combination effect: A makes errors that are hidden by a limited list of candidates by B. This seems to be the case for both generative models of A, but A, A, A, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B"}, {"heading": "3.2 Score combination", "text": "If the cross-scoring setup has an implicit combination effect, where strong performance results from searching in one model and scoring with the other, we could expect significant further performance improvements by explicitly combining the scores of both models. To this end, we evaluate each parse by taking a weighted sum of the log probabilities assigned by both models (Hayashi et al., 2013), using an interpolation parameter that we set to maximize F1 at the development stage. These results are given in the columns RD + RG and RD + LM in Table 2. We note that combining the scores of both models alone improves the use of the scores of both models, regardless of the source of the candidates. In all cases, these improvements are statistically significant. The score combination also compensates for the reduction in performance we have previously seen in the addition of candidates from the generative model, more than wett: RG, RG + RG, RD + RD, both models."}, {"heading": "3.3 Strengthening model combination", "text": "Given the success of the model combination between the base model and a single generative model, we are also investigating the hypothesis that the generative models are complementary."}, {"heading": "4 Discussion", "text": "Direct searching in the generative models yields results that are in part surprising, since they reveal the presence of parses that favor the generative models, but that result in lower performance than the candidates proposed by the base model. However, the results are not surprising either in that the explicit combination of scores alone enables the reranking setup to perform better than implicit combinations that use only the scores of a single model. Furthermore, we see support for the hypothesis that the generative models alone can achieve good results, with the generative LSTM model performing particularly strongly and independently. While this search method allows us to examine these generative models by unraveling reranking effects and combination effects, the increase in performance by extending the candidate lists with the results of the search might not be worth the computational effort required in a practical parser."}, {"heading": "Acknowledgments", "text": "We would like to thank Adhiguna Kuncoro and Do Kook Choe for their help in providing data and answering questions about their work, as well as Jacob Andreas, John DeNero and the anonymous reviewers for their suggestions. DF is supported by a scholarship from NDSEG. MS is supported by an NSF Graduate Research Fellowship."}], "references": [{"title": "Generative incremental dependency parsing with neural networks", "author": ["Jan Buys", "Phil Blunsom."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Buys and Blunsom.,? 2015", "shortCiteRegEx": "Buys and Blunsom.", "year": 2015}, {"title": "A maximum-entropy-inspired parser", "author": ["Eugene Charniak."], "venue": "Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Charniak.,? 2000", "shortCiteRegEx": "Charniak.", "year": 2000}, {"title": "Top-down nearly-contextsensitive parsing", "author": ["Eugene Charniak."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Charniak.,? 2010", "shortCiteRegEx": "Charniak.", "year": 2010}, {"title": "Parsing as language modeling", "author": ["Do Kook Choe", "Eugene Charniak."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Choe and Charniak.,? 2016", "shortCiteRegEx": "Choe and Charniak.", "year": 2016}, {"title": "Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles", "author": ["James Cross", "Liang Huang."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Cross and Huang.,? 2016", "shortCiteRegEx": "Cross and Huang.", "year": 2016}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A Smith."], "venue": "Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Efficient stacked dependency parsing by forest reranking", "author": ["Katsuhiko Hayashi", "Shuhei Kondo", "Yuji Matsumoto."], "venue": "Transactions of the Association for Computational Linguistics 1:139\u2013150.", "citeRegEx": "Hayashi et al\\.,? 2013", "shortCiteRegEx": "Hayashi et al\\.", "year": 2013}, {"title": "Inducing history representations for broad coverage statistical parsing", "author": ["James Henderson."], "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Henderson.,? 2003", "shortCiteRegEx": "Henderson.", "year": 2003}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Wang Ling", "Chris Dyer", "Alan Black", "Isabel Trancoso."], "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Probabilistic top-down parsing", "author": ["Linguistics. Brian Roark"], "venue": null, "citeRegEx": "Roark.,? \\Q2001\\E", "shortCiteRegEx": "Roark.", "year": 2001}], "referenceMentions": [{"referenceID": 5, "context": "Recent work on neural constituency parsing (Dyer et al., 2016; Choe and Charniak, 2016) has found multiple cases where generative scoring models for which inference is complex outperform base models for which inference is simpler.", "startOffset": 43, "endOffset": 87}, {"referenceID": 3, "context": "Recent work on neural constituency parsing (Dyer et al., 2016; Choe and Charniak, 2016) has found multiple cases where generative scoring models for which inference is complex outperform base models for which inference is simpler.", "startOffset": 43, "endOffset": 87}, {"referenceID": 5, "context": "In this paper, we present experiments to isolate the degree to which each gain occurs for each of two state-of-the-art generative neural parsing models: the Recurrent Neural Network Grammar generative parser (RG) of Dyer et al. (2016), and the", "startOffset": 216, "endOffset": 235}, {"referenceID": 1, "context": "LSTM language modeling generative parser (LM) of Choe and Charniak (2016).", "startOffset": 58, "endOffset": 74}, {"referenceID": 5, "context": "a strong base parser (the RNNG discriminative parser, RD (Dyer et al., 2016)), performance decreases when compared to using just candidates from the base parser, i.", "startOffset": 57, "endOffset": 76}, {"referenceID": 5, "context": "We refer to Dyer et al. (2016) for a complete description of these actions, and the constraints on them necessary to ensure valid parse trees.", "startOffset": 12, "endOffset": 31}, {"referenceID": 5, "context": ", 2015) or even greedy search, as in the case of RD (Dyer et al., 2016).", "startOffset": 52, "endOffset": 71}, {"referenceID": 7, "context": "decoding procedures developed for other generative models (Henderson, 2003; Titov and Henderson, 2010; Buys and Blunsom, 2015) and can be", "startOffset": 58, "endOffset": 126}, {"referenceID": 0, "context": "decoding procedures developed for other generative models (Henderson, 2003; Titov and Henderson, 2010; Buys and Blunsom, 2015) and can be", "startOffset": 58, "endOffset": 126}, {"referenceID": 7, "context": "viewed as a simplified version of the procedure used in the generative top-down parsers of Roark (2001) and Charniak (2010).", "startOffset": 91, "endOffset": 104}, {"referenceID": 1, "context": "viewed as a simplified version of the procedure used in the generative top-down parsers of Roark (2001) and Charniak (2010).", "startOffset": 108, "endOffset": 124}, {"referenceID": 1, "context": "For the LSTM generative model (LM), we use the pre-trained model released by Choe and Charniak (2016). We train RNNG discriminative (RD) and generative (RG)", "startOffset": 86, "endOffset": 102}, {"referenceID": 5, "context": "models, following Dyer et al. (2016) by using the same hyperparameter settings, and using pre-", "startOffset": 18, "endOffset": 37}, {"referenceID": 7, "context": "trained word embeddings from Ling et al. (2015) for the discriminative model.", "startOffset": 29, "endOffset": 48}, {"referenceID": 4, "context": "The automaticallypredicted part-of-speech tags we use as input for RD are the same as those used by Cross and Huang (2016).", "startOffset": 100, "endOffset": 123}, {"referenceID": 1, "context": "The LM generative model was originally used to rerank a 50-best list taken from the Charniak parser (Charniak, 2000).", "startOffset": 100, "endOffset": 116}, {"referenceID": 6, "context": "To do so, we score each parse by taking a weighted sum of the log-probabilities assigned by both models (Hayashi et al., 2013), using an interpolation parameter which we tune to maximize F1 on the development set.", "startOffset": 104, "endOffset": 126}, {"referenceID": 2, "context": "7 \u2013 Dyer et al. (2016)-discriminative 91.", "startOffset": 4, "endOffset": 23}, {"referenceID": 2, "context": "7 \u2013 Dyer et al. (2016)-discriminative 91.7 \u2013 Dyer et al. (2016)-generative 93.", "startOffset": 4, "endOffset": 64}, {"referenceID": 1, "context": "3 \u2013 Choe and Charniak (2016) 92.", "startOffset": 13, "endOffset": 29}, {"referenceID": 1, "context": "Semi-supervised silver data Choe and Charniak (2016) found a substantial increase in performance by training on external data in addition to trees from the Penn Treebank.", "startOffset": 37, "endOffset": 53}, {"referenceID": 1, "context": "When training with silver data, we use a 1-to-1 ratio of silver data updates per gold data updates, which we found to give significantly faster convergence times on development set perplexity for RD and RG compared to the 10-to-1 ratio used by Choe and Charniak (2016) for LM.", "startOffset": 253, "endOffset": 269}], "year": 2017, "abstractText": "Recent work has proposed several generative neural models for constituency parsing that achieve state-of-the-art results. Since direct search in these generative models is difficult, they have primarily been used to rescore candidate outputs from base parsers in which decoding is more straightforward. We first present an algorithm for direct search in these generative models. We then demonstrate that the rescoring results are at least partly due to implicit model combination rather than reranking effects. Finally, we show that explicit model combination can improve performance even further, resulting in new state-of-the-art numbers on the PTB of 94.25 F1 when training only on gold data and 94.66 F1 when using external data.", "creator": "LaTeX with hyperref package"}}}