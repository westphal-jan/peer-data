{"id": "1707.03497", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jul-2017", "title": "Value Prediction Network", "abstract": "This paper proposes a novel deep reinforcement learning (RL) architecture, called Value Prediction Network (VPN), which integrates model-free and model-based RL methods into a single neural network. In contrast to typical model-based RL methods, VPN learns a dynamics model whose abstract states are trained to make option-conditional predictions of future values (discounted sum of rewards) rather than of future observations. Our experimental results show that VPN has several advantages over both model-free and model-based baselines in a stochastic environment where careful planning is required but building an accurate observation-prediction model is difficult. Furthermore, VPN outperforms Deep Q-Network (DQN) on several Atari games even with short-lookahead planning, demonstrating its potential as a new way of learning a good state representation.", "histories": [["v1", "Tue, 11 Jul 2017 23:32:36 GMT  (2705kb,D)", "http://arxiv.org/abs/1707.03497v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["junhyuk oh", "satinder singh", "honglak lee"], "accepted": true, "id": "1707.03497"}, "pdf": {"name": "1707.03497.pdf", "metadata": {"source": "CRF", "title": "Value Prediction Network", "authors": ["Junhyuk Oh", "Satinder Singh", "Honglak Lee"], "emails": ["junhyuk@umich.edu,", "baveja@umich.edu,", "honglak@umich.edu,", "honglak@google.com"], "sections": [{"heading": "1 Introduction", "text": "We will call such models an observation prediction model to distinguish them from another form of the model introduced in this paper. Building a precise observation model is often very difficult when the observation space is large. [23, 7, 14, 4] (e.g. high-dimensional pixel images), and even more difficult when the environment is stochastical. Therefore, it is a natural question whether it is possible to plan future observations without raw observations containing information that is unnecessary for planning, such as dynamically changing backgrounds in terms of their value / utility. One could try to predict abstractions of observations, but it is unclear how to roll such predictions forward."}, {"heading": "2 Related Work", "text": "In fact, it is such that most of us are able to put ourselves in a different world, in which they are able, in which they are able, in which they are able, in which they are able, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in"}, {"heading": "3 Value Prediction Network", "text": "The value prediction network was developed for semi-Markov decision-making processes (SMDPs). Let's leave xt to be the observation or a history of observations for partially observable MDPs (henceforth called observation only) and let it not be the option [33, 31, 26] at a time. Each option maps observations to primitive actions, and the following Bellman equation applies to all policies \u03c0: Q\u03c0 (xt, ot) = E [\u2211 k \u2212 1 i = 0 \u03b3 irt + i + \u03b3 kV \u03c0 (xt + k)], where \u03b3 is a discount factor, rt is the immediate reward at the time t, and k is the number of time steps learnt by the ot option before termination of observation xt + k.A VPN not only learns an option-value function Q\u03b8 (xt, ot) through a neural network parameterized like model-free RL, but also the dynamics of the rewards / values learnt by the option xt + k.A VPN to perform planning as described in section VQ 3.N]."}, {"heading": "3.1 Architecture", "text": "The VPN module consists of the following modules, parameterized by \u03b8 = {\u03b8enc, \u03b8value, \u03b8out, \u03b8trans}: Encoding fenc\u03b8: x 7 \u2192 s Value fvalue\u03b8: s 7 \u2192 V\u03b8 (s) Outcome fout\u03b8: s, o 7 \u2192 r, \u03b3 Transition f trans: s, o 7 \u2192 s \u2032 \u2022 Encoding module maps the observation (x) to the abstract state (s-Rm) using neural networks (e.g. CNN for visual observation). Note therefore that s is an abstract state representation learned from the network (and not an ambient state or even an approximation to one). \u2022 Value module estimates the value of the abstract state (VTB). Note that the value module is not a function of observation, but a function of the abstract state that replaces the VPK option. \u2022 Outcome module cancels the option reward (R) for executing the option."}, {"heading": "3.2 Planning", "text": "Although many existing planning methods (e.g. MCTS) can be applied to the VPN, we implement a simple planning method that performs rollouts with the VPN to a certain depth (say), henceforth referred to as planning depth, and aggregates all intermediate estimates described in Algorithm 1 and Figure 2. More formally, when an abstract state s = fenc\u043a (x) and an option o, the Q value calculated from the d-step planning is defined as: Qd planning (s, o) = r-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p)."}, {"heading": "3.4 Relationship to Existing Approaches", "text": "VPN is model-based in the sense that it learns a function that is sufficient to predict rewards / discounts / values without predicting observations. In the meantime, VPN can be considered model-free in the sense that it learns to directly estimate the value of the abstract state. From this perspective, VPN uses several additional prediction tasks, such as reward and discount predictions, to learn a good representation of the abstract state. An interesting feature of VPN is that its planning ability is used to calculate the bootstrapped target as well as to select options during Q-Learning. Therefore, since VPN improves the quality of its future predictions, it can not only achieve better results through its improved planning ability, but also generate more accurate target Q values during training, which promotes faster convergence compared to conventional Q-Learning."}, {"heading": "4 Experiments", "text": "Our experiments examined the following questions: 1) Does VPN outperform model-free baselines (e.g. DQN)? 2) What is the advantage of planning with a VPN over observation-based planning? 3) Is VPN useful for complex domains with high-dimensional sensory inputs such as Atari games?"}, {"heading": "4.1 Experimental Setting", "text": "A CNN network was used as the encoding module of VPN, and the transition module consists of an optional convolution layer that uses different weights according to the option, followed by a few more convolution layers. We used a residual connection [11] from the previous abstract state to the next abstract state, so the transition module learns to change the abstract state. The result module is similar to the transition module except that there is no residual connection and two fully connected layers are used to produce reward and discount. The value module consists of two fully connected layers. The number of layers and hidden units varies by domain. These details are described in the appendix. Our algorithm is based on asynchronous n-step-Q-learning [22] where n 10 and 16 threads are used. The target network is synchronized according to all 10K steps. We used the Adam Optimizer [15], and the best 0.0001 were chosen."}, {"heading": "4.2 Collect Domain", "text": "We define a simple but challenging 2D navigation task in which the agent should collect as many targets as possible within a span of time, as illustrated in Figure 4. In this task, the agent is the targets and walls are randomly predicted for each episode. The agent has four options: shift left / right / up / down to the first crossing branch or the end of the corridor in the chosen direction. Although it is easy to learn a suboptimal policy that collects nearby targets, finding the optimal trajectory in each episode requires careful planning, because the optimal solution cannot be calculated in polynomial span of time. An observation is presented as a 3D tensor (R3 \u00d7 10 \u00d7 10) with binary values indicating the presence / absence of each object."}, {"heading": "4.3 Atari Games", "text": "To investigate how VPN handles complex visual observations, we evaluated them on several Atari games [2]. Unlike in the Collect domain, most primitive actions in Atari games have little impact on value, and it is difficult to design useful extended options by hand. Nevertheless, we investigated whether VPNs in Atari games are useful even for short-term planning, using simple options that repeat the same primitive actions over long periods of time by pre-processing the game screen to 84 \u00d7 84 grayscale images. All architectures take the last 4 frames as input. We doubled the number of hidden units of the fully networked layer for DQN to roughly adjust the number of parameters. VPN learns to predict rewards and values, but no discount (since this is fixed), 3Much of the previous work on Atari games has used a frame skip of 4."}, {"heading": "5 Conclusion", "text": "We introduced Value Prediction Networks (VPNs) as a new in-depth method for integrating planning and learning, while learning about the dynamics of abstract states that make option-based predictions of future rewards / discounts / values rather than future observations. Our empirical evaluations showed that VPNs outperform model-free DQN baselines in several areas and outperform traditional observation-based planning in a stochastic area. An interesting future direction would be to develop methods that automatically learn the options that enable good planning in VPNs."}, {"heading": "Acknowledgement", "text": "This work has been supported by the NSF grant IIS-1526059. Any opinions, findings, conclusions or recommendations expressed herein are those of the authors and do not necessarily reflect the views of the sponsor."}, {"heading": "A Examples of Trajectories on Collect Domain", "text": "A.1 Comparison between VPN and DQN in deterministic environmentA.2 Comparison between VPN and OPN in stochastic environments"}, {"heading": "B Examples of Planning on Atari Games", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C Details of Learning", "text": "Algorithm 2 describes our algorithm for Training Value Forecasting Network (VPN). We observed that training the Results Module (reward and discount forecasting) on the basis of additional data collected from a random policy slightly improves performance because it reduces bias against agent behavior. Specifically, we fill a replay memory with R transitions from a random policy prior to training and sample transitions from the replay memory to train the Results Module for each iteration. This procedure is described in line 4 and lines 20-24 of Algorithm 2. This method was used in our experiment only for Collect Domain (not for Atari) Forecast Forecast Forecast Forecast Forecast Forecast Prognostics Prognostics Prognostics Prognostics Prognostic Network 0, by augmenting 1M transitions from a random policy. \u2212 Algorithm 2 Asynchronous n-Step Q-Learning with k-Step-Forecast and d-Planning Forecast Prognostics Prognostics Prognostics Prognostics Prognostics Prognostics Prognostics Prognostics Prognostic Network 0, not: 1 Project Planning for a random policy Study Area: 1: (not)."}, {"heading": "D Details of Hyperparameters", "text": "D.1 CollectThe encoding module of our VPN consists of Conv (32-3x3-1) -Conv (32-3x3-1) -Conv (64-4x4-2), where Conv (NKxK-1) -Conv (64-3x3-1) -Conv (64-1x1) -Conv (64-3x3-1) -Conv (64-3x3-1) -Conv (64-3x3-1) -Conv (64-3x1) and a separate Conv (64-1x1-1) for the mask that uses the 3rd conversion layer of the transition module."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "This paper proposes a novel deep reinforcement learning (RL) architecture, called<lb>Value Prediction Network (VPN), which integrates model-free and model-based<lb>RL methods into a single neural network. In contrast to typical model-based<lb>RL methods, VPN learns a dynamics model whose abstract states are trained<lb>to make option-conditional predictions of future values (discounted sum of re-<lb>wards) rather than of future observations. Our experimental results show that<lb>VPN has several advantages over both model-free and model-based baselines in a<lb>stochastic environment where careful planning is required but building an accurate<lb>observation-prediction model is difficult. Furthermore, VPN outperforms Deep<lb>Q-Network (DQN) on several Atari games even with short-lookahead planning,<lb>demonstrating its potential as a new way of learning a good state representation.", "creator": "LaTeX with hyperref package"}}}