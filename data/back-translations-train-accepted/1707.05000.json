{"id": "1707.05000", "review": {"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jul-2017", "title": "In-Order Transition-based Constituent Parsing", "abstract": "Both bottom-up and top-down strategies have been used for neural transition-based constituent parsing. The parsing strategies differ in terms of the order in which they recognize productions in the derivation tree, where bottom-up strategies and top-down strategies take post-order and pre-order traversal over trees, respectively. Bottom-up parsers benefit from rich features from readily built partial parses, but lack lookahead guidance in the parsing process; top-down parsers benefit from non-local guidance for local decisions, but rely on a strong encoder over the input to predict a constituent hierarchy before its construction.To mitigate both issues, we propose a novel parsing system based on in-order traversal over syntactic trees, designing a set of transition actions to find a compromise between bottom-up constituent information and top-down lookahead information. Based on stack-LSTM, our psycholinguistically motivated constituent parsing system achieves 91.8 F1 on WSJ benchmark. Furthermore, the system achieves 93.6 F1 with supervised reranking and 94.2 F1 with semi-supervised reranking, which are the best results on the WSJ benchmark.", "histories": [["v1", "Mon, 17 Jul 2017 04:27:11 GMT  (144kb,D)", "http://arxiv.org/abs/1707.05000v1", "Accepted by TACL"]], "COMMENTS": "Accepted by TACL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiangming liu", "yue zhang"], "accepted": true, "id": "1707.05000"}, "pdf": {"name": "1707.05000.pdf", "metadata": {"source": "CRF", "title": "In-Order Transition-based Constituent Parsing", "authors": ["Jiangming Liu", "Yue Zhang"], "emails": ["zhang}@sutd.edu.sg"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are a kind of conspiracy theory, reflected in the way it has evolved, and it is only a matter of time before it is able to unfold."}, {"heading": "2 Transition-based constituent parsing", "text": "Transition-based constituent parsing performs a left-right scan of the input set in which a stack is used to maintain partially constructed phrase structures while the input words are stored in a buffer. Formally, a state is defined as [\u03c3, i, f] where \u03c3 is the stack, i is the front index of the buffer, and f is a tool value indicating whether the parsing is complete. At each step, a transition action is applied to consume an input word or construct a new phrase structure."}, {"heading": "2.1 Bottom-up system", "text": "We use the bottom-up system of Sagae and Lavie (2005) as our bottom-up baseline. Starting from one state, the transition actions are \u2022 SHIFT: Popp the foreword from the buffer and push it onto the stack. \u2022 REDUCE-L / R-X: Popp the top two components from the stack, combine them to a new component with Label X and push the new component onto the stack. \u2022 UNARY-X: Popp the top component from the stack, lift it to a new component with Label X and push the new component onto the stack. \u2022 FINISH: Popp the root node from the stack and ending parsing.The bottom-up parser can be summarized as a deductive system in Figure 3 (a). Given the set with the binarized syntactic tree in Figure 1 (b), the sequence of actions SHIFT, SHIFT, SHIFT, SHIFT, DUR, SHIFT, SHIFT-SHIFT, SHIFT-SHIFT-SHIFT, SHIFT-SHIFT-SHIFT, SHIFT-SHIFT-SHIFT, SHIFT-SHIFT-SHIFT, SHIFT-SHIFT-SHIFT-SHIFT, SHIFT-SHIFT-SHIFT-SHIFT, SHIFT-SHIFT-SHIFT-SHIFT, SHIFT-SHIFT-SHIFT-SHIFT, SHIFT-SHIFT-SHIFT-SHIFT-SHIFT, SHIFT-SHIFT-SHIFT-SHIFT, SHIFT-SHIFT-SHIFT-SHIFT, SHIFT-SHIFT-SHIFT-SHIFT-SHIFT, SHIFT-SHIFT-SHIFT-SHIFT, SHIFT-SHIFT-SHIFT-SHIFT-X, SHIFT-SHIFT-X, SHIFT-SHIFT-SHIFT-SHIFT-X, SHIFT-SHIFT-SHIFT-SHIFT-SHIFT-"}, {"heading": "2.2 Top-down system", "text": "We take the top-down system from Dyer et al. (2016) as our top-down baseline. Considering a state, the series of transitional actions is \u2022 SHIFT: pop the front word from the buffer and push it onto the stack. \u2022 NT-X: open a nonterminal labeled X on the stack. \u2022 REDUZ: repeatedly pop completed partial trees or terminal symbols from the stack until an open nonterminal is encountered, and then this open NT is cracked and used as the labeling of a new component that has the cracked partial trees as its children. This new completed component is pressed onto the stack as a single composite label. The trigger system for the process is shown in Figure 3 (b) 2. Given the sentence in Figure 1, the sequence of actions NT-S, NT-NP, SHIFT, SHIFT, SHIFT, DURECE, to complete the system, can be used to complete and complete the DURECE."}, {"heading": "3 In-order system", "text": "We propose a novel order system for transition-based parsing of the components. Similar to bottom-up and top-down systems, the order system maintains a stack and a buffer to represent a state. The series of transition actions is defined as: \u2022 SHIFT: pops the foreword from the buffer and pushes it onto the stack. \u2022 PJ-X: project a nonterminal with the label X on the stack. \u2022 REDUCE: repeats pop completed subtrees or terminal symbols from the stack until a projected nonterminal is encountered, and then this projected nonterminal is packed and used as the label of a new component, and in addition, another element is packed on top of the stack as the leftiest child of the new component and the cracked subtrees are shown as its remaining children. These new completed components are called SHIFT-S, SHIFT system is pushed onto the stack as a single root element SHIFT-H (SHIFT system is shown in the SHIFT-ISP)."}, {"heading": "4 Neural parsing model", "text": "We use the stack LSTM parsing model from Dyer et al. (2016) for the three types of transition-based parsing systems in Section 2.1, 2.2, and 3, respectively, where a stack LSTM is used to represent the stack, a stack LSTM is used to represent the buffer, and a vanilla LSTM is used to represent the action history as shown in Figure 4."}, {"heading": "4.1 Word representation", "text": "We follow Dyer et al. (2015), which represents each word with three different types of embedding, including pre-trained word embedding, ewi, which is not finely tuned during parser training, randomly initialized embedding ewi, which is finely tuned, and randomly initialized part-of-speech embedding, which is finely tuned. The three embedding are concatenated and then passed to nonlinear layers to derive the final word embedding: xi = f (Winput [epi; ewi; ewi] + binput), where winput and binput are model parameters, wi and pi are the shape and POS tag of the ith input word, or f is a nonlinear function. In this essay, we use ReLu for f."}, {"heading": "4.2 Stack representation", "text": "We use a bidirectional LSTM as a composition function to represent components on stack 3. When parsing from top to bottom and parsing in order according to Dyer et al. (2016), as shown in Figure 5 (a), the composition representation scomp is calculated as follows: scomp = (LSTMfwd [ent, s0,..., sm]; LSTMbwd [ent, sm,..., s0]), where sent is the representation of a non-terminal, sj, j [0, m] is the j. child node, and m is the number of child nodes. In bottom-up parsing, we use header information in the composition function by dictating the order that the header node always precedes the non-terminal node in the bidirectional LSTM, as shown in Figure 5 (b)."}, {"heading": "4.3 Greedy action classification", "text": "In view of a sentence w0, w1,..., wn \u2212 1, where wi is the ith word and n is the length of the sentence, our parser makes incremental decisions on the local action classification. For the kest parsing state like [sj,..., s1, s0, i, false], the probability distribution of the current action is p: p = SOFTMAX (W [hstk; hbuf; hah] + b), (*) where W and b are model parameters, the representation of the stack information stack is ishstk = stack-LSTM [s0, s1,..., sj], the representation of the buffer information is hbuf ishbuf = stack-LSTM [xi, xi + 1,..., xn], x is the word representation, and the representation of the action history is hah ishah = LSTM [eactk \u2212 1, eactk \u2212 2,..., eact0] where actk-LSTX = STX, STX = STX, and STX = STX is the representation."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Data", "text": "We empirically compare our bottom-up, top-down and in-order parsers, and the experiments are conducted in both English and Chinese. For English data, we use the standard benchmark of the WSJ sections in the PTB (Marcus et al., 1993), using sections 2-21 for training data, section 22 for development data, and section 23 for testing both for dependency analysis and constituency parsing. We use the pre-trained English word embeddings generated on the AFP part of the English Gigaword. For Chinese data, we use version 5.1 of the Penn Chinese Treebank (CTB) (Xue et al., 2005). We use articles 001-270 and 440-1151 for training, articles 301-325 for system development, and articles 271-300 for final performance evaluation. We use the pre-taught Chinese word embeddings that are automatically assigned to the entire Chinese Gigaword work (the same 2016 Gigaus corridor as the English word in both the Peripi tags)."}, {"heading": "5.2 Settings", "text": "Hyper parameters For both English and Chinese experiments, we use the same hyper parameters as the work of Dyer et al. (2016) without further optimization as in Table 1. Reranking experiments After the same re-anking setting by Dyer et al. (2016) and Choe and Charniak (2016), we obtain 100 samples from our bottom-up, top-down and in-order model (Section 4) with exponence strategy (\u03b1 = 0.8) using the probability distribution (equation *). We use the re-anchor of Choe and Charniak (2016) both as our English re-anchor and as semi-monitored re-anchor and the generative re-anchor of Dyer et al. (2016) as our Chinese re-anchor."}, {"heading": "5.3 Development experiments", "text": "Table 2 shows the development results of the three parsing systems. The bottom-up system performs slightly better than the top-down system. The inorder system outperforms both the bottom-up and the top-down system."}, {"heading": "5.4 Results", "text": "Table 3 shows the analysis results on the English test dataset. We find that the bottom-up parser and the top-down parser have similar results under the greedy environment, and the in-order parser outperforms both of them. Also, the in-order parser achieves the best results. English constituent results We compare our models with previous work as shown in Table 4. With the full oversight setting 5, the inorder parser outperforms the state-of-the-art discrete parser (Shindo et al., 2012; Zhu et al., 2013), the state-of-the-art neural parser (Cross and Huang, 5Here, consider only the work of individual models 2016; Watanabe and Sumita, 2015), and the status-the-the-art parser (Durrett and Klein, 2015)."}, {"heading": "6 Analysis", "text": "We analyze the results of section 23 in WSJ, which provide our model (i.e. in-order parser) and two baseline models (i.e. bottom-up parser and top-down parser) based on sentence length, span, and constituent type."}, {"heading": "6.1 Influence of sentence length", "text": "Figure 6 shows the F1 values of the three parsers for sentences of different lengths. Compared to the top-down parser, the bottom-up parser performs better for short sentences with a falling length in the range [20-40]. This is probably because the bottom-up parser takes advantage of rich local characteristics from partially constructed trees that are useful for analyzing short sentences, but these local structures cannot be sufficient to analyze long sentences due to the spread of errors. On the other hand, the top-down parser performs better for long sentences with a falling length in the range [40-50], because the longer the sentences are, the richer the characteristics of the preview could become and they could be displayed correctly by the LSTM, which is advantageous for parsing non-local structures."}, {"heading": "6.2 Influence of span length", "text": "Figure 7 shows the F1 values of the three parsers on spans of different lengths. The trend of performance of the two baseline parsers is similar. Compared to the baseline parsers, the In-Order parser achieves significant improvements on long spans. This is linguistically due to the fact that the In-Order crossing over a tree allows constituent stress types to be correctly projected based on the information about the beginning (the most left nodes) of the spans, and then the projected components restrict the construction of long spans that differ from the top-down parser that generates constituent stress types without leaving traces of the spans."}, {"heading": "6.3 Influence of constituent type", "text": "Table 7 shows the F1 values of the three parsers for common stock types. The bottom-up parser performs better than the top-down parser for constituent types such as NP, S, SBAR, QP. We note that predicting these stock types explicitly requires modeling bottom-up structures. In other words, bottom-up information is necessary for us to know whether the span can be a noun phrase (NP) or a sentence (S). On the other hand, the top-down parser performs better at WHNP, which may be because a WHNP starts with a specific question word, making predicting without bottom-up information easy."}, {"heading": "6.4 Examples", "text": "We give sample output from the test to qualitatively compare the performance of the three parsers using the fully monitored model without any re-evaluation, as in Table 9. For example, given sentence # 2006, the bottom-up and in-order parsers both provide correct results. However, the top-down parser makes the wrong decision to generate an S, resulting in subsequent wrong decisions about VP. Sentence pattern ambiguaty allows top-down guidance by recognizing the word \"plans\" as a verb, while more bottom-up information is useful for local disamusion. In view of sentence # 308, the bottom-up parser prefers the construction of more local ones, such as \"once producers and customers,\" ignoring the possible SBAR clause, which is captured by the in-order parser projecting a constituent SBAR and continuing to complete the clause."}, {"heading": "7 Related work", "text": "Rosenkrantz and Lewis (1970) formalize this in automata theory, which often appears in compilation literature. Roark and Johnson (1999) apply the strategy to parsing. Typical work examines the transformation of syntactic trees based on rules from the left corner (Roark, 2001; Schuler et al., 2010; Van Schijndel and Schuler, 2013). In contrast, we propose a novel general transient-based parsing system (Dyer et al., 2016; Kuncoro et al., 2017) and CCG parsing (Xu, 2016; Lewis et al., 2016)."}, {"heading": "8 Conclusion", "text": "We proposed a novel psycho-linguistically motivated parsing system based on the inorder crossing of syntactic trees and aimed at finding a compromise between bottom-up information and top-down information on forward-looking developments. In the standard WSJ benchmark, our system outperforms bottom-up analyses of non-local ambiguities and top-down analyses of local decisions. The resulting parser achieves the results of the state-of-the-art analysis of constituents by achieving 94.2 F1 and dependency analyses by obtaining 96.2% UAS and 95.2% LAS."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their detailed and constructive comments. Yue Zhang is the corresponding author."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "ACL, pages 2442\u20132452.", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Training with exploration improves a greedy stack-lstm parser", "author": ["Miguel Ballesteros", "Yoav Goldberg", "Chris Dyer", "Noah A Smith."], "venue": "EMNLP.", "citeRegEx": "Ballesteros et al\\.,? 2016", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2016}, {"title": "Coarse-tofine n-best parsing and MaxEnt discriminative reranking", "author": ["Eugene Charniak", "Mark Johnson."], "venue": "ACL.", "citeRegEx": "Charniak and Johnson.,? 2005", "shortCiteRegEx": "Charniak and Johnson.", "year": 2005}, {"title": "A maximum-entropy-inspired parser", "author": ["Eugene Charniak."], "venue": "NAACL, pages 132\u2013139. Association for Computational Linguistics.", "citeRegEx": "Charniak.,? 2000", "shortCiteRegEx": "Charniak.", "year": 2000}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher Manning."], "venue": "EMNLP, pages 740\u2013750, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Bi-directional attention with agreement for dependency parsing", "author": ["Hao Cheng", "Hao Fang", "Xiaodong He", "Jianfeng Gao", "Li Deng."], "venue": "EMNLP.", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Parsing as language modeling", "author": ["Do Kook Choe", "Eugene Charniak."], "venue": "EMNLP.", "citeRegEx": "Choe and Charniak.,? 2016", "shortCiteRegEx": "Choe and Charniak.", "year": 2016}, {"title": "Head-driven statistical models for natural language parsing", "author": ["Michael Collins."], "venue": "Computational linguistics, 29(4):589\u2013637.", "citeRegEx": "Collins.,? 2003", "shortCiteRegEx": "Collins.", "year": 2003}, {"title": "Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles", "author": ["James Cross", "Liang Huang."], "venue": "EMNLP.", "citeRegEx": "Cross and Huang.,? 2016", "shortCiteRegEx": "Cross and Huang.", "year": 2016}, {"title": "Deep biaffine attention for neural dependency parsing", "author": ["Timothy Dozat", "Christopher D. Manning."], "venue": "ICLR.", "citeRegEx": "Dozat and Manning.,? 2017", "shortCiteRegEx": "Dozat and Manning.", "year": 2017}, {"title": "Neural crf parsing", "author": ["Greg Durrett", "Dan Klein."], "venue": "ACL.", "citeRegEx": "Durrett and Klein.,? 2015", "shortCiteRegEx": "Durrett and Klein.", "year": 2015}, {"title": "Transitionbased dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "ACL.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith."], "venue": "NAACL.", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Forest reranking: Discriminative parsing with non-local features", "author": ["Liang Huang."], "venue": "ACL, pages 586\u2013 594.", "citeRegEx": "Huang.,? 2008", "shortCiteRegEx": "Huang.", "year": 2008}, {"title": "Pcfg models of linguistic tree representations", "author": ["Mark Johnson."], "venue": "Computational Linguistics, 24(4):613\u2013 632.", "citeRegEx": "Johnson.,? 1998", "shortCiteRegEx": "Johnson.", "year": 1998}, {"title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "Transactions of the Association of Computational Linguistics, 4:313\u2013327.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "What do recurrent neural network grammars learn about syntax", "author": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Graham Neubig", "Noah A. Smith"], "venue": "In EACL,", "citeRegEx": "Kuncoro et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kuncoro et al\\.", "year": 2017}, {"title": "Lstm ccg parsing", "author": ["Mike Lewis", "Kenton Lee", "Luke Zettlemoyer."], "venue": "NAACL, pages 221\u2013231.", "citeRegEx": "Lewis et al\\.,? 2016", "shortCiteRegEx": "Lewis et al\\.", "year": 2016}, {"title": "Shift-Reduce Constituent Parsing with Neural Lookahead Features", "author": ["Jiangming Liu", "Yue Zhang."], "venue": "Transactions of the Association of Computational Linguistics, 5:45\u201358.", "citeRegEx": "Liu and Zhang.,? 2017", "shortCiteRegEx": "Liu and Zhang.", "year": 2017}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Efficient probabilistic top-down and left-corner parsing", "author": ["Brian Roark", "Mark Johnson."], "venue": "ACL, pages 421\u2013428. Association for Computational Linguistics.", "citeRegEx": "Roark and Johnson.,? 1999", "shortCiteRegEx": "Roark and Johnson.", "year": 1999}, {"title": "Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing", "author": ["Brian Roark", "Asaf Bachrach", "Carlos Cardenas", "Christophe Pallier."], "venue": "EMNLP, pages 324\u2013333. Association for Computa-", "citeRegEx": "Roark et al\\.,? 2009", "shortCiteRegEx": "Roark et al\\.", "year": 2009}, {"title": "Robust probabilistic predictive syntactic processing: motivations, models, and applications", "author": ["Brian Roark."], "venue": "Ph.D. thesis.", "citeRegEx": "Roark.,? 2001", "shortCiteRegEx": "Roark.", "year": 2001}, {"title": "Deterministic left corner parsing", "author": ["Daniel J. Rosenkrantz", "Philip M. Lewis."], "venue": "IEEE Conference Record of 11th Annual Symposium on Switching and Automata Theory, pages 139\u2013152. IEEE.", "citeRegEx": "Rosenkrantz and Lewis.,? 1970", "shortCiteRegEx": "Rosenkrantz and Lewis.", "year": 1970}, {"title": "A classifier-based parser with linear run-time complexity", "author": ["Kenji Sagae", "Alon Lavie."], "venue": "IWPT, pages 125\u2013132. Association for Computational Linguistics.", "citeRegEx": "Sagae and Lavie.,? 2005", "shortCiteRegEx": "Sagae and Lavie.", "year": 2005}, {"title": "Broad-coverage parsing using human-like memory constraints", "author": ["William Schuler", "Samir AbdelRahman", "Tim Miller", "Lane Schwartz."], "venue": "Computational Linguistics, 36(1):1\u201330.", "citeRegEx": "Schuler et al\\.,? 2010", "shortCiteRegEx": "Schuler et al\\.", "year": 2010}, {"title": "Bayesian symbol-refined tree substitution grammars for syntactic parsing", "author": ["Hiroyuki Shindo", "Yusuke Miyao", "Akinori Fujino", "Masaaki Nagata."], "venue": "ACL, pages 440\u2013448. Association for Computational Linguistics.", "citeRegEx": "Shindo et al\\.,? 2012", "shortCiteRegEx": "Shindo et al\\.", "year": 2012}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "ACL, pages 455\u2013465.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "The syntactic process, volume 24", "author": ["Mark Steedman."], "venue": "MIT Press.", "citeRegEx": "Steedman.,? 2000", "shortCiteRegEx": "Steedman.", "year": 2000}, {"title": "An analysis of frequency-and memory-based processing costs", "author": ["Marten Van Schijndel", "William Schuler."], "venue": "HLT-NAACL, pages 95\u2013105.", "citeRegEx": "Schijndel and Schuler.,? 2013", "shortCiteRegEx": "Schijndel and Schuler.", "year": 2013}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "ICLR.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Joint POS tagging and transition-based constituent parsing in Chinese with non-local features", "author": ["Zhiguo Wang", "Nianwen Xue."], "venue": "ACL, pages 733\u2013742, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Wang and Xue.,? 2014", "shortCiteRegEx": "Wang and Xue.", "year": 2014}, {"title": "Feature optimization for constituent parsing via neural networks", "author": ["Zhiguo Wang", "Haitao Mi", "Nianwen Xue."], "venue": "ACL-IJCNLP, pages 1138\u20131147, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Transitionbased neural constituent parsing", "author": ["Taro Watanabe", "Eiichiro Sumita."], "venue": "ACL, pages 1169\u2013 1179.", "citeRegEx": "Watanabe and Sumita.,? 2015", "shortCiteRegEx": "Watanabe and Sumita.", "year": 2015}, {"title": "Lstm shift-reduce ccg parsing", "author": ["Wenduan Xu."], "venue": "EMNLP.", "citeRegEx": "Xu.,? 2016", "shortCiteRegEx": "Xu.", "year": 2016}, {"title": "The Penn Chinese treebank: Phrase structure annotation of a large corpus", "author": ["Naiwen Xue", "Fei Xia", "Fu-dong Chiou", "Martha Palmer."], "venue": "Natural Language Engineering, 11(2):207\u2013238.", "citeRegEx": "Xue et al\\.,? 2005", "shortCiteRegEx": "Xue et al\\.", "year": 2005}, {"title": "Transition-based parsing of the chinese treebank using a global discriminative model", "author": ["Yue Zhang", "Stephen Clark."], "venue": "IWPT, pages 162\u2013171. Association for Computational Linguistics.", "citeRegEx": "Zhang and Clark.,? 2009", "shortCiteRegEx": "Zhang and Clark.", "year": 2009}, {"title": "Fast and accurate shift-reduce constituent parsing", "author": ["Muhua Zhu", "Yue Zhang", "Wenliang Chen", "Min Zhang", "Jingbo Zhu."], "venue": "ACL, pages 434\u2013443.", "citeRegEx": "Zhu et al\\.,? 2013", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 24, "context": "There are two popular transition-based constituent parsing systems, namely bottom-up parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015) and top-down parsing (Dyer et al.", "startOffset": 93, "endOffset": 184}, {"referenceID": 36, "context": "There are two popular transition-based constituent parsing systems, namely bottom-up parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015) and top-down parsing (Dyer et al.", "startOffset": 93, "endOffset": 184}, {"referenceID": 37, "context": "There are two popular transition-based constituent parsing systems, namely bottom-up parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015) and top-down parsing (Dyer et al.", "startOffset": 93, "endOffset": 184}, {"referenceID": 33, "context": "There are two popular transition-based constituent parsing systems, namely bottom-up parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015) and top-down parsing (Dyer et al.", "startOffset": 93, "endOffset": 184}, {"referenceID": 12, "context": ", 2013; Watanabe and Sumita, 2015) and top-down parsing (Dyer et al., 2016; Kuncoro et al., 2017).", "startOffset": 56, "endOffset": 97}, {"referenceID": 16, "context": ", 2013; Watanabe and Sumita, 2015) and top-down parsing (Dyer et al., 2016; Kuncoro et al., 2017).", "startOffset": 56, "endOffset": 97}, {"referenceID": 37, "context": "When making local decisions, rich information is available from readily built partial trees (Zhu et al., 2013; Watanabe and Sumita, 2015; Cross and Huang, 2016), which contributes to local disambiguation.", "startOffset": 92, "endOffset": 160}, {"referenceID": 33, "context": "When making local decisions, rich information is available from readily built partial trees (Zhu et al., 2013; Watanabe and Sumita, 2015; Cross and Huang, 2016), which contributes to local disambiguation.", "startOffset": 92, "endOffset": 160}, {"referenceID": 8, "context": "When making local decisions, rich information is available from readily built partial trees (Zhu et al., 2013; Watanabe and Sumita, 2015; Cross and Huang, 2016), which contributes to local disambiguation.", "startOffset": 92, "endOffset": 160}, {"referenceID": 14, "context": "However, there is lack of top-down guidance from lookahead information, which can be useful (Johnson, 1998; Roark and Johnson, 1999; Charniak, 2000; Liu and Zhang, 2017).", "startOffset": 92, "endOffset": 169}, {"referenceID": 20, "context": "However, there is lack of top-down guidance from lookahead information, which can be useful (Johnson, 1998; Roark and Johnson, 1999; Charniak, 2000; Liu and Zhang, 2017).", "startOffset": 92, "endOffset": 169}, {"referenceID": 3, "context": "However, there is lack of top-down guidance from lookahead information, which can be useful (Johnson, 1998; Roark and Johnson, 1999; Charniak, 2000; Liu and Zhang, 2017).", "startOffset": 92, "endOffset": 169}, {"referenceID": 18, "context": "However, there is lack of top-down guidance from lookahead information, which can be useful (Johnson, 1998; Roark and Johnson, 1999; Charniak, 2000; Liu and Zhang, 2017).", "startOffset": 92, "endOffset": 169}, {"referenceID": 24, "context": "In addition, binarization must be applied to trees, as shown in Figure 1(b), to ensure a constant number of actions (Sagae and Lavie, 2005), and to take advantage of lexical head information (Collins, 2003).", "startOffset": 116, "endOffset": 139}, {"referenceID": 7, "context": "In addition, binarization must be applied to trees, as shown in Figure 1(b), to ensure a constant number of actions (Sagae and Lavie, 2005), and to take advantage of lexical head information (Collins, 2003).", "startOffset": 191, "endOffset": 206}, {"referenceID": 30, "context": "Thanks to the use of recurrent neural networks, which makes it possible to represent a sentence globally before syntactic tree construction, seminal work of neural top-down parsing directly generates bracketed constituent trees using sequence-to-sequence models (Vinyals et al., 2015).", "startOffset": 262, "endOffset": 284}, {"referenceID": 11, "context": "Dyer et al. (2016) design set of top-down transition actions for standard stack buffer action node [] [The little .", "startOffset": 0, "endOffset": 19}, {"referenceID": 21, "context": "Furthermore, in-order traversal is psycholinguistically motivated (Roark et al., 2009; Steedman, 2000).", "startOffset": 66, "endOffset": 102}, {"referenceID": 28, "context": "Furthermore, in-order traversal is psycholinguistically motivated (Roark et al., 2009; Steedman, 2000).", "startOffset": 66, "endOffset": 102}, {"referenceID": 6, "context": "6 F1 with supervised reranking (Choe and Charniak, 2016) and a 94.", "startOffset": 31, "endOffset": 56}, {"referenceID": 9, "context": "a top-down system and a bottomup system) under the same neural transition-based framework of Dyer et al. (2016). Our final models outperform both of the bottom-up and top-down transition-based constituent parsing by achieving a 91.", "startOffset": 93, "endOffset": 112}, {"referenceID": 24, "context": "We take the bottom-up system of Sagae and Lavie (2005) as our bottom-up baseline.", "startOffset": 32, "endOffset": 55}, {"referenceID": 11, "context": "We take the top-down system of Dyer et al. (2016) as our top-down baseline.", "startOffset": 31, "endOffset": 50}, {"referenceID": 11, "context": "We employ the stack-LSTM parsing model of Dyer et al. (2016) for the three types of transition-based parsing systems in Section 2.", "startOffset": 42, "endOffset": 61}, {"referenceID": 11, "context": "We follow Dyer et al. (2015), representing each word using three different types of embeddings, including pretrained word embedding, ewi , which is not fine-tuned during training of the parser, randomly initialized embeddings ewi , which is finetuned, and the randomly initialized part-of-speech embeddings, which is fine-tuned.", "startOffset": 10, "endOffset": 29}, {"referenceID": 11, "context": "For top-down parsing and in-order parsing, following Dyer et al. (2016), as shown in Figure 5(a), the composition representation scomp is computed as:", "startOffset": 53, "endOffset": 72}, {"referenceID": 19, "context": "For English data, we use the standard benchmark of WSJ sections in PTB (Marcus et al., 1993), where the sections 2-21 are taken for training data, section 22 for development data and section 23 for test for both dependency parsing and constituency parsing.", "startOffset": 71, "endOffset": 92}, {"referenceID": 35, "context": "1 of the Penn Chinese Treebank (CTB) (Xue et al., 2005).", "startOffset": 37, "endOffset": 55}, {"referenceID": 9, "context": "We adopt the pretrained Chinese word embeddings generated on the complete Chinese Gigaword corpus, The POS tags in both the English data and the Chinese data are automatically assigned as the same as the work of Dyer et al. (2016), using Stanford tagger.", "startOffset": 212, "endOffset": 231}, {"referenceID": 3, "context": "We follow the work of Choe and Charniak (2016) and adopt the AFP portion of English Gigaword as the extra resources for the semi-supervised reranking.", "startOffset": 31, "endOffset": 47}, {"referenceID": 9, "context": "Hyper-parameters For both English and Chinese experiments, we use the same hyper-parameters as the work of Dyer et al. (2016) without further optimization, as shown in Table 1.", "startOffset": 107, "endOffset": 126}, {"referenceID": 9, "context": "Hyper-parameters For both English and Chinese experiments, we use the same hyper-parameters as the work of Dyer et al. (2016) without further optimization, as shown in Table 1. Reranking experiments Following the same reranking setting of Dyer et al. (2016) and Choe and Charniak (2016), we obtain 100 samples from our bottom-up, top-down, and in-order model (sec-", "startOffset": 107, "endOffset": 258}, {"referenceID": 3, "context": "(2016) and Choe and Charniak (2016), we obtain 100 samples from our bottom-up, top-down, and in-order model (sec-", "startOffset": 20, "endOffset": 36}, {"referenceID": 3, "context": "We adopt the reranker of Choe and Charniak (2016) as both our English fullysupervised reranker and semi-supervised reranker, and the generative reranker of Dyer et al.", "startOffset": 34, "endOffset": 50}, {"referenceID": 3, "context": "We adopt the reranker of Choe and Charniak (2016) as both our English fullysupervised reranker and semi-supervised reranker, and the generative reranker of Dyer et al. (2016) as our Chinese supervised reranker.", "startOffset": 34, "endOffset": 175}, {"referenceID": 26, "context": "With the fully-supervision setting5, the inorder parser outperforms the state-of-the-art discrete parser (Shindo et al., 2012; Zhu et al., 2013), the state-of-the-art neural parsers (Cross and Huang,", "startOffset": 105, "endOffset": 144}, {"referenceID": 37, "context": "With the fully-supervision setting5, the inorder parser outperforms the state-of-the-art discrete parser (Shindo et al., 2012; Zhu et al., 2013), the state-of-the-art neural parsers (Cross and Huang,", "startOffset": 105, "endOffset": 144}, {"referenceID": 15, "context": "Here, we only consider the work of single model Model F1 fully-supervision Socher et al. (2013) 90.", "startOffset": 75, "endOffset": 96}, {"referenceID": 15, "context": "Here, we only consider the work of single model Model F1 fully-supervision Socher et al. (2013) 90.4 Zhu et al. (2013) 90.", "startOffset": 75, "endOffset": 119}, {"referenceID": 15, "context": "Here, we only consider the work of single model Model F1 fully-supervision Socher et al. (2013) 90.4 Zhu et al. (2013) 90.4 Vinyals et al. (2015) 90.", "startOffset": 75, "endOffset": 146}, {"referenceID": 15, "context": "Here, we only consider the work of single model Model F1 fully-supervision Socher et al. (2013) 90.4 Zhu et al. (2013) 90.4 Vinyals et al. (2015) 90.7 Watanabe and Sumita (2015) 90.", "startOffset": 75, "endOffset": 178}, {"referenceID": 15, "context": "7 Shindo et al. (2012) 91.", "startOffset": 2, "endOffset": 23}, {"referenceID": 6, "context": "1 Durrett and Klein (2015) 91.", "startOffset": 2, "endOffset": 27}, {"referenceID": 6, "context": "1 Durrett and Klein (2015) 91.1 Dyer et al. (2016) 91.", "startOffset": 2, "endOffset": 51}, {"referenceID": 5, "context": "2 Cross and Huang (2016) 91.", "startOffset": 2, "endOffset": 25}, {"referenceID": 5, "context": "2 Cross and Huang (2016) 91.3 Liu and Zhang (2017) 91.", "startOffset": 2, "endOffset": 51}, {"referenceID": 5, "context": "2 Cross and Huang (2016) 91.3 Liu and Zhang (2017) 91.7 Top-down parser 91.2 Bottom-up parser 91.3 In-order parser 91.8 reranking Huang (2008) 91.", "startOffset": 2, "endOffset": 143}, {"referenceID": 2, "context": "7 Charniak and Johnson (2005) 91.", "startOffset": 2, "endOffset": 30}, {"referenceID": 2, "context": "7 Charniak and Johnson (2005) 91.5 Choe and Charniak (2016) 92.", "startOffset": 2, "endOffset": 60}, {"referenceID": 2, "context": "7 Charniak and Johnson (2005) 91.5 Choe and Charniak (2016) 92.6 Dyer et al. (2016) 93.", "startOffset": 2, "endOffset": 84}, {"referenceID": 2, "context": "7 Charniak and Johnson (2005) 91.5 Choe and Charniak (2016) 92.6 Dyer et al. (2016) 93.3 Kuncoro et al. (2017) 93.", "startOffset": 2, "endOffset": 111}, {"referenceID": 2, "context": "7 Charniak and Johnson (2005) 91.5 Choe and Charniak (2016) 92.6 Dyer et al. (2016) 93.3 Kuncoro et al. (2017) 93.6 Top-down parser 93.3 Bottom-up parser 93.3 In-order parser 93.6 semi-supervised reranking Choe and Charniak (2016) 93.", "startOffset": 2, "endOffset": 231}, {"referenceID": 10, "context": "2016; Watanabe and Sumita, 2015) and the stateof-the-art hybrid parsers (Durrett and Klein, 2015; Liu and Zhang, 2017), achieving the state-of-theart results.", "startOffset": 72, "endOffset": 118}, {"referenceID": 18, "context": "2016; Watanabe and Sumita, 2015) and the stateof-the-art hybrid parsers (Durrett and Klein, 2015; Liu and Zhang, 2017), achieving the state-of-theart results.", "startOffset": 72, "endOffset": 118}, {"referenceID": 13, "context": "With the reranking setting, the in-order parser outperforms the best discrete parser (Huang, 2008) and have the same performance of Kuncoro et al.", "startOffset": 85, "endOffset": 98}, {"referenceID": 6, "context": "With the semi-supervised setting, the in-order parser outperforms the best semi-supervised parser (Choe and Charniak, 2016) by achieving 94.", "startOffset": 98, "endOffset": 123}, {"referenceID": 6, "context": "English dependency results As shown in Table 5, by converting to Stanford Dependencies, without additional training data, our models achieves similar performance with the state-of-the-art system (Choe and Charniak, 2016); with the same additional training data, our models achieves new state-of-the-art results on dependency parsing by achieving 96.", "startOffset": 195, "endOffset": 220}, {"referenceID": 8, "context": "2016; Watanabe and Sumita, 2015) and the stateof-the-art hybrid parsers (Durrett and Klein, 2015; Liu and Zhang, 2017), achieving the state-of-theart results. With the reranking setting, the in-order parser outperforms the best discrete parser (Huang, 2008) and have the same performance of Kuncoro et al. (2017), which extend the work of Dyer et al.", "startOffset": 73, "endOffset": 313}, {"referenceID": 8, "context": "2016; Watanabe and Sumita, 2015) and the stateof-the-art hybrid parsers (Durrett and Klein, 2015; Liu and Zhang, 2017), achieving the state-of-theart results. With the reranking setting, the in-order parser outperforms the best discrete parser (Huang, 2008) and have the same performance of Kuncoro et al. (2017), which extend the work of Dyer et al. (2016) by adding gated attention mechanism on composition functions.", "startOffset": 73, "endOffset": 358}, {"referenceID": 3, "context": "9 Cheng et al. (2016) \u2020 94.", "startOffset": 2, "endOffset": 22}, {"referenceID": 0, "context": "5 Andor et al. (2016) 94.", "startOffset": 2, "endOffset": 22}, {"referenceID": 0, "context": "5 Andor et al. (2016) 94.6 92.8 Dyer et al. (2016) -re 95.", "startOffset": 2, "endOffset": 51}, {"referenceID": 0, "context": "5 Andor et al. (2016) 94.6 92.8 Dyer et al. (2016) -re 95.6 94.4 Dozat and Manning (2017)\u2020 95.", "startOffset": 2, "endOffset": 90}, {"referenceID": 0, "context": "5 Andor et al. (2016) 94.6 92.8 Dyer et al. (2016) -re 95.6 94.4 Dozat and Manning (2017)\u2020 95.7 94.0 Kuncoro et al. (2017) -re 95.", "startOffset": 2, "endOffset": 123}, {"referenceID": 0, "context": "5 Andor et al. (2016) 94.6 92.8 Dyer et al. (2016) -re 95.6 94.4 Dozat and Manning (2017)\u2020 95.7 94.0 Kuncoro et al. (2017) -re 95.7 94.5 Choe and Charniak (2016) -sre 95.", "startOffset": 2, "endOffset": 162}, {"referenceID": 28, "context": "Parser F1 fully-supervision Zhu et al. (2013) 83.", "startOffset": 28, "endOffset": 46}, {"referenceID": 25, "context": "2 Wang et al. (2015) 83.", "startOffset": 2, "endOffset": 21}, {"referenceID": 9, "context": "2 Dyer et al. (2016) 84.", "startOffset": 2, "endOffset": 21}, {"referenceID": 9, "context": "2 Dyer et al. (2016) 84.6 Liu and Zhang (2017) 85.", "startOffset": 2, "endOffset": 47}, {"referenceID": 2, "context": "1 rerank Charniak and Johnson (2005) 82.", "startOffset": 9, "endOffset": 37}, {"referenceID": 2, "context": "1 rerank Charniak and Johnson (2005) 82.3 Dyer et al. (2016) 86.", "startOffset": 9, "endOffset": 61}, {"referenceID": 2, "context": "1 rerank Charniak and Johnson (2005) 82.3 Dyer et al. (2016) 86.9 Top-down parser 86.9 Bottom-up parser 87.5 In-order parser 88.0 semi-supervision Zhu et al. (2013) 85.", "startOffset": 9, "endOffset": 165}, {"referenceID": 2, "context": "1 rerank Charniak and Johnson (2005) 82.3 Dyer et al. (2016) 86.9 Top-down parser 86.9 Bottom-up parser 87.5 In-order parser 88.0 semi-supervision Zhu et al. (2013) 85.6 Wang and Xue (2014) 86.", "startOffset": 9, "endOffset": 190}, {"referenceID": 2, "context": "1 rerank Charniak and Johnson (2005) 82.3 Dyer et al. (2016) 86.9 Top-down parser 86.9 Bottom-up parser 87.5 In-order parser 88.0 semi-supervision Zhu et al. (2013) 85.6 Wang and Xue (2014) 86.3 Wang et al. (2015) 86.", "startOffset": 9, "endOffset": 214}, {"referenceID": 1, "context": "0 Ballesteros et al. (2016) 87.", "startOffset": 2, "endOffset": 28}, {"referenceID": 1, "context": "0 Ballesteros et al. (2016) 87.7 86.2 Kiperwasser and Goldberg (2016) 87.", "startOffset": 2, "endOffset": 70}, {"referenceID": 1, "context": "0 Ballesteros et al. (2016) 87.7 86.2 Kiperwasser and Goldberg (2016) 87.6 86.1 Cheng et al. (2016) \u2020 88.", "startOffset": 2, "endOffset": 100}, {"referenceID": 1, "context": "0 Ballesteros et al. (2016) 87.7 86.2 Kiperwasser and Goldberg (2016) 87.6 86.1 Cheng et al. (2016) \u2020 88.1 85.7 Dozat and Manning (2017) \u2020 89.", "startOffset": 2, "endOffset": 137}, {"referenceID": 22, "context": "Typical works investigate the transformation of syntactic trees based on left-corner rules (Roark, 2001; Schuler et al., 2010; Van Schijndel and Schuler, 2013).", "startOffset": 91, "endOffset": 159}, {"referenceID": 25, "context": "Typical works investigate the transformation of syntactic trees based on left-corner rules (Roark, 2001; Schuler et al., 2010; Van Schijndel and Schuler, 2013).", "startOffset": 91, "endOffset": 159}, {"referenceID": 9, "context": "Neural networks have achieved the state-of-theart for parsing under various grammar formalisms, including dependency (Dozat and Manning, 2017) constituent (Dyer et al.", "startOffset": 117, "endOffset": 142}, {"referenceID": 12, "context": "Neural networks have achieved the state-of-theart for parsing under various grammar formalisms, including dependency (Dozat and Manning, 2017) constituent (Dyer et al., 2016; Kuncoro et al., 2017) and CCG parsing (Xu, 2016; Lewis et al.", "startOffset": 155, "endOffset": 196}, {"referenceID": 16, "context": "Neural networks have achieved the state-of-theart for parsing under various grammar formalisms, including dependency (Dozat and Manning, 2017) constituent (Dyer et al., 2016; Kuncoro et al., 2017) and CCG parsing (Xu, 2016; Lewis et al.", "startOffset": 155, "endOffset": 196}, {"referenceID": 34, "context": ", 2017) and CCG parsing (Xu, 2016; Lewis et al., 2016).", "startOffset": 24, "endOffset": 54}, {"referenceID": 17, "context": ", 2017) and CCG parsing (Xu, 2016; Lewis et al., 2016).", "startOffset": 24, "endOffset": 54}, {"referenceID": 4, "context": "Seminal work employs transition-based methods (Chen and Manning, 2014).", "startOffset": 46, "endOffset": 70}, {"referenceID": 33, "context": "This method has been extended by investigating more complex representations of configurations for constituent parsing (Watanabe and Sumita, 2015; Dyer et al., 2016).", "startOffset": 118, "endOffset": 164}, {"referenceID": 12, "context": "This method has been extended by investigating more complex representations of configurations for constituent parsing (Watanabe and Sumita, 2015; Dyer et al., 2016).", "startOffset": 118, "endOffset": 164}, {"referenceID": 14, "context": "Rosenkrantz and Lewis (1970) formalize this in automata theory, which have appeared frequently in the compiler literature.", "startOffset": 0, "endOffset": 29}, {"referenceID": 10, "context": "Roark and Johnson (1999) apply the strategy into parsing.", "startOffset": 10, "endOffset": 25}, {"referenceID": 4, "context": "Seminal work employs transition-based methods (Chen and Manning, 2014). This method has been extended by investigating more complex representations of configurations for constituent parsing (Watanabe and Sumita, 2015; Dyer et al., 2016). Dyer et al. (2016) employ stack-LSTM onto topdown system, which is the same as our top-down parser.", "startOffset": 47, "endOffset": 257}, {"referenceID": 4, "context": "Seminal work employs transition-based methods (Chen and Manning, 2014). This method has been extended by investigating more complex representations of configurations for constituent parsing (Watanabe and Sumita, 2015; Dyer et al., 2016). Dyer et al. (2016) employ stack-LSTM onto topdown system, which is the same as our top-down parser. Watanabe and Sumita (2015) employ treeLSTM to model the complex representation in stack in bottom-up system.", "startOffset": 47, "endOffset": 365}], "year": 2017, "abstractText": "Both bottom-up and top-down strategies have been used for neural transition-based constituent parsing. The parsing strategies differ in terms of the order in which they recognize productions in the derivation tree, where bottom-up strategies and top-down strategies take post-order and pre-order traversal over trees, respectively. Bottom-up parsers benefit from rich features from readily built partial parses, but lack lookahead guidance in the parsing process; top-down parsers benefit from non-local guidance for local decisions, but rely on a strong encoder over the input to predict a constituent hierarchy before its construction. To mitigate both issues, we propose a novel parsing system based on in-order traversal over syntactic trees, designing a set of transition actions to find a compromise between bottom-up constituent information and top-down lookahead information. Based on stack-LSTM, our psycholinguistically motivated constituent parsing system achieves 91.8 F1 on WSJ benchmark. Furthermore, the system achieves 93.6 F1 with supervised reranking and 94.2 F1 with semi-supervised reranking, which are the best results on the WSJ benchmark.", "creator": "LaTeX with hyperref package"}}}