{"id": "1707.05114", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jul-2017", "title": "Towards Bidirectional Hierarchical Representations for Attention-based Neural Machine Translation", "abstract": "This paper proposes a hierarchical attentional neural translation model which focuses on enhancing source-side hierarchical representations by covering both local and global semantic information using a bidirectional tree-based encoder. To maximize the predictive likelihood of target words, a weighted variant of an attention mechanism is used to balance the attentive information between lexical and phrase vectors. Using a tree-based rare word encoding, the proposed model is extended to sub-word level to alleviate the out-of-vocabulary (OOV) problem. Empirical results reveal that the proposed model significantly outperforms sequence-to-sequence attention-based and tree-based neural translation models in English-Chinese translation tasks.", "histories": [["v1", "Mon, 17 Jul 2017 12:09:08 GMT  (205kb)", "http://arxiv.org/abs/1707.05114v1", "Accepted for publication at EMNLP 2017"]], "COMMENTS": "Accepted for publication at EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["baosong yang", "derek f wong", "tong xiao", "lidia s chao", "jingbo zhu"], "accepted": true, "id": "1707.05114"}, "pdf": {"name": "1707.05114.pdf", "metadata": {"source": "CRF", "title": "Towards Bidirectional Hierarchical Representations for Attention-Based Neural Machine Translation", "authors": ["Baosong Yang", "Derek F. Wong", "Tong Xiao", "Lidia S. Chao", "Jingbo Zhu"], "emails": ["nlp2ct.baosong@gmail.com,", "derekfw@umac.mo,", "lidiasc@umac.mo,", "xiaotong@mail.neu.edu.cn", "zhujingbo@mail.neu.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 170 7.05 114v 1 [cs.C L] 17 July 2 017This paper proposes a hierarchical, attention-oriented neural translation model that focuses on improving source-side hierarchical representations by covering both local and global semantic information using a bidirectional tree-based encoder. To maximize the predictability of target words, a weighted variant of an attention mechanism is used to balance attentive information between lexical and phrase vectors. Using tree-based coding of rare words, the proposed model is extended to subword level to alleviate the problem of non-vocabulary (OV). Empirical results show that the proposed model significantly exceeds sequence-to-sequence attention-based and tree-based neural translation models in English-Chinese translation tasks."}, {"heading": "1 Introduction", "text": "The results of the various translation tasks (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). The most common model is the encoder decoder frame (Bahdanau et al., 2015), in which the source set is encoded into a dense representation, followed by a decoding process that generates the target translation. By exploiting the attention mechanism (Bahdanau et al., 2015), the generation of target words is tied to the hidden states rather than to the authority of the context vector alone. From a model architecture perspective, previous studies of the attentive encoder decoder translation models are mainly divided into two types."}, {"heading": "2 Tree-Based Neural Machine Translation", "text": "A Neural Machine Translation System (NMT) aims to use a single neural network to build a translation model trained to maximize conditional distribution of sentence pairs using a parallel training corpus (Kalchburner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b, a). By incorporating syntactical information, the tree-based NMT uses an additional syntactical structure of the source sentence to improve translation. Since most existing NMTs each generate a target word, the conditional probability of a target sentence y = (y1,..., yM) is formally expressed as: p (y | x, tr) = M-1p (yj | y1,..., yj \u2212 1, x, tr; \u03b8), with the corresponding sentence representing the parameters."}, {"heading": "2.1 Tree-Based Encoder", "text": "In a tree-based encoder, the source language x is given a given syntactical structure k = i k = i k = i k = i k = i k = i k = i k = i k = i k = i k = i k = b b b = i (r). As shown in Figure 2, Eriguchi et al. (2016), a forward-facing long-short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al. (Gers et al., 2000) was used to encode recurring neuronal units (RNN) instead of activating lexical nodes and a treeLSTM (Tai et al., 2015) to generate phrase representations in bottom-up manners. In the present study, we use the gated recurrent unit (GNN) (Cho et al., 2014b) instead of a LM (Tai et al., 2015) to generate the phrase representations in bottom-up manners."}, {"heading": "2.2 Decoding with a Tree-Based Attention Mechanism", "text": "When generating the target words, we use a sequential decoder with an input-feeding method (Luong et al., 2015) and an attention mechanism (Bahdanau et al., 2015).The conditional probability of the j-th target word yj is calculated using a nonlinear function fsoftmax: p (yj | y1,..., yj \u2212 1, x, tr; \u03b8) = fsoftmax (cj), where cj is the compound hidden state consisting of a hidden state sj and a context vector dj: cj = ftanh ([sj, dj]).Given the previous target word yj \u2212 1, the concatenation of the previous hidden state sj \u2212 1 and the previous context vector cj \u2212 1 (input-feeding) (Luong et al., 2015), sj \u2212 h, is calculated."}, {"heading": "3 The Bidirectional Hierarchical Model", "text": "Although the tree-based encoder from Eriguchi et al. (2016) has shown some advantages in translation tasks involving remote language pairs, such as English-Japanese, the representation of a phrase is based exclusively on its child node, and the leaf-level representation only takes into account sequential information. We argue that the inclusion of more hierarchical information in representations can help improve translation, and the use of global information in particular can help distinguish the differences between word meanings. Based on this hypothesis, we propose an alternative architecture, the bidirectional hierarchical model, to improve source-side representation."}, {"heading": "3.1 Bidirectional Leaf-Node Encoding", "text": "As discussed in section 1, the unidirectional recursive neural network reads an input sequence in order from the first symbol to the last. To generate annotation vectors for leaf nodes that take into account both preceding and subsequent annotations, we use a bidirectional RNN encoder (Bahdanau et al., 2015). The hidden state of the i-leaf node hli is the concatenation of forward and backward vectors: hli = [\u2212 \u2192 h li, \u2190 \u2212 h li], where \u2212 \u2192 h li is achieved by a right-directional GRU as shown in equation 1, and a left-directional GRU calculates \u2190 \u2212 h li = f \u2190 GRU (xi, \u2190 \u2212 h li \u2212 1), where \u2190 \u2212 h li \u2212 1 is the previous hidden state."}, {"heading": "3.2 Bidirectional Tree-Node Encoding", "text": "I'm not sure what I'm going to do with it, I'm not going to do it, I'm not going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it."}, {"heading": "3.3 Handling Out-of-Vocabulary: Tree-Based Rare Word Encoding", "text": "In NMT, the translation of rare words and unknown words is an open problem, as the cost of calculation increases with the size of the vocabulary. Sennrich et al. (2016) proposed a simple and effective approach to handling them outside of the vocabulary, by presenting rare words as a sequence of subwords segmented by byte-pair encoding (Gage, 1994). We propose a variant tree-based approach to encoding rare words that extends the tree-based model to the level of the subordinates. Subordinate units are encoded according to an additional binary lexical tree. For a sentence x = (x1,..., xi,..., xN), BPE segments the word xi into a sequence of subordinate units (x1i,..., x n). The binary lexical tree is simply represented by the composition of two nodes in the right way (((x1i, 2, 3,...), i), 1 (i), x), and 1 (i) in the figure."}, {"heading": "3.4 Decoder with Weighted Variant of Attention Mechanism", "text": "Since each representation contains both local and global information, the observance of lexical and phrase representations in each decoding step can cause the problem of translation (by repeatedly picking up and translating the same component of a sentence).An alternative approach is to balance the attentive information between the lexical and phrase vectors in the context vector. To make effective use of these hierarchical representations, we propose a weighted variant of the tree-based attention mechanism (the original is defined in Equation 2).Formally, the calculation of the context vector dj in step j is modified in step j as: dj = (1 \u2212 \u03b2j) n \u00b2 i = 1\u03b1j (i) h + \u03b2jn \u2212 1 \u00b2 k = 1\u03b1j (k) h p (4), whereby \u03b2j \u00b2 [0, 1] in step j is weighted the expected meaning of the representations."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data", "text": "We evaluate the proposed model using an English-Chinese translation task. For computational efficiency, we extracted 1.4M pairs of sentences, with the maximum length of the sentence being 40, from the LDC parallel corpus3 as our training data. The models were developed using NIST-mt08 data and examined using NIST-mt04, mt05, and mt06 data. The number of sentences in each record is shown in Table 1. (2016) On the Chinese side, the sentences are segmented using the Chinese word segmentation tool from NiuTrans (Zeng et al., 2014, 2015) to generate a binary syntactic tree for each sentence, as opposed to using the HPSG parser from Eriguchi et al. (2016). To avoid data economy, words referring to time, date, and number are primary compressed using the Chinese word segmentation tool from Nius Tranguchi et al."}, {"heading": "4.2 Experimental Settings", "text": "As shown in Table 2, which contains the statistics of the token types, we limit the size of the source and target vocabulary to 40,000 to cover all English and Chinese tokens; the dimensions of the word embedding and the hidden level are set to 620 and 1,000, respectively; due to the concatenation in the bi-directional leaf-node encoding, the dimensions of the forward and backward vectors, which are half as large as those of the other hidden states, are set to 500. To prevent overadjustment, the training data is reshuffled after each epoch. Furthermore, the model parameters are optimized using AdaDelta (Zeiler, 2012), as it is able to dynamically adjust the learning rate. To allow a fair comparison, we set the minibatch size to 16 and the beam search size to 5. The accuracy of the translation in relation to a reference is assessed using the BLEU metrics (Papineni et al., 2002)."}, {"heading": "4.3 Enhanced Hierarchical Representations", "text": "First, the effectiveness of the extended hierarchical representations is evaluated through a series of experiments, the results of which are summarized in Table 3.3. Compared to the original tree-based encoder (Eriguchi et al., 2016), the bi-directional leaf-node encoding model (described in Section 3.1) performs better. This also shows that the future context at the leaf level can contribute to word prediction. Second, although the representations of leaf nodes are learned in a sequential, context-sensitive manner, the translation quality is further improved by taking global semantic information into account in the top-down encoding (Section 3.2). 4 https: / / github.com / nyu-dl / dl4mt-tutorialBy incorporating the above improvements into the model, the proposed semantic encoding is significantly improved in both the sequential and sequential sequential encoding model."}, {"heading": "4.4 Weighted Attention Model", "text": "As discussed in Section 3.4, in order to effectively use hierarchical representations in generating the target word, although \u03b2 is used only slightly, we adopt a variant-weighted, tree-based attention mechanism that incorporates a scalar to control the ratio of conditional information between word and phrase vectors. Thus, by manually or automatically changing the weight \u03b2, the use of the weighted attention model is evaluated for four cases: \u2022 \u03b2 = 0.0: We set the weight of phrase vectors manually to 0.0; in other words, the decoder is forced to ignore the phrase vectors. The final translation is therefore generated by mere summary of the leaf vectors. \u2022 \u03b2 = 0.5: The representations of phrase vectors without leaf nodes and leaf nodes participate equally in the translation process. The decoder of this case therefore uses the same attention mechanism as the original model (Section 2.2)."}, {"heading": "5 Qualitative Analysis", "text": "As a matter of fact, most of them are able to survive themselves if they do not see themselves able to survive themselves. Most of them are able to survive themselves if they are not able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves if they are not able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves if they are not able to survive themselves. Most of them are not able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves if they are not able to survive themselves. Most of them are able to survive themselves if they are not able to survive themselves."}, {"heading": "6 Conclusion", "text": "In this paper, we propose an improved NMT system with a novel bidirectional hierarchical encoder that improves the source-side representation of a sentence, both phrases and words, with local and global context information. By introducing tree-based rare word encoding, the hierarchical model is extended to the subword level to alleviate the problem of OOVs. To make effective use of the extended hierarchical representations, we also propose a weighted variant of the attention model that dynamically adjusts the relationship of conditional information between the lexical and phrase comment vectors. Experimental results for NIST translation tasks in English and Chinese show that the proposed model significantly exceeds the vanilla tree-based and sequential NMT models."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the National Natural Science Foundation of China (grant number 61672555), a multi-year research grant from the University of Macau (grant number MYRG2017-00087-FST, MYRG201500175-FST and MYRG2015-00188-FST) and the Macao Science and Technological Development Fund (grant number 057 / 2014 / A), and the work of Tong Xiao and Jingbo Zhu was partially supported by the National Natural Science Foundation of China (grant numbers 61672138 and 61432013), the Fundamental Research Funds for the Central Universities, and the Opening Project of Beijing Key Laboratory of Internet Culture and Digital Dissemination Research."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "KyunghyunCho", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Doubly-Attentive Decoder for Multi-modal Neural Machine Translation", "author": ["Iacer Calixto", "Qun Liu", "Nick Campbell."], "venue": "CoRR, abs/1702.01287.", "citeRegEx": "Calixto et al\\.,? 2017", "shortCiteRegEx": "Calixto et al\\.", "year": 2017}, {"title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Pro-", "citeRegEx": "Cho et al\\.,? 2014b", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "Advances in neural information pro-", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Tree-to-SequenceAttentional Neural Machine Translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Proceedings of the 54th", "citeRegEx": "Eriguchi et al\\.,? 2016", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "A New Algorithm for Data Compression", "author": ["Philip Gage."], "venue": "The C Users Journal, 12(2):23\u201338.", "citeRegEx": "Gage.,? 1994", "shortCiteRegEx": "Gage.", "year": 1994}, {"title": "Learning to Forget: Continual Prediction with LSTM", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins."], "venue": "Neural Computation, 12(10):2451\u20132471.", "citeRegEx": "Gers et al\\.,? 2000", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "An Empirical Exploration of Recurrent Network Architectures", "author": ["Rafal J\u00f3zefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings", "citeRegEx": "J\u00f3zefowicz et al\\.,? 2015", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2015}, {"title": "Recurrent Continuous Translation Models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Structural Attention Neural Networks for Improved Sentiment Analysis", "author": ["Filippos Kokkinos", "Alexandros Potamianos."], "venue": "CoRR, abs/1701.01811.", "citeRegEx": "Kokkinos and Potamianos.,? 2017", "shortCiteRegEx": "Kokkinos and Potamianos.", "year": 2017}, {"title": "Effective Approaches to Attentionbased Neural Machine Translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "BLEU: A Method for Automatic Evaluation of Machine Translation", "author": ["Kishore Papineni", "Salim Roukos", "ToddWard", "WeiJing Zhu."], "venue": "Proceedings", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Bidirectional Recurrent Neural Networks", "author": ["Mike Schuster", "Kuldip K Paliwal."], "venue": "IEEE Transactions on Signal Processing, 45(11):2673\u20132681.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Linguistic Input Features Improve Neural Machine Translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "Proceedings of the First Conference on Machine Translation, pages 83\u201390.", "citeRegEx": "Sennrich and Haddow.,? 2016", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2016}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1715\u20131725.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Syntactically Guided Neural Machine Translation", "author": ["Felix Stahlberg", "Eva Hasler", "Aurelien Waite", "Bill Byrne."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 299\u2013305.", "citeRegEx": "Stahlberg et al\\.,? 2016", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2016}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems (NIPS 2014), pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation", "author": ["Tong Xiao", "Jingbo Zhu", "Hao Zhang", "Qiang Li."], "venue": "Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics, Sys-", "citeRegEx": "Xiao et al\\.,? 2012", "shortCiteRegEx": "Xiao et al\\.", "year": 2012}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio."], "venue": "Proceedings of the 32nd International", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D. Zeiler."], "venue": "CoRR, abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Graph-Based Lexicon Regularization for PCFG With Latent Annotations", "author": ["Xiaodong Zeng", "Derek F. Wong", "Lidia S. Chao", "Isabel Trancoso."], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23(3):441\u2013450.", "citeRegEx": "Zeng et al\\.,? 2015", "shortCiteRegEx": "Zeng et al\\.", "year": 2015}, {"title": "Lexicon Expansion for Latent Variable Grammars", "author": ["Xiaodong Zeng", "Derek F. Wong", "Lidia S. Chao", "Isabel Trancoso", "Liangye He", "Qiuping Huang."], "venue": "Pattern Recognition Letters, (42):47\u201355.", "citeRegEx": "Zeng et al\\.,? 2014", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Modelling Sentence Pairs with Tree-structured Attentive Encoder", "author": ["Yao Zhou", "Cong Liu", "Yan Pan."], "venue": "Proceedings of 26th International Conference on Computational Linguistics, pages 2912\u2013 2922.", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": "tion tasks (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 11, "endOffset": 90}, {"referenceID": 18, "context": "tion tasks (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 11, "endOffset": 90}, {"referenceID": 0, "context": "tion tasks (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 11, "endOffset": 90}, {"referenceID": 18, "context": "decoder framework (Sutskever et al., 2014), in which the source sentence is encoded into a dense", "startOffset": 18, "endOffset": 42}, {"referenceID": 0, "context": "By exploiting the attention mechanism (Bahdanau et al., 2015), the generation of target words is conditional on the source hidden states, rather than on", "startOffset": 38, "endOffset": 61}, {"referenceID": 18, "context": "The most fundamental approaches transform the source sentence sequentially into a fixed-length context vector, and the annotation vector of each word summarizes the preceding words (Sutskever et al., 2014; Cho et al., 2014b).", "startOffset": 181, "endOffset": 224}, {"referenceID": 3, "context": "The most fundamental approaches transform the source sentence sequentially into a fixed-length context vector, and the annotation vector of each word summarizes the preceding words (Sutskever et al., 2014; Cho et al., 2014b).", "startOffset": 181, "endOffset": 224}, {"referenceID": 14, "context": "(2015) used a bidirectional recurrent neural network (RNN) (Schuster and Paliwal, 1997) to consider preceding and following words jointly,", "startOffset": 59, "endOffset": 87}, {"referenceID": 0, "context": "Although Bahdanau et al. (2015) used a bidirectional recurrent neural network (RNN) (Schuster and Paliwal, 1997) to consider preceding and following words jointly,", "startOffset": 9, "endOffset": 32}, {"referenceID": 5, "context": "these sequential representations are insufficient to fully capture the semantics of a sentence, due to the fact that they do not account for the syntactic interpretations of sentence structure (Eriguchi et al., 2016; Tai et al., 2015).", "startOffset": 193, "endOffset": 234}, {"referenceID": 19, "context": "these sequential representations are insufficient to fully capture the semantics of a sentence, due to the fact that they do not account for the syntactic interpretations of sentence structure (Eriguchi et al., 2016; Tai et al., 2015).", "startOffset": 193, "endOffset": 234}, {"referenceID": 5, "context": "these sequential representations are insufficient to fully capture the semantics of a sentence, due to the fact that they do not account for the syntactic interpretations of sentence structure (Eriguchi et al., 2016; Tai et al., 2015). By incorporating additional features into a sequential model, Sennrich and Haddow (2016) and Stahlberg et al.", "startOffset": 194, "endOffset": 325}, {"referenceID": 5, "context": "these sequential representations are insufficient to fully capture the semantics of a sentence, due to the fact that they do not account for the syntactic interpretations of sentence structure (Eriguchi et al., 2016; Tai et al., 2015). By incorporating additional features into a sequential model, Sennrich and Haddow (2016) and Stahlberg et al. (2016) suggest that a greater amount of linguistic information can improve the", "startOffset": 194, "endOffset": 353}, {"referenceID": 19, "context": "The existing tree-based encoders (Tai et al., 2015; Eriguchi et al., 2016; Zhou et al., 2016) recursively generate phrase (sentence) representations in a bottom-up fashion, whereby the annotation vector of each phrase is derived from its constituent sub-phrases.", "startOffset": 33, "endOffset": 93}, {"referenceID": 5, "context": "The existing tree-based encoders (Tai et al., 2015; Eriguchi et al., 2016; Zhou et al., 2016) recursively generate phrase (sentence) representations in a bottom-up fashion, whereby the annotation vector of each phrase is derived from its constituent sub-phrases.", "startOffset": 33, "endOffset": 93}, {"referenceID": 25, "context": "The existing tree-based encoders (Tai et al., 2015; Eriguchi et al., 2016; Zhou et al., 2016) recursively generate phrase (sentence) representations in a bottom-up fashion, whereby the annotation vector of each phrase is derived from its constituent sub-phrases.", "startOffset": 33, "endOffset": 93}, {"referenceID": 5, "context": "To address the above problems, we propose a novel architecture, a bidirectional hierarchical encoder, which extends the existing attentive treestructured models (Eriguchi et al., 2016).", "startOffset": 161, "endOffset": 184}, {"referenceID": 14, "context": "(2016), we first use a bidirectional RNN (Schuster and Paliwal, 1997) at lexical level to concatenate the forward and backward states as the hidden states of source words, to capture the preceding and following contexts (described in Section 3.", "startOffset": 41, "endOffset": 69}, {"referenceID": 5, "context": ", 2015; Eriguchi et al., 2016; Zhou et al., 2016) recursively generate phrase (sentence) representations in a bottom-up fashion, whereby the annotation vector of each phrase is derived from its constituent sub-phrases. As a result, the learned representations are limited to local information, while failing to capture the global meaning of a sentence. As illustrated in Figure 1, the phrases \u201ctake up\u201d and \u201ca position\u201d have different meanings in different contexts. However, in composing the representations hVP3 and hNP7 for phrases VP3 and NP7, the current approaches do not account for the differences in meaning which arise as a result of ignoring the neighboring context as well as the remote context, i.e. hNP7 \u2190 hPP8 (sibling) and hVP3 \u2190 hNP7 (child of sibling). More specifically, at the encoding step t, the generated phrase is based on the results at the previous time steps ht\u22121 and ht\u22122, but has no information about the parent phrases ht\u2032 for t \u2032 > t. To address the above problems, we propose a novel architecture, a bidirectional hierarchical encoder, which extends the existing attentive treestructured models (Eriguchi et al., 2016). In contrast to the model of Eriguchi et al. (2016), we first use a bidirectional RNN (Schuster and Paliwal, 1997) at lexical level to concatenate the forward and backward states as the hidden states of source words, to capture the preceding and following contexts (described in Section 3.", "startOffset": 8, "endOffset": 1203}, {"referenceID": 5, "context": "Figure 2: The tree-based model of Eriguchi et al. (2016) comprising a structured and sequential encoder.", "startOffset": 34, "endOffset": 57}, {"referenceID": 16, "context": "To alleviate the out-of-vocabulary (OOV) problem, we further extend the proposed tree-based model to the sub-word level by integrating byte-pair encoding (BPE) (Sennrich et al., 2016) into the treebased model (as described in Section 3.", "startOffset": 160, "endOffset": 183}, {"referenceID": 5, "context": "Experimental results for the NIST English-toChinese translation task reveal that the proposed model significantly outperforms the vanilla treebased (Eriguchi et al., 2016) and sequential NMT models (Bahdanau et al.", "startOffset": 148, "endOffset": 171}, {"referenceID": 0, "context": ", 2016) and sequential NMT models (Bahdanau et al., 2015) (Section 4.", "startOffset": 34, "endOffset": 57}, {"referenceID": 8, "context": "(2016) employed a forward Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) recurrent neural network (RNN) to encode the lexical nodes and a treeLSTM (Tai et al.", "startOffset": 56, "endOffset": 109}, {"referenceID": 7, "context": "(2016) employed a forward Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) recurrent neural network (RNN) to encode the lexical nodes and a treeLSTM (Tai et al.", "startOffset": 56, "endOffset": 109}, {"referenceID": 19, "context": ", 2000) recurrent neural network (RNN) to encode the lexical nodes and a treeLSTM (Tai et al., 2015) to generate the phrase representations in a bottom-up fashion.", "startOffset": 82, "endOffset": 100}, {"referenceID": 3, "context": "In the present study, we utilize the gated recurrent unit (GRU) (Cho et al., 2014b) instead of an LSTM, in view of its comparable performance (Chung et al.", "startOffset": 64, "endOffset": 83}, {"referenceID": 4, "context": ", 2014b) instead of an LSTM, in view of its comparable performance (Chung et al., 2014) and since it yields even better results for certain tasks (J\u00f3zefowicz et al.", "startOffset": 67, "endOffset": 87}, {"referenceID": 9, "context": ", 2014) and since it yields even better results for certain tasks (J\u00f3zefowicz et al., 2015).", "startOffset": 66, "endOffset": 91}, {"referenceID": 2, "context": "As shown in Figure 2, Eriguchi et al. (2016) employed a forward Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al.", "startOffset": 22, "endOffset": 45}, {"referenceID": 25, "context": "The parent hidden state h\u2191i,j summarizes its left child h \u2191 i,k and right child h\u2191k+1,j (i < k < j) by applying the tree-GRU (Zhou et al., 2016) as follows:", "startOffset": 125, "endOffset": 144}, {"referenceID": 12, "context": "In generating the target words, we employ a sequential decoder with an input-feeding method (Luong et al., 2015) and attention mechanism (Bahdanau et al.", "startOffset": 92, "endOffset": 112}, {"referenceID": 0, "context": ", 2015) and attention mechanism (Bahdanau et al., 2015).", "startOffset": 32, "endOffset": 55}, {"referenceID": 12, "context": "Given the previous target word yj\u22121, the concatenation of the previous hidden state sj\u22121 and the previous context vector cj\u22121 (input-feeding) (Luong et al., 2015), sj , is calculated using a standard sequential GRU network:", "startOffset": 142, "endOffset": 162}, {"referenceID": 5, "context": "Eriguchi et al. (2016) adopted a tree-based attention mechanism to consider both the word and phrase vectors:", "startOffset": 0, "endOffset": 23}, {"referenceID": 5, "context": "Although the tree-based encoder of Eriguchi et al. (2016) has shown certain advantages in translation tasks involving distant language pairs, e.", "startOffset": 35, "endOffset": 58}, {"referenceID": 0, "context": "In order to generate leaf node annotation vectors which jointly take into account both preceding and following annotations, we exploit a bidirectional RNN encoder (Bahdanau et al., 2015).", "startOffset": 163, "endOffset": 186}, {"referenceID": 11, "context": "Contrary to the similar top-down encoding for sentiment classification (Kokkinos and Potamianos, 2017), which uses same weighting parameters to handle both left and right child nodes, f ld GRU and f rd GRU with different parameters are applied in the proposed model to distinguish the left and right structural information.", "startOffset": 71, "endOffset": 102}, {"referenceID": 3, "context": "According to the definition of a GRU (Cho et al., 2014b), f ld GRU uses an update gate z \u2193 i,k, a reset gate r i,k and a candidate activation h\u0303 \u2193 i,k to generate h\u2193i,k, as follows:", "startOffset": 37, "endOffset": 56}, {"referenceID": 6, "context": "(2016) proposed a simple and effective approach to handling out-ofvocabulary by representing rare words as a sequence of sub-word units, which are segmented using byte-pair encoding (BPE) (Gage, 1994).", "startOffset": 188, "endOffset": 200}, {"referenceID": 15, "context": "Sennrich et al. (2016) proposed a simple and effective approach to handling out-ofvocabulary by representing rare words as a sequence of sub-word units, which are segmented using byte-pair encoding (BPE) (Gage, 1994).", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "Inspired by work on a multi-modal NMT (Calixto et al., 2017) which exploits a gating scalar (Xu et al.", "startOffset": 38, "endOffset": 60}, {"referenceID": 21, "context": ", 2017) which exploits a gating scalar (Xu et al., 2015) to weight the image context vector, we use such a scalar in our model in order to dynamically adapt the weighting scalar.", "startOffset": 39, "endOffset": 56}, {"referenceID": 1, "context": "This distinguishes the model from that introduced by Calixto et al. (2017), in which the context vectors of the source sentence and image (bi-modal) are measured using two independent attention models and the gating scalar is merely used to weight the image context vector.", "startOffset": 53, "endOffset": 75}, {"referenceID": 20, "context": "On the Chinese side, the sentences are segmented using the Chinese word segmentation toolkit of NiuTrans (Xiao et al., 2012).", "startOffset": 105, "endOffset": 124}, {"referenceID": 5, "context": ", 2014, 2015) to produce a binary syntactic tree for each sentence, in constrast to the use of the HPSG parser by Eriguchi et al. (2016). On the Chinese side, the sentences are segmented using the Chinese word segmentation toolkit of NiuTrans (Xiao et al.", "startOffset": 114, "endOffset": 137}, {"referenceID": 22, "context": "Moreover, the model parameters are optimized using AdaDelta (Zeiler, 2012), due to its capability for dynamically adapting the learning rate.", "startOffset": 60, "endOffset": 74}, {"referenceID": 13, "context": "The accuracy of the translation relative to a reference is assessed using the BLEU metric (Papineni et al., 2002).", "startOffset": 90, "endOffset": 113}, {"referenceID": 5, "context": "Compared with the original tree-based encoder (Eriguchi et al., 2016), the model with bidirectional leaf-node encoding (described in Section 3.", "startOffset": 46, "endOffset": 69}, {"referenceID": 16, "context": "In addition, we evaluate our tree-based rare word encoding method against the conventional rare word encoding (Sennrich et al., 2016) using the sequen-", "startOffset": 110, "endOffset": 133}, {"referenceID": 0, "context": "tial encoder (Bahdanau et al., 2015).", "startOffset": 13, "endOffset": 36}], "year": 2017, "abstractText": "This paper proposes a hierarchical attentional neural translation model which focuses on enhancing source-side hierarchical representations by covering both local and global semantic information using a bidirectional tree-based encoder. To maximize the predictive likelihood of target words, a weighted variant of an attention mechanism is used to balance the attentive information between lexical and phrase vectors. Using a tree-based rare word encoding, the proposed model is extended to sub-word level to alleviate the out-of-vocabulary (OOV) problem. Empirical results reveal that the proposed model significantly outperforms sequence-to-sequence attention-based and tree-based neural translation models in English-Chinese translation tasks.", "creator": "LaTeX with hyperref package"}}}