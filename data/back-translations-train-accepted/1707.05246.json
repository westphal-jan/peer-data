{"id": "1707.05246", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jul-2017", "title": "Learning to select data for transfer learning with Bayesian Optimization", "abstract": "Domain similarity measures can be used to gauge adaptability and select suitable data for transfer learning, but existing approaches define ad hoc measures that are deemed suitable for respective tasks. Inspired by work on curriculum learning, we propose to \\emph{learn} data selection measures using Bayesian Optimization and evaluate them across models, domains and tasks. Our learned measures outperform existing domain similarity measures significantly on three tasks: sentiment analysis, part-of-speech tagging, and parsing. We show the importance of complementing similarity with diversity, and that learned measures are -- to some degree -- transferable across models, domains, and even tasks.", "histories": [["v1", "Mon, 17 Jul 2017 15:53:18 GMT  (259kb,D)", "http://arxiv.org/abs/1707.05246v1", "EMNLP 2017. Code available at:this https URL"]], "COMMENTS": "EMNLP 2017. Code available at:this https URL", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["sebastian ruder", "barbara plank"], "accepted": true, "id": "1707.05246"}, "pdf": {"name": "1707.05246.pdf", "metadata": {"source": "CRF", "title": "Learning to select data for transfer learning with Bayesian Optimization", "authors": ["Sebastian Ruder", "Barbara Plank"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Domain adaptation is a prominent approach to learning transfer that can help bridge this gap; many approaches have been proposed so far (Blitzer et al., 2007; Daum\u00e9 III, 2007; Jiang and Zhai, 2007; Ma et al., 2014; Beak and Sagittarius, 2014); however, most work has focused on one-to-one scenarios. Recently, the use of multiple sources has been considered. Such studies are rare and typically rely on specific model transfer approaches (Mansour, 2009; Wu and Huang, 2016).Inspired by the work on curriculum learning (Bengio et al., 2009; Tsvetkov et al., 2016), we suggest instead - to the best of our knowledge - to translate the first model agnostic data selection approach."}, {"heading": "2 Background: Transfer learning", "text": "Transfer Learning generally encompasses the concepts of a domain and a task (Pan and Yang, 2010).A domain D consists of a attribute space X and a marginal probability distribution P (X) over X, ar Xiv: 170 7,05 246v 1 [cs.C L] 17 July 2 017wo X = {x1, \u00b7 \u00b7, xn}. In document classification with a bag area Y and a conditional probability distribution P (Y | X), xi is the i-th document vector, and X is a sample of documents.Given a domain D = {X, P (X)}, a task T consists of a label space Y and a conditional probability distribution P (Y | X), which is typically learned from training data consisting of pairs {xi, yi} and a document sample."}, {"heading": "3 Data selection model", "text": "To select training data for adaptation to a task T, existing approaches classify the available n training examples X = {x1, x2, \u00b7 \u00b7 \u00b7, xn} of the k source domains D = {D1, D2, \u00b7 \u00b7 \u00b7, Dk} using a domain similarity measurement S and select the top m samples for training their algorithm. Although this has been proven empirically (Moore and Lewis, 2010; Axelrod et al., 2011; Plank and van Noord, 2011; Van Asch and Daelemans, 2010; Remus, 2012), we cannot adapt to the properties of our task T and the target domain DT and do without additional knowledge that can be gained from the interaction of different metrics. For this reason, we propose to learn the following linear domain similarities S as a linear combination of characteristic values: S = definition (X) \u00b7 w (1), where small (J) examples of the differences in each characteristic are represented by the T during the size of each characteristic T."}, {"heading": "3.1 Bayesian Optimization for data selection", "text": "Similar to Tsvetkov et al. (2016), we use Bayesian Optimization (Brochu et al., 2010), which has emerged as an efficient framework for optimizing each function, repeatedly finding better settings for neural network hyperparameters than domain experts (Snoek et al., 2012). Faced with a black box function f: X \u2192 R, Bayesian Optimization aims to find an input x-argminx value that minimizes f (x) globally, requiring a prior p (f) via the function and a capture function ap (f): X \u2192 R, which calculates the benefit of each rating at any x.Bayesian Optimization point. (Bayesian Optimization-f) is iterative."}, {"heading": "3.2 Features", "text": "Some measures have been proposed in the literature (Van Asch and Daelemans, 2010; Plank and van Noord, 2011; Lee, 2012), but have so far only been in isolation.Only the selection of training instances related to the target domain does not take into account the instances that are richer and better suited for knowledge acquisition. Therefore, to our knowledge for the first time, we are looking at whether intrinsic qualities of training data for diversity are useful for domain adaptation in NLP.Similarity We are using a number of similarity metrics that may be more suitable for some tasks, while different metrics may capture complementary information. Therefore, we are using the following metrics as characteristics for learning a more effective domain similarity metric.We are defining similarity characteristics in accordance with existing literature (Plank and van Noord, 2011)."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Tasks, datasets, and models", "text": "We evaluate our approach to three tasks: Sentiment Analysis, Part-of-speech (POS) tagging, and dependency parsing. We use the n examples with the highest score determined by the learned data selection for training our models.2 We show statistics for all datasets in Table 1.Sentiment Analysis For sensation analysis, we rate the vocabulary size on the Amazon Ratings Dataset (Blitzer et al., 2006). We set the vocabulary size to 10,000 and the number of training examples n = 1600 to match existing approaches (Bollegala et al., 2011) and stratify the training set.POS Tagging For POS tagging and parsing, we evaluate the coarse POS data and the POS 4S data (12 universal POS data) of the SANCL 2012 shared task (Petrov and McDonald, 2012).Each domain - except J - for WOS - we each contain two sets of SOS and two sets of 100,000."}, {"heading": "T Domain # labeled # unlabeled", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2 Training details", "text": "In practice, we have found that it is helpful to apply normalization in a manner similar to Tsvetkov et al. (2016), since characteristic values occupy different ranges, and we limit the weights w to [\u2212 1, 1]. For each set of data, we treat each domain as a target domain and all other domains as source domains. Similar to Bousmalis et al. (2016), we use a small number of (100) target domain examples as validation sets. We optimize each similarity measure using Bayesian optimization with 300 iterations according to the objective metric J of each task (accuracy in mood analysis and POS tagging; LAS in parsing) with respect to the validation of the corresponding target domain. Unlabeled data is additionally used to calculate the representation of the target domain with 300 iterations and calculate the source domain representation for the most similar domain base."}, {"heading": "4.3 Baselines and features", "text": "We compare the learned metrics with three baselines: i) a random baseline that randomly selects n training samples from all source domains; ii) randomly selected examples from the most similar source domain determined by Jensen-Shannon divergence (JS domain); and we compare against training using all available source data (6,000 examples of mood analysis; 14,700-17,569 examples of POS tagging and analysis depending on the target domain). We optimize data selection using Bayean optimization with each set of characteristics: similarity characteristics or based on i) word embedding, ii) term distributions and ii) topic distributions; and iv) diversity characteristics. In addition, we examine how well different representations help each other by examining the similarities with two similarity-based characteristics, whether we compare the respective distributions with each other's characteristics and, finally, the other characteristics."}, {"heading": "5 Results", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "5.1 Transfer across models", "text": "In practice, a metric can be learned on the basis of a model that is cheap to evaluate and, similar to uptraining, serves as a proxy for a state-of-the-art model (Petrov et al., 2010). To this end, we use the characteristics learned on the basis of the Structured Perceptron model to select data for POS marking and use them to select data for the Bi-LSTM tagger. Results in Table 4 suggest that cross-model transfer is indeed possible, with most transmitted sets of characteristics achieving similar results or even exceeding the characteristics learned with the Bi-LSTM. In particular, transmitted diversity significantly exceeds their equivalent in the model, which is encouraging as it allows learning a metric for data selection with less complex models."}, {"heading": "5.2 Transfer across domains", "text": "We study whether data selection parameters learned for a target domain are transferred to other target domains. For each domain, we use the weights with the highest performance on the validation set and use them to select data with the remaining domains as the target domains. We perform 10 runs for the most powerful feature sets for sentiment analysis and report on the average accuracy values in Table 5 (for POS tagging, see Table 6).The transfer of weights learned with Bayean optimization is quite robust in most cases. Feature sets such as similarity or diversity trained on books exceed the strong JS - > D baseline in all 6 cases, for electronics and cooking in 4 / 6 cases (off-diagonals for fields 2 and 3 in Table 5). In some cases, the transferred weights even exceed the data selection metric learned for that domain, e.g. on D- Simulation E - with simulation - > + Dispersion and percentage points - almost."}, {"heading": "5.3 Transfer across tasks", "text": "Finally, we examine whether the data selection is task-specific or whether one parameter learned in a task can be transferred to another. For each feature set, we use the learned weights for each domain in the source task (for mood analysis, we use the best weights in the validation group; for POS marking, we use the Structured Perceptron model) and conduct experiments with them for all domains in the target task. 7 We report on the averaged accuracy 7E.g. for SA- > POS, we receive for each feature set a set of weights for each of the 4 SA domains we use for transferring across all tasks in Table 7. The transfer is productive between related tasks, i.e. POS marking and analysis of results are similar to those obtained with the data selection for the respective task. We observe large performance declines when transferring between unrelated tasks, so we select data for the 6 POS domains, resulting in results of 6 \u00b7 6 = 24."}, {"heading": "6 Related work", "text": "Most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to"}, {"heading": "7 Conclusion", "text": "We propose to use Bayesian optimization to learn data selection measures for transfer learning. Our results surpass existing domain similarity metrics in three areas (sentiment analysis, POS tagging, and parsing) and are competitive with a state-of-the-art domain adaptation approach. More importantly, we present the first study on the transferability of such measures, which shows promising results for porting them to models, domains, and related tasks."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their valuable feedback. Sebastian is supported by the funding number EBPPG / 2014 / 30 of the Irish Research Council and the funding number SFI / 12 / RC / 2289 of the Science Foundation Ireland. Barbara is supported by the NVIDIA Corporation and the data centre of the University of Groningen."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Domain similarity measures can be used<lb>to gauge adaptability and select suitable<lb>data for transfer learning, but existing ap-<lb>proaches define ad hoc measures that are deemed suitable for respective tasks. In-<lb>spired by work on curriculum learning, we<lb>propose to learn data selection measures<lb>using Bayesian Optimization and evaluate<lb>them across models, domains and tasks. Our learned measures outperform existing<lb>domain similarity measures significantly<lb>on three tasks: sentiment analysis, part-<lb>of-speech tagging, and parsing. We show<lb>the importance of complementing similarity with diversity, and that learned mea-<lb>sures are\u2014to some degree\u2014transferable<lb>across models, domains, and even tasks.", "creator": "LaTeX with hyperref package"}}}