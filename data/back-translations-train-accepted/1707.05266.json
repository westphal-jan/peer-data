{"id": "1707.05266", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jul-2017", "title": "A Simple Language Model based on PMI Matrix Approximations", "abstract": "In this study, we introduce a new approach for learning language models by training them to estimate word-context pointwise mutual information (PMI), and then deriving the desired conditional probabilities from PMI at test time. Specifically, we show that with minor modifications to word2vec's algorithm, we get principled language models that are closely related to the well-established Noise Contrastive Estimation (NCE) based language models. A compelling aspect of our approach is that our models are trained with the same simple negative sampling objective function that is commonly used in word2vec to learn word embeddings.", "histories": [["v1", "Mon, 17 Jul 2017 16:21:46 GMT  (28kb)", "http://arxiv.org/abs/1707.05266v1", "Accepted to EMNLP 2017"]], "COMMENTS": "Accepted to EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["oren melamud", "ido dagan", "jacob goldberger"], "accepted": true, "id": "1707.05266"}, "pdf": {"name": "1707.05266.pdf", "metadata": {"source": "CRF", "title": "A Simple Language Model based on PMI Matrix Approximations", "authors": ["Oren Melamud", "Jacob Goldberger"], "emails": ["oren.melamud@ibm.com", "dagan@cs.biu.ac.il", "jacob.goldberger@biu.ac.il"], "sections": [{"heading": null, "text": "ar Xiv: 170 7.05 266v 1 [cs.C L] 17 Jul 2 017 for learning language models by training them to estimate mutual word context information (PMI) and then derive the desired conditional probabilities from PMI at the test date. In particular, we show that with minor modifications of the word2vec algorithm we obtain principled language models that are closely related to the established language models of Noise Contrastive Estimation (NCE). A convincing aspect of our approach is that our models are trained with the same simple negative sampling lens function commonly used in word2vec to learn word embedding."}, {"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead in order to achieve the objectives I have mentioned."}, {"heading": "2 NCE-based Language Modeling", "text": "It is not the manner in which a mixture occurs, but the manner in which a mixture occurs, a mixture in which a mixture occurs, a mixture in which a mixture occurs, a mixture in which a mixture occurs, a mixture in which a mixture occurs, a mixture in which it occurs, a mixture in which it occurs, a mixture in which it occurs, a mixture in which it occurs, a mixture in which it occurs, a mixture in which it occurs, a mixture in which it occurs, a mixture in which it occurs, a mixture in which it occurs, a mixture in which it occurs, a mixture in which it occurs, a mixture in which it occurs, a mixture in which it occurs, a mixture in which it occurs, a mixture in which it occurs, a mixture in which it occurs, a mixture in which it occurs in which it occurs, a mixture in which it occurs in which it occurs, a mixture in which it occurs, a mixture in which it occurs in which it occurs, a mixture in which it occurs in which it occurs, a mixture in which it occurs in which it occurs in which it occurs, a mixture in which it occurs in which it occurs in which it occurs in a mixture in which it occurs, a mixture in which it occurs in which, a mixture in which it occurs in which it occurs in which it occurs in which it occurs in which it occurs in a mixture in which it occurs in a mixture in which it occurs, a mixture in which it occurs in which it occurs in which it occurs in which it occurs in a mixture in which it occurs in which it occurs in which it occurs in which it occurs in a mixture in which it occurs in which it occurs in which it occurs in which it occurs in a mixture in which it occurs in which it occurs in which it occurs in which it does not in which it occurs in which it occurs in a mixture in which it occurs in which it occurs in which, in which it occurs in"}, {"heading": "3 PMI-based Language Modeling", "text": "This year, it is only a matter of time before we reach an agreement."}, {"heading": "4 Experiments", "text": "The idea behind this is that it is a matter of a way in which people move in the world in the most diverse living environments, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live."}, {"heading": "5 Conclusions", "text": "In this paper, we have shown that the negative sampling objective function of word2vec, which is popularized in the context of learning word representations, can also be used to effectively learn parametric language models. These language models are closely related to the NCE language models, but use a simpler, potentially more robust objective function. More generally, our theoretical analysis shows that any word2vec model trained with negative sampling can in principle be used to estimate the conditional distribution p (w | c) by following our proposed method at test time."}, {"heading": "Acknowledgments", "text": "This work is supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)."}], "references": [{"title": "When and why are loglinear models self-normalizing? In NAACL", "author": ["J. Andreas", "D. Klein"], "venue": null, "citeRegEx": "Andreas and Klein.,? \\Q2015\\E", "shortCiteRegEx": "Andreas and Klein.", "year": 2015}, {"title": "Quick training of probabilistic neural nets by importance sampling", "author": ["Y. Bengio", "J. Senecal"], "venue": "AISTATS.", "citeRegEx": "Bengio and Senecal,? 2003", "shortCiteRegEx": "Bengio and Senecal", "year": 2003}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["C. Chelba", "T. Mikolov", "M. Schuster", "Q. Ge", "T. Brants", "P. Koehn", "T. Robinson."], "venue": "arXiv preprint arXiv:1312.3005.", "citeRegEx": "Chelba et al\\.,? 2013", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "Strategies for training large vocabulary neural language models", "author": ["W. Chen", "D. Grangier", "M. Auli."], "venue": "CoRR, abs/1512.04906.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Recurrent neural network language model training with noise contrastive estimation for speech recognition", "author": ["X. Chen", "X. Liu", "M. Gales", "P.C. Woodland."], "venue": "ICASSP.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Notes on noise contrastive estimation and negative sampling", "author": ["C. Dyer."], "venue": "arXiv preprint arXiv:1410.8251.", "citeRegEx": "Dyer.,? 2014", "shortCiteRegEx": "Dyer.", "year": 2014}, {"title": "Noisecontrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["M.U. Gutmann", "A. Hyvarinen."], "venue": "Journal of Machine Learning Research, 13:307\u2013 361.", "citeRegEx": "Gutmann and Hyvarinen.,? 2012", "shortCiteRegEx": "Gutmann and Hyvarinen.", "year": 2012}, {"title": "Blackout: Speeding up recurrent neural network language models with very large vocabularies", "author": ["S. Ji", "S. Vishwanathan", "N. Satish", "A. Nadathur", "J. Michael", "P. Dubey."], "venue": "ICLR.", "citeRegEx": "Ji et al\\.,? 2016", "shortCiteRegEx": "Ji et al\\.", "year": 2016}, {"title": "Exploring the limits of language modeling", "author": ["R. Jozefowicz", "O. Vinyals", "M. Schuster", "N. Shazeer", "Y. Wu."], "venue": "arXiv preprint arXiv:1602.02410.", "citeRegEx": "Jozefowicz et al\\.,? 2016", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["O. Levy", "Y. Goldberg."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Informationtheory interpretation of the skip-gram negativesampling objective function", "author": ["O. Melamud", "J. Goldberger."], "venue": "Proceedings of ACL.", "citeRegEx": "Melamud and Goldberger.,? 2017", "shortCiteRegEx": "Melamud and Goldberger.", "year": 2017}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Minh", "G.E. Hinton."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Minh and Hinton.,? 2008", "shortCiteRegEx": "Minh and Hinton.", "year": 2008}, {"title": "A fast and simple algorithm for training neural probabilistic languagemodels", "author": ["A. Mnih", "Y.W. Teh."], "venue": "ICML.", "citeRegEx": "Mnih and Teh.,? 2012", "shortCiteRegEx": "Mnih and Teh.", "year": 2012}, {"title": "Neural machine translation of rare words with subword units", "author": ["R. Sennrich", "B. Haddow", "A. Birch."], "venue": "CoRR, abs/1508.07909.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Chainer: a next-generation open source framework for deep learning", "author": ["S. Tokui", "K. Oono", "S. Hido", "J. Clayton."], "venue": "Workshop on Machine Learning Systems (LearningSys) in The 29th Annual Conference on Neural Information Processing Systems.", "citeRegEx": "Tokui et al\\.,? 2015", "shortCiteRegEx": "Tokui et al\\.", "year": 2015}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["A. Vaswani", "Y. Zhao", "V. Fossum", "D. Chiang."], "venue": "EMNLP.", "citeRegEx": "Vaswani et al\\.,? 2013", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Scaling recurrent neural network language models", "author": ["W. Williams", "N. Prasad", "D. Mrva", "T. Ash", "T. Robinson."], "venue": "ICASSP.", "citeRegEx": "Williams et al\\.,? 2015", "shortCiteRegEx": "Williams et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals."], "venue": "arXiv preprint arXiv:1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Simple, fast noise-contrastive estimation for large RNN vocabularies", "author": ["B. Zoph", "A. Vaswani", "J. May", "K. Knight."], "venue": "NAACL.", "citeRegEx": "Zoph et al\\.,? 2016", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "Recurrent Neural Network (RNN) language models recently outperformed traditional n-gram LMs across a range of tasks (Jozefowicz et al., 2016).", "startOffset": 116, "endOffset": 141}, {"referenceID": 13, "context": "This approach pling (Bengio and et al, 2003), hierarchical softmax (Minh and Hinton, 2008), BlackOut (Ji et al.", "startOffset": 67, "endOffset": 90}, {"referenceID": 7, "context": "This approach pling (Bengio and et al, 2003), hierarchical softmax (Minh and Hinton, 2008), BlackOut (Ji et al., 2016) and Noise Contrastive Estimation (NCE) (Gutmann and Hyvarinen, 2012).", "startOffset": 101, "endOffset": 118}, {"referenceID": 6, "context": ", 2016) and Noise Contrastive Estimation (NCE) (Gutmann and Hyvarinen, 2012).", "startOffset": 47, "endOffset": 76}, {"referenceID": 14, "context": "NCE has been applied to train neural LMs with large vocabularies (Mnih and Teh, 2012) and more recently was also successfully used to train LSTM-RNN LMs (Vaswani et al.", "startOffset": 65, "endOffset": 85}, {"referenceID": 17, "context": "NCE has been applied to train neural LMs with large vocabularies (Mnih and Teh, 2012) and more recently was also successfully used to train LSTM-RNN LMs (Vaswani et al., 2013; Chen et al., 2015; Zoph et al., 2016).", "startOffset": 153, "endOffset": 213}, {"referenceID": 4, "context": "NCE has been applied to train neural LMs with large vocabularies (Mnih and Teh, 2012) and more recently was also successfully used to train LSTM-RNN LMs (Vaswani et al., 2013; Chen et al., 2015; Zoph et al., 2016).", "startOffset": 153, "endOffset": 213}, {"referenceID": 20, "context": "NCE has been applied to train neural LMs with large vocabularies (Mnih and Teh, 2012) and more recently was also successfully used to train LSTM-RNN LMs (Vaswani et al., 2013; Chen et al., 2015; Zoph et al., 2016).", "startOffset": 153, "endOffset": 213}, {"referenceID": 8, "context": "NCE-based language models achieved near state-of-the-art performance on language modeling tasks (Jozefowicz et al., 2016; Chen et al., 2016), and as we later show, are closely related to the method presented in this paper.", "startOffset": 96, "endOffset": 140}, {"referenceID": 3, "context": "NCE-based language models achieved near state-of-the-art performance on language modeling tasks (Jozefowicz et al., 2016; Chen et al., 2016), and as we later show, are closely related to the method presented in this paper.", "startOffset": 96, "endOffset": 140}, {"referenceID": 12, "context": "In particular, the skip-gram with negative sampling (NEG) embedding algorithm (Mikolov et al., 2013) as implemented in the word2vec toolkit, has become one of the most pop-", "startOffset": 78, "endOffset": 100}, {"referenceID": 5, "context": "The NEG objective function is considered a simplification of the NCE\u2019s objective, unsuitable for learning language models (Dyer, 2014).", "startOffset": 122, "endOffset": 134}, {"referenceID": 9, "context": "Recently, Levy and Goldberg (2014) offered a motivation for the NEG objective function, showing that by maximizing this function, the skip-gram algorithm implicitly attempts to factorize a wordcontext pointwise mutual information (PMI) matrix.", "startOffset": 10, "endOffset": 35}, {"referenceID": 9, "context": "Recently, Levy and Goldberg (2014) offered a motivation for the NEG objective function, showing that by maximizing this function, the skip-gram algorithm implicitly attempts to factorize a wordcontext pointwise mutual information (PMI) matrix. Melamud and Goldberger (2017) rederived this result by offering an information-theory interpretation of NEG.", "startOffset": 10, "endOffset": 274}, {"referenceID": 8, "context": "has notable merits (Jozefowicz et al., 2016; Sennrich et al., 2016), but is out of the scope of this paper.", "startOffset": 19, "endOffset": 67}, {"referenceID": 15, "context": "has notable merits (Jozefowicz et al., 2016; Sennrich et al., 2016), but is out of the scope of this paper.", "startOffset": 19, "endOffset": 67}, {"referenceID": 17, "context": "Furthermore, conveniently, it also has a notably more simplified objective function formulation inherited from word2vec, which allows it to avoid the heuristic components and initialization procedures used in various implementations of NCE language models (Vaswani et al., 2013; Chen et al., 2015; Zoph et al., 2016).", "startOffset": 256, "endOffset": 316}, {"referenceID": 4, "context": "Furthermore, conveniently, it also has a notably more simplified objective function formulation inherited from word2vec, which allows it to avoid the heuristic components and initialization procedures used in various implementations of NCE language models (Vaswani et al., 2013; Chen et al., 2015; Zoph et al., 2016).", "startOffset": 256, "endOffset": 316}, {"referenceID": 20, "context": "Furthermore, conveniently, it also has a notably more simplified objective function formulation inherited from word2vec, which allows it to avoid the heuristic components and initialization procedures used in various implementations of NCE language models (Vaswani et al., 2013; Chen et al., 2015; Zoph et al., 2016).", "startOffset": 256, "endOffset": 316}, {"referenceID": 0, "context": "Mnih and Teh (2012) found empirically that setting Zc = 1 didn\u2019t hurt the performance (see also discussion in (Andreas and Klein, 2015)).", "startOffset": 110, "endOffset": 135}, {"referenceID": 17, "context": "Recent works (Vaswani et al., 2013; Zoph et al., 2016) used Zc = 1 and also initialized NCE\u2019s bias term from Eq.", "startOffset": 13, "endOffset": 54}, {"referenceID": 20, "context": "Recent works (Vaswani et al., 2013; Zoph et al., 2016) used Zc = 1 and also initialized NCE\u2019s bias term from Eq.", "startOffset": 13, "endOffset": 54}, {"referenceID": 11, "context": "Mnih and Teh (2012) found empirically that setting Zc = 1 didn\u2019t hurt the performance (see also discussion in (Andreas and Klein, 2015)).", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Mnih and Teh (2012) found empirically that setting Zc = 1 didn\u2019t hurt the performance (see also discussion in (Andreas and Klein, 2015)). Chen et al. (2015) reported that setting log(Zc) = 9 gave them the best results.", "startOffset": 111, "endOffset": 157}, {"referenceID": 12, "context": "The algorithm optimizes the following NEG objective function (Mikolov et al., 2013):", "startOffset": 61, "endOffset": 83}, {"referenceID": 10, "context": "Levy and Goldberg (2014) showed that this objective function achieves its maximal value when for every word-context pair w, c:", "startOffset": 0, "endOffset": 25}, {"referenceID": 11, "context": "imation criterion optimized by the skip-gram algorithm can be found in (Melamud and Goldberger, 2017).", "startOffset": 71, "endOffset": 101}, {"referenceID": 10, "context": "Our study is based on two simple observations regarding this finding of Levy and Goldberg (2014). First, Equation (5) can be reformulated as follows to derive an estimate of the conditional distribution p(w|c):", "startOffset": 72, "endOffset": 97}, {"referenceID": 14, "context": "Hence, we have no real motivation to include additional learned normalization parameters, as considered in comparable NCE language models (Mnih and Teh, 2012; Zoph et al., 2016).", "startOffset": 138, "endOffset": 177}, {"referenceID": 20, "context": "Hence, we have no real motivation to include additional learned normalization parameters, as considered in comparable NCE language models (Mnih and Teh, 2012; Zoph et al., 2016).", "startOffset": 138, "endOffset": 177}, {"referenceID": 17, "context": "This may explain heuristics used by prior work for initializing the values of bw (Vaswani et al., 2013; Zoph et al., 2016).", "startOffset": 81, "endOffset": 122}, {"referenceID": 20, "context": "This may explain heuristics used by prior work for initializing the values of bw (Vaswani et al., 2013; Zoph et al., 2016).", "startOffset": 81, "endOffset": 122}, {"referenceID": 16, "context": "All of the models described hereafter were implemented using the Chainer toolkit (Tokui et al., 2015).", "startOffset": 81, "endOffset": 101}, {"referenceID": 17, "context": "For our NCE baseline, we used the heuristics that worked well in (Vaswani et al., 2013; Zoph et al., 2016), initializing NCE\u2019s bias term from Eq.", "startOffset": 65, "endOffset": 106}, {"referenceID": 20, "context": "For our NCE baseline, we used the heuristics that worked well in (Vaswani et al., 2013; Zoph et al., 2016), initializing NCE\u2019s bias term from Eq.", "startOffset": 65, "endOffset": 106}, {"referenceID": 17, "context": "For our NCE baseline, we used the heuristics that worked well in (Vaswani et al., 2013; Zoph et al., 2016), initializing NCE\u2019s bias term from Eq. (2) to bw = \u2212 log |V |, where V is the word vocabulary, and using Zc = 1. The first dataset we used is a version of the Penn Tree Bank (PTB), commonly used to evaluate language models. It consists of 929K training words, 73K validation words and 82K test words with a 10K word vocabulary. To build and train the compared models in this setting, we followed the work of Zaremba et al. (2014), who achieved excellent results on this dataset.", "startOffset": 66, "endOffset": 537}, {"referenceID": 20, "context": "We set the negative sampling parameter to k = 100 following Zoph et al. (2016), who showed highly competitive performance with NCE LMs trained with this number of samples.", "startOffset": 60, "endOffset": 79}, {"referenceID": 2, "context": "As the second dataset, we used the much larger WMT 1B-word benchmark introduced by Chelba et al. (2013). This dataset comprises about 0.", "startOffset": 83, "endOffset": 104}, {"referenceID": 19, "context": "tgz Zaremba et al. (2014) used larger models with more units and also applied dropout to the output of the top LSTM layer, which we did not.", "startOffset": 4, "endOffset": 26}, {"referenceID": 18, "context": "However, we followed previous works (Williams et al., 2015; Ji et al., 2016) and trimmed the vocabulary further down to the top 64K most frequent words in order to successfully fit a neural model to this data using reasonably modest compute resources.", "startOffset": 36, "endOffset": 76}, {"referenceID": 7, "context": "However, we followed previous works (Williams et al., 2015; Ji et al., 2016) and trimmed the vocabulary further down to the top 64K most frequent words in order to successfully fit a neural model to this data using reasonably modest compute resources.", "startOffset": 36, "endOffset": 76}, {"referenceID": 9, "context": "We trained our model for only one epoch using the Adam optimizer (Kingma and Ba, 2014) with default parameters, which we found to converge more quickly and effectively than SGD.", "startOffset": 65, "endOffset": 86}, {"referenceID": 7, "context": ", 2015; Ji et al., 2016) and trimmed the vocabulary further down to the top 64K most frequent words in order to successfully fit a neural model to this data using reasonably modest compute resources. To build and train our models, we used a similar method to the one used with PTB, with the following differences. We used a single-layer 512hidden-unit LSTM to represent the preceding context. We followed Jozefowicz et al. (2016), who found a 10% dropout rate to be sufficient for relatively small models fitted to this large training corpus.", "startOffset": 8, "endOffset": 430}, {"referenceID": 19, "context": "For example, on the small PTB test set, Zaremba et al. (2014) achieved 78.", "startOffset": 40, "endOffset": 62}, {"referenceID": 8, "context": "On the larger WMT dataset, Jozefowicz et al. (2016) achieved 46.", "startOffset": 27, "endOffset": 52}], "year": 2017, "abstractText": "In this study, we introduce a new approach for learning language models by training them to estimate word-context pointwise mutual information (PMI), and then deriving the desired conditional probabilities from PMI at test time. Specifically, we show that with minor modifications to word2vec\u2019s algorithm, we get principled language models that are closely related to the well-established Noise Contrastive Estimation (NCE) based language models. A compelling aspect of our approach is that our models are trained with the same simple negative sampling objective function that is commonly used in word2vec to learn word embeddings.", "creator": "LaTeX with hyperref package"}}}