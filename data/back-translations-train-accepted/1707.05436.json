{"id": "1707.05436", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jul-2017", "title": "Improved Neural Machine Translation with a Syntax-Aware Encoder and Decoder", "abstract": "Most neural machine translation (NMT) models are based on the sequential encoder-decoder framework, which makes no use of syntactic information. In this paper, we improve this model by explicitly incorporating source-side syntactic trees. More specifically, we propose (1) a bidirectional tree encoder which learns both sequential and tree structured representations; (2) a tree-coverage model that lets the attention depend on the source-side syntax. Experiments on Chinese-English translation demonstrate that our proposed models outperform the sequential attentional model as well as a stronger baseline with a bottom-up tree encoder and word coverage.", "histories": [["v1", "Tue, 18 Jul 2017 01:53:58 GMT  (127kb,D)", "http://arxiv.org/abs/1707.05436v1", "Accepted as a long paper by ACL 2017"]], "COMMENTS": "Accepted as a long paper by ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["huadong chen", "shujian huang", "david chiang", "jiajun chen"], "accepted": true, "id": "1707.05436"}, "pdf": {"name": "1707.05436.pdf", "metadata": {"source": "CRF", "title": "Improved Neural Machine Translation with a Syntax-Aware Encoder and Decoder", "authors": ["Huadong Chen", "Shujian Huang", "David Chiang", "Jiajun Chen"], "emails": ["chenhd@nlp.nju.edu.cn", "huangsj@nlp.nju.edu.cn", "chenjj@nlp.nju.edu.cn", "dchiang@nd.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is as if most people who deal with the question of how to behave must put themselves and themselves at the centre. (...) It is not as if they feel able to understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world themselves. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. \"(...) It is as if they do not understand the world. (...) It is as if they do not want to understand the world themselves. (...) It is as if they do not want to understand the world. (...) It is as if they do not understand the world."}, {"heading": "2 Neural Machine Translation", "text": "Most NMT systems follow the encoder decoder system first proposed by Bahdanau et al. (2015) with attention. In view of a source sentence x = x1 \u00b7 \u00b7 xi \u00b7 \u00b7 \u00b7 xI and a target sentence y = y1 \u00b7 \u00b7 \u00b7 y j \u00b7 \u00b7 \u00b7 yJ, NMT aims to directly model the translation probability: P (y | x; \u03b8) = J-1 P (y j | y < j, x; \u03b8), (1) which is a set of parameters and y < j is the sequence of previously generated target words. Here we briefly describe the underlying framework of the encoder decoder NMT system."}, {"heading": "2.1 Encoder Model", "text": "Following Bahdanau et al. (2015), we use a bidirectional gated recurrent unit (GRU) (Cho et al., 2014b) to encode the source sentence so that the annotation of each word contains a summary of both the preceding and the following words. Bidirectional GRU consists of a forward and a backward GRU, as shown in Figure 2. Forward GRU reads the source sentence from left to right and computes a sequence of forward hidden states (\u2212 \u2192 h1,., \u2212 hI). Backward GRU scans the source sentence from right to left, resulting in a sequence of backward hidden states (\u2190 \u2212 h1,., \u2190 \u2212 hI). Also \u2212 hi = GRU (\u2212 hi \u2212 1, si). \u2212 hi = GRU (\u2212 hi = 1, si)."}, {"heading": "2.2 Decoder Model", "text": "The decoder is a forward GRU that predicts the translation y word by word; the probability of generating the j-th word y j is: P (y j | y < j, x; \u03b8) = Softmax (t j \u2212 1, d j, c j) (3), where t j \u2212 1 is the word embedding the (j \u2212 1) - th target, d j is the hidden time state of the decoder j, and c j is the context vector at the time j. The state d j is calculated asd j = GRU (d j \u2212 1, t j \u2212 1, c j), (4), where GRU (\u00b7) is extended to more than two arguments by first linking all arguments except the first one. The attention mechanism computes asd j = GRU (d j \u2212 1, t j \u2212 1, c j), (4), whereby GRU (\u00b7) is first extended to include all arguments except the first. The attention mechanism c \u00b2 computes the context vector ci as a weighted sum of the notes from the source, c j = I \u00b2 i = 1\u03b1 j, i hi (5), whereby c \u00b2, c, c, c \u00b2 a, c, c, c, a \u00b2, c, a, c, c, a \u00b2, c, c, a, (1), j \u00b2, j, j, j, i i i \u2212 p, c, c, a \u00b2 i, a, a, c, a \u00b2, c, a, a, c, a, a \u00b2, c, a, a, a, (5), c, c, c, c, a \u00b2, a, a, 1, a, a, a, a \u00b2, a, a, a, b, b, a \u00b2, a, b, b, a, a, b, p, p, b, p, p, a, b, b, b, b, a \u00b2, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, j, j, j, j, j, j, i \u2212 i, j, j, j, j, j, j, j, j,"}, {"heading": "3 Tree Structure Enhanced Neural Machine Translation", "text": "Although the syntax has shown its effectiveness in non-neural statistical machine translation systems (SMT) (Yamada and Knight, 2001; Koehn et al., 2003; Liu et al., 2006; Chiang, 2007), most proposed NMT models (one notable exception being Eriguchi et al. (2016)) process a sentence only as a sequence of words and do not explicitly exploit the inherent structure of natural language sentences. In this section, we present models that integrate syntactic family trees directly into the encoder decoder framework."}, {"heading": "3.1 Preliminaries", "text": "Like Eriguchi et al. (2016), we are currently focusing on source-side syntactic trees that can be calculated before translation. While Eriguchi et al. (2016) uses HPSG trees, we are using phrase structure trees as in Penn Chinese Treebank (Xue et al., 2005). Currently, we are using only the structure information from the tree without the syntactic designations. Therefore, our approach should apply to any syntactic grammar that provides such a tree structure (Figure 1 (b)). Formally, the encoder receives a source set x = x1 \u00b7 xI and a source tree whose leaves are labeled x1,..., xI. We assume that this tree is strictly binary branching. For the sake of simplicity, each node is assigned an index 1,..., which is the same as their word indices. For each node with index, we leave denk (if) the stem node (if there are (R) and (L) nodes."}, {"heading": "3.2 Tree-GRU Encoder", "text": "We first describe tree encoders (Tai et al., 2015; Eriguchi et al., 2016) and then discuss our improvements. Following Eriguchi et al. (2016), we build a tree encoder on the sequential encoder (as shown in Figure 3 (a). If the node k is a leaf node, its hidden state is the combination of its previously calculated left child state hL (k) and the right child state hR (k): h (h). The encoder is able to capture both the sequential context and the syntactic context. If the node k is an internal node, its hidden state is the combination of its previously calculated left child state hL (k) and the right child state hR (k): h (h). L (k), h)."}, {"heading": "3.3 Bidirectional Tree Encoder", "text": "Although the bottom-up tree encoder can take advantage of the syntactic structure, the learned representation of a node Q = Q is only based on its subtree; it contains no information from higher up in the tree. Specifically, the representation of leaf nodes is still the sequential one, so no syntactical information is fed into words. Similar to the bidirectional sequential encoder, we propose a natural extension of the bottom-up tree encoder: the bidirectional tree encoder (Figure 3 (b)). Unlike the bottom-up tree encoder or the right-left sequential encoder, the top-down encoder itself would not have lexical information as input. To address this problem, we feed the hidden states of the bottom-up encoder to the top-down encoder. In this way, the information of the entire syntactic tree is passed on to the root node and transmitted."}, {"heading": "3.4 Tree-Coverage Model", "text": "We are extending the decoder to include information about the source syntax in the attention model. We have observed two problems in translations produced with the help of the tree encoder. First, a syntactic phrase in the source set is often wrongly translated into discontinuous words in the output. Second, since the non-leaf node's annotations contain more information than the leaf node's annotations, the attention model prefers to focus too often on the non-leaf nodes - which can exacerbate the translation problem of the cover (by translating the same part of the sentence more than once). As shown in Figure 4 (a), almost all of the model's non-leaf nodes are visited too often during decoding; as a result, the Chinese phrase zhu manila is translated twice because the model pays attention to the zhu manila node, even though both words have already been translated; there is no mechanism to prevent this."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data", "text": "The parallel training data consists of 1.6 million sentence pairs extracted from LDC corpora, 3 containing 46.6 million Chinese words and 52.5 million English words, respectively. We use NIST MT02 as development data and NIST MT03-06 as test data. These data are mostly in the same genre (message wire) and avoid the additional consideration of domain adjustments. Table 1 shows the statistics of the data sets. The Chinese side of the corpora is word segmented using ICTCLAS.4 We3LDC2002E18, LDC2003E14, the Hansards portion of LDC2004T08 and LDC2005T06.4http: / ictclas.nlpir.orgparse the Chinese sentences using the Berkeley Parser5 (Petrov and Klein, 2007) and binarize the resulting trees according to Zhang and Clark (2009). The English side of the Kora is also a predetermined sentence, containing all of approximately 930,000 sentences."}, {"heading": "4.2 Model and Training Details", "text": "We compare our proposed models with several state-of-the-art NMT systems and techniques: \u2022 NMT: the standard attentive NMT model (Bahdanau et al., 2015). \u2022 Tree-LSTM: the attentive NMT model extended by the Tree LSTM encoder (Eriguchi et al., 2016). \u2022 Coverage: the attentive NMT model extended by word coverage (Tu et al., 2016). We used dl4mt implementation of the attentive model, 6 reimplementation of the tree and word coverage model. The word embedding measure is 512. The hidden layer sizes both forward and backward sequential encoders are 1024 (except where specified). Because our TreeGRU encoders are based on the bidirectional sequential encoder and word cover models, each of which is 48 in size."}, {"heading": "4.3 Tree Encoders", "text": "This set of experiments evaluates the effectiveness of our proposed tree encoder. Table 2, row 2 confirms the results of Eriguchi et al. (2016) that a tree LSTM encoder helps, and row 3 shows that our tree GRU encoder achieves a better result (+ 0.87 BLEU, v.s. row 2). To verify our assumption that the consistency of the model is important for performance, we also conduct experiments to compare TreeLSTM and Tree-GRU on top of the LSTM encoder settings. Tree-Lstm with the sequential model can achieve 1.02 BLEU improvement (Table 3, row 13), while Tree-LSTM with the sequential model only receives 0.75 BLEU improvement. Although Tree-Lstm with the sequential model achieves a slightly better result (+ 0.22 BLEU, v.s. table 3)."}, {"heading": "4.4 Tree-Coverage Model", "text": "Lines 5-8 in Table 2 show that the Word Coverage Model by Tu et al. (2016) consistently helps when used with our proposed tree encoders, with the bidirectional tree encoder remaining the best. However, the improvements in the tree encoder models are smaller than those of the baseline system. This could be caused by the fact that the word coverage model neglects the relationship between the trees, e.g. the relationship between children and parent nodes. Our tree coverage model consistently further improves performance (lines 9-11). Our best model combines our bidirectional tree encoder with our tree coverage model (line 11), resulting in a net improvement of + 3.54 BLEU over the standard attention model (line 1) and + 1.90 BLEU over the stronger baseline, which allows both the bottom-up tree encoder and the coverage model from previous work on the left not to include the coverage model of each of the previous line (as already mentioned by neighbors)."}, {"heading": "4.5 Analysis By Sentence Length", "text": "According to Bahdanau et al. (2015), we sort the development and test the sentences by length and show BLEU values for each trash can in Figure 5. The proposed bidirectional tree encoder outperforms the sequential NMT system and the tree GRU encoder in all lengths. The improvements are greater for sentences longer than 20 words, and the biggest improvement is for sentences longer than 50 words. This provides some evidence of the importance of syntactic information for long sentences."}, {"heading": "5 Related Work", "text": "Recently, many studies have focused on the use of explicit syntactic tree structures to learn sentence representations for various sentence classification tasks. For example, Teng and Zhang (2016) and Kokkinos and Potamianos (2017) extend the bottom-up model to a bidirectional model for classification tasks by using tree LSTMs with head flexicalization or tree GRUs. We draw on some of these ideas and apply them to machine translation. We use representation derived from tree structures to enhance the original sequential model, and use this syntactic information during the generation phase. In NMT systems, the attention model (Bahdanau et al., 2015) becomes a critical part of the decoder model. Cohn et al. (2016) and Feng et al. (2016) extend the attention model to include structural distortions from word-based alignment models to integrate more deeply into the network."}, {"heading": "6 Conclusion", "text": "Our experiments have shown that a top-down encoder is a useful extension to the original bottom-up tree encoder (Eriguchi et al., 2016); and incorporating syntactic structure information into the decoder can better control translation. Our analysis suggests that the benefit of source-side syntax is particularly strong for long sentences. In our current work, only the structural part of the syntactic tree is used, without the labels. For future work, it will be interesting to use labels from tree nodes or use syntactic information on the landing page."}, {"heading": "Acknowledgments", "text": "The authors thank the anonymous reviewers for their valuable comments. This work is supported by the National Science Foundation of China (No. 61672277, 61300158 and 61472183). Part of Huadong Chen's contribution was made during his visit to the University of Notre Dame. His visit was supported by the joint doctoral program of the China Scholarship Council."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR 2015. http://arxiv.org/abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "Compututational Linguistics 33(2):201\u2013228. https://doi.org/10.1162/coli.2007.33.2.201.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proc. Eighth Workshop on Syntax, Semantics and Struc-", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Incorporating structural alignment biases into an attentional neural translation model", "author": ["Trevor Cohn", "Cong Duy Vu Hoang", "Ekaterina Vymolova", "Kaisheng Yao", "Chris Dyer", "Gholamreza Haffari."], "venue": "Proc. NAACL HLT . pages 876\u2013885.", "citeRegEx": "Cohn et al\\.,? 2016", "shortCiteRegEx": "Cohn et al\\.", "year": 2016}, {"title": "Tree-to-sequence attentional neural machine translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Proc. ACL. pages 823\u2013 833. http://www.aclweb.org/anthology/P16-1078.", "citeRegEx": "Eriguchi et al\\.,? 2016", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "Improving attention modeling with implicit distortion and fertility for machine translation", "author": ["Shi Feng", "Shujie Liu", "Nan Yang", "Mu Li", "Ming Zhou", "Kenny Q. Zhu."], "venue": "Proc. COLING. pages 3082\u2013 3092. http://aclweb.org/anthology/C16-1290.", "citeRegEx": "Feng et al\\.,? 2016", "shortCiteRegEx": "Feng et al\\.", "year": 2016}, {"title": "Phrasal cohesion and statistical machine translation", "author": ["Heidi J. Fox."], "venue": "Proc. EMNLP. pages 304\u2013 3111. https://doi.org/10.3115/1118693.1118732.", "citeRegEx": "Fox.,? 2002", "shortCiteRegEx": "Fox.", "year": 2002}, {"title": "Structured attention networks", "author": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush."], "venue": "Proc. ICLR. http://arxiv.org/abs/1702.00887.", "citeRegEx": "Kim et al\\.,? 2017", "shortCiteRegEx": "Kim et al\\.", "year": 2017}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proc. NAACL HLT . pages 48\u201354. https://doi.org/10.3115/1073445.1073462.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Structural attention neural networks for improved sentiment analysis", "author": ["Filippos Kokkinos", "Alexandros Potamianos."], "venue": "Proc. EACL. pages 586\u2013591. http://www.aclweb.org/anthology/E17-2093.", "citeRegEx": "Kokkinos and Potamianos.,? 2017", "shortCiteRegEx": "Kokkinos and Potamianos.", "year": 2017}, {"title": "When are tree structures necessary for deep learning of representations? In Proc", "author": ["Jiwei Li", "Thang Luong", "Dan Jurafsky", "Eduard Hovy."], "venue": "EMNLP. pages 2304\u20132314. http://aclweb.org/anthology/D15-1278.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Treeto-string alignment template for statistical machine translation", "author": ["Yang Liu", "Qun Liu", "Shouxun Lin."], "venue": "Proc. ACL. pages 609\u2013616. https://doi.org/10.3115/1220175.1220252.", "citeRegEx": "Liu et al\\.,? 2006", "shortCiteRegEx": "Liu et al\\.", "year": 2006}, {"title": "Coverage embedding models for neural machine translation", "author": ["Haitao Mi", "Baskaran Sankaran", "Zhiguo Wang", "Abe Ittycheriah."], "venue": "Proc. EMNLP. pages 955\u2013960. https://aclweb.org/anthology/D16-1096.", "citeRegEx": "Mi et al\\.,? 2016", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proc. ACL. pages 311\u2013318. https://doi.org/10.3115/1073083.1073135.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Improved inference for unlexicalized parsing", "author": ["Slav Petrov", "Dan Klein."], "venue": "Proc. NAACL HLT . pages 404\u2013411. http://www.aclweb.org/anthology/N/N07/N071051.", "citeRegEx": "Petrov and Klein.,? 2007", "shortCiteRegEx": "Petrov and Klein.", "year": 2007}, {"title": "Does string-based neural MT learn source syntax? In Proc", "author": ["Xing Shi", "Inkit Padhi", "Kevin Knight."], "venue": "EMNLP. pages 1526\u20131534. https://aclweb.org/anthology/D16-1159.", "citeRegEx": "Shi et al\\.,? 2016", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems 27, pages 3104\u2013 3112. http://papers.nips.cc/paper/5346-sequence-", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proc. ACL-IJCNLP. pages 1556\u20131566. http://www.aclweb.org/anthology/P15-1150.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Bidirectional tree-structured LSTM with head lexicalization", "author": ["Zhiyang Teng", "Yue Zhang."], "venue": "arXiv:1611.06788. http://arxiv.org/abs/1611.06788.", "citeRegEx": "Teng and Zhang.,? 2016", "shortCiteRegEx": "Teng and Zhang.", "year": 2016}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "Proc. ACL. pages 76\u201385. http://www.aclweb.org/anthology/P16-1008.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "The penn chinese treebank: Phrase structure annotation of a large corpus", "author": ["Naiwen Xue", "Fei Xia", "Fu-dong Chiou", "Marta Palmer."], "venue": "Nat. Lang. Eng. 11(2):207\u2013238. https://doi.org/10.1017/S135132490400364X.", "citeRegEx": "Xue et al\\.,? 2005", "shortCiteRegEx": "Xue et al\\.", "year": 2005}, {"title": "A syntax-based statistical translation model", "author": ["Kenji Yamada", "Kevin Knight."], "venue": "Proc. ACL. pages 523\u2013530. https://doi.org/10.3115/1073012.1073079.", "citeRegEx": "Yamada and Knight.,? 2001", "shortCiteRegEx": "Yamada and Knight.", "year": 2001}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "CoRR abs/1212.5701. http://arxiv.org/abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Transition-based parsing of the Chinese Treebank using a global discriminative model", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Proc. IWPT . pages 162\u2013171. http://www.aclweb.org/anthology/W09-3825.", "citeRegEx": "Zhang and Clark.,? 2009", "shortCiteRegEx": "Zhang and Clark.", "year": 2009}], "referenceMentions": [{"referenceID": 17, "context": "Recently, neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015) have obtained state-of-the-art performance on many language pairs.", "startOffset": 50, "endOffset": 97}, {"referenceID": 0, "context": "Recently, neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015) have obtained state-of-the-art performance on many language pairs.", "startOffset": 50, "endOffset": 97}, {"referenceID": 16, "context": "Perhaps as evidence of this, current NMT models still suffer from syntactic errors such as attachment (Shi et al., 2016).", "startOffset": 102, "endOffset": 120}, {"referenceID": 11, "context": "Li et al. (2015) show that for tasks in which long-distance semantic dependencies matter, representations learned from recursive models using syntactic structures may be more powerful than those from sequential recurrent models.", "startOffset": 0, "endOffset": 17}, {"referenceID": 0, "context": "Although the attention model (Bahdanau et al., 2015) and the coverage model (Tu et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 20, "context": ", 2015) and the coverage model (Tu et al., 2016; Mi et al., 2016) provide effective mechanisms to control the generation of translation, these mechanisms work at the word level and cannot capture phrasal cohesion between the two languages (Fox, 2002; Kim et al.", "startOffset": 31, "endOffset": 65}, {"referenceID": 13, "context": ", 2015) and the coverage model (Tu et al., 2016; Mi et al., 2016) provide effective mechanisms to control the generation of translation, these mechanisms work at the word level and cannot capture phrasal cohesion between the two languages (Fox, 2002; Kim et al.", "startOffset": 31, "endOffset": 65}, {"referenceID": 7, "context": ", 2016) provide effective mechanisms to control the generation of translation, these mechanisms work at the word level and cannot capture phrasal cohesion between the two languages (Fox, 2002; Kim et al., 2017).", "startOffset": 181, "endOffset": 210}, {"referenceID": 8, "context": ", 2016) provide effective mechanisms to control the generation of translation, these mechanisms work at the word level and cannot capture phrasal cohesion between the two languages (Fox, 2002; Kim et al., 2017).", "startOffset": 181, "endOffset": 210}, {"referenceID": 5, "context": "3), we improve the tree encoder of Eriguchi et al. (2016) by introducing a bidirectional tree encoder.", "startOffset": 35, "endOffset": 58}, {"referenceID": 7, "context": "With this tree-coverage model, we can better guide the generation phase of translation, for example, to learn a preference for phrasal cohesion (Fox, 2002).", "startOffset": 144, "endOffset": 155}, {"referenceID": 19, "context": "4), we incorporate source syntactic tree structure into the attention model via an extension of the coverage model of Tu et al. (2016). With this tree-coverage model, we can better guide the generation phase of translation, for example, to learn a preference for phrasal cohesion (Fox, 2002).", "startOffset": 118, "endOffset": 135}, {"referenceID": 5, "context": "90 BLEU over a stronger NMT system with a Tree-LSTM encoder (Eriguchi et al., 2016) and a coverage model (Tu et al.", "startOffset": 60, "endOffset": 83}, {"referenceID": 20, "context": ", 2016) and a coverage model (Tu et al., 2016).", "startOffset": 29, "endOffset": 46}, {"referenceID": 0, "context": "Most NMT systems follow the encoder-decoder framework with attention, first proposed by Bahdanau et al. (2015). Given a source sentence x = x1 \u00b7 \u00b7 \u00b7 xi \u00b7 \u00b7 \u00b7 xI and a target sentence y = y1 \u00b7 \u00b7 \u00b7 y j \u00b7 \u00b7 \u00b7 yJ , NMT aims to directly model the translation probability:", "startOffset": 88, "endOffset": 111}, {"referenceID": 0, "context": "Following Bahdanau et al. (2015), we use a bidirectional gated recurrent unit (GRU) (Cho et al.", "startOffset": 10, "endOffset": 33}, {"referenceID": 2, "context": "where si is the i-th source word\u2019s word embedding, and GRU is a gated recurrent unit; see the paper by Cho et al. (2014b) for a definition.", "startOffset": 103, "endOffset": 122}, {"referenceID": 22, "context": "Although syntax has shown its effectiveness in non-neural statistical machine translation (SMT) systems (Yamada and Knight, 2001; Koehn et al., 2003; Liu et al., 2006; Chiang, 2007), most proposed NMT models (a notable exception being that of Eriguchi et al.", "startOffset": 104, "endOffset": 181}, {"referenceID": 9, "context": "Although syntax has shown its effectiveness in non-neural statistical machine translation (SMT) systems (Yamada and Knight, 2001; Koehn et al., 2003; Liu et al., 2006; Chiang, 2007), most proposed NMT models (a notable exception being that of Eriguchi et al.", "startOffset": 104, "endOffset": 181}, {"referenceID": 12, "context": "Although syntax has shown its effectiveness in non-neural statistical machine translation (SMT) systems (Yamada and Knight, 2001; Koehn et al., 2003; Liu et al., 2006; Chiang, 2007), most proposed NMT models (a notable exception being that of Eriguchi et al.", "startOffset": 104, "endOffset": 181}, {"referenceID": 1, "context": "Although syntax has shown its effectiveness in non-neural statistical machine translation (SMT) systems (Yamada and Knight, 2001; Koehn et al., 2003; Liu et al., 2006; Chiang, 2007), most proposed NMT models (a notable exception being that of Eriguchi et al.", "startOffset": 104, "endOffset": 181}, {"referenceID": 1, "context": ", 2006; Chiang, 2007), most proposed NMT models (a notable exception being that of Eriguchi et al. (2016)) process a sentence only as a sequence of words, and do not explicitly exploit the inherent structure of natural language sentences.", "startOffset": 8, "endOffset": 106}, {"referenceID": 21, "context": "(2016) use HPSG trees, we use phrase-structure trees as in the Penn Chinese Treebank (Xue et al., 2005).", "startOffset": 85, "endOffset": 103}, {"referenceID": 5, "context": "Like Eriguchi et al. (2016), we currently focus on source side syntactic trees, which can be computed prior to translation.", "startOffset": 5, "endOffset": 28}, {"referenceID": 5, "context": "Like Eriguchi et al. (2016), we currently focus on source side syntactic trees, which can be computed prior to translation. Whereas Eriguchi et al. (2016) use HPSG trees, we use phrase-structure trees as in the Penn Chinese Treebank (Xue et al.", "startOffset": 5, "endOffset": 155}, {"referenceID": 18, "context": "We first describe tree encoders (Tai et al., 2015; Eriguchi et al., 2016), and then discuss our improvements.", "startOffset": 32, "endOffset": 73}, {"referenceID": 5, "context": "We first describe tree encoders (Tai et al., 2015; Eriguchi et al., 2016), and then discuss our improvements.", "startOffset": 32, "endOffset": 73}, {"referenceID": 5, "context": "Following Eriguchi et al. (2016), we build a tree encoder on top of the sequential encoder (as shown in Figure 3(a)).", "startOffset": 10, "endOffset": 33}, {"referenceID": 18, "context": "where f (\u00b7) is a nonlinear function, originally a Tree-LSTM (Tai et al., 2015; Eriguchi et al., 2016).", "startOffset": 60, "endOffset": 101}, {"referenceID": 5, "context": "where f (\u00b7) is a nonlinear function, originally a Tree-LSTM (Tai et al., 2015; Eriguchi et al., 2016).", "startOffset": 60, "endOffset": 101}, {"referenceID": 18, "context": "Kokkinos and Potamianos (2017) propose a similar bidirectional Tree-GRU for sentiment analysis, which differs from ours in several respects: in the bottom-up encoder, we use separate reset/update gates for left and right children, analogous to Tree-LSTMs (Tai et al., 2015); in the topdown encoder, we use separate weights for left and right children.", "startOffset": 255, "endOffset": 273}, {"referenceID": 4, "context": "Inspired by the approaches of Cohn et al. (2016), Feng et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 4, "context": "Inspired by the approaches of Cohn et al. (2016), Feng et al. (2016), Tu et al.", "startOffset": 30, "endOffset": 69}, {"referenceID": 4, "context": "Inspired by the approaches of Cohn et al. (2016), Feng et al. (2016), Tu et al. (2016) and Mi et al.", "startOffset": 30, "endOffset": 87}, {"referenceID": 4, "context": "Inspired by the approaches of Cohn et al. (2016), Feng et al. (2016), Tu et al. (2016) and Mi et al. (2016), we propose to use prior knowledge to control the attention mechanism.", "startOffset": 30, "endOffset": 108}, {"referenceID": 20, "context": "In particular, we build our model on top of the word coverage model proposed by Tu et al. (2016), which alleviate the problems of over-translation and under-translation (failing to translate part of a sentence).", "startOffset": 80, "endOffset": 97}, {"referenceID": 15, "context": "parse the Chinese sentences with the Berkeley Parser5 (Petrov and Klein, 2007) and binarize the resulting trees following Zhang and Clark (2009).", "startOffset": 54, "endOffset": 78}, {"referenceID": 15, "context": "parse the Chinese sentences with the Berkeley Parser5 (Petrov and Klein, 2007) and binarize the resulting trees following Zhang and Clark (2009). The English side of the corpora is lowercased and tokenized.", "startOffset": 55, "endOffset": 145}, {"referenceID": 0, "context": "\u2022 NMT: the standard attentional NMT model (Bahdanau et al., 2015).", "startOffset": 42, "endOffset": 65}, {"referenceID": 5, "context": "\u2022 Tree-LSTM: the attentional NMT model extended with the Tree-LSTM encoder (Eriguchi et al., 2016).", "startOffset": 75, "endOffset": 98}, {"referenceID": 20, "context": "\u2022 Coverage: the attentional NMT model extended with word coverage (Tu et al., 2016).", "startOffset": 66, "endOffset": 83}, {"referenceID": 20, "context": "\u201cno\u201d, \u201cword\u201d and \u201ctree\u201d in column \u201cCoverage\u201d represents the decoder part for using no coverage (standard attention), word coverage (Tu et al., 2016) and our proposed tree-coverage model, respectively.", "startOffset": 131, "endOffset": 148}, {"referenceID": 23, "context": "We use Adadelta (Zeiler, 2012) for optimization using a mini-batch size of 32.", "startOffset": 16, "endOffset": 30}, {"referenceID": 0, "context": "All other settings are the same as in Bahdanau et al. (2015).", "startOffset": 38, "endOffset": 61}, {"referenceID": 14, "context": "We use case insensitive 4-gram BLEU (Papineni et al., 2002) for evaluation, as calculated by multi-bleu.", "startOffset": 36, "endOffset": 59}, {"referenceID": 5, "context": "Table 2, row 2 confirms the finding of Eriguchi et al. (2016) that a Tree-LSTM encoder helps, and row 3 shows that our Tree-GRU encoder gets a better result (+0.", "startOffset": 39, "endOffset": 62}, {"referenceID": 19, "context": "We also compared our bidirectional tree encoder with the head-lexicalization based bidirectional tree encoder proposed by Teng and Zhang (2016), which forms the input vector for each nonleaf node by a bottom-up head propagation mechanism (Table 4, row 14\u2032).", "startOffset": 122, "endOffset": 144}, {"referenceID": 19, "context": "The bidirectional tree encoder using head-lexicalization (Bidirectional-head), proposed by (Teng and Zhang, 2016), does not work as well as our simpler bidirectional tree encoder (Bidirectional).", "startOffset": 91, "endOffset": 113}, {"referenceID": 20, "context": "Rows 5\u20138 in Table 2 show that the word coverage model of Tu et al. (2016) consistently helps when used with our proposed tree encoders, with the bidirectional tree encoder remaining the best.", "startOffset": 57, "endOffset": 74}, {"referenceID": 0, "context": "Following Bahdanau et al. (2015), we bin the development and test sentences by length and show BLEU scores for each bin in Figure 5.", "startOffset": 10, "endOffset": 33}, {"referenceID": 18, "context": "For example, Teng and Zhang (2016) and Kokkinos and Potamianos (2017) extend the bottom-up model to a bidirectional model for classification tasks, using Tree-LSTMs with head lexicalization and Tree-GRUs, respectively.", "startOffset": 13, "endOffset": 35}, {"referenceID": 10, "context": "For example, Teng and Zhang (2016) and Kokkinos and Potamianos (2017) extend the bottom-up model to a bidirectional model for classification tasks, using Tree-LSTMs with head lexicalization and Tree-GRUs, respectively.", "startOffset": 39, "endOffset": 70}, {"referenceID": 0, "context": "In NMT systems, the attention model (Bahdanau et al., 2015) becomes a crucial part of the", "startOffset": 36, "endOffset": 59}, {"referenceID": 4, "context": "Cohn et al. (2016) and Feng et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 4, "context": "Cohn et al. (2016) and Feng et al. (2016) extend the attentional model to include structural biases from word based alignment models.", "startOffset": 0, "endOffset": 42}, {"referenceID": 4, "context": "Cohn et al. (2016) and Feng et al. (2016) extend the attentional model to include structural biases from word based alignment models. Kim et al. (2017) incorporate richer structural distributions within deep networks to extend the attention model.", "startOffset": 0, "endOffset": 152}, {"referenceID": 5, "context": "Our experiments have demonstrated that a top-down encoder is a useful enhancement for the original bottom-up tree encoder (Eriguchi et al., 2016); and incorporating syntactic structure information into the decoder can better control the translation.", "startOffset": 122, "endOffset": 145}], "year": 2017, "abstractText": "Most neural machine translation (NMT) models are based on the sequential encoder-decoder framework, which makes no use of syntactic information. In this paper, we improve this model by explicitly incorporating source-side syntactic trees. More specifically, we propose (1) a bidirectional tree encoder which learns both sequential and tree structured representations; (2) a tree-coverage model that lets the attention depend on the source-side syntax. Experiments on Chinese-English translation demonstrate that our proposed models outperform the sequential attentional model as well as a stronger baseline with a bottom-up tree encoder and word coverage.1", "creator": "LaTeX with hyperref package"}}}