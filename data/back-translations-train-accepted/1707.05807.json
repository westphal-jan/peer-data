{"id": "1707.05807", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jul-2017", "title": "Improving Gibbs Sampler Scan Quality with DoGS", "abstract": "The pairwise influence matrix of Dobrushin has long been used as an analytical tool to bound the rate of convergence of Gibbs sampling. In this work, we use Dobrushin influence as the basis of a practical tool to certify and efficiently improve the quality of a discrete Gibbs sampler. Our Dobrushin-optimized Gibbs samplers (DoGS) offer customized variable selection orders for a given sampling budget and variable subset of interest, explicit bounds on total variation distance to stationarity, and certifiable improvements over the standard systematic and uniform random scan Gibbs samplers. In our experiments with joint image segmentation and object recognition, Markov chain Monte Carlo maximum likelihood estimation, and Ising model inference, DoGS consistently deliver higher-quality inferences with significantly smaller sampling budgets than standard Gibbs samplers.", "histories": [["v1", "Tue, 18 Jul 2017 18:17:55 GMT  (8434kb,D)", "http://arxiv.org/abs/1707.05807v1", "ICML 2017"]], "COMMENTS": "ICML 2017", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.PR stat.ME", "authors": ["ioannis mitliagkas", "lester w mackey"], "accepted": true, "id": "1707.05807"}, "pdf": {"name": "1707.05807.pdf", "metadata": {"source": "CRF", "title": "Improving Gibbs Sampler Scan Quality with DoGS", "authors": ["IOANNIS MITLIAGKAS", "LESTER MACKEY"], "emails": ["imit@stanford.edu,", "lmackey@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "The Gibbs sampler sampler sampler in the series of variable variables with same frequency that we know it is not. [GG84], Gibbs sampler sampler sampler sampler sampler sampler sampler sampler sampler is conditional simulation: individual variables are successively simulated from the univariate conditionals of a multivariate target distribution [Gey91], and bayesian inference [Lun + 00].The hallmark of all Gibbs sampler sampler sampler is conditional simulation: individual variables are successively variable variable are scan be from the univariate conditionals multivariate target distribution. The principal measure of freedom is the scan, the order in the variable variables are sampled [He + 16] variable variable variable variable variable variable variable variable variable variable variable. While it is common variable variable scan, moving through each variable in the series, or a uniform random variable variable variable variable variable variable variable variable variable variable variable variable variable variable variable variable."}, {"heading": "2 Gibbs sampling and total variation", "text": "Consider a target distribution \u03c0 on a finite p-dimensional state space, X P. Our subsequent goal is to approximate expectations - averages, moments, marginals and more complex function averages, E\u03c0 [f (X)] = \u2211 x-X p \u03c0 (x) f (x) - below \u03c0, but we assume that both exact calculation and direct sampling from \u03c0 are prohibitive due to the large number of states. Markov chain Monte Carlo (MCMC) algorithms try to circumvent this inconsistency by simulating a sequence of random vectors X0, X1,..., XT-X p from tractable distributions, so that expectations via XT are close to expectations below \u03c0."}, {"heading": "2.1 Gibbs sampling", "text": "Algorithm 1 summarizes the specific recipe used by the Gibbs sampler [GG84], a leading MCMC algorithm that simulates individual variables successively on the basis of their tractable conditional distributions. Gibbs samplers \"main degree of freedom is the scan, the sequence of p-dimensional probability vectors q1,..., qT to determine the probability of re-capturing each variable in each round of Gibbs sampling. Typically, one chooses between the uniform random scanner qt = (1 / p,., 1 / p) for all t, selecting variable indexes at random in each round and the systematic scan, qt = e (t mod p) + 1 for each t that cycles repeatedly through each variable in sequence. Non-uniform scans are known to lead to better approximations [LWK95; LC06], where the need for practical sampler evaluation and sampler enhancement of the sampler-1 is possible."}, {"heading": "2.2 Total variation", "text": "Let's show the distribution of the T-step, Xt, of a Gibbs sampler. The quality of a T-step Gibbs sampler and its scan is typically measured in terms of total variation (TV) between \u03c0T and the target distribution \u03c0: Definition 1. Total variation distance between the probability measures \u00b5 and \u03bd is the maximum difference in expectations across all [0, 1] -rated functions. We see television as a limit to the bias of a large class of Gibbs sampler expectations; however, note that television does not control the variance of these expectations."}, {"heading": "2.3 Marginal and weighted total variation", "text": "While we typically scan all p variables in the Gibbs sampling process, it is common for some variables to be of greater interest than others. For example, when modeling a large particle system, behavior in the local region of the system is of interest in principle. In these cases, it is more obvious to consider a marginal total variation that measures the discrepancy in expectation of only these variables of interest. Definition 2 (Marginal total variation): The marginal total variation between probability measurements \u00b5 and a subset of variables S \u00b2 [p] is the maximum difference in expectations for all [0, 1] -weighted functions of X \u00b2 -S, the limitation of X to the coordinates in S: marginal total variance between lipid and PI to a subset of variables S \u00b2 [1]."}, {"heading": "3 Measuring scan quality with Dobrushin variation", "text": "Since direct calculation of total variation is typically prohibitive, we will define an efficiently calculated upper limit for the weighted total variation of definition 4. Our design is inspired by the Gibbs-Sampler convergence analysis of Dobrushin and Shlosman [DS85].The first step in Dobrushin's approach is to control total variation with respect to coupled random vectors, (Xt, Yt) T = 0, where Xt has the distribution, the tth step of the Gibbs samplers, and Y t follows the target distribution. For each such coupling, we can define the marginal coupling probability pt, i, P (Xti 6 = Y ti ti) ti. The following problem, a generalization of the results in [DS85; Hay06], shows that weighted total variation is controlled by these marginal coupling probabilities."}, {"heading": "4 Improving scan quality with DoGS", "text": "Next, we present an efficient algorithm to improve the quality of all Gibbs sampler scans by minimizing the Dobrushin variation. We refer to the resulting customized Gibbs samplers as Dobrushin-optimized Gibbs samplers, or DoGS for short. Algorithm 2 optimizes the Dobrushin variation using coordinate lineage, with the selection distribution qt serving as the coordinate for each time step. Since the Dobrushin variation in each Qt is linear, each coordinate optimization (in the absence of bindings) selects a degenerated distribution, a single coordinate that results in a completely deterministic scan. If m \u2264 p is tied to the size of the Markov ceiling of each variable, then our forward algorithm runs in time O (in the absence of bindings)."}, {"heading": "4.1 Bounding influence", "text": "A key input to our algorithms is the influence matrix (1). \u2212 \u2212 Sp \u2212 \u2212 Sp \u2212 \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 Sp \u2212 \u2212 -p \u2212 Sp \u2212 Sp \u2212 Sp \u2212 \u2212 Sp \u2212 \u2212 \u2212 \u2212 Sp \u2212 Sp \u2212 \u2212 Sp \u2212 p \u2212 Sp \u2212 \u2212 \u2212 p \u2212 \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212"}, {"heading": "4.2 Related Work", "text": "In related work, Latuszynski, Roberts, and Rosenthal [LRR13] recently analyzed an abstract class of Gibbs adaptive samplers parameterized by an arbitrary scan selection rule. However, as noted in their reem. 5.13, no explicit scan selection rules were provided in this paper. However, the only concrete scan selection rules we are aware of are the Minimax Adaptive Scans with asymptotic variance or convergence rate objective functions [LC06]. Unless a substantial approximation is made, it is unclear how these procedures are to be implemented if the target distribution of interest is not Gaussian. Levine and Casella [LC06] approach these Minimax Adaptive Scans for specific mixing models by taking into account individual ad-hoc features of interest; the approach has many hyperparameters to determine the order of Taylor expansion points to read sampling points."}, {"heading": "5 Experiments", "text": "In this section, we will show how our proposed metric for scan quality and efficient optimization programs can be used to evaluate and improve Gibbs sampler scans when either the full distribution or a boundary distribution is of principal interest. For all experiments with binary MRFs, we use the model parameterization of (3) (without additional temperature parameters) and use Theorem 11 to generate the dobrushhin-bound C value. In all the following diagrams, the numbers in the legend give the best guarantee for each algorithm assigned. For space reasons, we only show one representative diagram per experiment; the analog diagrams from independent replicates of each experiment can be found in Appendix B."}, {"heading": "5.1 Evaluating and optimizing Gibbs sampler scans", "text": "In our first experiment we show how Dobrushin variation can be used to select between standard scans and how DoGS can be used to efficiently improve the standard scan quality when the total quality of variation is of interest. We remind the reader that both the scan evaluation and the scan selection are performed offline before any expensive simulation from the Gibbs sampler. Our goal is a 10-by-10 output model arranged in a two-dimensional grid, a standard model of ferromagnetism in statistical physics. In the notation of (3) we consistently extract the simple parameters {0, 1}, and the interaction parameters uniformly randomly: uniform ([0, 0.25])."}, {"heading": "5.2 End-to-end wall-clock time performance", "text": "In this experiment, we show that the use of DoGS to optimize a scan can lead to dramatic inferential speed increases, an effect that is particularly pronounced in targets with a large number of variables and in settings that require repeated scan from a slightly biased Gibbs sampler. The setting is exactly the same as in the previous experiment, except for the model size: Here, we simulate a 103 x 103 output model with a total of 1 million variables. We aim at a single marginal X1 scan with d = e1 and take a systematic scan of length T = 2 \u00d7 106 as the input scan. After measuring the dobrushin variation of the systematic scan, we use an efficient length doubling scheme to select a DoGS scan: (0) initialize T = 2; (1) perform algorithm 2 with the first T-steps of the systematic scan variance as input; (2) if the resulting DoGS scan indicates it as a DoGS scan."}, {"heading": "5.3 Accelerated MCMC maximum likelihood estimation", "text": "Next, we will demonstrate how DoGS can be used to accelerate the maximum MCMC probability estimation while providing guarantees for the quality of the parameter estimation. We will replicate the experiment of the maximum probability estimation of the Ising model of [Dom15, paragraph 6] and show how we can provide the same level of accuracy faster. Our goal is to learn the parameters of binary MRFs based on training samples with independent wheel maker entries. At each step of the MCMC-MLE, Domke will use Gibbs sampling with a uniform random scan to create an estimate of the gradient of the log probability. Our DoGS variant uses algorithm 2 with d = 1, an early stop parameter = 0.01 and a Dobrushin influence designed from the latest parameter estimation. We will specify the number of trench steps, MC steps per trench, and each of the proposed modeling cycles, and a Dobrushin influence designed from the latest parameter estimation."}, {"heading": "5.4 Customized scans for fast marginal mixing", "text": "In this section, we show how DoGS can be used to dramatically accelerate marginal conclusions while providing target-dependent guarantees. We use a 40 x 40 non-toroidal issuing model and set our feature to be the upper left variable with d = e1. Figure 4 compares guarantees for a unified random scan and a systematic scan; we also see how we can further improve the overall variation guarantees by inserting a systematic scan into Algorithm 2. Again, we see that a single run of Algorithm 2 provides the bulk of the improvement and iterated applications provide little further benefit. For the DoGS sequence, the figure also shows a histogram of the spacing of the scanned variables from the target variable X1 in the upper left corner of the grid."}, {"heading": "5.5 Targeted image segmentation and object recognition", "text": "The Verbeek and Triggs Markov Field Aspect Model (MFAM) [VT07] is a generative model for images that automatically split an image into its components (image segmentation) and label each part with its semantic object class (object recognition).For each test image k, the MFAM extracts a discrete feature descriptor from each image patch i, assigns each patch a latent object class label Xi-X, and induces the rear distribution scheme k (X | y; k).exp (i, j) space neighbor \u03c3I {Xi = Xj} (4) + \u2211 i log (i log) from each patch X, a\u03b2a, yiI {Xi = a}), via the configuration of the patch levels X. If the potts parameters of the VDP = 0, this model is reduced to the probable latent latent semantic analysis (PL01), [an image category 07] belonging to the VT07 patches."}, {"heading": "5.6 Using loose influence bounds in computations", "text": "In our previous experiments, we used theorem 11 to generate the Dobrushin influence limit C-1. In this section, we evaluate the performance of DoGS using marginal conclusions when the upper limit, C-2, used in all calculations is not narrow. Figure 8 shows that the performance of DoGS-2 is declining gracefully as the upper limit loosens from left to right. The lower row shows the quality of the empirical estimates obtained, and the results suggest that the use of a loose influence limit (in this case up to 50%) does not result in serious impairment of accuracy."}, {"heading": "6 Discussion", "text": "We introduced a practical quality measure - the Dobrushin Variation - to evaluate and compare existing Gibbs sampler scans and efficient procedures - DoGS - to develop bespoke high-speed mixing scans tailored to margins or distribution features of interest. We used DoGS for three common Gibbs sampler applications - common image segmentation and object recognition, MCMC Highest Probability Estimate and Ising Model Inference - and in each case achieved higher-quality conclusions with significantly lower sampling budgets than standard Gibbs samplers. In the future, we aim to use DoGS for additional applications in computer vision and natural language processing, extend the reach of DoGS to models that include continuous variables, and integrate DoGS into large Gibbs sampling engines."}, {"heading": "A Proofs", "text": "In the sequence (Zi) pi = 0, so that Z0, Zp, Y and Zi \u2212 1, Zi \u2212 2, Zi \u2212 2, Zi \u2212 3, Zi \u2212 3, Zi \u2212 3, Zi \u2212 3, Zi \u2212 4, Zi \u2212 4, Zi \u2212 3, Zi \u2212 3, Zi \u2212 3, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212 \u2212 4, Zi \u2212 4, Zi, Zi, Zi, 4, Zi, Zi, Zi \u2212 4, Zi, Zi \u2212 4, Zi \u2212 4, Zi, Zi \u2212 4, Zi, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212, Zi \u2212 4, Zi \u2212, Zi \u2212 4, Zi \u2212 4, Zi \u2212, Zi \u2212 4, Zi \u2212 4, Zi \u2212, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212, Zi \u2212 4, Zi \u2212 4, Zi \u2212, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212, Zi \u2212 4, Zi \u2212 4, Zi \u2212, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi, Zi \u2212 4, Zi, Zi \u2212 4, Zi, Zi \u2212 4, Zi \u2212 4, Zi \u2212 4, Zi, Zi, Zi \u2212 4, Zi, Z"}, {"heading": "B Additional experiments", "text": "We offer a few experimental results that were excluded from the main body of the paper due to space limitation.B.1 Independent replicates the evaluation and optimization of Gibbs sampler scans ExperimentFigure 9 shows the results of nine independent replicates of the \"Evaluation and Optimization of Gibbs sampler scans\" experiment from Section 5, with independently drawn non-uniform and binary potentials. B.2 Independent replicates the results of the end-to-end wall clock time performance experiment Figure 10 repeats the timing experiment from Section 2, which provides three additional independent studies. Note that if the user specifies the \"Early Stop\" parameter, Algorithm 2 terminates the scan by way once its scan is within the Dobrushin variation of the target. The long-term bias observed in the DoGS estimation of Figure 2 is a result of this user-controlled early experiment, and can be reduced by way over a smaller expectation of an experiment-wise section."}], "references": [{"title": "Maximum Likelihood Learning With Arbitrary Treewidth via Fast-Mixing Parameter Sets", "author": ["Justin Domke"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Domke.,? \\Q2015\\E", "shortCiteRegEx": "Domke.", "year": 2015}, {"title": "Constructive criterion for the uniqueness of Gibbs field", "author": ["Roland Lvovich Dobrushin", "Senya B Shlosman"], "venue": "Statistical physics and dynamical systems. Springer,", "citeRegEx": "Dobrushin and Shlosman.,? \\Q1985\\E", "shortCiteRegEx": "Dobrushin and Shlosman.", "year": 1985}, {"title": "Ensuring Rapid Mixing and Low Bias for Asynchronous Gibbs Sampling", "author": ["Christopher De Sa", "Kunle Olukotun", "Christopher R\u00e9"], "venue": null, "citeRegEx": "Sa et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sa et al\\.", "year": 2016}, {"title": "What can be sampled locally?", "author": ["Weiming Feng", "Yuxin Sun", "Yitong Yin"], "venue": null, "citeRegEx": "Feng et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2017}, {"title": "Markov chain Monte Carlo Maximum Likelihood", "author": ["C.J. Geyer"], "venue": "Computer Science and Statistics: Proc. 23rd Symp. Interface", "citeRegEx": "Geyer.,? \\Q1991\\E", "shortCiteRegEx": "Geyer.", "year": 1991}, {"title": "Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images", "author": ["Stuart Geman", "Donald Geman"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on", "citeRegEx": "Geman and Geman.,? \\Q1984\\E", "shortCiteRegEx": "Geman and Geman.", "year": 1984}, {"title": "A simple condition implying rapid mixing of single-site dynamics on spin systems", "author": ["Thomas P Hayes"], "venue": "Foundations of Computer Science,", "citeRegEx": "Hayes.,? \\Q2006\\E", "shortCiteRegEx": "Hayes.", "year": 2006}, {"title": "Scan Order in Gibbs Sampling: Models in Which it Matters and Bounds on How Much", "author": ["Bryan D He"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "He,? \\Q2016\\E", "shortCiteRegEx": "He", "year": 2016}, {"title": "Unsupervised learning by probabilistic latent semantic analysis", "author": ["Thomas Hofmann"], "venue": "Machine learning", "citeRegEx": "Hofmann.,? \\Q2001\\E", "shortCiteRegEx": "Hofmann.", "year": 2001}, {"title": "Monte Carlo methods in classical statistical physics", "author": ["Wolfhard Janke"], "venue": "Computational Many-Particle Physics. Springer,", "citeRegEx": "Janke.,? \\Q2008\\E", "shortCiteRegEx": "Janke.", "year": 2008}, {"title": "Optimizing random scan Gibbs samplers", "author": ["R.A. Levine", "G. Casella"], "venue": "Journal of Multivariate Analysis", "citeRegEx": "Levine and Casella.,? \\Q2006\\E", "shortCiteRegEx": "Levine and Casella.", "year": 2006}, {"title": "Projecting Markov random field parameters for fast mixing", "author": ["Xianghang Liu", "Justin Domke"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Liu and Domke.,? \\Q2014\\E", "shortCiteRegEx": "Liu and Domke.", "year": 2014}, {"title": "Implementing random scan Gibbs samplers", "author": ["Richard A Levine"], "venue": "Computational Statistics", "citeRegEx": "Levine,? \\Q2005\\E", "shortCiteRegEx": "Levine", "year": 2005}, {"title": "Approximate inference for the loss-calibrated Bayesian.", "author": ["Simon Lacoste-Julien", "Ferenc Husz\u00e1r", "Zoubin Ghahramani"], "venue": "AISTATS", "citeRegEx": "Lacoste.Julien et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lacoste.Julien et al\\.", "year": 2011}, {"title": "Adaptive Gibbs samplers and related MCMC methods", "author": ["Krzysztof Latuszynski", "Gareth O. Roberts", "Jeffrey S. Rosenthal"], "venue": "In: Ann. Appl. Probab", "citeRegEx": "Latuszynski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Latuszynski et al\\.", "year": 2013}, {"title": "WinBUGS-a Bayesian modelling framework: concepts, structure, and extensibility", "author": ["David J Lunn"], "venue": "In: Statistics and computing", "citeRegEx": "Lunn,? \\Q2000\\E", "shortCiteRegEx": "Lunn", "year": 2000}, {"title": "Covariance structure and convergence rate of the Gibbs sampler with various scans", "author": ["Jun S Liu", "Wing H Wong", "Augustine Kong"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological)", "citeRegEx": "Liu et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Liu et al\\.", "year": 1995}, {"title": "Comparison theorems for Gibbs measures", "author": ["Patrick Rebeschini", "Ramon van Handel"], "venue": "Journal of Statistical Physics", "citeRegEx": "Rebeschini and Handel.,? \\Q2014\\E", "shortCiteRegEx": "Rebeschini and Handel.", "year": 2014}, {"title": "Region classification with markov field aspect models", "author": ["Jakob Verbeek", "Bill Triggs"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "Verbeek and Triggs.,? \\Q2007\\E", "shortCiteRegEx": "Verbeek and Triggs.", "year": 2007}], "referenceMentions": [], "year": 2017, "abstractText": "The pairwise influence matrix of Dobrushin has long been used as an analytical tool to bound the rate of convergence of Gibbs sampling. In this work, we use Dobrushin influence as the basis of a practical tool to certify and efficiently improve the quality of a discrete Gibbs sampler. Our Dobrushin-optimized Gibbs samplers (DoGS) offer customized variable selection orders for a given sampling budget and variable subset of interest, explicit bounds on total variation distance to stationarity, and certifiable improvements over the standard systematic and uniform random scan Gibbs samplers. In our experiments with joint image segmentation and object recognition, Markov chain Monte Carlo maximum likelihood estimation, and Ising model inference, DoGS consistently deliver higher-quality inferences with significantly smaller sampling budgets than standard Gibbs samplers.", "creator": "LaTeX with hyperref package"}}}