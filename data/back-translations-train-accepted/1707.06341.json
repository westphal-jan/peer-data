{"id": "1707.06341", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jul-2017", "title": "A Sub-Character Architecture for Korean Language Processing", "abstract": "We introduce a novel sub-character architecture that exploits a unique compositional structure of the Korean language. Our method decomposes each character into a small set of primitive phonetic units called jamo letters from which character- and word-level representations are induced. The jamo letters divulge syntactic and semantic information that is difficult to access with conventional character-level units. They greatly alleviate the data sparsity problem, reducing the observation space to 1.6% of the original while increasing accuracy in our experiments. We apply our architecture to dependency parsing and achieve dramatic improvement over strong lexical baselines.", "histories": [["v1", "Thu, 20 Jul 2017 02:09:23 GMT  (30kb)", "http://arxiv.org/abs/1707.06341v1", "EMNLP 2017"], ["v2", "Fri, 21 Jul 2017 16:00:50 GMT  (30kb)", "http://arxiv.org/abs/1707.06341v2", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["karl stratos"], "accepted": true, "id": "1707.06341"}, "pdf": {"name": "1707.06341.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["stratos@ttic.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 7.06 341v 1 [cs.C L] 20 July 2 017Architecture that exploits a unique compositional structure of the Korean language. Our method splits each character into a small group of primitive phonetic units, so-called jamos, from which representations at the character and word level are generated. Jamos disseminate syntactical and semantic information that is difficult to access with conventional character level units. They greatly alleviate the problem of data sparseness, reduce the observation space to 1.6% of the original and increase accuracy in our experiments. We apply our architecture to parsing dependencies and achieve dramatic improvements over strong lexical baselines."}, {"heading": "1 Introduction", "text": "In fact, each character consists of a small, fixed set of basic phonetic units called jamo letters. Despite the important role that Jamo plays in encoding words, it has been neglected in existing modern processing algorithms. In this paper, we bridge this gap by explicitly using the subcharacter information. In particular, we perform the decomposition of Unicode on each Korean character to recover its underlying letters and construct representations at word level."}, {"heading": "2 Related Work", "text": "Our work follows the successful line of work on integrating sublexical information into neural models. Various character-based architectures have been proposed, for example, Ma and Hovy (2016) and Kim et al. (2016) use CNN on signs, whereas Lample et al. (2016) and Ballesteros et al. (2015) use bidirectional LSTMs (BiLSTMs). Both approaches have proven profitable; we use a BiLSTM-based approach. Many previous work has also considered morphems to complement lexical models (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell et al., 2016). Subcharacter models are 1 https: / / github.com / karlstratos / karlstratos radical composition is much rarer; an extreme case is considered by Gillick et al. (2016), which is text as a sequence of Korean letters."}, {"heading": "3 Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Jamo Structure of the Korean Language", "text": "In many languages, c-C is the most basic unit that makes sense. In Korean, each character consists of a small group of fixed phonetic units called jamo letters J, where | J | = 51. The jamo letters are categorized as head consonants Jh, vowels Jv, or tail consonants Jt. The composition is completely systematized. c-Jh, cv-Jv, and ct-Jt letters are such that their composition leads to c. Conversely, any ch-Jh, cv-Jv, and ct-Jt can be put together to produce a valid letter c."}, {"heading": "3.2 Jamo Architecture", "text": "The parameters associated with this layer are \u2022 Embedding el-Rd for each letter l-J \u2022 UJ, V-J, WJ-Rd \u00b7 d and bJ-RdG for each letter l-J \u2022 W-J, ct-J. We compose the letters to create a representation of c-H = tanh (UJ ech + V-J ecv + WJ ect + bJ) This representation is then associated with a drawing plane, and the result is embedded in an LSTM to create a word representation. We use an LSTM (Hochreiter and Schmidhuber, 1997) simply as a mapping word ect + bJ. This representation is then associated with a drawing plane, and the result is embedded in an LSTM."}, {"heading": "3.3 Unicode Decomposition", "text": "Our architecture requires the dynamic extraction of jam letters with any Korean character, achieved by simple Unicode manipulation. For each Korean character with a Unicode value U (c), leave U (c) = U (c) \u2212 44032 and T (c) = U (c) mod 28. Then you get the Unicode values U (ch), U (cv) and U (ct), which correspond to the header, vowel and tail consonants, by U (ch) = 1 + E (c) 588, U (c) + 0x10ffU (cv) = 1 + E (U (c) \u2212 T (c) mod 58828 + 0x1160U (ct) = 1 + T (c) + 0x11a7, with ct set to \"if T (ct) = 0."}, {"heading": "3.4 Why Use Jamo Letters?", "text": "The most obvious benefit of using jam letters is to reduce data economy by flattening the combinatorial space of Korean characters. We discuss some other explicit advantages. First, jam letters often indicate syntactic properties of words. Thus, a tail consonant can strongly imply that the word is a past verb, as in (went), (came) and (tat). Therefore, a jam plane model can more effectively identify invisible verbs than word or character plane models. Second, jam letters dictate the sound of a character. It is pronounced this way, for example, because the head consonant is pronounced with the sound g, the vowel with o and the tail consonant with t. This is clearly critical for speech recognition / synthesis and has actually been studied in the language community (Lee et al., 1994; Sakti et al., 2010)."}, {"heading": "4 Experiments", "text": "In fact, most of them will be able to move to another world in which they will be able to integrate."}, {"heading": "5 Discussion of Future Work", "text": "We have presented a natural subcharacter architecture to model the unique compositional orthography of the Korean language, which induces word / sentence representations from a small group of phonetic entities called jamos, made possible by the efficient and deterministic unicode decomposition of characters. We have focused on analyzing dependencies to demonstrate the utility of our approach as an economic and effective method of combating data scarcity. However, we believe that the true utility of this architecture in language processing will be more obvious, as jamo's letters are definitions of sounds in the language. Another potentially interesting application is informal text on the Internet. Sick-shaped words such as (abbreviation for, anonymous expression of laughter) and (abbreviation, a transcript of) are ubiquitous on social media."}, {"heading": "Acknowledgments", "text": "The author thanks Lingpeng Kong for his help in using the DyNet library, Mohammad Rasooli for his help in using the Yara parser and Karen Livescu for helpful comments."}], "references": [{"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah A. Smith."], "venue": "Proc. EMNLP.", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Compositional morphology for word representations and language modelling", "author": ["Jan A Botha", "Phil Blunsom."], "venue": "ICML, pages 1899\u20131907.", "citeRegEx": "Botha and Blunsom.,? 2014", "shortCiteRegEx": "Botha and Blunsom.", "year": 2014}, {"title": "A glossary of historical linguistics", "author": ["Lyle Campbell", "Mauricio J Mixco."], "venue": "Edinburgh University Press.", "citeRegEx": "Campbell and Mixco.,? 2007", "shortCiteRegEx": "Campbell and Mixco.", "year": 2007}, {"title": "Morphological smoothing and extrapolation of word embeddings", "author": ["Ryan Cotterell", "Hinrich Sch\u00fctze", "Jason Eisner."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, volume 1, pages 1651\u20131660.", "citeRegEx": "Cotterell et al\\.,? 2016", "shortCiteRegEx": "Cotterell et al\\.", "year": 2016}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proc. ACL.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Multilingual language processing from bytes", "author": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya."], "venue": "Proceedings of NAACL.", "citeRegEx": "Gillick et al\\.,? 2016", "shortCiteRegEx": "Gillick et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."], "venue": "Thirtieth AAAI Conference on Artificial Intelligence.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Simple and accurate dependency parsing using bidirectional lstm feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "Transactions of the Association for Computational Linguistics, 4:313\u2013327.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Neural architectures for named entity recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "KazuyaKawakami", "Chris Dyer."], "venue": "Proceedings of NAACL.", "citeRegEx": "Lample et al\\.,? 2016", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Phonemie-level, speech and natural, language integration for agglutinative languages", "author": ["Geunbae Lee", "Jong-Hyeok Lee", "Kyunghee Kim."], "venue": "GGGGGGGG 0.", "citeRegEx": "Lee et al\\.,? 1994", "shortCiteRegEx": "Lee et al\\.", "year": 1994}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Thang Luong", "Richard Socher", "Christopher D Manning."], "venue": "CoNLL, pages 104\u2013113.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "End-to-end sequence labeling via bi-directional lstm-cnns-crf", "author": ["Xuezhe Ma", "Eduard Hovy."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1064\u20131074, Berlin, Ger-", "citeRegEx": "Ma and Hovy.,? 2016", "shortCiteRegEx": "Ma and Hovy.", "year": 2016}, {"title": "Universal dependency annotation for multilingual parsing", "author": ["Ryan TMcDonald", "Joakim Nivre", "Yvonne QuirmbachBrundage", "Yoav Goldberg", "Dipanjan Das", "Kuzman Ganchev", "Keith B Hall", "Slav Petrov", "Hao Zhang", "Oscar T\u00e4ckstr\u00f6m"], "venue": null, "citeRegEx": "TMcDonald et al\\.,? \\Q2013\\E", "shortCiteRegEx": "TMcDonald et al\\.", "year": 2013}, {"title": "Dynet: The dynamic neural network toolkit", "author": ["Lingpeng Kong", "Adhiguna Kuncoro", "Gaurav Kumar", "Chaitanya Malaviya", "Paul Michel", "Yusuke Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin."], "venue": "arXiv preprint", "citeRegEx": "Kong et al\\.,? 2017", "shortCiteRegEx": "Kong et al\\.", "year": 2017}, {"title": "Yara parser: A fast and accurate dependency parser", "author": ["Rasooli", "Joel R. Tetreault."], "venue": "CoRR, abs/1503.06733.", "citeRegEx": "Rasooli and Tetreault.,? 2015", "shortCiteRegEx": "Rasooli and Tetreault.", "year": 2015}, {"title": "Korean pronunciation variation modeling with probabilistic bayesian networks", "author": ["Sakriani Sakti", "Andrew Finch", "Ryosuke Isotani", "Hisashi Kawai", "Satoshi Nakamura."], "venue": "Universal Communication Symposium (IUCS), 2010 4th International,", "citeRegEx": "Sakti et al\\.,? 2010", "shortCiteRegEx": "Sakti et al\\.", "year": 2010}, {"title": "The Korean language: Structure, use and context", "author": ["Jae Jung Song."], "venue": "Routledge.", "citeRegEx": "Song.,? 2006", "shortCiteRegEx": "Song.", "year": 2006}, {"title": "Model-based word embeddings from decompositions of count matrices", "author": ["Karl Stratos", "Michael Collins", "Daniel Hsu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Stratos et al\\.,? 2015", "shortCiteRegEx": "Stratos et al\\.", "year": 2015}, {"title": "Radical-enhanced chinese character embedding", "author": ["Yaming Sun", "Lei Lin", "Nan Yang", "Zhenzhou Ji", "Xiaolong Wang."], "venue": "International Conference on Neural Information Processing, pages 279\u2013286. Springer.", "citeRegEx": "Sun et al\\.,? 2014", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Multi-granularity chinese word embedding", "author": ["Rongchao Yin", "Quan Wang", "Rui Li", "Peng Li", "Bin Wang."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing.", "citeRegEx": "Yin et al\\.,? 2016", "shortCiteRegEx": "Yin et al\\.", "year": 2016}, {"title": "Transition-based dependency parsing with rich non-local features", "author": ["Yue Zhang", "Joakim Nivre."], "venue": "Proceedings of the 49th AnnualMeeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages", "citeRegEx": "Zhang and Nivre.,? 2011", "shortCiteRegEx": "Zhang and Nivre.", "year": 2011}], "referenceMentions": [{"referenceID": 17, "context": "Korean is generally recognized as a language isolate: that is, it has no apparent genealogical relationship with other languages (Song, 2006; Campbell and Mixco, 2007).", "startOffset": 129, "endOffset": 167}, {"referenceID": 2, "context": "Korean is generally recognized as a language isolate: that is, it has no apparent genealogical relationship with other languages (Song, 2006; Campbell and Mixco, 2007).", "startOffset": 129, "endOffset": 167}, {"referenceID": 19, "context": "The radical structure of a Chinese character does not follow any systematic process, requiring an incomplete dictionary mapping between characters and radicals to take advantage of this information (Sun et al., 2014; Yin et al., 2016).", "startOffset": 198, "endOffset": 234}, {"referenceID": 20, "context": "The radical structure of a Chinese character does not follow any systematic process, requiring an incomplete dictionary mapping between characters and radicals to take advantage of this information (Sun et al., 2014; Yin et al., 2016).", "startOffset": 198, "endOffset": 234}, {"referenceID": 8, "context": "In experiments, we decisively improve the LAS of the lexical BiLSTM parser of Kiperwasser and Goldberg (2016) from 82.", "startOffset": 78, "endOffset": 110}, {"referenceID": 11, "context": "Many previous works have also considered morphemes to augment lexical models (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell et al., 2016).", "startOffset": 77, "endOffset": 146}, {"referenceID": 1, "context": "Many previous works have also considered morphemes to augment lexical models (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell et al., 2016).", "startOffset": 77, "endOffset": 146}, {"referenceID": 3, "context": "Many previous works have also considered morphemes to augment lexical models (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell et al., 2016).", "startOffset": 77, "endOffset": 146}, {"referenceID": 6, "context": "For instance, Ma and Hovy (2016) and Kim et al.", "startOffset": 14, "endOffset": 33}, {"referenceID": 4, "context": "For instance, Ma and Hovy (2016) and Kim et al. (2016) use CNNs over characters whereas Lample et al.", "startOffset": 37, "endOffset": 55}, {"referenceID": 4, "context": "For instance, Ma and Hovy (2016) and Kim et al. (2016) use CNNs over characters whereas Lample et al. (2016) and Ballesteros et al.", "startOffset": 37, "endOffset": 109}, {"referenceID": 0, "context": "(2016) and Ballesteros et al. (2015) use bidirectional LSTMs (BiLSTMs).", "startOffset": 11, "endOffset": 37}, {"referenceID": 5, "context": "com/karlstratos/koreannet substantially rarer; an extreme case is considered by Gillick et al. (2016) who process text as a sequence of bytes.", "startOffset": 80, "endOffset": 102}, {"referenceID": 19, "context": "There exists a line of work on exploiting graphical components of Chinese characters called radicals (Sun et al., 2014; Yin et al., 2016).", "startOffset": 101, "endOffset": 137}, {"referenceID": 20, "context": "There exists a line of work on exploiting graphical components of Chinese characters called radicals (Sun et al., 2014; Yin et al., 2016).", "startOffset": 101, "endOffset": 137}, {"referenceID": 8, "context": "For completeness, we describe the entire forward pass of the transition-based BiLSTM parser of Kiperwasser and Goldberg (2016) that we use in our experiments.", "startOffset": 95, "endOffset": 127}, {"referenceID": 6, "context": "We use an LSTM (Hochreiter and Schmidhuber, 1997) simply as a mapping \u03c6 : R1 \u00d7R2 \u2192 R2 that takes an input vector x and a state vector h to output a new state vector h = \u03c6(x, h).", "startOffset": 15, "endOffset": 49}, {"referenceID": 8, "context": "Since this part is not a contribution of this paper, we refer to Kiperwasser and Goldberg (2016) for details.", "startOffset": 65, "endOffset": 97}, {"referenceID": 10, "context": "This is clearly critical for speech recognition/synthesis and indeed has been investigated in the speech community (Lee et al., 1994; Sakti et al., 2010).", "startOffset": 115, "endOffset": 153}, {"referenceID": 16, "context": "This is clearly critical for speech recognition/synthesis and indeed has been investigated in the speech community (Lee et al., 1994; Sakti et al., 2010).", "startOffset": 115, "endOffset": 153}, {"referenceID": 8, "context": ", 2017) and plug it into the BiLSTM parser of Kiperwasser and Goldberg (2016). For Korean syllable manipulation, we use the freely available toolkit by Joshua Dong.", "startOffset": 46, "endOffset": 78}, {"referenceID": 15, "context": "\u2022 Yara: A beam-search transition-based parser of Rasooli and Tetreault (2015) based on the rich non-local features in Zhang and Nivre (2011).", "startOffset": 49, "endOffset": 78}, {"referenceID": 15, "context": "\u2022 Yara: A beam-search transition-based parser of Rasooli and Tetreault (2015) based on the rich non-local features in Zhang and Nivre (2011). We use beam width 64.", "startOffset": 49, "endOffset": 141}, {"referenceID": 8, "context": "\u2022 K&G16: The basic BiLSTM parser of Kiperwasser and Goldberg (2016) without the sub-lexical architecture introduced in this work.", "startOffset": 36, "endOffset": 68}, {"referenceID": 4, "context": "Dyer15 denotes the word-level variant (Dyer et al., 2015).", "startOffset": 38, "endOffset": 57}, {"referenceID": 0, "context": "Ballesteros15 denotes the character-level variant (Ballesteros et al., 2015).", "startOffset": 50, "endOffset": 76}, {"referenceID": 18, "context": "For pre-trained word embeddings, we apply the spectral algorithm of Stratos et al. (2015) on a 2015 Korean Wikipedia dump to induce 285,933 embeddings of dimension 100.", "startOffset": 68, "endOffset": 90}], "year": 2017, "abstractText": "We introduce a novel sub-character architecture that exploits a unique compositional structure of the Korean language. Our method decomposes each character into a small set of primitive phonetic units called jamo letters from which characterand word-level representations are induced. The jamo letters divulge syntactic and semantic information that is difficult to access with conventional character-level units. They greatly alleviate the data sparsity problem, reducing the observation space to 1.6% of the original while increasing accuracy in our experiments. We apply our architecture to dependency parsing and achieve dramatic improvement over strong lexical baselines.", "creator": "LaTeX with hyperref package"}}}