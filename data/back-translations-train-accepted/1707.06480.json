{"id": "1707.06480", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jul-2017", "title": "Syllable-aware Neural Language Models: A Failure to Beat Character-aware Ones", "abstract": "Syllabification does not seem to improve word-level RNN language modeling quality when compared to character-based segmentation. However, our best syllable-aware language model, achieving performance comparable to the competitive character-aware model, has 18%-33% fewer parameters and is trained 1.2-2.2 times faster.", "histories": [["v1", "Thu, 20 Jul 2017 12:46:09 GMT  (75kb,D)", "http://arxiv.org/abs/1707.06480v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.NE stat.ML", "authors": ["zhenisbek assylbekov", "rustem takhanov", "bagdat myrzakhmetov", "jonathan washington"], "accepted": true, "id": "1707.06480"}, "pdf": {"name": "1707.06480.pdf", "metadata": {"source": "CRF", "title": "Syllable-aware Neural Language Models: A Failure to Beat Character-aware Ones", "authors": ["Zhenisbek Assylbekov", "Rustem Takhanov"], "emails": ["zhassylbekov@nu.edu.kz", "rustem.takhanov@nu.edu.kz", "bagdat.myrzakhmetov@nu.edu.kz", "@swarthmore.edu"], "sections": [{"heading": "1 Introduction", "text": "Recent advances in Neural Language Modeling (NLM) are related to sign-conscious models (Kim et al., 2016; Ling et al., 2015b; Verwimp et al., 2017). This is a promising approach, and we propose the following direction in this context: We want to ensure that the pursuit of the most fine-grained representations does not miss possible intermediate forms of segmentation, e.g. by syllables. We believe that syllables are better supported than linguistic units than individual syllables. In most languages, words can be naturally divided into syllables: ES: el par-la-men-to a-po-yo-la-en-mien-da RU: par-la-ment pod-der-z-al po-prav-ku. Based on this observation, we tried to determine whether syllable-conscious NLM has any advantages over sign-conscious NLM."}, {"heading": "2 Related Work", "text": "Much research has been done at the subword level and subword aware1, where authors make word predictions when subwords are characters (Ling et al., 2015b; Kim et al., 2016; Verwimp et al., 2017) or morphemes (Botha and Blunsom, 2014; Qiu et al., 2014; Cotterell and Schutze, 2015). However, not much work has been done at the syllable level or syllable-conscious NLM. Mikolov et al. (2012) show that language models at the subword level exceed the character level. 2 They keep the most common words intact and divide all other words into syllable-like units. Our approach differs mainly in the following aspects: We make predictions at the word level, use a more linguistically sound syllable level and look at syllable-ification algorithms, and consider a variety of more advanced neural architectures."}, {"heading": "3 Syllable-aware word embeddings", "text": "Let ES-R | S | \u00d7 dS be an embedding matrix for syllables - i.e., it is a matrix in which the sth line (referred to as s) corresponds to an embedding of the syllable s-S. Each word w-W is a sequence of its syllables (s1, s2,..., snw) and can therefore be represented as a sequence of the corresponding syllable vectors: [s1, s2,..., snw]. (1) The question is: How should we put the sequence (1) into a single vector x-RdW to create a better embedding of the word w? 3 In our case, \"better\" means better \"than a sign-conscious embedding of w via the Char CNN model by Kim et al (2016)."}, {"heading": "3.1 Recurrent sequential model (Syl-LSTM)", "text": "Since the syllables are in a sequence, it is natural to try out a recurring sequence model: ht = f (st, ht \u2212 1), h0 = 0, (2), which converts the sequence of syllable vectors (1) into a sequence of state vectors h1: nw. The same question applies to any model that splits words into a sequence of characters or other subword units. State vector hnw is assumed to contain information about the entire sequence (1) and is therefore used as word embedding for w. There is a wide variety of transformations from which f can be selected in (2); however, a recent thorough evaluation (Jozefowicz et al., 2015) shows that the LSTM (Hochreiter and Schmidhuber, 1997), with its forgetfulness initialized at 1, surpasses other popular architectures, and we have decided to use it for our experiments. We will call this model the STM-LM."}, {"heading": "3.2 Convolutional model (Syl-CNN)", "text": "Inspired by recent work on character-conscious neural language models (Kim et al., 2016), we decided to try this approach (Char-CNN) on syllables. Our case differs mainly in the following two aspects: 1. The sentence of syllables S is usually larger than the sentence of characters C, 4, and also the dimensionality dS of syllable vectors is likely to be larger than the dimensionality dC of character vectors. Both factors lead to an assignment of more parameters to syllable embeddings compared to letter embeddings. 2. On average, one word contains fewer syllables than characters, and therefore we need tighter convolutionary filters for syllables. As a result, fewer parameters are output per nesting. This means that by varying dS and the maximum width of the Convolutionary Filters L, we can still adjust the parameter budget of Kim et al. (2016) to allow a fair comparison of the models."}, {"heading": "3.3 Linear combinations", "text": "We have also considered using linear combinations of syllable vectors to represent the word embedding: x = \u2211 nwt = 1 \u03b1t (st) \u00b7 st. (3) The choice for \u03b1t is mainly motivated by the existing approaches (see below) that have proven successful for other tasks. Syl-Sum: Summing syllable vectors to obtain a word vector can be achieved by setting \u03b1t (st) = 1.4In languages with alphabetical writing systems, this approach was used by Botha and Blunsom (2014) to combine a word and its morpheme embedding into a single word vector. Syl-Avg: A simple average of syllable vectors can be achieved by setting \u03b1t (st) = 1 / nw. This approach can also be called a \"continuous bag of syllables,\" in an analogy to a CBOW model (Mikolov al et al al, as adjacent words in 2013)."}, {"heading": "3.4 Concatenation (Syl-Concat)", "text": "In this model, we simply concatenate syllable vectors (1) into a single word vector: x = [s1; s2;....; snw; 0;...; 0 n \u2212 nw] We fill x so that all word vectors have the same length n \u00b7 dS to allow batch processing, and then feed x into a stack of highway layers."}, {"heading": "4 Word-level language model", "text": "Once we have word embedding x1: k for a word sequence w1: k, we can use a linguistic RNN model at word level to generate a state sequence h1: k and then predict the next word according to the probability distributionPr (wk + 1 | w1: k) = softmax (hkW + b), with W-RdLM \u00d7 | W |, b-R | W |, and dLM being the hidden layer size of the RNN. Formation of the model is about minimizing the negative protocol probability across the corpus w1: K: \u2212 Kk = 1 log Pr (wk | w1: k \u2212 1) \u2212 \u2192 min (5) As mentioned in Section 3.1, there is a huge variety of RNN architectures to choose from. The most advanced recurring neural architectures, at the time of this writing, are recurring motorway networks (Zilly et al., 2017) and a new model that we can achieve directly by using the Kim architecture."}, {"heading": "5 Experimental Setup", "text": "We search for the best model in two steps: First, we block the LSTM architecture at word level and select the three best models under a small parameter budget (5M), and then we match the hyperparameters of these three best models under a larger budget (20M). Pre-selection: We fix dLM (hidden layer size of LSTM at word level) at 300 units per layer and perform each syllable embedding method from Section 3 of the English PTB dataset (Marcus et al., 1993) while maintaining the overall parameter budget at 5M. Architectural decisions are specified in Appendix A. Hyperparameter tuning: The hyperparameters of the three most powerful models from the preselection are then thoroughly compared to the same English PTB data by a random search according to the marginal parameter budgets based on these marginal distributions (liper parameter tuning: \u2022 dS-U (20 Dem650), 5 total modlog (HALW) and three DTB (HALT) data (three)."}, {"heading": "6 Results", "text": "The results of the preselection are shown in Table 1. All syllable-conscious models show stable results and directly show when they exceed the Char-CNN results when the budget is limited to 5M parameters. Surprisingly, a pure word-level model, 6 LSTM-Word, can also exceed the character-conscious such a budget. The three best configurations are Syl-Concat, Syl-Sum and Syl-CNN-3 (hereinafter referred to as Syl-CNN), and the tuning of their hyperparameters gives the architectures in Table 2. The results of the evaluation of these three models on small (1M tokens) and medium-sized (17M-57M tokens) datasets against Char-CNN for different languages are provided in Table 3. The models show similar performance on small data, but Char-CNN scales significantly better on medium-sized datasets."}, {"heading": "7 Conclusion", "text": "However, the use of syllable formation can reduce the total number of parameters and increase the training speed, albeit at the expense of language-dependent pre-processing. Morphological segmentation is a remarkable alternative to syllable formation: a simple morpheme-conscious model summarizing the embedding of morphemes looks promising, and its study is postponed to future work. 9M stands for morphemes."}, {"heading": "A Pre-selection", "text": "In all models with highway layers there are two of them and the nonlinear activation of each highway layer is a ReLU. LSTM-Word: dW = 108, dLM = 300. Syl-LSTM: dS = 50, dLM = 300. Syl-CNN- [L]: dS = 50, coil filter widths are [1,..., L], the corresponding coil filter depths are [c \u00b7 l] Ll = 1, dHW = c \u00b7 (1 +.. + L). Here we have L = 2, 3, 4. The corresponding values of c are chosen so that they correspond to the total parameter budget. CNN activation is tanh. Linear combinations: We give syllable vectors a higher dimensionality (compared to other models), because the resulting word vector will have the same size as syllable vectors (see (3)."}, {"heading": "B Optimization", "text": "LSTM-based models: We perform the training (5) by truncated BPTT (Werbos, 1990; Graves, 2013). We propagate for 70 time steps on DATA-S and for 35 time steps on DATA-L with stochastic gradient descent, where the learning rate is initially set to 1.0 and halved if the perplexity on validation does not decrease after one epoch. We use batch sizes of 20 for DATA-S and 100 for DATA-L. We train for 50 epochs on DATA-S and for 25 epochs on DATA-L, selecting the most powerful model on the validation platform. Parameters of the models are randomly initialized in [\u2212 0.05, 0.05] except the bias of the word level LSTM, which is initialized to 1."}, {"heading": "C Sizes and speeds", "text": "For DATA-S, Syl-Concat has 28% -33% fewer parameters than Char-CNN, and for DATA-L, the reduction is 18% -27% (see Figure 4). Training speeds are shown in Table 6. Models were implemented in TensorFlow and executed on NVIDIA Titan X (Pascal)."}, {"heading": "Acknowledgements", "text": "The work of Bagdat Myrzakhmetov was financed by the Science Committee of the Ministry of Education and Science of the Republic of Kazakhstan under the Targeted Program O.0743 (0115PK02473). The authors thank anonymous reviewers and Aibek Makazhanov for valuable feedback, Makat Tlebaliyev and Dmitriy Polynin for IT support and Yoon Kim for providing the prepared data sets."}], "references": [{"title": "Compositional morphology for word representations and language modelling", "author": ["Jan Botha", "Phil Blunsom."], "venue": "Proceedings of ICML.", "citeRegEx": "Botha and Blunsom.,? 2014", "shortCiteRegEx": "Botha and Blunsom.", "year": 2014}, {"title": "Strategies for training large vocabulary neural language models", "author": ["Wenlin Chen", "David Grangier", "Michael Auli."], "venue": "Proceedings of ACL.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Morphological word-embeddings", "author": ["Ryan Cotterell", "Hinrich Sch\u00fctze."], "venue": "Proceedings of HLTNAACL.", "citeRegEx": "Cotterell and Sch\u00fctze.,? 2015", "shortCiteRegEx": "Cotterell and Sch\u00fctze.", "year": 2015}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["Mathias Creutz", "Krista Lagus."], "venue": "ACM Transactions on Speech and Language Processing (TSLP), 4(1):3.", "citeRegEx": "Creutz and Lagus.,? 2007", "shortCiteRegEx": "Creutz and Lagus.", "year": 2007}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal", "Zoubin Ghahramani."], "venue": "Proceedings of NIPS.", "citeRegEx": "Gal and Ghahramani.,? 2016", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Efficient softmax approximation for gpus", "author": ["Edouard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou."], "venue": "arXiv preprint arXiv:1609.04309.", "citeRegEx": "Grave et al\\.,? 2016", "shortCiteRegEx": "Grave et al\\.", "year": 2016}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv preprint arXiv:1308.0850.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of ACL-IJCNLP.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings of ICML.", "citeRegEx": "Jozefowicz et al\\.,? 2015", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."], "venue": "Proceedings of AAAI.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Word Hy-phen-a-tion by Com-put-er", "author": ["Franklin Mark Liang."], "venue": "Citeseer.", "citeRegEx": "Liang.,? 1983", "shortCiteRegEx": "Liang.", "year": 1983}, {"title": "Not all contexts are created equal: Better word representations with variable attention", "author": ["Wang Ling", "Lin Chu-Cheng", "Yulia Tsvetkov", "Silvio Amir."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Ling et al\\.,? 2015a", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis."], "venue": "Proceedings of", "citeRegEx": "Ling et al\\.,? 2015b", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Subword language modeling with neural networks", "author": ["Tom\u00e1\u0161 Mikolov", "Ilya Sutskever", "Anoop Deoras", "HaiSon Le", "Stefan Kombrink", "Jan Cernocky."], "venue": "preprint (http://www. fit. vutbr. cz/imikolov/rnnlm/char. pdf).", "citeRegEx": "Mikolov et al\\.,? 2012", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio."], "venue": "Proceedings of AISTATS.", "citeRegEx": "Morin and Bengio.,? 2005", "shortCiteRegEx": "Morin and Bengio.", "year": 2005}, {"title": "Co-learning of word representations and morpheme representations", "author": ["Siyu Qiu", "Qing Cui", "Jiang Bian", "Bin Gao", "Tie-Yan Liu."], "venue": "Proceedings of COLING.", "citeRegEx": "Qiu et al\\.,? 2014", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of ACL.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Training very deep networks", "author": ["Rupesh K Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "Proceedings of NIPS.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "From characters to words to in between: Do we capture morphology", "author": ["Clara Vania", "Adam Lopez"], "venue": "In Proceedings of ACL", "citeRegEx": "Vania and Lopez.,? \\Q2017\\E", "shortCiteRegEx": "Vania and Lopez.", "year": 2017}, {"title": "Character-word lstm language models", "author": ["Lyan Verwimp", "Joris Pelemans", "Patrick Wambacq"], "venue": "In Proceedings of EACL", "citeRegEx": "Verwimp et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Verwimp et al\\.", "year": 2017}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos."], "venue": "Proceedings of the IEEE, 78(10).", "citeRegEx": "Werbos.,? 1990", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Recurrent highway networks", "author": ["Julian Georg Zilly", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of ICML", "citeRegEx": "Zilly et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zilly et al\\.", "year": 2017}, {"title": "Neural architecture search with reinforcement learning", "author": ["Barret Zoph", "Quoc V Le."], "venue": "Proceedings of ICLR.", "citeRegEx": "Zoph and Le.,? 2017", "shortCiteRegEx": "Zoph and Le.", "year": 2017}], "referenceMentions": [{"referenceID": 10, "context": "Recent advances in neural language modeling (NLM) are connected with character-aware models (Kim et al., 2016; Ling et al., 2015b; Verwimp et al., 2017).", "startOffset": 92, "endOffset": 152}, {"referenceID": 13, "context": "Recent advances in neural language modeling (NLM) are connected with character-aware models (Kim et al., 2016; Ling et al., 2015b; Verwimp et al., 2017).", "startOffset": 92, "endOffset": 152}, {"referenceID": 23, "context": "Recent advances in neural language modeling (NLM) are connected with character-aware models (Kim et al., 2016; Ling et al., 2015b; Verwimp et al., 2017).", "startOffset": 92, "endOffset": 152}, {"referenceID": 13, "context": "Much research has been done on subword-level and subword-aware1 neural language modeling when subwords are characters (Ling et al., 2015b; Kim et al., 2016; Verwimp et al., 2017) or morphemes (Botha and Blunsom, 2014; Qiu et al.", "startOffset": 118, "endOffset": 178}, {"referenceID": 10, "context": "Much research has been done on subword-level and subword-aware1 neural language modeling when subwords are characters (Ling et al., 2015b; Kim et al., 2016; Verwimp et al., 2017) or morphemes (Botha and Blunsom, 2014; Qiu et al.", "startOffset": 118, "endOffset": 178}, {"referenceID": 23, "context": "Much research has been done on subword-level and subword-aware1 neural language modeling when subwords are characters (Ling et al., 2015b; Kim et al., 2016; Verwimp et al., 2017) or morphemes (Botha and Blunsom, 2014; Qiu et al.", "startOffset": 118, "endOffset": 178}, {"referenceID": 0, "context": ", 2017) or morphemes (Botha and Blunsom, 2014; Qiu et al., 2014; Cotterell and Sch\u00fctze, 2015).", "startOffset": 21, "endOffset": 93}, {"referenceID": 18, "context": ", 2017) or morphemes (Botha and Blunsom, 2014; Qiu et al., 2014; Cotterell and Sch\u00fctze, 2015).", "startOffset": 21, "endOffset": 93}, {"referenceID": 2, "context": ", 2017) or morphemes (Botha and Blunsom, 2014; Qiu et al., 2014; Cotterell and Sch\u00fctze, 2015).", "startOffset": 21, "endOffset": 93}, {"referenceID": 22, "context": "We have recently come across a concurrent paper (Vania and Lopez, 2017) where the authors systematically compare different subword units (characters, character trigrams, BPE (Sennrich et al.", "startOffset": 48, "endOffset": 71}, {"referenceID": 19, "context": "We have recently come across a concurrent paper (Vania and Lopez, 2017) where the authors systematically compare different subword units (characters, character trigrams, BPE (Sennrich et al., 2016), morphemes) and different representation models (CNN, Bi-LSTM, summation) on languages with various morphological typology.", "startOffset": 174, "endOffset": 197}, {"referenceID": 0, "context": ", 2017) or morphemes (Botha and Blunsom, 2014; Qiu et al., 2014; Cotterell and Sch\u00fctze, 2015). However, not much work has been done on syllable-level or syllable-aware NLM. Mikolov et al. (2012) show that subword-level language models outperform character-level ones.", "startOffset": 22, "endOffset": 195}, {"referenceID": 10, "context": "The question is: How shall we pack the sequence (1) into a single vector x \u2208 RdW to produce a better embedding of the word w?3 In our case \u201cbetter\u201d means \u201cbetter than a character-aware embedding of w via the Char-CNN model of Kim et al. (2016)\u201d.", "startOffset": 226, "endOffset": 244}, {"referenceID": 9, "context": "There is a big variety of transformations from which one can choose f in (2); however, a recent thorough evaluation (Jozefowicz et al., 2015) shows that the LSTM (Hochreiter and Schmidhuber, 1997) with its forget bias initialized to 1 outperforms other popular architectures on almost all tasks, and we decided to use it for our experiments.", "startOffset": 116, "endOffset": 141}, {"referenceID": 7, "context": ", 2015) shows that the LSTM (Hochreiter and Schmidhuber, 1997) with its forget bias initialized to 1 outperforms other popular architectures on almost all tasks, and we decided to use it for our experiments.", "startOffset": 28, "endOffset": 62}, {"referenceID": 10, "context": "Inspired by recent work on character-aware neural language models (Kim et al., 2016) we decided to try this approach (Char-CNN) on syllables.", "startOffset": 66, "endOffset": 84}, {"referenceID": 21, "context": "Like in Char-CNN, our syllable-aware model, which is referred to as Syl-CNN-[L], utilizes maxpooling and highway layers (Srivastava et al., 2015) to model interactions between the syllables.", "startOffset": 120, "endOffset": 145}, {"referenceID": 10, "context": "Inspired by recent work on character-aware neural language models (Kim et al., 2016) we decided to try this approach (Char-CNN) on syllables. Our case differs mainly in the following two aspects: 1. The set of syllables S is usually bigger than the set of characters C,4 and also the dimensionality dS of syllable vectors is expected to be greater than the dimensionality dC of character vectors. Both of these factors result in allocating more parameters on syllable embeddings compared to character embeddings. 2. On average a word contains fewer syllables than characters, and therefore we need narrower convolutional filters for syllables. This results in spending fewer parameters per convolution. This means that by varying dS and the maximum width of convolutional filters L we can still fit the parameter budget of Kim et al. (2016) to allow fair comparison of the models.", "startOffset": 67, "endOffset": 841}, {"referenceID": 15, "context": "This can be also called a \u201ccontinuous bag of syllables\u201d in an analogy to a CBOW model (Mikolov et al., 2013), where vectors of neighboring words are averaged to get a word embedding of the current word.", "startOffset": 86, "endOffset": 108}, {"referenceID": 0, "context": "This approach was used by Botha and Blunsom (2014) to combine a word and its morpheme embeddings into a single word vector.", "startOffset": 26, "endOffset": 51}, {"referenceID": 12, "context": "This approach is motivated by recent work on using an attention mechanism in the CBOW model (Ling et al., 2015a).", "startOffset": 92, "endOffset": 112}, {"referenceID": 26, "context": "The most advanced recurrent neural architectures, at the time of this writing, are recurrent highway networks (Zilly et al., 2017) and a novel model which was obtained through a neural architecture search with reinforcement learning (Zoph and Le, 2017).", "startOffset": 110, "endOffset": 130}, {"referenceID": 27, "context": ", 2017) and a novel model which was obtained through a neural architecture search with reinforcement learning (Zoph and Le, 2017).", "startOffset": 110, "endOffset": 129}, {"referenceID": 4, "context": "These models can be spiced up with the most recent regularization techniques for RNNs (Gal and Ghahramani, 2016) to reach state-of-theart.", "startOffset": 86, "endOffset": 112}, {"referenceID": 4, "context": "These models can be spiced up with the most recent regularization techniques for RNNs (Gal and Ghahramani, 2016) to reach state-of-theart. However, to make our results directly comparable to those of Kim et al. (2016) we select a two-layer LSTM and regularize it as in Zaremba et al.", "startOffset": 87, "endOffset": 218}, {"referenceID": 4, "context": "These models can be spiced up with the most recent regularization techniques for RNNs (Gal and Ghahramani, 2016) to reach state-of-theart. However, to make our results directly comparable to those of Kim et al. (2016) we select a two-layer LSTM and regularize it as in Zaremba et al. (2014).", "startOffset": 87, "endOffset": 291}, {"referenceID": 14, "context": "Pre-selection: We fix dLM (hidden layer size of the word-level LSTM) at 300 units per layer and run each syllable-aware word embedding method from Section 3 on the English PTB data set (Marcus et al., 1993), keeping the total parameter budget at 5M.", "startOffset": 185, "endOffset": 206}, {"referenceID": 13, "context": "Pre-selection: We fix dLM (hidden layer size of the word-level LSTM) at 300 units per layer and run each syllable-aware word embedding method from Section 3 on the English PTB data set (Marcus et al., 1993), keeping the total parameter budget at 5M. The architectural choices are specified in Appendix A. Hyperparameter tuning: The hyperparameters of the three best-performing models from the preselection step are then thoroughly tuned on the same English PTB data through a random search according to the marginal distributions: \u2022 dS \u223c U(20, 650),5 \u2022 log(dHW) \u223c U(log(160), log(2000)), \u2022 log(dLM) \u223c U(log(300), log(2000)), with the restriction dS < dLM.", "startOffset": 186, "endOffset": 586}, {"referenceID": 13, "context": "Pre-selection: We fix dLM (hidden layer size of the word-level LSTM) at 300 units per layer and run each syllable-aware word embedding method from Section 3 on the English PTB data set (Marcus et al., 1993), keeping the total parameter budget at 5M. The architectural choices are specified in Appendix A. Hyperparameter tuning: The hyperparameters of the three best-performing models from the preselection step are then thoroughly tuned on the same English PTB data through a random search according to the marginal distributions: \u2022 dS \u223c U(20, 650),5 \u2022 log(dHW) \u223c U(log(160), log(2000)), \u2022 log(dLM) \u223c U(log(300), log(2000)), with the restriction dS < dLM.", "startOffset": 186, "endOffset": 623}, {"referenceID": 10, "context": "The total parameter budget is kept at 20M to allow for easy comparison to the results of Kim et al. (2016). Then these three best models (with their hyperparameters tuned on PTB) are trained and evaluated on small- (DATAS) and medium-sized (DATA-L) data sets in six languages.", "startOffset": 89, "endOffset": 107}, {"referenceID": 11, "context": "Since these are not always available for lessresourced languages, we decided to utilize Liang\u2019s widely-used hyphenation algorithm (Liang, 1983).", "startOffset": 130, "endOffset": 143}, {"referenceID": 24, "context": "Optimizaton is performed in almost the same way as in the work of Zaremba et al. (2014). See Appendix B for details.", "startOffset": 66, "endOffset": 88}, {"referenceID": 26, "context": "1)? To find out whether this was the case we replaced the LSTM by a Variational RHN (Zilly et al., 2017), and that resulted in a significant reduction of perplexities on PTB for both Char-CNN and Syl-Concat (Table 5).", "startOffset": 84, "endOffset": 104}, {"referenceID": 24, "context": "LSTM-based models: We perform the training (5) by truncated BPTT (Werbos, 1990; Graves, 2013).", "startOffset": 65, "endOffset": 93}, {"referenceID": 6, "context": "LSTM-based models: We perform the training (5) by truncated BPTT (Werbos, 1990; Graves, 2013).", "startOffset": 65, "endOffset": 93}, {"referenceID": 20, "context": "For regularization we use dropout (Srivastava et al., 2014) with probability 0.", "startOffset": 34, "endOffset": 59}, {"referenceID": 25, "context": "These choices were guided by previous work on wordlevel language modeling with LSTMs (Zaremba et al., 2014).", "startOffset": 85, "endOffset": 107}, {"referenceID": 8, "context": "To speed up training on DATA-L we use a sampled softmax (Jean et al., 2015) with the number of samples equal to 20% of the vocabulary size (Chen et al.", "startOffset": 56, "endOffset": 75}, {"referenceID": 1, "context": ", 2015) with the number of samples equal to 20% of the vocabulary size (Chen et al., 2016).", "startOffset": 71, "endOffset": 90}, {"referenceID": 17, "context": "(2016) used a hierarchical softmax (Morin and Bengio, 2005) for the same purpose, a recent study (Grave et al.", "startOffset": 35, "endOffset": 59}, {"referenceID": 5, "context": "(2016) used a hierarchical softmax (Morin and Bengio, 2005) for the same purpose, a recent study (Grave et al., 2016) shows that it is outperformed by sampled softmax on the Europarl corpus, from which DATA-L was derived (Botha and Blunsom, 2014).", "startOffset": 97, "endOffset": 117}, {"referenceID": 0, "context": ", 2016) shows that it is outperformed by sampled softmax on the Europarl corpus, from which DATA-L was derived (Botha and Blunsom, 2014).", "startOffset": 111, "endOffset": 136}, {"referenceID": 0, "context": ", 2015) with the number of samples equal to 20% of the vocabulary size (Chen et al., 2016). Although Kim et al. (2016) used a hierarchical softmax (Morin and Bengio, 2005) for the same purpose, a recent study (Grave et al.", "startOffset": 72, "endOffset": 119}, {"referenceID": 0, "context": ", 2016) shows that it is outperformed by sampled softmax on the Europarl corpus, from which DATA-L was derived (Botha and Blunsom, 2014). RHN-based models are optimized as in Zilly et al. (2017), except that we unrolled the networks for 70 time steps in truncated BPTT, and dropout rates were chosen to be as follows: 0.", "startOffset": 112, "endOffset": 195}], "year": 2017, "abstractText": "Syllabification does not seem to improve word-level RNN language modeling quality when compared to characterbased segmentation. However, our best syllable-aware language model, achieving performance comparable to the competitive character-aware model, has 18%\u201333% fewer parameters and is trained 1.2\u20132.2 times faster.", "creator": "LaTeX with hyperref package"}}}