{"id": "1707.06556", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jul-2017", "title": "High-risk learning: acquiring new word vectors from tiny data", "abstract": "Distributional semantics models are known to struggle with small data. It is generally accepted that in order to learn 'a good vector' for a word, a model must have sufficient examples of its usage. This contradicts the fact that humans can guess the meaning of a word from a few occurrences only. In this paper, we show that a neural language model such as Word2Vec only necessitates minor modifications to its standard architecture to learn new terms from tiny data, using background knowledge from a previously learnt semantic space. We test our model on word definitions and on a nonce task involving 2-6 sentences' worth of context, showing a large increase in performance over state-of-the-art models on the definitional task.", "histories": [["v1", "Thu, 20 Jul 2017 15:02:14 GMT  (21kb,D)", "http://arxiv.org/abs/1707.06556v1", "Accepted as short paper at EMNLP 2017"]], "COMMENTS": "Accepted as short paper at EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["aur\u00e9lie herbelot", "marco baroni"], "accepted": true, "id": "1707.06556"}, "pdf": {"name": "1707.06556.pdf", "metadata": {"source": "CRF", "title": "High-risk learning: acquiring new word vectors from tiny data", "authors": ["Aur\u00e9lie Herbelot", "Marco Baroni"], "emails": ["aurelie.herbelot@cantab.net", "marco.baroni@unitn.it"], "sections": [{"heading": "1 Introduction", "text": "That is, for a DS model to learn a word vector, that word must have seen a sufficient number of times, in sharp contrast to the human ability to perform fast mapping, i.e. the acquisition of a new concept from a single exposure to information (Lake et al., 2011; Trueswell et al., 2013; Lake et al., 2016). There are at least two reasons for wanting to acquire vectors from very small data. First, some words are simply rare in businesses, but potentially crucial for some applications (consider, for example, the processing of text with technical terminology)."}, {"heading": "2 Task description", "text": "We want to simulate the process in which a competent speaker encounters a new word in familiar contexts. That is, we assume that an existing vocabulary (i.e. a previously trained semantic space) that can help the speaker grasp the meaning of the new word once, we first use two sentences that each contain only one word (e.g. albedo, insulin), then extract the first sentence from the Wikipedia page that matches each target title (e.g. insulin is a peptide hormone produced by beta cells in the pancreas.) and tokenise that this sentence is the spacy toolkit.2 Every occurrence of the target is replaced by a slot."}, {"heading": "3 Baseline models", "text": "We are testing two state-of-the-art models that were calculated by calculating a frequency: a) Word2Vec (W2V) in its Gensim4 implementation, which allows us to update a previous semantic space; b) the additive model of Lazaridou et al. (2017), with a background space of W2V. We point out that both models allow a certain type of incrementality, which allows a context to be created at a time (or several, if mini-batches are implemented) in which we gradient descent after each new input. The network weights in the input corresponding to the generated word vectors can be verified in any timeframe. 5 Moreover, how it performs the ability to stop and restart the training can be found in distributional semantic models (see e.g. QasemiZadeh et al, 2017)."}, {"heading": "4 Nonce2Vec", "text": "Our system, Nonce2Vec (N2V), 6 modifies W2V in the following way: Initialization: Since the addition provides a good approximation of the word nonce, we initialize our vectors to the sum of all known words in the context sentences (see \u00a7 3). Note that this is not necessarily equivalent to the pure sum model, since subsampling takes care of frequent word deletions in this setup (as opposed to a stop word list). In practice, this means that the initialized vectors are of slightly lower quality than those from the sum model. Parameter selection: We experiment with higher learning rates coupled with larger window sizes. That is, the model should run the risk of exceeding a minimal error; b) greedily consider irrelevant contexts to increase its chance of learning something. We mitigate these risks by selective training and appropriate parameter modification (see below).Window resizing: We suppress the window size when we change the window size."}, {"heading": "5 Experiments", "text": "We have a set of values for the learning rate ([0.5, 0.8, 2, 5, 20]) and the subsampling rate ([500, 1000, 10000]). Here, considering the size of the words to consider, the minimum frequency for a word is very high. Best performance is a window of 15 words, 3 negative samplers, a subsampling rate of 10000, 10000, 10000."}, {"heading": "6 Conclusion", "text": "We have proposed Nonce2Vec, an architecture inspired by Word2Vec to learn new words from tiny data. It requires a high-risk strategy that combines an increased learning rate with greedy context processing. However, the system's particularly good performance on definitions makes us confident that it is possible to build a unique, unified algorithm for learning word meanings from arbitrary amounts of data. However, the less impressive performance on naturally occurring sentences suggests that an ideal system should modulate its learning depending on the informativeness of a context set, that is, taking risks \"at the right time.\" As emphasized in the introduction, Nonce2Vec is designed with the goal of being an essential component of an incremental concept learning architecture. In order to validate our system as a suitable, generic solution for word learning, we will need to test it on various data sizes, starting with the type of low to medium frequency terms, such as the word set found in the 2013 Luet."}, {"heading": "Acknowledgments", "text": "We would like to thank Katrin Erk for inspiring conversations about tiny data and fast mapping and Raffaella Bernardi and Sandro Pezzelle for comments on an early draft of this paper. We would also like to thank the anonymous reviewers for their time and valuable comments. We appreciate the ERC 2011 Starting Independent Research Grant No. 283554 (COMPOSES), which was also funded by the European Union's research and innovation programme Horizon 2020 under the Marie Sk\u0142odowska-Curie Funding Agreement No. 751250."}], "references": [{"title": "The WaCky wide web: a collection of very large linguistically processed", "author": ["Marco Baroni", "Silvia Bernardini", "Adriano Ferraresi", "Eros Zanchetta"], "venue": null, "citeRegEx": "Baroni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2009}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin."], "venue": "Journal of machine learning research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam Khanh Tran", "Marco Baroni."], "venue": "Journal of Artificial Intelligence Research, 49(1):1\u201347.", "citeRegEx": "Bruni et al\\.,? 2014", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Computational learning theory and language acquisition", "author": ["Alexander Clark", "Shalom Lappin."], "venue": "Ruth M Kempson, Tim Fernando, and Nicholas Asher, editors, Philosophy of linguistics, pages 445\u2013 475. Elsevier.", "citeRegEx": "Clark and Lappin.,? 2010", "shortCiteRegEx": "Clark and Lappin.", "year": 2010}, {"title": "Vector space models of lexical meaning", "author": ["Stephen Clark."], "venue": "Shalom Lappin and Chris Fox, editors, Handbook of Contemporary Semantics \u2013 second edition. Wiley-Blackwell.", "citeRegEx": "Clark.,? 2012", "shortCiteRegEx": "Clark.", "year": 2012}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Vector space models of word meaning and phrase meaning: a survey", "author": ["Katrin Erk."], "venue": "Language and Linguistics Compass, 6:635\u2013653.", "citeRegEx": "Erk.,? 2012", "shortCiteRegEx": "Erk.", "year": 2012}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Obtaining a Better Understanding of Distributional Models of German Derivational Morphology", "author": ["Max Kisselew", "Sebastian Pad\u00f3", "Alexis Palmer", "Jan \u0160najder."], "venue": "Proceedings of the 11th International Conference on Computational Semantics", "citeRegEx": "Kisselew et al\\.,? 2015", "shortCiteRegEx": "Kisselew et al\\.", "year": 2015}, {"title": "One-shot learning of simple visual concepts", "author": ["Brenden M Lake", "Ruslan Salakhutdinov", "Jason Gross", "Joshua B Tenenbaum."], "venue": "Proceedings of the 33rd Annual Meeting of the Cognitive Science Society (CogSci2012), Boston, MA.", "citeRegEx": "Lake et al\\.,? 2011", "shortCiteRegEx": "Lake et al\\.", "year": 2011}, {"title": "Building machines that learn and think like people", "author": ["Brenden M. Lake", "Tomer D. Ullman", "Joshua B. Tenenbaum", "Samuel J. Gershman."], "venue": "arxiv, abs/1604.00289.", "citeRegEx": "Lake et al\\.,? 2016", "shortCiteRegEx": "Lake et al\\.", "year": 2016}, {"title": "Multimodal word meaning induction from minimal exposure to natural text", "author": ["Angeliki Lazaridou", "Marco Marelli", "Marco Baroni."], "venue": "Cognitive Science, 41(S4):677\u2013705.", "citeRegEx": "Lazaridou et al\\.,? 2017", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2017}, {"title": "Compositional-ly Derived Representations of Morphologically Complex", "author": ["Angeliki Lazaridou", "Marco Marelli", "Roberto Zamparelli", "Marco Baroni"], "venue": null, "citeRegEx": "Lazaridou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2013}, {"title": "Better Word Representations with Recursive Neural Networks for Morphology", "author": ["Thang Luong", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 17th Conference on Computational Natural Language Learning (CoNLL2013),", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Ad-", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Predictability of distributional semantics in derivational word formation", "author": ["Sebastian Pad\u00f3", "Aur\u00e9lie Herbelot", "Max Kisselew", "Jan \u0160najder."], "venue": "Proceedings of the 26th International Conference on Computational Linguistics (COLING2016), Osaka,", "citeRegEx": "Pad\u00f3 et al\\.,? 2016", "shortCiteRegEx": "Pad\u00f3 et al\\.", "year": 2016}, {"title": "Non-Negative Randomized Word Embeddings", "author": ["Behrang QasemiZadeh", "Laura Kallmeyer", "Aur\u00e9lie Herbelot."], "venue": "Proceedings of Traitement automatique des langues naturelles (TALN2017), Orl\u00e9ans, France.", "citeRegEx": "QasemiZadeh et al\\.,? 2017", "shortCiteRegEx": "QasemiZadeh et al\\.", "year": 2017}, {"title": "Propose but verify: Fast mapping meets cross-situational word learning", "author": ["John C Trueswell", "Tamara Nicol Medina", "Alon Hafri", "Lila R Gleitman."], "venue": "Cognitive psychology, 66(1):126\u2013156.", "citeRegEx": "Trueswell et al\\.,? 2013", "shortCiteRegEx": "Trueswell et al\\.", "year": 2013}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D Turney", "Patrick Pantel."], "venue": "Journal of artificial intelligence research, 37:141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Training an adaptive dialogue policy for interactive learning of visually grounded word meanings", "author": ["Yanchao Yu", "Arash Eshghi", "Oliver Lemon."], "venue": "Proceedings of the 17th Annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL2016), pages", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Distributional models (DS: Turney and Pantel (2010); Clark (2012); Erk (2012)), and in particular neural network approaches (Bengio et al., 2003; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013), do not fare well in the absence of large corpora.", "startOffset": 124, "endOffset": 211}, {"referenceID": 5, "context": "Distributional models (DS: Turney and Pantel (2010); Clark (2012); Erk (2012)), and in particular neural network approaches (Bengio et al., 2003; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013), do not fare well in the absence of large corpora.", "startOffset": 124, "endOffset": 211}, {"referenceID": 7, "context": "Distributional models (DS: Turney and Pantel (2010); Clark (2012); Erk (2012)), and in particular neural network approaches (Bengio et al., 2003; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013), do not fare well in the absence of large corpora.", "startOffset": 124, "endOffset": 211}, {"referenceID": 14, "context": "Distributional models (DS: Turney and Pantel (2010); Clark (2012); Erk (2012)), and in particular neural network approaches (Bengio et al., 2003; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013), do not fare well in the absence of large corpora.", "startOffset": 124, "endOffset": 211}, {"referenceID": 9, "context": "the acquisition of a new concept from a single exposure to information (Lake et al., 2011; Trueswell et al., 2013; Lake et al., 2016).", "startOffset": 71, "endOffset": 133}, {"referenceID": 17, "context": "the acquisition of a new concept from a single exposure to information (Lake et al., 2011; Trueswell et al., 2013; Lake et al., 2016).", "startOffset": 71, "endOffset": 133}, {"referenceID": 10, "context": "the acquisition of a new concept from a single exposure to information (Lake et al., 2011; Trueswell et al., 2013; Lake et al., 2016).", "startOffset": 71, "endOffset": 133}, {"referenceID": 9, "context": "Distributional models (DS: Turney and Pantel (2010); Clark (2012); Erk (2012)), and in particular neural network approaches (Bengio et al.", "startOffset": 27, "endOffset": 52}, {"referenceID": 3, "context": "Distributional models (DS: Turney and Pantel (2010); Clark (2012); Erk (2012)), and in particular neural network approaches (Bengio et al.", "startOffset": 53, "endOffset": 66}, {"referenceID": 3, "context": "Distributional models (DS: Turney and Pantel (2010); Clark (2012); Erk (2012)), and in particular neural network approaches (Bengio et al.", "startOffset": 53, "endOffset": 78}, {"referenceID": 12, "context": "One way to deal with data sparsity issues when learning word vectors is to use morphological structure as a way to overcome the lack of primary data (Lazaridou et al., 2013; Luong et al., 2013; Kisselew et al., 2015; Pad\u00f3 et al., 2016).", "startOffset": 149, "endOffset": 235}, {"referenceID": 13, "context": "One way to deal with data sparsity issues when learning word vectors is to use morphological structure as a way to overcome the lack of primary data (Lazaridou et al., 2013; Luong et al., 2013; Kisselew et al., 2015; Pad\u00f3 et al., 2016).", "startOffset": 149, "endOffset": 235}, {"referenceID": 8, "context": "One way to deal with data sparsity issues when learning word vectors is to use morphological structure as a way to overcome the lack of primary data (Lazaridou et al., 2013; Luong et al., 2013; Kisselew et al., 2015; Pad\u00f3 et al., 2016).", "startOffset": 149, "endOffset": 235}, {"referenceID": 15, "context": "One way to deal with data sparsity issues when learning word vectors is to use morphological structure as a way to overcome the lack of primary data (Lazaridou et al., 2013; Luong et al., 2013; Kisselew et al., 2015; Pad\u00f3 et al., 2016).", "startOffset": 149, "endOffset": 235}, {"referenceID": 11, "context": "Another strand of research has been started by Lazaridou et al. (2017), who recently showed that by using simple sum-", "startOffset": 47, "endOffset": 71}, {"referenceID": 14, "context": "tecture of neural language models like Word2Vec (Mikolov et al., 2013) is actually suited to modelling words from a few occurrences only, providing minor adjustments are made to the model itself and its parameters.", "startOffset": 48, "endOffset": 70}, {"referenceID": 0, "context": "a length over 10 words), corresponding to targets which are frequent enough in the UkWaC corpus (Baroni et al. (2009), minimum frequency of 200).", "startOffset": 97, "endOffset": 118}, {"referenceID": 11, "context": "The Chimera dataset Our second dataset is the \u2018Chimera\u2019 dataset of (Lazaridou et al., 2017).", "startOffset": 67, "endOffset": 91}, {"referenceID": 11, "context": "We test two state-of-the art systems: a) Word2Vec (W2V) in its Gensim4 implementation, allowing for update of a prior semantic space; b) the additive model of Lazaridou et al. (2017), using a background space from W2V.", "startOffset": 159, "endOffset": 183}, {"referenceID": 2, "context": "We verify the quality of this space by calculating correlation with the similarity ratings in the MEN dataset (Bruni et al., 2014).", "startOffset": 110, "endOffset": 130}, {"referenceID": 11, "context": "Additive model Lazaridou et al. (2017) use a simple additive model, which sums the vectors of the context words of the nonce, taking as context the entire sentence where the target occurs.", "startOffset": 15, "endOffset": 39}, {"referenceID": 3, "context": "It also meets with general considerations on language acquisition, which accounts for the ability of young children to learn from limited \u2018primary linguistic data\u2019 by restricting explanatory models to those that provide such efficiency (Clark and Lappin, 2010).", "startOffset": 236, "endOffset": 260}, {"referenceID": 13, "context": "the Rare Words dataset (Luong et al., 2013), to", "startOffset": 23, "endOffset": 43}], "year": 2017, "abstractText": "Distributional semantics models are known to struggle with small data. It is generally accepted that in order to learn \u2018a good vector\u2019 for a word, a model must have sufficient examples of its usage. This contradicts the fact that humans can guess the meaning of a word from a few occurrences only. In this paper, we show that a neural language model such as Word2Vec only necessitates minor modifications to its standard architecture to learn new terms from tiny data, using background knowledge from a previously learnt semantic space. We test our model on word definitions and on a nonce task involving 2-6 sentences\u2019 worth of context, showing a large increase in performance over state-of-the-art models on the definitional task.", "creator": "LaTeX with hyperref package"}}}