{"id": "1707.06756", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jul-2017", "title": "An Infinite Hidden Markov Model With Similarity-Biased Transitions", "abstract": "We describe a generalization of the Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) which is able to encode prior information that state transitions are more likely between \"nearby\" states. This is accomplished by defining a similarity function on the state space and scaling transition probabilities by pair-wise similarities, thereby inducing correlations among the transition distributions. We present an augmented data representation of the model as a Markov Jump Process in which: (1) some jump attempts fail, and (2) the probability of success is proportional to the similarity between the source and destination states. This augmentation restores conditional conjugacy and admits a simple Gibbs sampler. We evaluate the model and inference method on a speaker diarization task and a \"harmonic parsing\" task using four-part chorale data, as well as on several synthetic datasets, achieving favorable comparisons to existing models.", "histories": [["v1", "Fri, 21 Jul 2017 04:39:10 GMT  (441kb,D)", "http://arxiv.org/abs/1707.06756v1", "16 pages, 4 figures, accepted to ICML 2017, includes supplemental appendix"]], "COMMENTS": "16 pages, 4 figures, accepted to ICML 2017, includes supplemental appendix", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG stat.ME", "authors": ["colin reimer dawson", "chaofan huang", "clayton t morrison"], "accepted": true, "id": "1707.06756"}, "pdf": {"name": "1707.06756.pdf", "metadata": {"source": "CRF", "title": "An Infinite Hidden Markov Model With Similarity-Biased Transitions", "authors": ["Colin Reimer Dawson", "Chaofan Huang", "Clayton T. Morrison"], "emails": ["<cdawson@oberlin.edu>."], "sections": [{"heading": "1. Introduction and Background", "text": "The Hierarchical Dirichlet Method Hides Markov Model (HDP-HMM) (Beal \u03b2 al., 2001; Teh et al., 2006) is a Bavarian model for time series data that generalizes the conventional hidden Markov model to allow for a numerically infinite state space. The hierarchical structure ensures that, despite the infinite state space, a common set of target states can be reached with a positive probability from each source state. HDP-HMM can be characterized by the following generative process. Each state indexed by j has parameters drawn from a baseline, H. A high-level sequence of State1 Oberlin College, Oberlin, OH. The University of Arizona, Tucson, USA. Correspondence to: Colin Dawson < cdawson @ edu >. Code: http: / colindawson.net / hdp-hmm-Procelt.edings.edu"}, {"heading": "2. An HDP-HMM With Local Transitions", "text": "We would like to add to the transition model the concept of a transition to a \"near\" state, in which transitions between states j and j \u00b2 are a priori more likely as they are \"near\" in a space of similarity. To achieve this, we first consider an alternative construction of transition distributions based on the representation of the DP's Normalized Gamma Process (Ishwaran & Zarepour, 2002; Ferguson, 1973)."}, {"heading": "2.1. A Normalized Gamma Process representation of the HDP-HMM", "text": "The Dirichlet process is an instance of a normalized, totally random measure (Kingman, 1967; Ferguson, 1973) that can be defined as G = 2. (6) The Dirichlet process is a measure that assigns sets 1 if they contain sets and 0 otherwise, and that is subject to the condition that p = 1 \u03b2k = 1 and 0 < p = 3 < p = 4 < p = 5. It has been shown (Ferguson, 1973; Paisley et al., 2012; Favaro et al., 2013) that the normalization constant T is almost certainly positive and finite and that G is distributed as DP with the basic measurement G0 = 1 \u03b2kp < p \u00b2 p. If we draw \u03b2 = (\u03b21, \u03b22,..) from the GEM (hieronym) a random process."}, {"heading": "2.2. Promoting \u201cLocal\u201d Transitions", "text": "In the previous HDP, the lines of the transition matrix are conditionally independent. We want to soften this assumption to incorporate the possible foreknowledge that certain pairs of states are in some sense \"close\" and therefore more likely than others to generate large transition weights between them (in both directions); that is, transitions are likely to be \"local.\" We achieve this by connecting each latent state j to a place \"j in any space\" and introducing a \"similarity function\": \"[0, 1] and scaling each element\" p \"(\" j, \"j\"). For example, we want a (possibly asymmetrical) divergence function d: [0, 1] and a phrase \"j\" (j, \"j\" j) = exp {\u2212 d, \"\" j, \"and\" jzt, \"so that transitions are less likely than two states."}, {"heading": "2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with \u201cFailed\u201d Transitions", "text": "In this section, we define a stochastic process that we call the Markov jump process with failed transitions (MJP-FT), from which we obtain the HDP-HMM-LT by going over some of those defined by the HDP-HMM-LT.Let \u03b2, \u03c0, and Tj, j = 1, 2, as in the last section. Let us consider a continuous Markov process over the states j = 1, 2, and assume that if the process makes a leap towards the state, we will not make the next leap that aims at the state + 1 that aims in time."}, {"heading": "2.4. Sticky and Semi-Markov Generalizations", "text": "We note that the local transition property of the HDPHMM-LT can be combined with the Sticky property of the Sticky HDP-HMM (Fox et al., 2008) or the non-geometric duration distributions of the HDP-HSMM (Johnson & Willsky, 2013) to add additional prior weight for self-transitions. In the first case, no changes in inference are required; one can simply add the additional mass to the shape parameter of the gamma before the \u03c0jj and use the same auxiliary variable method used by Fox et al. to distinguish \"Sticky\" from \"regular\" self-transitions. In the case of Semi-Markov, we can fix the diagonal elements of the gamma to zero and allow Dt observations to be transmitted, i.e. according to a state-specific duration distribution, and the latent state sequence using a suitable Semi-Markov message transmission algorithm (WillPHMM-Johnson & LT, as opposed to the original DP distribution is not required)."}, {"heading": "2.5. Obtaining the Factorial HMM as a Limiting Case", "text": "An environment in which a local transition property is desirable is the case in which the latent states encode several hidden features at a time t as a vector of categories. Such problems are often modeled using factorial HMMs (Ghahramaniet al., 1997). In fact, the HDP-HMM-LT returns the factorial HMM in the boundary as \u03b1, \u03b3 \u2192 \u221e, with each line of \u03c0 likely to be uniform, so that the dynamics are completely controlled by \u03c6. If A (d) is the transition matrix for chain d, the definition \u03c6 ('j,' j \u2032) = exp \u2212 d ('j,' j \u2032) with asymmetric \"divergences\" d ('j,' j \u2032) = \u2212 improved d log (A (d) 'jd,' j \u2032 d) results in the factorial transition model."}, {"heading": "2.6. An Infinite Factorial HDP-HMM-LT", "text": "Nonparametric extensions of the factorial HMM, such as the infinite factorial hidden Markov model (Gael et al., 2009) and the infinite factorial dynamic model (Valera et al., 2015), have been developed in recent years using the Indian buffet process (Ghahramani & Griffiths, 2005) as the previous state. Conceptually, it would be easy to combine the IBP state beforehand with the similarity bias of the LT model, provided the chosen similarity function is uniformly limited above the space of the infinite length of binary vectors (take for example \u03c6 (u, v) as an exposed negative hamming distance between u and v). Since the number of differences between two drawings from the IBP is finite with probability 1, this results in a reasonable similarity metric."}, {"heading": "3. Inference", "text": "We are developing a Gibbs scanning algorithm based on the MJPFT representation described in paragraph A, which supplements the data with the permanent variables u, the failed jump attempt counting matrix Q, and additional auxiliary variables. In this representation, the transition matrix is not represented directly, but is a deterministic function of the incalculable transition \"rate\" matrix, \u03c0, and the similarity matrix, \u03c6. The complete set of variables is divided into blocks: {\u03b3, \u03b1, \u03b2, \u03c0}, {z, u, Q, \u0432}, and {\u0432}, whereby \u0439} represents a series of auxiliary variables introduced below, \u03b8 represents the emission parameters (which can be further blocked depending on the specific choice of the model), and \u0442 represents additional parameters, such as free parameters of the similarity function, and any hyperparameters of the emission distribution."}, {"heading": "3.1. Sampling Transition Parameters and Hyperparameters", "text": "The common posterior distribution with a finite dirichlet distribution with a J component, where J is greater than J components, which we expect to have a weak boundary approximation to the HDP (Johnson & Willsky, 2013), which we can approximate to the HDP (Johnson & Willsky, 2013); the common posterior distribution with a finite dirichlet distribution with a J component, where J is greater than J component, which we expect to approximate the finite product with the HDP (Johnson & Willsky, 2013); the common posterior distribution with a finite dirichlet distribution with a J component, in the J component, where we can have a weak boundary approximation to the HDP (Johnson & Willsky, 2013), we approximate the faltering process with a finite dirichlet distribution with a J component, where J component is greater than J component, where we have a weak boundary approximation to the HDP (Johnson & Willsky, 2013)."}, {"heading": "3.2. Sampling z and the auxiliary variables", "text": "The common conditional distribution of these variables is directly defined by the generative model: p (D) = p (z) p (u | z) p (Q | u) p (M | z, Q) p (r | M) p (w | M)) Since we condition the transition matrix, we can scan the entire sequence z together with the forward-oriented algorithm, as in an ordinary HMM. Since we scan the labels together, this step requires O (TJ2) compilation by iteration, which represents the bottleneck of the inference algorithm for relatively large T or J (other updates are constant in T or in J). Once we have done this, we can scan scan scan u, Q, M, r and w from their forward distributions. It is also possible to apply a variant that adapts to the cost of Van, but not to the cost of 2008."}, {"heading": "3.3. Sampling state and emission parameters", "text": "Depending on the application \"the locations may or may not depend on the emission parameters \u03b8. If not, the sampling \u03b8 is unchanged due to z compared to the HDP-HMM. There is no universal method for sampling\" or for sampling \u03b8 in the dependent case due to the dependence on the form of \u03c6 and the emission model, but in the experiments below specific cases are illustrated."}, {"heading": "4. Experiments", "text": "The parameter space for the hidden states, the corresponding previous H on \u03b8 and the similarity function \u03c6 are application-specific; here we consider two cases: The first is a task of loudspeaker diarization, in which each state consists of a finite two-dimensional vector whose entries indicate which loudspeakers are currently speaking. In this experiment, the state vectors determine both the paired similarities and partially the emission distributions using a linear Gaussian model. In the second experiment, the data consist of Bach chorales, and the latents can be regarded as harmonic relationships. Here, the components of the states that regulate similarities are modeled independently of the emission distributions, which are categorical distributions over four-voices."}, {"heading": "4.1. Cocktail Party", "text": "The underlying signal consisted of D = 16 loudspeaker channels recorded at each of T = 2000 time steps, with the resulting T \u00b7 D signal matrix denoted by D = 12 microphone channels via a weight matrix, W. The 16 loudspeakers were divided into 4 talk groups of 4 each, in which the speakers speak alternately during a conversation (see fig. 2). In such a task, there are naively 2D possible states (here, 65536). However, due to the talk grouping, if at most one speaker speaks in a conversation at a given time, the state space is limited, with only c (sc + 1) states possible in which sc is the number of speakers in conversation c (in this case sc 4, for a total of 625 possible states)."}, {"heading": "4.2. Synthetic Data Without Local Transitions", "text": "We have generated data directly from the ordinary HDP-HMM, which was used as a health test in the cocktail experiment to examine the performance of the LT model without similarity bias. Results are shown in Fig. 3. If the \u03bb parameter is large, the LT model performs worse on these data than the non-LT model, but the \u03bb parameter settles near zero, as the model learns that local transitions are not more likely. If \u03bb = 0, the HDP-HMM-LT is an ordinary HDP-HMM. However, the LT model does not draw entirely the same conclusions as the non-LT model; in particular, the \u03b1 concentration parameter is larger."}, {"heading": "4.3. Bach Chorales", "text": "To test a version of the HDP-HMM-LT model, in which the components of the latent state of governing similarity have nothing to do with the emission distributions, we used our model to learn unattended \"grammar\" from a corpus of Bach chorales. However, the data was a corpus of 217 four-part major chorales by J.S. Bach from music214, of which 200 were randomly selected as training corpus, with the other 17 used as a test set to evaluate surprising (marginal log similarity per observation) by the trained models. All chorals were transposed into C major, and each different four-part chord (ordered with voices) was encoded as a single integrator. In total, there were 3307 different chord types and 20401 chord types in the 217 choral types, with 3165 types and 18818 tokens in the 200 choral oratories."}, {"heading": "5. Discussion", "text": "We have defined a new probabilistic model that generalizes the HDP-HMM by generalizing state spatial geometry by a similarity between the \"near\" states (\"local\" transitions). By introducing an augmented data representation, which we call the Markov jump process with misdirected transitions (MJP-FT), we obtain a Gibbs sampling algorithm that simplifies inference in both LT and ordinary HDMM."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was partially funded by the DARPA scholarship W911NF14-1-0395 under the Big Mechanism Program and the DARPA scholarship W911NF-16-1-0567 under the Communicating with Computers Program."}, {"heading": "A. Details of the Markov Jump Process with Failed Transitions Representation", "text": "We can gain a stronger intuition as well as simplify the subsequent conclusion by defining the HDP-HMM-LT as a continuous time jump process in which some of the attempts to jump from one state to another fail, and in which the probability of failure increases depending on the \"distance\" between states. Let's be defined as in the last section, and allow the number of attempts to jump from one state to another to be defined as in the \"normal\" HDP-HMM representation. That is, the probability of failure of the \"distance\" (24), the number of attempts to be defined in the last section, and allow the number of attempts to be defined in the \"normal\" HDP-HMM \"(24), the number of attempts to be defined (24), the number of attempts in the last section, and the number of attempts to be defined in the last section as being defined in the last section."}, {"heading": "C. Derivation of \u03b7 update in the Cocktail Party and Synthetic Data Experiments", "text": "In principle, we can have any distribution via binary vectors, but we assume that it can be included in D independent coordinate variants."}, {"heading": "D. Derivation of HMC update for ` in the Bach Chorale Experiment", "text": "In the previous version of the model, \"j\" was a binary state vector on which both the similarities and the emission distribution are based. In this case, we define the latent places \"j = (\" j1, \"jD) as places in RD, independent of the emission distributions, so that during the inference they are informed exclusively by the transitions. We define the latent places\" j = (\"j1,\" jD) as places in RD where djj \"is the Euclidean distance between\" j \"and\" j \"; that is, d2jj,\" \"jj,\" jj, \"(\" j \") as places where we are continuous places, we use Hamlitonian Monte Carlo (Duane et al., 1987; Neal et al., 2011) to test them together."}], "references": [{"title": "The infinite hidden Markov model", "author": ["Beal", "Matthew J", "Ghahramani", "Zoubin", "Rasmussen", "Carl E"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Beal et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Beal et al\\.", "year": 2001}, {"title": "Hybrid monte carlo", "author": ["Duane", "Simon", "Kennedy", "Anthony D", "Pendleton", "Brian J", "Roweth", "Duncan"], "venue": "Physics letters B,", "citeRegEx": "Duane et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Duane et al\\.", "year": 1987}, {"title": "Population genetics theory \u2013 the past and the future", "author": ["Ewens", "Warren John"], "venue": "In Mathematical and Statistical Developments of Evolutionary Theory,", "citeRegEx": "Ewens and John.,? \\Q1990\\E", "shortCiteRegEx": "Ewens and John.", "year": 1990}, {"title": "MCMC for normalized random measure mixture models", "author": ["Favaro", "Stefano", "Teh", "Yee Whye"], "venue": "Statistical Science,", "citeRegEx": "Favaro et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Favaro et al\\.", "year": 2013}, {"title": "A Bayesian analysis of some nonparametric problems", "author": ["Ferguson", "Thomas S"], "venue": "The annals of statistics,", "citeRegEx": "Ferguson and S.,? \\Q1973\\E", "shortCiteRegEx": "Ferguson and S.", "year": 1973}, {"title": "An HDP-HMM for systems with state persistence", "author": ["Fox", "Emily B", "Sudderth", "Erik B", "Jordan", "Michael I", "Willsky", "Alan S"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Fox et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2008}, {"title": "The infinite factorial hidden Markov model", "author": ["Gael", "Jurgen V", "Teh", "Yee W", "Ghahramani", "Zoubin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gael et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gael et al\\.", "year": 2009}, {"title": "Infinite latent feature models and the Indian buffet process", "author": ["Ghahramani", "Zoubin", "Griffiths", "Thomas L"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Ghahramani et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ghahramani et al\\.", "year": 2005}, {"title": "Factorial hidden Markov models", "author": ["Ghahramani", "Zoubin", "Jordan", "Michael I", "Smyth", "Padhraic"], "venue": "Machine learning,", "citeRegEx": "Ghahramani et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Ghahramani et al\\.", "year": 1997}, {"title": "Adaptive rejection sampling for Gibbs sampling", "author": ["Gilks", "Walter R", "Wild", "Pascal"], "venue": "Applied Statistics,", "citeRegEx": "Gilks et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Gilks et al\\.", "year": 1992}, {"title": "Exact and approximate sum representations for the Dirichlet process", "author": ["Ishwaran", "Hemant", "Zarepour", "Mahmoud"], "venue": "Canadian Journal of Statistics,", "citeRegEx": "Ishwaran et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ishwaran et al\\.", "year": 2002}, {"title": "Bayesian nonparametric hidden semi-Markov models", "author": ["Johnson", "Matthew J", "Willsky", "Alan S"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Johnson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2013}, {"title": "Completely random measures", "author": ["Kingman", "John"], "venue": "Pacific Journal of Mathematics,", "citeRegEx": "Kingman and John.,? \\Q1967\\E", "shortCiteRegEx": "Kingman and John.", "year": 1967}, {"title": "MCMC using Hamiltonian dynamics", "author": ["Neal", "Radford M"], "venue": "Handbook of Markov Chain Monte Carlo,", "citeRegEx": "Neal and M,? \\Q2011\\E", "shortCiteRegEx": "Neal and M", "year": 2011}, {"title": "The discrete infinite logistic normal distribution", "author": ["Paisley", "John", "Wang", "Chong", "Blei", "David M"], "venue": "Bayesian Analysis,", "citeRegEx": "Paisley et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Paisley et al\\.", "year": 2012}, {"title": "Correlated random measures", "author": ["Ranganath", "Rajesh", "Blei", "David M"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Ranganath et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2016}, {"title": "A constructive definition of Dirichlet processes", "author": ["Sethuraman", "Jayaram"], "venue": "Statistica Sinica,", "citeRegEx": "Sethuraman and Jayaram.,? \\Q1994\\E", "shortCiteRegEx": "Sethuraman and Jayaram.", "year": 1994}, {"title": "Hierarchical Dirichlet processes", "author": ["Teh", "Yee Whye", "Jordan", "Michael I", "Beal", "Matthew J", "Blei", "David M"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Infinite factorial dynamical model", "author": ["Valera", "Isabel", "Ruiz", "Francisco", "Svensson", "Lennart", "Perez-Cruz", "Fernando"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Valera et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Valera et al\\.", "year": 2015}, {"title": "Beam sampling for the infinite hidden Markov model", "author": ["Van Gael", "Jurgen", "Saatci", "Yunus", "Teh", "Yee Whye", "Ghahramani", "Zoubin"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Gael et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gael et al\\.", "year": 2008}, {"title": "Hidden Markov models with discrete infinite logistic normal distribution priors", "author": ["Zhu", "Hao", "Hu", "Jinsong", "Leung", "Henry"], "venue": "In Information Fusion (FUSION),", "citeRegEx": "Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Introduction and Background The hierarchical Dirichlet process hidden Markov model (HDP-HMM) (Beal et al., 2001; Teh et al., 2006) is a Bayesian model for time series data that generalizes the conventional hidden Markov Model to allow a countably infinite state space.", "startOffset": 93, "endOffset": 130}, {"referenceID": 17, "context": "Introduction and Background The hierarchical Dirichlet process hidden Markov model (HDP-HMM) (Beal et al., 2001; Teh et al., 2006) is a Bayesian model for time series data that generalizes the conventional hidden Markov Model to allow a countably infinite state space.", "startOffset": 93, "endOffset": 130}, {"referenceID": 5, "context": "The Sticky HDP-HMM (Fox et al., 2008) addresses this issue by adding an extra mass \u03ba at location j to the base measure of the DP that generates \u03c0j .", "startOffset": 19, "endOffset": 37}, {"referenceID": 5, "context": "The Sticky HDP-HMM (Fox et al., 2008) addresses this issue by adding an extra mass \u03ba at location j to the base measure of the DP that generates \u03c0j . That is, (2) is replaced by \u03c0j \u223c DP(\u03b1G0 + \u03ba\u03b4\u03b8j ). (5) An alternative approach that treats self-transitions as special is the HDP Hidden Semi-Markov Model (HDPHSMM; Johnson & Willsky (2013)), wherein state duration distributions are modeled separately, and ordinary selftransitions are ruled out.", "startOffset": 20, "endOffset": 338}, {"referenceID": 8, "context": "Factorial HMMs (Ghahramani et al., 1997) are commonly used in this setting, but this ignores dependence among chains, and hence may do poorly when some combinations of states are much more probable than suggested by the chain-wise dynamics.", "startOffset": 15, "endOffset": 40}, {"referenceID": 12, "context": "A rescaling and renormalization approach similar to the one used in the HDP-HMM-LT is used by Paisley et al. (2012) to define their Discrete Infinite Logistic Normal (DILN) model, an instance of a correlated random measure (Ranganath & Blei, 2016), in the setting of topic modeling.", "startOffset": 94, "endOffset": 116}, {"referenceID": 12, "context": "A rescaling and renormalization approach similar to the one used in the HDP-HMM-LT is used by Paisley et al. (2012) to define their Discrete Infinite Logistic Normal (DILN) model, an instance of a correlated random measure (Ranganath & Blei, 2016), in the setting of topic modeling. There, however, the contexts and the mixture components (topics) are distinct sets, and there is no notion of temporal dependence. Zhu et al. (2016) developed an HMM based directly on the DILN model1.", "startOffset": 94, "endOffset": 432}, {"referenceID": 14, "context": "It has been shown (Ferguson, 1973; Paisley et al., 2012; Favaro et al., 2013) that the normalization constant T is positive and finite almost surely, and thatG is distributed as a DP with base measure G0 = \u2211\u221e k=1 \u03b2k\u03b4\u03b8k .", "startOffset": 18, "endOffset": 77}, {"referenceID": 3, "context": "It has been shown (Ferguson, 1973; Paisley et al., 2012; Favaro et al., 2013) that the normalization constant T is positive and finite almost surely, and thatG is distributed as a DP with base measure G0 = \u2211\u221e k=1 \u03b2k\u03b4\u03b8k .", "startOffset": 18, "endOffset": 77}, {"referenceID": 20, "context": "The DILN-HMM (Zhu et al., 2016), employs a similar rescaling of transition probabilities via an exponentiated Gaussian Process, following (Paisley et al.", "startOffset": 13, "endOffset": 31}, {"referenceID": 14, "context": ", 2016), employs a similar rescaling of transition probabilities via an exponentiated Gaussian Process, following (Paisley et al., 2012), but the scaling function must be positive semi-definite, and in particular symmetric, whereas in the HDP-HMM-LT, \u03c6 need only take values in (0, 1].", "startOffset": 114, "endOffset": 136}, {"referenceID": 5, "context": "Sticky and Semi-Markov Generalizations We note that the local transition property of the HDPHMM-LT can be combined with the Sticky property of the Sticky HDP-HMM (Fox et al., 2008), or the nongeometric duration distributions of the HDP-HSMM (Johnson & Willsky, 2013), to add additional prior weight on self-transitions.", "startOffset": 162, "endOffset": 180}, {"referenceID": 8, "context": "Such problems are often modeled using factorial HMMs (Ghahramani et al., 1997).", "startOffset": 53, "endOffset": 78}, {"referenceID": 6, "context": "An Infinite Factorial HDP-HMM-LT Nonparametric extensions of the factorial HMM, such as the infinite factorial hidden Markov Model (Gael et al., 2009) and the infinite factorial dynamic model (Valera et al.", "startOffset": 131, "endOffset": 150}, {"referenceID": 18, "context": ", 2009) and the infinite factorial dynamic model (Valera et al., 2015), have been developed in recent years by making use of the Indian Buffet Process (Ghahramani & Griffiths, 2005) as a state prior.", "startOffset": 49, "endOffset": 70}, {"referenceID": 17, "context": "Following Teh et al. (2006), we can introduce auxiliary variables M = {mjj\u2032}, with p(mjj\u2032 |\u03b2j\u2032 , \u03b1,D) ind \u221d snjj\u2032+qjj\u2032 ,mjj\u2032\u03b1 jj\u2032\u03b2 mjj\u2032 j\u2032 (16) for integer mjj\u2032 ranging between 0 and njj\u2032 + qjj\u2032 , where sn,m is an unsigned Stirling number of the first kind.", "startOffset": 10, "endOffset": 28}, {"referenceID": 6, "context": "Following Gael et al. (2009) and Valera et al.", "startOffset": 10, "endOffset": 29}, {"referenceID": 6, "context": "Following Gael et al. (2009) and Valera et al. (2015), the weights were drawn independently from a Unif(0, 1) distribution, and independentN (0, 0.", "startOffset": 10, "endOffset": 54}, {"referenceID": 8, "context": "Results We attempted to infer the binary speaker matrices using five models: (1) a binary-state Factorial HMM (Ghahramani et al., 1997), where individual binary speaker sequences are modeled as independent, (2) an ordinary HDP-HMM without local transitions (Teh et al.", "startOffset": 110, "endOffset": 135}, {"referenceID": 17, "context": ", 1997), where individual binary speaker sequences are modeled as independent, (2) an ordinary HDP-HMM without local transitions (Teh et al., 2006), where the latent states are binary vectors, (3) a Sticky HDPHMM (Fox et al.", "startOffset": 129, "endOffset": 147}, {"referenceID": 5, "context": ", 2006), where the latent states are binary vectors, (3) a Sticky HDPHMM (Fox et al., 2008), (4) our HDP-HMM-LT model, and (5) a model that combines the Sticky and LT properties3.", "startOffset": 73, "endOffset": 91}, {"referenceID": 20, "context": "We evaluated the models at each iteration using both the We attempted to add a comparison to the DILN-HMM (Zhu et al., 2016) as well, but code could not be obtained, and the paper did not provide enough detail to reproduce their inference algorithm.", "startOffset": 106, "endOffset": 124}, {"referenceID": 1, "context": "Since the latent states are continuous, we use a Hamiltonian Monte Carlo (HMC) update (Duane et al., 1987; Neal et al., 2011) to update the `j simultaneously, conditioned on z and \u03c0 (see Appendix D for details).", "startOffset": 86, "endOffset": 125}, {"referenceID": 6, "context": "We focused on fixed-dimension binary vectors for the cocktail party and synthetic data experiments, but it would be straightforward to add the LT property to a model with nonparametric latent states, such as the iFHMM (Gael et al., 2009) and the infinite factorial dynamic model (Valera et al.", "startOffset": 218, "endOffset": 237}, {"referenceID": 18, "context": ", 2009) and the infinite factorial dynamic model (Valera et al., 2015), both of which use the Indian Buffet Process (IBP) (Ghahramani & Griffiths, 2005) as a state prior.", "startOffset": 49, "endOffset": 70}], "year": 2017, "abstractText": "We describe a generalization of the Hierarchical Dirichlet Process Hidden Markov Model (HDPHMM) which is able to encode prior information that state transitions are more likely between \u201cnearby\u201d states. This is accomplished by defining a similarity function on the state space and scaling transition probabilities by pairwise similarities, thereby inducing correlations among the transition distributions. We present an augmented data representation of the model as a Markov Jump Process in which: (1) some jump attempts fail, and (2) the probability of success is proportional to the similarity between the source and destination states. This augmentation restores conditional conjugacy and admits a simple Gibbs sampler. We evaluate the model and inference method on a speaker diarization task and a \u201charmonic parsing\u201d task using fourpart chorale data, as well as on several synthetic datasets, achieving favorable comparisons to existing models.", "creator": "LaTeX with hyperref package"}}}