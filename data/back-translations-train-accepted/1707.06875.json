{"id": "1707.06875", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jul-2017", "title": "Why We Need New Evaluation Metrics for NLG", "abstract": "The majority of NLG evaluation relies on automatic metrics, such as BLEU . In this paper, we motivate the need for novel, system- and data-independent automatic evaluation methods: We investigate a wide range of metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end NLG. We also show that metric performance is data- and system-specific. Nevertheless, our results also suggest that automatic metrics perform reliably at system-level and can support system development by finding cases where a system performs poorly.", "histories": [["v1", "Fri, 21 Jul 2017 12:47:03 GMT  (241kb,D)", "http://arxiv.org/abs/1707.06875v1", "accepted to EMNLP 2017"]], "COMMENTS": "accepted to EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jekaterina novikova", "ond\\v{r}ej du\\v{s}ek", "amanda cercas curry", "verena rieser"], "accepted": true, "id": "1707.06875"}, "pdf": {"name": "1707.06875.pdf", "metadata": {"source": "CRF", "title": "Why We Need New Evaluation Metrics for NLG", "authors": ["Jekaterina Novikova", "Ond\u0159ej Du\u0161ek", "Amanda Cercas", "Verena Rieser"], "emails": ["v.t.rieser@hw.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Automatic rating standards, such as BLEU (Papineni et al., 2002), are increasingly used to evaluate Natural Language Generation (NLG) systems: up to 60% of NLG research published between 2012 and 2015 is based on automatic metrics (Gkatzia and Mahamood, 2015). Automatic ratings are popular because they are cheaper and faster to perform than human ratings, and are required for automatic benchmarking and tuning of algorithms. However, the use of such metrics is only useful when it is known to correlate sufficiently with human preferences, which is rarely the case, as various studies in NLG show (Stent et al., 2005; Belz and Reiter, 2006; Reiter and Belz, 2009), as well as in related areas, such as dialog systems (Liu et al., 2016), machine translation (MT-Burch et al., 2006) and captioning (Elliott, and we compare the previous study, 2014, and Papilimar, two others)."}, {"heading": "2 End-to-End NLG Systems", "text": "These approaches do not require expensive semantic alignment between Meaning Representations (MR) and human references (also referred to as \"Ground Truth\" or \"Targets\"), but are available for download at: https: / / github.com / jeknov / EMNLP _ 17 _ submissionary _ S: The semantic alignment between Meaning Representations (MR) and human references (also referred to as \"Ground Truth\" or \"Targets\"). These approaches do not require expensive semantic alignment between Meaning Representations (MR) and human references (also referred to as \"Ground Truth\" or \"Targets\"), but are available for download at: https: / / github.com / jeknov / EMNLP _ 17 _ submissionar _ submissionary _ S: 170 7.06 875v 1 [cs.C L] 21 Jul 2 017based on datasets, which can be collected in quality and equty plans 17 submissionary _ S _ 75K (Surface)."}, {"heading": "3 Datasets", "text": "Table 1 shows the number of system outputs for each record. Each instance of data consists of an MR and one or more man-made natural language references, such as the following example from the BAGEL record: 52https: / / github.com / shawnwun / RNNLG 3https: / / github.com / UFAL-DSG / tgen 4https: / github.com / glampouras / JLOLS _ NLG 5Note that we use lexicalized versions of SFHOTEL and SFREST and a partially lexicalized version of BAGEL / tgen 4https in which proper names and place names are replaced by placeholders (\"X\") in accordance with the output data of the MR: inform (name = X, area = X, cerange = restaurant)."}, {"heading": "4 Metrics", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Word-based Metrics (WBMs)", "text": "The NLG evaluation has borrowed a set of automated metrics from related areas, such as MT, summary, or captions, which compare system-generated output texts with human-generated basic truth references. We refer to this group as word-based metrics, and the higher these values, the better or more similar the results. 6 The following order reflects the degree to which these metrics move from simple n-gram overlaps to consideration of term weighting (TF-IDF) and semantically similar words. \u2022 Word overlap metrics (WOMs): We consider commonly used metrics, including TER (Snover et al., 2006), BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Doddington, 2002), LEPOR (Han et al., 2012), CIDEr (Vantal et al., 2015), and MEOTEAL (similar)."}, {"heading": "4.2 Grammar-based metrics (GBMs)", "text": "Grammar-based metrics have been studied in related fields such as MT (Gime \u0301 nez and Ma \u00barquez, 2008) or grammatical error correction (Napoles et al., 2016) and, unlike WBMs, do not rely on references from the basic truth. However, to our knowledge, we are the first to consider GBMs for sentence-level NLG assessment. We focus here on two important characteristics of texts - readability and grammaticality: \u2022 Readability quantifies the difficulty with which a reader understands a text, such as for example when evaluating summarization (Kan et al., 2001) or text simplification (Francois and Bernhard, 2014). We measure readability using the Flesch Reading Ease Score (Flesch, 1979), which gives a ratio between the number of characters per sentence, the number of words per sentence, syllables per sentence, and the number of syllables per word."}, {"heading": "5 Human Data Collection", "text": "To collect human rankings, we presented the MR along with 2 expressions generated by different systems side by side with crowdworkers, who were asked to rate each utterance on a 6-step Likert scale: \u2022 Informativity: Does the utterance provide all the useful information from the meaning representation? \u2022 Naturality: Could the utterance have come from a native speaker? \u2022 Quality: How do you rate the overall quality of the utterance on the basis of grammatical correctness and fluidity? Each system output (see Table 1) was evaluated by 3 different crowdworkers. To reduce participants \"bias, the order of the expressions produced by each system was randomized and crowdworkers were allowed to rate a maximum of 20 expressions. Crowdworkers were selected only from English-speaking countries, based on their IP addresses, and asked to confirm that English was their native language. To evaluate the reliability of the assessments, we calculated the correlation coefficient between the intergenerational system (ICC) and the intergenerational system (ICC) for each of the three respondents."}, {"heading": "6 System Evaluation", "text": "Table 2 summarizes the overall performance of each system at the corpus level in terms of automatic and human values (details are given in the supplement material).All WOMs deliver similar results, with SIM showing different results for the restaurant sector (BAGEL and SFREST).Most GBMs show the same trend (with different levels of statistical significance), but RE shows inverse results. System performance is data-specific: for WBMs, the LOLS system consistently delivers better results on BAGEL compared to TGEN, while for SFREST and SFHOTEL LOLS, the RNLG interms of WBMs are exceeded. We observe that human informativeness ratings follow the same pattern as WBMs, while the average similarity value (SIM) is related to human quality ratings."}, {"heading": "7 Relation of Human and Automatic Metrics", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Human Correlation Analysis", "text": "We calculate the correlation between automatic metrics and human ratings using the Spearman coefficient (\u03c1). \u2022 We divide the data per data set and system to make valid pairs of comparisons. To handle outliers within human ratings, we use the mean score of the three human ratings. \u2022 Following Kilickaya et al. (2017), we use the Williams test (Williams, 1959) to determine significant differences between correlations. Table 3 summarizes the relationship between expression values and human values.8 As an alternative to using average human judgment for each element, a more effective way to use all human ratings could be by deriving Hovy et al al. (2013) the MACE tool for the reliability of judgments."}, {"heading": "7.2 Accuracy of Relative Rankings", "text": "We are now evaluating a rough metric, namely the ability of metrics to predict relative human ratings, that is, we are calculating the value of each metric for two sets of system output sets that correspond to the same MR. Predicting a metric is correct if it arranges the sentences in the same way as the average human ratings (note that bindings are allowed). According to previous work (Vedantam et al., 2015; Kilickaya et al., 2017), we mainly focus on WBMs. The results summarized in Table 4 show that most metrics do not differ significantly from those of a random value (Wilcoxonsigned rank test). While the random value varies between 25.4-44.5% predictive activity, the metrics reach between 30.6-49.8%. Also, the performance of the metrics is dataset-specific: metrics we use best on BAGEL data; for SFOTHEL metrics, metrics show mixed performance for SFREST."}, {"heading": "8 Error Analysis", "text": "In this section we will try to find out why automatic metrics work so poorly."}, {"heading": "8.1 Scales", "text": "We first examine the hypothesis that metrics are good at distinguishing extreme cases, i.e. system outcomes that are clearly rated as good or bad by human judges, but do not perform well for statements rated in the middle of the Likert scale, as proposed by Kilickaya et al. (2017). We \"classify\" our data into three groups: bad ones, which include low ratings (\u2264 2); good ones, which include high ratings (\u2265 5); and finally, a group which includes average ratings. We find that expressions with low human ratings correlate significantly better to informativeness and naturalness (p < 0.05) with automatic metrics than those with average and good human ratings. For example, as shown in Figure 3, the correlation between WBMs and human ratings correlates significantly better with low informativeness expressions between 0.3 \u2264 \u00b1 0.5 (moderate correlation) and 0.5 (moderate correlation), while the highest correlation with very low informativeness and very low correlation is achieved."}, {"heading": "8.2 Impact of Target Data", "text": "Characteristics of the data: In Section 7.1, we observed that data sets have a significant influence on how well automated metrics reflect human ratings. On closer inspection, the BAGEL data is significantly different from SFREST and SFHOTEL in terms of both grammatical and MR characteristics. BAGEL has significantly shorter references in terms of both the number of characters and the words compared to the other two data sets. Although they are shorter, the words in BAGEL references are significantly more polysyllabic. In addition, BAGEL consists only of statements generated from informative MRs, while SFREST and SFHOTEL also have less complex MR types, such as confirmation, goodbye, etc. Utterances produced from informative MRs are significantly longer and have a significantly higher correlation with human ratings of informativeness and naturalness than non-formality on MAGEL."}, {"heading": "8.3 Example-based Analysis", "text": "As shown in previous paragraphs, word-based measures of poor quality correspond with humans, but cannot distinguish between good and medium quality results. Table 5 provides examples from our three systems. 10 Again, we observe different behavior between WOMs and SIM values. In Example 1, LOLS produces a grammatically correct English sentence that well represents the meaning of MR, and as a result, this expression received high human ratings (median = 6) for informativity, naturalness, and quality. However, WOMs rate this expression low, i.e. the values of BLEU1-4, NIST, LEPOR, CIDER, ROUGE, and METEOR all remain below 1.5. This is because the system-generated expression has little overlap with the human / corpus references."}, {"heading": "9 Related Work", "text": "Table 6 summarizes the results of previous studies in related areas that examine the relationship between human values and automatic matches - 10Please note that WBMs tend to coincide with the reference closest to the output generated. Therefore, in Table 5, we include only the closest matches in terms of simplicity. These studies mainly looked at WBMs, while we are the first study to consider GBMs. In general, the correlations reported by previous work range from weak to strong. Results confirm that metrics can be reliable indicators at the system level (Reiter and Belz, 2009), while they are less reliable at the sentence level (\"e.g. accuracy,\" or \"correctness\"), and that the metrics may be more consistent at the system level (Reiter and Belz, 2009) than at the sentence level (\"general accuracy,\" or \"correctness,\" or \"correctness\")."}, {"heading": "10 Conclusions", "text": "This paper shows that modern automated evaluation metrics for NLG systems do not adequately reflect human ratings, emphasizing the need for human ratings. This finding is in contrast to the current trend of relying on automated ratings identified in (Gkatzia and Mahamood, 2015). Detailed error analysis suggests that automated metrics are particularly weak in distinguishing between medium and good quality results, in part due to the fact that human assessments and metrics are given at different scales. We also show that metric performance is data-specific and system-specific. However, our results also suggest that automated metrics can be useful for error analysis by helping to identify cases where the system is malfunctioning. Furthermore, we find reliable results at the system level, suggesting that metrics can be useful for system development."}, {"heading": "11 Future Directions", "text": "We argue that these assumptions are invalid for corpus-based NLG, especially when using crowdsourced data sets. Grammar-based metrics, on the other hand, do not rely on man-made references and are not influenced by their quality. However, these metrics can easily be manipulated with grammatically correct and easily readable results that have nothing to do with input. We have experimented with combining WBMs and GBMs using semester-based learning. However, while our model has achieved a high correlation with people within a single domain, its cross-domain performance is insufficient. Our paper clearly demonstrates the need for more advanced metrics, such as those used in related areas, including: evaluating output quality within the dialogue context, e.g. (Dus, ek and Jurc, ish, c, c, ek, 2016)."}, {"heading": "Acknowledgements", "text": "This research was funded by the EPSRC projects DILiGENt (EP / M005429 / 1) and MaDrIgAL (EP / N017536 / 1) and the Titan Xp used for this research was donated by NVIDIA Corporation."}, {"heading": "Appendix A: Detailed Results", "text": "0. 0 0. 0 0. 0 0. 0 0. 0. 0 0. 0. 0. 0. 0 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0."}], "references": [{"title": "Discrete vs", "author": ["Anja Belz", "Eric Kow."], "venue": "continuous rating scales for language evaluation in NLP. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Pa-", "citeRegEx": "Belz and Kow.,? 2011", "shortCiteRegEx": "Belz and Kow.", "year": 2011}, {"title": "Comparing automatic and human evaluation of NLG systems", "author": ["Anja Belz", "Ehud Reiter."], "venue": "Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics. Trento, Italy, pages 313\u2013320.", "citeRegEx": "Belz and Reiter.,? 2006", "shortCiteRegEx": "Belz and Reiter.", "year": 2006}, {"title": "Correlating human and automatic evaluation of a German surface realiser", "author": ["Aoife Cahill."], "venue": "Proceedings of the ACL-IJCNLP 2009 Conference Short Papers. Association for Computational Linguistics, Suntec, Singapore, pages 97\u2013100.", "citeRegEx": "Cahill.,? 2009", "shortCiteRegEx": "Cahill.", "year": 2009}, {"title": "Re-evaluating the role of BLEU in machine translation research", "author": ["Chris Callison-Burch", "Miles Osborne", "Philipp Koehn."], "venue": "Proceedings of the 11th Conference of the European Chapter of the Association for Computational", "citeRegEx": "Callison.Burch et al\\.,? 2006", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2006}, {"title": "Automatic evaluation of machine translation quality using n-gram cooccurrence statistics", "author": ["George Doddington."], "venue": "Proceedings of the Second International Conference on Human Language Technology Research. Morgan Kaufmann Publish-", "citeRegEx": "Doddington.,? 2002", "shortCiteRegEx": "Doddington.", "year": 2002}, {"title": "Referenceless quality estimation for natural language generation", "author": ["Ondrej Du\u0161ek", "Jekaterina Novikova", "Verena Rieser."], "venue": "Proceedings of the 1st Workshop on Learning to Generate Natural Language.", "citeRegEx": "Du\u0161ek et al\\.,? 2017", "shortCiteRegEx": "Du\u0161ek et al\\.", "year": 2017}, {"title": "Training a natural language generator from unaligned data", "author": ["Ond\u0159ej Du\u0161ek", "Filip Jur\u010d\u0131\u0301\u010dek"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Du\u0161ek and Jur\u010d\u0131\u0301\u010dek.,? \\Q2015\\E", "shortCiteRegEx": "Du\u0161ek and Jur\u010d\u0131\u0301\u010dek.", "year": 2015}, {"title": "A contextaware natural language generator for dialogue systems", "author": ["Ond\u0159ej Du\u0161ek", "Filip Jur\u010d\u0131\u0301\u010dek"], "venue": "In Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue", "citeRegEx": "Du\u0161ek and Jur\u010d\u0131\u0301\u010dek.,? \\Q2016\\E", "shortCiteRegEx": "Du\u0161ek and Jur\u010d\u0131\u0301\u010dek.", "year": 2016}, {"title": "Sequenceto-sequence generation for spoken dialogue via deep syntax trees and strings", "author": ["Ond\u0159ej Du\u0161ek", "Filip Jur\u010d\u0131\u0301\u010dek"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Berlin,", "citeRegEx": "Du\u0161ek and Jur\u010d\u0131\u0301\u010dek.,? \\Q2016\\E", "shortCiteRegEx": "Du\u0161ek and Jur\u010d\u0131\u0301\u010dek.", "year": 2016}, {"title": "Comparing automatic evaluation measures for image description", "author": ["Desmond Elliott", "Frank Keller."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computa-", "citeRegEx": "Elliott and Keller.,? 2014", "shortCiteRegEx": "Elliott and Keller.", "year": 2014}, {"title": "Further metaevaluation of broad-coverage surface realization", "author": ["Dominic Espinosa", "Rajakrishnan Rajkumar", "Michael White", "Shoshana Berleant."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Associa-", "citeRegEx": "Espinosa et al\\.,? 2010", "shortCiteRegEx": "Espinosa et al\\.", "year": 2010}, {"title": "How to write plain English: A book for lawyers and consumers", "author": ["Rudolf Franz Flesch."], "venue": "HarperCollins.", "citeRegEx": "Flesch.,? 1979", "shortCiteRegEx": "Flesch.", "year": 1979}, {"title": "Recent Advances in Automatic Readability Assessment and Text Simplification, volume", "author": ["Thomas Francois", "Delphine Bernhard", "editors"], "venue": "International Journal of Applied Linguistics. John Benjamins. http://doi.org/10.1075/itl.165.2", "citeRegEx": "Francois et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Francois et al\\.", "year": 2014}, {"title": "deltaBLEU: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceed-", "citeRegEx": "Galley et al\\.,? 2015", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "A smorgasbord of features for automatic MT evaluation", "author": ["Jes\u00fas Gim\u00e9nez", "Llu\u0131\u0301s M\u00e0rquez"], "venue": "In Proceedings of the Third Workshop on Statistical Machine Translation. Association for Computational Linguistics,", "citeRegEx": "Gim\u00e9nez and M\u00e0rquez.,? \\Q2008\\E", "shortCiteRegEx": "Gim\u00e9nez and M\u00e0rquez.", "year": 2008}, {"title": "Natural language generation enhances human decision-making with uncertain information", "author": ["Dimitra Gkatzia", "Oliver Lemon", "Verena Rieser."], "venue": "Proceedings of the 54th Annual", "citeRegEx": "Gkatzia et al\\.,? 2016", "shortCiteRegEx": "Gkatzia et al\\.", "year": 2016}, {"title": "Imitation learning for language generation from unaligned data", "author": ["Gerasimos Lampouras", "Andreas Vlachos."], "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016", "citeRegEx": "Lampouras and Vlachos.,? 2016", "shortCiteRegEx": "Lampouras and Vlachos.", "year": 2016}, {"title": "The measurement of observer agreement for categorical data", "author": ["J Richard Landis", "Gary G Koch."], "venue": "Biometrics 33(1):159\u2013174. https://doi.org/10.2307/2529310.", "citeRegEx": "Landis and Koch.,? 1977", "shortCiteRegEx": "Landis and Koch.", "year": 1977}, {"title": "METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments", "author": ["Alon Lavie", "Abhaya Agarwal."], "venue": "Proceedings of the Second Workshop on Statistical Machine Translation. Association for Computational", "citeRegEx": "Lavie and Agarwal.,? 2007", "shortCiteRegEx": "Lavie and Agarwal.", "year": 2007}, {"title": "ROUGE: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Text summarization branches out: Proceedings of the ACL04 workshop. Barcelona, Spain, pages 74\u201381. http://aclweb.org/anthology/W04-1013.", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau."], "venue": "In", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Phrase-based statistical language generation using graphical models and active learning", "author": ["Fran\u00e7ois Mairesse", "Milica Ga\u0161i\u0107", "Filip Jur\u010d\u0131\u0301\u010dek", "Simon Keizer", "Blaise Thomson", "Kai Yu", "Steve Young"], "venue": "In Proceedings of the 48th Annual Meeting of the Associa-", "citeRegEx": "Mairesse et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mairesse et al\\.", "year": 2010}, {"title": "What to talk about and how? Selective generation using LSTMs with coarse-tofine alignment", "author": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter."], "venue": "Proceedings of NAACL-HLT 2016. San Diego, CA, USA. arXiv:1509.00838.", "citeRegEx": "Mei et al\\.,? 2016", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "There\u2019s no comparison: Reference-less evaluation metrics in grammatical error correction", "author": ["Courtney Napoles", "Keisuke Sakaguchi", "Joel Tetreault."], "venue": "Proceedings of the 2016 Conference on Empirical Methods", "citeRegEx": "Napoles et al\\.,? 2016", "shortCiteRegEx": "Napoles et al\\.", "year": 2016}, {"title": "The E2E dataset: New challenges for end-to-end generation", "author": ["Jekaterina Novikova", "Ondrej Du\u0161ek", "Verena Rieser."], "venue": "Proceedings of the 18th Annual Meeting of the", "citeRegEx": "Novikova et al\\.,? 2017", "shortCiteRegEx": "Novikova et al\\.", "year": 2017}, {"title": "Crowd-sourcing NLG data: Pictures elicit better data", "author": ["Jekaterina Novikova", "Oliver Lemon", "Verena Rieser."], "venue": "Proceedings of the 9th International Natural Language Generation Conference. Edinburgh, UK, pages 265\u2013273. arXiv:1608.00339.", "citeRegEx": "Novikova et al\\.,? 2016", "shortCiteRegEx": "Novikova et al\\.", "year": 2016}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. Association for Compu-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "An investigation into the validity of some metrics for automatically evaluating natural language generation systems", "author": ["Ehud Reiter", "Anja Belz."], "venue": "Computational Linguistics 35(4):529\u2013558. https://doi.org/10.1162/coli.2009.35.4.35405.", "citeRegEx": "Reiter and Belz.,? 2009", "shortCiteRegEx": "Reiter and Belz.", "year": 2009}, {"title": "Natural language generation as incremental planning under uncertainty: Adaptive information presentation for statistical dialogue systems", "author": ["Verena Rieser", "Oliver Lemon", "Simon Keizer."], "venue": "IEEE/ACM Transactions on Audio,", "citeRegEx": "Rieser et al\\.,? 2014", "shortCiteRegEx": "Rieser et al\\.", "year": 2014}, {"title": "Natural language generation in dialogue using lexicalized and delexicalized data", "author": ["Shikhar Sharma", "Jing He", "Kaheer Suleman", "Hannes Schulz", "Philip Bachman."], "venue": "CoRR abs/1606.03632. http://arxiv.org/abs/1606.03632.", "citeRegEx": "Sharma et al\\.,? 2016", "shortCiteRegEx": "Sharma et al\\.", "year": 2016}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."], "venue": "Proceedings of the 7th Conference of the Association for Machine Translation of the Americas.", "citeRegEx": "Snover et al\\.,? 2006", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "Machine translation evaluation versus quality estimation", "author": ["Lucia Specia", "Dhwaj Raj", "Marco Turchi."], "venue": "Machine translation 24(1):39\u201350. https://doi.org/10.1007/s10590-010-9077-2.", "citeRegEx": "Specia et al\\.,? 2010", "shortCiteRegEx": "Specia et al\\.", "year": 2010}, {"title": "Evaluating evaluation methods for generation in the presence of variation", "author": ["Amanda Stent", "Matthew Marge", "Mohit Singhai."], "venue": "Computational Linguistics and Intelligent Text Processing: 6th International Conference, CICLing 2005, Mex-", "citeRegEx": "Stent et al\\.,? 2005", "shortCiteRegEx": "Stent et al\\.", "year": 2005}, {"title": "CIDEr: Consensusbased image description evaluation", "author": ["Ramakrishna Vedantam", "C. Lawrence Zitnick", "Devi Parikh."], "venue": "Proceedings of the 2015 IEEE Conference on", "citeRegEx": "Vedantam et al\\.,? 2015", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Multidomain neural network language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Lina Maria Rojas-Barahona", "Pei-hao Su", "David Vandyke", "Steve J. Young."], "venue": "Proceedings of the", "citeRegEx": "Wen et al\\.,? 2016", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Steve Young."], "venue": "Proceedings of the 2015 Confer-", "citeRegEx": "Wen et al\\.,? 2015", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Regression analysis", "author": ["Evan James Williams."], "venue": "John Wiley & Sons, New York, NY, USA.", "citeRegEx": "Williams.,? 1959", "shortCiteRegEx": "Williams.", "year": 1959}], "referenceMentions": [{"referenceID": 26, "context": "Automatic evaluation measures, such as BLEU (Papineni et al., 2002), are used with increasing frequency to evaluate Natural Language Generation (NLG) systems: Up to 60% of NLG research published between 2012\u20132015 relies on automatic metrics (Gkatzia and Mahamood, 2015).", "startOffset": 44, "endOffset": 67}, {"referenceID": 32, "context": "This is rarely the case, as shown by various studies in NLG (Stent et al., 2005; Belz and Reiter, 2006; Reiter and Belz, 2009), as well as in related fields, such as dialogue systems (Liu et al.", "startOffset": 60, "endOffset": 126}, {"referenceID": 1, "context": "This is rarely the case, as shown by various studies in NLG (Stent et al., 2005; Belz and Reiter, 2006; Reiter and Belz, 2009), as well as in related fields, such as dialogue systems (Liu et al.", "startOffset": 60, "endOffset": 126}, {"referenceID": 27, "context": "This is rarely the case, as shown by various studies in NLG (Stent et al., 2005; Belz and Reiter, 2006; Reiter and Belz, 2009), as well as in related fields, such as dialogue systems (Liu et al.", "startOffset": 60, "endOffset": 126}, {"referenceID": 20, "context": ", 2005; Belz and Reiter, 2006; Reiter and Belz, 2009), as well as in related fields, such as dialogue systems (Liu et al., 2016), machine translation (MT) (Callison-Burch et al.", "startOffset": 110, "endOffset": 128}, {"referenceID": 3, "context": ", 2016), machine translation (MT) (Callison-Burch et al., 2006), and image captioning (Elliott and Keller, 2014; Kilickaya et al.", "startOffset": 34, "endOffset": 63}, {"referenceID": 9, "context": ", 2006), and image captioning (Elliott and Keller, 2014; Kilickaya et al., 2017).", "startOffset": 30, "endOffset": 80}, {"referenceID": 6, "context": "In this paper, we focus on recent end-to-end, datadriven NLG methods, which jointly learn sentence planning and surface realisation from non-aligned data (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2015; Wen et al., 2015; Mei et al., 2016; Wen et al., 2016; Sharma et al., 2016; Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2016, Lampouras and Vlachos, 2016).", "startOffset": 154, "endOffset": 312}, {"referenceID": 35, "context": "In this paper, we focus on recent end-to-end, datadriven NLG methods, which jointly learn sentence planning and surface realisation from non-aligned data (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2015; Wen et al., 2015; Mei et al., 2016; Wen et al., 2016; Sharma et al., 2016; Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2016, Lampouras and Vlachos, 2016).", "startOffset": 154, "endOffset": 312}, {"referenceID": 22, "context": "In this paper, we focus on recent end-to-end, datadriven NLG methods, which jointly learn sentence planning and surface realisation from non-aligned data (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2015; Wen et al., 2015; Mei et al., 2016; Wen et al., 2016; Sharma et al., 2016; Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2016, Lampouras and Vlachos, 2016).", "startOffset": 154, "endOffset": 312}, {"referenceID": 34, "context": "In this paper, we focus on recent end-to-end, datadriven NLG methods, which jointly learn sentence planning and surface realisation from non-aligned data (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2015; Wen et al., 2015; Mei et al., 2016; Wen et al., 2016; Sharma et al., 2016; Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2016, Lampouras and Vlachos, 2016).", "startOffset": 154, "endOffset": 312}, {"referenceID": 29, "context": "In this paper, we focus on recent end-to-end, datadriven NLG methods, which jointly learn sentence planning and surface realisation from non-aligned data (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2015; Wen et al., 2015; Mei et al., 2016; Wen et al., 2016; Sharma et al., 2016; Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2016, Lampouras and Vlachos, 2016).", "startOffset": 154, "endOffset": 312}, {"referenceID": 25, "context": "(Novikova et al., 2016), and as such, enable rapid development of NLG components in new domains.", "startOffset": 0, "endOffset": 23}, {"referenceID": 20, "context": "(Novikova et al., 2016), and as such, enable rapid development of NLG components in new domains. In particular, we compare the performance of the following systems: \u2022 RNNLG:2 The system by Wen et al. (2015) uses a Long Short-term Memory (LSTM) network to jointly address sentence planning and surface realisation.", "startOffset": 1, "endOffset": 207}, {"referenceID": 6, "context": "\u2022 TGEN:3 The system by Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2015) learns to incrementally generate deepsyntax dependency trees of candidate sentence plans (i.", "startOffset": 23, "endOffset": 50}, {"referenceID": 6, "context": "\u2022 TGEN:3 The system by Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2015) learns to incrementally generate deepsyntax dependency trees of candidate sentence plans (i.e. which MR elements to mention and the overall sentence structure). Surface realisation is performed using a separate, domain-independent rule-based module. \u2022 LOLS:4 The system by Lampouras and Vlachos (2016) learns sentence planning and surface realisation using Locally Optimal Learning to Search (LOLS), an imitation learning framework which learns using BLEU and ROUGE as non-decomposable loss functions.", "startOffset": 23, "endOffset": 352}, {"referenceID": 35, "context": "\u2022 SFHOTEL & SFREST (Wen et al., 2015) provide information about hotels and restaurants in San Francisco.", "startOffset": 19, "endOffset": 37}, {"referenceID": 21, "context": "\u2022 BAGEL (Mairesse et al., 2010) provides information about restaurants in Cambridge.", "startOffset": 8, "endOffset": 31}, {"referenceID": 30, "context": "\u2022Word-overlap Metrics (WOMs): We consider frequently used metrics, including TER (Snover et al., 2006), BLEU (Papineni et al.", "startOffset": 81, "endOffset": 102}, {"referenceID": 26, "context": ", 2006), BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Doddington, 2002), LEPOR (Han et al.", "startOffset": 14, "endOffset": 37}, {"referenceID": 19, "context": ", 2002), ROUGE (Lin, 2004), NIST (Doddington, 2002), LEPOR (Han et al.", "startOffset": 15, "endOffset": 26}, {"referenceID": 4, "context": ", 2002), ROUGE (Lin, 2004), NIST (Doddington, 2002), LEPOR (Han et al.", "startOffset": 33, "endOffset": 51}, {"referenceID": 33, "context": ", 2012), CIDEr (Vedantam et al., 2015), and METEOR (Lavie and Agarwal, 2007).", "startOffset": 15, "endOffset": 38}, {"referenceID": 18, "context": ", 2015), and METEOR (Lavie and Agarwal, 2007).", "startOffset": 20, "endOffset": 45}, {"referenceID": 14, "context": "Grammar-based measures have been explored in related fields, such as MT (Gim\u00e9nez and M\u00e0rquez, 2008) or grammatical error correction (Napoles et al.", "startOffset": 72, "endOffset": 99}, {"referenceID": 23, "context": "Grammar-based measures have been explored in related fields, such as MT (Gim\u00e9nez and M\u00e0rquez, 2008) or grammatical error correction (Napoles et al., 2016), and, in contrast to WBMs, do not rely on ground-truth references.", "startOffset": 132, "endOffset": 154}, {"referenceID": 11, "context": "We measure readability by the Flesch Reading Ease score (RE) (Flesch, 1979), which calculates a ratio between the number of characters per sentence, the number of words per sentence, and the number of syllables per word.", "startOffset": 61, "endOffset": 75}, {"referenceID": 23, "context": "(Napoles et al., 2016), once they become publicly available.", "startOffset": 0, "endOffset": 22}, {"referenceID": 17, "context": "To assess the reliability of ratings, we calculated the intra-class correlation coefficient (ICC), which measures inter-observer reliability on ordinal data for more than two raters (Landis and Koch, 1977).", "startOffset": 182, "endOffset": 205}, {"referenceID": 36, "context": "(2017), we use the Williams\u2019 test (Williams, 1959) to determine significant differences between correlations.", "startOffset": 34, "endOffset": 50}, {"referenceID": 31, "context": "Note that similar inconsistencies between document- and sentence-level evaluation results are observed in MT (Specia et al., 2010).", "startOffset": 109, "endOffset": 130}, {"referenceID": 33, "context": "Following previous work (Vedantam et al., 2015; Kilickaya et al., 2017), we mainly concentrate on WBMs.", "startOffset": 24, "endOffset": 71}, {"referenceID": 33, "context": "Discussion: Our data differs from the one used in previous work (Vedantam et al., 2015; Kilickaya et al., 2017), which uses explicit relative rankings (\u201cWhich output do you prefer?\u201d), whereas we compare two Likert-scale ratings.", "startOffset": 64, "endOffset": 111}, {"referenceID": 33, "context": "Discussion: Our data differs from the one used in previous work (Vedantam et al., 2015; Kilickaya et al., 2017), which uses explicit relative rankings (\u201cWhich output do you prefer?\u201d), whereas we compare two Likert-scale ratings. As such, we have 3 possible outcomes (allowing ties). This way, we can account for equally valid system outputs, which is one of the main drawbacks of forced-choice approaches (Hodosh and Hockenmaier, 2016). Our results are akin to previous work: Kilickaya et al. (2017) report results between 60-74% accuracy for binary classification on machine-machine data, which is comparable to our results for 3-way classification.", "startOffset": 65, "endOffset": 500}, {"referenceID": 0, "context": "Note that this mismatch can also be accounted for by continuous rating scales, as suggested by Belz and Kow (2011). sults, we can see an improvement for predicting informativeness, where all WBMs now perform significantly better than the random baseline (see Table 4).", "startOffset": 95, "endOffset": 115}, {"referenceID": 13, "context": "This problem could possibly be solved by weighting multiple references according to their quality, as suggested by (Galley et al., 2015), or following a reference-less approach (Specia et al.", "startOffset": 115, "endOffset": 136}, {"referenceID": 31, "context": ", 2015), or following a reference-less approach (Specia et al., 2010).", "startOffset": 48, "endOffset": 69}, {"referenceID": 13, "context": "Again, weighting (Galley et al., 2015) or reference-less approaches (Specia et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 31, "context": ", 2015) or reference-less approaches (Specia et al., 2010) might remedy this issue.", "startOffset": 37, "endOffset": 58}, {"referenceID": 1, "context": "For example, Belz and Reiter (2006) find that human experts assign low rankings to their original corpus text.", "startOffset": 13, "endOffset": 36}, {"referenceID": 27, "context": "(Reiter and Belz, 2009) none strong positive (Pearson\u2019s r = 0.", "startOffset": 0, "endOffset": 23}, {"referenceID": 32, "context": "96, NIST) NLG, weather forecast (Stent et al., 2005) weak positive (\u03c1 = 0.", "startOffset": 32, "endOffset": 52}, {"referenceID": 20, "context": "(Liu et al., 2016) weak positive (\u03c1 = 0.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "(Elliott and Keller, 2014) positive (\u03c1 = 0.", "startOffset": 0, "endOffset": 26}, {"referenceID": 2, "context": "(Cahill, 2009) N/A negative (\u03c1 = \u22120.", "startOffset": 0, "endOffset": 14}, {"referenceID": 10, "context": "(Espinosa et al., 2010) weak positive (\u03c1 = 0.", "startOffset": 0, "endOffset": 23}, {"referenceID": 27, "context": "The results confirm that metrics can be reliable indicators at system-level (Reiter and Belz, 2009), while they perform less reliably at sentence-level (Stent et al.", "startOffset": 76, "endOffset": 99}, {"referenceID": 32, "context": "The results confirm that metrics can be reliable indicators at system-level (Reiter and Belz, 2009), while they perform less reliably at sentence-level (Stent et al., 2005).", "startOffset": 152, "endOffset": 172}, {"referenceID": 7, "context": "(Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2016); extrinsic evaluation metrics, such as NLG\u2019s contribution to task success, e.", "startOffset": 0, "endOffset": 27}, {"referenceID": 28, "context": "(Rieser et al., 2014; Gkatzia et al., 2016; Hastie et al., 2016); building discriminative models, e.", "startOffset": 0, "endOffset": 64}, {"referenceID": 15, "context": "(Rieser et al., 2014; Gkatzia et al., 2016; Hastie et al., 2016); building discriminative models, e.", "startOffset": 0, "endOffset": 64}, {"referenceID": 31, "context": "(Specia et al., 2010).", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "In current work (Du\u0161ek et al., 2017), we investigate a reference-less quality estimation approach based on recurrent neural networks, which predicts a quality score for a NLG system output by comparing it to the source meaning representation only.", "startOffset": 16, "endOffset": 36}, {"referenceID": 35, "context": "To remedy this, systems train on de-lexicalised versions (Wen et al., 2015), which bears the danger of ungrammatical lexicalisation (Sharma et al.", "startOffset": 57, "endOffset": 75}, {"referenceID": 29, "context": ", 2015), which bears the danger of ungrammatical lexicalisation (Sharma et al., 2016) and a possible overlap between testing and training set (Lampouras and Vlachos, 2016).", "startOffset": 64, "endOffset": 85}, {"referenceID": 16, "context": ", 2016) and a possible overlap between testing and training set (Lampouras and Vlachos, 2016).", "startOffset": 64, "endOffset": 93}], "year": 2017, "abstractText": "The majority of NLG evaluation relies on automatic metrics, such as BLEU. In this paper, we motivate the need for novel, systemand data-independent automatic evaluation methods: We investigate a wide range of metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end NLG. We also show that metric performance is dataand system-specific. Nevertheless, our results also suggest that automatic metrics perform reliably at system-level and can support system development by finding cases where a system performs poorly.", "creator": "LaTeX with hyperref package"}}}