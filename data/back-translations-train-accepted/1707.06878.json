{"id": "1707.06878", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jul-2017", "title": "Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation", "abstract": "Interpretability of a predictive model is a powerful feature that gains the trust of users in the correctness of the predictions. In word sense disambiguation (WSD), knowledge-based systems tend to be much more interpretable than knowledge-free counterparts as they rely on the wealth of manually-encoded elements representing word senses, such as hypernyms, usage examples, and images. We present a WSD system that bridges the gap between these two so far disconnected groups of methods. Namely, our system, providing access to several state-of-the-art WSD models, aims to be interpretable as a knowledge-based system while it remains completely unsupervised and knowledge-free. The presented tool features a Web interface for all-word disambiguation of texts that makes the sense predictions human readable by providing interpretable word sense inventories, sense representations, and disambiguation results. We provide a public API, enabling seamless integration.", "histories": [["v1", "Fri, 21 Jul 2017 12:56:06 GMT  (4529kb,D)", "http://arxiv.org/abs/1707.06878v1", "In Proceedings of the the Conference on Empirical Methods on Natural Language Processing (EMNLP 2017). 2017. Copenhagen, Denmark. Association for Computational Linguistics"]], "COMMENTS": "In Proceedings of the the Conference on Empirical Methods on Natural Language Processing (EMNLP 2017). 2017. Copenhagen, Denmark. Association for Computational Linguistics", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexander panchenko", "fide marten", "eugen ruppert", "stefano faralli", "dmitry ustalov", "simone paolo ponzetto", "chris biemann"], "accepted": true, "id": "1707.06878"}, "pdf": {"name": "1707.06878.pdf", "metadata": {"source": "CRF", "title": "Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation", "authors": ["Alexander Panchenko", "Fide Marten", "Eugen Ruppert", "Stefano Faralli", "Dmitry Ustalov", "Simone Paolo Ponzetto", "Chris Biemann"], "emails": ["panchenko@informatik.uni-hamburg.de", "marten@informatik.uni-hamburg.de", "ruppert@informatik.uni-hamburg.de", "biemann@informatik.uni-hamburg.de", "simone@informatik.uni-mannheim.de", "stefano@informatik.uni-mannheim.de", "dmitry.ustalov@urfu.ru"], "sections": [{"heading": "1 Introduction", "text": "The concept of the meaning of the word is of central importance for computer-aided lexical semantics. Word senses can either be encoded manually in lexical resources or automatically induced from text. Previous knowledge-based sensory representations, as found in the lexical semantic network BabelNet (Navigli and Ponzetto, 2012), are easy to interpret for humans due to the presence of definitions, application examples, taxonomic relationships, related words and images. The price of such interpretability is that each of the above-mentioned elements is manually encoded in one of the underlying resources such as Wikipedia. Unsupervised knowledge-free approaches, e.g. (Di Marco and Navigli, 2013; Bartunov et al., 2016), do not require manual work, but the resulting sensory representations do not require the above-mentioned features that enable interpretation."}, {"heading": "2 Related Work", "text": "In this section we list prominent WSD systems with open implementations.Knowledge-Based and / or Supervised Systems IMS (Zhong and Ng, 2010) is a monitored Allwords WSD system that allows users to integrate additional features and different classifiers. By default, the system relies on linear support vector machines with multiple features. The AutoExtend approach (Rothe and Protections, 2015) can be used to learn embedding for lexeme and synsets1https: / / github.com / uhh-lt / wsd 2http: / / jobimtext.org / wsdar Xiv: 170 7.06 878v 1 [cs.C L] 21 July 2 017 of a lexical resource. These representations have been successfully used to perform WSD with the IMS.DKPro WSD (Miller et al., 2013)."}, {"heading": "3 Unsupervised Knowledge-Free Interpretable WSD", "text": "This section describes (1) how WSD models are learned unsupervised from text, and (2) how the system uses these models to allow for human-interpretable ambiguities in context.3https: / / github.com / alvations / pywsd"}, {"heading": "3.1 Induction of the WSD Models", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves, and they will be able to play by the rules they have imposed on themselves."}, {"heading": "3.2 WSD API", "text": "In order to provide quick access to sensory inventories and effective parallel predictions, the WSD models obtained in the previous step were indexed in a relational database.5 In particular, each sense of the word is represented by its hypernyms, related words, and application examples.6 In addition, the database stores an aggregated contextual word representation for each sense in the form of a serialized object containing a sparse vector in Breeze format.6 During the disambiguation phrase, the input context is displayed in the same sparse feature space, and the classification is reduced to calculating the cosmic similarity between the context vector and the vectors of the sensor candidates retrieved from the database. This back-end is rendered as a RESTful API using the play framework.7"}, {"heading": "3.3 User Interface for Interpretable WSD", "text": "The graphical user interface of our system is implemented as a single page web application with the React Framework.8 The application performs disambiguation of a text entered by a user. Specifically, the web application has two modes: Single word disambiguation mode is illustrated in Figure 2. In this mode, a user is specifies5https: / / www.postgresql.org 6https: / / github.com / scalanlp / breeze 7https: / www.playframework.com 8https: / facebook.github.io / reactan ambiguous word and its context. The output of the system is a ranking of all the sense of words ordered by relevance of the input context. By default, only the best matching sense is displayed, which corresponds to the induced sense by looking at the hypernym and the image representing the meaning."}, {"heading": "4 Evaluation", "text": "In our previous paper (Panchenko et al., 2017), we conducted a thorough evaluation of the method implemented in our system based on two sets of data showing the state-of-the-art performance of the approach compared to other unattended knowledge-free methods for WSD, including participants in SemEval 2013 Task 13 (Jurgens and Klapaftis, 2013) and two unattended knowledge-free WSD systems based on literal embedding (Bartunov et al., 2016; Pelevina et al., 2016). These evaluations are based on the \"lexical example setting,\" in which the system is expected to predict a sensual identification of the ambiguous word. In this section, we perform an additional evaluation that assesses how well hypernyms of ambiguous words, e.g. in the context of our animal sign, the word \"jaguar\" is in the big task \"jaguar,\" with the exact quality of the \"jaguar system\" not dependent on the finite meaning. \""}, {"heading": "4.1 Dataset", "text": "In this experiment, we collected a data set consisting of definitions of BabelNet 3.7 senses with 1,219 common terms. 11 In total, we collected 56,003 sense definitions, each labeled with gold hypernyms derived from BabelNet's IsA relationships. The average polysemy of the words in the collected data set was 15.50 senses per word compared to 2.34 in the inventory of the induced sense. This enormous discrepancy in granularities meant that some test sets could not be correctly predicted by definition: Some (mostly rare) BabelNet senses simply do not have a corresponding sense in the induced inventory. To eliminate the influence of this idiosyncrasy, we kept only sentences containing at least one common hypernym with all hypernyms of all induced senses. The statistics of the resulting data set are presented in Table 1 and available in the project repository."}, {"heading": "4.2 Evaluation Metrics", "text": "WSD performance is measured by the accuracy of the sentences labeled with the direct hypernyms (Hypers) or an extended set of hypernyms, including hypernyms of hypernyms (Hy-11Most nouns come from the TWSI dataset (Biemann, 2012), while the remaining nouns were selected manually. PerHypers) Correct match occurs when the predicted meaning has at least one hypernym in common with the gold hypernyms of the target in a test sentence."}, {"heading": "4.3 Discussion of Results", "text": "All evaluated models exceed both the random and the most common sense baseline, see Table 2. The latter selects the sense that corresponds to the largest sense cluster (Panchenko et al., 2017). In the case of traditional \"per-word\" inventories, the model based on context characteristics exceeds the models that are based on cluster words. While meaning representations on the clusters of semantically related words contain highly precise characteristics, such representations are sparse, as one sense contains no more than 200 characteristics. Consequently, the model based on cluster words often does not contain common characteristics with the characteristics extracted from the input context. Sense representations based on the aggregated context clues are much less sparse, which explains their superior performance. In the case of the supersense inventory, the model based solely on cluster words provides better results than the contextual model. Note here that (1) the word models are larger than the superwords in the cluster sense, and thus the superwords become meaningful in the comparison."}, {"heading": "5 Conclusion", "text": "We present the first open-source word sense disambiguation system, which is unattended, knowledge-free and simultaneously interpretable. It extracts word and super sense inventories from a text corpus. Disambiguation models are learned unattended for all words in the corpus based on the induced inventories. The system's user interface provides efficient access to the produced WSD models via a RESTful API or via an interactive web-based graphical user interface. The system is available online and can be used directly from external applications. The code and WSD models are open source. Furthermore, in-house deployments of the system are simplified by using the docker containers."}, {"heading": "Acknowledgments", "text": "We would like to thank the DFG for their support in the \"JOIN-T\" project, the RFBR for the \"16-37-00354 mol a\" project, Amazon for the \"AWS Research Grants\" and Microsoft for the \"Azure for Research\" programmes. Finally, we would like to thank four anonymous reviewers for their helpful comments."}], "references": [{"title": "Breaking Sticks and Ambiguities with Adaptive Skip-gram", "author": ["S. Bartunov", "D. Kondrashkin", "A. Osokin", "D. Vetrov."], "venue": "Proc. AISTATS. Cadiz, Spain, pp. 130\u2013138.", "citeRegEx": "Bartunov et al\\.,? 2016", "shortCiteRegEx": "Bartunov et al\\.", "year": 2016}, {"title": "Chinese Whispers: An Efficient Graph Clustering Algorithm and Its Application to Natural Language Processing Problems", "author": ["C. Biemann."], "venue": "Proc. TextGraphs. New York, NY, USA, pp. 73\u201380.", "citeRegEx": "Biemann.,? 2006", "shortCiteRegEx": "Biemann.", "year": 2006}, {"title": "Turk Bootstrap Word Sense Inventory 2.0: A Large-Scale Resource for Lexical Substitution", "author": ["C. Biemann"], "venue": "In Proc. LREC. Istanbul,", "citeRegEx": "Biemann.,? \\Q2012\\E", "shortCiteRegEx": "Biemann.", "year": 2012}, {"title": "Text: now in 2D! A framework for lexical expansion with contextual similarity", "author": ["C. Biemann", "M. Riedl."], "venue": "Journal of Language Modelling 1(1):55\u2013", "citeRegEx": "Biemann and Riedl.,? 2013", "shortCiteRegEx": "Biemann and Riedl.", "year": 2013}, {"title": "Clustering and Diversifying Web Search Results with Graph-Based Word Sense Induction", "author": ["A. Di Marco", "R. Navigli."], "venue": "Computational Linguistics 39(3):709\u2013754.", "citeRegEx": "Marco and Navigli.,? 2013", "shortCiteRegEx": "Marco and Navigli.", "year": 2013}, {"title": "A New MinimallySupervised Framework for Domain Word Sense Disambiguation", "author": ["S. Faralli", "R. Navigli."], "venue": "Proc. EMNLP-CoNLL. Jeju Island, Korea, pp. 1411\u20131422.", "citeRegEx": "Faralli and Navigli.,? 2012", "shortCiteRegEx": "Faralli and Navigli.", "year": 2012}, {"title": "Linked Disambiguated Distributional Semantic Networks", "author": ["S. Faralli", "A. Panchenko", "C. Biemann", "S.P. Ponzetto."], "venue": "Proc. ISWC, Part II. Kobe, Japan, pp. 56\u201364.", "citeRegEx": "Faralli et al\\.,? 2016", "shortCiteRegEx": "Faralli et al\\.", "year": 2016}, {"title": "Automatic Acquisition of Hyponyms from Large Text Corpora", "author": ["M.A. Hearst."], "venue": "Proc. COLING. Nantes, France, pp. 539\u2013545.", "citeRegEx": "Hearst.,? 1992", "shortCiteRegEx": "Hearst.", "year": 1992}, {"title": "SemEval-2013 Task 13: Word Sense Induction for Graded and NonGraded Senses", "author": ["D. Jurgens", "I. Klapaftis."], "venue": "Proc. SemEval. Atlanta, GA, USA, pp. 290\u2013299.", "citeRegEx": "Jurgens and Klapaftis.,? 2013", "shortCiteRegEx": "Jurgens and Klapaftis.", "year": 2013}, {"title": "DKPro WSD: A Generalized UIMA-based Framework for Word Sense Disambiguation", "author": ["T. Miller"], "venue": "Proc. ACL. Sofia, Bulgaria, pp. 37\u201342.", "citeRegEx": "Miller,? 2013", "shortCiteRegEx": "Miller", "year": 2013}, {"title": "Entity Linking meets Word Sense Disambiguation: A Unified Approach", "author": ["A. Moro", "A. Raganato", "R. Navigli."], "venue": "Transactions of the Association for Computational Linguistics 2:231\u2013244.", "citeRegEx": "Moro et al\\.,? 2014", "shortCiteRegEx": "Moro et al\\.", "year": 2014}, {"title": "BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network", "author": ["R. Navigli", "S.P. Ponzetto."], "venue": "Artificial Intelligence 193:217\u2013250.", "citeRegEx": "Navigli and Ponzetto.,? 2012", "shortCiteRegEx": "Navigli and Ponzetto.", "year": 2012}, {"title": "Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space", "author": ["A. Neelakantan", "J. Shankar", "A. Passos", "A. McCallum."], "venue": "Proc. EMNLP. Doha, Qatar, pp. 1059\u20131069.", "citeRegEx": "Neelakantan et al\\.,? 2014", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Unsupervised Does Not Mean Uninterpretable: The Case for Word Sense Induction and Disambiguation", "author": ["A. Panchenko", "E. Ruppert", "S. Faralli", "S.P. Ponzetto", "C. Biemann."], "venue": "Proc. EACL. Valencia, Spain, pp. 86\u201398.", "citeRegEx": "Panchenko et al\\.,? 2017", "shortCiteRegEx": "Panchenko et al\\.", "year": 2017}, {"title": "Making Sense of Word Embeddings", "author": ["M. Pelevina", "N. Arefiev", "C. Biemann", "A. Panchenko."], "venue": "Proc. RepL4NLP. Berlin, Germany, pp. 174\u2013183.", "citeRegEx": "Pelevina et al\\.,? 2016", "shortCiteRegEx": "Pelevina et al\\.", "year": 2016}, {"title": "AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes", "author": ["S. Rothe", "H. Sch\u00fctze."], "venue": "Proc. ACL-IJCNLP. Beijing, China, pp. 1793\u20131803.", "citeRegEx": "Rothe and Sch\u00fctze.,? 2015", "shortCiteRegEx": "Rothe and Sch\u00fctze.", "year": 2015}, {"title": "Watset: Automatic Induction of Synsets from a Graph of Synonyms", "author": ["D. Ustalov", "A. Panchenko", "C. Biemann."], "venue": "Proc. ACL. Vancouver, Canada.", "citeRegEx": "Ustalov et al\\.,? 2017", "shortCiteRegEx": "Ustalov et al\\.", "year": 2017}, {"title": "It Makes Sense: A WideCoverage Word Sense Disambiguation System for Free Text", "author": ["Z. Zhong", "H.T. Ng."], "venue": "Proc. ACL Demos. Uppsala, Sweden, pp. 78\u201383.", "citeRegEx": "Zhong and Ng.,? 2010", "shortCiteRegEx": "Zhong and Ng.", "year": 2010}], "referenceMentions": [{"referenceID": 11, "context": "The former knowledgebased sense representations, such as those found in the BabelNet lexical semantic network (Navigli and Ponzetto, 2012), are easily interpretable by humans due to the presence of definitions, usage examples, taxonomic relations, related words, and images.", "startOffset": 110, "endOffset": 138}, {"referenceID": 0, "context": "(Di Marco and Navigli, 2013; Bartunov et al., 2016), require no manual labor, but the resulting sense representations lack the abovementioned features enabling interpretability.", "startOffset": 0, "endOffset": 51}, {"referenceID": 10, "context": "(2017) and is designed to reach interpretability level of knowledge-based systems, such as Babelfy (Moro et al., 2014), within an unsupervised knowledgefree framework.", "startOffset": 99, "endOffset": 118}, {"referenceID": 0, "context": "(Di Marco and Navigli, 2013; Bartunov et al., 2016), require no manual labor, but the resulting sense representations lack the abovementioned features enabling interpretability. For instance, systems based on sense embeddings are based on dense uninterpretable vectors. Therefore, the meaning of a sense can be interpreted only on the basis of a list of related senses. We present a system that brings interpretability of the knowledge-based sense representations into the world of unsupervised knowledge-free WSD models. The contribution of this paper is the first system for word sense induction and disambiguation, which is unsupervised, knowledge-free, and interpretable at the same time. The system is based on the WSD approach of Panchenko et al. (2017) and is designed to reach interpretability level of knowledge-based systems, such as Babelfy (Moro et al.", "startOffset": 29, "endOffset": 760}, {"referenceID": 17, "context": "Knowledge-Based and/or Supervised Systems IMS (Zhong and Ng, 2010) is a supervised allwords WSD system that allows users to integrate additional features and different classifiers.", "startOffset": 46, "endOffset": 66}, {"referenceID": 15, "context": "The AutoExtend (Rothe and Sch\u00fctze, 2015) approach can be used to learn embeddings for lexemes and synsets", "startOffset": 15, "endOffset": 40}, {"referenceID": 10, "context": "Babelfy (Moro et al., 2014) is a system based on the BabelNet that implements a multilingual graph-based approach to entity linking and WSD based on the identification of candidate meanings using the densest subgraph heuristic.", "startOffset": 8, "endOffset": 27}, {"referenceID": 0, "context": "AdaGram (Bartunov et al., 2016) is a system that learns sense embeddings using a Bayesian extension of the Skip-gram model and provides WSD functionality based on the induced sense inventory.", "startOffset": 8, "endOffset": 31}, {"referenceID": 14, "context": "SenseGram (Pelevina et al., 2016) is a system that transforms word embeddings to sense embeddings via graph clustering and uses them for WSD.", "startOffset": 10, "endOffset": 33}, {"referenceID": 11, "context": "Knowledge-Free and Unsupervised Systems Neelakantan et al. (2014) proposed a multi-sense extension of the Skip-gram model that features an open implementation.", "startOffset": 40, "endOffset": 66}, {"referenceID": 3, "context": "Instead, these are induced from the input text corpus using the JoBimText approach (Biemann and Riedl, 2013) implemented using the Apache Spark framework4, enabling seamless processing of large text collections.", "startOffset": 83, "endOffset": 108}, {"referenceID": 1, "context": "Second, word senses are induced by clustering of an ego-network of related words (Biemann, 2006).", "startOffset": 81, "endOffset": 96}, {"referenceID": 13, "context": "For more details about the model induction process refer to (Panchenko et al., 2017).", "startOffset": 60, "endOffset": 84}, {"referenceID": 1, "context": "Instead, these are induced from the input text corpus using the JoBimText approach (Biemann and Riedl, 2013) implemented using the Apache Spark framework4, enabling seamless processing of large text collections. Induction of a WSD model consists of several steps. First, a graph of semantically related words, i.e. a distributional thesaurus, is extracted. Second, word senses are induced by clustering of an ego-network of related words (Biemann, 2006). Each discovered word sense is represented as a cluster of words. Next, the induced sense inventory is used as a pivot to generate sense representations by aggregation of the context clues of cluster words. To improve interpretability of the sense clusters they are labeled with hypernyms, which are in turn extracted from the input corpus using Hearst (1992) patterns.", "startOffset": 84, "endOffset": 814}, {"referenceID": 1, "context": "To build this model, induced word senses are first globally clustered using the Chinese Whispers graph clustering algorithm (Biemann, 2006).", "startOffset": 124, "endOffset": 139}, {"referenceID": 6, "context": "The edges in this sense graph are established by disambiguation of the related words (Faralli et al., 2016; Ustalov et al., 2017).", "startOffset": 85, "endOffset": 129}, {"referenceID": 16, "context": "The edges in this sense graph are established by disambiguation of the related words (Faralli et al., 2016; Ustalov et al., 2017).", "startOffset": 85, "endOffset": 129}, {"referenceID": 5, "context": "Faralli and Navigli (2012) showed that Web search engines can be used to acquire information about word senses.", "startOffset": 0, "endOffset": 27}, {"referenceID": 13, "context": "In our prior work (Panchenko et al., 2017), we performed a thorough evaluation of the method implemented in our system on two datasets showing the state-of-the-art performance of the approach as compared to other unsupervised knowledge-free", "startOffset": 18, "endOffset": 42}, {"referenceID": 6, "context": "The induced senses are linked to BabelNet using the method of Faralli et al. (2016) (F).", "startOffset": 62, "endOffset": 84}, {"referenceID": 8, "context": "methods for WSD, including participants of the SemEval 2013 Task 13 (Jurgens and Klapaftis, 2013) and two unsupervised knowledge-free WSD systems based on word sense embeddings (Bartunov et al.", "startOffset": 68, "endOffset": 97}, {"referenceID": 0, "context": "methods for WSD, including participants of the SemEval 2013 Task 13 (Jurgens and Klapaftis, 2013) and two unsupervised knowledge-free WSD systems based on word sense embeddings (Bartunov et al., 2016; Pelevina et al., 2016).", "startOffset": 177, "endOffset": 223}, {"referenceID": 14, "context": "methods for WSD, including participants of the SemEval 2013 Task 13 (Jurgens and Klapaftis, 2013) and two unsupervised knowledge-free WSD systems based on word sense embeddings (Bartunov et al., 2016; Pelevina et al., 2016).", "startOffset": 177, "endOffset": 223}, {"referenceID": 2, "context": "Most of the nouns come from the TWSI (Biemann, 2012) dataset, while the remaining nouns were manually selected.", "startOffset": 37, "endOffset": 52}, {"referenceID": 13, "context": "The latter picks the sense that corresponds to the largest sense cluster (Panchenko et al., 2017).", "startOffset": 73, "endOffset": 97}], "year": 2017, "abstractText": "Interpretability of a predictive model is a powerful feature that gains the trust of users in the correctness of the predictions. In word sense disambiguation (WSD), knowledge-based systems tend to be much more interpretable than knowledge-free counterparts as they rely on the wealth of manually-encoded elements representing word senses, such as hypernyms, usage examples, and images. We present a WSD system that bridges the gap between these two so far disconnected groups of methods. Namely, our system, providing access to several state-of-the-art WSD models, aims to be interpretable as a knowledgebased system while it remains completely unsupervised and knowledge-free. The presented tool features a Web interface for all-word disambiguation of texts that makes the sense predictions human readable by providing interpretable word sense inventories, sense representations, and disambiguation results. We provide a public API, enabling seamless integration.", "creator": "LaTeX with hyperref package"}}}