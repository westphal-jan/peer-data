{"id": "1707.06885", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jul-2017", "title": "SGNMT -- A Flexible NMT Decoding Platform for Quick Prototyping of New Models and Search Strategies", "abstract": "This paper introduces SGNMT, our experimental platform for machine translation research. SGNMT provides a generic interface to neural and symbolic scoring modules (predictors) with left-to-right semantic such as translation models like NMT, language models, translation lattices, $n$-best lists or other kinds of scores and constraints. Predictors can be combined with other predictors to form complex decoding tasks. SGNMT implements a number of search strategies for traversing the space spanned by the predictors which are appropriate for different predictor constellations. Adding new predictors or decoding strategies is particularly easy, making it a very efficient tool for prototyping new research ideas. SGNMT is actively being used by students in the MPhil program in Machine Learning, Speech and Language Technology at the University of Cambridge for course work and theses, as well as for most of the research work in our group.", "histories": [["v1", "Fri, 21 Jul 2017 13:14:25 GMT  (194kb,D)", "http://arxiv.org/abs/1707.06885v1", "Accepted as EMNLP 2017 demo paper"]], "COMMENTS": "Accepted as EMNLP 2017 demo paper", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["felix stahlberg", "eva hasler", "danielle saunders", "bill byrne"], "accepted": true, "id": "1707.06885"}, "pdf": {"name": "1707.06885.pdf", "metadata": {"source": "CRF", "title": "SGNMT \u2013 A Flexible NMT Decoding Platform for Quick Prototyping of New Models and Search Strategies", "authors": ["Felix Stahlberg", "Eva Hasler", "Danielle Saunders", "Bill Byrne"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "We develop an open source decoding framework called SGNMT, short for Syntactically Guided Neural Machine Translation.1 The software package supports a number of well-known frameworks, including TensorFlow2 (Abadi et al., 2016), OpenFST (Allauzen et al., 2007), Blocks / Theano (Bastien et al., 2012; van Merrie \ufffd nboer et al., 2015), and NPLM (Vaswani et al., 2013).The two central concepts in the1http: / ucam-smt.github.io / sgnmt / html / html / 2SGNMT based on the TensorFlow fork available athttps: / github.com / ehasler / tensorflowSGNMT tool are predictors and decoders."}, {"heading": "2 Predictors", "text": "SGNMT therefore emphasizes flexibility and expandability by providing a common interface to a wide range of constraints or models used in MT research. The concept enables rapid prototyping of new research ideas. Our platform aims to minimize the effort required for implementation; the decryption speed is secondary as optimized code can be produced for production systems once an idea is successful within the SGNMT framework. In SGNMT, scores are assigned to partial hypotheses via one or many predictors. A predictor usually has a single responsibility as it represents a single model or a certain type of constraint. Predictors must implement the following methods: \u2022 initialize (src sentence) initialize the predictor state using the source set. \u2022 maintain the internal predictor state. \u2022 set the internal predictor state (state)."}, {"heading": "2.1 Example Predictor Constellations", "text": "In the case of multiple predictors, we combine the predictor values in a linear model. The following list illustrates that various interesting decoding tasks can be formulated as predictor combinations. \u2022 nmt: A single NMT predictor represents a pure NMT decoding within our framework. \u2022 nmt, nmt, nmt, nmt: The use of multiple NMT predictors is a natural way to represent ensemble decoding (Hansen and Salamon, 1990; Sutskever et al., 2014) within our framework. \u2022 fst, nmt: NMT decoding limited to one FST. This can be used for rescoring neural lattices (Stolberg et al., 2016) or other types of constraints, for example in the context of source-side simplification of NMT (Hasler et al., 2016) or chord progressions in \"Bach\" (Tomczak, 2016)."}, {"heading": "3 Decoders", "text": "The list of predictors determines how (partial) hypotheses are initialized by implementing the methods (\u00b7), is invoked always before decoding a new set (\u00b7), can be described about the remaining methods, and consumes (\u00b7), implements versions of these methods that are applicable to all predictors in the list (\u00b7), and consumes next (\u00b7), and consumes next (\u00b7)."}, {"heading": "4 Conclusion", "text": "In this paper, we introduced our SGNMT platform for prototyping new approaches to MT that include both neural and symbolic models. SGNMT supports a number of different models and constraints through a common interface (predictors) and different search strategies (decoders). In addition, SGNMT focuses on minimizing the implementation burden of adding new predictors and decoders by decoupling scoring modules from each other and from the search algorithm. SGNMT is actively used in teaching and research and makes important contributions to its development, for example by implementing new predictors to use models that have been trained with other frameworks and tools."}, {"heading": "Acknowledgments", "text": "This work was supported by the UK Engineering and Physical Sciences Research Council (EPSRC grant EP / L027623 / 1)."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Pushdown automata in statistical machine translation", "author": ["Cyril Allauzen", "Bill Byrne", "Adri\u00e0 de Gispert", "Gonzalo Iglesias", "Michael Riley."], "venue": "Computational Linguistics, 40(3):687\u2013723.", "citeRegEx": "Allauzen et al\\.,? 2014", "shortCiteRegEx": "Allauzen et al\\.", "year": 2014}, {"title": "OpenFst: A general and efficient weighted finite-state transducer library", "author": ["Cyril Allauzen", "Michael Riley", "Johan Schalkwyk", "Wojciech Skut", "Mehryar Mohri."], "venue": "International Conference on Implementation and Application of Automata, pages 11\u201323.", "citeRegEx": "Allauzen et al\\.,? 2007", "shortCiteRegEx": "Allauzen et al\\.", "year": 2007}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: New features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio."], "venue": "NIPS.", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Variable length word encodings for neural translation models", "author": ["Jiameng Gao."], "venue": "MPhil dissertation, University of Cambridge.", "citeRegEx": "Gao.,? 2016", "shortCiteRegEx": "Gao.", "year": 2016}, {"title": "Neural network ensembles", "author": ["Lars Kai Hansen", "Peter Salamon."], "venue": "IEEE transactions on pattern analysis and machine intelligence, 12(10):993\u2013 1001.", "citeRegEx": "Hansen and Salamon.,? 1990", "shortCiteRegEx": "Hansen and Salamon.", "year": 1990}, {"title": "Source sentence simplification for statistical machine translation", "author": ["Eva Hasler", "Adri\u00e0 de Gispert", "Felix Stahlberg", "Aurelien Waite", "Bill Byrne."], "venue": "Computer Speech & Language.", "citeRegEx": "Hasler et al\\.,? 2016", "shortCiteRegEx": "Hasler et al\\.", "year": 2016}, {"title": "A comparison of neural models for word ordering", "author": ["Eva Hasler", "Felix Stahlberg", "Marcus Tomalin", "Adri\u00e0 de Gispert", "Bill Byrne."], "venue": "INLG, Santiago de Compostela, Spain.", "citeRegEx": "Hasler et al\\.,? 2017", "shortCiteRegEx": "Hasler et al\\.", "year": 2017}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn."], "venue": "ACL, pages 690\u2013696, Sofia, Bulgaria.", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Is neural machine translation ready for deployment? a case study on 30 translation directions", "author": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Hieu Hoang."], "venue": "arXiv preprint arXiv:1610.01108.", "citeRegEx": "Junczys.Dowmunt et al\\.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley", "Jan Chorowski", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1506.00619.", "citeRegEx": "Merri\u00ebnboer et al\\.,? 2015", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "ASPEC: Asian scientific paper excerpt corpus", "author": ["Toshiaki Nakazawa", "Manabu Yaguchi", "Kiyotaka Uchimoto", "Masao Utiyama", "Eiichiro Sumita", "Sadao Kurohashi", "Hitoshi Isahara."], "venue": "LREC, pages 2204\u20132208, Portoroz, Slovenia.", "citeRegEx": "Nakazawa et al\\.,? 2016", "shortCiteRegEx": "Nakazawa et al\\.", "year": 2016}, {"title": "Artificial Intelligence: A Modern Approach, 2 edition", "author": ["Stuart J. Russell", "Peter Norvig."], "venue": "Pearson Education.", "citeRegEx": "Russell and Norvig.,? 2003", "shortCiteRegEx": "Russell and Norvig.", "year": 2003}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "ACL, pages 1715\u20131725, Berlin, Germany.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Efficient left-to-right hierarchical phrase-based translation with improved reordering", "author": ["Maryam Siahbani", "Baskaran Sankaran", "Anoop Sarkar."], "venue": "EMNLP, pages 1089\u20131099, Seattle, Washington, USA.", "citeRegEx": "Siahbani et al\\.,? 2013", "shortCiteRegEx": "Siahbani et al\\.", "year": 2013}, {"title": "Neural machine translation by minimising the Bayes-risk with respect to syntactic translation lattices", "author": ["Felix Stahlberg", "Adri\u00e0 de Gispert", "Eva Hasler", "Bill Byrne."], "venue": "EACL, pages 362\u2013368, Valencia, Spain.", "citeRegEx": "Stahlberg et al\\.,? 2017", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2017}, {"title": "Syntactically guided neural machine translation", "author": ["Felix Stahlberg", "Eva Hasler", "Aurelien Waite", "Bill Byrne."], "venue": "ACL, pages 299\u2013305, Berlin, Germany.", "citeRegEx": "Stahlberg et al\\.,? 2016", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2016}, {"title": "SRILM \u2013 an extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "Interspeech, volume 2002, page 2002.", "citeRegEx": "Stolcke,? 2002", "shortCiteRegEx": "Stolcke", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "NIPS, pages 3104\u20133112. MIT Press.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Bachbot", "author": ["Marcin Tomczak."], "venue": "MPhil dissertation, University of Cambridge.", "citeRegEx": "Tomczak.,? 2016", "shortCiteRegEx": "Tomczak.", "year": 2016}, {"title": "Decoding with largescale neural language models improves translation", "author": ["Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang."], "venue": "EMNLP, pages 1387\u20131392, Seattle, Washington, USA.", "citeRegEx": "Vaswani et al\\.,? 2013", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "1 The software package supports a number of well-known frameworks, including TensorFlow2 (Abadi et al., 2016), OpenFST (Allauzen et al.", "startOffset": 89, "endOffset": 109}, {"referenceID": 2, "context": ", 2016), OpenFST (Allauzen et al., 2007), Blocks/Theano (Bastien et al.", "startOffset": 17, "endOffset": 40}, {"referenceID": 4, "context": ", 2007), Blocks/Theano (Bastien et al., 2012; van Merri\u00ebnboer et al., 2015), and NPLM (Vaswani et al.", "startOffset": 23, "endOffset": 75}, {"referenceID": 21, "context": ", 2015), and NPLM (Vaswani et al., 2013).", "startOffset": 18, "endOffset": 40}, {"referenceID": 5, "context": "3 The first project involved using SGNMT with OpenFST for applying subword models in SMT (Gao, 2016).", "startOffset": 89, "endOffset": 100}, {"referenceID": 20, "context": "The second project developed automatic music composition by LSTMs where WFSAs were used to define the space of allowable chord progressions in \u2018Bach\u2019 chorales (Tomczak, 2016).", "startOffset": 159, "endOffset": 174}, {"referenceID": 4, "context": "Supports Blocks/Theano (Bastien et al., 2012; van Merri\u00ebnboer et al., 2015) and TensorFlow (Abadi et al.", "startOffset": 23, "endOffset": 75}, {"referenceID": 0, "context": ", 2015) and TensorFlow (Abadi et al., 2016).", "startOffset": 23, "endOffset": 43}, {"referenceID": 17, "context": "fst Predictor for rescoring deterministic lattices (Stahlberg et al., 2016).", "startOffset": 51, "endOffset": 75}, {"referenceID": 1, "context": "rtn Rescoring recurrent transition networks (RTNs) as created by HiFST (Allauzen et al., 2014) with late expansion.", "startOffset": 71, "endOffset": 94}, {"referenceID": 9, "context": "srilm n-gram Kneser-Ney language model using the SRILM (Heafield et al., 2013; Stolcke et al., 2002) toolkit.", "startOffset": 55, "endOffset": 100}, {"referenceID": 21, "context": "nplm Neural n-gram language models based on NPLM (Vaswani et al., 2013).", "startOffset": 49, "endOffset": 71}, {"referenceID": 8, "context": "bow Restricts the search space to a bag of words with or without repetition (Hasler et al., 2017).", "startOffset": 76, "endOffset": 97}, {"referenceID": 15, "context": "lrhiero Experimental implementation of leftto-right Hiero (Siahbani et al., 2013) for small grammars.", "startOffset": 58, "endOffset": 81}, {"referenceID": 0, "context": "Predictor Description nmt Attention-based neural machine translation following Bahdanau et al. (2015). Supports Blocks/Theano (Bastien et al.", "startOffset": 79, "endOffset": 102}, {"referenceID": 0, "context": ", 2015) and TensorFlow (Abadi et al., 2016). fst Predictor for rescoring deterministic lattices (Stahlberg et al., 2016). nfst Predictor for rescoring nondeterministic lattices. rtn Rescoring recurrent transition networks (RTNs) as created by HiFST (Allauzen et al., 2014) with late expansion. srilm n-gram Kneser-Ney language model using the SRILM (Heafield et al., 2013; Stolcke et al., 2002) toolkit. nplm Neural n-gram language models based on NPLM (Vaswani et al., 2013). rnnlm Integrates RNN language models with TensorFlow as described by Zaremba et al. (2014). forced Forced decoding with a single reference.", "startOffset": 24, "endOffset": 568}, {"referenceID": 0, "context": ", 2015) and TensorFlow (Abadi et al., 2016). fst Predictor for rescoring deterministic lattices (Stahlberg et al., 2016). nfst Predictor for rescoring nondeterministic lattices. rtn Rescoring recurrent transition networks (RTNs) as created by HiFST (Allauzen et al., 2014) with late expansion. srilm n-gram Kneser-Ney language model using the SRILM (Heafield et al., 2013; Stolcke et al., 2002) toolkit. nplm Neural n-gram language models based on NPLM (Vaswani et al., 2013). rnnlm Integrates RNN language models with TensorFlow as described by Zaremba et al. (2014). forced Forced decoding with a single reference. forcedlst n-best list rescoring. bow Restricts the search space to a bag of words with or without repetition (Hasler et al., 2017). lrhiero Experimental implementation of leftto-right Hiero (Siahbani et al., 2013) for small grammars. wc Number of words feature. unkc Applies a Poisson model for the number of UNKs in the output. ngramc Integrates external n-gram posteriors, e.g. for MBR-based NMT according Stahlberg et al. (2017). length Target sentence length model using simple source sentence features.", "startOffset": 24, "endOffset": 1049}, {"referenceID": 6, "context": "ble decoding (Hansen and Salamon, 1990; Sutskever et al., 2014) in our framework.", "startOffset": 13, "endOffset": 63}, {"referenceID": 19, "context": "ble decoding (Hansen and Salamon, 1990; Sutskever et al., 2014) in our framework.", "startOffset": 13, "endOffset": 63}, {"referenceID": 17, "context": "This can be used for neural lattice rescoring (Stahlberg et al., 2016) or other kinds of constraints, for example in the context of source side simplification in MT (Hasler et al.", "startOffset": 46, "endOffset": 70}, {"referenceID": 7, "context": ", 2016) or other kinds of constraints, for example in the context of source side simplification in MT (Hasler et al., 2016) or chord progressions in \u2018Bach\u2019 (Tomczak, 2016).", "startOffset": 102, "endOffset": 123}, {"referenceID": 20, "context": ", 2016) or chord progressions in \u2018Bach\u2019 (Tomczak, 2016).", "startOffset": 40, "endOffset": 55}, {"referenceID": 22, "context": "\u2022 nmt,rnnlm,srilm,nplm: Combining NMT with three kinds of language models: An RNNLM (Zaremba et al., 2014), a Kneser-Ney n-gram LM (Heafield et al.", "startOffset": 84, "endOffset": 106}, {"referenceID": 9, "context": ", 2014), a Kneser-Ney n-gram LM (Heafield et al., 2013; Stolcke et al., 2002), and a feedforward neural network LM (Vaswani et al.", "startOffset": 32, "endOffset": 77}, {"referenceID": 21, "context": ", 2002), and a feedforward neural network LM (Vaswani et al., 2013).", "startOffset": 45, "endOffset": 67}, {"referenceID": 3, "context": "(Bahdanau et al., 2015).", "startOffset": 0, "endOffset": 23}, {"referenceID": 13, "context": "astar A* search (Russell and Norvig, 2003).", "startOffset": 16, "endOffset": 42}, {"referenceID": 16, "context": "\u2022 nmt,ngramc,wc: MBR-based NMT following Stahlberg et al. (2017) with n-gram posteriors extracted from an SMT lattice (ngramc) and a simple word penalty (wc).", "startOffset": 41, "endOffset": 65}, {"referenceID": 10, "context": "6 This decoding speed seems to be slightly faster than sequential decoding with high-performance NMT decoders like Marian-NMT (Junczys-Dowmunt et al., 2016) with reported decoding speeds of 865 words per second.", "startOffset": 126, "endOffset": 156}, {"referenceID": 15, "context": "6 words per second with the word-based NMT model described in Stahlberg et al. (2016).6 This decoding speed seems to be slightly faster than sequential decoding with high-performance NMT decoders like Marian-NMT (Junczys-Dowmunt et al.", "startOffset": 62, "endOffset": 86}, {"referenceID": 14, "context": "This makes it possible to decode by combining scores from both a subword-unit (BPE) based NMT (Sennrich et al., 2016) and a word-based NMT model with character-based NMT, masking the BPE-based", "startOffset": 94, "endOffset": 117}, {"referenceID": 12, "context": "5 BLEU over a single system on the Japanese-English ASPEC test set (Nakazawa et al., 2016) by combining three BPE-based NMT models from Stahlberg et al.", "startOffset": 67, "endOffset": 90}, {"referenceID": 12, "context": "5 BLEU over a single system on the Japanese-English ASPEC test set (Nakazawa et al., 2016) by combining three BPE-based NMT models from Stahlberg et al. (2017) using the", "startOffset": 68, "endOffset": 160}], "year": 2017, "abstractText": "This paper introduces SGNMT, our experimental platform for machine translation research. SGNMT provides a generic interface to neural and symbolic scoring modules (predictors) with left-to-right semantic such as translation models like NMT, language models, translation lattices, n-best lists or other kinds of scores and constraints. Predictors can be combined with other predictors to form complex decoding tasks. SGNMT implements a number of search strategies for traversing the space spanned by the predictors which are appropriate for different predictor constellations. Adding new predictors or decoding strategies is particularly easy, making it a very efficient tool for prototyping new research ideas. SGNMT is actively being used by students in the MPhil program in Machine Learning, Speech and Language Technology at the University of Cambridge for course work and theses, as well as for most of the research work in our group.", "creator": "LaTeX with hyperref package"}}}