{"id": "1707.06887", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jul-2017", "title": "A Distributional Perspective on Reinforcement Learning", "abstract": "In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.", "histories": [["v1", "Fri, 21 Jul 2017 13:21:54 GMT  (721kb,D)", "http://arxiv.org/abs/1707.06887v1", "ICML 2017"]], "COMMENTS": "ICML 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["marc g bellemare", "will dabney", "r\u00e9mi munos"], "accepted": true, "id": "1707.06887"}, "pdf": {"name": "1707.06887.pdf", "metadata": {"source": "META", "title": "A Distributional Perspective on Reinforcement Learning", "authors": ["Marc G. Bellemare", "Will Dabney", "R\u00e9mi Munos"], "emails": ["<bellemare@google.com>."], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "2. Setting", "text": "We consider an actor who interacts with an environment in the usual way: At each step, the actor selects an action based on his current state, to which the environment responds with a reward and the next state. We model this interaction as a time-homogeneous Markov decision-making process (X, A, R, P, \u03b3). As usual, X and A are the state and action spaces, P is the transitional core P (\u00b7 | x, a), GP [0, 1] is the discount factor, and R is the reward function that we explicitly treat in this work as a random variable."}, {"heading": "2.1. Bellman\u2019s Equations", "text": "The yield Z\u03c0 is the sum of discounted rewards along the interaction curve of the agent: Q\u03c0 (x, a): = EZ\u03c0 (x, a) = E [\u221e \u2211 t = 0 \u03b3tR (xt, at)], (1) xt \u0445 P (\u00b7 | xt \u2212 1, at \u2212 1), at \u0445 \u03c0 (\u00b7 | xt), x0 = x, a0 = a.Basic for amplification learning is the use of Bellman's equation (Bellman, 1957) to describe the value function: Q\u03c0 (x, a) = ER (x, a) + E P (x, a \u2032, a \u2032).In amplification learning, we are typically interested in maximizing the return. The most common ap-proach for this includes the optimum equation Q (x, a)."}, {"heading": "3. The Distributional Bellman Operators", "text": "In this paper, we take the expectations in Bellman's equations and instead look at the complete distribution of the random variables Z\u03c0. From here, we look at Z\u03c0 as a mapping of state action pairs to yield distributions and call it the distribution of value. Our first goal is to gain an understanding of the theoretical behavior of the distribution analogies of Bellman operators, especially in the less well understood control setting. The reader who is strictly interested in the algorithmic contribution may choose to skip this section."}, {"heading": "3.1. Distributional Equations", "text": "The reader who is not familiar with the mea-safe theory could \"imagine the space of all possible results of an experiment\" (Billingsley, 1995).We will write \"u-p\" to denote the Lp norm of a vector u-RX for 1 \u2264 p; the same is true for vectors in RX-A. The Lp norm of a random vector U-RX (or RX-A) is then \"U-P\": = [E [N [E [E [E [E [E] p]]]] 1 / p, and for p = \u00b2 we have \"U-R\" = ess sup-U (N). (We will omit the dependence on \"N\" whenever it is unambiguous).We will call the c.d.f. of a random variable U by FU (y): = Pr {U-Buable (y) and its inverse distribution.f. by F \u2212 U-Q-U (1-Q) the equation between the two sides (equal)."}, {"heading": "3.2. The Wasserstein Metric", "text": "The main tool for our analysis is the Waterstone metric dp between cumulative distribution functions (see e.g. Bickel & Freedman, 1981, where it is called the Mallows metric).For F, G two c.d.fs above the real values, it is defined as asdp (F, G): = inf U, V, U \u2212 V, where the infimum is uniformly distributed to [0, 1]: dp (F, G) = F \u2212 1 (U) \u2212 G \u2212 1 (U) distributions p.p For p < Lemlt; this is more explicitly written asdp (F, G) f."}, {"heading": "3.3. Policy Evaluation", "text": "Within the framework of the policy assessment (Sutton & Barto, 1998) we are interested in the value function V \u03c0 associated with a given policy. The analogy to this is the value distribution Z\u03c0. In this section we characterize Z\u03c0 and examine the behavior of the policy assessment operator T \u03c0. We emphasize that Z\u03c0 describes the intrinsic randomness of the agent's interactions with its environment, and not any degree of uncertainty about the environment itself. We consider the reward function as a random vector R-Z and define the transitional operator P\u03c0 ZP\u03c0Z (x, a) D: = Z (X \u2032, A \u2032, A \u00b2) X \u00b2 P (\u00b7 x, a) A Vector R-Z and define the transitional operator: Z ZP\u03c0Z (x, a) D: = Z (X \u2032, A \u2032, A \u00b2) X \u00b2 P (x, a) A Vector R-Z (x, a)"}, {"heading": "3.3.2. CONTRACTION IN CENTERED MOMENTS", "text": "Note that d2 (U, V) (and more generally dp) refers to a coupling C (\u03c9): = U (\u03c9) \u2212 V (\u03c9), in the sense that d22 (U, V) \u2264 E [(U \u2212 V) 2] = V (C) + (EC) 2. Consequently, we cannot directly use d2 to bind the difference in variance | V (T \u03c0Z (x, a) \u2212 V (Z\u03c0 (x, a))). In fact, however, T \u03c0 is a contraction of variance (Sobel, 1982, see also Appendix). Generally, T \u03c0 is not a contraction at the pth-centered moment, p > 2, but the centered moments of the iterate {Zk} still converge exponentially fast with those of centered; the evidence extends to the result of Ro \ufffd sler (1992)."}, {"heading": "3.4. Control", "text": "So far, we have considered a fixed policy \u03c0 and studied the behavior of its associated operator T \u03c0. We have now set out to understand the distributive actors of the control - striving for a policy \u03c0 that maximizes value - and the corresponding notion of an optimal value distribution. As with the optimal value function, this notion is closely linked to that of an optimal policy. However, while all optimal strategies achieve the same value Q, in our case the difficulty arises: Generally, there are many optimal value distributions. In this section, we show that the distributional analog of the Bellman optimality operator approaches the amount of optimal value distributions in a weak sense. However, this operator is not a contradiction in any measure between distributions and is generally much more moderate than the political evaluation actors. We believe that the convergence issues we outline here are a symptom of the optimal instability of the optimal distributions policy, and are generally much more moderate than the political evaluation actors."}, {"heading": "T Z = T \u03c0Z for some \u03c0 \u2208 GZ .", "text": "As in the framework of the policy assessment, we are interested in the behavior of the iterates Zk + 1: = T Zk, Z0-Z. Our first result is the assertion that EZk behaves as expected. Tagma 4. Let us leave Z1, Z2-Z. Then we could expect that Zk converges quickly in d-p to a fixed point in Z-Z. Unfortunately, convergence is neither fast nor safe to reach a fixed point. In fact, the best we can hope for is pointless convergence, not even to the fixed Z group, but to the larger group of non-stationary optimal value distributions. Definition 3. A non-stationary optimal value distribution Z-Z is the value distribution, which corresponds to a sequence of optimal strategies."}, {"heading": "T Z\u2217 = T \u03c0Z\u2217 with \u03c0 \u2208 GZ\u2217 , \u03c0 \u227a \u03c0\u2032 \u2200\u03c0\u2032 \u2208 GZ\u2217 \\ {\u03c0}.", "text": "However, the comparison of Theorem 1 to Lemma 4 shows a significant difference between the distribution frame and the usual setting of the expected return. While the mean of Zk converges exponentially fast to Z. (Figure 2, left), its distribution need not be so good! To emphasize this difference, we now provide a number of negative results regarding T. Suggestion 1. The operator T is not a contraction. Consider the following example (Figure 2, left). There are two states, x1 and x2; a clear transition from x1 to x2; from x2, action a1 does not yield a reward, while the optimal action a2 is equally likely to yield 1 + or \u2212 1 +. Both actions are endless. There is a unique optimal policy and therefore a unique fixed score. Now, we consider Z sufficient in Figure 2 (right) and its distance to Z."}, {"heading": "4. Approximate Distributional Learning", "text": "In this section we propose an algorithm based on the distributional Bellman optimality operator, which requires in particular the selection of an approximate distribution. Although the Gaussian case has already been considered (Morimura et al., 2010a; Tamar et al., 2016), to the best of our knowledge we are the first to use a rich class of parametric distributions."}, {"heading": "4.1. Parametric Distribution", "text": "We will model the value distribution using a discrete distribution parameterized by N-N and VMIN, VMAX-R, whose carriers are the quantity of atoms {zi = VMIN + i4z: 0 \u2264 i < N}, 4z: = VMAX \u2212 VMINN \u2212 1. In a sense, these atoms are the \"canonical yields\" of our distribution. The atomic probabilities are given by a parametric model \u03b8: X \u00b7 A \u2192 RNZ\u03b8 (x, a) = zi w.p. pi (x, a): = e\u03b8i (x, a) \u2211 j e \u03b8j (x, a).The discrete distribution has the advantage of being highly expressive and computer friendly (see e.g. Van den Oord et al., 2016)."}, {"heading": "4.2. Projected Bellman Update", "text": "Using a discrete distribution, a problem arises: the Bellman update T ZTB and our parameterization ZTB almost always have discrete supports. Analysis of Section 3 reveals that the Waterstone metric (considered a loss) between T ZTB and ZTB, which is also conveniently robust to support discrepancies, prevents this: in practice, we are typically limited to learning from the sample transitions what is not possible under the Waterstone loss (see Prop. 5 and toy results in the appendix). Instead, we project the sample update T-Z to support ZTB (Figure 1, algorithm 1), effectively reducing the Bellman update to the multiclass classification. Let's get rid of the greedy policy w.r.t. Let's give the EZZ distribution a sample transition (x, r, x), we calculate the Bellman update T-zj: r = +."}, {"heading": "5. Evaluation on Atari 2600 Games", "text": "To understand the approach in a complex environment, we applied the categorical algorithm to games from Ar-1 algorithm 1, which calculates this projection in a temporally linear way in N.cade Learning Environment (ALE; Bellemare et al., 2013). While ALE is deterministic, stochasticity occurs in a number of manifestations: 1) from aliasing the state, 2) from learning non-stationary politics, and 3) from approximation errors. We used five training games (Fig 3) and 52 test games. For our study, we used the DQN architecture (Mnih et al., 2015), but the output of the atomic probabilities pi (x, a) instead of action values, and chose VMAX = \u2212 VMIN = 10 from preliminary experiments on the training games. We call the resulting architecture the DQN values."}, {"heading": "5.1. Varying the Number of Atoms", "text": "We started to study the performance of our algorithm in relation to the training games in relation to the number of atoms (Figure 3). For this experiment, we set = 0.05. The data show that too few atoms can lead to bad behaviour, and that more always increases performance; this is not immediately obvious, since we expected a saturation of network capacity. Particularly striking is the performance difference between the 51 atom version and DQN: the latter is exceeded in all five games, and in SEAQUEST we reach the state of the art. An additional point of comparison shows that the Bernoulli algorithm performs better than DQN in three out of five games with just one parameter, and above all more robust in ASTERIX.2For N = 51, our TensorFlow implementation trains at about 75% of the speed of the DQN."}, {"heading": "5.2. State-of-the-Art Results", "text": "The performance of the 51 Atomic Agent (from here, C51) in the training games presented in the last section is particularly noteworthy as it did not incorporate any of the other algorithmic ideas present in state-of-the-art agents. Next, we asked if the inclusion of the most common hyperparameter selection, namely a smaller training, could lead to even better results. Specifically, we use 0.01 (instead of 0.05), each million frames that evaluate the performance of our agent at = 0.001. We compare our algorithm with DQN (= 0.01), Double DQN (van Hasselt et al., 2016), the dual architecture (Wang et al., 2016) and Prioritized Replay (Schaul et al., 2016), achieving the best evaluation results during training, we see that C51 significantly outperforms these other algorithms (Figures 6 and 7)."}, {"heading": "6. Discussion", "text": "In this work, we tried to get a more complete picture of reinforcement learning, which includes value distributions, and found that learning value distributions are a powerful idea that allows us to surpass most of the gains previously made on Atari 2600 without making further algorithmic adjustments."}, {"heading": "6.1. Why does learning a distribution matter?", "text": "It is surprising that we have a policy aimed at maximising returns."}, {"heading": "Acknowledgements", "text": "The authors acknowledge the important role played by their colleagues at DeepMind throughout the development of this work. Special thanks are due to Yee Whye Teh, Alex Graves, Joel Veness, Guillaume Desjardins, Tom Schaul, David Silver, Andre Barreto, Max Jaderberg, Mohammad Azar, Georg Ostrovski, Bernardo Avila Pires, Olivier Pietquin, Audrunas Gruslys, Tom Stepleton, Aaron van den Oord and in particular Chris Maddison for his comprehensive review of an earlier draft. Thanks are also due to Marek Petrik for references to the relevant literature and Mark Rowland for fine-tuning details in the final version.Erratum The camera-enabled copy of this paper incorrectly gave an average value of 1010% for C51. The corrected number is 701%, which remains higher than the other comparable baseline. The median value remains unchanged at 178%. The error was attributable to standard rating phases in a game score of 830 minutes, while the other one was 830 minutes, which we believe lasted for 428 minutes."}, {"heading": "A. Related Work", "text": "The authors propose to explore the mechanisms of distribution for distributional justice. They also provide a theoretical analysis of distributional relations, including consistency in the distribution question. In contrast, we analyze the distributional relations in distributional justice. Distributional justice in distributional justice is increasing. Distributional justice in distributional justice is increasing."}, {"heading": "B. Proofs", "text": "It follows that we can choose Yi, Zi so that also Yi, Zi, Zi, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i"}, {"heading": "T Z\u2217 = T \u03c0Z\u2217 with \u03c0 \u2208 GZ\u2217 , \u03c0 \u227a \u03c0\u2032 \u2200\u03c0\u2032 \u2208 GZ\u2217 \\ {\u03c0},", "text": "Then we have a unique x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}, {"heading": "C. Algorithmic Details", "text": "While our training regime is closely aligned with that of DQN (Mnih et al., 2015), we use Adam (Kingma & Ba, 2015) instead of RMSProp (Tieleman & Hinton, 2012) to recalculate the gradients. We also performed some hyperparameter settings for our final results. Specifically, we evaluated two hyperparameters during our five training games and selected the values that performed best. Hyperparameter values we looked at were VMAX values (3, 10, 100) and adam values (1 / L, 0.1 / L, 0.01 / L, 0.001 / L, 0.001 / L, 0.0001 / L), with L = 32 being the minibatch size. We found that VMAX = 10 and adam = 0.01 / L performed best. We used the same step size value as DQN (\u03b1 = 0.0025). Pseudo code for the category algorithm update is then applied to the Bellman algorithm in the first algorithm."}, {"heading": "D. Comparison of Sampled Wasserstein Loss and Categorical Projection", "text": "Lemma 3 proves that for a fixed policy \u03c0 the distributional Bellman operator is a \u03b3 contraction in d-stone = 1.4 contractional functions, and therefore that T \u03c0 in the distribution will converge on the true distribution of returns Z\u03c0. In this section we empirically confirm these results shown on the CliffWalk domain in Figure 11. The dynamics of the problem coincide with those of Sutton & Barto (1998). We also examine the convergence of the distributional Bellman operator under the sampled Wasserstein loss and the categorical projection (Eq. 7), while fol-lowing a policy that tries to take the safe path but has a 10% chance of taking another action that is universally conducted in the categorical distribution. We calculate a soil-truth distribution of returns Z\u03c0 using 10000 Monte-Carlo (MC) rollouts from each state. We then conduct two experiments by approaching the value distribution with each state in our distributions."}, {"heading": "E. Supplemental Videos and Results", "text": "In Figure 13 you will find links to supplementary videos showing the C51 agent during training on various Atari 2600 games. Figure 12 shows the relative performance of C51 during training. Figure 14 contains a table with the evaluation results and compares C51 with other state-of-the-art agents. Figures 15-18 show particularly interesting frames."}], "references": [{"title": "On the sample complexity of reinforcement learning with a generative model", "author": ["Azar", "Mohammad Gheshlaghi", "Munos", "R\u00e9mi", "Kappen", "Hilbert"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Azar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2012}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Bellemare", "Marc G", "Naddaf", "Yavar", "Veness", "Joel", "Bowling", "Michael"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "The cramer distance as a solution to biased wasserstein gradients", "author": ["Bellemare", "Marc G", "Danihelka", "Ivo", "Dabney", "Will", "Mohamed", "Shakir", "Lakshminarayanan", "Balaji", "Hoyer", "Stephan", "Munos", "R\u00e9mi"], "venue": null, "citeRegEx": "Bellemare et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2017}, {"title": "Dynamic programming", "author": ["Bellman", "Richard E"], "venue": null, "citeRegEx": "Bellman and E.,? \\Q1957\\E", "shortCiteRegEx": "Bellman and E.", "year": 1957}, {"title": "Some asymptotic theory for the bootstrap", "author": ["Bickel", "Peter J", "Freedman", "David A"], "venue": "The Annals of Statistics,", "citeRegEx": "Bickel et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 1981}, {"title": "Probability and measure", "author": ["Billingsley", "Patrick"], "venue": null, "citeRegEx": "Billingsley and Patrick.,? \\Q1995\\E", "shortCiteRegEx": "Billingsley and Patrick.", "year": 1995}, {"title": "Multitask learning", "author": ["Caruana", "Rich"], "venue": "Machine Learning,", "citeRegEx": "Caruana and Rich.,? \\Q1997\\E", "shortCiteRegEx": "Caruana and Rich.", "year": 1997}, {"title": "Discounted mdps: Distribution functions and exponential utility maximization", "author": ["Chung", "Kun-Jen", "Sobel", "Matthew J"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Chung et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Chung et al\\.", "year": 1987}, {"title": "Bayesian Q-learning", "author": ["Dearden", "Richard", "Friedman", "Nir", "Russell", "Stuart"], "venue": "In Proceedings of the National Conference on Artificial Intelligence,", "citeRegEx": "Dearden et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Dearden et al\\.", "year": 1998}, {"title": "Reinforcement learning with gaussian processes", "author": ["Engel", "Yaakov", "Mannor", "Shie", "Meir", "Ron"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Engel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Engel et al\\.", "year": 2005}, {"title": "Kalman temporal differences", "author": ["Geist", "Matthieu", "Pietquin", "Olivier"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Geist et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Geist et al\\.", "year": 2010}, {"title": "Stable function approximation in dynamic programming", "author": ["Gordon", "Geoffrey"], "venue": "In Proceedings of the Twelfth International Conference on Machine Learning,", "citeRegEx": "Gordon and Geoffrey.,? \\Q1995\\E", "shortCiteRegEx": "Gordon and Geoffrey.", "year": 1995}, {"title": "Q(\u03bb) with off-policy corrections", "author": ["Harutyunyan", "Anna", "Bellemare", "Marc G", "Stepleton", "Tom", "Munos", "R\u00e9mi"], "venue": "In Proceedings of the Conference on Algorithmic Learning", "citeRegEx": "Harutyunyan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Harutyunyan et al\\.", "year": 2016}, {"title": "An expectation maximization algorithm for continuous markov decision processes with arbitrary reward", "author": ["Hoffman", "Matthew D", "de Freitas", "Nando", "Doucet", "Arnaud", "Peters", "Jan"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Hoffman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2009}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["Jaderberg", "Max", "Mnih", "Volodymyr", "Czarnecki", "Wojciech Marian", "Schaul", "Tom", "Leibo", "Joel Z", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Jaderberg et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2017}, {"title": "Markov decision processes with a new optimality criterion: Discrete time", "author": ["Jaquette", "Stratton C"], "venue": "The Annals of Statistics,", "citeRegEx": "Jaquette and C.,? \\Q1973\\E", "shortCiteRegEx": "Jaquette and C.", "year": 1973}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["Kakade", "Sham", "Langford", "John"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Kakade et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2002}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "PAC bounds for discounted MDPs", "author": ["Lattimore", "Tor", "Hutter", "Marcus"], "venue": "In Proceedings of the Conference on Algorithmic Learning Theory,", "citeRegEx": "Lattimore et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lattimore et al\\.", "year": 2012}, {"title": "Mean-variance optimization in markov decision processes", "author": ["Mannor", "Shie", "Tsitsiklis", "John N"], "venue": null, "citeRegEx": "Mannor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2011}, {"title": "Reinforcement learning with selective perception and hidden state", "author": ["McCallum", "Andrew K"], "venue": "PhD thesis, University of Rochester,", "citeRegEx": "McCallum and K.,? \\Q1995\\E", "shortCiteRegEx": "McCallum and K.", "year": 1995}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Parametric return density estimation for reinforcement learning", "author": ["Morimura", "Tetsuro", "Hachiya", "Hirotaka", "Sugiyama", "Masashi", "Tanaka", "Toshiyuki", "Kashima", "Hisashi"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Morimura et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Morimura et al\\.", "year": 2010}, {"title": "Nonparametric return distribution approximation for reinforcement learning", "author": ["Morimura", "Tetsuro", "Sugiyama", "Masashi", "Kashima", "Hisashi", "Hachiya", "Hirotaka", "Tanaka", "Toshiyuki"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Morimura et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Morimura et al\\.", "year": 2010}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["Nair", "Arun", "Srinivasan", "Praveen", "Blackwell", "Sam", "Alcicek", "Cagdas", "Fearon", "Rory", "De Maria", "Alessandro", "Panneershelvam", "Vedavyas", "Suleyman", "Mustafa", "Beattie", "Charles", "Petersen", "Stig"], "venue": "In ICML Workshop on Deep Learning,", "citeRegEx": "Nair et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2015}, {"title": "Actor-critic algorithms for risk-sensitive mdps", "author": ["LA Prashanth", "Ghavamzadeh", "Mohammad"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Prashanth et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Prashanth et al\\.", "year": 2013}, {"title": "A fixed point theorem for distributions", "author": ["R\u00f6sler", "Uwe"], "venue": "Stochastic Processes and their Applications,", "citeRegEx": "R\u00f6sler and Uwe.,? \\Q1992\\E", "shortCiteRegEx": "R\u00f6sler and Uwe.", "year": 1992}, {"title": "Prioritized experience replay", "author": ["Schaul", "Tom", "Quan", "John", "Antonoglou", "Ioannis", "Silver", "David"], "venue": "In Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Schaul et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2016}, {"title": "The variance of discounted markov decision processes", "author": ["Sobel", "Matthew J"], "venue": "Journal of Applied Probability,", "citeRegEx": "Sobel and J.,? \\Q1982\\E", "shortCiteRegEx": "Sobel and J.", "year": 1982}, {"title": "Reinforcement learning: An introduction", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagents Systems,", "citeRegEx": "Sutton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2011}, {"title": "Learning the variance of the reward-to-go", "author": ["Tamar", "Aviv", "Di Castro", "Dotan", "Mannor", "Shie"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tamar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tamar et al\\.", "year": 2016}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Tijmen", "Hinton", "Geoffrey"], "venue": "COURSERA: Neural networks for machine learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Probabilistic inference for solving discrete and continuous state markov decision processes", "author": ["Toussaint", "Marc", "Storkey", "Amos"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Toussaint et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Toussaint et al\\.", "year": 2006}, {"title": "On the convergence of optimistic policy iteration", "author": ["Tsitsiklis", "John N"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tsitsiklis and N.,? \\Q2002\\E", "shortCiteRegEx": "Tsitsiklis and N.", "year": 2002}, {"title": "Pixel recurrent neural networks", "author": ["Van den Oord", "Aaron", "Kalchbrenner", "Nal", "Kavukcuoglu", "Koray"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["van Hasselt", "Hado", "Guez", "Arthur", "Silver", "David"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Compress and control", "author": ["Veness", "Joel", "Bellemare", "Marc G", "Hutter", "Marcus", "Chua", "Alvin", "Desjardins", "Guillaume"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,", "citeRegEx": "Veness et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Veness et al\\.", "year": 2015}, {"title": "Dual representations for dynamic programming", "author": ["Wang", "Tao", "Lizotte", "Daniel", "Bowling", "Michael", "Schuurmans", "Dale"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Wang", "Ziyu", "Schaul", "Tom", "Hessel", "Matteo", "Hasselt", "Hado van", "Lanctot", "Marc", "de Freitas", "Nando"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Mean, variance, and probabilistic criteria in finite markov decision processes: a review", "author": ["D.J. White"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "White,? \\Q1988\\E", "shortCiteRegEx": "White", "year": 1988}, {"title": "embeds the return into a graphical model, and applies probabilistic inference", "author": ["Hoffman"], "venue": null, "citeRegEx": "Hoffman,? \\Q2009\\E", "shortCiteRegEx": "Hoffman", "year": 2009}, {"title": "2015) instead of RMSProp (Tieleman & Hinton, 2012) for gradient rescaling", "author": ["Adam (Kingma", "Ba"], "venue": "(Mnih et al.,", "citeRegEx": ".Kingma and Ba,? \\Q2015\\E", "shortCiteRegEx": ".Kingma and Ba", "year": 2015}], "referenceMentions": [{"referenceID": 40, "context": "Although the distributional perspective is almost as old as Bellman\u2019s equation itself (Jaquette, 1973; Sobel, 1982; White, 1988), in reinforcement learning it has thus far been subordinated to specific purposes: to model parametric uncertainty (Dearden et al.", "startOffset": 86, "endOffset": 128}, {"referenceID": 8, "context": "Although the distributional perspective is almost as old as Bellman\u2019s equation itself (Jaquette, 1973; Sobel, 1982; White, 1988), in reinforcement learning it has thus far been subordinated to specific purposes: to model parametric uncertainty (Dearden et al., 1998), to design risk-sensitive algorithms (Morimura et al.", "startOffset": 244, "endOffset": 266}, {"referenceID": 0, "context": ", 2010b;a), or for theoretical analysis (Azar et al., 2012; Lattimore & Hutter, 2012).", "startOffset": 40, "endOffset": 85}, {"referenceID": 0, "context": ", 2010b;a), or for theoretical analysis (Azar et al., 2012; Lattimore & Hutter, 2012). By contrast, we believe the value distribution has a central role to play in reinforcement learning. Contraction of the policy evaluation Bellman operator. Basing ourselves on results by R\u00f6sler (1992) we show that, for a fixed policy, the Bellman operator over value distributions is a contraction in a maximal form of the Wasserstein (also called Kantorovich or Mallows) metric.", "startOffset": 41, "endOffset": 288}, {"referenceID": 1, "context": "We will illustrate the practical benefits of the distributional perspective in the context of the Arcade Learning Environment (Bellemare et al., 2013).", "startOffset": 126, "endOffset": 150}, {"referenceID": 21, "context": "By modelling the value distribution within a DQN agent (Mnih et al., 2015), we obtain considerably increased performance across the gamut of benchmark Atari 2600 games, and in fact achieve stateof-the-art performance on a number of games.", "startOffset": 55, "endOffset": 74}, {"referenceID": 1, "context": "We will illustrate the practical benefits of the distributional perspective in the context of the Arcade Learning Environment (Bellemare et al., 2013). By modelling the value distribution within a DQN agent (Mnih et al., 2015), we obtain considerably increased performance across the gamut of benchmark Atari 2600 games, and in fact achieve stateof-the-art performance on a number of games. Our results echo those of Veness et al. (2015), who obtained extremely fast learning by predicting Monte Carlo returns.", "startOffset": 127, "endOffset": 438}, {"referenceID": 9, "context": "Distributional equations have been used in reinforcement learning by Engel et al. (2005); Morimura et al.", "startOffset": 69, "endOffset": 89}, {"referenceID": 9, "context": "Distributional equations have been used in reinforcement learning by Engel et al. (2005); Morimura et al. (2010a) among others, and in operations research by White (1988).", "startOffset": 69, "endOffset": 114}, {"referenceID": 9, "context": "Distributional equations have been used in reinforcement learning by Engel et al. (2005); Morimura et al. (2010a) among others, and in operations research by White (1988).", "startOffset": 69, "endOffset": 171}, {"referenceID": 12, "context": "Tsitsiklis (2002) and most recently Harutyunyan et al. (2016). Let \u03a0\u2217 be the set of optimal policies.", "startOffset": 36, "endOffset": 62}, {"referenceID": 31, "context": "Although the Gaussian case has previously been considered (Morimura et al., 2010a; Tamar et al., 2016), to the best of our knowledge we are the first to use a rich class of parametric distributions.", "startOffset": 58, "endOffset": 102}, {"referenceID": 2, "context": "We note that, while these algorithms appear unrelated to the Wasserstein metric, recent work (Bellemare et al., 2017) hints at a deeper connection.", "startOffset": 93, "endOffset": 117}, {"referenceID": 1, "context": "cade Learning Environment (ALE; Bellemare et al., 2013).", "startOffset": 26, "endOffset": 55}, {"referenceID": 21, "context": "For our study, we use the DQN architecture (Mnih et al., 2015), but output the atom probabilities pi(x, a) instead of action-values, and chose VMAX = \u2212VMIN = 10 from preliminary experiments over the training games.", "startOffset": 43, "endOffset": 62}, {"referenceID": 39, "context": ", 2016), the Dueling architecture (Wang et al., 2016), and Prioritized Replay (Schaul et al.", "startOffset": 34, "endOffset": 53}, {"referenceID": 27, "context": ", 2016), and Prioritized Replay (Schaul et al., 2016), comparing the best evaluation score achieved during training.", "startOffset": 32, "endOffset": 53}, {"referenceID": 14, "context": "We believe the gradient-based categorical algorithm is able to mitigate these effects by effectively averaging the different distri\u2020 The UNREAL results are not altogether comparable, as they were generated in the asynchronous setting with per-game hyperparameter tuning (Jaderberg et al., 2017).", "startOffset": 270, "endOffset": 294}, {"referenceID": 30, "context": "A recurring theme in artificial intelligence is the idea of an agent learning from a multitude of predictions (Caruana 1997; Utgoff & Stracuzzi 2002; Sutton et al. 2011; Jaderberg et al. 2017).", "startOffset": 110, "endOffset": 192}, {"referenceID": 14, "context": "A recurring theme in artificial intelligence is the idea of an agent learning from a multitude of predictions (Caruana 1997; Utgoff & Stracuzzi 2002; Sutton et al. 2011; Jaderberg et al. 2017).", "startOffset": 110, "endOffset": 192}, {"referenceID": 14, "context": "We believe the gradient-based categorical algorithm is able to mitigate these effects by effectively averaging the different distri\u2020 The UNREAL results are not altogether comparable, as they were generated in the asynchronous setting with per-game hyperparameter tuning (Jaderberg et al., 2017). butions, similar to conservative policy iteration (Kakade & Langford, 2002). While the chattering persists, it is integrated to the approximate solution. State aliasing. Even in a deterministic environment, state aliasing may result in effective stochasticity. McCallum (1995), for example, showed the importance of coupling representation learning with policy learning in partially observable domains.", "startOffset": 271, "endOffset": 573}], "year": 2017, "abstractText": "In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman\u2019s equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.", "creator": "LaTeX with hyperref package"}}}