{"id": "1707.06945", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jul-2017", "title": "Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector Space Specialisation", "abstract": "Existing approaches to automatic VerbNet-style verb classification are heavily dependent on feature engineering and therefore limited to languages with mature NLP pipelines. In this work, we propose a novel cross-lingual transfer method for inducing VerbNets for multiple languages. To the best of our knowledge, this is the first study which demonstrates how the architectures for learning word embeddings can be applied to this challenging syntactic-semantic task. Our method uses cross-lingual translation pairs to tie each of the six target languages into a bilingual vector space with English, jointly specialising the representations to encode the relational information from English VerbNet. A standard clustering algorithm is then run on top of the VerbNet-specialised representations, using vector dimensions as features for learning verb classes. Our results show that the proposed cross-lingual transfer approach sets new state-of-the-art verb classification performance across all six target languages explored in this work.", "histories": [["v1", "Fri, 21 Jul 2017 15:52:54 GMT  (232kb,D)", "http://arxiv.org/abs/1707.06945v1", "EMNLP 2017 (long paper)"]], "COMMENTS": "EMNLP 2017 (long paper)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ivan vulic", "nikola mrksic", "anna korhonen"], "accepted": true, "id": "1707.06945"}, "pdf": {"name": "1707.06945.pdf", "metadata": {"source": "CRF", "title": "Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector Space Specialisation", "authors": ["Ivan Vuli\u0107", "Anna Korhonen"], "emails": ["iv250@cam.ac.uk", "nm480@cam.ac.uk", "alk23@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is so that most people are able to understand themselves and understand what it is all about. (...) It is not so that people are able to understand themselves. (...) It is not so that they are able to understand the world. (...) It is as if they were able to understand the world. (...) It is not so that they are able to understand the world. (...) It is as if they are able to think in themselves the world of the world, as if they were in the world of the world, as if they were able to understand the world. (...) It is as if the world in itself. (...) It is as if they were able to think in themselves the world of the world, as if they were in the world of the world, as if they were in the world of the world, as if they were in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in itself, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in itself, in the world, in itself."}, {"heading": "2 Methodology: Specialising for VerbNet", "text": "In fact, the fact is that most people who are able, are able, are able to move, are also able, are able, are able to move, are able to move, are able, are able to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able to be able, to be able to be able, to be able to be able, to be able to be able, to be able to be able, to be able, to be able to be able, to be able to be able, to be able to be able, to be able to be able, to be able to be able to be the world, to be able to be the world, to be the world, to be in them, to be in them, to be in them, to be the world, to be in them, to be the world, to be in them."}, {"heading": "2.1 Vector Space Specialisation", "text": "The answer to this question is: \"What is the answer to this question?,\" \"What is the answer?,\" \"What is the answer?,\" \"What is the answer?,\" \"What is the answer?,\" \"What is the answer?,\" \"What is the answer?,\" \"What is the answer?,\" \"What is the answer?,\" \"What is the answer?,\" \"What is the answer?,\" \"What is the answer?,\" What is the answer?, \"What is the answer?,\" What is the answer?, \"What is the answer?,\" What is the answer?, \"What is the answer?,\" What is the answer?, \"The answer:\" What is the answer?, \"What is the answer?\""}, {"heading": "2.2 Clustering Algorithm", "text": "Following on from previous work (Brew and Schulte im Walde, 2002; Sun and Korhonen, 2009; Sun et al., 2010), we use the spectral cluster algorithm MNCut (Meila and Shi, 2001), which has extensive applicability in similar NLP tasks with high-dimensional feature spaces (Chen et al., 2006; von Luxburg et al., 2007; Scarton et al., 2014 and others). Again, after previous work (Sun et al., 2010, 2013), we estimate the number of clusters KClust using the self-tuning method of Zelnik-Manor and Perona (2004). This algorithm finds the optimal number by minimizing a cost function based on the eigenvector structure of the word similarity matrix."}, {"heading": "3 Experimental Setup", "text": "We are experimenting with six target languages: French (FR), Brazilian Portuguese (PT), Italian (IT), Polish (PL), Croatian (HR) and Finnish (FI). All statistics regarding the source and size of training and test data and linguistic constraints for each target language are summarized in Tab. 2.Automatic approaches to verb class induction have been tested in previous work for FR and PT. To make the most of our knowledge, our multilingual study is the first goal to generalize an automatic induction method for more languages using an underlying methodology that separates language pairs. Initially, Vector Space: Training Data and Setup All-Language Vectors were dressed on large monolingual running text using the same setup: 300-dimensional word vectors, the frequency cutoff set to 100, bag-of-words (BOW), contexts and the window of 2, Goldberg (2014)."}, {"heading": "4 Results and Discussion", "text": "This year is the highest in the history of the country."}, {"heading": "4.1 Further Discussion and Future Work", "text": "This work has demonstrated the potential to transfer lexical resources from resource-rich to resource-poor languages, using universal, multilingual dictionaries and bilingual vector spaces as a means of transfer within a semantic specialization framework. However, we believe that the proposed basic framework can be upgraded in future work and extended through multiple research paths. First, in our current work we have worked with standard monolingual and bilingual representations, effectively ignoring the problem of verb polysemy. While several polysemia-conscious verb classification models for English have recently been developed (Kawahara et al., 2014; Peterson et al., 2016), the current lack of polysema-sensitive assessment sets in other languages is hampering this line of research. Apart from valuation issues, one idea for future work is to use the ATTRACT-REPEL specification framework for meaningful, cross-border translations in other languages."}, {"heading": "5 Conclusion", "text": "We have introduced a novel cross-lingual transfer model that enables the automatic induction of verbNet-like verbal classifications across multiple languages, based on a word vector specialization framework that is used to directly model the assumption of the cross-lingual validity of verbNet-like classifications, and our results point to significant improvements in the accuracy of verbal classification across all six target languages studied, all of which are available at: github.com / cambridgeltl / verbnets."}, {"heading": "Acknowledgments", "text": "This work is supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (No. 648909).The authors thank the entire LEXICAL team, especially Roi Reichart, and the three anonymous reviewers for their helpful and constructive suggestions."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Existing approaches to automatic VerbNetstyle verb classification are heavily dependent on feature engineering and therefore limited to languages with mature NLP pipelines. In this work, we propose a novel cross-lingual transfer method for inducing VerbNets for multiple languages. To the best of our knowledge, this is the first study which demonstrates how the architectures for learning word embeddings can be applied to this challenging syntactic-semantic task. Our method uses cross-lingual translation pairs to tie each of the six target languages into a bilingual vector space with English, jointly specialising the representations to encode the relational information from English VerbNet. A standard clustering algorithm is then run on top of the VerbNet-specialised representations, using vector dimensions as features for learning verb classes. Our results show that the proposed cross-lingual transfer approach sets new state-of-the-art verb classification performance across all six target languages explored in this work.", "creator": "LaTeX with hyperref package"}}}