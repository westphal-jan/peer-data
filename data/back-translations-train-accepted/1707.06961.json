{"id": "1707.06961", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jul-2017", "title": "Mimicking Word Embeddings using Subword RNNs", "abstract": "Word embeddings improve generalization over lexical features by placing each word in a lower-dimensional space, using distributional information obtained from unlabeled data. However, the effectiveness of word embeddings for downstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which embeddings do not exist. In this paper, we present MIMICK, an approach to generating OOV word embeddings compositionally, by learning a function from spellings to distributional embeddings. Unlike prior work, MIMICK does not require re-training on the original word embedding corpus; instead, learning is performed at the type level. Intrinsic and extrinsic evaluations demonstrate the power of this simple approach. On 23 languages, MIMICK improves performance over a word-based baseline for tagging part-of-speech and morphosyntactic attributes. It is competitive with (and complementary to) a supervised character-based model in low-resource settings.", "histories": [["v1", "Fri, 21 Jul 2017 16:18:10 GMT  (315kb,D)", "http://arxiv.org/abs/1707.06961v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yuval pinter", "robert guthrie", "jacob eisenstein"], "accepted": true, "id": "1707.06961"}, "pdf": {"name": "1707.06961.pdf", "metadata": {"source": "CRF", "title": "Mimicking Word Embeddings using Subword RNNs", "authors": ["Yuval Pinter", "Robert Guthrie", "Jacob Eisenstein"], "emails": ["uvp@gatech.edu", "rguthrie3@gatech.edu", "jacobe@gatech.edu"], "sections": [{"heading": "1 Introduction", "text": "rE \"s rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc ide the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f"}, {"heading": "2 Related Work", "text": "Several studies make use of morphological or orthographic information in the formation of word embeddings, enabling the prediction of embeddings for invisible words based on their internal structure. Botha and Blunsom (2014) calculate word embeddings by summing up via embeddings of morphemes; Luong et al. (2013) construct a recursive neural network through the morphological parses of each word. (2016) we use morphological word embeddings as a prior distribution over probable word embeddings. While morphology-based approaches make use of meaningful linguistic substructures, they struggle with names and foreign language words that include vocabulary morphembeddings. Character-based approaches avoid these problems: for example, Kim et al. (2016) we train a recurrent neural network over words whose embeddings are constructed through constellation over character embeddings."}, {"heading": "3 MIMICK Word Embeddings", "text": "We approach the problem of word selection (OOV) embedding as a generational problem: Regardless of how the original embedding was created, we assume that there is a generative word form-based protocol for creating these embedding. By building a model over the existing vocabulary, we can later use this model to predict the embedding of an invisible word. Formally, given a language L, a vocabulary V of size V, and a pre-trained embedding table W, where each word {wk} Vk = 1 is assigned to a vector ek of dimension d, our model is trained to find the function f: L \u2192 Rd so that the projected function f (wk) assigns f (wk) ek. In the face of such a model, a new word wk, a new vector L\\ V, is assigned to our model."}, {"heading": "3.1 MIMICK Polyglot Embeddings", "text": "In fact, it is a way in which the people in the most diverse regions of the world, from the USA via the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA and from the USA to the United States of America, from the USA to the United States of America, from the USA to the United States of America and from the United States of America and from the United States of America and from the United States of America and from the United States of America and from the United States of America to the United States of America and from the United States of America to the United States of America and from the United States to the United States of America, from the United States to the United States of America, from the United States to the United States of America and from the United States to the United States of America, from the United States to the United States of America and from the United States of America to the United States of the United States, from the United States of America to the United States of the United States, from the United States to the United States of America and from the United States of America to the United States of the United States, from the United States of America to the United States of the United States of the United States, from the United States of America to the United States of the United States of the United States of the United States and from the United States of America to the United States of the United States of the United States, from the United States of the United States of the United States of the United States of America to the United States of the United States of the United States of the United States, from the United States of the United States of the United States of the United States of the United States of the United States of the United States, from the United States of the United States of the United States of the United States of the United States of America to the United States of the United States of the United States of the United States of the United States of the United States of the United States, from the United States of the United States of the United States of the United States of the United States of the United States of the United States of the United States of the United States of the United States of the United States of the"}, {"heading": "4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes", "text": "The Universal Dependencies (UD) scheme (De Marneffe et al., 2014) has a minimum number of 17 POS tags (Petrov et al., 2012) and supports the marking of other language-specific features with attribute-specific inventories. For example, a verb in Turkish could be assigned a value for the attribute of probative force, one that does not exist in Danish. These additional morphosyntactic attributes are presented in the UD datasets as optional attribute value pairs. Our approach to the marking of morphosyntactic attributes is similar to the part of the retagging model by Ling et al. (2015), which pairs a projection layer to the output of a set-level bidirectional attribute values. We extend this approach to morphosyntactic attributes of morphosyntactic attributes similar to the part of the retagging model by Ling et al."}, {"heading": "5 Experimental Settings", "text": "The morphological complexity and composition of words varies widely between languages. While a morphologically rich agglutinative language such as Hungarian contains words that carry many attributes as fully separable morphemes, a sentence in an analytical language such as Vietnamese may not contain a single polymorphological or inflected word in it. To see if this property has an impact on our MIMICK model and its performance in the downstream tagging task, we select languages that include a sample of multiple morphological patterns. Language family and type are other potentially influential factors in an orthography-based approach such as ours, and so we also vary along these parameters. We have also considered language selection recommendations from de Lhoneux and Nivre (2016) and Schluter and Agic (2017). As explained above, our approach is based on the polyglot-based word embedding system."}, {"heading": "5.1 Metrics", "text": "As mentioned above, we use the UD datasets to test our MIMICK algorithm on 23 languages4 with the included Train / Development / Test department. We measure part-time work using a general token-level accuracy. For morphosyntactic attributes, there does not seem to be an agreed metric for reporting performance. Dzeroski et al. (2000) report on the accuracy per day on a morphosyntactically marked corpus of Slovenian. Faruqui et al. (2016) report macro averages of F1 values in 11 languages from UD 1.1 for the various attributes (e.g. language part, case, gender, tension); memory and precision were calculated for the complete set of values of each attribute, pooled together.5 Agic and al. (2013) report separately on parts of the language and morphosyntactic attributes."}, {"heading": "5.2 Models", "text": "We implement and test the following models: No-Char. Word embeddings are initialized by Polyglot models, with invisible words assigned to the polyglot-supplied UNK vector. After tuning experiments in all languages with enveloped font, we found it advantageous to first use an OOV form for an OOV word when its embeddings exist, and only otherwise UNK.MIMICK. Word embeddings are initialized by Polyglot, with OOV embeddings derived from a MIMICK model (Section 3) that has been trained on polyglot embeddings. In contrast to the No-Char case, the abandonment of low-text embeddings before using the MIMICK output has not brought any conclusive benefits, and so we report on the simpler No-Backoff implementations (Section 3), which are trained to use the No-Char for simple-embeddings in contrast to the MIMK output."}, {"heading": "5.3 Hyperparameters", "text": "Based on experiments with development sets, we set the following hyperparameters for all models in all languages: two LSTM layers of hidden size 128, hidden MLP layers of hidden size equal to the number of possible values of each attribute; dynamic stochastic gradient descent at 0.01 learning rate; 40 training periods (80 at 5K settings) with a drop-out rate of 0.5. CHAR \u2192 TAG models use 20-dimensional character embedding and a single hidden layer of size 128."}, {"heading": "6 Results", "text": "In fact, it is that we are able to assert ourselves, that we are able to assert ourselves, that we are able to assert ourselves, that we are able to accomplish our goals, and that we are able to accomplish our goals, \"he said."}, {"heading": "7 Conclusion", "text": "We present a simple algorithm to derive OOV word embedding vectors from pre-trained 7-character coverage in Chinese polyglot, which is surprisingly good: Only eight characters from the UD dataset are invisible in polyglot, across more than 10,000 invisible word types. 8Persian is officially classified as agglutinative, but it is largely the case in terms of derivatives. Its word insertions are rare and usually fusional.Limited vocabulary models, without access to the emerging corpus. This method is particularly useful for languages with limited resources and tasks with little available data, and is actually task agnostic. Our method improves performance over word-based models on annotated sequence tagging tasks for a wide variety of languages in all dimensions of the family, orthography and morphology. In addition, we present a BiLSTM approach to mark morphosyntactic attributes on the morphology level of the paper (possibly used in this IMK model of the IMK)."}, {"heading": "8 Acknowledgments", "text": "We thank Umashanthi Pavalanathan, Sandeep Soni, Roi Reichart and our anonymous critics for their valuable contribution. We thank Manaal Faruqui and Ryan McDonald for their help in understanding the metrics for morphosyntactic markers. The project was supported by the Defense Threat Reduction Agency's HDTRA1-15-10019 project."}], "references": [{"title": "Lemmatization and morphosyntactic tagging of Croatian and Serbian", "author": ["\u017deljko Agi\u0107", "Nikola Ljube\u0161i\u0107", "Danijela Merkler."], "venue": "4th Biennial International Workshop on Balto-Slavic Natural Language Processing (BSNLP 2013).", "citeRegEx": "Agi\u0107 et al\\.,? 2013", "shortCiteRegEx": "Agi\u0107 et al\\.", "year": 2013}, {"title": "Polyglot: Distributed word representations for multilingual NLP", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."], "venue": "Proceedings of the Conference on Natural Language Learning (CoNLL), pages 183\u2013192, Sofia, Bulgaria.", "citeRegEx": "Al.Rfou et al\\.,? 2013", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Morphological priors for probabilistic neural word embeddings", "author": ["Parminder Bhatia", "Robert Guthrie", "Jacob Eisenstein."], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP).", "citeRegEx": "Bhatia et al\\.,? 2016", "shortCiteRegEx": "Bhatia et al\\.", "year": 2016}, {"title": "Enriching word vectors with subword information", "author": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1607.04606.", "citeRegEx": "Bojanowski et al\\.,? 2016", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2016}, {"title": "Compositional morphology for word representations and language modelling", "author": ["Jan A Botha", "Phil Blunsom."], "venue": "(?).", "citeRegEx": "Botha and Blunsom.,? 2014", "shortCiteRegEx": "Botha and Blunsom.", "year": 2014}, {"title": "Chinese\u2013spanish neural machine translation enhanced with character and word bitmap fonts", "author": ["Marta R Costa-juss\u00e0", "David Ald\u00f3n", "Jos\u00e9 AR Fonollosa."], "venue": "Machine Translation, pages 1\u201313.", "citeRegEx": "Costa.juss\u00e0 et al\\.,? 2017", "shortCiteRegEx": "Costa.juss\u00e0 et al\\.", "year": 2017}, {"title": "Universal stanford dependencies: A cross-linguistic typology", "author": ["Marie-Catherine De Marneffe", "Timothy Dozat", "Natalia Silveira", "Katri Haverinen", "Filip Ginter", "Joakim Nivre", "Christopher D Manning."], "venue": "Proceedings of the Language Resources and", "citeRegEx": "Marneffe et al\\.,? 2014", "shortCiteRegEx": "Marneffe et al\\.", "year": 2014}, {"title": "Morphosyntactic tagging of slovene: Evaluating taggers and tagsets", "author": ["Saso Dzeroski", "Tomaz Erjavec", "Jakub Zavrel."], "venue": "Proceedings of the Language Resources and Evaluation Conference (LREC).", "citeRegEx": "Dzeroski et al\\.,? 2000", "shortCiteRegEx": "Dzeroski et al\\.", "year": 2000}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay K Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Morpho-syntactic lexicon generation using graph-based semi-supervised learning", "author": ["Manaal Faruqui", "Ryan McDonald", "Radu Soricut."], "venue": "Transactions of the Association for Computational Linguistics, 4:1\u201316.", "citeRegEx": "Faruqui et al\\.,? 2016", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Feature-rich part-of-speech tagging for morphologically complex languages: Application to Bulgarian", "author": ["Georgi Georgiev", "Valentin Zhikov", "Petya Osenova", "Kiril Simov", "Preslav Nakov."], "venue": "Proceedings of the European Chapter of the Association for", "citeRegEx": "Georgiev et al\\.,? 2012", "shortCiteRegEx": "Georgiev et al\\.", "year": 2012}, {"title": "Tagging inflective languages: Prediction of morphological categories for a rich, structured tagset", "author": ["Jan Haji\u010d", "Barbora Hladk\u00e1."], "venue": "Proceedings of the Association for Computational Linguistics (ACL), pages 483\u2013490.", "citeRegEx": "Haji\u010d and Hladk\u00e1.,? 1998", "shortCiteRegEx": "Haji\u010d and Hladk\u00e1.", "year": 1998}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."], "venue": "Proceedings of the National Conference on Artificial Intelligence (AAAI).", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Ud treebank sampling for comparative parser evaluation", "author": ["Miryam de Lhoneux", "Joakim Nivre."], "venue": "The Sixth Swedish Language Technology Conference (SLTC).", "citeRegEx": "Lhoneux and Nivre.,? 2016", "shortCiteRegEx": "Lhoneux and Nivre.", "year": 2016}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the Conference on Natural Language Learning (CoNLL).", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Efficient higher-order CRFs for morphological tagging", "author": ["Thomas M\u00fcller", "Helmut Schmid", "Hinrich Sch\u00fctze."], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP), pages 322\u2013332.", "citeRegEx": "M\u00fcller et al\\.,? 2013", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2013}, {"title": "DyNet: The dynamic neural network toolkit", "author": ["Graham Neubig", "Chris Dyer", "Yoav Goldberg", "Austin Matthews", "Waleed Ammar", "Antonios Anastasopoulos", "Miguel Ballesteros", "David Chiang", "Daniel Clothiaux", "Trevor Cohn"], "venue": null, "citeRegEx": "Neubig et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Neubig et al\\.", "year": 2017}, {"title": "Tagging and morphological disambiguation of turkish text", "author": ["Kemal Oflazer", "\u00cclker Kuru\u00f6z."], "venue": "Proceedings of the fourth conference on Applied natural language processing, pages 144\u2013149. Association for Computational Linguistics.", "citeRegEx": "Oflazer and Kuru\u00f6z.,? 1994", "shortCiteRegEx": "Oflazer and Kuru\u00f6z.", "year": 1994}, {"title": "A universal part-of-speech tagset", "author": ["Slav Petrov", "Dipanjan Das", "Ryan McDonald."], "venue": "Proceedings of the Language Resources and Evaluation Conference (LREC).", "citeRegEx": "Petrov et al\\.,? 2012", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "author": ["Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "(?).", "citeRegEx": "Plank et al\\.,? 2016", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Cicero D. Santos", "Bianca Zadrozny."], "venue": "(?), pages 1818\u20131826.", "citeRegEx": "Santos and Zadrozny.,? 2014", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "Empirically sampling universal dependencies", "author": ["Natalie Schluter", "\u017deljko Agi\u0107."], "venue": "The NoDaLiDa Workshop on Universal Dependencies (UDW 2017).", "citeRegEx": "Schluter and Agi\u0107.,? 2017", "shortCiteRegEx": "Schluter and Agi\u0107.", "year": 2017}, {"title": "Building a tree-bank of modern hebrew text", "author": ["Khalil Sima\u2019an", "Alon Itai", "Yoad Winter", "Alon Altman", "Noa Nativ"], "venue": "Traitement Automatique des Langues,", "citeRegEx": "Sima.an et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sima.an et al\\.", "year": 2001}, {"title": "Charagram: Embedding words and sentences via character n-grams", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "arXiv preprint arXiv:1607.02789.", "citeRegEx": "Wieting et al\\.,? 2016", "shortCiteRegEx": "Wieting et al\\.", "year": 2016}, {"title": "Learning word meta-embeddings", "author": ["Wenpeng Yin", "Hinrich Sch\u00fctze."], "venue": "(?).", "citeRegEx": "Yin and Sch\u00fctze.,? 2016", "shortCiteRegEx": "Yin and Sch\u00fctze.", "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "Quantitative evaluation on the Stanford RareWord dataset (Luong et al., 2013) provides more evidence that these character-based embeddings capture word similarity for rare and unseen words.", "startOffset": 57, "endOffset": 77}, {"referenceID": 4, "context": "Botha and Blunsom (2014)", "startOffset": 0, "endOffset": 25}, {"referenceID": 15, "context": "compute word embeddings by summing over embeddings of the morphemes; Luong et al. (2013) construct a recursive neural network over each word\u2019s morphological parse; Bhatia et al.", "startOffset": 69, "endOffset": 89}, {"referenceID": 2, "context": "(2013) construct a recursive neural network over each word\u2019s morphological parse; Bhatia et al. (2016) use morpheme embeddings as a prior distribu-", "startOffset": 82, "endOffset": 103}, {"referenceID": 8, "context": "This is similar to the \u201cretrofitting\u201d approach of Faruqui et al. (2015), but rather than smoothing embeddings over a graph, we learn a function to build embeddings compositionally.", "startOffset": 50, "endOffset": 72}, {"referenceID": 22, "context": "For example, Santos and Zadrozny (2014) build word embeddings by convolution over characters, and then perform part-of-speech (POS) tagging using a local classifier; the tagging objective drives the entire learning process.", "startOffset": 13, "endOffset": 40}, {"referenceID": 12, "context": "(2015) propose a multi-level long shortterm memory (LSTM; Hochreiter and Schmidhuber, 1997), in which word embeddings are built compositionally from an LSTM over characters, and then tagging is performed by an LSTM over", "startOffset": 51, "endOffset": 91}, {"referenceID": 21, "context": "Plank et al. (2016) show that concatenating a character-level or bit-level LSTM network to a word representation helps immensely in POS tagging.", "startOffset": 0, "endOffset": 20}, {"referenceID": 19, "context": "The task of morpho-syntactic tagging dates back at least to the mid 1990s (Oflazer and Kuru\u00f6z, 1994; Haji\u010d and Hladk\u00e1, 1998), and interest has been rejuvenated by the availability of large-scale multilingual mor-", "startOffset": 74, "endOffset": 124}, {"referenceID": 11, "context": "The task of morpho-syntactic tagging dates back at least to the mid 1990s (Oflazer and Kuru\u00f6z, 1994; Haji\u010d and Hladk\u00e1, 1998), and interest has been rejuvenated by the availability of large-scale multilingual mor-", "startOffset": 74, "endOffset": 124}, {"referenceID": 6, "context": "phosyntactic annotations through the Universal Dependencies (UD) corpus (De Marneffe et al., 2014). For example, Faruqui et al. (2016) propose a graph-based technique for propagating typelevel morphological information across a lexicon, improving token-level morphosyntactic tagging in 11 languages, using an SVM tagger.", "startOffset": 76, "endOffset": 135}, {"referenceID": 6, "context": "phosyntactic annotations through the Universal Dependencies (UD) corpus (De Marneffe et al., 2014). For example, Faruqui et al. (2016) propose a graph-based technique for propagating typelevel morphological information across a lexicon, improving token-level morphosyntactic tagging in 11 languages, using an SVM tagger. In contrast, we apply a neural sequence labeling approach, inspired by the POS tagger of Plank et al. (2016).", "startOffset": 76, "endOffset": 430}, {"referenceID": 26, "context": "The training objective is similar to that of Yin and Sch\u00fctze (2016). We match the predicted em-", "startOffset": 45, "endOffset": 68}, {"referenceID": 1, "context": "The pretrained embeddings we use in our experiments are obtained from Polyglot (Al-Rfou et al., 2013), a multilingual word embedding effort.", "startOffset": 79, "endOffset": 101}, {"referenceID": 18, "context": "beddings randomly, and use DyNet to implement the model (Neubig et al., 2017).", "startOffset": 56, "endOffset": 77}, {"referenceID": 16, "context": "The Stanford RareWord evaluation corpus (Luong et al., 2013) focuses on predicting word similarity between pairs involving low-frequency English words, predominantly ones with common morphological affixes.", "startOffset": 40, "endOffset": 60}, {"referenceID": 2, "context": "For evaluation of our MIMICK model on the RareWord corpus, we trained the Variational Embeddings algorithm (VarEmbed; Bhatia et al., 2016) on a 20-million-token, 100,000type Wikipedia corpus, obtaining 128-dimension word embeddings for all words in the test corpus.", "startOffset": 107, "endOffset": 138}, {"referenceID": 3, "context": "compare to FastText (Bojanowski et al., 2016), a high-vocabulary, high-dimensionality embedding benchmark.", "startOffset": 20, "endOffset": 45}, {"referenceID": 20, "context": ", 2014) features a minimal set of 17 POS tags (Petrov et al., 2012) and supports tagging further language-specific features using attribute-specific inventories.", "startOffset": 46, "endOffset": 67}, {"referenceID": 15, "context": "Our approach for tagging morphosyntactic attributes is similar to the part-of-speech tagging model of Ling et al. (2015), who attach a projection layer to the output of a sentence-level bidirectional LSTM.", "startOffset": 102, "endOffset": 121}, {"referenceID": 24, "context": "Table 2: Nearest-neighbor examples for Hebrew (Transcriptions per Sima\u2019an et al. (2001)).", "startOffset": 66, "endOffset": 88}, {"referenceID": 14, "context": "We also considered language selection recommendations from de Lhoneux and Nivre (2016) and Schluter and Agi\u0107 (2017).", "startOffset": 62, "endOffset": 87}, {"referenceID": 14, "context": "We also considered language selection recommendations from de Lhoneux and Nivre (2016) and Schluter and Agi\u0107 (2017). As stated above, our approach is built on the Polyglot word embeddings.", "startOffset": 62, "endOffset": 116}, {"referenceID": 7, "context": "Dzeroski et al. (2000) report pertag accuracies on a morphosyntactically tagged corpus of Slovene.", "startOffset": 0, "endOffset": 23}, {"referenceID": 7, "context": "Dzeroski et al. (2000) report pertag accuracies on a morphosyntactically tagged corpus of Slovene. Faruqui et al. (2016) report macro-averages of F1 scores of 11 languages from", "startOffset": 0, "endOffset": 121}, {"referenceID": 0, "context": "5 Agi\u0107 et al. (2013) report separately on parts-of-speech and morphosyntactic attribute accuracies in Serbian and Croatian, as well as precision, recall, and F1 scores per tag.", "startOffset": 2, "endOffset": 21}, {"referenceID": 0, "context": "5 Agi\u0107 et al. (2013) report separately on parts-of-speech and morphosyntactic attribute accuracies in Serbian and Croatian, as well as precision, recall, and F1 scores per tag. Georgiev et al. (2012) report token-level accuracy for exact all-attribute tags (e.", "startOffset": 2, "endOffset": 200}, {"referenceID": 0, "context": "5 Agi\u0107 et al. (2013) report separately on parts-of-speech and morphosyntactic attribute accuracies in Serbian and Croatian, as well as precision, recall, and F1 scores per tag. Georgiev et al. (2012) report token-level accuracy for exact all-attribute tags (e.g. \u2018Ncmsh\u2019 for \u201cNoun short masculine singular definite\u201d) in Bulgarian, reaching a tagset of size 680. M\u00fcller et al. (2013) do the same for six other languages.", "startOffset": 2, "endOffset": 383}, {"referenceID": 21, "context": "ercase backoff), and appended with the output of a character-level LSTM updated during training (Plank et al., 2016).", "startOffset": 96, "endOffset": 116}, {"referenceID": 21, "context": "While test set OOVs are a strength of the CHAR\u2192TAG model (Plank et al., 2016), in many languages there are still considerable improvements to be obtained from the application of MIMICK initialization.", "startOffset": 57, "endOffset": 77}, {"referenceID": 21, "context": "* For reference, we copy the reported results of Plank et al. (2016)\u2019s analog to CHAR\u2192TAG.", "startOffset": 49, "endOffset": 69}, {"referenceID": 5, "context": "In this paper, the MIMICK model was trained using characters as input, but future work may consider the use of other subword units, such as morphemes, phonemes, or even bitmap representations of ideographic characters (Costa-juss\u00e0 et al., 2017).", "startOffset": 218, "endOffset": 244}], "year": 2017, "abstractText": "Word embeddings improve generalization over lexical features by placing each word in a lower-dimensional space, using distributional information obtained from unlabeled data. However, the effectiveness of word embeddings for downstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which embeddings do not exist. In this paper, we present MIMICK, an approach to generating OOV word embeddings compositionally, by learning a function from spellings to distributional embeddings. Unlike prior work, MIMICK does not require re-training on the original word embedding corpus; instead, learning is performed at the type level. Intrinsic and extrinsic evaluations demonstrate the power of this simple approach. On 23 languages, MIMICK improves performance over a word-based baseline for tagging part-of-speech and morphosyntactic attributes. It is competitive with (and complementary to) a supervised characterbased model in low-resource settings.", "creator": "LaTeX with hyperref package"}}}