{"id": "1707.07045", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jul-2017", "title": "End-to-end Neural Coreference Resolution", "abstract": "We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The model computes span embeddings that combine context-dependent boundary representations with a head-finding attention mechanism. It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources.", "histories": [["v1", "Fri, 21 Jul 2017 21:05:04 GMT  (38kb)", "http://arxiv.org/abs/1707.07045v1", "Accepted to EMNLP 2017"]], "COMMENTS": "Accepted to EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kenton lee", "luheng he", "mike lewis", "luke zettlemoyer"], "accepted": true, "id": "1707.07045"}, "pdf": {"name": "1707.07045.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Kenton Lee", "Luheng He", "Mike Lewis", "Luke Zettlemoyer"], "emails": ["lsz}@cs.washington.edu", "mikelewis@fb.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 7.07 045v 1 [cs.C L] 21 July 2 017ence Resolution Model and show that it significantly exceeds all previous work without using a syntactical parser or handmade mention detector. The basic idea is to view all the spans of a document directly as potential mentions and to learn distributions of possible precursors for each one. It calculates span embeddings that combine context-dependent boundary representations with a verifying attention mechanism. It is trained to maximize the marginal probability of gold precursor spans from core clusters and is included to enable aggressive pruning of potential mentions. Experiments show state-of-of-the-art performance with a gain of 1.5 F1 on the OntoNotes benchmark and around 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained without external resources."}, {"heading": "1 Introduction", "text": "We present the first state-of-the-art coreference resolution model to be learned end-to-end just to mark it as a cluster. All newer coreference models, including neural approaches that deliver impressive performance gains (Wiseman et al., 2016; Clark and Manning, 2016b, a), rely on syntactic parsers that both provide headwords and input for careful mention algorithms. We show for the first time that these resources are not needed, and in fact, performance can be improved without them by inventing an end-to-end neural model that turns out to be an entity and the entity."}, {"heading": "2 Related Work", "text": "Our machine learning methods have a long history in terms of correlation resolution (see Ng (2010) for a detailed survey), but the learning problem is challenging, and until recently, handmade systems based on automatically produced parse trees (Raghunathan et al., 2010) have outperformed all learning approaches. Durrett and Klein (2013) showed that highly lexical learning approaches reverse this trend, and newer neural models (Wiseman et al., 2016; Clark and Manning, 2016b, a) have achieved significant performance gains. However, all of these models still use parsers for header features and contain sophisticated mentioning algorithms. [1] Such pipeline systems have two major drawbacks: (1) Parse errors can cause cascade errors, and (2) many of the handmade rules do not generalize new languages or domains. We present the first unmade mentions algorithms while delivering more performance-oriented ones."}, {"heading": "3 Task", "text": "We formulate the task of end-to-end core resolution as a series of decisions for each possible span in the document. The input is a document D containing T-words along with metadata such as speaker and genre information.Letter N = T (T + 1) 2 is the number of possible spans in D. Identify the start and end indexes of a span i in D or by START (i) and END (i), 1For example, Raghunathan et al. (2010) use rules to remove pleonastic mentions indicated by 12 lexicalized regular expressions about English parsetrees.for 1 \u2264 i \u2264 n. We assume a sequence of spans based on START (i); spans with the same start index are ordered by END (i).The task is to assign each span i a precursor i and a precursor yi."}, {"heading": "4 Model", "text": "We aim to learn a conditional probability distribution P (y1,., yN | D), the most likely configuration of which generates the correct accumulation of information. We use a product of multinomials for each span: P (y1,.), where s (i, j) is a pair scoring for a correct linkage between chip i and span j in the documentation. We omit document D from the notation when the context is clear. There are three factors for this pair coreference score: (1) whether there is a mention, (2) whether there is a mention, and (3) whether it is an antecedent of i: s (i, j)."}, {"heading": "5 Inference", "text": "The size of the complete model described above is O (T 4) in document length T. To maintain computational efficiency, we greedily trim the candidate's spans both during training and during evaluation; we only consider spans with up to L-words and calculate their simple mention values sm (i) (as defined in Section 4); to further reduce the number of spans to be considered, we only consider the spans with the highest mention values, taking into account only up to K precursors; we also force non-crossing bracket structures with a simple suppression scheme. 2 We accept spans in decreasing order of mention values, unless, when considering span i, there is a previously accepted span j, so that START (i) < START (j) \u2264 2The official CoNLL-2012 evaluation considers predictions only without reference to START being valid for this document (START)."}, {"heading": "6 Learning", "text": "Only cluster information is observed in the training data. As the precursor data is latent, we optimize the marginal log probability of all correct precursor data implied by the gold cluster: log N-i = 1-y-Y-Y (i), GOLD (i), P (y), where GOLD (i) is the span of the gold cluster with the span i. If span i is not part of a gold cluster or all gold precursors have been truncated, GOLD (i) = (i). By optimizing this target, the model naturally learns to precisely truncate the span. While the initial cut is entirely random, only gold mentions receive positive updates. The model can quickly use this learning signal to appropriately allocate credit to the various factors, such as mentioning the gold values used for pruning."}, {"heading": "7 Experiments", "text": "We use the English coreference resolution data from the CoNLL-2012 Shared Task (Pradhan et al., 2012) in our experiments. This data set contains 2802 training documents, 343 development documents and 348 test documents. The training documents contain an average of 454 words and a maximum of 4009 words."}, {"heading": "7.1 Hyperparameters", "text": "Word embeddings are a fixed concatenation of 300-dimensional GloVe embeddings (Pennington et al., 2014) and 50-dimensional embeddings from Turian et al. (2010), which are both normalized as unit vectors. Word monsters are represented by a vector of zeros. In CNN, characters are represented as learned 8-dimensional embeddings. Environments have window sizes of 3, 4 and 5 characters, each consisting of 50 filters.Hidden Dimensions The hidden states in the LSTMs have 200 dimensions. Each forward-facing neural network consists of two hidden layers of 150 dimensions and reflected linear units (Nair and Hinton, 2010).Function coding We encode speaker information as a binary feature indicating whether a pair of spans from the same loudspeaker are embedded in the following features (Clark and E6b)."}, {"heading": "7.2 Ensembling", "text": "We also report on ensemble experiments with five models trained with different random initializations. Ensembling is performed for both the circumcision span and for previous decisions.3 https: / / github.com / kentonl / e2e-corefAt test date, we first calculate the average mention values sm (i) for each model before trimming the span. For the same trimmed span, each model then calculates the preceding values sa (i, j) separately and they are averaged to obtain the final values."}, {"heading": "8 Results", "text": "We specify the accuracy, recall and formula 1 for the standard metrics MUC, B3 and CEAF\u03c64 using the official CoNLL 2012 evaluation scripts. The main evaluation is the average formula 1 of the three metrics."}, {"heading": "8.1 Coreference Results", "text": "Table 1 compares our model to several previous systems that have made significant improvements to the OntoNotes benchmark in recent years. By all measures, we perform better than previous systems. Specifically, our single model improves the state-of-the-art F1 average by 1.5, and our 5-model ensemble improves it by 3.1. The most significant benefits result from improvements in recall, probably due to our final adjustment. During training, pipeline systems typically discard all mentions that the mention detector misses, which for Clark and Manning (2016a) accounts for more than 9% of the featured mentions in training data. By contrast, we discard only mentions that exceed our maximum mention width of 10, which accounts for less than 2% of mentions in training."}, {"heading": "8.2 Ablations", "text": "To show the importance of each component in our proposed model, we cut off different parts of the architecture and report on the average formula 1 on the development list of the data (see Figure 2).Characteristics The spacing between the spans and the width of the spans are critical signals for resolving the Korean references, which are consistent with previous results of other Korean reference models, contributing 3.8 F1 to the final result.Word representations Because our word embeddings are insensitive, access to a variety of word embeddings allows for a more meaningful model without overfitting. We assume that the different learning goals of GloVe and Turian embeddings provide orthogonal information (the former are sensitive to word and the latter to word order).Both embeddings contribute to some improvement in development, without overfitting. The CNN character provides morphological information and a way to withdraw for words outside the vocabulary. Since Korean referencing decisions often include rare F1 units, we see a contribution of 0.9 out of the article."}, {"heading": "8.3 Comparing Span Pruning Strategies", "text": "To distinguish between the contributions of improved evaluation of mentions and improved correlation decisions, we compare the results of our model with alternative pruning strategies. In these experiments, we use the alternative ranges for both training and evaluation. As shown in Table 3, mention candidates detected by the rule-based system over predicted parse reference trees (Raghunathan et al., 2010) deteriorate performance by 1 F1. We also provide oracle experiments in which we maintain the exact mentions that are present in gold correlation clusters. In oracle mentions, we see an improvement of 17.5 F1, which indicates enormous scope for improvement if our model can produce better mention values and anaphorism decisions."}, {"heading": "9 Analysis", "text": "In order to highlight the strengths and weaknesses of our model, we offer both quantitative and qualitative analyses. In the following discussion, we will use predictions from the single model and not from the so-called model."}, {"heading": "9.1 Mention Recall", "text": "Since singleton clusters are not explicitly labeled, the training data provide only a weak signal for spans corresponding to entity mentions. As a by-product of optimizing the marginal probability, our model automatically learns a useful ranking of spans over the simple mention values in Section 4.Peak voltages cover a large portion of mentions in gold clusters, as shown in Figure 3. With a similar number of obtained spans, our memory is comparable to the rule-based mention detector (Raghunathan et al., 2010), which generates 0.26 spans per word with a recall of 89.2%. As we increase the number of spans per word (\u03bb in Section 5), we observe a higher memory, but with decreasing yield. In our experiments, compliance with 0.4 spans per word results in 92.7% recall in the development data."}, {"heading": "9.2 Mention Precision", "text": "While the training data does not provide a direct measure of mention accuracy, we can use the syntactical gold structures contained in the data as a proxy. High-mention spans should correspond to syntactic components. In Figure 4, we show the precision of upward spans if 0.4 spans per word are maintained. At 2-5 word spans, 75-90% of the predictions are components, indicating that the vast majority of mentions are syntactically plausible. Longer spans, all relatively rare, prove more difficult for the model, and accuracy drops to 46% for 10-word spans."}, {"heading": "9.3 Head Agreement", "text": "We also study how well the learned head preferences correlate with syntactic heads. For each of the best-rated areas in the development data that correspond to the gold components, we calculate the word with the highest attention weight. In Figure 4, we record the proportion of these words that correspond to syntactic heads. Matching is between 68-93%, which is surprisingly high as there is no explicit monitoring of syntactic heads. Simply, the model learns from the cluster data that these headers are useful for making co-ference decisions."}, {"heading": "9.4 Qualitative Analysis", "text": "Our qualitative analysis in Table 4 highlights the strengths and weaknesses of our model. Each line is a visualization of a single core reference cluster predicted by the model. Bolded extends in parentheses to the predicted cluster, and the redness of a word shows its weight from the head finding attention mechanism (ai, t in Section 4).Strengths The effectiveness of the attention mechanism for making core reference decisions can be seen in Example 1. The model is dedicated to the fire in the span A fire in a garment factory in Bangladesh, so that it can successfully predict the core connection with the blaze. For a subspan of this mention, a Bangladesh garment factory pays the model the most attention to the factory, so that it successfully links the core link to the garment factory in Bangladesh. The task-specific nature is linked by the core reference."}, {"heading": "10 Conclusion", "text": "Our final set of models improves performance on the OntoNotes benchmark by over 3 F1 with no external pre-processing tools used by previous systems. We showed that our model implicitly learns to generate useful mention candidates from all possible voltage ranges. A novel headword attention mechanism also learns a task-specific preference for headwords, which empirically strongly correlates with traditional head-word definitions. While our model significantly advances state-of-the-art performance, the improvements potentially complement a variety of work on various strategies to improve correlation resolution, including entity-level conclusions and the inclusion of world knowledge, which are important opportunities for future work."}, {"heading": "Acknowledgements", "text": "The research was supported in part by DARPA through the DEFT program (FA8750-13-2-0019), the ARO (W911NF-16-1-0121), the NSF (IIS1252835, IIS-1562364), gifts from Google and Tencent, and an Allen Distinguished Investigator Award. We also thank the UW NLP group for helpful conversations and comments on the work."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyunghyunCho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Understanding the value of features for coreference resolution", "author": ["Eric Bengtson", "Dan Roth."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 294\u2013303. Association for Computational Linguistics.", "citeRegEx": "Bengtson and Roth.,? 2008", "shortCiteRegEx": "Bengtson and Roth.", "year": 2008}, {"title": "Learning structured perceptrons for coreference resolution with latent antecedents and non-local features", "author": ["Anders Bj\u00f6rkelund", "Jonas Kuhn."], "venue": "ACL.", "citeRegEx": "Bj\u00f6rkelund and Kuhn.,? 2014", "shortCiteRegEx": "Bj\u00f6rkelund and Kuhn.", "year": 2014}, {"title": "Entity-centric coreference resolution with model stacking", "author": ["Kevin Clark", "Christopher D. Manning."], "venue": "ACL.", "citeRegEx": "Clark and Manning.,? 2015", "shortCiteRegEx": "Clark and Manning.", "year": 2015}, {"title": "Deep reinforcement learning for mention-ranking coreference models", "author": ["Kevin Clark", "Christopher D. Manning."], "venue": "Empirical Methods on Natural Language Processing (EMNLP).", "citeRegEx": "Clark and Manning.,? 2016a", "shortCiteRegEx": "Clark and Manning.", "year": 2016}, {"title": "Improving coreference resolution by learning entitylevel distributed representations", "author": ["Kevin Clark", "Christopher D. Manning."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Clark and Manning.,? 2016b", "shortCiteRegEx": "Clark and Manning.", "year": 2016}, {"title": "Easy victories and uphill battles in coreference resolution", "author": ["Greg Durrett", "Dan Klein."], "venue": "EMNLP.", "citeRegEx": "Durrett and Klein.,? 2013", "shortCiteRegEx": "Durrett and Klein.", "year": 2013}, {"title": "A joint model for entity analysis: Coreference, typing, and linking", "author": ["Greg Durrett", "Dan Klein."], "venue": "TACL, 2:477\u2013490.", "citeRegEx": "Durrett and Klein.,? 2014", "shortCiteRegEx": "Durrett and Klein.", "year": 2014}, {"title": "Latent structure perceptron with feature induction for unrestricted coreference resolution", "author": ["Eraldo Rezende Fernandes", "C\u0131\u0301cero Nogueira Dos Santos", "Ruy Luiz Milidi\u00fa"], "venue": "In Joint Conference on EMNLP and CoNLL-Shared Task,", "citeRegEx": "Fernandes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fernandes et al\\.", "year": 2012}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["AndrewM Saxe", "James L McClelland", "Surya Ganguli."], "venue": "arXiv preprint arXiv:1312.6120.", "citeRegEx": "Saxe et al\\.,? 2013", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Word representations: A Simple and General Method for Semi-supervised Learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Learning global features for coreference resolution", "author": ["Sam Wiseman", "Alexander M Rush", "Stuart M Shieber."], "venue": "arXiv preprint arXiv:1604.03035.", "citeRegEx": "Wiseman et al\\.,? 2016", "shortCiteRegEx": "Wiseman et al\\.", "year": 2016}, {"title": "Learning anaphoricity and antecedent ranking features for coreference resolution", "author": ["Sam Wiseman", "Alexander M. Rush", "Stuart M. Shieber", "Jason Weston."], "venue": "ACL.", "citeRegEx": "Wiseman et al\\.,? 2015", "shortCiteRegEx": "Wiseman et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "The attention component is inspired by parser-derived head-word matching features from previous systems (Durrett and Klein, 2013), but is less susceptible to cascading errors.", "startOffset": 104, "endOffset": 129}, {"referenceID": 1, "context": "They can typically be categorized as (1) mention-pair classifiers (Ng and Cardie, 2002; Bengtson and Roth, 2008), (2) entity-level models (Haghighi and Klein, 2010; Clark and Manning, 2015, 2016b; Wiseman et al.", "startOffset": 66, "endOffset": 112}, {"referenceID": 11, "context": "They can typically be categorized as (1) mention-pair classifiers (Ng and Cardie, 2002; Bengtson and Roth, 2008), (2) entity-level models (Haghighi and Klein, 2010; Clark and Manning, 2015, 2016b; Wiseman et al., 2016), (3) latent-tree models (Fernandes et al.", "startOffset": 138, "endOffset": 218}, {"referenceID": 8, "context": ", 2016), (3) latent-tree models (Fernandes et al., 2012; Bj\u00f6rkelund and Kuhn, 2014; Martschat and Strube, 2015), or (4) mention-ranking models (Durrett and Klein, 2013; Wiseman et al.", "startOffset": 32, "endOffset": 111}, {"referenceID": 2, "context": ", 2016), (3) latent-tree models (Fernandes et al., 2012; Bj\u00f6rkelund and Kuhn, 2014; Martschat and Strube, 2015), or (4) mention-ranking models (Durrett and Klein, 2013; Wiseman et al.", "startOffset": 32, "endOffset": 111}, {"referenceID": 6, "context": ", 2012; Bj\u00f6rkelund and Kuhn, 2014; Martschat and Strube, 2015), or (4) mention-ranking models (Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016a).", "startOffset": 94, "endOffset": 167}, {"referenceID": 12, "context": ", 2012; Bj\u00f6rkelund and Kuhn, 2014; Martschat and Strube, 2015), or (4) mention-ranking models (Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016a).", "startOffset": 94, "endOffset": 167}, {"referenceID": 4, "context": ", 2012; Bj\u00f6rkelund and Kuhn, 2014; Martschat and Strube, 2015), or (4) mention-ranking models (Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016a).", "startOffset": 94, "endOffset": 167}, {"referenceID": 1, "context": "Durrett and Klein (2013) showed that highly lexical learning approaches reverse this trend, and more recent neural models (Wiseman et al.", "startOffset": 0, "endOffset": 25}, {"referenceID": 0, "context": "Instead of relying on syntactic parses, our model learns a taskspecific notion of headedness using an attention mechanism (Bahdanau et al., 2014) over words in each span:", "startOffset": 122, "endOffset": 145}, {"referenceID": 12, "context": "We experimented with these cost-sensitive alternatives, including margin-based variants (Wiseman et al., 2015; Clark and Manning, 2016a), but a simple maximum-likelihood objective proved to be most effective.", "startOffset": 88, "endOffset": 136}, {"referenceID": 4, "context": "We experimented with these cost-sensitive alternatives, including margin-based variants (Wiseman et al., 2015; Clark and Manning, 2016a), but a simple maximum-likelihood objective proved to be most effective.", "startOffset": 88, "endOffset": 136}, {"referenceID": 3, "context": "This learning objective can be considered a span-level, cost-insensitive analog of the learning objective proposed by Durrett and Klein (2013). We experimented with these cost-sensitive alternatives, including margin-based variants (Wiseman et al.", "startOffset": 118, "endOffset": 143}, {"referenceID": 2, "context": "7 Bj\u00f6rkelund and Kuhn (2014) 74.", "startOffset": 2, "endOffset": 29}, {"referenceID": 2, "context": "7 Bj\u00f6rkelund and Kuhn (2014) 74.3 67.5 70.7 62.7 55.0 58.6 59.4 52.3 55.6 61.6 Durrett and Klein (2013) 72.", "startOffset": 2, "endOffset": 104}, {"referenceID": 10, "context": ", 2014) and 50-dimensional embeddings from Turian et al. (2010), both normalized to be unit vectors.", "startOffset": 43, "endOffset": 64}, {"referenceID": 3, "context": "Following Clark and Manning (2016b), the distance features are binned into the following buckets [1, 2, 3, 4, 57, 8-15, 16-31, 32-63, 64+].", "startOffset": 10, "endOffset": 36}, {"referenceID": 9, "context": "The LSTM weights are initialized with random orthonormal matrices as described in Saxe et al. (2013). We apply 0.", "startOffset": 82, "endOffset": 101}, {"referenceID": 9, "context": "The LSTM weights are initialized with random orthonormal matrices as described in Saxe et al. (2013). We apply 0.5 dropout to the word embeddings and character CNN outputs. We apply 0.2 dropout to all hidden layers and feature embeddings. Dropout masks are shared across timesteps to preserve long-distance information as described in Gal and Ghahramani (2016). The learning rate is decayed by 0.", "startOffset": 82, "endOffset": 361}, {"referenceID": 3, "context": "During training, pipelined systems typically discard any mentions that the mention detector misses, which for Clark and Manning (2016a) consists of more than 9% of the labeled mentions in the training data.", "startOffset": 110, "endOffset": 136}], "year": 2017, "abstractText": "We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or handengineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The model computes span embeddings that combine context-dependent boundary representations with a headfinding attention mechanism. It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources.", "creator": "LaTeX with hyperref package"}}}