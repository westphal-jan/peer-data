{"id": "1707.07086", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jul-2017", "title": "Identifying civilians killed by police with distantly supervised entity-event extraction", "abstract": "We propose a new, socially-impactful task for natural language processing: from a news corpus, extract names of persons who have been killed by police. We present a newly collected police fatality corpus, which we release publicly, and present a model to solve this problem that uses EM-based distant supervision with logistic regression and convolutional neural network classifiers. Our model outperforms two off-the-shelf event extractor systems, and it can suggest candidate victim names in some cases faster than one of the major manually-collected police fatality databases.", "histories": [["v1", "Sat, 22 Jul 2017 01:47:36 GMT  (459kb,D)", "http://arxiv.org/abs/1707.07086v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["katherine a keith", "abram handler", "michael pinkham", "cara magliozzi", "joshua mcduffie", "brendan o'connor"], "accepted": true, "id": "1707.07086"}, "pdf": {"name": "1707.07086.pdf", "metadata": {"source": "CRF", "title": "Identifying civilians killed by police with distantly supervised entity-event extraction", "authors": ["Katherine A. Keith", "Abram Handler", "Michael Pinkham", "Cara Magliozzi", "Joshua McDuffie", "Brendan O\u2019Connor"], "emails": ["kkeith@cs.umass.edu,", "brenocon@cs.umass.edu"], "sections": [{"heading": null, "text": "We propose a new socially effective task for processing natural language: extracting the names of people killed by the police from a news corpus; presenting a newly collected police death corpus, which we publish publicly; and presenting a model for solving this problem that uses EM-based remote monitoring with logistic regression and revolutionary classifiers of neural networks. Our model surpasses two standard event extractor systems and, in some cases, can suggest victim names faster than one of the large manually recorded police death databases. Annex, software and data are available online at: http: / / slanglab.cs.umass. edu / PoliceKillingsExtraction / [This paper appears in the Proceedings of EMNLP 2017. This version contains the attachment.]"}, {"heading": "1 Introduction", "text": "In fact, most of them will be able to orient themselves in a different direction than in a different direction, namely the direction in which they are moving."}, {"heading": "2 Related Work", "text": "This task combines elements of information extraction, including: event extraction (alias: Hanndt 2010 semantic analysis), identification of descriptions of events, and2Konovalov et al. (2017) examines the database update task, in which edits of Wikipedia info boxes depict events. Their arguments from text and extraction of cross documents predicting semantic relationships across units. A death indicates the killing of a specific person; we would like to specifically identify the names of death victims mentioned in the text. Therefore, our task could be considered an uncommon relationship extraction: for a specific person mentioned in a corpus, were they killed by a police officer? Prior work in NLP has produced a number of event extraction systems based on text data labeled with a pre-specified ontology, including those identifying cases of killings (Li and Ji, 2014; Das et al., 2014)."}, {"heading": "3 Task and Data", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Cross-document entity-event extraction for police fatalties", "text": "From a corpus of documents D, the task is to extract a list of the names of the candidates, E, and to find for each e-E, dP (ye = 1 | xM (e)). (1) Here, y-i {0, 1} is the entity designation where ye = 1 means that a person (entity) e was killed by the police; xM (e) are the sentences containing the mentionM (e) of that person. A mention i-M (e) is a symbolic span in the corpus. Most entities have multiple mentions; a single sentence may contain multiple mentions of different entities."}, {"heading": "3.2 News documents", "text": "We download a collection of web news articles by continuously queriing Google News3 throughout 2016, with lists of police keywords (e.g. police, police officer, police officer, etc.) and death-related keywords (e.g. murder, shooting, murder, etc.). Keyword lists are compiled semi-automatically from cosmic similarity searches from the word2vec-trained word embeddings4 to select a series of high-recall keywords. Search is limited to what Google News defines as the \"regional edition\" of the \"United States (English),\" which seems roughly limited to US news, though we use anecdotally observed cases of news about events in the UK and other countries."}, {"heading": "3.3 Entity and mention extraction", "text": "We process all documents with the open source spaCy NLP package 5 to segment sentences and extract entity mentions. Mentions are token ranges that (1) have been identified as \"persons\" by spaCys and (2) have a (first name, last name) pair as analyzed by HAPNIS rule 3https: / / news.google.com / 4https: / / code.google.com / archive / p / word2vec / 5 version 0.101.0, https: / spacy.io / based name parser, 6, for example, which (John, Doe) is assigned to a string of Mr. John A. Doe Jr. 7To prepare sentence text for modeling, our preprocessor breaks down mentioning the candidate under a special TARGET symbol. To prevent other person names from being assigned to another PERSON symbol; \"e.g., this phrase was killed during an encounter with a police officer at least 617, or a police officer was killed during an encounter with an 18.857."}, {"heading": "4 Models", "text": "Our goal is to classify units according to whether they have been killed by the police (\u00a7 4.1). Since we do not have gold standard labels to train our model, we turn to remote supervision (Craven and Kumlien, 1999; Mintz et al., 2009), which aligns facts in a knowledge base heuristically with texts in a corpus to insert positive mention level labels for supervised learning. Previous work typically examines remote supervision related to the extraction of binary relationships (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011), but we are 6http: / / www.umiacs.umd.edu / HAPNIS / 7For both trainings and tests we use a name consistent with the assumption that there is a (firstname, surname) match between the mentions and the relativization of data."}, {"heading": "4.1 Approach: Latent disjunction model", "text": "Our discriminatory model is based on probability classifiers at the mention level. Let's remember that a single unit will have one or more mentions (i.e., the same name occurs in several sentences in our corpus). For a given mention i in sentence xi, our model predicts whether the person has been killed by the police, zi = 1, with a binary logistic model, P (zi = 1 | xi) = \u03c3 (\u03b2Tf\u03b3 (xi))). (2) We experiment with both logistic regressions (\u00a7 4.4) and with revolutionary neural networks (\u00a7 4.5) for this component, which use logistic regression weights \u03b2 and trait extractor parameters \u03b3. Then, we have to make somehow aggregate mention-level decisions to determine entity labels ye.9 If a human reader would observe at least one sentence stating that a person was killed by the police, they would conclude that the person was killed by the police."}, {"heading": "4.2 \u201cHard\u201d distant label training", "text": "Distance labelling is a term used heuristically for mentions in training data and is used directly for training purposes. We use two rules of labelling. First, only the name: zi = 1 if e G (train): name (i) = name (e). (7) This is the direct predicate analogous to Mintz et al. (2009) \"s Remote Surveillance Assumption, which considers any mention of a gold-positive entity to be a description of a police murder. This assumption is incorrect. We analyze a sample of positive mentions manually and find that 36 out of 100 sentences with only names did not express a police death event - for example, sentences contain comments or describe killings that were not carried out by the police. This is similar to the accuracy in remote monitoring of binary relationships found by Riedel et al. (2010), which 10-38% of sentences did not express the context."}, {"heading": "4.3 \u201cSoft\u201d (EM) joint training", "text": "In previous experiments, we experimented with other, more ad hoc aggregated rules using a \"hard\" trained model. Maximum and mean arithmetic functions are worse than Noisyor, which gives credibility to the disjunction model. The sum rule (i P (zi = 1 | xi) had a similar ranking to Noisyor - perhaps because it can also use weak signals, as opposed to mean or max - although it does not yield reasonable probabilities between 0 and 1 that the person was not killed by a police officer. Alternatively, we can treat zi as a latent variable during the training period and assume that at least one of the mentions asserts the fatality event, but conveys uncertainty about the mention (or multiple mentions) of this information."}, {"heading": "4.4 Feature-based logistic regression", "text": "We construct handmade features for regulated logistic regression (LR) (Table 4) that closely resemble the features of n-gram and syntactic dependence used in previous work on function-based semantic parsing (e.g. Das et al. (2014); Thomson et al. (2014). We use randomized feature hashing (Weinberger et al., 2009) to efficiently represent features in 450,000 dimensions that have performed similar performance as explicit feature representations. Logistic regression weights (\u03b2 in Equation 2) are learned with scikitlearn (Pedregosa et al., 2011).11 For EM- (Soft-LR) training, the range of the test set converges below the precision recall curve after 96 iterations (Figure 1).11With FeatureHasher, L2 regularization, \"lbffgs\" training, and inverse development dataset = 0.1."}, {"heading": "4.5 Convolutional neural network", "text": "We also train a Convolutionary Neural Network Analyzer (CNN) that uses word embeddings and their nonlinear compositions to potentially generalize better than sparse lexical and n-grammatical features. CNNs have proven useful for sentence classification tasks (Kim, 2014; Zhang and Wallace, 2015), relation classification (Zeng et al., 2014) and, similar to this setting, event detection (Nguyen and Grishman, 2015). We use Kim (2014)'s open source CNN implementation, 12, where a logistic function provides the final mention of the prediction based on max-pooled values values from three filter sizes whose parameters are learned (Q. 2). We use pre-formed word embeddings for initialization, 13 and update them during the training. We also add two special vectors for the randomly initialized and the PERGEART-SON symbols."}, {"heading": "4.6 Evaluation", "text": "Based on documents from the test period (Sept-Dec 2016), our entity-level models predict Labels12https: / / github.com / yoonkim / CNN Sentence 13 From the same word2vec embedding that runs in Section 3. 14Training with ADADADELTA (Zeiler, 2012). We tested several different settings of dropout and L2 regularization hyperparameters on a development list, but found mixed results, so we used their default values. P (ye = 1 | xM (e)) (Eq. 6), and we would like to evaluate whether retrieved entities are killed in Fatal Encounters than during September-Dec 2016. We evaluate entities using predicted probabilities to construct a precision retrieval curve (Fig. 4, Table 5). Area below the precision retrieval curve (AUPRC), the database is calculated using a trapezoidal rule."}, {"heading": "5 Off-the-shelf event extraction baselines", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "6 Results and discussion", "text": "In fact, most people who are able to survive are able to survive themselves, to survive themselves, \"he said in an interview with The New York Times.\" I don't think I'm able to survive myself, \"he said.\" I don't think I'm able to survive myself. \"He added,\" I don't think I'm able to survive myself. \"He added,\" I don't think I'm able to survive myself. \"He added,\" I don't think I'm able to survive myself. \""}, {"heading": "Acknowledgments", "text": "This work has been partially supported by the Amazon Web Services (AWS) Cloud Credits for Research Program. Thanks to D. Brian Burghart for advising in tracking police accidents and to David Belanger, Trapit Bansal, Patrick Verga, Rajarshi Das and Taylor Berg-Kirkpatrick for feedback."}, {"heading": "A Document retrieval from Google News", "text": "Our news dataset was created using documents collected via Google News. Specifically, we made searches to the regional edition of Google News19 United States in 2016. These lists were created semi-automatically by looking for the nearest neighbors of \"Police\" and \"kill\" (at cosinal distance) from Google's public release of word2vec vectors preschooled on a very large (proprietary) Google News corpus, 20 and then manually excluding a small number of misspelled words or redundant capital letters (e.g. \"Police\" and \"Police\"). Our list of police words includes: police, officers, officers, detectives, sheriff's officers, police officers, police officers, policemen, policemen, policemen, detectives, patrol officers, police officers, policemen, containeros, police officers, containeros, containers, containers, containers, troopers, puppeteers, etc. \"Our news listings were compiled narrowly with 22 police searches in 2016 (These lists were narrowly matched with 22 police states)."}, {"heading": "B Document preprocessing", "text": "Once documents are downloaded from URLs collected via Google messaging queries, we use text extraction with Lynx browser22 to extract text from HTML. (Newer open source packages such as Boilerpipe and Newspaper exist for text extraction, but we observed that they often failed in our web data.) 21We also collected data for part of 2015; the volume of search results varied over time due to internal changes at Google News. After the first few weeks of 2016, the volume was relatively constant. 22Version 2.8"}, {"heading": "C Mention-level preprocessing", "text": "From the corpus of scraped news documents to create the record at mention level (i.e. the sentences that contain candidates), we have: 1. Use the text-based Lynx web browser to extract the text of a webpage. 2. Segment sentences in two steps: (a) Segment documents to text fragments (typically paragraphs) by splitting up on the representation of HTML paragraphs, list markers and other dividers: duplicate lines and the characters -, *, | and #. (b) Apply the sentence segmenter from spaCy (and NLP pipeline) to achieve those paragraph-like text fragmentations.3. De-duplicate sentences as described below. 4. Remove sentences that contain less than 5 characters or more than 200.5."}, {"heading": "E Manual analysis of results", "text": "Manual analysis is available in Table 8."}, {"heading": "F Bootstrap", "text": "We perform three different methods of resampling capture by varying the objects sampled: 1. Entities 2. Documents3. Documents, with deduplication of mentions. 23. We pick up both test set entities and test set documents, because we are not currently aware of any literature implementing the argument for one over the23To, we take the 10,000 samples (with substitution) of documents and reduce them to the unique set of signed documents. This effectively eliminates duplicate mentions that occur in Method 2 when the same document is drawn more than once in a sample. Others, and both are probably relevant in our context. The sampling model assumes a given dataset representing a theoretically infinite population and asks what variability there would be if a finite sample were drawn again from the population."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We propose a new, socially-impactful task for natural language processing: from a news corpus, extract names of persons who have been killed by police. We present a newly collected police fatality corpus, which we release publicly, and present a model to solve this problem that uses EM-based distant supervision with logistic regression and convolutional neural network classifiers. Our model outperforms two off-the-shelf event extractor systems, and it can suggest candidate victim names in some cases faster than one of the major manually-collected police fatality databases. Appendix, software, and data are available online at: http://slanglab.cs.umass. edu/PoliceKillingsExtraction/ [This paper appears in Proceedings of EMNLP 2017. This version includes the appendix.]", "creator": "TeX"}}}