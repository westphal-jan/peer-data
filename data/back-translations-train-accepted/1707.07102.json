{"id": "1707.07102", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jul-2017", "title": "Obj2Text: Generating Visually Descriptive Language from Object Layouts", "abstract": "Generating captions for images is a task that has recently received considerable attention. In this work we focus on caption generation for abstract scenes, or object layouts where the only information provided is a set of objects and their locations. We propose OBJ2TEXT, a sequence-to-sequence model that encodes a set of objects and their locations as an input sequence using an LSTM network, and decodes this representation using an LSTM language model. We show that our model, despite encoding object layouts as a sequence, can represent spatial relationships between objects, and generate descriptions that are globally coherent and semantically relevant. We test our approach in a task of object-layout captioning by using only object annotations as inputs. We additionally show that our model, combined with a state-of-the-art object detector, improves an image captioning model from 0.863 to 0.950 (CIDEr score) in the test benchmark of the standard MS-COCO Captioning task.", "histories": [["v1", "Sat, 22 Jul 2017 04:17:42 GMT  (1340kb,D)", "http://arxiv.org/abs/1707.07102v1", "Accepted at EMNLP 2017"]], "COMMENTS": "Accepted at EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["xuwang yin", "vicente ordonez"], "accepted": true, "id": "1707.07102"}, "pdf": {"name": "1707.07102.pdf", "metadata": {"source": "CRF", "title": "OBJ2TEXT: Generating Visually Descriptive Language from Object Layouts", "authors": ["Xuwang Yin", "Vicente Ordonez"], "emails": ["vicente]@virginia.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 Task", "text": "The first task is an object layout in the form of a series of object categories and box pairs < o, l > = {< oi, li >}, and the output is in natural language. This task is similar to the second task of the caption, except that the input is an object layout instead of a standard grid image, which is presented as a pixel array. For both tasks, we experiment with the MS-COCO dataset. For the first task, object layouts are derived from bounding box annotations, and for the second task, object layouts are created with the outputs of an object detector above the input image."}, {"heading": "3 Related Work", "text": "Our work is related to previous work that used clipart scenes for visually based tasks, including sentence interpretation (Zitnick and Parikh, 2013; Zitnick et al., 2013), and predicting object dynamics (Fouhey and Zitnick, 2014). However, the cited advantage of abstract scene representations such as those provided by the clipart scenes requires objective knowledge of the world. Similar to our work, the work of Vedantam et al. (2015b); Eysenbach et al. (2016) suggests methods to learn common sense from clipart scenes, while the method of Yatskar et al. (2016), similar to our work, Leverages object annotations for natural images. Understanding abstract scenes has shown to be a useful ability for language."}, {"heading": "4 Model", "text": "In this section, we describe our basic OBJ2TEXT model for encoding object layouts to generate text (Section 4.1), as well as two other variants for using our model to generate captions for real images: OBJ2TEXT-YOLO, which uses the object detector YOLO (Redmon and Farhadi, 2017) to generate object locations from real images (Section 4.2), and OBJ2TEXT-YOLO + CNN-RNN, which further combines the previous model with an encoder and decoder captioning using a conventional neural network to encode the image (Section 4.3)."}, {"heading": "4.1 OBJ2TEXT", "text": "In fact, it is such that most people are able to recognize themselves and understand what they are doing. (...) It is the time in which people in the world move in the world, in which they move, in the world, in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "4.2 OBJ2TEXT-YOLO", "text": "For the caption task, we suggest OBJ2TEXT-YOLO. This model uses an image as input, extracts an object layout (object categories and locations) with a state-of-the-art object recognition model YOLO (Redmon and Farhadi, 2017), and uses OBJ2TEXT as described in Section 4.1 to create a natural language description of the input layout and thus the input image. The model is trained with the standard back propagation algorithm, but the error is not transmitted back to the object recognition module."}, {"heading": "4.3 OBJ2TEXT-YOLO + CNN-RNN", "text": "For the task of captioning, we experiment with a combined model (see Figure 2), in which we use an image as input and then use two separate branches of calculation to extract visual feature information and object layout information. These two streams of information are then passed on to a neural LSTM model to create a description. Visual features are extracted using the VGG-16 (Simonyan and Zisserman, 2015) Convolutionary Neural Network, which is pre-schooled in the ImageNet classification task (Russakovsky et al., 2015). Object layouts are extracted using the YOLO object recognition system and its output object positions are encoded using our proposed OBJ2TEXT encoder. These two information streams are encoded in vectors of the same size and their sum is propagated into the language model to generate a textual description, but the algorithm is well trained on the back-algorithm of both the standard, but the propagation is not."}, {"heading": "5 Experimental Setup", "text": "We evaluate the proposed models based on the MSCOCO (Lin et al., 2014) dataset, which is a popular caption benchmark that also includes annotations on the scope of the object. Characteristics of the generated descriptions are evaluated using both human ratings and automatic metrics. We train and validate our models based on the commonly accepted split regime (113,287 training images, 5000 validation and 5000 test images) used in (Karpathy et al., 2016), and also test our model in the MSCOCO official test benchmark. We implement our models based on the open source caption system Neuraltalk2 (Karpathy et al., 2016). Other configurations including data preprocessing and training also follow Neuraltalk2."}, {"heading": "6 Results", "text": "In fact, most people are able to survive themselves if they move to another world where they are not able to survive themselves. \"The results of the study show that most of them are able to survive themselves:\" I don't think they are able to survive themselves, \"he said.\" I don't think they are able to survive themselves. \"The results of the study show that most of them are able to survive themselves if they are not able to survive themselves.\" However, the results of the study also show that people are able to survive themselves, to survive themselves."}, {"heading": "7 Conclusion", "text": "We have introduced OBJ2TEXT, a sequence sequence model for creating visual descriptions for object layouts where only categories and locations are specified. Our proposed model shows that a proper visual input of concepts is not enough to create good descriptions, but object sizes, locations, and number of objects all contribute to generating more accurate image descriptions. Crucially, our encoding mechanism is able to capture useful spatial information using an LSTM network to create image descriptions even when the input is provided as a sequence rather than as an explicit 2D representation of objects. Furthermore, by using our proposed OBJ2TEXT model in combination with an existing subtitling model and a robust object detector, we have achieved improved results in the task of captioning."}, {"heading": "Acknowledgments", "text": "This is the question that arises whether we are in a phase, in which we are in a phase, in which we are in a phase, in which we are in a phase, in which we are in a phase, in which we are in a phase, in which we are in a phase, in which we are in a phase, in which we are in which we are in a phase, in which we are in which we are in a phase, in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we in which we are in which we are in which we are in which we in which we are in which we are in which we are in which we in which we are in which we in which we are in which we in which we are in which we in which we in which we are in which we in which we in which we in which we are in which we in which we are in which we in which we in which we are in which we in which we in which we in which we are in which we in which we in which we are in which we in which we in which we in which we are in which we in which we are in which we in which we in which we in which we are in which we in which we are we in which we in which we in which we are in which we in which we in which we in which we in which we are in which we in which we in which we in which we are in which we in which we are in which we in which we in which we in which we are in which we are in which we are in which we in which we are in which we in which we in which we are in which we are in which we in which we are in which we in which we are in which we are in which we in which we in which we are in which we are in which we in which we are in which we in which we are in which we in which we are in which we in which we in which we are in which we in which we in which we in which we are in which we in which we in which we in which we in"}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeffrey Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell."], "venue": "IEEE conference on com-", "citeRegEx": "Donahue et al\\.,? 2015", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "Image description using visual dependency representations", "author": ["Desmond Elliott", "Frank Keller."], "venue": "EMNLP, volume 13, pages 1292\u20131302.", "citeRegEx": "Elliott and Keller.,? 2013", "shortCiteRegEx": "Elliott and Keller.", "year": 2013}, {"title": "Who is mistaken? arXiv preprint arXiv:1612.01175", "author": ["Benjamin Eysenbach", "Carl Vondrick", "Antonio Torralba"], "venue": null, "citeRegEx": "Eysenbach et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Eysenbach et al\\.", "year": 2016}, {"title": "From captions to visual concepts and back", "author": ["Hao Fang", "Saurabh Gupta", "Forrest Iandola", "Rupesh K Srivastava", "Li Deng", "Piotr Doll\u00e1r", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John C Platt"], "venue": "In Proceedings of the IEEE conference on computer", "citeRegEx": "Fang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fang et al\\.", "year": 2015}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["Ali Farhadi", "Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian", "Julia Hockenmaier", "David Forsyth."], "venue": "European conference on computer vision, pages", "citeRegEx": "Farhadi et al\\.,? 2010", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "Predicting object dynamics in scenes", "author": ["David F Fouhey", "C Lawrence Zitnick."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2019\u20132026.", "citeRegEx": "Fouhey and Zitnick.,? 2014", "shortCiteRegEx": "Fouhey and Zitnick.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Natural language object retrieval", "author": ["Ronghang Hu", "Huazhe Xu", "Marcus Rohrbach", "Jiashi Feng", "Kate Saenko", "Trevor Darrell."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4555\u20134564.", "citeRegEx": "Hu et al\\.,? 2016", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "Summarizing source code using a neural attention model", "author": ["Srinivasan Iyer", "Ioannis Konstas", "Alvin Cheung", "Luke Zettlemoyer."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Iyer et al\\.,? 2016", "shortCiteRegEx": "Iyer et al\\.", "year": 2016}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128\u20133137.", "citeRegEx": "Karpathy and Fei.Fei.,? 2015", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Neuraltalk2", "author": ["Andrej Karpathy"], "venue": "https://github.com/karpathy/neuraltalk2/.", "citeRegEx": "Karpathy,? 2016", "shortCiteRegEx": "Karpathy", "year": 2016}, {"title": "Unsupervised concept-to-text generation with hypergraphs", "author": ["Ioannis Konstas", "Mirella Lapata."], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 752\u2013", "citeRegEx": "Konstas and Lapata.,? 2012", "shortCiteRegEx": "Konstas and Lapata.", "year": 2012}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image anno", "author": ["Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma"], "venue": null, "citeRegEx": "Krishna et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Krishna et al\\.", "year": 2017}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "Neural Information Processing Systems (NIPS), pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Design of a knowledge-based report generator", "author": ["Karen Kukich."], "venue": "Proceedings of the 21st annual meeting on Association for Computational Linguistics, pages 145\u2013150. Association for Computational Linguistics.", "citeRegEx": "Kukich.,? 1983", "shortCiteRegEx": "Kukich.", "year": 1983}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick."], "venue": "ECCV, pages 740\u2013 755. Springer.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "CoRR, abs/1508.04025.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan Yuille."], "venue": "ICLR.", "citeRegEx": "Mao et al\\.,? 2015", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Nonparametric method for data-driven image captioning", "author": ["Rebecca Mason", "Eugene Charniak."], "venue": "ACL (2), pages 592\u2013598.", "citeRegEx": "Mason and Charniak.,? 2014", "shortCiteRegEx": "Mason and Charniak.", "year": 2014}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["Vicente Ordonez", "Girish Kulkarni", "Tamara L Berg."], "venue": "Advances in Neural Information Processing Systems, pages 1143\u20131151.", "citeRegEx": "Ordonez et al\\.,? 2011", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models", "author": ["Bryan A Plummer", "Liwei Wang", "Chris M Cervantes", "Juan C Caicedo", "Julia Hockenmaier", "Svetlana Lazebnik."], "venue": "Proceedings of the IEEE", "citeRegEx": "Plummer et al\\.,? 2015", "shortCiteRegEx": "Plummer et al\\.", "year": 2015}, {"title": "Combining geometric, textual and visual features for predicting prepositions in image descriptions", "author": ["Arnau Ramisa", "JK Wang", "Ying Lu", "Emmanuel Dellandrea", "Francesc Moreno-Noguer", "Robert Gaizauskas."], "venue": "Conference on Empirical Meth-", "citeRegEx": "Ramisa et al\\.,? 2015", "shortCiteRegEx": "Ramisa et al\\.", "year": 2015}, {"title": "YOLO9000: better, faster, stronger", "author": ["Joseph Redmon", "Ali Farhadi."], "venue": "Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Redmon and Farhadi.,? 2017", "shortCiteRegEx": "Redmon and Farhadi.", "year": 2017}, {"title": "Grounding of textual phrases in images by reconstruction", "author": ["Anna Rohrbach", "Marcus Rohrbach", "Ronghang Hu", "Trevor Darrell", "Bernt Schiele."], "venue": "European Conference on Computer Vision, pages 817\u2013834. Springer.", "citeRegEx": "Rohrbach et al\\.,? 2016", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2016}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Word ordering without syntax", "author": ["Allen Schmaltz", "Alexander M. Rush", "Stuart Shieber."], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2319\u20132324, Austin, Texas.", "citeRegEx": "Schmaltz et al\\.,? 2016", "shortCiteRegEx": "Schmaltz et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Simonyan and Zisserman.,? 2015", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2015}, {"title": "Automatically extracting and representing collocations for language generation", "author": ["Frank A Smadja", "Kathleen R McKeown."], "venue": "Annual meeting of the Association for Computational Linguistics (ACL), pages 252\u2013259.", "citeRegEx": "Smadja and McKeown.,? 1990", "shortCiteRegEx": "Smadja and McKeown.", "year": 1990}, {"title": "Cider: Consensus-based image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4566\u20134575.", "citeRegEx": "Vedantam et al\\.,? 2015a", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Learning common sense through visual abstraction", "author": ["Ramakrishna Vedantam", "Xiao Lin", "Tanmay Batra", "C Lawrence Zitnick", "Devi Parikh."], "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 2542\u20132550.", "citeRegEx": "Vedantam et al\\.,? 2015b", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3156\u20133164.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrk\u0161i\u0107", "PeiHao Su", "David Vandyke", "Steve Young."], "venue": "Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Wen et al\\.,? 2015", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."], "venue": "International Conference on Machine", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Oracle performance for visual captioning", "author": ["Li Yao", "Nicolas Ballas", "Kyunghyun Cho", "John R. Smith", "Yoshua Bengio."], "venue": "British Machine Vision Conference (BMVC).", "citeRegEx": "Yao et al\\.,? 2016a", "shortCiteRegEx": "Yao et al\\.", "year": 2016}, {"title": "Boosting image captioning with attributes", "author": ["Ting Yao", "Yingwei Pan", "Yehao Li", "Zhaofan Qiu", "Tao Mei."], "venue": "arXiv preprint arXiv:1611.01646.", "citeRegEx": "Yao et al\\.,? 2016b", "shortCiteRegEx": "Yao et al\\.", "year": 2016}, {"title": "Stating the obvious: Extracting visual common sense knowledge", "author": ["Mark Yatskar", "Vicente Ordonez", "Ali Farhadi."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human", "citeRegEx": "Yatskar et al\\.,? 2016", "shortCiteRegEx": "Yatskar et al\\.", "year": 2016}, {"title": "Image captioning with semantic attention", "author": ["Quanzeng You", "Hailin Jin", "Zhaowen Wang", "Chen Fang", "Jiebo Luo."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4651\u20134659.", "citeRegEx": "You et al\\.,? 2016", "shortCiteRegEx": "You et al\\.", "year": 2016}, {"title": "Bringing semantics into focus using visual abstraction", "author": ["C Lawrence Zitnick", "Devi Parikh."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3009\u20133016.", "citeRegEx": "Zitnick and Parikh.,? 2013", "shortCiteRegEx": "Zitnick and Parikh.", "year": 2013}, {"title": "Learning the visual interpretation of sentences", "author": ["C Lawrence Zitnick", "Devi Parikh", "Lucy Vanderwende."], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1681\u20131688.", "citeRegEx": "Zitnick et al\\.,? 2013", "shortCiteRegEx": "Zitnick et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 15, "context": "There have already been several successes in applications such as financial reporting (Kukich, 1983; Smadja and McKeown, 1990), or weather forecasts (Konstas and Lapata, 2012; Wen et al.", "startOffset": 86, "endOffset": 126}, {"referenceID": 29, "context": "There have already been several successes in applications such as financial reporting (Kukich, 1983; Smadja and McKeown, 1990), or weather forecasts (Konstas and Lapata, 2012; Wen et al.", "startOffset": 86, "endOffset": 126}, {"referenceID": 12, "context": "There have already been several successes in applications such as financial reporting (Kukich, 1983; Smadja and McKeown, 1990), or weather forecasts (Konstas and Lapata, 2012; Wen et al., 2015), however it is still a challenging task for less structured and open domains.", "startOffset": 149, "endOffset": 193}, {"referenceID": 33, "context": "There have already been several successes in applications such as financial reporting (Kukich, 1983; Smadja and McKeown, 1990), or weather forecasts (Konstas and Lapata, 2012; Wen et al., 2015), however it is still a challenging task for less structured and open domains.", "startOffset": 149, "endOffset": 193}, {"referenceID": 32, "context": "bitrary images has received considerable attention (Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Mao et al., 2015).", "startOffset": 51, "endOffset": 119}, {"referenceID": 10, "context": "bitrary images has received considerable attention (Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Mao et al., 2015).", "startOffset": 51, "endOffset": 119}, {"referenceID": 18, "context": "bitrary images has received considerable attention (Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Mao et al., 2015).", "startOffset": 51, "endOffset": 119}, {"referenceID": 7, "context": "We propose OBJ2TEXT, a sequence-tosequence model that encodes object layouts using an LSTM network (Hochreiter and Schmidhuber, 1997), and decodes natural language descriptions using an LSTM-based neural language model1.", "startOffset": 99, "endOffset": 133}, {"referenceID": 16, "context": "Our model is trained in the standard MS-COCO dataset (Lin et al., 2014), which includes both object annotations for the task of object detection, and textual descriptions for the task of image captioning.", "startOffset": 53, "endOffset": 71}, {"referenceID": 7, "context": "We propose OBJ2TEXT, a sequence-tosequence model that encodes object layouts using an LSTM network (Hochreiter and Schmidhuber, 1997), and decodes natural language descriptions using an LSTM-based neural language model1. Natural language generation systems usually consist of two steps: content planning, and surface realization. The first step decides on the content to be included in the generated text, and the second step connects the concepts using structural language properties. In our proposed model, OBJ2TEXT, content planning is performed by the encoder, and surface realization is performed by the decoder. Our model is trained in the standard MS-COCO dataset (Lin et al., 2014), which includes both object annotations for the task of object detection, and textual descriptions for the task of image captioning. While most previous research has been devoted to any one of these two tasks, our paper presents, to our knowledge, the first approach for learning mappings between object annotations and textual descriptions. Using several lesioned versions of the proposed model we explored the effect of object counts and locations in the quality and accuracy of the generated natural language descriptions. Generating visually descriptive language requires beyond syntax, and semantics; an understanding of the physical word. We also take inspiration from recent work by Schmaltz et al. (2016) where the goal was to reconstruct a sentence from a bag-of-words (BOW) representation using a simple surface-level language model based on an encoder-decoder sequence-to-sequence architecture.", "startOffset": 100, "endOffset": 1403}, {"referenceID": 7, "context": "We propose OBJ2TEXT, a sequence-tosequence model that encodes object layouts using an LSTM network (Hochreiter and Schmidhuber, 1997), and decodes natural language descriptions using an LSTM-based neural language model1. Natural language generation systems usually consist of two steps: content planning, and surface realization. The first step decides on the content to be included in the generated text, and the second step connects the concepts using structural language properties. In our proposed model, OBJ2TEXT, content planning is performed by the encoder, and surface realization is performed by the decoder. Our model is trained in the standard MS-COCO dataset (Lin et al., 2014), which includes both object annotations for the task of object detection, and textual descriptions for the task of image captioning. While most previous research has been devoted to any one of these two tasks, our paper presents, to our knowledge, the first approach for learning mappings between object annotations and textual descriptions. Using several lesioned versions of the proposed model we explored the effect of object counts and locations in the quality and accuracy of the generated natural language descriptions. Generating visually descriptive language requires beyond syntax, and semantics; an understanding of the physical word. We also take inspiration from recent work by Schmaltz et al. (2016) where the goal was to reconstruct a sentence from a bag-of-words (BOW) representation using a simple surface-level language model based on an encoder-decoder sequence-to-sequence architecture. In contrast to this previous approach, our model is grounded on visual data, and its corresponding spatial information, so it goes beyond word re-ordering. Also relevant to our work is Yao et al. (2016a) which previously explored the task of oracle image captioning by providing a language generation model with a list of manually defined visual concepts known to be present in the image.", "startOffset": 100, "endOffset": 1800}, {"referenceID": 39, "context": "Our work is related to previous works that used clipart scenes for visually-grounded tasks including sentence interpretation (Zitnick and Parikh, 2013; Zitnick et al., 2013), and predicting object dynamics (Fouhey and Zitnick, 2014).", "startOffset": 125, "endOffset": 173}, {"referenceID": 40, "context": "Our work is related to previous works that used clipart scenes for visually-grounded tasks including sentence interpretation (Zitnick and Parikh, 2013; Zitnick et al., 2013), and predicting object dynamics (Fouhey and Zitnick, 2014).", "startOffset": 125, "endOffset": 173}, {"referenceID": 6, "context": ", 2013), and predicting object dynamics (Fouhey and Zitnick, 2014).", "startOffset": 40, "endOffset": 66}, {"referenceID": 39, "context": "The cited advantage of abstract scene representations such as the ones provided by the clipart scenes dataset proposed in (Zitnick and Parikh, 2013) is their ability to separate the complexity of pattern recognition from semantic visual representation.", "startOffset": 122, "endOffset": 148}, {"referenceID": 29, "context": "The works of Vedantam et al. (2015b); Eysenbach et al.", "startOffset": 13, "endOffset": 37}, {"referenceID": 3, "context": "(2015b); Eysenbach et al. (2016) proposed methods to learn common-sense knowledge from clipart scenes, while the method of Yatskar et al.", "startOffset": 9, "endOffset": 33}, {"referenceID": 3, "context": "(2015b); Eysenbach et al. (2016) proposed methods to learn common-sense knowledge from clipart scenes, while the method of Yatskar et al. (2016), similar to our work, leverages object annotations for natural images.", "startOffset": 9, "endOffset": 145}, {"referenceID": 5, "context": "Our work is also related to other language generation tasks such as image and video captioning (Farhadi et al., 2010; Ordonez et al., 2011; Mason and Charniak, 2014; Ordonez et al., 2015; Xu et al., 2015; Donahue et al., 2015; Mao et al., 2015; Fang et al., 2015).", "startOffset": 95, "endOffset": 263}, {"referenceID": 20, "context": "Our work is also related to other language generation tasks such as image and video captioning (Farhadi et al., 2010; Ordonez et al., 2011; Mason and Charniak, 2014; Ordonez et al., 2015; Xu et al., 2015; Donahue et al., 2015; Mao et al., 2015; Fang et al., 2015).", "startOffset": 95, "endOffset": 263}, {"referenceID": 19, "context": "Our work is also related to other language generation tasks such as image and video captioning (Farhadi et al., 2010; Ordonez et al., 2011; Mason and Charniak, 2014; Ordonez et al., 2015; Xu et al., 2015; Donahue et al., 2015; Mao et al., 2015; Fang et al., 2015).", "startOffset": 95, "endOffset": 263}, {"referenceID": 34, "context": "Our work is also related to other language generation tasks such as image and video captioning (Farhadi et al., 2010; Ordonez et al., 2011; Mason and Charniak, 2014; Ordonez et al., 2015; Xu et al., 2015; Donahue et al., 2015; Mao et al., 2015; Fang et al., 2015).", "startOffset": 95, "endOffset": 263}, {"referenceID": 1, "context": "Our work is also related to other language generation tasks such as image and video captioning (Farhadi et al., 2010; Ordonez et al., 2011; Mason and Charniak, 2014; Ordonez et al., 2015; Xu et al., 2015; Donahue et al., 2015; Mao et al., 2015; Fang et al., 2015).", "startOffset": 95, "endOffset": 263}, {"referenceID": 18, "context": "Our work is also related to other language generation tasks such as image and video captioning (Farhadi et al., 2010; Ordonez et al., 2011; Mason and Charniak, 2014; Ordonez et al., 2015; Xu et al., 2015; Donahue et al., 2015; Mao et al., 2015; Fang et al., 2015).", "startOffset": 95, "endOffset": 263}, {"referenceID": 4, "context": "Our work is also related to other language generation tasks such as image and video captioning (Farhadi et al., 2010; Ordonez et al., 2011; Mason and Charniak, 2014; Ordonez et al., 2015; Xu et al., 2015; Donahue et al., 2015; Mao et al., 2015; Fang et al., 2015).", "startOffset": 95, "endOffset": 263}, {"referenceID": 14, "context": "Fueled by recent advances in training deep neural networks (Krizhevsky et al., 2012) and the availability of large annotated datasets with images and captions such as the MS-COCO dataset (Lin et al.", "startOffset": 59, "endOffset": 84}, {"referenceID": 16, "context": ", 2012) and the availability of large annotated datasets with images and captions such as the MS-COCO dataset (Lin et al., 2014), recent methods on this task perform endto-end learning from pixels to text.", "startOffset": 110, "endOffset": 128}, {"referenceID": 10, "context": "Most recent approaches use a variation of an encoderdecoder model where a convolutional neural network (CNN) extracts visual features from the input image (encoder), and passes its outputs to a recurrent neural network (RNN) that generates a caption as a sequence of words (decoder) (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015).", "startOffset": 283, "endOffset": 333}, {"referenceID": 32, "context": "Most recent approaches use a variation of an encoderdecoder model where a convolutional neural network (CNN) extracts visual features from the input image (encoder), and passes its outputs to a recurrent neural network (RNN) that generates a caption as a sequence of words (decoder) (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015).", "startOffset": 283, "endOffset": 333}, {"referenceID": 9, "context": "Several other recent works use a similar sequenceto-sequence approach to generate text from source code input (Iyer et al., 2016), or to translate text from one language to another (Bahdanau et al.", "startOffset": 110, "endOffset": 129}, {"referenceID": 0, "context": ", 2016), or to translate text from one language to another (Bahdanau et al., 2015).", "startOffset": 59, "endOffset": 82}, {"referenceID": 2, "context": "The work of Elliott and Keller (2013) manually defined a dictionary of objectobject relations based on geometric cues.", "startOffset": 12, "endOffset": 38}, {"referenceID": 2, "context": "The work of Elliott and Keller (2013) manually defined a dictionary of objectobject relations based on geometric cues. The work of Ramisa et al. (2015) is focused on predicting preposition given two entities and their locations in an image.", "startOffset": 12, "endOffset": 152}, {"referenceID": 2, "context": "The work of Elliott and Keller (2013) manually defined a dictionary of objectobject relations based on geometric cues. The work of Ramisa et al. (2015) is focused on predicting preposition given two entities and their locations in an image. Previous works of Plummer et al. (2015) and Rohrbach et al.", "startOffset": 12, "endOffset": 281}, {"referenceID": 2, "context": "The work of Elliott and Keller (2013) manually defined a dictionary of objectobject relations based on geometric cues. The work of Ramisa et al. (2015) is focused on predicting preposition given two entities and their locations in an image. Previous works of Plummer et al. (2015) and Rohrbach et al. (2016) showed that switching from classification-based CNN network to detection-based Fast RCNN network improves performance for phrase localization.", "startOffset": 12, "endOffset": 308}, {"referenceID": 2, "context": "The work of Elliott and Keller (2013) manually defined a dictionary of objectobject relations based on geometric cues. The work of Ramisa et al. (2015) is focused on predicting preposition given two entities and their locations in an image. Previous works of Plummer et al. (2015) and Rohrbach et al. (2016) showed that switching from classification-based CNN network to detection-based Fast RCNN network improves performance for phrase localization. The work of Hu et al. (2016) showed that encoding image regions with spatial information is crucial for natural language object retrieval as the task explicitly asks for locations of target objects.", "startOffset": 12, "endOffset": 480}, {"referenceID": 24, "context": "1), as well as two further variations to use our model to generate captions for real images: OBJ2TEXT-YOLO which uses the YOLO object detector (Redmon and Farhadi, 2017) to generate layouts of object locations from real images (section 4.", "startOffset": 143, "endOffset": 169}, {"referenceID": 32, "context": "As a common practice for an approximate solution, we follow (Vinyals et al., 2015) and use beam search to limit the choices for words at each time-step by only using the ones with the highest probabilities.", "startOffset": 60, "endOffset": 82}, {"referenceID": 24, "context": "This model takes an image as input, extracts an object layout (object categories and locations) with a state-of-the-art object detection model YOLO (Redmon and Farhadi, 2017), and uses OBJ2TEXT as described in section 4.", "startOffset": 148, "endOffset": 174}, {"referenceID": 28, "context": "VGG-16 (Simonyan and Zisserman, 2015) convolutional neural network pre-trained on the ImageNet classification task (Russakovsky et al.", "startOffset": 7, "endOffset": 37}, {"referenceID": 26, "context": "VGG-16 (Simonyan and Zisserman, 2015) convolutional neural network pre-trained on the ImageNet classification task (Russakovsky et al., 2015).", "startOffset": 115, "endOffset": 141}, {"referenceID": 16, "context": "We evaluate the proposed models on the MSCOCO (Lin et al., 2014) dataset which is a popular image captioning benchmark that also contains object extent annotations.", "startOffset": 46, "endOffset": 64}, {"referenceID": 30, "context": "Impact of Object Locations and Counts: Figure 3a shows the CIDEr (Vedantam et al., 2015a), and BLEU-4 (Papineni et al.", "startOffset": 65, "endOffset": 89}, {"referenceID": 21, "context": ", 2015a), and BLEU-4 (Papineni et al., 2002) score history on our validation set during 400k iterations of training of OBJ2TEXT, as well as a version of our model that does not use object locations, and a version of our model that does not use neither object locations nor object counts.", "startOffset": 21, "endOffset": 44}, {"referenceID": 17, "context": "On top of OBJ2TEXT we additionally experimented with the global attention model proposed in (Luong et al., 2015) so that a weighted combination of the encoder hidden states are forwarded to the decoding neural language model, however we did not notice any overall gains in terms of accuracy from this formulation.", "startOffset": 92, "endOffset": 112}, {"referenceID": 0, "context": "We observed that this model provided gains only for larger input sequences where it is more likely that the LSTM network forgets its past history (Bahdanau et al., 2015).", "startOffset": 146, "endOffset": 169}, {"referenceID": 22, "context": "In case of object annotations the MS-COCO dataset only provides object labels and bounding-boxes, but there are other datasets such as Flick30K Entities (Plummer et al., 2015), and the Visual Genome dataset (Krishna et al.", "startOffset": 153, "endOffset": 175}, {"referenceID": 13, "context": ", 2015), and the Visual Genome dataset (Krishna et al., 2017) that provide richer region-tophrase correspondence annotations.", "startOffset": 39, "endOffset": 61}, {"referenceID": 34, "context": "For example, the work of You et al. (2016) and Yao et al.", "startOffset": 25, "endOffset": 43}, {"referenceID": 33, "context": "(2016) and Yao et al. (2016b) showed that visual features trained with semantic concepts (text entities mentioned in captions) instead of object labels is useful for image captioning, although they didn\u2019t consider encoding semantic concepts with spatial information.", "startOffset": 11, "endOffset": 30}], "year": 2017, "abstractText": "Generating captions for images is a task that has recently received considerable attention. In this work we focus on caption generation for abstract scenes, or object layouts where the only information provided is a set of objects and their locations. We propose OBJ2TEXT, a sequence-tosequence model that encodes a set of objects and their locations as an input sequence using an LSTM network, and decodes this representation using an LSTM language model. We show that our model, despite encoding object layouts as a sequence, can represent spatial relationships between objects, and generate descriptions that are globally coherent and semantically relevant. We test our approach in a task of object-layout captioning by using only object annotations as inputs. We additionally show that our model, combined with a state-of-the-art object detector, improves an image captioning model from 0.863 to 0.950 (CIDEr score) in the test benchmark of the standard MS-COCO Captioning task.", "creator": "LaTeX with hyperref package"}}}