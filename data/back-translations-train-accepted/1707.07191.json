{"id": "1707.07191", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jul-2017", "title": "MoodSwipe: A Soft Keyboard that Suggests Messages Based on User-Specified Emotions", "abstract": "We present MoodSwipe, a soft keyboard that suggests text messages given the user-specified emotions utilizing the real dialog data. The aim of MoodSwipe is to create a convenient user interface to enjoy the technology of emotion classification and text suggestion, and at the same time to collect labeled data automatically for developing more advanced technologies. While users select the MoodSwipe keyboard, they can type as usual but sense the emotion conveyed by their text and receive suggestions for their message as a benefit. In MoodSwipe, the detected emotions serve as the medium for suggested texts, where viewing the latter is the incentive to correcting the former. We conduct several experiments to show the superiority of the emotion classification models trained on the dialog data, and further to verify good emotion cues are important context for text suggestion.", "histories": [["v1", "Sat, 22 Jul 2017 16:32:16 GMT  (4100kb,D)", "http://arxiv.org/abs/1707.07191v1", "6 pages (including references), EMNLP 2017 Demo paper"]], "COMMENTS": "6 pages (including references), EMNLP 2017 Demo paper", "reviews": [], "SUBJECTS": "cs.CL cs.HC", "authors": ["chieh-yang huang", "tristan labetoulle", "ting-hao kenneth huang", "yi-pei chen", "hung-chen chen", "vallari srivastava", "lun-wei ku"], "accepted": true, "id": "1707.07191"}, "pdf": {"name": "1707.07191.pdf", "metadata": {"source": "CRF", "title": "MoodSwipe: A Soft Keyboard that Suggests Messages Based on User-Specified Emotions", "authors": ["Chieh-Yang Huang", "Tristan Labetoulle", "Ting-Hao (Kenneth) Huang", "Yi-Pei Chen", "Hung-Chen Chen", "Vallari Srivastava", "Lun-Wei Ku"], "emails": ["appleternity@iis.sinica.edu.tw", "ypc82@iis.sinica.edu.tw", "hankchen@iis.sinica.edu.tw", "lwku@iis.sinica.edu.tw", "contact@tristan-labetoulle.com", "tinghaoh@cs.cmu.edu", "vallari357@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it's as if most people who are able to express their feelings need to help themselves and their fellow human beings. (...) It's not as if people are able to express their emotions correctly. (...) It's as if people are able to express their emotions correctly. (...) It's as if people were able to express their emotions correctly. (...) It's as if people were able to express their emotions correctly. (...) It's as if people were able to express their feelings correctly. (...) It's as if people were able to transmit their anger to themselves. (...) It's as if people were able to trigger their emotions correctly. (...) It's as if they do it, as if they do it, as if they do it, as if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it. (...) It's as if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it."}, {"heading": "2 The MoodSwipe Keyboard", "text": "The user interface and workflow of MoodSwipe are shown in Figure 1. MoodSwipe's user interface contains three important parts: (i) a standard soft keyboard, (ii) a color bar above the keyboard, and (iii) a circle button to the right of the color bar. When the user starts typing, MoodSwipe detects the emotion of the input text in real time, and the color bar background changes color on-the-fly to reflect the current emotion of the user. MoodSwipe's seven emotions and their colors are shown in Figure 2, which was developed based on psychological work and user studies (Wang et al., 2016). Based on the Emotion MoodSwipe currently displays the proposed text. These suggested messages for entering \"I'm OK,\" they are listed in Figure 2. When the user swipes the color bar, one of the seven emotion colors is displayed, displayed on the right, according to the grading. \""}, {"heading": "3 Use Cases", "text": "In this section, we outline several possible uses of MoodSwipe. First, MoodSwipe can help users better understand the emotions of their own messages perceived by other users. Our previous study (Huang et al., 2017) shows that not all users are clearly aware of the emotions their messages will convey. In this scenario, MoodSwipe can act as an early assistant or reminder when it comes to sending a message. Second, MoodSwipe can help users express themselves better when \"words don't fit me.\" Sometimes, users may feel strong emotions and have difficulty finding good ways to express themselves via text, and they can enter keywords in MoodSwipe to search for better messages from their dialogue database. Third, users can alternate the perceived emotions in their own texts for different purposes. Thus, some people may need help to rephrase their angry messages into neutral descriptions, and some people may consciously want to express their anger in order to receive messages from their community, so that they can be used by young people as a tool to extract concessions from moodSwipe, eventually."}, {"heading": "4 Back-end System, Experiments and Discussions", "text": "Two important functions of the MoodSwipe keyboard are to advise users on the emotion of their current SMS and to suggest text based on it. In this section we describe several models developed by us and different settings for evaluating their performance. Our advantage in conducting these experiments is our emotion-based chat app EmotionPush (Wang et al., 2016) and the social dialogues it collects."}, {"heading": "4.1 Experimental Materials", "text": "For the experiments, we used the EmotionPush dataset (soon to be available). A total of 162,031 emotion logs were collected for this dataset. To evaluate the performance of the emotion classification, we had native speakers manually label the emotions of the 8,818 randomly selected messages. These manual emotion logs were all selected from the seven emotion logs defined for the keyboard. Of these 8,818 emotion logs, 70% were used for training, 10% for validation, and 20% for the test. By comparison, two different emotion corpus, LJ40K (Yang and Liu, 2013) and the Tweet data are used. LJ40K contains 40 emotions and for each of the emotions 1,000 blogs are collected, and the 40 emotions are then assigned to the 7 emotions according to our previous work (Wang et al., 2016). On the other hand, the tweet data is created by Twitter-streaming API2 with a filter for using 40 emotions as a hash tag."}, {"heading": "4.2 Emotion Classification", "text": "For the classification of emotions, two models have been developed: the general CNN (Kim, 2014) with 125 filters, including 25 filters for each filter length from 1 to 5, and the LSTM (Hochreiter and Schmidhuber, 1997), which are trained on blog data, tweet data or our dialog data and then tested on the dialog data. In Table 1, we report on the results of three major emotions with these two models, as the other emotions are insignificant and the training data is insufficient to build a reliable model (anticipation 1.77%, fatigue 0.8%, anxiety 1.19%, a total of 3.77%). Only accuracies trained on the dialog data for three major emotions are above 0.9 (see CNN3 and LSTM3), which supports the use of dialogs in MoodSwipe. Considering time-consuming problems, we have chosen CNN as the definitive model for MoodSwipe."}, {"heading": "4.3 Text Suggestion & Results", "text": "The purpose of the campaign is to determine whether the participants are people who are able to help themselves."}, {"heading": "4.4 Collecting User-reported Labels", "text": "One of the merits of MoodSwipe is the ability to collect user-reported labels. MoodSwipe can retrieve, select and swipe labels from two important user actions, respectively: 1. [Select] When the user first types a response, searches through all suggestions, and finally selects a proposed text, MoodSwipe can record the originally entered text of the user and mark his emotions as that of the selected text. 2. [Swipe] When the user first types a response, swipes directly to a particular feeling (e.g. Joy) and stops there, even without selecting the proposed text, this often still indicates that the user wants to express that emotion (e.g. Joy.) Therefore, MoodSwipe can record the current typed text of the user and mark it as the same feeling, even if the user ultimately does not select the proposed text."}, {"heading": "5 Conclusion", "text": "We developed the sender-side MoodSwipe to collaborate with the receiver-side applications and complete the emotion-sensitive communication framework. MoodSwipe provided a convenient user interface that makes it easier for users to use modern emotion classification and text suggestion techniques. In MoodSwipe, the data is automatically labeled using front-end cues in the background. We show that the user-specific emotion can benefit from text suggestions, although performance can be improved by increasing the dialog database. MoodSwipe is available on Google Play, and a demo video is available at https: / / www.youtube.com / watch? v = SZ1biWoiq3Y"}, {"heading": "Acknowledgments", "text": "The study of this work was partially supported by the Taiwanese Ministry of Science and Technology under Contract 105-2221-E-001-007-MY3."}], "references": [{"title": "Affective student modeling based on microphone and keyboard user actions", "author": ["Efthymios Alepis", "Maria Virvou", "Katerina Kabassi."], "venue": "Advanced", "citeRegEx": "Alepis et al\\.,? 2006", "shortCiteRegEx": "Alepis et al\\.", "year": 2006}, {"title": "Nice guys finish first: A symlog analysis of us naval commands", "author": ["Wallace Bachman."], "venue": "The SYMLOG practitioner: Applications of small group research, pages 133\u2013154.", "citeRegEx": "Bachman.,? 1988", "shortCiteRegEx": "Bachman.", "year": 1988}, {"title": "Group emotion: A view from top and bottom", "author": ["Sigal G Barsade", "Donald E Gibson."], "venue": "Research on managing groups and teams, 1(4):81\u2013102.", "citeRegEx": "Barsade and Gibson.,? 1998", "shortCiteRegEx": "Barsade and Gibson.", "year": 1998}, {"title": "Kinetic typography-based instant messaging", "author": ["Kerry Bodine", "Mathilde Pignol."], "venue": "CHI\u201903 Extended Abstracts on Human Factors in Computing Systems, pages 914\u2013915. ACM.", "citeRegEx": "Bodine and Pignol.,? 2003", "shortCiteRegEx": "Bodine and Pignol.", "year": 2003}, {"title": "Affectbutton: Towards a standard for dynamic affective user feedback", "author": ["Joost Broekens", "Willem-Paul Brinkman."], "venue": "Affective Computing and Intelligent Interaction and Workshops, 2009. ACII 2009. 3rd International Conference on, pages 1\u20138. IEEE.", "citeRegEx": "Broekens and Brinkman.,? 2009", "shortCiteRegEx": "Broekens and Brinkman.", "year": 2009}, {"title": "Conductive chat: Instant messaging with a skin conductivity channel", "author": ["Joan Morris DiMicco", "Vidya Lakshmipathy", "Andrew Tresolini Fiore."], "venue": "Proceedings of Conference on Computer Supported Cooperative Work.", "citeRegEx": "DiMicco et al\\.,? 2002", "shortCiteRegEx": "DiMicco et al\\.", "year": 2002}, {"title": "Faim: integrating automated facial affect analysis in instant messaging", "author": ["Rana El Kaliouby", "Peter Robinson."], "venue": "Proceedings of the 9th international conference on Intelligent user interfaces, pages 244\u2013 246. ACM.", "citeRegEx": "Kaliouby and Robinson.,? 2004", "shortCiteRegEx": "Kaliouby and Robinson.", "year": 2004}, {"title": "The kinedit system: affective messages using dynamic texts", "author": ["Jodi Forlizzi", "Johnny Lee", "Scott Hudson."], "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 377\u2013384. ACM.", "citeRegEx": "Forlizzi et al\\.,? 2003", "shortCiteRegEx": "Forlizzi et al\\.", "year": 2003}, {"title": "The positives of negative emotions: Willingness to express negative emotions promotes relationships", "author": ["Steven M Graham", "Julie Y Huang", "Margaret S Clark", "Vicki S Helgeson."], "venue": "Personality and Social Psychology Bulletin, 34(3):394\u2013406.", "citeRegEx": "Graham et al\\.,? 2008", "shortCiteRegEx": "Graham et al\\.", "year": 2008}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Challenges in providing automatic affective feedback in instant messaging applications", "author": ["Chieh-Yang Huang", "Lun-Wei Ku"], "venue": "arXiv preprint arXiv:1702.02736", "citeRegEx": "Huang and Ku,? \\Q2017\\E", "shortCiteRegEx": "Huang and Ku", "year": 2017}, {"title": "Smart reply: Automated response suggestion for email", "author": ["Anjuli Kannan", "Karol Kurach", "Sujith Ravi", "Tobias Kaufmann", "Andrew Tomkins", "Balint Miklos", "Greg Corrado", "L\u00e1szl\u00f3 Luk\u00e1cs", "Marina Ganea", "Peter Young"], "venue": null, "citeRegEx": "Kannan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Using kinetic typography to convey emotion in text-based interpersonal communication", "author": ["Joonhwan Lee", "Soojin Jun", "Jodi Forlizzi", "Scott E Hudson."], "venue": "Proceedings of the 6th conference on Designing Interactive systems, pages 41\u201349. ACM.", "citeRegEx": "Lee et al\\.,? 2006", "shortCiteRegEx": "Lee et al\\.", "year": 2006}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Michel Galley", "Jianfeng Gao", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1606.01541.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Quality of communication experience: definition, measurement, and implications for intercultural negotiations", "author": ["Leigh Anne Liu", "Chei Hwee Chua", "G\u00fcnter K Stahl."], "venue": "Journal of Applied Psychology, 95(3):469.", "citeRegEx": "Liu et al\\.,? 2010", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Lucene in Action, Second Edition: Covers Apache Lucene 3.0", "author": ["Michael McCandless", "Erik Hatcher", "Otis Gospodnetic"], "venue": null, "citeRegEx": "McCandless et al\\.,? \\Q2010\\E", "shortCiteRegEx": "McCandless et al\\.", "year": 2010}, {"title": "The probabilistic relevance framework: Bm25 and beyond", "author": ["Stephen Robertson", "Hugo Zaragoza"], "venue": "Foundations and Trends R", "citeRegEx": "Robertson and Zaragoza,? \\Q2009\\E", "shortCiteRegEx": "Robertson and Zaragoza", "year": 2009}, {"title": "Emotional intelligence", "author": ["Peter Salovey", "John D Mayer."], "venue": "Imagination, cognition and personality, 9(3):185\u2013211.", "citeRegEx": "Salovey and Mayer.,? 1990", "shortCiteRegEx": "Salovey and Mayer.", "year": 1990}, {"title": "Conveying mood and emotion in instant messaging by using a twodimensional model for affective states", "author": ["J Alfredo S\u00e1nchez", "Norma P Hern\u00e1ndez", "Julio C Penagos", "Yulia Ostr\u00f3vskaya."], "venue": "Proceedings of VII Brazilian symposium on Human factors", "citeRegEx": "S\u00e1nchez et al\\.,? 2006", "shortCiteRegEx": "S\u00e1nchez et al\\.", "year": 2006}, {"title": "2016. This smartphone keyboard app can read your emotions", "author": ["Cornell Tech"], "venue": null, "citeRegEx": "Tech.,? \\Q2016\\E", "shortCiteRegEx": "Tech.", "year": 2016}, {"title": "Sensing emotions in text messages: An application and deployment study of emotionpush", "author": ["Shih-Ming Wang", "Chun-Hui Li", "Yu-Chun Lo", "TingHao K Huang", "Lun-Wei Ku."], "venue": "arXiv preprint arXiv:1610.04758.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Hot under the collar: Mapping thermal feedback to dimensional models of emotion", "author": ["Graham Wilson", "Dobromir Dobrev", "Stephen A Brewster."], "venue": "Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, pages 4838\u20134849.", "citeRegEx": "Wilson et al\\.,? 2016", "shortCiteRegEx": "Wilson et al\\.", "year": 2016}, {"title": "Quantitative study of music listening behavior in a social and affective context", "author": ["Yi-Hsuan Yang", "Jen-Yu Liu."], "venue": "Multimedia, IEEE Transactions on, 15(6):1304\u20131315.", "citeRegEx": "Yang and Liu.,? 2013", "shortCiteRegEx": "Yang and Liu.", "year": 2013}, {"title": "Affective computing\u2014a rationale for measuring mood with mouse and keyboard", "author": ["Philippe Zimmermann", "Sissel Guttormsen", "Brigitta Danuser", "Patrick Gomez."], "venue": "International journal of occupational safety and ergonomics, 9(4):539\u2013551.", "citeRegEx": "Zimmermann et al\\.,? 2003", "shortCiteRegEx": "Zimmermann et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 18, "context": "Knowing how and when to express emotion is a key component of emotional intelligence (Salovey and Mayer, 1990).", "startOffset": 85, "endOffset": 110}, {"referenceID": 1, "context": "Effective leaders are good at expressing emotions (Bachman, 1988); expressing positive emotions in group activities improves group cooperation, fairness, and overall group performance (Barsade and Gibson, 1998); and expressing negative emotions can promote relationships (Graham et al.", "startOffset": 50, "endOffset": 65}, {"referenceID": 2, "context": "Effective leaders are good at expressing emotions (Bachman, 1988); expressing positive emotions in group activities improves group cooperation, fairness, and overall group performance (Barsade and Gibson, 1998); and expressing negative emotions can promote relationships (Graham et al.", "startOffset": 184, "endOffset": 210}, {"referenceID": 8, "context": "Effective leaders are good at expressing emotions (Bachman, 1988); expressing positive emotions in group activities improves group cooperation, fairness, and overall group performance (Barsade and Gibson, 1998); and expressing negative emotions can promote relationships (Graham et al., 2008).", "startOffset": 271, "endOffset": 292}, {"referenceID": 21, "context": "understand other people\u2019s emotions (Wang et al., 2016; Huang et al., 2017), these technologies have rarely been used to support user needs in expressing emotions.", "startOffset": 35, "endOffset": 74}, {"referenceID": 3, "context": "or dynamic text (Bodine and Pignol, 2003; Forlizzi et al., 2003; Lee et al., 2006) , affective buttons (Broekens and Brinkman, 2009), or text color and emoticons (S\u00e1nchez et al.", "startOffset": 16, "endOffset": 82}, {"referenceID": 7, "context": "or dynamic text (Bodine and Pignol, 2003; Forlizzi et al., 2003; Lee et al., 2006) , affective buttons (Broekens and Brinkman, 2009), or text color and emoticons (S\u00e1nchez et al.", "startOffset": 16, "endOffset": 82}, {"referenceID": 13, "context": "or dynamic text (Bodine and Pignol, 2003; Forlizzi et al., 2003; Lee et al., 2006) , affective buttons (Broekens and Brinkman, 2009), or text color and emoticons (S\u00e1nchez et al.", "startOffset": 16, "endOffset": 82}, {"referenceID": 4, "context": ", 2006) , affective buttons (Broekens and Brinkman, 2009), or text color and emoticons (S\u00e1nchez et al.", "startOffset": 28, "endOffset": 57}, {"referenceID": 19, "context": ", 2006) , affective buttons (Broekens and Brinkman, 2009), or text color and emoticons (S\u00e1nchez et al., 2006) to enable emotion expression in instant messengers.", "startOffset": 87, "endOffset": 109}, {"referenceID": 24, "context": "work explores the relations between user typing patterns and their emotions (Zimmermann et al., 2003; Alepis et al., 2006; Tech, 2016).", "startOffset": 76, "endOffset": 134}, {"referenceID": 0, "context": "work explores the relations between user typing patterns and their emotions (Zimmermann et al., 2003; Alepis et al., 2006; Tech, 2016).", "startOffset": 76, "endOffset": 134}, {"referenceID": 20, "context": "work explores the relations between user typing patterns and their emotions (Zimmermann et al., 2003; Alepis et al., 2006; Tech, 2016).", "startOffset": 76, "endOffset": 134}, {"referenceID": 5, "context": "Other work even describes attempts to incorporate body signals such as fluctuating skin conductivity levels (DiMicco et al., 2002), thermal feedback (Wilson et al.", "startOffset": 108, "endOffset": 130}, {"referenceID": 22, "context": ", 2002), thermal feedback (Wilson et al., 2016), or facial expressions (El Kaliouby and Robinson, 2004) to enrich emotion expression, but these require additional equipment and are less scalable.", "startOffset": 26, "endOffset": 47}, {"referenceID": 21, "context": "of psychologists and user studies (Wang et al., 2016).", "startOffset": 34, "endOffset": 53}, {"referenceID": 21, "context": "Most prior work powered by emotion detection focused on helping users when receiving messages (Wang et al., 2016) instead of when sending them.", "startOffset": 94, "endOffset": 113}, {"referenceID": 14, "context": "Classic response suggestion tasks such as dialog generation (Li et al., 2016) or automated email reply (Kannan et al.", "startOffset": 60, "endOffset": 77}, {"referenceID": 11, "context": ", 2016) or automated email reply (Kannan et al., 2016) assume that the in-themoment context of each user (e.", "startOffset": 33, "endOffset": 54}, {"referenceID": 21, "context": "MoodSwipe\u2019s seven emotions and their colors are shown in Figure 2, which was developed based on psychological work and user studies (Wang et al., 2016).", "startOffset": 132, "endOffset": 151}, {"referenceID": 21, "context": "Our advantage in conducting these experiments comes from our emotion-based chat app EmotionPush (Wang et al., 2016) and the social dialogs it has collected.", "startOffset": 96, "endOffset": 115}, {"referenceID": 23, "context": "Two different emotion corpus, LJ40K (Yang and Liu, 2013) and the tweet data, are utilized for comparison.", "startOffset": 36, "endOffset": 56}, {"referenceID": 21, "context": "previous work (Wang et al., 2016).", "startOffset": 14, "endOffset": 33}, {"referenceID": 12, "context": "Two models are developed for emotion classification: the general CNN (Kim, 2014) with 125 filters, including 25 filters for each filter length ranging from 1 to 5, and the LSTM (Hochreiter", "startOffset": 69, "endOffset": 80}, {"referenceID": 16, "context": "We designed a retrieval-based model utilizing Lucene (McCandless et al., 2010)", "startOffset": 53, "endOffset": 78}, {"referenceID": 21, "context": "overview Model Joy Anger Sadness Neutral (Wang et al., 2016) .", "startOffset": 41, "endOffset": 60}, {"referenceID": 15, "context": "clarity, comfort, and responsiveness of being the follow-up line of the given chat log (Liu et al., 2010) (rank = 1, 2, or 3.", "startOffset": 87, "endOffset": 105}], "year": 2017, "abstractText": "We present MoodSwipe, a soft keyboard that suggests text messages given the userspecified emotions utilizing the real dialog data. The aim of MoodSwipe is to create a convenient user interface to enjoy the technology of emotion classification and text suggestion, and at the same time to collect labeled data automatically for developing more advanced technologies. While users select the MoodSwipe keyboard, they can type as usual but sense the emotion conveyed by their text and receive suggestions for their message as a benefit. In MoodSwipe, the detected emotions serve as the medium for suggested texts, where viewing the latter is the incentive to correcting the former. We conduct several experiments to show the superiority of the emotion classification models trained on the dialog data, and further to verify good emotion cues are important context for text suggestion.", "creator": "LaTeX with hyperref package"}}}