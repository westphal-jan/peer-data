{"id": "1707.07212", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jul-2017", "title": "\"i have a feeling trump will win..................\": Forecasting Winners and Losers from User Predictions on Twitter", "abstract": "Social media users often make explicit predictions about upcoming events. Such statements vary in the degree of certainty the author expresses toward the outcome:\"Leonardo DiCaprio will win Best Actor\" vs. \"Leonardo DiCaprio may win\" or \"No way Leonardo wins!\". Can popular beliefs on social media predict who will win? To answer this question, we build a corpus of tweets annotated for veridicality on which we train a log-linear classifier that detects positive veridicality with high precision. We then forecast uncertain outcomes using the wisdom of crowds, by aggregating users' explicit predictions. Our method for forecasting winners is fully automated, relying only on a set of contenders as input. It requires no training data of past outcomes and outperforms sentiment and tweet volume baselines on a broad range of contest prediction tasks. We further demonstrate how our approach can be used to measure the reliability of individual accounts' predictions and retrospectively identify surprise outcomes.", "histories": [["v1", "Sat, 22 Jul 2017 20:34:53 GMT  (511kb,D)", "https://arxiv.org/abs/1707.07212v1", "Accepted at EMNLP 2017 (long paper)"], ["v2", "Tue, 25 Jul 2017 05:06:17 GMT  (511kb,D)", "http://arxiv.org/abs/1707.07212v2", "Accepted at EMNLP 2017 (long paper)"], ["v3", "Fri, 1 Sep 2017 01:15:44 GMT  (511kb,D)", "http://arxiv.org/abs/1707.07212v3", "Accepted at EMNLP 2017 (long paper)"]], "COMMENTS": "Accepted at EMNLP 2017 (long paper)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sandesh swamy", "alan ritter", "marie-catherine de marneffe"], "accepted": true, "id": "1707.07212"}, "pdf": {"name": "1707.07212.pdf", "metadata": {"source": "CRF", "title": "\u201ci have a feeling trump will win..................\u201d: Forecasting Winners and Losers from User Predictions on Twitter", "authors": ["Sandesh Swamy", "Alan Ritter", "Marie-Catherine de Marneffe", "Natalie Portman"], "emails": ["swamy.14@osu.edu,", "aritter@cse.ohio-state.edu", "mcdm@ling.ohio-state.edu"], "sections": [{"heading": "1 Introduction", "text": "In the digital era in which we live, millions of people send out their thoughts and opinions online, including predictions of upcoming events with as yet unknown outcomes, such as the Oscars or election results. Such statements vary to the extent to which their authors intend to convey the event. For example, (a) in Table 1, Natalie Portman's victory over Meryl Streep is strongly asserted, while (b) the assertion with 1The code and data can be found at https: / / github. com / SandeshS / Twitter-VeridicalityUncertainty. In contrast, (c) says nothing about the likelihood that Natalie Portman will win (although it clearly indicates that the author would like to win it).Prior's work has made predictions about competitions such as NFL games (Sinha et al., 2013) and elections with tweet volume (Tumasjan et al., 2010) or sentiment analysis (O-Condiction et al., 2010; Shi et al., 2012), but many of such events have proven to be useful in terms of their electoral usefulness, such as the ones we have examined in the Oscars."}, {"heading": "2 Related Work", "text": "In this section, we summarize the related work on text-driven predictions and computational models of verifiability. Text-driven prediction models (Smith, 2010) predict future response variables based on texts written in the present: e.g. predictions of film revenue based on reviews (Joshi et al., 2010), predictions of quotations from scientific articles (Yogatama et al., 2011) and the success of literary works (Ashok et al., 2015), predictions of economic indicators using query protocols (Choi et al., 2012), improvement of influenza predictions using Twitter data (Paul et al., 2014), predictions of betrayal in online strategy games (Niculae et al., 2015), and predictions of changes in a knowledge graph based on events mentioned in the text (Konovalal et et, 2017) may be sensitive to modelling methods such as adaptation to historical parameters."}, {"heading": "3 Measuring the Veridicality of Users\u2019 Predictions", "text": "The first step in our approach is to extract statements that make unambiguous predictions about unknown outcomes of future events. We specifically focus on contests that we define as events scheduled on a specific date when a number of candidates will compete against each other and a single winner will be selected. Example: Table 2 shows the 2016 Oscar contenders, highlighting the winner. To explore the accuracy of users \"predictions on social media, we gathered a corpus of tweets mentioning events that belong to one of the 10 types listed in Table 4. Relevant messages were collected by formulating queries to the Twitter search interface that include a candidate's name for a particular contest in connection with the keyword win. We limited the time span of queries to retrieve only messages written before the date of the contest to ensure that the results were unknown when the tweets were written."}, {"heading": "3.1 Mechanical Turk Annotation", "text": "For each tweet, we asked the Turks to judge the verifiability of a candidate who wins, as expressed in the tweet, as well as the author's desire for the event. For verifiability, we asked the Turks to assess whether the author believes that the event will occur on a scale of 1-5 (\"Definitely yes,\" \"Probably yes,\" \"Uncertain of the outcome,\" \"Probably no,\" 12A threshold of 0.7 was actually used. \"Definitely no\") We also added a question about the author's desire for the event to highlight the difference between verifiability and desire. For example, \"I really want Leonardo to win at the Oscars!,\" the author claims that Leonardo wins, but remains agnostic about the likelihood of this outcome, while \"Leonardo DiCaprio wins the verification results of the Oscars\" confidently predicts that the event.Figure 1 shows the verifiability interface that will be presented to the Turks."}, {"heading": "3.2 Veridicality Classifier", "text": "The goal of our system, TwiVer, is to automate the comment process by predicting how veridicala tweet affects a candidate who wins a contest: is the candidate the winner, or is the author uncertain? For the purpose of our experiments, we have divided the five terms for verifiability into three categories: positive verifiability (\"Definitely yes\" and \"Probably yes\"), neutral (\"Uncertain about the outcome\") and negative verifiability (\"Definitely no\" and \"Probably no\"). We model the conditional distribution over the verifiability of a tweet to a candidate c who wins a contest against a number of opponents, O, using a log-linear model: P (y = v | c, tweet)."}, {"heading": "3.3 Features", "text": "We use five feature templates: context words, distance between entities, presence of punctuation, dependency paths, and negated keyword. Target and opponent contexts. For each TARGET (t) and OPPONENT (o-O) entity in the tweet, we extract context words in a window of one to four words to the left and right of the TARGET (\"target context\") and OPPONENT (\"counter context\"), e.g. t will win, I go with t, owill win. Keyword context. For target and opponent entities, we also extract words between the entity and our specified keyword (k) (win in our case): t is predicted, o could k. Pair context. For the choice type of events where two target units are present (candidate and status)."}, {"heading": "3.4 Evaluation", "text": "We rated Twiverse's accuracy and recall based on our endured test set of 709 tweets. Figure 4 shows the precision / recall curve for positive verifiability. By setting a threshold for the probability value above 0.64, we achieve an accuracy of 80.1% and a recall of 44.3% in identifying tweets that express a positive verifiability toward a candidate who wins a contest."}, {"heading": "3.5 Performance on held-out event types", "text": "To assess the robustness of the verification classifier when applied to new types of events, we compared its performance when trained at all events with the performance of a category for testing. Table 9 shows the comparison: The second and third columns give an F1 value for training at all events compared to removing tweets in relation to the category in which we are testing. In most cases, we experience a relatively modest drop in performance after providing training data from the target event category, with the exception of elections. This suggests that our approach can be applied to new event types without requiring in-area training data for the verification classifier."}, {"heading": "3.6 Error Analysis", "text": "Table 7 shows some examples of TwiVer misclassifying. These errors indicate that while shallow features and dependency paths do a decent job of predicting verifiability, in some cases a deeper understanding of the text is required. \"The contrast between\" the heart... the mind \"in the first example is not trivial to grasp. Also, consideration of matrix clauses may be important (as shown in the last tweet\" There is no doubt. \")."}, {"heading": "4 Forecasting Contest Outcomes", "text": "We now have access to a classifier that can automatically detect positive verifiability predictions about a candidate winning a contest, allowing us to assess the accuracy of the audience's wisdom by retrospectively comparing popular beliefs (as extracted and summarized by TwiVer) with the known results of the contests. We will do this for each award category (Best Actor, Best Actress, Best Film and Best Director) at the Oscars from 2009 to 2016, for each state for both Republican and Democratic parties in the 2016 US primaries, for the candidates in each state for the last US presidential election in 2016, for each country in the final of the Eurovision Song Contest, for each Ballon d'Or candidate, for each party in each state in the 2014 Indian general election, and for the candidates in the finals at all sports events."}, {"heading": "4.1 Prediction", "text": "A simple voting mechanism is used to predict the results of the contest: We collect tweets about each candidate written before the date of the event, 13 and use TwiVer to measure the verifiability of users \"predictions of events. We then count for each candidate the number of tweets that are marked positive with a confidence above 0.64, as well as the number of tweets with positive verifiability for all other candidates. Table 11 illustrates these numbers for a contest, the Oscar Best Actress 2014. We then calculate a simple prediction score as follows: score = (| Tc | + 1) / (| Tc | + | TO | + 2) (1) 13These are tweets other than the TwiVer tweets that have been waiting for. | Tc | are the tweets that mention positive verifiability predictions for candidate c, and | TO | is the totality of all tweets predicting an opponent. For each contest, we simply predict the highest score as the candidate."}, {"heading": "4.2 Sentiment Baseline", "text": "We compare the performance of our approach to a current sentiment baseline (Mohammad et al., 2013). Prior to working on social media analysis, for example, O'Connor et al. (2010) used sentiment to make predictions about the results of the real world, and Tumasjan et al. (2010) used political sentiments to make predictions about the results of the German elections. We use a re-implementation of System14 (Mohammad et al., 2013) to estimate the mood for tweets in our corpus. We perform the tweets received for each candidate via the sentiment analysis system to obtain a number of positive labels. Sentiment scores are calculated analogously to Equation (1). For each contest, the candidate with the highest sentiment prediction14https: / / / github.com / ntietz / tweetmentmentment.com is predicted as the winner."}, {"heading": "4.3 Frequency Baseline", "text": "We also compare our approach to a simple frequency basis (Tweet Volume), calculating the number of tweets received for each candidate, and calculating frequency values in the same way as for verifiability and mood using Equation (1). For each contest, the candidate with the highest frequency rating will be crowned the winner."}, {"heading": "4.4 Results", "text": "Table 8 indicates the accuracy, recall, and maximum formula 1 values for verifiability, mood, and volume-based prediction of all competitions. The verifiability-based approach outperforms sentiment and volume-based predictions in 9 of the 10 events considered. In the tennis grand slam, the three approaches perform poorly. The performance difference in the verification approach is significantly smaller in tennis tournaments than in the other events. However, it is well known that the winners of tennis tournaments are very difficult to predict. Players \"performance in the final minutes of the match is critical, and even professionals have difficulty predicting tennis winners. Table 10 shows the 10 best predictions made by verifiability and sentiment-based systems on two of the events we are looking at - the Oscars and presidential primaries, highlighting correct predictions."}, {"heading": "4.5 Surprise Outcomes", "text": "In addition to providing a general method of predicting contest outcomes, our verifiability-based approach allows us to perform several novel analyses, including retrospectively identifying surprise outcomes that were generally unexpected. In Table 10, we see that the verifiability-based approach incorrectly predicts The Revenant as the Best Movie of 2016, which makes sense because the film was widely expected to win at the time, according to conventional wisdom. Numerous press sources, 15,16,17, qualify The Revenant, which did not win an Oscar, as a big surprise.Likewise, the two false predictions made by the verifiability-based approach were surprise losses. In newsarticles 18,19,20, the loss of Maine to Trump and the loss of Indiana to Clinton were actually reported as unexpected."}, {"heading": "4.6 Assessing the Reliability of Accounts", "text": "000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000"}, {"heading": "5 Conclusions", "text": "We have shown that verifiable statements on Twitter are a powerful predictor of winners of different types of events, and that our verifiability-based approach is better than the sensation and frequency base for predicting winners. In addition, our approach is able to identify surprise outcomes retrospectively, and we have shown how our approach enables an intuitive but novel method of assessing the reliability of information sources."}, {"heading": "Acknowledgments", "text": "We thank our anonymous critics for their valuable feedback. We also thank Wei Xu, Brendan O'Connor, and the Clippers group at Ohio State University for useful input. This material is based on work supported by the National Science Foundation under grant number IIS1464128 to Alan Ritter and IIS-1464252 to MarieCatherine de Marneffe. Alan Ritter is supported by the Department of Defense under contract number FA8702-15-D-0002 with Carnegie Mellon University to operate the Software Engineering Institute, a federally funded research and development center in addition to the Office of the Director of National Intelligence (ODNI) and the Intelligence Advanced Research Projects Activity (IARPA) through the Air Force Research Laboratory (AFRL) under contract number FA8750-16-C0114."}], "references": [{"title": "Success with style: Using writing style to predict the success of novels", "author": ["Vikas Ganjigunte Ashok", "Song Feng", "Yejin Choi."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Ashok et al\\.,? 2013", "shortCiteRegEx": "Ashok et al\\.", "year": 2013}, {"title": "Predicting the present with Google Trends", "author": ["Hyunyoung Choi", "Hal Varian."], "venue": "Economic Record, 88(1):2\u20139.", "citeRegEx": "Choi and Varian.,? 2012", "shortCiteRegEx": "Choi and Varian.", "year": 2012}, {"title": "From ADHD to SAD: Analyzing the language of mental health on Twitter through self-reported diagnoses", "author": ["Glen Coppersmith", "Mark Dredze", "Craig Harman", "Kristy Hollingshead."], "venue": "Proceedings of the Workshop on Computational Linguistics", "citeRegEx": "Coppersmith et al\\.,? 2015", "shortCiteRegEx": "Coppersmith et al\\.", "year": 2015}, {"title": "Measuring post traumatic stress disorder in Twitter", "author": ["Glen Coppersmith", "Craig Harman", "Mark Dredze."], "venue": "International Conference on Weblogs and Social Media.", "citeRegEx": "Coppersmith et al\\.,? 2014", "shortCiteRegEx": "Coppersmith et al\\.", "year": 2014}, {"title": "Google flu trends failure shows good data > big data", "author": ["Kaiser Fung."], "venue": "Harvard Business Review/HBR Blog Network[Online].", "citeRegEx": "Fung.,? 2014", "shortCiteRegEx": "Fung.", "year": 2014}, {"title": "Identifying sarcasm in Twitter: a closer look", "author": ["Roberto Gonz\u00e1lez-Ib\u00e1nez", "Smaranda Muresan", "Nina Wacholder."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short", "citeRegEx": "Gonz\u00e1lez.Ib\u00e1nez et al\\.,? 2011", "shortCiteRegEx": "Gonz\u00e1lez.Ib\u00e1nez et al\\.", "year": 2011}, {"title": "Learning whom to trust with MACE", "author": ["Dirk Hovy", "Taylor Berg-Kirkpatrick", "Ashish Vaswani", "Eduard Hovy."], "venue": "Proceedings of NAACL-HLT.", "citeRegEx": "Hovy et al\\.,? 2013", "shortCiteRegEx": "Hovy et al\\.", "year": 2013}, {"title": "A dependency parser for tweets", "author": ["Lingpeng Kong", "Nathan Schneider", "Swabha Swayamdipta", "Archna Bhatia", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Kong et al\\.,? 2014", "shortCiteRegEx": "Kong et al\\.", "year": 2014}, {"title": "Learning to extract events from knowledge base revisions", "author": ["Alexander Konovalov", "Benjamin Strauss", "Alan Ritter", "Brendan O\u2019Connor"], "venue": "In Proceedings of WWW", "citeRegEx": "Konovalov et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Konovalov et al\\.", "year": 2017}, {"title": "langid.py: An off-the-shelf language identification tool", "author": ["Marco Lui", "Timothy Baldwin"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Lui and Baldwin.,? \\Q2012\\E", "shortCiteRegEx": "Lui and Baldwin.", "year": 2012}, {"title": "Did it happen? The pragmatic complexity of veridicality assessment", "author": ["Marie-Catherine de Marneffe", "Christopher D Manning", "Christopher Potts."], "venue": "Computational linguistics, 38(2):301\u2013333.", "citeRegEx": "Marneffe et al\\.,? 2012", "shortCiteRegEx": "Marneffe et al\\.", "year": 2012}, {"title": "Predicting movie sales from blogger sentiment", "author": ["Gilad Mishne", "Natalie S Glance."], "venue": "AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs.", "citeRegEx": "Mishne and Glance.,? 2006", "shortCiteRegEx": "Mishne and Glance.", "year": 2006}, {"title": "NRC-Canada: Building the stateof-the-art in sentiment analysis of tweets", "author": ["Saif M. Mohammad", "Svetlana Kiritchenko", "Xiaodan Zhu."], "venue": "Proceedings of the seventh international workshop on Semantic Evaluation Exercises.", "citeRegEx": "Mohammad et al\\.,? 2013", "shortCiteRegEx": "Mohammad et al\\.", "year": 2013}, {"title": "Linguistic harbingers of betrayal: A case study on an online strategy game", "author": ["Vlad Niculae", "Srijan Kumar", "Jordan Boyd-Graber", "Cristian Danescu-Niculescu-Mizil."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Niculae et al\\.,? 2015", "shortCiteRegEx": "Niculae et al\\.", "year": 2015}, {"title": "From tweets to polls: Linking text sentiment to public opinion time series", "author": ["Brendan O\u2019Connor", "Ramnath Balasubramanyan", "Bryan R Routledge", "Noah A Smith"], "venue": "In Proceedings of the Fourth International AAAI Conference on Weblogs", "citeRegEx": "O.Connor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "O.Connor et al\\.", "year": 2010}, {"title": "Twitter improves influenza forecasting", "author": ["Michael J Paul", "Mark Dredze", "David Broniatowski."], "venue": "PLOS Currents Outbreaks, 6.", "citeRegEx": "Paul et al\\.,? 2014", "shortCiteRegEx": "Paul et al\\.", "year": 2014}, {"title": "Named entity recognition in tweets: An experimental study", "author": ["Alan Ritter", "Sam Clark", "Mausam", "Oren Etzioni."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Predicting US primary elections with Twitter", "author": ["Lei Shi", "Neeraj Agarwal", "Ankur Agarwal", "Rahul Garg", "Jacob Spoelstra."], "venue": "Social Network and Social Media Analysis: Methods, Models and Applications, NIPS.", "citeRegEx": "Shi et al\\.,? 2012", "shortCiteRegEx": "Shi et al\\.", "year": 2012}, {"title": "Predicting the NFL using Twitter", "author": ["Shiladitya Sinha", "Chris Dyer", "Kevin Gimpel", "Noah A. Smith."], "venue": "Proceedings of ECML/PKDD Workshop on Machine Learning and Data Mining for Sports Analytics.", "citeRegEx": "Sinha et al\\.,? 2013", "shortCiteRegEx": "Sinha et al\\.", "year": 2013}, {"title": "Using frame semantics for knowledge extraction from Twitter", "author": ["Anders S\u00f8gaard", "Barbara Plank", "Hector Martinez Alonso."], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence.", "citeRegEx": "S\u00f8gaard et al\\.,? 2015", "shortCiteRegEx": "S\u00f8gaard et al\\.", "year": 2015}, {"title": "Modeling factuality judgments in social media text", "author": ["Sandeep Soni", "Tanushree Mitra", "Eric Gilbert", "Jacob Eisenstein."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Soni et al\\.,? 2014", "shortCiteRegEx": "Soni et al\\.", "year": 2014}, {"title": "The wisdom of crowds", "author": ["James Surowiecki."], "venue": "Anchor Books, New York, NY.", "citeRegEx": "Surowiecki.,? 2005", "shortCiteRegEx": "Surowiecki.", "year": 2005}, {"title": "Predicting elections with Twitter: What 140 characters reveal about political sentiment", "author": ["Andranik Tumasjan", "Timm O. Sprenger", "Philipp G. Sandner", "Isabell M. Welpe."], "venue": "Proceedings of the Fourth International AAAI Conference on Weblogs", "citeRegEx": "Tumasjan et al\\.,? 2010", "shortCiteRegEx": "Tumasjan et al\\.", "year": 2010}, {"title": "Predicting a scientific community\u2019s response to an article", "author": ["Dani Yogatama", "Michael Heilman", "Brendan O\u2019Connor", "Chris Dyer", "Bryan R Routledge", "Noah A Smith"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Yogatama et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yogatama et al\\.", "year": 2011}, {"title": "Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization", "author": ["Ciyou Zhu", "Richard H Byrd", "Peihuang Lu", "Jorge Nocedal."], "venue": "ACM Transactions on Mathematical Software (TOMS).", "citeRegEx": "Zhu et al\\.,? 1997", "shortCiteRegEx": "Zhu et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 18, "context": "Prior work has made predictions about contests such as NFL games (Sinha et al., 2013) and elections using tweet volumes (Tumasjan et al.", "startOffset": 65, "endOffset": 85}, {"referenceID": 22, "context": ", 2013) and elections using tweet volumes (Tumasjan et al., 2010) or sentiment analysis (O\u2019Connor et al.", "startOffset": 42, "endOffset": 65}, {"referenceID": 14, "context": ", 2010) or sentiment analysis (O\u2019Connor et al., 2010; Shi et al., 2012).", "startOffset": 30, "endOffset": 71}, {"referenceID": 17, "context": ", 2010) or sentiment analysis (O\u2019Connor et al., 2010; Shi et al., 2012).", "startOffset": 30, "endOffset": 71}, {"referenceID": 21, "context": "In this paper we explore whether the \u201cwisdom of crowds\u201d (Surowiecki, 2005), as measured by users\u2019 explicit predictions, can predict outcomes of future events.", "startOffset": 56, "endOffset": 74}, {"referenceID": 23, "context": ", 2010), predicting citation counts of scientific articles (Yogatama et al., 2011) and success of literary works (Ashok et al.", "startOffset": 59, "endOffset": 82}, {"referenceID": 0, "context": ", 2011) and success of literary works (Ashok et al., 2013), forecasting economic indicators using query logs (Choi and Varian, 2012), improving influenza forecasts using Twitter data (Paul et al.", "startOffset": 38, "endOffset": 58}, {"referenceID": 1, "context": ", 2013), forecasting economic indicators using query logs (Choi and Varian, 2012), improving influenza forecasts using Twitter data (Paul et al.", "startOffset": 58, "endOffset": 81}, {"referenceID": 15, "context": ", 2013), forecasting economic indicators using query logs (Choi and Varian, 2012), improving influenza forecasts using Twitter data (Paul et al., 2014), predicting betrayal in online strategy games (Niculae et al.", "startOffset": 132, "endOffset": 151}, {"referenceID": 13, "context": ", 2014), predicting betrayal in online strategy games (Niculae et al., 2015) and predicting changes to a knowledge-graph based on events mentioned in text (Konovalov et al.", "startOffset": 54, "endOffset": 76}, {"referenceID": 8, "context": ", 2015) and predicting changes to a knowledge-graph based on events mentioned in text (Konovalov et al., 2017).", "startOffset": 86, "endOffset": 110}, {"referenceID": 4, "context": "These methods typically require historical data for fitting model parameters, and may be sensitive to issues such as concept drift (Fung, 2014).", "startOffset": 131, "endOffset": 143}, {"referenceID": 14, "context": "Prior work has also demonstrated that user sentiment online directly correlates with various real-world time series, including polling data (O\u2019Connor et al., 2010) and movie revenues (Mishne and Glance, 2006).", "startOffset": 140, "endOffset": 163}, {"referenceID": 11, "context": ", 2010) and movie revenues (Mishne and Glance, 2006).", "startOffset": 27, "endOffset": 52}, {"referenceID": 19, "context": "Also related is prior work on detecting veridicality (de Marneffe et al., 2012; S\u00f8gaard et al., 2015) and sarcasm (Gonz\u00e1lez-Ib\u00e1nez et al.", "startOffset": 53, "endOffset": 101}, {"referenceID": 5, "context": ", 2015) and sarcasm (Gonz\u00e1lez-Ib\u00e1nez et al., 2011).", "startOffset": 20, "endOffset": 50}, {"referenceID": 3, "context": ", 2015) and sarcasm (Gonz\u00e1lez-Ib\u00e1nez et al., 2011). Soni et al. (2014) investigate how journalists frame quoted content on Twitter using predicates such as think, claim or admit.", "startOffset": 21, "endOffset": 71}, {"referenceID": 9, "context": "py (Lui and Baldwin, 2012).", "startOffset": 3, "endOffset": 26}, {"referenceID": 6, "context": "We used MACE (Hovy et al., 2013) to resolve differences between annotators and produce a single gold label for each tweet.", "startOffset": 13, "endOffset": 32}, {"referenceID": 16, "context": "To extract features f(c,O, tweet), we first preprocessed tweets retrieved for a specific event to identify named entities, using (Ritter et al., 2011)\u2019s Twitter NER system.", "startOffset": 129, "endOffset": 150}, {"referenceID": 7, "context": "We retrieve dependency paths between the two TARGET entities and between the TARGET and keyword (win) using the TweeboParser (Kong et al., 2014) after applying rules to normalize paths in the tree (e.", "startOffset": 125, "endOffset": 144}, {"referenceID": 24, "context": "MAP parameters were fit using LBFGS-B (Zhu et al., 1997).", "startOffset": 38, "endOffset": 56}, {"referenceID": 12, "context": "We compare the performance of our approach against a state-of-the-art sentiment baseline (Mohammad et al., 2013).", "startOffset": 89, "endOffset": 112}, {"referenceID": 12, "context": "We use a re-implementation of (Mohammad et al., 2013)\u2019s system14 to estimate sentiment for tweets in our corpus.", "startOffset": 30, "endOffset": 53}, {"referenceID": 12, "context": "We compare the performance of our approach against a state-of-the-art sentiment baseline (Mohammad et al., 2013). Prior work on social media analysis used sentiment to make predictions about real-world outcomes. For instance, O\u2019Connor et al. (2010) correlated sentiment with public opinion polls and Tumasjan et al.", "startOffset": 90, "endOffset": 249}, {"referenceID": 12, "context": "We compare the performance of our approach against a state-of-the-art sentiment baseline (Mohammad et al., 2013). Prior work on social media analysis used sentiment to make predictions about real-world outcomes. For instance, O\u2019Connor et al. (2010) correlated sentiment with public opinion polls and Tumasjan et al. (2010) use political sentiment to make predictions about outcomes in German elections.", "startOffset": 90, "endOffset": 323}], "year": 2017, "abstractText": "Social media users often make explicit predictions about upcoming events. Such statements vary in the degree of certainty the author expresses toward the outcome: \u201cLeonardo DiCaprio will win Best Actor\u201d vs. \u201cLeonardo DiCaprio may win\u201d or \u201cNo way Leonardo wins!\u201d. Can popular beliefs on social media predict who will win? To answer this question, we build a corpus of tweets annotated for veridicality on which we train a log-linear classifier that detects positive veridicality with high precision.1 We then forecast uncertain outcomes using the wisdom of crowds, by aggregating users\u2019 explicit predictions. Our method for forecasting winners is fully automated, relying only on a set of contenders as input. It requires no training data of past outcomes and outperforms sentiment and tweet volume baselines on a broad range of contest prediction tasks. We further demonstrate how our approach can be used to measure the reliability of individual accounts\u2019 predictions and retrospectively identify surprise outcomes.", "creator": "TeX"}}}