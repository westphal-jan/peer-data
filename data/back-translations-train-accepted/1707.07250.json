{"id": "1707.07250", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jul-2017", "title": "Tensor Fusion Network for Multimodal Sentiment Analysis", "abstract": "Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language. In this paper, we pose the problem of multimodal sentiment analysis as modeling intra-modality and inter-modality dynamics. We introduce a novel model, termed Tensor Fusion Network, which learns both such dynamics end-to-end. The proposed approach is tailored for the volatile nature of spoken language in online videos as well as accompanying gestures and voice. In the experiments, our model outperforms state-of-the-art approaches for both multimodal and unimodal sentiment analysis.", "histories": [["v1", "Sun, 23 Jul 2017 05:54:20 GMT  (3958kb,D)", "http://arxiv.org/abs/1707.07250v1", "Accepted as full paper in EMNLP 2017"]], "COMMENTS": "Accepted as full paper in EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["amir zadeh", "minghai chen", "soujanya poria", "erik cambria", "louis-philippe morency"], "accepted": true, "id": "1707.07250"}, "pdf": {"name": "1707.07250.pdf", "metadata": {"source": "CRF", "title": "Tensor Fusion Network for Multimodal Sentiment Analysis", "authors": ["Amir Zadeh", "Minghai Chen", "Soujanya Poria"], "emails": ["abagherz@cs.cmu.edu", "minghail@cs.cmu.edu", "sporia@ntu.edu.sg", "cambria@ntu.edu.sg", "morency@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Multimodal sentiment analysis (Morency et al., 2011; Zadeh et al., 2016b; Poria et al., 2015) is an increasingly popular area of affective computing research (Poria et al., 2017) that focuses on generalizing text-based sentiment analysis to opinionated videos, in which three communicative modalities are present: language (spoken words), visual (gestures), and acoustic (voice).This generalization is particularly important for a part of the NLP community engaged in opinion and sentiment analysis (Cambria et al, 2017), as there is a growing trend to share opinions in videos instead of text, especially in social media (Facebook, YouTube, etc.).The central challenge of multimodal sentiment analysis is to model intermodal dynamics: the intermodality of interactions between the means of equal action language, visual and acoustic behaviors that alter the perception of the feelings expressed."}, {"heading": "2 Related Work", "text": "Sentiment Analysis is a well-researched area of research in NLP (Pang et al., 2008). Various approaches have been proposed to model feelings from language, including methods that focus on idiosyncratic words (Hu and Liu, 2004; Taboada et al., 2011; Poria et al., 2014b; Cambria et al., 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al., 2013; Poria et al., 2014a; Agarwal et al., 2015; Tai et al., 2015), and distribution-specific representations of feelings (Iyyer et al., 2015). Multimodal Sentiment Analysis is an emerging field of research that integrates verbal and non-verbal behaviors al into the recognition of user feelings al al al."}, {"heading": "3 CMU-MOSI Dataset", "text": "Multimodal Opinion Sentiment Intensity (CMUMOSI) Dataset is a commented dataset of video0 100200300400500600Highly Negative Weakly Negatives Negatives Neutral Weakly Positives Highly Positive0% 10% 20% 30% 40% 50% 70% 80% 90% 100% 5 10 15 20 25 35Pe rcen days of S entim ent D egre esUtterance SizeHighly PositiveWeakly PositiveWeakly PositiveWeakly PositiveWeakly Positive Positive Positives Positive Positives Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive NeutralWeakly Negative NeutralWeakly Positive Positive Positive Positive Positive Positive Positive Positive Positive NeutralWeakly Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive NeutralWeakly NeutralWeakly Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive NeutralWeakly Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive Positive"}, {"heading": "4 Tensor Fusion Network", "text": "Our proposed TFN consists of three main components: 1) Modality Embedding Subnetworks take unimodal characteristics as input and output a rich modality embedding. 2) Tensor Fusion Layer explicitly models unimodal, bimodal and trimodal interactions using a triple Cartesian product of modality embedding. 3) Sentiment Inference Subnetwork is a network work based on the output of the Tensor Fusion Layer and performs sentiment inferences. Depending on the task in Section 3, the network output changes to include binary classification, 5-class classification or regression. Input into the TFN is an expression of opinion that includes three modalities of language, visual and acoustic. The following three subsections describe the TFN subnetworks and their inputs in detail."}, {"heading": "4.1 Modality Embedding Subnetworks", "text": "The first part shows the actual message and the rest is a speaker who finally grapples with the first part of dealing with this ephemeral nature of spoken language in order to build models that are able to operate in the presence of unreliable and idiosyncratic language by focusing on important parts of spoken language. The key factor in dealing with this ephemeral nature of spoken language is to build models that are able to use unreliable and idiosyncratic language by focusing on important parts of spoken language, is to learn a rich representation of spoken words in each word and use them as input to a fully connected deep network (Figure 3)."}, {"heading": "4.2 Tensor Fusion Layer", "text": "While previous work in multimodal research has used feature concatenation as an approach to multimodal fusion, we strive to form a fusion layer in TFN that unravels unimodal, bimodal, and trimodal dynamics by explicitly modelling each of them. We call this layer tensor fusion, which is defined as the following vector field, the triple cartesian product: {(zl, zv, za) zl, zv, za, za, za, za, za, za, za, za, the additional constant dimension with value 1 generates the unimodde and bimodde dynamics. Any neural coordinate (zl, zv, za, za) can be considered as a 3-D point in triple Cartesian space defined by the language, image, and acoustic embedding of dimensions [zl1] T, [zvvv1] T, and [za1] T."}, {"heading": "4.3 Sentiment Inference Subnetwork", "text": "After the tensor fusion layer, any expression can be represented as a multimodal tensor zm. We use a fully connected deep neural network called Sentiment Inference Subnetwork Us with the weights Ws conditioned by zm. The architecture of the network consists of two layers of 128 ReLU activation units connected to the decision layer. In our experiments, we defined the probability function of the Sentiment Inference Subnetwork as follows, where \u03c6 is the sentiment prediction: arg max \u03c6 p (\u03c6 | zm; Ws) = arg max \u03c6 Us (zm; Ws) In our experiments, we use three variations of the Us network. The first network is designed for the binary sentiment classification, with a single sigmoid output neuron using the binary cross-entropy loss. The second network is designed for the five-class sentiment classification and uses a soft-max probability function using neural network categories of cross-entropy loss."}, {"heading": "5 Experiments", "text": "Experiment 2: We study the importance of TFN subtensors and the effects of each modality (see Figure 4). We also compare the commonly used early fusion approaches. Experiment 3: We compare the performance of our three modality-specific networks (language, image, and acoustics) with the latest unimodal approaches. Section 5.4 describes our experimental methodology, which remains constant across all experiments. Section 6 will discuss our results in more detail with a qualitative analysis."}, {"heading": "5.1 E1: Multimodal Sentiment Analysis", "text": "In this section, we compare the performance of the TFN model with previously proposed multimodal sentiment analysis models. We compare the following baselines: C-MKL (Poria et al., 2015) Convolutional MKL-based model is a multimodal sentiment classification model that uses a CNN to extract textual characteristics and uses multiple kernel learning functions for sentiment analysis. It is current SOTA (State of the Art) on CMU-MOSI.SAL-CNN (Wang et al., 2016) Select-Additive Learning is a multimodal sentiment analysis model that attempts to prevent identity-related information from being learned in a deep neural network. The 5-fold cross-validation used in this model using code provided by the authors on github.SVM-MD (Zadeh et al., 2016b) of the SVM model is an SVM learning additive model that is trained on SVM features."}, {"heading": "5.2 E2: Tensor Fusion Evaluation", "text": "Table 4 shows the results of our ablation study. The first three lines show the performance of each modality when no intermodality dynamics are modeled. From this first experiment, we find that the language modality is the most predictable. In a second set of ablation experiments, we test our TFN approach when only the bimodal subtensors are used (TFNbimodal) or when only the trimodal subtensor is used (TFNbimodal). We find that bimodal subtensors are more meaningful when used without other subtensors. The most interesting comparison is between our complete TFN model and a variant (TFNnotrimodal) where the trimodal subtensor is removed (but all unimodal and bimodal subtensors are present)."}, {"heading": "5.3 E3: Modality Embedding Subnetworks Evaluation", "text": "In this experiment, we compare the performance of our Modality Embedding Networks with state-of-the-art approaches for speech-based, visual and acoustic mood analysis."}, {"heading": "5.3.1 Language Sentiment Analysis", "text": "We selected the following state-of-the-art approaches to incorporate diversity into their techniques, 2We also made other comparisons with variants of the early fusion model TFAlmost, in which we increased the number of parameters and neurons to replicate the numbers from our TFN model. In all cases, the performance was similar to TFAlmost (and lower than our TFN model), but due to space constraints, we were unable to integrate them into this paper. (Performances of the original pre-trained models are shown in parenthesis in Table 3), distributional representation of text (DAN) and convolutionary approaches (DynamicCNN). If possible, we retrain them based on the CMU-MOSI dataset (performances of the pre-tracted models are shown in parenthesis in Table 3), and compare them with our language only TFNlanguage.RNTN modalization models that are not based on neural (Soural, Soural and Recuralistic) methods."}, {"heading": "5.3.2 Visual Sentiment Analysis", "text": "We compare the performance of our models with visual information (TFNvisual) with the following known approaches in visual sentiment analysis and emotion recognition (retrained for sentiment analysis): 3DCNN (Byeon and Kwak, 2014) a network with 3D CNN is trained with the speaker's face. The speaker's face is extracted in 6 frames and reduced to 64 \u00d7 64 and used as input for the proposed network.CNN-LSTM (Ebrahimi Kahou et al., 2015) is a recurring model that turns over the face region at each time stamp and uses the output to an LSTM. Facial processing is similar to 3DCNN.LSTM-FA the two baselines above, information extracted by FACET is used as input to an LSTM with a memory dimension of 100 neurons. SAL-CNN-V, SVM-Uvisual-MCD-MV-KV-only used in the SAL-V models."}, {"heading": "5.3.3 Acoustic Sentiment Analysis", "text": "We compare the performance of our models using visual information (TFNacoustic) with the following known approaches in audio sentiment analysis and emotion detection (retrained for sentiment analysis): HL-RNN (Lee and Tashev, 2015) uses an LSTM for high-level audio features. We use the same functions that are extracted for Ua over time intervals of an average of 200 intervals. Adieu-Net (Trigeorgis et al., 2016) is an end-end approach to emotion detection in audio with direct PCM features. SER-LSTM (Lim et al., 2016) is a model that uses recursive neural networks above folding operations in the spectrogram of Audio.SAL-CNN-A, SVM-MD-A, CMKL-A, RF-A uses only acoustic modalities in multimodal baseline section 5.1."}, {"heading": "5.4 Methodology", "text": "All our experiments are performed independently of the loudspeaker identity, as no loudspeaker is shared between train and test kit to ensure the generalisability of the model with invisible speakers in the real world. Best hyperparameters are selected using a network search based on the performance of the model on a validation set (using the last 4 videos in tensile fold).The TFN model is trained with the Adam Optimizer (Kingma and Ba, 2014) at a learning rate of 5e4. Uv and Ua, US subnetworks are regulated by exposing on all hidden layers with p = 0.15 and L2 standard coefficient 0.01. Tensile, test and validation folds are exactly the same for all baselines."}, {"heading": "6 Qualitative Analysis", "text": "We analyze the impact of our proposed multimodal fusion approach by comparing it with the early fusion approach TFFast and the three unimodal models. Table 6 shows examples from the CMU-MOSI dataset. Each example is described with spoken words and acoustic and visual behaviors. Sentiment predictions and ground truth labels oscillate between strongly negative (-3) and strongly positive (+ 3). As a first general observation, we observe that the early fusion model TFFast has a strong preference for language modality and seems to neglect intermodality dynamics. We can detect this trend by comparing it to the linguistic unimodal model TFNlanguage. In comparison, our TFN approach seems to capture more complex interactions through bimodal and trimodal dynamics and thus perform better."}, {"heading": "7 Conclusion", "text": "We introduced a new end-to-end fusion method for mood analysis, which explicitly represents unimodal, bimodal, and trimodal interactions between behaviors. Our experiments with the publicly available CMU-MOSI dataset yielded state-of-the-art results compared to both multimodal approaches. In addition, our approach provides up-to-date results for linguistic, visual, and acoustic multimodal mood analysis on CMU-MOSI."}, {"heading": "Acknowledgments", "text": "This project was partially supported by the Oculus Research Fellowship. We would like to thank the reviewers for their valuable feedback."}], "references": [{"title": "Concept-level sentiment analysis with dependencybased semantic parsing: a novel approach", "author": ["Basant Agarwal", "Soujanya Poria", "Namita Mittal", "Alexander Gelbukh", "Amir Hussain."], "venue": "Cognitive Computation 7(4):487\u2013499.", "citeRegEx": "Agarwal et al\\.,? 2015", "shortCiteRegEx": "Agarwal et al\\.", "year": 2015}, {"title": "Glottal wave analysis with pitch synchronous iterative adaptive inverse filtering", "author": ["Paavo Alku."], "venue": "Speech communication 11(2-3):109\u2013118.", "citeRegEx": "Alku.,? 1992", "shortCiteRegEx": "Alku.", "year": 1992}, {"title": "Normalized amplitude quotient for parametrization of the glottal flow", "author": ["Paavo Alku", "Tom B\u00e4ckstr\u00f6m", "Erkki Vilkman."], "venue": "the Journal of the Acoustical Society of America 112(2):701\u2013710.", "citeRegEx": "Alku et al\\.,? 2002", "shortCiteRegEx": "Alku et al\\.", "year": 2002}, {"title": "Parabolic spectral parameter\u2014a new method for quantification of the glottal flow", "author": ["Paavo Alku", "Helmer Strik", "Erkki Vilkman."], "venue": "Speech Communication 22(1):67\u201379.", "citeRegEx": "Alku et al\\.,? 1997", "shortCiteRegEx": "Alku et al\\.", "year": 1997}, {"title": "Vqa: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh."], "venue": "Proceedings of the IEEE International Conference on Computer Vision. pages 2425\u20132433.", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Openface: an open source facial behavior analysis toolkit", "author": ["Tadas Baltru\u0161aitis", "Peter Robinson", "Louis-Philippe Morency."], "venue": "Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on. IEEE, pages 1\u201310.", "citeRegEx": "Baltru\u0161aitis et al\\.,? 2016", "shortCiteRegEx": "Baltru\u0161aitis et al\\.", "year": 2016}, {"title": "Facial expression recognition using 3d convolutional neural network", "author": ["Young-Hyen Byeon", "Keun-Chang Kwak."], "venue": "International Journal of Advanced Computer Science and Applications 5(12).", "citeRegEx": "Byeon and Kwak.,? 2014", "shortCiteRegEx": "Byeon and Kwak.", "year": 2014}, {"title": "A Practical Guide to Sentiment Analysis", "author": ["Erik Cambria", "Dipankar Das", "Sivaji Bandyopadhyay", "Antonio Feraco."], "venue": "Springer, Cham, Switzerland.", "citeRegEx": "Cambria et al\\.,? 2017", "shortCiteRegEx": "Cambria et al\\.", "year": 2017}, {"title": "SenticNet 4: A semantic resource for sentiment analysis based on conceptual primitives", "author": ["Erik Cambria", "Soujanya Poria", "Rajiv Bajpai", "Bj\u00f6rn Schuller."], "venue": "COLING. pages 2666\u20132677.", "citeRegEx": "Cambria et al\\.,? 2016", "shortCiteRegEx": "Cambria et al\\.", "year": 2016}, {"title": "Vocal quality factors: Analysis, synthesis, and perception", "author": ["Donald G Childers", "CK Lee."], "venue": "the Journal of the Acoustical Society of America 90(5):2394\u20132410.", "citeRegEx": "Childers and Lee.,? 1991", "shortCiteRegEx": "Childers and Lee.", "year": 1991}, {"title": "Covarep\u2014a collaborative voice analysis repository for speech technologies", "author": ["Gilles Degottex", "John Kane", "Thomas Drugman", "Tuomo Raitio", "Stefan Scherer."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference", "citeRegEx": "Degottex et al\\.,? 2014", "shortCiteRegEx": "Degottex et al\\.", "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeffrey Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell."], "venue": "Proceedings of the IEEE", "citeRegEx": "Donahue et al\\.,? 2015", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "Joint robust voicing detection and pitch estimation based on residual harmonics", "author": ["Thomas Drugman", "Abeer Alwan."], "venue": "Interspeech. pages 1973\u2013 1976.", "citeRegEx": "Drugman and Alwan.,? 2011", "shortCiteRegEx": "Drugman and Alwan.", "year": 2011}, {"title": "Detection of glottal closure instants from speech signals: A quantitative review", "author": ["Thomas Drugman", "Mark Thomas", "Jon Gudnason", "Patrick Naylor", "Thierry Dutoit."], "venue": "IEEE Transactions on Audio, Speech, and Language Processing 20(3):994\u20131006.", "citeRegEx": "Drugman et al\\.,? 2012", "shortCiteRegEx": "Drugman et al\\.", "year": 2012}, {"title": "Recurrent neural networks for emotion recognition in video", "author": ["Samira Ebrahimi Kahou", "Vincent Michalski", "Kishore Konda", "Roland Memisevic", "Christopher Pal."], "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction.", "citeRegEx": "Kahou et al\\.,? 2015", "shortCiteRegEx": "Kahou et al\\.", "year": 2015}, {"title": "An argument for basic emotions", "author": ["Paul Ekman."], "venue": "Cognition & emotion 6(3-4):169\u2013200.", "citeRegEx": "Ekman.,? 1992", "shortCiteRegEx": "Ekman.", "year": 1992}, {"title": "Facial signs of emotional experience", "author": ["Paul Ekman", "Wallace V Freisen", "Sonia Ancoli."], "venue": "Journal of personality and social psychology 39(6):1125\u2013 1134.", "citeRegEx": "Ekman et al\\.,? 1980", "shortCiteRegEx": "Ekman et al\\.", "year": 1980}, {"title": "Proposal and evaluation of models for the glottal source waveform", "author": ["Hiroya Fujisaki", "Mats Ljungqvist."], "venue": "Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP\u201986.. IEEE, volume 11, pages 1605\u20131608.", "citeRegEx": "Fujisaki and Ljungqvist.,? 1986", "shortCiteRegEx": "Fujisaki and Ljungqvist.", "year": 1986}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins."], "venue": "Neural computation 12(10):2451\u20132471.", "citeRegEx": "Gers et al\\.,? 2000", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Representation learning for speech emotion recognition", "author": ["Sayan Ghosh", "Eugene Laksana", "Louis-Philippe Morency", "Stefan Scherer."], "venue": "Interspeech 2016. pages 3603\u20133607. https://doi.org/10.21437/Interspeech.2016-692.", "citeRegEx": "Ghosh et al\\.,? 2016a", "shortCiteRegEx": "Ghosh et al\\.", "year": 2016}, {"title": "Representation learning for speech emotion recognition", "author": ["Sayan Ghosh", "Eugene Laksana", "Louis-Philippe Morency", "Stefan Scherer."], "venue": "Interspeech 2016 pages 3603\u20133607.", "citeRegEx": "Ghosh et al\\.,? 2016b", "shortCiteRegEx": "Ghosh et al\\.", "year": 2016}, {"title": "Multiple classifier systems for the classification of audio-visual emotional", "author": ["Michael Glodek", "Stephan Tschechne", "Georg Layher", "Martin Schels", "Tobias Brosch", "Stefan Scherer", "Markus K\u00e4chele", "Miriam Schmidt", "Heiko Neumann", "G\u00fcnther Palm"], "venue": null, "citeRegEx": "Glodek et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glodek et al\\.", "year": 2011}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Mining and summarizing customer reviews", "author": ["Minqing Hu", "Bing Liu."], "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, pages 168\u2013 177.", "citeRegEx": "Hu and Liu.,? 2004", "shortCiteRegEx": "Hu and Liu.", "year": 2004}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan L BoydGraber", "Hal Daum\u00e9 III."], "venue": "ACL (1). pages 1681\u20131691.", "citeRegEx": "Iyyer et al\\.,? 2015", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences pages 656\u2013666", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Wavelet maxima dispersion for breathy to tense voice discrimination", "author": ["John Kane", "Christer Gobl."], "venue": "IEEE Transactions on Audio, Speech, and Language Processing 21(6):1170\u20131179.", "citeRegEx": "Kane and Gobl.,? 2013", "shortCiteRegEx": "Kane and Gobl.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "High-level feature representation using recurrent neural network for speech emotion recognition", "author": ["Jinkyu Lee", "Ivan Tashev."], "venue": "INTERSPEECH. pages 1537\u20131540.", "citeRegEx": "Lee and Tashev.,? 2015", "shortCiteRegEx": "Lee and Tashev.", "year": 2015}, {"title": "Speech emotion recognition using convolutional and recurrent neural networks", "author": ["Wootaek Lim", "Daeyoung Jang", "Taejin Lee."], "venue": "Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2016 Asia-Pacific. IEEE,", "citeRegEx": "Lim et al\\.,? 2016", "shortCiteRegEx": "Lim et al\\.", "year": 2016}, {"title": "Towards multimodal sentiment analysis: Harvesting opinions from the web", "author": ["Louis-Philippe Morency", "Rada Mihalcea", "Payal Doshi."], "venue": "Proceedings of the 13th international conference on multimodal interfaces. ACM, pages 169\u2013176.", "citeRegEx": "Morency et al\\.,? 2011", "shortCiteRegEx": "Morency et al\\.", "year": 2011}, {"title": "Emoreact: a multimodal approach and dataset for recognizing emotional responses in children", "author": ["Behnaz Nojavanasghari", "Tadas Baltru\u0161aitis", "Charles E Hughes", "Louis-Philippe Morency."], "venue": "Proceedings of the 18th ACM International Conference on Multi-", "citeRegEx": "Nojavanasghari et al\\.,? 2016", "shortCiteRegEx": "Nojavanasghari et al\\.", "year": 2016}, {"title": "Opinion mining and sentiment analysis", "author": ["Bo Pang", "Lillian Lee"], "venue": "Foundations and Trends R", "citeRegEx": "Pang and Lee,? \\Q2008\\E", "shortCiteRegEx": "Pang and Lee", "year": 2008}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP. volume 14, pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Utterance-level multimodal sentiment analysis", "author": ["Ver\u00f3nica P\u00e9rez-Rosas", "Rada Mihalcea", "LouisPhilippe Morency."], "venue": "ACL (1). pages 973\u2013 982.", "citeRegEx": "P\u00e9rez.Rosas et al\\.,? 2013", "shortCiteRegEx": "P\u00e9rez.Rosas et al\\.", "year": 2013}, {"title": "Dependency-based semantic parsing for conceptlevel text analysis", "author": ["Soujanya Poria", "Basant Agarwal", "Alexander Gelbukh", "Amir Hussain", "Newton Howard."], "venue": "International Conference on Intelligent Text Processing and Computational Lin-", "citeRegEx": "Poria et al\\.,? 2014a", "shortCiteRegEx": "Poria et al\\.", "year": 2014}, {"title": "A review of affective computing: From unimodal analysis to multimodal fusion", "author": ["Soujanya Poria", "Erik Cambria", "Rajiv Bajpai", "Amir Hussain."], "venue": "Information Fusion 37:98\u2013125.", "citeRegEx": "Poria et al\\.,? 2017", "shortCiteRegEx": "Poria et al\\.", "year": 2017}, {"title": "Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis", "author": ["Soujanya Poria", "Erik Cambria", "Alexander F Gelbukh."], "venue": "EMNLP. pages 2539\u20132544.", "citeRegEx": "Poria et al\\.,? 2015", "shortCiteRegEx": "Poria et al\\.", "year": 2015}, {"title": "Sentic patterns: Dependency-based rules for concept-level sentiment analysis", "author": ["Soujanya Poria", "Erik Cambria", "Gregoire Winterstein", "Guang-Bin Huang."], "venue": "Knowledge-Based Systems 69:45\u201363.", "citeRegEx": "Poria et al\\.,? 2014b", "shortCiteRegEx": "Poria et al\\.", "year": 2014}, {"title": "Convolutional mkl based multimodal emotion recognition and sentiment analysis", "author": ["Soujanya Poria", "Iti Chaturvedi", "Erik Cambria", "Amir Hussain."], "venue": "Data Mining (ICDM), 2016 IEEE 16th International Conference on. IEEE, pages 439\u2013448.", "citeRegEx": "Poria et al\\.,? 2016", "shortCiteRegEx": "Poria et al\\.", "year": 2016}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the conference on", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "A shared task on multimodal machine translation and crosslingual image description", "author": ["Lucia Specia", "Stella Frank", "Khalil Sima\u2019an", "Desmond Elliott"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Specia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2016}, {"title": "Lexicon-based methods for sentiment analysis", "author": ["Maite Taboada", "Julian Brooke", "Milan Tofiloski", "Kimberly Voll", "Manfred Stede."], "venue": "Computational linguistics 37(2):267\u2013307.", "citeRegEx": "Taboada et al\\.,? 2011", "shortCiteRegEx": "Taboada et al\\.", "year": 2011}, {"title": "Improved semantic representations from tree-structured long short-term memory networks pages 1556\u20131566", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Vocal intensity in speakers and singers", "author": ["Ingo R Titze", "Johan Sundberg."], "venue": "the Journal of the Acoustical Society of America 91(5):2936\u20132946.", "citeRegEx": "Titze and Sundberg.,? 1992", "shortCiteRegEx": "Titze and Sundberg.", "year": 1992}, {"title": "Combating human trafficking with deep multimodal models", "author": ["Edmund Tong", "Amir Zadeh", "Louis-Philippe Morency."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Tong et al\\.,? 2017", "shortCiteRegEx": "Tong et al\\.", "year": 2017}, {"title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network", "author": ["George Trigeorgis", "Fabien Ringeval", "Raymond Brueckner", "Erik Marchi", "Mihalis A Nicolaou", "Bj\u00f6rn Schuller", "Stefanos Zafeiriou."], "venue": "Acous-", "citeRegEx": "Trigeorgis et al\\.,? 2016", "shortCiteRegEx": "Trigeorgis et al\\.", "year": 2016}, {"title": "Depression, mood, and emotion recognition workshop", "author": ["Michel Valstar", "Jonathan Gratch", "Bj\u00f6rn Schuller", "Fabien Ringeval", "Dennis Lalanne", "Mercedes Torres Torres", "Stefan Scherer", "Giota Stratou", "Roddy Cowie", "Maja Pantic"], "venue": "Avec", "citeRegEx": "Valstar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Valstar et al\\.", "year": 2016}, {"title": "Select-additive learning: Improving cross-individual generalization in multimodal sentiment analysis", "author": ["Haohan Wang", "Aaksha Meghawat", "Louis-Philippe Morency", "Eric P Xing."], "venue": "arXiv preprint arXiv:1609.05244 .", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Youtube movie reviews: Sentiment analysis in an audio-visual context", "author": ["Martin W\u00f6llmer", "Felix Weninger", "Tobias Knaup", "Bj\u00f6rn Schuller", "Congkai Sun", "Kenji Sagae", "LouisPhilippe Morency."], "venue": "IEEE Intelligent Systems 28(3):46\u201353.", "citeRegEx": "W\u00f6llmer et al\\.,? 2013", "shortCiteRegEx": "W\u00f6llmer et al\\.", "year": 2013}, {"title": "Extracting opinion expressions with semi-markov conditional random fields", "author": ["Bishan Yang", "Claire Cardie."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language", "citeRegEx": "Yang and Cardie.,? 2012", "shortCiteRegEx": "Yang and Cardie.", "year": 2012}, {"title": "Image captioning with semantic attention", "author": ["Quanzeng You", "Hailin Jin", "Zhaowen Wang", "Chen Fang", "Jiebo Luo."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 4651\u20134659.", "citeRegEx": "You et al\\.,? 2016", "shortCiteRegEx": "You et al\\.", "year": 2016}, {"title": "Micro-opinion sentiment intensity analysis and summarization in online videos", "author": ["Amir Zadeh."], "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction. ACM, pages 587\u2013 591.", "citeRegEx": "Zadeh.,? 2015", "shortCiteRegEx": "Zadeh.", "year": 2015}, {"title": "Convolutional experts constrained local model for facial landmark detection", "author": ["Amir Zadeh", "Tadas Baltru\u0161aitis", "Louis-Philippe Morency."], "venue": "Computer Vision and Pattern Recognition Workshop (CVPRW). IEEE.", "citeRegEx": "Zadeh et al\\.,? 2017", "shortCiteRegEx": "Zadeh et al\\.", "year": 2017}, {"title": "Mosi: Multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos", "author": ["Amir Zadeh", "Rowan Zellers", "Eli Pincus", "LouisPhilippe Morency."], "venue": "arXiv preprint arXiv:1606.06259 .", "citeRegEx": "Zadeh et al\\.,? 2016a", "shortCiteRegEx": "Zadeh et al\\.", "year": 2016}, {"title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages", "author": ["Amir Zadeh", "Rowan Zellers", "Eli Pincus", "LouisPhilippe Morency."], "venue": "IEEE Intelligent Systems 31(6):82\u201388.", "citeRegEx": "Zadeh et al\\.,? 2016b", "shortCiteRegEx": "Zadeh et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 30, "context": "Multimodal sentiment analysis (Morency et al., 2011; Zadeh et al., 2016b; Poria et al., 2015) is an increasingly popular area of affective computing research (Poria et al.", "startOffset": 30, "endOffset": 93}, {"referenceID": 55, "context": "Multimodal sentiment analysis (Morency et al., 2011; Zadeh et al., 2016b; Poria et al., 2015) is an increasingly popular area of affective computing research (Poria et al.", "startOffset": 30, "endOffset": 93}, {"referenceID": 37, "context": "Multimodal sentiment analysis (Morency et al., 2011; Zadeh et al., 2016b; Poria et al., 2015) is an increasingly popular area of affective computing research (Poria et al.", "startOffset": 30, "endOffset": 93}, {"referenceID": 36, "context": ", 2015) is an increasingly popular area of affective computing research (Poria et al., 2017) that focuses on generalizing text-based sentiment analysis to opinionated videos, where three communicative modalities are present: language (spoken words), visual (gestures), and acoustic (voice).", "startOffset": 72, "endOffset": 92}, {"referenceID": 7, "context": "This generalization is particularly vital to part of the NLP community dealing with opinion mining and sentiment analysis (Cambria et al., 2017) since there is a growing trend of sharing opinions in videos instead of text, specially in social media (Facebook, YouTube, etc.", "startOffset": 122, "endOffset": 144}, {"referenceID": 30, "context": "Early fusion consists in simply concatenating multimodal features mostly at input level (Morency et al., 2011; P\u00e9rez-Rosas et al., 2013; Poria et al., 2016).", "startOffset": 88, "endOffset": 156}, {"referenceID": 34, "context": "Early fusion consists in simply concatenating multimodal features mostly at input level (Morency et al., 2011; P\u00e9rez-Rosas et al., 2013; Poria et al., 2016).", "startOffset": 88, "endOffset": 156}, {"referenceID": 39, "context": "Early fusion consists in simply concatenating multimodal features mostly at input level (Morency et al., 2011; P\u00e9rez-Rosas et al., 2013; Poria et al., 2016).", "startOffset": 88, "endOffset": 156}, {"referenceID": 48, "context": "Late fusion, instead, consists in training unimodal classifiers independently and performing decision voting (Wang et al., 2016; Zadeh et al., 2016a).", "startOffset": 109, "endOffset": 149}, {"referenceID": 54, "context": "Late fusion, instead, consists in training unimodal classifiers independently and performing decision voting (Wang et al., 2016; Zadeh et al., 2016a).", "startOffset": 109, "endOffset": 149}, {"referenceID": 23, "context": "Various approaches have been proposed to model sentiment from language, including methods that focus on opinionated words (Hu and Liu, 2004; Taboada et al., 2011; Poria et al., 2014b; Cambria et al., 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al.", "startOffset": 122, "endOffset": 205}, {"referenceID": 42, "context": "Various approaches have been proposed to model sentiment from language, including methods that focus on opinionated words (Hu and Liu, 2004; Taboada et al., 2011; Poria et al., 2014b; Cambria et al., 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al.", "startOffset": 122, "endOffset": 205}, {"referenceID": 38, "context": "Various approaches have been proposed to model sentiment from language, including methods that focus on opinionated words (Hu and Liu, 2004; Taboada et al., 2011; Poria et al., 2014b; Cambria et al., 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al.", "startOffset": 122, "endOffset": 205}, {"referenceID": 8, "context": "Various approaches have been proposed to model sentiment from language, including methods that focus on opinionated words (Hu and Liu, 2004; Taboada et al., 2011; Poria et al., 2014b; Cambria et al., 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al.", "startOffset": 122, "endOffset": 205}, {"referenceID": 50, "context": ", 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al.", "startOffset": 37, "endOffset": 60}, {"referenceID": 40, "context": ", 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al., 2013; Poria et al., 2014a; Agarwal et al., 2015; Tai et al., 2015), and distributional representations for sentiment (Iyyer et al.", "startOffset": 119, "endOffset": 201}, {"referenceID": 35, "context": ", 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al., 2013; Poria et al., 2014a; Agarwal et al., 2015; Tai et al., 2015), and distributional representations for sentiment (Iyyer et al.", "startOffset": 119, "endOffset": 201}, {"referenceID": 0, "context": ", 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al., 2013; Poria et al., 2014a; Agarwal et al., 2015; Tai et al., 2015), and distributional representations for sentiment (Iyyer et al.", "startOffset": 119, "endOffset": 201}, {"referenceID": 43, "context": ", 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al., 2013; Poria et al., 2014a; Agarwal et al., 2015; Tai et al., 2015), and distributional representations for sentiment (Iyyer et al.", "startOffset": 119, "endOffset": 201}, {"referenceID": 24, "context": ", 2015), and distributional representations for sentiment (Iyyer et al., 2015).", "startOffset": 58, "endOffset": 78}, {"referenceID": 55, "context": "There exist several multimodal datasets that include sentiment annotations, including the newly-introduced CMU-MOSI dataset (Zadeh et al., 2016b), as well as other datasets including ICT-MMMO (W\u00f6llmer et al.", "startOffset": 124, "endOffset": 145}, {"referenceID": 49, "context": ", 2016b), as well as other datasets including ICT-MMMO (W\u00f6llmer et al., 2013), YouTube (Morency et al.", "startOffset": 55, "endOffset": 77}, {"referenceID": 30, "context": ", 2013), YouTube (Morency et al., 2011), and MOUD (P\u00e9rez-Rosas et al.", "startOffset": 17, "endOffset": 39}, {"referenceID": 34, "context": ", 2011), and MOUD (P\u00e9rez-Rosas et al., 2013), however CMUMOSI is the only English dataset with utterancelevel sentiment labels.", "startOffset": 18, "endOffset": 44}, {"referenceID": 37, "context": "The newest multimodal sentiment analysis approaches have used deep neural networks, including convolutional neural networks (CNNs) with multiple-kernel learning (Poria et al., 2015), SAL-CNN (Wang et al.", "startOffset": 161, "endOffset": 181}, {"referenceID": 48, "context": ", 2015), SAL-CNN (Wang et al., 2016) which learns generalizable features across speakers, and support vector machines (SVMs) with a multimodal dictionary (Zadeh, 2015).", "startOffset": 17, "endOffset": 36}, {"referenceID": 52, "context": ", 2016) which learns generalizable features across speakers, and support vector machines (SVMs) with a multimodal dictionary (Zadeh, 2015).", "startOffset": 125, "endOffset": 138}, {"referenceID": 36, "context": "Audio-Visual Emotion Recognition is closely tied to multimodal sentiment analysis (Poria et al., 2017).", "startOffset": 82, "endOffset": 102}, {"referenceID": 19, "context": "Both audio and visual features have been shown to be useful in the recognition of emotions (Ghosh et al., 2016a).", "startOffset": 91, "endOffset": 112}, {"referenceID": 21, "context": "Using facial expressions and audio cues jointly has been the focus of many recent studies (Glodek et al., 2011; Valstar et al., 2016; Nojavanasghari et al., 2016).", "startOffset": 90, "endOffset": 162}, {"referenceID": 47, "context": "Using facial expressions and audio cues jointly has been the focus of many recent studies (Glodek et al., 2011; Valstar et al., 2016; Nojavanasghari et al., 2016).", "startOffset": 90, "endOffset": 162}, {"referenceID": 31, "context": "Using facial expressions and audio cues jointly has been the focus of many recent studies (Glodek et al., 2011; Valstar et al., 2016; Nojavanasghari et al., 2016).", "startOffset": 90, "endOffset": 162}, {"referenceID": 51, "context": "Creative and novel applications of using multiple modalities have been among successful recent research directions in machine learning (You et al., 2016; Donahue et al., 2015; Antol et al., 2015; Specia et al., 2016; Tong et al., 2017).", "startOffset": 135, "endOffset": 235}, {"referenceID": 11, "context": "Creative and novel applications of using multiple modalities have been among successful recent research directions in machine learning (You et al., 2016; Donahue et al., 2015; Antol et al., 2015; Specia et al., 2016; Tong et al., 2017).", "startOffset": 135, "endOffset": 235}, {"referenceID": 4, "context": "Creative and novel applications of using multiple modalities have been among successful recent research directions in machine learning (You et al., 2016; Donahue et al., 2015; Antol et al., 2015; Specia et al., 2016; Tong et al., 2017).", "startOffset": 135, "endOffset": 235}, {"referenceID": 41, "context": "Creative and novel applications of using multiple modalities have been among successful recent research directions in machine learning (You et al., 2016; Donahue et al., 2015; Antol et al., 2015; Specia et al., 2016; Tong et al., 2017).", "startOffset": 135, "endOffset": 235}, {"referenceID": 45, "context": "Creative and novel applications of using multiple modalities have been among successful recent research directions in machine learning (You et al., 2016; Donahue et al., 2015; Antol et al., 2015; Specia et al., 2016; Tong et al., 2017).", "startOffset": 135, "endOffset": 235}, {"referenceID": 54, "context": "opinions from YouTube movie reviews (Zadeh et al., 2016a).", "startOffset": 36, "endOffset": 57}, {"referenceID": 40, "context": "Annotation of sentiment has closely followed the annotation scheme of the Stanford Sentiment Treebank (Socher et al., 2013), where sentiment is annotated on a seven-step Likert scale from very negative to very positive.", "startOffset": 102, "endOffset": 123}, {"referenceID": 33, "context": ", lTl ; lt \u2208 R300}, where Tl is the number of words in an utterance, be the set of spoken words represented as a sequence of 300-dimensional GloVe word vectors (Pennington et al., 2014).", "startOffset": 160, "endOffset": 185}, {"referenceID": 22, "context": "A LSTM network (Hochreiter and Schmidhuber, 1997) with a forget gate (Gers et al.", "startOffset": 15, "endOffset": 49}, {"referenceID": 18, "context": "A LSTM network (Hochreiter and Schmidhuber, 1997) with a forget gate (Gers et al., 2000) is used to learn time-dependent language representations", "startOffset": 69, "endOffset": 88}, {"referenceID": 15, "context": "The speaker\u2019s face is detected for each frame (sampled at 30Hz) and indicators of the seven basic emotions (anger, contempt, disgust, fear, joy, sadness, and surprise) and two advanced emotions (frustration and confusion) (Ekman, 1992) are extracted using FACET facial expression analysis framework1.", "startOffset": 222, "endOffset": 235}, {"referenceID": 16, "context": "A set of 20 Facial Action Units (Ekman et al., 1980), indicating detailed muscle movements on the face, are also extracted using FACET.", "startOffset": 32, "endOffset": 52}, {"referenceID": 5, "context": "Estimates of head position, head rotation, and 68 facial landmark locations also extracted per frame using OpenFace (Baltru\u0161aitis et al., 2016; Zadeh et al., 2017).", "startOffset": 116, "endOffset": 163}, {"referenceID": 53, "context": "Estimates of head position, head rotation, and 68 facial landmark locations also extracted per frame using OpenFace (Baltru\u0161aitis et al., 2016; Zadeh et al., 2017).", "startOffset": 116, "endOffset": 163}, {"referenceID": 10, "context": "Acoustic Embedding Subnetwork: For each opinion utterance audio, a set of acoustic features are extracted using COVAREP acoustic analysis framework (Degottex et al., 2014), including 12 MFCCs, pitch tracking and Voiced/UnVoiced segmenting features (using the additive noise robust Summation of Residual Harmonics (SRH) method (Drugman and Alwan, 2011)), glottal source parameters (estimated by glottal inverse filtering based on GCI synchronous IAIF (Drugman et al.", "startOffset": 148, "endOffset": 171}, {"referenceID": 12, "context": ", 2014), including 12 MFCCs, pitch tracking and Voiced/UnVoiced segmenting features (using the additive noise robust Summation of Residual Harmonics (SRH) method (Drugman and Alwan, 2011)), glottal source parameters (estimated by glottal inverse filtering based on GCI synchronous IAIF (Drugman et al.", "startOffset": 162, "endOffset": 187}, {"referenceID": 13, "context": ", 2014), including 12 MFCCs, pitch tracking and Voiced/UnVoiced segmenting features (using the additive noise robust Summation of Residual Harmonics (SRH) method (Drugman and Alwan, 2011)), glottal source parameters (estimated by glottal inverse filtering based on GCI synchronous IAIF (Drugman et al., 2012; Alku, 1992; Alku et al., 2002, 1997; Titze and Sundberg, 1992; Childers and Lee, 1991)), peak slope parameters (Degottex et al.", "startOffset": 286, "endOffset": 395}, {"referenceID": 1, "context": ", 2014), including 12 MFCCs, pitch tracking and Voiced/UnVoiced segmenting features (using the additive noise robust Summation of Residual Harmonics (SRH) method (Drugman and Alwan, 2011)), glottal source parameters (estimated by glottal inverse filtering based on GCI synchronous IAIF (Drugman et al., 2012; Alku, 1992; Alku et al., 2002, 1997; Titze and Sundberg, 1992; Childers and Lee, 1991)), peak slope parameters (Degottex et al.", "startOffset": 286, "endOffset": 395}, {"referenceID": 44, "context": ", 2014), including 12 MFCCs, pitch tracking and Voiced/UnVoiced segmenting features (using the additive noise robust Summation of Residual Harmonics (SRH) method (Drugman and Alwan, 2011)), glottal source parameters (estimated by glottal inverse filtering based on GCI synchronous IAIF (Drugman et al., 2012; Alku, 1992; Alku et al., 2002, 1997; Titze and Sundberg, 1992; Childers and Lee, 1991)), peak slope parameters (Degottex et al.", "startOffset": 286, "endOffset": 395}, {"referenceID": 9, "context": ", 2014), including 12 MFCCs, pitch tracking and Voiced/UnVoiced segmenting features (using the additive noise robust Summation of Residual Harmonics (SRH) method (Drugman and Alwan, 2011)), glottal source parameters (estimated by glottal inverse filtering based on GCI synchronous IAIF (Drugman et al., 2012; Alku, 1992; Alku et al., 2002, 1997; Titze and Sundberg, 1992; Childers and Lee, 1991)), peak slope parameters (Degottex et al.", "startOffset": 286, "endOffset": 395}, {"referenceID": 10, "context": ", 2002, 1997; Titze and Sundberg, 1992; Childers and Lee, 1991)), peak slope parameters (Degottex et al., 2014), maxima dispersion quotients (MDQ) (Kane and Gobl, 2013), and estimations of the Rd shape parameter of the Liljencrants-Fant (LF) glottal model (Fujisaki and Ljungqvist, 1986).", "startOffset": 88, "endOffset": 111}, {"referenceID": 26, "context": ", 2014), maxima dispersion quotients (MDQ) (Kane and Gobl, 2013), and estimations of the Rd shape parameter of the Liljencrants-Fant (LF) glottal model (Fujisaki and Ljungqvist, 1986).", "startOffset": 43, "endOffset": 64}, {"referenceID": 17, "context": ", 2014), maxima dispersion quotients (MDQ) (Kane and Gobl, 2013), and estimations of the Rd shape parameter of the Liljencrants-Fant (LF) glottal model (Fujisaki and Ljungqvist, 1986).", "startOffset": 152, "endOffset": 183}, {"referenceID": 20, "context": "These extracted features capture different characteristics of human voice and have been shown to be related to emotions (Ghosh et al., 2016b).", "startOffset": 120, "endOffset": 141}, {"referenceID": 37, "context": "C-MKL (Poria et al., 2015) Convolutional MKL-based model is a multimodal sentiment classification model which uses a CNN to extract textual features and uses multiple kernel learning for sentiment analysis.", "startOffset": 6, "endOffset": 26}, {"referenceID": 48, "context": "SAL-CNN (Wang et al., 2016) Select-Additive Learning is a multimodal sentiment analysis model that attempts to prevent identity-dependent information from being learned in a deep neural network.", "startOffset": 8, "endOffset": 27}, {"referenceID": 55, "context": "SVM-MD (Zadeh et al., 2016b) is a SVM model trained on multimodal features using early fusion.", "startOffset": 7, "endOffset": 28}, {"referenceID": 30, "context": "The model used in (Morency et al., 2011) and (P\u00e9rez-Rosas et al.", "startOffset": 18, "endOffset": 40}, {"referenceID": 34, "context": ", 2011) and (P\u00e9rez-Rosas et al., 2013) also similarly use SVM on multimodal concatenated features.", "startOffset": 12, "endOffset": 38}, {"referenceID": 40, "context": "RNTN (Socher et al., 2013)The Recursive Neural Tensor Network is among the most well-known sentiment analysis methods proposed for both binary and multi-class sentiment analysis that uses dependency structure.", "startOffset": 5, "endOffset": 26}, {"referenceID": 24, "context": "DAN (Iyyer et al., 2015) The Deep Average Network approach is a simple but efficient sentiment analysis model that uses information only from distributional representation of the words and not from the compositionality of the sentences.", "startOffset": 4, "endOffset": 24}, {"referenceID": 25, "context": "DynamicCNN (Kalchbrenner et al., 2014) DynamicCNN is among the state-of-the-art models in text-based sentiment analysis which uses a convolutional architecture adopted for the semantic modeling of sentences.", "startOffset": 11, "endOffset": 38}, {"referenceID": 6, "context": "3DCNN (Byeon and Kwak, 2014) a network using 3D CNN is trained using the face of the speaker.", "startOffset": 6, "endOffset": 28}, {"referenceID": 28, "context": "and emotion recognition (retrained for sentiment analysis): HL-RNN (Lee and Tashev, 2015) uses an LSTM on high-level audio features.", "startOffset": 67, "endOffset": 89}, {"referenceID": 46, "context": "Adieu-Net (Trigeorgis et al., 2016) is an endto-end approach for emotion recognition in audio using directly PCM features.", "startOffset": 10, "endOffset": 35}, {"referenceID": 29, "context": "SER-LSTM (Lim et al., 2016) is a model that uses recurrent neural networks on top of convolution operations on spectrogram of audio.", "startOffset": 9, "endOffset": 27}, {"referenceID": 54, "context": "All the models in this paper are tested using five-fold cross-validation proposed by CMUMOSI (Zadeh et al., 2016a).", "startOffset": 93, "endOffset": 114}, {"referenceID": 27, "context": "The TFN model is trained using the Adam optimizer (Kingma and Ba, 2014) with the learning rate 5e4.", "startOffset": 50, "endOffset": 71}], "year": 2017, "abstractText": "Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language. In this paper, we pose the problem of multimodal sentiment analysis as modeling intra-modality and inter-modality dynamics. We introduce a novel model, termed Tensor Fusion Network, which learns both such dynamics end-to-end. The proposed approach is tailored for the volatile nature of spoken language in online videos as well as accompanying gestures and voice. In the experiments, our model outperforms state-ofthe-art approaches for both multimodal and unimodal sentiment analysis.", "creator": "LaTeX with hyperref package"}}}