{"id": "1707.07273", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jul-2017", "title": "Hierarchical Embeddings for Hypernymy Detection and Directionality", "abstract": "We present a novel neural model HyperVec to learn hierarchical embeddings for hypernymy detection and directionality. While previous embeddings have shown limitations on prototypical hypernyms, HyperVec represents an unsupervised measure where embeddings are learned in a specific order and capture the hypernym$-$hyponym distributional hierarchy. Moreover, our model is able to generalize over unseen hypernymy pairs, when using only small sets of training data, and by mapping to other languages. Results on benchmark datasets show that HyperVec outperforms both state$-$of$-$the$-$art unsupervised measures and embedding models on hypernymy detection and directionality, and on predicting graded lexical entailment.", "histories": [["v1", "Sun, 23 Jul 2017 09:55:48 GMT  (50kb,D)", "http://arxiv.org/abs/1707.07273v1", "11 pages, accepted as long paper at EMNLP 2017"]], "COMMENTS": "11 pages, accepted as long paper at EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kim anh nguyen", "maximilian k\\\"oper", "sabine schulte im walde", "ngoc thang vu"], "accepted": true, "id": "1707.07273"}, "pdf": {"name": "1707.07273.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Embeddings for Hypernymy Detection and Directionality", "authors": ["Kim Anh Nguyen", "Maximilian K\u00f6per", "Sabine Schulte im Walde", "Ngoc Thang Vu"], "emails": ["nguyenkh@ims.uni-stuttgart.de", "koepermn@ims.uni-stuttgart.de", "schulte@ims.uni-stuttgart.de", "thangvu@ims.uni-stuttgart.de"], "sections": [{"heading": "1 Introduction", "text": "It is about the question to what extent it is about a way in which people who are able to understand the world and to understand how they have to understand themselves and how they have to understand how they should behave. (...) It is about the question of how they should behave. (...) It is about the question of how they should behave. (...) It is about the question of how they should behave. (...) It is about the question of how they should behave. (...) It is about how they should behave. (...) It is about how they should behave. (...) It is about how they should behave. (...) It is about how they should behave. (...) It is about how they should behave. (...) It is about how they should behave. (...) It is about how they should behave. (...) It is about how they should behave. (...) It is about how they should behave. (...) It is about how they should behave. (...)"}, {"heading": "2 Related Work", "text": "In fact, it is such that it is a semantically narrower term than v, but then a significant number of salient distributional properties of u is expected to be included in the feature vector of v as well. In addition, the vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-v"}, {"heading": "3 Hierarchical Embeddings", "text": "In this section, we present our HyperVec hierarchical embedding model. Section 3.1 describes how to learn Hypernymy embedding, and Section 3.2 introduces the HyperScore unattended measure applied to Hypernymy tasks."}, {"heading": "3.1 Learning Hierarchical Embeddings", "text": "Our approach uses a series of hypernyms that can be derived either from using the transitivity of the hypernymia relationship (Fallucchi and Zanzotto, 2011) or from lexical databases to learn hierarchical embeddings. We rely on WordNet, a large lexical database of English (Fellbaum, 1998), and extract all hypernym-hyponym pairs for nouns and verbs, including both direct and indirect hypernymia, e.g. animal-bird, bird-robin, and animal-robin. Before forming our model, we exclude all hypernym pairs that occur in all datasets for evaluation. In the following section 3.1.1 first describes the skip-gram model that is integrated into our model for optimization. Section 3.1.2 then describes the objective functions for forming hierarchical embeddings for hypernymia."}, {"heading": "3.1.1 Skip-gram Model", "text": "The Skip-gram model is a method for word embedding proposed by Mikolov et al. (2013b). Levy and Goldberg (2014) introduced a variant of the Skip-gram negative scan model (SGNS), in which the objective function is defined as follows: JSGNS = \u2211 w, VW, c, VC J (w, c) (1) J (w, c) = # (w, c) log \u03c3 (~ w, ~ c) + k \u00b7 EcN, PD [log \u03c3 (\u2212 w, ~ cN)] (2), in which the Skip-gram negative scan be trained on a corpus of words w, VW and their contexts c, c, VC, each with VW and VC, word and context vocabularies. Collection of observed words and context pairs is referred to as D; the term # (w, c) refers to the number of times in which the pair appears as a contextual function (the contextual), the contextual (the contextual) ipax, the ipax, the ipax."}, {"heading": "3.1.2 Hierarchical Hypernymy Model", "text": "In this way, they are insufficient to distinguish hypernymy from other paradigmatic relationships such as synonymy, but they still suffer from the distinction between hypernymy and meronymyms. Our novel approach presents two solutions to deal with these challenges. First, the embedding is learned in a specific order, so the similarity score for hypernymy is higher than the similarity score for other relationships. For example, the hypernymy pair animal-frog is assigned to a higher cosmic score than the co-hyponymy pair eagle-frog."}, {"heading": "3.2 Unsupervised Hypernymy Measure", "text": "HyperVec is expected to have the following two properties: (i) the hyponym and the hypernym are close together, and (ii) there is a distribution hierarchy between hypernymia and its hyponyms. In view of a hypernymia pair (u, v) in which u is the hyponym and v is the hypernym, we propose a measurement to detect hypernymia and determine the directional similarity of hypernymia based on the hierarchical embeddings as follows: HyperScore (u, v) = cos (~ u, ~ v)."}, {"heading": "4 Experiments", "text": "In this section, we first describe the experimental settings in our experiments (Section 4.1), then evaluate the performance of HyperVec in three different tasks: i) unattended hypernymia detection and directivity (Section 4.2), where we evaluate HyperVec in the ranking and classification of hypernymia; ii) supervised hypernymia detection (Section 4.3), where we apply a supervised classification to detect hypernymia; iii) graduated lexical disorders (Section 4.4), where we predict the severance of hypernymia pairs."}, {"heading": "4.1 Experimental Settings", "text": "We use the ENCOW14A corpus (Scheffer and Sculptor, 2012; Scheffer, 2015) with approximately 14.5 billion tokens to form the hierarchical embeddings and the standard SGNS model. We train our model with 100 dimensions, a window size of 5, 15 negative samples and 0.05 as the learning rate. The hypernymic resource for nouns consists of 105, 020 hyponyms, 24, 925 hypernyms and 1, 878, 484 hyponyms-hypernyms pairs. The hypernyme resource for verbs consists of 11, 328 hyponyms, 4, 848 hypernyms and 130, 350 hyponyms-hypernyms pairs."}, {"heading": "4.2 Unsupervised Hypernymy Detection and Directionality", "text": "In this section, we evaluate our model based on two experimental arrangements: i) a ranking-retrieval setup that expects hypernymy pairs to have a higher similarity value than instances from other semantic relationships; ii) a classification setup that requires both hypernymy recognition and directional effects."}, {"heading": "4.2.1 Ranking Retrieval", "text": "Shwartz et al. (2017) conducted a comprehensive evaluation of a large number of unattended distribution measures for hypernymy ranking retrieval proposed in previous work (Weeds et al., 2003; Santus et al., 2014; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2016) using four semantic relation datasets: BLESS (Baroni and Lenci, 2011), WEEDS (Weeds et al., 2004), EVALUTION (Santus et al., 2015) and LENCI & BENOTTO (Benotto, 2015). Table 1 describes the details of these datasets in terms of semantic relationships and number of instances. Average precision (AP) is used to evaluate the performance of the hyperdata table."}, {"heading": "4.2.2 Classification", "text": "In this setup, we are relying on three sets of semantic relationships, all previously used in different state-of-the-art approaches, which were brought together for the Hypernymy evaluation by Kiela et al. (2015). (i) A subset of BLESS contains 1337 hypernym pairs. The task is to predict the accuracy of Hypernymy within a binary classification. Our approach does not require a threshold; we only need to compare the magnitudes of the two words and assign the hypernym label to the word with the larger order of magnitude. Figure 1a indicates that the magnitude values of the SGNS model cannot distinguish between a hypernym and a hypernym, while the hierarchical embeddings provide a larger order of magnitude for the word with the larger order of magnitude. (ii) The following Weeds et al. (2014) We are conducting a binary classification with a subset of 1,168 BLESS pairs by means of a binary pair."}, {"heading": "4.3 Supervised Hypernymy Detection", "text": "For supervised hypernymy detection, we use the two sets of data: the full BLESS data set and the ENTAILMENT data pair (Baroni et al., 2012), which contains a total of 2,770 relationship pairs, including 1,385 hypernym pairs and 1,385 other relationship pairs. We follow the same procedure as Yu et al. (2015) and Tuan et al. (2016) to evaluate HyperVec on the two sets of data. With respect to BLESS, we extract pairs for four types of relationships: hypernymia, meronymia, co-hyponymy (or coordination), and add the random relationship for nouns. For evaluation, we randomly select a concept and its relation for testing, and train the supervised model on the 199 remaining concepts and its relative."}, {"heading": "4.4 Graded Lexical Entailment", "text": "In this experiment, we apply HyperVec to the dataset of graded lexical deprivation, HyperLex, as introduced by Vulic \u0301 et al. (2016). The HyperLex dataset provides soft lexical deprivation on a continuous scale, rather than simplifying it into a binary decision. HyperLex contains 2,616 pairs of words in seven semantic relationships and two word classes (nouns and verbs). Each word pair is evaluated by a score indicating the strength of the semantic relationship between the two words. For example, the score of the hypernym pair duck-animal is 5.9 out of 6.0, while the score of the reversed pair animal-duck is only 1.0.We compared HyperScore with the most prominent state-of-the-art hypernymy and lexical deprivation models (Siexical entailment models from 2003 \u2022 2004, Clarke's Wailment measures)."}, {"heading": "FR 0.279 SGNS 0.205", "text": "Table 5 shows that HyperScore significantly exceeds both state-of-the-art metrics and text embedding models. HyperScore even exceeds the previously best text embedding model PARAGRAM by 22 and the previously best metrics FR by 27. The reason why HyperVec outperforms all other models is that the hierarchy between Hypernym and Hypornym within HyperVec distinguishes Hyponym-Hypernym pairs from Hypernym-Hyponym pairs. Thus, the HyperScore for the duck-animal and / or animal-duck pairs is 3.02 and 0.30, respectively. Thus, the size portion of the Hypernym-Hyponym pair duck-animal is greater than that for the animal-duck pair."}, {"heading": "5 Generalizing Hypernymy", "text": "We have demonstrated the general capabilities of HyperVec to explore its potential for generalization in two different ways, (i) by relying on a small amount of seeds from a single seed rather than using a large number of training data; and (ii) by applying HyperVec to other languages, the motivation behind these concepts is threefold: i) these concepts are distinguishable and unmistakable; (ii) the concepts have been equally divided between living and non-living beings; iii) concepts have been grouped into 17 broader classes; based on seed collection, we have the hyponyms of each concept of WordNet, and then re-trained HyperVec."}, {"heading": "6 Conclusion", "text": "This work proposed a novel neural model HyperVec to learn hierarchical embedding for hypernymia. HyperVec has shown that it strengthens hypernymia similarity and captures the distribution hierarchy of hypernymia. Together with a newly proposed, unattended measure HyperScore, our experiments showed (i) significant improvements over modern measures and (ii) the ability to generalize hypernymia and learn the relationship instead of memorizing prototypical hypernyms."}, {"heading": "Acknowledgments", "text": "The research was supported by the Ministry of Education and Training of the Socialist Republic of Vietnam (Scholarship 977 / QD-BGDDT; KimAnh Nguyen), the DFG Collaborative Research Centre SFB 732 (Kim-Anh Nguyen, Maximilian Ko \u00b6 per, Ngoc Thang Vu) and the DFG Heisenberg Scholarship SCHU-2580 / 1 (Sabine Schulte im Walde)."}], "references": [{"title": "Entailment above the word level in distributional semantics", "author": ["Marco Baroni", "Raffaella Bernardi", "Ngoc-Quynh Do", "Chung-chieh Shan."], "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics", "citeRegEx": "Baroni et al\\.,? 2012", "shortCiteRegEx": "Baroni et al\\.", "year": 2012}, {"title": "How we blessed distributional semantic evaluation", "author": ["Marco Baroni", "Alessandro Lenci."], "venue": "Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics (GEMS), pages 1\u201310, Edinburgh, Scotland.", "citeRegEx": "Baroni and Lenci.,? 2011", "shortCiteRegEx": "Baroni and Lenci.", "year": 2011}, {"title": "Distributional models for semantic relations: A study on hyponymy and antonymy", "author": ["Giulia Benotto."], "venue": "Ph.D. thesis, University of Pisa.", "citeRegEx": "Benotto.,? 2015", "shortCiteRegEx": "Benotto.", "year": 2015}, {"title": "Classifying taxonomic relations between pairs of wikipedia articles", "author": ["Or Biran", "Kathleen McKeown."], "venue": "Proceddings of Sixth International Joint Conference on Natural Language Processing (IJCNLP), pages 788\u2013794, Nagoya, Japan.", "citeRegEx": "Biran and McKeown.,? 2013", "shortCiteRegEx": "Biran and McKeown.", "year": 2013}, {"title": "Context-theoretic semantics for natural language: An overview", "author": ["Daoud Clarke."], "venue": "Proceedings of the Workshop on Geometrical Models of Natural Language Semantics (GEMS), pages 112\u2013119, Athens, Greece.", "citeRegEx": "Clarke.,? 2009", "shortCiteRegEx": "Clarke.", "year": 2009}, {"title": "Recognizing Textual Entailment: Models and Applications", "author": ["Ido Dagan", "Dan Roth", "Mark Sammons", "Fabio Massimo Zanzotto."], "venue": "Synthesis Lectures on Human Language Technologies.", "citeRegEx": "Dagan et al\\.,? 2013", "shortCiteRegEx": "Dagan et al\\.", "year": 2013}, {"title": "A Simple, Fast, and Effective Reparameterization of IBM Model 2", "author": ["Chris Dyer", "Victor Chahuneau", "Noah A. Smith."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Dyer et al\\.,? 2013", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Inductive probabilistic taxonomy learning using singular value decomposition", "author": ["Francesca Fallucchi", "Fabio Massimo Zanzotto."], "venue": "Natural Language Engineering, 17(1):71\u201394.", "citeRegEx": "Fallucchi and Zanzotto.,? 2011", "shortCiteRegEx": "Fallucchi and Zanzotto.", "year": 2011}, {"title": "WordNet \u2013 An Electronic Lexical Database. Language, Speech, and Communication", "author": ["Christiane Fellbaum", "editor"], "venue": null, "citeRegEx": "Fellbaum and editor.,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum and editor.", "year": 1998}, {"title": "Papers in Linguistics 1934-51", "author": ["John R. Firth."], "venue": "Longmans, London, UK.", "citeRegEx": "Firth.,? 1957", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "The distributional inclusion hypotheses and lexical entailment", "author": ["Maayan Geffet", "Ido Dagan."], "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 107\u2013114, Michigan, US.", "citeRegEx": "Geffet and Dagan.,? 2005", "shortCiteRegEx": "Geffet and Dagan.", "year": 2005}, {"title": "Distributional structure", "author": ["Zellig S. Harris."], "venue": "Word, 10(23):146\u2013162.", "citeRegEx": "Harris.,? 1954", "shortCiteRegEx": "Harris.", "year": 1954}, {"title": "Exploiting image generality for lexical entailment detection", "author": ["Douwe Kiela", "Laura Rimell", "Ivan Vuli\u0107", "Stephen Clark."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Confer-", "citeRegEx": "Kiela et al\\.,? 2015", "shortCiteRegEx": "Kiela et al\\.", "year": 2015}, {"title": "Directional distributional similarity for lexical inference", "author": ["Lili Kotlerman", "Ido Dagan", "Idan Szpektor", "Maayan Zhitomirsky-Geffet."], "venue": "Natural Language Engineering, 16(4):359\u2013389.", "citeRegEx": "Kotlerman et al\\.,? 2010", "shortCiteRegEx": "Kotlerman et al\\.", "year": 2010}, {"title": "Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning", "author": ["Angeliki Lazaridou", "Georgiana Dinu", "Marco Baroni."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL), pages", "citeRegEx": "Lazaridou et al\\.,? 2015", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2015}, {"title": "Identifying hypernyms in distributional semantic spaces", "author": ["Alessandro Lenci", "Giulia Benotto."], "venue": "*SEM 2012: The First Joint Conference on Lexical and Computational Semantics \u2013 Volume 1: Proceedings of the main conference and the shared task, and", "citeRegEx": "Lenci and Benotto.,? 2012", "shortCiteRegEx": "Lenci and Benotto.", "year": 2012}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceddings of the 27th International Conference on Advances in Neural Information Processing Systems (NIPS), pages 2177\u20132185, Montr\u00e9al, Canada.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Do supervised distributional methods really learn lexical inference relations", "author": ["Omer Levy", "Steffen Remus", "Chris Biemann", "Ido Dagan"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Exploiting Similarities among Languages for Machine Translation", "author": ["Tomas Mikolov", "Quoc V. Le", "Ilya Sutskever."], "venue": "CoRR, abs/1309.4168.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of the 26th International Conference on Advances in Neural Informa-", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "WordNet: A lexical database for English", "author": ["George A. Miller."], "venue": "Communications of the ACM, 38(11):39\u201341.", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Semantic networks of english", "author": ["George A. Miller", "Christiane Fellbaum."], "venue": "Cognition, 41:197\u2013229.", "citeRegEx": "Miller and Fellbaum.,? 1991", "shortCiteRegEx": "Miller and Fellbaum.", "year": 1991}, {"title": "Counter-fitting word vectors to linguistic constraints", "author": ["Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "M. Lina Rojas-Barahona", "PeiHao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young."], "venue": "Proceedings of the 2016", "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? 2016", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2016}, {"title": "The Big Book of Concepts", "author": ["Gregory Murphy."], "venue": "MIT Press, Cambridge, MA, USA.", "citeRegEx": "Murphy.,? 2002", "shortCiteRegEx": "Murphy.", "year": 2002}, {"title": "A graph-based algorithm for inducing lexical taxonomies from scratch", "author": ["Roberto Navigli", "Paola Velardi", "Stefano Faralli."], "venue": "Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence (IJCAI), pages 1872\u20131877,", "citeRegEx": "Navigli et al\\.,? 2011", "shortCiteRegEx": "Navigli et al\\.", "year": 2011}, {"title": "Integrating distributional lexical contrast into word embeddings for antonymsynonym distinction", "author": ["Kim Anh Nguyen", "Sabine Schulte im Walde", "Ngoc Thang Vu."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Wordnet: : Similarity - measuring the relatedness of concepts", "author": ["Ted Pedersen", "Siddharth Patwardhan", "Jason Michelizzi."], "venue": "Proceedings of the 19th National Conference on Artificial Intelligence, Sixteenth Conference on Innovative Applications of Ar-", "citeRegEx": "Pedersen et al\\.,? 2004", "shortCiteRegEx": "Pedersen et al\\.", "year": 2004}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543,", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Distributional lexical entailment by topic coherence", "author": ["Laura Rimell."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 511\u2013 519, Gothenburg, Sweden.", "citeRegEx": "Rimell.,? 2014", "shortCiteRegEx": "Rimell.", "year": 2014}, {"title": "Inclusive yet selective: Supervised distributional hypernymy detection", "author": ["Stephen Roller", "Katrin Erk", "Gemma Boleda."], "venue": "Proceedings of the 25th International Conference on Computational Linguistics (COLING), pages 1025\u20131036, Dublin, Ireland.", "citeRegEx": "Roller et al\\.,? 2014", "shortCiteRegEx": "Roller et al\\.", "year": 2014}, {"title": "Unsupervised measure of word similarity: How to outperform cooccurrence and vector cosine in vsms", "author": ["Enrico Santus", "Alessandro Lenci", "Tin-Shing Chiu", "Qin Lu", "Chu-Ren Huang."], "venue": "Proceedings of the Thirtieth Conference on Artificial Intelli-", "citeRegEx": "Santus et al\\.,? 2016", "shortCiteRegEx": "Santus et al\\.", "year": 2016}, {"title": "Chasing hypernyms in vector spaces with entropy", "author": ["Enrico Santus", "Alessandro Lenci", "Qin Lu", "Sabine Schulte Im Walde."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL),", "citeRegEx": "Santus et al\\.,? 2014", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "Evalution 1.0: an evolving semantic dataset for training and evaluation of distributional semantic models", "author": ["Enrico Santus", "Frances Yung", "Alessandro Lenci", "Chu-Ren Huang"], "venue": "In Proceedings of the 4th Workshop on Linked Data in Linguistics: Resources", "citeRegEx": "Santus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santus et al\\.", "year": 2015}, {"title": "Processing and querying large web corpora with the COW14 architecture", "author": ["Roland Sch\u00e4fer."], "venue": "Proceedings of the 3rd Workshop on Challenges in the Management of Large Corpora, pages 28\u201334, Lancaster, UK.", "citeRegEx": "Sch\u00e4fer.,? 2015", "shortCiteRegEx": "Sch\u00e4fer.", "year": 2015}, {"title": "Building large corpora from the web using a new efficient tool chain", "author": ["Roland Sch\u00e4fer", "Felix Bildhauer."], "venue": "Proceedings of the 8th International Conference on Language Resources and Evaluation, pages 486\u2013493, Istanbul, Turkey.", "citeRegEx": "Sch\u00e4fer and Bildhauer.,? 2012", "shortCiteRegEx": "Sch\u00e4fer and Bildhauer.", "year": 2012}, {"title": "A Database of Paradigmatic Semantic Relation Pairs for German Nouns, Verbs, and Adjectives", "author": ["Silke Scheible", "Sabine Schulte im Walde."], "venue": "Proceedings of Workshop on Lexical and Grammatical Resources for Language Processing, pages 111\u2013", "citeRegEx": "Scheible and Walde.,? 2014", "shortCiteRegEx": "Scheible and Walde.", "year": 2014}, {"title": "Hypernyms under siege: Linguistically-motivated artillery for hypernymy detection", "author": ["Vered Shwartz", "Enrico Santus", "Dominik Schlechtweg."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for", "citeRegEx": "Shwartz et al\\.,? 2017", "shortCiteRegEx": "Shwartz et al\\.", "year": 2017}, {"title": "Nonparametric Statistics for the Behavioral Sciences", "author": ["Sidney Siegel", "N. John Castellan."], "venue": "McGraw-Hill, Boston, MA.", "citeRegEx": "Siegel and Castellan.,? 1988", "shortCiteRegEx": "Siegel and Castellan.", "year": 1988}, {"title": "Semantic taxonomy induction from heterogenous evidence", "author": ["Rion Snow", "Daniel Jurafsky", "Andrew Y. Ng."], "venue": "Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics (ACL), pages 801\u2013808, Sydney, Australia.", "citeRegEx": "Snow et al\\.,? 2006", "shortCiteRegEx": "Snow et al\\.", "year": 2006}, {"title": "Analisi computazionale delle relazioni semantiche: Uno studio della lingua italiana", "author": ["Irene Sucameli."], "venue": "B.s. thesis, University of Pisa.", "citeRegEx": "Sucameli.,? 2015", "shortCiteRegEx": "Sucameli.", "year": 2015}, {"title": "Learning term embeddings for taxonomic relation identification using dynamic weighting neural network", "author": ["Luu Anh Tuan", "Yi Tay", "Siu Cheung Hui", "See Kiong Ng."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Tuan et al\\.,? 2016", "shortCiteRegEx": "Tuan et al\\.", "year": 2016}, {"title": "From Frequency to Meaning: Vector Space Models of Semantics", "author": ["Peter D. Turney", "Patrick Pantel."], "venue": "Journal of Artificial Intelligence Research, 37:141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Order-embeddings of images and language", "author": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun."], "venue": "Proceedings of the 4th International Conference on Learning Representations (ICLR), San Juan, Puerto Rico.", "citeRegEx": "Vendrov et al\\.,? 2016", "shortCiteRegEx": "Vendrov et al\\.", "year": 2016}, {"title": "Word representations via gaussian embedding", "author": ["Luke Vilnis", "Andrew McCallum."], "venue": "Proceedings of the 3rd International Conference on Learning Representations (ICLR), California, USA.", "citeRegEx": "Vilnis and McCallum.,? 2015", "shortCiteRegEx": "Vilnis and McCallum.", "year": 2015}, {"title": "Hyperlex: A large-scale evaluation of graded lexical entailment", "author": ["Ivan Vuli\u0107", "Daniela Gerz", "Douwe Kiela", "Felix Hill", "Anna Korhonen."], "venue": "arXiv.", "citeRegEx": "Vuli\u0107 et al\\.,? 2016", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2016}, {"title": "Learning to distinguish hypernyms and co-hyponyms", "author": ["Julie Weeds", "Daoud Clarke", "Jeremy Reffin", "David J. Weir", "Bill Keller."], "venue": "Proceedings of the 25th International Conference on Computational Linguistics (COLING), pages 2249\u20132259, Dublin,", "citeRegEx": "Weeds et al\\.,? 2014", "shortCiteRegEx": "Weeds et al\\.", "year": 2014}, {"title": "A general framework for distributional similarity", "author": ["Julie Weeds", "David Weir."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 81\u201388, Stroudsburg, PA, USA.", "citeRegEx": "Weeds and Weir.,? 2003", "shortCiteRegEx": "Weeds and Weir.", "year": 2003}, {"title": "Characterising measures of lexical distributional similarity", "author": ["Julie Weeds", "David Weir", "Diana McCarthy."], "venue": "Proceedings of the 20th International Conference on Computational Linguistics (COLING), pages 1015\u20131021, Geneva, Switzerland.", "citeRegEx": "Weeds et al\\.,? 2004", "shortCiteRegEx": "Weeds et al\\.", "year": 2004}, {"title": "Verbs semantics and lexical selection", "author": ["Zhibiao Wu", "Martha Palmer."], "venue": "Proceedings of the 32nd Annual Meeting on Association for Computational Linguistics (ACL), pages 133\u2013138, Las Cruces, New Mexico.", "citeRegEx": "Wu and Palmer.,? 1994", "shortCiteRegEx": "Wu and Palmer.", "year": 1994}, {"title": "Learning term embeddings for hypernymy identification", "author": ["Zheng Yu", "Haixun Wang", "Xuemin Lin", "Min Wang."], "venue": "Proceedings of the 24th International Conference on Artificial Intelligence (IJCAI), pages 1390\u20131397, Buenos Aires, Argentina.", "citeRegEx": "Yu et al\\.,? 2015", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Bootstrapping distributional feature vector quality", "author": ["Maayan Zhitomirsky-Geffet", "Ido Dagan."], "venue": "Computational Linguistics, 35(3):435\u2013461.", "citeRegEx": "Zhitomirsky.Geffet and Dagan.,? 2009", "shortCiteRegEx": "Zhitomirsky.Geffet and Dagan.", "year": 2009}], "referenceMentions": [{"referenceID": 21, "context": "Hypernymy represents a major semantic relation and a key organization principle of semantic memory (Miller and Fellbaum, 1991; Murphy, 2002).", "startOffset": 99, "endOffset": 140}, {"referenceID": 23, "context": "Hypernymy represents a major semantic relation and a key organization principle of semantic memory (Miller and Fellbaum, 1991; Murphy, 2002).", "startOffset": 99, "endOffset": 140}, {"referenceID": 38, "context": "From a computational point of view, automatic hypernymy detection is useful for NLP tasks such as taxonomy creation (Snow et al., 2006; Navigli et al., 2011), recognizing textual entailment (Dagan et al.", "startOffset": 116, "endOffset": 157}, {"referenceID": 24, "context": "From a computational point of view, automatic hypernymy detection is useful for NLP tasks such as taxonomy creation (Snow et al., 2006; Navigli et al., 2011), recognizing textual entailment (Dagan et al.", "startOffset": 116, "endOffset": 157}, {"referenceID": 5, "context": ", 2011), recognizing textual entailment (Dagan et al., 2013), and text generation (Biran and McKeown, 2013), among many others.", "startOffset": 40, "endOffset": 60}, {"referenceID": 3, "context": ", 2013), and text generation (Biran and McKeown, 2013), among many others.", "startOffset": 29, "endOffset": 54}, {"referenceID": 10, "context": "Unsupervised measures exploit the distributional inclusion hypothesis (Geffet and Dagan, 2005; ZhitomirskyGeffet and Dagan, 2009), or the distributional", "startOffset": 70, "endOffset": 129}, {"referenceID": 31, "context": "informativeness hypothesis (Santus et al., 2014; Rimell, 2014).", "startOffset": 27, "endOffset": 62}, {"referenceID": 28, "context": "informativeness hypothesis (Santus et al., 2014; Rimell, 2014).", "startOffset": 27, "endOffset": 62}, {"referenceID": 29, "context": ", 2012; Roller et al., 2014; Weeds et al., 2014). The resulting vector is fed into a Support Vector Machine (SVM) or into Logistic Regression (LR), to predict hypernymy. Across approaches, Shwartz et al. (2017) demonstrated that", "startOffset": 8, "endOffset": 211}, {"referenceID": 17, "context": "Furthermore, Levy et al. (2015) showed that supervised methods memorize prototypical hypernyms instead of learning a relation between two words.", "startOffset": 13, "endOffset": 32}, {"referenceID": 48, "context": "Yu et al. (2015) proposed a supervised method to learn term embeddings for hypernymy identification, based on pre-extracted hypernymy pairs.", "startOffset": 0, "endOffset": 17}, {"referenceID": 40, "context": "Recently, Tuan et al. (2016) proposed a dynamic weighting neural model to learn term embeddings in which the model encodes not only the information of hypernyms vs.", "startOffset": 10, "endOffset": 29}, {"referenceID": 44, "context": "In addition, we apply the model to the task of graded lexical entailment (Vuli\u0107 et al., 2016), and we assess the capability of HyperVec on generalizing hypernymy by mapping to German and Italian.", "startOffset": 73, "endOffset": 93}, {"referenceID": 41, "context": "All of these approaches represent words as vectors in distributional semantic models (Turney and Pantel, 2010), relying on the distributional hypothesis (Harris, 1954; Firth, 1957).", "startOffset": 85, "endOffset": 110}, {"referenceID": 11, "context": "All of these approaches represent words as vectors in distributional semantic models (Turney and Pantel, 2010), relying on the distributional hypothesis (Harris, 1954; Firth, 1957).", "startOffset": 153, "endOffset": 180}, {"referenceID": 9, "context": "All of these approaches represent words as vectors in distributional semantic models (Turney and Pantel, 2010), relying on the distributional hypothesis (Harris, 1954; Firth, 1957).", "startOffset": 153, "endOffset": 180}, {"referenceID": 2, "context": "de/data/hypervec Lenci and Benotto, 2012) all rely on some variation of the distributional inclusion hypothesis: If u is a semantically narrower term than v, then a significant number of salient distributional features of u is expected to be included in the feature vector of v as well. In addition, Santus et al. (2014) proposed the distributional informativeness hypothesis, that hypernyms tend to be less informative than hyponyms, and that they occur in more general contexts than their hyponyms.", "startOffset": 27, "endOffset": 321}, {"referenceID": 19, "context": "represent words as low-dimensional and realvalued vectors (Mikolov et al., 2013b; Pennington et al., 2014).", "startOffset": 58, "endOffset": 106}, {"referenceID": 27, "context": "represent words as low-dimensional and realvalued vectors (Mikolov et al., 2013b; Pennington et al., 2014).", "startOffset": 58, "endOffset": 106}, {"referenceID": 0, "context": "Each hypernymy pair is encoded by some combination of the two word vectors, such as concatenation (Baroni et al., 2012) or difference (Roller et al.", "startOffset": 98, "endOffset": 119}, {"referenceID": 29, "context": ", 2012) or difference (Roller et al., 2014; Weeds et al., 2014).", "startOffset": 22, "endOffset": 63}, {"referenceID": 45, "context": ", 2012) or difference (Roller et al., 2014; Weeds et al., 2014).", "startOffset": 22, "endOffset": 63}, {"referenceID": 17, "context": "Because word embeddings are trained for similar and symmetric vectors, it is however unclear whether the supervised methods do actually learn the asymmetry in hypernymy (Levy et al., 2015).", "startOffset": 169, "endOffset": 188}, {"referenceID": 49, "context": "Yu et al. (2015) proposed a dynamic distance-margin model to learn term embeddings that capture properties of hypernymy.", "startOffset": 0, "endOffset": 17}, {"referenceID": 40, "context": "Tuan et al. (2016) introduced a dynamic weighting neural network to learn term embeddings that encode information about hypernymy and also about their contexts, considering all words between a hypernym and its", "startOffset": 0, "endOffset": 19}, {"referenceID": 20, "context": "The proposed model is trained on a set of hypernym relations extracted from WordNet (Miller, 1995).", "startOffset": 84, "endOffset": 98}, {"referenceID": 20, "context": "The proposed model is trained on a set of hypernym relations extracted from WordNet (Miller, 1995). The embeddings are applied as features to detect hypernymy, using an SVM classifier. Tuan et al. (2016) handles the drawback of the approach by Yu et al.", "startOffset": 85, "endOffset": 204}, {"referenceID": 20, "context": "The proposed model is trained on a set of hypernym relations extracted from WordNet (Miller, 1995). The embeddings are applied as features to detect hypernymy, using an SVM classifier. Tuan et al. (2016) handles the drawback of the approach by Yu et al. (2015), considering the contextual information between two terms; however the method still is not able to determine the directionality of a hypernym pair.", "startOffset": 85, "endOffset": 261}, {"referenceID": 20, "context": "The proposed model is trained on a set of hypernym relations extracted from WordNet (Miller, 1995). The embeddings are applied as features to detect hypernymy, using an SVM classifier. Tuan et al. (2016) handles the drawback of the approach by Yu et al. (2015), considering the contextual information between two terms; however the method still is not able to determine the directionality of a hypernym pair. Vendrov et al. (2016) proposed a method to encode order into learned distributed representations, to explicitly model partial order structure of the visual-semantic hierarchy or the hierarchy of hypernymy in WordNet.", "startOffset": 85, "endOffset": 431}, {"referenceID": 17, "context": "The Skip-gram model is a word embeddings method suggested by Mikolov et al. (2013b). Levy and Goldberg (2014) introduced a variant of the Skip-gram model with negative sampling (SGNS), in which the objective function is defined as follows: JSGNS = \u2211", "startOffset": 61, "endOffset": 84}, {"referenceID": 16, "context": "Levy and Goldberg (2014) introduced a variant of the Skip-gram model with negative sampling (SGNS), in which the objective function is defined as follows: JSGNS = \u2211", "startOffset": 0, "endOffset": 25}, {"referenceID": 25, "context": "Inspired by the distributional lexical contrast model in Nguyen et al. (2016) for distinguishing antonymy from synonymy, this paper proposes two objective functions to learn hierarchical embeddings for hypernymy.", "startOffset": 57, "endOffset": 78}, {"referenceID": 34, "context": "We use the ENCOW14A corpus (Sch\u00e4fer and Bildhauer, 2012; Sch\u00e4fer, 2015) with approx.", "startOffset": 27, "endOffset": 71}, {"referenceID": 33, "context": "We use the ENCOW14A corpus (Sch\u00e4fer and Bildhauer, 2012; Sch\u00e4fer, 2015) with approx.", "startOffset": 27, "endOffset": 71}, {"referenceID": 1, "context": "The evaluation was performed on four semantic relation datasets: BLESS (Baroni and Lenci, 2011), WEEDS (Weeds et al.", "startOffset": 71, "endOffset": 95}, {"referenceID": 47, "context": "The evaluation was performed on four semantic relation datasets: BLESS (Baroni and Lenci, 2011), WEEDS (Weeds et al., 2004), EVALUTION (Santus et al.", "startOffset": 103, "endOffset": 123}, {"referenceID": 32, "context": ", 2004), EVALUTION (Santus et al., 2015), and LENCI&BENOTTO (Benotto, 2015).", "startOffset": 19, "endOffset": 40}, {"referenceID": 2, "context": ", 2015), and LENCI&BENOTTO (Benotto, 2015).", "startOffset": 27, "endOffset": 42}, {"referenceID": 36, "context": "In comparison to the state-of-the-art unsupervised measures compared by Shwartz et al. (2017) (henceforth, baseline models), we apply our unsupervised measure HyperScore (Equation 7) to rank hypernymy against other relations.", "startOffset": 72, "endOffset": 94}, {"referenceID": 12, "context": "In this setup, we rely on three datasets of semantic relations, which were all used in various state-of-the-art approaches before, and brought together for hypernymy evaluation by Kiela et al. (2015). (i) A subset of BLESS contains 1,337 hyponym-hypernym pairs.", "startOffset": 180, "endOffset": 200}, {"referenceID": 45, "context": "(ii) Following Weeds et al. (2014), we conduct a binary classification with a subset of 1,168 BLESS word pairs.", "startOffset": 15, "endOffset": 35}, {"referenceID": 12, "context": "Instead of using a manually defined threshold as done by Kiela et al. (2015), we decided to run 1 000 iterations which randomly sampled only 2% of the available pairs for learning a threshold, using the", "startOffset": 57, "endOffset": 77}, {"referenceID": 0, "context": "For supervised hypernymy detection, we make use of the two datasets: the full BLESS dataset, and ENTAILMENT (Baroni et al., 2012), containing 2,770 relation pairs in total, including 1,385 hypernym pairs and 1,385 other relations pairs.", "startOffset": 108, "endOffset": 129}, {"referenceID": 0, "context": "For supervised hypernymy detection, we make use of the two datasets: the full BLESS dataset, and ENTAILMENT (Baroni et al., 2012), containing 2,770 relation pairs in total, including 1,385 hypernym pairs and 1,385 other relations pairs. We follow the same procedure as Yu et al. (2015) and Tuan et al.", "startOffset": 109, "endOffset": 286}, {"referenceID": 0, "context": "For supervised hypernymy detection, we make use of the two datasets: the full BLESS dataset, and ENTAILMENT (Baroni et al., 2012), containing 2,770 relation pairs in total, including 1,385 hypernym pairs and 1,385 other relations pairs. We follow the same procedure as Yu et al. (2015) and Tuan et al. (2016) to assess HyperVec on the two datasets.", "startOffset": 109, "endOffset": 309}, {"referenceID": 40, "context": "Table 4 shows the performance of HyperVec and the two baseline models reported by Tuan et al. (2016). HyperVec slightly outperforms the method of Tuan et al.", "startOffset": 82, "endOffset": 101}, {"referenceID": 40, "context": "Table 4 shows the performance of HyperVec and the two baseline models reported by Tuan et al. (2016). HyperVec slightly outperforms the method of Tuan et al. (2016) on the BLESS dataset, and is equivalent to the performance of their method on the ENTAILMENT dataset.", "startOffset": 82, "endOffset": 165}, {"referenceID": 40, "context": "Table 4 shows the performance of HyperVec and the two baseline models reported by Tuan et al. (2016). HyperVec slightly outperforms the method of Tuan et al. (2016) on the BLESS dataset, and is equivalent to the performance of their method on the ENTAILMENT dataset. In comparison to the method of Yu et al. (2015), HyperVec achieves significant improvements.", "startOffset": 82, "endOffset": 315}, {"referenceID": 44, "context": "In this experiment, we apply HyperVec to the dataset of graded lexical entailment, HyperLex, as introduced by Vuli\u0107 et al. (2016). The HyperLex dataset provides soft lexical entailment on a conModels BLESS ENTAILMENT", "startOffset": 110, "endOffset": 130}, {"referenceID": 40, "context": "87 Tuan et al. (2016) 0.", "startOffset": 3, "endOffset": 22}, {"referenceID": 31, "context": "\u2022 Generality measures (SQLS) (Santus et al., 2014)", "startOffset": 29, "endOffset": 50}, {"referenceID": 12, "context": "\u2022 Visual generality measures (VIS) (Kiela et al., 2015)", "startOffset": 35, "endOffset": 55}, {"referenceID": 44, "context": "\u2022 Consideration of concept frequency ratio (FR) (Vuli\u0107 et al., 2016)", "startOffset": 48, "endOffset": 68}, {"referenceID": 48, "context": "\u2022 WordNet-based similarity measures (WN) (Wu and Palmer, 1994; Pedersen et al., 2004)", "startOffset": 41, "endOffset": 85}, {"referenceID": 26, "context": "\u2022 WordNet-based similarity measures (WN) (Wu and Palmer, 1994; Pedersen et al., 2004)", "startOffset": 41, "endOffset": 85}, {"referenceID": 42, "context": "\u2022 Order embeddings (OrderEmb) (Vendrov et al., 2016)", "startOffset": 30, "endOffset": 52}, {"referenceID": 19, "context": "\u2022 Skip-gram embeddings (SGNS) (Mikolov et al., 2013b; Levy and Goldberg, 2014)", "startOffset": 30, "endOffset": 78}, {"referenceID": 16, "context": "\u2022 Skip-gram embeddings (SGNS) (Mikolov et al., 2013b; Levy and Goldberg, 2014)", "startOffset": 30, "endOffset": 78}, {"referenceID": 22, "context": "database with linguistic constraints (PARAGRAM) (Mrk\u0161i\u0107 et al., 2016)", "startOffset": 48, "endOffset": 69}, {"referenceID": 43, "context": "\u2022 Gaussian embeddings (Word2Gauss) (Vilnis and McCallum, 2015)", "startOffset": 35, "endOffset": 62}, {"referenceID": 37, "context": "The performance of the models is assessed through Spearman\u2019s rank-order correlation coefficient \u03c1 (Siegel and Castellan, 1988), comparing the ranks of the models\u2019 scores and the human judgments for the given word pairs.", "startOffset": 98, "endOffset": 126}, {"referenceID": 18, "context": "a source language (German, Italian) and our English HyperVec space is learned, by relying on the least-squares error method from previous work using cross-lingual data (Mikolov et al., 2013a) and different modalities (Lazaridou et al.", "startOffset": 168, "endOffset": 191}, {"referenceID": 14, "context": ", 2013a) and different modalities (Lazaridou et al., 2015).", "startOffset": 34, "endOffset": 58}, {"referenceID": 6, "context": "Word alignment counts were extracted using fast align (Dyer et al., 2013).", "startOffset": 54, "endOffset": 73}, {"referenceID": 39, "context": "The 1,350 Italian pairs were collected via Crowdflower by Sucameli (2015) in the same way.", "startOffset": 58, "endOffset": 74}], "year": 2017, "abstractText": "We present a novel neural model HyperVec to learn hierarchical embeddings for hypernymy detection and directionality. While previous embeddings have shown limitations on prototypical hypernyms, HyperVec represents an unsupervised measure where embeddings are learned in a specific order and capture the hypernym\u2013hyponym distributional hierarchy. Moreover, our model is able to generalize over unseen hypernymy pairs, when using only small sets of training data, and by mapping to other languages. Results on benchmark datasets show that HyperVec outperforms both state-of-theart unsupervised measures and embedding models on hypernymy detection and directionality, and on predicting graded lexical entailment.", "creator": "LaTeX with hyperref package"}}}