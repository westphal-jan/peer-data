{"id": "1707.07328", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jul-2017", "title": "Adversarial Examples for Evaluating Reading Comprehension Systems", "abstract": "Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of $75\\%$ F1 score to $36\\%$; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to $7\\%$. We hope our insights will motivate the development of new models that understand language more precisely.", "histories": [["v1", "Sun, 23 Jul 2017 18:26:29 GMT  (836kb,D)", "http://arxiv.org/abs/1707.07328v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["robin jia", "percy liang"], "accepted": true, "id": "1707.07328"}, "pdf": {"name": "1707.07328.pdf", "metadata": {"source": "CRF", "title": "Adversarial Examples for Evaluating Reading Comprehension Systems", "authors": ["Robin Jia", "Percy Liang"], "emails": ["robinjia@cs.stanford.edu", "pliang@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, the models can succeed in this paradigm by predicting most of the test examples while ignoring the deeper, more difficult phenomena. We focus on the SQuAD reading of the conceptual task (Paperno et al., 2016).In this paper, we propose the adversarial evaluation of NLP, in which the systems are evaluated instead on adversarially selected inputs."}, {"heading": "2 The SQuAD Task and Models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Task", "text": "The SQuAD dataset (Rajpurkar et al., 2016) contains 107,785 man-made reading comprehension questions about Wikipedia articles. Each question relates to one paragraph of an article, and the corresponding answer guarantees a span in that paragraph."}, {"heading": "2.2 Models", "text": "In developing and testing our methods, we focused on two published model architectures: BiDAF (Seo et al., 2016) and Match-LSTM (Wang and Jiang, 2016), both of which are in-depth learning architectures that predict a probability distribution over the correct answer. Each model has a single and an ensemble version, resulting in a total of four systems. We also validate our key findings on twelve other published models with publicly available test time code: single and ensemble versions ReasoNet (Shen et al., 2017), single and ensemble versions MnemonicReader (Hu et al., 2017), Structural Embedding of Dependency Trees (SEDT) single versions and ensemble versions (Liu et al., 2017), jNet (Zhang et al., 2017), Ruminating Reader (Gong and Bowman, 2017), Structural Embedding of Dependency Trependency Trees (Single DSED, 2017), single versions and ensemble versions (Liu et al., 2017), jnet (Zhang et al., 2017), Rumour versions (Zhang et al., 2017), Ruminating Reader (Gong and Bowman, 2017), Structural Embedding of Dependency Trependency Trees (Single DSED, 2017)."}, {"heading": "2.3 Standard Evaluation", "text": "For a model f that accepts pairs of paragraphs (p, q) and prints an answer a), the default accuracy via a test set is Dtest simpleAcc (f) def = 1 | Dtest | \u2211 (p, q, a) and Dtestv ((p, q, a), f), where v is the F1 value between the true answer a and the predicted answer f (p, q) (see Rajpurkar et al. (2016) for details)."}, {"heading": "3 Adversarial Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 General Framework", "text": "Weissenborn et al. (2017) argue that many SQuAD questions can be answered with heuristics based on type and keyword matching. To determine whether existing models have learned much more than such simple patterns, we introduce opponents who confuse flawed models by changing test examples. Consider the example in Figure 1: The BiDAF ensemble model originally gives the correct answer, but gets confused when an opposing distracting sentence is added to the paragraph. We define an opponent A as a function that takes up an example (p, q, a), optionally with a model f, and returns a new example (p, q, a). The opposing accuracy with respect to A isAdv (f) def = 1 | Dtest (p, q, a), optionally with a model f (p, q, a)."}, {"heading": "3.2 Semantics-preserving Adversaries", "text": "In image classification, contrary examples are usually generated by adding an imperceptible amount of noise to the input (Szegedy et al., 2014; Goodfellow et al., 2015). These disturbances do not alter the semantics of the image, but they can alter the predictions of models that are hypersensitive to semantically conserving changes. For language, the direct analogy would be to paraphrase the input (Madnani and Dorr, 2010). However, generating high-precision paraphrases is a challenge, as most edits of a sentence actually change its meaning."}, {"heading": "3.3 Concatenative Adversaries", "text": "Instead of relying on paraphrases, we use perturbations that alter semantics to form concatenating adversaries that produce examples of the form (p + s, q, a) of some sentences. In other words, concatenating adversaries add a new sentence to the end of the paragraph and leave the question and answer unchanged. Valid opposing examples are precisely those for which s does not contradict the correct answer; we refer to such sentences that are compatible with (p, q, a). We use semantically altering perturbations to ensure that s is compatible, even if it may have many words in common with the question q. Existing models are bad at distinguishing these sentences from sentences that actually address the question, suggesting that they are not suffering from hypersensitivity, but from overstability, semantically altering editions. Table 1 summarizes this important distinction. The decision to always append to the end of English grammaries is slightly more bitter than this topic, although we are likely to have the two initial variants of the ADS."}, {"heading": "3.3.1 ADDSENT", "text": "In fact, it is the case that most people are able to understand themselves and understand what they are doing. In fact, it is the case that most people are able to know themselves and understand what they are doing. It is as if people are able to know themselves and understand what they are doing. In fact, it is as if people are able to know themselves and understand what they are doing. In fact, it is as if people are able to know themselves and understand what they are doing. In fact, it is as if people are able to know themselves and understand what they are doing. In fact, it is as if people are able to know themselves and understand what they are doing. In fact, it is as if people are able to know themselves and understand themselves."}, {"heading": "3.3.2 ADDANY", "text": "In fact, the majority of people who are in a position to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Setup", "text": "For all experiments, we measure the hostile F1 value (Rajpurkar et al., 2016) using 1000 randomly selected examples from the SQuAD development set (the test set is not publicly available). Downsampling was helpful because ADDANY and ADDCOMMON can issue thousands of model queries per example, which makes them very slow. As the efficiency parameters we measured are large, this downsampling does not harm statistical significance."}, {"heading": "4.2 Main Experiments", "text": "Table 2 shows the performance of the MatchLSTM and BiDAF models over all four adversaries. Each model suffered a significant decrease in accuracy for each form of the opposing rating. ADDANY caused the average F1 value to drop from 75.7% to 31.3% for all four models. ADDANY was even more effective, bringing the average F1 value down to 6.7%. ADDONESENT retained much of the effectiveness of ADDSENT, even though it was model-independent. Finally, ADDCOMON caused the F1 value to drop to 46.1% in superage, although only generic words were added. We also confirmed that our adversaries were generic enough to deceive models that we did not use during development. We conducted ADDSENT over twelve published models for which we found a publicly available test time code; we did not perform ADDANY on these models because not all models suspended production distributions."}, {"heading": "4.3 Human Evaluation", "text": "To ensure that our results are valid, we have verified that people are not deceived by our adversarial examples. As ADDANY requires too many model queries to compete against humans, we focused on ADDSENT. We presented each original and adversarial paragraph pair to three crowdworkers and asked them to select the correct answer from the paragraph by copy-and-paste. Afterwards, we voted overwhelmingly for the three answers (if all three answers were different, we randomly selected one). These results are shown in Table 4. In original examples, our people are actually slightly better than the stated number of 91.2 F1 across the entire development group. In ADDSENT, human accuracy drops by 13.1 F1 points, much less than in computer systems. Furthermore, much of this decrease can be explained by errors that have nothing to do with our adversarial judgments."}, {"heading": "4.4 Analysis", "text": "Next, we tried to better understand the behavior of our four main negative rating models. To highlight the errors caused by the opponent, we focused on examples where the model originally predicted the (exact) correct answer. We divided this set into \"model successes\" - examples where the model continued to be correct during the opposing rating - and \"model errors\" - examples where the model gave an incorrect answer during the opposing rating."}, {"heading": "4.4.1 Manual verification", "text": "First, we checked 100 randomly selected errors of the BiDAF ensemble. We found only one where the sentence could be interpreted as an answer to the question: In this case, ADDSENT replaced the word \"Muslim\" with the related word \"Islamic,\" so that the resulting negative sentence still contradicted the correct answer. Furthermore, we found 7 minor grammatical errors, such as discrepancies between subject and verb (e.g. \"The Alaska archipelago consists almost exclusively of hamsters.\") and the misuse of functional words (e.g. \"Nitrogen gas accounts for 21.8% of the Martian atmosphere.\"), but no errors that significantly impaired the understanding of the sentence. We also checked the compatibility with ADDANY. We found no violations out of 100 randomly selected errors of the BiDAF ensemble."}, {"heading": "4.4.2 Error analysis", "text": "Next, we wanted to understand what types of errors the models made based on the ADDSENT examples. In 96.6% of the model errors, the model predicted a margin in the opposing sentence. The lengths of the predicted answers were largely similar to those of the correct answers, but the BiDAF models occasionally predicted a very long margin. The BiDAF single model predicted an answer of more than 29 words - the length of the longest answer in the SQuAD development sentence - to 5.0% of the model errors; in the BiDAF ensemble, this figure was 1.6%. Since the BiDAF models independently predict the start and end positions of the answer, they can predict a very long span if the end pointer is affected by the opposing sentence, but not the start pointer. Match-LSTM has a similar structure, but also has a hard-coded rule that prevents the answer from predicting very long answers."}, {"heading": "4.4.3 Categorizing ADDSENT sentences", "text": "We then examined sentences manually generated by ADDSENT. In 100 failures of the BiDAF ensemble, we found 75 cases in which an entity name was changed in the opposing sentence, 17 cases in which numbers or data were changed, and 33 cases in which a response to a question word was used. 5 In addition, in 7 sentences during step 4 of ADDSENT there were other crowd-worker disturbances. For example, in a question about the \"Kalven Report,\" the opposing sentence \"The statement quoting Kalven\" was discussed; in another case, the question \"How does Kenya fight corruption?\" was answered with the unhelpful sentence \"Tanzania fights corruption\" (the model simply answered \"Corruption\")."}, {"heading": "4.4.4 Reasons for model successes", "text": "Finally, we tried to understand the factors that influence whether the model is resilient to hostile interference in a particular example. First, we found that models perform well when the question has an exact n-gram match with the original paragraph. Figure 3 shows the fraction of examples for which an n-gram appears literally in the question in the original passage; this is much higher with model successes. For example, 41.5% of BiDAF ensemble successes had a 4-gram in common with the original paragraph, compared with only 21.0% of model failures. We also found that models were more likely to succeed with short questions. Figure 4 shows the dis-5 These numbers add up to more than 100 because more than one word per example can be changed."}, {"heading": "4.5 Transferability across Models", "text": "In computer vision, hostile examples that deceive a model also tend to deceive other models (Szegedy et al., 2014; Moosavi-Dezfooli et al., 2017); we examine whether the same pattern applies to us. Examples from ADDONESENT clearly transfer between models, since ADDONESENT always adds the same adverse theorem regardless of the model. Table 5 shows the results of the evaluation of the four main models based on adverse examples generated by using ADDSENT or ADDANY against each model. Anti-ADDONESENT examples transfer between models quite effectively; in particular, they are more difficult than ADDONESENT examples, which implies that examples that deceive a model are more likely to deceive other models."}, {"heading": "4.6 Training on Adversarial Examples", "text": "Due to the prohibitive cost of operating ADDSENT or ADDANY over the entire training set, we have instead performed only steps 1-3 of ADDSENT (everything except crowdsourcing) to generate a raw opposing set for each training example.6The results of the evaluation of these models are presented in Table 6. At first glance, training on contrary data appears effective as it largely protects against ADDSENT. However, further research shows that training on these examples has limited usefulness. To demonstrate this, we have referred to a variant of ADDSENT as ADDSENTMOD, which differs from the ADDSENT model in two ways: it uses another set of fake answers (e.g. PDSDSDSDSENT models that use the ADDSENTMOD model to improve ADDSENTMOD models)."}, {"heading": "5 Discussion and Related Work", "text": "In fact, the fact is that most of them are able to survive on their own, without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in"}], "references": [{"title": "Build it, break it: The language edition", "author": ["E.M. Bender", "H. Daume III", "A. Ettinger", "H. Kannan", "S. Rao", "E. Rothschild."], "venue": "https://bibinlp. umiacs.umd.edu/.", "citeRegEx": "Bender et al\\.,? 2017", "shortCiteRegEx": "Bender et al\\.", "year": 2017}, {"title": "Generating sentences from a continuous space", "author": ["R. Jozefowicz", "S. Bengio."], "venue": "Computational Natural Language Learning (CoNLL).", "citeRegEx": "Jozefowicz and Bengio.,? 2016", "shortCiteRegEx": "Jozefowicz and Bengio.", "year": 2016}, {"title": "Adversarial classification", "author": ["N. Dalvi", "P. Domingos", "Mausam", "S. Sanghai", "D. Verma."], "venue": "International Conference on Knowledge Discovery and Data Mining (KDD).", "citeRegEx": "Dalvi et al\\.,? 2004", "shortCiteRegEx": "Dalvi et al\\.", "year": 2004}, {"title": "WordNet: An Electronic Lexical Database", "author": ["C. Fellbaum."], "venue": "MIT Press.", "citeRegEx": "Fellbaum.,? 1998", "shortCiteRegEx": "Fellbaum.", "year": 1998}, {"title": "Nightmare at test time: robust learning by feature deletion", "author": ["A. Globerson", "S. Roweis."], "venue": "International Conference on Machine Learning (ICML), pages 353\u2013360.", "citeRegEx": "Globerson and Roweis.,? 2006", "shortCiteRegEx": "Globerson and Roweis.", "year": 2006}, {"title": "Ruminating reader: Reasoning with gated multi-hop attention", "author": ["Y. Gong", "S.R. Bowman."], "venue": "arXiv.", "citeRegEx": "Gong and Bowman.,? 2017", "shortCiteRegEx": "Gong and Bowman.", "year": 2017}, {"title": "Generative adversarial nets", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Goodfellow et al\\.,? 2015", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Mnemonic reader for machine comprehension", "author": ["M. Hu", "Y. Peng", "X. Qiu."], "venue": "arXiv.", "citeRegEx": "Hu et al\\.,? 2017", "shortCiteRegEx": "Hu et al\\.", "year": 2017}, {"title": "Data recombination for neural semantic parsing", "author": ["R. Jia", "P. Liang."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Jia and Liang.,? 2016", "shortCiteRegEx": "Jia and Liang.", "year": 2016}, {"title": "Learning recurrent span representations for extractive question answering", "author": ["K. Lee", "S. Salant", "T. Kwiatkowski", "A. Parikh", "D. Das", "J. Berant."], "venue": "arXiv.", "citeRegEx": "Lee et al\\.,? 2017", "shortCiteRegEx": "Lee et al\\.", "year": 2017}, {"title": "On our best behaviour", "author": ["H.J. Levesque."], "venue": "International Joint Conference on Artificial Intelligence (IJCAI).", "citeRegEx": "Levesque.,? 2013", "shortCiteRegEx": "Levesque.", "year": 2013}, {"title": "Adversarial learning for neural dialogue generation", "author": ["J. Li", "W. Monroe", "T. Shi", "A. Ritter", "D. Jurafsky."], "venue": "arXiv preprint arXiv:1701.06547.", "citeRegEx": "Li et al\\.,? 2017", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Structural embedding of syntactic trees for machine comprehension", "author": ["R. Liu", "J. Hu", "W. Wei", "Z. Yang", "E. Nyberg."], "venue": "arXiv.", "citeRegEx": "Liu et al\\.,? 2017", "shortCiteRegEx": "Liu et al\\.", "year": 2017}, {"title": "Adversarial learning", "author": ["D. Lowd", "C. Meek."], "venue": "International Conference on Knowledge Discovery and Data Mining (KDD).", "citeRegEx": "Lowd and Meek.,? 2005", "shortCiteRegEx": "Lowd and Meek.", "year": 2005}, {"title": "Generating phrasal and sentential paraphrases: A survey of data-driven methods", "author": ["N. Madnani", "B.J. Dorr."], "venue": "Computational Linguistics, 36(3):341\u2013 387.", "citeRegEx": "Madnani and Dorr.,? 2010", "shortCiteRegEx": "Madnani and Dorr.", "year": 2010}, {"title": "The stanford coreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky."], "venue": "ACL system demonstrations.", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Universal adversarial perturbations", "author": ["S. Moosavi-Dezfooli", "A. Fawzi", "O. Fawzi", "P. Frossard."], "venue": "Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Moosavi.Dezfooli et al\\.,? 2017", "shortCiteRegEx": "Moosavi.Dezfooli et al\\.", "year": 2017}, {"title": "Simple black-box adversarial perturbations for deep networks", "author": ["N. Narodytska", "S.P. Kasiviswanathan."], "venue": "arXiv preprint arXiv:1612.06299.", "citeRegEx": "Narodytska and Kasiviswanathan.,? 2016", "shortCiteRegEx": "Narodytska and Kasiviswanathan.", "year": 2016}, {"title": "The LAMBADA dataset: Word prediction requiring a broad discourse context", "author": ["D. Paperno", "G. Kruszewski", "A. Lazaridou", "Q.N. Pham", "R. Bernardi", "S. Pezzelle", "M. Baroni", "G. Boleda", "R. Fernandez."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Paperno et al\\.,? 2016", "shortCiteRegEx": "Paperno et al\\.", "year": 2016}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow", "S. Jha", "Z. Celik", "A. Swami."], "venue": "Proceedings of the ACM Asia Conference on Computer and Communications Security.", "citeRegEx": "Papernot et al\\.,? 2017", "shortCiteRegEx": "Papernot et al\\.", "year": 2017}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["P. Rajpurkar", "J. Zhang", "K. Lopyrev", "P. Liang."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Unbounded dependency recovery for parser evaluation", "author": ["L. Rimell", "S. Clark", "M. Steedman."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Rimell et al\\.,? 2009", "shortCiteRegEx": "Rimell et al\\.", "year": 2009}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["M. Seo", "A. Kembhavi", "A. Farhadi", "H. Hajishirzi."], "venue": "arXiv.", "citeRegEx": "Seo et al\\.,? 2016", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Reasonet: Learning to stop reading in machine comprehension", "author": ["Y. Shen", "P. Huang", "J. Gao", "W. Chen."], "venue": "International Conference on Knowledge Discovery and Data Mining (KDD).", "citeRegEx": "Shen et al\\.,? 2017", "shortCiteRegEx": "Shen et al\\.", "year": 2017}, {"title": "Adversarial evaluation for models of natural language", "author": ["N.A. Smith."], "venue": "arXiv preprint arXiv:1207.0245.", "citeRegEx": "Smith.,? 2012", "shortCiteRegEx": "Smith.", "year": 2012}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Szegedy et al\\.,? 2014", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Machine comprehension using match-LSTM and answer pointer", "author": ["S. Wang", "J. Jiang."], "venue": "arXiv preprint arXiv:1608.07905.", "citeRegEx": "Wang and Jiang.,? 2016", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Multi-perspective context matching for machine comprehension", "author": ["Z. Wang", "H. Mi", "W. Hamza", "R. Florian."], "venue": "arXiv.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Making neural qa as simple as possible but not simpler", "author": ["D. Weissenborn", "G. Wiese", "L. Seiffe."], "venue": "arXiv.", "citeRegEx": "Weissenborn et al\\.,? 2017", "shortCiteRegEx": "Weissenborn et al\\.", "year": 2017}, {"title": "End-to-end answer chunk extraction and ranking for reading comprehension", "author": ["Y. Yu", "W. Zhang", "K. Hasan", "M. Yu", "B. Xiang", "B. Zhou."], "venue": "arXiv.", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Exploring question understanding and adaptation in neural-network-based question answering", "author": ["J. Zhang", "X. Zhu", "Q. Chen", "L. Dai", "S. Wei", "H. Jiang."], "venue": "arXiv.", "citeRegEx": "Zhang et al\\.,? 2017", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 11, "context": "Quantifying the extent to which a computer system exhibits intelligent behavior is a longstanding problem in AI (Levesque, 2013).", "startOffset": 112, "endOffset": 128}, {"referenceID": 23, "context": "However, models can succeed in this paradigm by recognizing patterns that happen to be predictive on most of the test examples, while ignoring deeper, more difficult phenomena (Rimell et al., 2009; Paperno et al., 2016).", "startOffset": 176, "endOffset": 219}, {"referenceID": 19, "context": "However, models can succeed in this paradigm by recognizing patterns that happen to be predictive on most of the test examples, while ignoring deeper, more difficult phenomena (Rimell et al., 2009; Paperno et al., 2016).", "startOffset": 176, "endOffset": 219}, {"referenceID": 22, "context": "SQuAD reading comprehension task (Rajpurkar et al., 2016), in which systems answer questions about paragraphs from Wikipedia.", "startOffset": 33, "endOffset": 57}, {"referenceID": 27, "context": "Prior work in computer vision adds imperceptible adversarial perturbations to input images, relying on the fact that such small perturbations cannot change an image\u2019s true label (Szegedy et al., 2014; Goodfellow et al., 2015).", "startOffset": 178, "endOffset": 225}, {"referenceID": 7, "context": "Prior work in computer vision adds imperceptible adversarial perturbations to input images, relying on the fact that such small perturbations cannot change an image\u2019s true label (Szegedy et al., 2014; Goodfellow et al., 2015).", "startOffset": 178, "endOffset": 225}, {"referenceID": 22, "context": "The SQuAD dataset (Rajpurkar et al., 2016) contains 107,785 human-generated reading comprehension questions about Wikipedia articles.", "startOffset": 18, "endOffset": 42}, {"referenceID": 24, "context": "When developing and testing our methods, we focused on two published model architectures: BiDAF (Seo et al., 2016) and Match-LSTM (Wang and Jiang, 2016).", "startOffset": 96, "endOffset": 114}, {"referenceID": 28, "context": ", 2016) and Match-LSTM (Wang and Jiang, 2016).", "startOffset": 23, "endOffset": 45}, {"referenceID": 25, "context": "We also validate our major findings on twelve other published models with publicly available test-time code: ReasoNet Single and Ensemble versions (Shen et al., 2017), Mnemonic Reader Single and Ensemble versions (Hu et al.", "startOffset": 147, "endOffset": 166}, {"referenceID": 8, "context": ", 2017), Mnemonic Reader Single and Ensemble versions (Hu et al., 2017), Structural Embedding of Dependency Trees (SEDT) Single and Ensemble versions (Liu et al.", "startOffset": 54, "endOffset": 71}, {"referenceID": 13, "context": ", 2017), Structural Embedding of Dependency Trees (SEDT) Single and Ensemble versions (Liu et al., 2017), jNet (Zhang et al.", "startOffset": 86, "endOffset": 104}, {"referenceID": 32, "context": ", 2017), jNet (Zhang et al., 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version (Wang et al.", "startOffset": 14, "endOffset": 34}, {"referenceID": 5, "context": ", 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version (Wang et al.", "startOffset": 27, "endOffset": 50}, {"referenceID": 29, "context": ", 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version (Wang et al., 2016), RaSOR (Lee et al.", "startOffset": 108, "endOffset": 127}, {"referenceID": 10, "context": ", 2016), RaSOR (Lee et al., 2017), Dynamic Chunk Reader (DCR) (Yu et al.", "startOffset": 15, "endOffset": 33}, {"referenceID": 31, "context": ", 2017), Dynamic Chunk Reader (DCR) (Yu et al., 2016), and the Logistic Regression Baseline (Rajpurkar et al.", "startOffset": 36, "endOffset": 53}, {"referenceID": 22, "context": ", 2016), and the Logistic Regression Baseline (Rajpurkar et al., 2016).", "startOffset": 46, "endOffset": 70}, {"referenceID": 22, "context": "where v is the F1 score between the true answer a and the predicted answer f(p, q) (see Rajpurkar et al. (2016) for details).", "startOffset": 88, "endOffset": 112}, {"referenceID": 30, "context": "Weissenborn et al. (2017) argue that many SQuAD questions can be answered with heuristics based on type and keyword-matching.", "startOffset": 0, "endOffset": 26}, {"referenceID": 27, "context": "Images from Szegedy et al. (2014).", "startOffset": 12, "endOffset": 34}, {"referenceID": 27, "context": "In image classification, adversarial examples are commonly generated by adding an imperceptible amount of noise to the input (Szegedy et al., 2014; Goodfellow et al., 2015).", "startOffset": 125, "endOffset": 172}, {"referenceID": 7, "context": "In image classification, adversarial examples are commonly generated by adding an imperceptible amount of noise to the input (Szegedy et al., 2014; Goodfellow et al., 2015).", "startOffset": 125, "endOffset": 172}, {"referenceID": 15, "context": "For language, the direct analogue would be to paraphrase the input (Madnani and Dorr, 2010).", "startOffset": 67, "endOffset": 91}, {"referenceID": 3, "context": "We replace nouns and adjectives with antonyms from WordNet (Fellbaum, 1998), and change named entities and numbers to the nearest word in GloVe word vector space2 (Pennington et al.", "startOffset": 59, "endOffset": 75}, {"referenceID": 21, "context": "We replace nouns and adjectives with antonyms from WordNet (Fellbaum, 1998), and change named entities and numbers to the nearest word in GloVe word vector space2 (Pennington et al., 2014) with the same part of speech.", "startOffset": 163, "endOffset": 188}, {"referenceID": 16, "context": "of 26 types, corresponding to NER and POS tags from Stanford CoreNLP (Manning et al., 2014), plus a few custom categories (e.", "startOffset": 69, "endOffset": 91}, {"referenceID": 20, "context": "In contrast with prior work in computer vision (Papernot et al., 2017; Narodytska and Kasiviswanathan, 2016; Moosavi-Dezfooli et al., 2017), ADDONESENT does not require any access to the model or to any training data: it generates adversarial examples based solely on the intuition that existing models are overly stable.", "startOffset": 47, "endOffset": 139}, {"referenceID": 18, "context": "In contrast with prior work in computer vision (Papernot et al., 2017; Narodytska and Kasiviswanathan, 2016; Moosavi-Dezfooli et al., 2017), ADDONESENT does not require any access to the model or to any training data: it generates adversarial examples based solely on the intuition that existing models are overly stable.", "startOffset": 47, "endOffset": 139}, {"referenceID": 17, "context": "In contrast with prior work in computer vision (Papernot et al., 2017; Narodytska and Kasiviswanathan, 2016; Moosavi-Dezfooli et al., 2017), ADDONESENT does not require any access to the model or to any training data: it generates adversarial examples based solely on the intuition that existing models are overly stable.", "startOffset": 47, "endOffset": 139}, {"referenceID": 27, "context": "\u201cProbabilistic\u201d query access is still weaker than access to gradients, as is common in computer vision (Szegedy et al., 2014; Goodfellow et al., 2015).", "startOffset": 103, "endOffset": 150}, {"referenceID": 7, "context": "\u201cProbabilistic\u201d query access is still weaker than access to gradients, as is common in computer vision (Szegedy et al., 2014; Goodfellow et al., 2015).", "startOffset": 103, "endOffset": 150}, {"referenceID": 22, "context": "For all experiments, we measure adversarial F1 score (Rajpurkar et al., 2016) across 1000 randomly sampled examples from the SQuAD development set (the test set is not publicly available).", "startOffset": 53, "endOffset": 77}, {"referenceID": 8, "context": "It is noteworthy that the Mnemonic Reader models (Hu et al., 2017) outperform the other models by about 6 F1 points.", "startOffset": 49, "endOffset": 66}, {"referenceID": 27, "context": "In computer vision, adversarial examples that fool one model also tend to fool other models (Szegedy et al., 2014; Moosavi-Dezfooli et al., 2017); we investigate whether the same pattern holds for us.", "startOffset": 92, "endOffset": 145}, {"referenceID": 17, "context": "In computer vision, adversarial examples that fool one model also tend to fool other models (Szegedy et al., 2014; Moosavi-Dezfooli et al., 2017); we investigate whether the same pattern holds for us.", "startOffset": 92, "endOffset": 145}, {"referenceID": 4, "context": "For certain classes of models and adversaries, efficient training strategies exist: for example, Globerson and Roweis (2006) train classifiers that are optimally robust to adversarial feature deletion.", "startOffset": 97, "endOffset": 125}, {"referenceID": 24, "context": "6 All previous experiments used parameters released by Seo et al. (2016)", "startOffset": 55, "endOffset": 73}, {"referenceID": 7, "context": "versarial training (Goodfellow et al., 2015) can be used for any model trained with stochastic gradient descent, but it requires generating new adversarial examples at every iteration; this is feasible for images, where fast gradient-based adversaries exist, but is infeasible for domains where only slower adversaries are available.", "startOffset": 19, "endOffset": 44}, {"referenceID": 26, "context": "Generative adversarial models pit a generative model, whose goal is to generate realistic outputs, against a discriminative model, whose goal is to distinguish the generator\u2019s outputs from real data (Smith, 2012; Goodfellow et al., 2014).", "startOffset": 199, "endOffset": 237}, {"referenceID": 6, "context": "Generative adversarial models pit a generative model, whose goal is to generate realistic outputs, against a discriminative model, whose goal is to distinguish the generator\u2019s outputs from real data (Smith, 2012; Goodfellow et al., 2014).", "startOffset": 199, "endOffset": 237}, {"referenceID": 6, "context": "Generative adversarial models pit a generative model, whose goal is to generate realistic outputs, against a discriminative model, whose goal is to distinguish the generator\u2019s outputs from real data (Smith, 2012; Goodfellow et al., 2014). Bowman et al. (2016) and Li et al.", "startOffset": 213, "endOffset": 260}, {"referenceID": 6, "context": "Generative adversarial models pit a generative model, whose goal is to generate realistic outputs, against a discriminative model, whose goal is to distinguish the generator\u2019s outputs from real data (Smith, 2012; Goodfellow et al., 2014). Bowman et al. (2016) and Li et al. (2017) used such a setup for sentence and dialogue generation, respectively.", "startOffset": 213, "endOffset": 281}, {"referenceID": 2, "context": "Dalvi et al. (2004) formulated such tasks as a game between a classifier and an adversary, and analyzed optimal strategies for each player.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Dalvi et al. (2004) formulated such tasks as a game between a classifier and an adversary, and analyzed optimal strategies for each player. Lowd and Meek (2005) described an efficient attack by which an adversary can reverseengineer the weights of a linear classifier, in order to then generate adversarial inputs.", "startOffset": 0, "endOffset": 161}, {"referenceID": 15, "context": "As discussed previously, paraphrase generation systems (Madnani and Dorr, 2010) could be used for adversarial evaluation on a wide range of language tasks.", "startOffset": 55, "endOffset": 79}, {"referenceID": 9, "context": "We could also adversarially generate new examples by combining multiple existing ones, in the spirit of Data Recombination (Jia and Liang, 2016).", "startOffset": 123, "endOffset": 144}, {"referenceID": 0, "context": "The Build It, Break It shared task (Bender et al., 2017) encourages researchers to adversarially design minimal pairs to fool sentiment analysis and semantic role labeling systems.", "startOffset": 35, "endOffset": 56}, {"referenceID": 9, "context": "Levesque (2013) proposed the Winograd Schema challenge, in which computers must resolve coreference resolution problems that were handcrafted to require extensive world knowledge.", "startOffset": 0, "endOffset": 16}, {"referenceID": 9, "context": "Levesque (2013) proposed the Winograd Schema challenge, in which computers must resolve coreference resolution problems that were handcrafted to require extensive world knowledge. Paperno et al. (2016) constructed the LAMBADA dataset, which tests the ability of language models to handle long-range dependencies.", "startOffset": 0, "endOffset": 202}, {"referenceID": 9, "context": "Levesque (2013) proposed the Winograd Schema challenge, in which computers must resolve coreference resolution problems that were handcrafted to require extensive world knowledge. Paperno et al. (2016) constructed the LAMBADA dataset, which tests the ability of language models to handle long-range dependencies. Their method relies on the availability of a large initial dataset, from which they distill a difficult subset; such initial data may be unavailable for many tasks. Rimell et al. (2009) showed that dependency parsers that seem very accurate by standard metrics perform poorly on a subset of the test data that has unbounded dependency constructions.", "startOffset": 0, "endOffset": 499}], "year": 2017, "abstractText": "Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely.", "creator": "LaTeX with hyperref package"}}}