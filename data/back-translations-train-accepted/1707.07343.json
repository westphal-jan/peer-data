{"id": "1707.07343", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jul-2017", "title": "A Sequential Model for Classifying Temporal Relations between Intra-Sentence Events", "abstract": "We present a sequential model for temporal relation classification between intra-sentence events. The key observation is that the overall syntactic structure and compositional meanings of the multi-word context between events are important for distinguishing among fine-grained temporal relations. Specifically, our approach first extracts a sequence of context words that indicates the temporal relation between two events, which well align with the dependency path between two event mentions. The context word sequence, together with a parts-of-speech tag sequence and a dependency relation sequence that are generated corresponding to the word sequence, are then provided as input to bidirectional recurrent neural network (LSTM) models. The neural nets learn compositional syntactic and semantic representations of contexts surrounding the two events and predict the temporal relation between them. Evaluation of the proposed approach on TimeBank corpus shows that sequential modeling is capable of accurately recognizing temporal relations between events, which outperforms a neural net model using various discrete features as input that imitates previous feature based models.", "histories": [["v1", "Sun, 23 Jul 2017 20:47:02 GMT  (134kb,D)", "http://arxiv.org/abs/1707.07343v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["prafulla kumar choubey", "ruihong huang"], "accepted": true, "id": "1707.07343"}, "pdf": {"name": "1707.07343.pdf", "metadata": {"source": "CRF", "title": "A Sequential Model for Classifying Temporal Relations between Intra-Sentence Events", "authors": ["Prafulla Kumar Choubey", "Ruihong Huang"], "emails": ["huangrh)@tamu.edu"], "sections": [{"heading": null, "text": "The key observation is that the general syntactic structure and compositional meaning of the multiword context between events are important for distinguishing between fine-grained temporal relationships. Specifically, our approach first extracts a sequence of context words that indicate the temporal relationship between two events that correspond well to the dependence path between two event mentions, then provides the contextual word sequence as input into bi-directional recursive neural network models (LSTM), along with a sequence of language building blocks and a sequence of dependency relationships generated according to the word sequence. Neural networks learn compositional syntactic and semantic representations of contexts that surround the two events, and predict the temporal relationship between them. Evaluating the proposed approach based on TimeBank corpus shows that sequential neural modeling is capable of detecting the primordial relationships between events based on a primordial time model."}, {"heading": "1 Introduction", "text": "It has a direct application in tasks such as answering questions, generating events, and summarizing documents. Previous work examined this task as a classification problem based on discrete characteristics defined by lexicosyntactic, semantic, and discourse characteristics. However, these characteristics are often derived from local contexts of two events and are only able to capture direct evidence of the temporal relationship. In particular, when two events are far apart or separated by other events, compositional approaches often fail to use compositional evidence that is difficult to encrypt with discrete characteristics."}, {"heading": "2 Related Works", "text": "Most of the previous work on the classification of temporal relations is based on feature-based classifiers. Mani et al. (2006) built MaxEnt classifiers on hand-tagged characteristics in the corpus, including tension, aspect, modality, polarity, and event class to classify temporal relations. Later, Chambers et al. (2007) used a two-step classifier that first learned imperfect event attributes and then combined them with other linguistic characteristics in the second stage to perform the classification. Subsequent work largely extended the characteristics (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2013; Chambers, Laoculrat et al., 2013). Specifically, Chambers (2013) used the direct dependence path between events to grasp syntactical relationships."}, {"heading": "3 Temporal Link Labeling", "text": "In this section we describe the task of classifying temporal relations, the dataset, the context word sequence extraction model, and the recursive classifier used for neural networks."}, {"heading": "3.1 Task description", "text": "Early work on the classification of temporal relationships Mani et al. (2006); Chambers et al. (2007) and the first two versions of TempEval (Verhagen et al., 2007, 2010) simplified the task by looking at only six types of relationships; they combined the two types of relationships, which are the opposite of each other, and ignored the relationships during and during the inv. then TempEval-3 (Uzzaman et al., 2013) extended the task to the problem of 14-class classification, and all subsequent work looked at all 14 relationships. According to the most recent work, our model performs a 14-class classification, as this is arguably more difficult (Ng, 2013). In addition, we look at gold-annotated event pairs, mainly because the corpus is small and the distribution of relationships is very distorted. All previous work focusing on the problem of classifying temporal relationship types assumed a gold annotation."}, {"heading": "3.2 Dataset", "text": "The corpus consists of 14 temporal relationships between 2308 pairs of events located within the same set. These relationships (Saur\u0131 et al., 2006) are simultaneous before, after, ivor, inach, begins, begins, ends, ends, ends, encompasses, is recorded, during, during, during, inv, identity. Six pairs of these are inverse to each other and other two types are commutative (e1Re2 \u2261 e2Re1, R: identical, simultaneous). Our sequential model requires that the relationship should always be between e1 and e2, with e1 occurring before e2 in the set. Therefore, before extracting the sequence, we have reversed the relationship types in cases where the relationship type was noted in the opposite order. The final distribution of the data set is given in Table 1."}, {"heading": "3.3 Extracting Context Word Sequence", "text": "First, we extract words that are in the dependency path between two event mentions. However, pairs of events can be very far within a sentence and can be involved in complex syntactic structures. Therefore, we also apply two heuristic rules to handle complex syntactic structures, for example, two event mentions are contained in separate clauses and have punctuation marks in their context. Below, we describe our specific rules. We used the Stanford parser (Chen and Manning, 2014) to generate dependency relationships and parts of language blocks, and all notations follow extended universal dependencies (De Marneffe and Manning, 2008). Rule 1 (Punctuation): Comma directly affects the meaning of the text and can change the meaning of the phrase if omitted. Therefore, insert commas it precedes or follows e1, e2 or its modifications. Rule 2 (Children): Modifiers will now contain information about events as they were yesterday, if they are linked to events, etc."}, {"heading": "3.4 Sequences and Classifier", "text": "We build three sequences on the extracted context words (with t-words) based on (i) parts-of-speech tags: PT = p1, p2,..., pt (ii) dependency relationships: DT = d1, d2,..., dn2 and (iii) word forms: WT = w1, w2,..., wt.We transform each pi and di into a uniform vector and each wi into a pre-trained embedding vector (Pennington et al., 2014). Subsequently, each sequence is encoded by vectors using their corresponding forward (LSTMf) and backward (LSTMb) LSTM layers."}, {"heading": "4 Evaluation", "text": "In fact, it is a way in which people are able to understand the world and understand what they are doing in order to understand and understand it."}, {"heading": "4.1 Results and Discussion", "text": "Table 2 reports accuracy values for all systems. We see that simple sequential models exceed the strong characterization-based system, Baseline I, which uses various discrete characteristics. Note that dependence relations and POS tag sequences alone achieve a relatively high accuracy. This implies that an important aspect of the temporal relationship is contained in the syntactic context of event references. In addition, Mirza and Tonelli (2014) terms observed that discrete characteristics based on the parse tree of dependence do not contribute to improving the accuracy of their classifier. On the contrary, the use of the sequence of dependence relations results in a high accuracy in our setting, which means the advantages of the use of sequential representations for this task. Our full model achieves a performance gain of 11.35% over Baseline I. We developed two more Baseline (Baseline II and III), which do not require syntactical information, as well as the direct dependence model 07, respectively, that does not use rules, 57%, that."}, {"heading": "5 Conclusion and Future work", "text": "In this work, we have focused on modeling syntactic structural information and compositional semantics of contexts in predicting temporal relationships between events in the same sentence. Our approach extracts lexical and syntactic sequences from contexts between two events and feeds them into recurrent neural networks. Evaluation shows that our sequential models are promising in distinguishing fine-grained temporal relationships. In the future, we will expand our sequential models to predict temporal relationships for event pairs that extend across multiple sentences, for example by including discourse relationships between sentences in a sequence."}, {"heading": "Acknowledgments", "text": "We would like to thank our anonymous reviewers for their insightful review comments and suggestions, which have helped to make the reviews more comprehensive."}], "references": [{"title": "Cleartk-timeml: A minimalist approach to tempeval 2013", "author": ["Steven Bethard."], "venue": "Second Joint Conference on Lexical and Computational Semantics (* SEM), volume 2, pages 10\u201314.", "citeRegEx": "Bethard.,? 2013", "shortCiteRegEx": "Bethard.", "year": 2013}, {"title": "Cu-tmp: Temporal relation classification using syntactic and semantic features", "author": ["Steven Bethard", "James H Martin."], "venue": "Proceedings of the 4th International Workshop on Semantic Evaluations, pages 129\u2013132. Association for Computational Linguis-", "citeRegEx": "Bethard and Martin.,? 2007", "shortCiteRegEx": "Bethard and Martin.", "year": 2007}, {"title": "Navytime: Event and time ordering from raw text", "author": ["Nathanael Chambers."], "venue": "Technical report, DTIC Document.", "citeRegEx": "Chambers.,? 2013", "shortCiteRegEx": "Chambers.", "year": 2013}, {"title": "Classifying temporal relations between events", "author": ["Nathanael Chambers", "Shan Wang", "Dan Jurafsky."], "venue": "Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 173\u2013176. Association for Computa-", "citeRegEx": "Chambers et al\\.,? 2007", "shortCiteRegEx": "Chambers et al\\.", "year": 2007}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "EMNLP, pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Naist", "author": ["Yuchang Cheng", "Masayuki Asahara", "Yuji Matsumoto."], "venue": "japan: Temporal relation identification using dependency parsed tree. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 245\u2013248. Association for Com-", "citeRegEx": "Cheng et al\\.,? 2007", "shortCiteRegEx": "Cheng et al\\.", "year": 2007}, {"title": "Verbocean: Mining the web for fine-grained semantic verb relations", "author": ["Timothy Chklovski", "Patrick Pantel."], "venue": "EMNLP, volume 4, pages 33\u201340.", "citeRegEx": "Chklovski and Pantel.,? 2004", "shortCiteRegEx": "Chklovski and Pantel.", "year": 2004}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet."], "venue": "https://github. com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Stanford typed dependencies manual", "author": ["Marie-Catherine De Marneffe", "Christopher D Manning."], "venue": "Technical report, Technical report, Stanford University.", "citeRegEx": "Marneffe and Manning.,? 2008", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "Using signals to improve automatic classification of temporal relations", "author": ["Leon Derczynski", "Robert Gaizauskas."], "venue": "arXiv preprint arXiv:1203.5055.", "citeRegEx": "Derczynski and Gaizauskas.,? 2012", "shortCiteRegEx": "Derczynski and Gaizauskas.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Simple and accurate dependency parsing using bidirectional lstm feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "arXiv preprint arXiv:1603.04351.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Extracting narrative timelines as temporal dependency structures", "author": ["Oleksandr Kolomiyets", "Steven Bethard", "MarieFrancine Moens."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-", "citeRegEx": "Kolomiyets et al\\.,? 2012", "shortCiteRegEx": "Kolomiyets et al\\.", "year": 2012}, {"title": "Uttime: Temporal relation classification using deep syntactic features", "author": ["Natsuda Laokulrat", "Makoto Miwa", "Yoshimasa Tsuruoka", "Takashi Chikayama"], "venue": null, "citeRegEx": "Laokulrat et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Laokulrat et al\\.", "year": 2013}, {"title": "Machine learning of temporal relations", "author": ["Inderjeet Mani", "Marc Verhagen", "Ben Wellner", "Chong Min Lee", "James Pustejovsky."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting", "citeRegEx": "Mani et al\\.,? 2006", "shortCiteRegEx": "Mani et al\\.", "year": 2006}, {"title": "Classifying temporal relations with simple features", "author": ["Paramita Mirza", "Sara Tonelli."], "venue": "EACL, volume 14, pages 308\u2013317.", "citeRegEx": "Mirza and Tonelli.,? 2014", "shortCiteRegEx": "Mirza and Tonelli.", "year": 2014}, {"title": "Dependency parsing features for semantic parsing", "author": ["Will Monroe", "Yushi Wang"], "venue": null, "citeRegEx": "Monroe and Wang.,? \\Q2014\\E", "shortCiteRegEx": "Monroe and Wang.", "year": 2014}, {"title": "Exploiting discourse analysis for article-wide temporal classification", "author": ["Jun-Ping Ng", "Min-Yen Kan", "Ziheng Lin", "Vanessa Wei Feng", "Bin Chen", "Jian Su", "Chew Lim Tan."], "venue": "EMNLP, pages 12\u201323.", "citeRegEx": "Ng et al\\.,? 2013", "shortCiteRegEx": "Ng et al\\.", "year": 2013}, {"title": "Classifying temporal relations with rich linguistic knowledge", "author": ["Vincent Ng"], "venue": null, "citeRegEx": "Ng.,? \\Q2013\\E", "shortCiteRegEx": "Ng.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP, volume 14, pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Transforming dependency structures to logical forms for semantic parsing", "author": ["Siva Reddy", "Oscar T\u00e4ckstr\u00f6m", "Michael Collins", "Tom Kwiatkowski", "Dipanjan Das", "Mark Steedman", "Mirella Lapata."], "venue": "Transactions of the Association for Computational", "citeRegEx": "Reddy et al\\.,? 2016", "shortCiteRegEx": "Reddy et al\\.", "year": 2016}, {"title": "Timeml annotation guidelines", "author": ["Roser Saur\u0131", "Jessica Littman", "Bob Knippen", "Robert Gaizauskas", "Andrea Setzer", "James Pustejovsky"], "venue": null, "citeRegEx": "Saur\u0131 et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Saur\u0131 et al\\.", "year": 2006}, {"title": "Tempeval-3: Evaluating events, time expressions, and temporal relations", "author": ["Naushad UzZaman", "Hector Llorens", "James Allen", "Leon Derczynski", "Marc Verhagen", "James Pustejovsky."], "venue": "arXiv preprint arXiv:1206.5333.", "citeRegEx": "UzZaman et al\\.,? 2012", "shortCiteRegEx": "UzZaman et al\\.", "year": 2012}, {"title": "Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations", "author": ["Naushad Uzzaman", "Hector Llorens", "Leon Derczynski", "Marc Verhagen", "James Allen", "James Pustejovsky"], "venue": null, "citeRegEx": "Uzzaman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uzzaman et al\\.", "year": 2013}, {"title": "Semeval-2007 task 15: Tempeval temporal relation identification", "author": ["Marc Verhagen", "Robert Gaizauskas", "Frank Schilder", "Mark Hepple", "Graham Katz", "James Pustejovsky."], "venue": "Proceedings of the 4th International Workshop on Semantic Evaluations,", "citeRegEx": "Verhagen et al\\.,? 2007", "shortCiteRegEx": "Verhagen et al\\.", "year": 2007}, {"title": "Semeval-2010 task 13: Tempeval-2", "author": ["Marc Verhagen", "Roser Sauri", "Tommaso Caselli", "James Pustejovsky."], "venue": "Proceedings of the 5th international workshop on semantic evaluation, pages 57\u201362. Association for Computational Linguistics.", "citeRegEx": "Verhagen et al\\.,? 2010", "shortCiteRegEx": "Verhagen et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 14, "context": "Mani et al. (2006) built MaxEnt classifier on hand-tagged features in the corpus, including tense, aspect, modality, polarity and event class for", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "Later Chambers et al. (2007) used a two-stage classifier which first learned imperfect event attributes and then combined them with other linguistic features in the second stage to perform the classification.", "startOffset": 6, "endOffset": 29}, {"referenceID": 5, "context": "ture sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013).", "startOffset": 10, "endOffset": 158}, {"referenceID": 1, "context": "ture sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013).", "startOffset": 10, "endOffset": 158}, {"referenceID": 22, "context": "ture sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013).", "startOffset": 10, "endOffset": 158}, {"referenceID": 0, "context": "ture sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013).", "startOffset": 10, "endOffset": 158}, {"referenceID": 12, "context": "ture sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013).", "startOffset": 10, "endOffset": 158}, {"referenceID": 2, "context": "ture sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013).", "startOffset": 10, "endOffset": 158}, {"referenceID": 13, "context": "ture sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013).", "startOffset": 10, "endOffset": 158}, {"referenceID": 0, "context": ", 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013). Specifically, Chambers (2013) used direct dependency path between event pairs", "startOffset": 8, "endOffset": 167}, {"referenceID": 13, "context": "Laokulrat et al. (2013) used 3-grams of paths between two event mentions in a dependency tree as features instead of full paths as those are too sparse.", "startOffset": 0, "endOffset": 24}, {"referenceID": 16, "context": "Ng et al. (2013) also showed the effectiveness of different discourse analysis frameworks for this task.", "startOffset": 0, "endOffset": 17}, {"referenceID": 15, "context": "Later Mirza and Tonelli (2014) showed that a simpler approach based on lexico-syntactic features achieved results comparable to Ng (2013).", "startOffset": 6, "endOffset": 31}, {"referenceID": 15, "context": "Later Mirza and Tonelli (2014) showed that a simpler approach based on lexico-syntactic features achieved results comparable to Ng (2013). They also reported that dependency order between events, either governordependent or dependent-governor, was not useful in their experiments.", "startOffset": 6, "endOffset": 138}, {"referenceID": 12, "context": "Early works on temporal relation classification Mani et al. (2006); Chambers et al.", "startOffset": 48, "endOffset": 67}, {"referenceID": 2, "context": "(2006); Chambers et al. (2007) and the first two versions of TempEval (Verhagen et al.", "startOffset": 8, "endOffset": 31}, {"referenceID": 23, "context": "Then TempEval-3 (Uzzaman et al., 2013) extended the task to complete 14 class classification problem and all later works have considered", "startOffset": 16, "endOffset": 38}, {"referenceID": 18, "context": "Our model performs 14-class classification following the recent works, as this is arguably more challenging (Ng, 2013).", "startOffset": 108, "endOffset": 118}, {"referenceID": 21, "context": "relations (Saur\u0131 et al., 2006) are simultaneous,", "startOffset": 10, "endOffset": 30}, {"referenceID": 19, "context": "We transform each pi and di to a one-hot vector and each wi to a pre-trained embedding vector (Pennington et al., 2014).", "startOffset": 94, "endOffset": 119}, {"referenceID": 7, "context": "entropy (Chollet, 2015) .", "startOffset": 8, "endOffset": 23}, {"referenceID": 6, "context": "The features used are: POS tag, dependency relation, token and lemma of e1(e2); dependency relations between e1(e2) and their children; binary features indicating if e1 and e2 are related with the \u2019happensbefore\u2019 or the \u2019similar\u2019 relation according to VerbOcean (Chklovski and Pantel, 2004), if e1 and e2 have the same POS tag, or if e1(e2) is the root and e1 modifies (or governs) e2; the dependency relation between e1 and e2 if they are directly connected in the dependency parse tree; prepositions that modify (or govern) e1(e2); signal words (Derczynski and Gaizauskas, 2012) and entity distance between e1 and e2.", "startOffset": 262, "endOffset": 290}, {"referenceID": 9, "context": "The features used are: POS tag, dependency relation, token and lemma of e1(e2); dependency relations between e1(e2) and their children; binary features indicating if e1 and e2 are related with the \u2019happensbefore\u2019 or the \u2019similar\u2019 relation according to VerbOcean (Chklovski and Pantel, 2004), if e1 and e2 have the same POS tag, or if e1(e2) is the root and e1 modifies (or governs) e2; the dependency relation between e1 and e2 if they are directly connected in the dependency parse tree; prepositions that modify (or govern) e1(e2); signal words (Derczynski and Gaizauskas, 2012) and entity distance between e1 and e2.", "startOffset": 547, "endOffset": 580}, {"referenceID": 16, "context": "Tonelli (2014); Ng (2013). The features used are: POS tag, dependency relation, token and lemma of e1(e2); dependency relations between e1(e2) and their children; binary features indicating if e1 and e2 are related with the \u2019happensbefore\u2019 or the \u2019similar\u2019 relation according to VerbOcean (Chklovski and Pantel, 2004), if e1 and e2 have the same POS tag, or if e1(e2) is the root and e1 modifies (or governs) e2; the dependency relation between e1 and e2 if they are directly connected in the dependency parse tree; prepositions that modify (or govern) e1(e2); signal words (Derczynski and Gaizauskas, 2012) and entity distance between e1 and e2.", "startOffset": 16, "endOffset": 26}, {"referenceID": 11, "context": "Baseline III: a neural network classifier based on event embeddings for both event mentions that were learned using bidirectional LSTMs (Kiperwasser and Goldberg, 2016).", "startOffset": 136, "endOffset": 168}, {"referenceID": 15, "context": "Moreover, Mirza and Tonelli (2014) observed that discrete features based on depen-", "startOffset": 10, "endOffset": 35}], "year": 2017, "abstractText": "We present a sequential model for temporal relation classification between intrasentence events. The key observation is that the overall syntactic structure and compositional meanings of the multi-word context between events are important for distinguishing among fine-grained temporal relations. Specifically, our approach first extracts a sequence of context words that indicates the temporal relation between two events, which well align with the dependency path between two event mentions. The context word sequence, together with a parts-of-speech tag sequence and a dependency relation sequence that are generated corresponding to the word sequence, are then provided as input to bidirectional recurrent neural network (LSTM) models. The neural nets learn compositional syntactic and semantic representations of contexts surrounding the two events and predict the temporal relation between them. Evaluation of the proposed approach on TimeBank corpus shows that sequential modeling is capable of accurately recognizing temporal relations between events, which outperforms a neural net model using various discrete features as input that imitates previous feature based models.", "creator": "LaTeX with hyperref package"}}}