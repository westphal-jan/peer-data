{"id": "1707.07402", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jul-2017", "title": "Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback", "abstract": "Machine translation is a natural candidate problem for reinforcement learning from human feedback: users provide quick, dirty ratings on candidate translations to guide a system to improve. Yet, current neural machine translation training focuses on expensive human-generated reference translations. We describe a reinforcement learning algorithm that improves neural machine translation systems from simulated human feedback. Our algorithm combines the advantage actor-critic algorithm (Mnih et al., 2016) with the attention-based neural encoder-decoder architecture (Luong et al., 2015). This algorithm (a) is well-designed for problems with a large action space and delayed rewards, (b) effectively optimizes traditional corpus-level machine translation metrics, and (c) is robust to skewed, high-variance, granular feedback modeled after actual human behaviors.", "histories": [["v1", "Mon, 24 Jul 2017 04:35:19 GMT  (1008kb,D)", "http://arxiv.org/abs/1707.07402v1", "11 pages, 5 figures, In Proceedings of Empirical Methods in Natural Language Processing (EMNLP) 2017"], ["v2", "Tue, 1 Aug 2017 17:19:01 GMT  (1008kb,D)", "http://arxiv.org/abs/1707.07402v2", "11 pages, 5 figures, In Proceedings of Empirical Methods in Natural Language Processing (EMNLP) 2017"], ["v3", "Fri, 13 Oct 2017 06:10:55 GMT  (1008kb,D)", "http://arxiv.org/abs/1707.07402v3", "11 pages, 5 figures, In Proceedings of Empirical Methods in Natural Language Processing (EMNLP) 2017"]], "COMMENTS": "11 pages, 5 figures, In Proceedings of Empirical Methods in Natural Language Processing (EMNLP) 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.HC cs.LG", "authors": ["khanh nguyen", "hal daum\u00e9 iii", "jordan l boyd-graber"], "accepted": true, "id": "1707.07402"}, "pdf": {"name": "1707.07402.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback", "authors": ["Khanh Nguyen", "Jordan Boyd-Graber"], "emails": ["kxnguyen@umiacs.umd.edu", "hal@umiacs.umd.edu", "jbg@umiacs.umd.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is as if it is a matter of a way in which people are able to survive themselves, and in which people are able to survive themselves. (...) In fact, it is as if they are able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves."}, {"heading": "2 Bandit Machine Translation", "text": "The problem of structured bandit predictions (Chang et al., 2015; Sokolov et al., 2015) is an extension of the problem of contextual bandits (Kakade et al., 2008; Langford et al., 2008) to structured predictions. The structured prediction of bandits works over time i = 1.. K as: 1. World reveals context x (i) 2. Algorithm says structured predictions y (i) 3. World reveals reward R (i), x (i)). We consider the problem of learning human evaluations in a structured prediction frame for bandits. In each turn, a translation model receives a source sentence x (i), produces a translation y (i) and receives a rating R (i), x (i))))) from a human that reflects the qual-ity of translation."}, {"heading": "3 Effective Algorithm for Bandit MT", "text": "This section describes the architecture of the neural machine translation of our system (\u00a7 3.1). We formulate Bandit Neural Machine Translation as a reinforcing learning problem (\u00a7 3.2) and discuss why standard algorithms of the actor-critic struggle with this problem (\u00a7 3.3). Finally, we describe a more effective training approach based on the advantage of the actor-critic algorithm (\u00a7 3.4)."}, {"heading": "3.1 Neural machine translation", "text": "Our Neural Machine Translation (NMT) model is a neural encoder decoder that directly calculates the probability of translating a target sentence y = (y1, \u00b7 \u00b7 \u00b7, ym) from the source sentence x: PTB (y | x) = 1 PTB (yt | y < t, x) (1), where PTB (yt | y < t, x) is the probability of issuing the next word yt in due course step t with a translation prefix y < t and a source sentence x.We use an encoder decoder NMT architecture with global attention (Luong et al., 2015) where both encoders and decoders are recursive neural networks (RNN) (see Appendix A for a more detailed description)."}, {"heading": "3.2 Bandit NMT as Reinforcement Learning", "text": "The action space is the vocabulary of the target language. To generate a translation from a source sentence x, an NMT model must start from an initial state hdec0 (< 1: a representation of x, calculated by the encoder). < 1: a representation of x, calculated by the encoder. < 1: the model decides the next action by defining a stochastic political sentence x. (yt | y < t, x), which is directly parameterized by the parameters of the model. < 2: a policy that takes the current state hdect \u2212 1 as input and generates a probability distribution over all actions (target vocabulary words). The next action y y y y y y y y is chosen by finding out arg max or sampling from this distribution."}, {"heading": "3.3 Why are actor-critic algorithms not effective for bandit NMT?", "text": "The actor-critic algorithm (Konda and Tsitsiklis) uses function approximation to directly model the Q function, which is called the critic model. In our early experiments on Bandit NMT, we have the actor-critic algorithm for NMT in Bahdanau et al. (2017), which uses the algorithm in a supervised learning environment. Specifically, while an encoder decoder decoder critic models, as a replacement for the TrueQ function in Eq 3, it allows full expectation (because the critic model can be queried with any state action pair), we cannot achieve reasonable results with this approach."}, {"heading": "3.4 Advantage Actor-Critic for Bandit NMT", "text": "We follow the approach of the Advantage Actuator Critics (Mnih et al., 2016, A2C) and combine it with the neural encoder decoder architecture. The resulting algorithm - which we call NED-A2C - approaches the gradient in Eq 3 by a single sample y-Q (\u00b7 x value) and centers the reward R (y value) using the government's expected future reward V (y value) < t) to reduce the variance. ("}, {"heading": "4 Modeling Imperfect Ratings", "text": "Our goal is to establish the feasibility of using real human feedback to optimize a machine translation system in an environment where you can collect expert feedback and only collect expert feedback. In all cases, we consider expert feedback to be the \"gold standard\" that we want to optimize. In order to determine the feasibility of learning from human feedback without conducting a full, costly user study, we start with a simulation study. The key aspects (Figure 2) of the human feedback we collect are: (a) mismatch between training goal and feedback-maximizing goal, (b) human feedback is typically bundled (\u00a7 4.1), (c) individual human feedback exhibits a high deviation (\u00a7 4.2), and (d) non-expert feedback may be distorted with respect to expert feedback (\u00a7 4.3).In our EU simulation study, we begin with gold modeling of BL3."}, {"heading": "4.1 Humans Provide Granular Feedback", "text": "A classic example is the Likert scale for human agreement (Likert, 1932) or star ratings for product ratings. By insisting that human judges provide continuous values (or feedback with too fine a granularity), we can demotivate raters without improving the rating quality (Preston and Colman, 2000).To model granular feedback, we use a simple rounding process. For example, for an integer parameter g for the degree of granularity, we define: Pertgran (s; g) = 1g round (gs; g) (8) This error function divides the range of possible results into g + 1 containers. Since most set BLEU values are much closer to zero than to one, many of the larger containers are often empty."}, {"heading": "4.2 Experts Have High Variance", "text": "A natural goal for a human commentator variance model is to simulate - as accurately as possible - how human commentators actually perform. We use human valuation data recently collected as part of the joint WMT task (Graham et al., 2017).The data consists of 7200 sentences, which are multiplied by giving non-knowledgeable commentators on Amazon Mechanical Turk a reference sentence and a single system translation and asking commentators to assess the adequacy of the translation.4 From this data, we treat the average human rating as a basic truth and consider how the individual human ratings differ around this mean. To illustrate these results with grain density estimates (normal cores) of the standard deviation, Figure 3 shows the mean rating (x-axis) and the deviation of human ratings (y-axis) in any meaning. 5As expected, the standard deviation is small at the extremes and large at the center (this is an interbounded Wvy axis, with all 64 being a mean deviation)."}, {"heading": "4.3 Non-Experts are Skewed from Experts", "text": "The two previous models assume that the reward models exactly the value we want to optimize (has the same mean), which may not be the case with ratings by non-expertise.4Typical machine translation ratings rate pairs and ask commentators to choose which value is better.5A current limitation of this model is that the simulated noise i.e. depends on the rating (homoscedastic noise).This is a stronger and more realistic model than if no noise is assumed, but real noise is likely to be heteroscedastic: depending on input. 0 20 40 60 80 100 sentence level avg rating020406080 100hu human rats (1 -1 00) + - o ne s td de vmean \u00b1 stddev linear fit0 20 40 60 80 100 sentence level avg rating02040 60 sentence-0 etc."}, {"heading": "5 Experimental Setup", "text": "We select two language pairs from different language families with different typological characteristics: German-English and (De-En) and Chinese-English (Zh-En). For these language pairs, we use parallel transcriptions of TED conversations from the IWSLT 2014 and 2015 machine translation track (Cettolo et al., 2014, 2015, 2012). For each language pair, we divide its data into four sentences for supervised training, bandit training, development and examination (Table 1). For English and German, we compile and purify sentences with Moses (Koehn et al., 2007). For Chinese, we use the Chinese word segmentator (Chang et al., 2008) to segment sentences and tokenize. We remove all sentences with a length of more than 50, resulting in an average sentence length of 18. We use IWSLT 2015 data for supervised training and development, IWSLT 2014 data for test training and previous evaluation dates."}, {"heading": "5.1 Evaluation Framework", "text": "For each task, we first use the monitored training set to pre-train a reference model for NMT based on monitored learning. On the same training set, we also train the critic model with translations we took from the pre-trained NMT model. Next, we get into a bandit learning mode in which our models only note the source sentences of the bandit training set. Unless otherwise specified, we train the NMT models with NED-A2C for a passage through the bandit training set. If a fault function is applied to the set BLEU, it is applied only at this stage, not at the pre-education stage. We measure the improvement of a score metric S based on bandit training: \u2206 S = SA2C \u2212 Sref, where Sref is the score metric calculated at the reference level BLEU score, and SA2C is the metric that is calculated using models that have NED-2C as our main interaction."}, {"heading": "5.2 Model configuration", "text": "Both the NMT model and the Critic model are encoder decoder models with global attention (Luong et al., 2015). The encoder and the decoder are unidirectional single-layer LSTMs. They have the same word embedding size and the hidden LSTM size of 500. The source and target vocabulary size are both 50K. We do not use dropouts in our experiments. We train our models through the Adam Optimizer (Kingma and Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.999 and a lot size of 64. For Adam's \u03b1 hyperparameters, we use 10 \u2212 3 during pre-training and 10 \u2212 4 during bandit learning (both for the NMT model and the Critic model). During pre-training, we decay by a factor of 0.5 from the fifth pass if the perplexity on the development set increases. The NMT model reaches its highest corpus level after the data model A is pre-aggregated for the 4th run."}, {"heading": "6 Results and Analysis", "text": "In this section we describe the results of our experiments, divided into the following questions: how NED-A2C improves reference models (\u00a7 6.1); what influence the three perturbations have on the algorithm (\u00a7 6.2); and whether the algorithm improves a corpus-level metric that agrees well with human judgments (\u00a7 6.3)."}, {"heading": "6.1 Effectiveness of NED-A2C under Un-perturbed Bandit Feedback", "text": "We evaluate our methodology in an ideal environment where undisturbed per-sentence BLEU evaluates the evaluations during training and evaluation (Table 2).In this environment, our models only observe each source set once and before creating its translation. Political gradient algorithms have difficulty in improving problems with a large scope for action, as they use model-based explorations that are ineffective when most actions have the same probabilities (Bahdanau et al., Ranzato et al., to see if NED-A2C can solve the problems with a large scope for action)."}, {"heading": "6.2 Effect of Perturbed Bandit Feedback", "text": "We use disturbance functions defined in \u00a7 4.1 on per-set BLEU values, and use the disturbed values as a reward during Bandit training (Figure 5). Granular Rewards. We discredit the raw PerSentence BLEU values with Pertgran (s; g) (\u00a7 4.1). We vary g from one to ten (number of containers varies from two to eleven). Compared to continuous rewards for both language pairs, the BLEU values per set BLEU are not affected by at least five (at least six containers). As the granularity decreases, the BLEU values are monotonically degraded. However, even if g = 1 (values either 0 or 1), the models still improve by at least one point. We simulate the loud rewards using the human rating variance model pertvar (s) (\u00a7 4.2) with its models."}, {"heading": "6.3 Held-out Translation Quality", "text": "Our method also improves pre-trained models in Heldout BLEU, a measure that correlates better with translation quality than Per-Sentence BLEU (Table 2). When values are disturbed by our rating model, we observe patterns similar to those of PerSentence BLEU: models are resilient to most interference except when values are very coarse or very hard or have very high variance (Figure 5, second row). Supervised learning improves Heldout BLEU better, possibly because maximizing the log probability of reference translations correlates more closely with maximizing Heldout BLEU of predicted translations than maximizing Per-Sentence BLEU of predicted translations."}, {"heading": "7 Related Work and Discussion", "text": "Ranzato et al. (2016) describe MIXER as effective learning signals for machines. Reinforcement learning has become the de facto standard for incorporating this feedback into various tasks such as voice control of robots (Tenorio-Gonzalez et al., 2010), myoelectric control (Pilarski et al., 2011) and virtual assistants (Isbell et al., 2001). Recently, this learning framework has been combined with recurring neural networks to solve machine translation (Bahdanau et al., 2017), dialogue generation (Li et al., 2016), neural architecture search (Zoph and Le, 2017), and device placement (Mirhoseini et al., 2017). Other approaches for more general structured prediction among bandit feedback (Chang et al., 2015; Sokolov et al., 2016a, b) show the broader effectiveness of this model."}, {"heading": "Acknowledgements", "text": "Many thanks to Yvette Graham for her help with the WMT Human Evaluation Data. We thank the UMD CLIP lab members for useful discussions that led to the ideas behind this paper. We also thank the anonymous reviewers for their thorough and insightful comments. This work was supported by NSF grants IIS-1320538. Boyd-Graber is also partially supported by NSF grants IIS-1409287, IIS1564275, IIS-IIS-1652666 and NCSE-1422492. Daume \u0301 III is also supported by NSF grants IIS1618193 and an Amazon Research Award."}, {"heading": "A Neural MT Architecture", "text": "Our Neural Machine Translation Model (NMT) consists of an encoder and a decoder, each of which is a recursive neural network (RNN). We closely follow (Luong et al., 2015) the structure of our model. It directly models the posterior distribution P\u03b8 (y | x) of the translation of a source sen-tence x = (x1, \u00b7, xn) to a target sentence y = (y1, \u00b7, ym): P\u03b8 (y | x) = m = 1 P\u03b8 (y | x) of the translation of a source sen-tence x = (11), where y < t precedes all tokens in the target sentence yt.Each local disitribution P\u03b8 (yy < t) is modelled as a multinomial distribution via the vocabulary of the target language."}], "references": [{"title": "Impact of data characteristics on recommender systems performance", "author": ["Gediminas Adomavicius", "Jingjing Zhang."], "venue": "ACM Transactions on Management Information Systems (TMIS) 3(1):3.", "citeRegEx": "Adomavicius and Zhang.,? 2012", "shortCiteRegEx": "Adomavicius and Zhang.", "year": 2012}, {"title": "Active learning in heteroscedastic noise", "author": ["Andr\u00e1s Antos", "Varun Grover", "Csaba Szepesv\u00e1ri."], "venue": "Theoretical Computer Science 411(29-30):2712\u20132728.", "citeRegEx": "Antos et al\\.,? 2010", "shortCiteRegEx": "Antos et al\\.", "year": 2010}, {"title": "An actor-critic algorithm for sequence prediction", "author": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2017", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2017}, {"title": "Wit: Web inventory of transcribed and translated talks", "author": ["Mauro Cettolo", "Christian Girardi", "Marcello Federico."], "venue": "Conference of the European Association for Machine Translation (EAMT). Trento, Italy.", "citeRegEx": "Cettolo et al\\.,? 2012", "shortCiteRegEx": "Cettolo et al\\.", "year": 2012}, {"title": "The IWSLT 2015 evaluation campaign", "author": ["Mauro Cettolo", "Jan Niehues", "Sebastian St\u00fcker", "Luisa Bentivogli", "Roldano Cattoni", "Marcello Federico."], "venue": "International Workshop on Spoken Language Translation (IWSLT).", "citeRegEx": "Cettolo et al\\.,? 2015", "shortCiteRegEx": "Cettolo et al\\.", "year": 2015}, {"title": "Report on the 11th IWSLT evaluation campaign, IWSLT 2014", "author": ["Mauro Cettolo", "Jan Niehues", "Sebastian St\u00fcker", "Luisa Bentivogli", "Marcello Federico."], "venue": "International Workshop on Spoken Language Translation (IWSLT).", "citeRegEx": "Cettolo et al\\.,? 2014", "shortCiteRegEx": "Cettolo et al\\.", "year": 2014}, {"title": "Learning to search better than your teacher", "author": ["Kai-Wei Chang", "Akshay Krishnamurthy", "Alekh Agarwal", "Hal Daum\u00e9 III", "John Langford."], "venue": "Proceedings of the International Conference on Machine Learning (ICML).", "citeRegEx": "Chang et al\\.,? 2015", "shortCiteRegEx": "Chang et al\\.", "year": 2015}, {"title": "Optimizing chinese word segmentation for machine translation performance", "author": ["Pi-Chuan Chang", "Michel Galley", "Chris Manning."], "venue": "Workshop on Machine Translation.", "citeRegEx": "Chang et al\\.,? 2008", "shortCiteRegEx": "Chang et al\\.", "year": 2008}, {"title": "A systematic comparison of smoothing techniques for sentencelevel bleu", "author": ["Boxing Chen", "Colin Cherry."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Chen and Cherry.,? 2014", "shortCiteRegEx": "Chen and Cherry.", "year": 2014}, {"title": "Can machine translation systems be evaluated by the crowd alone", "author": ["Yvette Graham", "Timothy Baldwin", "Alistair Moffat", "Justin Zobel."], "venue": "Natural Language Engineering 23(1):3\u201330.", "citeRegEx": "Graham et al\\.,? 2017", "shortCiteRegEx": "Graham et al\\.", "year": 2017}, {"title": "Don\u2019t until the final verb wait: Reinforcement learning for simultaneous machine translation", "author": ["Alvin Grissom II", "He He", "Jordan Boyd-Graber", "John Morgan", "Hal Daum\u00e9 III."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "II et al\\.,? 2014", "shortCiteRegEx": "II et al\\.", "year": 2014}, {"title": "Interpretese vs", "author": ["He He", "Jordan Boyd-Graber", "Hal Daum\u00e9 III."], "venue": "translationese: The uniqueness of human strategies in simultaneous interpretation. In Conference of the North American Chapter of the Association for Computational Linguistics", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Syntax-based rewriting for simultaneous machine translation", "author": ["He He", "Alvin Grissom II", "Jordan Boyd-Graber", "Hal Daum\u00e9 III."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Explaining collaborative filtering recommendations", "author": ["Jonathan L Herlocker", "Joseph A Konstan", "John Riedl."], "venue": "ACM Conference on Computer Supported Cooperative Work.", "citeRegEx": "Herlocker et al\\.,? 2000", "shortCiteRegEx": "Herlocker et al\\.", "year": 2000}, {"title": "Crowdsourced monolingual translation", "author": ["Chang Hu", "Philip Resnik", "Benjamin B Bederson."], "venue": "ACM Transactions on Computer-Human Interaction (TOCHI) 21(4):22.", "citeRegEx": "Hu et al\\.,? 2014", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "A social reinforcement learning agent", "author": ["Charles Isbell", "Christian R Shelton", "Michael Kearns", "Satinder Singh", "Peter Stone."], "venue": "International Conference on Autonomous Agents (AA).", "citeRegEx": "Isbell et al\\.,? 2001", "shortCiteRegEx": "Isbell et al\\.", "year": 2001}, {"title": "Efficient bandit algorithms for online multiclass prediction", "author": ["Sham M Kakade", "Shai Shalev-Shwartz", "Ambuj Tewari."], "venue": "International Conference on Machine learning (ICML).", "citeRegEx": "Kakade et al\\.,? 2008", "shortCiteRegEx": "Kakade et al\\.", "year": 2008}, {"title": "Most likely heteroscedastic gaussian process regression", "author": ["Kristian Kersting", "Christian Plagemann", "Patrick Pfaff", "Wolfram Burgard."], "venue": "International Conference on Machine Learning (ICML).", "citeRegEx": "Kersting et al\\.,? 2007", "shortCiteRegEx": "Kersting et al\\.", "year": 2007}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Bandit structured prediction for neural sequence-to-sequence learning", "author": ["Julia Kreutzer", "Artem Sokolov", "Stefan Riezler."], "venue": "Association of Computational Linguistics (ACL).", "citeRegEx": "Kreutzer et al\\.,? 2017", "shortCiteRegEx": "Kreutzer et al\\.", "year": 2017}, {"title": "The epochgreedy algorithm for multi-armed bandits with side information", "author": ["John Langford", "Tong Zhang."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Langford and Zhang.,? 2008", "shortCiteRegEx": "Langford and Zhang.", "year": 2008}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Michel Galley", "Jianfeng Gao", "Dan Jurafsky."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "A technique for the measurement of attitudes", "author": ["Rensis Likert."], "venue": "Archives of Psychology 22(140):1\u201355.", "citeRegEx": "Likert.,? 1932", "shortCiteRegEx": "Likert.", "year": 1932}, {"title": "A strategy-aware technique for learning behaviors from discrete human feedback", "author": ["Robert Loftin", "James MacGlashan", "Michael L Littman", "Matthew E Taylor", "David L Roberts."], "venue": "Technical report, North Carolina State University. Dept. of Computer", "citeRegEx": "Loftin et al\\.,? 2014", "shortCiteRegEx": "Loftin et al\\.", "year": 2014}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Device placement optimization with reinforcement learning", "author": ["Azalia Mirhoseini", "Hieu Pham", "Quoc V Le", "Benoit Steiner", "Rasmus Larsen", "Yuefeng Zhou", "Naveen Kumar", "Mohammad Norouzi", "Samy Bengio", "Jeff Dean."], "venue": "International Conference", "citeRegEx": "Mirhoseini et al\\.,? 2017", "shortCiteRegEx": "Mirhoseini et al\\.", "year": 2017}, {"title": "Motivating personality-aware machine translation", "author": ["Shachar Mirkin", "Scott Nowson", "Caroline Brun", "Julien Perez."], "venue": "The 2015 Conference on Empirical Methods on Natural Language Processing (EMNLP).", "citeRegEx": "Mirkin et al\\.,? 2015", "shortCiteRegEx": "Mirkin et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu."], "venue": "International Conference on Ma-", "citeRegEx": "Mnih et al\\.,? 2016", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Estimation with heteroscedastic error terms", "author": ["Rolla E Park."], "venue": "Econometrica 34(4):888.", "citeRegEx": "Park.,? 1966", "shortCiteRegEx": "Park.", "year": 1966}, {"title": "The benefits of a model of annotation", "author": ["Rebecca J Passonneau", "Bob Carpenter."], "venue": "Transactions of the Association for Computational Linguistics (TACL) 2:311\u2013326.", "citeRegEx": "Passonneau and Carpenter.,? 2014", "shortCiteRegEx": "Passonneau and Carpenter.", "year": 2014}, {"title": "Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning", "author": ["Patrick M Pilarski", "Michael R Dawson", "Thomas Degris", "Farbod Fahimi", "Jason P Carey", "Richard S Sutton."], "venue": "IEEE International Conference on", "citeRegEx": "Pilarski et al\\.,? 2011", "shortCiteRegEx": "Pilarski et al\\.", "year": 2011}, {"title": "Optimal number of response categories in rating scales: reliability, validity, discriminating power, and respondent preferences", "author": ["Carolyn C Preston", "Andrew M Colman."], "venue": "Acta Psychologica 104(1):1\u2013", "citeRegEx": "Preston and Colman.,? 2000", "shortCiteRegEx": "Preston and Colman.", "year": 2000}, {"title": "Personalized machine translation: Preserving original author traits", "author": ["Ella Rabinovich", "Shachar Mirkin", "Raj Nath Patel", "Lucia Specia", "Shuly Wintner."], "venue": "Association for Computational Linguistics (ACL) .", "citeRegEx": "Rabinovich et al\\.,? 2017", "shortCiteRegEx": "Rabinovich et al\\.", "year": 2017}, {"title": "Sequence level training with recurrent neural networks. International Conference on Learning Representations (ICLR", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "Cheap and fast\u2014but is it good?: Evaluating non-expert annotations for natural language tasks. In Empirical Methods in Natural Language Processing (EMNLP)", "author": ["Rion Snow", "Brendan O\u2019Connor", "Daniel Jurafsky", "Andrew Y Ng"], "venue": null, "citeRegEx": "Snow et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2008}, {"title": "Learning structured predictors from bandit feedback for interactive NLP", "author": ["Artem Sokolov", "Julia Kreutzer", "Christopher Lo", "Stefan Riezler."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Sokolov et al\\.,? 2016a", "shortCiteRegEx": "Sokolov et al\\.", "year": 2016}, {"title": "Stochastic structured prediction under bandit feedback", "author": ["Artem Sokolov", "Julia Kreutzer", "Stefan Riezler."], "venue": "Advances In Neural Information Processing Systems (NIPS).", "citeRegEx": "Sokolov et al\\.,? 2016b", "shortCiteRegEx": "Sokolov et al\\.", "year": 2016}, {"title": "A coactive learning view of online structured prediction in statistical machine translation", "author": ["Artem Sokolov", "Stefan Riezler", "Shay B Cohen."], "venue": "SIGNLL Conference on Computational Natural Language Learning (CoNLL).", "citeRegEx": "Sokolov et al\\.,? 2015", "shortCiteRegEx": "Sokolov et al\\.", "year": 2015}, {"title": "Dynamic reward shaping: training a robot by voice", "author": ["Ana C Tenorio-Gonzalez", "Eduardo F Morales", "Luis Villase\u00f1or-Pineda."], "venue": "Ibero-American Conference on Artificial Intelligence. Springer, pages 483\u2013492.", "citeRegEx": "Tenorio.Gonzalez et al\\.,? 2010", "shortCiteRegEx": "Tenorio.Gonzalez et al\\.", "year": 2010}, {"title": "Teachable robots: Understanding human teaching behavior to build more effective robot learners", "author": ["Andrea L Thomaz", "Cynthia Breazeal."], "venue": "Artificial Intelligence 172(6-7):716\u2013737.", "citeRegEx": "Thomaz and Breazeal.,? 2008", "shortCiteRegEx": "Thomaz and Breazeal.", "year": 2008}, {"title": "Reinforcement learning with human teachers: Evidence of feedback and guidance with implications for learning performance. In Association for the Advancement of Artificial Intelligence (AAAI)", "author": ["Andrea Lockerd Thomaz", "Cynthia Breazeal"], "venue": null, "citeRegEx": "Thomaz and Breazeal,? \\Q2006\\E", "shortCiteRegEx": "Thomaz and Breazeal", "year": 2006}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Neural architecture search with reinforcement learning", "author": ["Barret Zoph", "Quoc V. Le."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Zoph and Le.,? 2017", "shortCiteRegEx": "Zoph and Le.", "year": 2017}], "referenceMentions": [{"referenceID": 28, "context": "Our algorithm combines the advantage actor-critic algorithm (Mnih et al., 2016) with the attention-based neural encoderdecoder architecture (Luong et al.", "startOffset": 60, "endOffset": 79}, {"referenceID": 25, "context": ", 2016) with the attention-based neural encoderdecoder architecture (Luong et al., 2015).", "startOffset": 68, "endOffset": 88}, {"referenceID": 6, "context": ", translation) and then the world reveals a score that measures how good or bad that output is, but provides neither a \u201ccorrect\u201d output nor feedback on any other possible output (Chang et al., 2015; Sokolov et al., 2015).", "startOffset": 178, "endOffset": 220}, {"referenceID": 38, "context": ", translation) and then the world reveals a score that measures how good or bad that output is, but provides neither a \u201ccorrect\u201d output nor feedback on any other possible output (Chang et al., 2015; Sokolov et al., 2015).", "startOffset": 178, "endOffset": 220}, {"referenceID": 14, "context": "Furthermore, one can often collect even less expensive ratings from \u201cnon-experts\u201d who may or may not be bilingual (Hu et al., 2014).", "startOffset": 114, "endOffset": 131}, {"referenceID": 9, "context": "To make our simulated ratings as realistic as possible, we study recent human evaluation data (Graham et al., 2017) and fit models to match the noise profiles in actual human ratings (\u00a74.", "startOffset": 94, "endOffset": 115}, {"referenceID": 25, "context": "1 We combine an encoderdecoder architecture of machine translation (Luong et al., 2015) with the advantage actor-critic algorithm (Mnih et al.", "startOffset": 67, "endOffset": 87}, {"referenceID": 28, "context": ", 2015) with the advantage actor-critic algorithm (Mnih et al., 2016), yielding an approach that is simple to implement but works on lowresource bandit machine translation.", "startOffset": 50, "endOffset": 69}, {"referenceID": 6, "context": "The bandit structured prediction problem (Chang et al., 2015; Sokolov et al., 2015) is an extension of the contextual bandits problem (Kakade et al.", "startOffset": 41, "endOffset": 83}, {"referenceID": 38, "context": "The bandit structured prediction problem (Chang et al., 2015; Sokolov et al., 2015) is an extension of the contextual bandits problem (Kakade et al.", "startOffset": 41, "endOffset": 83}, {"referenceID": 16, "context": ", 2015) is an extension of the contextual bandits problem (Kakade et al., 2008; Langford and Zhang, 2008) to structured prediction.", "startOffset": 58, "endOffset": 105}, {"referenceID": 21, "context": ", 2015) is an extension of the contextual bandits problem (Kakade et al., 2008; Langford and Zhang, 2008) to structured prediction.", "startOffset": 58, "endOffset": 105}, {"referenceID": 25, "context": "We use an encoder-decoder NMT architecture with global attention (Luong et al., 2015), where both the encoder and decoder are recurrent neural networks (RNN) (see Appendix A for a more detailed description).", "startOffset": 65, "endOffset": 85}, {"referenceID": 42, "context": "Na\u0131\u0308ve Monte Carlo reinforcement learning methods such as REINFORCE (Williams, 1992) estimates Q values by sample means but yields very high variance when the action space is large, leading to training instability.", "startOffset": 68, "endOffset": 84}, {"referenceID": 2, "context": "In our early attempts on bandit NMT, we adapted the actor-critic algorithm for NMT in Bahdanau et al. (2017), which employs the algorithm in a supervised learning setting.", "startOffset": 86, "endOffset": 109}, {"referenceID": 2, "context": "There are two properties in Bahdanau et al. (2017) that our problem lacks but are key elements for a successful actor-critic.", "startOffset": 28, "endOffset": 51}, {"referenceID": 2, "context": "There are two properties in Bahdanau et al. (2017) that our problem lacks but are key elements for a successful actor-critic. The first is access to reference translations: while the critic model is able to observe reference translations during training in their setting, bandit NMT assumes those are never available. The second is per-step rewards: while the reward function in their setting is known and can be exploited to compute immediate rewards after taking each action, in bandit NMT, the actorcritic algorithm struggles with credit assignment because it only receives reward when a translation is completed. Bahdanau et al. (2017) report that the algorithm degrades if rewards are delayed until the end, consistent with our observations.", "startOffset": 28, "endOffset": 640}, {"referenceID": 2, "context": "Bahdanau et al. (2017) add an ad-hoc regularization term to the loss function to mitigate this issue and further stablizes the algorithm with a delay update scheme, but at the same time introduces extra tuning hyper-parameters.", "startOffset": 0, "endOffset": 23}, {"referenceID": 28, "context": "These attractive properties were not studied in A2C\u2019s original paper (Mnih et al., 2016).", "startOffset": 69, "endOffset": 88}, {"referenceID": 8, "context": "In our simulated study, we begin by modeling gold standard human ratings using add-onesmoothed sentence-level BLEU (Chen and Cherry, 2014).", "startOffset": 115, "endOffset": 138}, {"referenceID": 8, "context": "\u201cSmoothing 2\u201d in Chen and Cherry (2014).", "startOffset": 17, "endOffset": 40}, {"referenceID": 23, "context": "A classic example is the Likert scale for human agreement (Likert, 1932) or star ratings for product reviews.", "startOffset": 58, "endOffset": 72}, {"referenceID": 32, "context": "Insisting that human judges provide continuous values (or feedback at too fine a granularity) can demotivate raters without improving rating quality (Preston and Colman, 2000).", "startOffset": 149, "endOffset": 175}, {"referenceID": 9, "context": "We use human evaluation data recently collected as part of the WMT shared task (Graham et al., 2017).", "startOffset": 79, "endOffset": 100}, {"referenceID": 40, "context": "raters are skewed both for reinforcement learning (Thomaz et al., 2006; Thomaz and Breazeal, 2008; Loftin et al., 2014) and recommender systems (Herlocker et al.", "startOffset": 50, "endOffset": 119}, {"referenceID": 24, "context": "raters are skewed both for reinforcement learning (Thomaz et al., 2006; Thomaz and Breazeal, 2008; Loftin et al., 2014) and recommender systems (Herlocker et al.", "startOffset": 50, "endOffset": 119}, {"referenceID": 13, "context": ", 2014) and recommender systems (Herlocker et al., 2000; Adomavicius and Zhang, 2012), but are typically bimodal: some are harsh (typically provide very low scores, even for \u201cokay\u201d outputs) and some are motivational (providing high scores for \u201cokay\u201d outputs).", "startOffset": 32, "endOffset": 85}, {"referenceID": 0, "context": ", 2014) and recommender systems (Herlocker et al., 2000; Adomavicius and Zhang, 2012), but are typically bimodal: some are harsh (typically provide very low scores, even for \u201cokay\u201d outputs) and some are motivational (providing high scores for \u201cokay\u201d outputs).", "startOffset": 32, "endOffset": 85}, {"referenceID": 19, "context": "For English and German, we tokenize and clean sentences using Moses (Koehn et al., 2007).", "startOffset": 68, "endOffset": 88}, {"referenceID": 7, "context": "For Chinese, we use the Stanford Chinese word segmenter (Chang et al., 2008) to segment sentences and tokenize.", "startOffset": 56, "endOffset": 76}, {"referenceID": 25, "context": "Both the NMT model and the critic model are encoder-decoder models with global attention (Luong et al., 2015).", "startOffset": 89, "endOffset": 109}, {"referenceID": 18, "context": "We train our models by the Adam optimizer (Kingma and Ba, 2015) with \u03b21 = 0.", "startOffset": 42, "endOffset": 63}, {"referenceID": 2, "context": "Policy gradient algorithms have difficulty improving from poor initializations, especially on problems with a large action space, because they use model-based exploration, which is ineffective when most actions have equal probabilities (Bahdanau et al., 2017; Ranzato et al., 2016).", "startOffset": 236, "endOffset": 281}, {"referenceID": 34, "context": "Policy gradient algorithms have difficulty improving from poor initializations, especially on problems with a large action space, because they use model-based exploration, which is ineffective when most actions have equal probabilities (Bahdanau et al., 2017; Ranzato et al., 2016).", "startOffset": 236, "endOffset": 281}, {"referenceID": 31, "context": "2010), myoelectric control (Pilarski et al., 2011), and virtual assistants (Isbell et al.", "startOffset": 27, "endOffset": 50}, {"referenceID": 15, "context": ", 2011), and virtual assistants (Isbell et al., 2001).", "startOffset": 32, "endOffset": 53}, {"referenceID": 2, "context": "Recently, this learning framework has been combined with recurrent neural networks to solve machine translation (Bahdanau et al., 2017), dialogue generation (Li et al.", "startOffset": 112, "endOffset": 135}, {"referenceID": 22, "context": ", 2017), dialogue generation (Li et al., 2016), neural architecture search (Zoph and Le, 2017), and device placement (Mirhoseini et al.", "startOffset": 29, "endOffset": 46}, {"referenceID": 43, "context": ", 2016), neural architecture search (Zoph and Le, 2017), and device placement (Mirhoseini et al.", "startOffset": 36, "endOffset": 55}, {"referenceID": 26, "context": ", 2016), neural architecture search (Zoph and Le, 2017), and device placement (Mirhoseini et al., 2017).", "startOffset": 78, "endOffset": 103}, {"referenceID": 2, "context": "Recently, this learning framework has been combined with recurrent neural networks to solve machine translation (Bahdanau et al., 2017), dialogue generation (Li et al., 2016), neural architecture search (Zoph and Le, 2017), and device placement (Mirhoseini et al., 2017). Other approaches to more general structured prediction under bandit feedback (Chang et al., 2015; Sokolov et al., 2016a,b) show the broader efficacy of this framework. Ranzato et al. (2016) describe MIXER for training neural encoder-decoder models, which is a reinforcement learning approach closely related to ours but requires a policy-mixing strategy and only uses a linear critic model.", "startOffset": 113, "endOffset": 462}, {"referenceID": 2, "context": "Recently, this learning framework has been combined with recurrent neural networks to solve machine translation (Bahdanau et al., 2017), dialogue generation (Li et al., 2016), neural architecture search (Zoph and Le, 2017), and device placement (Mirhoseini et al., 2017). Other approaches to more general structured prediction under bandit feedback (Chang et al., 2015; Sokolov et al., 2016a,b) show the broader efficacy of this framework. Ranzato et al. (2016) describe MIXER for training neural encoder-decoder models, which is a reinforcement learning approach closely related to ours but requires a policy-mixing strategy and only uses a linear critic model. Among work on bandit MT, ours is closest to Kreutzer et al. (2017), which also tackle this problem using neural encoder-decoder models, but we (a) take advantage of a state-of-the-art reinforcement learning method; (b) devise a strategy to simulate noisy rewards; and (c) demonstrate the robustness of our method on noisy simulated rewards.", "startOffset": 113, "endOffset": 730}, {"referenceID": 35, "context": "This is despite that errors in human annotations hurt machine learning models in many NLP tasks (Snow et al., 2008).", "startOffset": 96, "endOffset": 115}, {"referenceID": 30, "context": "An obvious question is whether we could extend our framework to model individual annotator preferences (Passonneau and Carpenter, 2014) or learn personalized models (Mirkin et al.", "startOffset": 103, "endOffset": 135}, {"referenceID": 27, "context": "An obvious question is whether we could extend our framework to model individual annotator preferences (Passonneau and Carpenter, 2014) or learn personalized models (Mirkin et al., 2015; Rabinovich et al., 2017), and handle heteroscedastic noise (Park, 1966; Kersting et al.", "startOffset": 165, "endOffset": 211}, {"referenceID": 33, "context": "An obvious question is whether we could extend our framework to model individual annotator preferences (Passonneau and Carpenter, 2014) or learn personalized models (Mirkin et al., 2015; Rabinovich et al., 2017), and handle heteroscedastic noise (Park, 1966; Kersting et al.", "startOffset": 165, "endOffset": 211}, {"referenceID": 29, "context": ", 2017), and handle heteroscedastic noise (Park, 1966; Kersting et al., 2007; Antos et al., 2010).", "startOffset": 42, "endOffset": 97}, {"referenceID": 17, "context": ", 2017), and handle heteroscedastic noise (Park, 1966; Kersting et al., 2007; Antos et al., 2010).", "startOffset": 42, "endOffset": 97}, {"referenceID": 1, "context": ", 2017), and handle heteroscedastic noise (Park, 1966; Kersting et al., 2007; Antos et al., 2010).", "startOffset": 42, "endOffset": 97}, {"referenceID": 12, "context": ", 2014) and reordering (He et al., 2015) among other strategies to both minimize delay and effectively translate a sentence (He et al.", "startOffset": 23, "endOffset": 40}, {"referenceID": 11, "context": ", 2015) among other strategies to both minimize delay and effectively translate a sentence (He et al., 2016).", "startOffset": 91, "endOffset": 108}, {"referenceID": 25, "context": "We closely follow (Luong et al., 2015) for the structure of our model.", "startOffset": 18, "endOffset": 38}, {"referenceID": 25, "context": "We use the \u201cconcat\u201d global attention in (Luong et al., 2015).", "startOffset": 40, "endOffset": 60}], "year": 2017, "abstractText": "Machine translation is a natural candidate problem for reinforcement learning from human feedback: users provide quick, dirty ratings on candidate translations to guide a system to improve. Yet, current neural machine translation training focuses on expensive human-generated reference translations. We describe a reinforcement learning algorithm that improves neural machine translation systems from simulated human feedback. Our algorithm combines the advantage actor-critic algorithm (Mnih et al., 2016) with the attention-based neural encoderdecoder architecture (Luong et al., 2015). This algorithm (a) is well-designed for problems with a large action space and delayed rewards, (b) effectively optimizes traditional corpus-level machine translation metrics, and (c) is robust to skewed, high-variance, granular feedback modeled after actual human behaviors.", "creator": "LaTeX with hyperref package"}}}