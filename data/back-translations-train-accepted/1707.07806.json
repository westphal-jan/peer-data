{"id": "1707.07806", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jul-2017", "title": "Macro Grammars and Holistic Triggering for Efficient Semantic Parsing", "abstract": "To learn a semantic parser from denotations, a learning algorithm must search over a combinatorially large space of logical forms for ones consistent with the annotated denotations. We propose a new online learning algorithm that searches faster as training progresses. The two key ideas are using macro grammars to cache the abstract patterns of useful logical forms found thus far, and holistic triggering to efficiently retrieve the most relevant patterns based on sentence similarity. On the WikiTableQuestions dataset, we first expand the search space of an existing model to improve the state-of-the-art accuracy from 38.7% to 42.7%, and then use macro grammars and holistic triggering to achieve an 11x speedup and an accuracy of 43.7%.", "histories": [["v1", "Tue, 25 Jul 2017 03:42:40 GMT  (359kb,D)", "http://arxiv.org/abs/1707.07806v1", "EMNLP 2017"], ["v2", "Thu, 31 Aug 2017 23:35:23 GMT  (359kb,D)", "http://arxiv.org/abs/1707.07806v2", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yuchen zhang", "panupong pasupat", "percy liang"], "accepted": true, "id": "1707.07806"}, "pdf": {"name": "1707.07806.pdf", "metadata": {"source": "CRF", "title": "Macro Grammars and Holistic Triggering for Efficient Semantic Parsing", "authors": ["Yuchen Zhang", "Panupong Pasupat", "Percy Liang"], "emails": ["zhangyuc@cs.stanford.edu", "ppasupat@cs.stanford.edu", "pliang@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "2 Background", "text": "We base our exposure on the task of answering questions on a knowledge base. In a natural language expression x, a semantic parser maps the enunciation to a logical form z. The logical form is executed on a knowledge base w to generate the designation JzKw. The aim is to train a semantic parser from a training set of enunciation-denotation pairs."}, {"heading": "2.1 Knowledge base and logical forms", "text": "A knowledge base refers to a collection of units and relationships. For the ongoing example \"Who ranked directly after Turkey?\" we use Table 1 from Wikipedia as a knowledge base. Table cells (e.g. Turkey) and rows (e.g. r3 = the 3rd line) are treated as units. Relationships connect units: for example, the relation nation maps r3 to Turkey and a special relation next maps r3 to r4.A logical form z is a small program that can be executed on the knowledge base. We use Lambda DCS (Liang, 2013) as the language of logical forms. The smallest units of Lambda DCS are units (e.g. Turkey) and relationships (e.g. nation). Larger logical forms are composed of smaller ones, and the denotation of the new logical form can be composed of denotations of their components."}, {"heading": "2.2 Grammar rules", "text": "The space of logical forms is defined recursively by grammar rules. In this setting, each constructed logical form belongs to a category (e.g. Entity, Rel, Set), with a special category root for complete logical forms. A rule defines the categories of arguments, category of the resulting logical form, and how the logical form is constructed from the arguments. For example, the rule Rel [z1] + Set [z2] \u2192 Set [z1.z2] (2) can specify that a partially logical form z1 of category Rel and z2 of category Set can be combined into z1.z2 of category Set. With this rule, we can construct Nation.Turkey when we construct a nation of type Rel and Turkey of type Set.We consider the rules used by Pasupat and Liang (2015) for their floating form."}, {"heading": "2.3 Learning a semantic parser", "text": "The parameters of the semantic parser are derived from the training data {(xi, wi, yi)} ni = 1. In a training example with an expression x, a knowledge base w and a target designation y, the learning algorithm constructs a set of logical forms of the candidate specified by Z. Subsequently, it extracts a feature vector \u03c6 (x, w, z) for each z-Z and defines a log-linear distribution over the candidates z: p\u03b8 (z | x, w). If the search space is prohibitively large, it is common practice to use the beam search. More precisely, the algorithm constructs partial logical forms recursively by the rules, but for each category and each search depth, it only zotorizes the logical forms according to the model."}, {"heading": "3 Learning a macro grammar", "text": "Basic grammar usually defines a large search space containing many irrelevant logical forms. For example, the grammar in Pasupat and Liang (2015) can generate long chains of connection operations (e.g. R [Silver].Rank.R [Gold].Bronze.2) that rarely express meaningful computations.Algorithm 1: Processing a training example Data: Example (x, w, y), Macrogrammar, Basic grammar with terminal rules T 1 Select a SetR of macro rules (Section 3.4); 2 Generate a set of logical formulas for candidates from the rulesR \u0109T (Section 2.3); 3 if Z contains consistent logical forms, then refresh 4 model parameters (Section 3.5); 5 otherwise 6 use the basic grammar to search for a consistent logical form (Section 2.3); 7 add macrogrammar successively (Section 3.6); 8 associate the logical form with the grading x."}, {"heading": "3.1 Logical form macros", "text": "A macro characterizes an abstract logical form structure. We define the macro for each given logical form z by transforming its derivative tree as shown in Figure 1b. First, we replace the rule for each terminal rule (leaf node) with a placeholder and name it with the category on the right side of the rule. Then, we merge leaf nodes that represent the same partial logical form. For example, the logical form (1) uses the relation nation twice, so in Figure 1b we merge the two leaf nodes to impose such a constraint. Although the resulting macro may not be tree-like, we call each node root or leaf if it is a root node or a leaf node of the associated derivative tree."}, {"heading": "3.2 Constructing macro rules from macros", "text": "For each given M macro, we can construct a series of macro rules that, when combined with terminal rules from basic grammar, generate exactly the logical forms that fulfill the M macro. The simple approach is to assign a unique rule to each macro: provided that its k nodes contain the categories c1,...,..., ck, we can define a rule: c1 [z1] + \u00b7 \u00b7 + ck [zk] \u2192 root [f (z1,..., zk)], (7) where f z1,..., zk is in the corresponding leaf nodes of macro M. For example, the rule for the macro in Figure 1b is Rel [z1] + Ent [z2] \u2192 Root [R [z1].R [Continue].z1.z2]."}, {"heading": "3.3 Decomposed macro rules", "text": "The definition of a unique rule for each macro is mathematically suboptimal, because the common structures of macros are not exploited. For example, the partial logical form Gold.Num.2 belongs to different macros, and we want to avoid it being created and marked more than once. To reuse such common parts, we split macros into sub-macros and define rules based on them. A subgraph M is a sub-macro rule if (1) M macro rules contain at least one non-leaf node; and (2) M macro connects only through a node (the root of M macro) and defines rules based on it. A subgraph M macro is a sub-macro macro rule."}, {"heading": "3.4 Triggering macro rules", "text": "Throughout the training, we follow a set of S of training expressions associated with a consistent logical form. (The set S is updated by step 9 of algorithm 1.) Then, in the face of a training expression set x, we calculate its K-nearest neighboring expressions in S and select all macro rules extracted from their associated logical forms. These macro rules are used to write expressions x. We use Token-Levenshtein distances as a distance meter to calculate the nearest neighbors. More specifically, each expression is written as a sequence of lemmatized tokens x = (x (1),.., x (m). After we have removed all determinants and rare nouns that appear in less than 2% of the training expressions nearby, the distance between two expressions x and x \u2032 is defined as a sequence of lemmatized tokens x = (x (1),.,., x (m)))))."}, {"heading": "3.5 Updating model parameters", "text": "After calculating the triggered macro rules R + i | xi, we combine them with the terminal rules T from basic grammar (e.g. for constructing Ent and Rel) to create a grammar for uttering x per example. We use this grammar to generate logical forms using standard bar search. We follow Section 2.3 to create a set of logical forms for candidate Z and to update model parameters. However, we deviate from Section 2.3 in one way. If we create a set of logical forms for candidates for a particular training example (xi, wi, yi), we select the logical form z + i with the highest model probability among consistent logical forms and select z \u2212 i with the highest model probability among inconsistent logical forms, then perform a gradient update of the objective function by: J (\u03b8) = 1n \u00b2 i = 1 [log + i p + i + + i + + p \u2212 we select a logical form \u2212 9 with \u2212 wi and \u2212 i as the highest model probability \u2212 p \u2212)."}, {"heading": "3.6 Updating the macro grammar", "text": "If the triggered macro rules do not succeed in finding a uniform logical form, we resort to a bar search for the basic grammar. For efficiency reasons, we end the search either if a uniform logical form is found, or if the total number of generated logical forms exceeds a threshold T. The double-edged criteria prevent the search algorithm from spending too much time on a complex example. We might miss consistent logical forms in such examples, but because the basic grammar is used only to generate macro rules, not to update model parameters, we might be able to derive the same macro rules from other examples. For example, if an example has a formulation that matches too many knowledge base entries, it would be more efficient to skip the example; the macro that would have been extracted from this example grammatical can then find the same macro rules from less ambiguous examples with the same fragment type Derhibit the macrological form, if macrological extractions are completely unique to the macrological form, and if macrological extractions are not."}, {"heading": "3.7 Prediction", "text": "At test date, we follow steps 1-2 of algorithm 1 to generate a series Z of logical forms for candidates from the triggered macro rules and then output the logical form with the highest score in Z. Since the basic grammar is never used at test date, prediction is generally faster than training."}, {"heading": "4 Experiments", "text": "We report on experiments with the WIKITABLEQUESTIONS dataset (Pasupat and Liang, 2015), comparing our algorithm with the parser that is only trained with basic grammar, the floating parser of Pasupat and Liang (2015) (PL15), the Neural Programmer parser (Neelakantan et al., 2016) and the Neural Multi-Step Reasoning parser (Haug et al., 2017). Our algorithm not only exceeds the others, but also achieves an acceleration of the order of magnitude of the parser that is trained with the basic grammar and the parser in PL15."}, {"heading": "4.1 Setup", "text": "The dataset contains 22,033 complex questions on 2,108 Wikipedia tables. Each question comes with a table, and the tables during the evaluation are disjoint from those during the training. The training and test sets contain 14,152 and 4,344 examples, respectively. 2 After PL15, the development accuracy is averaged over the first three 80-20 training data splits specified in the dataset packet. The test accuracy is applied to the tension test data splitter. We use the same features and logical intersection strategies as PL15, but generalize their basic grammar. To control the search space, the actual system in PL15 restricts the superlative operators argmax and argmin, which are applied only to the set of table rows. We also allow these operators to be applied to the set of table cells so that the grammar captures certain logical forms not covered by PL15 (see Table 3). Additionally, we allow f (span) to generate logical units."}, {"heading": "4.2 Coverage of the macro grammar", "text": "With the basic grammar, our parser generates an average of 13,700 partial logical forms for each training example and encounters consistent logical forms in 81.0% of training examples. With the macro rules of holistic triggering, these numbers become 1300 and 75.6%. The macro rules generate much less partial logical forms, but at the expense of slightly less coverage. However, these coverage numbers are calculated based on the search for a logical form that is executed for the correct denotation, including incorrect logical forms that do not reflect the semantics of the question, but happen to coincide with the correct denotation. (For example, the question \"Who has the same number of silvers as France?\" in Table 1 may be incorrectly labeled as R [Nation].R [Next].France, which represents the nation listed for France.) To evaluate the \"true\" coverage, we stitch 300 training examples and manually label their logical forms."}, {"heading": "4.3 Accuracy and speedup", "text": "With a more general basic grammar (additional superlatives and approximate match) and by optimizing the objective function (9), our base parser exceeds PL15 (42.7% versus 37.1%). Learning macrogrammar slightly improves accuracy on the test set to 43.7%. For the three pull dev splits, the average accuracy achieved by the basic grammar and macrogrammar is close (40.6% versus 40.4%).In Table 5, we compare the training and prediction time of PL15 as well as our parsers. To make a fair comparison, we trained all parsers using the SEMPRE toolkit (Berant et al., 2013) on a machine with Xeon 2.6GHz CPU and 128GB of memory without parallel licensing. The time for constructing the macrogrammar is included as part of the training time. Table 5 shows that our parameters with the base are more holistic than the overall base of the exercises with no more expensive exercises."}, {"heading": "4.4 Influence of hyperparameters", "text": "Figure 2a shows that for all bar sizes, training with macro grammar is more efficient than training with basic grammar, and the acceleration rate increases with bar size. The test time accuracy of macro grammar is robust for different bar sizes, as long as B is \u2265 25. Figure 2b shows the influence of neighboring size K. A smaller neighborhood triggers fewer macro rules, which results in a faster calculation. Then, the precision peaks at K = 40 slightly decrease with large K. This limit means that the smaller number of neighbors are regularizers.Figure 2c reports an experiment in which we limit the number of fallback calls to basic grammar at m. Once the limit is reached, subsequent training examples that require fallback calls are simply skipped. This limit means that macro grammar is augmented during most m times during training (Figure 2c reports an experiment that we limit the number of grammar on the case back)."}, {"heading": "5 Related work and discussion", "text": "In fact, the fact is that most of us are able to develop in a way that they have lived in the United States in recent years, in the way that they have lived in the United States: in the way that they have lived in the United States, in the way that they have lived in the United States, in the way that they have lived in the United States, in the way that they have lived in the United States, in the way that they have lived in the way that they have lived in the United States, in the way that they have lived in the United States, in the way that they have lived in the way that they have lived, in the way that they have lived. \""}, {"heading": "6 Summary", "text": "We have presented a method to accelerate semantic parsing using macrograms, the main source of efficiency being the reduced size of the eloquent form space. By implementing some macro rules associated with the expressions of others, we have limited the search space to semantically relevant logical forms. At the same time, we maintain the coverage of the logical form space by occasionally resorting to basic grammar and using the consistent logical forms found to enrich macrogrammatics. The increased efficiency allows us to expand the basic grammar without having to worry much about speed: our model achieves state-of-the-art accuracy and at the same time enjoys an order of magnitude that doubles. We thank Tenent for their support in this project. Reproducibility. Code, data and experiments for this paper are available on the CodaLab platform: https: / / worksheets.coets.codalab.org / worksheets / 0xc4da444444da44433d33d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d4d44d4d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d44d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d26d"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "To learn a semantic parser from denota-<lb>tions, a learning algorithm must search<lb>over a combinatorially large space of log-<lb>ical forms for ones consistent with the an-<lb>notated denotations. We propose a new<lb>online learning algorithm that searches<lb>faster as training progresses. The two<lb>key ideas are using macro grammars to<lb>cache the abstract patterns of useful log-<lb>ical forms found thus far, and holistic trig-<lb>gering to efficiently retrieve the most rele-<lb>vant patterns based on sentence similarity.<lb>On the WIKITABLEQUESTIONS dataset,<lb>we first expand the search space of an ex-<lb>isting model to improve the state-of-the-<lb>art accuracy from 38.7% to 42.7%, and<lb>then use macro grammars and holistic trig-<lb>gering to achieve an 11x speedup and an<lb>accuracy of 43.7%.", "creator": "LaTeX with hyperref package"}}}