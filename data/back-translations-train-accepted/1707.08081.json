{"id": "1707.08081", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jul-2017", "title": "Learning Word Relatedness over Time", "abstract": "Search systems are often focused on providing relevant results for the \"now\", assuming both corpora and user needs that focus on the present. However, many corpora today reflect significant longitudinal collections ranging from 20 years of the Web to hundreds of years of digitized newspapers and books. Understanding the temporal intent of the user and retrieving the most relevant historical content has become a significant challenge. Common search features, such as query expansion, leverage the relationship between terms but cannot function well across all times when relationships vary temporally. In this work, we introduce a temporal relationship model that is extracted from longitudinal data collections. The model supports the task of identifying, given two words, when they relate to each other. We present an algorithmic framework for this task and show its application for the task of query expansion, achieving high gain.", "histories": [["v1", "Tue, 25 Jul 2017 16:41:49 GMT  (1050kb,D)", "https://arxiv.org/abs/1707.08081v1", "11 pages, EMNLP 2017"], ["v2", "Sun, 30 Jul 2017 17:03:35 GMT  (1021kb,D)", "http://arxiv.org/abs/1707.08081v2", "11 pages, EMNLP 2017"]], "COMMENTS": "11 pages, EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["guy d rosin", "eytan adar", "kira radinsky"], "accepted": true, "id": "1707.08081"}, "pdf": {"name": "1707.08081.pdf", "metadata": {"source": "CRF", "title": "Learning Word Relatedness over Time", "authors": ["Guy D. Rosin", "Eytan Adar", "Kira Radinsky"], "emails": ["guyrosin@cs.technion.ac.il,", "kirar@cs.technion.ac.il,", "eadar@umich.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point where there is only one person who is able to retaliate."}, {"heading": "2 Related Work", "text": "Understanding the semantic alteration of words has become an active research topic (Section 2.1). Most work has focused on the temporal analysis of texts: Several methods of temporal information extraction have recently been proposed (Ling and Weld, 2010; Kuzey and Weikum, 2012; Talukdar et al., 2012), as well as publicly published knowledge bases such as YAGO2 (Hoffart et al., 2013), which automatically extract temporal relational facts from free text or semi-structured data. Furthermore, Pustejovsky et al. (2003); UsZaman et al. (2012) and other commented texts chronologically and extracted events, as well as temporal modes of expression (work in Information Retrieval, Section 2.2) discussed the concept of \"time\" as a date overview for a better understanding of this temporal context depending on temporal intentions."}, {"heading": "2.1 Word Dynamics", "text": "Continuous word embedding (Mikolov et al., 2013) has shown that words that occur at the same time in history have a stronger relationship. In our work, we focus on determining when a relationship holds. Numerous projects have examined the change in the meaning of words over time and focused specifically on identifying the change itself. Sagi et al. (2009) used latent semantic analysis to detect changes in the meaning of words. Wijaya and Yeniterzi (2011) characterized 20 clusters to describe the nature of the change in meaning over time, while Mitra et al. (2014) used other cluster techniques to detect changes in the meaning of words. Mihalcea and Nastase (2012) identified changes in word usage over time through the change in their related part of speech. Others have investigated the use of words to identify the frequency of words."}, {"heading": "2.2 Temporal Search", "text": "The temporal aspects of queries and ranking have received significant attention in the IR literature, with some focusing on the characterization of query behavior over time. For example, various queries change over time (Wang et al., 2003) and even by time of day (Beitzel et al., 2004). Jones and Diaz (2007) described queries to have three temporal patterns: atemporal, temporally unambiguous, and temporally ambiguous. Others have used temporal deviations to search for indicators of query intent (Kulkarni et al., 2011). Several efforts have been made (Shimshoni et al., 2009; Chien and Immorlica, 2005; M. Vlachos and Gunopulos, 2004; Zhao et al., 2006; Shokouhi et al, 2011; Radinsky et al al al al al al., 2012) to not only characterize the temporal query behavior, but also to model it using time series analysis."}, {"heading": "3 Temporal Relatedness Dynamics and Semantics", "text": "To solve the problem of understanding temporal references, our approach consists of three main steps: (1) representation of references by word embedding over time (Section 3.1); (2) model references change over time as time series (Section 3.2); (3) combine them to determine when reference relationships exist in time (Section 3.3)."}, {"heading": "3.1 Representing Relatedness using Word Embeddings", "text": "In our thesis we use the distributed representation framework of Word2Vec (Mikolov et al., 2013) (specific skip-grams). Intuitively, given a word wt, skip-grams try to predict surrounding words, e.g. wt \u2212 2, wt \u2212 1, wt \u2212 1, wt + 2 for a window size of c = 2.Definition 3.1. Let Ci be the word context for a word wi. In this thesis we consider the context as the surrounding words of a window size of n Ci = {wi \u2212 1, wi \u2212 1, wi + n}. We define Cti as the context for a word wi in a period wi, i.e. only in documents written during time t.Definition 3.2. We define the word context of a specific embedding of a year y for a word wi, wi \u2212 1, wi \u2212 1, wc, wi + n}."}, {"heading": "3.2 Understanding Relatedness Dynamics", "text": "In this section, we present an algorithm to determine these \"periods of interest.\" Intuitively, if we look at the dynamics of two entities, their peaks represent the lowest distances over time. Formally, if two entities are e1, e2, we want to construct their dynamics and find peaks in them, i.e. the following sequence of periods: {ti | cos (vti1, v ti 2) is relatively high, and ti < tj if i < j}. Traditional peak detection algorithms focus on detecting peaks that are sharp and isolated (i.e. not too many surrounding points have similar values) (Palshikar et al., 2009). In our case, relationships often imply continuous periods of peaks, e.g. Obama was president for eight years. Therefore, we are not strictly necessary as peaks (i.e.: 1)."}, {"heading": "3.3 Learning Temporal Relatedness", "text": "We define the task of learning temporal relationships: if you base two units e1, e2, you can tell if they are related to each other during a given year y, i.e. if the temporal relationship (e1, e2, y) is correct. In Figure 1a, for example, Obama was related to the president in 2010, but not in 2005."}, {"heading": "3.3.1 Specific Classifier", "text": "The first method we present to classify word relations uses a classifier that gets as an input vi corresponding to the units ei and an additional characteristic of the year. In our evaluation (section 4) we will use data from about 40 years, i.e. the year attribute will have 40 possible values. The classifier will predict on the basis of two units whether they will relate to each other during a referenced year.Let Cl: Rn \u2192 {0, 1} be a classifier that maps a vector of n characteristics F = (f1,.,., fn) to a designation L = 0, 1}. Let our characteristic vector be of the form: F = (vy1, v y 2, y) (2), where Vy1 is the specific embedding of n characteristics F = (i.e. at the time), vy2 is the specific temporal embedding and y is the year. As a first step, we must use Cl to a multiple date relationship (i.e., we have the wrong relationship between the first year and this relationship, i.e. Vy2 is the wrong time)."}, {"heading": "3.3.2 Temporal Classifier", "text": "Here we use a classifier similar to the one described in Section 3.3.1 (the training is done the same way), combined with inputs of entity dynamics. First, we build the dynamics, i.e. build specific word embeddings of e1 and e2 for each year y and calculate their cosinal similarity cos (vy1, v y 2). Then we apply our peak recognition algorithm (Section 3.2) to dynamics and use its output as one of the features of the classifier (referred to as isPeak)."}, {"heading": "3.4 Leveraging World Knowledge", "text": "We apply our techniques to two corpora: the first is a temporal corpora (Section 3.4.1), which we used to create word embeddings, and the second is a relational corpora (Section 3.4.2), which we used to train our models and evaluate."}, {"heading": "3.4.1 Temporal Corpora", "text": "For the construction of the corpora, we used the Archiv2 of the New York Times with articles from 1981 to 2016 (a total of 9 GB of text). Specific word embedding was generated for each period (i.e. year) using Word2Vec. Data from each year was used to create word embedding using the Word2Vec negative sampling approach (Mikolov et al., 2013), using the Gensim library (R-ehu-r-Vek and Sojka, 2010). We trained the models with the following parameters: window size of 5, learning rate of 0.05, and dimensionality of 140. During this year, we filtered words with fewer than 30 occurrences. We observed both ambiguity (Apple, the company, and the fruit) and variability (various phrases referring to the same entity, e.g. President Obama, Barack H. Obama, Obama). While such a \"additional noise\" may be problematic to the New York Times as well as the purification bank."}, {"heading": "3.4.2 Relational Corpora", "text": "In this work, we use YAGO2 (Hoffart et al., 2013) as our relationship corpora due to its temporal focus. YAGO2 knowledge base contains millions of facts about entities that we automatically extract from 2http: / / spiderbites.nytimes.com / Wikipedia and other sources. We use relationships from YAGO2 to build our own dataset of temporal relationships, which we use in all of our algorithms and evaluations - as a source for temporal relations. The dataset consists of temporal relationships in the following format: (Entity1, Entity2, year, type, class) where Entity1 and Entity2 are entities, type is a relationship type, and class is true if the relationship is based on the year. For example (Tim Burton, Batman Returns, 1992, Directed, true) and (Battle Mogadishu, Somalia, 2010, HappenedIn, true) 1 shows the exact composition of the dataset."}, {"heading": "4 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experimental Methodology", "text": "We compare the methods described in Section 3.3, where we chose Cl to use a Support Vector Machine (SVM) 3, with an RBF kernel and C = 1.0 (empirically selected). For comparison, two basic lines were used: The first is the common non-time model, i.e. a classifier that uses the global (all-time) word embeddings and the following characteristics: the global embeddings of the two units and one year. Formally F = (v1 | v2 | y) (4) In view of a new temporal relationship, the classifier predicts whether it is true during the referenced year, and we output the prediction of the classifier. The second baseline against which we compare is a standard text3We have used the implementation by the Scikit Learning Library (Pedregosa et al., 2011). Classifier that uses the global word embeddings as the only characteristics, i.e. we have selected F = 2 for the evaluation, which is sufficient for.v2)."}, {"heading": "4.2 Dataset Construction", "text": "Remember that our relationship corpora consists of 80K temporal relationships in the following format: (Entity 1, Entity 2, Year, Type, Class), where type is a relationship type and class is true if the relationship lasts a year. To train and evaluate our classifiers, we need both negative and positive examples. We generate negative examples in the following way: For each relationship in the corpora, sample 10 negative examples. We take the years of true examples from the year range of the data set and then randomly choose years for the negative examples. To illustrate this, consider the case of Obama, President: Obama was president from 2009-2016, so let's sample negative examples from 1981-2008, such as (Obama, President, 1990, HoldsPoliticalPosition, wrong). The resulting data set contains 420K relationships. We call it Temporal Relations Dataset4."}, {"heading": "4.3 Main Results", "text": "Table 2 shows the results of our experiments. Baselines: The Baselines: The Global and Global + Year Baselines returned AUC of 0.55 and 0.57, respectively. Both performed significantly worse than our methods with an AUC of 0.13. Specific Classifier returned an AUC of 0.72. It has the highest recall value of all methods (0.88), but its other values are relatively low. Temporal Classifier returned an AUC of 0.83. As reported in Table 2, it performed significantly better than all other methods with p < 0.05. We used the Wilcoxon Sign Rank test to calculate statistical meaning.4https: / / github.com / guyrosin / learning-word-relatedness"}, {"heading": "4.4 Performance Analysis", "text": "We optimized Word2Vec parameters through empirical testing on a random subset of our dataset: we set the vector size at 140 and used a minimum threshold of 30 occurrences (per year); we found that this balanced the removal of noisy data while ensuring that important units were preserved; the amount of data is critical to creating the Word2Vec models, or it can lead to unreliable (Hellrich and Hahn, 2016) or inaccurate results; we saw a clear correlation between the accuracy and number of occurrences of a participating word; and this led us to evaluate our algorithms only in articles in the New York Times beginning in 1981 - where the number of articles per year is sufficiently large."}, {"heading": "5 Task Example: Query Expansion", "text": "For example, it is a common practice in IR to expand user queries to improve query performance (Carpineto and Romano, 2012). Our task is to expand Q with additional search terms to complement them in order to improve the query of relevant documents. Consider, for example, the query \"Trump Businessman\" (Figure 2). Current QE methods that have no time aspect focus on Donald Trump as President of the United States, a potentially flawed result depending on the time focus of the seeker. A reasonable time expansion could include terms that refer to Donald Trump's business activities, such as \"billionaires\" or \"real estate.\""}, {"heading": "5.1 Query Expansion Algorithms", "text": "We describe alternative strategies for temporal search query expansion, ranging from a generic baseline to algorithms that effectively use our embedding and classifiers. As an ongoing example, we use the search query: \"Steven Spielberg, Saving Private Ryan\" (Spielberg directed in 1998). \"Sensible\" (temporally relevant) extensions to this search query could be: actors who have starred in this movie, other Spielberg movies or similar movies from the same time and genre, etc."}, {"heading": "5.1.1 Baseline", "text": "Following the results of Roy et al. (2016), we consider a baseline method that expands each company separately on the basis of global similarity to Word2Vec. We define the set of candidate expansion terms asC = e Q NNK (e) (6), i.e. for each company we select the closest global K terms. For each c C we calculate the proximity of Mcos (c, Q) and sort the terms in C based on this value. K-top candidates are selected as actual expansion terms. The baseline (bad) expands the query \"Steven Spielberg, Saving Private Ryan\" to \"Inglourious [Basterds], George Lucas.\" In a way you can see the relationship - both are war movies, and Lucas and Spielberg have worked together. However, Inglourious Basterds was created in 2009 and was directed by Quentin Tarantino."}, {"heading": "5.1.2 Globally-Based Classifier", "text": "We use heuristics and assume that the most relevant timeframe t for the entities of the query is the time in which the entities were closest. We use the classifier from our basic method in Section 4.1, the aim of which is to estimate how relevant a year is to a given set of n entities. Its features are the global word embeddings and a year: F = (v1, v2,...., vn, y). We apply the classifier to each year and select the most relevant time with the highest probability of the true label returned. t.t = argmax y {Cl (v1, v2,.., vn, y)} (7). To train the classifier for each entity from that year as an expansion candidate, we separately use the K terms that come closest to the respective entities: C = e, Q NN tK (e) (8) is then filtered as described in the baseline."}, {"heading": "5.1.3 Temporal Classifier", "text": "As we have seen in the previous subsection, the global-based classifier is limited in cases where time-specific knowledge could yield better results. Therefore, in this method we use the specific classifier from Section 3.3.1. Its characteristics are the specific embedding of entities and a year: F = (vy1, vy1, vy, 2,...). We then proceed as described in the previous method (find, select and filter candidates).For our example, this method correctly chooses t = 1998, which is exactly the year of the release of Private Ryan. Its extension is \"Tom Hanks, Movie.\" Since Tom Hanks had a starring role in the movie, the expansion is reasonable. The next algorithm also produces the same extension."}, {"heading": "5.1.4 Temporal Model Classifier", "text": "This method uses the temporal classifier from Section 3.3.2. Its characteristic vector is F = (vy1-v y 2-3-3-3), the rest is the same as described in Section 5.1.3."}, {"heading": "5.2 Query Expansion Evaluation", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "5.3 Textual Relevance", "text": "In terms of the HoldsPoliticalPosition, HappenedIn and IsMarriedTo relationships, the temporal algorithms achieved an accuracy of about 50%, compared to only 20% for Directed and Produced. This difference is reasonable because our models were built on a news corpus. Consider an example of the use of QE algorithms with the query \"Vicente Fox President\" (Fox was president of Mexico from 2000 to 2006), the baseline expands with Mexico's two previous presidents (Zedillo and Salinas), which makes sense because the baseline does not take time into account. Global-based classifier is expanding with Roh Moohyun, who was president of Korea during the same period. Temporal Classifier is expanding with \"Ricardo Lagos, National Action Party\" (Lagos was president of Chile at the time)."}, {"heading": "6 Conclusions", "text": "We believe that as corpora evolves into time-varying datasets, new techniques need to be developed to support both traditional and new IR methods. In this paper, we have introduced a novel technique for extracting relationships in temporal datasets, which is highly efficient and works unattended on a large scale. Our experiments demonstrate the feasibility of the extraction technique and describe ways it can be used in downstream applications. In particular, we highlight a number of query expansion algorithms that can benefit from this technique."}], "references": [{"title": "The web changes everything: understanding the dynamics of web content", "author": ["Eytan Adar", "Jaime Teevan", "Susan T. Dumais", "Jonathan L. Elsas."], "venue": "WSDM\u201909.", "citeRegEx": "Adar et al\\.,? 2009", "shortCiteRegEx": "Adar et al\\.", "year": 2009}, {"title": "Hourly analysis of a very large topically categorized web query log", "author": ["S.M. Beitzel", "E.C. Jensen", "A. Chowdhury", "D. Grossman", "O. Frieder."], "venue": "Proceedings of the Special Interest Group on Information Retrieval (SIGIR).", "citeRegEx": "Beitzel et al\\.,? 2004", "shortCiteRegEx": "Beitzel et al\\.", "year": 2004}, {"title": "the digitization of newspaper archives: Opportunities and challenges for historians", "author": ["Adrian Bingham."], "venue": "Twentieth Century British History, 21(2):225\u2013 231.", "citeRegEx": "Bingham.,? 2010", "shortCiteRegEx": "Bingham.", "year": 2010}, {"title": "Evaluating wordnet-based measures of lexical semantic", "author": ["Alexander Budanitsky", "Graeme Hirst"], "venue": null, "citeRegEx": "Budanitsky and Hirst.,? \\Q2006\\E", "shortCiteRegEx": "Budanitsky and Hirst.", "year": 2006}, {"title": "A survey of automatic query expansion in information retrieval", "author": ["Claudio Carpineto", "Giovanni Romano."], "venue": "ACM Comput. Surv., 44(1).", "citeRegEx": "Carpineto and Romano.,? 2012", "shortCiteRegEx": "Carpineto and Romano.", "year": 2012}, {"title": "Semantic similarity between search engine queries using temporal correlation", "author": ["S. Chien", "N. Immorlica."], "venue": "WWW, pages 2\u201311.", "citeRegEx": "Chien and Immorlica.,? 2005", "shortCiteRegEx": "Chien and Immorlica.", "year": 2005}, {"title": "Query expansion with locally-trained word embeddings", "author": ["Fernando Diaz", "Bhaskar Mitra", "Nick Craswell."], "venue": "ACL\u201916.", "citeRegEx": "Diaz et al\\.,? 2016", "shortCiteRegEx": "Diaz et al\\.", "year": 2016}, {"title": "Diachronic word embeddings reveal statistical laws of semantic change", "author": ["William L. Hamilton", "Jure Leskovec", "Dan Jurafsky."], "venue": "ACL\u201916.", "citeRegEx": "Hamilton et al\\.,? 2016", "shortCiteRegEx": "Hamilton et al\\.", "year": 2016}, {"title": "Bad company - neighborhoods in neural embedding spaces considered harmful", "author": ["Johannes Hellrich", "Udo Hahn."], "venue": "COLING.", "citeRegEx": "Hellrich and Hahn.,? 2016", "shortCiteRegEx": "Hellrich and Hahn.", "year": 2016}, {"title": "Yago2: A spatially and temporally enhanced knowledge base from wikipedia", "author": ["Johannes Hoffart", "Fabian M Suchanek", "Klaus Berberich", "Gerhard Weikum."], "venue": "Artificial Intelligence, 194:28\u201361.", "citeRegEx": "Hoffart et al\\.,? 2013", "shortCiteRegEx": "Hoffart et al\\.", "year": 2013}, {"title": "A framework for analyzing semantic change of words across time", "author": ["Adam Jatowt", "Kevin Duh."], "venue": "JCDL \u201914.", "citeRegEx": "Jatowt and Duh.,? 2014", "shortCiteRegEx": "Jatowt and Duh.", "year": 2014}, {"title": "Temporal profiles of queries", "author": ["Rosie Jones", "Fernando Diaz."], "venue": "ACM Transactions on Information Systems, 25.", "citeRegEx": "Jones and Diaz.,? 2007", "shortCiteRegEx": "Jones and Diaz.", "year": 2007}, {"title": "Ad hoc monitoring of vocabulary shifts over time", "author": ["Tom Kenter", "Melvin Wevers", "Pim Huijnen", "Maarten de Rijke."], "venue": "CIKM \u201915.", "citeRegEx": "Kenter et al\\.,? 2015", "shortCiteRegEx": "Kenter et al\\.", "year": 2015}, {"title": "Understanding temporal query dynamics", "author": ["Anagha Kulkarni", "Jaime Teevan", "Krysta M. Svore", "Susan T. Dumais"], "venue": null, "citeRegEx": "Kulkarni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "Extraction of temporal facts and events from wikipedia", "author": ["Erdal Kuzey", "Gerhard Weikum."], "venue": "2nd Temporal Web Analytics Workshop, pages 25\u2013", "citeRegEx": "Kuzey and Weikum.,? 2012", "shortCiteRegEx": "Kuzey and Weikum.", "year": 2012}, {"title": "Query expansion using word embeddings", "author": ["Saar Kuzi", "Anna Shtok", "Oren Kurland."], "venue": "CIKM\u201916.", "citeRegEx": "Kuzi et al\\.,? 2016", "shortCiteRegEx": "Kuzi et al\\.", "year": 2016}, {"title": "Temporal information extraction", "author": ["Xiao Ling", "Daniel S Weld."], "venue": "AAAI, volume 10, pages 1385\u2013 1390.", "citeRegEx": "Ling and Weld.,? 2010", "shortCiteRegEx": "Ling and Weld.", "year": 2010}, {"title": "Identifying similarities, periodicities and bursts for online search queries", "author": ["Z. Vagena M. Vlachos", "C. Meek", "D. Gunopulos."], "venue": "SIGMOD.", "citeRegEx": "Vlachos et al\\.,? 2004", "shortCiteRegEx": "Vlachos et al\\.", "year": 2004}, {"title": "Word epoch disambiguation: Finding how words change over time", "author": ["Rada Mihalcea", "Vivi Nastase."], "venue": "ACL\u201912.", "citeRegEx": "Mihalcea and Nastase.,? 2012", "shortCiteRegEx": "Mihalcea and Nastase.", "year": 2012}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "That\u2019s sick dude!: Automatic identification of word sense change across different timescales", "author": ["Sunny Mitra", "Ritwik Mitra", "Martin Riedl", "Chris Biemann", "Animesh Mukherjee", "Pawan Goyal."], "venue": "ACL\u201914.", "citeRegEx": "Mitra et al\\.,? 2014", "shortCiteRegEx": "Mitra et al\\.", "year": 2014}, {"title": "Simple algorithms for peak detection in time-series", "author": ["G Palshikar"], "venue": "Proc. 1st Int. Conf. Advanced Data Analysis, Business Analytics and Intelligence, pages 1\u201313.", "citeRegEx": "Palshikar,? 2009", "shortCiteRegEx": "Palshikar", "year": 2009}, {"title": "Scikit-learn: Machine learning", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": null, "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Behind the times: Detecting epoch changes using large corpora", "author": ["Octavian Popescu", "Carlo Strapparava."], "venue": "IJCNLP\u201913.", "citeRegEx": "Popescu and Strapparava.,? 2013", "shortCiteRegEx": "Popescu and Strapparava.", "year": 2013}, {"title": "Timeml: Robust specification of event and temporal expressions in text", "author": ["James Pustejovsky", "Jos\u00e9 M Castano", "Robert Ingria", "Roser Sauri", "Robert J Gaizauskas", "Andrea Setzer", "Graham Katz", "Dragomir R Radev."], "venue": "New directions in question", "citeRegEx": "Pustejovsky et al\\.,? 2003", "shortCiteRegEx": "Pustejovsky et al\\.", "year": 2003}, {"title": "A word at a time: Computing word relatedness using temporal semantic analysis", "author": ["Kira Radinsky", "Eugene Agichtein", "Evgeniy Gabrilovich", "Shaul Markovitch."], "venue": "Proceedings of International World Wide Web Conference (WWW).", "citeRegEx": "Radinsky et al\\.,? 2011", "shortCiteRegEx": "Radinsky et al\\.", "year": 2011}, {"title": "Modeling and predicting behavioral dynamics on the web", "author": ["Kira Radinsky", "Krysta Svore", "Susan Dumais", "Jaime Teevan", "Alex Bocharov", "Eric Horvitz."], "venue": "Proceedings of International World Wide Web Conference (WWW).", "citeRegEx": "Radinsky et al\\.,? 2012", "shortCiteRegEx": "Radinsky et al\\.", "year": 2012}, {"title": "Behavioral dynamics on the web: Learning, modeling, and prediction", "author": ["Kira Radinsky", "Krysta M. Svore", "Susan T. Dumais", "Milad Shokouhi", "Jaime Teevan", "Alex Bocharov", "Eric Horvitz."], "venue": "ACM Transactions on Information Systems, 31(3):16.", "citeRegEx": "Radinsky et al\\.,? 2013", "shortCiteRegEx": "Radinsky et al\\.", "year": 2013}, {"title": "Software Framework for Topic Modeling with Large Corpora", "author": ["Radim \u0158eh\u016f\u0159ek", "Petr Sojka."], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Valletta, Malta. ELRA.", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka.,? 2010", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka.", "year": 2010}, {"title": "Using word embeddings for automatic query expansion", "author": ["Dwaipayan Roy", "Debjyoti Paul", "Mandar Mitra", "Utpal Garain."], "venue": "arXiv preprint arXiv:1606.07608.", "citeRegEx": "Roy et al\\.,? 2016", "shortCiteRegEx": "Roy et al\\.", "year": 2016}, {"title": "Semantic density analysis: Comparing word meaning across time and phonetic space", "author": ["Eyal Sagi", "Stefan Kaufmann", "Brady Clark."], "venue": "Workshop on Geometrical Models of Natural Language Semantics.", "citeRegEx": "Sagi et al\\.,? 2009", "shortCiteRegEx": "Sagi et al\\.", "year": 2009}, {"title": "On the predictability of search trends", "author": ["Yair Shimshoni", "Niv Efron", "Yossi Matias."], "venue": "Technical Report.", "citeRegEx": "Shimshoni et al\\.,? 2009", "shortCiteRegEx": "Shimshoni et al\\.", "year": 2009}, {"title": "Detecting seasonal queries by time-series analysis", "author": ["Milad Shokouhi."], "venue": "Proceedings of the Special Interest Group on Information Retrieval (SIGIR).", "citeRegEx": "Shokouhi.,? 2011", "shortCiteRegEx": "Shokouhi.", "year": 2011}, {"title": "Timesensitive query auto-completion", "author": ["Milad Shokouhi", "Kira Radinsky."], "venue": "Proceedings of the Special Interest Group on Information Retrieval (SIGIR).", "citeRegEx": "Shokouhi and Radinsky.,? 2012", "shortCiteRegEx": "Shokouhi and Radinsky.", "year": 2012}, {"title": "History by diversity: Helping historians search news archives", "author": ["Jaspreet Singh", "Wolfgang Nejdl", "Avishek Anand."], "venue": "CHIIR\u201916, pages 183\u2013192.", "citeRegEx": "Singh et al\\.,? 2016", "shortCiteRegEx": "Singh et al\\.", "year": 2016}, {"title": "Coupled temporal scoping of relational facts", "author": ["Partha Pratim Talukdar", "Derry Wijaya", "Tom Mitchell."], "venue": "5th ACM international conference on Web search and data mining, pages 73\u201382. ACM.", "citeRegEx": "Talukdar et al\\.,? 2012", "shortCiteRegEx": "Talukdar et al\\.", "year": 2012}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D Turney", "Patrick Pantel"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "Tempeval-3: Evaluating events, time expressions, and temporal relations", "author": ["Naushad UzZaman", "Hector Llorens", "James Allen", "Leon Derczynski", "Marc Verhagen", "James Pustejovsky."], "venue": "arXiv preprint arXiv:1206.5333.", "citeRegEx": "UzZaman et al\\.,? 2012", "shortCiteRegEx": "UzZaman et al\\.", "year": 2012}, {"title": "Mining longitudinal web queries: trends and patterns", "author": ["P. Wang", "M.W. Berry", "Y. Yang."], "venue": "Journal of the American Society for Information Science and Technology (JASIST), 54:743\u2013758.", "citeRegEx": "Wang et al\\.,? 2003", "shortCiteRegEx": "Wang et al\\.", "year": 2003}, {"title": "Understanding semantic change of words over centuries", "author": ["Derry Tanti Wijaya", "Reyyan Yeniterzi."], "venue": "2011 International Workshop on DETecting and Exploiting Cultural diversiTy on the Social Web, DETECT \u201911.", "citeRegEx": "Wijaya and Yeniterzi.,? 2011", "shortCiteRegEx": "Wijaya and Yeniterzi.", "year": 2011}, {"title": "Finding information in books: Characteristics of full-text searches in a collection of 10 million books", "author": ["Craig Willis", "Miles Efron."], "venue": "Proceedings of the American Society for Information Science and Technology, 50(1):1\u201310.", "citeRegEx": "Willis and Efron.,? 2013", "shortCiteRegEx": "Willis and Efron.", "year": 2013}, {"title": "Time-dependent semantic similarity measure of queries using historical click-through data", "author": ["Qiankun Zhao", "Steven C.H. Hoi", "Tie yan Liu."], "venue": "Proceedings of International World Wide Web Conference (WWW).", "citeRegEx": "Zhao et al\\.,? 2006", "shortCiteRegEx": "Zhao et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 27, "context": "In this context, features as basic as disambiguation and spelling corrections are fixed to what is most likely today or within the past few years (Radinsky et al., 2013), query expansions and synonyms are weighted towards current information (Shokouhi and Radinsky, 2012), and results tend to include the most recent and popular content.", "startOffset": 146, "endOffset": 169}, {"referenceID": 33, "context": ", 2013), query expansions and synonyms are weighted towards current information (Shokouhi and Radinsky, 2012), and results tend to include the most recent and popular content.", "startOffset": 80, "endOffset": 109}, {"referenceID": 0, "context": "While this problem would seem to be speculative in that it will be years until we need to address it, the reality is the rate of change (Adar et al., 2009) of the Web, language, and culture have simply compressed the time in which critical changes happen.", "startOffset": 136, "endOffset": 155}, {"referenceID": 40, "context": ", Hathitrust (Willis and Efron, 2013), historical news corpora, the Internet Archives, and even fast changing Twitter feeds).", "startOffset": 13, "endOffset": 37}, {"referenceID": 34, "context": "Singh et al. (2016) refer to this as a Historical Query Intent.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "While these might satisfy the searcher if they are looking for the latest results, or the results most covered by the press, there are clearly other possible needs (Bingham, 2010).", "startOffset": 164, "endOffset": 179}, {"referenceID": 3, "context": "meronymy, antonymy, functional association) and is often more useful for computational linguistics applications than the more narrow notion of similarity (Budanitsky and Hirst, 2006).", "startOffset": 154, "endOffset": 182}, {"referenceID": 15, "context": "algorithms (Kuzi et al., 2016; Diaz et al., 2016).", "startOffset": 11, "endOffset": 49}, {"referenceID": 6, "context": "algorithms (Kuzi et al., 2016; Diaz et al., 2016).", "startOffset": 11, "endOffset": 49}, {"referenceID": 16, "context": "A lot of effort has been made into analyzing texts temporally: several methods for temporal information extraction were recently proposed (Ling and Weld, 2010; Kuzey and Weikum, 2012; Talukdar et al., 2012), as well as publicly released knowledge bases, such as YAGO2 (Hoffart et al.", "startOffset": 138, "endOffset": 206}, {"referenceID": 14, "context": "A lot of effort has been made into analyzing texts temporally: several methods for temporal information extraction were recently proposed (Ling and Weld, 2010; Kuzey and Weikum, 2012; Talukdar et al., 2012), as well as publicly released knowledge bases, such as YAGO2 (Hoffart et al.", "startOffset": 138, "endOffset": 206}, {"referenceID": 35, "context": "A lot of effort has been made into analyzing texts temporally: several methods for temporal information extraction were recently proposed (Ling and Weld, 2010; Kuzey and Weikum, 2012; Talukdar et al., 2012), as well as publicly released knowledge bases, such as YAGO2 (Hoffart et al.", "startOffset": 138, "endOffset": 206}, {"referenceID": 9, "context": ", 2012), as well as publicly released knowledge bases, such as YAGO2 (Hoffart et al., 2013).", "startOffset": 69, "endOffset": 91}, {"referenceID": 9, "context": ", 2012), as well as publicly released knowledge bases, such as YAGO2 (Hoffart et al., 2013). These methods automatically extract temporal relational facts from free text or semi-structured data. In addition, Pustejovsky et al. (2003); UzZaman et al.", "startOffset": 70, "endOffset": 234}, {"referenceID": 9, "context": ", 2012), as well as publicly released knowledge bases, such as YAGO2 (Hoffart et al., 2013). These methods automatically extract temporal relational facts from free text or semi-structured data. In addition, Pustejovsky et al. (2003); UzZaman et al. (2012) and others annotated texts temporally, and extracted events as well as temporal expressions.", "startOffset": 70, "endOffset": 257}, {"referenceID": 19, "context": "Continuous word embeddings (Mikolov et al., 2013) have been shown to effectively encapsulate relatedness between words.", "startOffset": 27, "endOffset": 49}, {"referenceID": 23, "context": "Others have investigated the use of word frequency to identify epochs (Popescu and Strapparava, 2013).", "startOffset": 70, "endOffset": 101}, {"referenceID": 15, "context": "Continuous word embeddings (Mikolov et al., 2013) have been shown to effectively encapsulate relatedness between words. Radinsky et al. (2011) used temporal patterns of words from a large corpus for the task of word similarity.", "startOffset": 28, "endOffset": 143}, {"referenceID": 15, "context": "Continuous word embeddings (Mikolov et al., 2013) have been shown to effectively encapsulate relatedness between words. Radinsky et al. (2011) used temporal patterns of words from a large corpus for the task of word similarity. They showed that words that co-occur in history have a stronger relation. In our work, we focus on identifying when a relation holds. Numerous projects have studied the change of word meanings over time, and specifically focused on identification of the change itself. Sagi et al. (2009) used Latent Semantic Analysis for detecting changes in word meaning.", "startOffset": 28, "endOffset": 516}, {"referenceID": 15, "context": "Continuous word embeddings (Mikolov et al., 2013) have been shown to effectively encapsulate relatedness between words. Radinsky et al. (2011) used temporal patterns of words from a large corpus for the task of word similarity. They showed that words that co-occur in history have a stronger relation. In our work, we focus on identifying when a relation holds. Numerous projects have studied the change of word meanings over time, and specifically focused on identification of the change itself. Sagi et al. (2009) used Latent Semantic Analysis for detecting changes in word meaning. Wijaya and Yeniterzi (2011) characterized 20 clusters to describe the nature of meaning change over time, whereas Mitra et al.", "startOffset": 28, "endOffset": 613}, {"referenceID": 15, "context": "Continuous word embeddings (Mikolov et al., 2013) have been shown to effectively encapsulate relatedness between words. Radinsky et al. (2011) used temporal patterns of words from a large corpus for the task of word similarity. They showed that words that co-occur in history have a stronger relation. In our work, we focus on identifying when a relation holds. Numerous projects have studied the change of word meanings over time, and specifically focused on identification of the change itself. Sagi et al. (2009) used Latent Semantic Analysis for detecting changes in word meaning. Wijaya and Yeniterzi (2011) characterized 20 clusters to describe the nature of meaning change over time, whereas Mitra et al. (2014) used other clustering techniques to find changes in word senses.", "startOffset": 28, "endOffset": 719}, {"referenceID": 15, "context": "Mihalcea and Nastase (2012) identified changes in word usage over time by the change in their related part-of-speech.", "startOffset": 0, "endOffset": 28}, {"referenceID": 9, "context": "Jatowt and Duh (2014) represented a word embedding over the Google Books corpus (granularity of decades) and presented qualitative evaluation for several words.", "startOffset": 0, "endOffset": 22}, {"referenceID": 7, "context": "Hamilton et al. (2016) built Word2Vec embedding models on the Google Books corpus to detect known word shifts over 30 words and presented a dozen new shifts from the data.", "startOffset": 0, "endOffset": 23}, {"referenceID": 7, "context": "Hamilton et al. (2016) built Word2Vec embedding models on the Google Books corpus to detect known word shifts over 30 words and presented a dozen new shifts from the data. The authors presented two laws that govern the change of words \u2013 frequent words change more slowly and polysemous words change more quickly. Finally, Kenter et al. (2015) studied changes in meaning (represented by a few seed words), and monitored the changing set of words that are used to denote it.", "startOffset": 0, "endOffset": 343}, {"referenceID": 38, "context": "For example, different queries change in popularity over time (Wang et al., 2003) and even by time of day (Beitzel et al.", "startOffset": 62, "endOffset": 81}, {"referenceID": 1, "context": ", 2003) and even by time of day (Beitzel et al., 2004).", "startOffset": 32, "endOffset": 54}, {"referenceID": 13, "context": "Others have leveraged temporal variance to look for indicators of query intent (Kulkarni et al., 2011).", "startOffset": 79, "endOffset": 102}, {"referenceID": 31, "context": "Several efforts (Shimshoni et al., 2009; Chien and Immorlica, 2005; M. Vlachos and Gunopulos, 2004; Zhao et al., 2006; Shokouhi, 2011; Radinsky et al., 2012) were done to not only characterize the temporal query behavior but also model it via time-series analysis.", "startOffset": 16, "endOffset": 157}, {"referenceID": 5, "context": "Several efforts (Shimshoni et al., 2009; Chien and Immorlica, 2005; M. Vlachos and Gunopulos, 2004; Zhao et al., 2006; Shokouhi, 2011; Radinsky et al., 2012) were done to not only characterize the temporal query behavior but also model it via time-series analysis.", "startOffset": 16, "endOffset": 157}, {"referenceID": 41, "context": "Several efforts (Shimshoni et al., 2009; Chien and Immorlica, 2005; M. Vlachos and Gunopulos, 2004; Zhao et al., 2006; Shokouhi, 2011; Radinsky et al., 2012) were done to not only characterize the temporal query behavior but also model it via time-series analysis.", "startOffset": 16, "endOffset": 157}, {"referenceID": 32, "context": "Several efforts (Shimshoni et al., 2009; Chien and Immorlica, 2005; M. Vlachos and Gunopulos, 2004; Zhao et al., 2006; Shokouhi, 2011; Radinsky et al., 2012) were done to not only characterize the temporal query behavior but also model it via time-series analysis.", "startOffset": 16, "endOffset": 157}, {"referenceID": 26, "context": "Several efforts (Shimshoni et al., 2009; Chien and Immorlica, 2005; M. Vlachos and Gunopulos, 2004; Zhao et al., 2006; Shokouhi, 2011; Radinsky et al., 2012) were done to not only characterize the temporal query behavior but also model it via time-series analysis.", "startOffset": 16, "endOffset": 157}, {"referenceID": 27, "context": "Radinsky and colleagues modeled changes in the frequency of clicked URLs, queries, and clicked query-URL pairs by using time-series analysis and show its application for improving ranking and query autosuggestions (Radinsky et al., 2013; Shokouhi and Radinsky, 2012).", "startOffset": 214, "endOffset": 266}, {"referenceID": 33, "context": "Radinsky and colleagues modeled changes in the frequency of clicked URLs, queries, and clicked query-URL pairs by using time-series analysis and show its application for improving ranking and query autosuggestions (Radinsky et al., 2013; Shokouhi and Radinsky, 2012).", "startOffset": 214, "endOffset": 266}, {"referenceID": 1, "context": ", 2003) and even by time of day (Beitzel et al., 2004). Jones and Diaz (2007) described queries to have three temporarilty patterns: atemporal, temporally unambiguous and temporally ambiguous.", "startOffset": 33, "endOffset": 78}, {"referenceID": 1, "context": ", 2003) and even by time of day (Beitzel et al., 2004). Jones and Diaz (2007) described queries to have three temporarilty patterns: atemporal, temporally unambiguous and temporally ambiguous. Others have leveraged temporal variance to look for indicators of query intent (Kulkarni et al., 2011). Several efforts (Shimshoni et al., 2009; Chien and Immorlica, 2005; M. Vlachos and Gunopulos, 2004; Zhao et al., 2006; Shokouhi, 2011; Radinsky et al., 2012) were done to not only characterize the temporal query behavior but also model it via time-series analysis. Radinsky and colleagues modeled changes in the frequency of clicked URLs, queries, and clicked query-URL pairs by using time-series analysis and show its application for improving ranking and query autosuggestions (Radinsky et al., 2013; Shokouhi and Radinsky, 2012). Singh et al. (2016) focused on serving the specific needs of historians, and introduced the notion of a Historical Query Intent for this purpose.", "startOffset": 33, "endOffset": 850}, {"referenceID": 19, "context": "In our work we leverage the distributed representation framework of Word2Vec (Mikolov et al., 2013) (specifically skip-grams).", "startOffset": 77, "endOffset": 99}, {"referenceID": 19, "context": "Each year\u2019s data was used to create word embeddings using Word2Vec\u2019s skip-gram with negative sampling approach (Mikolov et al., 2013), with the Gensim library (\u0158eh\u016f\u0159ek and Sojka, 2010).", "startOffset": 111, "endOffset": 133}, {"referenceID": 28, "context": ", 2013), with the Gensim library (\u0158eh\u016f\u0159ek and Sojka, 2010).", "startOffset": 33, "endOffset": 58}, {"referenceID": 9, "context": "In this work, we use YAGO2 (Hoffart et al., 2013) as our relational corpora due to its temporal focus.", "startOffset": 27, "endOffset": 49}, {"referenceID": 22, "context": "We used the implementation by the scikit-learn library (Pedregosa et al., 2011).", "startOffset": 55, "endOffset": 79}, {"referenceID": 8, "context": "For constructing the Word2Vec models, the amount of data is crucial or it may lead to unreliable (Hellrich and Hahn, 2016) or inaccurate results.", "startOffset": 97, "endOffset": 122}, {"referenceID": 4, "context": "For example, it is a common practice in IR to expand user queries to improve retrieval performance (Carpineto and Romano, 2012).", "startOffset": 99, "endOffset": 127}, {"referenceID": 29, "context": "Following the results of Roy et al. (2016), we consider a baseline method that expands each entity separately, based on global Word2Vec similarity.", "startOffset": 25, "endOffset": 43}, {"referenceID": 29, "context": "Baseline (Roy et al., 2016) 14.", "startOffset": 9, "endOffset": 27}], "year": 2017, "abstractText": "Search systems are often focused on providing relevant results for the \u201cnow\u201d, assuming both corpora and user needs that focus on the present. However, many corpora today reflect significant longitudinal collections ranging from 20 years of the Web to hundreds of years of digitized newspapers and books. Understanding the temporal intent of the user and retrieving the most relevant historical content has become a significant challenge. Common search features, such as query expansion, leverage the relationship between terms but cannot function well across all times when relationships vary temporally. In this work, we introduce a temporal relationship model that is extracted from longitudinal data collections. The model supports the task of identifying, given two words, when they relate to each other. We present an algorithmic framework for this task and show its application for the task of query expansion, achieving high gain.", "creator": "LaTeX with hyperref package"}}}