{"id": "1707.08475", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jul-2017", "title": "DARLA: Improving Zero-Shot Transfer in Reinforcement Learning", "abstract": "Domain adaptation is an important open problem in deep reinforcement learning (RL). In many scenarios of interest data is hard to obtain, so agents may learn a source policy in a setting where data is readily available, with the hope that it generalises well to the target domain. We propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act. DARLA's vision is based on learning a disentangled representation of the observed environment. Once DARLA can see, it is able to acquire source policies that are robust to many domain shifts - even with no access to the target domain. DARLA significantly outperforms conventional baselines in zero-shot domain adaptation scenarios, an effect that holds across a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C and EC).", "histories": [["v1", "Wed, 26 Jul 2017 14:50:51 GMT  (5748kb,D)", "http://arxiv.org/abs/1707.08475v1", "ICML 2017"]], "COMMENTS": "ICML 2017", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["irina higgins", "arka pal", "andrei a rusu", "lo\u00efc matthey", "christopher burgess", "alexander pritzel", "matthew botvinick", "charles blundell", "alexander lerchner"], "accepted": true, "id": "1707.08475"}, "pdf": {"name": "1707.08475.pdf", "metadata": {"source": "META", "title": "DARLA: Improving Zero-Shot Transfer in Reinforcement Learning", "authors": ["Irina Higgins", "Arka Pal", "Andrei Rusu", "Loic Matthey", "Christopher Burgess", "Alexander Pritzel", "Matthew Botvinick", "Charles Blundell", "Alexander Lerchner"], "emails": ["<irinah@google.com>,", "<arkap@google.com>."], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2. Framework", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Domain adaptation in Reinforcement Learning", "text": "We now formalize domain adaptation scenarios in a reinforcement learning setting. We designate the source and target domains as DS and DT, respectively. Each domain corresponds to an MDP defined as a tuple DS \u2261 (SS, AS, TS, RS) or DT \u2261 (ST, AT, TT, RT) (we assume a common fixed discount factor \u03b3), each with its own state space S, action space A, transition function T, and reward function R.1 In domain adaptation scenarios, the states S of the source and target domains can be quite different, while the action spaces A are divided and the transitions T and the reward functions R exhibit structural similarity. Consider, for example, a domain adaptation scenario for the Jaco robot arm in which the MuJoCo (Todorov et al., 2012) simulation of the arm is the source domain and the real-world setting is the target domain."}, {"heading": "2.2. DARLA", "text": "To describe our proposed DARLA system, we assume that there are a number of MDPs that relate to the SDPs, and that the SDPs (SDP-D) -D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-"}, {"heading": "2.3. Learning disentangled representations", "text": "To learn FU, DARLA uses \u03b2-UAE (Higgins et al., 2017), a state-of-the-art, unsupervised model for the automated discovery of factored latent representations from raw image data. \u03b2-UAE is a modification of the variational autoencoder framework (Kingma & Welling, 2014; Rezende et al., 2014) that controls the nature of learned latent representations by introducing an adjustable hyperparameter \u03b2 to balance reconstruction accuracy with latent channel capacities and independence constraints. It maximizes the goal: L (\u03b8; x, z, \u03b2) = equality (z | x) [log pdul (x | z)] \u2212 \u03b2 DKL (q\u03c6 (z | x) | p (z))) (1), where the two parameters increase the distributions of the encoder or the decoder. Well-chosen values of \u03b2 - usually greater than one \u03b2 (more limited > 1) lead to an overmatch of the Higgins (in a rule)."}, {"heading": "2.3.1. PERCEPTUAL SIMILARITY LOSS", "text": "The cost of increasing \u03b2 is that crucial information about the scene in the latent representation \u03b2 \u03b2 = b) can be discarded, especially if this information takes up a small proportion of \u03b2 observations x in the pixel space. We have encountered this problem in some of our tasks, as discussed in Section 3.1. the deficiencies in calculating the log probability term termEq\u03c6 (z | x) [log p\u03b8 (x | z)] on a pro-pixel basis are known and have been addressed in the past by calculating the reconstruction costs in an abstract, high-level feature space given by another neural network model, such as a GAN (Goodfellow et al., 2014) or a pre-formed AlexNet (Krizhevsky et al., 2012; Larsen et al., 2016; Dosovitskiy & Brox, 2016; Warde-Farley & Bengio, 2017).In practice, we have found that prior to training a high-level DAxx model."}, {"heading": "2.4. Reinforcement Learning Algorithms", "text": "We have used various RL algorithms (DQN, A3C and AEP) to learn the source policy in the second phase of the pipeline, namely in the form of deep UAE models in the first phase of the pipeline. It uses a neural network to approximate the action function Q (s, a)."}, {"heading": "3. Tasks", "text": "We evaluate the performance of DARLA on various task and environment setups that subtly examine different aspects of domain adaptation. Remember: In Sec. 2.2 we define S 'as a state space that contains all possible conjunctions of high-level variation factors necessary to generate any naturalistic observations in any Di'M. While domain adaptation scenarios generate agent observation states according to soS-SimS (sS) and soT-SimT (sT) for the source and target domains in which s S and s-T are sampled by some distributions or processes, the agent observation states are based on s-S-GS (S) and s-T-SimT simulations. We use DeepMind Lab (Beattie et al., 2016) to test a version of the domain adaptup."}, {"heading": "3.1. DeepMind Lab", "text": "DeepMind Lab is a first person 3D game environment with rich visuals and reality physics. We used a standard seekavoid object collection setup, where a room with an equal number of randomly placed objects of two different types is initialized, one of which is \"good\" (its collection becomes + 1), while the other is \"bad\" (its collection is punished with -1). Complete state room S consists of all subjunctions of two room types (pink and green based on the color of the walls) and four object types (hat, cake and balloon) (see Fig. 2A). Source domain DS-held environments with hats / cans presented in the green room, and balloons presented in either the green or pink room. Target domain DT contained hats / cans presented in the pink room. In both domains, trained objects and balloons were the rewarded objects."}, {"heading": "3.2. Jaco Arm and MuJoCo", "text": "In fact, most of them are able to survive by themselves if they are not able to play by the rules. (...) Most of them are able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...)"}, {"heading": "Disentangled Entangled", "text": "The physics engine is also not a perfect model of reality. Therefore, sim2real tests the agent's ability to cross the gap between perception and reality and to generalize its source politics \u03c0S to the real world (see Fig. 2C, right)."}, {"heading": "4. Results", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves. (...) Most of them are able to understand the rules. (...) Most of them are able to understand the rules. (...) Most of them are able to understand the rules. (...) Most of them are not able to understand the rules. (...) Most of them are not able to understand the rules. (...) Most of them are not able to understand the rules. (...) Most of them are not able to understand the rules. (...) Most of them are not able to understand the rules. (...) Most of them are not able to understand the rules. (...) Most of them are not able to understand the rules. (...) Most of them are not able to understand the rules. (...)"}, {"heading": "5. Conclusion", "text": "In particular, we proposed DARLA, a multi-level RL agent. DARLA first learns a visual system that encodes the observations it receives from the environment in a completely unattended way as unbundled representations, and then uses these representations to learn a robust source policy capable of performing a zero-shot domain adaptation. We demonstrated the effectiveness of this approach in a number of domains and tasks: a 3D naturalistic environment (DeepMind Lab), a simulated graphics and physics engine (MuJoCo), and the crossing of simulation to reality (MuJoCo to Jaco sim2real). We also demonstrated that the effect of unbundling is consistent across very different RL algorithms (DQN, A3C, EC) and achieves significant improvements over base algorithms (median 2.7 times in this time is the best improvement in knowledge depth)."}, {"heading": "A. Supplementary Materials", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1. The Reinforcement Learning Paradigm", "text": "The paradigm of enhanced learning (RL) consists in the fact that an actor receives a sequence of observations that represent a function of the environmental states st-S and can be accompanied by rewards rt + 1-R, which depend on the actions in the respective chosen step T (Sutton & Barto, 1998). We assume that these interactions can be modelled as a Markov decision process (MDP) (Puterman, 1994), which is defined as a tuple D-Z (S, A, T, R, \u03b3). T = p (s | st, at) is a transition function that models the distribution of all possible next states that take action in the state st for all st-S and in the state-A. Each transition st at \u2192 st + 1 can be accompanied by a reward signal rt + 1 (st, st + 1). Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z policy modeling, in the state state st, a probability distribution of the future function maximizes the Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z, which is the expected to be the Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Mapping, a Z-Z-Z-Z-"}, {"heading": "A.2.1. DEEPMIND LAB", "text": "As described in Fig. 3.1, in each DeepMind Lab source sequence, the agent was presented with one of three randomly selected room / object conjunctions, marked DS in Fig. 2. Setup was a search avoidance-style task in which one of the two object types in the room received a reward of + 1 and the other a reward of -1. The agent was allowed to pick up objects for 60 seconds, whereupon the episode would end and a new one would begin; if the agent was able to pick up all \"good\" objects in less than 60 seconds, a new episode immediately began. The agent was tracked down at the beginning of each new episode at a random location in the room. During the transfer, the agent was placed in the stretched connection between object types and space background; see Fig. 2 Visual pre-training was performed in other conjunctions of object type and space background, which was in Fig. 2.HepWHC (the observation size of DexW3)."}, {"heading": "A.2.2. MUJOCO/JACO ARM EXPERIMENTS", "text": "This year, it has reached the stage where it will be able to put itself at the forefront in order to pave the way for the future."}, {"heading": "A.3.4. DENOISING AUTOENCODER FOR BASELINE", "text": "For the base model, DARLADAE, we trained a denoting autoencoder with occlusion-like masking noise as described in Appendix Section A.3.1. The architecture used was exactly the same as the \u03b2-VAE described in Appendix Section A.3.2 - however, all stochastic nodes were replaced by deterministic neurons. Adam was the optimizer used with a learning rate of 1e-4.7Typically, the mean of the reconstruction distribution is used, but this does not put pressure on the Gaussians to parameterise the decoder to reduce their deviations."}, {"heading": "A.4. Reinforcement Learning Algorithm Details", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.4.1. DEEPMIND LAB", "text": "The scope of action in the DeepMind Lab task consisted of 8 discrete actions. DQN: in DQN, the revolutionary (or \"vision\") part of the Q network was frozen by the \u03b2-VAEDAE encoder from level 1 and. DQN takes four consecutive frames as input to capture an aspect of environmental dynamics in the state of the agent. To match this in our setup with a pre-trained vision stack FU, we passed each observation frame so {1.. 4} through the pre-trained model sz {1.. 4} = FU (so {1.. 4}) and linked the outputs together to form the k-dimensional stack FU."}, {"heading": "A.4.2. MUJOCO/JACO ARM EXPERIMENTS", "text": "For the real Jaco arm and its MuJoCo simulation, each of the nine joints could independently perform 11 different actions (a linear discretization of the continuous velocity action space), so the size of the action space for MuJoCo / Jaco was 99.DARLA was based on the feedback A3C (Mnih et al., 2016). We followed the setup of the simulation training (Rusu et al., 2016) for forward-facing networks with visual raw input only. However, instead of the usual conv stack, we used the \u03b2-VAE encoder as described in Appendix A.3.3, followed by a linear layer with 512 units, a ReLU nonlinearity, and a collection of 9 linear and Softmax layers for the 9 independent policy issues, as well as a single value output layer that gave the value function."}, {"heading": "A.5. Disentanglement Evaluation", "text": "A.5.1. VISUAL HEURISTIC DETAILSIn order to select the optimal value of \u03b2 for the \u03b2-UAE PCS models and to evaluate the suitability of the representations learned in Level 1 of our pipeline (in terms of the unbundling achieved), we used the visual inspection euristics described in Higgins et al., 2017. Heuristics included clustering trained \u03b2-UAE-based models based on the number of informative latents (estimated as the number of latent zi with an average derived standard deviation below 0.75).For each cluster, we examined the degree of learned unbundling by drawing conclusions from a number of seed images and then traversing each latent unit z {i} individually over three standard deviations from their average derived mean, while all other latent z {\\ i} remained fixed to their derived values. This allowed us to visually examine in 2016 whether each latent had learned to interpret a single factor in each unit."}, {"heading": "A.5.2. TRANSFER METRIC DETAILS", "text": "In the case of DeepMind Lab, we were able to use the soil truth markings that corresponded to the two variation factors of the object type and the background to design a proxy for the placement metric proposed in (Higgins et al., 2017). The procedure used consisted of the following steps: 1) Train the model under consideration on observations soU to learn FU, as described in Stage 1 of the DARLA pipeline. 2) Learn a linear model L: SzV \u2192 M \u00d7 N from the representations szV = FV (soV), where M \u00b2 {0, 1} corresponds to the amount of possible spaces and N \u00b2 {0, 1, 2, 3} corresponds to the amount of possible objects. Therefore, we learn a low VC dimension classifier to predict the space and object class from the latent representation of the model. Crucially, the linear model L is trained only on a subset of the cartesian product nevalistic plaiting."}, {"heading": "A.6. Background on RL Algorithms", "text": "In this appendix, we provide background information on the various RL algorithms that the DARLA framework has been tested in this Paper.A.6.1. DQN (DQN) (Mnih et al., 2015) is a variant of the Q-Learning algorithm (Watkins, 1989) that uses deep learning processes. It uses a neural network to approximate the action value function Q (s, a; \u03b8) using parameters. These parameters are updated by adding the mean square error of a 1-step lookahead loss LQ = E (rt + \u03b3maxa \u2032 Q (s, a \u2032, a \u2032) -D (s, a) -D (s) -D (s) -D (s) -D (s) -D (s) -D (s) -D (D) -D (D) -D (D) -D (D) -D (D) -D (D) -D (D) -D (D) -D (D) -D (D) -D (D (D) -D (D) -D (D) -D (D (D) -D (D) -D (D (D) -D (D) -D (D (D) -D (D) -D (D (D) -D (D) -D (D -D) -D (D (D) -D (D) -D (D -D) -D (D (D) -D (D) -D (D -D) -D (D -D (D) -D -D (D) -D -D -D -D -D (D) -D -D -D -D (D) -D -D -D -D) -D -D -D -D -D -D -D -D -D -D -D -D (D) -D -D -D -D -D -D -D -D -D -D -D -D -D -D -D -D -D -D -D -D -D -D -D -D -D -D -D -D -D) -D -D -D -D -D -D -D -D -D -D -D -D -D -D -D -D"}, {"heading": "A.6.4. EPISODIC CONTROL", "text": "At the end of each episode, QEC (s, a) is set to Rt if (st, at) / QEC, where Rt is the discounted rate of return. Otherwise, QEC (s, a) = max {QEC (s, a), Rt} is used. To generalize its policy to novel states that are not in QEC, EC uses a non-parametric search for Q-EC (s, a) = 1 k-k = 1QEC (si, a), where si, i = 1,... k k k states are the closest to the novel state. Like DQN, EC takes a concatenation of four frames as input. The EC algorithm is proposed as a model for fast hippocampal instance learning in the brain (Marr, 1971; Sutherland, 1989; McClully, 2003)."}, {"heading": "A.7. Source Task Performance Results", "text": "In order to compare the behavior of the models with respect to the source task, we examined the training curves (see Figures 7-10) and noted in particular their: 1. Asymptotic task performance, i.e. the rewards per episode at the point at which the agent approached taking into account.2. Data efficiency, i.e. how fast the training curve was able to achieve convergence, we found the following consistent trends within the results: 1. The use of DARLA gave the learning performance an initial thrust depending on the degree of disentanglement of the representation, which was particularly observed in A3C, see Figure 8.2. Baseline algorithms where F could be fine-tuned to the source task: 1. The use of DARLA resulted in an initial thrust of learning performance depending on the degree of disentanglement of the representation."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Abadi", "Martin", "Agarwal", "Ashish", "Paul Barham"], "venue": "Preliminary White Paper,", "citeRegEx": "Abadi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2015}, {"title": "Successor features for transfer in reinforcement learning", "author": ["Barreto", "Andr\u00e9", "Munos", "R\u00e9mi", "Schaul", "Tom", "Silver", "David"], "venue": "CoRR, abs/1606.05312,", "citeRegEx": "Barreto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Barreto et al\\.", "year": 2016}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "In IEEE Transactions on Pattern Analysis & Machine Intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Model-free episodic control", "author": ["Blundell", "Charles", "Uria", "Benigno", "Pritzel", "Alexander", "Li", "Yazhe", "Ruderman", "Avraham", "Leibo", "Joel Z", "Rae", "Jack", "Wierstra", "Daan", "Hassabis", "Demis"], "venue": null, "citeRegEx": "Blundell et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Blundell et al\\.", "year": 2016}, {"title": "Retinal image quality and postnatal visual experience during infancy", "author": ["Candy", "T. Rowan", "Wang", "Jingyun", "Ravikumar", "Sowmya"], "venue": "Optom Vis Sci,", "citeRegEx": "Candy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Candy et al\\.", "year": 2009}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Chen", "Xi", "Duan", "Yan", "Houthooft", "Rein", "Schulman", "John", "Sutskever", "Ilya", "Abbeel", "Pieter"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Discovering hidden factors of variation in deep networks", "author": ["Cheung", "Brian", "Levezey", "Jesse A", "Bansal", "Arjun K", "Olshausen", "Bruno A"], "venue": "In Proceedings of the International Conference on Learning Representations, Workshop Track,", "citeRegEx": "Cheung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheung et al\\.", "year": 2015}, {"title": "Transformation properties of learned visual representations", "author": ["T. Cohen", "M. Welling"], "venue": "In ICLR,", "citeRegEx": "Cohen and Welling,? \\Q2015\\E", "shortCiteRegEx": "Cohen and Welling", "year": 2015}, {"title": "Learning the irreducible representations of commutative lie groups", "author": ["Cohen", "Taco", "Welling", "Max"], "venue": null, "citeRegEx": "Cohen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2014}, {"title": "Learning transferable policies for monocular reactive mav control", "author": ["Daftry", "Shreyansh", "Bagnell", "J. Andrew", "Hebert", "Martial"], "venue": "International Symposium on Experimental Robotics,", "citeRegEx": "Daftry et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Daftry et al\\.", "year": 2016}, {"title": "Model-free reinforcement learning with continuous action in practice", "author": ["Degris", "Thomas", "Pilarski Patrick M", "Sutton", "Richard S"], "venue": "American Control Conference (ACC),", "citeRegEx": "Degris et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Degris et al\\.", "year": 2012}, {"title": "Disentangling factors of variation via generative entangling", "author": ["G. Desjardins", "A. Courville", "Y. Bengio"], "venue": null, "citeRegEx": "Desjardins et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Desjardins et al\\.", "year": 2012}, {"title": "Generating images with perceptual similarity metrics based on deep networks", "author": ["Dosovitskiy", "Alexey", "Brox", "Thomas"], "venue": "arxiv,", "citeRegEx": "Dosovitskiy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2016}, {"title": "Deep spatial autoencoders for visuomotor learning", "author": ["Finn", "Chelsea", "Tan", "Xin Yu", "Duan", "Yan", "Darrell", "Trevor", "Levine", "Sergey", "Abbeel", "Pieter"], "venue": "arxiv,", "citeRegEx": "Finn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2015}, {"title": "Generalizing skills with semi-supervised reinforcement learning", "author": ["Finn", "Chelsea", "Yu", "Tianhe", "Fu", "Justin", "Abbeel", "Pieter", "Levine", "Sergey"], "venue": null, "citeRegEx": "Finn et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2017}, {"title": "Towards deep symbolic reinforcement learning", "author": ["Garnelo", "Marta", "Arulkumaran", "Kai", "Shanahan", "Murray"], "venue": null, "citeRegEx": "Garnelo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garnelo et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. WardeFarley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Learning to linearize under uncertainty", "author": ["Goroshin", "Ross", "Mathieu", "Michael", "LeCun", "Yann"], "venue": null, "citeRegEx": "Goroshin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goroshin et al\\.", "year": 2015}, {"title": "Efficient bayesadaptive reinforcement learning using sample-based search", "author": ["Guez", "Arthur", "Silver", "David", "Dayan", "Peter"], "venue": null, "citeRegEx": "Guez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Guez et al\\.", "year": 2012}, {"title": "Learning invariant feature spaces to transfer skills with reinforcement learning", "author": ["Gupta", "Abhishek", "Devin", "Coline", "Liu", "YuXuan", "Abbeel", "Pieter", "Levine", "Sergey"], "venue": null, "citeRegEx": "Gupta et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2017}, {"title": "Learning continuous control policies by stochastic value gradients", "author": ["Heess", "Nicolas", "Wayne", "Gregory", "Silver", "David", "Lillicrap", "Timothy P", "Erez", "Tom", "Tassa", "Yuval"], "venue": null, "citeRegEx": "Heess et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2015}, {"title": "Beta-vae: Learning basic visual concepts with a constrained variational framework", "author": ["Higgins", "Irina", "Matthey", "Loic", "Pal", "Arka", "Burgess", "Christopher", "Glorot", "Xavier", "Botvinick", "Matthew", "Mohamed", "Shakir", "Lerchner", "Alexander"], "venue": "In ICLR,", "citeRegEx": "Higgins et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Higgins et al\\.", "year": 2017}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["Jaderberg", "Max", "Mnih", "Volodymyr", "Czarnecki", "Wojciech Marian", "Schaul", "Tom", "Leibo", "Joel Z", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": null, "citeRegEx": "Jaderberg et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2017}, {"title": "Bayesian representation learning with oracle constraints", "author": ["Karaletsos", "Theofanis", "Belongie", "Serge", "Rtsch", "Gunnar"], "venue": null, "citeRegEx": "Karaletsos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Karaletsos et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "Ba", "Jimmy"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "ICLR,", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Deep convolutional inverse graphics", "author": ["Kulkarni", "Tejas", "Whitney", "William", "Kohli", "Pushmeet", "Tenenbaum", "Joshua"], "venue": null, "citeRegEx": "Kulkarni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "Building machines that learn and think like people", "author": ["Lake", "Brenden M", "Ullman", "Tomer D", "Tenenbaum", "Joshua B", "Gershman", "Samuel J"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2016}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["Larsen", "Anders Boesen Lindbo", "Snderby", "Sren Kaae", "Larochelle", "Hugo", "Winther", "Ole"], "venue": null, "citeRegEx": "Larsen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Larsen et al\\.", "year": 2016}, {"title": "Development of visual acuity and contrast sensitivity in children", "author": ["Leat", "Susan J", "Yadav", "Naveen K", "Irving", "Elizabeth L"], "venue": "Journal of Optometry,", "citeRegEx": "Leat et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Leat et al\\.", "year": 2009}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["Levine", "Sergey", "Abbeel", "Pieter"], "venue": null, "citeRegEx": "Levine et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2014}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": null, "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Predictive representations of state", "author": ["Littman", "Michael L", "Sutton", "Richard S", "Singh", "Satinder"], "venue": null, "citeRegEx": "Littman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Littman et al\\.", "year": 2001}, {"title": "Simple memory: A theory for archicortex", "author": ["D. Marr"], "venue": "Philosophical Transactions of the Royal Society of London, pp", "citeRegEx": "Marr,? \\Q1971\\E", "shortCiteRegEx": "Marr", "year": 1971}, {"title": "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory", "author": ["McClelland", "James L", "McNaughton", "Bruce L", "OReilly", "Randall C"], "venue": "Psychological review,", "citeRegEx": "McClelland et al\\.,? \\Q1995\\E", "shortCiteRegEx": "McClelland et al\\.", "year": 1995}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David S", "Rusu", "Andrei A"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Badia", "Adri Puigdomnech", "Mirza", "Mehdi", "Graves", "Alex", "Lillicrap", "Timothy P", "Harley", "Tim", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "URL https://arxiv. org/pdf/1602.01783.pdf", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Incremental semantically grounded learning from demonstration", "author": ["Niekum", "Scott", "Chitta", "Sachin", "Barto", "Andrew G", "Marthi", "Bhaskara", "Osentoski", "Sarah"], "venue": "Robotics: Science and Systems,", "citeRegEx": "Niekum et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Niekum et al\\.", "year": 2013}, {"title": "Modeling hippocampal and neocortical contributions to recognition memory: a complementary-learning-systems approach", "author": ["Norman", "Kenneth A", "O\u2019Reilly", "Randall C"], "venue": "Psychological review,", "citeRegEx": "Norman et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Norman et al\\.", "year": 2003}, {"title": "A survey on transfer learning", "author": ["Pan", "Sinno Jialin", "Yang", "Quiang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Pan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2009}, {"title": "Actormimic: Deep multitask and transfer reinforcement learning", "author": ["Parisotto", "Emilio", "Ba", "Jimmy", "Salakhutdinov", "Ruslan"], "venue": null, "citeRegEx": "Parisotto et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Parisotto et al\\.", "year": 2015}, {"title": "Context encoders: Feature learning by inpainting", "author": ["Pathak", "Deepak", "Kr\u00e4henb\u00fchl", "Philipp", "Donahue", "Jeff", "Darrell", "Trevor", "Efros", "Alexei A"], "venue": "CoRR, abs/1604.07379,", "citeRegEx": "Pathak et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pathak et al\\.", "year": 2016}, {"title": "Efficient dynamic programming-based learning for control", "author": ["J. Peng"], "venue": "PhD thesis, Northeastern University,", "citeRegEx": "Peng,? \\Q1993\\E", "shortCiteRegEx": "Peng", "year": 1993}, {"title": "Incremental multi-step qlearning", "author": ["Peng", "Jing", "Williams", "Ronald J"], "venue": "Machine Learning,", "citeRegEx": "Peng et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Peng et al\\.", "year": 1996}, {"title": "Unsupervised learning of state representations for multiple tasks", "author": ["Raffin", "Antonin", "Hfer", "Sebastian", "Jonschkowski", "Rico", "Brock", "Oliver", "Stulp", "Freek"], "venue": null, "citeRegEx": "Raffin et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Raffin et al\\.", "year": 2017}, {"title": "Attend, adapt and transfer: Attentive deep architecture for adaptive transfer from multiple sources in the same domain", "author": ["Rajendran", "Janarthanan", "Lakshminarayanan", "Aravind", "Khapra", "P Mitesh M", "Prasanna", "Ravindran", "Balaraman"], "venue": null, "citeRegEx": "Rajendran et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Rajendran et al\\.", "year": 2017}, {"title": "Learning to disentangle factors of variation with manifold interaction", "author": ["Reed", "Scott", "Sohn", "Kihyuk", "Zhang", "Yuting", "Lee", "Honglak"], "venue": null, "citeRegEx": "Reed et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2014}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": null, "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "A survey of inductive biases for factorial Representation-Learning. arXiv, 2016", "author": ["Ridgeway", "Karl"], "venue": "URL http:// arxiv.org/abs/1612.05299", "citeRegEx": "Ridgeway and Karl.,? \\Q2016\\E", "shortCiteRegEx": "Ridgeway and Karl.", "year": 2016}, {"title": "High-dimensional probability estimation with deep density models", "author": ["Rippel", "Oren", "Adams", "Ryan Prescott"], "venue": null, "citeRegEx": "Rippel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rippel et al\\.", "year": 2013}, {"title": "Sim-to-real robot learning from pixels with progressive nets", "author": ["Rusu", "Andrei A", "Vecerik", "Matej", "Rothrl", "Thomas", "Heess", "Nicolas", "Pascanu", "Razvan", "Hadsell", "Raia"], "venue": null, "citeRegEx": "Rusu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Learning factorial codes by predictability minimization", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1992\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1992}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Moritz", "Philipp", "Jordan", "Michael I", "Abbeel", "Pieter"], "venue": null, "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["Schulman", "John", "Moritz", "Philipp", "Levine", "Sergey", "Jordan", "Michael", "Abbeel", "Pieter"], "venue": null, "citeRegEx": "Schulman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2016}, {"title": "Configural association theory: The role of the hippocampal formation in learning, memory, and amnesia", "author": ["Sutherland", "Robert J", "Rudy", "Jerry W"], "venue": "Psychobiology, 17:129144,", "citeRegEx": "Sutherland et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Sutherland et al\\.", "year": 1989}, {"title": "Reinforcement Learning: An Introduction", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "An experts algorithm for transfer learning", "author": ["Talvitie", "Erik", "Singh", "Satinder"], "venue": "In Proceedings of the 20th international joint conference on Artifical intelligence,", "citeRegEx": "Talvitie et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Talvitie et al\\.", "year": 2007}, {"title": "Domain randomization for transferring deep neural networks from simulation to the real world", "author": ["Tobin", "Josh", "Fong", "Rachel", "Ray", "Alex", "Schneider", "Jonas", "Zaremba", "Wojciech", "Abbeel", "Pieter"], "venue": "arxiv,", "citeRegEx": "Tobin et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Tobin et al\\.", "year": 2017}, {"title": "Mujoco: A physics engine for model-based control", "author": ["E. Todorov", "T. Erez", "Y. Tassa"], "venue": null, "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Longlasting perceptual priming and semantic learning in amnesia: a case experiment", "author": ["Tulving", "Endel", "CA Hayman", "Macdonald", "Carol A"], "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition,", "citeRegEx": "Tulving et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Tulving et al\\.", "year": 1991}, {"title": "Adapting deep visuomotor representations with weak pairwise constraints", "author": ["Tzeng", "Eric", "Devin", "Coline", "Hoffman", "Judy", "Finn", "Chelsea", "Abbeel", "Pieter", "Levine", "Sergey", "Saenko", "Kate", "Darrell", "Trevor"], "venue": null, "citeRegEx": "Tzeng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tzeng et al\\.", "year": 2016}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": null, "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Improving generative adversarial networks with denoising feature matching", "author": ["Warde-Farley", "David", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Warde.Farley et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Warde.Farley et al\\.", "year": 2017}, {"title": "Learning from delayed rewards", "author": ["Watkins", "Christopher John Cornish Hellaby"], "venue": "PhD thesis,", "citeRegEx": "Watkins and Hellaby.,? \\Q1989\\E", "shortCiteRegEx": "Watkins and Hellaby.", "year": 1989}, {"title": "Understanding visual concepts with continuation learning. arXiv, 2016", "author": ["Whitney", "William F", "Chang", "Michael", "Kulkarni", "Tejas", "Tenenbaum", "Joshua B"], "venue": "URL http://arxiv. org/pdf/1602.06822.pdf", "citeRegEx": "Whitney et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Whitney et al\\.", "year": 2016}, {"title": "Weakly-supervised disentangling with recurrent transformations for 3d view synthesis", "author": ["Yang", "Jimei", "Reed", "Scott", "Ming-Hsuan", "Lee", "Honglak"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 36, "context": "deep learning and its successful combination with RL has enabled end-to-end learning of such embeddings directly from raw inputs, sparking success in a wide variety of previously challenging RL domains (Mnih et al., 2015; 2016; Jaderberg et al., 2017).", "startOffset": 202, "endOffset": 251}, {"referenceID": 22, "context": "deep learning and its successful combination with RL has enabled end-to-end learning of such embeddings directly from raw inputs, sparking success in a wide variety of previously challenging RL domains (Mnih et al., 2015; 2016; Jaderberg et al., 2017).", "startOffset": 202, "endOffset": 251}, {"referenceID": 15, "context": "These include data inefficiency, the reactive nature and general brittleness of learnt policies to changes in input data distribution, and lack of model interpretability (Garnelo et al., 2016; Lake et al., 2016).", "startOffset": 170, "endOffset": 211}, {"referenceID": 28, "context": "These include data inefficiency, the reactive nature and general brittleness of learnt policies to changes in input data distribution, and lack of model interpretability (Garnelo et al., 2016; Lake et al., 2016).", "startOffset": 170, "endOffset": 211}, {"referenceID": 2, "context": "This paper focuses on one of these outstanding issues: the ability of RL agents to deal with changes to the input distribution, a form of transfer learning known as domain adaptation (Bengio et al., 2013).", "startOffset": 183, "endOffset": 204}, {"referenceID": 13, "context": "Past attempts to build RL agents with strong domain adaptation performance highlighted the importance of learning good internal representations of raw observations (Finn et al., 2015; Raffin et al., 2017; Pan & Yang, 2009; Barreto et al., 2016; Littman et al., 2001).", "startOffset": 164, "endOffset": 266}, {"referenceID": 45, "context": "Past attempts to build RL agents with strong domain adaptation performance highlighted the importance of learning good internal representations of raw observations (Finn et al., 2015; Raffin et al., 2017; Pan & Yang, 2009; Barreto et al., 2016; Littman et al., 2001).", "startOffset": 164, "endOffset": 266}, {"referenceID": 1, "context": "Past attempts to build RL agents with strong domain adaptation performance highlighted the importance of learning good internal representations of raw observations (Finn et al., 2015; Raffin et al., 2017; Pan & Yang, 2009; Barreto et al., 2016; Littman et al., 2001).", "startOffset": 164, "endOffset": 266}, {"referenceID": 33, "context": "Past attempts to build RL agents with strong domain adaptation performance highlighted the importance of learning good internal representations of raw observations (Finn et al., 2015; Raffin et al., 2017; Pan & Yang, 2009; Barreto et al., 2016; Littman et al., 2001).", "startOffset": 164, "endOffset": 266}, {"referenceID": 61, "context": "Typically, these approaches tried to align the source and target domain representations by utilising observation and reward signals from both domains (Tzeng et al., 2016; Daftry et al., 2016; Parisotto et al., 2015; Guez et al., 2012; Talvitie & Singh, 2007; Niekum et al., 2013; Gupta et al., 2017; Finn et al., 2017; Rajendran et al., 2017).", "startOffset": 150, "endOffset": 342}, {"referenceID": 9, "context": "Typically, these approaches tried to align the source and target domain representations by utilising observation and reward signals from both domains (Tzeng et al., 2016; Daftry et al., 2016; Parisotto et al., 2015; Guez et al., 2012; Talvitie & Singh, 2007; Niekum et al., 2013; Gupta et al., 2017; Finn et al., 2017; Rajendran et al., 2017).", "startOffset": 150, "endOffset": 342}, {"referenceID": 41, "context": "Typically, these approaches tried to align the source and target domain representations by utilising observation and reward signals from both domains (Tzeng et al., 2016; Daftry et al., 2016; Parisotto et al., 2015; Guez et al., 2012; Talvitie & Singh, 2007; Niekum et al., 2013; Gupta et al., 2017; Finn et al., 2017; Rajendran et al., 2017).", "startOffset": 150, "endOffset": 342}, {"referenceID": 18, "context": "Typically, these approaches tried to align the source and target domain representations by utilising observation and reward signals from both domains (Tzeng et al., 2016; Daftry et al., 2016; Parisotto et al., 2015; Guez et al., 2012; Talvitie & Singh, 2007; Niekum et al., 2013; Gupta et al., 2017; Finn et al., 2017; Rajendran et al., 2017).", "startOffset": 150, "endOffset": 342}, {"referenceID": 38, "context": "Typically, these approaches tried to align the source and target domain representations by utilising observation and reward signals from both domains (Tzeng et al., 2016; Daftry et al., 2016; Parisotto et al., 2015; Guez et al., 2012; Talvitie & Singh, 2007; Niekum et al., 2013; Gupta et al., 2017; Finn et al., 2017; Rajendran et al., 2017).", "startOffset": 150, "endOffset": 342}, {"referenceID": 19, "context": "Typically, these approaches tried to align the source and target domain representations by utilising observation and reward signals from both domains (Tzeng et al., 2016; Daftry et al., 2016; Parisotto et al., 2015; Guez et al., 2012; Talvitie & Singh, 2007; Niekum et al., 2013; Gupta et al., 2017; Finn et al., 2017; Rajendran et al., 2017).", "startOffset": 150, "endOffset": 342}, {"referenceID": 14, "context": "Typically, these approaches tried to align the source and target domain representations by utilising observation and reward signals from both domains (Tzeng et al., 2016; Daftry et al., 2016; Parisotto et al., 2015; Guez et al., 2012; Talvitie & Singh, 2007; Niekum et al., 2013; Gupta et al., 2017; Finn et al., 2017; Rajendran et al., 2017).", "startOffset": 150, "endOffset": 342}, {"referenceID": 46, "context": "Typically, these approaches tried to align the source and target domain representations by utilising observation and reward signals from both domains (Tzeng et al., 2016; Daftry et al., 2016; Parisotto et al., 2015; Guez et al., 2012; Talvitie & Singh, 2007; Niekum et al., 2013; Gupta et al., 2017; Finn et al., 2017; Rajendran et al., 2017).", "startOffset": 150, "endOffset": 342}, {"referenceID": 14, "context": "In many scenarios, such as robotics, this reliance on target domain information can be problematic, as the data may be expensive or difficult to obtain (Finn et al., 2017; Rusu et al., 2016).", "startOffset": 152, "endOffset": 190}, {"referenceID": 51, "context": "In many scenarios, such as robotics, this reliance on target domain information can be problematic, as the data may be expensive or difficult to obtain (Finn et al., 2017; Rusu et al., 2016).", "startOffset": 152, "endOffset": 190}, {"referenceID": 28, "context": "On the other hand, policies learnt exclusively on the source domain using existing deep RL approaches that have few constraints on the nature of the learnt representations often overfit to the source input distribution, resulting in poor domain adaptation performance (Lake et al., 2016; Rusu et al., 2016).", "startOffset": 268, "endOffset": 306}, {"referenceID": 51, "context": "On the other hand, policies learnt exclusively on the source domain using existing deep RL approaches that have few constraints on the nature of the learnt representations often overfit to the source input distribution, resulting in poor domain adaptation performance (Lake et al., 2016; Rusu et al., 2016).", "startOffset": 268, "endOffset": 306}, {"referenceID": 27, "context": "We think of these factors as a set of high-level parameters that can be used by a world graphics engine to generate a particular natural visual scene (Kulkarni et al., 2015).", "startOffset": 150, "endOffset": 173}, {"referenceID": 11, "context": "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).", "startOffset": 174, "endOffset": 506}, {"referenceID": 27, "context": "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).", "startOffset": 174, "endOffset": 506}, {"referenceID": 47, "context": "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).", "startOffset": 174, "endOffset": 506}, {"referenceID": 66, "context": "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).", "startOffset": 174, "endOffset": 506}, {"referenceID": 17, "context": "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).", "startOffset": 174, "endOffset": 506}, {"referenceID": 27, "context": "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).", "startOffset": 174, "endOffset": 506}, {"referenceID": 6, "context": "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).", "startOffset": 174, "endOffset": 506}, {"referenceID": 65, "context": "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).", "startOffset": 174, "endOffset": 506}, {"referenceID": 23, "context": "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).", "startOffset": 174, "endOffset": 506}, {"referenceID": 5, "context": "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).", "startOffset": 174, "endOffset": 506}, {"referenceID": 21, "context": "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).", "startOffset": 174, "endOffset": 506}, {"referenceID": 2, "context": "Disentangled representations are defined as interpretable, factorised latent representations where either a single latent or a group of latent units are sensitive to changes in single ground truth factors of variation used to generate the visual world, while being invariant to changes in other factors (Bengio et al., 2013).", "startOffset": 303, "endOffset": 324}, {"referenceID": 2, "context": "The theoretical utility of disentangled representations for supervised and reinforcement learning has been described before (Bengio et al., 2013; Higgins et al., 2017; Ridgeway, 2016); however, to our knowledge, it has not been empirically validated to date.", "startOffset": 124, "endOffset": 183}, {"referenceID": 21, "context": "The theoretical utility of disentangled representations for supervised and reinforcement learning has been described before (Bengio et al., 2013; Higgins et al., 2017; Ridgeway, 2016); however, to our knowledge, it has not been empirically validated to date.", "startOffset": 124, "endOffset": 183}, {"referenceID": 30, "context": "by utilising a stream of raw unlabelled observations \u2013 not unlike human babies in their first few months of life (Leat et al., 2009; Candy et al., 2009).", "startOffset": 113, "endOffset": 152}, {"referenceID": 4, "context": "by utilising a stream of raw unlabelled observations \u2013 not unlike human babies in their first few months of life (Leat et al., 2009; Candy et al., 2009).", "startOffset": 113, "endOffset": 152}, {"referenceID": 59, "context": "These effects hold consistently across a number of different RL environments (DeepMind Lab and Jaco/MuJoCo: Beattie et al., 2016; Todorov et al., 2012) and algorithms (DQN, A3C and Episodic Control: Mnih et al.", "startOffset": 77, "endOffset": 151}, {"referenceID": 3, "context": ", 2012) and algorithms (DQN, A3C and Episodic Control: Mnih et al., 2015; 2016; Blundell et al., 2016).", "startOffset": 23, "endOffset": 102}, {"referenceID": 59, "context": "For example, consider a domain adaptation scenario for the Jaco robotic arm, where the MuJoCo (Todorov et al., 2012) simulation of the arm is the source domain, and the real world setting is the target domain.", "startOffset": 94, "endOffset": 116}, {"referenceID": 51, "context": "The state spaces (raw pixels) of the source and the target domains differ significantly due to the perceptual-reality gap (Rusu et al., 2016); that is to say, SS 6= ST .", "startOffset": 122, "endOffset": 141}, {"referenceID": 28, "context": "We contend that this is a reasonable assumption that permits inclusion of many interesting problems, including being able to characterise our own reality (Lake et al., 2016).", "startOffset": 154, "endOffset": 173}, {"referenceID": 21, "context": "1) Learn to see (unsupervised learning of FU ) \u2013 the task of inferring a factorised set of generative factors S \u015c = \u015c from observations S is the goal of the extensive disentangled factor learning literature (e.g. Chen et al., 2016; Higgins et al., 2017).", "startOffset": 207, "endOffset": 253}, {"referenceID": 21, "context": "In order to learn FU , DARLA utilises \u03b2-VAE (Higgins et al., 2017), a state-of-the-art unsupervised model for automated discovery of factorised latent representations from raw image data.", "startOffset": 44, "endOffset": 66}, {"referenceID": 48, "context": "\u03b2-VAE is a modification of the variational autoencoder framework (Kingma & Welling, 2014; Rezende et al., 2014) that controls the nature of the learnt latent representations by introducing an adjustable hyperparameter \u03b2 to balance reconstruction accuracy with latent channel capacity and independence constraints.", "startOffset": 65, "endOffset": 111}, {"referenceID": 21, "context": "Well-chosen values of \u03b2 - usually larger than one (\u03b2 > 1) - typically result in more disentangled latent representations z by limiting the capacity of the latent information channel, and hence encouraging a more efficient factorised encoding through the increased pressure to match the isotropic unit Gaussian prior p(z) (Higgins et al., 2017).", "startOffset": 321, "endOffset": 343}, {"referenceID": 16, "context": "The shortcomings of calculating the log-likelihood term Eq\u03c6(z|x)[log p\u03b8(x|z)] on a per-pixel basis are known and have been addressed in the past by calculating the reconstruction cost in an abstract, high-level feature space given by another neural network model, such as a GAN (Goodfellow et al., 2014) or a pre-trained AlexNet (Krizhevsky et al.", "startOffset": 278, "endOffset": 303}, {"referenceID": 26, "context": ", 2014) or a pre-trained AlexNet (Krizhevsky et al., 2012; Larsen et al., 2016; Dosovitskiy & Brox, 2016; Warde-Farley & Bengio, 2017).", "startOffset": 33, "endOffset": 134}, {"referenceID": 29, "context": ", 2014) or a pre-trained AlexNet (Krizhevsky et al., 2012; Larsen et al., 2016; Dosovitskiy & Brox, 2016; Warde-Farley & Bengio, 2017).", "startOffset": 33, "endOffset": 134}, {"referenceID": 62, "context": "In practice we found that pre-training a denoising autoencoder (Vincent et al., 2010) on data from the visual pre-training MDP DU \u2208 M worked best as the reconstruction targets for \u03b2-VAE to match (see Fig.", "startOffset": 63, "endOffset": 85}, {"referenceID": 48, "context": "2 we are no longer optimising the variational lower bound, and \u03b2-VAEDAE with \u03b2 = 1 loses its equivalence to the Variational Autoencoder (VAE) framework as proposed by (Kingma & Welling, 2014; Rezende et al., 2014).", "startOffset": 167, "endOffset": 213}, {"referenceID": 3, "context": "We used various RL algorithms (DQN, A3C and Episodic Control: Mnih et al., 2015; 2016; Blundell et al., 2016) to learn the source policy \u03c0 during stage two of the pipeline using the latent states s acquired by \u03b2-VAE based models during stage one of the DARLA pipeline.", "startOffset": 30, "endOffset": 109}, {"referenceID": 36, "context": "Deep Q Network (DQN) (Mnih et al., 2015) is a variant of the Q-learning algorithm (Watkins, 1989) that utilises deep learning.", "startOffset": 21, "endOffset": 40}, {"referenceID": 37, "context": "Asynchronous Advantage Actor-Critic (A3C) (Mnih et al., 2016) is an asynchronous implementation of the advantage actor-critic paradigm (Sutton & Barto, 1998; Degris & Sutton, 2012), where separate threads run in parallel and perform updates to shared parameters.", "startOffset": 42, "endOffset": 61}, {"referenceID": 3, "context": "Model-Free Episodic Control (EC) (Blundell et al., 2016) was proposed as a complementary learning system to the other RL algorithms described above.", "startOffset": 33, "endOffset": 56}, {"referenceID": 22, "context": "We also compared our approach to that of UNREAL (Jaderberg et al., 2017), a recently proposed RL algorithm which also attempts to utilise unsupervised data in the environment.", "startOffset": 48, "endOffset": 72}, {"referenceID": 37, "context": "The UNREAL agent takes as a base an LSTM A3C agent (Mnih et al., 2016) and augments it with a number of unsupervised auxiliary tasks that make use of the rich perceptual data available to the agent besides the (sometimes very sparse) extrinsic reward signals.", "startOffset": 51, "endOffset": 70}, {"referenceID": 59, "context": "We use the Jaco arm with a matching MuJoCo simulation environment (Todorov et al., 2012) in two domain adaptation scenarios: simulation to simulation (sim2sim) and simulation to reality (sim2real).", "startOffset": 66, "endOffset": 88}, {"referenceID": 51, "context": "The sim2real setup, on the other hand, is based on identical processes GS = GT , but different observation simulators SimS 6= SimT corresponding to the MuJoCo simulation and the real world, which results in the so-called \u2018perceptual reality gap\u2019 (Rusu et al., 2016).", "startOffset": 246, "endOffset": 265}, {"referenceID": 3, "context": "Pre-trained \u03b2-VAEDAE from stage one was used as the \u2018vision\u2019 part of various RL algorithms (DQN, A3C and Episodic Control: Mnih et al., 2015; 2016; Blundell et al., 2016) to learn a source policy \u03c0S that picks up balloons and avoids cakes in both the green and the pink rooms, and picks up cans and avoids hats in the green rooms.", "startOffset": 91, "endOffset": 170}, {"referenceID": 59, "context": "We used frames from an RGB camera facing a robotic Jaco arm, or a matching rendered camera view from a MuJoCo physics simulation environment (Todorov et al., 2012) to investigate the performance of DARLA in two domain adaptation scenarios: 1) simulation to simulation (sim2sim), and 2) simulation to reality (sim2real).", "startOffset": 141, "endOffset": 163}, {"referenceID": 53, "context": "The sim2real setup is of particular importance, since the progress that deep RL has brought to control tasks in simulation (Schulman et al., 2015; Mnih et al., 2016; Levine & Abbeel, 2014; Heess et al., 2015; Lillicrap et al., 2015; Schulman et al., 2016) has not yet translated as well to reality, despite various attempts (Tobin et al.", "startOffset": 123, "endOffset": 255}, {"referenceID": 37, "context": "The sim2real setup is of particular importance, since the progress that deep RL has brought to control tasks in simulation (Schulman et al., 2015; Mnih et al., 2016; Levine & Abbeel, 2014; Heess et al., 2015; Lillicrap et al., 2015; Schulman et al., 2016) has not yet translated as well to reality, despite various attempts (Tobin et al.", "startOffset": 123, "endOffset": 255}, {"referenceID": 20, "context": "The sim2real setup is of particular importance, since the progress that deep RL has brought to control tasks in simulation (Schulman et al., 2015; Mnih et al., 2016; Levine & Abbeel, 2014; Heess et al., 2015; Lillicrap et al., 2015; Schulman et al., 2016) has not yet translated as well to reality, despite various attempts (Tobin et al.", "startOffset": 123, "endOffset": 255}, {"referenceID": 32, "context": "The sim2real setup is of particular importance, since the progress that deep RL has brought to control tasks in simulation (Schulman et al., 2015; Mnih et al., 2016; Levine & Abbeel, 2014; Heess et al., 2015; Lillicrap et al., 2015; Schulman et al., 2016) has not yet translated as well to reality, despite various attempts (Tobin et al.", "startOffset": 123, "endOffset": 255}, {"referenceID": 54, "context": "The sim2real setup is of particular importance, since the progress that deep RL has brought to control tasks in simulation (Schulman et al., 2015; Mnih et al., 2016; Levine & Abbeel, 2014; Heess et al., 2015; Lillicrap et al., 2015; Schulman et al., 2016) has not yet translated as well to reality, despite various attempts (Tobin et al.", "startOffset": 123, "endOffset": 255}, {"referenceID": 58, "context": ", 2016) has not yet translated as well to reality, despite various attempts (Tobin et al., 2017; Tzeng et al., 2016; Daftry et al., 2016; Finn et al., 2015; Rusu et al., 2016).", "startOffset": 76, "endOffset": 175}, {"referenceID": 61, "context": ", 2016) has not yet translated as well to reality, despite various attempts (Tobin et al., 2017; Tzeng et al., 2016; Daftry et al., 2016; Finn et al., 2015; Rusu et al., 2016).", "startOffset": 76, "endOffset": 175}, {"referenceID": 9, "context": ", 2016) has not yet translated as well to reality, despite various attempts (Tobin et al., 2017; Tzeng et al., 2016; Daftry et al., 2016; Finn et al., 2015; Rusu et al., 2016).", "startOffset": 76, "endOffset": 175}, {"referenceID": 13, "context": ", 2016) has not yet translated as well to reality, despite various attempts (Tobin et al., 2017; Tzeng et al., 2016; Daftry et al., 2016; Finn et al., 2015; Rusu et al., 2016).", "startOffset": 76, "endOffset": 175}, {"referenceID": 51, "context": ", 2016) has not yet translated as well to reality, despite various attempts (Tobin et al., 2017; Tzeng et al., 2016; Daftry et al., 2016; Finn et al., 2015; Rusu et al., 2016).", "startOffset": 76, "endOffset": 175}, {"referenceID": 22, "context": "Finally, we compared the performance of DARLA to an UNREAL (Jaderberg et al., 2017) agent with the same architecture.", "startOffset": 59, "endOffset": 83}], "year": 2017, "abstractText": "Domain adaptation is an important open problem in deep reinforcement learning (RL). In many scenarios of interest data is hard to obtain, so agents may learn a source policy in a setting where data is readily available, with the hope that it generalises well to the target domain. We propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act. DARLA\u2019s vision is based on learning a disentangled representation of the observed environment. Once DARLA can see, it is able to acquire source policies that are robust to many domain shifts even with no access to the target domain. DARLA significantly outperforms conventional baselines in zero-shot domain adaptation scenarios, an effect that holds across a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C and EC).", "creator": "LaTeX with hyperref package"}}}