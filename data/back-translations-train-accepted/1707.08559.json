{"id": "1707.08559", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jul-2017", "title": "Video Highlight Prediction Using Audience Chat Reactions", "abstract": "Sports channel video portals offer an exciting domain for research on multimodal, multilingual analysis. We present methods addressing the problem of automatic video highlight prediction based on joint visual features and textual analysis of the real-world audience discourse with complex slang, in both English and traditional Chinese. We present a novel dataset based on League of Legends championships recorded from North American and Taiwanese Twitch.tv channels (will be released for further research), and demonstrate strong results on these using multimodal, character-level CNN-RNN model architectures.", "histories": [["v1", "Wed, 26 Jul 2017 17:44:38 GMT  (638kb,D)", "http://arxiv.org/abs/1707.08559v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV cs.LG cs.MM", "authors": ["cheng-yang fu", "joon lee", "mohit bansal", "alexander c berg"], "accepted": true, "id": "1707.08559"}, "pdf": {"name": "1707.08559.pdf", "metadata": {"source": "CRF", "title": "Video Highlight Prediction Using Audience Chat Reactions", "authors": ["Cheng-Yang Fu", "Joon Lee", "Mohit Bansal", "Alexander C. Berg"], "emails": ["aberg}@cs.unc.edu"], "sections": [{"heading": "1 Introduction", "text": "In recent years, it has become apparent that the number of people living in the city has steadily increased in recent years, both in the cities and in the cities where they live."}, {"heading": "2 Related Work", "text": "We briefly discuss a small sample of related work on language and vision datasets, summarizing several interesting issues in 2016 and highlighting predictions. There has been a wave of vision and language datasets focusing on captions in recent years, (Rashtchian et al., 2010; Ordonez et al., 2011; Lin et al., 2014), followed by efforts to focus on more specific parts of the images (Krishna et al., 2016), or referring to expressions (Kazemzadeh et al., 2014), or to the broader context (Huang et al., 2016). For video, similar efforts have been collected descriptions (Chen and Dolan, 2011), while other existing descriptive video service (DVS) sources (Rohrbach et al., 2015; Torabi et al., 2015). Beyond descriptions, other datasets we use questions about images and language (Antol et al., 2015)."}, {"heading": "3 Data Collection", "text": "Our record includes 218 videos of NALCS and 103 of LMS for a total of 321 videos from week 1 to week 9 in each tournament. Each week, there are 10 matches for NALCS and 6 matches for LMS. Matches are best of 3, so consist of two games or three games. The first and third games are used for training, the second games in the first 4 weeks are used as validation and the rest of the second games are used as testing. Table 1 lists the numbers of videos on the turn, validation and test subsets.Each game ranges from 30 to 50 minutes in length, the image and chat data are linked to find the specific timestamp of the game."}, {"heading": "4 Model", "text": "In this section, we explain the proposed models and components. We first describe the notation and definition of the problem, plus the evaluation of the metrics used. Next, we explain our vision model VCNN-LSTM and the language model L-Char-LSTM. Finally, we describe the common multimodal model lv-LSTM.Problem definition Our basic task is to determine whether a frame of the full input video should be labeled as part of the output highlights or not. To simplify our notation, we use X = {x1, x2, xt} to name a sequence of features for frames. Chats are expressed as C = {c1, ts1), where each chat comes with a timestamp. Methods take the image attributes and / or chats and predict labels for the frames, Y = {y1, y2, yt}."}, {"heading": "5 Experiments and Results", "text": "In recent years it has become clear that the number of unemployed in Germany has fallen by more than half compared to the previous year."}, {"heading": "6 Conclusion", "text": "We introduced a new dataset and multimodal methods for predicting highlights based on visual cues and textual audience reactions in multiple languages. We hope that our new dataset can inspire further multilingual multimodal research."}, {"heading": "Acknowledgments", "text": "We thank Tamara Berg, Phil Ammirato and the reviewers for their helpful suggestions and thank NSF 1533771 for their support."}], "references": [{"title": "VQA: Visual Question Answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh."], "venue": "ICCV.", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Vsumm: A mechanism designed to produce static video summaries and a novel evaluation method", "author": ["Sandra E.F. de Avila", "Ana P.B. Lopes", "Antonio da Luz Jr.", "Arnaldo de A. Arajo."], "venue": "Pattern Recognition Letters.", "citeRegEx": "Avila et al\\.,? 2011", "shortCiteRegEx": "Avila et al\\.", "year": 2011}, {"title": "Soccer videos highlight prediction and annotation in real time", "author": ["M. Bertini", "A. Del Bimbo", "W. Nunziati."], "venue": "ICIAP.", "citeRegEx": "Bertini et al\\.,? 2005", "shortCiteRegEx": "Bertini et al\\.", "year": 2005}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["David L. Chen", "William B. Dolan."], "venue": "ACL.", "citeRegEx": "Chen and Dolan.,? 2011", "shortCiteRegEx": "Chen and Dolan.", "year": 2011}, {"title": "Fusion of audio and motion information on hmmbased highlight extraction for baseball games", "author": ["Chih-Chieh Cheng", "Chiou-Ting Hsu."], "venue": "IEEE Trans. Multimedia.", "citeRegEx": "Cheng and Hsu.,? 2006", "shortCiteRegEx": "Cheng and Hsu.", "year": 2006}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "author": ["Sumit Chopra", "Michael Auli", "Alexander M. Rush."], "venue": "NAACL.", "citeRegEx": "Chopra et al\\.,? 2016", "shortCiteRegEx": "Chopra et al\\.", "year": 2016}, {"title": "Sentence compression by deletion with lstms", "author": ["Katja Filippova", "Enrique Alfonseca", "Carlos A Colmenares", "Lukasz Kaiser", "Oriol Vinyals."], "venue": "EMNLP.", "citeRegEx": "Filippova et al\\.,? 2015", "shortCiteRegEx": "Filippova et al\\.", "year": 2015}, {"title": "The lack of parallel data in sentence compression", "author": ["Katja Filippova", "Yasemin Altun."], "venue": "EMNLP.", "citeRegEx": "Filippova and Altun.,? 2013", "shortCiteRegEx": "Filippova and Altun.", "year": 2013}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach."], "venue": "EMNLP.", "citeRegEx": "Fukui et al\\.,? 2016", "shortCiteRegEx": "Fukui et al\\.", "year": 2016}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "Neural computation.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Creating summaries from user videos", "author": ["Michael Gygli", "Helmut Grabner", "Hayko Riemenschneider", "Luc Van Gool."], "venue": "ECCV.", "citeRegEx": "Gygli et al\\.,? 2014", "shortCiteRegEx": "Gygli et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "CVPR.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Live semantic sport highlight detection based on analyzing tweets of twitter", "author": ["Liang-Chi Hsieh", "Ching-Wei Lee", "Tzu-Hsuan Chiu", "Winston Hsu."], "venue": "ICME.", "citeRegEx": "Hsieh et al\\.,? 2012", "shortCiteRegEx": "Hsieh et al\\.", "year": 2012}, {"title": "Referitgame: Referring to objects in photographs of natural scenes", "author": ["Sahar Kazemzadeh", "Vicente Ordonez", "Mark Matten", "Tamara Berg."], "venue": "EMNLP.", "citeRegEx": "Kazemzadeh et al\\.,? 2014", "shortCiteRegEx": "Kazemzadeh et al\\.", "year": 2014}, {"title": "Visual genome: Connecting language and vision", "author": ["Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma", "Michael Bernstein", "Li Fei-Fei"], "venue": null, "citeRegEx": "Krishna et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krishna et al\\.", "year": 2016}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollr", "C. Lawrence Zitnick."], "venue": "ECCV.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Hierarchical question-image coattention for visual question answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh."], "venue": "NIPS.", "citeRegEx": "Lu et al\\.,? 2016", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "What to talk about and how? selective generation using lstms with coarse-to-fine alignment", "author": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter."], "venue": "NAACL.", "citeRegEx": "Mei et al\\.,? 2016", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Abstractive text summarization using sequence-to-sequence rnns and beyond", "author": ["Ramesh Nallapati", "Bowen Zhou", "Caglar Gulcehre", "Bing Xiang"], "venue": "In CoNLL", "citeRegEx": "Nallapati et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["Vicente Ordonez", "Girish Kulkarni", "Tamara L Berg."], "venue": "NIPS.", "citeRegEx": "Ordonez et al\\.,? 2011", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk", "author": ["Cyrus Rashtchian", "Peter Young", "Micah Hodosh", "Julia Hockenmaier."], "venue": "NAACL HLT workshop.", "citeRegEx": "Rashtchian et al\\.,? 2010", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "A dataset for movie description", "author": ["Anna Rohrbach", "Marcus Rohrbach", "Niket Tandon", "Bernt Schiele."], "venue": "CVPR.", "citeRegEx": "Rohrbach et al\\.,? 2015", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei."], "venue": "IJCV.", "citeRegEx": "Russakovsky et al\\.,? 2015", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Get to the point: Summarization with pointergenerator networks", "author": ["Abigail See", "Peter J Liu", "Christopher D Manning."], "venue": "ACL.", "citeRegEx": "See et al\\.,? 2017", "shortCiteRegEx": "See et al\\.", "year": 2017}, {"title": "Real-time video highlights for yahoo esports", "author": ["Yale Song."], "venue": "arXiv:1611.08780.", "citeRegEx": "Song.,? 2016", "shortCiteRegEx": "Song.", "year": 2016}, {"title": "Tvsum: Summarizing web videos using titles", "author": ["Yale Song", "Jordi Vallmitjana", "Amanda Stent", "Alejandro Jaimes."], "venue": "CVPR.", "citeRegEx": "Song et al\\.,? 2015", "shortCiteRegEx": "Song et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Movieqa: Understanding stories in movies through question-answering", "author": ["Makarand Tapaswi", "Yukun Zhu", "Rainer Stiefelhagen", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler."], "venue": "CVPR.", "citeRegEx": "Tapaswi et al\\.,? 2016", "shortCiteRegEx": "Tapaswi et al\\.", "year": 2016}, {"title": "Using descriptive video services to create a large data source for video annotation research", "author": ["Atousa Torabi", "Christopher Pal", "Hugo Larochelle", "Aaron Courville."], "venue": "arXiv:1503.01070v1.", "citeRegEx": "Torabi et al\\.,? 2015", "shortCiteRegEx": "Torabi et al\\.", "year": 2015}, {"title": "Sports highlight detection from keyword sequences using hmm", "author": ["Jinjun Wang", "Changsheng Xu", "Engsiong Chng", "Qi Tian."], "venue": "ICME.", "citeRegEx": "Wang et al\\.,? 2004", "shortCiteRegEx": "Wang et al\\.", "year": 2004}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher."], "venue": "ICML.", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Visual madlibs: Fill-in-theblank image description and question answering", "author": ["Licheng Yu", "Eunbyung Park", "Alexander C. Berg", "Tamara L. Berg."], "venue": "ICCV.", "citeRegEx": "Yu et al\\.,? 2015", "shortCiteRegEx": "Yu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "There has been a surge of vision and language datasets focusing on captions over the last few years, (Rashtchian et al., 2010; Ordonez et al., 2011; Lin et al., 2014), followed by efforts to focus on more specific parts of images (Krishna et al.", "startOffset": 101, "endOffset": 166}, {"referenceID": 19, "context": "There has been a surge of vision and language datasets focusing on captions over the last few years, (Rashtchian et al., 2010; Ordonez et al., 2011; Lin et al., 2014), followed by efforts to focus on more specific parts of images (Krishna et al.", "startOffset": 101, "endOffset": 166}, {"referenceID": 15, "context": "There has been a surge of vision and language datasets focusing on captions over the last few years, (Rashtchian et al., 2010; Ordonez et al., 2011; Lin et al., 2014), followed by efforts to focus on more specific parts of images (Krishna et al.", "startOffset": 101, "endOffset": 166}, {"referenceID": 14, "context": ", 2014), followed by efforts to focus on more specific parts of images (Krishna et al., 2016), or referring expressions (Kazemzadeh et al.", "startOffset": 71, "endOffset": 93}, {"referenceID": 13, "context": ", 2016), or referring expressions (Kazemzadeh et al., 2014), or on the broader context (Huang et al.", "startOffset": 34, "endOffset": 59}, {"referenceID": 3, "context": "For video, similar efforts have collected descriptions (Chen and Dolan, 2011), while others use existing descriptive video service (DVS) sources (Rohrbach et al.", "startOffset": 55, "endOffset": 77}, {"referenceID": 21, "context": "For video, similar efforts have collected descriptions (Chen and Dolan, 2011), while others use existing descriptive video service (DVS) sources (Rohrbach et al., 2015; Torabi et al., 2015).", "startOffset": 145, "endOffset": 189}, {"referenceID": 28, "context": "For video, similar efforts have collected descriptions (Chen and Dolan, 2011), while others use existing descriptive video service (DVS) sources (Rohrbach et al., 2015; Torabi et al., 2015).", "startOffset": 145, "endOffset": 189}, {"referenceID": 0, "context": "Beyond descriptions, other datasets use questions to relate images and language (Antol et al., 2015; Yu et al., 2015).", "startOffset": 80, "endOffset": 117}, {"referenceID": 31, "context": "Beyond descriptions, other datasets use questions to relate images and language (Antol et al., 2015; Yu et al., 2015).", "startOffset": 80, "endOffset": 117}, {"referenceID": 10, "context": "The related problem of visually summarizing videos (as opposed to finding the highlights) has produced datasets of holiday and sports events with multiple users making summary videos (Gygli et al., 2014) and multiple users selecting summary key-frames (de Avila et al.", "startOffset": 183, "endOffset": 203}, {"referenceID": 7, "context": "For language-based summarization, Extractive models (Filippova and Altun, 2013; Filippova et al., 2015) generate summaries by selecting important sentences and then assembling these, while Abstractive models (Chopra et al.", "startOffset": 52, "endOffset": 103}, {"referenceID": 6, "context": "For language-based summarization, Extractive models (Filippova and Altun, 2013; Filippova et al., 2015) generate summaries by selecting important sentences and then assembling these, while Abstractive models (Chopra et al.", "startOffset": 52, "endOffset": 103}, {"referenceID": 5, "context": ", 2015) generate summaries by selecting important sentences and then assembling these, while Abstractive models (Chopra et al., 2016; Mei et al., 2016; Nallapati et al., 2016; See et al., 2017) generate/rewrite the summaries from scratch.", "startOffset": 112, "endOffset": 193}, {"referenceID": 17, "context": ", 2015) generate summaries by selecting important sentences and then assembling these, while Abstractive models (Chopra et al., 2016; Mei et al., 2016; Nallapati et al., 2016; See et al., 2017) generate/rewrite the summaries from scratch.", "startOffset": 112, "endOffset": 193}, {"referenceID": 18, "context": ", 2015) generate summaries by selecting important sentences and then assembling these, while Abstractive models (Chopra et al., 2016; Mei et al., 2016; Nallapati et al., 2016; See et al., 2017) generate/rewrite the summaries from scratch.", "startOffset": 112, "endOffset": 193}, {"referenceID": 23, "context": ", 2015) generate summaries by selecting important sentences and then assembling these, while Abstractive models (Chopra et al., 2016; Mei et al., 2016; Nallapati et al., 2016; See et al., 2017) generate/rewrite the summaries from scratch.", "startOffset": 112, "endOffset": 193}, {"referenceID": 0, "context": "Beyond descriptions, other datasets use questions to relate images and language (Antol et al., 2015; Yu et al., 2015). This approach is extended to movies in Tapaswi et al. (2016). The related problem of visually summarizing videos (as opposed to finding the highlights) has produced datasets of holiday and sports events with multiple users making summary videos (Gygli et al.", "startOffset": 81, "endOffset": 180}, {"referenceID": 4, "context": "Closer to our setting, there has been work on highlight prediction in football (soccer) and basketball based on audio of broadcasts (Cheng and Hsu, 2006) (Wang et al.", "startOffset": 132, "endOffset": 153}, {"referenceID": 29, "context": "Closer to our setting, there has been work on highlight prediction in football (soccer) and basketball based on audio of broadcasts (Cheng and Hsu, 2006) (Wang et al., 2004) where commentators may have an outsized impact or visual features (Bertini et al.", "startOffset": 154, "endOffset": 173}, {"referenceID": 2, "context": ", 2004) where commentators may have an outsized impact or visual features (Bertini et al., 2005).", "startOffset": 74, "endOffset": 96}, {"referenceID": 12, "context": "In the spirit of our study, there has been work looking at tweets during sporting events (Hsieh et al., 2012), but the tweets are not as immediate or as well aligned with the games as the eSports comments.", "startOffset": 89, "endOffset": 109}, {"referenceID": 2, "context": ", 2004) where commentators may have an outsized impact or visual features (Bertini et al., 2005). In the spirit of our study, there has been work looking at tweets during sporting events (Hsieh et al., 2012), but the tweets are not as immediate or as well aligned with the games as the eSports comments. More closely related to our work, Song (2016) collects videos for Heroes of the Storm, League of Legends, and Dota2 on online broadcasting websites of around 327 hours total.", "startOffset": 75, "endOffset": 350}, {"referenceID": 10, "context": "Following (Gygli et al., 2014; Song et al., 2015), we use the harmonic mean F-score in Eq.", "startOffset": 10, "endOffset": 49}, {"referenceID": 25, "context": "Following (Gygli et al., 2014; Song et al., 2015), we use the harmonic mean F-score in Eq.", "startOffset": 10, "endOffset": 49}, {"referenceID": 11, "context": "V-CNN We use the ResNet-34 model (He et al., 2016) to represent frames, motivated by its strong results on the ImageNet Challenge (Russakovsky et al.", "startOffset": 33, "endOffset": 50}, {"referenceID": 22, "context": ", 2016) to represent frames, motivated by its strong results on the ImageNet Challenge (Russakovsky et al., 2015).", "startOffset": 87, "endOffset": 113}, {"referenceID": 26, "context": "Word-level LSTM-RNN models (Sutskever et al., 2014) are a common approach to embedding sentences.", "startOffset": 27, "endOffset": 51}, {"referenceID": 9, "context": "Therefore, alternatively, we model the audience chat with a character-level LSTM-RNN model (Graves, 2013).", "startOffset": 91, "endOffset": 105}, {"referenceID": 8, "context": ", Bilinear Pooling (Fukui et al., 2016), Memory Networks (Xiong et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 30, "context": ", 2016), Memory Networks (Xiong et al., 2016), and Attention Models (Lu et al.", "startOffset": 25, "endOffset": 45}, {"referenceID": 16, "context": ", 2016), and Attention Models (Lu et al., 2016); this is future work.", "startOffset": 30, "endOffset": 47}], "year": 2017, "abstractText": "Sports channel video portals offer an exciting domain for research on multimodal, multilingual analysis. We present methods addressing the problem of automatic video highlight prediction based on joint visual features and textual analysis of the real-world audience discourse with complex slang, in both English and traditional Chinese. We present a novel dataset based on League of Legends championships recorded from North American and Taiwanese Twitch.tv channels (will be released for further research), and demonstrate strong results on these using multimodal, character-level CNN-RNN model architectures.", "creator": "LaTeX with hyperref package"}}}