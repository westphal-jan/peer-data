{"id": "1707.08713", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jul-2017", "title": "Determining Semantic Textual Similarity using Natural Deduction Proofs", "abstract": "Determining semantic textual similarity is a core research subject in natural language processing. Since vector-based models for sentence representation often use shallow information, capturing accurate semantics is difficult. By contrast, logical semantic representations capture deeper levels of sentence semantics, but their symbolic nature does not offer graded notions of textual similarity. We propose a method for determining semantic textual similarity by combining shallow features with features extracted from natural deduction proofs of bidirectional entailment relations between sentence pairs. For the natural deduction proofs, we use ccg2lambda, a higher-order automatic inference system, which converts Combinatory Categorial Grammar (CCG) derivation trees into semantic representations and conducts natural deduction proofs. Experiments show that our system was able to outperform other logic-based systems and that features derived from the proofs are effective for learning textual similarity.", "histories": [["v1", "Thu, 27 Jul 2017 05:49:51 GMT  (94kb,D)", "http://arxiv.org/abs/1707.08713v1", "11 pages, 5 figures, accepted as long paper of EMNLP2017"]], "COMMENTS": "11 pages, 5 figures, accepted as long paper of EMNLP2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hitomi yanaka", "koji mineshima", "pascual mart\u00ednez-g\u00f3mez", "daisuke bekki"], "accepted": true, "id": "1707.08713"}, "pdf": {"name": "1707.08713.pdf", "metadata": {"source": "CRF", "title": "Determining Semantic Textual Similarity using Natural Deduction Proofs", "authors": ["Hitomi Yanaka", "Koji Mineshima", "Daisuke Bekki"], "emails": ["hitomiyanaka@g.ecc.u-tokyo.ac.jp", "mineshima.koji@ocha.ac.jp", "pascual.mg@aist.go.jp", "bekki@is.ocha.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "Determining semantic textual similarity (STS) is one of the most critical tasks in retrieving information and processing natural language. Vector-based sentence representation models have been widely used to compare and evaluate words, phrases or sentences using various similarity and kinship values (Wong and Raghavan, 1984; Mitchell and Lapata, 2010; Le and Mikolov, 2014). Accordingly, neural network-based sentence representation models (Mueller and Thyagarajan, 2016; Hill et al., 2016) have been proposed to learn textual similarities. However, these vector-based models often use flat information, such as words and characters, and whether they can account for phenomena such as negation and quantification is unclear. Consider the sentences: Tom did not meet some of the players and Tom did not meet any of the players. If functional words such as some or any of them are ignored or presented as the same vector, then these sentences are represented by identical similarities."}, {"heading": "2 Related Work", "text": "Vector-based models of semantic composition have been widely studied in relation to the calculation of STS. Mitchell and Lapata (2008, 2010) ar Xiv: 170 7,08 713v 1 [cs.C L] 27 Jul 2 017 proposed sentence vector models with word vector additions or component-based multiplications. Addition and multiplication are commutative and associative abilities and therefore ignore the word order. Polajnar et al. (2015) proposed a discourse-based sentence vector model that takes into account additional intra-mental contexts. A categorical compositional semantic model was also developed to detect textual entanglements and the calculation of STS (Grefenstette and Sadrzadeh, 2011; Kartsaklis et al. 2014; Kartsaklis and Sadrzadeh, 2016). These previous studies largely deal with the structures of basic phrases or sentences and do not address logical words."}, {"heading": "3 System Overview", "text": "Figure 1 shows an overview of the system that extracts textual similarity learning functions from logical proofs. To produce and automatically prove semantic representations of sentences, we use ccg2lambda (Mart\u0131 \u0301 nez-Go \u0301 mez et al., 2016), which is a semantic parser combined with an inference system based on natural derivative structures. First, sentences are analyzed in syntactic trees based on combinatorial categorical grammar (CCG, 2000), which is based on syntactic theory suitable for semantic composition of syntactic structures. Representations are obtained based on semantic templates and combinatorial rules for CCG trees. Semantic templates are defined manually based on formal semantics. Combinatorial rules specify the syntactic behavior of words and compositional rules for CCG trees. In ccg2lambda, two wide-reaching CCG, we are 2007, PartoG-Clark and PartoG-C."}, {"heading": "4 Proof Strategy for Learning Textual Similarity", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Overview of the proof strategy", "text": "We grasp the similarity between the proposition pair (A, B) as a function of the evidential force of bidirectional entanglement relationships for (A, B) and combine it with shallow characteristics. After obtaining the logical formulas A \"and B\" of A and B, we try to examine the bidirectional entanglement relationships, A \"and B.\" If the original natural derivative proofs fail, we redo the proof by adding relevant external axioms or skipping unproven subtargets until the proof is complete. Afterwards, characteristics for learning textual similarity are extracted by quantifying the bidirectional entanglement relations. The details of the procedure are as follows. First, we try a natural derivative proof without using external axioms, with the aim of proving relationships."}, {"heading": "4.2 Proving entailment relations", "text": "To illustrate how our natural derivative proofs work, let us consider the case of proving consequences for the following pair of sentences: A: A man sings in a bar. B: A man sings. The sentences A and B are therefore based on logical formulas A: - and B: - goals based on event semantics via CCG: - semantic composition, as follows. A: - sentences A: - and B: - goals (e1) - and B: - goals (x2) - and B: - goals (x2) - and B: - proofs (e1, x2) - and B: - proofs (man (x1) - and P: - proofs (e1) - and B: - proofs (e1). First, let us try a natural derivative proof for A: - and B: fixing A: - rules as premise and B: - as goal."}, {"heading": "4.3 Proving the contradiction", "text": "The evidence strategy presented here can be applied quite simply to the proof of the contradiction by using a statement constant False to encode the contradiction. Thus, the consequence rules for denial can be taken as special cases of implication rules, as in Figure 4. Consider the following pair of sentences for illustration: A: No Man Sings. B: A Man Sings Aloud. Figure 5 shows the evidentiary process. Movements A and B are mapped to P0 and P1 respectively using compositional semantics and the target G0 is set to False. By dissecting P1 using elimination rules and then combining P2, P3 and P4, we obtain P6. From P0 and P6 we can then derive the contradiction. This evidence is carried out by an automated proof for the use of evidence tactics (using evidence tactics)."}, {"heading": "5 Description of the Features", "text": "To maximize accuracy in learning text similarity, we take a hybrid approach that uses both logic-based characteristics taken from the natural proof of deduction as well as other non-logic-based characteristics. All characteristics are scaled to the range [0, 1]."}, {"heading": "5.1 Logic-based Features", "text": "\"so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green, so for the green."}, {"heading": "5.2 Non-logic-based Features", "text": "We use the following eight non-logic-based characteristics, so the noun / verb overlap in every other sentence. We extract and lemmatize allnouns and verbs from the sentence pairs, using the degree of overlap of the noun and verb lemmas as characteristics. Part-of-speech overlap (POS) tags for all the words in the sentence pairs are obtained by marking them first with the Penn Treebank Project tokenizer1 and then with POS tagger (Curran and Clark, 2003). The degree of overlap between the POS tags of the sentences is used as a feature. Synset overlap of overlaps with the Penn Treebank Project tokenizer1 and then mark them with C & C POS tagger (Curran and Clark, 2003). The degree of overlap between the POS tags is used as a feature."}, {"heading": "6 Experiments and Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Experimental Conditions", "text": "We examined our System3 using two datasets: the SemEval 2014 version of the SICK dataset (Marelli et al., 2014) and the SemEval2012 version of the MSR paraphrase video corpus dataset (MSR-vid) (Agirre et al., 2012). Experimental conditions were as follows."}, {"heading": "6.1.1 The SICK dataset", "text": "The SICK dataset is a data set for studying STS and recognizing textual relationships (RTE). Originally developed to evaluate compositional distribution semantics, it contains logically challenging expressions such as quantifiers, negations, conjunctions, and disjunctions. The dataset contains 9927 sentence pairs with a 5000 / 4927 training / test split. These sentence pairs are manually assigned three types of terms: yes (correlations), no (contradiction), or unknown (neutral), and semantic affiliation in [1, 5] (see Table 1 for a sample). In this dataset, sentence pairs whose gold match labels do not tend to be rated slightly higher than average, while those whose labels are unknown have a wide range of ratings."}, {"heading": "6.1.2 The MSR-vid dataset", "text": "The MSR-vid dataset is our second dataset for the STS task and contains 1500 set pairs with a 750 / 750 training / test split. All set pairs are commented with semantic relationship values in the range [0, 5]. We used this dataset to compare our system with the best system of SemEval2012 (Ba \ufffd r et al., 2012) and the logic-based UTexas system (Beltagy et al., 2014a)."}, {"heading": "6.2 Results", "text": "Table 2 shows the results of our experiments with the SICK dataset. Although the state-of-the-art neural network-based system delivered the best results overall in terms of Pearson correlation and Spearman correlation, our system achieved higher scores than SemEval 2014 submissions, including the two logic-based systems (The Meaning Factory and UTexas). The main reason for the lower performance of our system in terms of MSE is that some theorems could not be proven due to a lack of lexical knowledge. In the current work, we only consider word-level knowledge (word-for-word paraphrasing); we could expand the knowledge base in the future by using more external resources.As we mentioned above, the as unknown commented sentence pairs yielded a wide range of results. The Pearson correlation of the unknown part of the SICK dataset was 0.766, suggesting that our logics-based system can also be applied to neutral sentence pairs based on our SICS-based results, as shown in the MURS-based table with higher results."}, {"heading": "6.3 Positive examples and error analysis", "text": "In this case, however, the pairs of sentences are not superficially similar. By using logical formulas based on event semantics, we were able to correctly interpret the sentence containing the passive clause and judge that the passive and non-passive sentences are similar. In ID 891, one sentence contains a negative clause while the other does not. Word overlap is small and the predictive score was much lower than the correct score."}, {"heading": "7 Conclusion", "text": "The results of our experiments with two sets of data show that our system has been able to outperform other logic-based systems. Furthermore, the results show that information about the natural deduction can be used to create effective traits for learning text-based similarity. Since these logic-based traits provide accuracy improvements that are largely additive to those that do not provide logic-based traits, neural network-based systems could also benefit from their use. In future work, we will refine our system to apply it to other tasks such as answering questions. Compared to neural network-based systems, our natural derivative-based system can not only assess how similar sentence pairs are, but also explain what the sources of simularity / deviation are by referring to information about sub-targets in the answer. Given this interpretation capability, we believe that our natural derivative system can also be a logical formulation of other text tasks."}, {"heading": "Acknowledgments", "text": "We thank the three anonymous reviewers for their detailed comments. This work was supported by JST CREST Grant Number JPMJCR1301, Japan."}], "references": [{"title": "A tableau prover for natural logic and language", "author": ["Lasha Abzianidze."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP-15), pages 2492\u20132502, Lisbon, Portugal. Association for Computational Linguis-", "citeRegEx": "Abzianidze.,? 2015", "shortCiteRegEx": "Abzianidze.", "year": 2015}, {"title": "Natural solution to FraCaS entailment problems", "author": ["Lasha Abzianidze."], "venue": "Proceedings of the 5th Joint Conference on Lexical and Computational Semantics, pages 64\u201374, Berlin, Germany. Association for Computational Linguistics.", "citeRegEx": "Abzianidze.,? 2016", "shortCiteRegEx": "Abzianidze.", "year": 2016}, {"title": "SemEval-2012 Task 6: A", "author": ["Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre"], "venue": null, "citeRegEx": "Agirre et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2012}, {"title": "UKP: Computing semantic textual similarity by combining multiple content similarity measures", "author": ["Daniel B\u00e4r", "Chris Biemann", "Iryna Gurevych", "Torsten Zesch."], "venue": "Proceedings of the Sixth International Workshop on Semantic Evalu-", "citeRegEx": "B\u00e4r et al\\.,? 2012", "shortCiteRegEx": "B\u00e4r et al\\.", "year": 2012}, {"title": "Contextpassing and underspecification in dependent type semantics", "author": ["Daisuke Bekki", "Koji Mineshima."], "venue": "Stergios Chatzikyriakidis and Zhaohui Luo, editors, Modern Perspectives in Type Theoretical Semantics, Studies of Linguistics and Philoso-", "citeRegEx": "Bekki and Mineshima.,? 2017", "shortCiteRegEx": "Bekki and Mineshima.", "year": 2017}, {"title": "Probabilistic soft logic for semantic textual similarity", "author": ["Islam Beltagy", "Katrin Erk", "Raymond Mooney."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-2014), pages 1210\u20131219, Baltimore,", "citeRegEx": "Beltagy et al\\.,? 2014a", "shortCiteRegEx": "Beltagy et al\\.", "year": 2014}, {"title": "UTexas: Natural language semantics using distributional semantics and probabilistic logic", "author": ["Islam Beltagy", "Stephen Roller", "Gemma Boleda", "Katrin Erk", "Raymond Mooney."], "venue": "Proceedings of the 8th International Workshop on Semantic Evalu-", "citeRegEx": "Beltagy et al\\.,? 2014b", "shortCiteRegEx": "Beltagy et al\\.", "year": 2014}, {"title": "Association for Computational Linguistics and Dublin City University", "author": ["Dublin", "Ireland"], "venue": null, "citeRegEx": "Dublin and Ireland.,? \\Q2014\\E", "shortCiteRegEx": "Dublin and Ireland.", "year": 2014}, {"title": "Interactive Theorem Proving and Program Development: Coq\u2019Art The Calculus of Inductive Constructions", "author": ["Yves Bertot", "Pierre Castran."], "venue": "Springer Publishing Company, Incorporated, New York, USA.", "citeRegEx": "Bertot and Castran.,? 2010", "shortCiteRegEx": "Bertot and Castran.", "year": 2010}, {"title": "The Meaning Factory: Formal semantics for recognizing textual entailment and determining semantic similarity", "author": ["Johannes Bjerva", "Johan Bos", "Rob van der Goot", "Malvina Nissim."], "venue": "Proceedings of the 8th International Workshop on Semantic Eval-", "citeRegEx": "Bjerva et al\\.,? 2014", "shortCiteRegEx": "Bjerva et al\\.", "year": 2014}, {"title": "Association for Computational Linguistics and Dublin City University", "author": ["Dublin", "Ireland"], "venue": null, "citeRegEx": "Dublin and Ireland.,? \\Q2014\\E", "shortCiteRegEx": "Dublin and Ireland.", "year": 2014}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan."], "venue": "Journal of Machine Learning, 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Widecoverage efficient statistical parsing with CCG and log-linear models", "author": ["Stephen Clark", "James R. Curran."], "venue": "Computational Linguistics, 33(4):493\u2013552.", "citeRegEx": "Clark and Curran.,? 2007", "shortCiteRegEx": "Clark and Curran.", "year": 2007}, {"title": "Investigating GIS and smoothing for maximum entropy taggers", "author": ["James R Curran", "Stephen Clark."], "venue": "Proceedings of the tenth conference on European chapter of the Association for Computational", "citeRegEx": "Curran and Clark.,? 2003", "shortCiteRegEx": "Curran and Clark.", "year": 2003}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T. Dumais", "Thomas K. Landauer", "Richard Harshman."], "venue": "Journal of the American Society for Information Science, 41(6):391\u2013407.", "citeRegEx": "Deerwester et al\\.,? 1990", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP-2011),", "citeRegEx": "Grefenstette and Sadrzadeh.,? 2011", "shortCiteRegEx": "Grefenstette and Sadrzadeh.", "year": 2011}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["Felix Hill", "Kyunghyun Cho", "Anna Korhonen."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Resolving lexical ambiguity in tensor regression models of meaning", "author": ["Dimitri Kartsaklis", "Nal Kalchbrenner", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-2014),", "citeRegEx": "Kartsaklis et al\\.,? 2014", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2014}, {"title": "Distributional inclusion hypothesis for tensor-based composition", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers (COLING-2016), pages 2849\u2013", "citeRegEx": "Kartsaklis and Sadrzadeh.,? 2016", "shortCiteRegEx": "Kartsaklis and Sadrzadeh.", "year": 2016}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V. Le", "Tomas Mikolov."], "venue": "Proceedings of the 31th International Conference on Machine Learning, (ICML-2014), pages 1188\u2013 1196, Beijing, China.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "A* CCG parsing with a supertag-factored model", "author": ["Mike Lewis", "Mark Steedman."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP-2014), pages 990\u20131000, Doha, Qatar. Association for Com-", "citeRegEx": "Lewis and Steedman.,? 2014", "shortCiteRegEx": "Lewis and Steedman.", "year": 2014}, {"title": "A SICK cure for the evaluation of compositional distributional semantic models", "author": ["Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli."], "venue": "Proceedings of the 9th International Conference on", "citeRegEx": "Marelli et al\\.,? 2014", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "On-demand injection of lexical knowledge for recognising textual entailment", "author": ["Pascual Mart\u0131\u0301nez-G\u00f3mez", "Koji Mineshima", "Yusuke Miyao", "Daisuke Bekki"], "venue": "In Proceedings of the 15th Conference of the European Chapter of the Association for Compu-", "citeRegEx": "Mart\u0131\u0301nez.G\u00f3mez et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mart\u0131\u0301nez.G\u00f3mez et al\\.", "year": 2017}, {"title": "Association for Computational Linguistics", "author": ["Valencia", "Spain"], "venue": null, "citeRegEx": "Valencia and Spain.,? \\Q2017\\E", "shortCiteRegEx": "Valencia and Spain.", "year": 2017}, {"title": "Some uses of higher-order logic in computational linguistics", "author": ["Dale A. Miller", "Gopalan Nadathur."], "venue": "Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, pages 247\u2013256, New York, New York, USA. Asso-", "citeRegEx": "Miller and Nadathur.,? 1986", "shortCiteRegEx": "Miller and Nadathur.", "year": 1986}, {"title": "WordNet: A lexical database for English", "author": ["George A. Miller."], "venue": "Communications of the ACM, 38(11):39\u201341.", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Higher-order logical inference with compositional semantics", "author": ["Koji Mineshima", "Pascual Mart\u0131\u0301nez-G\u00f3mez", "Yusuke Miyao", "Daisuke Bekki"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP-", "citeRegEx": "Mineshima et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mineshima et al\\.", "year": 2015}, {"title": "Building compositional semantics and higher-order inference system for a wide-coverage Japanese CCG parser", "author": ["Koji Mineshima", "Ribeka Tanaka", "Pascual Mart\u0131\u0301nezG\u00f3mez", "Yusuke Miyao", "Daisuke Bekki"], "venue": "In Proceedings of the 2016 Conference", "citeRegEx": "Mineshima et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mineshima et al\\.", "year": 2016}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08), pages 236\u2013 244, Columbus, Ohio. Association for Computa-", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive Science, 34(8):1388\u20131429.", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Siamese recurrent architectures for learning sentence similarity", "author": ["Jonas Mueller", "Aditya Thyagarajan."], "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-2016), pages 2786\u2013 2792, Arizona, USA. Association for the Advance-", "citeRegEx": "Mueller and Thyagarajan.,? 2016", "shortCiteRegEx": "Mueller and Thyagarajan.", "year": 2016}, {"title": "Events in The Semantics of English: a Study in Subatomic Semantics", "author": ["Terence Parsons."], "venue": "MIT Press, Cambridge, USA.", "citeRegEx": "Parsons.,? 1990", "shortCiteRegEx": "Parsons.", "year": 1990}, {"title": "An exploration of discourse-based sentence spaces for compositional distributional semantics", "author": ["Tamara Polajnar", "Laura Rimell", "Stephen Clark."], "venue": "Proceedings of the 1st Workshop on Linking Computational Models of Lexical, Sentential and", "citeRegEx": "Polajnar et al\\.,? 2015", "shortCiteRegEx": "Polajnar et al\\.", "year": 2015}, {"title": "Natural Deduction \u2013 A ProofTheoretical Study", "author": ["Dag Prawitz."], "venue": "Almqvist & Wiksell, Stockholm, Sweden.", "citeRegEx": "Prawitz.,? 1965", "shortCiteRegEx": "Prawitz.", "year": 1965}, {"title": "The Syntactic Process", "author": ["Mark Steedman."], "venue": "MIT Press, Cambridge, USA.", "citeRegEx": "Steedman.,? 2000", "shortCiteRegEx": "Steedman.", "year": 2000}, {"title": "Vector space model of information retrieval: A reevaluation", "author": ["S.K.M. Wong", "Vijay V. Raghavan."], "venue": "Proceedings of the 7th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 167\u2013185.", "citeRegEx": "Wong and Raghavan.,? 1984", "shortCiteRegEx": "Wong and Raghavan.", "year": 1984}, {"title": "ECNU: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment", "author": ["Jiang Zhao", "Tiantian Zhu", "Man Lan."], "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014),", "citeRegEx": "Zhao et al\\.,? 2014", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 35, "context": "Vectorbased sentence representation models have been widely used to compare and rank words, phrases or sentences using various similarity and relatedness scores (Wong and Raghavan, 1984; Mitchell and Lapata, 2010; Le and Mikolov, 2014).", "startOffset": 161, "endOffset": 235}, {"referenceID": 29, "context": "Vectorbased sentence representation models have been widely used to compare and rank words, phrases or sentences using various similarity and relatedness scores (Wong and Raghavan, 1984; Mitchell and Lapata, 2010; Le and Mikolov, 2014).", "startOffset": 161, "endOffset": 235}, {"referenceID": 19, "context": "Vectorbased sentence representation models have been widely used to compare and rank words, phrases or sentences using various similarity and relatedness scores (Wong and Raghavan, 1984; Mitchell and Lapata, 2010; Le and Mikolov, 2014).", "startOffset": 161, "endOffset": 235}, {"referenceID": 30, "context": "Recently, neural network-based sentence representation models (Mueller and Thyagarajan, 2016; Hill et al., 2016) have been proposed for learning textual similarity.", "startOffset": 62, "endOffset": 112}, {"referenceID": 16, "context": "Recently, neural network-based sentence representation models (Mueller and Thyagarajan, 2016; Hill et al., 2016) have been proposed for learning textual similarity.", "startOffset": 62, "endOffset": 112}, {"referenceID": 15, "context": "Also, a categorical compositional distributional semantic model has been developed for recognizing textual entailment and for calculating STS (Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2014; Kartsaklis and Sadrzadeh, 2016).", "startOffset": 142, "endOffset": 233}, {"referenceID": 17, "context": "Also, a categorical compositional distributional semantic model has been developed for recognizing textual entailment and for calculating STS (Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2014; Kartsaklis and Sadrzadeh, 2016).", "startOffset": 142, "endOffset": 233}, {"referenceID": 18, "context": "Also, a categorical compositional distributional semantic model has been developed for recognizing textual entailment and for calculating STS (Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2014; Kartsaklis and Sadrzadeh, 2016).", "startOffset": 142, "endOffset": 233}, {"referenceID": 30, "context": "Neural network-based models of semantic composition (Mueller and Thyagarajan, 2016; Hill et al., 2016) have also been proposed.", "startOffset": 52, "endOffset": 102}, {"referenceID": 16, "context": "Neural network-based models of semantic composition (Mueller and Thyagarajan, 2016; Hill et al., 2016) have also been proposed.", "startOffset": 52, "endOffset": 102}, {"referenceID": 27, "context": "Polajnar et al. (2015) proposed a discourse-based sentence vector model considering extra-intra sentential context.", "startOffset": 0, "endOffset": 23}, {"referenceID": 9, "context": "The Meaning Factory (Bjerva et al., 2014) uses both shallow and logic-based features for learning textual similarity.", "startOffset": 20, "endOffset": 41}, {"referenceID": 6, "context": "UTexas (Beltagy et al., 2014b) uses Probabilistic Soft Logic for learning textual similarity.", "startOffset": 7, "endOffset": 30}, {"referenceID": 4, "context": "In our study, we determine the semantic similarity of sentences based on the conception of prooftheoretic semantics (Bekki and Mineshima, 2017).", "startOffset": 116, "endOffset": 143}, {"referenceID": 24, "context": "In addition, higherorder predicate logic makes the logical structure of a sentence more explicit than first-order predicate logic does, so it can simplify the process of proof search (Miller and Nadathur, 1986).", "startOffset": 183, "endOffset": 210}, {"referenceID": 34, "context": "First, sentences are parsed into syntactic trees based on Combinatory Categorial Grammar (CCG) (Steedman, 2000).", "startOffset": 95, "endOffset": 111}, {"referenceID": 12, "context": "In ccg2lambda, two wide-coverage CCG parsers, C&C (Clark and Curran, 2007) and EasyCCG (Lewis and Steedman, 2014), are used for converting tokenized sentences into CCG trees robustly.", "startOffset": 50, "endOffset": 74}, {"referenceID": 20, "context": "In ccg2lambda, two wide-coverage CCG parsers, C&C (Clark and Curran, 2007) and EasyCCG (Lewis and Steedman, 2014), are used for converting tokenized sentences into CCG trees robustly.", "startOffset": 87, "endOffset": 113}, {"referenceID": 31, "context": "The semantic representations are based on Neo-Davidsonian event semantics (Parsons, 1990; Mineshima et al., 2015), in which every verb is decomposed into a predicate over events and a set of functional expressions re-", "startOffset": 74, "endOffset": 113}, {"referenceID": 26, "context": "The semantic representations are based on Neo-Davidsonian event semantics (Parsons, 1990; Mineshima et al., 2015), in which every verb is decomposed into a predicate over events and a set of functional expressions re-", "startOffset": 74, "endOffset": 113}, {"referenceID": 8, "context": "For this purpose, we use Coq (Bertot and Castran, 2010), which can be used for efficient theorem-proving for natural language inference using both first-order and higherorder logic (Mineshima et al.", "startOffset": 29, "endOffset": 55}, {"referenceID": 26, "context": "For this purpose, we use Coq (Bertot and Castran, 2010), which can be used for efficient theorem-proving for natural language inference using both first-order and higherorder logic (Mineshima et al., 2015).", "startOffset": 181, "endOffset": 205}, {"referenceID": 33, "context": "Coq\u2019s proof calculus is based on natural deduction (Prawitz, 1965), a proof system based on inference rules called introduction and elimination rules for logical connectives.", "startOffset": 51, "endOffset": 66}, {"referenceID": 22, "context": "The natural deduction system is particularly suitable for injecting external axioms during the theorem-proving process (Mart\u0131\u0301nez-G\u00f3mez et al., 2017).", "startOffset": 119, "endOffset": 149}, {"referenceID": 21, "context": "In the SICK (Sentences Involving Compositional Knowledge) dataset (Marelli et al., 2014) (see Section 6.", "startOffset": 66, "endOffset": 88}, {"referenceID": 22, "context": "We then attempt to prove A\u2032 \u21d2 B\u2032 and B\u2032 \u21d2 A\u2032 using axiom injection, following the method introduced in Mart\u0131\u0301nez-G\u00f3mez et al. (2017). In axiom injection, unproved sub-goals are candidates to form axioms.", "startOffset": 103, "endOffset": 133}, {"referenceID": 25, "context": "In this study, we use WordNet (Miller, 1995) as the source of lexical knowledge.", "startOffset": 30, "endOffset": 44}, {"referenceID": 13, "context": "We obtain part-ofspeech (POS) tags for all words in the sentence pairs by first tokenizing them with the Penn Treebank Project tokenizer1 and then POS tagging them with C&C POS tagger (Curran and Clark, 2003).", "startOffset": 184, "endOffset": 208}, {"referenceID": 14, "context": "We calculate sentence similarity by using three major vector space models, TF-IDF, latent semantic analysis (LSA) (Deerwester et al., 1990), and latent Dirichlet allocation (LDA) (Blei et al.", "startOffset": 114, "endOffset": 139}, {"referenceID": 11, "context": ", 1990), and latent Dirichlet allocation (LDA) (Blei et al., 2003).", "startOffset": 47, "endOffset": 66}, {"referenceID": 21, "context": "We evaluated our system3 using two datasets: the SemEval-2014 version of the SICK dataset (Marelli et al., 2014) and the SemEval2012 version of the MSR-paraphrase video corpus dataset (MSR-vid) (Agirre et al.", "startOffset": 90, "endOffset": 112}, {"referenceID": 2, "context": ", 2014) and the SemEval2012 version of the MSR-paraphrase video corpus dataset (MSR-vid) (Agirre et al., 2012).", "startOffset": 89, "endOffset": 110}, {"referenceID": 30, "context": "We compared our system with the following systems: the state-of-the-art neural network-based system (Mueller and Thyagarajan, 2016); the best system (Zhao et al.", "startOffset": 100, "endOffset": 131}, {"referenceID": 36, "context": "We compared our system with the following systems: the state-of-the-art neural network-based system (Mueller and Thyagarajan, 2016); the best system (Zhao et al., 2014) from SemEval-2014; and two of the logic-based systems stated in Sec-", "startOffset": 149, "endOffset": 168}, {"referenceID": 9, "context": "tion 2: namely The Meaning Factory (Bjerva et al., 2014) and UTexas (Beltagy et al.", "startOffset": 35, "endOffset": 56}, {"referenceID": 6, "context": ", 2014) and UTexas (Beltagy et al., 2014b).", "startOffset": 19, "endOffset": 42}, {"referenceID": 3, "context": "We used this dataset to compare our system with the best system from SemEval2012 (B\u00e4r et al., 2012) and the logic-based UTexas system (Beltagy et al.", "startOffset": 81, "endOffset": 99}, {"referenceID": 5, "context": ", 2012) and the logic-based UTexas system (Beltagy et al., 2014a).", "startOffset": 42, "endOffset": 65}], "year": 2017, "abstractText": "Determining semantic textual similarity is a core research subject in natural language processing. Since vector-based models for sentence representation often use shallow information, capturing accurate semantics is difficult. By contrast, logical semantic representations capture deeper levels of sentence semantics, but their symbolic nature does not offer graded notions of textual similarity. We propose a method for determining semantic textual similarity by combining shallow features with features extracted from natural deduction proofs of bidirectional entailment relations between sentence pairs. For the natural deduction proofs, we use ccg2lambda, a higherorder automatic inference system, which converts Combinatory Categorial Grammar (CCG) derivation trees into semantic representations and conducts natural deduction proofs. Experiments show that our system was able to outperform other logicbased systems and that features derived from the proofs are effective for learning textual similarity.", "creator": "LaTeX with hyperref package"}}}