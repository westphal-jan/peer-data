{"id": "1707.08866", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jul-2017", "title": "Deep Residual Learning for Weakly-Supervised Relation Extraction", "abstract": "Deep residual learning (ResNet) is a new method for training very deep neural networks using identity map-ping for shortcut connections. ResNet has won the ImageNet ILSVRC 2015 classification task, and achieved state-of-the-art performances in many computer vision tasks. However, the effect of residual learning on noisy natural language processing tasks is still not well understood. In this paper, we design a novel convolutional neural network (CNN) with residual learning, and investigate its impacts on the task of distantly supervised noisy relation extraction. In contradictory to popular beliefs that ResNet only works well for very deep networks, we found that even with 9 layers of CNNs, using identity mapping could significantly improve the performance for distantly-supervised relation extraction.", "histories": [["v1", "Thu, 27 Jul 2017 13:56:36 GMT  (244kb,D)", "http://arxiv.org/abs/1707.08866v1", "Accepted by EMNLP 2017"]], "COMMENTS": "Accepted by EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["yi yao huang", "william yang wang"], "accepted": true, "id": "1707.08866"}, "pdf": {"name": "1707.08866.pdf", "metadata": {"source": "CRF", "title": "Deep Residual Learning for Weakly-Supervised Relation Extraction", "authors": ["Yi Yao Huang", "William Yang Wang"], "emails": ["b02901042@ntu.edu.tw", "william@cs.ucsb.edu"], "sections": [{"heading": "1 Introduction", "text": "Relation extraction is the task of predicting attributes and relationships for businesses in one sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005; Yu et al., 2017). For example, given a sentence \"Barack Obama was born in Honolulu, Hawaii.,\" a relationship classifier aims to predict the relationship between \"bornInCity.\" Relation extraction is the key component for building relationships knowledge graphs, and it is crucial for natural language processing applications such as structured search, sentiment analysis, answering questions, and summarization. A major problem for relationship extraction is the lack of labeled training data. In recent years, remote monitoring (Mintz et al., 2011; Surdeanu et al., 2012) emerges as the most popular method for relation extraction - it uses knowledge base facts to select a set of loud data."}, {"heading": "2 Deep Residual Networks for Relation Extraction", "text": "In this section, we describe a novel deep-residual learning architecture for remote relation extraction. Figure 1 describes the architecture of our model."}, {"heading": "2.1 Vector Representation", "text": "Let xi be the i-th word in the sentence and e1, e2 be the two corresponding entities. Each word accesses two embedding tables to get the word that embeds WFi and the position that embeds PFi. Then we concatenate the two embedding and designate each word as a vector of vi = [WFi, PFi]."}, {"heading": "2.1.1 Word Embeddings", "text": "Each representation vi corresponding to xi is a real vector. All vectors are encoded in an embedding matrix Vw, Rdw, and V, where V is a fixed-size vocabulary."}, {"heading": "2.1.2 Position Embeddings", "text": "In connection with (Zeng et al., 2014), a PF is the combination of the relative distances of the current word to the first entity e1 and the second entity e2. In the sentence \"Steve Jobs is the founder of Apple,\" the relative distances from the founder to e1 (Steve Job) or e2 3 or -2 respectively. We then transform the relative distances into real value vectors by taking a randomly initialized position for embedding the matrices Vp-Rdp-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P"}, {"heading": "2.2 Convolution", "text": "Let vi: i + j point to the concatenation of words vi, vi + 1,..., vi + j. A convolution operation involves a filter w-Rhd applied to a window of h-words to create a new attribute. A attribute ci is created from a window of the word vi: i + h \u2212 1 byci = f (w \u00b7 xi: i + h \u2212 1 + b) Here b-R is a distorted term and f is a nonlinear function. This filter is applied to any window of words from v1 to vn to create attribute c = [c1, c2,..., cn \u2212 h + 1] with c-Rs (s = n \u2212 h + 1)."}, {"heading": "2.3 Residual Convolution Block", "text": "Each remaining folding block is a sequence of two folding layers, each followed by a ReLU activation. The core size of all folding layers is h, with the padding being such that the new feature will have the same size as the original one. Here, there are two folding filters w1, w2, w2, Rh x 1. For the first folding layer: c \u0441i = f (w1 \u00b7 ci: i + h \u2212 1 + b1) For the second folding layer: c'i = f (w2 \u00b7 c \u0441i: i + h \u2212 1 + b2) Here b1, b2 are distortion terms. For the remaining learning mode: c = c + c \u00b2 Usually, the notation of c on the left is changed to define as the output vectors of the block."}, {"heading": "2.4 Max Pooling, Softmax Output", "text": "We then apply a max pooling operation on the attribute and take the maximum value c = max {c}. We have described the process by which a attribute is extracted from a filter. Let's take all the attribute into a highly extracted attribute z = [c-1, c-2,..., c-m] (note that we have m-filter here). Then, these attribute are passed to a fully connected Softmax layer, whose output is the probability distribution over relationships. Instead of using y = w \u00b7 z + b for the output unit y in forward propagation, Dropout uses y = w \u00b7 (z-r) + b, where \u0435 is the elemental multiplication operation and r-Rm is a \"masking\" vector of Bernoulli random variables with the probability p of 1. In the test procedure, the learned weight vectors are scaled by p, so that w = undrop-out is used."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Experimental Settings", "text": "In this paper, we use the word embeddings released by (Lin et al., 2016), which are trained on the NYT Freebase Corpus (Riedel et al., 2010). We match our model with the training data by validating it. The word embedding is of size 50. The input text is padded to a fixed size of 100. Implementation is done with a tensor flow adam optimizer, using a mini-batch of size 64, an initial learning rate of 0.001. We initialize our revolutionary layers subsequently (Glorot and Bengio, 2010). Implementation is done with Tensorflow 0.11. All experiments are performed on a single NVidia Titan X (Pascal) GPU. In Table 1, we show all the parameters used in the experiments. We experiment with several state-of-the-art baselines and variants of the ATx model. \u2022 CNN-B: Our implementation of the CNN Titan X (Pascal) GPU. In Table 1, we show all the parameters used in the experiments. We experiment with several state-of-the-the-art baselines and variants of the ATx model. \u2022 CNN-B: Our implementation of the CNN Titan X (Zconline) model 2016 (Netconline, a net evolutionary layer, 2014, a full and evolutionary layer)."}, {"heading": "3.2 NYT-Freebase Dataset Performance", "text": "The advantage of this data set is that there are 522,611 sets in the training data and 172,448 sets in the net data, and this size can help us form a deep network. Similar to previous work (Zeng et al., 2015; Lin CNN et al., 2016), we evaluate our model using several fully connected layers. We report both the overall curves and the curves that are reflected in the precision, as well as the precision @ N (P @ N).In Figure 2, we compare the proposed ResCNN model with various CNN layers that achieve very good results, which is a novel finding. The results also suggest that deeper CNNs with residual learning help extract signals from sharp remote monitoring data. We observe that the overlap occurs when we try to add more layers and add the performance of CNN-9 is much worse than CNN-9. We find out that Net-9 can solve this problem better than Net-Res9 can."}, {"heading": "4 Conclusion", "text": "We show that deeper convolutionary models help to distill signals from noisy inputs, significantly improving performance through abbreviations and identification mapping, and these results are consistent with a recent study (Conneau et al., 2017) that suggests that deeper CNNs have a positive impact on noisy NLP problems."}], "references": [{"title": "Subsequence kernels for relation extraction", "author": ["Razvan Bunescu", "Raymond J Mooney."], "venue": "NIPS, pages 171\u2013178.", "citeRegEx": "Bunescu and Mooney.,? 2005", "shortCiteRegEx": "Bunescu and Mooney.", "year": 2005}, {"title": "Very deep convolutional networks for natural language processing", "author": ["Alexis Conneau", "Holger Schwenk", "Lo\u0131\u0308c Barrault", "Yann Lecun"], "venue": null, "citeRegEx": "Conneau et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Conneau et al\\.", "year": 2017}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "International Conference on Artificial Intelligence and Statistics, pages 249\u2013256.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Exploring various knowledge in relation extraction", "author": ["Zhou GuoDong", "Su Jian", "Zhang Jie", "Zhang Min."], "venue": "Proceedings of the 43rd annual meeting on association for computational linguistics, pages 427\u2013434. Association for Computational Linguis-", "citeRegEx": "GuoDong et al\\.,? 2005", "shortCiteRegEx": "GuoDong et al\\.", "year": 2005}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Knowledgebased weak supervision for information extraction of overlapping relations", "author": ["Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computa-", "citeRegEx": "Hoffmann et al\\.,? 2011", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Neural relation extraction with selective attention over instances", "author": ["Yankai Lin", "Shiqi Shen", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun."], "venue": "ACL 2016.", "citeRegEx": "Lin et al\\.,? 2016", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum."], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 148\u2013163. Springer.", "citeRegEx": "Riedel et al\\.,? 2010", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Attentionbased convolutional neural network for semantic relation extraction", "author": ["Yatian Shen", "Xuanjing Huang."], "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers.", "citeRegEx": "Shen and Huang.,? 2016", "shortCiteRegEx": "Shen and Huang.", "year": 2016}, {"title": "Multi-instance multi-label learning for relation extraction", "author": ["Mihai Surdeanu", "Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com-", "citeRegEx": "Surdeanu et al\\.,? 2012", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Improved neural relation detection for knowledge base question answering. CoRR, abs/1704.06194", "author": ["Mo Yu", "Wenpeng Yin", "Kazi Saidul Hasan", "C\u0131\u0301cero Nogueira dos Santos", "Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2017}, {"title": "Kernel methods for relation extraction", "author": ["Dmitry Zelenko", "Chinatsu Aone", "Anthony Richardella."], "venue": "Journal of machine learning research, 3(Feb):1083\u20131106.", "citeRegEx": "Zelenko et al\\.,? 2003", "shortCiteRegEx": "Zelenko et al\\.", "year": 2003}, {"title": "Distant supervision for relation extraction via piecewise convolutional neural networks", "author": ["Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), Lis-", "citeRegEx": "Zeng et al\\.,? 2015", "shortCiteRegEx": "Zeng et al\\.", "year": 2015}, {"title": "Relation classification via convolutional deep neural network", "author": ["Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "In COLING,", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Deep residual learning (ResNet) (He et al., 2016) is a new method for training very deep neural networks using identity mapping for shortcut connections.", "startOffset": 32, "endOffset": 49}, {"referenceID": 12, "context": "Relation extraction is the task of predicting attributes and relations for entities in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005; Yu et al., 2017).", "startOffset": 98, "endOffset": 185}, {"referenceID": 0, "context": "Relation extraction is the task of predicting attributes and relations for entities in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005; Yu et al., 2017).", "startOffset": 98, "endOffset": 185}, {"referenceID": 3, "context": "Relation extraction is the task of predicting attributes and relations for entities in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005; Yu et al., 2017).", "startOffset": 98, "endOffset": 185}, {"referenceID": 11, "context": "Relation extraction is the task of predicting attributes and relations for entities in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005; Yu et al., 2017).", "startOffset": 98, "endOffset": 185}, {"referenceID": 7, "context": "In recent years, distant supervision (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012) emerges as the most popular method for relation extraction\u2014 it uses knowledge base facts to select a set of noisy instances from unlabeled data.", "startOffset": 37, "endOffset": 103}, {"referenceID": 5, "context": "In recent years, distant supervision (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012) emerges as the most popular method for relation extraction\u2014 it uses knowledge base facts to select a set of noisy instances from unlabeled data.", "startOffset": 37, "endOffset": 103}, {"referenceID": 10, "context": "In recent years, distant supervision (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012) emerges as the most popular method for relation extraction\u2014 it uses knowledge base facts to select a set of noisy instances from unlabeled data.", "startOffset": 37, "endOffset": 103}, {"referenceID": 14, "context": "Among all the machine learning approaches for distant supervision, the recently proposed Convolutional Neural Networks (CNNs) model (Zeng et al., 2014) achieved the state-of-the-art performance.", "startOffset": 132, "endOffset": 151}, {"referenceID": 6, "context": "Various attention strategies (Lin et al., 2016; Shen and Huang, 2016) for CNNs are also proposed, obtaining impressive results.", "startOffset": 29, "endOffset": 69}, {"referenceID": 9, "context": "Various attention strategies (Lin et al., 2016; Shen and Huang, 2016) for CNNs are also proposed, obtaining impressive results.", "startOffset": 29, "endOffset": 69}, {"referenceID": 5, "context": ", 2009; Hoffmann et al., 2011; Surdeanu et al., 2012) emerges as the most popular method for relation extraction\u2014 it uses knowledge base facts to select a set of noisy instances from unlabeled data. Among all the machine learning approaches for distant supervision, the recently proposed Convolutional Neural Networks (CNNs) model (Zeng et al., 2014) achieved the state-of-the-art performance. Following their success, Zeng et al. (2015) proposed a piece-wise max-pooling strategy to improve the CNNs.", "startOffset": 8, "endOffset": 438}, {"referenceID": 4, "context": "More specifically, we designed a convolutional neural network based on residual learning (He et al., 2016)\u2014we show how one can incorporate word embeddings and position embeddings into a deep residual network, while feeding identity feedback to convolutional layers for this noisy relation prediction task.", "startOffset": 89, "endOffset": 106}, {"referenceID": 8, "context": "Empirically, we evaluate on the NYT-Freebase dataset (Riedel et al., 2010), and demonstrate the state-of-the-art performance using deep CNNs with identify mapping and shortcuts.", "startOffset": 53, "endOffset": 74}, {"referenceID": 14, "context": "Following (Zeng et al., 2014), a PF is the combination of the relative distances of the current word to the first entity e1 and the second entity e2.", "startOffset": 10, "endOffset": 29}, {"referenceID": 6, "context": "In this paper, we use the word embeddings released by (Lin et al., 2016) which are trained on the NYT-Freebase corpus (Riedel et al.", "startOffset": 54, "endOffset": 72}, {"referenceID": 8, "context": ", 2016) which are trained on the NYT-Freebase corpus (Riedel et al., 2010).", "startOffset": 53, "endOffset": 74}, {"referenceID": 2, "context": "We initialize our convolutional layers following (Glorot and Bengio, 2010).", "startOffset": 49, "endOffset": 74}, {"referenceID": 14, "context": "\u2022 CNN-B: Our implementation of the CNN baseline (Zeng et al., 2014) which contains one convolutional layer, and one fully connected layer.", "startOffset": 48, "endOffset": 67}, {"referenceID": 6, "context": "\u2022 CNN+ATT: CNN-B with attention over instance learning (Lin et al., 2016).", "startOffset": 55, "endOffset": 73}, {"referenceID": 6, "context": "\u2022 PCNN+ATT: Piecewise CNN-B with attention over instance learning (Lin et al., 2016).", "startOffset": 66, "endOffset": 84}, {"referenceID": 8, "context": "We evaluate our models on the widely used NYT freebase larger dataset (Riedel et al., 2010).", "startOffset": 70, "endOffset": 91}, {"referenceID": 4, "context": "Note that ImageNet dataset used by the original ResNet paper (He et al., 2016) has 1.", "startOffset": 61, "endOffset": 78}, {"referenceID": 13, "context": "Similar to previous work (Zeng et al., 2015; Lin et al., 2016), we evaluate our model using the held-out evaluation.", "startOffset": 25, "endOffset": 62}, {"referenceID": 6, "context": "Similar to previous work (Zeng et al., 2015; Lin et al., 2016), we evaluate our model using the held-out evaluation.", "startOffset": 25, "endOffset": 62}, {"referenceID": 1, "context": "These results aligned with a recent study (Conneau et al., 2017), suggesting that deeper CNNs do have positive effects on noisy NLP problems.", "startOffset": 42, "endOffset": 64}], "year": 2017, "abstractText": "Deep residual learning (ResNet) (He et al., 2016) is a new method for training very deep neural networks using identity mapping for shortcut connections. ResNet has won the ImageNet ILSVRC 2015 classification task, and achieved state-of-theart performances in many computer vision tasks. However, the effect of residual learning on noisy natural language processing tasks is still not well understood. In this paper, we design a novel convolutional neural network (CNN) with residual learning, and investigate its impacts on the task of distantly supervised noisy relation extraction. In contradictory to popular beliefs that ResNet only works well for very deep networks, we found that even with 9 layers of CNNs, using identity mapping could significantly improve the performance for distantly-supervised relation extraction.", "creator": "LaTeX with hyperref package"}}}