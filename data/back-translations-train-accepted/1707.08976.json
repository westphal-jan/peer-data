{"id": "1707.08976", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jul-2017", "title": "Effective Inference for Generative Neural Parsing", "abstract": "Generative neural models have recently achieved state-of-the-art results for constituency parsing. However, without a feasible search procedure, their use has so far been limited to reranking the output of external parsers in which decoding is more tractable. We describe an alternative to the conventional action-level beam search used for discriminative neural models that enables us to decode directly in these generative models. We then show that by improving our basic candidate selection strategy and using a coarse pruning function, we can improve accuracy while exploring significantly less of the search space. Applied to the model of Choe and Charniak (2016), our inference procedure obtains 92.56 F1 on section 23 of the Penn Treebank, surpassing prior state-of-the-art results for single-model systems.", "histories": [["v1", "Thu, 27 Jul 2017 18:01:18 GMT  (29kb)", "http://arxiv.org/abs/1707.08976v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mitchell stern", "daniel fried", "dan klein"], "accepted": true, "id": "1707.08976"}, "pdf": {"name": "1707.08976.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["mitchell@cs.berkeley.edu", "dfried@cs.berkeley.edu", "klein@cs.berkeley.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 7.08 976v 1 [cs.C L] 27 Jul 2 017 State-of-the-art results achieved in parsing constituencies. However, without a practicable search method, their use has so far been limited to re-analysing the results of external parsers, where the decoding is more comprehensible. We describe an alternative to the traditional action-level beam search, which is used for discriminatory neural models and allows us to decode directly in these generative models. Subsequently, we show that by improving our basic candidate selection strategy and using a coarse interface function, we can improve accuracy and thereby explore significantly less search space. Applied to the Choe and Charniak model (2016), our reference method obtains 92.56 F1 in section 23 of Penn Treebank, thus exceeding previous state-of-the-art results for individual model systems."}, {"heading": "1 Introduction", "text": "A recent series of papers has shown the success of generative neural models for parsing constituencies (Dyer et al., 2016; Choe and Charniak, 2016). As with discriminatory neural parsers, these models lack a dynamic program for exact conclusions due to their modeling of unlimited dependencies. However, while discriminatory neural parsers are able to achieve strong results through greedy search (Dyer et al., 2016) or beam search with a small beam (Vinyals et al., 2015), we find that a simple action-level approach fails outright in generative setting. Perhaps for this reason, the application of generative neural models has so far been limited to reassessing the results of external parsers (Vinyals et al., 2015), since a generative parser defines a common distribution across sentences and parsetrees, the probability mass between constituencies is unevenly distributed between a small number of common structural measures and a large vocabulary element."}, {"heading": "2 Common Framework", "text": "The generative neural parsers of Dyer et al. (2016) and Choe and Charniak (2016) can be unified under a common shift-reduce frame. Both systems build parse trees in left to right depth first order by performing a sequence of actions as shown in Figure 1. These actions can be grouped according to the sentence \"He had an idea.\" The tree is constructed in left depth first order. The tree contains only nonterminals and words; parts-of-speech tags are not included. OPEN (X) and CLOSE (X) are represented as \"(X\" and \"X\") as \"dense state.\" The tree is divided into three main types: OPEN (X) and CLOSE (X), with the word X being \"denser.\""}, {"heading": "3 Model and Training Setup", "text": "We reimplemented the generative model described in Choe and Charniak (2016) and trained it at Penn Treebank (Marcus et al., 1993) using 1The model described in Dyer et al. (2016) has only one CLOSE action, while the model described in Choe and Charniak (2016) comments CLOSE (X) actions with their nonterminals. The more general version we present here: their published hyperparameters and pre-processing. However, instead of selecting the final model based on reranking performance, we instead perform an early stop based on development set perplexity. We use Sections 2-21 of Penn Treebank for training, Section 22 for development and Section 23 for testing. The action space of the model consists of 26 matching pairs of OPEN and CLOSE actions, one for each nonterminal and 6,870 SHIFT actions, one pre-processed for each word type we use for each subsequent type of generative, during our subsequent type 2 experiments."}, {"heading": "4 Action-Level Search", "text": "Given that the usual search at the action level has been successfully applied to discriminatory neural parsers (Vinyals et al., 2015; Dyer et al., 2016), it provides a reasonable starting point for decoding in generative models. However, even with large beam sizes, the following pathological behavior occurs in generative decoding, which means that reasonable parses are not found. Regardless of the sequence of actions taken so far, the generative model tends to assign much higher probabilities to structural OPEN and CLOSE actions than lexical SHIFT actions, as shown in Figure 2. Therefore, the model prefers to constantly open new components until a hard limit is reached, as the alternative at each step is to take the low probability shift of the next word. The resulting sequence typically has a significantly lower overall expectation than a plausible analysis, but the myopic comparison between structural and rational actions prevents rational candidates from remaining on the beam."}, {"heading": "5 Word-Level Search", "text": "The imbalance between the probabilities of structural and lexical actions suggests that the two types of actions should not compete within a bar, which leads us to consider an extended state space in which they are kept systematically separate, as was done by Fried et al. (2017). In conventional radiation search at the action level, hypotheses are grouped by the length of their action history | A |. Let Ai designate the amount of actions taken since the ith layer action, instead we group hypotheses by the pair (i, | Ai |), with i ranging from 0 to the length of the sentence. Let k specify the target bar size. The search process begins with the empty hypothesis in the bucket (0, 0). Word steps are then performed according to the following procedure for i = 0, 1,.. up to the length of the sentence (inclusive)."}, {"heading": "6 Fast-Track Candidate Selection", "text": "The word-level search described in Section 5 takes a step toward improving the problem that leads to the failure of the action-level search, namely direct competition between common structural actions with a high probability and actions with a low frequency shift with a low probability. However, the problem does exist to some extent, as successors of both types are merged from a given bucket and filtered as a single collection before being sent to their respective destinations. We therefore propose a more direct solution to the problem, where a small number of k-k-SHIFT successors are quickly tracked to the next bucket at word level before filtering takes place. These fast-tracked candidates completely bypass the competition with potentially highly rated OPEN or CLOSE successors, and in practice provide higher-quality results with minimal overhead. See Figure 3 for an illustration."}, {"heading": "7 OPEN Action Pruning", "text": "At any point during the course of a hypothesis, either 0 or all 26 OPEN actions are available, compared to a maximum of 1 CLOSE action and a maximum of 1 SHIFT action. Therefore, OPEN actions, if available, comprise most or all of the succession actions of a candidate. To reduce this part of the search area, it is natural to consider whether some of these actions could be excluded by a rough model of pruning."}, {"heading": "7.1 Coarse Model", "text": "We look at a class of simple crop models that condition the c \u2265 0 most recent actions and the next word in the sentence, and predict a probability distribution for the next action. In the interest of efficiency, we split all SHIFT actions into a single SHIFT action unleashed, significantly reducing the size of the output vocabulary. Input into the crop model in due time t is concatenating a vector embedded for each action in context (at \u2212 c + 1,.., at \u2212 1) and a vector embedded for the next word w: vt = [eat \u2212 c; eat \u2212 c + 1;..; eat \u2212 1; ew], each ej being a learned vector loss. The expression model itself is implemented by embedding the input vector through a single-layer feed network with a ReLU non-linearity."}, {"heading": "7.2 Strategy and Empirical Lower Bound", "text": "As mentioned above, if a hypothesis is entitled to open a new constituent, most of its successors are obtained through OPEN actions. Accordingly, we use the rough model to limit the amount of OPEN actions to be examined. If we evaluate the pool of successors for a given set of hypotheses during the beam search, we run the rough model for each hypothesis to obtain a distribution of their next possible actions, and collect all the rough values of the potential OPEN successors. We then discard the OPEN successors whose rough values are below the top 1 \u2212 p quantity for a fixed 0 < p < p < 1 to guarantee that no more than a p-fraction of the OPEN successors will be considered for evaluation."}, {"heading": "7.3 Pruning Results", "text": "We repeat our best experiment from section 6 with a trimming function of the 2nd order and trimming fractions p = 6 / 26,..., 11 / 26. The results are listed in Table 4. We observe that at p already at 8 / 26 \u2248 0.308 the performance is equivalent to the untrimmed setting (maximum 0.1 absolute difference in F1 value). If p is set to 7 / 26 \u2248 0.269, there is a decrease of 0.18, and if p is set to 6 / 26 \u2248 0.231, there is a decrease of 0.40. Therefore, the degradation starts directly around the above mentioned empirically motivated threshold of 6 / 26, but we can trim 1 \u2212 8 / 26 \u2248 69.2% of OPEN successors with minimal performance changes."}, {"heading": "8 Final Results and Conclusion", "text": "We note that the best overall settings are a bar size of k = 2000, a word bar size of kw = 200 and ks = 20 fast-track candidates per step, as these 2 thresholds are not exact due to the fact that our trimming process works with the collection of successors of multiple hypotheses at the time of conclusion and not with the successors of a single hypothesis. We achieve both the highest probabilities under the model and the highest development rate F1. We report our test results under Section 23 of the Penn Treebank under these settings in Table 5 both with and without circumcision, as well as a number of other recent results. We achieve F1 values of 92.56 on the non-circumcision test set and 92.53 when 1 \u2212 8 / 26 \u2248 69.2% of the successors of OPEN are circumcised, achieving a performance significantly higher than the previous state of the art, achieving results for individual model parsers. This shows that the Choe and Charniak (2016) model works well if we continue to use a preheat-sensitive system that confirms the characteristics of our own."}, {"heading": "Acknowledgments", "text": "MS is supported by an NSF Graduate Research Fellowship. DF is supported by an NDSEG Fellowship."}], "references": [{"title": "Generative incremental dependency parsing with neural networks", "author": ["Jan Buys", "Phil Blunsom."], "venue": "Proceedings of the 53rd Annual Meeting of the", "citeRegEx": "Buys and Blunsom.,? 2015", "shortCiteRegEx": "Buys and Blunsom.", "year": 2015}, {"title": "Top-down nearly-contextsensitive parsing", "author": ["Eugene Charniak."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 674\u2013683. Association for Computational Linguistics.", "citeRegEx": "Charniak.,? 2010", "shortCiteRegEx": "Charniak.", "year": 2010}, {"title": "Parsing as language modeling", "author": ["Do Kook Choe", "Eugene Charniak."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2331\u20132336, Austin, Texas. Association for Computational Linguistics.", "citeRegEx": "Choe and Charniak.,? 2016", "shortCiteRegEx": "Choe and Charniak.", "year": 2016}, {"title": "Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles", "author": ["James Cross", "Liang Huang."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1\u201311, Austin,", "citeRegEx": "Cross and Huang.,? 2016", "shortCiteRegEx": "Cross and Huang.", "year": 2016}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Improving neural parsing by disentangling model combination and reranking effects", "author": ["Daniel Fried", "Mitchell Stern", "Dan Klein."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Van-", "citeRegEx": "Fried et al\\.,? 2017", "shortCiteRegEx": "Fried et al\\.", "year": 2017}, {"title": "Inducing history representations for broad coverage statistical parsing", "author": ["James Henderson."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-", "citeRegEx": "Henderson.,? 2003", "shortCiteRegEx": "Henderson.", "year": 2003}, {"title": "Shift-reduce constituent parsing with neural lookahead features", "author": ["Jiangming Liu", "Yue Zhang."], "venue": "Transactions of the Association for Computational Linguistics, 5:45\u201358.", "citeRegEx": "Liu and Zhang.,? 2017", "shortCiteRegEx": "Liu and Zhang.", "year": 2017}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Probabilistic top-down parsing and language modeling", "author": ["Brian Roark."], "venue": "Computational linguistics, 27(2):249\u2013276.", "citeRegEx": "Roark.,? 2001", "shortCiteRegEx": "Roark.", "year": 2001}, {"title": "Bayesian symbol-refined tree substitution grammars for syntactic parsing", "author": ["Hiroyuki Shindo", "Yusuke Miyao", "Akinori Fujino", "Masaaki Nagata."], "venue": "In", "citeRegEx": "Shindo et al\\.,? 2012", "shortCiteRegEx": "Shindo et al\\.", "year": 2012}, {"title": "A minimal span-based neural constituency parser. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Mitchell Stern", "Jacob Andreas", "andDan Klein"], "venue": null, "citeRegEx": "Stern et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Stern et al\\.", "year": 2017}, {"title": "A latent variable model for generative dependency parsing", "author": ["Ivan Titov", "James Henderson."], "venue": "Trends in Parsing Technology, pages 35\u201355. Springer.", "citeRegEx": "Titov and Henderson.,? 2010", "shortCiteRegEx": "Titov and Henderson.", "year": 2010}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems, pages 2773\u20132781.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Applied to the model of Choe and Charniak (2016), our inference procedure obtains 92.", "startOffset": 33, "endOffset": 49}, {"referenceID": 4, "context": "A recent line of work has demonstrated the success of generative neural models for constituency parsing (Dyer et al., 2016; Choe and Charniak, 2016).", "startOffset": 104, "endOffset": 148}, {"referenceID": 2, "context": "A recent line of work has demonstrated the success of generative neural models for constituency parsing (Dyer et al., 2016; Choe and Charniak, 2016).", "startOffset": 104, "endOffset": 148}, {"referenceID": 4, "context": "However, while discriminative neural parsers are able to obtain strong results using greedy search (Dyer et al., 2016) or beam search with a small beam (Vinyals et al.", "startOffset": 99, "endOffset": 118}, {"referenceID": 13, "context": ", 2016) or beam search with a small beam (Vinyals et al., 2015), we find that a simple action-level approach fails outright in the generative setting.", "startOffset": 41, "endOffset": 63}, {"referenceID": 6, "context": "A notion of equal competition among hypotheses is then desirable, an idea that has previously been explored in generative models for constituency parsing (Henderson, 2003) and dependency parsing (Titov and Henderson, 2010; Buys and Blunsom, 2015), among other tasks.", "startOffset": 154, "endOffset": 171}, {"referenceID": 12, "context": "A notion of equal competition among hypotheses is then desirable, an idea that has previously been explored in generative models for constituency parsing (Henderson, 2003) and dependency parsing (Titov and Henderson, 2010; Buys and Blunsom, 2015), among other tasks.", "startOffset": 195, "endOffset": 246}, {"referenceID": 0, "context": "A notion of equal competition among hypotheses is then desirable, an idea that has previously been explored in generative models for constituency parsing (Henderson, 2003) and dependency parsing (Titov and Henderson, 2010; Buys and Blunsom, 2015), among other tasks.", "startOffset": 195, "endOffset": 246}, {"referenceID": 0, "context": "A notion of equal competition among hypotheses is then desirable, an idea that has previously been explored in generative models for constituency parsing (Henderson, 2003) and dependency parsing (Titov and Henderson, 2010; Buys and Blunsom, 2015), among other tasks. We describe a related state-augmented beam search for neural generative constituency parsers in which lexical actions compete only with each other rather than with structural actions. Applying this inference procedure to the generative model of Choe and Charniak (2016), we find that it yields a self-contained generative parser that achieves high performance.", "startOffset": 223, "endOffset": 537}, {"referenceID": 8, "context": "Additionally, motivated by the look-ahead heuristic used in the top-down parsers of Roark (2001) and Charniak (2010), we also experiment with a simple coarse pruning function that allows us to reduce the number of states expanded per candidate by several times without compromising accuracy.", "startOffset": 84, "endOffset": 97}, {"referenceID": 1, "context": "Additionally, motivated by the look-ahead heuristic used in the top-down parsers of Roark (2001) and Charniak (2010), we also experiment with a simple coarse pruning function that allows us to reduce the number of states expanded per candidate by several times without compromising accuracy.", "startOffset": 101, "endOffset": 117}, {"referenceID": 2, "context": "The generative neural parsers of Dyer et al. (2016) and Choe and Charniak (2016) can be unified under a common shift-reduce framework.", "startOffset": 33, "endOffset": 52}, {"referenceID": 1, "context": "(2016) and Choe and Charniak (2016) can be unified under a common shift-reduce framework.", "startOffset": 20, "endOffset": 36}, {"referenceID": 4, "context": "For a given hypothesis, this requirement implies several constraints on the successor set (Dyer et al., 2016); e.", "startOffset": 90, "endOffset": 109}, {"referenceID": 8, "context": "We reimplemented the generative model described in Choe and Charniak (2016) and trained it on the Penn Treebank (Marcus et al., 1993) using", "startOffset": 112, "endOffset": 133}, {"referenceID": 1, "context": "We reimplemented the generative model described in Choe and Charniak (2016) and trained it on the Penn Treebank (Marcus et al.", "startOffset": 60, "endOffset": 76}, {"referenceID": 2, "context": "The model described in Dyer et al. (2016) has only a single CLOSE action, whereas the model described in Choe and Charniak (2016) annotates CLOSE(X) actions with their nonterminals.", "startOffset": 23, "endOffset": 42}, {"referenceID": 1, "context": "(2016) has only a single CLOSE action, whereas the model described in Choe and Charniak (2016) annotates CLOSE(X) actions with their nonterminals.", "startOffset": 79, "endOffset": 95}, {"referenceID": 13, "context": "Given that ordinary action-level search has been applied successfully to discriminative neural parsers (Vinyals et al., 2015; Dyer et al., 2016), it offers a sensible starting point for decoding in generative models.", "startOffset": 103, "endOffset": 144}, {"referenceID": 4, "context": "Given that ordinary action-level search has been applied successfully to discriminative neural parsers (Vinyals et al., 2015; Dyer et al., 2016), it offers a sensible starting point for decoding in generative models.", "startOffset": 103, "endOffset": 144}, {"referenceID": 5, "context": "This leads us to consider an augmented state space in which they are kept separate by design, as was done by Fried et al. (2017). In conventional action-level beam search, hypotheses are grouped by the length of their action history |A|.", "startOffset": 109, "endOffset": 129}, {"referenceID": 3, "context": "1 Cross and Huang (2016) 90.", "startOffset": 2, "endOffset": 25}, {"referenceID": 3, "context": "1 Cross and Huang (2016) 90.5 92.1 91.3 Dyer et al. (2016) \u2013 \u2013 91.", "startOffset": 2, "endOffset": 59}, {"referenceID": 7, "context": "5 Shindo et al. (2012) (ensemble) \u2013 \u2013 92.", "startOffset": 2, "endOffset": 23}, {"referenceID": 1, "context": "4 Choe and Charniak (2016) (rerank) \u2013 \u2013 92.", "startOffset": 11, "endOffset": 27}, {"referenceID": 1, "context": "4 Choe and Charniak (2016) (rerank) \u2013 \u2013 92.6 Dyer et al. (2016) (rerank) \u2013 \u2013 93.", "startOffset": 11, "endOffset": 64}, {"referenceID": 1, "context": "This demonstrates that the model of Choe and Charniak (2016) works well as an accurate, self-contained system.", "startOffset": 45, "endOffset": 61}], "year": 2017, "abstractText": "Generative neural models have recently achieved state-of-the-art results for constituency parsing. However, without a feasible search procedure, their use has so far been limited to reranking the output of external parsers in which decoding is more tractable. We describe an alternative to the conventional action-level beam search used for discriminative neural models that enables us to decode directly in these generative models. We then show that by improving our basic candidate selection strategy and using a coarse pruning function, we can improve accuracy while exploring significantly less of the search space. Applied to the model of Choe and Charniak (2016), our inference procedure obtains 92.56 F1 on section 23 of the Penn Treebank, surpassing prior state-of-the-art results for single-model systems.", "creator": "LaTeX with hyperref package"}}}