{"id": "1707.09067", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jul-2017", "title": "Adapting Sequence Models for Sentence Correction", "abstract": "In a controlled experiment of sequence-to-sequence approaches for the task of sentence correction, we find that character-based models are generally more effective than word-based models and models that encode subword information via convolutions, and that modeling the output data as a series of diffs improves effectiveness over standard approaches. Our strongest sequence-to-sequence model improves over our strongest phrase-based statistical machine translation model, with access to the same data, by 6 M2 (0.5 GLEU) points. Additionally, in the data environment of the standard CoNLL-2014 setup, we demonstrate that modeling (and tuning against) diffs yields similar or better M2 scores with simpler models and/or significantly less data than previous sequence-to-sequence approaches.", "histories": [["v1", "Thu, 27 Jul 2017 22:50:55 GMT  (33kb,D)", "http://arxiv.org/abs/1707.09067v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["allen schmaltz", "yoon kim", "alexander m rush", "stuart m shieber"], "accepted": true, "id": "1707.09067"}, "pdf": {"name": "1707.09067.pdf", "metadata": {"source": "CRF", "title": "Adapting Sequence Models for Sentence Correction", "authors": ["Allen Schmaltz", "Yoon Kim", "Alexander M. Rush", "Stuart M. Shieber"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The task of sentence correction is to convert a set of natural language that may or may not convert errors into a corrected version. It is seen as a component of a learning tool or a writing assistant and has been driven by a series of common tasks since 2011 (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014). Recent work on language correction focuses on the data shared by CoNLL-2014 (Ng et al., 2014), a set of corrected essays by second-language speakers. CoNLLL2014 data consists of only about 60,000 sentences, and as such competing systems have made large volumes of corrected text without annotation."}, {"heading": "2 Background and Methods", "text": "In fact, most of them are able to survive themselves, and they are able to survive themselves, \"he said in an interview with The New York Times.\" I don't think I'm able to survive myself, \"he said.\" I don't think I'm able to survive myself. \"He added,\" I don't think I'm able to survive myself. \"He added,\" I don't think I'm able to survive myself. \"He added,\" I don't think I'm able to survive myself and I'm able to survive myself. \""}, {"heading": "3 Experiments", "text": "The results of the study show that the individual countries are not only countries, but also regions in which they are regions, in which most people are able to move, and in which most of them are able to move. The results of the study show that the individual countries are countries in which most people are able to move, and that these are countries in which most of them are able to move, and that they are regions in which they are able to move, in which most of them are able to move. The results of the study show that they are countries in which most of them are able to move, and that they are regions in which they are able to move, in which they live and live in which they live, in which they live, in which they live and live in which they live, in which they live and live in which they live, in which they live, in which they live and live, in which they live, in which they live, in which they live, in which they live and live, in which they live, in which they live, in which they live, in which they live, in which they live and live in which they live, in which they live and in which they live, in which they live and in which they live, in which they live, in which they live and in which they live in which they live, in which they live, in which they live and in which they live in which they live, in which they live, in which they live and in which they live in which they live, in which they live, in which they live and in which they live in which they live in which they live, in which they live, in which they live and in which they live in which they live, in which they live, in which they live and in which they live in which they live, in which they live and in which they live in which they live."}, {"heading": "4 Results and Analysis: AESW", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "5 Results and Analysis: CoNLL", "text": "Table 4 shows the results on the CoNLL development table, and Table 5 shows the final test results. As the CoNLL data does not contain enough data to form neural models, previous work adds the Lang-8 data obtained from crowdsourcing; however, these data are not professionally commentated.Because the distribution of corrections differs between the development environment / test and training sets, we need to adjust the precision and remember this. As shown in Table 4, the effectiveness of WORD + BI increases significantly by significantly increasing the weight8 assigned to the diff tags on the CoNLL-2013. Note that in early experiments at AESW, we improved the weight sequence negligibly. 9The single model with the highest M2 value was then executed on the test set. Here, a single set is used for coordination and development."}, {"heading": "6 Conclusion", "text": "Our experiments show that on a large, professionally annotated dataset, a sequence sequence-based, character-based model of differences can lead to considerable gains in effectiveness over a modern SMT system with task-specific characteristics, ceteris paribus. Furthermore, modelling differences in the crowd-sourced environment of CoNLL data, where there are comparatively few professionally annotated sentences in training, enables tuning that improves the effectiveness of sequence-to-sequence models for the task. 10 For reference, the reported M2 results of the carefully optimized SMT system from Junczys-Dowmunt and Grundkiewicz (2016), which was trained on NUCLE and Lang-8, with parameter vectors averaged over multiple passes, with a Wikipedia LM of 45.95 and the addition of a Common Crawl LM of 49.49."}, {"heading": "A Supplemental Material", "text": "Further model trainings and inferences of the publicly available code will be converted in the previous way. Our code and related materials are available at: https: / / github.com / allenschmaltz / grammar.The training and tuning sizes of the AESW dataset are those that include sets of more than 126 characters on the source or destination page (into source sequences or target sequences with different annotations) from the raw AESW datasets that are based on set lengths without filtering. As part of pre-processing, the sets from the AESW XML models will be tokenized in Penn Treebank style. Case will be retained and digits will not be replaced by holder symbols for the sequence-to-sequence-to-sequence models. SMT models will use the truecasing11 and tokenization pipeline of the publicly available code."}], "references": [{"title": "Adapting grammatical error correction based on the native language of writers with neural network joint models", "author": ["Shamil Chollampatt", "Duc Tam Hoang", "Hwee Tou Ng."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Chollampatt et al\\.,? 2016a", "shortCiteRegEx": "Chollampatt et al\\.", "year": 2016}, {"title": "Neural network translation models for grammatical error correction", "author": ["Shamil Chollampatt", "Kaveh Taghipour", "Hwee Tou Ng."], "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI\u201916, pages 2768\u20132774.", "citeRegEx": "Chollampatt et al\\.,? 2016b", "shortCiteRegEx": "Chollampatt et al\\.", "year": 2016}, {"title": "Better evaluation for grammatical error correction", "author": ["Daniel Dahlmeier", "Hwee Tou Ng."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Dahlmeier and Ng.,? 2012", "shortCiteRegEx": "Dahlmeier and Ng.", "year": 2012}, {"title": "Building a large annotated corpus of learner english: The nus corpus of learner english", "author": ["Daniel Dahlmeier", "Hwee Tou Ng", "Siew Mei Wu."], "venue": "Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applica-", "citeRegEx": "Dahlmeier et al\\.,? 2013", "shortCiteRegEx": "Dahlmeier et al\\.", "year": 2013}, {"title": "Hoo 2012: A report on the preposition and determiner error correction shared task", "author": ["Robert Dale", "Ilya Anisimoff", "George Narroway."], "venue": "Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 54\u201362,", "citeRegEx": "Dale et al\\.,? 2012", "shortCiteRegEx": "Dale et al\\.", "year": 2012}, {"title": "Helping our own: The hoo 2011 pilot shared task", "author": ["Robert Dale", "Adam Kilgarriff."], "venue": "Proceedings of the 13th European Workshop on Natural Language Generation, ENLG \u201911, pages 242\u2013249, Stroudsburg, PA, USA. Association for Computa-", "citeRegEx": "Dale and Kilgarriff.,? 2011", "shortCiteRegEx": "Dale and Kilgarriff.", "year": 2011}, {"title": "Automated evaluation of scientific writing data set (version 1.2) [data file", "author": ["Vidas Daudaravicius"], "venue": null, "citeRegEx": "Daudaravicius.,? \\Q2016\\E", "shortCiteRegEx": "Daudaravicius.", "year": 2016}, {"title": "A report on the automatic evaluation of scientific writing shared task", "author": ["Vidas Daudaravicius", "Rafael E. Banchs", "Elena Volodina", "Courtney Napoles."], "venue": "Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Appli-", "citeRegEx": "Daudaravicius et al\\.,? 2016", "shortCiteRegEx": "Daudaravicius et al\\.", "year": 2016}, {"title": "Can markov models over minimal translation units help phrasebased smt", "author": ["Nadir Durrani", "Alexander Fraser", "Helmut Schmid", "Hieu Hoang", "Philipp Koehn"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Durrani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Durrani et al\\.", "year": 2013}, {"title": "Randomized significance tests in machine translation", "author": ["Yvette Graham", "Nitika Mathur", "Timothy Baldwin."], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 266\u2013274, Baltimore, Maryland, USA. Association for Compu-", "citeRegEx": "Graham et al\\.,? 2014", "shortCiteRegEx": "Graham et al\\.", "year": 2014}, {"title": "Exploiting n-best hypotheses to improve an smt approach to grammatical error correction", "author": ["Duc Tam Hoang", "Shamil Chollampatt", "Hwee Tou Ng."], "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJ-", "citeRegEx": "Hoang et al\\.,? 2016", "shortCiteRegEx": "Hoang et al\\.", "year": 2016}, {"title": "A Nested Attention Neural Hybrid Model for Grammatical Error Correction", "author": ["J. Ji", "Q. Wang", "K. Toutanova", "Y. Gong", "S. Truong", "J. Gao."], "venue": "ArXiv eprints.", "citeRegEx": "Ji et al\\.,? 2017", "shortCiteRegEx": "Ji et al\\.", "year": 2017}, {"title": "Phrase-based machine translation is state-ofthe-art for automatic grammatical error correction", "author": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages", "citeRegEx": "Junczys.Dowmunt and Grundkiewicz.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt and Grundkiewicz.", "year": 2016}, {"title": "Character-Aware Neural Language Models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."], "venue": "Proceedings of AAAI.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "Proceedings of EMNLP 2004, pages 388\u2013395, Barcelona, Spain. Association for Computational Linguistics.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1412\u20131421, Lisbon, Portugal. As-", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "The effect of learner corpus size in grammatical error correction of ESL writings", "author": ["Tomoya Mizumoto", "Yuta Hayashibe", "Mamoru Komachi", "Masaaki Nagata", "Yuji Matsumoto."], "venue": "Proceedings of COLING 2012: Posters, pages 863\u2013872, Mumbai,", "citeRegEx": "Mizumoto et al\\.,? 2012", "shortCiteRegEx": "Mizumoto et al\\.", "year": 2012}, {"title": "GLEU without tuning", "author": ["Courtney Napoles", "Keisuke Sakaguchi", "Matt Post", "Joel Tetreault."], "venue": "eprint arXiv:1605.02592 [cs.CL].", "citeRegEx": "Napoles et al\\.,? 2016", "shortCiteRegEx": "Napoles et al\\.", "year": 2016}, {"title": "The CoNLL-2014 shared task on grammatical error correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant."], "venue": "Proceedings of", "citeRegEx": "Ng et al\\.,? 2014", "shortCiteRegEx": "Ng et al\\.", "year": 2014}, {"title": "The CoNLL2013 shared task on grammatical error correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared", "citeRegEx": "Ng et al\\.,? 2013", "shortCiteRegEx": "Ng et al\\.", "year": 2013}, {"title": "The cambridge learner corpus: Error coding and analysis for lexicography and ELT", "author": ["Diane Nicholls."], "venue": "Proceedings of the Corpus Linguistics 2003 conference, pages 572\u2013581.", "citeRegEx": "Nicholls.,? 2003", "shortCiteRegEx": "Nicholls.", "year": 2003}, {"title": "Grammatical error correction: Machine translation and classifiers", "author": ["Alla Rozovskaya", "Dan Roth."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2205\u20132215, Berlin, Germany.", "citeRegEx": "Rozovskaya and Roth.,? 2016", "shortCiteRegEx": "Rozovskaya and Roth.", "year": 2016}, {"title": "Sentence-level grammatical error identification as sequence-to-sequence correction", "author": ["Allen Schmaltz", "Yoon Kim", "Alexander M. Rush", "Stuart Shieber."], "venue": "Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Ap-", "citeRegEx": "Schmaltz et al\\.,? 2016", "shortCiteRegEx": "Schmaltz et al\\.", "year": 2016}, {"title": "Srilm \u2013 an extensible language modeling toolkit", "author": ["Andreas Stolcke."], "venue": "Proc. Intl. Conf. on Spoken Language Processing, volume 2, pages 901\u2013904, Denver.", "citeRegEx": "Stolcke.,? 2002", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "Tense and aspect error correction for esl learners using global context", "author": ["Toshikazu Tajiri", "Mamoru Komachi", "Yuji Matsumoto."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Pa-", "citeRegEx": "Tajiri et al\\.,? 2012", "shortCiteRegEx": "Tajiri et al\\.", "year": 2012}, {"title": "Neural language correction with character-based attention", "author": ["Ziang Xie", "Anand Avati", "Naveen Arivazhagan", "Dan Jurafsky", "Andrew Y. Ng."], "venue": "CoRR, abs/1603.09727.", "citeRegEx": "Xie et al\\.,? 2016", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Grammatical error correction using neural machine translation", "author": ["Zheng Yuan", "Ted Briscoe."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Yuan and Briscoe.,? 2016", "shortCiteRegEx": "Yuan and Briscoe.", "year": 2016}, {"title": "Since for our controlled data experiments we removed the language model features associated with external data, we did not use the word-class language model feature, so for the sparse features, we used the set of edit oper", "author": ["Durrani"], "venue": null, "citeRegEx": "Durrani,? \\Q2013\\E", "shortCiteRegEx": "Durrani", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "The task is envisioned as a component of a learning tool or writing-assistant, and has seen increased interest since 2011 driven by a series of shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014).", "startOffset": 157, "endOffset": 226}, {"referenceID": 4, "context": "The task is envisioned as a component of a learning tool or writing-assistant, and has seen increased interest since 2011 driven by a series of shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014).", "startOffset": 157, "endOffset": 226}, {"referenceID": 18, "context": "focused on the data provided by the CoNLL-2014 shared task (Ng et al., 2014), a set of corrected essays by second-language learners.", "startOffset": 59, "endOffset": 76}, {"referenceID": 12, "context": "In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific features is the state-of-the-art for the task (Junczys-Dowmunt and Grundkiewicz, 2016), outperforming word-", "startOffset": 168, "endOffset": 208}, {"referenceID": 26, "context": "and character-based sequence-to-sequence models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017), phrase-based systems with neural features (Chollampatt et al.", "startOffset": 48, "endOffset": 107}, {"referenceID": 25, "context": "and character-based sequence-to-sequence models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017), phrase-based systems with neural features (Chollampatt et al.", "startOffset": 48, "endOffset": 107}, {"referenceID": 11, "context": "and character-based sequence-to-sequence models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017), phrase-based systems with neural features (Chollampatt et al.", "startOffset": 48, "endOffset": 107}, {"referenceID": 10, "context": ", 2016b,a), re-ranking output from phrase-based systems (Hoang et al., 2016),", "startOffset": 56, "endOffset": 76}, {"referenceID": 21, "context": "and combining phrase-based systems with classifiers trained for hand-picked subsets of errors (Rozovskaya and Roth, 2016).", "startOffset": 94, "endOffset": 121}, {"referenceID": 15, "context": "We use a standard word-based model (WORD), similar to that of Luong et al. (2015), as well as a model that uses a convolutional neural network (CNN) and a highway network over char-", "startOffset": 62, "endOffset": 82}, {"referenceID": 13, "context": "acters (CHARCNN), based on the work of Kim et al. (2016), instead of word embeddings as the input to the encoder and decoder.", "startOffset": 39, "endOffset": 57}, {"referenceID": 26, "context": "professionally annotated dataset, the most effective sequence-to-sequence approach can significantly outperform a state-of-the-art SMT system without augmenting the sequence-to-sequence model with a secondary model to handle lowfrequency words (Yuan and Briscoe, 2016) or an additional model to improve precision or intersecting a large language model (Xie et al.", "startOffset": 244, "endOffset": 268}, {"referenceID": 25, "context": "professionally annotated dataset, the most effective sequence-to-sequence approach can significantly outperform a state-of-the-art SMT system without augmenting the sequence-to-sequence model with a secondary model to handle lowfrequency words (Yuan and Briscoe, 2016) or an additional model to improve precision or intersecting a large language model (Xie et al., 2016).", "startOffset": 352, "endOffset": 370}, {"referenceID": 11, "context": "We also demonstrate improvements over these previous sequence-to-sequence approaches on the CoNLL-2014 data and competitive results with Ji et al. (2017), despite using significantly less data.", "startOffset": 137, "endOffset": 154}, {"referenceID": 11, "context": "We also demonstrate improvements over these previous sequence-to-sequence approaches on the CoNLL-2014 data and competitive results with Ji et al. (2017), despite using significantly less data. The work of Schmaltz et al. (2016) applies WORD and CHARCNN models to the distinct binary classification task of error identification.", "startOffset": 137, "endOffset": 229}, {"referenceID": 14, "context": "GLEU and M differences on test are statistically significant via paired bootstrap resampling (Koehn, 2004; Graham et al., 2014) at the 0.", "startOffset": 93, "endOffset": 127}, {"referenceID": 9, "context": "GLEU and M differences on test are statistically significant via paired bootstrap resampling (Koehn, 2004; Graham et al., 2014) at the 0.", "startOffset": 93, "endOffset": 127}, {"referenceID": 6, "context": "Data AESW (Daudaravicius, 2016; Daudaravicius et al., 2016) consists of sentences taken from academic articles annotated with corrections by professional editors used for the AESW shared", "startOffset": 10, "endOffset": 59}, {"referenceID": 7, "context": "Data AESW (Daudaravicius, 2016; Daudaravicius et al., 2016) consists of sentences taken from academic articles annotated with corrections by professional editors used for the AESW shared", "startOffset": 10, "endOffset": 59}, {"referenceID": 11, "context": "The primary focus of the present study is conducting controlled experiments on the AESW dataset, but we also investigate results on the CoNLL-2014 shared task data in light of recent neural results (Ji et al., 2017) and to serve as a baseline of comparison against existing sequenceto-sequence approaches (Yuan and Briscoe, 2016; Xie et al.", "startOffset": 198, "endOffset": 215}, {"referenceID": 26, "context": ", 2017) and to serve as a baseline of comparison against existing sequenceto-sequence approaches (Yuan and Briscoe, 2016; Xie et al., 2016).", "startOffset": 97, "endOffset": 139}, {"referenceID": 25, "context": ", 2017) and to serve as a baseline of comparison against existing sequenceto-sequence approaches (Yuan and Briscoe, 2016; Xie et al., 2016).", "startOffset": 97, "endOffset": 139}, {"referenceID": 3, "context": "of public data appearing in past work for training: the National University of Singapore (NUS) Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) and the publicly available Lang-8", "startOffset": 129, "endOffset": 153}, {"referenceID": 24, "context": "data (Tajiri et al., 2012; Mizumoto et al., 2012).", "startOffset": 5, "endOffset": 49}, {"referenceID": 16, "context": "data (Tajiri et al., 2012; Mizumoto et al., 2012).", "startOffset": 5, "endOffset": 49}, {"referenceID": 20, "context": "We do not make use of the non-public Cambridge Learner Corpus (CLC) (Nicholls, 2003), which contains over 1.", "startOffset": 68, "endOffset": 84}, {"referenceID": 17, "context": "Evaluation We follow past work and use the Generalized Language Understanding Evaluation (GLEU) (Napoles et al., 2016) and MaxMatch (M2) metrics (Dahlmeier and Ng, 2012).", "startOffset": 96, "endOffset": 118}, {"referenceID": 2, "context": ", 2016) and MaxMatch (M2) metrics (Dahlmeier and Ng, 2012).", "startOffset": 34, "endOffset": 58}, {"referenceID": 12, "context": "Statistical Machine Translation As a baseline of comparison, we experiment with a phrase-based machine translation approach (SMT) shown to be state-of-the-art for the CoNLL-2014 shared task data in previous work (Junczys-Dowmunt and Grundkiewicz, 2016), which adds task specific features and the M2 metric as a scorer to the Moses statistical machine translation system.", "startOffset": 212, "endOffset": 252}, {"referenceID": 23, "context": "4 million sentences after filtering derived from the Lang-8 language-learning website SRI International provided access to SRILM (Stolcke, 2002) for running Junczys-Dowmunt and Grundkiewicz (2016) We found that including the features and data associated with the large language models of Junczys-Dowmunt and Grundkiewicz (2016), created from Common Crawl text", "startOffset": 129, "endOffset": 144}, {"referenceID": 12, "context": "4 million sentences after filtering derived from the Lang-8 language-learning website SRI International provided access to SRILM (Stolcke, 2002) for running Junczys-Dowmunt and Grundkiewicz (2016) We found that including the features and data associated with the large language models of Junczys-Dowmunt and Grundkiewicz (2016), created from Common Crawl text", "startOffset": 157, "endOffset": 197}, {"referenceID": 12, "context": "4 million sentences after filtering derived from the Lang-8 language-learning website SRI International provided access to SRILM (Stolcke, 2002) for running Junczys-Dowmunt and Grundkiewicz (2016) We found that including the features and data associated with the large language models of Junczys-Dowmunt and Grundkiewicz (2016), created from Common Crawl text", "startOffset": 157, "endOffset": 328}, {"referenceID": 11, "context": "2 M2 of Ji et al. (2017), despite using over 1 million fewer sentence pairs, and exceeds the M2 scores of Xie et al.", "startOffset": 8, "endOffset": 25}, {"referenceID": 11, "context": "2 M2 of Ji et al. (2017), despite using over 1 million fewer sentence pairs, and exceeds the M2 scores of Xie et al. (2016) and Yuan and Briscoe (2016) without the secondary models of those systems.", "startOffset": 8, "endOffset": 124}, {"referenceID": 11, "context": "2 M2 of Ji et al. (2017), despite using over 1 million fewer sentence pairs, and exceeds the M2 scores of Xie et al. (2016) and Yuan and Briscoe (2016) without the secondary models of those systems.", "startOffset": 8, "endOffset": 152}, {"referenceID": 11, "context": "Notably, SMT systems (with LMs) are still more effective than reported sequence-to-sequence results, as in Ji et al. (2017), on CoNLL.", "startOffset": 107, "endOffset": 124}, {"referenceID": 12, "context": "For reference, the reported M results of the carefully optimized SMT system of Junczys-Dowmunt and Grundkiewicz (2016) trained on NUCLE and Lang-8, with parameter vectors averaged over multiple runs, with a Wikipedia LM is 45.", "startOffset": 79, "endOffset": 119}], "year": 2017, "abstractText": "In a controlled experiment of sequence-tosequence approaches for the task of sentence correction, we find that characterbased models are generally more effective than word-based models and models that encode subword information via convolutions, and that modeling the output data as a series of diffs improves effectiveness over standard approaches. Our strongest sequence-to-sequence model improves over our strongest phrase-based statistical machine translation model, with access to the same data, by 6 M2 (0.5 GLEU) points. Additionally, in the data environment of the standard CoNLL-2014 setup, we demonstrate that modeling (and tuning against) diffs yields similar or better M2 scores with simpler models and/or significantly less data than previous sequence-to-sequence approaches.", "creator": "LaTeX with hyperref package"}}}