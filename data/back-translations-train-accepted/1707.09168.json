{"id": "1707.09168", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jul-2017", "title": "Learning to Predict Charges for Criminal Cases with Legal Basis", "abstract": "The charge prediction task is to determine appropriate charges for a given case, which is helpful for legal assistant systems where the user input is fact description. We argue that relevant law articles play an important role in this task, and therefore propose an attention-based neural network method to jointly model the charge prediction task and the relevant article extraction task in a unified framework. The experimental results show that, besides providing legal basis, the relevant articles can also clearly improve the charge prediction results, and our full model can effectively predict appropriate charges for cases with different expression styles.", "histories": [["v1", "Fri, 28 Jul 2017 09:46:29 GMT  (1455kb,D)", "http://arxiv.org/abs/1707.09168v1", "10 pages, accepted by EMNLP 2017"]], "COMMENTS": "10 pages, accepted by EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bingfeng luo", "yansong feng", "jianbo xu", "xiang zhang", "dongyan zhao"], "accepted": true, "id": "1707.09168"}, "pdf": {"name": "1707.09168.pdf", "metadata": {"source": "CRF", "title": "Learning to Predict Charges for Criminal Cases with Legal Basis", "authors": ["Bingfeng Luo", "Yansong Feng", "Jianbo Xu", "Xiang Zhang", "Dongyan Zhao"], "emails": ["zhaody}@pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to put themselves into another world, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able to change the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they are able to put themselves, in which they are able to put themselves."}, {"heading": "2 Related Work", "text": "The prosecution aims at a comprehensive understanding of a case based on the facts of a case. Previous work has considered this task in a multi-level classification framework that takes the actual situation as the input and starting point for an indictment. (Liu and Hsieh, 2006) The question is whether this is a substandard case in which an indictment is being brought. (Liu and Hsieh, 2006) The question is whether this is an indictment. (Lin et al., 2012) It is about a deeper understanding of a case in which an indictment is being brought. (Lin which an indictment is being brought.) It is about an indictment that is not an indictment, but an indictment. (Lin which it is being brought.) It is about an indictment. \"It is about an indictment.\" It is about an indictment. \"It is about an indictment.\" It is about an indictment. \""}, {"heading": "3 Data Preparation", "text": "Our data comes from China Judgments Online1, where the Chinese government has been publishing sentencing documents since 2013. We randomly select 50,000 documents for training, 5,000 for validation, and 5,000 for review. To ensure sufficient training data for each indictment, we classify only the charges that appear more than 80 times in the training data, and treat documents with other charges as negative data. In terms of legal articles, we consider those in the Criminal Code of the People's Republic of China. The resulting data set contains 50 unique indictments, 321 different articles, an average of 383 words per fact description, 3.81 articles per case, and 3.56% cases with more than one batch. An example sentencing document is shown in Figure 1, in which we highlight the indicator clauses we have used to divide a document into three parts."}, {"heading": "4 Our Approach", "text": "As illustrated in Fig. 2, our approach includes the following steps: (1) The description of the input facts is passed to a document encoder to generate the fact embedding of df, with ufw and ufs being global context vectors at the word and sentence level that are used to carefully select informative words and phrases. (2) At the same time, the fact description is also passed on to an article extractor to find the most important k-relevant legal articles. (3) These articles are embedded by another document encoder and passed on to an article aggregator to carefully select supporting articles and produce the aggregated article that embeds there. Specifically, three context vectors, i.e. uaw, uas and uad, are dynamically generated from df to generate attention values within the article encoder and the article aggregator. (4) Finally, df and da are linked to predict the cost distribution for the software max."}, {"heading": "4.1 Document Encoder", "text": "Intuitively, a sentence is a sequence of words, and a document is a sequence of sentences. (The document embedded problem can therefore be embedded in two sequences (Tang et al., 2015; Yang et al., 2016). As shown in Fig. 3, we can first embed each sentence with a sentence-level encoder, and then use the regular expression with a document-level encoder to extract the sequence sequence sequence: \"While these two encoders may have different architectures, we use the same element for simplicity. (Bi-GRU Sequence Encoder A challenge in building a sequence encoder is how to take into account the correlation between different elements. A promising solution here is for simplicity. (Bi-GRU Sequence Encoder) A challenge in building a sequence encoder is how to consider the correlation between different elements."}, {"heading": "4.2 Using Law Articles", "text": "One of the challenges of using legal articles in support of fee prediction lies in the fact that the legal provisions contain a large number of articles that make the application of complex models to these articles directly time consuming and thus difficult to scale up. The multi-label nature of the relevant article extraction also requires a model that can output multiple articles. We therefore adopt a two-step approach, specific, we first set up a fast and easy-to-scale classification system to filter out a large portion of the irrelevant articles, and retain the top-k articles to make a comprehensive understanding of the top-k articles, and then we use the article-side attention module to select the most supportive ones. We treat the relevant article extraction task as multiple binary classifications. Specifically, we build a binary classification for each article that focuses on its relevance, which complies with the input results contained in the 21 classifications."}, {"heading": "4.3 The Output", "text": "To make the final charge prediction, we first link the document in which df and the aggregated article are embedded into two consecutive full connection layers to generate a new vector d \u2032, which is then passed to a Softmax classifier to generate the predicted charge distribution.We use the validation theorem to determine a threshold value, and consider all charges whose probability of ejection is higher as positive predictions. Input into the first full connection layer can also be only df or there, which means that we only use facts or articles to make the prediction.The loss function for the training is cross-entropy: loss = \u2212 N \u2211 i = 1 l = 1 yillog (oil) (5), where N is the number of training data, L is the number of charges, il and oil is the target and the predicted probability of the charge in case iThe target charge is generated by setting positive to 1mi and the number of positive labels."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experimental Setup", "text": "We use HanLP3 for Chinese word segmentation and POS tagging. Word embeddings are trained using word2vec (Mikolov et al., 2013) on judgement documents, websites of several legal forums and Baidu Encyclopedia. The resulting word embeddings contain 573,353 words with 100 dimensions. We randomly initialize a 50 d vector for each POS tag associated with the word embeddedness as final input. Each GRU in the Bi-GRU is size 75, the two full connection layers are size 200 and 150. The relevant article extractor generates top 20 articles, the weight of the article attention loss (\u03b2 in Equation 6) is 0.1, and the prediction wave is 0.4. We use Stochastic Gradient Descent (SGD) for training, with learning rate 0.1, and batch size 8. We compare our complete model with the SVG model with the facts mentioned below, using only two article supervision: the SVDF and the description of the two variations."}, {"heading": "5.2 Charge Prediction Results", "text": "This year it is so far that it only takes a few days to reach an agreement."}, {"heading": "5.3 Article Extraction Results", "text": "We also rate our SVM article extractor, which reaches 77.60%, 88.96%, 94.21% and 96.53% recall with respect to the top 5, 10, 20 and 30 articles respectively. Although simple, the SVM extractor can achieve over 94% recall for the top 20 articles, which is sufficient for further refinement. However, the micro-F1 value of the extractor in the test kit is only 61.08%, which will cause serious error propagation problems if we use the predictive results of the extractor directly. Therefore, we design the article attention mechanism to handle the noise in the top 20 articles. Table 2 shows the revaluation of our article attention module (column 2-3) and the corresponding charge prediction results (column 4) under different weights for the article's attention (\u03b2 in equation 6). Prec @ 1 refers to the top precision, M1 and M1."}, {"heading": "5.4 Performance on News Data", "text": "There are usually significant differences between the terms used by legal practitioners and people with no legal background, so it is important to see how our model behaves when describing facts written by non-legal professionals. We create a news dataset by asking 3 law students to comment on the reasonable fees for 100 social news reports on criminal cases from two news websites5, with an average of 262 words and 25 different charges. Results are presented in Table 3, where we report only microstatistics due to the relatively small size of the dataset compared to the number of different charms. We can see that SVM actually suffers a significant decline in news data in order to reach agreement on conflicting annotations. Results are shown in Table 3, where we report microstatistics due to the relatively small size of the dataset compared to the number of different charms."}, {"heading": "6 Conclusion", "text": "In this paper, we propose an attention-based neural network that can jointly model the task of charge prediction and the task of relevant article extraction, with the weighted relevant articles serving as the legal basis for charge prediction. Experimental results from criminal trial judgments in China show the effectiveness of our model in both charge prediction and relevant article extraction. Comparing different variants of our model also shows the importance of legal articles in decision-making in the civil justice system. By experimenting with intelligence data, we show that our model, while trained in judgment documents, also has adequate generalization capability in fact descriptions written by non-legal professionals. Although our model is promising, it still cannot explicitly deal with cases of defendants, and there is also a clear gap between our model and improving the ceiling that relevant articles can reach."}, {"heading": "Acknowledgement", "text": "This work is supported by the National High Technology R & D Program of China (2015AA015403); the National Natural Science Foundation of China (61672057, 61672058); KLSTSPI Key Lab. of Intelligent Press Media Technology."}], "references": [{"title": "Predicting judicial decisions of the european court of human rights: A natural language processing perspective", "author": ["Nikolaos Aletras", "Dimitrios Tsarapatsanis", "Daniel Preo\u0163iuc-Pietro", "Vasileios Lampos."], "venue": "PeerJ Computer Science, 2:e93.", "citeRegEx": "Aletras et al\\.,? 2016", "shortCiteRegEx": "Aletras et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Lexicalmorphological modeling for legal text analysis", "author": ["Danilo S Carvalho", "Minh-Tien Nguyen", "Tran Xuan Chien", "Minh Le Nguyen."], "venue": "arXiv preprint arXiv:1609.00799.", "citeRegEx": "Carvalho et al\\.,? 2016", "shortCiteRegEx": "Carvalho et al\\.", "year": 2016}, {"title": "A text mining approach to assist the general public in the retrieval of legal documents", "author": ["Yen-Liang Chen", "Yi-Hung Liu", "Wu-Liang Ho."], "venue": "Journal of the American Society for Information Science and Technology, 64(2):280\u2013290.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Schwenk", "Yoshua Bengio."], "venue": "Proceedings of EMNLP, pages 1724\u20131734.", "citeRegEx": "Schwenk and Bengio.,? 2014", "shortCiteRegEx": "Schwenk and Bengio.", "year": 2014}, {"title": "Learning to classify text using support vector machines: Methods, theory and algorithms", "author": ["Thorsten Joachims."], "venue": "Kluwer Academic Publishers.", "citeRegEx": "Joachims.,? 2002", "shortCiteRegEx": "Joachims.", "year": 2002}, {"title": "A general approach for predicting the behavior of the supreme court of the united states", "author": ["Daniel Martin Katz", "II Bommarito", "J Michael", "Josh Blackman."], "venue": "arXiv preprint arXiv:1612.03473.", "citeRegEx": "Katz et al\\.,? 2016", "shortCiteRegEx": "Katz et al\\.", "year": 2016}, {"title": "COLIEE-14", "author": ["Mi-Young Kim", "Randy Goebe", "Ken Satoh."], "venue": "http://webdocs. cs.ualberta.ca/ \u0303miyoung2/jurisin_ task/index.html.", "citeRegEx": "Kim et al\\.,? 2014a", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Legal question answering using ranking svm and syntactic/semantic similarity", "author": ["Mi-Young Kim", "Ying Xu", "Randy Goebel."], "venue": "JSAI International Symposium on Artificial Intelligence, pages 244\u2013 258. Springer.", "citeRegEx": "Kim et al\\.,? 2014b", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings EMNLP, pages 1746\u20131751.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Improved neural network-based multi-label classification with better initialization leveraging label cooccurrence", "author": ["Gakuto Kurata", "Bing Xiang", "Bowen Zhou."], "venue": "Proceedings of NAACL-HLT, pages 521\u2013526.", "citeRegEx": "Kurata et al\\.,? 2016", "shortCiteRegEx": "Kurata et al\\.", "year": 2016}, {"title": "Exploiting machine learning models for chinese legal documents labeling, case classification, and sentencing prediction", "author": ["Wan-Chen Lin", "Tsung-Ting Kuo", "Tung-Jia Chang."], "venue": "ROCLING XXIV (2012), page 140.", "citeRegEx": "Lin et al\\.,? 2012", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "Case instance generation and refinement for case-based criminal summary judgments in chinese", "author": ["Chao-Lin Liu", "Cheng-Tsung Chang", "Jim-How Ho."], "venue": "Journal of Information Science and Engineering, 20(4):783\u2013800.", "citeRegEx": "Liu et al\\.,? 2004", "shortCiteRegEx": "Liu et al\\.", "year": 2004}, {"title": "Exploring phrase-based classification of judicial documents for criminal charges in chinese", "author": ["Chao-Lin Liu", "Chwen-Dar Hsieh."], "venue": "International Symposium on Methodologies for Intelligent Systems, pages 681\u2013690. Springer.", "citeRegEx": "Liu and Hsieh.,? 2006", "shortCiteRegEx": "Liu and Hsieh.", "year": 2006}, {"title": "Classifying criminal charges in chinese for web-based legal services", "author": ["Chao-Lin Liu", "Ting-Ming Liao."], "venue": "Asia-Pacific Web Conference, pages 64\u201375. Springer.", "citeRegEx": "Liu and Liao.,? 2005", "shortCiteRegEx": "Liu and Liao.", "year": 2005}, {"title": "Predicting associated statutes for legal problems", "author": ["Yi-Hung Liu", "Yen-Liang Chen", "Wu-Liang Ho."], "venue": "Information Processing & Management, 51(1):194\u2013211.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Proceedings of NIPS, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Largescale multi-label text classification\u2014revisiting neural networks", "author": ["Jinseok Nam", "Jungi Kim", "Eneldo Loza Menc\u0131\u0301a", "Iryna Gurevych", "Johannes F\u00fcrnkranz"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery", "citeRegEx": "Nam et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nam et al\\.", "year": 2014}, {"title": "Analyzing the extraction of relevant legal judgments using paragraph-level and citation information", "author": ["K Raghav", "P Krishna Reddy", "V Balakista Reddy."], "venue": "AI4J\u2013Artificial Intelligence for Justice, page 30.", "citeRegEx": "Raghav et al\\.,? 2016", "shortCiteRegEx": "Raghav et al\\.", "year": 2016}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1422\u20131432.", "citeRegEx": "Tang et al\\.,? 2015", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Matching networks for one shot learning", "author": ["Oriol Vinyals", "Charles Blundell", "Tim Lillicrap", "Daan Wierstra"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2016}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Sida Wang", "Christopher D Manning."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 90\u201394. As-", "citeRegEx": "Wang and Manning.,? 2012", "shortCiteRegEx": "Wang and Manning.", "year": 2012}, {"title": "Hierarchical attention networks for document classification", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "Existing attempts formulate the task of automatic charge prediction as a single-label classification problem, by either adopting a k-Nearest Neighbor (KNN) (Liu et al., 2004; Liu and Hsieh, 2006) as the classifier with shallow textual features, or manually designing key factors for specific charges to help text understanding (Lin et al.", "startOffset": 156, "endOffset": 195}, {"referenceID": 13, "context": "Existing attempts formulate the task of automatic charge prediction as a single-label classification problem, by either adopting a k-Nearest Neighbor (KNN) (Liu et al., 2004; Liu and Hsieh, 2006) as the classifier with shallow textual features, or manually designing key factors for specific charges to help text understanding (Lin et al.", "startOffset": 156, "endOffset": 195}, {"referenceID": 11, "context": ", 2004; Liu and Hsieh, 2006) as the classifier with shallow textual features, or manually designing key factors for specific charges to help text understanding (Lin et al., 2012), which make those works hard to scale to more types of charges.", "startOffset": 160, "endOffset": 178}, {"referenceID": 14, "context": "A simple solution is to convert this multi-label problem into a multiclass classification task by only considering a fixed set of article combinations (Liu and Liao, 2005; Liu and Hsieh, 2006), which can only be applied to a small set of articles and does not fit to real applications.", "startOffset": 151, "endOffset": 192}, {"referenceID": 13, "context": "A simple solution is to convert this multi-label problem into a multiclass classification task by only considering a fixed set of article combinations (Liu and Liao, 2005; Liu and Hsieh, 2006), which can only be applied to a small set of articles and does not fit to real applications.", "startOffset": 151, "endOffset": 192}, {"referenceID": 1, "context": "GRU) (Bahdanau et al., 2015) with a stack of factside attention components to model the correlations among words and sentences, in order to capture the whole story as well as important details of the case.", "startOffset": 5, "endOffset": 28}, {"referenceID": 12, "context": "(Liu et al., 2004; Liu and Hsieh, 2006) use KNN to classify 12 and 6 criminal charges in Taiwan.", "startOffset": 0, "endOffset": 39}, {"referenceID": 13, "context": "(Liu et al., 2004; Liu and Hsieh, 2006) use KNN to classify 12 and 6 criminal charges in Taiwan.", "startOffset": 0, "endOffset": 39}, {"referenceID": 11, "context": "(Lin et al., 2012) propose to make deeper understanding of a case by identifying charge-specific factors that are manually designed for 2 charges.", "startOffset": 0, "endOffset": 18}, {"referenceID": 15, "context": "(Liu et al., 2015) instead design a scalable two-step approach by first using Support Vector Machine (SVM) for preliminary article classification, and then re-ranking the results using word level features and co-occurence tendency among articles.", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "The target can be which party will the outcome side with (Aletras et al., 2016), or whether the present court will affirm or reverse the decision of a lower court (Katz et al.", "startOffset": 57, "endOffset": 79}, {"referenceID": 7, "context": "We also share similar spirit with the legal question answering task (Kim et al., 2014a), which aims at answering the yes/no questions in the Japanese legal bar exams, that we all believe that relevant law articles are important for decisions in", "startOffset": 68, "endOffset": 87}, {"referenceID": 8, "context": "latter phase is considered as a textual entailment task (Kim et al., 2014b; Carvalho et al., 2016).", "startOffset": 56, "endOffset": 98}, {"referenceID": 2, "context": "latter phase is considered as a textual entailment task (Kim et al., 2014b; Carvalho et al., 2016).", "startOffset": 56, "endOffset": 98}, {"referenceID": 9, "context": "Recently, various neural network (NN) architectures such as Convolutional Neural Network (CNN) (Kim, 2014) and Recurrent Neural Network (RNN) have been used for document embedding, which is further used for classification.", "startOffset": 95, "endOffset": 106}, {"referenceID": 19, "context": "(Tang et al., 2015) propose a two-layer scheme, RNN or CNN for sentence embedding, and another RNN for document embedding.", "startOffset": 0, "endOffset": 19}, {"referenceID": 22, "context": "(Yang et al., 2016) further use global context vectors to attentively distinguish informative words or sentences from non-informative ones during embed-", "startOffset": 0, "endOffset": 19}, {"referenceID": 17, "context": "difference is the multi-label nature of our task, where, rather than optimizing as multiple binary classification tasks (Nam et al., 2014), we convert the multi-label target to label distribution during training with cross entropy as loss function (Ku-", "startOffset": 120, "endOffset": 138}, {"referenceID": 19, "context": "The document embedding problem, therefore, can be converted to two sequence embedding problems (Tang et al., 2015; Yang et al., 2016).", "startOffset": 95, "endOffset": 133}, {"referenceID": 22, "context": "The document embedding problem, therefore, can be converted to two sequence embedding problems (Tang et al., 2015; Yang et al., 2016).", "startOffset": 95, "endOffset": 133}, {"referenceID": 1, "context": "A promising solution is Bi-directional Gated Recurrent Units (Bi-GRU) (Bahdanau et al., 2015), which encodes the context of each element by using a gating mechanism to track the state of sequence.", "startOffset": 70, "endOffset": 93}, {"referenceID": 22, "context": "Inspired by (Yang et al., 2016), we use a context vector to attentively aggregate the elements, but instead of using a global context vector, we allow the context vector to be dynamically generated when extra guidance is", "startOffset": 12, "endOffset": 31}, {"referenceID": 15, "context": "Similar to the preliminary classification phase of (Liu et al., 2015), we also use word-based SVM as our binary classifier, which is fast and performs well in text classification (Joachims, 2002; Wang and Manning, 2012).", "startOffset": 51, "endOffset": 69}, {"referenceID": 5, "context": ", 2015), we also use word-based SVM as our binary classifier, which is fast and performs well in text classification (Joachims, 2002; Wang and Manning, 2012).", "startOffset": 117, "endOffset": 157}, {"referenceID": 21, "context": ", 2015), we also use word-based SVM as our binary classifier, which is fast and performs well in text classification (Joachims, 2002; Wang and Manning, 2012).", "startOffset": 117, "endOffset": 157}, {"referenceID": 20, "context": "Although the order of the top k extracted articles is not fully reliable, (Vinyals et al., 2016) suggests that it is still beneficial to use a bi-directional RNN to embed the context of each element even in a set, where the order does not exist.", "startOffset": 74, "endOffset": 96}, {"referenceID": 16, "context": "Word embeddings are trained using word2vec (Mikolov et al., 2013) on judgement documents, web pages from several legal forums and Baidu Encyclopedia.", "startOffset": 43, "endOffset": 65}, {"referenceID": 22, "context": "The latter one is similar to the state-of-art document classification model (Yang et al., 2016), but adapted to the multi-", "startOffset": 76, "endOffset": 95}, {"referenceID": 15, "context": "We also implement an SVM model, which is effective and scales well in many fact-description-related tasks in the field of artificial intelligence and law (Liu et al., 2015; Aletras et al., 2016).", "startOffset": 154, "endOffset": 194}, {"referenceID": 0, "context": "We also implement an SVM model, which is effective and scales well in many fact-description-related tasks in the field of artificial intelligence and law (Liu et al., 2015; Aletras et al., 2016).", "startOffset": 154, "endOffset": 194}], "year": 2017, "abstractText": "The charge prediction task is to determine appropriate charges for a given case, which is helpful for legal assistant systems where the user input is fact description. We argue that relevant law articles play an important role in this task, and therefore propose an attention-based neural network method to jointly model the charge prediction task and the relevant article extraction task in a unified framework. The experimental results show that, besides providing legal basis, the relevant articles can also clearly improve the charge prediction results, and our full model can effectively predict appropriate charges for cases with different expression styles.", "creator": "LaTeX with hyperref package"}}}