{"id": "1707.09468", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jul-2017", "title": "Zero-Shot Activity Recognition with Verb Attribute Induction", "abstract": "In this paper, we investigate large-scale zero-shot activity recognition by modeling the visual and linguistic attributes of action verbs. For example, the verb \"salute\" has several properties, such as being a light movement, a social act, and short in duration. We use these attributes as the internal mapping between visual and textual representations to reason about a previously unseen action. In contrast to much prior work that assumes access to gold standard attributes for zero-shot classes and focuses primarily on object attributes, our model uniquely learns to infer action attributes from dictionary definitions and distributed word representations. Experimental results confirm that action attributes inferred from language can provide a predictive signal for zero-shot prediction of previously unseen activities.", "histories": [["v1", "Sat, 29 Jul 2017 06:05:52 GMT  (931kb)", "http://arxiv.org/abs/1707.09468v1", "accepted to EMNLP 2017"], ["v2", "Sat, 2 Sep 2017 19:53:20 GMT  (933kb)", "http://arxiv.org/abs/1707.09468v2", "accepted to EMNLP 2017"]], "COMMENTS": "accepted to EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["rowan zellers", "yejin choi"], "accepted": true, "id": "1707.09468"}, "pdf": {"name": "1707.09468.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["rowanz@cs.washington.edu", "yejin@cs.washington.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 7.09 468v 1 [cs.C L] 29 July 2 017Zero-shot activity detection by modelling the visual and linguistic attributes of action verbs. For example, the verb \"salut\" has several properties, such as light movement, a social act and short duration. We use these attributes as an internal representation between visual and textual representations to puzzle over a previously invisible action. Unlike many previous work, which required access to gold standard attributes for zero-shot classes and focused mainly on object properties, our model uniquely learns to derive action attributes from dictionary definitions and distributed word representations. Experimental results confirm that action attributes derived from language can provide a predictive signal for zero-shot prediction of hitherto invisible activities."}, {"heading": "1 Introduction", "text": "We are investigating the problem of deriving action verb attributes from the way the word is defined and used in context. For example, given a verb like \"swig,\" shown in Figure 1, we want to infer various characteristics of actions such as movement dynamics (moderate movement), social dynamics (solitary act), involved body parts (face, arms, hands), and duration (less than 1 minute) that generally apply to the range of actions that can be designated by the verb \"twist.\" Our ultimate goal is to improve zero shot learning of activities in computer vision: predicting an activity that has not been seen before by integrating background knowledge about the conceptual properties of actions. For example, a computer vision system might have seen images of \"drinking activities,\" but not \"swig.\" Ideally, the system should infer the probable visual properties of \"twist-and-out\" activities in computer vision."}, {"heading": "2 Action Verb Attributes", "text": "In fact, most of them will be able to move to another world in which they will be able to integrate."}, {"heading": "3 Learning Verb Attributes from Language", "text": "In this section, we present our models for learning verb attributes from the language. We look at two complementary types of language-based input: dictionary definitions and word embeddings. The approach based on dictionary definitions is similar to how people acquire the meaning of a new word from a dictionary lookup, while the approach to word embeddings is similar to how people acquire the meaning of words in context. This task follows the standard supervised learning approach, where the goal is to predict K attributes per word in the vocabulary V. Let xv, X represent the input of a word v, V could mark the meaning of words as embeddings, or a definition of dictionary letters (modeled as a list of tokens). Our goal is to produce a model f: X \u2192 Rd that allows input to represent the dimension."}, {"heading": "3.1 Learning from Distributed Embeddings", "text": "Intuitively, we expect similar verbs to have similar distributions of nearby nouns and adverbs, which can greatly help us in zero-shot prediction. In our experiments, we use 300-dimensional GloVe vectors trained on 840B tokens of web data (Pennington et al., 2014). We use logistical regression to predict each attribute, as we have found that additional hidden layers did not improve performance. Therefore, we leave f emb (xv) = wv, the GloVe embedding of v, and use Equation 1 to obtain distribution via labels.We are also experimenting with retrofitted embedding, in which embedding is mapped according to a lexical resource. Following the approach of Faruqui et al. (2015), we are equipping embedding with WordNet (Miller, 1995), Paraphse DB (199b-vitrach) and Gankeal (2013)."}, {"heading": "3.2 Learning from Dictionary Definitions", "text": "We also propose a model that learns the attribute-based meaning of verbs through dictionary definitions, similar to the task of using a dictionary to predict word embeddings (Hill et al., 2016).BGRU encoder Our first model includes a bi-directional gated recurrent unit (BGRU) encoder (?).Let xv, 1: T be a definition of verb v, with T characters. To encode the input, we run it through the GRU equation: ~ ht = GRU (xv, t, ~ ht \u2212 1). (2) Let ~ h1 output a GRU to the inverse input xv, T: 1. Then the BGRU encoder is the concatenation f bgru = ~ hT-of-words encoder."}, {"heading": "3.3 Combining Dictionary and Embedding Representations", "text": "We assume that the two modalities of the dictionary and the distribution embedding are complementary. Therefore, we propose an early merger (concatenation) of both categories. Figure 2 describes the embedding model of GRU + - in other words: fBGRU + emb = fBGRU-emb. This can also be done with arbitrary definition encoders and word embedding."}, {"heading": "4 Zero-Shot Activity Recognition", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Verb Attributes as Latent Mappings", "text": "Given the learned attributes for a collection of activities, we would like to evaluate their performance in describing these activities from the real world images4For the (Neural) bag of words models, we also tried concatenating the definition together per verb and then doing the encoding. However, we found that this resulted in worse outcomes. In a zero-shot setting. So we look at several models that classify the label of an image by pivoting through an attribute representation. Overview A formal description of the task is as follows: Let the space of labels V, split into Vtrain and Vtest. Let zv-Z represent an image with label v-V; our goal is to correctly predict this label under verbs v-Vtest at test dates, although we do not see images with labels in Vtest during the training. Generalization is done by a lookup table A, with known attributes for each v-V label. Formally, for each attribute."}, {"heading": "4.1.1 Connection to Other Attribute Models", "text": "Our model is similar to? and Romera-Paredes and Torr (2015) in that we predict the attributes indirectly and train the model by the class names.5 It differs from several other zero-shot models, such as the Direct Attribute Prediction (DAP) model by Lampert et al. (2014) in that DAP is trained by maximizing the probability of predicting each attribute and multiplying the probabilities at test points."}, {"heading": "4.2 Verb Embeddings as Latent Mappings", "text": "Frome et al. (2013) build DeVISE, a model for zero-shot learning, on ImageNet object recognition, where the goal of the image model is to directly predict the word embedding of a class. DeVISE is trained by minimizing zv max {0,.1 + (wv \u2032 \u2212 wv) W embg (zv)} for each image. We compare a version of this model with fixed GloVe embedding. Additionally, we use a variant of our model only using word embedding. The equation is the same as Equation 4, except for the use of the matrix Aemb as a matrix of word embedding: i.e. for each label v that we look at, we have Aembv = wv."}, {"heading": "4.3 Joint Prediction from Attributes and Embeddings", "text": "To combine the representational power of the attribute and the embedding of models, we form an ensemble that combines the two models by adding the logits before applying the Softmax: softmax v \u2032 (\u2211 kA (k) W (k) g (zv) + A emb W embg (zv))) A diagram is shown in Figure 3. We find that this model can easily overmatch during optimization, presumably by over-adjusting the embedding and attribute components. To solve this, we train the model to minimize the cross entropy of three sources independently: the attributes alone, the embedding and the sum, each weighted equally. Including predicted and gold attributes, we also experiment with an ensemble of our model and combine predictive and gold attributes of Vtest. This allows the model to hedge against cases in which a verb may have multiple possible responses."}, {"heading": "5 Actions and Attributes Dataset", "text": "To evaluate our hypotheses on action attributes and zero-shot learning, we constructed a dataset using crowdsourcing experiments.The dataset Actions and Attributes consists of annotations on 1710 verb templates, each consisting of a verb and an optional particle (e.g. \"put\" or \"put up\").We selected all verbs from the imSitu corpus, which consists of images representing verbs from many categories (Yatskar et al., 2016), then added the MPII video description and ScriptBase dataset (Rohrbach et al., 2015; Gorinski and Lapata, 2015).We used the spaCy Dependency Parser (Honnibal and Johnson, 2015) to extract the verb template for each sentence, and collected annotations on mechanical verbs to filter out non-literary and abstract verbs."}, {"heading": "6 Experimental Setup", "text": "For the BGRU model, we use an internal dimension of 300 and embed the words in a size of 300. Vocabulary size is set at 30,000 (including all verbs for which we have definitions). During the training, we keep the same architecture, except for another 300-dimensional last layer is used to predict GloVe embedding. (Following Hill et al. (2016), we use a ranking loss. Leave w = Wembf (x) the predicted word embedding for each definition x of a word in the dictionary (not necessarily a verb)."}, {"heading": "7 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Predicting Action Attributes from Text", "text": "Our results on the Text-to-Attributes Challenge confirm that it is a challenging task for two reasons. First, there is noise associated with the attributes: Many verb attributes are difficult to comment on because verb meanings can change in context. 7 Second, there is a lack of training data that is inherent to the problem: There are not many common verbs in English, and it can be difficult to find annotations for rare ones. Third, any system needs to compete with strong frequency-based baselines because attributes are generally sparse. In addition, we suspect that more attributes have been collected (to cover obscure patterns), would only increase the spareness of attributes. Despite this, we report strong baseline results based on this issue, especially with our embedding models. The performance gap between embedding models and only definition models may possibly be explained by the fact."}, {"heading": "7.2 Zero-shot Action Recognition", "text": "Although our attribute models do not exceed our embedding models and DeVISE alone, we find that our common attribute and our embedding model perform best overall, achieving 18.10% in top-1 and 41.46% in top-5 precision when using annotations on gold attributes for zero-shot verbs. This result is perhaps surprising given the small number of attributes overall (K = 24), most of which tend to be sparse (as shown in baseline performance in Table 1), so we suspect that collecting more activity attributes would further improve performance. We also note the success of performing zero-shot attributes with predicted attributes."}, {"heading": "8 Related Work", "text": "The CAAP model by Al-Halah et al. (2016) also predicts the object attributes of concrete nouns for use in zero-shot learning. In contrast, we predict verb attributes. A related task is to improve word embedding using multimodal data and linguistic resources (Faruqui et al., 2015; Silberer et al., 2013; Vendrov et al., 2016). Our work is orthogonal to this, as we focus on word embedding as a tool for zero-shot activity detection. Zero-shot learning with objects Although clear, our work is related to zero-shot learning of objects in computer vision. There are several datasets (??) and models designed for this task."}, {"heading": "9 Future Work and Conclusion", "text": "First, more attributes could be collected and evaluated, possibly including other linguistic theories of verbs. Second, future work could go beyond the use of attributes as a linchpin between speech and visual domains. Since our experiments show that unattended word embedding significantly improves performance, it may be desirable to learn data-driven attributes directly from a large corpus or dictionary definitions. Third, future research on action attributes should ideally include videos to better capture attributes that require temporal signals. Overall, however, our work represents a strong early step toward zerRo-shot activity detection, a relatively less studied task that presents several unique challenges to zero-shot object detection. We are introducing new action attributes motivated by linguistic theories and demonstrating their empirical use for reflections on hitherto invisible activities."}, {"heading": "A Supplemental", "text": "In fact, most of them will be able to move to another world, in which they can move to another world."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org", "author": ["houcke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "houcke et al\\.,? \\Q2015\\E", "shortCiteRegEx": "houcke et al\\.", "year": 2015}, {"title": "Recovering the missing link: Predicting class-attribute associations for unsupervised zero-shot learning", "author": ["Ziad Al-Halah", "Makarand Tapaswi", "Rainer Stiefelhagen."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni-", "citeRegEx": "Al.Halah et al\\.,? 2016", "shortCiteRegEx": "Al.Halah et al\\.", "year": 2016}, {"title": "The Berkeley FrameNet project", "author": ["Collin F. Baker", "Charles J. Fillmore", "John B. Lowe."], "venue": "COLING-ACL \u201998: Proceedings of the Conference, pages 86\u201390, Montreal, Canada.", "citeRegEx": "Baker et al\\.,? 1998a", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "The berkeley framenet project", "author": ["Collin F Baker", "Charles J Fillmore", "John B Lowe."], "venue": "Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics-", "citeRegEx": "Baker et al\\.,? 1998b", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Verbs: Aspect and causal structure", "author": ["William Croft."], "venue": "OUP Oxford. Pg. 16.", "citeRegEx": "Croft.,? 2012", "shortCiteRegEx": "Croft.", "year": 2012}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. FeiFei"], "venue": null, "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Retrofitting Word Vectors to Semantic Lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Automatic prediction of aspectual class of verbs in context", "author": ["Annemarie Friedrich", "Alexis Palmer."], "venue": "ACL (2), pages 517\u2013523.", "citeRegEx": "Friedrich and Palmer.,? 2014", "shortCiteRegEx": "Friedrich and Palmer.", "year": 2014}, {"title": "DeViSE: A Deep Visual-Semantic Embedding Model", "author": ["Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Tomas Mikolov"], "venue": null, "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Ppdb: The paraphrase database", "author": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch."], "venue": "HLT-NAACL, pages 758\u2013764.", "citeRegEx": "Ganitkevitch et al\\.,? 2013", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Movie script summarization as graph-based scene extraction", "author": ["Philip John Gorinski", "Mirella Lapata."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Gorinski and Lapata.,? 2015", "shortCiteRegEx": "Gorinski and Lapata.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013 778.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Learning to Understand Phrases by Embedding the Dictionary", "author": ["Felix Hill", "KyungHyun Cho", "Anna Korhonen", "Yoshua Bengio."], "venue": "Transactions of the Association for Computational Linguistics, 4(0):17\u201330.", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "An improved non-monotonic transition system for dependency parsing", "author": ["Matthew Honnibal", "Mark Johnson."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1373\u20131378, Lisbon, Portugal. Association for", "citeRegEx": "Honnibal and Johnson.,? 2015", "shortCiteRegEx": "Honnibal and Johnson.", "year": 2015}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan L BoydGraber", "Hal Daum\u00e9 III."], "venue": "ACL (1), pages 1681\u20131691.", "citeRegEx": "Iyyer et al\\.,? 2015", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Attribute-based classification for zero-shot visual object categorization", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(3):453\u2013465.", "citeRegEx": "Lampert et al\\.,? 2014", "shortCiteRegEx": "Lampert et al\\.", "year": 2014}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["Christoph H Lampert", "Hannes Nickisch", "Stefan Harmeling."], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 951\u2013958. IEEE.", "citeRegEx": "Lampert et al\\.,? 2009", "shortCiteRegEx": "Lampert et al\\.", "year": 2009}, {"title": "English verb classes and alternations : a preliminary investigation", "author": ["Beth Levin"], "venue": null, "citeRegEx": "Levin.,? \\Q1993\\E", "shortCiteRegEx": "Levin.", "year": 1993}, {"title": "Recognizing human actions by attributes", "author": ["Jingen Liu", "Benjamin Kuipers", "Silvio Savarese."], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 3337\u20133344. IEEE.", "citeRegEx": "Liu et al\\.,? 2011", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Scikit-learn: Machine learning", "author": ["E. Duchesnay"], "venue": null, "citeRegEx": "Duchesnay.,? \\Q2011\\E", "shortCiteRegEx": "Duchesnay.", "year": 2011}, {"title": "An embarrassingly simple approach to zero-shot learning", "author": ["Bernardino Romera-Paredes", "Philip HS Torr."], "venue": "ICML, pages 2152\u20132161.", "citeRegEx": "Romera.Paredes and Torr.,? 2015", "shortCiteRegEx": "Romera.Paredes and Torr.", "year": 2015}, {"title": "How well do distributional models capture different types of semantic knowledge", "author": ["Dana Rubinstein", "Effi Levi", "Roy Schwartz", "Ari Rappoport"], "venue": "In Proceedings of the 53nd Annual Meeting of the Association", "citeRegEx": "Rubinstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rubinstein et al\\.", "year": 2015}, {"title": "Learning methods to combine linguistic indicators: Improving aspectual classification and revealing linguistic insights", "author": ["Eric V Siegel", "Kathleen R McKeown."], "venue": "Computational Linguistics, 26(4):595\u2013628.", "citeRegEx": "Siegel and McKeown.,? 2000", "shortCiteRegEx": "Siegel and McKeown.", "year": 2000}, {"title": "Models of semantic representation with visual attributes", "author": ["Vittorio Ferrari", "Mirella Lapata."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Ferrari and Lapata.,? 2013", "shortCiteRegEx": "Ferrari and Lapata.", "year": 2013}, {"title": "Grounded models of semantic representation", "author": ["Carina Silberer", "Mirella Lapata."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Silberer and Lapata.,? 2012", "shortCiteRegEx": "Silberer and Lapata.", "year": 2012}, {"title": "Human activity recognition with metric learning", "author": ["Du Tran", "Alexander Sorokin."], "venue": "Computer Vision\u2013 ECCV 2008, pages 548\u2013561.", "citeRegEx": "Tran and Sorokin.,? 2008", "shortCiteRegEx": "Tran and Sorokin.", "year": 2008}, {"title": "Verbs and Times", "author": ["Zeno Vendler."], "venue": "The Philosophical Review, 66(2):143\u2013160.", "citeRegEx": "Vendler.,? 1957", "shortCiteRegEx": "Vendler.", "year": 1957}, {"title": "Order-embeddings of images and language", "author": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun."], "venue": "ICLR.", "citeRegEx": "Vendrov et al\\.,? 2016", "shortCiteRegEx": "Vendrov et al\\.", "year": 2016}, {"title": "Transductive zero-shot action recognition by word-vector embedding", "author": ["Xun Xu", "Timothy Hospedales", "Shaogang Gong."], "venue": "International Journal of Computer Vision, pages 1\u201325.", "citeRegEx": "Xu et al\\.,? 2017", "shortCiteRegEx": "Xu et al\\.", "year": 2017}, {"title": "Situation recognition: Visual semantic role labeling for image understanding", "author": ["M. Yatskar", "L. Zettlemoyer", "A. Farhadi."], "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5534\u20135542.", "citeRegEx": "Yatskar et al\\.,? 2016", "shortCiteRegEx": "Yatskar et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": ", aspectual verb classes of Vendler (1957)) and also drawing inspiration from studies on linguistic categorization of verbs and their properties (Friedrich and Palmer, 2014; Siegel and McKeown, 2000).", "startOffset": 145, "endOffset": 199}, {"referenceID": 24, "context": ", aspectual verb classes of Vendler (1957)) and also drawing inspiration from studies on linguistic categorization of verbs and their properties (Friedrich and Palmer, 2014; Siegel and McKeown, 2000).", "startOffset": 145, "endOffset": 199}, {"referenceID": 26, "context": ", aspectual verb classes of Vendler (1957)) and also drawing inspiration from studies on linguistic categorization of verbs and their properties (Friedrich and Palmer, 2014; Siegel and McKeown, 2000).", "startOffset": 28, "endOffset": 43}, {"referenceID": 31, "context": "In sum, we present the first study aiming to recover general action attributes for a diverse collection of verbs, and probe their predictive power for zero-shot activity recognition on the recently introduced imSitu dataset (Yatskar et al., 2016).", "startOffset": 224, "endOffset": 246}, {"referenceID": 28, "context": "[1] Aspectual Classes We include the aspectual verb classes of Vendler (1957):", "startOffset": 63, "endOffset": 78}, {"referenceID": 19, "context": "The study of action verb attributes is also closely related to formal studies on verb categorization based on the characteristics of the actions or states that a verb typically associates with (Levin, 1993), and cognitive linguistics literature that focus on causal structure and force dynamics of verb meanings (Croft, 2012).", "startOffset": 193, "endOffset": 206}, {"referenceID": 4, "context": "The study of action verb attributes is also closely related to formal studies on verb categorization based on the characteristics of the actions or states that a verb typically associates with (Levin, 1993), and cognitive linguistics literature that focus on causal structure and force dynamics of verb meanings (Croft, 2012).", "startOffset": 312, "endOffset": 325}, {"referenceID": 2, "context": ", relates to the original Frame theories of Baker et al. (1998a). The study of action verb attributes is also closely related to formal studies on verb categorization based on the characteristics of the actions or states that a verb typically associates with (Levin, 1993), and cognitive linguistics literature that focus on causal structure and force dynamics of verb meanings (Croft, 2012).", "startOffset": 44, "endOffset": 65}, {"referenceID": 17, "context": "They apply it on the Animals with Attributes dataset, which contains 50 animal classes, each described by 85 attributes (Lampert et al., 2014).", "startOffset": 120, "endOffset": 142}, {"referenceID": 1, "context": "Al-Halah et al. (2016) build the \u201cClassAttribute Association Prediction\u201d model (CAAP) that classifies the attributes of an object class from its name.", "startOffset": 0, "endOffset": 23}, {"referenceID": 9, "context": "(2015), we retrofit embeddings using WordNet (Miller, 1995), Paraphrase-DB (Ganitkevitch et al., 2013), and FrameNet (Baker et al.", "startOffset": 75, "endOffset": 102}, {"referenceID": 3, "context": ", 2013), and FrameNet (Baker et al., 1998b).", "startOffset": 22, "endOffset": 43}, {"referenceID": 4, "context": "Following the approach of Faruqui et al. (2015), we retrofit embeddings using WordNet (Miller, 1995), Paraphrase-DB (Ganitkevitch et al.", "startOffset": 26, "endOffset": 48}, {"referenceID": 12, "context": "This is similar in spirit to the task of using a dictionary to predict word embeddings (Hill et al., 2016).", "startOffset": 87, "endOffset": 106}, {"referenceID": 15, "context": "Additionally, we try out a Neural Bag-of-Words model where the GloVe embeddings in a definition are averaged (Iyyer et al., 2015).", "startOffset": 109, "endOffset": 129}, {"referenceID": 11, "context": "Our image encoder is a CNN with the ResNet-152 architecture (He et al., 2016).", "startOffset": 60, "endOffset": 77}, {"referenceID": 5, "context": "We use weights pretrained on ImageNet (Deng et al., 2009) and perform additional pretraining on imSitu using the classes Vtrain.", "startOffset": 38, "endOffset": 57}, {"referenceID": 20, "context": "Our model is similar to those of ? and Romera-Paredes and Torr (2015) in that we predict the attributes indirectly and train the model through the class labels.", "startOffset": 39, "endOffset": 70}, {"referenceID": 17, "context": "It differs from several other zero-shot models, such as Lampert et al. (2014)\u2019s Direct Attribute Prediction (DAP) model, in that DAP is trained by maximizing the probability of predicting each attribute and then multiplying the probabilities at test time.", "startOffset": 56, "endOffset": 78}, {"referenceID": 8, "context": "Frome et al. (2013) build DeVISE, a model for zero-shot learning on ImageNet object recognition where the objective is for the image model to predict a class\u2019s word embedding directly.", "startOffset": 0, "endOffset": 20}, {"referenceID": 31, "context": "We selected all verbs from the imSitu corpus, which consists of images representing verbs from many categories (Yatskar et al., 2016), then extended the set using the MPII movie visual description dataset and ScriptBase datasets, (Rohrbach et al.", "startOffset": 111, "endOffset": 133}, {"referenceID": 10, "context": ", 2016), then extended the set using the MPII movie visual description dataset and ScriptBase datasets, (Rohrbach et al., 2015; Gorinski and Lapata, 2015).", "startOffset": 104, "endOffset": 154}, {"referenceID": 14, "context": "We used the spaCy dependency parser (Honnibal and Johnson, 2015) to extract the verb template for each sentence, and collected annotations on Mechanical Turk to filter out nonliteral and abstract verbs.", "startOffset": 36, "endOffset": 64}, {"referenceID": 12, "context": "BGRU pretaining We pretrain the BGRU model on the Dictionary Challenge, a collection of 800,000 word-definition pairs obtained from Wordnik and Wikipedia articles (Hill et al., 2016); the objective is to obtain a word\u2019s embedding given one of its definitions.", "startOffset": 163, "endOffset": 182}, {"referenceID": 12, "context": "Following Hill et al. (2016), we use a ranking loss.", "startOffset": 10, "endOffset": 29}, {"referenceID": 31, "context": "The images represent a variety of different semantic role labels (Yatskar et al., 2016).", "startOffset": 65, "endOffset": 87}, {"referenceID": 8, "context": "We additionally compare against DeVISE (Frome et al., 2013), as mentioned in Section 4.", "startOffset": 39, "endOffset": 59}, {"referenceID": 19, "context": "Romera-Paredes and Torr (2015) propose \u201cEmbarassingly Simple Zero-shot Learning\u201d (ESZL), a linear model that directly predicts class labels through attributes and incorporates several types of regularization.", "startOffset": 0, "endOffset": 31}, {"referenceID": 16, "context": "We compare against a variant of Lampert et al. (2014)\u2019s DAP model discussed in Section 4.", "startOffset": 32, "endOffset": 54}, {"referenceID": 8, "context": "Unfortunately, we were unable to mitigate this problem in a way that also led to better quantitative results (for instance, by using a ranking loss as in DeVISE (Frome et al., 2013)).", "startOffset": 161, "endOffset": 181}, {"referenceID": 1, "context": "Interestingly, better performance with predicted attributes is also reported by Al-Halah et al. (2016): predicting the attributes with their CAAP model and then running the DAP model on these predicted attributes outperforms the use of gold attributes at test time.", "startOffset": 80, "endOffset": 103}, {"referenceID": 6, "context": "A related task is that of improving word embeddings using multimodal data and linguistic resources (Faruqui et al., 2015; Silberer et al., 2013; Vendrov et al., 2016).", "startOffset": 99, "endOffset": 166}, {"referenceID": 29, "context": "A related task is that of improving word embeddings using multimodal data and linguistic resources (Faruqui et al., 2015; Silberer et al., 2013; Vendrov et al., 2016).", "startOffset": 99, "endOffset": 166}, {"referenceID": 1, "context": "Likewise, the CAAP model of Al-Halah et al. (2016) predicts the object attributes of concrete nouns for use in zero-shot learning.", "startOffset": 28, "endOffset": 51}, {"referenceID": 20, "context": "(Romera-Paredes and Torr (2015); Lampert et al.", "startOffset": 1, "endOffset": 32}, {"referenceID": 17, "context": "(Romera-Paredes and Torr (2015); Lampert et al. (2014); Mukherjee and Hospedales (2016); ?).", "startOffset": 33, "endOffset": 55}, {"referenceID": 17, "context": "(Romera-Paredes and Torr (2015); Lampert et al. (2014); Mukherjee and Hospedales (2016); ?).", "startOffset": 33, "endOffset": 88}, {"referenceID": 20, "context": "The MIXED action dataset, itself a combination of three action recognition datasets, has 2910 labeled videos with 21 actions, each described by 34 action attributes (Liu et al., 2011).", "startOffset": 165, "endOffset": 183}, {"referenceID": 20, "context": "The MIXED action dataset, itself a combination of three action recognition datasets, has 2910 labeled videos with 21 actions, each described by 34 action attributes (Liu et al., 2011). These action attributes are concrete binary attributes corresponding to low-level physical movements, for instance, \u201carm only motion,\u201d \u201cleg: up-forward motion.\u201d By using word embeddings instead of attributes, Xu et al. (2017) study video activity recognition on a variety of action datasets, albeit in the transductive setting wherein access to the test labels is provided during training.", "startOffset": 166, "endOffset": 411}], "year": 2017, "abstractText": "In this paper, we investigate large-scale zero-shot activity recognition by modeling the visual and linguistic attributes of action verbs. For example, the verb \u201csalute\u201d has several properties, such as being a light movement, a social act, and short in duration. We use these attributes as the internal mapping between visual and textual representations to reason about a previously unseen action. In contrast to much prior work that assumes access to gold standard attributes for zero-shot classes and focuses primarily on object attributes, our model uniquely learns to infer action attributes from dictionary definitions and distributed word representations. Experimental results confirm that action attributes inferred from language can provide a predictive signal for zero-shot prediction of previously unseen activities.", "creator": "LaTeX with hyperref package"}}}