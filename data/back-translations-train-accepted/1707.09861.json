{"id": "1707.09861", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jul-2017", "title": "Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging", "abstract": "In this paper we show that reporting a single performance score is insufficient to compare non-deterministic approaches. We demonstrate for common sequence tagging tasks that the seed value for the random number generator can result in statistically significant (p &lt; 10^-4) differences for state-of-the-art systems. For two recent systems for NER, we observe an absolute difference of one percentage point F1-score depending on the selected seed value, making these systems perceived either as state-of-the-art or mediocre. Instead of publishing and reporting single performance scores, we propose to compare score distributions based on multiple executions. Based on the evaluation of 50.000 LSTM-networks for five sequence tagging tasks, we present network architectures that produce both superior performance as well as are more stable with respect to the remaining hyperparameters.", "histories": [["v1", "Mon, 31 Jul 2017 14:25:24 GMT  (541kb,D)", "http://arxiv.org/abs/1707.09861v1", "Accepted at EMNLP 2017"]], "COMMENTS": "Accepted at EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL stat.ML", "authors": ["nils reimers", "iryna gurevych"], "accepted": true, "id": "1707.09861"}, "pdf": {"name": "1707.09861.pdf", "metadata": {"source": "CRF", "title": "Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging", "authors": ["Nils Reimers", "Iryna Gurevych"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is so that we are in a time, in which we are in a time, in which we are in a time, in which we are in a phase, in which we are in a phase, in which we are in a phase, in which we are in a phase, in which we are in a phase, in which we are in a phase, in which we are in which we are in a phase, in which we are in which we are in a phase, in which we are in which we are in a phase, in which we are in a phase, in which we are in which we are in a phase, in which we are in which we are in which we are in a phase, in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we in which we in which we are in which we in which we are in which we in which we in which we are in which we in which we in which we are in which we in which we in which we are in which we in which we are in which we in which we in which we are in which we in which we are in which we in which we in which we in which we are in which we in which we are in which we in which we in which we in which we are in which we in which we in which we are in which we in which we are in which we in which we are in which we in which we in which we in which we are in which we in which we are in which we in which we in which we in which we in which we in which we are in which we in which we in which we in which we in which we are in which we in which we are in which we in which we are in which we in which we are in which we in which we in which we in which we are in which we in which we in which we in which we in which we in which we in which we in which we in which we in which we are in which we in which we in which we in which we in which we in which we in which we are in which we in which we in which we in which we in which we in which we in which we in which we in which we in which we in"}, {"heading": "2 Background", "text": "The validation and reproduction of results is an important activity in science in order to manifest the correctness of earlier conclusions and gain new insights into the approaches presented. Fokkens et al. (2013) show that the reproduction of results is not always easy, since factors such as pre-processing (e.g. tokenization), experimental setup (e.g. data splitting), the version of components, the exact implementation of characteristics and the treatment of bonds can have a large impact on the performance achieved and sometimes on the conclusions drawn. For approaches with non-deterministic training methods, such as neural networks, the reproduction of exact results becomes even more difficult, as randomness can play an important role in the outcome of experiments. The error function of a neural network is a highly non-convex function of the parameters with the potential for many pronounced local minima (ErCun et al., 1998; Erhan et al., 2010), the sequence of random numbers plays an important role during the network convergence process."}, {"heading": "3 Impact of Randomness in the Evaluation of Neural Networks", "text": "In fact, most of us are able to put ourselves in a different world, in which they put ourselves in another world, in which they put themselves in another world, in which they put themselves in another world, in which they find themselves in another world, in which they find themselves in another world, in which they find themselves in another world, in which they live in another world, in which they live themselves, in which they live themselves, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "4 Experimental Setup", "text": "To find LSTM network architectures that deliver robust performance across multiple tasks, we have selected five classic NLP tasks as benchmark tasks: Partof-Speech tagging (POS), Chunking, Named Entity Recognition (NER), Entity Recognition (Entities), and Event Detection (Events). For part-of-speech tagging, we use the benchmark setup described by Toutanova et al. (2003). Using the full training set for POS conferencing would impede our ability to detect design decisions that are consistently better than others. The error rate for this dataset is about 3% (Marcus et al., 1993), so any improvements above 97% accuracy are likely to be the result of chance. An accuracy of 97.24% was achieved by Toutanova et al. (2003)."}, {"heading": "4.1 Model", "text": "We use a BiLSTM network for sequence marking, as described in (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016).In order to evaluate a large number of different network configurations, we have optimized our implementation in terms of efficiency and reduced the time required per epoch by a factor of 6 compared to Ma and Hovy (2016)."}, {"heading": "4.2 Evaluated Parameters", "text": "We evaluate the following design decisions and hyperparameters: Pre-trained Word Embeddings. We evaluate the Google News Embeddings (G. News) 7 by Mikolov et al. (2013), the Bag of Words (Le. BoW) and the Dependency based Embeddings (Le. Dep.) 8 by Levy and Goldberg (2014), three different GloVe embeddings9 by Pennington et al. (2014), which are either on Wikipedia 2014 + Gigaword 5 (GloVe1 with 100 dimensions and GloVe2 with 300 dimensions) or on Common Crawl (GloVe3), and the Komninos and Manandhar (2016) Embeddings (Comn.) 10. We also evaluate the approach of Bojanowski et al. (2016) (FastText), which defines embeddings for n-grams with the length of 3 to 6."}, {"heading": "5 Robust Model Evaluation", "text": "However, in order to truly understand the capabilities of an approach, it is interesting to test the approach with different sets of hyperparameters for the complete network. Training and tuning a neural network can be time-consuming, sometimes taking several days to train a single instance of a network. If an approach performs well only for a narrow set of parameters, it is difficult to achieve the best performance, and the selection of parameters often makes the difference between mediocre and state-of-the-art performance (Hutter et al., 2014). If an approach performs well only for a narrow set of parameters, it might be difficult to adapt the approach to new tasks, new domains or new languages, as a wide range of possible parameters needs to be evaluated, requiring a significant amount of training time each time. Therefore, it is desirable that the approach produces stable results for a wide range of parameters."}, {"heading": "6 Results", "text": "This section highlights our key findings in evaluating various design decisions for BiLSTM architectures. For reasons of brevity, we limit the number of results. For detailed information, see (Reimers and Gurevych, 2017).11"}, {"heading": "6.1 Classifier", "text": "Table 3 shows a comparison between the use of a Softmax classifier as the last layer and the use of a CRF classifier. The BiLSTM CRF architecture by Huang et al. (2015) achieves better performance on 4 of 5 tasks. For the NERS task, it also achieves a 27% lower standard deviation (statistically significant at p < 10 \u2212 3), indicating that it is less sensitive to the remaining configuration of the network.The CRF classifier only fails in the task of event detection. This task has almost no dependency between tags, as often only a single token comments as an event trigger in a sentence. We investigated the differences between these two classifiers in the number of LSTM layers. As Figure 3 shows, a Softmax classifier benefits from a deep LSTM network with multiple stacked layers. On the other hand, the effect of additional LSTM layers is much less when a CRF classifier is used."}, {"heading": "6.2 Optimizer", "text": "For the optimizers SGD, Adagrad and Adadelta, we observed a large standard deviation in terms of test performance, namely 11https: / / public.ukp.informatik. tu-darmstadt.de / reimers / Optimal _ Hyperparameter _ for _ Deep _ LSTM-Networks. The optimizers RMSProp, Adam and Nadam, on the other hand, achieved much more stable results. Not only were the medians for these three optimizers higher, but also the standard deviation was 0.1328 for Adagrad and 0.0138. for Adadelta, the optimizers RMSProp, Adam and Nadam achieved significantly more stable results."}, {"heading": "6.3 Word Embeddings", "text": "The embedding of Komninos and Manandhar (2016) yielded the best performance for the POS, the entities and the events task. For the chunking task, the dependency-based embedding of Levy and Goldberg (2014) is slightly ahead of the Komninos embedding, the significance level is p = 0.025. For NER, the GloVe embedding, which was trained on Common Crawl, is particularly important for data sets with small training sets. For the POS task, we observed an average difference of 4.97% between the Komninos embedding and the GloVe2 embedding. Note: We only evaluated the pre-trained embedding provided by different authors, but not the underlying algorithms that do not allow high-quality embedding."}, {"heading": "6.4 Character Representation", "text": "We evaluate the approaches of Ma and Hovy (2016) using Convolutionary Neural Networks (CNN) and the approach of Lample et al. (2016) using LSTM networks to derive character-based representations. Table 5 shows that character-based representations make a statistically significant difference only for the POS, chunking, and event task. NER and Entity Recognition consider the difference to derive character-based representation to be statistically insignificant for all tasks (p > 0.01). The difference between the CNN approach of Ma and Hovy (2016) and the LSTM approach of Lample et al. (2016) to derive character-based representation is statistically insignificant, as both approaches have fundamentally different properties: The CNN approach of Ma and Hovy (2016) only takes into account trigrams. It is also location-independent, i.e. the network will not be able to distinguish between trigrams at the beginning, in the middle, or at the end of the word, which is crucial to the position of the word."}, {"heading": "6.5 Gradient Clipping and Normalization", "text": "In the gradient section (Mikolov, 2012), we found no improvement in the thresholds of 1, 3, 5, and 10 for any of the five tasks; normalization of the gradient gradient has a better theoretical justification (Pascanu et al., 2013), and we can confirm from our experiments that it performs better; normalization of the gradient gradient gradient was the best option for only 5.5% of the 492 configurations evaluated (under zero hypothesis, we would expect 20%); which threshold to choose as long as it is not too small or too large is of minor importance; in most cases, a threshold of 1 was the best option (30.5% of the evaluated configurations); we observed a large increase in performance compared to non-normalization of the gradient gradient; the median increase was between 0.29 percentage points F1 for the chunking task and 0.82 percentage points for the POS task."}, {"heading": "6.6 Dropout", "text": "Dropout is a popular method of dealing with neural network overadjustment (Srivastava et al., 2014). We observed that variable dropout (Gal and Ghahramani, 2016) significantly exceeded na\u00efve dropout and did not use dropout. It was the best dropout in 83.5% of the 479 configurations evaluated. Median performance gains compared to abandoning dropout ranged from 0.31 percentage points for the POS task to 1.98 for the entities task. We also observed a large improvement compared to na\u00efve dropout, ranging from 0.19 percentage points for the POS task to 1.32 percentage points for the entities task. Variative dropout showed the smallest standard deviation, suggesting that it is less dependent on the remaining hyperparameters and the random number sequence. We also investigated whether variable dropout should be applied to the production units of the LSTM network, to the recursive areas or both."}, {"heading": "6.7 Further Evaluated Parameters", "text": "The BIO and IOBES marking schemes performed equally well on 4 out of 5 tasks. For entity tasks, the BIO scheme performed best on 88.7% of the configurations tested, with the mean difference being \u2206 F1 = \u2212 1.01%. For the assessed tasks, 2 stacked LSTM layers performed best. For the POStagging task, 1 and 2 layers performed best. For flat networks with a single LSTM layer, about 150 recurring units performed best. For networks with 2 or 3 layers, about 100 recurring units per network performed best. However, the impact of the number of recurring units per network was extremely low. For tasks with small training sets, smaller minibatch sizes from 1 to 16 appeared to be a good choice. For larger training sets from 8 to 32, the BIO and IOBES marking schemes seem to be a good choice."}, {"heading": "7 Conclusion", "text": "In this paper, we have shown that the sequence of random numbers has a statistically significant impact on test performance and that incorrect conclusions can be drawn when comparing performance values based on individual runs. We have shown this for the two current state-of-the-art NER systems by Maand Hovy (2016) and Lample et al. (2016). Based on the published performance values, Ma and Hovy conclude a significant improvement over the approach of Lample et al. However, the re-execution of the provided implementations with different seed values showed that the implementation of Let al. leads to a better generalization of score values. Comparing score distributions reduces the risk of rejecting promising approaches or falsely accepting weaker approaches. Furthermore, it may lead to new insights about the properties of an approach. We have shown this for ten design decisions and hyperparameters of LSTM networks for five tasks. By studying the standard parameters, we can provide new insights into the properties of each approach."}, {"heading": "Acknowledgements", "text": "This work was funded by the Deutsche Forschungsgemeinschaft (German Research Foundation) as part of the Research Training Group Adaptive Processing of Information from Heterogeneous Sources (AIPHES) under the funding number RTG 1994 / 1. Calculations for this purpose were carried out on the Lichtenberg high-performance computer of the TU Darmstadt."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "CoRR, abs/1603.06042.", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Enriching Word Vectors with Subword Information", "author": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1607.04606.", "citeRegEx": "Bojanowski et al\\.,? 2016", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2016}, {"title": "Incorporating Nesterov Momentum into Adam", "author": ["Timothy Dozat"], "venue": null, "citeRegEx": "Dozat.,? \\Q2015\\E", "shortCiteRegEx": "Dozat.", "year": 2015}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "J. Mach. Learn. Res., 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Why Does Unsupervised Pre-training Help Deep Learning", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol", "Pascal Vincent", "Samy Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Offspring from Reproduction Problems: What Replication Failure Teaches Us", "author": ["Antske Fokkens", "Marieke van Erp", "Marten Postma", "Ted Pedersen", "Piek Vossen", "Nuno Freire."], "venue": "ACL (1), pages 1691\u20131701. The Association for Computer Linguistics.", "citeRegEx": "Fokkens et al\\.,? 2013", "shortCiteRegEx": "Fokkens et al\\.", "year": 2013}, {"title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks", "author": ["Yarin Gal", "Zoubin Ghahramani."], "venue": "Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10,", "citeRegEx": "Gal and Ghahramani.,? 2016", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and Statis-", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Neural Networks for Machine Learning - Lecture 6a - Overview of mini-batch gradient descent", "author": ["Geoffrey Hinton"], "venue": null, "citeRegEx": "Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Hinton.", "year": 2012}, {"title": "Flat Minima", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(1):1\u201342.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997a", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997b", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Bidirectional LSTM-CRF Models for Sequence Tagging", "author": ["Zhiheng Huang", "Wei Xu", "Kai Yu."], "venue": "CoRR, abs/1508.01991.", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "An Efficient Approach for Assessing Hyperparameter Importance", "author": ["Frank Hutter", "Holger Hoos", "Kevin Leyton-Brown."], "venue": "Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32, ICML\u201914, pages I\u2013", "citeRegEx": "Hutter et al\\.,? 2014", "shortCiteRegEx": "Hutter et al\\.", "year": 2014}, {"title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "author": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang."], "venue": "CoRR, abs/1609.04836.", "citeRegEx": "Keskar et al\\.,? 2016", "shortCiteRegEx": "Keskar et al\\.", "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR, abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Dependency based embeddings for sentence classification tasks", "author": ["Alexandros Komninos", "Suresh Manandhar."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Komninos and Manandhar.,? 2016", "shortCiteRegEx": "Komninos and Manandhar.", "year": 2016}, {"title": "Neural architectures for named entity recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."], "venue": "CoRR, abs/1603.01360.", "citeRegEx": "Lample et al\\.,? 2016", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Backpropagation Applied to Handwritten Zip Code Recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel."], "venue": "Neural Computation, 1(4):541\u2013551.", "citeRegEx": "LeCun et al\\.,? 1989", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Efficient BackProp", "author": ["Yann LeCun", "L\u00e9on Bottou", "Genevieve B. Orr", "Klaus-Robert M\u00fcller."], "venue": "Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop, pages 9\u201350, London, UK, UK. Springer-Verlag.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "DependencyBased Word Embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 2: Short Papers, pages 302\u2013308.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Joint Event Extraction via Structured Prediction with Global Features", "author": ["Qi Li", "Heng Ji", "Liang Huang."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 73\u201382, Sofia, Bulgaria. Association for", "citeRegEx": "Li et al\\.,? 2013", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF", "author": ["Xuezhe Ma", "Eduard H. Hovy."], "venue": "CoRR, abs/1603.01354.", "citeRegEx": "Ma and Hovy.,? 2016", "shortCiteRegEx": "Ma and Hovy.", "year": 2016}, {"title": "Building a Large Annotated Corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Comput. Linguist., 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "The kolmogorov-smirnov test for goodness of fit", "author": ["Frank J. Massey."], "venue": "Journal of the American Statistical Association, 46(253):68\u201378.", "citeRegEx": "Massey.,? 1951", "shortCiteRegEx": "Massey.", "year": 1951}, {"title": "Statistical language models based on neural networks", "author": ["Tom\u00e1\u0161 Mikolov."], "venue": "Ph.D. thesis, Brno University of Technology.", "citeRegEx": "Mikolov.,? 2012", "shortCiteRegEx": "Mikolov.", "year": 2012}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR, abs/1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A method of solving a convex programming problem with convergence rate O(1/sqr(k))", "author": ["Yurii Nesterov."], "venue": "Soviet Mathematics Doklady, 27:372\u2013 376. Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.", "citeRegEx": "Nesterov.,? 1983", "shortCiteRegEx": "Nesterov.", "year": 1983}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543. Nils Reimers and Iryna Gurevych. 2017. Optimal Hy-", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Association for Computational Linguistics (Volume 2: Short Papers), pages 231\u2013235, Berlin, Germany", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Toutanova et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 1929}], "referenceMentions": [{"referenceID": 21, "context": "to achieve state-of-the-art performance for a wide range of NLP tasks, including many sequence tagging tasks (Ma and Hovy, 2016), dependency parsing (Andor et al.", "startOffset": 109, "endOffset": 128}, {"referenceID": 0, "context": "to achieve state-of-the-art performance for a wide range of NLP tasks, including many sequence tagging tasks (Ma and Hovy, 2016), dependency parsing (Andor et al., 2016), and machine translation (Wu et al.", "startOffset": 149, "endOffset": 169}, {"referenceID": 18, "context": "non-convex function of the parameters with the potential for many distinct local minima (LeCun et al., 1998; Erhan et al., 2010).", "startOffset": 88, "endOffset": 128}, {"referenceID": 4, "context": "non-convex function of the parameters with the potential for many distinct local minima (LeCun et al., 1998; Erhan et al., 2010).", "startOffset": 88, "endOffset": 128}, {"referenceID": 21, "context": "For the recent NER system by Ma and Hovy (2016) we observed that, depending on the random seed value, the performance on unseen data varies between 89.", "startOffset": 29, "endOffset": 48}, {"referenceID": 10, "context": "Further we will focus on Long-Short-Term-Memory (LSTM) Networks (Hochreiter and Schmidhuber, 1997b), as those demonstrated state-of-the-art performance for a wide variety of sequence tagging tasks (Ma and Hovy, 2016; Lample et al.", "startOffset": 64, "endOffset": 99}, {"referenceID": 21, "context": "Further we will focus on Long-Short-Term-Memory (LSTM) Networks (Hochreiter and Schmidhuber, 1997b), as those demonstrated state-of-the-art performance for a wide variety of sequence tagging tasks (Ma and Hovy, 2016; Lample et al., 2016; S\u00f8gaard and Goldberg, 2016).", "startOffset": 197, "endOffset": 265}, {"referenceID": 16, "context": "Further we will focus on Long-Short-Term-Memory (LSTM) Networks (Hochreiter and Schmidhuber, 1997b), as those demonstrated state-of-the-art performance for a wide variety of sequence tagging tasks (Ma and Hovy, 2016; Lample et al., 2016; S\u00f8gaard and Goldberg, 2016).", "startOffset": 197, "endOffset": 265}, {"referenceID": 5, "context": "Fokkens et al. (2013) show that reproducing results is not always straightforward, as factors like preprocessing (e.", "startOffset": 0, "endOffset": 22}, {"referenceID": 18, "context": "The error function of a neural network is a highly non-convex function of the parameters with the potential for many distinct local minima (LeCun et al., 1998; Erhan et al., 2010).", "startOffset": 139, "endOffset": 179}, {"referenceID": 4, "context": "The error function of a neural network is a highly non-convex function of the parameters with the potential for many distinct local minima (LeCun et al., 1998; Erhan et al., 2010).", "startOffset": 139, "endOffset": 179}, {"referenceID": 4, "context": "Erhan et al. (2010) showed for the MNIST handwritten digit recognition task that different random seeds result in largely varying performances.", "startOffset": 0, "endOffset": 20}, {"referenceID": 13, "context": "Figure 1: A conceptual sketch of flat and sharp minima from Keskar et al. (2016). The Y-axis indicates values of the error function and the Xaxis the weight-space.", "startOffset": 60, "endOffset": 81}, {"referenceID": 9, "context": "As (informally) defined by Hochreiter and Schmidhuber (1997a), a minimum can be flat, where the error function remains approximately constant for a large connected region in weight-space, or it can be sharp, where the error function increases rapidly in", "startOffset": 27, "endOffset": 62}, {"referenceID": 13, "context": "On the other hand, flat minima generalize better on new data (Keskar et al., 2016).", "startOffset": 61, "endOffset": 82}, {"referenceID": 4, "context": "Some methods like the weight initialization (Erhan et al., 2010; Glorot and Bengio, 2010) or small-batch training (Keskar et al.", "startOffset": 44, "endOffset": 89}, {"referenceID": 7, "context": "Some methods like the weight initialization (Erhan et al., 2010; Glorot and Bengio, 2010) or small-batch training (Keskar et al.", "startOffset": 44, "endOffset": 89}, {"referenceID": 13, "context": ", 2010; Glorot and Bengio, 2010) or small-batch training (Keskar et al., 2016) help to avoid bad (e.", "startOffset": 57, "endOffset": 78}, {"referenceID": 20, "context": "Two recent, state-of-the-art systems for NER are proposed by Ma and Hovy (2016)5 and by Lample et al.", "startOffset": 61, "endOffset": 80}, {"referenceID": 16, "context": "Two recent, state-of-the-art systems for NER are proposed by Ma and Hovy (2016)5 and by Lample et al. (2016)6.", "startOffset": 88, "endOffset": 109}, {"referenceID": 23, "context": "Using a Kolmogorov-Smirnov significance test (Massey, 1951), we observe a statistically significant difference between these two distributions (p < 0.", "startOffset": 45, "endOffset": 59}, {"referenceID": 17, "context": "The main difference between these two approaches is in the generation of character-based representations: Ma and Hovy uses a Convolutional Neural Network (CNN) (LeCun et al., 1989), while Lample et al.", "startOffset": 160, "endOffset": 180}, {"referenceID": 20, "context": "Table 1: The system by Ma and Hovy (2016) and Lample et al.", "startOffset": 23, "endOffset": 42}, {"referenceID": 16, "context": "Table 1: The system by Ma and Hovy (2016) and Lample et al. (2016) were run multiple times with different seed values.", "startOffset": 46, "endOffset": 67}, {"referenceID": 28, "context": "For Part-of-Speech tagging, we use the benchmark setup described by Toutanova et al. (2003). Using the full training set for POS tagging would hinder our ability to detect design choices that are consistently better than others.", "startOffset": 68, "endOffset": 92}, {"referenceID": 29, "context": "24% accuracy was achieved by Toutanova et al. (2003). Hence, we reduced the training set size from over 38.", "startOffset": 29, "endOffset": 53}, {"referenceID": 20, "context": "We use the same data split as Li et al. (2013). For the Event Detection task, we use the TempEval3 Task B setup.", "startOffset": 30, "endOffset": 47}, {"referenceID": 11, "context": "We use a BiLSTM-network for sequence tagging as described in (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016).", "startOffset": 61, "endOffset": 121}, {"referenceID": 21, "context": "We use a BiLSTM-network for sequence tagging as described in (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016).", "startOffset": 61, "endOffset": 121}, {"referenceID": 16, "context": "We use a BiLSTM-network for sequence tagging as described in (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016).", "startOffset": 61, "endOffset": 121}, {"referenceID": 11, "context": "We use a BiLSTM-network for sequence tagging as described in (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016). To be able to evaluate a large number of different network configurations, we optimized our implementation for efficiency, reducing by a factor of 6 the time required per epoch compared to Ma and Hovy (2016).", "startOffset": 62, "endOffset": 331}, {"referenceID": 21, "context": "News)7 from Mikolov et al. (2013), the Bag of Words (Le.", "startOffset": 12, "endOffset": 34}, {"referenceID": 17, "context": ")8 by Levy and Goldberg (2014), three different GloVe embeddings9 from Pennington et al.", "startOffset": 6, "endOffset": 31}, {"referenceID": 17, "context": ")8 by Levy and Goldberg (2014), three different GloVe embeddings9 from Pennington et al. (2014) trained either on Wikipedia 2014 + Gigaword 5 (GloVe1 with 100 dimensions and GloVe2 with 300 dimensions) or on Common Crawl (GloVe3), and the Komninos and Manandhar (2016) embeddings (Komn.", "startOffset": 6, "endOffset": 96}, {"referenceID": 14, "context": "(2014) trained either on Wikipedia 2014 + Gigaword 5 (GloVe1 with 100 dimensions and GloVe2 with 300 dimensions) or on Common Crawl (GloVe3), and the Komninos and Manandhar (2016) embeddings (Komn.", "startOffset": 150, "endOffset": 180}, {"referenceID": 1, "context": "We also evaluate the approach of Bojanowski et al. (2016) (FastText), which trains embeddings for n-grams with length 3 to 6.", "startOffset": 33, "endOffset": 58}, {"referenceID": 20, "context": "We evaluate the approaches of Ma and Hovy (2016) using Convolutional Neural Networks (CNN) as well as the approach of Lample et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 16, "context": "We evaluate the approaches of Ma and Hovy (2016) using Convolutional Neural Networks (CNN) as well as the approach of Lample et al. (2016) using LSTMnetworks to derive character-based representations.", "startOffset": 118, "endOffset": 139}, {"referenceID": 3, "context": "Besides Stochastic Gradient Descent (SGD), we evaluate Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), RMSProp (Hinton, 2012), Adam (Kingma and Ba, 2014), and Nadam (Dozat, 2015), an Adam variant that incorporates Nesterov momentum (Nesterov, 1983) as optimizers.", "startOffset": 63, "endOffset": 83}, {"referenceID": 8, "context": ", 2011), Adadelta (Zeiler, 2012), RMSProp (Hinton, 2012), Adam (Kingma and Ba, 2014), and Nadam (Dozat, 2015), an Adam variant that incorporates Nesterov momentum (Nesterov, 1983) as optimizers.", "startOffset": 42, "endOffset": 56}, {"referenceID": 14, "context": ", 2011), Adadelta (Zeiler, 2012), RMSProp (Hinton, 2012), Adam (Kingma and Ba, 2014), and Nadam (Dozat, 2015), an Adam variant that incorporates Nesterov momentum (Nesterov, 1983) as optimizers.", "startOffset": 63, "endOffset": 84}, {"referenceID": 2, "context": ", 2011), Adadelta (Zeiler, 2012), RMSProp (Hinton, 2012), Adam (Kingma and Ba, 2014), and Nadam (Dozat, 2015), an Adam variant that incorporates Nesterov momentum (Nesterov, 1983) as optimizers.", "startOffset": 96, "endOffset": 109}, {"referenceID": 26, "context": ", 2011), Adadelta (Zeiler, 2012), RMSProp (Hinton, 2012), Adam (Kingma and Ba, 2014), and Nadam (Dozat, 2015), an Adam variant that incorporates Nesterov momentum (Nesterov, 1983) as optimizers.", "startOffset": 163, "endOffset": 179}, {"referenceID": 24, "context": "uk/nlp/extvec/ ent problem are gradient clipping (Mikolov, 2012) and gradient normalization (Pascanu et al.", "startOffset": 49, "endOffset": 64}, {"referenceID": 6, "context": "We compare no dropout, naive dropout, and variational dropout (Gal and Ghahramani, 2016).", "startOffset": 62, "endOffset": 88}, {"referenceID": 12, "context": "A priori it is hard to know which hyperparameters will yield the best performance and the selection of the parameters often makes the difference between mediocre and state-of-the-art performance (Hutter et al., 2014).", "startOffset": 195, "endOffset": 216}, {"referenceID": 11, "context": "The BiLSTM-CRF architecture by Huang et al. (2015) achieves a better performance on 4 out of 5 tasks.", "startOffset": 31, "endOffset": 51}, {"referenceID": 15, "context": "The embeddings by Komninos and Manandhar (2016) resulted in the best performance for the POS, the Entities and the Events task.", "startOffset": 18, "endOffset": 48}, {"referenceID": 15, "context": "The embeddings by Komninos and Manandhar (2016) resulted in the best performance for the POS, the Entities and the Events task. For the Chunking task, the dependency-based embeddings of Levy and Goldberg (2014) are slightly ahead of the Komninos embeddings, the significance level is at p = 0.", "startOffset": 18, "endOffset": 211}, {"referenceID": 20, "context": "We evaluate the approaches of Ma and Hovy (2016) using Convolutional Neural Networks (CNN) as well as the approach of Lample et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 16, "context": "We evaluate the approaches of Ma and Hovy (2016) using Convolutional Neural Networks (CNN) as well as the approach of Lample et al. (2016) using LSTM-networks to derive character-based representations.", "startOffset": 118, "endOffset": 139}, {"referenceID": 20, "context": "The difference between the CNN approach by Ma and Hovy (2016) and the LSTM approach by Lample et al.", "startOffset": 43, "endOffset": 62}, {"referenceID": 16, "context": "The difference between the CNN approach by Ma and Hovy (2016) and the LSTM approach by Lample et al. (2016) to derive a character-based representations is statistically insignificant for all tasks.", "startOffset": 87, "endOffset": 108}, {"referenceID": 21, "context": "This is quite surprising, as both approaches have fundamentally different properties: The CNN approach from Ma and Hovy (2016) takes only trigrams into account.", "startOffset": 108, "endOffset": 127}, {"referenceID": 16, "context": "The BiLSTM approach from Lample et al. (2016) takes all characters of the word into account.", "startOffset": 25, "endOffset": 46}, {"referenceID": 24, "context": "For gradient clipping (Mikolov, 2012) we couldn\u2019t observe any improvement for the thresholds of 1, 3, 5, and 10 for any of the five tasks.", "startOffset": 22, "endOffset": 37}, {"referenceID": 16, "context": "2016) or LSTMs (Lample et al., 2016) to derive character-based representations.", "startOffset": 15, "endOffset": 36}, {"referenceID": 28, "context": "Dropout is a popular method to deal with overfitting for neural networks (Srivastava et al., 2014).", "startOffset": 73, "endOffset": 98}, {"referenceID": 6, "context": "We could observe that variational dropout (Gal and Ghahramani, 2016) clearly outperforms naive dropout and not using dropout.", "startOffset": 42, "endOffset": 68}, {"referenceID": 20, "context": "We demonstrated this for the two recent state-of-the-art NER systems by Ma and Hovy (2016) and Lample et al.", "startOffset": 72, "endOffset": 91}, {"referenceID": 16, "context": "We demonstrated this for the two recent state-of-the-art NER systems by Ma and Hovy (2016) and Lample et al. (2016). Based on the published performance scores, Ma and Hovy draw the conclusion of a significant improvement over the approach of Lample et al.", "startOffset": 95, "endOffset": 116}], "year": 2017, "abstractText": "In this paper we show that reporting a single performance score is insufficient to compare non-deterministic approaches. We demonstrate for common sequence tagging tasks that the seed value for the random number generator can result in statistically significant (p < 10\u22124) differences for state-of-the-art systems. For two recent systems for NER, we observe an absolute difference of one percentage point F1-score depending on the selected seed value, making these systems perceived either as state-of-the-art or mediocre. Instead of publishing and reporting single performance scores, we propose to compare score distributions based on multiple executions. Based on the evaluation of 50.000 LSTMnetworks for five sequence tagging tasks, we present network architectures that produce both superior performance as well as are more stable with respect to the remaining hyperparameters. The full experimental results are published in (Reimers and Gurevych, 2017).1 The implementation of our network is publicly available.2", "creator": "LaTeX with hyperref package"}}}