{"id": "1707.09920", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jul-2017", "title": "Regularization techniques for fine-tuning in neural machine translation", "abstract": "We investigate techniques for supervised domain adaptation for neural machine translation where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset. In this scenario, overfitting is a major challenge. We investigate a number of techniques to reduce overfitting and improve transfer learning, including regularization techniques such as dropout and L2-regularization towards an out-of-domain prior. In addition, we introduce tuneout, a novel regularization technique inspired by dropout. We apply these techniques, alone and in combination, to neural machine translation, obtaining improvements on IWSLT datasets for English-&gt;German and English-&gt;Russian. We also investigate the amounts of in-domain training data needed for domain adaptation in NMT, and find a logarithmic relationship between the amount of training data and gain in BLEU score.", "histories": [["v1", "Mon, 31 Jul 2017 15:31:12 GMT  (34kb,D)", "http://arxiv.org/abs/1707.09920v1", "EMNLP 2017 short paper; for bibtex, seethis http URL"]], "COMMENTS": "EMNLP 2017 short paper; for bibtex, seethis http URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["antonio valerio miceli barone", "barry haddow", "ulrich germann", "rico sennrich"], "accepted": true, "id": "1707.09920"}, "pdf": {"name": "1707.09920.pdf", "metadata": {"source": "CRF", "title": "Regularization techniques for fine-tuning in neural machine translation", "authors": ["Antonio Valerio Miceli Barone", "Barry Haddow", "Ulrich Germann", "Rico Sennrich"], "emails": ["ugermann}@inf.ed.ac.uk", "rico.sennrich@ed.ac.uk"], "sections": [{"heading": null, "text": "We are investigating supervised domain adaptation techniques for neural machine translation, where an existing model that has been trained on a large dataset outside the domain is adapted to a small intra-domain dataset. In this scenario, overfitting is a major challenge. We are examining a range of techniques to reduce overfitting and improve transfer learning, including regularizing techniques such as dropout and L2regularization toward a non-domain dataset. In addition, we are introducing tuneout, a novel dropout-inspired regulation technology. We are applying these techniques alone and in combination with neural machine translation, and are achieving improvements in IWSLT datasets for English \u2192 German and English \u2192 Russian. We are also examining the amounts of intra-domain training data required for domain adaptation in NMT, and finding a logarithmic relationship between the amount of training data and the EU gain in the BLScore."}, {"heading": "1 Introduction", "text": "Neural machine translation (Bahdanau et al., 2015; Sutskever et al., 2014) has established itself as the new state of the art in recent common translation tasks (Bojar et al., 2016; Cettolo et al., 2016). In order to achieve good generalization techniques, Neural Machine Translation, like most other large machine learning systems, requires large amounts of training examples, which proceed from a distribution as close as possible to the distribution of the inputs seen during execution. However, in many applications only a small amount of parallel text is available for the specific scope, and it is therefore desirable to use larger out-domain data. Due to the incremental nature of stochastic gradient-based education algorithms, a simple but effective approach to transferring learning for neural networks is fine-tuning (Hinton and Salakhutdinov, 2006; Mesnil et al, 2012; Yosinski, 2014)."}, {"heading": "2 Regularization Techniques for Transfer Learning", "text": "A major challenge in transfer learning for domain adaptation is to adapt to the small amount of potentially available in-domain training data. We are investigating the impact of different regulatory techniques to reduce overadaptation and improve the quality of transfer learning."}, {"heading": "2.1 Dropout", "text": "The first variant we are considering is fine-tuning with dropouts. Dropouts (Srivastava et al., 2014) is a stochastic regularization technique for neural networks. In particular, we consider \"Bayesian\" dropouts for recurring neural networks (Gal and Ghahramani, 2016). In this technique, the columns of the weight matrices of the neural network are randomly set to zero, regardless of each example and epoch, but with the proviso that if the same weight matrix appears several times in the unwanted calculation curve of a given example, the same columns are set to zero. For an arbitrary layer that takes an input vector h and calculates the preactivation vector v (ignoring the bias parameter), vi, j = W \u00b7 MW, i \u00b7 hi, j (1), etropdiag."}, {"heading": "2.2 MAP-L2", "text": "Chelba and Acero (2006) extended this technique to the transfer of learning by punishing the weights of the in-domain model by their L2 distance from the weights of the previously trained out-domain model. For each parameter matrix W is the punitive term LW = \u03bb \u00b7 0-W, where W is the in-domain parameter matrix to be learned, and W \u0442 the associated fixed-out-domain parameter matrix. Bias parameters can also be regulated. In linear models, this corresponds to the maximum of a posteriori inference w.r.t. of a diagonal Gaussian task with mean equal to the out-of-domain parameter matrix."}, {"heading": "2.3 Tuneout", "text": "We also propose a novel transfer learning method, which we call tuneout. Like Bayesian dropouts, we randomly drop columns of weight matrices during training, but instead of setting them to zero, we set them to the corresponding columns of non-domain parameter matrices. Alternatively, this can be considered as learning matrices of parameter differences between domain-internal and non-domain models with default dropouts, starting with zero initialization at the beginning of fine tuning. Therefore, Equation 2 results in the following: vi, j = (W-W, i, j) \u00b7 hi, j (3), with W-W being the specified non-domain parameter matrix, and W being the parameter differential matrix to be learned, and M-W, i, j a Bayesian dropout mask."}, {"heading": "3 Evaluation", "text": "We evaluate transfer learning using test kits from the IWSLT shared translation task (Cettolo et al., 2012)."}, {"heading": "3.1 Data and Methods", "text": "In fact, the majority of people who are in a position to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight,"}, {"heading": "3.2 Results", "text": "This year, it has come to the point where it will be able to retaliate, \"he said.\" We've never lost so much time, \"he said.\" We're not there yet, \"he said.\" We've never waited so long to be able to unite. \""}, {"heading": "4 Conclusion", "text": "We studied fine-tuning domain adjustment in neural machine translation with varying amounts of domain-internal training data and strategies to avoid overadjustments. We found that our baseline, which relies only on early stop, performed strongly, but fine-tuned with recurring dropouts and with MAP-L2 regularization, brought additional small improvements in the order of 0.3 BLEU points for both English-German and English-Russian, while improvements in terms of the final translation accuracy of the tuneout seemed to be less consistent. In addition, we found that regularization techniques that we considered more robust to overadjustments, which is especially helpful in scenarios where only small amounts of domain data are available, making an early stop impractical, since it relies on a sufficiently large amount of validation in domains to stabilize us, are more efficient with overadjustments, especially in which scenarios where only small amounts of domain data are available, making an early stop impracticable impracticality impracticable, since it relies on a sufficiently large amount of validation in domains to stabilize our domains, which we are using our AP for both the specific DAP and DAP tasks, as we recommend them are easy to perform in our experiments, as well as in terms of both Drop2."}, {"heading": "Acknowledgments", "text": "This project was funded by the European Union's Horizon 2020 research and innovation programme under funding agreements 644333 (TraMOOC) and 645487 (ModernMT) and we thank Booking.com for their support."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Findings of the 2016 Conference on Machine Translation (WMT16)", "author": ["Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri."], "venue": "Proceedings of the First Conference on Machine Translation, Vol-", "citeRegEx": "Post et al\\.,? 2016", "shortCiteRegEx": "Post et al\\.", "year": 2016}, {"title": "WIT: Web Inventory of Transcribed and Translated Talks", "author": ["Mauro Cettolo", "Christian Girardi", "Marcello Federico."], "venue": "Proceedings of the 16 Conference of the European Association for Machine Translation (EAMT), pages 261\u2013268, Trento,", "citeRegEx": "Cettolo et al\\.,? 2012", "shortCiteRegEx": "Cettolo et al\\.", "year": 2012}, {"title": "Report on the 13th IWSLT Evaluation Campaign", "author": ["Mauro Cettolo", "Jan Niehues", "Sebastian St\u00fcker", "Luisa Bentivogli", "Marcello Federico."], "venue": "IWSLT 2016, Seattle, USA.", "citeRegEx": "Cettolo et al\\.,? 2016", "shortCiteRegEx": "Cettolo et al\\.", "year": 2016}, {"title": "Adaptation of maximum entropy capitalizer: Little data can help a lot", "author": ["Ciprian Chelba", "Alex Acero."], "venue": "Computer Speech & Language, 20(4):382\u2013399.", "citeRegEx": "Chelba and Acero.,? 2006", "shortCiteRegEx": "Chelba and Acero.", "year": 2006}, {"title": "An Empirical Comparison of Simple Domain Adaptation Methods for Neural Machine Translation", "author": ["Chenhui Chu", "Raj Dabre", "Sadao Kurohashi."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Vancouver,", "citeRegEx": "Chu et al\\.,? 2017", "shortCiteRegEx": "Chu et al\\.", "year": 2017}, {"title": "Frustratingly Easy Domain Adaptation", "author": ["Hal Daume III."], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256\u2013263, Prague, Czech Republic. Association for Computational Linguistics.", "citeRegEx": "III.,? 2007", "shortCiteRegEx": "III.", "year": 2007}, {"title": "Hierarchical Bayesian Domain Adaptation", "author": ["Jenny Rose Finkel", "Christopher D. Manning."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Finkel and Manning.,? 2009", "shortCiteRegEx": "Finkel and Manning.", "year": 2009}, {"title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks", "author": ["Yarin Gal", "Zoubin Ghahramani."], "venue": "Advances in Neural Information Processing Systems 29 (NIPS).", "citeRegEx": "Gal and Ghahramani.,? 2016", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov."], "venue": "Science, 313(5786):504\u2013507.", "citeRegEx": "Hinton and Salakhutdinov.,? 2006", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "The International Conference on Learning Representations, San Diego, California, USA.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Overcoming catastrophic forgetting in neural networks", "author": ["Raia Hadsell."], "venue": "Proceedings of the National Academy of Sciences, 114(13):3521\u20133526.", "citeRegEx": "Hadsell.,? 2017", "shortCiteRegEx": "Hadsell.", "year": 2017}, {"title": "Statistical Significance Tests for Machine Translation Evaluation", "author": ["Philipp Koehn."], "venue": "Proceedings of EMNLP 2004, pages 388\u2013395, Barcelona, Spain.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Stanford Neural Machine Translation Systems for Spoken Language Domains", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "Proceedings of the International Workshop on Spoken Language Translation 2015, Da Nang, Vietnam.", "citeRegEx": "Luong and Manning.,? 2015", "shortCiteRegEx": "Luong and Manning.", "year": 2015}, {"title": "Unsupervised and Transfer Learning Challenge: a Deep Learning Approach", "author": ["Gr\u00e9goire Mesnil", "Yann Dauphin", "Xavier Glorot", "Salah Rifai", "Yoshua Bengio", "Ian J Goodfellow", "Erick Lavoie", "Xavier Muller", "Guillaume Desjardins", "David Warde-Farley"], "venue": null, "citeRegEx": "Mesnil et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2012}, {"title": "Nematus: a Toolkit for Neural Machine", "author": ["Rico Sennrich", "Orhan Firat", "Kyunghyun Cho", "Alexandra Birch", "Barry Haddow", "Julian Hitschler", "Marcin Junczys-Dowmunt", "Samuel L\u00e4ubli", "Antonio Valerio Miceli Barone", "Jozef Mokry", "Maria Nadejde"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2017}, {"title": "Edinburgh Neural Machine Translation Systems for WMT 16", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the First Conference on Machine Translation, pages 371\u2013 376, Berlin, Germany. Association for Computa-", "citeRegEx": "Sennrich et al\\.,? 2016a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Improving Neural Machine Translation Models with Monolingual Data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-", "citeRegEx": "Sennrich et al\\.,? 2016b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u2013", "citeRegEx": "Sennrich et al\\.,? 2016c", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, pages 3104\u20133112,", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Hierarchical Incremental Adaptation for Statistical Machine Translation", "author": ["Joern Wuebker", "Spence Green", "John DeNero."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1059\u20131065, Lisbon,", "citeRegEx": "Wuebker et al\\.,? 2015", "shortCiteRegEx": "Wuebker et al\\.", "year": 2015}, {"title": "How transferable are features in deep neural networks", "author": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Neural machine translation (Bahdanau et al., 2015; Sutskever et al., 2014) has established itself as the new state of the art at recent shared translation tasks (Bojar et al.", "startOffset": 27, "endOffset": 74}, {"referenceID": 20, "context": "Neural machine translation (Bahdanau et al., 2015; Sutskever et al., 2014) has established itself as the new state of the art at recent shared translation tasks (Bojar et al.", "startOffset": 27, "endOffset": 74}, {"referenceID": 3, "context": ", 2014) has established itself as the new state of the art at recent shared translation tasks (Bojar et al., 2016; Cettolo et al., 2016).", "startOffset": 94, "endOffset": 136}, {"referenceID": 9, "context": "Owing to the incremental nature of stochastic gradient-based training algorithms, a simple yet effective approach to transfer learning for neural networks is fine-tuning (Hinton and Salakhutdinov, 2006; Mesnil et al., 2012; Yosinski et al., 2014): to continue training an existing model which was trained on out-of-domain data with indomain training data.", "startOffset": 170, "endOffset": 246}, {"referenceID": 14, "context": "Owing to the incremental nature of stochastic gradient-based training algorithms, a simple yet effective approach to transfer learning for neural networks is fine-tuning (Hinton and Salakhutdinov, 2006; Mesnil et al., 2012; Yosinski et al., 2014): to continue training an existing model which was trained on out-of-domain data with indomain training data.", "startOffset": 170, "endOffset": 246}, {"referenceID": 22, "context": "Owing to the incremental nature of stochastic gradient-based training algorithms, a simple yet effective approach to transfer learning for neural networks is fine-tuning (Hinton and Salakhutdinov, 2006; Mesnil et al., 2012; Yosinski et al., 2014): to continue training an existing model which was trained on out-of-domain data with indomain training data.", "startOffset": 170, "endOffset": 246}, {"referenceID": 13, "context": "This strategy was also found to be very effective for neural machine translation (Luong and Manning, 2015; Sennrich et al., 2016b).", "startOffset": 81, "endOffset": 130}, {"referenceID": 17, "context": "This strategy was also found to be very effective for neural machine translation (Luong and Manning, 2015; Sennrich et al., 2016b).", "startOffset": 81, "endOffset": 130}, {"referenceID": 7, "context": "Other works consider techniques that jointly train on the outdomain and in-domain corpora, distinguishing them using specific input features (Daume III, 2007; Finkel and Manning, 2009; Wuebker et al., 2015).", "startOffset": 141, "endOffset": 206}, {"referenceID": 21, "context": "Other works consider techniques that jointly train on the outdomain and in-domain corpora, distinguishing them using specific input features (Daume III, 2007; Finkel and Manning, 2009; Wuebker et al., 2015).", "startOffset": 141, "endOffset": 206}, {"referenceID": 5, "context": "In fact, Chu et al. (2017) successfully apply fine-tuning in combination with joint training.", "startOffset": 9, "endOffset": 27}, {"referenceID": 19, "context": "Dropout (Srivastava et al., 2014) is a stochastic regularization technique for neural networks.", "startOffset": 8, "endOffset": 33}, {"referenceID": 8, "context": "In particular, we consider \"Bayesian\" dropout for recurrent neural networks (Gal and Ghahramani, 2016).", "startOffset": 76, "endOffset": 102}, {"referenceID": 8, "context": "Gal and Ghahramani (2016) have shown that this corresponds to approximate variational Bayesian inference over the weight matrices considered as model-wise random variables, where the individual weights have a Gaussian prior with zero mean and small diagonal covariance.", "startOffset": 0, "endOffset": 26}, {"referenceID": 4, "context": "Chelba and Acero (2006) extended this technique to transfer learning by penalizing the weights of the in-domain model by their L2-distance from the weights of the previously trained out-of-domain model.", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "We evaluate transfer learning on test sets from the IWSLT shared translation task (Cettolo et al., 2012).", "startOffset": 82, "endOffset": 104}, {"referenceID": 17, "context": "For the out-of-domain systems, we use training data from the WMT shared translation task,2 which is considered permissible for IWSLT tasks, including back-translations of monolingual training data (Sennrich et al., 2016b), i.", "startOffset": 197, "endOffset": 221}, {"referenceID": 15, "context": "(2016a), using Nematus (Sennrich et al., 2017) as the neural machine translation toolkit.", "startOffset": 23, "endOffset": 46}, {"referenceID": 10, "context": "We differ from their setup only in that we use Adam (Kingma and Ba, 2015) for optimization.", "startOffset": 52, "endOffset": 73}, {"referenceID": 14, "context": "We train out-of-domain systems following tools and hyperparameters reported by Sennrich et al. (2016a), using Nematus (Sennrich et al.", "startOffset": 79, "endOffset": 103}, {"referenceID": 18, "context": "data is tokenized, truecased and segmented into subword units using byte-pair encoding (BPE) (Sennrich et al., 2016c).", "startOffset": 93, "endOffset": 117}, {"referenceID": 12, "context": "Statistical significance on the concatenated test sets scores is determined via bootstrap resampling (Koehn, 2004).", "startOffset": 101, "endOffset": 114}], "year": 2017, "abstractText": "We investigate techniques for supervised domain adaptation for neural machine translation where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset. In this scenario, overfitting is a major challenge. We investigate a number of techniques to reduce overfitting and improve transfer learning, including regularization techniques such as dropout and L2regularization towards an out-of-domain prior. In addition, we introduce tuneout, a novel regularization technique inspired by dropout. We apply these techniques, alone and in combination, to neural machine translation, obtaining improvements on IWSLT datasets for English\u2192German and English\u2192Russian. We also investigate the amounts of in-domain training data needed for domain adaptation in NMT, and find a logarithmic relationship between the amount of training data and gain in BLEU score.", "creator": "LaTeX with hyperref package"}}}