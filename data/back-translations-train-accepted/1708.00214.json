{"id": "1708.00214", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Aug-2017", "title": "Natural Language Processing with Small Feed-Forward Networks", "abstract": "We show that small and shallow feed-forward neural networks can achieve near state-of-the-art results on a range of unstructured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models. Motivated by resource-constrained environments like mobile phones, we showcase simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget.", "histories": [["v1", "Tue, 1 Aug 2017 09:13:44 GMT  (107kb,D)", "http://arxiv.org/abs/1708.00214v1", "EMNLP 2017 short paper"]], "COMMENTS": "EMNLP 2017 short paper", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["jan a botha", "emily pitler", "ji ma", "anton bakalov", "alex salcianu", "david weiss", "ryan t mcdonald", "slav petrov"], "accepted": true, "id": "1708.00214"}, "pdf": {"name": "1708.00214.pdf", "metadata": {"source": "CRF", "title": "Natural Language Processing with Small Feed-Forward Networks", "authors": ["Jan A. Botha", "Emily Pitler", "Ji Ma", "Anton Bakalov", "Alex Salcianu", "David Weiss", "Ryan McDonald", "Slav Petrov"], "emails": ["jabot@google.com", "epitler@google.com", "maji@google.com", "abakalov@google.com", "salcianu@google.com", "djweiss@google.com", "ryanmcd@google.com", "slav@google.com"], "sections": [{"heading": "1 Introduction", "text": "Deep and recurrent neural networks with large network capacity have become increasingly accurate for demanding language processing tasks. For example, machine translation models have achieved impressive accuracy with models that use hundreds of millions (Bahdanau et al., 2014; Wu et al., 2016) or billions (Shazeer et al., 2017) of parameters, but these models may not be feasible in all computational settings, especially models that run on mobile devices are often limited in terms of memory and compatibility. Long-term memory (LSTM) models (Hochreiter and Schmidhuber, 1997) have achieved good results with small memory spaces using character-based input representations: for example, the parts of the language tagging models by Gillick et al. (2016) we have only about 900,000 parameters. Latency, however, can be due to the large number of matrix multiplications we need."}, {"heading": "2 Small Feed-Forward Network Models", "text": "rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide eeisrrdteeVnlrrrrrtee\u00fce\u00fcgznlrteeeirsn rf\u00fc ide rf\u00fc ide eeirsrrrrrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeerso \"rrso\" rrr\u00fc \"rrrrso\" rrrrso \"rrrrrso\" lrrrrrrrrrlrrrrlrrlrrrlrrlrlrlrrlrlrlrrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrrlrlrlrlrlrlrlrlrlrlrrlrlrlrlrlrrlrlrlrrrrlrlrrrrlrlrrrlrlrlrrlrlrrrrrlrrrrrrlrrrrrlrrrlrrrlrrlrrrrrrrrrrrrrrlrrrrrrrlrrrrrrrrrrrrrlrrrrrrrrrrr"}, {"heading": "3 Experiments", "text": "We are experimenting with small feed-forward networks for four different NLP tasks: language identification, part-of-speech tagging, word segmentation, and pre-ordering for statistical machine translation. Evaluation metrics In addition to standard task-specific quality metrics, our evaluations also take model size and computational costs into account. We bypass implementation details by calculating the size as the number of kilobytes (1KB = 1024 bytes) required to represent all model parameters and resources. We approach the computational cost as the number of floating point operations (FLOPs) that are performed for a forward pass through the network, with an embedding vector h0 given. These costs are dominated by the matrix multiplications to calculate (uncalibrated) activation units, so our metric excludes nonlinearity and softmax normalization from the network, but still takes into account the last layer of the Intel X50th to measure them."}, {"heading": "3.1 Language Identification", "text": "Recent common tasks on code switching (Molina et al., 2016) and dialects (Malmasi et al., 2016) have rekindled interest in language identification. We limit our focus to unified language identification in different languages and compare the work of Baldwin and Lui (2010) in predicting the language of Wikipedia text in 66 languages. For this task, we get h0 input by calculating the embeddings for each gram length separately (N = [1, 4]), as the summation does not show good results.Table 1 shows that we exceed the low memory model of Baldwin and Lui (2010).Their nearest neighbor model is the most accurate, but its memory scales linearly with the size of the training data. In addition, we can apply quantification to the embedding of matrix without compromising the accuracy of the prediction: it is better to use less precision for each dimension, but to use more of the following dimensions of CLI."}, {"heading": "3.2 POS Tagging", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "3.3 Segmentation", "text": "Recently, neural networks have significantly improved segmentation accuracy (Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016; Yang et al., 2017; Kong et al., 2015). We are using a structured model based on the transition system in Table 3 and similar to that of Zhang and Clark (2007). We are conducting the segmentation experiments at the Chinese Tree Base 6.0 with the recommended data splits. No external resources or pre-formed embeddings are used. Hashing was detrimental to quality in our preliminary experiments, so we are not using it for this task. To learn embedding for unknown characters, we are throwing characters that occur only once in the training set onto a special symbol.Selected features Since we are not using hashing here, we have to be careful about the size of the input vocabulary. The neural network with its detailed line theory, which is tedious, does not connect with cognition."}, {"heading": "3.4 Preordering", "text": "Pre-ordering source-side words into the target order is a useful pre-processing task for statistical machine translation (Xia and McCord, 2004; Collins et al., 2005; Nakagawa, 2015; de Gispert et al., 2015). We propose a novel transitional system for this task (Table 5) so that we can repeatedly use a small network to produce these permutations. Inspired by a non-projecting transition system (Nivre, 2009), the system uses a SWAP action to permutate chips. It is solid for permutations: each derivative ends with all input words in a permutated order and is complete: all permutations are available (use SHIFT and SWAP operations to perform a bubble sorting, then APPEND n \u2212 1 time to form a single span."}, {"heading": "4 Conclusions", "text": "This paper shows that small feed-forward networks are sufficient to achieve useful accuracy across a wide range of tasks. In resource-constrained environments, speed and memory are important metrics for optimization and accuracy. While large and deep recurring models are likely to be most accurate whenever they are available, feed-forward networks can provide better values in terms of runtime and memory and should be considered as a strong baseline."}, {"heading": "Acknowledgments", "text": "We thank Kuzman Ganchev, Fernando Pereira and the anonymous reviewers for their useful comments."}, {"heading": "A Quantization Details", "text": "For quantification, we first calculate a scale factor si for each embedding vector ei assi = 1b \u2212 1 max j | eij |.Each weight eij is then quantified into an 8-bit integer asqij = b 1 2 + eij si + bc, where the bias is b = 128. Consequently, the number of bits required to store the embedding matrix is reduced by a factor of 4, in exchange for storing the additional scale values V. At the time of inference, the embedding is dequantized on-the-fly."}, {"heading": "B FLOPs Calculation", "text": "The product of A-RP \u00b7 Q and b-RQ includes P (2Q \u2212 1) FLOPs, and our only hidden ReLu layer requires this operation once per time step (P = M, Q = H0). H0 stands for the size of the embedding vector h0, which corresponds to 408, 464, and 260 for our respective POS models, as shown in Table 2. In contrast, each LSTM layer requires eight products per time step, and the BTS model has four layers (P = Q = 320). The special sequence sequence sequence equation scheme by Gillick et al. (2016) requires at least four time intervals to generate meaningful output: the individual input byte (s) and start, length, and label of the predicted time span. A single time step is therefore a relaxed lower limit for the number of FLOPs needed for the BTS inference."}, {"heading": "C Word Clusters", "text": "The word clusters we use are for the 250k most common words from a large, uncommented corpus grouped into 256 classes using the distributed exchange algorithm (Uszkoreit and Brants, 2008) and the procedure described in Appendix A of Ta \ufffd ckstro \ufffd m et al. (2012). The space required to store them in a bloom map is calculated using the formula derived from Talbot and Talbot (2008): Each entry requires 1.23% (log 1 + H) bits, where H is the entropy of the distribution on the set of values, and = 2 \u2212 E, where E is the number of error bits used. We use 0 error bits and assume a uniform distribution for the 256 values, i.e. H = 8, so we need 9.84 bits per entry, or 300KB for the 250k entries."}, {"heading": "D Lang-ID Details", "text": "In our evaluation of language identification, the 1,2,3,4 gram embedding vectors have 6 or 16 dimensions, depending on the experimental setting, and their hashed vocabulary sizes (Vg) are 100, 1000, 5000, and 5000, respectively, and the hidden layer size is set to M = 208. We process data by removing non-alphabetical characters and parts of text (that is, anything between < and >, including parentheses). If this results in an empty string, we skip the markup removal at test date, and if this still leads to an empty string, we process the original string. This procedure is an artifact of the Wikipedia dataset, where some documents contain only punctuation or trivial HTML code, but we need to make predictions so that the results are directly comparable to literature."}, {"heading": "E POS Details", "text": "The small FF model compared to BTS uses 2,3,4 grams and some byte unigrams (see feature templates in Table VII). The n-grams have embed sizes of 16 and the byte unigrams get 4 dimensions. In our 12-dimensional setting, the above dimensions are halved to 8 and 2.cluster characteristics. (The hidden layer sizes are fixed to M = 320.bytes). (0, 1), [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0],. \"(N), [1), [0, [0]. (0,]. (0,). (0,). (0,)."}, {"heading": "F Segmentation Details", "text": "Feature templates used in segmentation experiments are listed in Table viii. In addition, we define length feature as the number of characters between the upper end of \u03c3 and the front of \u03b2, this maximum feature value is truncated to 100. The length feature is used in all segmentation models and the embedding dimension is set to 6. We set the cutoff for character and character bigrams to 2 to learn unknown characters / bigrams embeddings. The hidden layer size is set to M = 256."}, {"heading": "G Preordering Details", "text": "GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB The Feature Templates for the Preorderer view the top four spans on the stack and the first four spans on the buffer; for each span view the Feature Templates view the first two words and last two words within the span. The \"Vanilla\" variant of the preorderer contains signs n-grammes, word bytes and whether the span ever participated in a SWAP transition. The POS features are the predicted tags for the words in these positions. Table ix shows the full feature templates for the preorder.Small FF 6 dim Small FF 16 dim # L.R. Momes 96.6 SGB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB GB"}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Language identification: The long and the short of the matter", "author": ["Timothy Baldwin", "Marco Lui."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Baldwin and Lui.,? 2010", "shortCiteRegEx": "Baldwin and Lui.", "year": 2010}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L\u00e9on Bottou."], "venue": "Proceedings of COMPSTAT\u20192010, pages 177\u2013186. Springer.", "citeRegEx": "Bottou.,? 2010", "shortCiteRegEx": "Bottou.", "year": 2010}, {"title": "Neural word segmentation learning for Chinese", "author": ["Deng Cai", "Hai Zhao."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 409\u2013420, Berlin, Germany. Association for Compu-", "citeRegEx": "Cai and Zhao.,? 2016", "shortCiteRegEx": "Cai and Zhao.", "year": 2016}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D. Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Clause restructuring for statistical machine translation", "author": ["M. Collins", "P. Koehn", "I. Ku\u010derov\u00e1."], "venue": "Proceedings of ACL, pages 531\u2013540.", "citeRegEx": "Collins et al\\.,? 2005", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "Small statistical models by random feature mixing", "author": ["Kuzman Ganchev", "Mark Dredze."], "venue": "Proceedings of the ACL-08- HLT Workshop on Mobile Language Processing, pages 18\u201319. Association for Computational Linguistics.", "citeRegEx": "Ganchev and Dredze.,? 2008", "shortCiteRegEx": "Ganchev and Dredze.", "year": 2008}, {"title": "Multilingual language processing from bytes", "author": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya."], "venue": "Proceedings of NAACL-HLT, pages 1296\u20131306, San Diego, USA. Association for Computational Linguistics.", "citeRegEx": "Gillick et al\\.,? 2016", "shortCiteRegEx": "Gillick et al\\.", "year": 2016}, {"title": "Fast and accurate preordering for SMT using neural networks", "author": ["Adri\u00e0 de Gispert", "Gonzalo Iglesias", "Bill Byrne."], "venue": "Proceedings of NAACL, pages 1012\u20131017.", "citeRegEx": "Gispert et al\\.,? 2015", "shortCiteRegEx": "Gispert et al\\.", "year": 2015}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally."], "venue": "arXiv preprint arXiv:1510.00149.", "citeRegEx": "Han et al\\.,? 2015", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "A practical guide to training restricted Boltzmann machines", "author": ["Geoffrey E. Hinton."], "venue": "Neural Networks: Tricks of the Trade (2nd ed.), Lecture Notes in Computer Science, pages 599\u2013619. Springer.", "citeRegEx": "Hinton.,? 2012", "shortCiteRegEx": "Hinton.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Sequencelevel knowledge distillation", "author": ["Yoon Kim", "Alexander M. Rush."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317\u20131327, Austin, Texas. Association for Computational Linguistics.", "citeRegEx": "Kim and Rush.,? 2016", "shortCiteRegEx": "Kim and Rush.", "year": 2016}, {"title": "Segmental recurrent neural networks", "author": ["Lingpeng Kong", "Chris Dyer", "Noah A. Smith."], "venue": "CoRR, abs/1511.06018.", "citeRegEx": "Kong et al\\.,? 2015", "shortCiteRegEx": "Kong et al\\.", "year": 2015}, {"title": "Simple semi-supervised dependency parsing", "author": ["Terry Koo", "Xavier Carreras", "Michael Collins."], "venue": "Proceedings of ACL-08: HLT, pages 595\u2013603.", "citeRegEx": "Koo et al\\.,? 2008", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Exploring segment representations for neural segmentation models", "author": ["Yijia Liu", "Wanxiang Che", "Jiang Guo", "Bing Qin", "Ting Liu."], "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY,", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Discriminating between similar languages and Arabic dialect identification: A report on the third DSL shared task", "author": ["Shervin Malmasi", "Marcos Zampieri", "Nikola Ljube\u0161i\u0107", "Preslav Nakov", "Ahmed Ali", "J\u00f6rg Tiedemann."], "venue": "Proceedings of the Third", "citeRegEx": "Malmasi et al\\.,? 2016", "shortCiteRegEx": "Malmasi et al\\.", "year": 2016}, {"title": "Overview for the second shared task on language identification in code-switched data", "author": ["Giovanni Molina", "Fahad AlGhamdi", "Mahmoud Ghoneim", "Abdelati Hawwari", "Nicolas ReyVillamizar", "Mona Diab", "Thamar Solorio."], "venue": "Proceedings", "citeRegEx": "Molina et al\\.,? 2016", "shortCiteRegEx": "Molina et al\\.", "year": 2016}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton."], "venue": "Proceedings of the 27th International Conference on Machine Learning, pages 807\u2013814.", "citeRegEx": "Nair and Hinton.,? 2010", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Efficient top-down BTG parsing for machine translation preordering", "author": ["Tetsuji Nakagawa."], "venue": "Proceedings of ACL, pages 208\u2013218.", "citeRegEx": "Nakagawa.,? 2015", "shortCiteRegEx": "Nakagawa.", "year": 2015}, {"title": "Non-projective dependency parsing in expected linear time", "author": ["Joakim Nivre."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages", "citeRegEx": "Nivre.,? 2009", "shortCiteRegEx": "Nivre.", "year": 2009}, {"title": "Distributed Word Clustering for Large Scale ClassBased Language Modeling in Machine Translation", "author": ["Jakob Uszkoreit", "Thorsten Brants."], "venue": "ACL, pages 755\u2013762.", "citeRegEx": "Uszkoreit and Brants.,? 2008", "shortCiteRegEx": "Uszkoreit and Brants.", "year": 2008}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, pages 323\u2013333.", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Improving a statistical MT system with automatically learned rewrite patterns", "author": ["Fei Xia", "Michael McCord."], "venue": "Proceedings of COLING, page 508.", "citeRegEx": "Xia and McCord.,? 2004", "shortCiteRegEx": "Xia and McCord.", "year": 2004}, {"title": "Neural word segmentation with rich pretraining", "author": ["Jie Yang", "Yue Zhang", "Fei Dong."], "venue": "CoRR, abs/1704.08960.", "citeRegEx": "Yang et al\\.,? 2017", "shortCiteRegEx": "Yang et al\\.", "year": 2017}, {"title": "Transition-based neural word segmentation", "author": ["Meishan Zhang", "Yue Zhang", "Guohong Fu."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 421\u2013431, Berlin, Germany. Associa-", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Chinese segmentation with a word-based perceptron algorithm", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 840\u2013 847, Prague, Czech Republic. Association for Com-", "citeRegEx": "Zhang and Clark.,? 2007", "shortCiteRegEx": "Zhang and Clark.", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "For example, machine translation models have been able to attain impressive accuracies, with models that use hundreds of millions (Bahdanau et al., 2014; Wu et al., 2016) or billions (Shazeer et al.", "startOffset": 130, "endOffset": 170}, {"referenceID": 11, "context": "Long Short-Term Memory (LSTM) models (Hochreiter and Schmidhuber, 1997) have achieved good results with small memory footprints by using character-based input representations: e.", "startOffset": 37, "endOffset": 71}, {"referenceID": 7, "context": ", the part-of-speech tagging models of Gillick et al. (2016) have only roughly 900,000 parameters.", "startOffset": 39, "endOffset": 61}, {"referenceID": 7, "context": ", the part-of-speech tagging models of Gillick et al. (2016) have only roughly 900,000 parameters. Latency, however, can still be an issue with LSTMs, due to the large number of matrix multiplications they require (eight per LSTM cell): Kim and Rush (2016) report speeds of only 8.", "startOffset": 39, "endOffset": 257}, {"referenceID": 4, "context": "diction tasks, we use transition systems (Titov and Henderson, 2007, 2010) with feature embeddings as proposed by Chen and Manning (2014), and introduce two novel transition systems for the last two tasks.", "startOffset": 114, "endOffset": 138}, {"referenceID": 19, "context": "A single hidden layer, h1, with M rectified linear units (Nair and Hinton, 2010) is fully connected to h0.", "startOffset": 57, "endOffset": 80}, {"referenceID": 4, "context": "Hashed Character n-grams Previous applications of this network structure used (pretrained) word embeddings to represent words (Chen and Manning, 2014; Weiss et al., 2015).", "startOffset": 126, "endOffset": 170}, {"referenceID": 23, "context": "Hashed Character n-grams Previous applications of this network structure used (pretrained) word embeddings to represent words (Chen and Manning, 2014; Weiss et al., 2015).", "startOffset": 126, "endOffset": 170}, {"referenceID": 15, "context": "Inspired by the success of character-based representations (Ling et al., 2015), we use features defined over character n-grams instead of relying on word embeddings, and learn their embeddings from scratch.", "startOffset": 59, "endOffset": 78}, {"referenceID": 6, "context": "We use a distinct feature group g for each ngram length N , and control the size Vg directly by applying random feature mixing (Ganchev and Dredze, 2008).", "startOffset": 127, "endOffset": 153}, {"referenceID": 9, "context": "Quantization A commonly used strategy for compressing neural networks is quantization, using less precision to store parameters (Han et al., 2015).", "startOffset": 128, "endOffset": 146}, {"referenceID": 2, "context": "we use mini-batched averaged stochastic gradient descent with momentum (Bottou, 2010; Hinton, 2012) and exponentially decaying learning rates.", "startOffset": 71, "endOffset": 99}, {"referenceID": 10, "context": "we use mini-batched averaged stochastic gradient descent with momentum (Bottou, 2010; Hinton, 2012) and exponentially decaying learning rates.", "startOffset": 71, "endOffset": 99}, {"referenceID": 18, "context": "Recent shared tasks on code-switching (Molina et al., 2016) and dialects (Malmasi et al.", "startOffset": 38, "endOffset": 59}, {"referenceID": 17, "context": ", 2016) and dialects (Malmasi et al., 2016) have generated renewed interest in language identification.", "startOffset": 21, "endOffset": 43}, {"referenceID": 1, "context": "We restrict our focus to single language identification across diverse languages, and compare to the work of Baldwin and Lui (2010) on predicting the language of Wikipedia text in 66 languages.", "startOffset": 109, "endOffset": 132}, {"referenceID": 1, "context": "Table 1 shows that we outperform the lowmemory nearest-prototype model of Baldwin and Lui (2010). Their nearest neighbor model is the most accurate but its memory scales linearly with the size of the training data.", "startOffset": 74, "endOffset": 97}, {"referenceID": 7, "context": "We apply our model as an unstructured classifier to predict a POS tag for each token independently, and compare its performance to that of the byteto-span (BTS) model (Gillick et al., 2016).", "startOffset": 167, "endOffset": 189}, {"referenceID": 1, "context": "The two models from Baldwin and Lui (2010) are the nearest neighbor (NN) and nearest prototype (NP) approaches.", "startOffset": 20, "endOffset": 43}, {"referenceID": 14, "context": "known that word clusters can be powerful features in linear models for a variety of tasks (Koo et al., 2008; Turian et al., 2010).", "startOffset": 90, "endOffset": 129}, {"referenceID": 7, "context": "In order to compare against the monolingual setting of Gillick et al. (2016), we train models for the same set of 13 languages from the Universal Dependency treebanks v1.", "startOffset": 55, "endOffset": 77}, {"referenceID": 26, "context": "Recently, neural networks have significantly improved segmentation accuracy (Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016; Yang et al., 2017; Kong et al., 2015).", "startOffset": 76, "endOffset": 172}, {"referenceID": 3, "context": "Recently, neural networks have significantly improved segmentation accuracy (Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016; Yang et al., 2017; Kong et al., 2015).", "startOffset": 76, "endOffset": 172}, {"referenceID": 16, "context": "Recently, neural networks have significantly improved segmentation accuracy (Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016; Yang et al., 2017; Kong et al., 2015).", "startOffset": 76, "endOffset": 172}, {"referenceID": 25, "context": "Recently, neural networks have significantly improved segmentation accuracy (Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016; Yang et al., 2017; Kong et al., 2015).", "startOffset": 76, "endOffset": 172}, {"referenceID": 13, "context": "Recently, neural networks have significantly improved segmentation accuracy (Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016; Yang et al., 2017; Kong et al., 2015).", "startOffset": 76, "endOffset": 172}, {"referenceID": 3, "context": ", 2016; Cai and Zhao, 2016; Liu et al., 2016; Yang et al., 2017; Kong et al., 2015). We use a structured model based on the transition system in Table 3, and similar to the one proposed by Zhang and Clark (2007). We conduct the segmentation experiments on the Chinese Treebank 6.", "startOffset": 8, "endOffset": 212}, {"referenceID": 26, "context": "shown that explicitly using character bigram features leads to better accuracy (Zhang et al., 2016; Pei et al., 2014).", "startOffset": 79, "endOffset": 117}, {"referenceID": 26, "context": "shown that explicitly using character bigram features leads to better accuracy (Zhang et al., 2016; Pei et al., 2014). Zhang et al. (2016) suggests that embedding manually specified feature conjunctions further improves accuracy (\u2018Zhang et al.", "startOffset": 80, "endOffset": 139}, {"referenceID": 26, "context": "shown that explicitly using character bigram features leads to better accuracy (Zhang et al., 2016; Pei et al., 2014). Zhang et al. (2016) suggests that embedding manually specified feature conjunctions further improves accuracy (\u2018Zhang et al. (2016)-combo\u2019 in Table 4).", "startOffset": 80, "endOffset": 251}, {"referenceID": 24, "context": "Preordering source-side words into the target-side word order is a useful preprocessing task for statistical machine translation (Xia and McCord, 2004; Collins et al., 2005; Nakagawa, 2015; de Gispert et al., 2015).", "startOffset": 129, "endOffset": 214}, {"referenceID": 5, "context": "Preordering source-side words into the target-side word order is a useful preprocessing task for statistical machine translation (Xia and McCord, 2004; Collins et al., 2005; Nakagawa, 2015; de Gispert et al., 2015).", "startOffset": 129, "endOffset": 214}, {"referenceID": 20, "context": "Preordering source-side words into the target-side word order is a useful preprocessing task for statistical machine translation (Xia and McCord, 2004; Collins et al., 2005; Nakagawa, 2015; de Gispert et al., 2015).", "startOffset": 129, "endOffset": 214}, {"referenceID": 21, "context": "Inspired by a non-projective parsing transition system (Nivre, 2009), the system uses a SWAP action to permute spans.", "startOffset": 55, "endOffset": 68}, {"referenceID": 5, "context": "Preordering source-side words into the target-side word order is a useful preprocessing task for statistical machine translation (Xia and McCord, 2004; Collins et al., 2005; Nakagawa, 2015; de Gispert et al., 2015). We propose a novel transition system for this task (Table 5), so that we can repeatedly apply a small network to produce these permutations. Inspired by a non-projective parsing transition system (Nivre, 2009), the system uses a SWAP action to permute spans. The system is sound for permutations: any derivation will end with all of the input words in a permuted order, and complete: all permutations are reachable (use SHIFT and SWAP operations to perform a bubble sort, then APPEND n \u2212 1 times to form a single span). For training and evaluation, we use the English-Japanese manual word alignments from Nakagawa (2015).", "startOffset": 152, "endOffset": 837}], "year": 2017, "abstractText": "We show that small and shallow feedforward neural networks can achieve near state-of-the-art results on a range of unstructured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models. Motivated by resource-constrained environments like mobile phones, we showcase simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget.", "creator": "LaTeX with hyperref package"}}}