{"id": "1708.00415", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Aug-2017", "title": "A Generative Parser with a Discriminative Recognition Algorithm", "abstract": "Generative models defining joint distributions over parse trees and sentences are useful for parsing and language modeling, but impose restrictions on the scope of features and are often outperformed by discriminative models. We propose a framework for parsing and language modeling which marries a generative model with a discriminative recognition model in an encoder-decoder setting. We provide interpretations of the framework based on expectation maximization and variational inference, and show that it enables parsing and language modeling within a single implementation. On the English Penn Treen-bank, our framework obtains competitive performance on constituency parsing while matching the state-of-the-art single-model language modeling score.", "histories": [["v1", "Tue, 1 Aug 2017 17:02:45 GMT  (24kb)", "http://arxiv.org/abs/1708.00415v1", "ACL 2017"], ["v2", "Thu, 17 Aug 2017 13:39:14 GMT  (24kb)", "http://arxiv.org/abs/1708.00415v2", "ACL 2017"]], "COMMENTS": "ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jianpeng cheng 0001", "adam lopez", "mirella lapata"], "accepted": true, "id": "1708.00415"}, "pdf": {"name": "1708.00415.pdf", "metadata": {"source": "CRF", "title": "A Generative Parser with a Discriminative Recognition Algorithm", "authors": ["Jianpeng Cheng", "Adam Lopez"], "emails": ["jianpeng.cheng@ed.ac.uk,", "alopez@inf.ed.ac.uk", "mlap@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 170 8.00 415v 1 [cs.C L] 1A ug2 017tions over parse trees and sets are useful for parsing and language modeling, but impose limits on the scope of features and are often outperformed by discriminatory models. We propose a framework for parsing and language modeling that combines a generative model with a discriminatory recognition model in an encoder decoder setting. We offer interpretations of the framework based on expectation maximization and inference of variation and show that it enables parsing and language modeling within a single implementation. On the English Penn Treenbank, our framework achieves competitive performance in parsing constituencies while complying with state-of-the-art technology."}, {"heading": "1 Introduction", "text": "Generative models that define common distributions across parse trees and sentences are good theoretical models for interpreting natural speech data and appealing tools for tasks such as parsing, grammar induction, and speech modeling (Collins, 1999; Henderson, 2003; Titov and Henderson, 2007; Petrov and Klein, 2007; Dyer et al., 2016). However, they often impose strong assumptions of independence that restrict the use of arbitrary features for effective disamination. Furthermore, generative parsers are typically trained by maximizing the common probability of the parse tree and sentence - a goal that is only indirectly related to the goal of parsing. In the test period, these models require relatively expensive recognition algo-1Our code is available at https: / github.com / cheng6076 / virnng.git.rithm (Collins, 1999; Titov and Henderson, 2007)."}, {"heading": "2 Preliminaries", "text": "In this section, we briefly describe Recurrent Neural Network Grammars (RNNGs; Dyer et al. 2016), a top-down transition-based algorithm for parsing and generating. There are two versions of RNNG, one discriminatory, the other generative. We follow the original paper by first using the discriminatory variant. The discriminatory RNNG follows a shift reduction parser that converts a sequence of words into a parse tree. As in standard layer-reducing parsers, the RNNG uses a buffer to store unprocessed terminal symbols and a stack to store partially completed syntactical components. At each timestep, one of the following three operations2 is executed: \u2022 NT (X) passes an open non-terminal X buffer to the top of the stack, which is displayed as an open bracket, followed by X, for example (NP)."}, {"heading": "3 Methodology", "text": "Our framework combines generative and discriminatory parsers within a single training target. To illustrate, we adopt the two RNG variants presented above with our tailor-made features. Our starting point is the generative model (\u00a7 3.1), which allows us to make explicit statements about the generative process of natural linguistic sentences. As this model alone lacks a bottom-up recognition mechanism, we introduce a discriminatory recognition model (\u00a7 3.2) and link it to the generative model in an encoder decoder setting. To provide a clear interpretation of the training target (\u00a7 3.3), we first consider the parse tree as latent and the sentence as observed. Then, we discuss extensions that take marked parse trees into account. Finally, we introduce various follow-up techniques for parsing and speech modelling within the framework (\u00a7 3.4)."}, {"heading": "3.1 Decoder (Generative Model)", "text": "The decoder is a generative RNNG that models the common probability p (x, y) of a latent parse tree y and an observed sentence x. Since the parse tree is defined by a sequence of transition actions a, we write p (x, y) as p (x, a).3 The common distribution p (x, a) is factored into a sequence of transition predictions and terminal probabilities (if actions are GEN) that are parameterized by a transitional state that u: p (x, a) p (x, a) is factored into a sequence of transition predictions and terminal probabilities (if actions are GEN) (1), where I am an indicator function and represent the state embedded in the time step t. Specifically: the conditional probability of the next action is: p (at | ut) = the next action is: p (at | ut) p (d) (Gut) (d) (p) (I)."}, {"heading": "3.2 Encoder (Recognition Model)", "text": "The encoder is a discriminatory RNNG that calculates the conditional probability q (a | x) of the transition action sequence of a given record x. This conditional probability is factored in over time as follows: q (a | x) = | a | x = 1q (at | vt) (5), where vt is the transition state of embedding the encoder in the time step. The next action is predicted similarly to Equation (2), but conditionally on vt. Thanks to the discriminatory property, vt has access to all contextual characteristics defined throughout the record and the stack - q (a | x) acts as a context-sensitive subordinate approximation. Our features 4: 1) are the stack that embeds the received word with a stack LSTM that encodes the stack of the encoder and the stack; 2) the input buffer that embeds the stack is used as a cushion; we use it as a buffer to combine each word with a STM."}, {"heading": "3.3 Training", "text": "(Consider an auto encoder whose encoder guides the latent parse tree and the decoder generates the observed sentence from the parse tree. (5) The maximum probability estimate of the decoder parameters is determined by the marginal probability of the protocol: log p (x) = log \u2211 ap (x, a) (7) We use expectation maximization and variation techniques to construct a lower limit of the above quantity (due to Jensen's inequality), which is called log p (x) \u2265 Eq (a) log p (x, a) q (a) q (a) q (a) x) = Lx (8), where p (x, a) = p (x | a) p (a) comes from the decoder of the generative model, and q (a | x) comes from the encoder or detection model."}, {"heading": "3.4 Inference", "text": "Parsing In parsing, we are interested in the parse tree that maximizes the trailing p (a | x) (or the connection p (a, x). However, the decoder alone does not have a bottom-up detection mechanism for calculating the trailing p (a | x). Thanks to the encoder, we can calculate an approximate trailing q (a | x) in linear time and select the parse tree that maximizes this approximation. An alternative is to generate candidate trees using samples from q (a | x), re-classify them with respect to the connection p (x, a) (which is proportional to the true trailing x), and select the sample that maximizes the true trailing value. Language modeling Our goal is to calculate the marginal probability p (x) = a p (x, a), which is typically unpredictable."}, {"heading": "4 Related Work", "text": "Our framework is related to a class of variable autoencoders (Kingma and Welling, 2014) that use neural networks for posterior approximation in variable inferences, a technique previously used for theme modeling (Miao et al., 2016) and sentence compression7 Reminder: The language modeling target is exp (NLL / T), where NLL denotes the total negative log probability of test data and T denotes the number of tokens. (Miao and Blunsom, 2016) Another interpretation of the proposed framework is from the perspective of guided policy search in enhanced learning (Bachman and Precup, 2015), where a generative parser is trained to mimic the track of a discriminatory parser. Further connections can be made with the importance-based sample conclusion of Dyer et al. (2016), where a generative RNG and a discriminatory RNG are separately trained."}, {"heading": "5 Experiments", "text": "We conducted experiments with the English Penn Treebank dataset; we used sections 2-21 for training, 24 for validation, and 23 for testing. Following Dyer et al. (2015), we represent each word in three ways: as a learned vector, a pre-formed vector, and a POS tag vector. The encoder-word embedding is concatenating all three vectors, while the decoder uses only the first two, as we do not consider the POS tags in the generation. Table 1 presents details of the hyper parameters we use. To find the MAP parse tree argmaxa p (a, x) (where p (a, x) is used the output of q (a, x), and to calculate the linguistic perplexity (where a q (a | x))), we collect 100 samples from q (a, x) to calculate the comparison values."}, {"heading": "6 Conclusions", "text": "We have proposed a framework that integrates a generative parser with a discriminatory recognition model and demonstrated how to instantiate it with RNNGs. We have demonstrated that a unified framework based on expectation maximization and inference of variation enables effective analysis and language modeling algorithms. Evaluation at Penn Treebank in England has shown that our framework achieves competitive performance in parsing constituencies and state-of-the-art results in modelling individual models. In the future, we would like to perform grammar induction based on Equation (8), with gradient pedigree and downstream regulatory techniques (Ganchev et al., 2010). Thank you to three anonymous reviewers and ILCC members for valuable feedback and Muhua Zhu and James Cross for their help with data preparation. The support of the European Research Council under the prize number 681760 \"Translating into Multiple Modalities Text\" is gratefully acknowledged."}, {"heading": "A Comparison to Importance Sampling", "text": "(Dyer et al., 2016) In this appendix, we highlight the links between sampling meaning and variational inference q (q), comparing our method with Dyer et al. (2016). Consider a simple directed graphical model with discriminatory latent variables a (e.g., a is the transition sequence) and observed variables x (e.g., x is the sentence). Model proof, or the marginal probability p (x) = x, a) q (x, a) is often intractable calculation. Importance sampling transforms the above quantity into an expectation of a distribution q (a) that is known and easy to sample, p (x) ap (a) q (a) q (a) q (a). Importance sampling sampling transforms the above quantity into an expectation of a distribution q (a)."}], "references": [{"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Posterior regularization for structured latent variable models", "author": ["Kuzman Ganchev", "Jennifer Gillenwater", "Ben Taskar"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Ganchev et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ganchev et al\\.", "year": 2010}, {"title": "Variational inference for Bayesian mixtures of factor analysers", "author": ["Zoubin Ghahramani", "Matthew J. Beal."], "venue": "Advances in Neural Information Processing Systems, MIT Press, pages 449\u2013455.", "citeRegEx": "Ghahramani and Beal.,? 2000", "shortCiteRegEx": "Ghahramani and Beal.", "year": 2000}, {"title": "An efficient algorithm for easy-first non-directional dependency parsing", "author": ["Yoav Goldberg", "Michael Elhadad."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Goldberg and Elhadad.,? 2010", "shortCiteRegEx": "Goldberg and Elhadad.", "year": 2010}, {"title": "Inducing history representations for broad coverage statistical parsing", "author": ["James Henderson."], "venue": "Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics. Edmon-", "citeRegEx": "Henderson.,? 2003", "shortCiteRegEx": "Henderson.", "year": 2003}, {"title": "Forest reranking: Discriminative parsing with non-local features", "author": ["Liang Huang."], "venue": "Proceedings of ACL-08: HLT. Columbus, Ohio, pages 586\u2013594.", "citeRegEx": "Huang.,? 2008", "shortCiteRegEx": "Huang.", "year": 2008}, {"title": "Semi-supervised learning with deep generative models", "author": ["Diederik P Kingma", "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling."], "venue": "Advances in Neural Information Processing Systems, MIT Press, pages 3581\u20133589.", "citeRegEx": "Kingma et al\\.,? 2014", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Autoencoding variational Bayes", "author": ["Diederik P Kingma", "Max Welling."], "venue": "Proceedings of the International Conference on Learning Representations. Banff, Canada.", "citeRegEx": "Kingma and Welling.,? 2014", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "Language as a latent variable: Discrete generative models for sentence compression", "author": ["Yishu Miao", "Phil Blunsom."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas, pages 319\u2013328.", "citeRegEx": "Miao and Blunsom.,? 2016", "shortCiteRegEx": "Miao and Blunsom.", "year": 2016}, {"title": "Neural variational inference for text processing", "author": ["Yishu Miao", "Lei Yu", "Phil Blunsom."], "venue": "Proceedings of The 33rd International Conference on Machine Learning. New York, New York, USA, pages 1727\u20131736.", "citeRegEx": "Miao et al\\.,? 2016", "shortCiteRegEx": "Miao et al\\.", "year": 2016}, {"title": "Neural variational inference and learning in belief networks", "author": ["Andriy Mnih", "Karol Gregor."], "venue": "Proceedings of the 31st International Conference on Machine Learning. Beijing, China, pages 1791\u2013 1799.", "citeRegEx": "Mnih and Gregor.,? 2014", "shortCiteRegEx": "Mnih and Gregor.", "year": 2014}, {"title": "Maltparser: A language-independent system for data-driven dependency parsing", "author": ["Joakim Nivre", "Johan Hall", "Jens Nilsson", "Atanas Chanev", "G\u00fclsen Eryigit", "Sandra K\u00fcbler", "Svetoslav Marinov", "Erwin Marsi."], "venue": "Natural Language Engineering", "citeRegEx": "Nivre et al\\.,? 2007", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Doha, Qatar, pages 1532\u2013", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Improved inference for unlexicalized parsing", "author": ["Slav Petrov", "Dan Klein."], "venue": "Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Confer-", "citeRegEx": "Petrov and Klein.,? 2007", "shortCiteRegEx": "Petrov and Klein.", "year": 2007}, {"title": "Simulation and the Monte Carlo method", "author": ["Reuven Y Rubinstein", "Dirk P Kroese."], "venue": "John Wiley & Sons.", "citeRegEx": "Rubinstein and Kroese.,? 2008", "shortCiteRegEx": "Rubinstein and Kroese.", "year": 2008}, {"title": "A generative re-ranking model for dependency parsing", "author": ["Federico Sangati", "Willem Zuidema", "Rens Bod."], "venue": "Proceedings of the 11th International Conference on Parsing Technologies (IWPT\u201909). Paris, France, pages 238\u2013241.", "citeRegEx": "Sangati et al\\.,? 2009", "shortCiteRegEx": "Sangati et al\\.", "year": 2009}, {"title": "Bayesian symbol-refined tree substitution grammars for syntactic parsing", "author": ["Hiroyuki Shindo", "Yusuke Miyao", "Akinori Fujino", "Masaaki Nagata."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long", "citeRegEx": "Shindo et al\\.,? 2012", "shortCiteRegEx": "Shindo et al\\.", "year": 2012}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Ng Andrew Y."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Sofia,", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Constituent parsing with incremental sigmoid belief networks", "author": ["Ivan Titov", "James Henderson."], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Prague, Czech Republic, pages 632\u2013639.", "citeRegEx": "Titov and Henderson.,? 2007", "shortCiteRegEx": "Titov and Henderson.", "year": 2007}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems, MIT Press, pages 2773\u20132781.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Fast and accurate shiftreduce constituent parsing", "author": ["Muhua Zhu", "Yue Zhang", "Wenliang Chen", "Min Zhang", "Jingbo Zhu."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Sofia,", "citeRegEx": "Zhu et al\\.,? 2013", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": "appealing tools for tasks such as parsing, grammar induction and language modeling (Collins, 1999; Henderson, 2003; Titov and Henderson, 2007; Petrov and Klein, 2007; Dyer et al., 2016).", "startOffset": 83, "endOffset": 185}, {"referenceID": 18, "context": "appealing tools for tasks such as parsing, grammar induction and language modeling (Collins, 1999; Henderson, 2003; Titov and Henderson, 2007; Petrov and Klein, 2007; Dyer et al., 2016).", "startOffset": 83, "endOffset": 185}, {"referenceID": 13, "context": "appealing tools for tasks such as parsing, grammar induction and language modeling (Collins, 1999; Henderson, 2003; Titov and Henderson, 2007; Petrov and Klein, 2007; Dyer et al., 2016).", "startOffset": 83, "endOffset": 185}, {"referenceID": 0, "context": "appealing tools for tasks such as parsing, grammar induction and language modeling (Collins, 1999; Henderson, 2003; Titov and Henderson, 2007; Petrov and Klein, 2007; Dyer et al., 2016).", "startOffset": 83, "endOffset": 185}, {"referenceID": 18, "context": "rithm (Collins, 1999; Titov and Henderson, 2007) to recover the parse tree, but the parsing performance consistently lags behind their discriminative competitors (Nivre et al.", "startOffset": 6, "endOffset": 48}, {"referenceID": 11, "context": "rithm (Collins, 1999; Titov and Henderson, 2007) to recover the parse tree, but the parsing performance consistently lags behind their discriminative competitors (Nivre et al., 2007; Huang, 2008; Goldberg and Elhadad, 2010), which are directly trained to maximize the conditional probability of the parse tree given the sentence, where linear-time decoding algorithms exist (e.", "startOffset": 162, "endOffset": 223}, {"referenceID": 5, "context": "rithm (Collins, 1999; Titov and Henderson, 2007) to recover the parse tree, but the parsing performance consistently lags behind their discriminative competitors (Nivre et al., 2007; Huang, 2008; Goldberg and Elhadad, 2010), which are directly trained to maximize the conditional probability of the parse tree given the sentence, where linear-time decoding algorithms exist (e.", "startOffset": 162, "endOffset": 223}, {"referenceID": 3, "context": "rithm (Collins, 1999; Titov and Henderson, 2007) to recover the parse tree, but the parsing performance consistently lags behind their discriminative competitors (Nivre et al., 2007; Huang, 2008; Goldberg and Elhadad, 2010), which are directly trained to maximize the conditional probability of the parse tree given the sentence, where linear-time decoding algorithms exist (e.", "startOffset": 162, "endOffset": 223}, {"referenceID": 15, "context": "Sangati et al. (2009) follow the opposite direction and employ a generative model to re-rank the dependency trees produced by a discriminative parser.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "We showcase the framework using Recurrent Neural Network Grammars (RNNGs; Dyer et al. 2016), a recently proposed probabilistic model of phrase-structure trees based on neural transition systems.", "startOffset": 66, "endOffset": 91}, {"referenceID": 0, "context": "In this section we briefly describe Recurrent Neural Network Grammars (RNNGs; Dyer et al. 2016), a top-down transition-based algorithm for parsing and generation.", "startOffset": 70, "endOffset": 95}, {"referenceID": 0, "context": "an adaptive buffer embedding \u012bt; the latter is computed by having the stack embedding et attend Compared to Dyer et al. (2016), the new features we introduce are 3) and 4), which we found empirically useful.", "startOffset": 108, "endOffset": 127}, {"referenceID": 0, "context": "an adaptive buffer embedding \u012bt; the latter is computed by having the stack embedding et attend Compared to Dyer et al. (2016), the new features we introduce are 3) and 4), which we found empirically useful. to all remaining embeddings on the buffer with the attention function in Vinyals et al. (2015); and 4) the parent non-terminal embedding nt.", "startOffset": 108, "endOffset": 303}, {"referenceID": 8, "context": "This objective can be optimized with the methods shown in Miao and Blunsom (2016).", "startOffset": 58, "endOffset": 82}, {"referenceID": 0, "context": "See \u00a7 4 and Appendix A for comparison between this objective and the importance sampler of Dyer et al. (2016).", "startOffset": 91, "endOffset": 110}, {"referenceID": 0, "context": "Another way of computing p(x) (without lower bounding) would be to use the variational approximation q(a|x) as the proposal distribution as in the importance sampler of Dyer et al. (2016). We discuss details in Appendix A.", "startOffset": 169, "endOffset": 188}, {"referenceID": 7, "context": "Our framework is related to a class of variational autoencoders (Kingma and Welling, 2014), which use neural networks for posterior approximation in variational inference.", "startOffset": 64, "endOffset": 90}, {"referenceID": 9, "context": "This technique has been previously used for topic modeling (Miao et al., 2016) and sentence compression", "startOffset": 59, "endOffset": 78}, {"referenceID": 15, "context": "Discriminative parsers Socher et al. (2013) 90.", "startOffset": 23, "endOffset": 44}, {"referenceID": 15, "context": "Discriminative parsers Socher et al. (2013) 90.4 Zhu et al. (2013) 90.", "startOffset": 23, "endOffset": 67}, {"referenceID": 0, "context": "4 Dyer et al. (2016) 91.", "startOffset": 2, "endOffset": 21}, {"referenceID": 0, "context": "4 Dyer et al. (2016) 91.7 Cross and Huang (2016) 89.", "startOffset": 2, "endOffset": 49}, {"referenceID": 0, "context": "4 Dyer et al. (2016) 91.7 Cross and Huang (2016) 89.9 Vinyals et al. (2015) 92.", "startOffset": 2, "endOffset": 76}, {"referenceID": 12, "context": "Generative parsers Petrov and Klein (2007) 90.", "startOffset": 19, "endOffset": 43}, {"referenceID": 12, "context": "Generative parsers Petrov and Klein (2007) 90.1 Shindo et al. (2012) 92.", "startOffset": 19, "endOffset": 69}, {"referenceID": 0, "context": "4 Dyer et al. (2016) 93.", "startOffset": 2, "endOffset": 21}, {"referenceID": 8, "context": "(Miao and Blunsom, 2016).", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "Further connections can be drawn with the importance-sampling based inference of Dyer et al. (2016). There, a generative RNNG and a discriminative RNNG are trained separately; during language modeling, the output of the discriminative model serves as the proposal distribution of an importance sampler p(x) =", "startOffset": 81, "endOffset": 100}, {"referenceID": 0, "context": "Following Dyer et al. (2015), we represent each word in three ways: as a learned vector, a pretrained vector, and a POS tag vector.", "startOffset": 10, "endOffset": 29}, {"referenceID": 0, "context": "Following Dyer et al. (2015), we represent each word in three ways: as a learned vector, a pretrained vector, and a POS tag vector. The encoder word embedding is the concatenation of all three vectors while the decoder uses only the first two since we do not consider POS tags in generation. Table 1 presents details on the hyper-parameters we used. To find the MAP parse tree argmaxa p(a, x) (where p(a, x) is used rank the output of q(a|x)) and to compute the language modeling perplexity (where a \u223c q(a|x)), we collect 100 samples from q(a|x), same as Dyer et al. (2016).", "startOffset": 10, "endOffset": 574}, {"referenceID": 0, "context": "4 Dyer et al. (2016) 102.", "startOffset": 2, "endOffset": 21}, {"referenceID": 0, "context": "methods for parsing, ranking approximated MAP trees from q(a|x) with respect to p(a, x) yields a small improvement, as in Dyer et al. (2016). It is worth noting that our parsing performance lags behind Dyer et al.", "startOffset": 122, "endOffset": 141}, {"referenceID": 0, "context": "methods for parsing, ranking approximated MAP trees from q(a|x) with respect to p(a, x) yields a small improvement, as in Dyer et al. (2016). It is worth noting that our parsing performance lags behind Dyer et al. (2016). We believe this is due to implementation disparities, such as the modeling of the reduce operation.", "startOffset": 122, "endOffset": 221}, {"referenceID": 0, "context": "methods for parsing, ranking approximated MAP trees from q(a|x) with respect to p(a, x) yields a small improvement, as in Dyer et al. (2016). It is worth noting that our parsing performance lags behind Dyer et al. (2016). We believe this is due to implementation disparities, such as the modeling of the reduce operation. While Dyer et al. (2016) use an LSTM as the syntactic composition function of each subtree, we adopt a rather simple composition function based on embedding averaging, which gains computational efficiency but loses accuracy.", "startOffset": 122, "endOffset": 347}, {"referenceID": 0, "context": "methods for parsing, ranking approximated MAP trees from q(a|x) with respect to p(a, x) yields a small improvement, as in Dyer et al. (2016). It is worth noting that our parsing performance lags behind Dyer et al. (2016). We believe this is due to implementation disparities, such as the modeling of the reduce operation. While Dyer et al. (2016) use an LSTM as the syntactic composition function of each subtree, we adopt a rather simple composition function based on embedding averaging, which gains computational efficiency but loses accuracy. On language modeling, our framework achieves lower perplexity compared to Dyer et al. (2016) and baseline models.", "startOffset": 122, "endOffset": 640}, {"referenceID": 0, "context": "methods for parsing, ranking approximated MAP trees from q(a|x) with respect to p(a, x) yields a small improvement, as in Dyer et al. (2016). It is worth noting that our parsing performance lags behind Dyer et al. (2016). We believe this is due to implementation disparities, such as the modeling of the reduce operation. While Dyer et al. (2016) use an LSTM as the syntactic composition function of each subtree, we adopt a rather simple composition function based on embedding averaging, which gains computational efficiency but loses accuracy. On language modeling, our framework achieves lower perplexity compared to Dyer et al. (2016) and baseline models. This gain possibly comes from the joint optimization of both the generative and discriminative components towards a language modeling objective. However, we acknowledge a subtle difference between Dyer et al. (2016) and our approach compared to baseline language models: while the latter incrementally estimate the next word probability, our approach (and Dyer et al.", "startOffset": 122, "endOffset": 877}, {"referenceID": 0, "context": "methods for parsing, ranking approximated MAP trees from q(a|x) with respect to p(a, x) yields a small improvement, as in Dyer et al. (2016). It is worth noting that our parsing performance lags behind Dyer et al. (2016). We believe this is due to implementation disparities, such as the modeling of the reduce operation. While Dyer et al. (2016) use an LSTM as the syntactic composition function of each subtree, we adopt a rather simple composition function based on embedding averaging, which gains computational efficiency but loses accuracy. On language modeling, our framework achieves lower perplexity compared to Dyer et al. (2016) and baseline models. This gain possibly comes from the joint optimization of both the generative and discriminative components towards a language modeling objective. However, we acknowledge a subtle difference between Dyer et al. (2016) and our approach compared to baseline language models: while the latter incrementally estimate the next word probability, our approach (and Dyer et al. 2016) directly assigns probability to the entire sentence. Overall, the advantage of our framework compared to Dyer et al. (2016) is that it opens an avenue to unsupervised training.", "startOffset": 122, "endOffset": 1159}, {"referenceID": 1, "context": "Equation (8), with gradient descent and posterior regularization techniques (Ganchev et al., 2010).", "startOffset": 76, "endOffset": 98}, {"referenceID": 0, "context": "A Comparison to Importance Sampling (Dyer et al., 2016)", "startOffset": 36, "endOffset": 55}, {"referenceID": 0, "context": "In this appendix we highlight the connections between importance sampling and variational inference, thereby comparing our method with Dyer et al. (2016).", "startOffset": 135, "endOffset": 154}, {"referenceID": 14, "context": "As shown in Rubinstein and Kroese (2008), the optimal choice of the proposal distribution is in fact the true posterior p(a|x), in which", "startOffset": 12, "endOffset": 41}, {"referenceID": 0, "context": "In Dyer et al. (2016), the proposal distribution depends on x, i.", "startOffset": 3, "endOffset": 22}, {"referenceID": 5, "context": "To further support the observed variable a, we augment this objective with supervised terms shown in Equation (10), following Kingma et al. (2014) and Miao and Blunsom (2016).", "startOffset": 126, "endOffset": 147}, {"referenceID": 5, "context": "To further support the observed variable a, we augment this objective with supervised terms shown in Equation (10), following Kingma et al. (2014) and Miao and Blunsom (2016). Equation (12) can be also used to approximate the marginal likelihood p(x) (e.", "startOffset": 126, "endOffset": 175}, {"referenceID": 2, "context": "Ghahramani and Beal (2000) show that this proposal distribution leads to improved results of importance samplers.", "startOffset": 0, "endOffset": 27}], "year": 2017, "abstractText": "Generative models defining joint distributions over parse trees and sentences are useful for parsing and language modeling, but impose restrictions on the scope of features and are often outperformed by discriminative models. We propose a framework for parsing and language modeling which marries a generative model with a discriminative recognition model in an encoder-decoder setting. We provide interpretations of the framework based on expectation maximization and variational inference, and show that it enables parsing and language modeling within a single implementation. On the English Penn Treenbank, our framework obtains competitive performance on constituency parsing while matching the state-of-the-art singlemodel language modeling score.", "creator": "LaTeX with hyperref package"}}}