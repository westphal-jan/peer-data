{"id": "1708.00625", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Aug-2017", "title": "Deep Recurrent Generative Decoder for Abstractive Text Summarization", "abstract": "We propose a new framework for abstractive text summarization based on a sequence-to-sequence oriented encoder-decoder model equipped with a deep recurrent generative decoder (DRGN).", "histories": [["v1", "Wed, 2 Aug 2017 07:47:14 GMT  (413kb,D)", "http://arxiv.org/abs/1708.00625v1", "10 pages, EMNLP 2017"]], "COMMENTS": "10 pages, EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["piji li", "wai lam", "lidong bing", "zihao wang"], "accepted": true, "id": "1708.00625"}, "pdf": {"name": "1708.00625.pdf", "metadata": {"source": "CRF", "title": "Deep Recurrent Generative Decoder for Abstractive Text Summarization\u2217", "authors": ["Piji Li", "Wai Lam", "Lidong Bing", "Zihao Wang"], "emails": ["zhwang}@se.cuhk.edu.hk,", "lyndonbing@tencent.com", "@POTUS", "@POTUS"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in fact, in which they are able to live, in which they are"}, {"heading": "2 Related Works", "text": "In fact, most people who are able to move to another world will move to another world in which they will not find themselves."}, {"heading": "3 Framework Description", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Overview", "text": "As shown in Figure 2, the basic framework of our approach is a neural network-based encoder framework for sequence-to-sequence learning; the input is a variable-length sequence X = {x1, x2,.., xm} that represents the source code; the word embedding xt is randomly initialized and learned during the optimization process; the output is also a sequence Y = {y1, y2,.., yn} that represents the generated abstract summaries; Gated Recurrent Unit (GRU) (Cho et al., 2014) is used as the basic sequence modeling component for the encoder and the decoder; for latent structure modeling, we add historical dependencies to the latent variables of variational auto-encoders (VAEs); and propose a deeply recursive or decursive (GDRD) decoding component to the target coders, the abstract coding structures and the summaries."}, {"heading": "3.2 Recurrent Generative Decoder", "text": "Suppose we have performed the source text operations as follows: dt = W + W (Gated Recurrent Unit) (Cho et al., 2014)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "3.3 Abstractive Summary Generation", "text": "The encoder component and the decoder component are integrated into a uniform abstract summary. (Wxrxt \u2212 1 + br) zt (Wxzxt \u2212 Whzht \u2212 1) gt = tanh (Wxhxt \u2212 1) Whh + Whh Whh (rt ht \u2212 1) ht (rt ht \u2212 1) ht (rt ht \u2212 1) ht the reset gate, zt \u2212 2) is the reset gate. (Wxhxt \u2212 1) Whh (rt ht \u2212 1) ht (rt ht \u2212 1) ht (zt \u2212 1 \u2212 zt) gtwo the reset gate, zt \u2212 zht \u2212 2) is the reset gate. (Wxhxt \u2212 1)"}, {"heading": "3.4 Learning", "text": "Although the proposed model contains a recursive generative decoder, the entire framework is fully differentiable. As shown in Section 3.3, both the recursive deterministic decoder and the recursive generative decoder can be developed on the basis of neural networks. Therefore, all parameters in our model are optimized in a holistic paradigm using back propagation. We use {X} N and {Y} N to designate the training source and target sequence. Generally, the goal of our framework consists of two terms. One is the negative logarithality of the summaries generated, and the other is the variable lower boundary L (Dutch, Dutch; Y). Since the variable lower boundary L (Dutch, Dutch; Y) also contains a probability term, we can merge it with the probability duration of the generated summaries n.The final target function, which must be minimized, n is formulated as follows (Dutch, Dutch; Y) n (J (Dutch; Y) also contains a probability term (n) n (n) n (n), n n (n) n (n), n (n) n (n) n (n (n) n (n) n (n (n) n (n), n (n) n (n) n (n) n n (n) n (n) n (n (n) n (n) n (n (n) n) n n (n n n n n), n n (n) n n (n), n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n (n), n) n (n) n (n n (n) n) n (n (n) n (n (n) n, n) n (n) n (n (n) n (n) n (n) n, n (n) n (n (n) n) n (n) n, n (n) n, n (n) n (n) n (n) n (n, n, n (n) n) n (n, n, n (n) n, n (n) n ("}, {"heading": "4 Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datesets", "text": "Gigawords is an English sentence summary data set created on the basis of Annotated Gigawords1 by extracting the first set of articles with the heading to form a source pair. We directly download the prepared data set used in (Rush et al., 2015). It contains approximately 3.8M training pairs, 190K validation pairs, and 2,000 test pairs. DUC-20042 is another English data set used for testing only in our experiments. It contains 500 documents. Each document contains 4 expert-written model summaries. The length of the summary is limited to 75 bytes. LCSTS is a large-area Chinese short text data set consisting of pairs of (short text, abstract) collected from Sina Weibo3 (Hu et al., 2015). We take Part I as a training set, Part II as a development set, and Part III as a test set consisting of pairs of (short text, abstract) collected from Sina Weibo3 (Hu et al., 2015). We take Part I as a training set, Part III as a development set, and Part III as a set of less than a human abstract, each representing a pair of 7k and a pair of 7k."}, {"heading": "4.2 Evaluation Metrics", "text": "We use the ROUGE score (Lin, 2004) as a benchmark with standard options. The basic idea of ROUGE is to count the number of overlapping units between generated summaries and reference summaries, such as overlapping n-grams, word strings, and word pairs. F measurements of ROUGE-1 (R-1), ROUGE-2 (R-2), ROUGE-L (RL), and ROUGE-SU4 (R-SU4) are given."}, {"heading": "4.3 Comparative Methods", "text": "We compare our model with some baselines and state-of-the-art methods. Because the datasets are1https: / / catalog.ldenn.edu / ldc2012t21 2http: / / duc.nist.gov / duc2004 3http: / / www.weibo.comquite standard, so we only extract the results from their papers. Therefore, the baseline methods on different datasets may be slightly different. \u2022 TOPIARY (Zajic et al., 2004) is the best on DUC2004 Task-1 for compressing text summaries. It combines a system with linguistic-based translations and an unsupervised theme recognition algorithm for compressed texts. \u2022 MOSES + (Rush et al., 2015) uses a phrase-based statistical translation system that trains on gigaword."}, {"heading": "4.4 Experimental Settings", "text": "For the experiments with the English data set Gigawords, we set the dimension of the Word embedding to 300 and the dimension of the hidden states and latent variables to 500. The maximum length of the documents and summaries is 100 or 50. The stack size of the mini-batch training is 256. For DUC-2004, the maximum length of the summaries is 75 bytes. For the LCSTS data set, the dimension of the Word embedding is 350. In addition, we set the dimension of the hidden states and latent variables to 500. The maximum length of the documents and summaries is 120 or 25 bytes, and the stack size is also 256. The beam size of the decoder was set to 10. Adadelta (Schmidhuber, 2015) with hyperparameters between 0, 95 and 1, 6."}, {"heading": "5 Results and Discussions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 ROUGE Evaluation", "text": "First, we present the performance of our model DRGD by comparing it with the standard decoders (StanD) of our own implementation. Comparison results of the validation datasets of Gigawords and LCSTS are presented in Table 1. Results show that our proposed generative decoders DRGD can indeed achieve obvious improvements in abstract summarization compared to the standard decoders. In fact, the performance of the standard decoders is comparable to the popular base methods mentioned above. Results of the English datasets of Gigawords and DUC-2004 are shown in Table 2 and Table 3 respectively. Our model DRGD achieves the best summary performance on all ROUGE metrics. Although ASC + FSC1 also uses a generative method for modeling the latent summary variables, the representation ability is limited and cannot bring any appreciable improvements."}, {"heading": "5.2 Summary Case Analysis", "text": "To analyze the reasons for the improvement in performance, we compare the summaries generated by DRGD with the standard decoders used by StanD in some other work such as (Chopra et al., 2016).The source code, golden summaries, and generated summaries are in Table 5. Based on the cases, we can conclude that DRGD can actually capture some latent structures that match the golden summaries. For example, our result for S (1) \"Wuhan wins the men's soccer title in Chinese city games\" matches the \"Who Action What\" structure. However, the standard StanD decoder ignores the latent structures and generates some loose sentences, such as the results for S (1) \"Results of male volleyball in Chinese city games\" are not the main points. This is because the recurring variable auto encoders used in our framework have better visualization capability and can capture more effective latent structures from the sequence data."}, {"heading": "6 Conclusions", "text": "We propose a Deep Recurrent Generative Decoder (DRGD) to improve abstract summary.The model is a sequence-to-sequence encoder decoder framework with a latent structural modeling component.Abstractive summaries are built on both latent variables and deterministic states. Extensive experiments with benchmark data sets show that DRGD achieves improvements over modern methods."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We propose a new framework for abstractive text summarization based on a sequence-to-sequence oriented encoderdecoder model equipped with a deep recurrent generative decoder (DRGN). Latent structure information implied in the target summaries is learned based on a recurrent latent random model for improving the summarization quality. Neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables. Abstractive summaries are generated based on both the generative latent variables and the discriminative deterministic states. Extensive experiments on some benchmark datasets in different languages show that DRGN achieves improvements over the state-ofthe-art methods.", "creator": "LaTeX with hyperref package"}}}