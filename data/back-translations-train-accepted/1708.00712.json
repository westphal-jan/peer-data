{"id": "1708.00712", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Aug-2017", "title": "Dynamic Data Selection for Neural Machine Translation", "abstract": "Intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and translation performance for phrase-based machine translation (PBMT). With the recent increase in popularity of neural machine translation (NMT), we explore in this paper to what extent and how NMT can also benefit from data selection. While state-of-the-art data selection (Axelrod et al., 2011) consistently performs well for PBMT, we show that gains are substantially lower for NMT. Next, we introduce dynamic data selection for NMT, a method in which we vary the selected subset of training data between different training epochs. Our experiments show that the best results are achieved when applying a technique we call gradual fine-tuning, with improvements up to +2.6 BLEU over the original data selection approach and up to +3.1 BLEU over a general baseline.", "histories": [["v1", "Wed, 2 Aug 2017 11:55:57 GMT  (99kb,D)", "http://arxiv.org/abs/1708.00712v1", "Accepted at EMNLP2017"]], "COMMENTS": "Accepted at EMNLP2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marlies van der wees", "arianna bisazza", "christof monz"], "accepted": true, "id": "1708.00712"}, "pdf": {"name": "1708.00712.pdf", "metadata": {"source": "CRF", "title": "Dynamic Data Selection for Neural Machine Translation", "authors": ["Marlies van der Wees", "Arianna Bisazza", "Christof Monz"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In recent years, we have seen a rapid shift from phrase harvesters (PBMTs) to neural machine transfers (PSDs), in the way that they have occurred in recent years: in the way that they have occurred in recent years, in the way that they have occurred in recent years, in the way that they have occurred in recent years, in the way that they have occurred in recent years, in the way that they have occurred in recent years, in the way that they have occurred in recent years. \""}, {"heading": "2 Static data selection", "text": "This year it has come to the point where it will be able to put itself at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We are able to put ourselves at the top, \"he said."}, {"heading": "3 Dynamic data selection", "text": "While the data selection is aimed at discarding irrelevant data, it can also exacerbate the problem of low vocabulary and unreliable statistics for rarer words in the \"long tail\" areas, which are both important topics in NMT (Luong et al., 2017).To overcome this problem, we have introduced a dynamic data selection in which we vary the selected data subsets during training. Unlike other MT paradigms that require training data to be corrected throughout the training process, NMT iterates over the training corpus in multiple epochs, 1We use four-layered LSTMs with hidden sizes of 1.024 that we use."}, {"heading": "4 Experimental settings", "text": "We evaluate the static and dynamic data selection on a German \u2192 English translation task, which consists of four test sets. In the following, we describe the MT systems and data specifications."}, {"heading": "4.1 Machine translation systems", "text": "While the main objective of this work is to improve data selection for NMT, we also conduct comparative experiments with PBMT. Our PBMT system is an in-house system similar to Moses (Koehn et al., 2007). In order to create optimal PBMT systems given the available resources, we apply test set specific parameter tuning using PRO (Hopkins and May, 2011). In addition, we use a linear interpolated target-side language model that smoothes with KneserNey to 480M tokens of data in different areas. LM interpolation weights are also optimized per test set. In accordance with Axelrod et al. (2011), we do not vary the target-side LM between different experiments on the same test set. All ngram models in our work are 5-gramm.For our NMT experiments we use an in-house encoder decoder-3 model with global attention as in Lueong-al (2015a) and ngram-all models are."}, {"heading": "4.2 Training and evaluation data", "text": "It is indeed the case that we will be able to put ourselves at the top, in the same way as we have done in the past."}, {"heading": "5 Results", "text": "Below we discuss the results of our translation experiments with static and dynamic data selection, which measure translation quality with case-insensitive untokenized BLEU (Papineni et al., 2002)."}, {"heading": "5.1 Static data selection for PBMT and NMT", "text": "We compare the effects of static data selection with the NMT and PBMT with the different selection parameters. Specifically, we select the uppermost set pairs so that the number of selected tokens in the BLEU is sufficient for our four test sets. The advantages of n-g-based data selection for the PBMT are confirmed: In all test categories, the selection of the size, the dotted vertical line, is better than the use of the in-domain data."}, {"heading": "5.2 Dynamic data selection for NMT", "text": "Equipped with a relevance ranking of the pairs of sentences in Bitext G, we now examine two variants of dynamic data selection, as in Section 3.We are interested in reducing the training time while limiting the negative effect on BLEU for different areas. Therefore, we report on BLEU and the relative training time of each experiment. Since wall clock times depend on other factors such as NMT architecture and memory speed, we define the training time as the total number of tokens observed during the training of the NMT system, i.e., the sum of tokens in the selected sub-sets of all eras. We report on all training times relative to the training time of our complete Bittext baseline (i.e., 4.3M tokens x 16 epochs). Note that this measurement of training time closely, but not exactly, corresponds to the number of model updates, as the latter is based on the number of sentences that vary in length rather than the number of tokens in the training data."}, {"heading": "6 Further analysis", "text": "In this section we will perform some additional experiments and analyses."}, {"heading": "7 Related work", "text": "A few research topics relate to our work. In terms of data selection for SMT, the previous work has two objectives: reducing model sizes and training times, or adapting to new domains. Data selection methods for domain adaptation usually include information theory metrics to evaluate training sets according to their relevance to the domain, which has been applied monolingually (Gao et al., 2002) as well as bilingually (Yasuda et al., 2008). In more recent work, training sets are typically evaluated according to their cross-entropy difference between domain and general domain."}, {"heading": "8 Conclusions", "text": "With the recent rise in popularity of Neural Machine Translation (NMT), we have investigated in this paper to what extent and how NMT can benefit from data selection. First, we have demonstrated that a modern method of data selection provides unreliable results for NMT while consistently delivering good results for PBMT. Second, we have introduced a dynamic data selection for NMT that involves variation of the selected subset of training data between different training periods. We examined two techniques of dynamic data selection and found that our gradual fine-tuning, where we gradually reduce the training size, consistently improves over conventional static data selection (up to + 2.6 BLEU) and over a general high resource baseline (up to + 3.1 BLEU)."}, {"heading": "Acknowledgments", "text": "This research was partially funded by NWO under project numbers 639,022,213 and 639,021,646. We thank Ke Tran for providing the NMT system and the reviewers for their valuable comments."}], "references": [{"title": "Domain adaptation via pseudo in-domain data selection", "author": ["Amittai Axelrod", "Xiaodong He", "Jianfeng Gao."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 355\u2013362.", "citeRegEx": "Axelrod et al\\.,? 2011", "shortCiteRegEx": "Axelrod et al\\.", "year": 2011}, {"title": "Data selection with fewer words", "author": ["Amittai Axelrod", "Philip Resnik", "Xiaodong He", "Mari Ostendorf."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 58\u201365.", "citeRegEx": "Axelrod et al\\.,? 2015", "shortCiteRegEx": "Axelrod et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Neural versus phrasebased machine translation quality: a case study", "author": ["Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo", "Marcello Federico."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages", "citeRegEx": "Bentivogli et al\\.,? 2016", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2016}, {"title": "Findings of the 2016 conference on machine translation (WMT16)", "author": ["Ondrej Bojar", "Rajen Chatterjee", "Christian Federmann", "Yvette Graham", "Barry Haddow", "Matthias Huck", "Antonio Jimeno Yepes", "Philipp Koehn", "Varvara Logacheva", "Christof Monz"], "venue": null, "citeRegEx": "Bojar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2016}, {"title": "Wit: Web inventory of transcribed and translated talks", "author": ["Mauro Cettolo", "Christian Girardi", "Marcello Federico."], "venue": "Proceedings of the 16 Conference of the European Association for Machine Translation (EAMT), pages 261\u2013268.", "citeRegEx": "Cettolo et al\\.,? 2012", "shortCiteRegEx": "Cettolo et al\\.", "year": 2012}, {"title": "Semi-supervised convolutional networks for translation adaptation with tiny amount of in-domain data", "author": ["Boxing Chen", "Fei Huang."], "venue": "Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages", "citeRegEx": "Chen and Huang.,? 2016", "shortCiteRegEx": "Chen and Huang.", "year": 2016}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "An empirical comparison of simple domain adaptation methods for neural machine translation", "author": ["Chenhui Chu", "Raj Dabre", "Sadao Kurohashi."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Chu et al\\.,? 2017", "shortCiteRegEx": "Chu et al\\.", "year": 2017}, {"title": "SYSTRAN\u2019s pure neural machine translation systems", "author": ["Josep Crego", "Jungi Kim", "Guillaume Klein", "Anabel Rebollo", "Kathy Yang", "Jean Senellart", "Egor Akhanov", "Patrice Brunelle", "Aurelien Coquard", "Yongchao Deng"], "venue": null, "citeRegEx": "Crego et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Crego et al\\.", "year": 2016}, {"title": "Adaptation data selection using neural language models: Experiments in machine translation", "author": ["Kevin Duh", "Graham Neubig", "Katsuhito Sudoh", "Hajime Tsukada."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Duh et al\\.,? 2013", "shortCiteRegEx": "Duh et al\\.", "year": 2013}, {"title": "Low cost portability for statistical machine translation based on n-gram frequency and TF-IDF", "author": ["Matthias Eck", "Stephan Vogel", "Alex Waibel."], "venue": "Proceedings of the 2005 International Workshop on Spoken Language Translation, pages 61\u201367.", "citeRegEx": "Eck et al\\.,? 2005", "shortCiteRegEx": "Eck et al\\.", "year": 2005}, {"title": "Survey of data-selection methods in statistical machine translation", "author": ["Sauleh Eetemadi", "William Lewis", "Kristina Toutanova", "Hayder Radha."], "venue": "Machine Translation, 29(3-4):189\u2013223.", "citeRegEx": "Eetemadi et al\\.,? 2015", "shortCiteRegEx": "Eetemadi et al\\.", "year": 2015}, {"title": "Data augmentation for low-resource neural machine translation", "author": ["Marzieh Fadaee", "Arianna Bisazza", "Christof Monz."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Fadaee et al\\.,? 2017", "shortCiteRegEx": "Fadaee et al\\.", "year": 2017}, {"title": "Neural vs", "author": ["M. Amin Farajian", "Marco Turchi", "Matteo Negri", "Nicola Bertoldi", "Marcello Federico."], "venue": "phrase-based machine translation in a multi-domain scenario. In Proceedings of the 15th Conference of the European Chapter of the Association for Com-", "citeRegEx": "Farajian et al\\.,? 2017", "shortCiteRegEx": "Farajian et al\\.", "year": 2017}, {"title": "Fast domain adaptation for neural machine translation", "author": ["Markus Freitag", "Yaser Al-Onaizan."], "venue": "arXiv preprint arXiv:1612.06897.", "citeRegEx": "Freitag and Al.Onaizan.,? 2016", "shortCiteRegEx": "Freitag and Al.Onaizan.", "year": 2016}, {"title": "Toward a unified approach to statistical language modeling for chinese", "author": ["Jianfeng Gao", "Joshua Goodman", "Mingjing Li", "KaiFu Lee."], "venue": "ACM Transactions on Asian Language Information Processing (TALIP), 1(1):3\u201333.", "citeRegEx": "Gao et al\\.,? 2002", "shortCiteRegEx": "Gao et al\\.", "year": 2002}, {"title": "Does more data always yield better translations", "author": ["Guillem Gasc\u00f3", "Martha-Alicia Rocha", "Germ\u00e1n Sanchis-Trilles", "Jes\u00fas Andr\u00e9s-Ferrer", "Francisco Casacuberta"], "venue": "In Proceedings of the 13th Conference of the European Chapter of the Association", "citeRegEx": "Gasc\u00f3 et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gasc\u00f3 et al\\.", "year": 2012}, {"title": "Tuning as ranking", "author": ["Mark Hopkins", "Jonathan May."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352\u20131362.", "citeRegEx": "Hopkins and May.,? 2011", "shortCiteRegEx": "Hopkins and May.", "year": 2011}, {"title": "Neural machine translation in linear time", "author": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "Aaron van den Oord", "Alex Graves", "Koray Kavukcuoglu."], "venue": "arXiv preprint arXiv:1610.10099.", "citeRegEx": "Kalchbrenner et al\\.,? 2016", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Sequencelevel knowledge distillation", "author": ["Yoon Kim", "Alexander M. Rush."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317\u20131327.", "citeRegEx": "Kim and Rush.,? 2016", "shortCiteRegEx": "Kim and Rush.", "year": 2016}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Richard Zens", "Chris Dyer", "Ondrej Bojar", "Alexandra Constantin", "Evan Herbst."], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Demo and", "citeRegEx": "Zens et al\\.,? 2007", "shortCiteRegEx": "Zens et al\\.", "year": 2007}, {"title": "Six challenges for neural machine translation", "author": ["Philipp Koehn", "Rebecca Knowles."], "venue": "arXiv preprint arXiv:1706.03872.", "citeRegEx": "Koehn and Knowles.,? 2017", "shortCiteRegEx": "Koehn and Knowles.", "year": 2017}, {"title": "Dramatically reducing training data size through vocabulary saturation", "author": ["William D. Lewis", "Sauleh Eetemadi."], "venue": "Proceedings of the 8th Workshop on Statistical Machine Translation, pages 281\u2013291.", "citeRegEx": "Lewis and Eetemadi.,? 2013", "shortCiteRegEx": "Lewis and Eetemadi.", "year": 2013}, {"title": "Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles", "author": ["Pierre Lison", "J\u00f6rg Tiedemann."], "venue": "Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC 2016).", "citeRegEx": "Lison and Tiedemann.,? 2016", "shortCiteRegEx": "Lison and Tiedemann.", "year": 2016}, {"title": "Stanford neural machine translation systems for spoken language domains", "author": ["Minh-Thang Luong", "Christopher D Manning."], "venue": "Proceedings of the 12th International Workshop on Spoken Language Translation, pages 76\u201379.", "citeRegEx": "Luong and Manning.,? 2015", "shortCiteRegEx": "Luong and Manning.", "year": 2015}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Data selection for compact adapted SMT models", "author": ["Shachar Mirkin", "Laurent Besacier."], "venue": "Proceedings of the 11th Conference of the Association for Machine Translation in the Americas, pages 301\u2013314.", "citeRegEx": "Mirkin and Besacier.,? 2014", "shortCiteRegEx": "Mirkin and Besacier.", "year": 2014}, {"title": "Intelligent selection of language model training data", "author": ["Robert C. Moore", "William Lewis."], "venue": "Proceedings of the ACL 2010 Conference Short Papers, pages 220\u2013224.", "citeRegEx": "Moore and Lewis.,? 2010", "shortCiteRegEx": "Moore and Lewis.", "year": 2010}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Learning to select data for transfer learning with bayesian optimization", "author": ["Sebastian Ruder", "Barbara Plank."], "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Ruder and Plank.,? 2017", "shortCiteRegEx": "Ruder and Plank.", "year": 2017}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 86\u201396.", "citeRegEx": "Sennrich et al\\.,? 2016a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1715\u20131725.", "citeRegEx": "Sennrich et al\\.,? 2016b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "News from OPUS-a collection of multilingual parallel corpora with tools and interfaces", "author": ["J\u00f6rg Tiedemann."], "venue": "Recent advances in natural language processing, volume 5, pages 237\u2013248.", "citeRegEx": "Tiedemann.,? 2009", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "A multifaceted evaluation of neural versus phrasebased machine translation for 9 language directions", "author": ["Antonio Toral", "V\u0131\u0301ctor M. S\u00e1nchez-Cartagena"], "venue": "In Proceedings of the 15th Conference of the European Chapter of the Association", "citeRegEx": "Toral and S\u00e1nchez.Cartagena.,? \\Q2017\\E", "shortCiteRegEx": "Toral and S\u00e1nchez.Cartagena.", "year": 2017}, {"title": "Measuring the effect of conversational aspects on machine translation quality", "author": ["Marlies van der Wees", "Arianna Bisazza", "Christof Monz."], "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics, pages 2571\u2013", "citeRegEx": "Wees et al\\.,? 2016", "shortCiteRegEx": "Wees et al\\.", "year": 2016}, {"title": "Method of selecting training data to build a compact and efficient translation model", "author": ["Keiji Yasuda", "Ruiqiang Zhang", "Hirofumi Yamamoto", "Eiichiro Sumit."], "venue": "International Joint Conference on Natural Language Processing (IJCNLP), pages 655\u2013660.", "citeRegEx": "Yasuda et al\\.,? 2008", "shortCiteRegEx": "Yasuda et al\\.", "year": 2008}, {"title": "Boosting neural machine translation", "author": ["Dakun Zhang", "Jungi Kim", "Joseph Crego", "Jean Senellart."], "venue": "arXiv preprint arXiv:1612.06138.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Transfer learning for lowresource neural machine translation", "author": ["Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight."], "venue": "arXiv preprint arXiv:1604.02201.", "citeRegEx": "Zoph et al\\.,? 2016", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "While state-of-the-art data selection (Axelrod et al., 2011) consistently performs well for PBMT, we show that gains are substantially lower for NMT.", "startOffset": 38, "endOffset": 60}, {"referenceID": 34, "context": "Recent years have shown a rapid shift from phrase-based (PBMT) to neural machine translation (NMT) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014) as the most common machine translation paradigm.", "startOffset": 99, "endOffset": 164}, {"referenceID": 7, "context": "Recent years have shown a rapid shift from phrase-based (PBMT) to neural machine translation (NMT) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014) as the most common machine translation paradigm.", "startOffset": 99, "endOffset": 164}, {"referenceID": 2, "context": "Recent years have shown a rapid shift from phrase-based (PBMT) to neural machine translation (NMT) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014) as the most common machine translation paradigm.", "startOffset": 99, "endOffset": 164}, {"referenceID": 4, "context": "With large quantities of parallel data, NMT outperforms PBMT for an increasing number of language pairs (Bojar et al., 2016).", "startOffset": 104, "endOffset": 124}, {"referenceID": 29, "context": "In PBMT, this issue has been addressed by applying intelligent data selection, and it has consistently been shown that using more data does not always improve translation quality (Moore and Lewis, 2010; Axelrod et al., 2011; Gasc\u00f3 et al., 2012).", "startOffset": 179, "endOffset": 244}, {"referenceID": 0, "context": "In PBMT, this issue has been addressed by applying intelligent data selection, and it has consistently been shown that using more data does not always improve translation quality (Moore and Lewis, 2010; Axelrod et al., 2011; Gasc\u00f3 et al., 2012).", "startOffset": 179, "endOffset": 244}, {"referenceID": 17, "context": "In PBMT, this issue has been addressed by applying intelligent data selection, and it has consistently been shown that using more data does not always improve translation quality (Moore and Lewis, 2010; Axelrod et al., 2011; Gasc\u00f3 et al., 2012).", "startOffset": 179, "endOffset": 244}, {"referenceID": 7, "context": "While data selection has been applied to NMT to reduce the size of the data (Cho et al., 2014; Luong et al., 2015b), the effects on translation quality have not been investigated.", "startOffset": 76, "endOffset": 115}, {"referenceID": 27, "context": "While data selection has been applied to NMT to reduce the size of the data (Cho et al., 2014; Luong et al., 2015b), the effects on translation quality have not been investigated.", "startOffset": 76, "endOffset": 115}, {"referenceID": 40, "context": "1, this is a challenging task; NMT systems are known to under-perform when trained on limited parallel data (Zoph et al., 2016; Fadaee et al., 2017), and do not have a separate large-scale target-side language model to compensate for smaller parallel training data.", "startOffset": 108, "endOffset": 148}, {"referenceID": 13, "context": "1, this is a challenging task; NMT systems are known to under-perform when trained on limited parallel data (Zoph et al., 2016; Fadaee et al., 2017), and do not have a separate large-scale target-side language model to compensate for smaller parallel training data.", "startOffset": 108, "endOffset": 148}, {"referenceID": 0, "context": "(i) We compare the effects of a commonly used data selection approach (Axelrod et al., 2011) on PBMT and NMT using four different test sets.", "startOffset": 70, "endOffset": 92}, {"referenceID": 0, "context": "As a first step towards dynamic data selection for NMT, we compare the effects of a commonly used, state-of-the-art data selection method (Axelrod et al., 2011) on both neural and phrase-based MT.", "startOffset": 138, "endOffset": 160}, {"referenceID": 0, "context": ", have high HG, we compute for each sentence pair s the bilingual cross-entropy difference CEDs following Axelrod et al. (2011):", "startOffset": 106, "endOffset": 128}, {"referenceID": 29, "context": "Following related work by Moore and Lewis (2010), we restrict the vocabulary of the LMs to the words occurring at least twice in the in-domain corpus.", "startOffset": 26, "endOffset": 49}, {"referenceID": 27, "context": "While data selection aims to discard irrelevant data, it can also exacerbate the problem of low vocabulary coverage and unreliable statistics for rarer words in the \u2018long tail\u2019, which are major issues in NMT (Luong et al., 2015b; Sennrich et al., 2016b).", "startOffset": 208, "endOffset": 253}, {"referenceID": 33, "context": "While data selection aims to discard irrelevant data, it can also exacerbate the problem of low vocabulary coverage and unreliable statistics for rarer words in the \u2018long tail\u2019, which are major issues in NMT (Luong et al., 2015b; Sennrich et al., 2016b).", "startOffset": 208, "endOffset": 253}, {"referenceID": 40, "context": "In addition, it has been shown that NMT performance drops tremendously in low-resource scenarios (Zoph et al., 2016; Fadaee et al., 2017; Koehn and Knowles, 2017).", "startOffset": 97, "endOffset": 162}, {"referenceID": 13, "context": "In addition, it has been shown that NMT performance drops tremendously in low-resource scenarios (Zoph et al., 2016; Fadaee et al., 2017; Koehn and Knowles, 2017).", "startOffset": 97, "endOffset": 162}, {"referenceID": 22, "context": "In addition, it has been shown that NMT performance drops tremendously in low-resource scenarios (Zoph et al., 2016; Fadaee et al., 2017; Koehn and Knowles, 2017).", "startOffset": 97, "endOffset": 162}, {"referenceID": 23, "context": "(2011), dynamic data selection can also be applied using other ranking criteria, for example limiting redundancy in the training data (Lewis and Eetemadi, 2013) or complementing similarity with diversity (Ruder and Plank, 2017).", "startOffset": 134, "endOffset": 160}, {"referenceID": 31, "context": "(2011), dynamic data selection can also be applied using other ranking criteria, for example limiting redundancy in the training data (Lewis and Eetemadi, 2013) or complementing similarity with diversity (Ruder and Plank, 2017).", "startOffset": 204, "endOffset": 227}, {"referenceID": 0, "context": "While we use in this work a domain-relevance ranking of the bitext following Axelrod et al. (2011), dynamic data selection can also be applied using other ranking criteria, for example limiting redundancy in the training data (Lewis and Eetemadi, 2013) or complementing similarity with diversity (Ruder and Plank, 2017).", "startOffset": 77, "endOffset": 99}, {"referenceID": 25, "context": "Gradual fine-tuning The second dynamic data selection technique, see Figure 1b, is inspired by the success of domain-specific fine-tuning (Luong and Manning, 2015; Zoph et al., 2016; Sennrich et al., 2016a; Freitag and Al-Onaizan, 2016), in which a model trained on a large general-domain bitext is trained for a few additional epochs only on small in-domain data.", "startOffset": 138, "endOffset": 236}, {"referenceID": 40, "context": "Gradual fine-tuning The second dynamic data selection technique, see Figure 1b, is inspired by the success of domain-specific fine-tuning (Luong and Manning, 2015; Zoph et al., 2016; Sennrich et al., 2016a; Freitag and Al-Onaizan, 2016), in which a model trained on a large general-domain bitext is trained for a few additional epochs only on small in-domain data.", "startOffset": 138, "endOffset": 236}, {"referenceID": 32, "context": "Gradual fine-tuning The second dynamic data selection technique, see Figure 1b, is inspired by the success of domain-specific fine-tuning (Luong and Manning, 2015; Zoph et al., 2016; Sennrich et al., 2016a; Freitag and Al-Onaizan, 2016), in which a model trained on a large general-domain bitext is trained for a few additional epochs only on small in-domain data.", "startOffset": 138, "endOffset": 236}, {"referenceID": 15, "context": "Gradual fine-tuning The second dynamic data selection technique, see Figure 1b, is inspired by the success of domain-specific fine-tuning (Luong and Manning, 2015; Zoph et al., 2016; Sennrich et al., 2016a; Freitag and Al-Onaizan, 2016), in which a model trained on a large general-domain bitext is trained for a few additional epochs only on small in-domain data.", "startOffset": 138, "endOffset": 236}, {"referenceID": 18, "context": "To create optimal PBMT systems given the available resources, we apply test-set-specific parameter tuning using PRO (Hopkins and May, 2011).", "startOffset": 116, "endOffset": 139}, {"referenceID": 0, "context": "Consistent with Axelrod et al. (2011), we do not vary the target-side LM between different experiments on the same test set.", "startOffset": 16, "endOffset": 38}, {"referenceID": 0, "context": "Consistent with Axelrod et al. (2011), we do not vary the target-side LM between different experiments on the same test set. All ngram models in our work are 5-gram. For our NMT experiments we use an in-house encoder-decoder3 model with global attention as described in Luong et al. (2015a). This choice comes at the cost of optimal translation quality but allows for a relatively fast realization of largescale experiments given our available resources.", "startOffset": 16, "endOffset": 291}, {"referenceID": 35, "context": "We evaluate all experiments on four domains: (i) EMEA medical guidelines (Tiedemann, 2009), (ii) movie dialogues (van der Wees et al.", "startOffset": 73, "endOffset": 90}, {"referenceID": 24, "context": ", 2016) constructed from OpenSubtitles (Lison and Tiedemann, 2016), (iii) TED talks (Cettolo et al.", "startOffset": 39, "endOffset": 66}, {"referenceID": 5, "context": ", 2016) constructed from OpenSubtitles (Lison and Tiedemann, 2016), (iii) TED talks (Cettolo et al., 2012), and (iv) WMT news.", "startOffset": 84, "endOffset": 106}, {"referenceID": 32, "context": "After selection, we apply Byte-pair encoding (BPE, Sennrich et al. (2016b)) with 40K merge operations on either side of the complete mix-of-domains training bitext.", "startOffset": 51, "endOffset": 75}, {"referenceID": 30, "context": "Below we discuss the results of our translation experiments using static and dynamic data selection, measuring translation quality with case-insensitive untokenized BLEU (Papineni et al., 2002).", "startOffset": 170, "endOffset": 193}, {"referenceID": 10, "context": "For PBMT, similar results have been reported when replacing n-gram LMs with recurrent neural LMs (Duh et al., 2013).", "startOffset": 97, "endOffset": 115}, {"referenceID": 16, "context": "This has been applied monolingually (Gao et al., 2002) as well as bilingually (Yasuda et al.", "startOffset": 36, "endOffset": 54}, {"referenceID": 38, "context": ", 2002) as well as bilingually (Yasuda et al., 2008).", "startOffset": 31, "endOffset": 52}, {"referenceID": 29, "context": "In more recent work, training sentences are typically ranked according to their cross-entropy difference between in-domain and general-domain data (Moore and Lewis, 2010; Axelrod et al., 2011, 2015), favoring sentences that are similar to the test domain and at the same time dissimilar from the general domain.", "startOffset": 147, "endOffset": 198}, {"referenceID": 0, "context": "In more recent work, training sentences are typically ranked according to their cross-entropy difference between in-domain and general-domain data (Moore and Lewis, 2010; Axelrod et al., 2011, 2015), favoring sentences that are similar to the test domain and at the same time dissimilar from the general domain. Duh et al. (2013) and Chen and Huang (2016) present similar methods in which n-gram LMs are replaced by neural LMs or neural classifiers, respectively.", "startOffset": 171, "endOffset": 330}, {"referenceID": 0, "context": "In more recent work, training sentences are typically ranked according to their cross-entropy difference between in-domain and general-domain data (Moore and Lewis, 2010; Axelrod et al., 2011, 2015), favoring sentences that are similar to the test domain and at the same time dissimilar from the general domain. Duh et al. (2013) and Chen and Huang (2016) present similar methods in which n-gram LMs are replaced by neural LMs or neural classifiers, respectively.", "startOffset": 171, "endOffset": 356}, {"referenceID": 11, "context": "minimum amount of data while still maintaining high vocabulary coverage (Eck et al., 2005; Gasc\u00f3 et al., 2012; Lewis and Eetemadi, 2013).", "startOffset": 72, "endOffset": 136}, {"referenceID": 17, "context": "minimum amount of data while still maintaining high vocabulary coverage (Eck et al., 2005; Gasc\u00f3 et al., 2012; Lewis and Eetemadi, 2013).", "startOffset": 72, "endOffset": 136}, {"referenceID": 23, "context": "minimum amount of data while still maintaining high vocabulary coverage (Eck et al., 2005; Gasc\u00f3 et al., 2012; Lewis and Eetemadi, 2013).", "startOffset": 72, "endOffset": 136}, {"referenceID": 11, "context": "minimum amount of data while still maintaining high vocabulary coverage (Eck et al., 2005; Gasc\u00f3 et al., 2012; Lewis and Eetemadi, 2013). In a comparative study, Mirkin and Besacier (2014) find that similarity-objected methods perform best if the test domain and general corpus are very different, while a coverage-objected method is superior if test and general corpus are relatively similar.", "startOffset": 73, "endOffset": 189}, {"referenceID": 11, "context": "minimum amount of data while still maintaining high vocabulary coverage (Eck et al., 2005; Gasc\u00f3 et al., 2012; Lewis and Eetemadi, 2013). In a comparative study, Mirkin and Besacier (2014) find that similarity-objected methods perform best if the test domain and general corpus are very different, while a coverage-objected method is superior if test and general corpus are relatively similar. A comprehensive survey on data selection for SMT is provided by Eetemadi et al. (2015). While in this work we have used a similarity objective to rank our bitext, one could also apply dynamic data selection using a coverage objective.", "startOffset": 73, "endOffset": 481}, {"referenceID": 25, "context": "Domain adaptation in NMT typically involves training a model on the complete bitext, followed by fine-tuning the parameters on a smaller in-domain corpus (Luong and Manning, 2015; Zoph et al., 2016).", "startOffset": 154, "endOffset": 198}, {"referenceID": 40, "context": "Domain adaptation in NMT typically involves training a model on the complete bitext, followed by fine-tuning the parameters on a smaller in-domain corpus (Luong and Manning, 2015; Zoph et al., 2016).", "startOffset": 154, "endOffset": 198}, {"referenceID": 8, "context": "Other work combines fine-tuning with model ensembles (Freitag and AlOnaizan, 2016) or with domain-specific tags in the training corpus (Chu et al., 2017).", "startOffset": 135, "endOffset": 153}, {"referenceID": 8, "context": "Other work combines fine-tuning with model ensembles (Freitag and AlOnaizan, 2016) or with domain-specific tags in the training corpus (Chu et al., 2017). Finally, Sennrich et al. (2016a) adapt their systems by backtranslating in-domain data, which is then added to the training data and used for fine-tuning.", "startOffset": 136, "endOffset": 188}, {"referenceID": 19, "context": ", 2016), modifying the NMT network structure (Kalchbrenner et al., 2016), decreasing the number of parameters through knowledge distillation (Crego et al.", "startOffset": 45, "endOffset": 72}, {"referenceID": 9, "context": ", 2016), decreasing the number of parameters through knowledge distillation (Crego et al., 2016; Kim and Rush, 2016), or by boosting parts of the data that are \u2018challenging\u2019 to the NMT system (Zhang et al.", "startOffset": 76, "endOffset": 116}, {"referenceID": 20, "context": ", 2016), decreasing the number of parameters through knowledge distillation (Crego et al., 2016; Kim and Rush, 2016), or by boosting parts of the data that are \u2018challenging\u2019 to the NMT system (Zhang et al.", "startOffset": 76, "endOffset": 116}, {"referenceID": 39, "context": ", 2016; Kim and Rush, 2016), or by boosting parts of the data that are \u2018challenging\u2019 to the NMT system (Zhang et al., 2016).", "startOffset": 103, "endOffset": 123}, {"referenceID": 3, "context": "Finally, recent work comparing various aspects for PBMT and NMT includes (Bentivogli et al., 2016; Farajian et al., 2017; Toral and S\u00e1nchezCartagena, 2017; Koehn and Knowles, 2017).", "startOffset": 73, "endOffset": 180}, {"referenceID": 14, "context": "Finally, recent work comparing various aspects for PBMT and NMT includes (Bentivogli et al., 2016; Farajian et al., 2017; Toral and S\u00e1nchezCartagena, 2017; Koehn and Knowles, 2017).", "startOffset": 73, "endOffset": 180}, {"referenceID": 22, "context": "Finally, recent work comparing various aspects for PBMT and NMT includes (Bentivogli et al., 2016; Farajian et al., 2017; Toral and S\u00e1nchezCartagena, 2017; Koehn and Knowles, 2017).", "startOffset": 73, "endOffset": 180}], "year": 2017, "abstractText": "Intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and translation performance for phrase-based machine translation (PBMT). With the recent increase in popularity of neural machine translation (NMT), we explore in this paper to what extent and how NMT can also benefit from data selection. While state-of-the-art data selection (Axelrod et al., 2011) consistently performs well for PBMT, we show that gains are substantially lower for NMT. Next, we introduce dynamic data selection for NMT, a method in which we vary the selected subset of training data between different training epochs. Our experiments show that the best results are achieved when applying a technique we call gradual fine-tuning, with improvements up to +2.6 BLEU over the original data selection approach and up to +3.1 BLEU over a general baseline.", "creator": "LaTeX with hyperref package"}}}