{"id": "1708.00781", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Aug-2017", "title": "Dynamic Entity Representations in Neural Language Models", "abstract": "Understanding a long document requires tracking how entities are introduced and evolve over time. We present a new type of language model, EntityNLM, that can explicitly model entities, dynamically update their representations, and contextually generate their mentions. Our model is generative and flexible; it can model an arbitrary number of entities in context while generating each entity mention at an arbitrary length. In addition, it can be used for several different tasks such as language modeling, coreference resolution, and entity prediction. Experimental results with all these tasks demonstrate that our model consistently outperforms strong baselines and prior work.", "histories": [["v1", "Wed, 2 Aug 2017 14:49:03 GMT  (595kb)", "http://arxiv.org/abs/1708.00781v1", "EMNLP 2017 camera-ready version"]], "COMMENTS": "EMNLP 2017 camera-ready version", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["yangfeng ji", "chenhao tan", "sebastian martschat", "yejin choi", "noah a smith"], "accepted": true, "id": "1708.00781"}, "pdf": {"name": "1708.00781.pdf", "metadata": {"source": "CRF", "title": "Dynamic Entity Representations in Neural Language Models", "authors": ["Yangfeng Ji", "Chenhao Tan", "Sebastian Martschat", "Yejin Choi", "Noah A. Smith"], "emails": ["yangfeng@cs.washington.edu", "chenhao@cs.washington.edu", "yejin@cs.washington.edu", "nasmith@cs.washington.edu", "martschat@cl.uni-heidelberg.de"], "sections": [{"heading": null, "text": "ar Xiv: 170 8.00 781v 1 [cs.C L] 2A ug2 017tracking how entities are introduced and developed over time. We present a new language model, ENTITYNLM, which can explicitly model entities, dynamically update their representations, and contextually generate their mentions. Our model is generative and flexible; it can model any number of entities in context, generating any entity mention at any length. Furthermore, it can be used for various tasks such as voice modeling, co-conference resolution, and entity prediction. Experimental results with all these tasks show that our model consistently outperforms strong baselines and preparatory work."}, {"heading": "1 Introduction", "text": "To understand a narrative, you have to trace your participants across a long-term context. As a story unfolds, the information a reader associates with each character in a story increases, and expectations of what will happen next. Currently, models of natural language do not explicitly trace entities; in fact, entities in today's language models are no more than the words used to mention them. In this paper, we give a generative language model the ability to build a dynamic representation of each entity mentioned in the text. Our language model defines a probability distribution across the entire text, with a unique generative history for entity mentions. It groups these mentions explicitly and associates with each entity a continuous representation that is updated by each contextualized mention of the entity, which in turn influences the text that follows."}, {"heading": "2 Model", "text": "A language model defines distribution over sequences of word marks; let Xt denote the random variable for the dead word in the sequence, xt the value of Xt, and xt the distributed representation (embedding) of that word. Our starting point for speech modeling is a recursive neural network (Mikolov et al., 2010) that defines (along with word embedding xt) = Softmax (Whht \u2212 1 + b) (1) ht \u2212 1 = LSTM (ht \u2212 2, xt \u2212 1) (2), where Wh and b are parameters of the model (along with word embedding xt), LSTM is the widely used recurring function known as \"long-term memory\" (Hochreiter and Schmidhuber, 1997), and ht is a hidden state of the LSTM that encodes the history of the sequence down to the tallest word."}, {"heading": "2.1 Additional random variables and representations for entities", "text": "To introduce our model, we associate with each word an additional set of random variables. At position t, \u2022 Rt is a binary random variable that indicates whether xt belongs to a unit (Rt = 1) or not (Rt = 0). Although this is not examined here, it is easy to generalize to a categorical variable for the type of unit (e.g. person, organization, etc.). \u2022 Lt = 1,... \"max\" is a categorical random variable when Rt = 1 indicates the number of remaining words in that mention, with the current word (i.e. Lt = 1 for the last word in any mention) included. \"max\" is a predefined maximum length that is set to 25, which is an empirical value derived from the training company used in the experiments. \"IfRt = 0, then Lt = 1. We denote the value of Lt according to\" Et. \""}, {"heading": "2.2 Generative story", "text": "The generative history for the word (and other variables) in timestep t is as follows: \u2022 Select rt (Eq.3). \u2022 If rt = 0, set t = 1 and et = \u00f8; then go to step 3. Otherwise: - If there is no embedding for the new candidate unit with index 1 + maxt \"< t et,\" create one according to \u00a7 2.4. - Select the unit et from {1,.., 1 + maxt \u2032 < t \u00b2 (Eq.4). - Set ecurrent = eet, t \u2212 1, which is the embedding of the unit et before timestep. - Select the unit et from {1,.., 1 + maxt \u2032 t \u00b2 \"(Eq.4)."}, {"heading": "2.3 Probability distributions", "text": "In fact, it is so that most of them will be able to be able to be able to be able to be. (...) Most of them are not able to be able to move. (...) Most of them are not able to be able to be able to move. (...) Most of them are not able to be able to move. (...) Most of them are not able to put themselves in a position. (...) Most of them are not able to put themselves in a position. (...) Most of them are not able to put themselves in a position. (...) Most of them are not able to move. (...) Most of them are not able to put themselves in a position. (...) Most of them are not able to put themselves in a position. \"(...) Most of them will not be able to be able to be able to move. (...)"}, {"heading": "2.4 Dynamic entity representations", "text": "Before predicting the entity in step t, we need an embedding for the new candidate unit with index e \"= 1 + maxed\" < t \"and\" if it does not exist. \"The new embedding is randomly generated according to a normal distribution and then projected onto the unit sphere: u\" N (r1, \u03c3 2 I); ee, \"t\" 1 = u \"2, (7), where \u03c3 = 0.01. The time step t \u2212 1 in ee,\" t \"1 means that the current embedding does not contain information from step t, although it is updated as soon as we press and when Et = e.\" r1 is the parameterized embedding for Rt = 1, which is optimized along with other parameters and which is expected to encode some generic information about entities. All initial embedding of the entity is centered on the mean r1, which is used in equation 3, to determine whether the next one belongs to a death."}, {"heading": "2.5 Training objective", "text": "The model is designed to maximize the log of joint probability of R, E, L and X: (\u03b8) = log P (R, E, L, X; \u03b8) = \u2211 tlogP (Rt, Et, Lt, Xt; \u03b8), (9) where \u03b8 is the collection of all parameters in this model. Based on the formulation in \u00a7 2.3, Eq.9 can be broken down as the sum of the conditional log probabilities of each random variable in each time step. This goal requires the training data described in Figure 2. We do not assume that these variables will be observed at the test date."}, {"heading": "3 Implementation Details", "text": "Our model is implemented with DyNet (Neubig et al., 2017) and is available at https: / / github.com / jiyfeng / entitynlm. We use AdaGrad (Duchi et al., 2011) with learning rate \u03bb = 0.1 and ADAM (Kingma and Ba, 2014) with default learning rate \u03bb = 0.001 as candidate optimizer of our model. For all parameters, we use the initialization tricks recommended by Glorot and Bengio (2010). To avoid overadjustment, we also use dropouts (Srivastava et al., 2014) with candidate rates {0.2, 0.5}. In addition, there are two tunable hyperparameters of ENTITYNLM: the size of the word embedding and the dimension of the LSTM hidden states. For both, we consider the values {32, 48, 64, 128, 256}. We also experiment with the option of using either the pre-defined word states and the hidden states (during the STM) to best use the parameters of the inserted in the training states."}, {"heading": "4 Evaluation Tasks and Datasets", "text": "We evaluate our model in various use scenarios: (i) speech modeling, (ii) nuclear resolution, and (iii) entity prediction. The evaluation of voice modeling shows how internal entity prediction, when marginalized, can improve the perplexity of speech models.) The evaluation of the entity prediction shows how to improve a competing entity resolution system."}, {"heading": "5 Experiments", "text": "In this section we present the experimental results of the three evaluation tasks."}, {"heading": "5.1 Language modeling", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "5.2 Coreference reranking", "text": "This year it has come to the point that it is a purely reactionary, reactionary, reactionary, reactionary and reactionary project."}, {"heading": "5.3 Entity prediction", "text": "Task description. Based on Modi et al. (2017), we are introducing a novel prediction task that attempts to predict the next entity based on the previous text. For a given text such as Figure 3, this task makes a prediction based only on the left context, which differs from the co-reference resolution, where both left and right contexts of a given entity are used in decoding. It is also different from language modelling, as this task requires only the prediction of entities. Since ENTITYNLM is generative, it can be applied directly to this task. To predict entities in the test data, Rt is always given and ENTITYNLM only needs to be predicted Et if Rt = 1 baselines and human prediction. We are introducing two baselines in this task: (i) the ever new baseline that predicts a new entity, the ever new entity that always predicts a new entity."}, {"heading": "6 Related Work", "text": "In fact, most people who are able to survive themselves are not able to survive themselves; most of them, who are able to survive themselves, are able to survive themselves; most of them, who are able to survive themselves, are able to survive themselves; most of them, who are able to survive themselves, are able to survive themselves; most of them, who are not able to survive themselves; most of them, who are able to survive themselves; most of them, who are able to survive themselves; most of them, who are able to survive themselves; most of them, who are not able to survive themselves; and most of them, who are not able to survive themselves."}, {"heading": "7 Conclusion", "text": "We have introduced a neural language model, ENTITYNLM, which defines a distribution across texts and the entities mentioned. It provides vector representations for the entities and dynamically updates them in context. Dynamic representations are further used to generate specific entity mentions and the following text. This model surpasses strong baselines and previous work on three tasks: speech modeling, correlation resolution and entity prediction."}, {"heading": "Acknowledgments", "text": "We thank anonymous reviewers for their helpful feedback on this work. We also thank the members of Noah's ARK and XLab at the University of Washington for their valuable comments, especially Eunsol Choi for pointing out the InScript corpus. This research was supported in part by an Innovation Award from the University of Washington, Samsung GRO, NSF funding IIS-1524371, the DARPA CwCprogram by ARO (W911NF-15-1-0543) and gifts from Google and Facebook."}], "references": [{"title": "Algorithms for scoring coreference chains", "author": ["Amit Bagga", "Breck Baldwin."], "venue": "LREC Workshop on Linguistic Coreference.", "citeRegEx": "Bagga and Baldwin.,? 1998", "shortCiteRegEx": "Bagga and Baldwin.", "year": 1998}, {"title": "Pragmatic neural language modelling in machine translation", "author": ["Paul Baltescu", "Phil Blunsom."], "venue": "NAACL.", "citeRegEx": "Baltescu and Blunsom.,? 2015", "shortCiteRegEx": "Baltescu and Blunsom.", "year": 2015}, {"title": "Learning structured perceptrons for coreference resolution with latent antecedents and non-local features", "author": ["Anders Bj\u00f6rkelund", "Jonas Kuhn."], "venue": "ACL.", "citeRegEx": "Bj\u00f6rkelund and Kuhn.,? 2014", "shortCiteRegEx": "Bj\u00f6rkelund and Kuhn.", "year": 2014}, {"title": "Unsupervised Learning of Narrative Event Chains", "author": ["Nathanael Chambers", "Daniel Jurafsky."], "venue": "ACL.", "citeRegEx": "Chambers and Jurafsky.,? 2008", "shortCiteRegEx": "Chambers and Jurafsky.", "year": 2008}, {"title": "Deep reinforcement learning for mention-ranking coreference models", "author": ["Kevin Clark", "Christopher D. Manning."], "venue": "EMNLP.", "citeRegEx": "Clark and Manning.,? 2016a", "shortCiteRegEx": "Clark and Manning.", "year": 2016}, {"title": "Improving coreference resolution by learning entitylevel distributed representations", "author": ["Kevin Clark", "Christopher D. Manning."], "venue": "ACL.", "citeRegEx": "Clark and Manning.,? 2016b", "shortCiteRegEx": "Clark and Manning.", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith."], "venue": "EMNLP.", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "AISTATS, pages 249\u2013256.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Classes for fast maximum entropy training", "author": ["Joshua Goodman."], "venue": "ICASSP.", "citeRegEx": "Goodman.,? 2001", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "Coreference resolution in a modular, entity-centered model", "author": ["Aria Haghighi", "Dan Klein."], "venue": "NAACL.", "citeRegEx": "Haghighi and Klein.,? 2010", "shortCiteRegEx": "Haghighi and Klein.", "year": 2010}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn."], "venue": "ACL.", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Tracking the world state with recurrent entity networks", "author": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun."], "venue": "arXiv:1612.03969.", "citeRegEx": "Henaff et al\\.,? 2016", "shortCiteRegEx": "Henaff et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Document context language models", "author": ["Yangfeng Ji", "Trevor Cohn", "LingpengKong", "Chris Dyer", "Jacob Eisenstein."], "venue": "ICLR (workshop track).", "citeRegEx": "Ji et al\\.,? 2016a", "shortCiteRegEx": "Ji et al\\.", "year": 2016}, {"title": "A latent variable recurrent neural network for discourse-driven language models", "author": ["Yangfeng Ji", "Gholamreza Haffari", "Jacob Eisenstein."], "venue": "NAACL-HLT.", "citeRegEx": "Ji et al\\.,? 2016b", "shortCiteRegEx": "Ji et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "On coreference resolution performance metrics", "author": ["Xiaoqiang Luo."], "venue": "HLT-EMNLP.", "citeRegEx": "Luo.,? 2005", "shortCiteRegEx": "Luo.", "year": 2005}, {"title": "A mentionsynchronous coreference resolution algorithm based on the Bell tree", "author": ["Xiaoqiang Luo", "Abe Ittycheriah", "Hongyan Jing", "Nanda Kambhatla", "Salim Roukos."], "venue": "ACL.", "citeRegEx": "Luo et al\\.,? 2004", "shortCiteRegEx": "Luo et al\\.", "year": 2004}, {"title": "Latent structures for coreference resolution", "author": ["Sebastian Martschat", "Michael Strube."], "venue": "Transactions of the Association for Computational Linguistics, 3:405\u2013418.", "citeRegEx": "Martschat and Strube.,? 2015", "shortCiteRegEx": "Martschat and Strube.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Modeling semantic expectation: Using script knowledge for referent prediction", "author": ["Ashutosh Modi", "Ivan Titov", "Vera Demberg", "Asad Sayeed", "Manfred Pinkal."], "venue": "Transactions of the Association of Computational Linguistics, 5:31\u201344.", "citeRegEx": "Modi et al\\.,? 2017", "shortCiteRegEx": "Modi et al\\.", "year": 2017}, {"title": "A corpus and evaluation framework for deeper understanding of commonsense stories", "author": ["Nasrin Mostafazadeh", "Nathanael Chambers", "Xiaodong He", "Devi Parikh", "Dhruv Batra", "Lucy Vanderwende", "Pushmeet Kohli", "James Allen."], "venue": "NAACL.", "citeRegEx": "Mostafazadeh et al\\.,? 2016", "shortCiteRegEx": "Mostafazadeh et al\\.", "year": 2016}, {"title": "Dynet: The dynamic neural network toolkit", "author": ["Graham Neubig", "Chris Dyer", "Yoav Goldberg", "Austin Matthews", "Waleed Ammar", "Antonios Anastasopoulos", "Miguel Ballesteros", "David Chiang", "Daniel Clothiaux", "Trevor Cohn"], "venue": null, "citeRegEx": "Neubig et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Neubig et al\\.", "year": 2017}, {"title": "Machine learning for coreference resolution: From local classification to global ranking", "author": ["Vincent Ng."], "venue": "ACL.", "citeRegEx": "Ng.,? 2005", "shortCiteRegEx": "Ng.", "year": 2005}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Exchangeable and partially exchangeable random partitions", "author": ["Jim Pitman."], "venue": "Probability Theory and Related Fields, 102(2):145\u2013158.", "citeRegEx": "Pitman.,? 1995", "shortCiteRegEx": "Pitman.", "year": 1995}, {"title": "Scoring coreference partitions of predicted mentions: A reference implementation", "author": ["Sameer Pradhan", "Xiaoqiang Luo", "Marta Recasens", "Eduard Hovy", "Vincent Ng", "Michael Strube."], "venue": "ACL.", "citeRegEx": "Pradhan et al\\.,? 2014", "shortCiteRegEx": "Pradhan et al\\.", "year": 2014}, {"title": "CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes", "author": ["Sameer Pradhan", "Alessandro Moschitti", "Nianwen Xue", "Olga Uryupina", "Yuchen Zhang."], "venue": "EMNLPCoNLL.", "citeRegEx": "Pradhan et al\\.,? 2012", "shortCiteRegEx": "Pradhan et al\\.", "year": 2012}, {"title": "Discriminative reranking for machine translation", "author": ["Libin Shen", "Anoop Sarkar", "Franz Josef Och."], "venue": "NAACL.", "citeRegEx": "Shen et al\\.,? 2004", "shortCiteRegEx": "Shen et al\\.", "year": 2004}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Recurrent memory networks for languagemodeling", "author": ["Ke Tran", "Arianna Bisazza", "Christof Monz."], "venue": "NAACL-HLT.", "citeRegEx": "Tran et al\\.,? 2016", "shortCiteRegEx": "Tran et al\\.", "year": 2016}, {"title": "A modeltheoretic coreference scoring scheme", "author": ["Marc Vilain", "John Burger", "John Aberdeen", "Dennis Connolly", "Lynette Hirschman."], "venue": "MUC.", "citeRegEx": "Vilain et al\\.,? 1995", "shortCiteRegEx": "Vilain et al\\.", "year": 1995}, {"title": "Learning global features for coreference resolution", "author": ["Sam Wiseman", "Alexander M. Rush", "Stuart M. Shieber."], "venue": "NAACL.", "citeRegEx": "Wiseman et al\\.,? 2016", "shortCiteRegEx": "Wiseman et al\\.", "year": 2016}, {"title": "Reference-aware language models", "author": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling."], "venue": "arXiv:1611.01628.", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "ICLR.", "citeRegEx": "Zaremba et al\\.,? 2015", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 28, "context": "1, we find that it outperforms both a strong n-gram language model and a strong recurrent neural network language model on the English test set of the CoNLL 2012 shared task on coreference evaluation (Pradhan et al., 2012).", "startOffset": 200, "endOffset": 222}, {"referenceID": 21, "context": "3, it achieves state-of-the-art performance on the InScript corpus (Modi et al., 2017).", "startOffset": 67, "endOffset": 86}, {"referenceID": 20, "context": "Our starting point for language modeling is a recurrent neural network (Mikolov et al., 2010), which defines", "startOffset": 71, "endOffset": 93}, {"referenceID": 13, "context": "where Wh and b are parameters of the model (along with word embeddings xt), LSTM is the widely used recurrent function known as \u201clong short-term memory\u201d (Hochreiter and Schmidhuber, 1997), and ht is a LSTM hidden state encoding the history of the sequence up to the tth word.", "startOffset": 153, "endOffset": 187}, {"referenceID": 35, "context": "Great success has been reported for this model (Zaremba et al., 2015), which posits nothing explicitly about the words appearing in the text sequence.", "startOffset": 47, "endOffset": 69}, {"referenceID": 10, "context": "A generative model with a similar hierarchical structure was used by Haghighi and Klein (2010) for coreference resolution.", "startOffset": 69, "endOffset": 95}, {"referenceID": 19, "context": "This term can also be extended to include other surface-form features for coreference resolution (Martschat and Strube, 2015; Clark and Manning, 2016b).", "startOffset": 97, "endOffset": 151}, {"referenceID": 5, "context": "This term can also be extended to include other surface-form features for coreference resolution (Martschat and Strube, 2015; Clark and Manning, 2016b).", "startOffset": 97, "endOffset": 151}, {"referenceID": 9, "context": "CFSM is a class factorized softmax function (Goodman, 2001; Baltescu and Blunsom, 2015).", "startOffset": 44, "endOffset": 87}, {"referenceID": 1, "context": "CFSM is a class factorized softmax function (Goodman, 2001; Baltescu and Blunsom, 2015).", "startOffset": 44, "endOffset": 87}, {"referenceID": 12, "context": "A similar updating scheme has been used by Henaff et al. (2016) for the \u201cmemory blocks\u201d in their recurrent entity network models.", "startOffset": 43, "endOffset": 64}, {"referenceID": 23, "context": "Our model is implemented with DyNet (Neubig et al., 2017) and available at https://github.", "startOffset": 36, "endOffset": 57}, {"referenceID": 6, "context": "We use AdaGrad (Duchi et al., 2011) with learning rate \u03bb = 0.", "startOffset": 15, "endOffset": 35}, {"referenceID": 16, "context": "1 and ADAM (Kingma and Ba, 2014) with default learning rate \u03bb = 0.", "startOffset": 11, "endOffset": 32}, {"referenceID": 30, "context": "To avoid overfitting, we also employ dropout (Srivastava et al., 2014) with the candidate rates as {0.", "startOffset": 45, "endOffset": 70}, {"referenceID": 25, "context": "We also experiment with the option to either use the pretrained GloVe word embeddings (Pennington et al., 2014) or randomly initialized word embeddings (then updated during training).", "startOffset": 86, "endOffset": 111}, {"referenceID": 6, "context": "We use AdaGrad (Duchi et al., 2011) with learning rate \u03bb = 0.1 and ADAM (Kingma and Ba, 2014) with default learning rate \u03bb = 0.001 as the candidate optimizers of our model. For all the parameters, we use the initialization tricks recommended by Glorot and Bengio (2010). To avoid overfitting, we also employ dropout (Srivastava et al.", "startOffset": 16, "endOffset": 270}, {"referenceID": 28, "context": "For language modeling and coreference resolution, we use the English benchmark data from the CoNLL 2012 shared task on coreference resolution (Pradhan et al., 2012).", "startOffset": 142, "endOffset": 164}, {"referenceID": 21, "context": "It includes 910 crowdsourced simple narrative texts in total and 18 stories were ignored due to labeling problems (Modi et al., 2017).", "startOffset": 114, "endOffset": 133}, {"referenceID": 21, "context": "We use the same training/development/test split as in (Modi et al., 2017), which includes 619, 91, 182 texts, respectively.", "startOffset": 54, "endOffset": 73}, {"referenceID": 21, "context": "For entity prediction, we employ the InScript corpus created by Modi et al. (2017). It consists of 10 scenarios, including grocery shopping, taking a flight, etc.", "startOffset": 64, "endOffset": 83}, {"referenceID": 7, "context": "A similar idea of using importance sampling for language modeling evaluation has been used by Dyer et al. (2016). For language modeling evaluation, we train our model on the training set from the CoNLL 2012 dataset with coreference annotation.", "startOffset": 94, "endOffset": 113}, {"referenceID": 11, "context": "We compare the language modeling performance with two competitive baselines: 5gram language model implemented in KenLM (Heafield et al., 2013) and RNNLM with LSTM units implemented in DyNet (Neubig et al.", "startOffset": 119, "endOffset": 142}, {"referenceID": 23, "context": ", 2013) and RNNLM with LSTM units implemented in DyNet (Neubig et al., 2017).", "startOffset": 55, "endOffset": 76}, {"referenceID": 29, "context": "This task is analogous to the reranking approach used in machine translation (Shen et al., 2004).", "startOffset": 77, "endOffset": 96}, {"referenceID": 19, "context": "This is the dominant approach for coreference resolution (Martschat and Strube, 2015; Clark and Manning, 2016a).", "startOffset": 57, "endOffset": 111}, {"referenceID": 4, "context": "This is the dominant approach for coreference resolution (Martschat and Strube, 2015; Clark and Manning, 2016a).", "startOffset": 57, "endOffset": 111}, {"referenceID": 27, "context": "For coreference resolution evaluation, we employ the CoNLL scorer (Pradhan et al., 2014).", "startOffset": 66, "endOffset": 88}, {"referenceID": 32, "context": "It computes three commonly used evaluation measures MUC (Vilain et al., 1995), B (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005).", "startOffset": 56, "endOffset": 77}, {"referenceID": 0, "context": ", 1995), B (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005).", "startOffset": 11, "endOffset": 36}, {"referenceID": 17, "context": ", 1995), B (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005).", "startOffset": 48, "endOffset": 59}, {"referenceID": 19, "context": "We employed CORT (Martschat and Strube, 2015) as our baseline coreference resolution system.", "startOffset": 17, "endOffset": 45}, {"referenceID": 19, "context": "The numbers of the baseline are higher than the results reported in Martschat and Strube (2015) since the feature set of CORT was subsequently extended.", "startOffset": 68, "endOffset": 96}, {"referenceID": 21, "context": "Figure 3: A short story on bicycles from the InScript corpus (Modi et al., 2017).", "startOffset": 61, "endOffset": 80}, {"referenceID": 21, "context": "Figure 3: A short story on bicycles from the InScript corpus (Modi et al., 2017). The entity prediction task requires predicting xxxx given the preceding text either by choosing a previously mentioned entity or deciding that this is a \u201cnew entity\u201d. In this example, the ground-truth prediction is [tire]4. For training, ENTITYNLM attempts to predict every entity. While, for testing, it predicts a maximum of 30 entities after the first three sentences, which is consistent with the experimental setup suggested by Modi et al. (2017).", "startOffset": 62, "endOffset": 534}, {"referenceID": 21, "context": "Based on Modi et al. (2017), we introduce a novel entity prediction task that tries to predict the next entity given the preceding text.", "startOffset": 9, "endOffset": 28}, {"referenceID": 21, "context": "We introduce two baselines in this task: (i) the always-new baseline that always predicts \u201cnew entity\u201d; (ii) a linear classification model using shallow features from Modi et al. (2017), including the recency of an entity\u2019s last mention and the frequency.", "startOffset": 167, "endOffset": 186}, {"referenceID": 21, "context": "We introduce two baselines in this task: (i) the always-new baseline that always predicts \u201cnew entity\u201d; (ii) a linear classification model using shallow features from Modi et al. (2017), including the recency of an entity\u2019s last mention and the frequency. We also compare with the model proposed by Modi et al. (2017). Their work assumes that the model has prior knowledge of all the participant types, which are specific to each scenario and fine-grained, e.", "startOffset": 167, "endOffset": 318}, {"referenceID": 19, "context": "CORT is the best-performing model of Martschat and Strube (2015) with greedy decoding.", "startOffset": 37, "endOffset": 65}, {"referenceID": 21, "context": "Modi et al. (2017) 62.", "startOffset": 0, "endOffset": 19}, {"referenceID": 21, "context": "More details about human predictions are discussed in (Modi et al., 2017).", "startOffset": 54, "endOffset": 73}, {"referenceID": 21, "context": "For each entity slot, Modi et al. (2017) acquired 20 human predictions, and the majority vote was selected.", "startOffset": 22, "endOffset": 41}, {"referenceID": 14, "context": "Previous work focuses on contextual information from previous sentences (Ji et al., 2016a) or discourse relations between adjacent sentences (Ji et al.", "startOffset": 72, "endOffset": 90}, {"referenceID": 15, "context": ", 2016a) or discourse relations between adjacent sentences (Ji et al., 2016b), showing improvements to language modeling and related tasks like coherence evaluation and discourse relation prediction.", "startOffset": 59, "endOffset": 77}, {"referenceID": 14, "context": "Previous work focuses on contextual information from previous sentences (Ji et al., 2016a) or discourse relations between adjacent sentences (Ji et al., 2016b), showing improvements to language modeling and related tasks like coherence evaluation and discourse relation prediction. In this work, ENTITYNLM adds explicit entity information to the language model, which is another way of adding a memory network for language modeling. Unlike the work by Tran et al. (2016), where memory blocks are used to store general contextual information for language modeling, ENTITYNLM assigns each memory block specifically to only one entity.", "startOffset": 73, "endOffset": 471}, {"referenceID": 23, "context": "Two recent approaches to modeling entities in text are closely related to our model. The first is the \u201creference-aware\u201d language models proposed by Yang et al. (2016), where the referred entities are from either a predefined item list, an external database, or the context from the same document.", "startOffset": 31, "endOffset": 167}, {"referenceID": 23, "context": "Two recent approaches to modeling entities in text are closely related to our model. The first is the \u201creference-aware\u201d language models proposed by Yang et al. (2016), where the referred entities are from either a predefined item list, an external database, or the context from the same document. Yang et al. (2016) present three models, one for each case.", "startOffset": 31, "endOffset": 316}, {"referenceID": 12, "context": "Our entity updating scheme is similar to the \u201cdynamic memory\u201d method used by Henaff et al. (2016). Our entity representations are dynamically allocated and updated only when an entity appears up, while the EntNet from Henaff et al.", "startOffset": 77, "endOffset": 98}, {"referenceID": 12, "context": "Our entity updating scheme is similar to the \u201cdynamic memory\u201d method used by Henaff et al. (2016). Our entity representations are dynamically allocated and updated only when an entity appears up, while the EntNet from Henaff et al. (2016) does not model entities and their relationships explicitly.", "startOffset": 77, "endOffset": 239}, {"referenceID": 10, "context": "The hierarchical structure of our entity generation model is inspired by Haghighi and Klein (2010). They implemented this idea as a probabillistic graphical model with", "startOffset": 73, "endOffset": 99}, {"referenceID": 26, "context": "the distance-dependent Chinese Restaurant Process (Pitman, 1995) for entity assignment, while our model is built on a recurrent neural network architecture.", "startOffset": 50, "endOffset": 64}, {"referenceID": 24, "context": "The reranking method considered in our coreference resolution evaluation could also be extended with samples from additional coreference resolution systems, to produce more variety (Ng, 2005).", "startOffset": 181, "endOffset": 191}, {"referenceID": 18, "context": "In previous work, such information has been added as features (Luo et al., 2004; Bj\u00f6rkelund and Kuhn, 2014) or by computing distributed entity representations (Wiseman et al.", "startOffset": 62, "endOffset": 107}, {"referenceID": 2, "context": "In previous work, such information has been added as features (Luo et al., 2004; Bj\u00f6rkelund and Kuhn, 2014) or by computing distributed entity representations (Wiseman et al.", "startOffset": 62, "endOffset": 107}, {"referenceID": 33, "context": ", 2004; Bj\u00f6rkelund and Kuhn, 2014) or by computing distributed entity representations (Wiseman et al., 2016; Clark and Manning, 2016b).", "startOffset": 86, "endOffset": 134}, {"referenceID": 5, "context": ", 2004; Bj\u00f6rkelund and Kuhn, 2014) or by computing distributed entity representations (Wiseman et al., 2016; Clark and Manning, 2016b).", "startOffset": 86, "endOffset": 134}, {"referenceID": 21, "context": "3 is based on work by Modi et al. (2017). The main difference is that we do not assume that all entities belong to a previously known set of entity types specified for each narrative scenario.", "startOffset": 22, "endOffset": 41}], "year": 2017, "abstractText": "Understanding a long document requires tracking how entities are introduced and evolve over time. We present a new type of language model, ENTITYNLM, that can explicitly model entities, dynamically update their representations, and contextually generate their mentions. Our model is generative and flexible; it can model an arbitrary number of entities in context while generating each entity mention at an arbitrary length. In addition, it can be used for several different tasks such as language modeling, coreference resolution, and entity prediction. Experimental results with all these tasks demonstrate that our model consistently outperforms strong baselines and prior work.", "creator": "LaTeX with hyperref package"}}}