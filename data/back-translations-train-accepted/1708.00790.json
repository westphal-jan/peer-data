{"id": "1708.00790", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Aug-2017", "title": "Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition", "abstract": "Unsupervised dependency parsing aims to learn a dependency parser from unannotated sentences. Existing work focuses on either learning generative models using the expectation-maximization algorithm and its variants, or learning discriminative models using the discriminative clustering algorithm. In this paper, we propose a new learning strategy that learns a generative model and a discriminative model jointly based on the dual decomposition method. Our method is simple and general, yet effective to capture the advantages of both models and improve their learning results. We tested our method on the UD treebank and achieved a state-of-the-art performance on thirty languages.", "histories": [["v1", "Wed, 2 Aug 2017 15:10:28 GMT  (44kb,D)", "http://arxiv.org/abs/1708.00790v1", "In EMNLP 2017"], ["v2", "Sun, 24 Sep 2017 14:30:34 GMT  (44kb,D)", "http://arxiv.org/abs/1708.00790v2", "In EMNLP 2017. A typo fixed in Algo 2"]], "COMMENTS": "In EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yong jiang", "wenjuan han", "kewei tu"], "accepted": true, "id": "1708.00790"}, "pdf": {"name": "1708.00790.pdf", "metadata": {"source": "CRF", "title": "Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition\u2217", "authors": ["Yong Jiang", "Wenjuan Han", "Kewei Tu"], "emails": ["jiangyong@shanghaitech.edu.cn", "hanwj@shanghaitech.edu.cn", "tukw@shanghaitech.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "It identifies dependencies between words in a sentence that have been shown to have a positive impact on other tasks such as semantic role designation (Lei et al., 2015) and discriminatory classification (Ma et al., 2015). However, supervised dependency learning requires the addition of a training corpus by linguistic experts, which can take up time and resources. Unsupervised dependency analyses eliminate the need for dependency annotations by directly learning from unwritten texts. Presence of unverified dependencies that focus mainly on generative learning models, such as the dependency model with its value (DMV) and combinatorial categorical grammars (CCG)."}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 DMV", "text": "The Valence Dependency Model (DMV) (Klein and Manning, 2004) is the first generative model that exceeds the left-branched baseline in unattended dependency parsing. In DMV, a set is generated by recursively applying three types of grammar rules to construct a parse tree from top to bottom. Probability of the generated sentence and parse tree is the probability product of all rules used in the creation process. To learn the parameters (rule probabilities) of DMV, the expectation maximization algorithm is often used. Noji et al. (2016) used two universal syntactic distortions in learning DMV: restriction of the depth of embedding in the middle and promotion of short dependencies. They achieved a comparable performance with state-of-the-art approaches."}, {"heading": "2.2 Convex-MST", "text": "Convex-MST (Grave and Elhadad, 2015) is a discrimination model for unattended dependency analysis based on the parser of the maximum first order tree dependence (McDonald et al., 2005). In the face of a sentence, a first order dependency graph is built over the words of the sentence, whether any dependency exists or not. The weight of each edge is calculated by wT f (x, i, j), where w is the parameters and f (x, i, j) is the handmade feature vector of the dependence from the i-th word to the j-th word in the sentence x. For sentence x of the length n, we can represent it as matrix X, where each raw tree is a feature vector. The parameter tree y is an exciting tree of the graph and can be represented as dep\u03b1 of the j-th word in the sentence x."}, {"heading": "2.3 Dual Decomposition", "text": "Dual decomposition (Dantzig and Wolfe, 1960), a specific case of Lagran relaxation, is an optimization method that splits a hard problem into several small sub-problems, and has been widely applied in machine learning (Komodakis et al., 2007) and natural language processing (Koo et al., 2010; Rush and Collins, 2012). Komodakis et al. (2007) proposed using dual decomposition to draw MAP conclusions for Markov random fields. Koo et al. (2010) proposed a new dependency saver based on dual decomposition by combining a graph-based dependency model with a non-projective head automaton. In the work of Rush et al. (2010), they demonstrated that dual decomposition can effectively integrate two lexicalized parsing models or two correlated tasks."}, {"heading": "2.4 Agreement based Learning", "text": "Liang et al. (2008) proposed agreement-based learning, which trains several tractable generative models together and encourages them to agree on certain latent variables. To effectively train the system, a product EM algorithm was used, showing that the common model performs better than any independent model in terms of accuracy or convergence speed. They also showed that the objective function of Klein and Manning's work (2004) is a special case of the product EM algorithm for grammar induction. Our approach has a similar motivation to agreement-based learning, but has two important advantages: firstly, while their approach only combines generative models, our approach can use both generative and discriminatory models. Secondly, while their approach requires that the sub-models share the same dynamic programming structure when performing decoding, our approach has no such limitation."}, {"heading": "3 Joint Training", "text": "We minimize the following objective function, which combines two different models of uncontrolled dependency analysis: J (MF, MG) = N \u2211 \u03b1 = 1 min y\u03b1Y\u03b1 (F (x\u03b1, y\u03b1; MF) + G (x\u03b1, y\u03b1; MG)), where N is the size of the training data, MF and MG are the parameters of the first and second model, F and G are their respective learning objectives, and Y\u03b1 is the set of valid dependency parameters of the set x\u03b1. While this goal can in principle be used to combine many different types of models, here we are considering two state-of-the-art models of uncontrolled dependence economy analysis, a generative model LC-DMV (Noji et al., 2016) and a discriminatory model Convex-MST (Grave and Elhadad, 2015)."}, {"heading": "F (x\u03b1,y\u03b1; \u0398) = \u2212 log (P\u0398(x\u03b1,y\u03b1)f(x\u03b1,y\u03b1))", "text": "G (x\u03b1, y\u03b1; w) = 12n\u03b1 | | y\u03b1 \u2212 X\u03b1w | | 22 + \u03bb2N | | w | | 22 \u2212 \u00b5vTywhere P\u0442 (x\u03b1, y\u03b1) is the common probability of theorem x\u03b1 and parse y\u03b1, f is a constraint factor, and the notations in the second objective function are explained in Section 2.2."}, {"heading": "3.1 Learning", "text": "In each iteration we first fix the parameters and find the best dependency parameters of the training sets (see Section 3.2); then we fix the parameters and optimize the parameters. The detailed algorithm is shown in Algorithm 1.Pretraining of the two models by executing their original learning algorithms separately. Once the parameters of the training sets are fixed, it is easy to show that the parameters of the two models can be optimized separately. Updating the parameters of LC-DMV can be done by simply counting the number of times each rule is used in the parameter trees and then normalizing the counting to obtain the most likely probabilities.The parameters w of Convex-MST can be updated by stochastic descent from LC-DMV."}, {"heading": "3.2 Joint Decoding", "text": "The aim of decoding is to find the best parse tree: y = arg min y Y12n | | y \u2212 Xw | | 22 \u2212 \u00b5vTy \u2212 logP\u0442 (x, y) We use the dual decomposition algorithm to solve this problem (shown in algorithm 2), the most important part of the algorithm being the solution of the two separate decoding problems: y = arg min y Y \u2212 log (P\u044b (x, y) f (x, y) + uTyz = arg min z Y12n | | z \u2212 Xw | | 22 \u2212 \u00b5vT z \u2212 uT zThe first decoding problem can be solved by a modified CYK parse algorithm that takes into account the information in the vector u. The second decoding problem can be solved using the same algorithm of Grave and Elhadad (2015) (we use the projective approach in our version)."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Setup", "text": "We use UD Treebank 1.4 as our data sets. We sorted the data sets in the tree base according to the number of training sets of length \u2264 15 and selected the thirty best data sets, which is similar to the structure of Noji et al. (2016). For each data set, we trained our method on the training data with length \u2264 15 and tested our method on the test data with length \u2264 40. We tuned the hyperparameters of our method on the English language data set and reported the results on the thirty data sets without further parameter tuning. We compared our method with four baselines. The first two baselines are Convex-MST and LC-DMV, which are trained independently of each other. To construct the third baseline, we used the independently trained ConvexMST baseline to parse all training sets and then use the parameters to initialize the formation of LC-DMV. This can be seen as a simple method to combine two approaches of the MLC-VLC base line, therefore we do not have two different approaches to the MLC-VLC-Vex function."}, {"heading": "4.2 Results", "text": "In Table 1, we compare our jointly trained models with the four baselines. We see that LC-DMV and Convex-MST with joint training and independent decoding can achieve a better overall performance than when they are trained separately with or without mutual initialization. Joint decoding with our jointly trained models works worse than independent decoding. We made the same observation when applying joint decoding to the separately trained models (not shown in the table). We believe that this is because unattended parsers have relatively low accuracy and forcing them to coordinate would not lead to better parses. On the other hand, joint decoding during training contributes to spreading useful inductive distortions between models and thus leads to better trained models."}, {"heading": "4.3 Analysis of Parsing Results", "text": "We analyze the analysis results from the two models to see how they favor each other with shared training. Note that LC-DMV limits the depth of embedding in the center and promotes a shorter length of dependency, while Convex-MST promotes dependencies that meet predetermined linguistic rules. Therefore, we want to see if the jointly trained LC-DMV produces more dependencies that satisfy the linguistic priorities than its separately trained counterpart, and whether the jointly trained Convex-MST produces parsetrewith less embedding and shorter dependencies than its separately trained counterpart. Figure 1 shows the percentages of dependencies that satisfy linguistic rules when using separately and jointly trained LC-DMV to analyze the test sentences in the English database."}, {"heading": "5 Conclusion", "text": "In this paper, we have proposed a new learning strategy for the unsupervised analysis of dependencies that a generative and discriminatory model jointly learns, based on dual decomposition. We show that two state-of-the-art models can positively influence each other through joint training and achieve better performance than their separately trained counterparts."}], "references": [{"title": "Painless unsupervised learning with features", "author": ["Taylor Berg-Kirkpatrick", "Alexandre Bouchard-C\u00f4t\u00e9", "John DeNero", "Dan Klein."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association", "citeRegEx": "Berg.Kirkpatrick et al\\.,? 2010", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2010}, {"title": "Simple robust grammar induction with combinatory categorial grammars", "author": ["Yonatan Bisk", "Julia Hockenmaier"], "venue": null, "citeRegEx": "Bisk and Hockenmaier.,? \\Q2012\\E", "shortCiteRegEx": "Bisk and Hockenmaier.", "year": 2012}, {"title": "Logistic normal priors for unsupervised probabilistic grammar induction", "author": ["Shay B Cohen", "Kevin Gimpel", "Noah A Smith."], "venue": "Advances in Neural Information Processing Systems, pages 321\u2013328.", "citeRegEx": "Cohen et al\\.,? 2008", "shortCiteRegEx": "Cohen et al\\.", "year": 2008}, {"title": "Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction", "author": ["Shay B Cohen", "Noah A Smith."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chap-", "citeRegEx": "Cohen and Smith.,? 2009", "shortCiteRegEx": "Cohen and Smith.", "year": 2009}, {"title": "Decomposition principle for linear programs", "author": ["George B Dantzig", "Philip Wolfe."], "venue": "Operations research, 8(1):101\u2013111.", "citeRegEx": "Dantzig and Wolfe.,? 1960", "shortCiteRegEx": "Dantzig and Wolfe.", "year": 1960}, {"title": "A convex and feature-rich discriminative approach to dependency grammar induction", "author": ["Edouard Grave", "No\u00e9mie Elhadad."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Grave and Elhadad.,? 2015", "shortCiteRegEx": "Grave and Elhadad.", "year": 2015}, {"title": "Unsupervised neural dependency parsing", "author": ["Yong Jiang", "Wenjuan Han", "Kewei Tu."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 763\u2013771, Austin, Texas. Association for Computational Lin-", "citeRegEx": "Jiang et al\\.,? 2016", "shortCiteRegEx": "Jiang et al\\.", "year": 2016}, {"title": "Corpusbased induction of syntactic structure: Models of dependency and constituency", "author": ["Dan Klein", "Christopher D Manning."], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 478. Association for Com-", "citeRegEx": "Klein and Manning.,? 2004", "shortCiteRegEx": "Klein and Manning.", "year": 2004}, {"title": "Mrf optimization via dual decomposition: Message-passing revisited", "author": ["Nikos Komodakis", "Nikos Paragios", "Georgios Tziritas."], "venue": "Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on, pages 1\u20138. IEEE.", "citeRegEx": "Komodakis et al\\.,? 2007", "shortCiteRegEx": "Komodakis et al\\.", "year": 2007}, {"title": "Dual decomposition for parsing with non-projective head automata", "author": ["Terry Koo", "Alexander M Rush", "Michael Collins", "Tommi Jaakkola", "David Sontag."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Koo et al\\.,? 2010", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "High-order low-rank tensors for semantic role labeling", "author": ["Tao Lei", "Yuan Zhang", "Llu\u0131\u0301s M\u00e0rquez", "Alessandro Moschitti", "Regina Barzilay"], "venue": "In Proceedings of the 2015 Conference of the North", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Agreement-based learning", "author": ["Percy S Liang", "Dan Klein", "Michael I. Jordan."], "venue": "J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 913\u2013920. Curran Associates, Inc.", "citeRegEx": "Liang et al\\.,? 2008", "shortCiteRegEx": "Liang et al\\.", "year": 2008}, {"title": "Dependency-based convolutional neural networks for sentence embedding", "author": ["Mingbo Ma", "Liang Huang", "Bing Xiang", "Bowen Zhou."], "venue": "arXiv preprint arXiv:1507.01839.", "citeRegEx": "Ma et al\\.,? 2015", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Online large-margin training of dependency parsers", "author": ["Ryan McDonald", "Koby Crammer", "Fernando Pereira."], "venue": "Proceedings of the 43rd annual meeting on association for computational linguistics, pages 91\u201398. Association for Computa-", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Using left-corner parsing to encode universal structural constraints in grammar induction", "author": ["Hiroshi Noji", "Yusuke Miyao", "Mark Johnson."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 33\u201343,", "citeRegEx": "Noji et al\\.,? 2016", "shortCiteRegEx": "Noji et al\\.", "year": 2016}, {"title": "A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing", "author": ["Alexander M. Rush", "Michael Collins."], "venue": "J. Artif. Int. Res., 45(1):305\u2013362.", "citeRegEx": "Rush and Collins.,? 2012", "shortCiteRegEx": "Rush and Collins.", "year": 2012}, {"title": "On dual decomposition and linear programming relaxations for natural language processing", "author": ["Alexander M Rush", "David Sontag", "Michael Collins", "Tommi Jaakkola."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language", "citeRegEx": "Rush et al\\.,? 2010", "shortCiteRegEx": "Rush et al\\.", "year": 2010}, {"title": "Annealing structural bias in multilingual weighted grammar induction", "author": ["Noah A Smith", "Jason Eisner."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Compu-", "citeRegEx": "Smith and Eisner.,? 2006", "shortCiteRegEx": "Smith and Eisner.", "year": 2006}], "referenceMentions": [{"referenceID": 10, "context": "It identifies dependencies between words in a sentence, which have been shown to benefit other tasks such as semantic role labeling (Lei et al., 2015) and sentence classification (Ma et al.", "startOffset": 132, "endOffset": 150}, {"referenceID": 12, "context": ", 2015) and sentence classification (Ma et al., 2015).", "startOffset": 36, "endOffset": 53}, {"referenceID": 7, "context": "Previous work on unsupervised dependency parsing mainly focuses on learning generative models, such as the dependency model with valence (DMV) (Klein and Manning, 2004) and combinatory categorial grammars (CCG) (Bisk and Hockenmaier, 2012).", "startOffset": 143, "endOffset": 168}, {"referenceID": 1, "context": "Previous work on unsupervised dependency parsing mainly focuses on learning generative models, such as the dependency model with valence (DMV) (Klein and Manning, 2004) and combinatory categorial grammars (CCG) (Bisk and Hockenmaier, 2012).", "startOffset": 211, "endOffset": 239}, {"referenceID": 17, "context": "In addition, many types of inductive bias, such as those favoring short dependency arcs (Smith and Eisner, 2006), encouraging correlations between POS tags (Cohen et al.", "startOffset": 88, "endOffset": 112}, {"referenceID": 2, "context": "In addition, many types of inductive bias, such as those favoring short dependency arcs (Smith and Eisner, 2006), encouraging correlations between POS tags (Cohen et al., 2008; Cohen and Smith, 2009; BergKirkpatrick et al., 2010; Jiang et al., 2016), and limiting center embedding (Noji et al.", "startOffset": 156, "endOffset": 249}, {"referenceID": 3, "context": "In addition, many types of inductive bias, such as those favoring short dependency arcs (Smith and Eisner, 2006), encouraging correlations between POS tags (Cohen et al., 2008; Cohen and Smith, 2009; BergKirkpatrick et al., 2010; Jiang et al., 2016), and limiting center embedding (Noji et al.", "startOffset": 156, "endOffset": 249}, {"referenceID": 6, "context": "In addition, many types of inductive bias, such as those favoring short dependency arcs (Smith and Eisner, 2006), encouraging correlations between POS tags (Cohen et al., 2008; Cohen and Smith, 2009; BergKirkpatrick et al., 2010; Jiang et al., 2016), and limiting center embedding (Noji et al.", "startOffset": 156, "endOffset": 249}, {"referenceID": 14, "context": ", 2016), and limiting center embedding (Noji et al., 2016), can be incorporated into generative models to achieve better parsing accuracy.", "startOffset": 39, "endOffset": 58}, {"referenceID": 5, "context": "Recently, a feature-rich discriminative model for unsupervised parsing is proposed that captures the global context information of sentences (Grave and Elhadad, 2015).", "startOffset": 141, "endOffset": 166}, {"referenceID": 14, "context": "In this paper we propose to jointly train two state-of-the-art models of unsupervised dependency parsing: a generative model called LCDMV (Noji et al., 2016) and a discriminative model called Convex-MST (Grave and Elhadad, 2015).", "startOffset": 138, "endOffset": 157}, {"referenceID": 5, "context": ", 2016) and a discriminative model called Convex-MST (Grave and Elhadad, 2015).", "startOffset": 53, "endOffset": 78}, {"referenceID": 4, "context": "We employ a learning algorithm based on the dual decomposition (Dantzig and Wolfe, 1960) inference algorithm, which encourages the two models to influence each other during training.", "startOffset": 63, "endOffset": 88}, {"referenceID": 7, "context": "The dependency model with valence (DMV) (Klein and Manning, 2004) is the first generative model that outperforms the left-branching baseline in unsupervised dependency parsing.", "startOffset": 40, "endOffset": 65}, {"referenceID": 7, "context": "The dependency model with valence (DMV) (Klein and Manning, 2004) is the first generative model that outperforms the left-branching baseline in unsupervised dependency parsing. In DMV, a sentence is generated by recursively applying three types of grammar rules to construct a parse tree from the top down. The probability of the generated sentence and parse tree is the probability product of all the rules used in the generation process. To learn the parameters (rule probabilities) of DMV, the expectation maximization algorithm is often used. Noji et al. (2016) exploited two universal syntactic biases in learning DMV: restricting the center-embedding depth and encouraging short dependencies.", "startOffset": 41, "endOffset": 566}, {"referenceID": 5, "context": "Convex-MST (Grave and Elhadad, 2015) is a discriminative model for unsupervised dependency parsing based on the first-order maximum spanning tree dependency parser (McDonald et al.", "startOffset": 11, "endOffset": 36}, {"referenceID": 13, "context": "Convex-MST (Grave and Elhadad, 2015) is a discriminative model for unsupervised dependency parsing based on the first-order maximum spanning tree dependency parser (McDonald et al., 2005).", "startOffset": 164, "endOffset": 187}, {"referenceID": 4, "context": "Dual decomposition (Dantzig and Wolfe, 1960), a special case of Lagrangian relaxation, is an optimization method that decomposes a hard problem into several small sub-problems.", "startOffset": 19, "endOffset": 44}, {"referenceID": 8, "context": "It has been widely used in machine learning (Komodakis et al., 2007) and natural language processing (Koo et al.", "startOffset": 44, "endOffset": 68}, {"referenceID": 9, "context": ", 2007) and natural language processing (Koo et al., 2010; Rush and Collins, 2012).", "startOffset": 40, "endOffset": 82}, {"referenceID": 15, "context": ", 2007) and natural language processing (Koo et al., 2010; Rush and Collins, 2012).", "startOffset": 40, "endOffset": 82}, {"referenceID": 7, "context": "They also showed that the objective function of the work of Klein and Manning (2004) is a special case of the product EM algorithm for grammar induction.", "startOffset": 60, "endOffset": 85}, {"referenceID": 14, "context": "While in principle this objective can be used to combine many different types of models, here we consider two state-ofthe-art models of unsupervised dependency parsing, a generative model LC-DMV (Noji et al., 2016) and a discriminative model Convex-MST (Grave and Elhadad, 2015).", "startOffset": 195, "endOffset": 214}, {"referenceID": 5, "context": ", 2016) and a discriminative model Convex-MST (Grave and Elhadad, 2015).", "startOffset": 46, "endOffset": 71}, {"referenceID": 5, "context": "The second decoding problem can be solved using the same algorithm of Grave and Elhadad (2015) (we use the projective version in our approach).", "startOffset": 70, "endOffset": 95}, {"referenceID": 14, "context": "of training sentences of length \u2264 15 and selected the top thirty datasets, which is similar to the setup of Noji et al. (2016). For each dataset, we trained our method on the training data with length \u2264 15 and tested our method on the testing data with length \u2264 40.", "startOffset": 108, "endOffset": 127}], "year": 2017, "abstractText": "Unsupervised dependency parsing aims to learn a dependency parser from unannotated sentences. Existing work focuses on either learning generative models using the expectation-maximization algorithm and its variants, or learning discriminative models using the discriminative clustering algorithm. In this paper, we propose a new learning strategy that learns a generative model and a discriminative model jointly based on the dual decomposition method. Our method is simple and general, yet effective to capture the advantages of both models and improve their learning results. We tested our method on the UD treebank and achieved a state-ofthe-art performance on thirty languages.", "creator": "LaTeX with hyperref package"}}}