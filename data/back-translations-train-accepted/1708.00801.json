{"id": "1708.00801", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Aug-2017", "title": "Dependency Grammar Induction with Neural Lexicalization and Big Training Data", "abstract": "We study the impact of big models (in terms of the degree of lexicalization) and big data (in terms of the training corpus size) on dependency grammar induction. We experimented with L-DMV, a lexicalized version of Dependency Model with Valence and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence. We find that L-DMV only benefits from very small degrees of lexicalization and moderate sizes of training corpora. L-NDMV can benefit from big training data and lexicalization of greater degrees, especially when enhanced with good model initialization, and it achieves a result that is competitive with the current state-of-the-art.", "histories": [["v1", "Wed, 2 Aug 2017 15:43:30 GMT  (1101kb,D)", "http://arxiv.org/abs/1708.00801v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenjuan han", "yong jiang", "kewei tu"], "accepted": true, "id": "1708.00801"}, "pdf": {"name": "1708.00801.pdf", "metadata": {"source": "CRF", "title": "Dependency Grammar Induction with Neural Lexicalization and Big Training Data\u2217", "authors": ["Wenjuan Han", "Yong Jiang", "Kewei Tu"], "emails": ["@shanghaitech.edu.cn", "tukw@shanghaitech.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In the most common setting, grammar is unleashed with POS tags as tokens, and the training data is the WSJ10 corpus (the Wall Street Journal corpus contains sentences no longer than 10 words), which contains no more than 6,000 training sets (Cohen et al., 2008; Berg-Kirkpatrick et al., 2010; Tu and Honavar, 2012). Dictionalized grammar induction aims to incorporate lexical information into the learned grammar to increase its representation and improve learning accuracy. The simplest approach to encrypting lexical information is complete lexicalization (Pate and Johnson, 2016; Spitkovsky et al., 2013). A major problem with this work has been supported by the National Natural Science Foundation of China (61503248). Full lexicalization is that grammar is much larger and therefore learning is more sophisticated."}, {"heading": "2 Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Lexicalized DMV", "text": "We choose an extended version of DMV (Gillenwater et al., 2010). We use a sim-ar Xiv: 170 8.00 801v 1 [cs.C L] 2A ug2 017ilar approach to the of Spitkovsky et al. (2013) and Blunsom and Cohn (2010) and represent each token as a word / POS pair. If a pair occurs rarely in the corpus, we simply ignore the word and represent it only with the POS tag. We control the degree of lexicalization by replacing words that appear less than a cutoff number in the WSJ10 corpus with their POS tags. If a very large cutoff number is used, the grammar is practically unexicalized; but if the cutoff number becomes smaller, the grammar approaches full lexicalization. Note that our method differs from previous practice, replacing rare words with a special \"\" symbol (Headoff III, al.)."}, {"heading": "2.2 Lexicalized NDMV", "text": "With a greater degree of lexicography, grammar contains more symbols and thus more parameters (i.e. grammar rules probabilities) that require more data for accurate learning. Smoothing is a useful technique to reduce the demand for data in this case. Here, we use a neural approach to smoothing. Specifically, we propose a lexical extension of the neural DMV (Jiang et al., 2016) and we call the resulting approach L-NDMV. Extended Model: The model structure of LNDMV is similar to the representation of the head and child of the CHILD and DECISION rules. The network structure for predicting the probabilities of WC1, Pc2, PSD] (m is the vocabulary size; ci is the i-th token."}, {"heading": "2.3 Model Initialization", "text": "It has already been shown that the heuristic KM initialization method of Klein and Manning (2004) does not work well with lexicalized grammar induction (Headden III et al., 2009; Pate and Johnson, 2016) and it is very helpful to initialize learning with a model learned by another grammar induction method (Le and Zuidema, 2015; Jiang et al., 2016). We tested both the KM initialization and the following initialization method: We first learned the unexicalized DMV with the grammar induction method of Naseem et al. (2010) and used it to analyze the training corpus; then, from the parsis trees, we perform an estimate with maximum probability to create the initial lexicalized model."}, {"heading": "3 Experimental Setup", "text": "In fact, most of them will be able to play by the rules they have set themselves."}, {"heading": "4 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Results on English", "text": "Figure 2 (a) shows the Directional Dependency Accuracy (DDA) of the learned lexical DMV with KM initialization. It can be seen that on the smallest WSJ10 training corpus, lexicography only improves learning if the degree of lexicography is low; with further lexicography, learning accuracy could decrease significantly. On the three larger training corpus, the effect of lexicography on learning accuracy is still negative, but it is less severe. Overall, lexicography seems to be very demanding and even our largest training corpus could not achieve the benefit of lexicography."}, {"heading": "4.2 Results on Chinese", "text": "Figure 2 (d) shows the results of the three approaches at the Chinese tree bank. Because the corpus is relatively small, we did not study the effects of the corpus size. Similar to the case of English, the accuracy of lexicalized DMV decreases with increasing lexicalization. However, accuracy with L-NDMV increases significantly even without good model initialization. Good initialization further increases the performance of L-NDMV, but the benefit of lexicalization is less significant (from 0.55 to 0.58)."}, {"heading": "5 Effect of Grammar Rule Probability Initialization", "text": "We compare four initialization methods with LNDMV: uniform initialization, random initialization, KM initialization (Klein and Manning, 2004), and good initialization as in Section 2.3 in Figure 3. Here, we trained the L-NDMV model on the WSJ10 corpus with the same experimental setup as in Section 3.Again, we found that good initialization leads to better performance than KM initialization, and both good initialization and KM initialization are significantly better than random and uniform initialization. Note that our results differ from those of Pate and Johnson (2016), who found that uniform initialization leads to similar performance as KM initialization. We suspect this is due to the difference in learning approaches (we use neural networks that are more sensitive to initialization) and the training and test corpora (we use news articles while using phone scripts)."}, {"heading": "6 Conclusion and Future Work", "text": "We are experimenting with lexicalized DMV (L-DMV) and our lexicalized extension of Neural DMV (L-NDMV). We note that L-DMV benefits only from a very low level of lexicalization and moderate sizes of training corpus. In contrast, L-NDMV can benefit from large training data and lexicalization of larger degrees, especially if improved by good model initialization, and it achieves a result that competes with the state of the art. In the future, we plan to study higher degrees of lexicalization or complete lexicalization as well as even larger training corpus (such as the Wikipedia corpus). We would also like to experiment with other grammar induction approaches with lexicalization and large training data."}], "references": [{"title": "Painless unsupervised learning with features", "author": ["Taylor Berg-Kirkpatrick", "Alexandre Bouchard-C\u00f4t\u00e9", "John DeNero", "Dan Klein."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association", "citeRegEx": "Berg.Kirkpatrick et al\\.,? 2010", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2010}, {"title": "Unsupervised induction of tree substitution grammars for dependency parsing", "author": ["Phil Blunsom", "Trevor Cohn."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1204\u20131213. Association for Com-", "citeRegEx": "Blunsom and Cohn.,? 2010", "shortCiteRegEx": "Blunsom and Cohn.", "year": 2010}, {"title": "Logistic normal priors for unsupervised probabilistic grammar induction", "author": ["Shay B Cohen", "Kevin Gimpel", "Noah A Smith."], "venue": "Advances in Neural Information Processing Systems, pages 321\u2013328.", "citeRegEx": "Cohen et al\\.,? 2008", "shortCiteRegEx": "Cohen et al\\.", "year": 2008}, {"title": "Sparsity in dependency grammar induction", "author": ["Jennifer Gillenwater", "Kuzman Ganchev", "Joao Gra\u00e7a", "Fernando Pereira", "Ben Taskar."], "venue": "Proceedings of the ACL 2010 Conference Short Papers, pages 194\u2013 199. Association for Computational Linguistics.", "citeRegEx": "Gillenwater et al\\.,? 2010", "shortCiteRegEx": "Gillenwater et al\\.", "year": 2010}, {"title": "Improving unsupervised dependency parsing with richer contexts and smoothing", "author": ["William P Headden III", "Mark Johnson", "David McClosky."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American", "citeRegEx": "III et al\\.,? 2009", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Unsupervised neural dependency parsing", "author": ["Yong Jiang", "Wenjuan Han", "Kewei Tu."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 763\u2013771, Austin, Texas. Association for Computational Lin-", "citeRegEx": "Jiang et al\\.,? 2016", "shortCiteRegEx": "Jiang et al\\.", "year": 2016}, {"title": "Corpusbased induction of syntactic structure: Models of dependency and constituency", "author": ["Dan Klein", "Christopher D. Manning."], "venue": "Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics, ACL \u201904, Stroudsburg, PA, USA.", "citeRegEx": "Klein and Manning.,? 2004", "shortCiteRegEx": "Klein and Manning.", "year": 2004}, {"title": "Unsupervised dependency parsing: Let\u2019s use supervised parsers", "author": ["Phong Le", "Willem Zuidema."], "venue": "arXiv preprint arXiv:1504.04666.", "citeRegEx": "Le and Zuidema.,? 2015", "shortCiteRegEx": "Le and Zuidema.", "year": 2015}, {"title": "Online em for unsupervised models", "author": ["Percy Liang", "Dan Klein."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL \u201909, pages", "citeRegEx": "Liang and Klein.,? 2009", "shortCiteRegEx": "Liang and Klein.", "year": 2009}, {"title": "Stopprobability estimates computed on a large corpus improve unsupervised dependency parsing", "author": ["David Marecek", "Milan Straka."], "venue": "ACL (1), pages 281\u2013290.", "citeRegEx": "Marecek and Straka.,? 2013", "shortCiteRegEx": "Marecek and Straka.", "year": 2013}, {"title": "Using universal linguistic knowledge to guide grammar induction", "author": ["Tahira Naseem", "Harr Chen", "Regina Barzilay", "Mark Johnson."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1234\u20131244. Asso-", "citeRegEx": "Naseem et al\\.,? 2010", "shortCiteRegEx": "Naseem et al\\.", "year": 2010}, {"title": "Inductive dependency parsing", "author": ["Joakim Nivre."], "venue": "Springer.", "citeRegEx": "Nivre.,? 2006", "shortCiteRegEx": "Nivre.", "year": 2006}, {"title": "Grammar induction from (lots of) words alone", "author": ["John K Pate", "Mark Johnson"], "venue": null, "citeRegEx": "Pate and Johnson.,? \\Q2016\\E", "shortCiteRegEx": "Pate and Johnson.", "year": 2016}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["Radim \u0158eh\u016f\u0159ek", "Petr Sojka."], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Valletta, Malta. ELRA. http://is.muni.cz/", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka.,? 2010", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka.", "year": 2010}, {"title": "Breaking out of local optima with count transforms and model recombination: A study in grammar induction", "author": ["Valentin I Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky."], "venue": "EMNLP, pages 1983\u2013 1995.", "citeRegEx": "Spitkovsky et al\\.,? 2013", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2013}, {"title": "Feature-rich part-ofspeech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Toutanova et al\\.,? 2003", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Unambiguity regularization for unsupervised learning of probabilistic grammars", "author": ["Kewei Tu", "Vasant Honavar."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-", "citeRegEx": "Tu and Honavar.,? 2012", "shortCiteRegEx": "Tu and Honavar.", "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "We experimented with L-DMV, a lexicalized version of Dependency Model with Valence (Klein and Manning, 2004) and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence (Jiang et al.", "startOffset": 83, "endOffset": 108}, {"referenceID": 5, "context": "We experimented with L-DMV, a lexicalized version of Dependency Model with Valence (Klein and Manning, 2004) and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence (Jiang et al., 2016).", "startOffset": 191, "endOffset": 211}, {"referenceID": 2, "context": "In the most common setting, the grammar is unlexicalized with POS tags being the tokens, and the training data is the WSJ10 corpus (the Wall Street Journal corpus with sentences no longer than 10 words) containing no more than 6,000 training sentences (Cohen et al., 2008; Berg-Kirkpatrick et al., 2010; Tu and Honavar, 2012).", "startOffset": 252, "endOffset": 325}, {"referenceID": 0, "context": "In the most common setting, the grammar is unlexicalized with POS tags being the tokens, and the training data is the WSJ10 corpus (the Wall Street Journal corpus with sentences no longer than 10 words) containing no more than 6,000 training sentences (Cohen et al., 2008; Berg-Kirkpatrick et al., 2010; Tu and Honavar, 2012).", "startOffset": 252, "endOffset": 325}, {"referenceID": 16, "context": "In the most common setting, the grammar is unlexicalized with POS tags being the tokens, and the training data is the WSJ10 corpus (the Wall Street Journal corpus with sentences no longer than 10 words) containing no more than 6,000 training sentences (Cohen et al., 2008; Berg-Kirkpatrick et al., 2010; Tu and Honavar, 2012).", "startOffset": 252, "endOffset": 325}, {"referenceID": 12, "context": "The most straightforward approach to encoding lexical information is full lexicalization (Pate and Johnson, 2016; Spitkovsky et al., 2013).", "startOffset": 89, "endOffset": 138}, {"referenceID": 14, "context": "The most straightforward approach to encoding lexical information is full lexicalization (Pate and Johnson, 2016; Spitkovsky et al., 2013).", "startOffset": 89, "endOffset": 138}, {"referenceID": 5, "context": "One example is Neural DMV (NDMV) (Jiang et al., 2016) which incorporates neural networks into DMV and can automatically smooth correlated grammar rule probabilities.", "startOffset": 33, "endOffset": 53}, {"referenceID": 1, "context": "(2009) and Blunsom and Cohn (2010) used partial lexicalization in which infrequent words are replaced by special symbols or their POS tags.", "startOffset": 11, "endOffset": 35}, {"referenceID": 1, "context": "(2009) and Blunsom and Cohn (2010) used partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Another straightforward way to mitigate the data scarcity problem of lexicalization is to use training corpora larger than the standard WSJ corpus. For example, Pate and Johnson (2016) used two large corpora containing more than 700k sentences; Marecek and Straka (2013) utilized a very large corpus based on Wikipedia in learning an unlexicalized dependency grammar.", "startOffset": 11, "endOffset": 325}, {"referenceID": 1, "context": "(2009) and Blunsom and Cohn (2010) used partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Another straightforward way to mitigate the data scarcity problem of lexicalization is to use training corpora larger than the standard WSJ corpus. For example, Pate and Johnson (2016) used two large corpora containing more than 700k sentences; Marecek and Straka (2013) utilized a very large corpus based on Wikipedia in learning an unlexicalized dependency grammar.", "startOffset": 11, "endOffset": 411}, {"referenceID": 6, "context": "We experimented with a lexicalized version of Dependency Model with Valence (L-DMV) (Klein and Manning, 2004) and our lexicalized extension of NDMV (L-NDMV).", "startOffset": 84, "endOffset": 109}, {"referenceID": 3, "context": "We choose to lexicalize an extended version of DMV (Gillenwater et al., 2010).", "startOffset": 51, "endOffset": 77}, {"referenceID": 12, "context": "ilar approach to that of Spitkovsky et al. (2013) and Blunsom and Cohn (2010) and represent each token as a word/POS pair.", "startOffset": 25, "endOffset": 50}, {"referenceID": 1, "context": "(2013) and Blunsom and Cohn (2010) and represent each token as a word/POS pair.", "startOffset": 11, "endOffset": 35}, {"referenceID": 5, "context": "Specifically, we propose a lexicalized extension of neural DMV (Jiang et al., 2016) and we call the resulting approach L-NDMV.", "startOffset": 63, "endOffset": 83}, {"referenceID": 8, "context": "Our algorithm can be seen as an extension of online EM (Liang and Klein, 2009) to accommodate neural network training.", "startOffset": 55, "endOffset": 78}, {"referenceID": 12, "context": "It was previously shown that the heuristic KM initialization method by Klein and Manning (2004) does not work well for lexicalized grammar induction (Headden III et al., 2009; Pate and Johnson, 2016) and it is very helpful to initialize learning with a model learned by a different grammar induction method (Le and Zuidema, 2015; Jiang et al.", "startOffset": 149, "endOffset": 199}, {"referenceID": 7, "context": ", 2009; Pate and Johnson, 2016) and it is very helpful to initialize learning with a model learned by a different grammar induction method (Le and Zuidema, 2015; Jiang et al., 2016).", "startOffset": 139, "endOffset": 181}, {"referenceID": 5, "context": ", 2009; Pate and Johnson, 2016) and it is very helpful to initialize learning with a model learned by a different grammar induction method (Le and Zuidema, 2015; Jiang et al., 2016).", "startOffset": 139, "endOffset": 181}, {"referenceID": 4, "context": "It was previously shown that the heuristic KM initialization method by Klein and Manning (2004) does not work well for lexicalized grammar induction (Headden III et al.", "startOffset": 71, "endOffset": 96}, {"referenceID": 10, "context": "an unlexicalized DMV using the grammar induction method of Naseem et al. (2010) and use it to parse the training corpus; then, from the parse trees we run maximum likelihood estimation to produce the initial lexicalized model.", "startOffset": 59, "endOffset": 80}, {"referenceID": 15, "context": "In order to solve the compatibility issue as well as improve the POS tagging accuracy, we used the Stanford tagger (Toutanova et al., 2003) to retag the BLLIP corpus and selected the sentences for which the new tags are consistent with the original tags, which resulted in 182244 sentences with length less than or equal to 10 after removing punctuations.", "startOffset": 115, "endOffset": 139}, {"referenceID": 11, "context": "0 (CTB) after converting data to dependency structures via Penn2Malt (Nivre, 2006) and then stripping off punctuations.", "startOffset": 69, "endOffset": 82}, {"referenceID": 13, "context": "Brown Laboratory for Linguistic Information Processing (BLLIP) 1987-89 WSJ Corpus Release 1 ral network by learning a CBOW model using the Gensim package (\u0158eh\u016f\u0159ek and Sojka, 2010).", "startOffset": 154, "endOffset": 179}, {"referenceID": 1, "context": "0 TSG-DMV (Blunsom and Cohn, 2010) 65.", "startOffset": 10, "endOffset": 34}, {"referenceID": 3, "context": "1 PR-S (Gillenwater et al., 2010) 64.", "startOffset": 7, "endOffset": 33}, {"referenceID": 10, "context": "3 HDP-DEP (Naseem et al., 2010) 73.", "startOffset": 10, "endOffset": 31}, {"referenceID": 16, "context": "8 UR-A E-DMV (Tu and Honavar, 2012) 71.", "startOffset": 13, "endOffset": 35}, {"referenceID": 5, "context": "0 Neural E-DMV(Jiang et al., 2016) 72.", "startOffset": 14, "endOffset": 34}, {"referenceID": 1, "context": "Systems Using Lexical Information and/or More Data LexTSG-DMV (Blunsom and Cohn, 2010) 67.", "startOffset": 62, "endOffset": 86}, {"referenceID": 14, "context": "8 CS (Spitkovsky et al., 2013) 72.", "startOffset": 5, "endOffset": 30}, {"referenceID": 7, "context": "4 MaxEnc (Le and Zuidema, 2015) 73.", "startOffset": 9, "endOffset": 31}, {"referenceID": 6, "context": "We compare four initialization methods to LNDMV: uniform initialization, random initialization, KM initialization (Klein and Manning, 2004), and good initialization as described in section 2.", "startOffset": 114, "endOffset": 139}, {"referenceID": 12, "context": "Note that our results are different from those by Pate and Johnson (2016), who found that uniform initialization leads to similar performance to KM initialization.", "startOffset": 50, "endOffset": 74}], "year": 2017, "abstractText": "We study the impact of big models (in terms of the degree of lexicalization) and big data (in terms of the training corpus size) on dependency grammar induction. We experimented with L-DMV, a lexicalized version of Dependency Model with Valence (Klein and Manning, 2004) and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence (Jiang et al., 2016). We find that L-DMV only benefits from very small degrees of lexicalization and moderate sizes of training corpora. L-NDMV can benefit from big training data and lexicalization of greater degrees, especially when enhanced with good model initialization, and it achieves a result that is competitive with the current state-of-the-art.", "creator": "LaTeX with hyperref package"}}}