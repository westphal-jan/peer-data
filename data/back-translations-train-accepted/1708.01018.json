{"id": "1708.01018", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Aug-2017", "title": "CRF Autoencoder for Unsupervised Dependency Parsing", "abstract": "Unsupervised dependency parsing, which tries to discover linguistic dependency structures from unannotated data, is a very challenging task. Almost all previous work on this task focuses on learning generative models. In this paper, we develop an unsupervised dependency parsing model based on the CRF autoencoder. The encoder part of our model is discriminative and globally normalized which allows us to use rich features as well as universal linguistic priors. We propose an exact algorithm for parsing as well as a tractable learning algorithm. We evaluated the performance of our model on eight multilingual treebanks and found that our model achieved comparable performance with state-of-the-art approaches.", "histories": [["v1", "Thu, 3 Aug 2017 06:45:31 GMT  (135kb,D)", "http://arxiv.org/abs/1708.01018v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiong cai", "yong jiang", "kewei tu"], "accepted": true, "id": "1708.01018"}, "pdf": {"name": "1708.01018.pdf", "metadata": {"source": "CRF", "title": "CRF Autoencoder for Unsupervised Dependency Parsing\u2217", "authors": ["Jiong Cai", "Yong Jiang", "Kewei Tu"], "emails": ["tukw}@shanghaitech.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year it has come to the point that it will be able to retaliate, \"he said.\" We have never lost as much time as this year, \"he said.\" We have never come as far as this year, \"he said."}, {"heading": "2 Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Model", "text": "Figure 1 shows our model using an example input set. For an input set x = (x1, x2,.., xn), we consider its parse tree as a latent structure, represented by a sequence y = (y1, y2,.., yn) in which yi is a pair < ti, hi >, ti is the header of the dependency that connects to the token xi in the parse tree, and hi is the index of that header token in the set. The model also contains a reconstruction output that is a token sequence x = (x-1, x-2,., x-n). Throughout the course of this paper, we place x-x = x.The encoder in our model is a log-linear model represented by a first-order dependency tree."}, {"heading": "2.1.1 Features", "text": "According to McDonald et al. (2005) and Grave et al. (2015), we define the characteristic vector of a dependency based on the part-of-speech tags (POS) of the head, child and context words, the direction and distance between the head and child of the dependency. The characteristic template used in our parser is shown in Table 1."}, {"heading": "2.1.2 Parsing", "text": "Considering the parameters w and \u03b8, we can parse a set of x by looking for a dependency tree y with the highest probability P (x, y | x). y \u043a = arg max y (x, y | x) logP (x, y | x) = arg max y \u2211 Y (x) n \u2211 i = 1 (\u03c6 (x, hi, i) + log \u03b8x \u0418i | ti) For the projective dependency saver, we can use Eisner's algorithm (1996) to find the best parser in O (n3) time. For the non-projective dependency saver, we can use the Chu-Liu / Edmond algorithm (Chu and Liu, 1965; Edmonds, 1967; Tarjan, 1977) to find the best parser in O (n2) time."}, {"heading": "2.2 Parameter Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.2.1 Objective Function", "text": "Spitkovsky et al. (2010) show that Viterbi EM can improve the performance of unattended dependency analyses compared to EM. Thus, instead of using negative conditional log probability as our objective function, we opt for negative conditional Viterbi log probability, \u2212 N \u2211 i = 1 log (max y Y (xi) P (x \u0441i, y | xi))) + 5 (1), where \"w\" is an L1 regularization term of the encoder parameter w and \u03bb is a hyperparameter that controls the strength of the regulation. In order to promote the learning of dependency relationships that satisfy universal linguistic knowledge, we add a soft constraint to the parameter tree based on the universal syntactical rules according to Naseem et al. (2010) and \"Grave et al al\" (2015). This results in our objective function N \u2212 xi int = 1 int (Q) (Q) and (X) x (Q) being the factor."}, {"heading": "2.2.2 Algorithm", "text": "In each optimization step of w, we perform two epochs of stochastic gradient descent, and in each optimization step of \u03b8, we perform two iterations of the Viterbi EM algorithm. To update w with stochastic gradient descent, we can first run the parsing algorithm for each set x to find the best parse tree y. (x) (P (x, y | x) Q\u03b1 (x, y)); then we can calculate the gradient of the objective function based on the following derivative."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Setup", "text": "We experimented with projective parsing and used the sound initialization method proposed by Klein and Manning (2004) to initialize our model before learning it. We tested our model both with and without the use of universal linguistic rules. We used AdaGrad to optimize w. We used POS tags of the input set as symbols in our model. We learned our model from training sets of length \u2264 10 and reported on the directed dependence on test sets of length \u2264 10 and on all test sets."}, {"heading": "3.2 Results on English", "text": "We tested our model on the corpus of the Wall Street Journal. We trained our model on sections 2-21, optimized the hyperparameters on sections 22 and tested our model on sections 23. Table 3 shows the directional dependency accuracy of our model (CRFAE) compared to recently published results. It turns out that our method performs comparably with modern systems. We also compared the performance of the CRF autoencoder with an objective function and negative log probability compared to the use of our Viterbi version of the objective function (Eq.1). We found that the Viterbi version leads to significantly better performance (55.7 vs. 41.8 at the parsing accuracy of WSJ), reflecting Spitkovsky et al.'s results on Viterbi EM (2010)."}, {"heading": "3.3 Multilingual Results", "text": "We examined our model using seven languages from the PASCAL Challenge on Grammar Induction (Geling et al., 2012). We did not use the Arabic corpus because the number of training sets with a length of \u2264 10 is less than 1000. The result is shown in Table 4. The accuracy of DMV and neuronal DMV comes from Jiang et.al (2016). Both our model (CRFAE) and Convex-MST were matched to the validation quantity of each corpus. It shows that our method achieves the best results on average. In addition, our method ConvexMST performs better both with and without linguistic knowledge. The results also show that the use of universal linguistic knowledge significantly improves the performance of Convex-MST and our model."}, {"heading": "4 Conclusion", "text": "In this article, we propose a new discrimination model for unattended dependency analysis based on the CRF autoencoder. Both the learning and the conclusion of our model are comprehensible. We tested our method in eight languages and show that our model competes with the state-of-the-art systems. Code is available at https: / / github. com / caijiong / CRFAE-Dep-Parser."}], "references": [{"title": "Conditional random field autoencoders for unsupervised structured prediction", "author": ["Waleed Ammar", "Chris Dyer", "Noah A Smith."], "venue": "Advances in Neural Information Processing Systems, pages 3311\u20133319.", "citeRegEx": "Ammar et al\\.,? 2014", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "Painless unsupervised learning with features", "author": ["Taylor Berg-Kirkpatrick", "Alexandre Bouchard-C\u00f4t\u00e9", "John DeNero", "Dan Klein."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association", "citeRegEx": "Berg.Kirkpatrick et al\\.,? 2010", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2010}, {"title": "Unsupervised induction of tree substitution grammars for dependency parsing", "author": ["Phil Blunsom", "Trevor Cohn."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1204\u20131213. Association for Com-", "citeRegEx": "Blunsom and Cohn.,? 2010", "shortCiteRegEx": "Blunsom and Cohn.", "year": 2010}, {"title": "On shortest arborescence of a directed graph", "author": ["Yoeng-Jin Chu", "Tseng-Hong Liu."], "venue": "Scientia Sinica, 14(10):1396.", "citeRegEx": "Chu and Liu.,? 1965", "shortCiteRegEx": "Chu and Liu.", "year": 1965}, {"title": "Logistic normal priors for unsupervised probabilistic grammar induction", "author": ["Shay B Cohen", "Kevin Gimpel", "Noah A Smith."], "venue": "Advances in Neural Information Processing Systems, pages 321\u2013328.", "citeRegEx": "Cohen et al\\.,? 2008", "shortCiteRegEx": "Cohen et al\\.", "year": 2008}, {"title": "Optimum branchings", "author": ["Jack Edmonds."], "venue": "Journal of Research of the National Bureau of Standards B, 71(4):233\u2013240.", "citeRegEx": "Edmonds.,? 1967", "shortCiteRegEx": "Edmonds.", "year": 1967}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["Jason M Eisner."], "venue": "Proceedings of the 16th conference on Computational linguistics-Volume 1, pages 340\u2013345. Association for Computational Linguistics.", "citeRegEx": "Eisner.,? 1996", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "The pascal challenge on grammar induction", "author": ["Douwe Gelling", "Trevor Cohn", "Phil Blunsom", "Joao Gra\u00e7a."], "venue": "Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 64\u201380. Association for Computational Linguistics.", "citeRegEx": "Gelling et al\\.,? 2012", "shortCiteRegEx": "Gelling et al\\.", "year": 2012}, {"title": "A convex and feature-rich discriminative approach to dependency grammar induction", "author": ["Edouard Grave", "No\u00e9mie Elhadad."], "venue": "ACL (1), pages 1375\u2013 1384.", "citeRegEx": "Grave and Elhadad.,? 2015", "shortCiteRegEx": "Grave and Elhadad.", "year": 2015}, {"title": "Unsupervised neural dependency parsing", "author": ["Yong Jiang", "Wenjuan Han", "Kewei Tu."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 763\u2013771, Austin, Texas. Association for Computational Lin-", "citeRegEx": "Jiang et al\\.,? 2016", "shortCiteRegEx": "Jiang et al\\.", "year": 2016}, {"title": "Corpusbased induction of syntactic structure: Models of dependency and constituency", "author": ["Dan Klein", "Christopher D Manning."], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 478. Association for Com-", "citeRegEx": "Klein and Manning.,? 2004", "shortCiteRegEx": "Klein and Manning.", "year": 2004}, {"title": "Structured prediction models via the matrix-tree theorem", "author": ["Terry Koo", "Amir Globerson", "Xavier Carreras P\u00e9rez", "Michael Collins."], "venue": "Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language", "citeRegEx": "Koo et al\\.,? 2007", "shortCiteRegEx": "Koo et al\\.", "year": 2007}, {"title": "Unsupervised dependency parsing: Let\u2019s use supervised parsers", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Le and Zuidema.,? 2015", "shortCiteRegEx": "Le and Zuidema.", "year": 2015}, {"title": "Online large-margin training of dependency parsers", "author": ["Ryan McDonald", "Koby Crammer", "Fernando Pereira."], "venue": "Proceedings of the 43rd annual meeting on association for computational linguistics, pages 91\u201398. Association for Computa-", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Using universal linguistic knowledge to guide grammar induction", "author": ["Tahira Naseem", "Harr Chen", "Regina Barzilay", "Mark Johnson."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1234\u20131244. Asso-", "citeRegEx": "Naseem et al\\.,? 2010", "shortCiteRegEx": "Naseem et al\\.", "year": 2010}, {"title": "Cubic-time parsing and learning algorithms for grammatical bigram models", "author": ["Mark A Paskin."], "venue": "Citeseer.", "citeRegEx": "Paskin.,? 2001", "shortCiteRegEx": "Paskin.", "year": 2001}, {"title": "Breaking out of local optima with count transforms and model recombination: A study in grammar induction", "author": ["Valentin I Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky."], "venue": "EMNLP, pages 1983\u2013 1995.", "citeRegEx": "Spitkovsky et al\\.,? 2013", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2013}, {"title": "Viterbi training improves unsupervised dependency parsing", "author": ["Valentin I Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky", "Christopher D Manning."], "venue": "Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 9\u201317. As-", "citeRegEx": "Spitkovsky et al\\.,? 2010", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2010}, {"title": "Finding optimum branchings", "author": ["Robert Endre Tarjan."], "venue": "Networks, 7(1):25\u201335.", "citeRegEx": "Tarjan.,? 1977", "shortCiteRegEx": "Tarjan.", "year": 1977}, {"title": "Unambiguity regularization for unsupervised learning of probabilistic grammars", "author": ["Kewei Tu", "Vasant Honavar."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-", "citeRegEx": "Tu and Honavar.,? 2012", "shortCiteRegEx": "Tu and Honavar.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "Many approaches have been proposed to enhance these generative models, for example, by designing advanced Bayesian priors (Cohen et al., 2008), representing dependencies with features (Berg-Kirkpatrick et al.", "startOffset": 122, "endOffset": 142}, {"referenceID": 1, "context": ", 2008), representing dependencies with features (Berg-Kirkpatrick et al., 2010), and representing discrete tokens with continuous vectors (Jiang et al.", "startOffset": 49, "endOffset": 80}, {"referenceID": 9, "context": ", 2010), and representing discrete tokens with continuous vectors (Jiang et al., 2016).", "startOffset": 66, "endOffset": 86}, {"referenceID": 6, "context": "Most of the previous work on unsupervised dependency parsing is based on generative models such as the dependency model with valence (DMV) introduced by Klein and Manning (2004). Many approaches have been proposed to enhance these generative models, for example, by designing advanced Bayesian priors (Cohen et al.", "startOffset": 153, "endOffset": 178}, {"referenceID": 1, "context": ", 2008), representing dependencies with features (Berg-Kirkpatrick et al., 2010), and representing discrete tokens with continuous vectors (Jiang et al., 2016). Besides generative approaches, Grave and Elhadad (2015) proposed an unsupervised discrim\u2217This work was supported by the National Natural Science Foundation of China (61503248).", "startOffset": 50, "endOffset": 217}, {"referenceID": 0, "context": "Conditional random field autoencoder (Ammar et al., 2014) is a new framework for unsupervised structured prediction.", "startOffset": 37, "endOffset": 57}, {"referenceID": 0, "context": "Conditional random field autoencoder (Ammar et al., 2014) is a new framework for unsupervised structured prediction. There are two components of this model: an encoder and a decoder. The encoder is a globally normalized feature-rich CRF model predicting the conditional distribution of the latent structure given the observed structured input. The decoder of the model is a generative model generating a transformation of the structured input from the latent structure. Ammar et al. (2014) applied the model to two sequential structured prediction tasks, part-of-speech induction and word alignment and showed that by utilizing context information the model can achieve better performance than previous generative models and locally normalized models.", "startOffset": 38, "endOffset": 490}, {"referenceID": 15, "context": "The partition function can be efficiently computed in O(n3) time using a variant of the inside-outside algorithm (Paskin, 2001) for projective tree structures, or using the Matrix-Tree Theorem for nonprojective tree structures (Koo et al.", "startOffset": 113, "endOffset": 127}, {"referenceID": 11, "context": "The partition function can be efficiently computed in O(n3) time using a variant of the inside-outside algorithm (Paskin, 2001) for projective tree structures, or using the Matrix-Tree Theorem for nonprojective tree structures (Koo et al., 2007).", "startOffset": 227, "endOffset": 245}, {"referenceID": 13, "context": "1 Features Following McDonald et al. (2005) and Grave et al.", "startOffset": 21, "endOffset": 44}, {"referenceID": 13, "context": "1 Features Following McDonald et al. (2005) and Grave et al. (2015), we define the feature vector of a dependency based on the part-of-speech tags (POS) of the head, child and context words, the direction, and the distance between the head and child of the dependency.", "startOffset": 21, "endOffset": 68}, {"referenceID": 3, "context": "For non-projective dependency parsing, we can use the Chu-Liu/Edmond algorithm (Chu and Liu, 1965; Edmonds, 1967; Tarjan, 1977) to find the best parse in O(n2) time.", "startOffset": 79, "endOffset": 127}, {"referenceID": 5, "context": "For non-projective dependency parsing, we can use the Chu-Liu/Edmond algorithm (Chu and Liu, 1965; Edmonds, 1967; Tarjan, 1977) to find the best parse in O(n2) time.", "startOffset": 79, "endOffset": 127}, {"referenceID": 18, "context": "For non-projective dependency parsing, we can use the Chu-Liu/Edmond algorithm (Chu and Liu, 1965; Edmonds, 1967; Tarjan, 1977) to find the best parse in O(n2) time.", "startOffset": 79, "endOffset": 127}, {"referenceID": 4, "context": "For projective dependency parsing, we can use Eisners algorithm (1996) to find the best parse in O(n3) time.", "startOffset": 46, "endOffset": 71}, {"referenceID": 16, "context": "1 Objective Function Spitkovsky et al. (2010) shows that Viterbi EM can improve the performance of unsupervised dependency parsing in comparison with EM.", "startOffset": 21, "endOffset": 46}, {"referenceID": 14, "context": "To encourage learning of dependency relations that satisfy universal linguistic knowledge, we add a soft constraint on the parse tree based on the universal syntactic rules following Naseem et al. (2010) and Grave et al.", "startOffset": 183, "endOffset": 204}, {"referenceID": 14, "context": "To encourage learning of dependency relations that satisfy universal linguistic knowledge, we add a soft constraint on the parse tree based on the universal syntactic rules following Naseem et al. (2010) and Grave et al. (2015). Hence our objective function becomes", "startOffset": 183, "endOffset": 228}, {"referenceID": 14, "context": "The universal linguistic rules that we use are shown in Table 2 (Naseem et al., 2010).", "startOffset": 64, "endOffset": 85}, {"referenceID": 10, "context": "Our model is compared with DMV (Klein and Manning, 2004), Neural DMV (Jiang et al.", "startOffset": 31, "endOffset": 56}, {"referenceID": 9, "context": "Our model is compared with DMV (Klein and Manning, 2004), Neural DMV (Jiang et al., 2016), and Convex-MST (Grave and Elhadad, 2015)", "startOffset": 69, "endOffset": 89}, {"referenceID": 8, "context": ", 2016), and Convex-MST (Grave and Elhadad, 2015)", "startOffset": 24, "endOffset": 49}, {"referenceID": 1, "context": "Methods WSJ10 WSJ Basic Setup Feature DMV (Berg-Kirkpatrick et al., 2010) 63.", "startOffset": 42, "endOffset": 73}, {"referenceID": 19, "context": "0 UR-A E-DMV (Tu and Honavar, 2012) 71.", "startOffset": 13, "endOffset": 35}, {"referenceID": 9, "context": "0 Neural E-DMV (Jiang et al., 2016) 69.", "startOffset": 15, "endOffset": 35}, {"referenceID": 9, "context": "5 Neural E-DMV (Good Init) (Jiang et al., 2016) 72.", "startOffset": 27, "endOffset": 47}, {"referenceID": 8, "context": "6 Basic Setup + Universal Linguistic Prior Convex-MST (Grave and Elhadad, 2015) 60.", "startOffset": 54, "endOffset": 79}, {"referenceID": 14, "context": "6 HDP-DEP (Naseem et al., 2010) 71.", "startOffset": 10, "endOffset": 31}, {"referenceID": 2, "context": "7 Systems Using Extra Info LexTSG-DMV (Blunsom and Cohn, 2010) 67.", "startOffset": 38, "endOffset": 62}, {"referenceID": 16, "context": "7 CS (Spitkovsky et al., 2013) 72.", "startOffset": 5, "endOffset": 30}, {"referenceID": 12, "context": "4 MaxEnc (Le and Zuidema, 2015) 73.", "startOffset": 9, "endOffset": 31}, {"referenceID": 11, "context": "The expected count can be efficiently computed using the Matrix-Tree Theorem (Koo et al., 2007) for non-projective tree structures or using a variant of the inside-outside algorithm for projective tree structures (Paskin, 2001).", "startOffset": 77, "endOffset": 95}, {"referenceID": 15, "context": ", 2007) for non-projective tree structures or using a variant of the inside-outside algorithm for projective tree structures (Paskin, 2001).", "startOffset": 125, "endOffset": 139}, {"referenceID": 10, "context": "We experimented with projective parsing and used the informed initialization method proposed by Klein and Manning (2004) to initialize our model before learning.", "startOffset": 96, "endOffset": 121}, {"referenceID": 16, "context": "8 in parsing accuracy of WSJ), which echoes Spitkovsky et al. \u2019s findings on Viterbi EM (2010).", "startOffset": 44, "endOffset": 95}, {"referenceID": 7, "context": "3 Multilingual Results We evaluated our model on seven languages from the PASCAL Challenge on Grammar Induction (Gelling et al., 2012).", "startOffset": 112, "endOffset": 134}, {"referenceID": 7, "context": "3 Multilingual Results We evaluated our model on seven languages from the PASCAL Challenge on Grammar Induction (Gelling et al., 2012). We did not use the Arabic corpus because the number of training sentences with length \u2264 10 is less than 1000. The result is shown in Table 4. The accuracies of DMV and Neural DMV are from Jiang et.al (2016). Both our model (CRFAE) and Convex-MST were tuned on the validation set of each corpus.", "startOffset": 113, "endOffset": 343}], "year": 2017, "abstractText": "Unsupervised dependency parsing, which tries to discover linguistic dependency structures from unannotated data, is a very challenging task. Almost all previous work on this task focuses on learning generative models. In this paper, we develop an unsupervised dependency parsing model based on the CRF autoencoder. The encoder part of our model is discriminative and globally normalized which allows us to use rich features as well as universal linguistic priors. We propose an exact algorithm for parsing as well as a tractable learning algorithm. We evaluated the performance of our model on eight multilingual treebanks and found that our model achieved comparable performance with state-of-the-art approaches.", "creator": "LaTeX with hyperref package"}}}