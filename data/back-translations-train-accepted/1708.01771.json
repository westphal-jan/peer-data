{"id": "1708.01771", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Aug-2017", "title": "Neural Machine Translation with Word Predictions", "abstract": "In the encoder-decoder architecture for neural machine translation (NMT), the hidden states of the recurrent structures in the encoder and decoder carry the crucial information about the sentence.These vectors are generated by parameters which are updated by back-propagation of translation errors through time. We argue that propagating errors through the end-to-end recurrent structures are not a direct way of control the hidden vectors. In this paper, we propose to use word predictions as a mechanism for direct supervision. More specifically, we require these vectors to be able to predict the vocabulary in target sentence. Our simple mechanism ensures better representations in the encoder and decoder without using any extra data or annotation. It is also helpful in reducing the target side vocabulary and improving the decoding efficiency. Experiments on Chinese-English and German-English machine translation tasks show BLEU improvements by 4.53 and 1.3, respectively", "histories": [["v1", "Sat, 5 Aug 2017 13:38:10 GMT  (1823kb,D)", "http://arxiv.org/abs/1708.01771v1", "Accepted at EMNLP2017"]], "COMMENTS": "Accepted at EMNLP2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rongxiang weng", "shujian huang", "zaixiang zheng", "xin-yu dai", "jiajun chen"], "accepted": true, "id": "1708.01771"}, "pdf": {"name": "1708.01771.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Translation with Word Predictions", "authors": ["Rongxiang Weng", "Shujian Huang", "Zaixiang Zheng", "Xinyu Dai", "Jiajun Chen"], "emails": ["chenjj}@nlp.nju.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "The aforementioned cerebral consecrated cerebral consecrated cerebral consecrated cerebral consecrated cerebral consecrated in the cerebral consecrated in the cerebral consecrated in the cerebral geeeeaeaeBnln nln nlrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaeeBnln in the cerebral geeeeaeeeeeeeeeeeeeeeeeaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaeVnlllrrrrrlu nlu nlrlrlrlrlrlu nlrllrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "2 Related Work", "text": "Many previous papers have noted the problem of forming an NMT system with many parameters, but some of them prefer to use the dropout technique (Srivastava et al., 2014; Luong et al., 2015b; Meng et al., 2016). Another possible choice is to combine several random starting point models (Sutskever et al., 2014; Jean et al., 2015; Luong and Manning, 2016). Both techniques could yield more stable and better results, but they are general neural network training techniques that do not specifically target the modelling of the translation process like ours. We will perform an empirical comparison with them in the experiments. The way we add the word prediction is similar to the exploration of multi-task learning. Dong et al. (2015) suggest sharing an encoder between different translation tasks. Luong et al al al, the common coding task, the parsing for different languages (2015a) and the parsing task."}, {"heading": "3 Notations and Backgrounds", "text": "We present a popular NMT framework with the encoder decoder architecture (Cho et al., 2014; Bahdanau et al., 2014) and attention networks (Luong et al., 2015b) on the basis of which we propose our word prediction mechanisms. Name a source-target sentence pair as {x, y} from the educational sentence, where x is the word sequence (x1, x2, \u00b7 x |) and y is the word sequence (y1, y2, y | y |), where x is the length of x and y the respective word sequence. In the encoding phase, a bidirectional neural network is used (Bahdanau et al., 2014) to encode x into a sequence of vectors (h1, \u00b7, h | x)."}, {"heading": "4 NMT with Word Predictions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Word Prediction for the Initial State", "text": "The decoder starts by generating the target sentence from the initial state s0 (equation 6), which is generated by the encoder. (Currently, the encoder is only updated if a translation error occurs in the encoder. The error is propagated by several time steps in the recurring structure until it reaches the encoder. (Since there are hundreds of millions of parameters in the NMT system, it is difficult for the model to learn the exact representation of the source sentences. As a result, the values of the initial state should not be exact during the translation process, which leads to poor translation performance. (We propose word prediction as a mechanism to control the values of the initial state.) The intuition is that the initial state is responsible for the translation of the entire target sentence, it should contain at least information of each word in the target sentence, which leads to poor translation performance. Thus, we optimize the initial state by making predictions for all the target words."}, {"heading": "4.2 Word Predictions for Decoder\u2019s Hidden States", "text": "Similar intuition also applies to the decoder. As the hidden states of the decoder are responsible for translating target words, they should also be able to predict the target words. The only difference is that we remove the already generated words from the prediction task. Therefore, any hidden state in the decoder is required to predict the target words that will not be translated. For the first state s1 of the decoder, the prediction task is identical to the task for the initial state. Since then, the prediction is no longer a separate training task, but integrated into every time step of the training process. We refer to this mechanism as WPD. As shown in Figure 2, for each time step j in the decoder, the hidden state sj is used for predicting (yj, yj + 1, \u00b7 \u00b7, \u00b7 \u00b7, y | y | y | |). The conditional probability of WPD is defined as: PWPD (yj, yj + 1, \u00b7 \u00b7 ltj, y, y, yp, yp, (yp, j, yp, yp, yp, yp, yp, yp, yp, yp, yp, j, yp, yp, yp, yp, yp, yp, yp, yp ()."}, {"heading": "4.3 Training", "text": "NMT models optimize the networks by maximizing the probability of the target translation y of the source sentence x, defined by LT.LT = 1 | y | | y | \u2211 j = 1 logP (yj | y < j, x) (22), where P (yj | y < j, x) is defined in Equation 7. To optimize the word prediction mechanism, we propose to include additional probability functions LWPE and LWPD in the training sequence. For the WPE, we optimize the probability of translation and word prediction directly as: L1 = LT + LWPE (23) LWPE = logPWPE (24), where PWPE is defined in Equation 14. For the WPD, we optimize the probability as: L2 = LT + LWPD (25) LWPD = | y | j + 1 logPWPD (26), where PWPD is defined in equation that the two arithmetic mechanisms together result in 27 WPD."}, {"heading": "4.4 Making Use of the Word Predictor", "text": "The previously proposed word prediction mechanism could only be used as an additional training target that is not calculated during translation, thus keeping the computational complexity of our translation models exactly the same. On the other hand, using a smaller and specific vocabulary for each sentence or stack improves translation efficiency. If the vocabulary is accurate enough, there is also the chance to improve translation quality (Jean et al., 2015; Mi et al., 2016; L'Hostis et al., 2016). Our word prediction mechanism WPE offers a natural solution for generating a possible set of target words at the sentence level. Prediction could be made from the initial s0 state without using additional resources such as dictionaries, extracted phrases or frequent word lists, as in Mi et al. (2016)."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Data", "text": "We conduct experiments on the tasks of machine translation Chinese-English (CH-EN) and German-English (DE-EN). For the CH-EN, the training data consists of approximately 8 million sentence pairs 1. We use NIST MT02 as validation set and the test sets NIST MT03, MT04 and MT05 as test sets. These sets have 878, 919, 1597 and 1082 source sets, respectively, with 4 references for each sentence. For the DE-EN, the experiments were trained on the standard benchmark WMT14, and there are about 4.5 million sentence pairs. As validation sets, we use newtest 2013 (NST13) and newtest 2014 (NST14) as test sets. These sentences have 3000 and 2737 source sets, respectively, with a reference for each sentence. Sentences were coded using byte pair coding (BPE) (Britz et al., 2017)."}, {"heading": "5.2 Systems and Techniques", "text": "We implement a base system with the bidirectional encoder (Bahdanau et al., 2014) and the attention mechanism (Luong et al., 2015b) as described in Section 3, referred to as baseNMT. Then, our proposed word prediction mechanisms for the initial state and the hidden states of the decoder are implemented on the baseNMT system, referred to as WPE or WPD. We refer to the system used as LDC2002E18, LDC2003E07, LDC2003E14, LDC2004E12, LDC2004T08, LDC2005T06, LDC2005T10, LDC2006E26 and LDC2007T09both techniques as WPED. We implement systems with variable vocabulary following the output techniques (Mi et al., 2016) For comparison with output systems, we also implement (with 0.5) ensembles."}, {"heading": "5.3 Implementation Details", "text": "Both our CH-EN and DE-EN experiments are implemented on the dl4mt 2 open source toolkit, with most of the default parameter settings remaining the same. We train the NMT systems with sentences of up to 50 words. The source and target vocabulary is limited to the most common 30K words for Chinese or English, with the words outside the vocabulary limited to a special UNK token. The dimension of the word embedding is set to 512 and the size of the hidden layer is 1024. The recurring weight matrices are initialized as random orthogonal matrices, and all preset vectors as zero. Other parameters are initialized by samples from the Gaussian distribution N (0, 0.01). We use the mini-batch stochastic gradient descent (SGD) approach to update the parameters, with a batch size of 32. The learning rate is pre-trained by Adadelta (for our pre-time control system)."}, {"heading": "5.4 Translation Experiments", "text": "In order to see the effects of word predictions in translation, we evaluate these systems in case-insensitive IBM-BLEU (Papineni et al., 2002) for both CHEN and DE-EN tasks. Detailed results are shown in Table 1 and Table 2. Compared to the baseNMT system, all our models achieve significant improvements. On the CH-EN experiment, the simple addition of word predictions to the initial state (WPE) already leads to significant improvements. The average improvement in the test rate is 2.53 BLEU, which shows that the limitation of the starting state leads to higher translation quality. Adding word predic-2https: / github.com / nyu-dl / dl4mt-tutorialtions to the hidden states in the decoder (WPD) leads to further improvements against baseNMT (4.15 BLEU), as the WPD supplements the restrictions on state transitions by different time steps in the decoder."}, {"heading": "5.5 Word Prediction Experiments", "text": "Since we include an explicit word prediction mechanism during the training of NMT systems, we also evaluate the prediction performance on the CH-EN experiments to see how the training is improved. For each sentence in the test set, we use the initial state of the given model to make predictions about the possible words. We refer to the set of top-n words as Tn, the word amount in all references. We define the prediction performance of BaseNMT and WPE as follows: Precision = Tn-words 100% (28) Recall = Tn-words 100% (29) We compare the prediction performance of BaseNMT and WPE. WPED has similar prediction results with WPE, so we omit their results. As shown in Table 5, baseNMT system has a relatively lower prediction accuracy, for example 45% in the top-10 prediction."}, {"heading": "5.6 Improving Decoding Efficiency", "text": "To make use of the word prediction, we conduct experiments with the predicted vocabulary, with different vocabulary sizes (1k to 10k) on the CHEN experiments, known as WPE-V and WPED-V. Comparisons are made both in the translation quality and in the decryption time. Since all our fixed vocabulary size models have exactly the same number of decryption parameters (additional vocabulary is used only for training), we record only the decryption time of the WPED for comparison. Figure 4 and 5 show the results. If we start the experiments with the uppermost vocabulary 1k (1 / 30 of the basic vocabulary), the translation quality of both WPE-V and WPED-V will already be higher than the baseNMT; while their decryption time is less than 1 / 3 of an NMT system with 30k vocabularies."}, {"heading": "5.7 Translation Analysis", "text": "We also analyze real-world translations to see the difference between the different systems (Table 6). It is easy to see that the baseNMT system does not take into account the translations of several important words such as \"advertising,\" \"1,5,\" which are underlined in the reference. It also erroneously translates the company name \"time warner Inc.\" as redundant information for \"Internet companies\"; \"america online\" as \"us line.\" The results of dropouts or ensembles show improvements compared to baseNMT. But they still make mistakes in translating \"online\" and the company name \"time warner Inc.\" With WPED, most of these errors no longer exist, because we force the encoder and decoder to carry the exact information during translation."}, {"heading": "6 Conclusions", "text": "In fact, we are able to assert ourselves, we are able to assert ourselves, we are able to assert ourselves, and we are able to assert ourselves, we are able to assert ourselves, we are able to assert ourselves, we are able to assert ourselves."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Statistical machine translation through", "author": ["Srinivas Bangalore", "Patrick Haffner", "Stephan Kanthak"], "venue": null, "citeRegEx": "Bangalore et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bangalore et al\\.", "year": 2007}, {"title": "Massive Exploration of Neural Machine Translation Architectures", "author": ["Denny Britz", "Anna Goldi", "Minh-Thang Luong", "Quoc Le."], "venue": "ArXiv e-prints .", "citeRegEx": "Britz et al\\.,? 2017", "shortCiteRegEx": "Britz et al\\.", "year": 2017}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1412.3555. http://arxiv.org/abs/1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Multi-task learning for multiple language translation", "author": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "A discriminative lexicon model for complex morphology", "author": ["Minwoo Jeong", "Kristina Toutanova", "Hisami Suzuki", "Chris Quirk."], "venue": "Proceedings of the Ninth Conference of the Association for Machine Translation in the Americas (AMTA 2010).", "citeRegEx": "Jeong et al\\.,? 2010", "shortCiteRegEx": "Jeong et al\\.", "year": 2010}, {"title": "Vocabulary selection strategies for neural machine translation", "author": ["Gurvan L\u2019Hostis", "David Grangier", "Michael Auli"], "venue": "CoRR abs/1610.00072", "citeRegEx": "L.Hostis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "L.Hostis et al\\.", "year": 2016}, {"title": "Multitask sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "CoRR abs/1511.06114. http://arxiv.org/abs/1511.06114.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Achieving open vocabulary neural machine translation with hybrid wordcharacter models", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "CoRR abs/1604.00788. http://arxiv.org/abs/1604.00788.", "citeRegEx": "Luong and Manning.,? 2016", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Extending statistical machine translation with discriminative and trigger-based lexicon models", "author": ["Arne Mauser", "Sa\u0161a Hasan", "Hermann Ney."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. Associa-", "citeRegEx": "Mauser et al\\.,? 2009", "shortCiteRegEx": "Mauser et al\\.", "year": 2009}, {"title": "Interactive attention for neural machine translation", "author": ["Fandong Meng", "Zhengdong Lu", "Hang Li", "Qun Liu."], "venue": "CoRR abs/1610.05011. http://arxiv.org/abs/1610.05011.", "citeRegEx": "Meng et al\\.,? 2016", "shortCiteRegEx": "Meng et al\\.", "year": 2016}, {"title": "Vocabulary manipulation for neural machine translation", "author": ["Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah."], "venue": "CoRR abs/1605.03209. http://arxiv.org/abs/1605.03209.", "citeRegEx": "Mi et al\\.,? 2016", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Asso-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "J. Mach. Learn. Res. 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Sys-", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Rnnbased encoder-decoder approach with word frequency estimation", "author": ["Jun Suzuki", "Masaaki Nagata."], "venue": "CoRR abs/1701.00138. http://arxiv.org/abs/1701.00138.", "citeRegEx": "Suzuki and Nagata.,? 2017", "shortCiteRegEx": "Suzuki and Nagata.", "year": 2017}, {"title": "Word translation prediction for morphologically rich languages with bilingual neural networks", "author": ["Ke Tran", "Arianna Bisazza", "Christof Monz."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Tran et al\\.,? 2014", "shortCiteRegEx": "Tran et al\\.", "year": 2014}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "CoRR abs/1212.5701. http://arxiv.org/abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Exploiting source-side monolingual data in neural machine translation", "author": ["Jiajun Zhang", "Chengqing Zong."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for", "citeRegEx": "Zhang and Zong.,? 2016", "shortCiteRegEx": "Zhang and Zong.", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "The encoder-decoder based neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014) have been developing rapidly.", "startOffset": 66, "endOffset": 108}, {"referenceID": 3, "context": "The encoder-decoder based neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014) have been developing rapidly.", "startOffset": 66, "endOffset": 108}, {"referenceID": 17, "context": "(2014) propose to encode the source sentence as a fixed-length vector representation, based on which the decoder generates the target sequence, where both the encoder and decoder are recurrent neural networks (RNN) (Sutskever et al., 2014) or their variants (Cho et al.", "startOffset": 215, "endOffset": 239}, {"referenceID": 3, "context": ", 2014) or their variants (Cho et al., 2014; Chung et al., 2014; Bahdanau et al., 2014).", "startOffset": 26, "endOffset": 87}, {"referenceID": 4, "context": ", 2014) or their variants (Cho et al., 2014; Chung et al., 2014; Bahdanau et al., 2014).", "startOffset": 26, "endOffset": 87}, {"referenceID": 0, "context": ", 2014) or their variants (Cho et al., 2014; Chung et al., 2014; Bahdanau et al., 2014).", "startOffset": 26, "endOffset": 87}, {"referenceID": 0, "context": "Later, attention mechanisms are proposed to enhance the source side representations (Bahdanau et al., 2014; Luong et al., 2015b).", "startOffset": 84, "endOffset": 128}, {"referenceID": 11, "context": "Later, attention mechanisms are proposed to enhance the source side representations (Bahdanau et al., 2014; Luong et al., 2015b).", "startOffset": 84, "endOffset": 128}, {"referenceID": 11, "context": "However, the hidden states in the recurrent decoder still originate from the single fixed-length representation (Luong et al., 2015b), or the average of the bi-directional representations (Bahdanau et al.", "startOffset": 112, "endOffset": 133}, {"referenceID": 0, "context": ", 2015b), or the average of the bi-directional representations (Bahdanau et al., 2014).", "startOffset": 63, "endOffset": 86}, {"referenceID": 1, "context": ", 2014; Cho et al., 2014) have been developing rapidly. Sutskever et al. (2014) propose to encode the source sentence as a fixed-length vector representation, based on which the decoder generates the target sequence, where both the encoder and decoder are recurrent neural networks (RNN) (Sutskever et al.", "startOffset": 8, "endOffset": 80}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2014). In this framework, the fixedlength vector plays the crucial role of transitioning the information of the sentence from the source side to the target side. Later, attention mechanisms are proposed to enhance the source side representations (Bahdanau et al., 2014; Luong et al., 2015b). The source side context is computed at each time-step of decoding, based on the attention weights between the source side representations and the current hidden state of the decoder. However, the hidden states in the recurrent decoder still originate from the single fixed-length representation (Luong et al., 2015b), or the average of the bi-directional representations (Bahdanau et al., 2014). Here we refer to the representation as initial state. Interestingly, Britz et al. (2017) find that the value of initial state does not affect the translation performance, and prefer to set the initial state to be a zero vector.", "startOffset": 8, "endOffset": 802}, {"referenceID": 16, "context": "Some of them prefer to use the dropout technique (Srivastava et al., 2014; Luong et al., 2015b; Meng et al., 2016).", "startOffset": 49, "endOffset": 114}, {"referenceID": 11, "context": "Some of them prefer to use the dropout technique (Srivastava et al., 2014; Luong et al., 2015b; Meng et al., 2016).", "startOffset": 49, "endOffset": 114}, {"referenceID": 13, "context": "Some of them prefer to use the dropout technique (Srivastava et al., 2014; Luong et al., 2015b; Meng et al., 2016).", "startOffset": 49, "endOffset": 114}, {"referenceID": 17, "context": "Another possible choice is to ensemble several models with random starting points (Sutskever et al., 2014; Jean et al., 2015; Luong and Manning, 2016).", "startOffset": 82, "endOffset": 150}, {"referenceID": 6, "context": "Another possible choice is to ensemble several models with random starting points (Sutskever et al., 2014; Jean et al., 2015; Luong and Manning, 2016).", "startOffset": 82, "endOffset": 150}, {"referenceID": 10, "context": "Another possible choice is to ensemble several models with random starting points (Sutskever et al., 2014; Jean et al., 2015; Luong and Manning, 2016).", "startOffset": 82, "endOffset": 150}, {"referenceID": 5, "context": "Dong et al. (2015) propose to share an encoder between different translation tasks.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "Dong et al. (2015) propose to share an encoder between different translation tasks. Luong et al. (2015a) propose to jointly learn the translation task for different languages, the parsing task and the image captioning task, with a shared encoder or decoder.", "startOffset": 0, "endOffset": 105}, {"referenceID": 5, "context": "Dong et al. (2015) propose to share an encoder between different translation tasks. Luong et al. (2015a) propose to jointly learn the translation task for different languages, the parsing task and the image captioning task, with a shared encoder or decoder. Zhang and Zong (2016) propose to use multitask learning for incorporating source side monolingual data.", "startOffset": 0, "endOffset": 280}, {"referenceID": 18, "context": "In the other sequence to sequence tasks, Suzuki and Nagata (2017) propose the idea for predicting words by using encoder information.", "startOffset": 41, "endOffset": 66}, {"referenceID": 1, "context": "The word prediction technique has been applied in the research of both statistical machine translation (SMT) (Bangalore et al., 2007; Mauser et al., 2009; Jeong et al., 2010; Tran et al., 2014) and NMT (Mi et al.", "startOffset": 109, "endOffset": 193}, {"referenceID": 12, "context": "The word prediction technique has been applied in the research of both statistical machine translation (SMT) (Bangalore et al., 2007; Mauser et al., 2009; Jeong et al., 2010; Tran et al., 2014) and NMT (Mi et al.", "startOffset": 109, "endOffset": 193}, {"referenceID": 7, "context": "The word prediction technique has been applied in the research of both statistical machine translation (SMT) (Bangalore et al., 2007; Mauser et al., 2009; Jeong et al., 2010; Tran et al., 2014) and NMT (Mi et al.", "startOffset": 109, "endOffset": 193}, {"referenceID": 19, "context": "The word prediction technique has been applied in the research of both statistical machine translation (SMT) (Bangalore et al., 2007; Mauser et al., 2009; Jeong et al., 2010; Tran et al., 2014) and NMT (Mi et al.", "startOffset": 109, "endOffset": 193}, {"referenceID": 14, "context": ", 2014) and NMT (Mi et al., 2016; L\u2019Hostis et al., 2016).", "startOffset": 16, "endOffset": 56}, {"referenceID": 8, "context": ", 2014) and NMT (Mi et al., 2016; L\u2019Hostis et al., 2016).", "startOffset": 16, "endOffset": 56}, {"referenceID": 3, "context": "We present a popular NMT framework with the encoder-decoder architecture (Cho et al., 2014; Bahdanau et al., 2014) and the attention networks (Luong et al.", "startOffset": 73, "endOffset": 114}, {"referenceID": 0, "context": "We present a popular NMT framework with the encoder-decoder architecture (Cho et al., 2014; Bahdanau et al., 2014) and the attention networks (Luong et al.", "startOffset": 73, "endOffset": 114}, {"referenceID": 11, "context": ", 2014) and the attention networks (Luong et al., 2015b), based on which we propose our word prediction mechanism.", "startOffset": 35, "endOffset": 56}, {"referenceID": 0, "context": "In the encoding stage, a bi-directional recurrent neural network is used (Bahdanau et al., 2014) to encode x into a sequence of vectors (h1,h2, \u00b7 \u00b7 \u00b7 ,h|x|).", "startOffset": 73, "endOffset": 96}, {"referenceID": 3, "context": "The gated recurrent unit (GRU) is used as the recurrent unit in each RNN, which is shown to have promising results in speech recognition and machine translation (Cho et al., 2014).", "startOffset": 161, "endOffset": 179}, {"referenceID": 0, "context": "In the decoding stage, the decoder starts with the initial state s0, which is the average of source representations (Bahdanau et al., 2014).", "startOffset": 116, "endOffset": 139}, {"referenceID": 11, "context": "and the context vector cj is from the attention mechanism (Luong et al., 2015b):", "startOffset": 58, "endOffset": 79}, {"referenceID": 9, "context": "The prediction task could be trained jointly with the translation task in a multi-task learning way (Luong et al., 2015a; Dong et al., 2015; Zhang and Zong, 2016), where both tasks share the same encoder.", "startOffset": 100, "endOffset": 162}, {"referenceID": 5, "context": "The prediction task could be trained jointly with the translation task in a multi-task learning way (Luong et al., 2015a; Dong et al., 2015; Zhang and Zong, 2016), where both tasks share the same encoder.", "startOffset": 100, "endOffset": 162}, {"referenceID": 21, "context": "The prediction task could be trained jointly with the translation task in a multi-task learning way (Luong et al., 2015a; Dong et al., 2015; Zhang and Zong, 2016), where both tasks share the same encoder.", "startOffset": 100, "endOffset": 162}, {"referenceID": 6, "context": "If the vocabulary is accurate enough, there is also a chance to improve the translation quality (Jean et al., 2015; Mi et al., 2016; L\u2019Hostis et al., 2016).", "startOffset": 96, "endOffset": 155}, {"referenceID": 14, "context": "If the vocabulary is accurate enough, there is also a chance to improve the translation quality (Jean et al., 2015; Mi et al., 2016; L\u2019Hostis et al., 2016).", "startOffset": 96, "endOffset": 155}, {"referenceID": 8, "context": "If the vocabulary is accurate enough, there is also a chance to improve the translation quality (Jean et al., 2015; Mi et al., 2016; L\u2019Hostis et al., 2016).", "startOffset": 96, "endOffset": 155}, {"referenceID": 6, "context": "If the vocabulary is accurate enough, there is also a chance to improve the translation quality (Jean et al., 2015; Mi et al., 2016; L\u2019Hostis et al., 2016). Our word prediction mechanism WPE provides a natural solution for generating a possible set of target words at sentence level. The prediction could be made from the initial state s0, without using extra resources such as word dictionaries, extracted phrases or frequent word lists, as in Mi et al. (2016).", "startOffset": 97, "endOffset": 462}, {"referenceID": 2, "context": "Sentences were encoded using byte-pair encoding (BPE) (Britz et al., 2017).", "startOffset": 54, "endOffset": 74}, {"referenceID": 0, "context": "We implement a baseline system with the bidirectional encoder (Bahdanau et al., 2014) and the attention mechanism (Luong et al.", "startOffset": 62, "endOffset": 85}, {"referenceID": 11, "context": ", 2014) and the attention mechanism (Luong et al., 2015b) as described in Section 3, denoted as baseNMT.", "startOffset": 36, "endOffset": 57}, {"referenceID": 14, "context": "We implement systems with variable-sized vocabulary following (Mi et al., 2016).", "startOffset": 62, "endOffset": 79}, {"referenceID": 20, "context": "The learning rate is controlled by AdaDelta (Zeiler, 2012).", "startOffset": 44, "endOffset": 58}, {"referenceID": 15, "context": "To see the effect of word predictions in translation, we evaluate these systems in case-insensitive IBM-BLEU (Papineni et al., 2002) on both CHEN and DE-EN tasks.", "startOffset": 109, "endOffset": 132}, {"referenceID": 6, "context": "baseNMT) is much higher than previous research of manipulating the vocabularies (Jean et al., 2015; Mi et al., 2016; L\u2019Hostis et al., 2016).", "startOffset": 80, "endOffset": 139}, {"referenceID": 14, "context": "baseNMT) is much higher than previous research of manipulating the vocabularies (Jean et al., 2015; Mi et al., 2016; L\u2019Hostis et al., 2016).", "startOffset": 80, "endOffset": 139}, {"referenceID": 8, "context": "baseNMT) is much higher than previous research of manipulating the vocabularies (Jean et al., 2015; Mi et al., 2016; L\u2019Hostis et al., 2016).", "startOffset": 80, "endOffset": 139}], "year": 2017, "abstractText": "In the encoder-decoder architecture for neural machine translation (NMT), the hidden states of the recurrent structures in the encoder and decoder carry the crucial information about the sentence.These vectors are generated by parameters which are updated by back-propagation of translation errors through time. We argue that propagating errors through the end-to-end recurrent structures are not a direct way of control the hidden vectors. In this paper, we propose to use word predictions as a mechanism for direct supervision. More specifically, we require these vectors to be able to predict the vocabulary in target sentence. Our simple mechanism ensures better representations in the encoder and decoder without using any extra data or annotation. It is also helpful in reducing the target side vocabulary and improving the decoding efficiency. Experiments on Chinese-English and German-English machine translation tasks show BLEU improvements by 4.53 and 1.3, respectively.", "creator": "LaTeX with hyperref package"}}}