{"id": "1708.01980", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Aug-2017", "title": "Translating Phrases in Neural Machine Translation", "abstract": "Phrases play an important role in natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005). However, it is difficult to integrate them into current neural machine translation (NMT) which reads and generates sentences word by word. In this work, we propose a method to translate phrases in NMT by integrating a phrase memory storing target phrases from a phrase-based statistical machine translation (SMT) system into the encoder-decoder architecture of NMT. At each decoding step, the phrase memory is first re-written by the SMT model, which dynamically generates relevant target phrases with contextual information provided by the NMT model. Then the proposed model reads the phrase memory to make probability estimations for all phrases in the phrase memory. If phrase generation is carried on, the NMT decoder selects an appropriate phrase from the memory to perform phrase translation and updates its decoding state by consuming the words in the selected phrase. Otherwise, the NMT decoder generates a word from the vocabulary as the general NMT decoder does. Experiment results on the Chinese to English translation show that the proposed model achieves significant improvements over the baseline on various test sets.", "histories": [["v1", "Mon, 7 Aug 2017 03:46:48 GMT  (108kb,D)", "http://arxiv.org/abs/1708.01980v1", "Accepted by EMNLP 2017"]], "COMMENTS": "Accepted by EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xing wang", "zhaopeng tu", "deyi xiong", "min zhang"], "accepted": true, "id": "1708.01980"}, "pdf": {"name": "1708.01980.pdf", "metadata": {"source": "CRF", "title": "Translating Phrases in Neural Machine Translation", "authors": ["Xing Wang", "Zhaopeng Tu", "Deyi Xiong", "Min Zhang"], "emails": ["xingwsuda@gmail.com,", "minzhang}@suda.edu.cn", "tuzhaopeng@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most people are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the German Press Agency.\" I don't think people are able to survive themselves, \"he said.\" I don't think they are able to survive themselves. \"He added:\" I don't think they are able to survive themselves. \"He added:\" I don't think they are able to survive themselves. \"He added:\" I don't think they are able to be able to survive themselves, to be able to be able to be able to be able to be in, to be in, to be in, to be in, to be in, to be in, to be in. \""}, {"heading": "2 Background", "text": "The bidirectional RNN encoder, which consists of a forward-looking RNN and a backward-looking RNN, reads a source sentence x = x1, x2,..., xTx and converts it into word annotations of the entire source sentence h = h1, h2,..., hTx. The decoder uses the annotations to emit a target sentence y = y1, y2,..., yTy in a word-by-word manner. In the training phase, NMT models the conditional probability asfollows, P (y | x) = Ty, i = 1 P (yi | y < i, x) (1), where yi is the target word used by the decoder at step i and y & lt.i \u2212 i = 1, yi \u2212 i, i \u2212 i, i \u2212 i \u2212 i \u2212 (i), i \u2212 (i), i \u2212 (i), ti (i), ti (& l), (\u2212 ln) and \u2212 ln."}, {"heading": "3 Approach", "text": "In this section, we present the proposed model that integrates phrase memory into NMT's encoder decoder architecture. Inspired by recent work to attach an external structure to the encoder decoder architecture (Gulcehre et al., 2016; Gu et al., 2016; Tang et al., 2016; Wang et al., 2017), we take a similar approach to integrating phrase memory into NMT."}, {"heading": "3.1 Framework", "text": "\"We have the ability to decode a word from the NMT model for each step.\" The trade-off between the word generation mode and the phrase generation mode is balanced by a weight generated by a neural network. \"We will store the phrases from the phrase.\" \"We will store the phrases from the phrase.\" \"We will store the phrases from the phrase.\" \"We will store the phrases from the phrase.\" \"We will store the phrases from the phrase.\" \"We will store the phrases from the phrase.\" \"We will store the phrases from the phrase.\" \"We will store the phrases from the phrase.\" \"We will store the phrases from the phrase.\" \"We will store the phrases from the phrase.\""}, {"heading": "3.2 Phrase Memory", "text": "The phrase memory stores relevant target phrases provided by an SMT model trained on the same bilingual corpora. (At each decoding step, the memory is first cleared and rewritten by the SMT model, with the decoding based on the translation information of the NMT model.) Then, the proposed model asks for possible phrases to be extracted from the translation table. Phrases are identified with multiple SMT characteristics, including the translation probability of the phrase. (yt \u2212 1) The SMT model selects potential phrases to be extracted from the translation table."}, {"heading": "3.3 Training", "text": "Formally, we train both the standard parameters of the standard NMT and the new parameters associated with phrase generation using a series of training examples {[xn, yn]} Nn = 1: C (\u03b8) = N \u2211 n = 1logP (yn | xn) (13), where P (yn | xn) is defined in Equation 7. Ideally, the trained model is expected to produce a higher equilibrium weight and phrase probability when a phrase is selected from memory, and lower values in other cases."}, {"heading": "3.4 Decoding", "text": "During the test, the NMT decoder generates a target sentence consisting of a mixture of words and phrases. Due to the different granularity of words and phrases, we design a variant of the beam search strategy: In decoding step i, we first compute pphrase for all phrases in the phrase memory2Bilingual resources can be used in two ways: first, we can store the bilingual resources in a static memory and make all elements available to the NMT for the entire decoding period; second, we can integrate the bilingual resources in SMT and then dynamically feed them into the phrase memory; and the pword for all words in the NMT vocabulary. Then, the balancer gives a balance weight \u03bbi, which is used to scale the phrase and word probabilities: The phrase and (1 \u2212 1) \u00d7 pword are now used as normalized probabilities for concatenating the phrase of the NMT codise, when the latter phrase corresponds to the highest coding vocabulary in the general vocabulary."}, {"heading": "4 Experiments", "text": "This year, it is in our hands that people in the United States and other parts of the world have the same problems as we have, \"he told the German Press Agency in an interview with the New York Times.\" We have the same problems, \"he said,\" but we have the same problems. \"He added,\" We have the same problems that we have to solve. \""}, {"heading": "4.1 Main Results", "text": "We observe that our implementation of RNNSearch exceeds Moses by 2.34 BLEU points. (+ memory + chunking tag), which is the proposed model with the phrase memory, achieves an improvement of 0.47 BLEU points over the baseline RNNSearch. With the source-side chunking tag function, the base model RNNSearch exceeds (+ memory + chunking tag) by 1.07 BLEU points, showing the effectiveness of syntactic categories in selecting suitable target phrases. From now on, we use \"+ memory + chunking tag\" as the default in the following experiments, unless otherwise specified. Number of sentences affected by generated phrases We also check the number of translations that contain phrases generated by the proposed model, as shown in Table 2."}, {"heading": "4.2 Analysis on Generated Phrases", "text": "In fact, most people who are able to retaliate are not able to retaliate."}, {"heading": "4.3 Effect of Balancer", "text": "We conducted an additional experiment to verify the effectiveness of the neural network-based balancer. We used the + memory + chunking tag setting as the base system to perform the experiments. In this experiment, we set the balancing weight \u03bb (equation 8) during training and testing to 0.1 and report the results. As shown in Table 6, using the fixed value for the balancing weight (constant = 0.1) significantly reduces the translation performance, showing that the neural network-based balancer is an essential component of the proposed model."}, {"heading": "4.4 Comparison to Word-Level Recommendations and Discussions", "text": "Our approach refers to our previous work (Wang et al., 2017), in which SMT word-level knowledge was integrated into the NMT. To make a comparison, we conducted experiments that followed the word-level model proposed in (Wang et al., 2017) by 0.28 BLEU points. In our approach, the SMT model translates source-side chunk phrases using NMT decryption information. Although we use high-quality target phrases as phrase recommendations, our approach still suffers from errors in sequencing and chunking. For example, the target phrase \"laptop computers\" cannot be recommended by the SMT model if the Chinese phrase \"If the Chinese phrase\" is not used as a phrase unit."}, {"heading": "5 Related work", "text": "Our work relates to the following research topics on NMT: Phrase generation for NMT. In these studies, the generated NMT multiword phrases 2016 are used either by an SMT model or a bilingual dictionary. In this way, syntactic phrases are generated by the NMT decoder (Stahlberg et al., 2016b). Zhang and Zong (2016) use an SMT translation system that integrates an additional bilingual dictionary to synthesize synthetic parallel sentences and translate the sentences into the formation of NMT. Tang et al. (2016) we propose an external phrase storage system that stores pairs of phrases in symbolic forms for NMT."}, {"heading": "6 Conclusion", "text": "In this paper, we have presented a novel model for translating source code and generating target phrase translations into NMT by integrating the phrase memory into the encoder decoder architecture.When decoding, the SMT model dynamically generates relevant target phrases with the context information provided by the NMT model and writes them into the phrase memory. Subsequently, the proposed model reads the phrase memory and uses the balancer to make probability estimates for the phrases in the phrase memory. Finally, the NMT decoder selects a word from the vocabulary with the highest probability of generation.Test results on the Chinese \u2192 English translation have shown that the proposed model can significantly improve translation performance."}, {"heading": "Acknowledgments", "text": "We would like to thank three anonymous reviewers for their insightful comments and also Zhengdong Lu, Lili Mou for useful discussions, which was supported by the National Natural Science Foundation of China (Grants No.61525205, 61373095 and 61622209)."}], "references": [{"title": "Towards string-to-tree neural machine translation", "author": ["Roee Aharoni", "Yoav Goldberg."], "venue": "arXiv preprint arXiv:1704.04743.", "citeRegEx": "Aharoni and Goldberg.,? 2017", "shortCiteRegEx": "Aharoni and Goldberg.", "year": 2017}, {"title": "Incorporating discrete translation lexicons into neural machine translation", "author": ["Philip Arthur", "Graham Neubig", "Satoshi Nakamura."], "venue": "Proceedings of the 2016 Conference on EMNLP.", "citeRegEx": "Arthur et al\\.,? 2016", "shortCiteRegEx": "Arthur et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Peter F Brown", "Vincent J Della Pietra", "Stephen A Della Pietra", "Robert L Mercer."], "venue": "Computational linguistics.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang."], "venue": "Proceedings of the 43rd ACL.", "citeRegEx": "Chiang.,? 2005", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "Natural language understanding with distributed representation", "author": ["Kyunghyun Cho."], "venue": "arXiv preprint.", "citeRegEx": "Cho.,? 2015", "shortCiteRegEx": "Cho.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 54th ACL, pages 1693\u20131703, Berlin, Germany.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Incorporating structural alignment biases into an attentional neural translation model", "author": ["Haffari."], "venue": "Proceedings of NAACL 2016, San Diego, California.", "citeRegEx": "Haffari.,? 2016", "shortCiteRegEx": "Haffari.", "year": 2016}, {"title": "Character-based neural machine translation", "author": ["Marta R. Costa-juss\u00e0", "Jos\u00e9 A.R. Fonollosa."], "venue": "Proceedings of the 54th ACL, pages 357\u2013361, Berlin, Germany.", "citeRegEx": "Costa.juss\u00e0 and Fonollosa.,? 2016", "shortCiteRegEx": "Costa.juss\u00e0 and Fonollosa.", "year": 2016}, {"title": "Tree-to-sequence attentional neural machine translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Proceedings of the 54th ACL, pages 823\u2013833, Berlin, Germany.", "citeRegEx": "Eriguchi et al\\.,? 2016", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "Factored neural machine translation", "author": ["Mercedes Garc\u0131\u0301a-Mart\u0131\u0301nez", "Lo\u0131\u0308c Barrault", "Fethi Bougares"], "venue": "arXiv preprint arXiv:1609.04621", "citeRegEx": "Garc\u0131\u0301a.Mart\u0131\u0301nez et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Mart\u0131\u0301nez et al\\.", "year": 2016}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li."], "venue": "Proceedings of the 54th ACL, Berlin, Germany.", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Pointing the unknown words", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."], "venue": "Proceedings of ACL 2016, Berlin, Germany.", "citeRegEx": "Gulcehre et al\\.,? 2016", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Improved neural machine translation with smt features", "author": ["Wei He", "Zhongjun He", "Hua Wu", "Haifeng Wang."], "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the 53rd ACL and the 7th IJCNLP.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on EMNLP.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 NAACL.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Modeling source syntax for neural machine translation", "author": ["Junhui Li", "Deyi Xiong", "Zhaopeng Tu", "Muhua Zhu", "Min Zhang", "Guodong Zhou."], "venue": "arXiv preprint arXiv:1705.01020.", "citeRegEx": "Li et al\\.,? 2017", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Neural machine translation with supervised attention", "author": ["Lemao Liu", "Masao Utiyama", "Andrew Finch", "Eiichiro Sumita."], "venue": "Proceedings of COLING 2016, pages 3093\u20133102, Osaka, Japan.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "Proceedings of the 54th ACL.", "citeRegEx": "Luong and Manning.,? 2016", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the 53rd ACL and the 7th IJCNLP.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Supervised attentions for neural machine translation", "author": ["Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah."], "venue": "Proceedings of EMNLP 2016, pages 2283\u20132288, Austin, Texas.", "citeRegEx": "Mi et al\\.,? 2016", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Pre-translation for neural machine translation", "author": ["Jan Niehues", "Eunah Cho", "Thanh-Le Ha", "Alex Waibel."], "venue": "Proceedings of COLING 2016, Osaka, Japan.", "citeRegEx": "Niehues et al\\.,? 2016", "shortCiteRegEx": "Niehues et al\\.", "year": 2016}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proceedings of the 41st ACL.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "Proceedings of the 30th International Conference on Machine Learning, ICML 2013, pages 1310C\u20131318.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Multiword expressions: A pain in the neck for nlp", "author": ["Ivan A Sag", "Timothy Baldwin", "Francis Bond", "Ann Copestake", "Dan Flickinger."], "venue": "International Conference on Intelligent Text Processing and Computational Linguistics, pages 1\u201315. Springer.", "citeRegEx": "Sag et al\\.,? 2002", "shortCiteRegEx": "Sag et al\\.", "year": 2002}, {"title": "Linguistic input features improve neural machine translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "Proceedings of the First Conference on Machine Translation, pages 83\u201391, Berlin, Germany.", "citeRegEx": "Sennrich and Haddow.,? 2016", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th ACL, pages 1715\u20131725, Berlin, Germany.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation by minimising the bayes-risk with respect to syntactic translation lattices", "author": ["Felix Stahlberg", "Adri\u00e0 de Gispert", "Eva Hasler", "Bill Byrne."], "venue": "arXiv preprint arXiv:1612.03791.", "citeRegEx": "Stahlberg et al\\.,? 2016a", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2016}, {"title": "Syntactically guided neural machine translation", "author": ["Felix Stahlberg", "Eva Hasler", "Aurelien Waite", "Bill Byrne."], "venue": "Proceedings of the 54th ACL (Volume 2: Short Papers), Berlin, Germany.", "citeRegEx": "Stahlberg et al\\.,? 2016b", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems 27.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Neural machine translation with external phrase memory", "author": ["Yaohua Tang", "Fandong Meng", "Zhengdong Lu", "Hang Li", "Philip LH Yu."], "venue": "arXiv preprint arXiv:1606.01792.", "citeRegEx": "Tang et al\\.,? 2016", "shortCiteRegEx": "Tang et al\\.", "year": 2016}, {"title": "Context gates for neural machine translation", "author": ["Zhaopeng Tu", "Yang Liu", "Zhengdong Lu", "Xiaohua Liu", "Hang Li."], "venue": "Transactions of the Association of Computational Linguistics.", "citeRegEx": "Tu et al\\.,? 2017a", "shortCiteRegEx": "Tu et al\\.", "year": 2017}, {"title": "Neural machine translation with reconstruction", "author": ["Zhaopeng Tu", "Yang Liu", "Lifeng Shang", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of the 31th AAAI Conference on Artificial Intelligence.", "citeRegEx": "Tu et al\\.,? 2017b", "shortCiteRegEx": "Tu et al\\.", "year": 2017}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of the 54th ACL.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Introduction to the special issue on multiword expressions: Having a crack at a hard nut", "author": ["Aline Villavicencio", "Francis Bond", "Anna Korhonen", "Diana McCarthy."], "venue": "Computer Speech & Language, 19(4):365\u2013377.", "citeRegEx": "Villavicencio et al\\.,? 2005", "shortCiteRegEx": "Villavicencio et al\\.", "year": 2005}, {"title": "Neural machine translation advised by statistical machine translation", "author": ["Xing Wang", "Zhengdong Lu", "Zhaopeng Tu", "Hang Li", "Deyi Xiong", "Min Zhang."], "venue": "Proceedings of the 31th AAAI Conference on Artificial Intelligence.", "citeRegEx": "Wang et al\\.,? 2017", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "Models and inference for prefix-constrained machine translation", "author": ["Joern Wuebker", "Spence Green", "John DeNero", "Sasa Hasan", "Minh-Thang Luong."], "venue": "Proceedings of the 54th ACL, pages 66\u2013 75, Berlin, Germany.", "citeRegEx": "Wuebker et al\\.,? 2016", "shortCiteRegEx": "Wuebker et al\\.", "year": 2016}, {"title": "Chinese word segmentation as lmr tagging", "author": ["Nianwen Xue", "Libin Shen."], "venue": "Proceedings of the Second SIGHAN Workshop on Chinese Language Processing, pages 176\u2013179, Sapporo, Japan.", "citeRegEx": "Xue and Shen.,? 2003", "shortCiteRegEx": "Xue and Shen.", "year": 2003}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Bridging neural machine translation and bilingual dictionaries", "author": ["Jiajun Zhang", "Chengqing Zong."], "venue": "arXiv preprint arXiv:1610.07272.", "citeRegEx": "Zhang and Zong.,? 2016", "shortCiteRegEx": "Zhang and Zong.", "year": 2016}, {"title": "Topic-informed neural machine translation", "author": ["Jian Zhang", "Liangyou Li", "Andy Way", "Qun Liu."], "venue": "Proceedings of COLING 2016, pages 1807\u20131817, Osaka, Japan.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Neural system combination for machine translation", "author": ["Long Zhou", "Wenpeng Hu", "Jiajun Zhang", "Chengqing Zong."], "venue": "arXiv preprint arXiv:1704.06393.", "citeRegEx": "Zhou et al\\.,? 2017", "shortCiteRegEx": "Zhou et al\\.", "year": 2017}, {"title": "Niuparser: A chinese syntactic and semantic parsing toolkit", "author": ["Jingbo Zhu", "Muhua Zhu", "Qiang Wang", "Tong Xiao."], "venue": "Proceedings of ACL-IJCNLP 2015 System Demonstrations, Beijing, China.", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "Phrases play an important role in natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005).", "startOffset": 89, "endOffset": 135}, {"referenceID": 36, "context": "Phrases play an important role in natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005).", "startOffset": 89, "endOffset": 135}, {"referenceID": 16, "context": "\u2217Corresponding author translation performance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 46, "endOffset": 160}, {"referenceID": 6, "context": "\u2217Corresponding author translation performance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 46, "endOffset": 160}, {"referenceID": 31, "context": "\u2217Corresponding author translation performance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 46, "endOffset": 160}, {"referenceID": 2, "context": "\u2217Corresponding author translation performance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 46, "endOffset": 160}, {"referenceID": 3, "context": "Significantly different from conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT adopts a big neural network to perform the entire translation process in one shot, for which an encoderdecoder architecture is widely used.", "startOffset": 80, "endOffset": 134}, {"referenceID": 17, "context": "Significantly different from conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT adopts a big neural network to perform the entire translation process in one shot, for which an encoderdecoder architecture is widely used.", "startOffset": 80, "endOffset": 134}, {"referenceID": 4, "context": "Significantly different from conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT adopts a big neural network to perform the entire translation process in one shot, for which an encoderdecoder architecture is widely used.", "startOffset": 80, "endOffset": 134}, {"referenceID": 26, "context": "Phrases, especially multi-word expressions, are crucial for natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005) as the meaning of a phrase cannot be always deducible from the meanings of its individual words or parts.", "startOffset": 115, "endOffset": 161}, {"referenceID": 36, "context": "Phrases, especially multi-word expressions, are crucial for natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005) as the meaning of a phrase cannot be always deducible from the meanings of its individual words or parts.", "startOffset": 115, "endOffset": 161}, {"referenceID": 7, "context": "Unfortunately current NMT is essentially a word-based or character-based (Chung et al., 2016; Costa-juss\u00e0 and Fonollosa, 2016; Luong and Manning, 2016) translation system where phrases are not considered as translation units.", "startOffset": 73, "endOffset": 151}, {"referenceID": 9, "context": "Unfortunately current NMT is essentially a word-based or character-based (Chung et al., 2016; Costa-juss\u00e0 and Fonollosa, 2016; Luong and Manning, 2016) translation system where phrases are not considered as translation units.", "startOffset": 73, "endOffset": 151}, {"referenceID": 20, "context": "Unfortunately current NMT is essentially a word-based or character-based (Chung et al., 2016; Costa-juss\u00e0 and Fonollosa, 2016; Luong and Manning, 2016) translation system where phrases are not considered as translation units.", "startOffset": 73, "endOffset": 151}, {"referenceID": 30, "context": "Recently, there have been some attempts on multi-word phrase generation in NMT (Stahlberg et al., 2016b; Zhang and Zong, 2016).", "startOffset": 79, "endOffset": 126}, {"referenceID": 41, "context": "Recently, there have been some attempts on multi-word phrase generation in NMT (Stahlberg et al., 2016b; Zhang and Zong, 2016).", "startOffset": 79, "endOffset": 126}, {"referenceID": 6, "context": "Here we adopt Gated Recurrent Unit (Cho et al., 2014) as the recurrent unit for the encoder and decoder.", "startOffset": 35, "endOffset": 53}, {"referenceID": 5, "context": "given the training data with N bilingual sentences (Cho, 2015).", "startOffset": 51, "endOffset": 62}, {"referenceID": 13, "context": "Inspired by the recent work on attaching an external structure to the encoder-decoder architecture (Gulcehre et al., 2016; Gu et al., 2016; Tang et al., 2016; Wang et al., 2017), we adopt a similar approach to incorporate the phrase memory into NMT.", "startOffset": 99, "endOffset": 177}, {"referenceID": 12, "context": "Inspired by the recent work on attaching an external structure to the encoder-decoder architecture (Gulcehre et al., 2016; Gu et al., 2016; Tang et al., 2016; Wang et al., 2017), we adopt a similar approach to incorporate the phrase memory into NMT.", "startOffset": 99, "endOffset": 177}, {"referenceID": 32, "context": "Inspired by the recent work on attaching an external structure to the encoder-decoder architecture (Gulcehre et al., 2016; Gu et al., 2016; Tang et al., 2016; Wang et al., 2017), we adopt a similar approach to incorporate the phrase memory into NMT.", "startOffset": 99, "endOffset": 177}, {"referenceID": 37, "context": "Inspired by the recent work on attaching an external structure to the encoder-decoder architecture (Gulcehre et al., 2016; Gu et al., 2016; Tang et al., 2016; Wang et al., 2017), we adopt a similar approach to incorporate the phrase memory into NMT.", "startOffset": 99, "endOffset": 177}, {"referenceID": 39, "context": "Following the commonly-used strategy in sequence tagging tasks (Xue and Shen, 2003), we allow the words in a phrase to share the same chunk tag and introduce a special tag for the beginning word.", "startOffset": 63, "endOffset": 83}, {"referenceID": 27, "context": "Partially motivated by the work on integrating linguistic features into NMT (Sennrich and Haddow, 2016), we represent the encoder input as the combination of word embeddings and chunking tag embeddings, instead of word embeddings alone in the conventional NMT.", "startOffset": 76, "endOffset": 103}, {"referenceID": 37, "context": "Specially, the reordering score depends on alignment information between source and target words, which is derived from attention distribution produced by the NMT model (Wang et al., 2017).", "startOffset": 169, "endOffset": 188}, {"referenceID": 37, "context": "SMT coverage vector in (Wang et al., 2017) is also introduced to avoid repeat phrasal recommendations.", "startOffset": 23, "endOffset": 42}, {"referenceID": 24, "context": "The feature weights can be tuned by the minimum error rate training (MERT) algorithm (Och, 2003).", "startOffset": 85, "endOffset": 96}, {"referenceID": 24, "context": ", 2007) with its default settings, where feature function weights are tuned by the minimum error rate training (MERT) algorithm (Och, 2003).", "startOffset": 128, "endOffset": 139}, {"referenceID": 2, "context": "* RNNSearch: an in-house implementation of the attention-based NMT system (Bahdanau et al., 2015) with its default settings.", "startOffset": 74, "endOffset": 97}, {"referenceID": 17, "context": "com/code/kenlm/ directions and applied the \u201cgrow-diag-final\u201d refinement rule (Koehn et al., 2003) to obtain word alignments.", "startOffset": 77, "endOffset": 97}, {"referenceID": 40, "context": "We used a minibatch stochastic gradient descent (SGD) algorithm of size 80 together with Adadelta (Zeiler, 2012) to train the NMT models.", "startOffset": 98, "endOffset": 112}, {"referenceID": 25, "context": "0 (Pascanu et al., 2013).", "startOffset": 2, "endOffset": 24}, {"referenceID": 44, "context": "For the proposed model, we used a Chinese chunker6 (Zhu et al., 2015) to chunk the sourceside Chinese sentences.", "startOffset": 51, "endOffset": 69}, {"referenceID": 37, "context": "Our approach is related to our previous work (Wang et al., 2017) which integrates the SMT word-level knowledge into NMT.", "startOffset": 45, "endOffset": 64}, {"referenceID": 37, "context": "\u201c+word level recommendation\u201d is the proposed model in (Wang et al., 2017).", "startOffset": 54, "endOffset": 73}, {"referenceID": 37, "context": "tings in (Wang et al., 2017).", "startOffset": 9, "endOffset": 28}, {"referenceID": 37, "context": "We find that our approach is marginally better than the word-level model proposed in (Wang et al., 2017) by 0.", "startOffset": 85, "endOffset": 104}, {"referenceID": 30, "context": "In this way, syntactic phrases are generated by the NMT decoder (Stahlberg et al., 2016b).", "startOffset": 64, "endOffset": 89}, {"referenceID": 29, "context": "In this way, syntactic phrases are generated by the NMT decoder (Stahlberg et al., 2016b). Zhang and Zong (2016) use an SMT translation system, which is integrated an additional bilingual dictionary, to synthesize pseudo-parallel sentences and feed the sentences into the training of NMT in order to translate low-frequency words or phrases.", "startOffset": 65, "endOffset": 113}, {"referenceID": 29, "context": "In this way, syntactic phrases are generated by the NMT decoder (Stahlberg et al., 2016b). Zhang and Zong (2016) use an SMT translation system, which is integrated an additional bilingual dictionary, to synthesize pseudo-parallel sentences and feed the sentences into the training of NMT in order to translate low-frequency words or phrases. Tang et al. (2016) propose an external phrase memory that stores phrase pairs in symbolic forms for NMT.", "startOffset": 65, "endOffset": 361}, {"referenceID": 28, "context": ", words, subwords (Sennrich et al., 2016), characters (Chung et al.", "startOffset": 18, "endOffset": 41}, {"referenceID": 7, "context": ", 2016), characters (Chung et al., 2016; Costa-juss\u00e0 and Fonollosa, 2016), as non-linguistic symbols.", "startOffset": 20, "endOffset": 73}, {"referenceID": 9, "context": ", 2016), characters (Chung et al., 2016; Costa-juss\u00e0 and Fonollosa, 2016), as non-linguistic symbols.", "startOffset": 20, "endOffset": 73}, {"referenceID": 6, "context": ", 2016), characters (Chung et al., 2016; Costa-juss\u00e0 and Fonollosa, 2016), as non-linguistic symbols. However, linguistic information can be viewed as the taskspecific knowledge, which may be a useful supplementary to the sequence to sequence mapping network. To this end, various kinds of linguistic annotations have been introduced into NMT to improve its translation performance. Sennrich and Haddow (2016) enrich the input units of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features.", "startOffset": 21, "endOffset": 410}, {"referenceID": 6, "context": ", 2016), characters (Chung et al., 2016; Costa-juss\u00e0 and Fonollosa, 2016), as non-linguistic symbols. However, linguistic information can be viewed as the taskspecific knowledge, which may be a useful supplementary to the sequence to sequence mapping network. To this end, various kinds of linguistic annotations have been introduced into NMT to improve its translation performance. Sennrich and Haddow (2016) enrich the input units of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features. Garc\u0131\u0301a-Mart\u0131\u0301nez et al. (2016) propose factored NMT using the morphological and grammatical decomposition of the words (factors) in output units.", "startOffset": 21, "endOffset": 601}, {"referenceID": 6, "context": ", 2016), characters (Chung et al., 2016; Costa-juss\u00e0 and Fonollosa, 2016), as non-linguistic symbols. However, linguistic information can be viewed as the taskspecific knowledge, which may be a useful supplementary to the sequence to sequence mapping network. To this end, various kinds of linguistic annotations have been introduced into NMT to improve its translation performance. Sennrich and Haddow (2016) enrich the input units of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features. Garc\u0131\u0301a-Mart\u0131\u0301nez et al. (2016) propose factored NMT using the morphological and grammatical decomposition of the words (factors) in output units. Eriguchi et al. (2016) explore the phrase structures of input sentences and propose a tree-to-sequence attention model for the vanilla NMT model.", "startOffset": 21, "endOffset": 739}, {"referenceID": 6, "context": ", 2016), characters (Chung et al., 2016; Costa-juss\u00e0 and Fonollosa, 2016), as non-linguistic symbols. However, linguistic information can be viewed as the taskspecific knowledge, which may be a useful supplementary to the sequence to sequence mapping network. To this end, various kinds of linguistic annotations have been introduced into NMT to improve its translation performance. Sennrich and Haddow (2016) enrich the input units of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features. Garc\u0131\u0301a-Mart\u0131\u0301nez et al. (2016) propose factored NMT using the morphological and grammatical decomposition of the words (factors) in output units. Eriguchi et al. (2016) explore the phrase structures of input sentences and propose a tree-to-sequence attention model for the vanilla NMT model. Li et al. (2017) propose to linearize source-side parse trees to obtain structural label sequences and explicitly incorporated the structural sequences into NMT, while Aharoni and Goldberg (2017) propose to", "startOffset": 21, "endOffset": 879}, {"referenceID": 0, "context": "(2017) propose to linearize source-side parse trees to obtain structural label sequences and explicitly incorporated the structural sequences into NMT, while Aharoni and Goldberg (2017) propose to", "startOffset": 158, "endOffset": 186}, {"referenceID": 42, "context": "Zhang et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation.", "startOffset": 0, "endOffset": 20}, {"referenceID": 21, "context": "(2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al., 2015; Jean et al., 2015) and coverage problem (Tu et al.", "startOffset": 142, "endOffset": 181}, {"referenceID": 15, "context": "(2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al., 2015; Jean et al., 2015) and coverage problem (Tu et al.", "startOffset": 142, "endOffset": 181}, {"referenceID": 35, "context": ", 2015) and coverage problem (Tu et al., 2016).", "startOffset": 29, "endOffset": 46}, {"referenceID": 37, "context": "(2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017).", "startOffset": 232, "endOffset": 251}, {"referenceID": 38, "context": "The combination of NMT and SMT has been also introduced in interactive machine translation to improve the system\u2019s suggestion quality (Wuebker et al., 2016).", "startOffset": 134, "endOffset": 156}, {"referenceID": 22, "context": "In addition, word alignments from the traditional SMT pipeline are also used to improve the attention mechanism in NMT (Cohn et al., 2016; Mi et al., 2016; Liu et al., 2016).", "startOffset": 119, "endOffset": 173}, {"referenceID": 19, "context": "In addition, word alignments from the traditional SMT pipeline are also used to improve the attention mechanism in NMT (Cohn et al., 2016; Mi et al., 2016; Liu et al., 2016).", "startOffset": 119, "endOffset": 173}, {"referenceID": 13, "context": "He et al. (2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 1, "context": "Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT.", "startOffset": 0, "endOffset": 465}, {"referenceID": 1, "context": "Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT. They combine NMT posteriors with SMT word recommendations through linear interpolation implemented by a gating function which dynamically assigns the weights. Niehues et al. (2016) propose to use SMT to pre-translate the inputs into target translations and employ the target pre-translations as input sequences in NMT.", "startOffset": 0, "endOffset": 721}, {"referenceID": 1, "context": "Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT. They combine NMT posteriors with SMT word recommendations through linear interpolation implemented by a gating function which dynamically assigns the weights. Niehues et al. (2016) propose to use SMT to pre-translate the inputs into target translations and employ the target pre-translations as input sequences in NMT. Zhou et al. (2017) propose a neural system combination framework to directly combine NMT and SMT outputs.", "startOffset": 0, "endOffset": 878}], "year": 2017, "abstractText": "Phrases play an important role in natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005). However, it is difficult to integrate them into current neural machine translation (NMT) which reads and generates sentences word by word. In this work, we propose a method to translate phrases in NMT by integrating a phrase memory storing target phrases from a phrase-based statistical machine translation (SMT) system into the encoder-decoder architecture of NMT. At each decoding step, the phrase memory is first re-written by the SMT model, which dynamically generates relevant target phrases with contextual information provided by the NMT model. Then the proposed model reads the phrase memory to make probability estimations for all phrases in the phrase memory. If phrase generation is carried on, the NMT decoder selects an appropriate phrase from the memory to perform phrase translation and updates its decoding state by consuming the words in the selected phrase. Otherwise, the NMT decoder generates a word from the vocabulary as the general NMT decoder does. Experiment results on the Chinese\u2192English translation show that the proposed model achieves significant improvements over the baseline on various test sets.", "creator": "LaTeX with hyperref package"}}}