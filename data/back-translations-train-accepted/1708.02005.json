{"id": "1708.02005", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Aug-2017", "title": "Memory-augmented Neural Machine Translation", "abstract": "Neural machine translation (NMT) has achieved notable success in recent times, however it is also widely recognized that this approach has limitations with handling infrequent words and word pairs. This paper presents a novel memory-augmented NMT (M-NMT) architecture, which stores knowledge about how words (usually infrequently encountered ones) should be translated in a memory and then utilizes them to assist the neural model. We use this memory mechanism to combine the knowledge learned from a conventional statistical machine translation system and the rules learned by an NMT system, and also propose a solution for out-of-vocabulary (OOV) words based on this framework. Our experiments on two Chinese-English translation tasks demonstrated that the M-NMT architecture outperformed the NMT baseline by $9.0$ and $2.7$ BLEU points on the two tasks, respectively. Additionally, we found this architecture resulted in a much more effective OOV treatment compared to competitive methods.", "histories": [["v1", "Mon, 7 Aug 2017 06:47:23 GMT  (282kb,D)", "http://arxiv.org/abs/1708.02005v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yang feng", "shiyue zhang", "andi zhang", "dong wang", "andrew abel"], "accepted": true, "id": "1708.02005"}, "pdf": {"name": "1708.02005.pdf", "metadata": {"source": "CRF", "title": "Memory-augmented Neural Machine Translation", "authors": ["Yang Feng", "Shiyue Zhang", "Andi Zhang", "Dong Wang", "Andrew Abel"], "emails": ["fengyang@ict.ac.cn,", "andizhang912}@gmail.com", "wangdong99@mails.tsinghua.edu.cn,", "andrew.abel@xjtlu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to retaliate without being able to retaliate, and that they are able to retaliate."}, {"heading": "2 Attention-based NMT", "text": "Before presenting our M-NMT architecture, we give a brief overview of our implementation of the attention-based RNN model, which was first presented by Bahdanau et al. (2015). This model is considered the state-of-the-art model and is used as the base system in this study. In addition, the neural model component of the M-NMT architecture uses the same attention-based RNN model as it is presented below. \u2212 The attention-based RNN model is based on an encoder decoder framework in which the input sequence [x1, x2,...] is generated in the source language as a sequence of hidden states [h1, h2,...] by a bidirectional RNN model with GRU as the hidden units, and another RNN is used to generate the target sentence [y1, y2,...]."}, {"heading": "3 Memory-Augmented NMT", "text": "This section introduces the M-NMT architecture, starts with the model, and then describes the structure of the memory."}, {"heading": "3.1 The Architecture", "text": "The M-NMT architecture is shown in Figure 1. It comprises two components: the model and the memory components. The model component is an atypical attention-based RNN model as shown in Section 2, which is considered good at handling common words and pairs, and the memory component provides knowledge for rare words and pairs that is not easy to learn for the neural model component. Results from the two components are combined to produce a final consolidated translation."}, {"heading": "3.2 Memory Elements", "text": "We define each element of memory as an assignment of one word in the source language to its translation in the target language. If there are multiple translations for a word, then, depending on the probability of the translation, several of the best will be added to memory until the maximum number of target words is reached. A memory element can be formally written by: ujl = [yjl xj], where yjl is the l-th translation of the word xj. This assignment is stored as a memory element and is used during translation. We speak of this memory as a global memory that is static throughout runtime. Global memory is shown at the bottom right of the figure. To translate an input set, the memory elements whose source words are in the input set are selected to form a local memory. This is shown in the right center of Figure 1. To include the context information in the local memory, the source part xj is replaced by its source element: jhjk = hl in each case."}, {"heading": "3.3 Memory Attention", "text": "In order to use the information stored in memory to improve NMT, we need to remove appropriate elements from the local memory at each translation step. A similar attention mechanism as in the neural model is designed. Specify the attention factor of each memory element uk at each translation step i of \u03b1mik, and assume it is derived from a relevance function emik: \u03b1mik = emik + W m \u00b2 K = 1 m ik, where K is the number of targets in the merged memory. The relevance function can be changed, but in this study we use a simple design: emik = (v m) > tanh (Wms si \u2212 1 + W m uk + W m y yi \u2212 1), where tanh (\u00b7) is the hyperbolic function si \u2212 1 is the current state of the decoder of the neural model, and yi \u2212 1 is the generated word."}, {"heading": "3.4 Memory for SMT Integration", "text": "The M-NMT architecture is a flexible framework that provides additional knowledge to the traditional model-based NMT. When the knowledge is generated by a conventional SMT system, it is essentially an elegant combination of SMT and NMT. In this work, we use the translation dictionary produced by an SMT system as knowledge to create the memory, first aligning the training set pairs using the GIZA + + toolkit (Och and Ney, 2003) in both directions and applying the \"interface\" refinement rules (Koehn et al., 2003) to obtain a single 1: 1 alignment for each sentence pair, and then extracting the translation dictionary based on these alignments. We can consider the dictionary as phrase pairs of length 1 and leave the phrase pairs longer than 1 as future work. The key information provided by the dictionary is most likely to use the target word twice (the number of local pairs), and the number of local pairs is the most likely to be used twice (the number of local pairs)."}, {"heading": "3.5 Memory for OOV Treatment", "text": "The memory also provides a flexible way to address OOV words. OOV words can be defined in several ways, but here we are focusing on real OOVs that are completely new in both bilingual and monolingual data (i.e. rare words that are not present in any training data). An example is when a model is migrated to a particular domain. To address these OOVs, we first need a manually defined dictionary to specify how to translate an OOV word, with the target word being either a callword or an OOV. This dictionary is used as knowledge to construct the local memory at runtime. In particular, if an OOV word is encountered on the source or destination page during local memory construction, the vector of a similar word is borrowed to represent the OV word. Since the words OOV are completely new, the similar word must be manually specified so that the OV is not predefined in the local word, so that the other word should be confused with the existing one."}, {"heading": "4 Related Work", "text": "The idea of memory expansion was inspired by the recent advances in the neural Turing machine (Graves et al., 2014,) and the memory network (Weston et al., 2014).These new models equip neural networks with an external memory that can be accessed and manipulated via some key operations. The memory idea was used in NMT. In contrast, our work uses memory to extend the state of the RNN decoder in the attention-based NMT. In this case, the contribution of memory is temporary variables to decode RNN. Our work uses memory to store knowledge."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Data", "text": "The experiments were performed for the Chinese translation with two sets of data, the relatively small IWSLT dataset and the much larger NIST dataset. As we will see, the NMT and SMT approaches on these two datasets exhibit different behaviors, and the memory expansion approach offers them different contributions. The IWSLT corpus Training data consists of 44K tourism and travel records. The development set consisted of the ASR Develset 1 and Develset 2 from the IWSLT 2005, and the test used the IWSLT 2005 test set. The NIST corpus Training data consists of 1M set pairs with 19M source tokens and 24M target tokens from the LDC corpus of LDC2002E18, LDC2003E07, LDC2003E14 and Hansards part of the LDC2004T08 and KoDC6-50S6 data pairs were used as the data pairs for both development."}, {"heading": "5.2 Systems", "text": "We used a conventional SMT system and an attention-based RNN NMT system as the base system and examined a variety of M-NMT architectures.SMT baseline: For the SMT system (referred to by Moses), Moses (Koehn et al., 2007), which includes a state-of-the-art open source toolkit, the default configuration was used, where the phrase length 7 and the following features were used: relative translation frequency and lexic translation probabilities in both directions, distortion distance, language model and word punishment. For the language model, the KenLM toolkit (Heafield, 2011) was used to build a 5 gram language model (using the Keneser-Ney-Smoothing) on the target side of the training data.NMT basic data: For NMT, we reproduced the attention-based RNT model, which is proposed by MT in 2015."}, {"heading": "5.3 SMT-NMT Integration Experiments", "text": "In the first experiment, the M-NMT architecture combines SMT and NMT by using SMT to enhance memory to support NMT. For comparison purposes, the lexical predictive approach of (Arthur et al., 2016) was also implemented, using the phrase table of SMT to enhance NMT. Our implementation is a linear combination, and for fair comparison, the neural model part has been left unchanged. At each step i, the auxiliary probability provided by the lexical part, P (yi) = \u2211 j \u03b1ijP (yi | xj), where \u03b1ij is the attention weight from the neural model, and P (yi | xj) is derived from the phrase table. This can be considered a simple memory approach, whereby the memory attention is borrowed from the neural model, rather than learned from NST05, SMTMIT data can be observed separately with the MIT-MIT data, but the small MIT-MIT data system can be observed initially with the small BLAST."}, {"heading": "5.4 OOV Treatment Experiments", "text": "Here, the M-NMT architecture was used to handle OOV words. Experiments were conducted on the NIST dataset, for which we collected 312 sets of OOV words. This test set was divided into two subsets: the T-ONE words in their entirety, including 276 words in the target language; and the T-ONE translations have only OOOV translations. We have a translation table with OOV words in their entirety."}, {"heading": "5.5 Frequency Analysis", "text": "To better understand the memory mechanism, we divide the test sentences into four containers according to the lowest word frequency within the sentence and calculate the retrieval rates for words in these containers. As soon as a generated translation word is also in one of the references, we treat it as a hit. We conducted the experiment with the NIST dataset. Figure 2 shows that M-NMT offers more improvements in rare words, in line with our reasoning that the memory mechanism NMT helps with rare words. Finally, we demonstrate the translation with MNMT using the example of Table 1, as shown in Table 5. It is clearly visible that the memory has remembered the rare word \"chromosome,\" resulting in improved translation."}, {"heading": "6 Conclusions", "text": "Our experiments showed that the new architecture is highly effective, offering a performance improvement of 9.0 and 2.7 BLEU values for two sets of Chinese-English translation data, respectively. In addition, it provides a very flexible and effective OOV treatment. In our experiments, OOV recalls are 28% and 40%, respectively, for the OOV words whose target words are INV and OOV, a significant improvement over competing methods. Future work will investigate better integration of model and memory, e.g. through joint training."}, {"heading": "Acknowledgement", "text": "This work was supported by the National Natural Science Foundation of China (NSFC) as part of Project No.61371136, No.61633013, No.61472428, No.61472428."}], "references": [{"title": "Incorporating discrete translation lexicons into neural machine translation", "author": ["Philip Arthur", "Graham Neubig", "Satoshi Nakamura."], "venue": "Proc. of EMNLP, pages 1557\u20131567.", "citeRegEx": "Arthur et al\\.,? 2016", "shortCiteRegEx": "Arthur et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR\u20192015.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Neural versus phrasebased machine translation quality: a case study", "author": ["Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo", "Marcello Federico."], "venue": "arXiv preprint arXiv:1608.04631.", "citeRegEx": "Bentivogli et al\\.,? 2016", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2016}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.1259.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Incorporating structural alignment biases into an attentional neural translation model", "author": ["Trevor Cohn", "Cong Duy Vu Hoang", "Ekaterina Vymolova", "Kaisheng Yao", "Chris Dyer", "Gholamreza Haffari."], "venue": "Proc. of NAACL, pages 876\u2013885.", "citeRegEx": "Cohn et al\\.,? 2016", "shortCiteRegEx": "Cohn et al\\.", "year": 2016}, {"title": "Neural Turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka."], "venue": "arXiv preprint arXiv:1410.5401.", "citeRegEx": "Graves et al\\.,? 2014", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Kenlm: Faster and smaller language model queries", "author": ["Kenneth Heafield."], "venue": "Proc. of the Sixth Workshop on Statistical Machine Translation, pages 187\u2013 197.", "citeRegEx": "Heafield.,? 2011", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "ACL\u201915.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation", "author": ["Melvin Johnson", "Mike Schuster", "Quoc V Le", "Maxim Krikun", "Yonghui Wu", "Zhifeng Chen", "Nikhil Thorat", "Fernanda Vi\u00e9gas", "Martin Wattenberg", "Greg Corrado"], "venue": null, "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proc. of NAACL, pages 48\u201354. Association for Computational Linguistics.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Towards zero unknown word in neural machine translation", "author": ["Xiaoqing Li", "Jiajun Zhang", "Chengqing Zong."], "venue": "Proc. of IJCAI, pages 2852\u20132858.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "arXiv preprint arXiv:1410.8206.", "citeRegEx": "Luong et al\\.,? 2014", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Coverage embedding models for neural machine translation", "author": ["Haitao Mi", "Baskaran Sankaran", "Zhiguo Wang", "Abe Ittycheriah."], "venue": "arXiv preprint arXiv:1605.03148.", "citeRegEx": "Mi et al\\.,? 2016", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Frans J. Och", "Hermann Ney."], "venue": "Computational Linguistics, 29:19\u201351.", "citeRegEx": "Och and Ney.,? 2003", "shortCiteRegEx": "Och and Ney.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of ACL, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "ACL 16.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Syntactically guided neural machine translation", "author": ["Felix Stahlberg", "Eva Hasler", "Aurelien Waite", "Bill Byrne."], "venue": "Proc. of ACL, pages 299\u2013305.", "citeRegEx": "Stahlberg et al\\.,? 2016", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Memory-enhanced decoder for neural machine translation", "author": ["Mingxuan Wang", "Zhengdong Lu", "Hang Li", "Qun Liu."], "venue": "Proc. of EMNLP, pages 278\u2013286.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."], "venue": "arXiv preprint arXiv:1410.3916.", "citeRegEx": "Weston et al\\.,? 2014", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Deep neural networks in machine translation: An overview", "author": ["Jiajun Zhang", "Chengqing Zong"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "Zhang and Zong,? \\Q2015\\E", "shortCiteRegEx": "Zhang and Zong", "year": 2015}, {"title": "Flexible and creative chinese poetry generation using neural memory", "author": ["Jiyuan Zhang", "Yang Feng", "Dong Wang", "Yang Wang", "Andrew Abel", "Shiyue Zhang", "Andi Zhang."], "venue": "arXiv preprint arXiv:1705.03773.", "citeRegEx": "Zhang et al\\.,? 2017", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 8, "context": "Neural Machine Translation (NMT) has been shown to have highly promising performance, particularly when a large amount of training data is available (Wu et al., 2016; Johnson et al., 2016; Mi et al., 2016).", "startOffset": 149, "endOffset": 205}, {"referenceID": 12, "context": "Neural Machine Translation (NMT) has been shown to have highly promising performance, particularly when a large amount of training data is available (Wu et al., 2016; Johnson et al., 2016; Mi et al., 2016).", "startOffset": 149, "endOffset": 205}, {"referenceID": 17, "context": "Although there are different model architectures (Sutskever et al., 2014; Bahdanau et al., 2015), the common principle behind the NMT approach is the same: encoding the meaning of the input into a concept space and performing translation based on this encoding.", "startOffset": 49, "endOffset": 96}, {"referenceID": 1, "context": "Although there are different model architectures (Sutskever et al., 2014; Bahdanau et al., 2015), the common principle behind the NMT approach is the same: encoding the meaning of the input into a concept space and performing translation based on this encoding.", "startOffset": 49, "endOffset": 96}, {"referenceID": 9, "context": ", words and phrases (Koehn et al., 2003).", "startOffset": 20, "endOffset": 40}, {"referenceID": 7, "context": ", Luong et al. (2014); Cho et al.", "startOffset": 2, "endOffset": 22}, {"referenceID": 1, "context": "(2014); Cho et al. (2014); Li et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 1, "context": "(2014); Cho et al. (2014); Li et al. (2016); Arthur et al.", "startOffset": 8, "endOffset": 44}, {"referenceID": 0, "context": "(2016); Arthur et al. (2016); Bentivogli et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "(2016); Arthur et al. (2016); Bentivogli et al. (2016); Zhang et al.", "startOffset": 8, "endOffset": 55}, {"referenceID": 0, "context": "(2016); Arthur et al. (2016); Bentivogli et al. (2016); Zhang et al. (2017). ar X iv :1 70 8.", "startOffset": 8, "endOffset": 76}, {"referenceID": 1, "context": "Before introducing our M-NMT architecture, we will give a brief review of our implementation of the attention-based RNN model first presented by Bahdanau et al. (2015). This model is regarded as the state-of-the-art model and will be used as the baseline system in this study.", "startOffset": 145, "endOffset": 168}, {"referenceID": 1, "context": "To force the generation to focus on a particular segment of the input at each generation step, Bahdanau et al. (2015) proposed an attention mechanism.", "startOffset": 95, "endOffset": 118}, {"referenceID": 1, "context": "We used Tensorflow to implement this model, and the training recipe largely followed the seminal paper of Bahdanau et al. (2015).", "startOffset": 106, "endOffset": 129}, {"referenceID": 20, "context": "The optimization algorithm is the stochastic gradient descent (SGD) with AdaDelta to adjust the learning rate (Zeiler, 2012).", "startOffset": 110, "endOffset": 124}, {"referenceID": 13, "context": "In this work, we use the translation dictionary produced by an SMT system as the knowledge to create the memory, which involves first aligning the training sentence pairs using the GIZA++ toolkit (Och and Ney, 2003) in both directions, and applying the \u201cintersection\u201d refinement rules (Koehn et al.", "startOffset": 196, "endOffset": 215}, {"referenceID": 9, "context": "In this work, we use the translation dictionary produced by an SMT system as the knowledge to create the memory, which involves first aligning the training sentence pairs using the GIZA++ toolkit (Och and Ney, 2003) in both directions, and applying the \u201cintersection\u201d refinement rules (Koehn et al., 2003) to get a single one-to-one alignment for each sentence pair, and then extracting the translation dictionary based on these alignments.", "startOffset": 285, "endOffset": 305}, {"referenceID": 19, "context": ", 2014, 2016) and memory network (Weston et al., 2014).", "startOffset": 33, "endOffset": 54}, {"referenceID": 5, "context": "The idea of memory augmentation was inspired by recent advances in the neural Turing machine (Graves et al., 2014, 2016) and memory network (Weston et al., 2014). These new models equip neural networks with an external memory that can be accessed and manipulated via some trainable operations. The memory idea has been utilized in NMT. For example, Wang et al. (2016) used a memory to extend the state of the decoder RNN in the attention-based NMT.", "startOffset": 94, "endOffset": 368}, {"referenceID": 20, "context": "The idea of combining SMT and NMT was adopted by early NMT research, but these combinations were mostly based on the SMT framework, as discussed in depth in the review paper from Zhang et al. (2015). Cohn et al.", "startOffset": 179, "endOffset": 199}, {"referenceID": 3, "context": "Cohn et al. (2016) proposed to enhance the attention-based NMT by using some structural knowledge from a word-based alignment model.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "Arthur et al. (2016) proposed to involve lexical knowledge to assist with translation, particularly for low-frequency words.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "Compared to the work of (Luong et al., 2014) and (Li et al.", "startOffset": 24, "endOffset": 44}, {"referenceID": 10, "context": ", 2014) and (Li et al., 2016), which relies on postprocessing, our M-NMT approach is more like pre-processing.", "startOffset": 12, "endOffset": 29}, {"referenceID": 10, "context": "Nevertheless, we do share the same idea of using similar words as in (Li et al., 2016), which we think is inevitable if the OOV words are totally new.", "startOffset": 69, "endOffset": 86}, {"referenceID": 7, "context": "Regarding handling OOV words, Jean et al. (2015) presented an efficient training method to support a larger vocabulary, which helps alleviate the OOV problem significantly.", "startOffset": 30, "endOffset": 49}, {"referenceID": 7, "context": "Regarding handling OOV words, Jean et al. (2015) presented an efficient training method to support a larger vocabulary, which helps alleviate the OOV problem significantly. Stahlberg et al. (2016) used SMT to produce candidate results in the form of lattice and NMT to re-score the results.", "startOffset": 30, "endOffset": 197}, {"referenceID": 7, "context": "Regarding handling OOV words, Jean et al. (2015) presented an efficient training method to support a larger vocabulary, which helps alleviate the OOV problem significantly. Stahlberg et al. (2016) used SMT to produce candidate results in the form of lattice and NMT to re-score the results. As SMT uses a larger vocabulary than NMT, some OOV words can be retained. Sennrich et al. (2016) proposed a subword approach, where OOV words are expected to be spelled out by subword units.", "startOffset": 30, "endOffset": 388}, {"referenceID": 7, "context": "Regarding handling OOV words, Jean et al. (2015) presented an efficient training method to support a larger vocabulary, which helps alleviate the OOV problem significantly. Stahlberg et al. (2016) used SMT to produce candidate results in the form of lattice and NMT to re-score the results. As SMT uses a larger vocabulary than NMT, some OOV words can be retained. Sennrich et al. (2016) proposed a subword approach, where OOV words are expected to be spelled out by subword units. Luong et al. (2014) proposed a post-processing approach that learns the position of the source word when an UNK symbol is produced during decoding.", "startOffset": 30, "endOffset": 502}, {"referenceID": 7, "context": "Regarding handling OOV words, Jean et al. (2015) presented an efficient training method to support a larger vocabulary, which helps alleviate the OOV problem significantly. Stahlberg et al. (2016) used SMT to produce candidate results in the form of lattice and NMT to re-score the results. As SMT uses a larger vocabulary than NMT, some OOV words can be retained. Sennrich et al. (2016) proposed a subword approach, where OOV words are expected to be spelled out by subword units. Luong et al. (2014) proposed a post-processing approach that learns the position of the source word when an UNK symbol is produced during decoding. By this position information, the UNK symbol (unknown words) can be replaced by the correct translation using a lexical table. Li et al. (2016) proposed a replace-and-restore approach that replaces infrequent words with similar words before the training and decoding, and restores rare words and their target words, obtained from a lexical table.", "startOffset": 30, "endOffset": 774}, {"referenceID": 9, "context": "Memory data To construct the memory, we used the GIZA++ toolkit (Koehn et al., 2003) to align the training data in both directions, and kept the word pairs that appeared in the phrase tables of both directions.", "startOffset": 64, "endOffset": 84}, {"referenceID": 6, "context": "For the language model, the KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model (with the Keneser-Ney smoothing) on the target side of the training data.", "startOffset": 42, "endOffset": 58}, {"referenceID": 1, "context": "NMT baseline: For NMT, we reproduced the attention-based RNN model proposed by Bahdanau et al. (2015), which is denoted by NMT.", "startOffset": 79, "endOffset": 102}, {"referenceID": 14, "context": "Evaluation metrics The translation performance was evaluated using the BLEU score with caseinsensitive n \u2264 4-grams (Papineni et al., 2002).", "startOffset": 115, "endOffset": 138}, {"referenceID": 0, "context": "For comparison purposes, the lexical prediction approach proposed by (Arthur et al., 2016) was also implemented.", "startOffset": 69, "endOffset": 90}, {"referenceID": 0, "context": "However, the improvement seems less significant than reported in (Arthur et al., 2016).", "startOffset": 65, "endOffset": 86}, {"referenceID": 11, "context": "posed by Luong et al. (2014) was also implemented.", "startOffset": 9, "endOffset": 29}, {"referenceID": 10, "context": "We also implemented the replace-and-restore approach reported by Li et al. (2016), but found performance to be poor (the BLEU scores are 13.", "startOffset": 65, "endOffset": 82}], "year": 2017, "abstractText": "Neural machine translation (NMT) has achieved notable success in recent times, however it is also widely recognized that this approach has limitations with handling infrequent words and word pairs. This paper presents a novel memoryaugmented NMT (M-NMT) architecture, which stores knowledge about how words (usually infrequently encountered ones) should be translated in a memory and then utilizes them to assist the neural model. We use this memory mechanism to combine the knowledge learned from a conventional statistical machine translation system and the rules learned by an NMT system, and also propose a solution for out-of-vocabulary (OOV) words based on this framework. Our experiments on two Chinese-English translation tasks demonstrated that the M-NMT architecture outperformed the NMT baseline by 9.0 and 2.7 BLEU points on the two tasks, respectively. Additionally, we found this architecture resulted in a much more effective OOV treatment compared to competitive methods.", "creator": "LaTeX with hyperref package"}}}