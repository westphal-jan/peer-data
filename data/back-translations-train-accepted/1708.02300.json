{"id": "1708.02300", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Aug-2017", "title": "Reinforced Video Captioning with Entailment Rewards", "abstract": "Sequence-to-sequence models have shown promising improvements on the temporal task of video captioning, but they optimize word-level cross-entropy loss during training. First, using policy gradient and mixed-loss methods for reinforcement learning, we directly optimize sentence-level task-based metrics (as rewards), achieving significant improvements over the baseline, based on both automatic metrics and human evaluation on multiple datasets. Next, we propose a novel entailment-enhanced reward (CIDEnt) that corrects phrase-matching based metrics (such as CIDEr) to only allow for logically-implied partial matches and avoid contradictions, achieving further significant improvements over the CIDEr-reward model. Overall, our CIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset.", "histories": [["v1", "Mon, 7 Aug 2017 20:50:24 GMT  (1956kb)", "http://arxiv.org/abs/1708.02300v1", "EMNLP 2017 (9 pages)"]], "COMMENTS": "EMNLP 2017 (9 pages)", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV cs.LG", "authors": ["ramakanth pasunuru", "mohit bansal"], "accepted": true, "id": "1708.02300"}, "pdf": {"name": "1708.02300.pdf", "metadata": {"source": "CRF", "title": "Reinforced Video Captioning with Entailment Rewards", "authors": ["Ramakanth Pasunuru"], "emails": ["mbansal}@cs.unc.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 8.02 300v 1 [cs.C L] 7A ug2 017Promising improvements in the temporal task of captioning videos, but they optimize cross-word entropy during training. First, we directly optimize task-based metrics at the sentence level (as a reward) by making significant improvements across the baseline, both based on automatic metrics and on human evaluation of multiple datasets. Next, we propose a novel entailment-enhanced reward (CIDEnt) that corrects phrase matching-based metrics (such as CIDEnt) to allow only logically implicit partial matches and avoid contradictions, thereby achieving further significant improvements over the CIDEr reward model. Overall, our CIDEnt reward model achieves the state-of-the-art based MR-SVT data set."}, {"heading": "1 Introduction", "text": "The task of video subtitling (Fig. 1) is an important next step to image subtitling, with additional modeling of temporal knowledge and action sequences, and has several applications in online content search, support of visually impaired, etc. Advancements in neural sequences reward learning, however, has shown promising improvements in this task, based on encoder decoders, attention, and hierarchical models (Venugopalan et al., 2015a; Pan et al., 2016a). However, these models are still trained with a verbatim cross-entropy loss that does not correlate well with the set-level metrics that the task is eventually evaluated, CIDER, BLEU). Moreover, these models suffer from exposure bias (Ran-zato et al., 2016), which occurs when a model is exposed only to the distribution of training data, rather than its own predictions."}, {"heading": "2 Related Work", "text": "Previous work has presented several sequence sequence models for captioning videos, using attention, hierarchical RNNs, 3D CNN video functions, common embedding spaces, speech fusion, etc., but using cross-word entropy loss training (Venugopalan et al., 2015a; Yao et al., 2015; Pan et al., 2016a, b; Venugopalan et al., 2016). The political gradient for captioning was recently presented by Ranzato et al. (2016), using a paradigm for the use of non-differentiated valuation metrics as a reward."}, {"heading": "3 Models", "text": "The results are similar to the results of the LSTM-RNN study with an attention mechanism. Let's evaluate the results of the LSTM-RNN study with an attention mechanism, in which we apply the model parameters and parameters of the LSTM-RNN study. Let's evaluate the results of the LSTM-RNN study with an attention mechanism, in which the LSTM-RNN study also compensates for the loss of ground truth, then the cross-entropy loss function is: L (\u03b8) = \u2212 m study t = 1log p (w \u0445 1: t \u2212 n), where the learning phase (wt: 1) = softmax (W Thdt), W T is the projection matrix, and h d t are the generated word and the RNN decoders hidden states in time."}, {"heading": "4 Reward Functions", "text": "Image caption Metric Reward Previous Image Captioning Papers have used traditional captioning metrics as as as CIDER, BLEU, or METEOR as reward functions, based on the match between the generated caption sample and the ground-truth reference (s). First, it was also used by Vedantam2We with the curriculum learning \"MIXER's strategy of Ranzato et al. (2015) that CIDER, based on a consensus measure across several human reference captions, has a higher correlation with human evaluation as other metrics as as as as as as METEOR, ROUGE and BLEU. They further showed that CIDER gets better with more number of human references (and this is a good for our video captions) datasets, which have other metrics as METEOR, and BLEU."}, {"heading": "5 Experimental Setup", "text": "Datasets We use 2 datasets: MSR-VTT (Xu et al., 2016) has 10 000 videos, 20 references / video; and YouTube2Text / MSVD (Chen and Dolan, 2011) has 1970 videos, 40 references / video. Standard splits and other details in heading. Automatic rating We also use several standard automated rating metrics: METEOR, BLEU4, CIDEr-D and ROUGE-L (from MS-COCO rating server (Chen et al., 2015)). Human evaluation We also present human ratings for comparing baseline-XE, CIDEr-RL and CIDEnt-Williams-RL models, especially because the automatic metrics cannot be trusted exclusively. Relevance measurements of how related the generated image caption w.r.t is to the video content, whereas coherence of the readability of the overall image gain generated by Williams-RL models is based on other caption.Our classification is highly accurate (Our classification)."}, {"heading": "6 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Primary Results", "text": "\"I think it's going to take a lot of time to get to the bottom of it, but I think it's going to take a lot of time to get to the bottom of it,\" he said, adding that he had no idea if he would be able to get to the bottom of it."}, {"heading": "6.2 Other Datasets", "text": "We have also tried our CIDER and CIDEnt reward models on the YouTube2Text dataset. In Table 5, we first see strong improvements over our CIDER-RL model above the cross-entropy baseline. Next, the CIDEnt-RL model also shows some improvements over the CIDER-RL model, such as the BLEU value and the new entailment-corrected CIDEnt value. In addition, it achieves significant improvements in the assessment of human relevance (250 samples).8"}, {"heading": "6.3 Other Metrics as Reward", "text": "As discussed in paragraph 4, CIDER is the most promising metric that can be used as a reward for subtitling, based on the results of previous work and also on ours. We investigated the use of other metrics as a reward. When using BLEU as a reward (on MSR-VTT), we found that this BLEU-RL model achieves BLEU-metric improvements, but is worse than the cross-entropy baseline for human evaluation. Similarly, a BLEUEnt-RL model achieves BLEU and BLEUEnt-metric improvements, but is worse in human evaluation. 8This dataset has a very small Dev set, which causes tuning problems - we plan to use a better turn / dev repartition in future work. We also experimented with the new SPICE metric (Anderson et al., 2016) as a reward, but this led to long, repeated phases (such as 2016b)."}, {"heading": "6.4 Analysis", "text": "Fig. 1 shows an example where our CIDEntreward model correctly generates a caption in the style of truth, while the CIDER reward model generates a caption with no consequences because that caption still gets a high phrase matching score."}, {"heading": "7 Conclusion", "text": "We first presented an approach to the mixed loss policy gradient in video subtitling that enables metric optimization; next, we presented a entailment-corrected CIDEnt reward that further improves results and advances the state of the art in MSR-VTT. In future work, we will apply our entailment-corrected rewards to other directed generational tasks such as captions and document summaries (using the new multi-domain NLI corpus (Williams et al., 2017))."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their helpful comments. This work was supported by a Google Faculty Research Award, an IBM Faculty Award, a Bloomberg Data Science Research Grant and NVidia GPU Awards."}, {"heading": "A Attention-based Baseline Model (Cross-Entropy)", "text": "Our attention base model is similar to the Bahdanau et al. (2015) architecture, in which we encode video functions at the input image level into a bidirectional LSTM RNN and then generate the caption with a single layer LSTM-RNN, with an attention mechanism. Let {f1, f2,..., fn} be the frame level characteristics of a video clip and {w1, w2,..., wm} be the sequence of words forming a caption. Distribution of the words at the time t against the previously generated words and input image functions is as follows: p (wt | w1, f1: n) = Softmax (W Thdt) (6), where wt and h d t is the generated word and the hidden state of the LSTM decoder at the time. W T is the projection matrix, hdt is the projection matrix."}, {"heading": "B Reinforcement Learning (Policy Gradient)", "text": "Traditional video capture systems minimize entropy loss during training, but are typically evaluated using phrase matching metrics (BLEU, METEOR, CIDER and ROUGE-L. This discrepancy can be achieved by directly optimizing the distinguishable metric values using policy gradients p\u03b8, where \u03b8 represents the model parameters. In our capture system, our base model acts as an agent and interacts with its environment (video and caption). At each step of time, the agent generates a word (action), and the generation of the end-of-sequence token results in a reward r to the agent. Our training goal is to minimize the negative expected reward function provided by: L (\u03b8) = Ews (s)] s (11) s: where ws = {ws 1, ws 2,..., wsm} and w s t: the word sampled from the model at the time."}, {"heading": "C Experimental Setup", "text": "C.1 MSR-VTT DatasetMSR-VTT is a diverse collection of 10,000 video clips (41.2 hours running time) from a commercial video search engine. Each video has 20 human-annotated reference labels collected by Amazon Mechanical Turk (AMT). We use the standard split provided in (Xu et al., 2016), i.e. 6513 for training, 497 for testing and remaining for testing. For each video, we sample at 3fps and extract Inception-v4 (Szegedy et al., 2016) functions from these sampled frames and we also remove all punctuation from the text data. C.2 YouTube2Text DatasetWe also rate our models on YouTube2Text Dataset (Chen and Dolan, 2011). This dataset has 1970 video clips and each clip is commented with an average of 40 human-inflected labels. We also use our models on YouTube2Text Dataset (Chen and Dolan, 2011). This dataset has 1970 video clips and each clip is commented with an average of 40 human-inflected labels."}, {"heading": "D Training Details", "text": "For each of our main models (baseline, CIDER and CIDEnt), we report the results of a 5-avg ensemble in which we execute the model 5 times with different initialization random sizes and take the average probabilities for each time step of the decoder during the inference time. We use a fixed size step LSTM-RNN encoder decoder decoder, with the step size of the encoder of 50 and the step size of the decoder of 16. Each LSTM has a hidden size of 1024. We use Inception v4 functions as video frame level features. We use text embedding size of 512. We also project the 1536 dimensions of the image functions (Inception v4) to 512 dimensions. We apply dropout to vertical connections, as suggested in S\u00e9mba et al. (2014), with a value of 0.5 and a serve clip size of 10."}, {"heading": "E Analysis", "text": "Figure 3 shows some examples where our CIDEnt reward model produces better captions than the captions generated by the CIDErreward model. This is because the CIDEr stylecapturing metrics achieve a high score even when the generation does not necessarily include the basic truth, but merely represents a high phrase overlap. Obviously, this can cause problems when a single wrong word such as negation, contradiction, or wrong action / object is inserted. On the other hand, our de-ailment-enhanced CIDEnt score is high only when both CIDER and the entanglement classifier achieve high scores. Again, the CIDEr RL model produces better captions than the baseline cross-entropy model, which has no knowledge of bylaws at all."}], "references": [{"title": "SPICE: Semantic propositional image caption evaluation", "author": ["Peter Anderson", "Basura Fernando", "Mark Johnson", "Stephen Gould."], "venue": "ECCV, pages 382\u2013398.", "citeRegEx": "Anderson et al\\.,? 2016", "shortCiteRegEx": "Anderson et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyunghyunCho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning."], "venue": "EMNLP.", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["David L Chen", "William B Dolan."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 190\u2013200.", "citeRegEx": "Chen and Dolan.,? 2011", "shortCiteRegEx": "Chen and Dolan.", "year": 2011}, {"title": "Microsoft COCO captions: Data collection and evaluation server", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C Lawrence Zitnick."], "venue": "arXiv preprint arXiv:1504.00325.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."], "venue": "EMNLP.", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "The PASCAL recognising textual entailment challenge", "author": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini."], "venue": "Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment, pages 177\u2013", "citeRegEx": "Dagan et al\\.,? 2006", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Michael Denkowski", "Alon Lavie."], "venue": "EACL.", "citeRegEx": "Denkowski and Lavie.,? 2014", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "An introduction to the bootstrap", "author": ["Bradley Efron", "Robert J Tibshirani."], "venue": "CRC press.", "citeRegEx": "Efron and Tibshirani.,? 1994", "shortCiteRegEx": "Efron and Tibshirani.", "year": 1994}, {"title": "Comparing automatic evaluation measures for image description", "author": ["Desmond Elliott", "Frank Keller."], "venue": "ACL, pages 452\u2013457.", "citeRegEx": "Elliott and Keller.,? 2014", "shortCiteRegEx": "Elliott and Keller.", "year": 2014}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier."], "venue": "Journal of Artificial Intelligence Research, 47:853\u2013899.", "citeRegEx": "Hodosh et al\\.,? 2013", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "UNAL-NLP: Combining soft cardinality features for semantic textual similarity, relatedness and entailment", "author": ["Sergio Jimenez", "George Duenas", "Julia Baquero", "Alexander Gelbukh", "Av Juan Dios B\u00e1tiz", "AvMendiz\u00e1bal."], "venue": "In SemEval, pages", "citeRegEx": "Jimenez et al\\.,? 2014", "shortCiteRegEx": "Jimenez et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "ICLR.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Illinois-LH: A denotational and distributional approach to semantics", "author": ["Alice Lai", "Julia Hockenmaier."], "venue": "Proc. SemEval, 2:5.", "citeRegEx": "Lai and Hockenmaier.,? 2014", "shortCiteRegEx": "Lai and Hockenmaier.", "year": 2014}, {"title": "ROUGE: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 workshop, volume 8.", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau."], "venue": "EMNLP.", "citeRegEx": "Liu et al\\.,? 2016a", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Improved image captioning via policy gradient optimization of SPIDEr", "author": ["Siqi Liu", "Zhenhai Zhu", "Ning Ye", "Sergio Guadarrama", "Kevin Murphy."], "venue": "arXiv preprint arXiv:1612.00370.", "citeRegEx": "Liu et al\\.,? 2016b", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Computer-intensive methods for testing hypotheses", "author": ["Eric W Noreen."], "venue": "Wiley New York.", "citeRegEx": "Noreen.,? 1989", "shortCiteRegEx": "Noreen.", "year": 1989}, {"title": "Hierarchical recurrent neural encoder for video representation with application to captioning", "author": ["Pingbo Pan", "ZhongwenXu", "Yi Yang", "FeiWu", "Yueting Zhuang."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages", "citeRegEx": "Pan et al\\.,? 2016a", "shortCiteRegEx": "Pan et al\\.", "year": 2016}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Yingwei Pan", "Tao Mei", "Ting Yao", "Houqiang Li", "Yong Rui."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4594\u20134602.", "citeRegEx": "Pan et al\\.,? 2016b", "shortCiteRegEx": "Pan et al\\.", "year": 2016}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "ToddWard", "WeiJing Zhu."], "venue": "ACL, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A decomposable attention model for natural language inference", "author": ["Ankur P Parikh", "Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Jakob Uszkoreit."], "venue": "EMNLP.", "citeRegEx": "Parikh et al\\.,? 2016", "shortCiteRegEx": "Parikh et al\\.", "year": 2016}, {"title": "Multitask video captioning with video and entailment generation", "author": ["Ramakanth Pasunuru", "Mohit Bansal."], "venue": "Proceedings of ACL.", "citeRegEx": "Pasunuru and Bansal.,? 2017", "shortCiteRegEx": "Pasunuru and Bansal.", "year": 2017}, {"title": "A deep reinforced model for abstractive summarization", "author": ["Romain Paulus", "Caiming Xiong", "Richard Socher."], "venue": "arXiv preprint arXiv:1705.04304.", "citeRegEx": "Paulus et al\\.,? 2017", "shortCiteRegEx": "Paulus et al\\.", "year": 2017}, {"title": "Sequence level training with recurrent neural networks. In ICLR", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "Self-critical sequence training for image captioning", "author": ["Steven J Rennie", "Etienne Marcheret", "Youssef Mroueh", "Jarret Ross", "Vaibhava Goel."], "venue": "arXiv preprint arXiv:1612.00563.", "citeRegEx": "Rennie et al\\.,? 2016", "shortCiteRegEx": "Rennie et al\\.", "year": 2016}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom."], "venue": "ICLR.", "citeRegEx": "Rockt\u00e4schel et al\\.,? 2016", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2016}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning", "author": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke."], "venue": "CoRR.", "citeRegEx": "Szegedy et al\\.,? 2016", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "CIDEr: Consensus-based image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh."], "venue": "CVPR, pages 4566\u20134575.", "citeRegEx": "Vedantam et al\\.,? 2015", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Improving lstm-based video description with linguistic knowledge mined from text", "author": ["Subhashini Venugopalan", "Lisa Anne Hendricks", "Raymond Mooney", "Kate Saenko."], "venue": "EMNLP.", "citeRegEx": "Venugopalan et al\\.,? 2016", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2016}, {"title": "Sequence to sequence-video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeffrey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko."], "venue": "CVPR, pages 4534\u20134542.", "citeRegEx": "Venugopalan et al\\.,? 2015a", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["Subhashini Venugopalan", "Huijuan Xu", "Jeff Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko."], "venue": "NAACL HLT.", "citeRegEx": "Venugopalan et al\\.,? 2015b", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "A broad-coverage challenge corpus for sentence understanding through inference", "author": ["Adina Williams", "Nikita Nangia", "Samuel R Bowman."], "venue": "arXiv preprint arXiv:1704.05426.", "citeRegEx": "Williams et al\\.,? 2017", "shortCiteRegEx": "Williams et al\\.", "year": 2017}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning, 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "MSR-VTT: A large video description dataset for bridging video and language", "author": ["Jun Xu", "Tao Mei", "Ting Yao", "Yong Rui."], "venue": "InCVPR, pages 5288\u2013 5296.", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville."], "venue": "CVPR, pages 4507\u20134515.", "citeRegEx": "Yao et al\\.,? 2015", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Reinforcement learning neural turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever."], "venue": "arXiv preprint arXiv:1505.00521, 362.", "citeRegEx": "Zaremba and Sutskever.,? 2015", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 30, "context": "Advancements in neural sequenceto-sequence learning has shown promising improvements on this task, based on encoderdecoder, attention, and hierarchical models (Venugopalan et al., 2015a; Pan et al., 2016a).", "startOffset": 159, "endOffset": 205}, {"referenceID": 18, "context": "Advancements in neural sequenceto-sequence learning has shown promising improvements on this task, based on encoderdecoder, attention, and hierarchical models (Venugopalan et al., 2015a; Pan et al., 2016a).", "startOffset": 159, "endOffset": 205}, {"referenceID": 24, "context": "First, using a sequence-level training, policy gradient approach (Ranzato et al., 2016), we allow video captioning models to directly optimize these nondifferentiable metrics, as rewards in a reinforcement learning paradigm.", "startOffset": 65, "endOffset": 87}, {"referenceID": 23, "context": "We also address the exposure bias issue by using a mixed-loss (Paulus et al., 2017; Wu et al., 2016), i.", "startOffset": 62, "endOffset": 100}, {"referenceID": 30, "context": ", but using word-level cross entropy loss training (Venugopalan et al., 2015a; Yao et al., 2015; Pan et al., 2016a,b; Venugopalan et al., 2016).", "startOffset": 51, "endOffset": 143}, {"referenceID": 35, "context": ", but using word-level cross entropy loss training (Venugopalan et al., 2015a; Yao et al., 2015; Pan et al., 2016a,b; Venugopalan et al., 2016).", "startOffset": 51, "endOffset": 143}, {"referenceID": 29, "context": ", but using word-level cross entropy loss training (Venugopalan et al., 2015a; Yao et al., 2015; Pan et al., 2016a,b; Venugopalan et al., 2016).", "startOffset": 51, "endOffset": 143}, {"referenceID": 20, "context": "Policy gradient for image captioning was recently presented by Ranzato et al. (2016), using a mixed sequence level training paradigm to use non-differentiable evaluation metrics as rewards.", "startOffset": 63, "endOffset": 85}, {"referenceID": 14, "context": "Liu et al. (2016b) and Rennie et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "Liu et al. (2016b) and Rennie et al. (2016) improve upon this using Monte Carlo roll-outs and a test inference baseline, respectively.", "startOffset": 0, "endOffset": 44}, {"referenceID": 14, "context": "(2016) improve upon this using Monte Carlo roll-outs and a test inference baseline, respectively. Paulus et al. (2017) presented summarization results with ROUGE rewards, in a mixed-loss setup.", "startOffset": 78, "endOffset": 119}, {"referenceID": 6, "context": "Recognizing Textual Entailment (RTE) is a traditional NLP task (Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014), boosted by a large dataset (SNLI) recently introduced by Bowman et al.", "startOffset": 63, "endOffset": 132}, {"referenceID": 13, "context": "Recognizing Textual Entailment (RTE) is a traditional NLP task (Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014), boosted by a large dataset (SNLI) recently introduced by Bowman et al.", "startOffset": 63, "endOffset": 132}, {"referenceID": 11, "context": "Recognizing Textual Entailment (RTE) is a traditional NLP task (Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014), boosted by a large dataset (SNLI) recently introduced by Bowman et al.", "startOffset": 63, "endOffset": 132}, {"referenceID": 5, "context": "There have been several leaderboard models on SNLI (Cheng et al., 2016; Rockt\u00e4schel et al., 2016); we focus on the decomposable, intra-sentence attention model of Parikh et al.", "startOffset": 51, "endOffset": 97}, {"referenceID": 26, "context": "There have been several leaderboard models on SNLI (Cheng et al., 2016; Rockt\u00e4schel et al., 2016); we focus on the decomposable, intra-sentence attention model of Parikh et al.", "startOffset": 51, "endOffset": 97}, {"referenceID": 2, "context": ", 2014), boosted by a large dataset (SNLI) recently introduced by Bowman et al. (2015). There have been several leaderboard models on SNLI (Cheng et al.", "startOffset": 66, "endOffset": 87}, {"referenceID": 2, "context": ", 2014), boosted by a large dataset (SNLI) recently introduced by Bowman et al. (2015). There have been several leaderboard models on SNLI (Cheng et al., 2016; Rockt\u00e4schel et al., 2016); we focus on the decomposable, intra-sentence attention model of Parikh et al. (2016). Recently, Pasunuru and Bansal (2017) used multi-task learning to combine video captioning with entailment and video generation.", "startOffset": 66, "endOffset": 272}, {"referenceID": 2, "context": ", 2014), boosted by a large dataset (SNLI) recently introduced by Bowman et al. (2015). There have been several leaderboard models on SNLI (Cheng et al., 2016; Rockt\u00e4schel et al., 2016); we focus on the decomposable, intra-sentence attention model of Parikh et al. (2016). Recently, Pasunuru and Bansal (2017) used multi-task learning to combine video captioning with entailment and video generation.", "startOffset": 66, "endOffset": 310}, {"referenceID": 28, "context": "Several papers have presented the relative comparison of image captioning metrics, and their pros and cons (Vedantam et al., 2015; Anderson et al., 2016; Liu et al., 2016b; Hodosh et al., 2013; Elliott and Keller, 2014).", "startOffset": 107, "endOffset": 219}, {"referenceID": 0, "context": "Several papers have presented the relative comparison of image captioning metrics, and their pros and cons (Vedantam et al., 2015; Anderson et al., 2016; Liu et al., 2016b; Hodosh et al., 2013; Elliott and Keller, 2014).", "startOffset": 107, "endOffset": 219}, {"referenceID": 16, "context": "Several papers have presented the relative comparison of image captioning metrics, and their pros and cons (Vedantam et al., 2015; Anderson et al., 2016; Liu et al., 2016b; Hodosh et al., 2013; Elliott and Keller, 2014).", "startOffset": 107, "endOffset": 219}, {"referenceID": 10, "context": "Several papers have presented the relative comparison of image captioning metrics, and their pros and cons (Vedantam et al., 2015; Anderson et al., 2016; Liu et al., 2016b; Hodosh et al., 2013; Elliott and Keller, 2014).", "startOffset": 107, "endOffset": 219}, {"referenceID": 9, "context": "Several papers have presented the relative comparison of image captioning metrics, and their pros and cons (Vedantam et al., 2015; Anderson et al., 2016; Liu et al., 2016b; Hodosh et al., 2013; Elliott and Keller, 2014).", "startOffset": 107, "endOffset": 219}, {"referenceID": 1, "context": "Attention Baseline (Cross-Entropy) Our attention-based seq-to-seq baseline model is similar to the Bahdanau et al. (2015) architecture, where we encode input frame level video features {f1:n} via a bi-directional LSTM-RNN and then generate the caption w1:m using an LSTM-RNN with an attention mechanism.", "startOffset": 99, "endOffset": 122}, {"referenceID": 33, "context": "Based on the REINFORCE algorithm (Williams, 1992), the gradients of this nondifferentiable, reward-based loss function are:", "startOffset": 33, "endOffset": 49}, {"referenceID": 24, "context": "We follow Ranzato et al. (2016) approximating the above gradients via a single sampled word", "startOffset": 10, "endOffset": 32}, {"referenceID": 15, "context": "Mixed Loss During reinforcement learning, optimizing for only the reinforcement loss (with automatic metrics as rewards) doesn\u2019t ensure the readability and fluency of the generated caption, and there is also a chance of gaming the metrics without actually improving the quality of the output (Liu et al., 2016a).", "startOffset": 292, "endOffset": 311}, {"referenceID": 23, "context": "Hence, for training our reinforcement based policy gradients, we use a mixed loss function, which is a weighted combination of the cross-entropy loss (XE) and the reinforcement learning loss (RL), similar to the previous work (Paulus et al., 2017; Wu et al., 2016).", "startOffset": 226, "endOffset": 264}, {"referenceID": 23, "context": "We also experimented with the curriculum learning \u2018MIXER\u2019 strategy of Ranzato et al. (2016), where the XE+RL annealing is based on the decoder time-steps; however, the mixed loss function strategy (described above) performed better in terms of maintaining output caption fluency.", "startOffset": 70, "endOffset": 92}, {"referenceID": 14, "context": "(2016), where the XE+RL annealing is based on the decoder time-steps; however, the mixed loss function strategy (described above) performed better in terms of maintaining output caption fluency. et al. (2015) that CIDEr, based on a consensus measure across several human reference captions, has a higher correlation with human evaluation than other metrics such as METEOR, ROUGE, and BLEU.", "startOffset": 29, "endOffset": 209}, {"referenceID": 24, "context": "More recently, Rennie et al. (2016) further showed that CIDEr as a reward in image captioning outperforms all other metrics as a reward, not just in terms of improvements on CIDEr metric, but also on all other metrics.", "startOffset": 15, "endOffset": 36}, {"referenceID": 21, "context": "To achieve an accurate entailment score, we adapt the state-of-theart decomposable-attention model of Parikh et al. (2016) trained on the SNLI corpus (image caption domain).", "startOffset": 102, "endOffset": 123}, {"referenceID": 34, "context": "Datasets We use 2 datasets: MSR-VTT (Xu et al., 2016) has 10, 000 videos, 20 references/video; and YouTube2Text/MSVD (Chen and Dolan, 2011) has 1970 videos, 40 references/video.", "startOffset": 36, "endOffset": 53}, {"referenceID": 3, "context": ", 2016) has 10, 000 videos, 20 references/video; and YouTube2Text/MSVD (Chen and Dolan, 2011) has 1970 videos, 40 references/video.", "startOffset": 71, "endOffset": 93}, {"referenceID": 4, "context": "Automatic Evaluation We use several standard automated evaluation metrics: METEOR, BLEU4, CIDEr-D, and ROUGE-L (from MS-COCO evaluation server (Chen et al., 2015)).", "startOffset": 143, "endOffset": 162}, {"referenceID": 20, "context": "Our entailment classifier based on Parikh et al. (2016) is 92% accurate on entailment in the caption domain, hence serving as a highly accurate reward score.", "startOffset": 35, "endOffset": 56}, {"referenceID": 20, "context": "Our entailment classifier based on Parikh et al. (2016) is 92% accurate on entailment in the caption domain, hence serving as a highly accurate reward score. For other domains in future tasks such as new summarization, we plan to use the new multi-domain dataset by Williams et al. (2017). Training Details All the hyperparameters are tuned on the validation set.", "startOffset": 35, "endOffset": 289}, {"referenceID": 17, "context": "05 for BLEU, based on the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1994).", "startOffset": 41, "endOffset": 83}, {"referenceID": 8, "context": "05 for BLEU, based on the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1994).", "startOffset": 41, "endOffset": 83}, {"referenceID": 20, "context": "com/leaderboard), plus the 10-ensemble video+entailment generation multi-task model of Pasunuru and Bansal (2017). Statistical significance of p < 0.", "startOffset": 87, "endOffset": 114}, {"referenceID": 32, "context": "4 Yao et al. (2015) 35.", "startOffset": 2, "endOffset": 20}, {"referenceID": 32, "context": "2 Xu et al. (2016) 36.", "startOffset": 2, "endOffset": 19}, {"referenceID": 21, "context": "9 Pasunuru and Bansal (2017) 40.", "startOffset": 2, "endOffset": 29}, {"referenceID": 0, "context": "We also experimented with the new SPICE metric (Anderson et al., 2016) as a reward, but this produced long repetitive phrases (as also discussed in Liu et al.", "startOffset": 47, "endOffset": 70}, {"referenceID": 0, "context": "We also experimented with the new SPICE metric (Anderson et al., 2016) as a reward, but this produced long repetitive phrases (as also discussed in Liu et al. (2016b)).", "startOffset": 48, "endOffset": 167}, {"referenceID": 32, "context": "In future work, we are applying our entailment-corrected rewards to other directed generation tasks such as image captioning and document summarization (using the new multi-domain NLI corpus (Williams et al., 2017)).", "startOffset": 191, "endOffset": 214}, {"referenceID": 1, "context": "Our attention baseline model is similar to the Bahdanau et al. (2015) architecture, where we encode input frame level video features to a bi-directional LSTM-RNN and then generate the caption using a single layer LSTM-RNN, with an attention mechanism.", "startOffset": 47, "endOffset": 70}, {"referenceID": 33, "context": "Based on the REINFORCE algorithm (Williams, 1992), the gradients of the non-differentiable, reward-based loss function can be computed as follows:", "startOffset": 33, "endOffset": 49}, {"referenceID": 33, "context": "Adding a baseline estimator reduces this variance (Williams, 1992) without changing the expected gradient.", "startOffset": 50, "endOffset": 66}, {"referenceID": 36, "context": "\u2202L \u2202st is given by (Zaremba and Sutskever, 2015) as follows:", "startOffset": 19, "endOffset": 48}, {"referenceID": 34, "context": "We use the standard split as provided in (Xu et al., 2016), i.", "startOffset": 41, "endOffset": 58}, {"referenceID": 27, "context": "For each video, we sample at 3fps and we extract Inception-v4 (Szegedy et al., 2016) features from these sampled frames and we also remove all the punctuations from the text data.", "startOffset": 62, "endOffset": 84}, {"referenceID": 3, "context": "We also evaluate our models on YouTube2Text dataset (Chen and Dolan, 2011).", "startOffset": 52, "endOffset": 74}, {"referenceID": 30, "context": "We use the standard split as given in (Venugopalan et al., 2015a), i.", "startOffset": 38, "endOffset": 65}, {"referenceID": 7, "context": "We use several standard automated evaluation metrics: METEOR (Denkowski and Lavie, 2014), BLEU-4 (Papineni et al.", "startOffset": 61, "endOffset": 88}, {"referenceID": 20, "context": "We use several standard automated evaluation metrics: METEOR (Denkowski and Lavie, 2014), BLEU-4 (Papineni et al., 2002), CIDEr-D (Vedantam et al.", "startOffset": 97, "endOffset": 120}, {"referenceID": 28, "context": ", 2002), CIDEr-D (Vedantam et al., 2015), and ROUGE-L (Lin, 2004).", "startOffset": 17, "endOffset": 40}, {"referenceID": 14, "context": ", 2015), and ROUGE-L (Lin, 2004).", "startOffset": 21, "endOffset": 32}, {"referenceID": 4, "context": "We use the standard Microsoft-COCO evaluation server (Chen et al., 2015).", "startOffset": 53, "endOffset": 72}, {"referenceID": 12, "context": "We use Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.", "startOffset": 22, "endOffset": 43}, {"referenceID": 23, "context": "All our reward-based models use mixed loss optimization (Paulus et al., 2017; Wu et al., 2016), where we train the model based on weighted (\u03b3) combination of crossentropy loss and reinforcement loss.", "startOffset": 56, "endOffset": 94}, {"referenceID": 34, "context": "We apply dropout to vertical connections as proposed in Zaremba et al. (2014), with a value 0.", "startOffset": 56, "endOffset": 78}], "year": 2017, "abstractText": "Sequence-to-sequence models have shown promising improvements on the temporal task of video captioning, but they optimize word-level cross-entropy loss during training. First, using policy gradient and mixed-loss methods for reinforcement learning, we directly optimize sentence-level task-based metrics (as rewards), achieving significant improvements over the baseline, based on both automatic metrics and human evaluation on multiple datasets. Next, we propose a novel entailment-enhanced reward (CIDEnt) that corrects phrase-matching based metrics (such as CIDEr) to only allow for logically-implied partial matches and avoid contradictions, achieving further significant improvements over the CIDEr-reward model. Overall, our CIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset.", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}