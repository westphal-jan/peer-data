{"id": "1708.02383", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Aug-2017", "title": "Learning how to Active Learn: A Deep Reinforcement Learning Approach", "abstract": "Active learning aims to select a small subset of data for annotation such that a classifier learned on the data is highly accurate. This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of heuristics varies between datasets. To address these shortcomings, we introduce a novel formulation by reframing the active learning as a reinforcement learning problem and explicitly learning a data selection policy, where the policy takes the role of the active learning heuristic. Importantly, our method allows the selection policy learned using simulation on one language to be transferred to other languages. We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning.", "histories": [["v1", "Tue, 8 Aug 2017 07:06:48 GMT  (143kb,D)", "http://arxiv.org/abs/1708.02383v1", "To appear in EMNLP 2017"]], "COMMENTS": "To appear in EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["meng fang", "yuan li", "trevor cohn"], "accepted": true, "id": "1708.02383"}, "pdf": {"name": "1708.02383.pdf", "metadata": {"source": "CRF", "title": "Learning how to Active Learn: A Deep Reinforcement Learning Approach", "authors": ["Meng Fang", "Yuan Li"], "emails": ["meng.fang@unimelb.edu.au,", "yuanl4@student.unimelb.edu.au,", "t.cohn@unimelb.edu.au"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Related work", "text": "Since supervised learning methods often require a lot of training data, active learning is a technique that selects a subset of data recorded to train the best classifier. Existing active learning algorithms (AL) can generally be considered three categories: 1) uncertainty samples (Lewis and Gale, 1994; Tong and Koller, 2001), which select the data about which the current classifier is most uncertain; 2) query by the committee (Seung et al., 1992), which selects the data on which the \"committee\" has the most disagreements; and 3) expected error reduction (Roy and McCallum, 2001), which selects the data that can contribute to the greatest reduction in model loss for the current classifier once designated. Applications of active learning to NLP include text classification (McCallumzy and Nigamy, 1998; Tong and Koller, 2001), relational classification of languages and collages."}, {"heading": "3 Methodology", "text": "We will now show how active learning can be formalised as a decision-making process, and then show how this leads to the policy of active learning selection being learned on the basis of data through in-depth reinforcement learning. Later, we will present a method for transferring policy between languages."}, {"heading": "3.1 Active learning as a decision process", "text": "Active learning is a simple technique for labeling data, in which a few instances are first selected from an unmarked data set, which are then commented by a human oracle, which is then repeated many times until a cancellation criterion is met, e.g. the annotation budget is exhausted. Mostly, the selection function is based on the predictions of a trained model that is suitable for the labeled data set at each stage of the algorithm, where data points are selected based on the predictive uncertainty of the model (Lewis and Gale, 1994) or on variations in the predictions of an entire ensemble (Seung et al., 1992). The key idea of these methods is to find the instances on which the model is most likely to make mistakes, so that once labeled and included in the training set, the model becomes more resilient to these types of errors when the labeling is based on invisible data."}, {"heading": "3.2 Stream-based learning", "text": "For simplicity, we are assuming a streaming assumption where unlabeled data (sentences) arrive in a stream (Lewis and Gale, 1994).2 Upon arrival of each instance, an actor must decide what action to take, whether or not to manually comment on the instance. This process is illustrated in Figure 1, which illustrates the space of decision sequences for a small corpus. As part of this process, a separate model, p\u03c6, is trained on the basis of the labeled data and updated accordingly, as the labeled data set is expanded at each stage as new annotation. ar-2This differs from pool-based active learning, which selects one of several options for annotation. Our setup enables easier learning while being sufficiently universal. This model is central to selecting labeling actions at each stage and determining the reward for a sequence of actions."}, {"heading": "3.2.1 State", "text": "The state at present i comprises the candidate instance to be considered for annotation, and the designated data set constructed in steps 1 and 2. I. We represent the state by using a continuous vector, using the concatenation of the vector representation of xi, and the results of the model p\u03c6, which was formed through the designated data. I. These results use both the predictive marginal distributions of the model on the instance and a representation of the model of trust. We will now deal with each component. The key input to the agent is the content of the sentence xi, which we encode using a revolutionary neural network to arrive at a fixed vector representation following Kim (2014). This includes embedding each of the n words into the sentence to generate a matrix Xi = {xi, 1, xi, 2, xi, n}."}, {"heading": "3.2.2 Action", "text": "We now turn to the action, which indicates whether the human oracle must comment on the current sentence. The agent either selects the comment xi, in this case ai = 1, or not, with ai = 0, whereupon the agent considers the next instance, xi + 1. When the action ai = 1 is selected, an oracle is prompted to comment on the sentence, and the newly commented sentence is added to the training data and updated accordingly. A special \"termination option\" applies when no further data is left or the annotation budget that completes the active course is exhausted (hereinafter referred to as \"episode\" or \"game\")."}, {"heading": "3.2.3 Reward", "text": "The most obvious reward is to wait until a game is completed and then measure the sustained performance of the model trained on the basis of the marked data. However, this reward is delayed and difficult to relate to individual actions after a long game. To compensate for this, we use reward formulas that assign small intermediate rewards that speed up the learning process (Ng, 2003; Lample and Chaplot, 2016). At each step, the intermediate reward is defined as a change in the performance endured, i.e. R (si \u2212 1, a) = Acc (inspi \u2212 1) = Acc (inspi \u2212 1), where Acc stands for predictive accuracy (F1 score here), and Higi is the trained model after an action has taken place, which may involve an additional training case. Accordingly, when considering the overall effect over a game, a reward may be one that cancels out the total performance over the total performance (F1 score here)."}, {"heading": "3.2.4 Budget", "text": "There is a fixed budget B for the total number of commented instances that corresponds to the final state in the MDP. It is a predefined number that is selected according to time and cost constraints. A game ends when the data is exhausted or the budget is reached, and the end result is the data set thus created on which the final model is trained."}, {"heading": "3.2.5 Reinforcement learning", "text": "The remaining question is how the above mentioned components can be used to learn a good policy. Different strategies make different data selections Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q ="}, {"heading": "3.3 Cross-lingual policy transfer", "text": "GivenAlgorithm 3 Active learning through policy and model transfer, for \"cold start\" scenario Input: unlabeled data D, budget B, policy \u03c0, model \u03c6 Output: Dl1: Dl \u2190 \u2205 2: for | Dl | 6 = B and D not empty do 3: Random sample xi from the data pool D and construct the state si 4: The agent selects an action ai according to toai = argmaxQ (si, a) 5: if ai = 1 then 6: Dl + (xi, \u2212) 7: End, if 8: D \u2190 D\\ xi 9: End for10: Return all annotations for Dl 11: return the extensive use of the training data set, the application of the policy only makes sense if it is used in another data setting, e.g. if the domain, task or language is different. For this paper we consider a linguistic application of the word set."}, {"heading": "3.4 Cold-start transfer", "text": "The above-mentioned transfer algorithm has some limitations that may not be realistic for low-resource settings: the requirement for endured evaluation data and the embedding of the oracle annotator in-3In addition, the algorithm can be extended to a traditional batch setting by evaluating a batch of data instances and selecting the best k instances for policy labeling, either in the transfer step (algorithm 2) or in the initial policy training (algorithm 1) or on both sides of the learning loop. The former implies more monitoring than is ideal in a low-resource setting, while the latter imposes limitations on communication with the annotator and a need for real-time processing, both of which are unlikely in a linguistic environment. For this data and low-communication setting, which is called a cold start, we only allow a chance to request labels for the target data."}, {"heading": "4 Experiments", "text": "We conduct experiments to validate the proposed active learning method in a multilingual environment, with an active learning policy based on a source language that we use for a multilingual learning method, while allowing a single episode to mimic a language without available resources. We use the different learning methods that we use for the IO Label-4 / 2003 to describe the different learning methods in English (en), German (de), Spanish (es) and Dutch (nl), each individual learning method using the IOB1 labeling scheme that we use on the IO Label-4 / / www.cnts.be / conll2002 / ner /, http: / www.cnts.be / conll2003 / ner / ing scheme."}, {"heading": "5 Conclusion", "text": "In this paper, we have proposed a new active learning algorithm that is capable of learning active learning strategies from data. We formalize active learning under a Markov decision framework, where active learning corresponds to a sequence of binary annotation decisions applied to a data stream. Building on this, we design an active learning algorithm as a policy based on deep reinforcement learning. We show how these learned active learning strategies can be transferred between languages, empirically demonstrating that they provide consistent and significant improvements over basic methods, including traditional uncertainty samples, even in a very difficult cold start situation where no evaluation data is available and there is no ability to respond to comments."}, {"heading": "Acknowledgments", "text": "This work was funded by the Defense Advanced Research Projects Agency Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program, issued by DARPA / I2O under contract number HR0011-15-C-0114. Views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the US government. Trevor Cohn was supported by an Australian Research Council Future Fellowship."}], "references": [{"title": "Massively multilingual word embeddings", "author": ["Waleed Ammar", "George Mulcaire", "Yulia Tsvetkov", "Guillaume Lample", "Chris Dyer", "Noah A Smith."], "venue": "arXiv preprint arXiv:1602.01925 .", "citeRegEx": "Ammar et al\\.,? 2016", "shortCiteRegEx": "Ammar et al\\.", "year": 2016}, {"title": "Multiple object recognition with visual attention", "author": ["Jimmy Ba", "Volodymyr Mnih", "Koray Kavukcuoglu."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Ba et al\\.,? 2015", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Yoshua Bengio."], "venue": "Proceedings of ICML Workshop on Unsupervised and Transfer Learning. pages 17\u201336.", "citeRegEx": "Bengio.,? 2012", "shortCiteRegEx": "Bengio.", "year": 2012}, {"title": "Model transfer for tagging low-resource languages using a bilingual dictionary", "author": ["Meng Fang", "Trevor Cohn."], "venue": "Proceedings of the 55th Annual Meeting on Association for Computational Linguistics (ACL).", "citeRegEx": "Fang and Cohn.,? 2017", "shortCiteRegEx": "Fang and Cohn.", "year": 2017}, {"title": "Learning a part-of-speech tagger from two hours of annotation", "author": ["Dan Garrette", "Jason Baldridge."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Garrette and Baldridge.,? 2013", "shortCiteRegEx": "Garrette and Baldridge.", "year": 2013}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods on Natural Language Processing (EMNLP).", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning (ICML).", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Playing FPS games with deep reinforcement learning", "author": ["Guillaume Lample", "Devendra Singh Chaplot."], "venue": "arXiv preprint arXiv:1609.05521 .", "citeRegEx": "Lample and Chaplot.,? 2016", "shortCiteRegEx": "Lample and Chaplot.", "year": 2016}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel."], "venue": "Journal of Machine Learning Research 17(39):1\u201340.", "citeRegEx": "Levine et al\\.,? 2016", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "A sequential algorithm for training text classifiers", "author": ["David D Lewis", "William A Gale."], "venue": "Proceedings of the 17th International ACM SIGIR Conference on Research and Development in Information Retrieval. pages 3\u201312.", "citeRegEx": "Lewis and Gale.,? 1994", "shortCiteRegEx": "Lewis and Gale.", "year": 1994}, {"title": "Move evaluation in go using deep convolutional neural networks", "author": ["Chris J Maddison", "Aja Huang", "Ilya Sutskever", "David Silver."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Maddison et al\\.,? 2015", "shortCiteRegEx": "Maddison et al\\.", "year": 2015}, {"title": "Employing em and pool-based active learning for text classification", "author": ["Andrew Kachites McCallumzy", "Kamal Nigamy."], "venue": "Proceedings of the 15th International Conference on Machine Learning (ICML). pages 359\u2013367.", "citeRegEx": "McCallumzy and Nigamy.,? 1998", "shortCiteRegEx": "McCallumzy and Nigamy.", "year": 1998}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["Silver."], "venue": "Proceedings of ICML Workshop on Deep Learning.", "citeRegEx": "Silver.,? 2015", "shortCiteRegEx": "Silver.", "year": 2015}, {"title": "Improving information extraction by acquiring external evidence with reinforcement learning", "author": ["Karthik Narasimhan", "Adam Yala", "Regina Barzilay."], "venue": "Proceedings of the 2016 Conference on Empirical Methods on Natural Language Processing", "citeRegEx": "Narasimhan et al\\.,? 2016", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2016}, {"title": "Shaping and Policy Search in Reinforcement Learning", "author": ["Andrew Y. Ng."], "venue": "Ph.D. thesis, University of California, Berkeley.", "citeRegEx": "Ng.,? 2003", "shortCiteRegEx": "Ng.", "year": 2003}, {"title": "Actor-mimic: Deep multitask and transfer reinforcement learning", "author": ["Emilio Parisotto", "Jimmy Lei Ba", "Ruslan Salakhutdinov."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Parisotto et al\\.,? 2016", "shortCiteRegEx": "Parisotto et al\\.", "year": 2016}, {"title": "Bilingual active learning for relation classification via pseudo parallel corpora", "author": ["Longhua Qian", "Haotian Hui", "Yanan Hu", "Guodong Zhou", "Qiaoming Zhu."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Qian et al\\.,? 2014", "shortCiteRegEx": "Qian et al\\.", "year": 2014}, {"title": "Toward optimal active learning through monte carlo estimation of error reduction", "author": ["Nicholas Roy", "Andrew McCallum."], "venue": "Proceedings of the 18th International Conference on Machine Learning (ICML). pages 441\u2013448.", "citeRegEx": "Roy and McCallum.,? 2001", "shortCiteRegEx": "Roy and McCallum.", "year": 2001}, {"title": "Progressive neural networks", "author": ["Andrei A Rusu", "Neil C Rabinowitz", "Guillaume Desjardins", "Hubert Soyer", "James Kirkpatrick", "Koray Kavukcuoglu", "Razvan Pascanu", "Raia Hadsell."], "venue": "arXiv preprint arXiv:1606.04671 .", "citeRegEx": "Rusu et al\\.,? 2016", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Active learning literature survey", "author": ["Burr Settles."], "venue": "University of Wisconsin, Madison 52(55-66):11.", "citeRegEx": "Settles.,? 2010", "shortCiteRegEx": "Settles.", "year": 2010}, {"title": "An analysis of active learning strategies for sequence labeling tasks", "author": ["Burr Settles", "Mark Craven."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 1070\u20131079.", "citeRegEx": "Settles and Craven.,? 2008", "shortCiteRegEx": "Settles and Craven.", "year": 2008}, {"title": "Query by committee", "author": ["H Sebastian Seung", "Manfred Opper", "Haim Sompolinsky."], "venue": "Proceedings of the 5th annual workshop on Computational Learning Theory. pages 287\u2013294.", "citeRegEx": "Seung et al\\.,? 1992", "shortCiteRegEx": "Seung et al\\.", "year": 1992}, {"title": "Multi-criteria-based active learning for named entity recognition", "author": ["Dan Shen", "Jie Zhang", "Jian Su", "Guodong Zhou", "Chew-Lim Tan."], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics (ACL).", "citeRegEx": "Shen et al\\.,? 2004", "shortCiteRegEx": "Shen et al\\.", "year": 2004}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Simple semisupervised pos tagging", "author": ["Karl Stratos", "Michael Collins."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the", "citeRegEx": "Stratos and Collins.,? 2015", "shortCiteRegEx": "Stratos and Collins.", "year": 2015}, {"title": "Active learning for natu", "author": ["mond J Mooney"], "venue": null, "citeRegEx": "Mooney.,? \\Q1999\\E", "shortCiteRegEx": "Mooney.", "year": 1999}], "referenceMentions": [{"referenceID": 21, "context": "Thus active learning has been applied to NLP tasks to minimise the expense of annotating data (Thompson et al., 1999; Tong and Koller, 2001; Settles and Craven, 2008).", "startOffset": 94, "endOffset": 166}, {"referenceID": 20, "context": "Active learning aims to reduce cost by identifying a subset of unlabelled data for annotation, which is selected to maximise the accuracy of a supervised model trained on the data (Settles, 2010).", "startOffset": 180, "endOffset": 195}, {"referenceID": 4, "context": "It is no doubt that active learning is extremely important for other languages, particularly lowresource languages, where annotation is typically difficult to obtain, and annotation budgets more modest (Garrette and Baldridge, 2013).", "startOffset": 202, "endOffset": 232}, {"referenceID": 12, "context": "An intelligent agent must decide whether or not to select data for annotation in a streaming setting, where the decision policy is learned using a deep Q-network (Mnih et al., 2015).", "startOffset": 162, "endOffset": 181}, {"referenceID": 13, "context": "For most Natural Language Processing (NLP) tasks, obtaining sufficient annotated text for training accurate models is a critical bottleneck. Thus active learning has been applied to NLP tasks to minimise the expense of annotating data (Thompson et al., 1999; Tong and Koller, 2001; Settles and Craven, 2008). Active learning aims to reduce cost by identifying a subset of unlabelled data for annotation, which is selected to maximise the accuracy of a supervised model trained on the data (Settles, 2010). There have been many successful applications to NLP, e.g., Tomanek et al. (2007) used an active learning algorithm for CoNLL corpus to get an F1 score 84% with a reduction of annotation cost of about 48%.", "startOffset": 19, "endOffset": 587}, {"referenceID": 21, "context": "Another main difference is that many active learning algorithms use a fixed data selection heuristic, such as uncertainty sampling (Settles and Craven, 2008; Stratos and Collins, 2015; Zhang et al., 2016).", "startOffset": 131, "endOffset": 204}, {"referenceID": 25, "context": "Another main difference is that many active learning algorithms use a fixed data selection heuristic, such as uncertainty sampling (Settles and Craven, 2008; Stratos and Collins, 2015; Zhang et al., 2016).", "startOffset": 131, "endOffset": 204}, {"referenceID": 9, "context": "Existing active learning (AL) algorithms can be generally considered as three categories: 1) uncertainty sampling (Lewis and Gale, 1994; Tong and Koller, 2001), which selects the data about which the current classifier is the most uncertain; 2) query by committee (Seung et al.", "startOffset": 114, "endOffset": 159}, {"referenceID": 22, "context": "Existing active learning (AL) algorithms can be generally considered as three categories: 1) uncertainty sampling (Lewis and Gale, 1994; Tong and Koller, 2001), which selects the data about which the current classifier is the most uncertain; 2) query by committee (Seung et al., 1992), which selects the data about which the \u201ccommittee\u201d disagree most; and 3) expected error reduction (Roy and McCallum, 2001), which selects the data that can contribute the largest model loss reduction for the current classifier once labelled.", "startOffset": 264, "endOffset": 284}, {"referenceID": 18, "context": ", 1992), which selects the data about which the \u201ccommittee\u201d disagree most; and 3) expected error reduction (Roy and McCallum, 2001), which selects the data that can contribute the largest model loss reduction for the current classifier once labelled.", "startOffset": 107, "endOffset": 131}, {"referenceID": 11, "context": "Applications of active learning to NLP include text classification (McCallumzy and Nigamy, 1998; Tong and Koller, 2001), relation classification (Qian et al.", "startOffset": 67, "endOffset": 119}, {"referenceID": 17, "context": "Applications of active learning to NLP include text classification (McCallumzy and Nigamy, 1998; Tong and Koller, 2001), relation classification (Qian et al., 2014), and structured prediction (Shen et al.", "startOffset": 145, "endOffset": 164}, {"referenceID": 23, "context": ", 2014), and structured prediction (Shen et al., 2004; Settles and Craven, 2008; Stratos and Collins, 2015; Fang and Cohn, 2017).", "startOffset": 35, "endOffset": 128}, {"referenceID": 21, "context": ", 2014), and structured prediction (Shen et al., 2004; Settles and Craven, 2008; Stratos and Collins, 2015; Fang and Cohn, 2017).", "startOffset": 35, "endOffset": 128}, {"referenceID": 25, "context": ", 2014), and structured prediction (Shen et al., 2004; Settles and Craven, 2008; Stratos and Collins, 2015; Fang and Cohn, 2017).", "startOffset": 35, "endOffset": 128}, {"referenceID": 3, "context": ", 2014), and structured prediction (Shen et al., 2004; Settles and Craven, 2008; Stratos and Collins, 2015; Fang and Cohn, 2017).", "startOffset": 35, "endOffset": 128}, {"referenceID": 12, "context": "Recently, there are some notable examples include deep Qlearning (Mnih et al., 2015), deep visuomotor policies (Levine et al.", "startOffset": 65, "endOffset": 84}, {"referenceID": 8, "context": ", 2015), deep visuomotor policies (Levine et al., 2016), attention with recurrent networks (Ba et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 1, "context": ", 2016), attention with recurrent networks (Ba et al., 2015), and model predictive control with embeddings (Watter et al.", "startOffset": 43, "endOffset": 60}, {"referenceID": 10, "context": ", 2016) and expert move prediction in the game of Go (Maddison et al., 2015), which produced policies matching those of the Monte Carlo tree search programs, and squarely beaten a professional player when combined with search (Silver et al.", "startOffset": 53, "endOffset": 76}, {"referenceID": 24, "context": ", 2015), which produced policies matching those of the Monte Carlo tree search programs, and squarely beaten a professional player when combined with search (Silver et al., 2016).", "startOffset": 157, "endOffset": 178}, {"referenceID": 14, "context": "For example, recently, DRL has been studied for information extraction problem (Narasimhan et al., 2016).", "startOffset": 79, "endOffset": 104}, {"referenceID": 2, "context": "Recent deep learning work has also looked at transfer learning (Bengio, 2012).", "startOffset": 63, "endOffset": 77}, {"referenceID": 16, "context": "More recent work in deep learning has also considered transferring policies by reusing policy parameters between environments (Parisotto et al., 2016; Rusu et al., 2016), using either regularization or novel neural network architectures, though this work has not looked at transfer active learning strategies between languages with shared feature space in state.", "startOffset": 126, "endOffset": 169}, {"referenceID": 19, "context": "More recent work in deep learning has also considered transferring policies by reusing policy parameters between environments (Parisotto et al., 2016; Rusu et al., 2016), using either regularization or novel neural network architectures, though this work has not looked at transfer active learning strategies between languages with shared feature space in state.", "startOffset": 126, "endOffset": 169}, {"referenceID": 9, "context": "dictions of a trained model, which has been fit to the labelled dataset at each stage in the algorithm, where datapoints are selected based on the model\u2019s predictive uncertainty (Lewis and Gale, 1994), or divergence in predictions over an ensemble (Seung et al.", "startOffset": 178, "endOffset": 200}, {"referenceID": 22, "context": "dictions of a trained model, which has been fit to the labelled dataset at each stage in the algorithm, where datapoints are selected based on the model\u2019s predictive uncertainty (Lewis and Gale, 1994), or divergence in predictions over an ensemble (Seung et al., 1992).", "startOffset": 248, "endOffset": 268}, {"referenceID": 9, "context": "For simplicity, we make a streaming assumption, whereby unlabelled data (sentences) arrive in a stream (Lewis and Gale, 1994).", "startOffset": 103, "endOffset": 125}, {"referenceID": 5, "context": "Content representation A key input to the agent is the content of the sentence, xi, which we encode using a convolutional neural network to arrive at a fixed sized vector representation, following Kim (2014). This involves embedding each of the n words in the sentence to produce a matrix", "startOffset": 197, "endOffset": 208}, {"referenceID": 15, "context": "To compensate for this, we use reward shaping, whereby small intermediate rewards are assigned which speeds up the learning process (Ng, 2003; Lample and Chaplot, 2016).", "startOffset": 132, "endOffset": 168}, {"referenceID": 7, "context": "To compensate for this, we use reward shaping, whereby small intermediate rewards are assigned which speeds up the learning process (Ng, 2003; Lample and Chaplot, 2016).", "startOffset": 132, "endOffset": 168}, {"referenceID": 12, "context": "We use a deep Q-learning approach (Mnih et al., 2015), which formalises the policy using function Q\u03c0(s, a)\u2192 Rwhich determines the utility of taking a from state s according to a policy \u03c0.", "startOffset": 34, "endOffset": 53}, {"referenceID": 12, "context": "Following Deep Q-learning (Mnih et al., 2015), we make use of a deep neural network to compute the expected Q-value, in order to update the parameters.", "startOffset": 26, "endOffset": 45}, {"referenceID": 12, "context": "Following (Mnih et al., 2015), we use an experi-", "startOffset": 10, "endOffset": 29}, {"referenceID": 0, "context": "For word embeddings, we use off the shelf CCA trained multilingual embeddings (Ammar et al., 2016),6 using a 40 dimensional embedding and fixing these during training of both the policy and model.", "startOffset": 78, "endOffset": 98}, {"referenceID": 6, "context": "As the model, we use a standard linear chain CRF (Lafferty et al., 2001) for the first two sets of experiments, while for cold-start case we use a basic RNN classifier with the same multilingual embeddings as before, and a 128 dimensional hidden layer.", "startOffset": 49, "endOffset": 72}, {"referenceID": 21, "context": "Uncertainty sampling we use the total token entropy measure (Settles and Craven, 2008), which takes the instance x maximising \u2211|x| t=1H(yt|x, \u03c6), where H is the token entropy.", "startOffset": 60, "endOffset": 86}], "year": 2017, "abstractText": "Active learning aims to select a small subset of data for annotation such that a classifier learned on the data is highly accurate. This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of heuristics varies between datasets. To address these shortcomings, we introduce a novel formulation by reframing the active learning as a reinforcement learning problem and explicitly learning a data selection policy, where the policy takes the role of the active learning heuristic. Importantly, our method allows the selection policy learned using simulation on one language to be transferred to other languages. We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning.", "creator": "LaTeX with hyperref package"}}}