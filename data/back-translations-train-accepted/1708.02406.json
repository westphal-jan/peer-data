{"id": "1708.02406", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Aug-2017", "title": "Robust Conditional Probabilities", "abstract": "Conditional probabilities are a core concept in machine learning. For example, optimal prediction of a label $Y$ given an input $X$ corresponds to maximizing the conditional probability of $Y$ given $X$. A common approach to inference tasks is learning a model of conditional probabilities. However, these models are often based on strong assumptions (e.g., log-linear models), and hence their estimate of conditional probabilities is not robust and is highly dependent on the validity of their assumptions.", "histories": [["v1", "Tue, 8 Aug 2017 08:42:09 GMT  (147kb,D)", "http://arxiv.org/abs/1708.02406v1", "24 pages, 1 figure"]], "COMMENTS": "24 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yoav wald", "amir globerson"], "accepted": true, "id": "1708.02406"}, "pdf": {"name": "1708.02406.pdf", "metadata": {"source": "CRF", "title": "Robust Conditional Probabilities", "authors": ["Yoav Wald", "Amir Globerson"], "emails": ["yoav.wald@mail.huji.ac.il", "gamir@post.tau.ac.il"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most people are able to keep to the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to keep to the rules. (...) It is not that they keep to the rules. (...) It is as if they keep to the rules. (...) It is as if they keep to the rules. (...) It is as if they keep to the rules. (...) It is as if they keep to the rules. (...) It is as if they keep to the rules. (...) It is as if they keep to the rules. (...) It is as if they keep to the rules. (...)"}, {"heading": "2 Problem Setup", "text": "Suppose we have n characteristics denoted by random variables X1...., Xn. If we have a single label, we will denote it by Y. Otherwise, a multivariate label is denoted by Y1,.., Yr. We assume that all variables are discrete (i.e., we can assume a finite set of values). Suppose that X, Y are generated by an unknown underlying distribution p (X, Y). Here, we assume that, although we do not know the expected value of any vector function f: X, Y \u2192 Rd under p. 23, we assume that we get a vector defined by an = Ep distribution p (f, Y). Since an x label is not uniquely set to x, we will be interested in the set of all distributions."}, {"heading": "2.1 The Robust Conditionals Problem", "text": "Our approach is to think about conditional distributions, using only the fact that p * and max * P (\u00b5) problems are linear. Our main goal is to reduce these conditions, as we can conclude that certain terms are very likely in cases where the lower limit is large. We will also be interested in upper and lower limit probabilities, as these will play a key role in limiting the conditions. Therefore, our goal is to solve the following optimization problems. Note, however, that p (\u00b5) p (x, y), max p (\u00b5) p (x, y), min p (\u00b5) p (\u00b5) p (p) p (y | x). (2) For all three problems, the constraint is linear in p. Note, however, that p is specified by an exponential number of variables (one per assignment x1,... xn), and therefore it is not possible to put these boundaries together into one LP solver."}, {"heading": "3 Related Work", "text": "An early example is the classic Chebyshev inequality, which limits the tail of a distribution in view of its first and second moments. This was significantly extended in the Chebyshev Markov Stieltje's inequality [2]. More recently, various generalized Chebyshev inequalities have been developed [3, 24, 29]. A typical statement is that there are several moments, and one seeks the minimum of a distribution that corresponds to the moments. As [3] noted, most of these problems are hard, with isolated cases of traceability. Such inequalities have been used to obtain minimized linear classifiers."}, {"heading": "4 Calculating Robust Conditional Probabilities", "text": "The optimization problems in Equation (2) are linear programs (LP) and fractional LPs, where the number of variables scales exponentially with n. However, as we show in this section and Section 5, they can be efficiently solved in many non-trivial cases. Our focus below is on the case where the paired margins correspond to a set E that forms a tree-structured graph. In the following sections, we will examine solutions of robust conditional probabilities under the tree, only here we will not make an inductive assumption about generative distribution (i.e., we will not make any of the conditional assumptions of independence implied by tree-structured graphical models). We will also discuss some extensions to the cyclical case. Finally, although the derivatives here apply to paired margins, they can be extended to the non-pair problem by looking at this problem more climatically."}, {"heading": "4.1 From Conditional Probabilities To Maximum Probabilities with Exclusion", "text": "The main result of this section will reduce the calculation of the robust conditional probability for p (y | x) to one of maximizing the probability of all terms except y. This reduction alone will not allow an efficient calculation of the desired conditional probabilities, since the new problem is also a large LP that needs to be solved. Nevertheless, the result will take us one step closer to a solution, since it reveals the probability mass that will assign a minimizing distribution p x. This part of the solution is related to a result from [10], in which the authors derive the solution from minp (\u00b5) p (x, y), proving that this problem has a simple closed form solution under the construction assumption given by the functional I (x, y; \u00b5) character: I (x, y; \u00b5) = Reposition i (1 \u2212 di) \u00b5i (xi, y) +."}, {"heading": "4.2 Minimizing and Maximizing Probabilities", "text": "To find an efficient solution to Eq (5), we turn to a class of common probability problems (5). Suppose we limit each variable X and Yj variable to a subset X-i, Y-j of their domain and want to think about the probability of this limited set of common mappings: U = {x, y) p (u) p (u) p [n] p [n] p [n] p [n] p [n] p [n] p [n] p [s] p [s] p [x] p (u) p (u) p (u) p [s) p) p [s) p [n] p [n] p) p [n] p [n] p [n] p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \".\""}, {"heading": "5 Closed Form Solutions and Combinatorial Algorithms", "text": "The results of the previous section suggest that the minimum condition can be found by solving a poly-size LP. Although this results in a polynomic runtime, it is interesting to improve the complexity of this calculation as much as possible. One reason for this is that applying the limits may require a repeated solution within some larger learning sample. For example, in classification tasks it may be necessary to solve Equation (4) for each sample in the dataset. An even more sophisticated procedure will emerge in our experimental evaluation, where we will learn characteristics that lead to a high level of confidence within our boundaries, where we will have to solve Equation (4) via mini-stacks of training data just to calculate a gradient for each training teration. As the use of an LP solver in these scenarios is impracticable, we next derive more efficient solutions for some specific cases of Equation (4)."}, {"heading": "5.1 Closed Form for Multiclass Problems", "text": "Multi-class configuration is a special case of Equation (4) if y is a single label variable (e.g. a digit name in mnist with values y {0,.., 9}). In this case, the problem in Equation (2) is: minp, P (\u00b5) p (y | x). The solution, of course, depends on the type of marginals provided in P (\u00b5). In this case, we assume that we have access to common marginals of the label y and pairs of characteristics xi, xj that correspond to the edges ij, E of a diagram G. We find that we can achieve similar results in cases where some additional \"unlabeled\" statistics \u00b5ij (xi, xj) are known. 4We leave the labels Y1,.., Yr out of this definition for notable convenience. Formally, consistency constraint is also enforced for edges with nodes that correspond to labels."}, {"heading": "5.2 Combinatorial Algorithms and Connection to Maximum Flow Problems", "text": "In some cases, fast algorithms for the optimization problem in Eq. (5) q can be derived by exploiting a close connection of our problems with the max flow problem. The problems are also closely related to the weighted set cover problem. To observe the connection to the latter, we consider an instance of set covers defined as follows. The universe includes all mappings x. Sets are defined for each i, j, xi, xj and are denoted by Sij, xi, xj. The set Sij, xi, xj contains all mappings x, whose values are in i, j, xj, xj, xj etc. Moreoever, the set Sij, xi, xj has weight w (Sij, xj) = \u00b5ij (xi, xj). Note that the number of items in sets is exponential, but there is a polynomial amount of sets. Now, let's assume that we want to cover these minimally with some sets."}, {"heading": "6 Experiments", "text": "In order to assess the usefulness of our limits, we consider their use in the settings of semi-monitored deep learning and structured prediction. In order for the limits to be useful, the marginal distributions must be sufficiently informative. In some data sets, the raw data already provide such information as we show in Section 6.3. In other cases, such as images, a single raw feature (i.e. one pixel) does not provide sufficient information about the label. These cases are discussed in Section 6.1, where we show how to learn new features that lead to meaningful limits. Using deep networks to learn these features proves to be an effective method for semi-monitored settings to achieve results that are close to those of Variational Autoencoders [14]. It would be interesting to use such learning methods for structured predictions as well, but this requires the incorporation of the Max Flow Algorithm into the optimization loop, and we are postponing this to future work."}, {"heading": "6.1 Deep Semi-Supervised Learning", "text": "We learn a neural network whose last layer is understood as the characteristics of the problem. The marginals of these values in the image are not used discreetly, as they are an output of a neural network. However, we will use a sigmoid activation for the last layer, so that the Zi values are between 0 and 1. For now we consider the Zi as actual discrete variables with values {0, 1}, and we will explain later how we can overcome their non-discrete values. Faced with an input x, we can calculate properties, and then calculate a set of values for p (y | z) for each value of y."}, {"heading": "6.2 MNIST Dataset", "text": "We trained the models described above on the basis of the MNIST dataset and used 100 and 1000 marked samples (see [14] for a similar setup). We used the two regulation parameters required for the entropy regularizer and the one required for our minimum probability regularizer with fivefold cross-validation. We used 10% of the training data as a validation set and compared error rates on the 10,000 samples of the test set. Results are shown in Figure 1. They show that in the 1000 example of VAE we are easily surpassed and lose 1% on 100 samples. Conductor networks outperform the other baselines. Accuracy vs. Coverage Curves: In self-training and co-training methods, a classifier adds his most reliable predictions of the training set and then repeats the training. A key factor for the success of such methods is the error in the pre-characterized data, rather than the confidence used by the classifier."}, {"heading": "6.3 Multilabel Structured Prediction", "text": "As already mentioned, it is more difficult in the structured prediction setting to learn characteristics that provide a high degree of certainty. Therefore, we offer a demonstration of our method on a dataset in which the raw characteristics are relatively informative. The Genbase dataset taken from [28] is a multi-label dataset for protein classification. It comprises 662 cases, divided into a training set of 463 samples and a test set of 199, each sample having 1185 binary characteristics and 27 binary identifiers. We used a structured SVM algorithm taken from [21] to obtain a classifier that returns a label y for each x in the dataset (the error of the resulting classifier was 2%). Subsequently, we used our probability limits to evaluate the predictions of the classifier based on their robust conditional probabilities. The limits were based on the set of limits \u00b5j (xi, j), the data were incorrectly calculated from each of these characteristics, and Xi was estimated from each of the 85%."}, {"heading": "7 Discussion", "text": "We have presented a method for limiting conditional probabilities of a distribution based only on knowledge of its low-order marginals. Our results can be considered a novel moment problem limiting a key component of the machine learning system, namely conditional distribution. As we show, the calculation of these limits raises many challenging optimization questions, which in some cases surprisingly result in closed form expressions. While the results were limited to the tree-structured case, some of the methods have natural extensions of the cyclic case that still lead to robust estimates. For example, the local boundary pole in Equation (7) can be taken over a cyclic structure and still result in a lower limit for maximum probabilities. Even in the presence of the cycles, it is possible to find the surrounding tree that induces the best bound onequation. (3) With the help of a maximally spanning tree algorithm in Equation (7), these solutions can be incorporated into equation and hence, a closer approximation can be used in our experiment."}, {"heading": "A Proof of Lem. 5.1", "text": "Let's start with the proof of Lem. 5.1, in which we derive the form of the solutions used in our experiments. (Proof. we begin by writing down the problem in the following way: min p (p) p (x, y) p (x, y) p (x, y) p (x, y) p (x, y) p (x, y) p (x, y) p (x, y) p (x, y) p (x, y), the lower target we get. We now notice that each of the assignments can be maximized or minimized because they occur in completely different constraints in P (p). This is true because all constraints in P (x, y) are in the form: zi, zj = xi, y (z), y (z) y (z) x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (p) p (x) p (x) p) p (x) p (x) p (x) p (x), y) p (x)."}, {"heading": "B Notations for Remainder of the Proofs", "text": "For more convenient notation, we will henceforth treat labels as hidden variables. That is, instead of n attributes and r markups, we will assume that there are only n variables X1,..., Xn. The first m are hidden (these will play the role of a label) and the last n \u2212 m will be observed, with m between 0 and n \u2212 1. For an assignment x, we will refer to the hidden part as xh and the observed variables as xo. The division into hidden and observed variables will mainly serve to prove Thm. 4.1, in other proofs it is easier not to refer the expressions to x, y.We will also refer to the subvector of xi via hidden variables and edges between them as \u00b5h. That is, taking into account the item effects of \u00b5, the properties of i (zi) that we have indexed are."}, {"heading": "C Proof of Lem. 5.2", "text": "We start by proving the connection to set covers and then proceed to Max-Flow.C.1 Connection to Set-CoverProof. Let's write down the dual of Eq. (9): min \u03bb \u00b7 \u00b5 (14) s.t. (xr). (xpa). (i). (xi). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i).)... (i)...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "D Proof of Thm. 4.2", "text": "The theorem re-formulates the following problems: max p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p). (18)."}, {"heading": "E Proof of Thm. 4.1", "text": "We remember the present problem of minimizing conditional probabilities: min p-P (\u00b5) p (xh) p (xo) p (max.) p (max.) p (max.) p (max.) p (max.) p (max.) p (max.) p (max.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p (.) p. (.) p. (.) p. (.) p. (.) p. (.) p. (.). (.) p. (.) p. (.) p. (.). (.) p. (.) p.). (.) p. (.). (.) p. (.) p.) p. (.) p. (.) p.) p. (.) p. (.) p. (.) p. (.) p.) p. (.) p. (.) p. (.). (.) p. (.).) p.). (. (.).). (. (. (.) p.).). (. (.). (.). (. (.). (.). (. (.).) p.). (. (.) p.).). (.).). (. (.). (.). (. (.). (.) p.). (. (.). (.). (.) p.). (.)..). (.) p.). (.)..). (.). (.). (...).). (..).). (...).). (."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Conditional probabilities are a core concept in machine learning. For ex-<lb>ample, optimal prediction of a label Y given an inputX corresponds to maximizing<lb>the conditional probability of Y given X . A common approach to inference tasks<lb>is learning a model of conditional probabilities. However, these models are often<lb>based on strong assumptions (e.g., log-linear models), and hence their estimate of<lb>conditional probabilities is not robust and is highly dependent on the validity of<lb>their assumptions.<lb>Here we propose a framework for reasoning about conditional probabilities without<lb>assuming anything about the underlying distributions, except knowledge of their<lb>second order marginals, which can be estimated from data. We show how this<lb>setting leads to guaranteed bounds on conditional probabilities, which can be calcu-<lb>lated efficiently in a variety of settings, including structured-prediction. Finally, we<lb>apply them to semi-supervised deep learning, obtaining results competitive with<lb>variational autoencoders.", "creator": "LaTeX with hyperref package"}}}