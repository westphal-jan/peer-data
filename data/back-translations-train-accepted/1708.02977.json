{"id": "1708.02977", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Aug-2017", "title": "Hierarchically-Attentive RNN for Album Summarization and Storytelling", "abstract": "We address the problem of end-to-end visual storytelling. Given a photo album, our model first selects the most representative (summary) photos, and then composes a natural language story for the album. For this task, we make use of the Visual Storytelling dataset and a model composed of three hierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album photos, select representative (summary) photos, and compose the story. Automatic and human evaluations show our model achieves better performance on selection, generation, and retrieval than baselines.", "histories": [["v1", "Wed, 9 Aug 2017 19:26:47 GMT  (6201kb,D)", "http://arxiv.org/abs/1708.02977v1", "To appear at EMNLP-2017 (7 pages)"]], "COMMENTS": "To appear at EMNLP-2017 (7 pages)", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV cs.LG", "authors": ["licheng yu", "mohit bansal", "tamara l berg"], "accepted": true, "id": "1708.02977"}, "pdf": {"name": "1708.02977.pdf", "metadata": {"source": "CRF", "title": "Hierarchically-Attentive RNN for Album Summarization and Storytelling", "authors": ["Licheng Yu", "Mohit Bansal", "Tamara L. Berg"], "emails": ["tlberg}@cs.unc.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people are able to decide whether they will be able to play by the rules or whether they will be able to play by the rules."}, {"heading": "2 Related work", "text": "Visual Captioning: Most recent approach to image captioning (Vinyals et al., 2015b; Xu et al., 2015), sequence-to-sequence models are commonly incorporated for both tasks to localize salient temporal or spatial information. Video Summarization: Similar to documentation summarization (Xu et al., 2015; Yu et al., 2016; Yao et al., 2015). Attention techniques (Xu et al., 2015; Yu et al., 2016; Yao et al., 2015) are commonly incorporated for both tasks to localize salient temporal or spatial information. Video Summarization: Similar to documentation summarization (Rush et al., 2015; Cheng and Lapata, 2016; Mei et al., Lapata, 2010)."}, {"heading": "3 Model", "text": "Our model (fig. 1) consists of three modules: album encoder, photo selector and story generator, which were learned together during the training."}, {"heading": "3.1 Album Encoder", "text": "Faced with an album A = {a1, a2,..., an} that consists of a series of photos, we use a bidirectional RNN to encode the local album context for each photo. First, we extract the 2048-dimensional visual representation fi-Rk for each photo with ResNet101 (He et al., 2016), then a bidirectional RNN is applied to encode the entire album. Following (Huang et al., 2016), we select a gated recurrent unit (GRU) as an RNN unit to encode the photo sequence. the sequence output at any time encodes the local album context for each photo (from both directions). Merged with the visual representation followed by ReLU, we obtain our final photo representation (upper module in fig. 1): fi = ResNet (ai) ~ hi = ~ GRUalbum (fi, ~ hi \u2212 1) ~ GRUalbum (album, + Refi, + 1) Lvi (hi)."}, {"heading": "3.2 Photo Selector", "text": "The photo selector (shown in the middle yellow part of fig. 1) identifies representative photos to summarize the contents of an album. As discussed, we do not assume that we will receive the album summaries during the training, but regard selection as a latent variable in end-to-end learning. Inspired by Pointer Networks (Vinyals et al., 2015a), we use another GRU-RNN to accomplish this task. 1. Given the album representation V n \u00d7 k, the photo selector gives probabilities pt-Rn (probability of selection as t-summary image) for all photos by means of soft attention."}, {"heading": "3.3 Story Generator", "text": "To generate the history of an album, we calculate probabilities pt = from the first two modules, given the album representation matrix V and the photo summary, the maximum summary representation gt-Rk (for the t-th summary step), which is a weighted sum of album representations, i.e. gt = pTt V. Each of these 5 gt embeddings (for t = 1 to 5) is then used to decipher 1 of each of the 5 story sentences, as in the blue part of Fig. 1. Given a story S = {st}, where st is the t-th summary sentence. Following Donahue et al. (2015), the l-th literal probability of the t-th sentence is: wt, l -1 = west, l \u2212 1, h t, l = GRUstory (wt, l \u2212 1, gt, l), the story (wt, l \u2212 t), the negative story (wt \u2212 1, t, l), which we embed in the previous state."}, {"heading": "4 Experiments", "text": "We use the Visual Storytelling Dataset (Huang et al., 2016), consisting of 10,000 albums of 200,000 photos. Each album contains 10-50 photos taken within a 48-hour period with two annotations: 1) 2 album summaries, each containing 5 selected representative photos, and 2) 5 stories describing the selected photos."}, {"heading": "4.1 Story Generation", "text": "We compare our model with two sequence-to-sequence baselines: 1) an encoder decoder model (enc-dec), in which the sequence of album photos is encoded and the last hidden state is fed into the decoder to generate stories, 2) an encoder decoder model (enc-attn-dec) (enc-dec) with weights calculated using a soft attention mechanism. At each decoding step, a weighted sum of hidden states is decoded from the encoder. For a fair comparison, we use the same album representation (sec. 3.1) for the baselines. We test two variants of our model, which are trained with and without ranking regularization, by controlling the processes in our loss function, which are called h-attn (without ranking), and h-attn rank (with ranking)."}, {"heading": "4.2 Album Summarization", "text": "We evaluate the accuracy and retrieval of our generated summaries (output by the photo selector) compared to human selections (the combined set2We test also calculates the p-value of Meteor on 100K samples via the bootstrap test (Efron and Tibshirani, 1994), as Meteor matches human judgments better than Bleu / Rouge (Huang et al., 2016).Our h-attn-rank model has a strong statistical significance (p = 0.01) compared to the enc-dec and enc-attn-dec models (and is similar to the h-attn model) of both humanly selected 5-photo stories. For comparison, we evaluate enc-attn-dec for the same task by aggregating the predicted attention and selecting the 5 photos with the highest accumulated attention. Additionally, we perform DPP-based video summaries through (Kulesza et al, 2012), also have a higher performance in the baseline compared to our models."}, {"heading": "4.3 Output Example Analysis", "text": "Fig. 2 and Fig. 3 show several output examples of the common album summary and storytelling generation. We compare our full model h-attnrank with the base model enc-attn-dec, as both models are able to do the album summary and storytelling tasks together. In Fig. 2 and Fig. 3, we use a blue dashed field and a red field to display the album summary through the two models. As a reference, we also show the Groundtrue album summaries by randomly selecting 1 of 2 human album summaries marked with a green field."}, {"heading": "4.4 Album Retrieval", "text": "In the face of a man-made story, we present a task to retrieve the album described by that story. We randomly select 1000 albums and a Ground Truth Story from each of these albums for evaluation. Based on the generational loss, we calculate the probability of each album Am in the face of the query story S and call the album with the highest generational probability, A = argmaxAmp (S | Am). For evaluation, we use Recall @ k and Median Rank. As shown in Table 4, we find that our models exceed the baseline, but the ranking term in Equation 2 does not significantly improve performance."}, {"heading": "5 Conclusion", "text": "Our proposed hierarchically attentive RNN-based models for end-to-end visual storytelling can effectively summarize and generate relevant stories from complete input photo albums. Automatic and human evaluations show that our method outperforms strong sequence-to-sequence baselines in selection, generation and retrieval tasks."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their helpful comments. This research is supported by NSF Awards # 1633295, 1444234, 1445409, 1562098."}], "references": [{"title": "Sort story: Sorting jumbled images and captions into stories", "author": ["Harsh Agrawal", "Arjun Chandrasekaran", "Dhruv Batra", "Devi Parikh", "Mohit Bansal."], "venue": "EMNLP.", "citeRegEx": "Agrawal et al\\.,? 2016", "shortCiteRegEx": "Agrawal et al\\.", "year": 2016}, {"title": "Neural summarization by extracting sentences and words", "author": ["Jianpeng Cheng", "Mirella Lapata."], "venue": "ACL.", "citeRegEx": "Cheng and Lapata.,? 2016", "shortCiteRegEx": "Cheng and Lapata.", "year": 2016}, {"title": "Textually customized video summaries", "author": ["Jinsoo Choi", "Tae-Hyun Oh", "In So Kweon."], "venue": "arXiv preprint arXiv:1702.01528.", "citeRegEx": "Choi et al\\.,? 2017", "shortCiteRegEx": "Choi et al\\.", "year": 2017}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeffrey Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell."], "venue": "CVPR.", "citeRegEx": "Donahue et al\\.,? 2015", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "An introduction to the bootstrap", "author": ["Bradley Efron", "Robert J Tibshirani."], "venue": "CRC press.", "citeRegEx": "Efron and Tibshirani.,? 1994", "shortCiteRegEx": "Efron and Tibshirani.", "year": 1994}, {"title": "Diverse sequential subset selection for supervised video summarization", "author": ["Boqing Gong", "Wei-Lun Chao", "Kristen Grauman", "Fei Sha."], "venue": "NIPS.", "citeRegEx": "Gong et al\\.,? 2014", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Video summarization by learning submodular mixtures of objectives", "author": ["Michael Gygli", "Helmut Grabner", "Luc Van Gool."], "venue": "CVPR.", "citeRegEx": "Gygli et al\\.,? 2015", "shortCiteRegEx": "Gygli et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "CVPR.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Visual storytelling. In NACCL", "author": ["Ting-Hao Kenneth Huang", "Francis Ferraro", "Nasrin Mostafazadeh", "Ishan Misra", "Aishwarya Agrawal", "Jacob Devlin", "Ross Girshick", "Xiaodong He", "Pushmeet Kohli", "Dhruv Batra"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Large-scale video summarization using web-image priors", "author": ["Aditya Khosla", "Raffay Hamid", "Chih-Jen Lin", "Neel Sundaresan."], "venue": "CVPR.", "citeRegEx": "Khosla et al\\.,? 2013", "shortCiteRegEx": "Khosla et al\\.", "year": 2013}, {"title": "Joint photo stream and blog post summarization and exploration", "author": ["Gunhee Kim", "Seungwhan Moon", "Leonid Sigal."], "venue": "CVPR.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Reconstructing storyline graphs for image recommendation from web community photos", "author": ["Gunhee Kim", "Eric P Xing."], "venue": "CVPR.", "citeRegEx": "Kim and Xing.,? 2014", "shortCiteRegEx": "Kim and Xing.", "year": 2014}, {"title": "Determinantal point processes for machine learning. Foundations and Trends R", "author": ["Alex Kulesza", "Ben Taskar"], "venue": null, "citeRegEx": "Kulesza and Taskar,? \\Q2012\\E", "shortCiteRegEx": "Kulesza and Taskar", "year": 2012}, {"title": "Story-driven summarization for egocentric video", "author": ["Zheng Lu", "Kristen Grauman."], "venue": "CVPR.", "citeRegEx": "Lu and Grauman.,? 2013", "shortCiteRegEx": "Lu and Grauman.", "year": 2013}, {"title": "What to talk about and how? selective generation using lstms with coarse-to-fine alignment", "author": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter."], "venue": "NAACL.", "citeRegEx": "Mei et al\\.,? 2016", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Yingwei Pan", "Tao Mei", "Ting Yao", "Houqiang Li", "Yong Rui."], "venue": "CVPR.", "citeRegEx": "Pan et al\\.,? 2016", "shortCiteRegEx": "Pan et al\\.", "year": 2016}, {"title": "Expressing an image stream with a sequence of natural sentences", "author": ["Cesc C Park", "Gunhee Kim."], "venue": "NIPS.", "citeRegEx": "Park and Kim.,? 2015", "shortCiteRegEx": "Park and Kim.", "year": 2015}, {"title": "Movie description", "author": ["Anna Rohrbach", "Atousa Torabi", "Marcus Rohrbach", "Niket Tandon", "Christopher Pal", "Hugo Larochelle", "Aaron Courville", "Bernt Schiele."], "venue": "IJCV.", "citeRegEx": "Rohrbach et al\\.,? 2016", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2016}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "EMNLP.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Learning visual storylines with skipping recurrent neural networks", "author": ["Gunnar A Sigurdsson", "Xinlei Chen", "Abhinav Gupta."], "venue": "ECCV.", "citeRegEx": "Sigurdsson et al\\.,? 2016", "shortCiteRegEx": "Sigurdsson et al\\.", "year": 2016}, {"title": "Sequence to sequence-video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeffrey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko."], "venue": "ICCV.", "citeRegEx": "Venugopalan et al\\.,? 2015", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "NIPS.", "citeRegEx": "Vinyals et al\\.,? 2015a", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "CVPR.", "citeRegEx": "Vinyals et al\\.,? 2015b", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Automatic generation of story highlights", "author": ["Kristian Woodsend", "Mirella Lapata."], "venue": "ACL.", "citeRegEx": "Woodsend and Lapata.,? 2010", "shortCiteRegEx": "Woodsend and Lapata.", "year": 2010}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio."], "venue": "ICML.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville."], "venue": "ICCV.", "citeRegEx": "Yao et al\\.,? 2015", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["Haonan Yu", "Jiang Wang", "Zhiheng Huang", "Yi Yang", "Wei Xu."], "venue": "CVPR.", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Summary transfer: Exemplar-based subset selection for video summarization", "author": ["Ke Zhang", "Wei-Lun Chao", "Fei Sha", "Kristen Grauman."], "venue": "CVPR.", "citeRegEx": "Zhang et al\\.,? 2016a", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Video summarization with long shortterm memory", "author": ["Ke Zhang", "Wei-Lun Chao", "Fei Sha", "Kristen Grauman."], "venue": "ECCV.", "citeRegEx": "Zhang et al\\.,? 2016b", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "Very recently, datasets have been introduced that extend this task to longer temporal sequences such as movies or photo albums (Rohrbach et al., 2016; Pan et al., 2016; Lu and Grauman, 2013; Huang et al., 2016).", "startOffset": 127, "endOffset": 210}, {"referenceID": 15, "context": "Very recently, datasets have been introduced that extend this task to longer temporal sequences such as movies or photo albums (Rohrbach et al., 2016; Pan et al., 2016; Lu and Grauman, 2013; Huang et al., 2016).", "startOffset": 127, "endOffset": 210}, {"referenceID": 13, "context": "Very recently, datasets have been introduced that extend this task to longer temporal sequences such as movies or photo albums (Rohrbach et al., 2016; Pan et al., 2016; Lu and Grauman, 2013; Huang et al., 2016).", "startOffset": 127, "endOffset": 210}, {"referenceID": 8, "context": "Very recently, datasets have been introduced that extend this task to longer temporal sequences such as movies or photo albums (Rohrbach et al., 2016; Pan et al., 2016; Lu and Grauman, 2013; Huang et al., 2016).", "startOffset": 127, "endOffset": 210}, {"referenceID": 19, "context": ", (Sigurdsson et al., 2016) learns the latent temporal dynamics given a large amount of albums, and (Kim and Xing, 2014) formulate the photo selection as a sparse time-varying directed graph.", "startOffset": 2, "endOffset": 27}, {"referenceID": 11, "context": ", 2016) learns the latent temporal dynamics given a large amount of albums, and (Kim and Xing, 2014) formulate the photo selection as a sparse time-varying directed graph.", "startOffset": 80, "endOffset": 100}, {"referenceID": 16, "context": "To drive this work (Park and Kim, 2015) collected a dataset mined from Blog Posts.", "startOffset": 19, "endOffset": 39}, {"referenceID": 8, "context": "A more direct dataset was recently released (Huang et al., 2016), where multi-sentence stories are collected describing photo albums via Amazon Mechanical Turk.", "startOffset": 44, "endOffset": 64}, {"referenceID": 8, "context": "In this paper, we make use of the Visual Storytelling Dataset (Huang et al., 2016).", "startOffset": 62, "endOffset": 82}, {"referenceID": 22, "context": "Visual Captioning: Most recent approaches to image captioning (Vinyals et al., 2015b; Xu et al., 2015) have used CNN-LSTM structures to generate descriptions.", "startOffset": 62, "endOffset": 102}, {"referenceID": 24, "context": "Visual Captioning: Most recent approaches to image captioning (Vinyals et al., 2015b; Xu et al., 2015) have used CNN-LSTM structures to generate descriptions.", "startOffset": 62, "endOffset": 102}, {"referenceID": 20, "context": "For captioning video or movie content (Venugopalan et al., 2015; Pan et al., 2016), sequence-to-sequence models are widely applied, where the first sequence encodes video frames and the second sequence decodes the description.", "startOffset": 38, "endOffset": 82}, {"referenceID": 15, "context": "For captioning video or movie content (Venugopalan et al., 2015; Pan et al., 2016), sequence-to-sequence models are widely applied, where the first sequence encodes video frames and the second sequence decodes the description.", "startOffset": 38, "endOffset": 82}, {"referenceID": 24, "context": "Attention techniques (Xu et al., 2015; Yu et al., 2016; Yao et al., 2015) are commonly incorporated for both tasks to localize salient temporal or spatial information.", "startOffset": 21, "endOffset": 73}, {"referenceID": 26, "context": "Attention techniques (Xu et al., 2015; Yu et al., 2016; Yao et al., 2015) are commonly incorporated for both tasks to localize salient temporal or spatial information.", "startOffset": 21, "endOffset": 73}, {"referenceID": 25, "context": "Attention techniques (Xu et al., 2015; Yu et al., 2016; Yao et al., 2015) are commonly incorporated for both tasks to localize salient temporal or spatial information.", "startOffset": 21, "endOffset": 73}, {"referenceID": 18, "context": "Video Summarization: Similar to documentation summarization (Rush et al., 2015; Cheng and Lapata, 2016; Mei et al., 2016; Woodsend and Lapata, 2010) which extracts key sentences and words, video summarization selects key frames or shots.", "startOffset": 60, "endOffset": 148}, {"referenceID": 1, "context": "Video Summarization: Similar to documentation summarization (Rush et al., 2015; Cheng and Lapata, 2016; Mei et al., 2016; Woodsend and Lapata, 2010) which extracts key sentences and words, video summarization selects key frames or shots.", "startOffset": 60, "endOffset": 148}, {"referenceID": 14, "context": "Video Summarization: Similar to documentation summarization (Rush et al., 2015; Cheng and Lapata, 2016; Mei et al., 2016; Woodsend and Lapata, 2010) which extracts key sentences and words, video summarization selects key frames or shots.", "startOffset": 60, "endOffset": 148}, {"referenceID": 23, "context": "Video Summarization: Similar to documentation summarization (Rush et al., 2015; Cheng and Lapata, 2016; Mei et al., 2016; Woodsend and Lapata, 2010) which extracts key sentences and words, video summarization selects key frames or shots.", "startOffset": 60, "endOffset": 148}, {"referenceID": 13, "context": "While some approaches use unsupervised learning (Lu and Grauman, 2013; Khosla et al., 2013) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (Gygli et al.", "startOffset": 48, "endOffset": 91}, {"referenceID": 9, "context": "While some approaches use unsupervised learning (Lu and Grauman, 2013; Khosla et al., 2013) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (Gygli et al.", "startOffset": 48, "endOffset": 91}, {"referenceID": 6, "context": ", 2013) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (Gygli et al., 2015; Zhang et al., 2016b,a; Gong et al., 2014).", "startOffset": 103, "endOffset": 165}, {"referenceID": 5, "context": ", 2013) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (Gygli et al., 2015; Zhang et al., 2016b,a; Gong et al., 2014).", "startOffset": 103, "endOffset": 165}, {"referenceID": 2, "context": "Recently, to better exploit semantics, (Choi et al., 2017) proposed textually customized summaries.", "startOffset": 39, "endOffset": 58}, {"referenceID": 11, "context": "Previous works include storyline graph modeling (Kim and Xing, 2014), unsupervised mining (Sigurdsson et al.", "startOffset": 48, "endOffset": 68}, {"referenceID": 19, "context": "Previous works include storyline graph modeling (Kim and Xing, 2014), unsupervised mining (Sigurdsson et al., 2016), blog-photo alignment (Kim et al.", "startOffset": 90, "endOffset": 115}, {"referenceID": 10, "context": ", 2016), blog-photo alignment (Kim et al., 2015), and language retelling (Huang et al.", "startOffset": 30, "endOffset": 48}, {"referenceID": 8, "context": ", 2015), and language retelling (Huang et al., 2016; Park and Kim, 2015).", "startOffset": 32, "endOffset": 72}, {"referenceID": 16, "context": ", 2015), and language retelling (Huang et al., 2016; Park and Kim, 2015).", "startOffset": 32, "endOffset": 72}, {"referenceID": 16, "context": "While (Park and Kim, 2015) collects data by mining Blog Posts, (Huang et al.", "startOffset": 6, "endOffset": 26}, {"referenceID": 8, "context": "While (Park and Kim, 2015) collects data by mining Blog Posts, (Huang et al., 2016) collects stories using Mechanical Turk, providing more directly relevant stories.", "startOffset": 63, "endOffset": 83}, {"referenceID": 7, "context": "We first extract the 2048-dimensional visual representation fi \u2208 Rk for each photo using ResNet101 (He et al., 2016), then a bi-directional RNN is applied to encode the full album.", "startOffset": 99, "endOffset": 116}, {"referenceID": 8, "context": "Following (Huang et al., 2016), we choose a Gated Recurrent Unit (GRU) as the RNN unit to encode the photo sequence.", "startOffset": 10, "endOffset": 30}, {"referenceID": 21, "context": "Inspired by Pointer Networks (Vinyals et al., 2015a), we use another GRU-RNN to perform this task 1.", "startOffset": 29, "endOffset": 52}, {"referenceID": 3, "context": "Following Donahue et al. (2015), the l-th word probability of the t-th sentence is:", "startOffset": 10, "endOffset": 32}, {"referenceID": 0, "context": "To further exploit the notion of temporal coherence in a story, we add an order-preserving constraint to order the sequence of sentences within a story (related to the story-sorting idea in Agrawal et al. (2016)).", "startOffset": 190, "endOffset": 212}, {"referenceID": 8, "context": "We use the Visual Storytelling Dataset (Huang et al., 2016), consisting of 10,000 albums with 200,000 photos.", "startOffset": 39, "endOffset": 59}, {"referenceID": 24, "context": "We compare our model with two sequence-to-sequence baselines: 1) an encoderdecoder model (enc-dec), where the sequence of album photos is encoded and the last hidden state is fed into the decoder for story generation, 2) an encoder-attention-decoder model (Xu et al., 2015) (enc-attn-dec) with weights computed using a soft-attention mechanism.", "startOffset": 256, "endOffset": 273}, {"referenceID": 4, "context": "We also compute the p-value of Meteor on 100K samples via the bootstrap test (Efron and Tibshirani, 1994), as Meteor has better agreement with human judgments than Bleu/Rouge (Huang et al.", "startOffset": 77, "endOffset": 105}, {"referenceID": 8, "context": "We also compute the p-value of Meteor on 100K samples via the bootstrap test (Efron and Tibshirani, 1994), as Meteor has better agreement with human judgments than Bleu/Rouge (Huang et al., 2016).", "startOffset": 175, "endOffset": 195}], "year": 2017, "abstractText": "We address the problem of end-to-end visual storytelling. Given a photo album, our model first selects the most representative (summary) photos, and then composes a natural language story for the album. For this task, we make use of the Visual Storytelling dataset and a model composed of three hierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album photos, select representative (summary) photos, and compose the story. Automatic and human evaluations show our model achieves better performance on selection, generation, and retrieval than baselines.", "creator": "LaTeX with hyperref package"}}}