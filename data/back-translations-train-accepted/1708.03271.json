{"id": "1708.03271", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Aug-2017", "title": "Neural Machine Translation Leveraging Phrase-based Models in a Hybrid Search", "abstract": "In this paper, we introduce a hybrid search for attention-based neural machine translation (NMT). A target phrase learned with statistical MT models extends a hypothesis in the NMT beam search when the attention of the NMT model focuses on the source words translated by this phrase. Phrases added in this way are scored with the NMT model, but also with SMT features including phrase-level translation probabilities and a target language model. Experimental results on German-&gt;English news domain and English-&gt;Russian e-commerce domain translation tasks show that using phrase-based models in NMT search improves MT quality by up to 2.3% BLEU absolute as compared to a strong NMT baseline.", "histories": [["v1", "Thu, 10 Aug 2017 15:48:33 GMT  (97kb,D)", "http://arxiv.org/abs/1708.03271v1", "To appear in Proceedings of EMNLP 2017"]], "COMMENTS": "To appear in Proceedings of EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["leonard dahlmann", "evgeny matusov", "pavel petrushkov", "shahram khadivi"], "accepted": true, "id": "1708.03271"}, "pdf": {"name": "1708.03271.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Translation Leveraging Phrase-based Models in a Hybrid Search", "authors": ["Leonard Dahlmann", "Shahram Khadivi"], "emails": ["skhadivi}@ebay.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is so that it will be able to eren.n the aforementioned lcihsrcehncS"}, {"heading": "1.1 Related Work", "text": "In the research line, which is closely related to our approach, neural models are used as additional features in vanilla phrase-based systems. Examples include the work of (Devlin et al., 2014), (JunczysDowmunt et al., 2016), etc. Such approaches have certain limitations: First, the search space of the model is still limited by what can be generated with a phrase table extracted from parallel data on word alignments. Second, organizing the search, in which only limited word coverage (e.g. 4 last target words) is available for each sub-hypothesis, makes it difficult to integrate recursive neural network LMs and translation models that take into account all previously generated target words. Therefore, for example, attention-based NMT models are usually used only in rescoring (Peter et al al al., 2016). In (Stahlberg et al al al al al al al al al al al al al al Mc 2017, a two-step translation method is proposed)."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Neural MT", "text": "The Neural MT proposed by (Bahdanau et al., 2014) therefore maximizes the conditional log probability of the target sentence E: e1,.., eI in view of the source sentence F: f1,.., fJ: HD = \u2212 1N N N = 1 log p\u03b8 (En | Fn), where (En, Fn) the conditional probability refers to the nth training set pair in a data set D, and N indicates the total number of sentence pairs in the training corpus. Using the encoder decoder architecture of (Cho et al., 2014), the conditional probability can be written as follows: p (e1 \u00b7 eI | f1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 fJ) = I-i = 1 p (ei \u2212 1 \u00b7 e1, c) with the encoder decoder architecture of (ei | ei \u2212 1, c) = g (si, ei \u2212 1, c)."}, {"heading": "2.2 Phrase-based MT", "text": "The log-linear model, as introduced in (Och and Ney, 2002), allows to decompile the translation probability Pr (eI1 | fJ1) by using any number of features hm (fJ1, e I 1). Each feature is multiplied by a corresponding scaling factor \u03bbm: Pr (eI1 | fJ1) = exp (\u2211 M m = 1 \u03bbmhm (f J 1, e I 1)). The standard PBMT approach uses a log-linear model in which bidirectional phrasal and lexical values, language model values, distortion values, word penalties and phrase penalties are combined as features."}, {"heading": "3 Hybrid Approach", "text": "In this section we describe our proposed hybrid NMT approach. This algorithm allows translations to be generated partly by phrases and partly by words.Section 3.1 describes the models we use to evaluate hypotheses. The search algorithm is presented in Section 3.2."}, {"heading": "3.1 Log-linear Combination", "text": "We use a log-linear model combination to introduce SMT models into the NMT search. Since translations can be partially generated by phrases, we introduce the phrase segmentation sK1 as a hidden variable into the models similar to (Zens and Ney, 2008), where K is the number of phrases used in the translation. Note that, unlike standard PBMT, sK1 does not have to cover the entire source sentence, since parts of the translation can be generated by words. (1) Leave f-k, e-k the selected phrase pairs in the segmentation sK1 = argmax I, eI1 {max sK1 M \u2211 m = 1 \u03bbmhm (f J 1, e I 1, s K 1). (1) In our experiments with the proposed hybrid search, we use the following features: 1. Segment NMT-1h1 in NMT1."}, {"heading": "3.2 Search", "text": "The algorithms are based on the search for a target phrase for NMT, which generates one word per time step in a process translated from left to right. We modify this search to allow hypotheses in addition to the usual word hypotheses. Phrases are proposed on the basis of neuronal attention, starting from the starting position with the maximum current attention. We only suggest phrases when a source position is focused. We check that the suggested phrases do not overlap with already translated source words by tracking the sum of attention in previous time steps for each source position. Thus, the problem of global reordering is completely limited to the NMT model and we follow the attention when the hypotheses formulate hypothetically. Phrases are dotted by NMT and SMT models. The beam is divided into two parts of fixed size: the word beam and the phrase beam is used to evaluate phrases used from a previous word."}, {"heading": "4 Experiments", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "4.1 E-commerce English\u2192Russian", "text": "The results of the LM system are summarized in Table 2.NMT vs. Phrase-based SMT systems. Pure NMT systems show great improvements over the phrase-based baseline4. These improvements are also significantly greater than when we use the NMT model to reform source and target phrase selection. NMT results are not improved when the beam size is increased from 12 to 128 hybrid searches, but only on the target side of the bilingual approach, where we get a phrase table on source and target phrase selection with BPE for compatibility with the NMT vocabularies. Using the hybrid approach of using an LM-trained only on the target side of bilingual data, we get an improvement of 0.3% BLEU on article descriptions and 1.4% BLEU on product descriptions on the pure NMT system. Using the LM-trained monolingual data EU, we get an overall improvement from 1.0% EU to the BL3% system with the BL3%."}, {"heading": "4.2 WMT 2016 German\u2192English", "text": "The results of the WMT English \u2192 English task are presented in Table 6. The initial phrase-based baseline uses the 5 gram language model, which is estimated on the landing page of bilingual data. By adding the News Crawl LM data, we gain 2.5% and 2.3% BLEU on the test sets, but PBMT is still behind NMT. For the hybrid approach, we use a beam size of 64 and a maximum number of beam steps of 1.5 \u00d7 J (instead of 2 \u00d7 J) to accelerate experiments. We use separate word-tightening functions, one for word-based hypotheses and one for phrase-based hypotheses, to allow better control of translation lengths. However, with the hybrid approach, which uses the 5 gram language model, which is estimated on the landing page of bilingual data, and phrase results, we get small improvements in BLEU across the NMT baseline. However, the TER increases to different thresholds, where receptivity = 1.0 and concentration = 0.1."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a novel hybrid search that adds phrase models to NMT. NMT beam search was modified to insert phrase translations based on the current and accumulated attention weights of the NMT decoder RNN. NMT model score was used in a log-linear model with standard phrase values and an n-gram language model. We described in detail the algorithm in which we maintain separate bars for NMT word hypotheses and hypotheses with incomplete phrase translation, as well as introduce parameters that control sentence coverage of the source. Numerous experiments on two large vocabulary translation tasks showed that hybrid search significantly improves BLEU values compared to a strong NMT baseline that already exceeds phrase-based SMT."}, {"heading": "Acknowledgments", "text": "We would like to thank Tamer Alkhouli and JanThorsten Peter for the helpful conversations and the anonymous reviewers for their suggestions."}], "references": [{"title": "Incorporating discrete translation lexicons into neural machine translation", "author": ["Philip Arthur", "Graham Neubig", "Satoshi Nakamura"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Arthur et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arthur et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR abs/1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Neural versus phrasebased machine translation quality: a case study", "author": ["Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo", "Marcello Federico"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Bentivogli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2016}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard M. Schwartz", "John Makhoul"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal", "Zoubin Ghahramani"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Gal and Ghahramani.,? \\Q2016\\E", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "On using monolingual corpora in neural machine translation", "author": ["\u00c7aglar G\u00fcl\u00e7ehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Lo\u0131\u0308c Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "CoRR abs/1503.03535", "citeRegEx": "G\u00fcl\u00e7ehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "G\u00fcl\u00e7ehre et al\\.", "year": 2015}, {"title": "A comparison between count and neural network models based on joint translation and reordering sequences", "author": ["Andreas Guta", "Tamer Alkhouli", "Jan-Thorsten Peter", "Joern Wuebker", "Hermann Ney"], "venue": "In Proceedings of the 2015 Conference on Empiri-", "citeRegEx": "Guta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guta et al\\.", "year": 2015}, {"title": "Improved neural machine translation with SMT features", "author": ["Wei He", "Zhongjun He", "Hua Wu", "Haifeng Wang"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "KyungHyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "The AMU-UEDIN submission to the WMT16 news translation task: Attention-based NMT models as feature functions in phrase-based SMT", "author": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Rico Sennrich"], "venue": "In Proceedings of the First Conference", "citeRegEx": "Junczys.Dowmunt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR abs/1412.6980", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "AppTek\u2019s APT Machine Translation System for IWSLT", "author": ["Evgeny Matusov", "Sel\u00e7uk K\u00f6pr\u00fc"], "venue": "International Workshop on Spoken Language Translation,", "citeRegEx": "Matusov and K\u00f6pr\u00fc.,? \\Q2010\\E", "shortCiteRegEx": "Matusov and K\u00f6pr\u00fc.", "year": 2010}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Discriminative training and maximum entropy models for statistical machine translation", "author": ["Franz Josef Och", "Hermann Ney"], "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Och and Ney.,? \\Q2002\\E", "shortCiteRegEx": "Och and Ney.", "year": 2002}, {"title": "The rwth aachen machine translation system for iwslt 2016", "author": ["Jan-Thorsten Peter", "Andreas Guta", "Nick Rossenbach", "Miguel Graa", "Hermann Ney"], "venue": "In International Workshop on Spoken Language", "citeRegEx": "Peter et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Peter et al\\.", "year": 2016}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "In Proceedings of the 54th Annual Meeting of the Association", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "In Proceedings of the 54th Annual Meeting of the Association", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation by minimising the Bayes-risk with respect to syntactic translation lattices", "author": ["Felix Stahlberg", "Adri\u00e0 de Gispert", "Eva Hasler", "Bill Byrne"], "venue": "In Proceedings of the 15th Conference of the European Chapter of the Association", "citeRegEx": "Stahlberg et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2017}, {"title": "Neural machine translation with external phrase memory", "author": ["Yaohua Tang", "Fandong Meng", "Zhengdong Lu", "Hang Li", "Philip L.H. Yu"], "venue": "CoRR abs/1606.01792", "citeRegEx": "Tang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2016}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li"], "venue": "In Proceedings of the 54th Annual Meeting of the Association", "citeRegEx": "Tu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Improvements in dynamic programming beam search for phrasebased statistical machine translation", "author": ["Richard Zens", "Hermann Ney"], "venue": "In International Workshop on Spoken Language Translation,", "citeRegEx": "Zens and Ney.,? \\Q2008\\E", "shortCiteRegEx": "Zens and Ney.", "year": 2008}], "referenceMentions": [{"referenceID": 2, "context": "Human analysis (Bentivogli et al., 2016) showed that NMT makes significantly fewer reordering errors, and also is able to select correct word forms more often than PBMT in the case of morphologically rich target languages.", "startOffset": 15, "endOffset": 40}, {"referenceID": 1, "context": "However, state-of-the-art NMT approaches based on an encoder-decoder architecture with an attention mechanism as introduced by (Bahdanau et al., 2014) exhibit weaknesses that sometimes lead to MT errors which a phrase-based MT system does not make.", "startOffset": 127, "endOffset": 150}, {"referenceID": 4, "context": "include the work of (Devlin et al., 2014), (JunczysDowmunt et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 16, "context": "That is why, for instance, the attention-based NMT models were usually applied only in rescoring (Peter et al., 2016).", "startOffset": 97, "endOffset": 117}, {"referenceID": 19, "context": "In (Stahlberg et al., 2017), a two-step translation process is used, where in the first step a SMT translation lattice is generated, and in the second step the NMT decoder combines NMT scores with the Bayes-risk of the translations according to the lattice.", "startOffset": 3, "endOffset": 27}, {"referenceID": 0, "context": "In (Arthur et al., 2016), a statistical word lexicon is used to influence NMT hypotheses, also based on the attention mechanism.", "startOffset": 3, "endOffset": 24}, {"referenceID": 6, "context": "(G\u00fcl\u00e7ehre et al., 2015) combine target n-gram LM scores with NMT scores to find the best translation.", "startOffset": 0, "endOffset": 23}, {"referenceID": 8, "context": "(He et al., 2016) also use a target LM, but add further SMT features such as word penalty and word lexica to the NMT beam search.", "startOffset": 0, "endOffset": 17}, {"referenceID": 20, "context": "In (Tang et al., 2016), the NMT decoder is modified to switch between using externally defined phrases and standard NMT word hypotheses.", "startOffset": 3, "endOffset": 22}, {"referenceID": 21, "context": "Somewhat related to our work is the concept of coverage-based NMT (Tu et al., 2016), where the model architecture is changed to explicitly account for source coverage.", "startOffset": 66, "endOffset": 83}, {"referenceID": 1, "context": "Neural MT proposed by (Bahdanau et al., 2014)", "startOffset": 22, "endOffset": 45}, {"referenceID": 3, "context": "When using the encoder-decoder architecture by (Cho et al., 2014), the conditional probability can be written as:", "startOffset": 47, "endOffset": 65}, {"referenceID": 1, "context": ", hJ), and hj contains information about the whole input sentence, but with a strong focus on the parts surrounding the j-th word (Bahdanau et al., 2014).", "startOffset": 130, "endOffset": 153}, {"referenceID": 15, "context": "The log-linear model, as introduced in (Och and Ney, 2002), allows decomposing the translation probability Pr(e1|f 1 ) by using an arbitrary number of features hm(f 1 , e I 1).", "startOffset": 39, "endOffset": 58}, {"referenceID": 22, "context": "Since translations can be partially generated by phrases, we introduce the phrase segmentation s1 as a hidden variable into the models similarly to (Zens and Ney, 2008), where K is the number of phrases used in the translation.", "startOffset": 148, "endOffset": 168}, {"referenceID": 14, "context": "The scaling factors \u03bbm are tuned with minimum error rate training (MERT) (Och, 2003) on n-best lists of the development set.", "startOffset": 73, "endOffset": 84}, {"referenceID": 13, "context": "For the phrase-based baselines, we use an inhouse phrase-decoder (Matusov and K\u00f6pr\u00fc, 2010) which is similar to the Moses decoder (Koehn et al.", "startOffset": 65, "endOffset": 90}, {"referenceID": 12, "context": "For the phrase-based baselines, we use an inhouse phrase-decoder (Matusov and K\u00f6pr\u00fc, 2010) which is similar to the Moses decoder (Koehn et al., 2007).", "startOffset": 129, "endOffset": 149}, {"referenceID": 7, "context": "including word-level and phrase-level translation probabilities, the distortion model, 5-gram LMs, and a 7-gram joint translation and reordering model reimplemented based on the work of (Guta et al., 2015).", "startOffset": 186, "endOffset": 205}, {"referenceID": 14, "context": "The tuning is performed using MERT (Och, 2003) to increase the BLEU score on the development set.", "startOffset": 35, "endOffset": 46}, {"referenceID": 11, "context": "The model is trained with maximum likelihood loss for 15 epochs using Adam optimizer (Kingma and Ba, 2014) on complete data in batches of 100 sentences.", "startOffset": 85, "endOffset": 106}, {"referenceID": 5, "context": "For regularization we use L2 loss with weight 10\u22127 and dropout following Gal and Ghahramani (2016). We set the dropout probability for input and recurrent connections of the RNN to 0.", "startOffset": 73, "endOffset": 99}, {"referenceID": 9, "context": "use approximate loss as described in (Jean et al., 2015).", "startOffset": 37, "endOffset": 56}], "year": 2017, "abstractText": "In this paper, we introduce a hybrid search for attention-based neural machine translation (NMT). A target phrase learned with statistical MT models extends a hypothesis in the NMT beam search when the attention of the NMT model focuses on the source words translated by this phrase. Phrases added in this way are scored with the NMT model, but also with SMT features including phrase-level translation probabilities and a target language model. Experimental results on German\u2192English news domain and English\u2192Russian ecommerce domain translation tasks show that using phrase-based models in NMT search improves MT quality by up to 2.3% BLEU absolute as compared to a strong", "creator": "LaTeX with hyperref package"}}}