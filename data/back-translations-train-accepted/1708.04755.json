{"id": "1708.04755", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2017", "title": "Learning Chinese Word Representations From Glyphs Of Characters", "abstract": "In this paper, we propose new methods to learn Chinese word representations. Chinese characters are composed of graphical components, which carry rich semantics. It is common for a Chinese learner to comprehend the meaning of a word from these graphical components. As a result, we propose models that enhance word representations by character glyphs. The character glyph features are directly learned from the bitmaps of characters by convolutional auto-encoder(convAE), and the glyph features improve Chinese word representations which are already enhanced by character embeddings. Another contribution in this paper is that we created several evaluation datasets in traditional Chinese and made them public.", "histories": [["v1", "Wed, 16 Aug 2017 03:17:57 GMT  (1576kb,D)", "http://arxiv.org/abs/1708.04755v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tzu-ray su", "hung-yi lee"], "accepted": true, "id": "1708.04755"}, "pdf": {"name": "1708.04755.pdf", "metadata": {"source": "CRF", "title": "Learning Chinese Word Representations From Glyphs Of Characters", "authors": ["Tzu-Ray Su", "Hung-Yi Lee"], "emails": ["b01901007@ntu.edu.tw", "hungyilee@ntu.edu.tw"], "sections": [{"heading": "1 Introduction", "text": "Regardless of which target language it is, high-quality word representation (also known as \"embedding\") is key to many tasks of natural language processing, such as sentence classification (Kim, 2014), answering questions (Zhou et al., 2015), machine translation (Sutskever et al., 2014), etc. In addition, word representation is a building block in the manufacture of phrase harvesters (Cho et al., 2014) and sentence representation (Kiros et al., 2015). In this paper, we focus on learning Chinese word representation. A Chinese word is made up of characters that contain rich semantics. The meaning of a Chinese word is often related to the meaning of its compositional characters. Therefore, Chinese word embedding can be improved through its compositional embedding of letters (Chen et al., 2015; Xu et al., 2016)."}, {"heading": "2 Background Knowledge and Related Works", "text": "To illustrate our own work, in Section 2.1 we briefly introduce the representative methods of learning word representation. In Section 2.2 we will introduce some of the linguistic properties of Chinese and then introduce the methods that use these properties to improve word representation."}, {"heading": "2.1 Word Representation Learning", "text": "Common research on word representation is based on the distribution hypothesis, meaning that words with similar contexts have similar meanings. Normally, a large-scale corpus is used, and word representations are produced from the random information of a word and its context. Existing methods of word representation can be divided into two families (Levy et al., 2015): number-based family (Turney and Pantel, 2010; Bullinaria and Levy, 2007) and predictive family. Word representations can be obtained by building neural network-based models (Bengio et al., 2003; Collobert et al., 2011). Representative methods are briefly presented below."}, {"heading": "2.1.1 CBOW and Skipgram", "text": "Both the Continuous Bag-of-Words (CBOW) model and the Skipgram model assign embedding to words and contexts in a movable local context window (Mikolovet al., 2013a). CBOW predicts the word based on its context embedding, while Skipgram predicts contexts based on its word embedding. Predicting the occurrence of word / context in CBOW and Skipgram models could be seen as learning a neural network with multiple classes (the number of classes corresponds to the size of the vocabulary). In (Mikolov et al., 2013b), the authors introduced several techniques to improve performance. Negative sampling is introduced to accelerate learning, and ampling of more frequent words is introduced into randomly sorted training examples with common words (such as \"the,\" \"a,\" \"of\") and has a similar effect to removing stops from words."}, {"heading": "2.1.2 GloVe", "text": "Instead of using local context windows, (Pennington et al., 2014) the GloVe model was proposed. Training of GloVe word representations begins with the creation of a coexistence matrix X from a corpus, where each matrix entry Xij represents the counts that the word wj occurs in the context of the word wi. In (Pennington et al., 2014) the authors used a harmonious weighting function for the coexistence count, that is, word-context pairs with the distance d contribute 1d to the global coexistence count. Let's use the word representation of the word wi and ~ wj the word representation of the word wj as context, the GloVe model minimizes the loss: \"i, j, j, non \u2212 zeroentries of X f (Xij) (~ w T i ~ w = max wj + bi + b \u00b2 j \u2212 log (Xij)), where bi is the distortion for the word wj, and Xij is the word \u00b2 distortion for the context wj = 7."}, {"heading": "2.2 Improving Chinese Word Representation Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.2.1 The Chinese Language", "text": "A Chinese word is composed of a sequence of characters, and the meanings of some Chinese words relate to the composition of the meanings of their characters. To improve Chinese word representation with subword information, the word \"embedding\" (CWE) is composed of two characters (Chen et al., 2015) in Section 2.2.2. A Chinese character is composed of several graphic components. Characters with the same component share similar semantic or phonetic characteristics. In a Chinese dictionary, characters with similar rough semantics are classified into categories. A Chinese character is composed of several graphic components. Characters with the same component share similar semantic or phonetic characteristics. In a Chinese dictionary, characters with similar semantic characteristics are grouped into categories. A common graphic component, which refers to the common semantic level, refers to the common semantic category."}, {"heading": "2.2.2 Character-enhanced Word Embedding (CWE)", "text": "The main idea of CWE is that word embedding is enhanced by its compositional embedding. CWE predicts the word from both word and character embedding of contexts, as illustrated in Fig. 5 (a). For wordwi, the CWE word embedding has the following form: ~ wcwei = ~ wi + 1 | C (i) | \u2211 cj-C (i) ~ cjwo ~ wi is the word embedding, ~ cj is the embedding of the j-th character in wi, and C (i) is the composition of compositional characters of the word wi. The mean value of CWE word embedding of contexts is then used to predict the word width. Sometimes, a character has several different meanings, this is known as an ambiguity problem. To deal with this, each character is assigned a bag of embedding."}, {"heading": "2.2.3 Multi-granularity Embedding (MGE)", "text": "Based on CBOW and CWE, (Yin et al., 2016) MGE suggested predicting the target word with its radical embedding and modified word embedding of the context in CWE, as shown in Fig.5 (b). There is no ambiguity of radicals, so each radical is embedded ~ r. We call ~ rk the radical embedding of the character core. MGE predicts the target word wi with the following hidden vector: ~ hi = 1 | C (i) | ck: C (i) ~ rk + 1 | W (i) | \u2211 wj: W (i) ~ wcwej, where W (i) is the set of context words, ~ wcwej is the CWE word embedding of wj. MGE chooses character embedding with the position-based method in CWE and selects radical embedding according to a character index constructed from a dictionary during the training."}, {"heading": "3 Model", "text": "In Section 3.1, we first extract glyphs features from bitmaps using the convenAE. Glyphs features are used in Section 3.2 to improve the existing learning models for displaying words. In Section 3.3, we try to learn word representations directly from the glyphs features."}, {"heading": "3.1 Character Bitmap Feature Extraction", "text": "A convex encoder (Masci et al., 2011) is used to reduce the dimensions of rendered bitmaps and capture high-level characteristics. The architecture of the convex is shown in Fig. 6. The convex consists of 5 convolutionary layers both in the encoder and in the decoder. The step larger than one is used instead of layers. Convolutionary and deconvolutionary layers on the same level share the same core. The input image is a 60 x 60 8-bit bitmap in grayscale, and the encoder extracts 512-dimensional characteristics. The characteristic of the character core from the encoder is referred to in the paper as the character lyphs attribute ~ gk."}, {"heading": "3.2 Glyph-Enhanced Word Embedding (GWE)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1 Enhanced by Context Word Glyphs", "text": "We modify the CWE model based on CBOW in Section 2.2.2 to integrate context character characteristics (ctxG. This modified word embedding ~ wctxGi of the word wi has the form: ~ wctxGi = ~ wi + 1 | C (i) | \u2211 cj \u0441C (i) (~ cj + ~ gj), where C (i) is the compositional characteristics of wi and ~ gj is the glyphs characteristic of cj. The model predicts the target word wi from ctxG word embedding of contexts, as shown in Fig.7. The parameters in the convAE are pre-trained, i.e. not learned together with embedding ~ w and ~ c, so character characteristics ~ g are fixed during the training."}, {"heading": "3.2.2 Enhanced by Target Word Glyphs", "text": "Here we propose another variant: In this model, the model structure is the same as in Fig.7. The difference lies in the hidden vector used to predict the target word. Instead of adding up the mean value of the character characteristics of the glyph characteristics of the contexts, it adds the mean value of the glyph characteristics of the target word (tarG) as shown in Fig.8. As in Section 3.2.1, the ConvAE is not learned jointly."}, {"heading": "3.3 Directly Learn From Character Glyph Features", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.3.1 RNN-Skipgram", "text": "As in Fig.9, a 2-layer gated recurrent unit (GRU) (Cho et al., 2014) network followed by 2 fully connected ELU (Clevert et al., 2015) layers generates the word representation ~ wi from the input sequence {~ gk} of the word wi. ~ wi is then used to predict the contexts of wi. In training, we use negative sampling and subsampling for common words (Mikolov et al., 2013b)."}, {"heading": "3.3.2 RNN-GloVe", "text": "We modify the GloVe model to learn directly from the character characteristics of the character glyphs, as in Fig.10. We feed the character characteristics {~ gk, ck-C (i)}, {~ gk, \"ck-C (j)} of word wi and context wj into a common GRU network. GRU output is then fed to two fully interconnected ELU layers to generate word representations ~ wi and ~ -wj. The inner product of ~ wi and ~ -wj is the prediction of the protocol (Xij). We apply the same loss function with weights in GloVe. We follow (Pennington et al., 2014) and use ~ wi + ~ wi to evaluate word representation."}, {"heading": "4 Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Preprocessing", "text": "From 1991 to 2002, we learned word representations with traditional Chinese texts from the Central News Agency newspapers (Chinese Giga Word, LDC2003T09), all foreign words, numerical words and punctuations were removed, and word segmentation was done using the open source Python package jieba3. In all 316,960,386 segmented words, we extracted 8780 unique characters and used a real font (BiauKai) to transfer each character into a 60 x 60 8-bit grayscale bitmap, and we also removed words with frequency < = 25, leaving 158,565 unique words as vocabulary."}, {"heading": "4.2 Extracting Visual Features of Character Bitmap", "text": "Inspired by (Zeiler et al., 2011), a layer-by-layer training has been applied to our ConvAE. From low level to higher, the core of each layer is trained individually, with other cores frozen for 100 epochs. Loss function is the Euclidean distance between input and reconstructed bitmap, and we have added l1 regulation to the activation of folding layers. As an optimization algorithm, we chose Adagrad and set the batch size = 20 and learning rate = 0.001.3https: / / github.com / fxsjy / jiebaThe comparison between the input bitmaps and their reconstructions is shown in Fig. 11. Input bitmaps are located in the upper row, while the reconstructions are in the lower row. Next, we visualized the extracted character glyphs features with tSNE (Maaten and Hinton, 2008). Part of the visualization result is shown in Fig. 12. We found out of the characteristics that are expressed in the clusters."}, {"heading": "4.3 Training Details of Word Representations", "text": "For CBOW, Skipgram, CWE, GWE and RNN-Skipgram, we used the following hyperparameters. The context window was set to 5 on both sides of a word. We used 10 negative samples, and the threshold t of subsampling was set to 10 \u2212 5. Since Yin did not publish her code, we followed her paper and reproduced the MGE model. We created the mapping between characters and radicals from the Unihan database 5. Each character corresponds to one of the 214 radicals in this data set, and the same hyperparameters were used in the training as above. Note that we did not separate the non-compositional words from each other during the training, like the original CWE and MGE databases.We used the GloVe code6 to assume the keywords Vecrist Glodn.com and Glodn.com for building Glodn.com = 75."}, {"heading": "5 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Word Similarity", "text": "In fact, it is that we are able to assert ourselves, that we are able to put ourselves in a position to put ourselves at the top, \"he said in an interview with the\" Welt. \""}, {"heading": "5.2 Word Analogy", "text": "An analogy problem has the following form: \"King\": \"Queen\" = \"Man\": \"WE,\" and \"Woman\" is answer to \"?.\" By correctly answering the question, the model is considered to be able to express semantic relationships. In addition, the analogy relationship could be expressed by vector arithmetic of word representations, as described in (Mikolov et al., 2013b). For the above problem, we find word wi such that wi = argmaxw cos (~ wking + wman).8We followed the method described in https: / / stats.stackexchange.com / Questions / As in the previous subsection, we used the word analogy Dataset in (Chen et al., 2015) to traditional. The dataset contains 3 groups of analogy problems: capitals / provinces of cities, and family relationships."}, {"heading": "5.3 Case Study", "text": "To examine the effect of glyphs more closely, in SimLex-999 we show the following pairs of words whose calculated cosmic similarities based on GWE models are greater than CWE. The pairs may not look alike, but their components have related semantics. For example, in \"\" (clever) \"the component\" \"(sharp) is compositively similar to the meaning of\" \"\" (acute) and describes someone with a sharp mind. Other examples show the ability to associate semantics with radicals. We also provide several counter-examples. Below we will find some pairs of words that are not similar, but GWE methods generate greater similarities than CBOW or CWE. Let's take for example the component \"\" (mountain) and \"(honey). Since they have no common letters, the only thing that is common is the component\", \"and we assume that this is the reason for the higher similarity."}, {"heading": "6 Conclusions", "text": "This work is a pioneer in improving Chinese word representation with character glyphs. Character characteristics are learned directly from character bitmaps by convenAE. Subsequently, we proposed two methods for learning Chinese word representation: The first is to use character glyphs as an enhancement; the second is to learn word representation directly from sequences of glyphs characteristics. In experiments, we found the latter completely impracticable. Training word representation with RNN without word and character information is a challenge. Nevertheless, the characteristics of the glyphs improved character-enhanced Chinese word representation, especially in the word analogy task related to the family. The results of using character glyphs characteristics when learning word representation were common. Perhaps the adjacent information in the corpus will play a greater role as glyphs characteristics in the corpus. Non-glyphs model is the idea of each Chinese character in 2013, so we could closely examine Zancon."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin."], "venue": "Journal of machine learning research, 3(Feb):1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Extracting semantic representations from word co-occurrence statistics: A computational study", "author": ["John A Bullinaria", "Joseph P Levy."], "venue": "Behavior research methods, 39(3):510\u2013526.", "citeRegEx": "Bullinaria and Levy.,? 2007", "shortCiteRegEx": "Bullinaria and Levy.", "year": 2007}, {"title": "Joint learning of character and word embeddings", "author": ["Xinxiong Chen", "Lei Xu", "Zhiyuan Liu", "Maosong Sun", "Huan-Bo Luan."], "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires,", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["Djork-Arn\u00e9 Clevert", "Thomas Unterthiner", "Sepp Hochreiter."], "venue": "arXiv preprint arXiv:1511.07289.", "citeRegEx": "Clevert et al\\.,? 2015", "shortCiteRegEx": "Clevert et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research, 12(Aug):2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics.", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in neural information processing systems, pages 3294\u20133302.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics, 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Component-enhanced chinese character embeddings", "author": ["Yanran Li", "Wenjie Li", "Fei Sun", "Sujian Li."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 829\u2013834, Lisbon, Portugal. Association", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Learning character-level compositionality with visual features", "author": ["Frederick Liu", "Han Lu", "Chieh Lo", "Graham Neubig."], "venue": "CoRR, abs/1704.04859.", "citeRegEx": "Liu et al\\.,? 2017", "shortCiteRegEx": "Liu et al\\.", "year": 2017}, {"title": "Visualizing data using t-SNE", "author": ["Laurens van der Maaten", "Geoffrey Hinton."], "venue": "Journal of Machine Learning Research, 9(Nov):2579\u20132605.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Stacked convolutional autoencoders for hierarchical feature extraction", "author": ["Jonathan Masci", "Ueli Meier", "Dan Cire\u015fan", "J\u00fcrgen Schmidhuber."], "venue": "In", "citeRegEx": "Masci et al\\.,? 2011", "shortCiteRegEx": "Masci et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP, volume 14, pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Radical-enhanced chinese character embedding", "author": ["Yaming Sun", "Lei Lin", "Nan Yang", "Zhenzhou Ji", "Xiaolong Wang."], "venue": "International Conference on Neural Information Processing, pages 279\u2013286. Springer.", "citeRegEx": "Sun et al\\.,? 2014", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D Turney", "Patrick Pantel."], "venue": "Journal of artificial intelligence research, 37:141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Improve chinese word embeddings by exploiting internal structure", "author": ["Jian Xu", "Jiawei Liu", "Liangang Zhang", "Zhengyu Li", "Huanhuan Chen."], "venue": "Proceedings of NAACL-HLT, pages 1041\u20131050.", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Multi-granularity chinese word embedding", "author": ["Rongchao Yin", "Quan Wang", "Peng Li", "Rui Li", "Bin Wang."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November", "citeRegEx": "Yin et al\\.,? 2016", "shortCiteRegEx": "Yin et al\\.", "year": 2016}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["Matthew D Zeiler", "Graham W Taylor", "Rob Fergus."], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on, pages 2018\u20132025. IEEE.", "citeRegEx": "Zeiler et al\\.,? 2011", "shortCiteRegEx": "Zeiler et al\\.", "year": 2011}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "Advances in neural information processing systems, pages 649\u2013657.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Deep learning for chinese word segmentation and pos tagging", "author": ["Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu."], "venue": "EMNLP, pages 647\u2013657.", "citeRegEx": "Zheng et al\\.,? 2013", "shortCiteRegEx": "Zheng et al\\.", "year": 2013}, {"title": "Learning continuous word embedding with metadata for question retrieval in community question answering", "author": ["Guangyou Zhou", "Tingting He", "Jun Zhao", "Po Hu."], "venue": "ACL (1), pages 250\u2013259.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "No matter which target language it is, high quality word representations (also known as word \u201cembeddings\u201d) are keys to many natural language processing tasks, for example, sentence classification (Kim, 2014), question answering (Zhou et al.", "startOffset": 196, "endOffset": 207}, {"referenceID": 25, "context": "No matter which target language it is, high quality word representations (also known as word \u201cembeddings\u201d) are keys to many natural language processing tasks, for example, sentence classification (Kim, 2014), question answering (Zhou et al., 2015), machine translation (Sutskever et al.", "startOffset": 228, "endOffset": 247}, {"referenceID": 18, "context": ", 2015), machine translation (Sutskever et al., 2014), etc.", "startOffset": 29, "endOffset": 53}, {"referenceID": 3, "context": "Besides, word-level representations are building blocks in producing phrase-level (Cho et al., 2014) and sentence-level (Kiros et al.", "startOffset": 82, "endOffset": 100}, {"referenceID": 8, "context": ", 2014) and sentence-level (Kiros et al., 2015) representations.", "startOffset": 27, "endOffset": 47}, {"referenceID": 2, "context": "Therefore, Chinese word embedding can be enhanced by its compositional character embeddings (Chen et al., 2015; Xu et al., 2016).", "startOffset": 92, "endOffset": 128}, {"referenceID": 20, "context": "Therefore, Chinese word embedding can be enhanced by its compositional character embeddings (Chen et al., 2015; Xu et al., 2016).", "startOffset": 92, "endOffset": 128}, {"referenceID": 21, "context": "By identifying the radical of a character, one obtains a rough meaning of that character, so it is used in learning Chinese word embedding (Yin et al., 2016) and character embedding (Sun et al.", "startOffset": 139, "endOffset": 157}, {"referenceID": 17, "context": ", 2016) and character embedding (Sun et al., 2014; Li et al., 2015).", "startOffset": 32, "endOffset": 67}, {"referenceID": 10, "context": ", 2016) and character embedding (Sun et al., 2014; Li et al., 2015).", "startOffset": 32, "endOffset": 67}, {"referenceID": 11, "context": "A similar idea was also used in (Liu et al., 2017) to help classifying wikipedia article titles into 12 categories.", "startOffset": 32, "endOffset": 50}, {"referenceID": 14, "context": "Then we use the models parallel to Skipgram (Mikolov et al., 2013a) or GloVe (Penning-", "startOffset": 44, "endOffset": 67}, {"referenceID": 9, "context": "Existing methods of producing word representations could be separated into two families (Levy et al., 2015): count-based family (Turney and Pantel, 2010; Bullinaria and Levy, 2007), and prediction-based family.", "startOffset": 88, "endOffset": 107}, {"referenceID": 19, "context": ", 2015): count-based family (Turney and Pantel, 2010; Bullinaria and Levy, 2007), and prediction-based family.", "startOffset": 28, "endOffset": 80}, {"referenceID": 1, "context": ", 2015): count-based family (Turney and Pantel, 2010; Bullinaria and Levy, 2007), and prediction-based family.", "startOffset": 28, "endOffset": 80}, {"referenceID": 0, "context": "Word representations can be obtained by training a neural-networkbased models (Bengio et al., 2003; Collobert et al., 2011).", "startOffset": 78, "endOffset": 123}, {"referenceID": 5, "context": "Word representations can be obtained by training a neural-networkbased models (Bengio et al., 2003; Collobert et al., 2011).", "startOffset": 78, "endOffset": 123}, {"referenceID": 14, "context": "Both continuous bag-of-words (CBOW) model and Skipgram model train with words and contexts in a sliding local context window (Mikolov et al., 2013a).", "startOffset": 125, "endOffset": 148}, {"referenceID": 15, "context": "In (Mikolov et al., 2013b), the authors introduced several techniques to improve the performance.", "startOffset": 3, "endOffset": 26}, {"referenceID": 16, "context": "Instead of using local context windows, (Pennington et al., 2014) proposed GloVe model.", "startOffset": 40, "endOffset": 65}, {"referenceID": 16, "context": "In (Pennington et al., 2014), the authors used a harmonic weighting function for co-occurrence count, that is, word-context pairs with distance d contributes 1 d to the global co-occurrence count.", "startOffset": 3, "endOffset": 28}, {"referenceID": 2, "context": "To improve Chinese word representations with sub-word information, character-enhanced word embedding (CWE) (Chen et al., 2015) in Section 2.", "startOffset": 107, "endOffset": 126}, {"referenceID": 21, "context": "Because radicals serve as a strong semantic indicator of a character, multigranularity embedding (MGE) (Yin et al., 2016) in Section 2.", "startOffset": 103, "endOffset": 121}, {"referenceID": 21, "context": "Based on CBOW and CWE, (Yin et al., 2016) proposed MGE, which predicts target word with its radical embeddings and modified word embeddings of context in CWE, as shown in Fig.", "startOffset": 23, "endOffset": 41}, {"referenceID": 13, "context": "A convAE (Masci et al., 2011) is used to reduce the dimensions of rendered character bitmaps and capture high-level features.", "startOffset": 9, "endOffset": 29}, {"referenceID": 3, "context": "9, a 2-layer Gated Recurrent Units (GRU) (Cho et al., 2014) network followed by 2 fully connected ELU (Clevert et al.", "startOffset": 41, "endOffset": 59}, {"referenceID": 4, "context": ", 2014) network followed by 2 fully connected ELU (Clevert et al., 2015) layers produces word representation ~ wi from input sequence {~gk} of word wi.", "startOffset": 50, "endOffset": 72}, {"referenceID": 15, "context": "In the training we use negative sampling and subsampling on frequent words from (Mikolov et al., 2013b).", "startOffset": 80, "endOffset": 103}, {"referenceID": 16, "context": "We follow (Pennington et al., 2014) and use ~ wi+ ~\u0303 wi for evaluations of word representation.", "startOffset": 10, "endOffset": 35}, {"referenceID": 22, "context": "Inspired by (Zeiler et al., 2011), layer-wise training was applied to our convAE.", "startOffset": 12, "endOffset": 33}, {"referenceID": 12, "context": "We further visualized the extracted character glyph features with tSNE (Maaten and Hinton, 2008).", "startOffset": 71, "endOffset": 96}, {"referenceID": 16, "context": "75 in (Pennington et al., 2014).", "startOffset": 6, "endOffset": 31}, {"referenceID": 2, "context": "Since there is little resource for traditional Chinese, we translated WordSim-240 and WordSim296 datasets provided by (Chen et al., 2015).", "startOffset": 118, "endOffset": 137}, {"referenceID": 6, "context": "Besides, we manually translated SimLex-999 (Hill et al., 2016) to traditional Chinese, and used it as the third testing dataset.", "startOffset": 43, "endOffset": 62}, {"referenceID": 21, "context": "As for MGE results, we were not able to reproduce the performance in (Yin et al., 2016).", "startOffset": 69, "endOffset": 87}, {"referenceID": 15, "context": "Furthermore, the analogy relation could be expressed by vector arithmetic of word representations as shown in (Mikolov et al., 2013b).", "startOffset": 110, "endOffset": 133}, {"referenceID": 2, "context": "As in the previous subsection, we translated the word analogy dataset in (Chen et al., 2015) to traditional.", "startOffset": 73, "endOffset": 92}, {"referenceID": 24, "context": "As more character-level models(Zheng et al., 2013; Kim, 2014; Zhang et al., 2015) are proposed in the NLP field, we believe glyph features could serve as an enhancement, and we will further examine the effect of glyph features on other tasks, such as word segmentation, POS tagging, dependency parsing, or downstream tasks such as text classification, or document retrieval.", "startOffset": 30, "endOffset": 81}, {"referenceID": 7, "context": "As more character-level models(Zheng et al., 2013; Kim, 2014; Zhang et al., 2015) are proposed in the NLP field, we believe glyph features could serve as an enhancement, and we will further examine the effect of glyph features on other tasks, such as word segmentation, POS tagging, dependency parsing, or downstream tasks such as text classification, or document retrieval.", "startOffset": 30, "endOffset": 81}, {"referenceID": 23, "context": "As more character-level models(Zheng et al., 2013; Kim, 2014; Zhang et al., 2015) are proposed in the NLP field, we believe glyph features could serve as an enhancement, and we will further examine the effect of glyph features on other tasks, such as word segmentation, POS tagging, dependency parsing, or downstream tasks such as text classification, or document retrieval.", "startOffset": 30, "endOffset": 81}], "year": 2017, "abstractText": "In this paper, we propose new methods to learn Chinese word representations. Chinese characters are composed of graphical components, which carry rich semantics. It is common for a Chinese learner to comprehend the meaning of a word from these graphical components. As a result, we propose models that enhance word representations by character glyphs. The character glyph features are directly learned from the bitmaps of characters by convolutional auto-encoder(convAE), and the glyph features improve Chinese word representations which are already enhanced by character embeddings. Another contribution in this paper is that we created several evaluation datasets in traditional Chinese and made them public.", "creator": "TeX"}}}