{"id": "1708.06555", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2017", "title": "Long-Short Range Context Neural Networks for Language Modeling", "abstract": "The goal of language modeling techniques is to capture the statistical and structural properties of natural languages from training corpora. This task typically involves the learning of short range dependencies, which generally model the syntactic properties of a language and/or long range dependencies, which are semantic in nature. We propose in this paper a new multi-span architecture, which separately models the short and long context information while it dynamically merges them to perform the language modeling task. This is done through a novel recurrent Long-Short Range Context (LSRC) network, which explicitly models the local (short) and global (long) context using two separate hidden states that evolve in time. This new architecture is an adaptation of the Long-Short Term Memory network (LSTM) to take into account the linguistic properties. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art language modeling techniques.", "histories": [["v1", "Tue, 22 Aug 2017 10:26:41 GMT  (110kb,D)", "http://arxiv.org/abs/1708.06555v1", "Published at EMNLP'16"]], "COMMENTS": "Published at EMNLP'16", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["youssef oualil", "mittul singh", "clayton greenberg", "dietrich klakow"], "accepted": true, "id": "1708.06555"}, "pdf": {"name": "1708.06555.pdf", "metadata": {"source": "CRF", "title": "Long-Short Range Context Neural Networks for Language Modeling", "authors": ["Youssef Oualil", "Mittul Singh", "Clayton Greenberg", "Dietrich Klakow"], "emails": ["firstname.lastname@lsv.uni-saarland.de"], "sections": [{"heading": null, "text": "The goal of language modeling techniques is to capture the statistical and structural properties of natural languages from training corpora. Typically, this task involves learning short-term dependencies, which generally model the syntactic properties of a language and / or long-term dependencies that are semantic in nature. In this paper, we propose a new multi-track architecture that models the short- and long-term context information separately while dynamically merging with each other to fulfill the language modeling task. This new architecture is an adaptation of the Long-Short Range Context (LSRC) network that explicitly models the local (short-) and global (long-term) context, using two separate hidden states that evolve over time. This new architecture is an adaptation of the Long-Short Term Memory Network (LSTM) to take into account the linguistic properties of the boundary text (s) that have been performed on the Penn and the Penn."}, {"heading": "1 Introduction", "text": "In fact, it is a purely reactionary project, which is a reactionary project, which is a reactionary diversionary manoeuvre, which is a reactionary diversionary manoeuvre, which is a reactionary diversionary manoeuvre, and which is a reactionary diversionary manoeuvre."}, {"heading": "2 Short vs Long Context Language Models", "text": "The aim of a language model is to estimate the probability distribution p (wT1) of the word sequences wT1 = w1, \u00b7 \u00b7 \u00b7, wT. With the help of the chain rule, this distribution can be expressed as p (wT1) = T \u0442t = 1 p (wt | wt \u2212 11) (1). This probability is generally approximated under various simplistic assumptions, which are typically derived on the basis of different linguistic observations. However, all of these assumptions aim to model the optimal context information, be it syntactical and / or semantic, in order to perform the word prediction. The resulting models can be roughly divided into two main categories: long-range and short-range context models. The rest of this section provides a brief overview of these categories with a special focus on neural network models (NN)."}, {"heading": "2.1 Short Range Context", "text": "This category includes models based on the Markov dependence assumption of the order N \u2212 1. That is, the prediction of the current word depends only on the last N \u2212 1 words in history. In this case, (1) the N \u2212 gram models (Rosenfeld, 2000; Kneser and Ney, 1995) and the FFNN model (Bengio et al., 2003) are the most popular methods adhering to this category, i.e. the N \u2212 gram models (Rosenfeld, 2000; Kneser and Ney, 1995) and the FFNN model (Bengio et al., 2003), which evaluates each of the terms contained in this product, i.e. p (wt \u2212 1t \u2212 N + 1), in a single bottom-up assessment of the net.Although these methods work well and are easy to learn, the natural languages they try to encode are not generated under a Markov model due to their dynamics and the long range they reveal."}, {"heading": "2.2 Long Range Context", "text": "To illustrate such triggering correlations over a large context, we use correlations defined over a distance d given by cd (w1, w2) = Pd (w1, w2) P (w1) P (w2). A value greater than 1 indicates that it is more likely that the word w1 w2 follows d at a distance than expected without the occurrence of w2. In Figure 1, we show the variation of this correlation for pronouns with distance d. It can be observed that seeing another \"he\" about twenty words after the first \"he\" is much more likely than the word \"she.\" A similar observation can be made for the word \"she.\" However, it is surprising that seeing \"he\" after \"he\" is three times more likely than seeing \"he\" after \"she.\""}, {"heading": "2.2.1 Elman-Type RNN-based LM", "text": "The classical RNN (Mikolov et al., 2010) estimates each of the product terms in (3) corresponding to Ht = f (Xt \u2212 1 + V \u00b7 Ht \u2212 1) (4) Pt = g (W \u00b7 Ht) (5), where Xt \u2212 1 is a continuous representation (i.e. embedding) of the word wt \u2212 1, V encodes the recurring connection weights and W the hidden connection weights. These parameters define the network and are learned during the training. Furthermore, f (\u00b7) is an activation function, whereas g (\u00b7) is the Softmax function. Figure (2) shows an example of the standard RNN architecture. Theoretically, the recurring connections of an RNN allow an unlimited cycle in the network and thus the modeling of a long context. In practice, however, Hai Son et al. (2012) have shown that this information changes rapidly over time and that in this paper we have experimentally confirmed with a NFFN report."}, {"heading": "2.2.2 Long-Short Term Memory Network", "text": "To mitigate the rapidly changing context problem in standard RNNs and to control the longevity of dependency models on the network, the LSTM architecture (Sundermeyer et al., 2012) introduces an internal memory state Ct, which explicitly controls the amount of information to forget or add to the network before estimating the current hidden state. Formally, this is done according to {i, f, o} t = \u03c3 (U i, f, o \u00b7 Xt \u2212 1 + V i, f, o \u00b7 Ht \u2212 1) (6) C-t = f (U c \u00b7 Xt \u2212 1 + V \u00b7 Ht \u2212 1) (7) Ct = ft Ct \u2212 1 + it C-t (8) Ht = ot f (Ct) (9) Pt = g (W \u00b7 Ht) (10) C-T = f (10), where the elementary multiplication operator is, C-t is the memory candidate, although the input code is not STT and explicite."}, {"heading": "3 Multi-Span Language Models", "text": "Attempts to learn and combine short- and long-term dependencies in language modeling led to so-called multispan LMs (Bellegarda, 1998a), the aim of which is to learn the various constraints that exist both locally and globally in a language, typically using two different models that learn the local and global context separately and then combine their resulting linguistic information to perform word prediction. For example, Bellegarda (1998b) suggested using latent semantics analysis (LSA) to capture the global context and then combine it with the standard N-gram models that capture the local context, while Mikolov and Zweig (2012) suggested modeling the global topic information using latent dirichlet allocation (LDA), which is then combined with an RNN-based LM. This idea is not specific to language modeling (NP), but was also used in other languages."}, {"heading": "3.1 Long-Short Range Context Network", "text": "After the Gedankenwelt in (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) n (D) er (D) er (D) er (D) er (D) er (D) er (D) er (D) n \"n\" n \"n\" n \"n n n n (D) er (D) n\" n \"n\" n \"n (D) er (D) n\" n \"n\" n n n n (D) n \"n\" n \"n (D) er (D) n\" n \"n\" n \"n (D) n\" n \"n\" n (D) er (D) n \"n\" n \"n\" n (D) n \"n\" n \"n (D) er (D) er (D) n) n\" n \"n\" n \"n\" n \"n\" n (D) er (D) (D) n) er (D) n \"n\" n \"n\" n) n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n (D) er (D) n) n \"n\" n \"er (D) (D) n\" n \"n) n\" n \"n\" n \"n\" er (D) (D) n \"n) n\" n \"n\" n \"n\" er (D) (D) n \"n\" n \"n) (D) n\" n \"n\" n \"n\" n \"er (D) (D)."}, {"heading": "3.2 Context Range Estimation", "text": "For many NLP applications, capturing global context information can be a critical component in the development of successful systems, largely due to the inherent nature of languages in which a single idea or topic can span a few sentences, paragraphs, or an entire document. LSA-like approaches take advantage of this property and aim to extract some hidden \"concepts\" that best explain the data in a low-dimensional \"semantic space.\" To some extent, the hidden layer of LSRC / LSTM can be viewed as a vector in a similar space, but the information stored in that vector changes continuously based on the words processed. Furthermore, interpreting its content is generally difficult. Alternatively, measuring the temporal correlation of this hidden vector can be used as an indicator of the network's ability to model short- and long-term context dependencies."}, {"heading": "4 Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experimental Setup", "text": "We evaluated the proposed architecture using two different benchmark tasks. The first set of experiments was conducted on the commonly used Penn Treebank (PTB) corpus, with sections 0-20 being used for training, while sections 21-22 and 23-24 are used for validation and testing, respectively. Vocabulary was limited to the most common 10k words, while the remaining words were mapped onto the tokens. To evaluate how the proposed approach on large corpora compares to other methods, we run a second set of experiments on the Large Text Compression Benchmark (LTCB) (Mahoney, 2011), which is based on the enwik9 dataset containing the first 109 bytes of enwiki20060303-pages-articles.xml."}, {"heading": "4.2 Baseline Models", "text": "The proposed LSRC architecture is compared with various LM approaches that model short- or long-term contexts, including the commonly used N-gram Kneser-Ney (KN) (Kneser and Ney, 1995) models with and without cache (Kuhn and De Mori, 1990), as well as various feedforward and recurrent neural architectures. For short-term (fixed) context models, we compare our method with 1) the FFN-based LM (Bengio et al., 2003), and 2) the Fixed-size Ordinally Forgetting Encoding (FOFE) approach that is implemented in (Zhang et al., 2015) as a sentence-based model. For these short-term context models, we report on the results of different history window sizes (1, 2, and 4). The 1st, 2nd, and 4th order NFE results were either available in (Zhang et al RRRal al al al al al kit al al al Fal, 2015 or free for use)."}, {"heading": "4.3 PTB Experiments", "text": "For the PTB experiments, the FFNN and FOFE models, we use a word embedding size of 200 each, whereas the hidden layer (s) size is fixed at 400, with all the hidden units each using the Rectified Linear Unit (ReLu), i.e., f (x) = max (0, x) as the activation function. We also use the same learning setting that is used in (Zhang et al., 2015). Namely, we use the stochastic gradient descent algorithm with a mini batch size of 200, the learning rate is set to 0.9, the weight loss is set to 4 \u00d7 10 \u2212 5, whereas training is performed in epochs. Weights initialization follows the normalized initialization in (Glorot and Bengio, 2010). Similar to (Mikolov et al), the learning rate is halved if no significant improvement in validation data probability is observed."}, {"heading": "4.4 LTCB Experiments", "text": "The results shown in Table 3 follow the same experimental setup proposed in (Zhang et al., 2015). Specifically, these results were obtained without the use of dynamics or weight degradation (due to the long training time required for this corpus), the mini-batch size was set at 400, the learning rate was set at 0.4, and the BPTT step was set at 5. FFNN and FOFE architectures use 2 hidden layers of size 600, whereas RNN, LSTM, and LSRC have a single hidden layer of size 600. Furthermore, the word embedding size was set to 200 for all models except RNN, which was set to 600. We also report results for an LSTM with 2 recursive layers, and for LSRC with an additional non-recursive layer of the LSRC."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we investigated the importance followed by the ability of standard neural networks to encode long-distance and short-distance dependencies for speech modeling tasks, and demonstrated that these models were not specifically designed to explicitly and separately capture these two linguistic information. As an alternative solution, we proposed a novel long-distance context network that takes advantage of the LSTM's ability to capture long-distance dependencies and combines them with a classical RNN network, which typically encodes a much shorter context range, capable of encoding short- and long-distance linguistic dependencies based on two separate network states that evolve over time. Experiments at PTB and the large LTCB corpus have shown that the proposed approach significantly exceeds the different state of neural network architectures, including LSTM and RNN, even when smaller architectures are used."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the Cluster of Excellence for Multimodal Computing and Interaction, the German Research Foundation (DFG) in the framework of CRC 1102, the EU FP7 Metalogue Project (funding agreement No. 611073) and the EU Malorca Project (funding agreement No. 698824)."}], "references": [{"title": "A multispan language modeling framework for large vocabulary speech recognition", "author": ["J.R. Bellegarda"], "venue": "IEEE Transactions on Speech and Audio Processing,", "citeRegEx": "Bellegarda.,? \\Q1998\\E", "shortCiteRegEx": "Bellegarda.", "year": 1998}, {"title": "Exploiting both local and global constraints for multispan statistical language modeling", "author": ["Jerome R. Bellegarda"], "venue": "In Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Bellegarda.,? \\Q1998\\E", "shortCiteRegEx": "Bellegarda.", "year": 1998}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A statistical approach to machine translation", "author": ["Brown et al.1990] Peter F. Brown", "John Cocke", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Fredrick Jelinek", "John D. Lafferty", "Robert L. Mercer", "Paul S. Roossin"], "venue": "Comput. Linguist.,", "citeRegEx": "Brown et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1990}, {"title": "Exact training of a neural syntactic language model", "author": ["Emami", "Jelinek2004] Ahmad Emami", "Frederick Jelinek"], "venue": "In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "Emami et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Emami et al\\.", "year": 2004}, {"title": "A joint language model with fine-grain syntactic tags", "author": ["Filimonov", "Harper2009] Denis Filimonov", "Mary P. Harper"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP), A meeting of SIGDAT,", "citeRegEx": "Filimonov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Filimonov et al\\.", "year": 2009}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Bengio2010] Xavier Glorot", "Yoshua Bengio"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Measuring the influence of long range dependencies with neural network language models", "author": ["Hai Son et al.2012] Le Hai Son", "Alexandre Allauzen", "Fran\u00e7ois Yvon"], "venue": "In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-", "citeRegEx": "Son et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Son et al\\.", "year": 2012}, {"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["S. Katz"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Katz.,? \\Q1987\\E", "shortCiteRegEx": "Katz.", "year": 1987}, {"title": "A cache-based natural language", "author": ["De Mori"], "venue": null, "citeRegEx": "Mori.,? \\Q1990\\E", "shortCiteRegEx": "Mori.", "year": 1990}], "referenceMentions": [{"referenceID": 3, "context": "A high quality Language Model (LM) is considered to be an integral component of many systems for speech and language technology applications, such as machine translation (Brown et al., 1990), speech recognition (Katz, 1987), etc.", "startOffset": 170, "endOffset": 190}, {"referenceID": 8, "context": ", 1990), speech recognition (Katz, 1987), etc.", "startOffset": 28, "endOffset": 40}, {"referenceID": 2, "context": "In order to capture these properties, classical LMs were typically developed as fixed (short) context techniques such as, the word count-based methods (Rosenfeld, 2000; Kneser and Ney, 1995), commonly known as N -gram language models, as well as the Feedforward Neural Networks (FFNN) (Bengio et al., 2003), which were introduced as an alternative to overcome the exponential growth of parameters required for larger context sizes in N -gram models.", "startOffset": 285, "endOffset": 306}, {"referenceID": 0, "context": "In order to overcome the short context constraint and capture long range dependencies known to be present in language, Bellegarda (1998a) proposed to use Latent Semantic Analysis (LSA) to capture the global context, and then combine it with the standard N -gram models, which capture the local context.", "startOffset": 119, "endOffset": 138}, {"referenceID": 0, "context": "In order to overcome the short context constraint and capture long range dependencies known to be present in language, Bellegarda (1998a) proposed to use Latent Semantic Analysis (LSA) to capture the global context, and then combine it with the standard N -gram models, which capture the local context. In a similar but more recent approach, Mikolov and Zweig (2012) showed that Recurrent Neural Network (RNN)-based LM performance can be significantly improved using an additional global topic information obtained using Latent Dirichlet Allocation (LDA).", "startOffset": 119, "endOffset": 367}, {"referenceID": 0, "context": "In order to overcome the short context constraint and capture long range dependencies known to be present in language, Bellegarda (1998a) proposed to use Latent Semantic Analysis (LSA) to capture the global context, and then combine it with the standard N -gram models, which capture the local context. In a similar but more recent approach, Mikolov and Zweig (2012) showed that Recurrent Neural Network (RNN)-based LM performance can be significantly improved using an additional global topic information obtained using Latent Dirichlet Allocation (LDA). In fact, although recurrent architectures theoretically allow the context to indefinitely cycle in the network, Hai Son et al. (2012) have shown that, in practice, this information changes quickly in the classical RNN (Mikolov et al.", "startOffset": 119, "endOffset": 690}, {"referenceID": 2, "context": "The most popular methods that subscribe in this category are the N -gram models (Rosenfeld, 2000; Kneser and Ney, 1995) as well as the FFNN model (Bengio et al., 2003), which estimates each of the terms involved in this product, i.", "startOffset": 146, "endOffset": 167}, {"referenceID": 7, "context": "In practice, however, Hai Son et al. (2012) have shown that this information changes quickly over time, and that it is experimentally equivalent to an 8-gram FFNN.", "startOffset": 26, "endOffset": 44}, {"referenceID": 0, "context": "The attempts to learn and combine short and long range dependencies in language modeling led to what is known as multi-span LMs (Bellegarda, 1998a). The goal of these models is to learn the various constraints, both local and global, that are present in a language. This is typically done using two different models, which separately learn the local and global context, and then combine their resulting linguistic information to perform the word prediction. For instance, Bellegarda (1998b) proposed to use Latent Semantics Analysis (LSA) to capture the global context, and then combine it with the standard N -gram models, which capture the local context, whereas Mikolov and Zweig (2012) proposed to model the global topic information using Latent Dirichlet Allocation (LDA), which is then combined with an RNN-based LM.", "startOffset": 129, "endOffset": 491}, {"referenceID": 0, "context": "The attempts to learn and combine short and long range dependencies in language modeling led to what is known as multi-span LMs (Bellegarda, 1998a). The goal of these models is to learn the various constraints, both local and global, that are present in a language. This is typically done using two different models, which separately learn the local and global context, and then combine their resulting linguistic information to perform the word prediction. For instance, Bellegarda (1998b) proposed to use Latent Semantics Analysis (LSA) to capture the global context, and then combine it with the standard N -gram models, which capture the local context, whereas Mikolov and Zweig (2012) proposed to model the global topic information using Latent Dirichlet Allocation (LDA), which is then combined with an RNN-based LM.", "startOffset": 129, "endOffset": 690}, {"referenceID": 0, "context": "The attempts to learn and combine short and long range dependencies in language modeling led to what is known as multi-span LMs (Bellegarda, 1998a). The goal of these models is to learn the various constraints, both local and global, that are present in a language. This is typically done using two different models, which separately learn the local and global context, and then combine their resulting linguistic information to perform the word prediction. For instance, Bellegarda (1998b) proposed to use Latent Semantics Analysis (LSA) to capture the global context, and then combine it with the standard N -gram models, which capture the local context, whereas Mikolov and Zweig (2012) proposed to model the global topic information using Latent Dirichlet Allocation (LDA), which is then combined with an RNN-based LM. This idea is not particular to language modeling but has been also used in other Natural Language Processing (NLP) tasks, e.g., Anastasakos et al. (2014) proposed to use a local/global model to perform a spoken language understanding task.", "startOffset": 129, "endOffset": 977}, {"referenceID": 2, "context": "For short (fixed) size context models, we compare our method to 1) the FFNNbased LM (Bengio et al., 2003), as well as 2) the Fixed-size Ordinally Forgetting Encoding (FOFE) approach, which is implemented in (Zhang et al.", "startOffset": 84, "endOffset": 105}], "year": 2017, "abstractText": null, "creator": "LaTeX with hyperref package"}}}