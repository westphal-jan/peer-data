{"id": "1708.07149", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2017", "title": "Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses", "abstract": "Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality. Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem. We present an evaluation model (ADEM) that learns to predict human-like scores to input responses, using a new dataset of human response scores. We show that the ADEM model's predictions correlate significantly, and at a level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and system-level. We also show that ADEM can generalize to evaluating dialogue models unseen during training, an important step for automatic dialogue evaluation.", "histories": [["v1", "Wed, 23 Aug 2017 18:56:00 GMT  (1485kb,D)", "http://arxiv.org/abs/1708.07149v1", "ACL 2017"]], "COMMENTS": "ACL 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["ryan lowe", "michael noseworthy", "iulian vlad serban", "nicolas angelard-gontier", "yoshua bengio", "joelle pineau"], "accepted": true, "id": "1708.07149"}, "pdf": {"name": "1708.07149.pdf", "metadata": {"source": "CRF", "title": "Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses", "authors": ["Ryan Lowe", "Michael Noseworthy", "Iulian V. Serban", "Nicolas A.-Gontier", "Yoshua Bengio", "Joelle Pineau"], "emails": [], "sections": [{"heading": null, "text": "Unfortunately, existing automated evaluation metrics are skewed and correlate very poorly with human assessments of response quality. However, an accurate automated evaluation process is critical for dialogue research because it allows rapid prototyping and testing of new models with less expensive human assessments. In response to this challenge, we formulate automated dialogue evaluation as a learning problem. We present an evaluation model (ADEM) that learns to predict human-like values in order to enter responses, using a new set of human response values. We show that the predictions of the ADEM model correlate significantly and at a much higher level than metrics like BLEU, with human assessments both at the utterance and at the system level. We also show that ADEM can generate to evaluate dialogue models that are not seen during the training, an important step for automated dialogue assessment.1"}, {"heading": "1 Introduction", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight,"}, {"heading": "2 Data Collection", "text": "To obtain a model for predicting human outcomes from dialog responses, we first collect a data set of human judgments (scores) from Twitter responses using the crowdsourcing platform Amazon Mechanical Turk (AMT).3 The goal is to obtain precise human outcomes for a variety of dialog contexts - covering the full range of response qualities. For example, the responses should include both relevant and irrelevant responses, both coherent and non-coherent responses, and so on. To achieve this diversity, we use candidate responses from several different models. The following (Liu et al., 2016) we use the following 4 sources of candidate responses: (1) an answer selected by a TF-IDF retrieval-based model, (2) an answer selected by the Dual Encoder (DE) (Lowe et al., 2015) an answer that we have created or modelled using the hierarchically recurring encoder decoder models."}, {"heading": "3 Technical Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Recurrent Neural Networks", "text": "Recurrent neural networks (RNNs) are a kind of neural network with time-delayed connections between internal units. This leads to the formation of a hidden state ht, which is updated for each input: ht = f (Whhht \u2212 1 + Wihxt), where Whh and Wih are parameter matrices, f is a nonlinear activation function like tanh, and xt is the input at a given time. The hidden state allows RNNs to better model sequential data such as speech. In this paper we look at RNNs that are augmented by long-term short-term memory units (LSTM) (Hochreiter and Schmidhuber, 1997). LSTMs add a series of gates to the RNN that allow it to update the hidden state. LSTMs are one of the best established methods for dealing with the vanishing gradient problem in recursive networks (Hochreiter, 1991; Bengio et al, 1994)."}, {"heading": "3.2 Word-Overlap Metrics", "text": "One of the most popular approaches to automatically evaluating the quality of dialog answers is to calculate their word overlap with the reference answer. In particular, the most popular metrics are the BLEU and METEOR values used for machine translation, and the ROUGE value used for automatic summary. Although these metrics tend to correlate with human judgments in their target areas, they have recently shown to be highly biased and correlate very poorly with human judgments for the evaluation of dialog words (Liu et al., 2016). We describe BLEU briefly here and provide a more detailed summary of word overlaps in the supplement material. BLEU BLEU et al, 2002) analyzes the common occurrence of N-grams in the reference and the proposed answers. It calculates the n-gram precision for the entire dataset, which is then multiplied by a random penalty to punish short translations."}, {"heading": "4 An Automatic Dialogue Evaluation", "text": "The question of whether it is a dialogue, or a dialogue, in which it is a dialogue that addresses both the context and the reference persons in relation to the way in which we address the way in which we refer to the model. (1) The question of the meaning of the model in which we move is not the question of the way in which we apply it. (2) The question of the way in which we apply it is the question of the way in which we address it. (2) The question of the way in which we apply it is not the question of the way in which we apply it. (2) The question of the way in which we apply it is the question of the way in which we address it. (3) The question of the way in which we ask the question is the question of the way in which we ask the question."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experimental Procedure", "text": "To reduce the effective vocabulary size, we use a byte-pair encoding (BPE) (Gage, 1994; Sennrich et al., 2015), which divides each word into subwords or characters. We also use the layer normalization (Ba et al., 2016) for the hierarchical encoder, which we found worked better at the task of dialogue generation than the associated recurring batch normalization (Ioffe and Szegedy, 2015; Cooijmans et al., 2016). To train the VHRED model, we used several of the same techniques found in (Serban et al., 2016b) and (Bowman et al., 2016): We drop words in the decoder at a fixed rate of 25%, and we approximate the KL divergence term linearly linearly from 0 to 1 over the first 60,000 stacks. We use Adam as our optimizer (we use a partial result from 2014, and we do not use a result from L)."}, {"heading": "5.2 Results", "text": "First, we present new correlations at the level of correlations that arise at the level of existing word overlap metrics (in addition to the results with the embedding of baselines and ADEM, in Table 2. Baseline metrics are evaluated on the entire data set of 4,104 responses to provide the most accurate answers to the results. 5 We measure the correlation for ADEM at the level of validation and test sets that represent both the Spearman correlation and an analysis of the response data from (Liu et al., 2016), where pre-processing is standardized by having the first speakers > \"to 4We present both the Spearman correlation (calculated at ranks, representing monotonic relationships) and Pearson correlation (calculated on true values, representing linear relationships) scores.5Note that our word overlap correlations in Table 2 are also lower than those presented."}, {"heading": "6 Related Work", "text": "In terms of our approach, the literature on novel methods for evaluating machine translation systems is, in particular, through the WMT evaluation task (Callison-Burch et al., 2011; Macha \u0301 cek and Bojar, 2014; Stanojevic et al., 2015), but in particular, we need to evaluate our results in the context of the conversation that is not necessary in translation. A related work has also been done to assess the quality of responses in chat-oriented dialogue systems. (DeVault et al., 2011) train an automatic political evaluation of 19 structured role games enriched with para phrases and external referee annotations. (Gandhe and Traum, 2016)"}, {"heading": "7 Discussion", "text": "We use the Twitter Corpus to train our models, as it contains a wide range of non-task-oriented conversations, and it has been used to train many state-of-the-art models. However, our model could easily be extended to other general-purpose datasets such as Reddit once similar pre-trained models are publicly available. Such models are even necessary to create a test set in a new area that will help us determine whether ADEM generalizes to related dialogue domains. We will leave the study of ADEM's domain transferability for future work. The evaluation model proposed in this paper favors dialogue models that generate responses that are highly appropriate to humans. It is likely that this feature does not fully capture the desired end goal of chatbot systems. A problem with construction models to approximate human assessment quality of response systems is the problem of generic responses. Since humans often provide high scores for generic responses, which, due to their appropriateness, are important for 2016, is that many of these responses will be contextual to Li's work."}, {"heading": "Appendix A: Further Notes on Crowdsourcing Data Collection", "text": "We conducted two rounds of AMT experiments. We first asked AMT workers to provide an appropriate continuation of a Twitter dialog (i.e., to generate the next answer given the context of a conversation), and each survey contained 20 questions, including an attention test question. Workers were instructed to generate longer answers to avoid simple one-word answers. In total, we received about 2,000 specific answers on a scale of 1 (low quality) to 5 (high quality). Each user was asked to rate 4 answers from 50 different contexts. We included four additional attention test questions and a set of five contexts given to each participant to evaluate the inter-annotator agreement. We removed all users who either failed an attention check or scored lower."}, {"heading": "Appendix B: Metric Description", "text": "BLEU BLEU et al., 2002) analyzes the co-occurrences of n-grams in the basic truth and the proposed answers. It first calculates an n-gram precision for the entire dataset: Pn (r, r) is the number of n-grams k in r. Note that the minimum requirement in this equation is the number of co-occurrences of n-grams k (k, ri). (k, r) The number of co-occurrences of n-grams k (k, ri) is the number of co-occurrences of n-grams k between the basic truth answer r and the proposed answer r (k, r) is the number of appearances of k in both answers. To avoid the disadvantages of using a precision score, namely that it prefers shorter (candidate) judgments, the authors introduce a Brevity penalty. EU-N, where the maximum length of BLEU is defined as bb: n-length."}, {"heading": "Appendix C: Latent Variable Hierarchical Recurrent Encoder-Decoder (VHRED)", "text": "The VHRED model is an extension of the original hierarchical recurrent encoder decoder (HRED) context (HRED) model (Serban et al., 2016a) with an additional component: a high-dimensional stochastic latent variable in each dialog context. The dialog contexts are encrypted into a vector representation that uses the statement and context-level RNs from our encoder. VHRED samples are a multivariate Gaussian variable that is provided, along with the context summary vector, as input to the decoderRNN, which in turn generates the response wordby-word. We use representations from the VHRED model as it produces more diverse and coherent responses compared to its HRED counterpart. The VHRED model is trained to maximize a lower probability of the next response."}, {"heading": "Appendix D: Experiments & results", "text": "In fact, it is the case that we behaved in a way and manner in which we have experienced them in the past, in a way in which we have experienced them in the past and in the present, in a way in which we have experienced them in the past, how they have behaved in the past and in the present \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}], "references": [{"title": "Regression for sentence-level mt evaluation with pseudo references", "author": ["Joshua Albrecht", "Rebecca Hwa."], "venue": "ACL.", "citeRegEx": "Albrecht and Hwa.,? 2007", "shortCiteRegEx": "Albrecht and Hwa.", "year": 2007}, {"title": "Semi-formal evaluation of conversational characters", "author": ["Ron Artstein", "Sudeep Gandhe", "Jillian Gerten", "Anton Leuski", "David Traum."], "venue": "Languages: From Formal to Natural, Springer, pages 22\u201335.", "citeRegEx": "Artstein et al\\.,? 2009", "shortCiteRegEx": "Artstein et al\\.", "year": 2009}, {"title": "Layer normalization", "author": ["Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton."], "venue": "arXiv preprint arXiv:1607.06450 .", "citeRegEx": "Ba et al\\.,? 2016", "shortCiteRegEx": "Ba et al\\.", "year": 2016}, {"title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments", "author": ["Satanjeev Banerjee", "Alon Lavie."], "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or sum-", "citeRegEx": "Banerjee and Lavie.,? 2005", "shortCiteRegEx": "Banerjee and Lavie.", "year": 2005}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."], "venue": "IEEE transactions on neural networks 5(2):157\u2013166.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Generating sentences from a continuous space", "author": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio."], "venue": "COLING .", "citeRegEx": "Bowman et al\\.,? 2016", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Findings of the 2011 workshop on statistical machine translation", "author": ["Chris Callison-Burch", "Philipp Koehn", "Christof Monz", "Omar F Zaidan."], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation. Association for Computational", "citeRegEx": "Callison.Burch et al\\.,? 2011", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2011}, {"title": "A systematic comparison of smoothing techniques for sentencelevel bleu", "author": ["Boxing Chen", "Colin Cherry."], "venue": "ACL 2014 page 362.", "citeRegEx": "Chen and Cherry.,? 2014", "shortCiteRegEx": "Chen and Cherry.", "year": 2014}, {"title": "Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit", "author": ["Jacob Cohen."], "venue": "Psychological bulletin 70(4):213.", "citeRegEx": "Cohen.,? 1968", "shortCiteRegEx": "Cohen.", "year": 1968}, {"title": "Recurrent batch normalization", "author": ["Tim Cooijmans", "Nicolas Ballas", "C\u00e9sar Laurent", "Aaron Courville."], "venue": "arXiv preprint arXiv:1603.09025 .", "citeRegEx": "Cooijmans et al\\.,? 2016", "shortCiteRegEx": "Cooijmans et al\\.", "year": 2016}, {"title": "Toward learning and evaluation of dialogue policies with text examples", "author": ["David DeVault", "Anton Leuski", "Kenji Sagae."], "venue": "Proceedings of the SIGDIAL 2011 Conference. Association for Computational Linguistics, pages 39\u201348.", "citeRegEx": "DeVault et al\\.,? 2011", "shortCiteRegEx": "DeVault et al\\.", "year": 2011}, {"title": "Tweet2vec: Character-based distributed representations for social media", "author": ["Bhuwan Dhingra", "Zhong Zhou", "Dylan Fitzpatrick", "Michael Muehl", "William W Cohen."], "venue": "arXiv preprint arXiv:1605.03481 .", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Censoring representations with an adversary", "author": ["Harrison Edwards", "Amos Storkey."], "venue": "ICLR .", "citeRegEx": "Edwards and Storkey.,? 2016", "shortCiteRegEx": "Edwards and Storkey.", "year": 2016}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["Salah El Hihi", "Yoshua Bengio."], "venue": "NIPS. Citeseer, volume 400, page 409.", "citeRegEx": "Hihi and Bengio.,? 1995", "shortCiteRegEx": "Hihi and Bengio.", "year": 1995}, {"title": "A new algorithm for data compression", "author": ["Philip Gage."], "venue": "The C Users Journal 12(2):23\u201338.", "citeRegEx": "Gage.,? 1994", "shortCiteRegEx": "Gage.", "year": 1994}, {"title": "deltableu: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv", "citeRegEx": "Galley et al\\.,? 2015", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "A semiautomated evaluation metric for dialogue model coherence", "author": ["Sudeep Gandhe", "David Traum."], "venue": "Situated Dialog in Speech-Based Human-Computer Interaction, Springer, pages 217\u2013 225.", "citeRegEx": "Gandhe and Traum.,? 2016", "shortCiteRegEx": "Gandhe and Traum.", "year": 2016}, {"title": "Reval: A simple and effective machine translation evaluation metric based on recurrent neural networks", "author": ["Rohit Gupta", "Constantin Orasan", "Josef van Genabith."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language", "citeRegEx": "Gupta et al\\.,? 2015", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Evaluating coherence in open domain conversational systems", "author": ["Ryuichiro Higashinaka", "Toyomi Meguro", "Kenji Imamura", "Hiroaki Sugiyama", "Toshiro Makino", "Yoshihiro Matsuo."], "venue": "INTERSPEECH. pages 130\u2013134.", "citeRegEx": "Higashinaka et al\\.,? 2014", "shortCiteRegEx": "Higashinaka et al\\.", "year": 2014}, {"title": "Untersuchungen zu dynamischen neuronalen netzen", "author": ["Sepp Hochreiter."], "venue": "Diploma, Technische Universit\u00e4t M\u00fcnchen page 91.", "citeRegEx": "Hochreiter.,? 1991", "shortCiteRegEx": "Hochreiter.", "year": 1991}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy."], "venue": "arXiv preprint arXiv:1502.03167 .", "citeRegEx": "Ioffe and Szegedy.,? 2015", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Smart reply: Automated response suggestion for email", "author": ["Anjuli Kannan", "Karol Kurach", "Sujith Ravi", "Tobias Kaufmann", "Andrew Tomkins", "Balint Miklos", "Greg Corrado", "L\u00e1szl\u00f3 Luk\u00e1cs", "Marina Ganea", "Peter Young"], "venue": null, "citeRegEx": "Kannan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2016}, {"title": "Adversarial evaluation of dialogue models", "author": ["Anjuli Kannan", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1701.08198 .", "citeRegEx": "Kannan and Vinyals.,? 2017", "shortCiteRegEx": "Kannan and Vinyals.", "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in Neural Information Processing Systems. pages 3276\u20133284.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint arXiv:1510.03055 .", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A persona-based neural conversation model", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint arXiv:1603.06155 .", "citeRegEx": "Li et al\\.,? 2016a", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Learning to decode for future success", "author": ["Jiwei Li", "Will Monroe", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1701.06549 .", "citeRegEx": "Li et al\\.,? 2017", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1606.01541 .", "citeRegEx": "Li et al\\.,? 2016b", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Text summarization branches out: Proceedings of the ACL-04 workshop. Barcelona, Spain, volume 8.", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau."], "venue": "arXiv preprint", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1506.08909 .", "citeRegEx": "Lowe et al\\.,? 2015", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Results of the wmt14 metrics shared task", "author": ["Matou\u0161 Mach\u00e1cek", "Ondrej Bojar."], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation. Citeseer, pages 293\u2013301.", "citeRegEx": "Mach\u00e1cek and Bojar.,? 2014", "shortCiteRegEx": "Mach\u00e1cek and Bojar.", "year": 2014}, {"title": "For sympathetic ear, more chinese turn to smartphone program", "author": ["J. Markoff", "P. Mozur."], "venue": "NY Times .", "citeRegEx": "Markoff and Mozur.,? 2015", "shortCiteRegEx": "Markoff and Mozur.", "year": 2015}, {"title": "Memo: towards automatic usability evaluation of spoken dialogue services by user error", "author": ["Sebastian M\u00f6ller", "Roman Englert", "Klaus-Peter Engelbrecht", "Verena Vanessa Hafner", "Anthony Jameson", "Antti Oulasvirta", "Alexander Raake", "Norbert Reithinger"], "venue": null, "citeRegEx": "M\u00f6ller et al\\.,? \\Q2006\\E", "shortCiteRegEx": "M\u00f6ller et al\\.", "year": 2006}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Principal components analysis", "author": ["Karl Pearson."], "venue": "The London, Edinburgh and Dublin Philosophical Magazine and Journal 6(2):566.", "citeRegEx": "Pearson.,? 1901", "shortCiteRegEx": "Pearson.", "year": 1901}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan."], "venue": "Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguistics, pages 583\u2013593.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1508.07909 .", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "AAAI. pages 3776\u20133784.", "citeRegEx": "Serban et al\\.,? 2016a", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1605.06069 .", "citeRegEx": "Serban et al\\.,? 2016b", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "arXiv preprint arXiv:1503.02364 .", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Overview of the ntcir-12 short text conversation task", "author": ["Lifeng Shang", "Tetsuya Sakai", "Zhengdong Lu", "Hang Li", "Ryuichiro Higashinaka", "Yusuke Miyao."], "venue": "Proceedings of NTCIR-12 pages 473\u2013484.", "citeRegEx": "Shang et al\\.,? 2016", "shortCiteRegEx": "Shang et al\\.", "year": 2016}, {"title": "A hierarchical recurrent encoderdecoder for generative context-aware query suggestion", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob Grue Simonsen", "JianYun Nie."], "venue": "Proceedings of the 24th ACM International", "citeRegEx": "Sordoni et al\\.,? 2015a", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint", "citeRegEx": "Sordoni et al\\.,? 2015b", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Results of the wmt15 metrics shared task", "author": ["Milo\u0161 Stanojevic", "Amir Kamran", "Philipp Koehn", "Ondrej Bojar."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation. pages 256\u2013273.", "citeRegEx": "Stanojevic et al\\.,? 2015", "shortCiteRegEx": "Stanojevic et al\\.", "year": 2015}, {"title": "Computing machinery and intelligence", "author": ["Alan M Turing."], "venue": "Mind 59(236):433\u2013460.", "citeRegEx": "Turing.,? 1950", "shortCiteRegEx": "Turing.", "year": 1950}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "arXiv preprint arXiv:1506.05869 .", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Paradise: A framework for evaluating spoken dialogue agents", "author": ["Marilyn A Walker", "Diane J Litman", "Candace A Kamm", "Alicia Abella."], "venue": "Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics. Associa-", "citeRegEx": "Walker et al\\.,? 1997", "shortCiteRegEx": "Walker et al\\.", "year": 1997}, {"title": "ELIZAa computer program for the study of natural language communication between man and machine", "author": ["J. Weizenbaum."], "venue": "Communications of the ACM 9(1):36\u201345.", "citeRegEx": "Weizenbaum.,? 1966", "shortCiteRegEx": "Weizenbaum.", "year": 1966}, {"title": "Problematic situation analysis and automatic recognition for chi-nese online conversational system", "author": ["Yang Xiang", "Yaoyun Zhang", "Xiaoqiang Zhou", "Xiaolong Wang", "Yang Qin."], "venue": "Proc. CLP pages 43\u2013", "citeRegEx": "Xiang et al\\.,? 2014", "shortCiteRegEx": "Xiang et al\\.", "year": 2014}, {"title": "Strategy and policy learning for nontask-oriented conversational systems", "author": ["Zhou Yu", "Ziyu Xu", "Alan W Black", "Alex I Rudnicky."], "venue": "17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. page 404.", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Metric Description BLEU BLEU (Papineni et al., 2002) analyzes the co-occurrences of n-grams in the ground truth and the proposed responses. It first computes an n-gram precision for the whole dataset", "author": ["B Appendix"], "venue": null, "citeRegEx": "Appendix,? \\Q2002\\E", "shortCiteRegEx": "Appendix", "year": 2002}], "referenceMentions": [{"referenceID": 47, "context": "Building systems that can naturally and meaningfully converse with humans has been a central goal of artificial intelligence since the formulation of the Turing test (Turing, 1950).", "startOffset": 166, "endOffset": 180}, {"referenceID": 50, "context": "ing statements or asking questions (Weizenbaum, 1966).", "startOffset": 35, "endOffset": 53}, {"referenceID": 22, "context": "pact in industry, including Google\u2019s Smart Reply system (Kannan et al., 2016), and Microsoft\u2019s Xiaoice chatbot (Markoff and Mozur, 2015), which has over 20 million users.", "startOffset": 56, "endOffset": 77}, {"referenceID": 34, "context": ", 2016), and Microsoft\u2019s Xiaoice chatbot (Markoff and Mozur, 2015), which has over 20 million users.", "startOffset": 41, "endOffset": 66}, {"referenceID": 32, "context": "In the case of chatbots designed for specific conversation domains, it may also be difficult to find sufficient human evaluators with appropriate background in the topic (Lowe et al., 2015).", "startOffset": 170, "endOffset": 189}, {"referenceID": 36, "context": "The most widely used metric for evaluating such dialogue systems is BLEU (Papineni et al., 2002), a metric measuring word overlaps originally developed for machine translation.", "startOffset": 73, "endOffset": 96}, {"referenceID": 31, "context": "are biased and correlate poorly with human judgements of response quality (Liu et al., 2016).", "startOffset": 74, "endOffset": 92}, {"referenceID": 38, "context": "Despite this, many researchers still use BLEU to evaluate their dialogue models (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a), as there are few alternatives available that correlate with human judgements.", "startOffset": 80, "endOffset": 180}, {"referenceID": 45, "context": "Despite this, many researchers still use BLEU to evaluate their dialogue models (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a), as there are few alternatives available that correlate with human judgements.", "startOffset": 80, "endOffset": 180}, {"referenceID": 26, "context": "Despite this, many researchers still use BLEU to evaluate their dialogue models (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a), as there are few alternatives available that correlate with human judgements.", "startOffset": 80, "endOffset": 180}, {"referenceID": 15, "context": "Despite this, many researchers still use BLEU to evaluate their dialogue models (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a), as there are few alternatives available that correlate with human judgements.", "startOffset": 80, "endOffset": 180}, {"referenceID": 27, "context": "Despite this, many researchers still use BLEU to evaluate their dialogue models (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a), as there are few alternatives available that correlate with human judgements.", "startOffset": 80, "endOffset": 180}, {"referenceID": 31, "context": "Following (Liu et al., 2016), we use the following 4 sources of candidate responses: (1) a response selected by a TF-IDF retrieval-based model, (2) a response selected by the Dual Encoder (DE) (Lowe et al.", "startOffset": 10, "endOffset": 28}, {"referenceID": 32, "context": ", 2016), we use the following 4 sources of candidate responses: (1) a response selected by a TF-IDF retrieval-based model, (2) a response selected by the Dual Encoder (DE) (Lowe et al., 2015), (3) a response generated using the hierarchical recurrent encoder-decoder (HRED) model (Serban et al.", "startOffset": 172, "endOffset": 191}, {"referenceID": 40, "context": ", 2015), (3) a response generated using the hierarchical recurrent encoder-decoder (HRED) model (Serban et al., 2016a), and (4) human-generated responses.", "startOffset": 96, "endOffset": 118}, {"referenceID": 38, "context": "This is why we use the Twitter Corpus (Ritter et al., 2011), as such models are pre-trained and readily available.", "startOffset": 38, "endOffset": 59}, {"referenceID": 32, "context": "the very specific Ubuntu Dialogue Corpus (Lowe et al., 2015) \u2014 and therefore the model may also be suited to other chit-chat domains.", "startOffset": 41, "endOffset": 60}, {"referenceID": 20, "context": "In this paper, we consider RNNs augmented with long-short term memory (LSTM) units (Hochreiter and Schmidhuber, 1997).", "startOffset": 83, "endOffset": 117}, {"referenceID": 19, "context": "the vanishing gradient problem in recurrent networks (Hochreiter, 1991; Bengio et al., 1994).", "startOffset": 53, "endOffset": 92}, {"referenceID": 4, "context": "the vanishing gradient problem in recurrent networks (Hochreiter, 1991; Bengio et al., 1994).", "startOffset": 53, "endOffset": 92}, {"referenceID": 31, "context": "tend to correlate with human judgements in their target domains, they have recently been shown to highly biased and correlate very poorly with human judgements for dialogue response evaluation (Liu et al., 2016).", "startOffset": 193, "endOffset": 211}, {"referenceID": 36, "context": "BLEU BLEU (Papineni et al., 2002) analyzes the co-occurrences of n-grams in the reference and the proposed responses.", "startOffset": 10, "endOffset": 33}, {"referenceID": 1, "context": "However, in dialogue, the set of appropriate responses given a context is much larger (Artstein et al., 2009); in other words, there is a very high response diversity that is un-", "startOffset": 86, "endOffset": 109}, {"referenceID": 44, "context": "The hierarchical RNN encoder in our model consists of two layers of RNNs (El Hihi and Bengio, 1995; Sordoni et al., 2015a).", "startOffset": 73, "endOffset": 122}, {"referenceID": 40, "context": "This hierarchical structure is useful for incorporating information from early utterances in the context (Serban et al., 2016a).", "startOffset": 105, "endOffset": 127}, {"referenceID": 41, "context": "The dialogue model we employ for pre-training is the latent variable hierarchical recurrent encoderdecoder (VHRED) model (Serban et al., 2016b), shown in Figure 3.", "startOffset": 121, "endOffset": 143}, {"referenceID": 40, "context": "The VHRED model is an extension of the original hierarchical recurrent encoderdecoder (HRED) model (Serban et al., 2016a) with a turn-level stochastic latent variable.", "startOffset": 99, "endOffset": 121}, {"referenceID": 14, "context": "In order to reduce the effective vocabulary size, we use byte pair encoding (BPE) (Gage, 1994; Sennrich et al., 2015), which splits each word into sub-words or characters.", "startOffset": 82, "endOffset": 117}, {"referenceID": 39, "context": "In order to reduce the effective vocabulary size, we use byte pair encoding (BPE) (Gage, 1994; Sennrich et al., 2015), which splits each word into sub-words or characters.", "startOffset": 82, "endOffset": 117}, {"referenceID": 2, "context": "ization (Ba et al., 2016) for the hierarchical encoder, which we found worked better at the task of dialogue generation than the related recurrent batch normalization (Ioffe and Szegedy, 2015; Cooijmans et al.", "startOffset": 8, "endOffset": 25}, {"referenceID": 21, "context": ", 2016) for the hierarchical encoder, which we found worked better at the task of dialogue generation than the related recurrent batch normalization (Ioffe and Szegedy, 2015; Cooijmans et al., 2016).", "startOffset": 149, "endOffset": 198}, {"referenceID": 9, "context": ", 2016) for the hierarchical encoder, which we found worked better at the task of dialogue generation than the related recurrent batch normalization (Ioffe and Szegedy, 2015; Cooijmans et al., 2016).", "startOffset": 149, "endOffset": 198}, {"referenceID": 41, "context": "To train the VHRED model, we employed several of the same techniques found in (Serban et al., 2016b) and (Bowman et al.", "startOffset": 78, "endOffset": 100}, {"referenceID": 5, "context": ", 2016b) and (Bowman et al., 2016): we drop words in the decoder with a fixed rate of 25%, and we anneal the KL-divergence term linearly from 0 to 1 over the first 60,000 batches.", "startOffset": 13, "endOffset": 34}, {"referenceID": 24, "context": "We use Adam as our optimizer (Kingma and Ba, 2014).", "startOffset": 29, "endOffset": 50}, {"referenceID": 41, "context": "tendency to give a higher rating to shorter responses than to longer responses (Serban et al., 2016b), as shorter responses are often more generic and thus are more likely to be suitable to the context.", "startOffset": 79, "endOffset": 101}, {"referenceID": 37, "context": "Thus, after training VHRED, we use principal component analysis (PCA) (Pearson, 1901) to reduce the dimensionality of the context, model response, and reference response embeddings to n.", "startOffset": 70, "endOffset": 85}, {"referenceID": 11, "context": "\u2018ADEM (T2V)\u2019 indicates ADEM with tweet2vec embeddings (Dhingra et al., 2016), and \u2018VHRED\u2019 indicates the", "startOffset": 54, "endOffset": 76}, {"referenceID": 31, "context": "We also conduct an analysis of the response data from (Liu et al., 2016), where the pre-processing is standardized by removing \u2018<first speaker>\u2019 to-", "startOffset": 54, "endOffset": 72}, {"referenceID": 15, "context": "Note that our word-overlap correlation results in Table 2 are also lower than those presented in (Galley et al., 2015).", "startOffset": 97, "endOffset": 118}, {"referenceID": 31, "context": "observe from both this data, and the new data in Table 2, that the correlations for the word-overlap metrics are even lower than estimated in previous studies (Liu et al., 2016; Galley et al., 2015).", "startOffset": 159, "endOffset": 198}, {"referenceID": 15, "context": "observe from both this data, and the new data in Table 2, that the correlations for the word-overlap metrics are even lower than estimated in previous studies (Liu et al., 2016; Galley et al., 2015).", "startOffset": 159, "endOffset": 198}, {"referenceID": 38, "context": "In particular, this is the case for BLEU-4, which has frequently been used for dialogue response evaluation (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a).", "startOffset": 108, "endOffset": 208}, {"referenceID": 45, "context": "In particular, this is the case for BLEU-4, which has frequently been used for dialogue response evaluation (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a).", "startOffset": 108, "endOffset": 208}, {"referenceID": 26, "context": "In particular, this is the case for BLEU-4, which has frequently been used for dialogue response evaluation (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a).", "startOffset": 108, "endOffset": 208}, {"referenceID": 15, "context": "In particular, this is the case for BLEU-4, which has frequently been used for dialogue response evaluation (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a).", "startOffset": 108, "endOffset": 208}, {"referenceID": 27, "context": "In particular, this is the case for BLEU-4, which has frequently been used for dialogue response evaluation (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a).", "startOffset": 108, "endOffset": 208}, {"referenceID": 11, "context": "We also compare with ADEM using tweet2vec embeddings (Dhingra et al., 2016).", "startOffset": 53, "endOffset": 75}, {"referenceID": 11, "context": "a bidirectional GRU on a Twitter dataset for hashtag prediction (Dhingra et al., 2016).", "startOffset": 64, "endOffset": 86}, {"referenceID": 36, "context": "99 on 5 models in the translation domain (Papineni et al., 2002).", "startOffset": 41, "endOffset": 64}, {"referenceID": 0, "context": "In particular, (Albrecht and Hwa, 2007; Gupta et al., 2015) have proposed to evaluate machine translation systems using Regression and Tree-LSTMs respectively.", "startOffset": 15, "endOffset": 59}, {"referenceID": 17, "context": "In particular, (Albrecht and Hwa, 2007; Gupta et al., 2015) have proposed to evaluate machine translation systems using Regression and Tree-LSTMs respectively.", "startOffset": 15, "endOffset": 59}, {"referenceID": 10, "context": "(DeVault et al., 2011) train an automatic dialogue policy evaluation metric from 19 structured role-playing sessions, enriched with paraphrases and external referee annotations.", "startOffset": 0, "endOffset": 22}, {"referenceID": 16, "context": "(Gandhe and Traum, 2016) propose a semi-automatic eval-", "startOffset": 0, "endOffset": 24}, {"referenceID": 51, "context": "7 (Xiang et al., 2014) propose a framework to predict utterance-level problematic situations in a dataset of Chinese dialogues using intent and sen-", "startOffset": 2, "endOffset": 22}, {"referenceID": 18, "context": "Finally, (Higashinaka et al., 2014) train a classifier to distinguish user utterances from system-generated utterances using various dialogue features, such as dialogue acts, question types, and predicate-argument structures.", "startOffset": 9, "endOffset": 35}, {"referenceID": 29, "context": "For example, (Li et al., 2016b) use features related to ease of answering and information flow, and (Yu et al.", "startOffset": 13, "endOffset": 31}, {"referenceID": 52, "context": ", 2016b) use features related to ease of answering and information flow, and (Yu et al., 2016) use metrics related to turn-level appropriateness and conversational depth.", "startOffset": 77, "endOffset": 94}, {"referenceID": 49, "context": "These methods include the PARADISE framework (Walker et al., 1997) and MeMo (M\u00f6ller", "startOffset": 45, "endOffset": 66}, {"referenceID": 43, "context": "Since humans often provide high scores to generic responses due to their appropriateness for many given contexts (Shang et al., 2016), a model trained to predict these scores will exhibit the same behaviour.", "startOffset": 113, "endOffset": 133}, {"referenceID": 12, "context": "This could be done, for example, by censoring ADEM\u2019s representations (Edwards and Storkey, 2016) such that they do not contain any information about length.", "startOffset": 69, "endOffset": 96}, {"referenceID": 23, "context": "evaluation model (Kannan and Vinyals, 2017; Li et al., 2017) that assigns a score based on how easy it is to distinguish the dialogue model responses from human responses.", "startOffset": 17, "endOffset": 60}, {"referenceID": 28, "context": "evaluation model (Kannan and Vinyals, 2017; Li et al., 2017) that assigns a score based on how easy it is to distinguish the dialogue model responses from human responses.", "startOffset": 17, "endOffset": 60}], "year": 2017, "abstractText": "Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality. Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem. We present an evaluation model (ADEM) that learns to predict human-like scores to input responses, using a new dataset of human response scores. We show that the ADEM model\u2019s predictions correlate significantly, and at a level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and systemlevel. We also show that ADEM can generalize to evaluating dialogue models unseen during training, an important step for automatic dialogue evaluation.1", "creator": "LaTeX with hyperref package"}}}