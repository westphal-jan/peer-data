{"id": "1708.07367", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2017", "title": "Mixing Time Estimation in Reversible Markov Chains from a Single Sample Path", "abstract": "The spectral gap $\\gamma$ of a finite, ergodic, and reversible Markov chain is an important parameter measuring the asymptotic rate of convergence. In applications, the transition matrix $P$ may be unknown, yet one sample of the chain up to a fixed time $n$ may be observed. We consider here the problem of estimating $\\gamma$ from this data. Let $\\pi$ be the stationary distribution of $P$, and $\\pi_\\star = \\min_x \\pi(x)$. We show that if $n = \\tilde{O}\\bigl(\\frac{1}{\\gamma \\pi_\\star}\\bigr)$, then $\\gamma$ can be estimated to within multiplicative constants with high probability. When $\\pi$ is uniform on $d$ states, this matches (up to logarithmic correction) a lower bound of $\\tilde{\\Omega}\\bigl(\\frac{d}{\\gamma}\\bigr)$ steps required for precise estimation of $\\gamma$. Moreover, we provide the first procedure for computing a fully data-dependent interval, from a single finite-length trajectory of the chain, that traps the mixing time $t_{\\text{mix}}$ of the chain at a prescribed confidence level. The interval does not require the knowledge of any parameters of the chain. This stands in contrast to previous approaches, which either only provide point estimates, or require a reset mechanism, or additional prior knowledge. The interval is constructed around the relaxation time $t_{\\text{relax}} = 1/\\gamma$, which is strongly related to the mixing time, and the width of the interval converges to zero roughly at a $1/\\sqrt{n}$ rate, where $n$ is the length of the sample path.", "histories": [["v1", "Thu, 24 Aug 2017 12:05:11 GMT  (43kb)", "http://arxiv.org/abs/1708.07367v1", "34 pages, merges results ofarXiv:1506.02903andarXiv:1612.05330"]], "COMMENTS": "34 pages, merges results ofarXiv:1506.02903andarXiv:1612.05330", "reviews": [], "SUBJECTS": "math.ST cs.LG math.PR stat.ML stat.TH", "authors": ["daniel j hsu", "aryeh kontorovich", "csaba szepesv\u00e1ri"], "accepted": true, "id": "1708.07367"}, "pdf": {"name": "1708.07367.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 8.07 367v 1 [mat h.ST] 2 4A ug (1 \u03b3 \u0445 \u03c0), then \u03b3 can be estimated within multiplicative constants with a high probability. If \u03c0 is uniform on d states, this corresponds (until logarithmic correction) to a lower limit of the steps required for a precise estimation of the \u03b3 value. Furthermore, we provide the first method for calculating a fully data-dependent interval from a single finite orbit of the chain, which keeps the chain mix time mixture trapped at a prescribed confidence level. The interval does not require knowledge of any parameters of the chain, unlike previous approaches, which either provide point estimates or require a reset mechanism or additional foreknowledge. The interval is constructed around the relaxation time relaxation = 1 / 3, which is strongly related to the mix time, and the width of the interval converges to approximately zero or additional knowledge."}, {"heading": "1. Introduction", "text": "This paper addresses the challenge of constructing confidence intervals for the mixing time of reversible Markov chains on the basis of a single sample. Let (Xt) t = 1,2,... be an irreducible, aperiodic time-homogeneous Markov chain on a finite state distribution [d]: 1, 2,., d) with transition matrix P =. Under this assumption, the chain converges to its unique stationary distribution. The mixing time tmix of the Markov chain is the number of time steps required for the chain to be within a fixed threshold of its stationary distribution."}, {"heading": "2. Preliminaries", "text": "The non-negative part of a real number x is [x] +: = max {0, x}, and [x], and [x], and [x], [x], and [x], and [x], and [x], and [x], and [x], and [x], and [x], and [x], and [x], and [x], and [x], and [x], and [x], and [x], and [x], and [x], and [x], and [x], and [x], and [x], and [x]. We use [x], and their entries are referenced by subindexidation (e.g. vi, j). For a vector v that describes its euclidean norm; for a matrix M, [M] denotes its spectral norm. We use Diag (v) to diagonal matrix."}, {"heading": "3. Point estimation", "text": "The purpose of this section is to show lower limits on the number of observations necessary to achieve a fixed multiplicative (or even only additive) accuracy in estimating the spectral gap. Our first result even applies to two state-Markov chains and shows that a sequence length of the spectral gap (1 / 2) is necessary to achieve even a constant additive accuracy in estimating probability."}, {"heading": "4. A posteriori confidence intervals", "text": "In this section we describe and analyze an approach to constructing confidence intervals for the stationary probability and the spectral gap, which can only be achieved using the following confidence intervals (maximum confidence intervals) (maximum confidence intervals) (maximum confidence intervals) (maximum confidence intervals) (maximum confidence intervals) (maximum confidence intervals): 1 (maximum confidence intervals), 2 (maximum confidence intervals), 2 (maximum confidence intervals), 2 (maximum confidence intervals), 2 (maximum confidence intervals), 2 (maximum confidence intervals), 2 (maximum confidence intervals), 2 (maximum confidence intervals), 2 (maximum), 2 (maximum), 2 (maximum), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 1 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2 (2), 2 (2 (2), 2 (2), 2 (2 (2), 2 (2 (2), 2 (2), 2 (2 (2), 2 (2), 2 (2 (2), 2 (2), 2 (2 (2), 2 (2), 2 (2 (2), 2 (2 (2), 2 (2), 2), 2 (2 (2), 2 (2 (2), 2 (2), 2), 2 (2 (2), 2 (2)"}, {"heading": "5. Proofs of Theorems 3.1 and 3.2", "text": "In this section we prove Theorem 3.1 and Theorem 3.2.5.1. Evidence of Theorem 3.1. < p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p p p? p? p? p p p? p? p p p p? p p? p p p p p? p p p? p p p? p p? p p p? p p p p p? p p p? p p p p p? p p? p p p? p p p? p? p p p? p p p p? p? p p p p p? p p p p? p? p p p p p p? p p p? p p p p p? p p p? p? p p p p p? p p? p? p p? p p p p p? p p p? p p p p p? p p p p p? p? p? p p p p p p p p p p? p p? p p p p? p p p p p p? p p? p p p p p p? p p p? p"}, {"heading": "6. Proof of Theorem 3.3", "text": "In this section we prove that the marginal distribution of each Xt values in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any case in any"}, {"heading": "7. Proof of Theorem 3.4", "text": "In this section we prove that theorem 3.4."}, {"heading": "8. Proof of Theorem 4.1", "text": "In this section, we deduce algorithm 1 and prove theorem 4.1.8.1 = > Estimators for \u03c0 / 2 = > Estimators for a >. The algorithm forms the cost carrier P + 1: (Xt, Xt + 1) = (i, j \u2212 \u2212 n, j \u2212 n, j \u2212 n, dNi + dNi, j \u2212 n, j \u2212 n, that is, as a positive constant that we previously gave as a result of the smoothing, all entries of P \u2212 n are positive, and hence P is a transition probability for an ergodic Markov chain. Let us assume that the uniquestational distribution for P n, we form an estimator sym (L)."}, {"heading": "9. Proof of Theorem 4.2", "text": "Let us know that this is an easy solution. (Let us know). (Let us know.) Let us know. (Let us know.) Let us know. (Let us know.) (Let us know.). (Let us know.) (Let us know.) (Let us know.). (Let us know.). (Let us know.). (Let us know.). (Let us know.). (Let us know.). (Let us know.). (Let us know.). (Let us know.). (Let us know.). (Let us know.). (Let us know.). (Let us know.). (Let us know.). (Let us know.). (Let us know.). (Let us know.). (Let us know.). (Let us know.........................................................................)................................................)...............................................................................)"}], "references": [{"title": "Rates of uniform convergence", "author": ["R.L. Karandikar", "M. Vidyasagar"], "venue": null, "citeRegEx": "Karandikar and Vidyasagar,? \\Q2002\\E", "shortCiteRegEx": "Karandikar and Vidyasagar", "year": 2002}, {"title": "An improvement on the perturbation of the group inverse", "author": ["ical Society", "Providence", "X. RI. Li", "Y. Wei"], "venue": null, "citeRegEx": "Society et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Society et al\\.", "year": 2001}, {"title": "The Role of the Group Generalized Inverse in the Theory", "author": ["C.D. Meyer Jr."], "venue": "ficients\u201d. In: AISTATS,", "citeRegEx": "Jr.,? \\Q1975\\E", "shortCiteRegEx": "Jr.", "year": 1975}, {"title": "Stability bounds for non-iid processes", "author": ["M. Springer. Mohri", "A. Rostamizadeh"], "venue": null, "citeRegEx": "Mohri and Rostamizadeh,? \\Q2008\\E", "shortCiteRegEx": "Mohri and Rostamizadeh", "year": 2008}, {"title": "Sensitivity of finite Markov chains under perturbation", "author": ["E. Seneta"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1993}, {"title": "Fast Learning from Non-i.i.d", "author": ["I. Steinwart", "A. Christmann"], "venue": "Statistics & Probability Letters", "citeRegEx": "Steinwart and Christmann,? \\Q2009\\E", "shortCiteRegEx": "Steinwart and Christmann", "year": 2009}, {"title": "Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning)", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "A Bradford Book. isbn:", "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}], "referenceMentions": [{"referenceID": 6, "context": "Liu 2001), but the problem also arises in performance prediction involving time-correlated data, as is common in reinforcement learning (Sutton and Barto 1998).", "startOffset": 136, "endOffset": 159}, {"referenceID": 0, "context": "Many results from statistical learning and empirical process theory have been extended to sufficiently fast mixing, dependent data (e.g., Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.", "startOffset": 131, "endOffset": 287}, {"referenceID": 3, "context": "Many results from statistical learning and empirical process theory have been extended to sufficiently fast mixing, dependent data (e.g., Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.", "startOffset": 131, "endOffset": 287}, {"referenceID": 5, "context": "Many results from statistical learning and empirical process theory have been extended to sufficiently fast mixing, dependent data (e.g., Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.", "startOffset": 131, "endOffset": 287}, {"referenceID": 0, "context": ", Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.g., generalization error bounds). These results are often given in terms of mixing coefficients, which can be consistently estimated in some cases (McDonald, Shalizi, and Schervish 2011). However, the convergence rates of the estimates from McDonald, Shalizi, and Schervish (2011), which are needed to derive confidence bounds, are given in terms of unknown mixing coefficients.", "startOffset": 11, "endOffset": 471}, {"referenceID": 0, "context": ", Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.g., generalization error bounds). These results are often given in terms of mixing coefficients, which can be consistently estimated in some cases (McDonald, Shalizi, and Schervish 2011). However, the convergence rates of the estimates from McDonald, Shalizi, and Schervish (2011), which are needed to derive confidence bounds, are given in terms of unknown mixing coefficients. When the data comes from a Markov chain, these mixing coefficients can often be bounded in terms of mixing times, and hence our main results provide a way to make them fully empirical, at least in the limited setting we study. It is possible to eliminate many of the difficulties presented above when allowed more flexible access to the Markov chain. For example, given a sampling oracle that generates independent transitions from any given state (akin to a \u201creset\u201d device), the mixing time becomes an efficiently testable property in the sense studied by Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2000), Batu, Fortnow, Rubinfeld, W.", "startOffset": 11, "endOffset": 1182}, {"referenceID": 0, "context": ", Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.g., generalization error bounds). These results are often given in terms of mixing coefficients, which can be consistently estimated in some cases (McDonald, Shalizi, and Schervish 2011). However, the convergence rates of the estimates from McDonald, Shalizi, and Schervish (2011), which are needed to derive confidence bounds, are given in terms of unknown mixing coefficients. When the data comes from a Markov chain, these mixing coefficients can often be bounded in terms of mixing times, and hence our main results provide a way to make them fully empirical, at least in the limited setting we study. It is possible to eliminate many of the difficulties presented above when allowed more flexible access to the Markov chain. For example, given a sampling oracle that generates independent transitions from any given state (akin to a \u201creset\u201d device), the mixing time becomes an efficiently testable property in the sense studied by Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2000), Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2013), and Bhattacharya and Valiant (2015).", "startOffset": 11, "endOffset": 1239}, {"referenceID": 0, "context": ", Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.g., generalization error bounds). These results are often given in terms of mixing coefficients, which can be consistently estimated in some cases (McDonald, Shalizi, and Schervish 2011). However, the convergence rates of the estimates from McDonald, Shalizi, and Schervish (2011), which are needed to derive confidence bounds, are given in terms of unknown mixing coefficients. When the data comes from a Markov chain, these mixing coefficients can often be bounded in terms of mixing times, and hence our main results provide a way to make them fully empirical, at least in the limited setting we study. It is possible to eliminate many of the difficulties presented above when allowed more flexible access to the Markov chain. For example, given a sampling oracle that generates independent transitions from any given state (akin to a \u201creset\u201d device), the mixing time becomes an efficiently testable property in the sense studied by Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2000), Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2013), and Bhattacharya and Valiant (2015). Note that in this setting, Bhattacharya and Valiant (2015) asked if one could approximate tmix (up to logarithmic factors) with a number of queries that is linear in both d and tmix; our work answers the question affirmatively (up to logarithmic corrections) in the case when the stationary distribution is near uniform.", "startOffset": 11, "endOffset": 1276}, {"referenceID": 0, "context": ", Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.g., generalization error bounds). These results are often given in terms of mixing coefficients, which can be consistently estimated in some cases (McDonald, Shalizi, and Schervish 2011). However, the convergence rates of the estimates from McDonald, Shalizi, and Schervish (2011), which are needed to derive confidence bounds, are given in terms of unknown mixing coefficients. When the data comes from a Markov chain, these mixing coefficients can often be bounded in terms of mixing times, and hence our main results provide a way to make them fully empirical, at least in the limited setting we study. It is possible to eliminate many of the difficulties presented above when allowed more flexible access to the Markov chain. For example, given a sampling oracle that generates independent transitions from any given state (akin to a \u201creset\u201d device), the mixing time becomes an efficiently testable property in the sense studied by Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2000), Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2013), and Bhattacharya and Valiant (2015). Note that in this setting, Bhattacharya and Valiant (2015) asked if one could approximate tmix (up to logarithmic factors) with a number of queries that is linear in both d and tmix; our work answers the question affirmatively (up to logarithmic corrections) in the case when the stationary distribution is near uniform.", "startOffset": 11, "endOffset": 1336}, {"referenceID": 0, "context": ", Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.g., generalization error bounds). These results are often given in terms of mixing coefficients, which can be consistently estimated in some cases (McDonald, Shalizi, and Schervish 2011). However, the convergence rates of the estimates from McDonald, Shalizi, and Schervish (2011), which are needed to derive confidence bounds, are given in terms of unknown mixing coefficients. When the data comes from a Markov chain, these mixing coefficients can often be bounded in terms of mixing times, and hence our main results provide a way to make them fully empirical, at least in the limited setting we study. It is possible to eliminate many of the difficulties presented above when allowed more flexible access to the Markov chain. For example, given a sampling oracle that generates independent transitions from any given state (akin to a \u201creset\u201d device), the mixing time becomes an efficiently testable property in the sense studied by Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2000), Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2013), and Bhattacharya and Valiant (2015). Note that in this setting, Bhattacharya and Valiant (2015) asked if one could approximate tmix (up to logarithmic factors) with a number of queries that is linear in both d and tmix; our work answers the question affirmatively (up to logarithmic corrections) in the case when the stationary distribution is near uniform. Finally, when one only has a circuit-based description of the transition probabilities of a Markov chain over an exponentially-large state space, there are complexity-theoretic barriers for many MCMC diagnostic problems (Bhatnagar, Bogdanov, and Mossel 2011). This paper is based on the conference paper of Hsu, Kontorovich, and Szepesv\u00e1ri (2015), combined with the results in the unpublished manuscript of Levin and Peres (2016).", "startOffset": 11, "endOffset": 1945}, {"referenceID": 0, "context": ", Yu 1994; Karandikar and Vidyasagar 2002; Gamarnik 2003; Mohri and Rostamizadeh 2008; Steinwart and Christmann 2009; Steinwart, Hush, and Scovel 2009), providing learnability assurances (e.g., generalization error bounds). These results are often given in terms of mixing coefficients, which can be consistently estimated in some cases (McDonald, Shalizi, and Schervish 2011). However, the convergence rates of the estimates from McDonald, Shalizi, and Schervish (2011), which are needed to derive confidence bounds, are given in terms of unknown mixing coefficients. When the data comes from a Markov chain, these mixing coefficients can often be bounded in terms of mixing times, and hence our main results provide a way to make them fully empirical, at least in the limited setting we study. It is possible to eliminate many of the difficulties presented above when allowed more flexible access to the Markov chain. For example, given a sampling oracle that generates independent transitions from any given state (akin to a \u201creset\u201d device), the mixing time becomes an efficiently testable property in the sense studied by Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2000), Batu, Fortnow, Rubinfeld, W. D. Smith, and White (2013), and Bhattacharya and Valiant (2015). Note that in this setting, Bhattacharya and Valiant (2015) asked if one could approximate tmix (up to logarithmic factors) with a number of queries that is linear in both d and tmix; our work answers the question affirmatively (up to logarithmic corrections) in the case when the stationary distribution is near uniform. Finally, when one only has a circuit-based description of the transition probabilities of a Markov chain over an exponentially-large state space, there are complexity-theoretic barriers for many MCMC diagnostic problems (Bhatnagar, Bogdanov, and Mossel 2011). This paper is based on the conference paper of Hsu, Kontorovich, and Szepesv\u00e1ri (2015), combined with the results in the unpublished manuscript of Levin and Peres (2016).", "startOffset": 11, "endOffset": 2028}], "year": 2017, "abstractText": "The spectral gap \u03b3\u22c6 of a finite, ergodic, and reversible Markov chain is an important parameter measuring the asymptotic rate of convergence. In applications, the transition matrix P may be unknown, yet one sample of the chain up to a fixed time n may be observed. We consider here the problem of estimating \u03b3\u22c6 from this data. Let \u03c0 be the stationary distribution of P , and \u03c0\u22c6 = minx \u03c0(x). We show that if n = \u00d5 ( 1 \u03b3\u22c6\u03c0\u22c6 ) , then \u03b3 can be estimated to within multiplicative constants with high probability. When \u03c0 is uniform on d states, this matches (up to logarithmic correction) a lower bound of \u03a9\u0303 ( d \u03b3\u22c6 ) steps required for precise estimation of \u03b3\u22c6. Moreover, we provide the first procedure for computing a fully data-dependent interval, from a single finitelength trajectory of the chain, that traps the mixing time tmix of the chain at a prescribed confidence level. The interval does not require the knowledge of any parameters of the chain. This stands in contrast to previous approaches, which either only provide point estimates, or require a reset mechanism, or additional prior knowledge. The interval is constructed around the relaxation time trelax = 1/\u03b3\u22c6, which is strongly related to the mixing time, and the width of the interval converges to zero roughly at a 1/ \u221a n rate, where n is the length of the sample path.", "creator": "Creator"}}}